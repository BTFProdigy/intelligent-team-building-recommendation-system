The Automated Acquisit ion of Topic Signatures for Text 
Summarizat ion 
Chin -Yew L in  and  Eduard  Hovy  
In fo rmat ion  S(:i(umes I l l s t i tu te  
Un ivers i ty  of Southern  Ca l i fo rn ia  
Mar ina  del Rey, CA  90292, USA 
{ cyl,hovy }C~isi.edu 
Abst rac t  
In order to produce, a good summary, one has 
to identify the most relevant portions of a given 
text. We describe in this t)at)er a method for au- 
tomatically training tel)it, signatures--sets of related 
words, with associated weights, organized around 
head topics and illustrate with signatm'es we cre- 
;tt.ed with 6,194 TREC collection texts over 4 se- 
lected tot)ics. We descril)e the l)ossible integration 
of' tolli(: signatures with ontoh)gies and its evaluaton 
on an automate(l text summarization system. 
1 I n t roduct ion  
This t)aper describes the automated (:reation of what 
we call topic signatures, constructs that can I)lay a 
central role. in automated text summarization and 
information retrieval. ToI)ic signatures can lie used 
to identify the t)resence of a (:omph~x conce.pt a 
concept hat consists of several related coinl)onents 
in fixed relationships. \]~.c.sta'uvant-'uisit, for examph~, 
invoh,es at h,ast the concel)ts lltCgFIt, t'.(tt, pay, and 
possibly waiter, all(l Dragon Boat PcstivaI (in Tat- 
wan) involves the Ct)llC(!l)t,S cal(tlzt'lt,s (a talisman to 
ward off evil), rnoza (something with the t)ower of 
preventing pestilen(:e and strengthening health), pic- 
tures of Ch, un9 Kuei (a nemesis of evil spirits), eggs 
standing on end, etc. Only when the concepts co- 
occur is one licensed to infer the comph:x concept; 
cat or moza alone, for example, are not sufficient. At 
this time, we do not c.onsider the imerrelationships 
among tile concepts. 
Since many texts may describe all the compo- 
nents of a comI)lex concept without ever exI)lic- 
itly mentioning the mlderlying complex concel/t--a 
tol)ic--itself, systems that have to identify topic(s), 
for summarization or information retrieval, require 
a method of infcu'ring comt)h'x concelltS fl'om their 
component words in the text. 
2 Re la ted  Work  
In late 1970's, \])e.long (DeJong, 1982) developed a
system called I"tIUMP (Fast Reading Understand- 
ing and Memory Program) to skim newspaper sto- 
ries and extract the main details. FRUMP uses 
a data structure called sketchy script to organize 
its world knowh'dge. Each sketchy script is what 
FRUMI ) knows al)out what can occur in l)articu- 
lar situations such as denmnstrations, earthquakes, 
labor strike.s, an(t so on. FRUMP selects a t)artic- 
ular sketchy script based on clues to styled events 
in news articles. In other words, FRUMP selects an 
eml)t3 ~ t(uni)late 1whose slots will be tilled on the fly 
as t"F\[UMP reads a news artMe. A summary is gen- 
erated })ased on what has been (:al)tured or filled in 
the teml)Iate. 
The recent success of infornmtion extractk)n re- 
search has encore'aged the FI{UM1 ) api)roach. The 
SUMMONS (SUMMarizing Online News artMes) 
system (McKeown and Radev, 1999) takes tem- 
l)late outputs of information extra(:tion systems de- 
velofmd for MUC conference and generating smn- 
maries of multit)le news artMes. FRUMP and SUM- 
MONS both rely on t/rior knowledge of their do- 
mains, th)wever, to acquire such t)rior knowledge 
is lal)or-intensive and time-consuming. I~)r exam-- 
l)le, the Unive.rsity of Massa(:husetts CIRCUS sys- 
l.enl use(l ill the MUC-3 (SAIC, 1998) terrorism do- 
main required about 1500 i)erson-llours to define ex- 
traction lmtterns 2 (Rilotf, 1996). In order to make 
them practical, we need to reduce the knowhxlge n- 
gineering bottleneck and iml)rove the portability of 
FI{UMI ) or SUMMONS-like systems. 
Since the worhi contains thousands, or perhal)s 
millions, of COml)lex (:on(:et)ts , it is important; to be 
able to learn sketchy scripts or extraction patterns 
automatically from corpora -no existing knowledge 
base contains nearly enough information. (Rilotf aim 
Lorenzen, 1999) 1)resent a system AutoSlog-TS that 
generates extraction i)atterns and learns lexical con- 
straints automatically fl'om t)rec\]assified text to al- 
leviate the knowledge ngineering I)ottleneck men- 
tioned above. Although Riloff al)plied AutoSlog-TS 
l\Ve viewed sketchy s(:lil)tS and teml)lates as equivalent 
(ollstrllctS ill the sense that they sl)ecil ~, high level entities 
and relationships for specific tot)its. 
2Aii extra(:l;iOll pattt!rlk is essentially ;t case fraine contains 
its trigger word, enabling conditions, variable slots, and slot 
constraints. C IRCUS uses a database of extraction patterns 
to t~alSe texts (l{ilolI', 1996). 
495 
to text categorization and information extraction, 
the concept of relevancy signatures introduced by 
her is very similar to the topic si.qnatures we pro- 
posed in this paper. Relevancy signatures and topic 
signatures arc both trained on preclassitied ocu- 
ments of specific topics and used to identify the 
presence of the learned topics in previously unseen 
documents. The main differences to our approach 
are: relevancy signatures require a parser. They are 
sentence-based and applied to text categorization. 
On the contrary, topic signatures only rely on cor- 
pus statistics, arc docmnent-based a and used in text 
smnmarization. 
In the next section, we describe the automated 
text smmnarization system SUMMARIST that we 
used in the experiments to provide the context of 
discussion. We then define topic signatures and de- 
tail the procedures for automatically constructing 
topic signatures. In Section 5, we give an overview 
of the corpus used in the evaluation. In Section 6 we 
present he experimental results and the possibility 
of enriching topic signatures using an existing ontol- 
ogy. Finally, we end this paper with a conclusion. 
3 SUMMARIST  
SUMMARIST (How and Lin, 1999) is a system 
designed to generate summaries of multilingual in- 
put texts. At this time, SUMMARIST can process 
English, Arabic, Bahasa Indonesia, Japanese, Ko- 
rean, and Spanish texts. It combines robust natural 
language processing methods (morl)hologieal trans- 
formation and part-of-speech tagging), symbolic 
world knowledge, and information retrieval tech- 
niques (term distribution and frequency) to achieve 
high robustness and better concept-level generaliza-- 
tion. 
The core of SUMMARIST is based on the follow- 
ing 'equation!: 
summarization = topic identification + 
topic interpretation + generation. 
These three stages are: 
Topic Ident i f ieat lon:  Identify the most imtmrtant 
(central) topics of the texts. SUMMARIST 
uses positional importance, topic signature, and 
term frequency. Importance based on discourse 
structure will be added later. This is tile most 
developed stage in SUMMARIST. 
Topic I n te rpretat ion :  ~i~-) fllse concepts such as 
waiter, menu, and food into one generalized 
concept restaurant, we need more than the sin> 
pie word aggregation used in traditional infor- 
mation retrieval. We have investigated concept 
aWe would like to use only the relevant parts of documents 
to generate topic signatures in the future, qkext segmentation 
algorithms uch as TextTiling (Ilearst, 1997) can be used to 
find subtopic segments in text. 
ABCNEWS.cona  : De lay  in  Hand ing  F l ight  990  \ [ ' robe  to  FB I  
N'I 'SI3 C l la i tn lan  Jarl leS t la l l  says  Egypt ian  clff icials Iv811l to  I,~view res t l l t s  
of  t i le  invest igat ion  intcl lhe  cras l l  o f  l lggyptA i r  F l ight  990 before  t i le  case  
i~ lu r l led  over  Ic, t i le Fi31, 
Nt lv.  IG - U S. i lxvestigl~lo\[~ lLppear to  be leat l i l lg i i Iore thg l l  eveF low~trd 
t i le poss ib i l i ty  that  one  o f  the  cc~-pilot~ o f  EgyptA i r  F l ight  990 may have  
de \ [ ihera le ly  c rashed t i le p lane  las t  I lafl l lth, k i l l i ng  all 217 peop le  on  board .  
f la i l ' ever .  US .  o f f i c ia ls  say  t i le  Nat iona l  T ran~por ' ta t ion  Sa fety  Board  wi l l  
de lay  t rans fer r ing  tile invegt iga l ion  o f  the  Oct  31 c rash  to  tilt: FI31 - the  
agency  that  wot l id  lead i~ c r imina l  p robe  - for at  least  tt few days .  to  M Iow 
Egypt ian  exper ts  to rev iew ev idence  ill t i le case.  
gtts l ) ic iot l~ of  fou l  p lay  were  ra i sed  a f te r  invest igators  l i s ten ing  to  rt tape  
ftol l l  l i l t  cockp i t  vo ice recorder  i so la ted  a re l ig ious  prayer  or s ta te l l l e l l t  
made by t i le co -p i lo t  jus t  be fore  t i le  p lane 's  autop i lo t  was turned  o f f  
s l id  the  p lane  began i ts  in i t ia l  p lunge  in to  t i le A t lant i c  Ocean of f  Mas -  
s r tcht tset t$ '  Na l l tucket  \ [s ia l ld .  
Over  tile' pas t  week .  a f te r  muc i l  e f fo r t ,  t i le  NTSJB and  t i le  Navy  succeeded 
ill I ocat i l lg  the  p lane 's  two  "b lack  boxes , "  th~ cockp i t  vo ice recorder  and  
lhe  f l ight  data  recorder .  
The  tape  ind icates  t l l a t  shor t ly  a f te r  the  p lane  leve led ~ff  a t  i ts c ru i s ing  
a l t i tude  o f  as ,000  feet ,  t i le  cl~ief p i lo t  o f  t i le a i rc ra f t  left  the  p lane 's  
cockp i t ,  l eav ing  one  o f  t i le  twc~ co-p i lo ts  nIol le t i lere as the  a i rc ra f t  began 
its descent .  
Figure 1: A Nov. 16 1999 ABC News page sumnmry 
generated by SUMMARIST. 
counting and topic signatures to tackle tile fll- 
sion problem. 
Summary Generat ion :  SUMMARIST can pro- 
duce keyword and extract type summaries. 
Figure 1 shows an ABC News page summary 
about EgyptAir Flight 990 by SUMMARIST. SUM- 
MARIST employs several different heuristics in tile 
topic identification stage to score terms and sen- 
tences. The score of a sentence is simply the sum 
of all the scores of content-bearing terms in the sen- 
tence. These heuristics arc implemented in separate 
modules using inputs from preprocessing modules 
such as tokenizer, part-of-speech tagger, morpholog- 
ical analyzer, term frequency and tfidf weights cal- 
culator, sentence length calculator, and sentence lo- 
cation identifier. \Ve only activate the position mod- 
ule, tile tfidfmodule, and the. topic signature module 
for comparison. We discuss the effectiveness of these 
modules in Section 6. 
4 Top ic  S ignatures  
Before addressing the problem of world knowledge 
acquisition head-on, we decided to investigate what 
type of knowledge would be useflfl for summariza- 
tion. After all, one can spend a lifetime acquir- 
ing knowledge in just a small domain. But what 
is tile minimum amount of knowledge we need to 
enable effective topic identification ms illustrated by 
the restaurant-visit example? Our idea is simple. 
We would collect a set of terms 4 that were typi- 
cally highly correlated with a target concept from a 
preclassified corpus such as TREC collections, and 
then, during smnmarization, group the occurrence of 
the related terms by the target concept. For exam- 
pie, we would replace joint instances of table, inert'u, 
waiter, order, eat, pay, tip, and so on, by the single 
phrase restaurant-visit, in producing an indicative 
4Terms can be stemmed words, bigrams, or trigrams. 
496 
sulnlllary. \Ve thus defined a tot)it signat.ure as a 
family of related terms, as follows: 
~I'S = { topic, sifl~zutu.rc. } 
= {topic,< ( t , ,w l ) , . . . , ( t , , ,w , , )  >} (1) 
where topic is the target concet)t and .,d.q)zat~Lrc is 
a vector of related ternls. Each t, is an term ldghly 
correlated to topic with association weight w/. The 
number of related terms 7z can tie set empirically 
according to a cutot\[ associated weight. We. describe 
how to acquire related terms and their associated 
weights in the next section. 
4.1  S ignature  Term Ext rac t ion  and  Weight  
Es t imat ion  
()n the assumption that semantically related terms 
tend to co-occur, on(' can construct topic signa- 
tures fl'om preclassified text using the X 2 test, mu-. 
tual information, or other standard statistic tests 
and infornlation-theoreti(: measures. Instead of X '2, 
we use likclih.ood ratio (Dunniug, 1993) A, sin(:e A 
i,; more apI)rot)riate for si/arse data than X 2 test 
and the quantity -21o9A is asymi)t(/tically X~ dis- 
tril)ute(15. Therefore, we Call (leterndnc the (:onti- 
( lence level for a specific -21o9A value l/y looking ut) 
X :~ (tistril)ution table and use tlm value to sel(,,ct an 
at)i)rot)riate cutoff associated weight. 
We have documents l)\['e.classitied into a :;('~t, "R. of 
relevant exts and a set ~. of nonrelewmt exl;s for a 
given topic. Assuming the following two hyl)othe,'~es: 
t typothes is  1 ( I f l ) :  t'(~Pvlti) = P = P('PvltT/), i.e. 
the r(.,lewmcy of a d()(:|lment is in(teI)en(hmt, of 
t i .  
I \ ] \ [ypothes is  2 ( t t2) :  I'('Pv\[ti) == lh ~ 1)'2 - 
t)('Pvlt, i), i.e. th(~ t)r(.':;(;n(:(~ of t i indi(:~Lt(.~.'; strong 
r(~levan(:y ~ssunling \]h >> 1)2 ?
and the following 2-10=2 contingency tabl(;: 
where Ol~ is the fiequency of term ti occurring in 
the. l 'e lev;tnt  set ,  012 is the \ [ r ( !qu(nlcy of Lerm t i t)c- 
curring in the  \] lol lreleval lt ,  set ,  O21 is the  f le(l l lel l( :y 
of tt;rnl \ [ i?  ti occurring in the rtdevant set, O._,~ is 
the fl'equ(mcy of term l.i ? ti o(:curring in the non- 
l ' e leva i i t  seL.  
-kssmning a l)inomial distril)ution: 
C;) b(~; ,,., :/.) = :,:~(1 - .~:)(" ") (2 )  
5This assumes |ha l  the ratio is between the inaximuni like> 
\[ihood est, im&t.(! over a .qll})part of l;}l(! i)alatlllC't(~r sl)a(:(~ ;tll(\] l.h(! 
lllaxillUllll likelihood (}sI.i|II}tlA~ ov(!r the (Hltill! i)al'aillt~tt!r si);t(:e. 
Set! (Manning ;tnd Sch/itze, I999) t)ag, es 172 l.o 175 for d(!t.ails. 
then the likelihood for HI is: 
L(H~) = b(Ot~; 0~ + Ou,,p)b(O:,~; 0:,, + Om,,p) 
and for //2 is: 
L(H2)  = D(OI 1; O11 Jr" (')12, Pl )b(O21; (')21 Jr- (,)22,1)2) 
The -2log,\ value is then computed ms follows: 
1.(f/1 ) 
m --21o 9 - -  
L( i t  2 ) 
b(O 11 ; O I  1 + O12,  P) I J (021 : O21 + 022 , P) 
- -21o 9 
1'((-)1 l ; ( )11  + O1'-),  P I )h (O21 ; O21 q- ( )22  , P2 ) 
: - -2 ( (O l l  +021) lo r_ Jp+( ( )12+022) lo9(1 - - l~) - -  (,~1) 
(? ) l l l o ' JP l+Ol21og( l  " t '1 )+0211ogp2-~0221o0(1- f~2) ) )  
-- '.2.,~' x (~ ' i (7~) -  ;~(~19- ) )  (4 )  
= 2,'v x Z(P~;  T )  (5 )  
whel 'e  N = O l t  -F O12 -1- O21 -I- 022 is the  to ta l  l lum-. 
her of t, ernl occurrence, in the corpus, 7/('/~) is the 
entropy of terms over relevant and nonrelevant sets 
of documents, 7/ ( ' fe l t  ) is the entropy of a given term 
OVel" relev;inL ~/nd nonl ' ( .qeval l t  sets  of doel l inel lLS, ~tll(1 
Z('R.; T) i:; the inutual information between docu- 
ment relevancy and a given t('.rm. Equation 5 indi- 
cates that mutual inforntation 6 is an e(tuiwdent mea- 
sur(.' t() lik(.qiho(id ratio when we assume a binomial 
distribution and a 2-by-2 ('ontingency table. 
To crest(; topic .~dgnature for a given tot)ic , we: 
1. (:lassify doctunents as relevant or nonrclcwmt 
according t() tile given topic 
2. comt)ut.e the -21oflA wdue using Equation 3 for 
each Lcrm in the document colle(:Lion 
"{. rank t, erms according 1o their -2lo9~ value 
4. select a c(mfid(mce l , vel fi'om the A;: (listril)utiotl 
table; (letermin(~ the cutotf associated weight, 
mid the numl)(n" of t(nms to he included iIl the 
signatures 
5 The Corpus  
The training data derives Kern the Question and 
Answering summary evahmtion data provided l)y 
T IPSTEI / . -SUMMAC (Mani et al, 1998) that is a 
sttbset of the TREC collectioliS. The TREC data is a 
collection of texts, classified into various topics, used 
for formal ewduaLions of information retrieval sys- 
tems in a seri(~s of annual (:omparisons. This data set: 
contains essential text fragnients (phrases, (:Iausos, 
iuld sentences) which must 1)e included in SUllltIlarios 
to ~tnswer some TI{EC tel)its. These fl'agments are 
each judged 1)y a hmnan judge. As described in Se(:- 
tion 3, SUMMAI~IST employs several independent 
nlo(hlles to assign a score to each SelltA:llCe~ and Chell 
COlll})illeS the st.'or(.'.% L() decide which sentences to ex- 
tract from the input text;. ()n0. can gauge the efticacy 
(>l'he lllll\[lla} inrormalion is defined according to chapter 2 
of ((;over and Thomas, i991) and is not tile i)airwis(~ mutual 
inforlnalion us(!d in ((;hur(:h and llanks, 1990). 
497 
TREC Top ic  Da~cr lp t ion  
(nunQ Number :  151 
( t i t le}  Top ic :  Co, p ing  w i th  overc rowded pr i sons  
(dese} Deser i l l t io l l :  
The  doeu l laent  will p rov ide  in f ,~rn lat ion ol~ jai l  and  pr ison overc rowd iuK  
and  how i r lmates  are forced to cope  wi th  th,~se cond i t ions ;  or it wil l  
revea l  p lan~ to  re l ieve  ti le overc rowded ?o l ld i t lon .  
(nar t )  Nar ra t ive :  
A re levant  docunaent  will descr ibe  scene~ of overcro~vdi l lg that  have  
beco lne  all too  crlllllllOll ill ja i l s  and  pr i sons  a ro t tnd  the  count ry ,  T i le  
document  will i dent i fy  how inmates  are forced to  cope w i th  those  over -  
crowded cond i t ion~,  and/or  what  ti le Cc l r reet iona l  Syste l l l  is do ing ,  or 
ph lnn ing  to do,  to a l lev ia te  ti le c rowded col ld i t io l l .  
(/top) 
Test  Quest ions  
QI  Me'hat are  name and/or  locat ion  of ti le cor rec t ion  fae i l i l ies  
where  the  repor ted  ~vercrowd ing  ex is ts?  
Q2 x~Vhat negat ive  exper iences  have  there  been  at t i le overc rowded 
fac i l i t ies  (whether  or not tile)" are thought  to have  been  caused  
by  the  overc rowd lng)?  
Q3 What  measures  have  been  taken/p la ia i led / recommended (e tc . )  
to aecon l lnod~te  more  i l l l l la Ies zlt pena l  fac i l i t ies ,  e .g . ,  doub l i l l g  
tip, Ile~y COllStructlon? 
Q,I ~,Vhat measures  have  been  taken/planned/rec~mnlel,ded (etc .} 
to reduce  ti le l lt l l l lber of Dew il l l i \]ate$, e .g . ,  morator iums 
on admisMon,  a l te rnat ive  pena l t ies ,  p rograme to reduce  
c r ime/ rec ld iv i sm? 
Q5 What  measures  have  been  taken/p lanned/ recommended (e tc . )  
to reduce  ti le number  of ex i s t ing  inmates  at an overcrc~wded 
fac i l i ty ,  e .g . .  g rant ing  ear ly  re lease ,  t rnns fer ing  to  uncrowded 
fac i l i t ies?  
Sample  Answer  Keys  
(DOCNO)  AP891027-0063 ( /DOCNO)  
(F ILE ID)  AP -NR-  10-27-89 0615EDT( /F ILE ID)  
( IST_L INE) r  a PM-Cha ined Inmates  10-27 0335( / IST .L INE)  
(2ND-L INE)PM-Cha ined  lnmates ,0344 ( /2ND_L INE)  
( I IEAD)  lnmates  Cha ined  to 1.Vails in 13Mtimore Po l i ce  
S ta t ions ( / l lEAD)  
(DATEL INE)BALT IMOIT IE  (AP)  ( /DATEL INE} 
('tEXT) 
(Q ,q )pr i soner~ are  kept  cha ined  to the wall~ of local po l ice  lcJekup~ for 
as long as th ree  days  at a t ln~e I)ecattse of overc rowd ing  ill regu la r  je l l  
cel ls ,  pol ice sa id . ( /Q3} 
Overcrowd ing  at  the  (Q1) lqMt l rnore  County  Detent ion  Center ( /Q1)  
h~ forced pn l lee  tn  . .  
(/TEXT) 
Table 1: TREC topic description for topic 151, test 
questions expected to be answered by relewmt doc- 
uments, and a smnple document with answer key, s. 
of each module by comparing, for ditferent amounts 
of extraction, how many :good' sentences the module 
selects by itself. We rate a sentence as good simply 
if it also occurs in the ideal human-made xtract, 
and measure it using combined recall and precision 
(F-score). We used four topics r of total 6,194 doc- 
uments from the TREC collection. 138 of them are 
relevant documents with T IPSTER-SUMMAC pro- 
vided answer keys for the question and answering 
evaluation. Model extracts are created automati- 
cally from sentences contailfing answer keys. Table 
1 shows TREC topic description for topic 151, test 
questions expected to be answered by relevant doc- 
uments , and a sample relevant document with an- 
swer keys markup. 
7These four topics are: 
topic 151: Overcrowded Prisons, 1211 texts, 85 relevant; 
topic 257: Cigarette Consumption, 1727 texts, 126 relevant; 
topic 258: Computer Security, 1701 texts, 49 relevant; 
topic 271: Solar Power, 1555 texts, 59 relevant. 
SA relevant: document only needs to answer at least one of 
the five questions. 
6 Experimental Results 
In order to assess the utility of topic signatures in 
text sununarization, we follow the procedure de- 
scribed at the end of Section 4.1 to create topic 
signature for each selected TREC topic. Docu- 
ments are separated into relevant and nom'elevant 
sets according to their TREC relevancy judgments 
for each topic. We then run each document hrough 
a part-of-speech tagger and convert each word into 
its root form based on the \\h)rdNct lexical database. 
We also collect individual root word (unigram) fi'e- 
quency, two consecutive non-stopword 9 (bigram) fi'e- 
quency, and three consecutive non-stopwords (tri- 
gram) fi'equeney to facilitate the computation of the 
-21ogA value for each term. We expect high rank- 
ing bigram and trigram signature terms to be very 
informative. We set the cutoff associated weight at 
10.83 with confidence level ~t = 0.001 by looking up 
a X 2 statistical table. 
Table 2 shows the top 10 unigrmn, bigram, and tri- 
gram topic signature terms for each topic m. Several 
conclusions can be drawn directly. Terms with high 
-21ogA are indeed good indicators for their corre- 
sponding topics. The -2logA values decrease as the 
number of words in a term increases. This is rea- 
sonable, since longer terms usually occur less often 
than their constituents. However, bigram terms are 
more informative than nnigrant erms as we can ob- 
serve: jail//prison overervwding of topic 151, tobacco 
industry of topic 257, computer security of topic 258, 
and solar e'n, ergy/imwer of topic 271. These mLto- 
matically generated signature terms closely resemble 
or equal the given short TREC topic descriptions. 
Although trigram terms shown in the table, such 
as federal court order, philip morris 7~r, jet propul.. 
sion laboratory, and mobile telephone s:qstem are also 
meaningflfl, they do not demonstrate he closer term 
relationship among other terms in their respective 
topics that is seen in tlm bigram cases. We expect 
that more training data can improve tile situation. 
We notice that the -2logA values for topic 258 
are higher than those of the other three topics. As 
indicated by (Mani et al, 1998) the majority of rel- 
evant documents for topic 258 have the query topic 
as their main theme; while the others mostly have 
the query topics as their subsidiary themes. This 
implies that it is too liberal to assume all the terms 
in relevant documents of the other three topics are 
relevant. We plan to apply text segmentation algo- 
rithms such as TextTiling (Hearst, t997) to segment 
documents into subtopic units. We will then per- 
form the topic signature creation procedure only on 
tile relevant units to prevent inchlsion of noise terms. 
9\,Ve use the stopword list supplied with the SMAIIT re- 
trieval system. 
l?q'he -2logA values are not comparable across ngram cat- 
egories, since each ngraln category has its own sample space. 
498 
Top ic  
I :ll h~l al l l  -21,~gX \ ] l i~ la l l I  -21,,9X 
j a i l  t)3L I)1,1 e()tH~t 2, ja i l  Dit) 27:1 
c+,l l l l l} .IIJN ~21 eae ly  le+\]+.~lSt ? N,'~ :{t;\] 
, )v , . ) , '~ , ,wd ln~;  :?12:1. I~ ~ta l , .  D l~. , ,n  7.1 R72 
i l l ln?lt ," 2 : t l  7d5  s ta l , "  1,) i~, ,n, . i  67  ,3(~t~ 
~h+. l i f \ [  IF, I . i l o  ,1:~ 3 fill," l ; l  t(;2") 
s ta le  151 9t t~ ia i l  r l%l ' lctr~%vI\] l r ld I;1 ~\[ i  
I}l l~t l l i l ' l  I I  ~" I ";~' C(,tlt I + , l , i " t  t{ll.O!)l} 
i+tl-s,,rl 1,17, 3t),i h . .a l  j a i l  56  t i t+  
C l )y  133177 p l l . , )D  ( )vcy lc l , , i v th l l~  55 37:  +, 
, , v , . r , . r , ,wd ,+d 12N I)t)S i-(*lllt :l\[ fac i l i t  3 52 9o9  
10 S ignat t t ro  Torms o f  Tup ic  151  Ovorcrowdod Pr i sons  
"I'I I~I al I l  -21,,~11\ 
f - , I t . l~t l  c , ,u l~ <, l t t , . l  -I.', :),;11 
C, , l l l p \ ]y  c+,ll~('lll ,\]+'c\[+++" 3,5 12L 
+l,.kali+ i'i)iI i,\[~' +h, ' l l \ [ \ [  \[15 121 
~,11  i,) t l ; ,nk  :;.5 L21 
j , )o t l ; l l l k  IH ) l i5  :~.',.1'21 
pl l~C, l l , ' r  c+)l l : l l~ la i l  :~5 121 
91: i f , .  \ ] ) l l~t ) l l  i21) l l l t l~ ~N t).l\[\] 
t put  pl is+, l l  .2t~ :t-II 
c+~uuly  jaiL ~l ; l l , .  2 t~31 l 
h,,hl l,~e~l ja i l  2d  :l I I 
Top ic  10  S ig l ln t t l re  Tern ls  o f  Top ic  257  - ( l igar~t t~ Co| l s l l l ) l | ) t lo | l  
l :n i<r tun  21ogX I+i ~.rarll -- 2 / , , f / . \  'i r i4~am - 21, ,~A 
c lgrt l , . I t?+ .171; \[}:iN ~tlb:xt'c+) LIt( \ ] l l~l l~ ~il 7)iN I ,h i l ip  I I l , ) l l i+ t j l  2.~ ~DSI 
l ( )ht lcc~) : l l ; l  017  hn  t - lg / l l , - l l t -  t ;7 t2}I I r ) l \ ] i l I l a l l s  beDs~, l l  h<.(\[~f. 211 ~)t~\[l 
s I I IOk i l l~  28.t  19~ ph i l ip  t l l ( , l \ [ i~  5t  ()7;~ \[1111~ L'ikll('e\[ d '+\ [ l l l l  22 21. t  
~n l~,ke  15913.1  clarxl<'t1, :  %, 'at  t80 . t5  q t t  iri l ln cl l~ '.2'1 I IS  
I ,~ lh l l l a l l?  156. )375 to lh l l l l l l l~  i t l Y , ' l I l a t l t ) l lgk l  -t.t .13.1 qt l  q t t  f i~ ln  21 - I lS  
, ,~ha  I . tS 372  tobac? , )  elll()k.~ 112){}I  bll  b\[i bl l  20  22t i  
s ,~i la 12)i .121 ~il pat r i ck  t0 .  t55  c+)l lst l l l l l} l lo l l  bn  c lgar , . l te  2022d 
Illtll 113 ~+1~) c l~at+' l l~"  c~l l lpa l lV  :ID \[$1)D ~\[t+gtt a l l l . r \ [ iCtt l l  ~llI,lk?'(}llt 20226 
al l l , )k( ' l  10.1 I i0  ('el l l  l l l a lk+' l  36223 \[ l l l l~ Ca l l t 'e \ [  ht:gl\[ I  2(~ 22{i 
b\[~t 79 .90:1  ?~IN illt+ll+il++t :1t;.22:1 i l l a \ [ay~ia l l  +illk~\[tl>,ll+e t'4)l l lpi l l IV 2( I .22t \ ]  
Top ic  I0 S ignatur .  "I'~r)ns of Top ic  258 -- Co)nputor  Secur i ty  
I ~llial /lilt "2Ionia I t  i+/,r al l l  21,QIX "I'1 i~ratn  --21o9, X 
(:+lll l l)l ltOr 115!~ :151 C4, l l lp I l l l ' t  ae l ' t l l l l y  213331 )e l  I l t t /p l l \ [~i() l l  \ ] l th t )h l t ( l l y  \ [~  ~5.t  
v i rus  927.G7-1 ~\[ ; idt lgt l , "  s l t l \ [ t l ' l l l  17~ 5NN I l lh . ' l l  I lilt) 9R 85, t  
hacker  867 .377 FOl l lp t l t , ' t  +yS le l l l  1 -16.32~ C,+ltl++\]l I l l l iVet~il~,'  ~ lad l l \ [ t le  7}) IJNI 
in,) l  rl+ +i+;+~ 2i13'.! ) l , .~+-arch c,+ulte\[  l ; i2  .l I :i l awte l l c l "  b , ' rk , '  *'j~ lal l , ) l  al+,l  ,,. 79.0N \[ 
c , , rn , ' l l  3P+5 6+4 c , : ,ml ,u t , ' r  wrus  12~k033 I~+,++, je t  p tO l , t l L+ io t+ 79 .0~1 
un lv+' l? i ty  31)5 .95~ corne l i  U lX iVe le i ty  1(1~4 7-t l  U l ; iV ,q+i )y  ~; radu lx t , .  +tx ldt .  lll 79U~1 
+ysl+' l l l  290 .3"17  Iltl(:l,P;ll %t++npl)ll 107 .283 lawt l l l e ,+ l i v+: r tn ( . te  I igt l i () l lal  i\]\[) l\[I;~ 
I / tb , . ra lL : ) ly  2N7 521 in i l i ta ry  t ' ( , l l lp , l l . : r  106 .522 l iv~qll l ,) l?" i lu) i ,maL  lubora lo ry  {59195 
\ [ab  225 .51) ;  v i tu~ plo~t<' l l l l  1U6 522 c,) l l lp l l I (~r S ,~CUl i ly  eXpet l  66 .19G 
mecLa ly  128 .515 %vesl ~et l l l a l l  82  2 \ [0  ~ecu\ [ i t? , '  cenl{~\[  13ethesda  -19423 
Top ic  10  S ignature  Ter lns  o f  Top ic  271  So lar  Power  
I ' l l i g ta ln  - -21oqX t i ig t  ~ltn --2logX "I'r i ~;r hi l l  - -21o!~A 
so la r  -1S- l .315 e,~la~ e l te t l4y  2{Di 521 d iv i~ i ,m Inu l l ip l ,~  acress  31 3-17 
) t lazd i t  :10Pt 0IY) s<,lal l , t lw , ' t  9,1 210  n l , )b i l , :  l , , l , -ph , ,n , .  #c iv ic , ,  313 .17  
le,) 271; .932  ( 'h r i~t ia l l  a id  8 f i .211  b l i l l sh  It .cl l l l i l l l )R}' g \ [ , , l l p  23510 
it JtLi It l l l  2.5N.71):"+ l++,a S3'Sl,*III 711 5:{5 el l l I} l  he iNht  llXile 23+5111 
pax+lh , ,n  2133 81 I ill++tlllt. Ie i t ' j ) l l l ) l le  (115;l+i I'illllllCilll I lack i l l+;  I l Jd l l l l l l  22i+51(1 
i)(~tltld 12 / ,121  i t i , l i un l  p l , , j , . c l  112.697 ~l,~l lal  In r )h i l ,  + sa l , ' l l i te  23  511J 
t , lw~r  12G.35:1 lei l i  <+, , ,d -  61.~111 ha l ld l le ld  IIled~il," t , ' l eph , , l l , :  '23510 
\ [ , , , , k , ,u t  125 .ll3t; scie. l lc, ,  pa lk  ~'>.1 NS{) i l l ( ,h i le  ~ate l l l l . ,  . v>tetn  23  510 
i i l \ [ l l l i lSRl  1O9728 ~()llkl t ' i l l l t ' l ' l l t l i l I l , l  51t ~5{} I l l l l t l l lvl i l l  i g id i l ln l  I> l , l j ec t  23 ,510  
hc ,ydsh , t l  7N :173 l)p s l l la l  ?+1 ; /17  act iv t -  s+,la\[ *ys tern  15673 
Tattle 2: Top 10 signat.m'e t.erm.~; of mfigram, bigram, and trigram for fore" TREe  t.opics. 
6.1 Comparing Summary Extraction 
Effectiveness Using Topic Signatures+ 
TFII)t",  and Bas(,line Algor i thms 
In orde)" I() (~vahla(.(~ the (d\[+:ct.iv(,im.~s nf l(>l)i(: .~dgna- 
l;lll'(~S llS(~(\] ill SlllllIIN/l'y (~Xtl'it(:t;iOll, W{,  ~ CtIllll)~ll'(~ +flit! 
Sllltllll~tl'y StHII~011('CS ex(~ract,(~d 1)y the tol) ic si~Ilil\[lll'0, 
module+, basulin(.' module, and tfidf lnothll(~s with lm- 
nta'n annot,  at(~(l lllo(lo,\] Sllllllll}ll'ios. \VC III(+~}/SIlI'(+ + l;h(; 
l)c'rfl)rmanc(~' using a c()ml)ined umasure of l'n'call (I~) 
and pr(~cisi(m (P), F. F-score is defined by: 
I " - -  (1 +H'2)I'l? where 
/3-'P + I~ 
t ) 
2 \7 . )  ,. 
f'~rln 
fVln 
~\,, 
# of  .sc,tcncc.~ c:rtratcd th,t  olso 
atqwar in. tim model ,s.mn)?lr!l 
# of  sc+lt(!ncc,s i11 tim nlo,h:l .~um.tav!l 
# of  ,s('./Itclwcs c:rlv?lclcd t)1,t ll*c .Sll.Slcln 
rclatic'c iml,ortancc of  l~ aml 1:' 
(6) 
(7) 
\Ve as.~um(~ (,(lual importance of re(:all iIIld preci- 
sion aim set H to 1 in our (+,Xl)('+rimtml;s. The Imselitm 
(I)ositi(m) module scores ('at:h S(!llt(':llC{} hy its I)osi- 
ti(>n in the text. The first sent(race gets the high- 
esc s(:ortL the last S(HIt(H1Co the lowest. The l)as(~liIl(~ 
method is eXlmCted to lm ('.f\[ectiv(~ for news gem'e. 
The tfidf module assigns a score t.o a tt++rllI ti at:cord- 
ing to the product; of its flequc, ncy within a dot:- 
lllll(Hlt .j ( t f i j )  and its illV(~I'S(} doctmmnt  t?equoncy  
(idfi lo.q ,~). N is the total mmfl)or of document.s 
in the (:()rlms and dfj is the, numl)er of (Io(:HnloAll;.q 
(:OlH:nining te rm ti. 
The topic sigjlla(.lll(++ module sciliis each ,q(~llt;(H1C(~: 
assigning to ('ach word that occurs in a topic signa- 
(ure thu weigh(, of that, keyword in t.hc' tol)ic signa- 
tltl'tL Eit{'h s(++llt(,+ItC(~ Ill(ill l'(:c(:ive.q a top ic  s ignature  
score equal to tlm total of all signature word scores it 
(:Olllailis, normalizc'd 1) 3' the. highest sentence score. 
This s(:ol( 3 indical.es l;h(~ l'(!l(wall(:(~ of l.h(; S(!llt.t~n(:(! to 
t, lw sigmmlre topic. 
SU.~\[.MAt/IST In'oduced (!xttat:ts of tlm samu 
l(~xI.q sui)aralely for each ,,lodul0, for a s(~l'i(,s of ex- 
tracts ranging from ()cX; to 100% of the. original l;(}xI. 
Althottgh many rel<want docttments are avaita})l+, 
for each t01>ic, Ollly SOlll0 o\[ \[h0111 htlv(~ allSWOl kc!y 
499 
markut)s. The mnnber of documents with answer 
keys are listed in the row labeled: "# of Relevant 
Does Used in Training". To ensure we utilize all 
the available data and conduct a sound evaluation, 
we perform a three-fold (:ross validation. We re- 
serve one-third of documents as test set, use the rest 
as training set, and ret)eat three times with non- 
overlapl)ing test set. Furthernmre, we use only uni- 
gram topic signatures fin" evaluation. 
The result is shown in Figure 2 and TaMe 3. We 
find that the topic signature method outperforms 
the other two methods and the tfidfmethod performs 
poorly. Among 40 possibh,, test points fl)r four topics 
with 10% SUmlnary length increment (0% means se- 
lect at least one sentence) as shown in Table 3, the 
topic signature method beats the baseline method 
34 times. This result is really encouraging and in- 
dicates that the topic signature method is a worthy 
addition to a variety of text summarization methods. 
6.2 Enriching Topic Signatures Using 
Existing Ontologies 
We have shown in the previous sections that topic 
signatures can be used to al)I)roximate topic iden- 
tification at the lexieal level. Although the au- 
tomatically acquired signature terms for a specific 
topic seem to 1)e bound by unknown relationships as 
shown in Table 2, it is hard to image how we can 
enrich the inherent fiat structure of tol)ie signatures 
as defined in Equation 1 to a construct as complex 
as a MUC template or script. 
As discussed in (Agirre et al, 2000), we propose 
using an existing ontology such as SENSUS (Knight 
and Luk, 1994) to identify signature term relations. 
The external hierarchical framework can be used to 
generalize topic signatures and suggest richer rep- 
resentations for topic signatures. Automated entity 
recognizers can be used to (:lassify unknown enti- 
ties into their appropriate SENSUS concept nodes. 
We are also investigating other approaches to attto- 
matieally learn signature term relations. The idea 
mentioned in this paper is just a starting point. 
'7 Conc lus ion  
In this paI)er we l)resented a t)rocedure to automati- 
(:ally acquire topic signatures and valuated the eflk~c- 
tiveness of applying tol)i(: signatures to extract ot)i(: 
relevant senten(:es against two other methods. The 
tot)ie signature method outt)erforms the baseline and 
the tfidfmethods for all test topics. Topic signatures 
can not only recognize related terms (topic identifi- 
(:ation), but grout) related terms togetlmr under one 
target concept (topic interpretation). 'IbI)i(: identi- 
fication and interpretation are two essential steps in 
a typical automated text summarization system as 
we l)resent in Section 3. 
'\]))pic: signatures (:an also been vie.wed as an in- 
verse process of query expansion. Query expansion 
intends to alleviate the word mismatch ln'oblenl in 
infornmtion retrieval, since documents are normally 
written in different vocabulary, ttow to atttomati- 
(ally identify highly e(nrelated terms and use them 
to improve information retrieval performance has 
been a main research issue since late 19611's. Re- 
cent advances in the query expansion (Xu and Croft, 
1996) can also shed some light on the creation of 
topic signatures. Although we focus the ltse of topic 
signatures to aid text summarization i this paper, 
we plan to explore the possibility of applying topic 
signatures to perform query expansion in the future. 
The results reported are encouraging enough to 
allow us to contimm with topic signatures as the ve- 
hMe for a first approximation to worht knowledge. 
We are now busy creating a large nmnber of signa- 
ture.s to overcome the world knowledge acquisition 
problem and use them in topic interpretation. 
8 Acknowledgements  
YVe thank the anonymous reviewers for very use- 
tiff suggestions. This work is supported in part by 
DARPA contract N66001-97-9538. 
References  
Eneko .~girre, Olatz Ansa, Edum'd Hovy, and David 
Martinez. 2000. Enriching very large ontologies 
using the www. In Proceedings of the Work,,;hop 
on Ontology Construction of the European Con- 
fl:rencc of AI (ECAI). 
Kenneth Church and Patrick Hanks. 1990. Word as- 
sociation IIOrlllS, mutual information and lexicog- 
raphy. In Proceedings of the 28th Annual Meeting 
of the Association for Computational Lingui.vtic.~" 
(,4CL-90), pages 76~-83. 
Thomas Cover and Joy A. Thomas. 1991. Elcment.~ 
of Information Theory..John Wiley & Sons. 
Gerald DeJong. 1982. An overview of the FRUMP 
system. In ~2mdy G. Lehnert and Martin H. 
Ringle, editors, Strategies for natural language 
processing, pages 149-76. Lawrence Erlbaum A.s- 
so(lares. 
Ted Dunning. 1993. A~i:eurate methods for the 
statistics of surprise and coincidence. Computa- 
tional Li'nguistics, 19:61--7'4. 
Marti Itearst. 1997. TextTiling: Segmenting text 
into nmlti-l)aragraph subtopic passages. Compu- 
tational Linguistics, 23:33-64. 
Eduard Hovy and Chin-Yew Lin. 1999. Automated 
text summarization i SUMMAIRIST. In Inder- 
jeer Mani and Mark T. Maybury, editors, Ad- 
vances in Automatic 71xxt Summarization, chap- 
ter 8, pages 81 94. MIT Press. 
Kevin Knight and Steve K. Luk. 1994. Building a 
large knowledge base for machine translation, ht 
Proceedings of the Eleventh National Coy@re'nee 
on Arti\]icial Intelligence (AAAI-9/~). 
500 
-~  ..~:,., .;~ ,5. " ' - -=-"  _..  _ . .&ass  
0 50000 n ~ .~ . . . . . . . . . . . . . .  n ~ . . . . . . . . . . . . . . . . . . . .  ~ . . . . . . . . . .  
- - ' - . , ,q ,~  . . . .  + +. ,x.  o + ~ ? 
. . . . .  1,* '+  .~  *+-  . . . .  - -  
. . ; -5; , :~:;  . . . .  h ~.:~.7~.~ ~ ~ ^' - -~- . .  
. . . . . . . .  , ? - -g2  . . . . . . . . . .  o 400OO f , ' " +- ~-" + ~ -, "~2x-+, 
\[ ? . . . .  : . . . . . . .  : 
\[ - ...... ; ""7 ........ 2,=_ 
~ 0 =0000 j '  +J" J j  1" " 
.,::i'ff "4. 4 "A- . I  - i+ ~. : 
4" .,~t . -a  + -a--  -#. - . -~-- - .a  ~ ' . 
0 ~o00o 'd-;9~7~ -7 '+ 5~:7~:=-+': ; :  ~ . . . .  =-~++:7:: ~ -:'~ +--~ ....... " ~5_~Ztt::~:ll;: ; i  
I " , ; .  A ' / , -?~-  <F" ~. " "~" ~ 257 
44" ? 
o ;oo~ ._~-.c_-__~ / 
0 00000 
I 
000 005 010 015 020 025 030 ,335 040 045 050 055 060 065 070 o75 050 085 090 095 ~00 
.~ umrn~i-~ Lenqth 
Figure 2:  F-measur(: w;. summary length for all fimr topics. ~bi)ic signature cl(mrly outperforin tfidf and 
baselin(', ex(:ei)t for tit(: case of topic 258 where t)(~rforman(:(; for tim thr(;e methods are roughly equal. 
I__ I - - - - - -~~g-  lO% I ___~o~a -ao~~a--- -  4o~ I ~0%- - \ [  ~o~ \[ ~,o~ ~o% I 9o~ I lOO% I 
\[ ~.+,_~.,~dl .... i . . :ms o.a-~9 I o..~.o o.aa4 o . ,~:c -  I ...ao=, \[ '__2 :~r'  I~  o.ara oar , ;  I . .a t , ,  I e.a.~w-I 
+4.58 +7.48 +15.6a +14.17 +8.66 +3. I 51_ top lc . s i~  I -2 ,7d  -2 .19- -  
\[ 257-h , , * , , l i  .... r--- (1.1-}98 {~.15.__.5 I c,,, ,,.is., ".'~L I o.,~~--F--,~.~,, I - -o  t,l o.1~, I ?.!s~ 
\[ '_,.~r_,a,,r \[ -55.11- -38.56 I -'".5U ~">' ~".0'; ' ' " '+ '  I S '~: ' '  I ~ ~ " " "  I +r  0 '~t  . . . . .  , 
257_,~i,ic.~ig +45.5~ +64.06 +31.88 ~ +20.40 \[ +20.60 \[ 4_-~01 +12.4&- I14 .24  - O.(h.~\] 
\[_ 25u_h~,~.,li . . . .  L_  o l  tk_ o 270 I "'4'-'2 ~' *'~:~ I ,, ~,r_, L_  "47t_ J  .4 r , ,  1 - - ~ - ' 1  o.~,'__,+~ o s'_,Z._J 
\[ 271_l,aseli . . . .  I <,at tT_.._,~.:,,~; T--,Ta77--- ..a:~ _L ,, :s:,r, .L .... ~~~~:~- i~-  T -  o.ae~ \] , ) . :~:~~\[--~ 
. . . . . . .  +a~. lO  j _ _  + 4 ~ _ ~ ~ s . r o . ~  +~.a,~ I +~.~o_ l l  0.,~, \] 
Table 3: F..measul'e t)erformanc(~ differen(:e compared to 1)aselin(~ nt(:thod in t)ercentage. Cohmms indicate 
at diffe.rent summary lengths related to fldl length docum(mts. Values in the 1)aselin(,. rows are F-measure 
s(:ores. Vahms in the tfidf and tot)i(: signatur(~ rows arc i)('.rformmlc(~ increase or (h,.crease divide(l by their 
(:orr(.,sI)ontling baseline scores and shown in I)er(:(mtag(!. 
Inderje(?t Mani, David House, Gary KMn, Lyn(~tt(~ 
ttirschman, Leo ()brst, Thdr6se Firmin, Micha(d 
Chrzanowski, and Beth Sundheim. 1998. 
The T IPSTER SUMMAC t~xl smmnm'iza- 
tion evaluation final r(:t)ort. %~(:hnical I/,ol)orl; 
MTR98W0000138, The MITRE Corporation. 
Christopher Manning and Hinrich Schiitzc. 1999. 
t'}mdatious of Statistical Natural Language Pro~ 
cessi'ng. MIT Press .  
Kathh~(m M(:K(!own and l)rag(mfir R. I ladev. 1999. 
( 'TO. I I (  ra t ,  i l l g  S l l l l l l l l ; l l ' i ( : s  o f  I l t l l l t ,  i \ [ ) l \ [~  l l ( !~vs  articles. 
In hMtu.iet~t Mani and Mark T. Maybury, edi 
t,ors, Admm.ces i'n Automatic Text Sv,.mmarization, 
chapter 24, pagc+s 381 :/89. MiT Press. 
Ellen Riloff and Jeffrey Lorenzen. 1999. Ext:raction- 
t)a:;e,d text cateI,dorization: Generating donmin- 
qmcitic role relationships atttonmtically. In 
Tomek Strzalkowski, editor, Natural Language In- 
formation, Retrieval. Kluwer Academic Publishc, r.q. 
Ellen tlilolf. 1996. An ompirical study of automated 
dictionary construction for information extraction 
in three domains. Artificial Intelligence ,Journal, 
85, August. 
SAIC. 1998. Introduction to information extraction. 
http://www.mu(:.sai(:.(:om. 
Jinxi Xu and W. Bruc(! Croft. 1996. Query ex- 
pal>ion using local and gh)bal document analysis. 
In l'rocee.dings of the 17th Annual Inter'national 
A(JM SIGIR Co't@rence. on Research and Devel- 
opment in Information l{et'rieval, pages 4 -11. 
50 I  
 Using Knowledge to Facilitate Factoid Answer Pinpointing 
Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
USA 
{hovy,ulf,cyl,ravichan}@isi.edu 
 
Abstract 
In order to answer factoid questions, the 
Webclopedia QA system employs a 
range of knowledge resources.  These 
include a QA Typology with answer 
patterns, WordNet, information about 
typical numerical answer ranges, and 
semantic relations identified by a robust 
parser, to filter out likely-looking but 
wrong candidate answers.  This paper 
describes the knowledge resources and 
their impact on system performance. 
1.   Introduction 
The TREC evaluations of QA systems 
(Voorhees, 1999) require answers to be drawn 
from a given source corpus.  Early QA systems 
used a simple filtering technique, question word 
density within a fixed n-word window, to 
pinpoint answers.  Robust though this may be, 
the window method is not accurate enough.  In 
response, factoid question answering systems 
have evolved into two types:  
? Use-Knowledge: extract query words from 
the input question, perform IR against the 
source corpus, possibly segment resulting 
documents, identify a set of segments 
containing likely answers, apply a set of 
heuristics that each consults a different 
source of knowledge to score each 
candidate, rank them, and select the best 
(Harabagiu et al, 2001; Hovy et al, 2001; 
Srihari and Li, 2000; Abney et al, 2000).  
? Use-the-Web: extract query words from the 
question, perform IR against the web, 
extract likely answer-bearing sentences, 
canonicalize the results, and select the most 
frequent answer(s).  Then, for justification, 
locate examples of the answers in the source 
corpus (Brill et al, 2001; Buchholz, 2001).  
Of course, these techniques can be combined: 
the popularity ratings from Use-the-Web can 
also be applied as a filtering criterion (Clarke et 
al., 2001), or the knowledge resource heuristics 
can filter the web results.  However, simply 
going to the web without using further 
knowledge (Brill et al, 2001) may return the 
web?s majority opinions on astrology, the killers 
of JFK, the cancerous effects of microwave 
ovens, etc.?fun but not altogether trustworthy.   
In this paper we describe the range of 
filtering techniques our system Webclopedia 
applies, from simplest to most sophisticated, and 
indicate their impact on the system.   
2.   Webclopedia Architecture  
As shown in Figure 1, Webclopedia adopts the 
Use-Knowledge architecture. Its modules are 
described in more detail in (Hovy et al, 2001; 
Hovy et al, 1999):  
? Question parsing: Using BBN?s 
IdentiFinder (Bikel et al, 1999), the 
CONTEX parser (Hermjakob, 1997) 
produces a syntactic-semantic analysis of 
the question and determines the QA type.   
? Query formation: Single- and multi-word 
units (content words) are extracted from the 
analysis, and WordNet synsets (Fellbaum, 
1998) are used for query expansion.  A 
series of Boolean queries of decreasing 
specificity is formed.  
? IR: The publicly available IR engine MG 
(Witten et al, 1994) returns the top-ranked 
N documents.  
 ? Selecting and ranking sentences: For each 
document, the most promising K sentences 
are located and scored using a formula that  
 rewards word and phrase overlap with the 
question and its expanded query words.  Results 
are ranked.   
? Parsing candidates: CONTEX parses the 
top-ranked 300 sentences.   
? Pinpointing: As described in Section 3, a 
number of knowledge resources are used to 
perform filtering/pinpointing operations.   
? Ranking of answers: The candidate 
answers? scores are compared and the 
winner(s) are output. 
3. Knowledge Used for Pinpointing 
3.1   Type 1: Question Word Matching 
Unlike (Prager et al, 1999), we do not first 
annotate the source corpus, but perform IR 
directly on the source text, using MG (Witten et 
al., 1994).  To determine goodness, we assign an 
initial base score to each retrieved sentence.  We 
then compare the sentence to the question and 
adapt this score as follows:  
? exact matches of proper names double the 
base score. 
? matching an upper-cased term adds a 60% 
bonus of the base score for multi-words 
terms and 30% for single words (matching 
?United States? is better than just ?United?).  
? matching a WordNet synonym of a term 
discounts by 10% (lower case) and 50% 
(upper case).  (When ?Cage? matches 
?cage?, the former may be the last name of a 
person and the latter an object; the case 
mismatch signals less reliability.)  
? lower-case term matches after Porter 
stemming are discounted 30%; upper-case 
matches 70% (Porter stemming is more 
aggressive than WordNet stemming).  
? Porter stemmer matches of both question 
and sentence words with lower case are 
discounted 60%; with upper case, 80%.  
? if CONTEX indicates a term as being 
qsubsumed (see Section 3.9) the term is 
discouned 90% (in ?Which country 
manufactures weapons of mass 
destruction??, ?country? will be marked as 
qsubsumed).   
The top-scoring 300 sentences are passed on for 
further filtering.   
3.2  Type 2: Qtargets, the QA Typology, 
and the Semantic Ontology 
We classify desired answers by their semantic 
type, which have been taxonomized in the 
Webclopedia QA Typology (Hovy et al, 2002), 
Candidate answer parsing
? Steps: parse sentences
? Engines: CONTEX
Matching
? Steps: match general constraint patterns against parse trees
             match desired semantic type against parse tree elements
             assign score to words in sliding window
? Engine: Matcher
Ranking and answer extraction
? Steps: rank candidate answers
             extract and format them
? Engine: Answer ranker/formatter
QA typology
? QA types, categorized in taxonomy
Constraint patterns
? Identify likely answers in relation to
   other parts of the sentence
Create query
Retrieve documents
Select & rank sentences
Parse top sentences
Parse question
Input question
Perform additional inference
Rank and prepare answers
Output answers
Question parsing
? Steps: parse question
             find desired semantic type
? Engines:  IdentiFinder  (BBN)
                 CONTEX
Match sentences against answers
Query creation
?  Steps: extract, combine important words
 expand query words using WordNet
 create queries, order by specificity
?  Engines: Query creator
IR
?  Steps: retrieve top 1000 documents
?  Engines: MG (RMIT Melbourne)
Sentence selection and ranking
?  Steps: score each sentence in each document
 rank sentences and pass top 300 along
?  Engines:Ranker
Figure 1. Webclopedia architecture. 
 http://www.isi.edu/natural-language/projects/we 
bclopedia/Taxonomy/taxonomy_toplevel.html). 
The currently approx. 180 classes,  which we 
call qtargets, were developed after an analysis of 
over 17,000 questions (downloaded in 1999 
from answers.com) and later enhancements to 
Webclopedia.  They are of several types:  
? common semantic classes such as PROPER-
PERSON, EMAIL-ADDRESS, LOCATION, 
PROPER-ORGANIZATION;  
? classes particular to QA such as YES:NO, 
ABBREVIATION-EXPANSION, and WHY-
FAMOUS;  
? syntactic classes such as NP and NOUN, 
when no semnatic type can be determined 
(e.g., ?What does Peugeot manufacture??);  
? roles and slots, such as REASON and TITLE-
P respectively, to indicate a desired relation 
with an anchoring concept.    
Given a question, the CONTEX parser uses a 
set of 276 hand-built rules to identify its most 
likely qtarget(s), and records them in a backoff 
scheme (allowing more general qtarget nodes to 
apply when more specific ones fail to find a 
match).  The generalizations are captured in a 
typical concept ontology, a 10,000-node extract 
of WordNet.   
The recursive part of pattern matching is 
driven mostly by interrogative phrases.  For 
example, the rule that determines the 
applicability of the qtarget WHY-FAMOUS 
requires the question word ?who?, followed by 
the copula, followed by a proper name.  When 
there is no match at the current level, the system 
examines any interrogative constituent, or words 
in special relations to it.  For example, the 
qtarget TEMPERATURE-QUANTITY (as in 
?What is the melting point of X?? requires as 
syntactic object something that in the ontology is 
subordinate to TEMP-QUANTIFIABLE-ABS-
TRACT with, as well, the word ?how? paired 
with ?warm?, ?cold?, ?hot?, etc., or the phrase  
?how many degrees? and a TEMPERATURE-
UNIT (as defined in the ontology).   
3.3 Type 3: Surface Pattern Matching 
Often qtarget answers are expressed using rather 
stereotypical words or phrases.  For example, the 
year of birth of a person is typically expressed 
using one of these phrases:  
<name> was born in <birthyear> 
<name> (<birthyear>?<deathyear>) 
We have developed a method to learn such 
patterns automatically from text on the web 
(Ravichandran and Hovy, 2002).  We have 
added into the QA Typology the patterns for 
appropriate qtargets (qtargets with closed-list 
answers, such as PLANETS, require no patterns).  
Where some QA systems use such patterns 
exclusively (Soubbotin and Soubbotin, 2001) or 
partially (Wang et al, 2001; Lee et al, 2001), 
we employ them as an additional source of 
evidence for the answer.  Preliminary results on 
for a range of qtargets, using the TREC-10 
questions and the TREC corpus, are:  
Question type 
(qtarget) 
Number of 
questions 
MRR on 
TREC docs 
BIRTHYEAR 8 0.47875 
INVENTORS 6 0.16667 
DISCOVERERS 4 0.1250 
DEFINITIONS 102 0.3445 
WHY-FAMOUS 3 0.6666 
LOCATIONS 16 0.75 
3.4  Type 4: Expected Numerical Ranges  
Quantity-targeting questions are often 
underspecified and rely on culturally shared  
cooperativeness rules and/or world knowledge: 
Q: How many people live in Chile?  
S1: ?From our correspondent comes good 
news about the nine people living in  Chile?? 
A1: nine  
While certainly nine people do live in Chile, 
we know what the questioner intends.  We have 
hand-implemented a rule that provides default 
range assumptions for POPULATION questions 
and biases quantity questions accordingly.  
3.5 Type 5: Abbreviation Expansion  
Abbreviations often follow a pattern: 
Q: What does NAFTA stand for? 
S1: ?This range of topics includes the North 
American Free Trade Agreement, NAFTA, 
and the world trade agreement GATT.?  
S2: ?The interview now changed to the subject 
of trade and pending economic issues, such as 
the issue of opening the rice market, NAFTA, 
and the issue of Russia repaying economic 
cooperation funds.?  
After Webclopedia identifies the qtarget as 
ABBREVIATION-EXPANSION, it extracts 
 possible answer candidates, including ?North 
American Free Trade Agreement? from S1 and 
?the rice market? from S2.  Rules for acronym 
matching easily prefer the former.  
3.6 Type 6: Semantic Type Matching  
Phone numbers, zip codes, email addresses, 
URLs, and different types of quantities obey 
lexicographic patterns that can be exploited for 
matching, as in  
Q: What is the zip code for Fremont, CA?  
S1: ??from Everex Systems Inc., 48431 
Milmont Drive, Fremont, CA 94538.?  
and  
Q: How hot is the core of the earth?  
S1. ?The temperature of Earth?s inner core 
may be as high as 9,000 degrees Fahrenheit 
(5,000 degrees Celsius).?  
Webclopedia identifies the qtargets respectively 
as ZIP-CODE and TEMPERATURE-QUANTITY.  
Approx. 30 heuristics (cascaded) apply to the 
input before parsing to mark up numbers and 
other orthographically recognizable units of all 
kinds, including (likely) zip codes, quotations, 
year ranges, phone numbers, dates, times, 
scores, cardinal and ordinal numbers, etc.  
Similar work is reported in (Kwok et al, 2001).  
3.7 Type 7: Definitions from WordNet  
We have found a 10% increase in accuracy in 
answering definition questions by using external 
glosses obtained from WordNet.  For  
Q: What is the Milky Way?  
Webclopedia identified two leading answer 
candidates:   
A1: outer regions  
A2: the galaxy that contains the Earth  
Comparing these with the WordNet gloss:  
WordNet: ?Milky Way?the galaxy containing 
the solar system?  
allows Webclopedia to straightforwardly match 
the candidate with the greater word overlap.   
Curiously, the system also needs to use 
WordNet to answer questions involving 
common knowledge, as in:  
Q: What is the capital of the United States?  
because authors of the TREC collection do not 
find it necessary to explain what Washington is:  
Ex: ?Later in the day, the president returned to 
Washington, the capital of the United States.?  
While WordNet?s definition  
Wordnet: ?Washington?the capital of the 
United States?  
directly provides the answer to the matcher, it 
also allows the IR module to focus its search on 
passages containing ?Washington?, ?capital?, 
and ?United States?, and the matcher to pick a 
good motivating passage in the source corpus.   
Clearly, this capability can be extended to 
include (definitional and other) information 
provided by other sources, including 
encyclopedias and the web (Lin 2002). 
3.8 Type 8: Semantic Relation Matching  
So far, we have considered individual words and 
groups of words.  But often this is insufficient to 
accurately score an answer.  As also noted in 
(Buchholz, 2001), pinpointing can be improved 
significantly by matching semantic relations 
among constituents:  
Q: Who killed Lee Harvey Oswald?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?Belli?s clients have included Jack Ruby, 
who killed John F. Kennedy assassin Lee 
Harvey Oswald, and Jim and Tammy Bakker.?  
S2: ?On Nov. 22, 1963, the building gained 
national notoriety when Lee Harvey Oswald 
allegedly shot and killed President John F. 
Kennedy from a sixth floor window as the 
presidential motorcade passed.?  
The CONTEX parser (Hermjakob, 1997; 
2001) provides the semantic relations.  The 
parser uses machine learning techniques to build 
a robust grammar that produces semantically 
annotated syntax parses of English (and Korean 
and Chinese) sentences at approx. 90% accuracy 
(Hermjakob, 1999).   
The matcher compares the parse trees of S1 
and S2 to that of the question.  Both S1 and S2 
receive credit for matching question words ?Lee 
Harvey Oswald? and ?kill? (underlined), as well 
as for finding an answer (bold) of the proper 
qtarget type (PROPER-PERSON).  However, is 
the answer ?Jack Ruby? or ?President John F. 
Kennedy??  The only way to determine this is to 
consider the semantic relationship between these 
 candidates and the verb ?kill? (parse trees 
simplified, and only portions shown here):   
 
[1] Who killed Lee Harvey Oswald?  [S-SNT] 
    (SUBJ) [2] Who  [S-INTERR-NP] 
        (PRED) [3] Who  [S-INTERR-PRON] 
    (PRED) [4] killed  [S-TR-VERB] 
    (OBJ) [5] Lee Harvey Oswald  [S-NP] 
        (PRED) [6] Lee?Oswald  [S-PROPER-NAME] 
            (MOD) [7] Lee  [S-PROPER-NAME] 
            (MOD) [8] Harvey  [S-PROPER-NAME] 
            (PRED) [9] Oswald  [S-PROPER-NAME] 
    (DUMMY) [10] ?  [D-QUESTION-MARK] 
 
[1] Jack Ruby, who killed John F. Kennedy assassin  
  Lee Harvey Oswald  [S-NP] 
   (PRED) [2] <Jack Ruby>1  [S-NP] 
   (DUMMY) [6] ,  [D-COMMA] 
   (MOD) [7] who killed John F. Kennedy assassin  
                 Lee Harvey Oswald  [S-REL-CLAUSE] 
     (SUBJ) [8] who<1>  [S-INTERR-NP] 
     (PRED) [10] killed  [S-TR-VERB] 
     (OBJ) [11] JFK assassin?Oswald  [S-NP] 
         (PRED) [12] JFK?Oswald [S-PROP-NAME] 
             (MOD) [13] JFK  [S-PROPER-NAME] 
             (MOD) [19] assassin  [S-NOUN] 
             (PRED) [20] ?Oswald [S-PROPER-NAME] 
Although the PREDs of both S1 and S2 
match that of the question ?killed?, only S1 
matches ?Lee Harvey Oswald? as the head of 
the logical OBJect.  Thus for S1, the matcher 
awards additional credit to node [2] (Jack Ruby) 
for being the logical SUBJect of the killing 
(using anaphora resolution). In S2, the parse tree 
correctly records that node [13] (?John F. 
Kennedy?) is not the object of the killing.  Thus 
despite its being closer to ?killed?, the candidate 
in S2 receives no extra credit from semantic 
relation matching.   
It is important to note that the matcher 
awards extra credit for each matching semantic 
relationship between two constituents, not only 
when everything matches.  This granularity 
improves robustness in the case of partial 
matches.   
Semantic relation matching applies not only 
to logical subjects and objects, but also to all 
other roles such as location, time, reason, etc. 
(for additional examples see http://www.isi.edu/ 
natural-language/projects/webclopedia/sem-rel-
examples.html).  It also applies at not only the 
sentential level, but at all levels, such as post-
modifying prepositional and pre-modifying 
determiner phrases  
Additionally, Webclopedia uses 10 lists of 
word variations with a total of 4029 entries for 
semantically related concepts such as ?to 
invent?, ?invention? and ?inventor?, and rules 
for handling them.  For example, via coercing 
?invention? to ?invent?, the system can give 
?Johan Vaaler? extra credit for being a likely 
logical subject of ?invention?:  
Q: Who invented the paper clip?  
Qtargets: PROPER-PERSON & PROPER-NAME, 
PROPER-ORGANIZATION  
S1: ?The paper clip, weighing a desk-crushing 
1,320 pounds, is a faithful copy of Norwegian 
Johan Vaaler?s 1899 invention, said Per 
Langaker of the Norwegian School of 
Management.?  
while ?David? actually loses points for being 
outside of the clausal scope of the inventing:  
S2: ??Like the guy who invented the safety pin, 
or the guy who invented the paper clip,? David 
added.?  
3.9 Type 9: Word Window Scoring  
Webclopedia also includes a typical window-
based scoring module that moves a window over 
the text and assigns a score to each window 
position depending on a variety of criteria (Hovy 
et al, 1999).  Unlike (Clarke et al, 2001; Lee et 
al., 2001; Chen et al, 2001), we have not 
developed a very sophisticated scoring function, 
preferring to focus on the modules that employ 
information deeper than the word level.  
This method is applied only when no other 
method provides a sufficiently high-scoring 
answer.  The window scoring function is  
S  = (500/(500+w))*(1/r) * ?[(?I1.5*q*e*b*u)1.5] 
Factors: 
w: window width (modulated by gaps of 
various lengths: ?white house? ? ?white car and 
house?), 
r: rank of qtarget in list returned by 
CONTEX, 
I: window word information content (inverse 
log frequency score of each word), summed,  
q: # different question words matched, plus 
specific rewards (bonus q=3.0),  
e: penalty if word matches one of question 
word?s WordNet synset items (e=0.8),  
 b: bonus for matching main verb, proper 
names, certain target words (b=2.0),  
u: (value 0 or 1) indicates whether a word has 
been qsubsumed (?subsumed? by the qtarget) 
and should not contribute (again) to the score.  
For example, ?In what year did Columbus 
discover America?? the qsubsumed words are 
?what? and ?year?. 
4. Performance Evaluation  
In TREC-10?s QA track, Webclopedia received 
an overall Mean Reciprocal Rank (MRR) score 
of 0.435, which put it among the top 4 
performers of the 68 entrants (the average MRR 
score for the main QA task was about 0.234).  
The pinpointing heuristics are fairly accurate: 
when Webclopedia finds answers, it usually 
ranks them in the first place (1st place: 35.5%; 
2nd: 8.94%; 3rd: 5.69%; 4th: 3.05%; 5th: 5.28%; 
not found: 41.87%).  
We determined the impact of each 
knowledge source on system performance, using 
the TREC-10 test corpus using the standard 
MRR scoring.  We applied the system to the 
questions of each knowledge type separately, 
with and without its specific knowledge 
source/algorithm.  Results are shown in Table 1, 
columns A (without) and B (with).  To indicate 
overall effect, we also show (in columns C and 
D) the percentage of questions in TREC-10 and 
-9 respecively of each knowledge type.   
5. Conclusions 
It is tempting to search for a single technique 
that will solve the whole problem (for example, 
Ittycheriah et al (2001) focus on the subset of 
factoid questions answerable by NPs, and train a 
statistical model to perform NP-oriented answer 
pinpointing).  Our experience, however, is that 
even factoid QA is varied enough to require 
various special-purpose techniques and 
knowledge.  The theoretical limits of the various 
techniques are not known, though Light et al?s 
(2001) interesting work begins to study this.   
Column A: % questions of the knowledge type  
     answered correctly without using knowlege 
Column B: % questions, now using knowledge 
Column C: % questions of type in TREC-10  
Column D: % questions of type in TREC-9  
 A B C D 
Abbreviation exp. 20.0 70.0  1.0 2.3 
Number ranges 50.0 50.0  1.2 1.8 
WordNet (def Qs) 48.3 67.5 20.9 5.1 
Semantic types     
- locator types N/A N/A  0.0 0.4 
- quantity types 22.5 48.7 10.8 5.5 
- date/year types 45.0 57.3  9.2 10.2 
Patterns      
- definitions ? 34.4 20.9 5.1 
- why-famous  ? 66.7 0.6 ? 
- locations ? 75.0 3.2 ? 
- birthyear ? 47.9 1.6 ? 
Semantic relations 39.4 46.5 72.2 85.7 
Table 1. Performance of knowledge sources. 
Semantic relation scores measured only on 
questions in which they could logically apply.   
We conclude that factoid QA performance 
can be significantly improved by the use of 
knowledge attuned to specific question types 
and specific information characteristics.  Most of 
the techniques for exploiting this knowledge 
require learning to ensure robustness.  To 
improve performance beyond this, we believe a 
combination of going to the web and turning to 
deeper world knowledge and automated 
inference (Harabagiu et al, 2001) to be the 
answer.  It remains an open question how much 
work these techniques would require, and what 
their payoff limits are.   
References  
Abney, S., M. Collins, and A. Singhal. 2000. Answer 
Extraction. Proceedings of the Applied Natural 
Language Processing Conference (ANLP-
NAACL-00), Seattle, WA, 296?301.  
Bikel, D., R. Schwartz, and R. Weischedel.  1999.  
An Algorithm that Learns What?s in a Name.  
Machine Learning?Special Issue on NL 
Learning, 34, 1?3. 
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering.  
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 183?189.  
Buchholz, S. 2001. Using Grammatical Relations, 
Answer Frequencies and the World Wide Web for 
TREC Question Answering. Proceedings of the 
TREC-10 Conference. NIST, 496?503.  
Chen, J., A.R. Diekema, M.D. Taffet, N. McCracken, 
N. Ercan Ozgencil, O. Yilmazel, and E.D. Liddy. 
 2001. CNLP at TREC-10 QA Track. Proceedings 
of the TREC-10 Conference. NIST, 480?490. 
Clarke, C.L.A., G.V. Cormack, T.R. Lynam, C.M. Li, 
and G.L. McLearn. 2001. Web Reinforced 
Question Answering. Proceedings of the TREC-
10 Conference. NIST, 620?626.  
Clarke, C.L.A., G.V. Cormack, and T.R. Lynam. 
2001. Exploiting Redundancy in Question 
Answering. Proceedings of the SIGIR 
Conference. New Orleans, LA, 358?365.  
Fellbaum, Ch. (ed). 1998. WordNet: An Electronic 
Lexical Database. Cambridge: MIT Press. 
Harabagiu, S., D. Moldovan, M. Pasca, R. Mihalcea, 
M. Surdeanu, R. Buneascu, R. G?rju, V. Rus and 
P. Morarescu. 2001. FALCON: Boosting 
Knowledge for Answer Engines. Proceedings of 
the 9th Text Retrieval Conference (TREC-9), 
NIST, 479?488.  
Hermjakob, U. 1997. Learning Parse and 
Translation Decisions from Examples with Rich 
Context.  Ph.D. dissertation, University of Texas 
Austin. file://ftp.cs.utexas.edu/pub/mooney/paper 
s/hermjakob-dissertation 97.ps.gz.  
Hermjakob, U. 2001. Parsing and Question 
Classification for Question Answering. 
Proceedings of the Workshop on Question 
Answering at ACL-2001.  Toulouse, France.  
Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and 
C.-Y. Lin. 1999. Question Answering in 
Webclopedia.  Proceedings of the TREC-9 
Conference.  NIST. Gaithersburg, MD, 655?673. 
Hovy, E.H., U. Hermjakob, and D. Ravichandran. 
2002. A Question/Answer Typology with Surface 
Text Patterns.  Poster in Proceedings of the 
DARPA Human Language Technology 
Conference (HLT).  San Diego, CA, 234?238.   
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. The 
Use of External Knowledge in Factoid QA.   
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 166?174.  
Ittycheriah, A., M. Franz, and S. Roukos. 2001. 
IBM?s Statistical Question Answering System. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 317?323.  
Kwok, K.L., L. Grunfeld, N. Dinstl, and M. Chan. 
2001. TREC2001 Question-Answer, Web and 
Cross Language experiments using PIRCS. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 447?451.  
Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. Lee, 
B-K. Kwak, J, Cha, D. Kim, J-H. An, H. Kim, 
and K. Kim. 2001. SiteQ: Engineering High 
Performance QA System Using Lexico=Semantic 
Pattern Matching and Shallow NLP. Proceedings 
of the TREC-10 Conference. NIST, Gaithersburg, 
MD, 437?446.  
Light, M., G.S. Mann, E. Riloff, and E. Breck. 2001. 
Analyses for Elucidating Current Question 
Answering Technology. Natural Language 
Engineering, 7:4, 325?342.  
Lin, C.-Y. 2002. The Effectiveness of Dictionary and 
Web-Based Answer Reranking.  Proceedings of 
the 19th International Conference on 
Computational Linguistics (COLING 2002), 
Taipei, Taiwan.  
Oh, JH., KS. Lee, DS. Chang, CW. Seo, and KS. 
Choi. 2001. TREC-10 Experiments at KAIST: 
Batch Filtering and Question Answering. 
Proceedings of the TREC-10 Conference. NIST, 
Gaithersburg, MD, 354?361. 
Prager, J., E. Brown, D.R. Radev, and K. Czuba. 
1999. One Search Engine or Two for Question 
Answering. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 235?240. 
Ravichandran, D. and E.H. Hovy. 2002. Learning 
Surface Text Patterns for a Question Answering 
System. Proceedings of the ACL conference. 
Philadelphia, PA.  
Soubbotin, M.M. and S.M. Soubbotin. 2001. Patterns 
of Potential Answer Expressions as Clues to the 
Right Answer. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 175?182.   
Srihari, R. and W. Li. 2000. A Question Answering 
System Supported by Information Extraction. 
Proceedings of the 1st Meeting of the North 
American Chapter of the Association for 
Computational Linguistics (ANLP-NAACL-00), 
Seattle, WA, 166?172. 
Voorhees, E. 1999. Overview of the Question 
Answering Track. Proceedings of the TREC-9 
Conference. NIST, Gaithersburg, MD, 71?81.  
Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. Bu, 
and S. Bai. 2001. TREC-10 Experiments at CAS-
ICT: Filtering, Web, and QA. Proceedings of the 
TREC-10 Conference. NIST, 229?241.  
Witten, I.H., A. Moffat, and T.C. Bell. 1994. 
Managing Gigabytes: Compressing and Indexing 
Documents and Images. New York: Van 
Nostrand Reinhold. 
 
Fine Grained Classification of Named Entities 
 
Michael Fleischman and Eduard Hovy 
USC Information Science Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
U.S.A. 
{fleisch, hovy} @ISI.edu 
 
Abstract 
 
While Named Entity extraction is useful in 
many natural language applications, the 
coarse categories that most NE extractors 
work with prove insufficient for complex 
applications such as Question Answering and 
Ontology generation.  We examine one 
coarse category of named entities, persons, 
and describe a method for automatically 
classifying person instances into eight finer-
grained subcategories.  We present a 
supervised learning method that considers the 
local context surrounding the entity as well as 
more global semantic information derived 
from topic signatures and WordNet.  We 
reinforce this method with an algorithm that 
takes advantage of the presence of entities in 
multiple contexts. 
 
1. Introduction 
 
There has been much interest in the recent past 
concerning automated categorization of named 
entities in text.  Recent advances have made some 
systems (such as BBN?s IdentiFinder (Bikel, 
1999)) very successful when classifying named 
entities into broad categories, such as person, 
organization, and location.  While the accurate 
classification of general named entities is useful in 
many areas of natural language research, more fine-
grained categorizations would be of particular 
value in areas such as Question Answering, 
information retrieval, and the automated 
construction of ontologies.   
The research presented here focuses on the 
subcategorization of person names, which extends 
research on the subcategorization of location names 
(Fleischman, 2001).  While locations can often be 
classified based solely on the words that surround 
the instance, person names are often more 
challenging because classification relies on much 
deeper semantic intuitions gained from the 
surrounding text.  Further, unlike the case with 
location names, exhaustive lists of person names by 
category do not exist and cannot be relied upon for 
training and test set generation.  Finally, the domain 
of person names presents a challenge because the 
same individual (e.g., ?Ronald Reagan?) is often 
represented differently at different points in the 
same text (e.g., ?Mr. Reagan?, ?Reagan?, etc.). 
The subcategorization of person names is not a 
trivial task for humans either, as the examples 
below illustrate.  Here, names of persons have been 
encrypted using a simple substitution cipher.  The 
names are of only three subtypes: politician, 
businessperson, and entertainer, yet prove 
remarkably difficult to classify based upon the 
context of the sentence. 
 
1.  Unfortunately, Mocpm_____ and his immediate 
family did not cooperate in the making of the film . 
2. "The idea that they'd introduce Npn Fuasm______ 
into that is amazing ,"he said. 
3. "It's dangerous to be right when government is 
wrong ," Lrsyomh______ told reporters 
 
1. Mocpm = Nixon: politician 
2. Npn Fuasm = Bob Dylan: entertainer 
3. Lrsyomh = Keating: businessperson 
 
In this work we examine how different features 
and learning algorithms can be employed to 
automatically subcategorize person names in text.  
In doing this we address how to inject semantic 
information into the feature space, how to 
automatically generate training sets for use with 
supervised learning algorithms, and how to handle 
orthographic inconsistencies between instances of 
the same person. 
 
2. Data Set Generation 
 
A large corpus of person instances was 
compiled from a TREC9 database consisting of 
articles from the Associated Press and the Wall 
Street Journal.  Data was word tokenized, stemmed 
 using the Porter stemming algorithm (Porter, 1980), 
part of speech tagged using Brill?s tagger (Brill, 
1994), and named entity tagged using BBN?s 
IdentiFinder (Bikel, 1999).  Person instances were 
classified into one of eight categories: athlete, 
politician/government, clergy, businessperson, 
entertainer/artist, lawyer, doctor/scientist, and 
police.  These eight categories were chosen because 
of their high frequency in the corpus and also 
because of their usefulness in applications such as 
Question Answering.  A training set of roughly 
25,000 person instances was then created using a 
partially automated classification system. 
In generating the training data automatically we 
first attempted to use the simple tagging method 
described for location names in (Fleischman, 
2001).  This method involved collecting lists of 
instances of each category from the Internet and 
using those lists to classify person names found by 
IdentiFinder.  Although robust with location names, 
this method proved inadequate with persons (in a 
sample of 300, over 25% of the instances were 
found to be incorrect).  This was due to the fact that 
the same name will often refer to multiple 
individuals (e.g., ?Paul Simon? refers to a 
politician, an entertainer, and Belgian scientist). 
In order to avoid this problem we implemented 
a simple bootstrapping procedure in which a seed 
data set of 100 instances of each of the eight 
categories was hand tagged and used to generate a 
decision list classifier using the C4.5 algorithm 
(Quinlan, 1993) with the word frequency and topic 
signature features described below.  This simple 
classifier was then run over a large corpus and 
classifications with a confidence score above a 
90% threshold were collected.  These confident 
instances were then compared to the lists collected 
from the Internet, and, only if there was agreement 
between the two sources, were the instances 
included in the final training set.  This procedure 
produced a large training set with very few 
misclassified instances (over 99% of the instances 
in a sample of 300 were found to be correct).  A 
validation set of 1000 instances from this set was 
then hand tagged to assure proper classification. 
A consequence of using this method for data 
generation is that the training set created is not a 
random sample of person instances in the real 
world.  Rather, the training set is highly skewed, 
including only those instances that are both easy 
enough to classify using a simple classifier and 
common enough to be included in lists found on 
the Internet.  To examine the generalizability of 
classifiers trained on such data, a held out data set 
of 1300 instances, also from the AP and WSJ, was 
collected and hand tagged.   
 
3. Features 
 
3.1 Word Frequency Features 
 
Each instance in the text is paired with a set of 
features that represents how often the words 
surrounding the target instance occur with a 
specific sub-categorization in the training set.  For 
example, in example sentence 2 in the introduction, 
the word ?introduce? occurs immediately before 
the person instance.  The feature set describing this 
instance would thus include eight different features; 
each denoting the frequency with which 
?introduce? occurred in the training set 
immediately preceding an instance of a politician, a 
businessperson, an entertainer, etc.  The feature set 
includes these eight different frequencies for 10 
distinct word positions (totaling 80 features per 
instance).  The positions used include the three 
individual words before the occurrence of the 
instance, the three individual words after the 
instance, the two-word bigrams immediately before 
and after the instance, and the three-word trigrams 
immediately before and after the instance (see 
Figure 1). 
 
# Position N-gram Category Freq. 
1 previous unigram ?introduce? politician 3 
2 previous unigram ?introduce? entertainer 43 
3 following bigram ?into that? politician 2 
4 following bigram ?into that? business 0 
Figure 1.  Subset of word frequency features for instance in 
example 2, above.  Shows the frequency with which an n-gram 
appears in the training data in a specific position relative to 
instances of a specific category. 
 
These word frequency features provide 
information similar to the binary word features that 
are often used in text categorization (Yang, 1997) 
with only a fraction of the dimensionality.  Such 
reduced dimensionality feature sets can be 
preferable when classifying very small texts 
(Fleischman, in preparation).  
 
3.2 Topic Signature Features 
 
Inspection of the data made clear the need for 
semantic information during classification.  We 
therefore created features that use topic signatures 
 for each of the person subcategories.  A topic 
signature, as described in (Lin and Hovy, 2000), is 
a list of terms that can be used to signal the 
membership of a text in the relevant topic or 
category.  Each term in a text is given a topic 
signature score that indicates its ability to signal 
that the text is in a relevant category (the higher the 
score, the more that term is indicative of that 
category).  The topic signatures are automatically 
generated for each specific term by computing the 
likelihood ratio (?-score) between two hypotheses 
(Dunning, 1993).  The first hypothesis (h1) is that 
the probability (p1) that the text is in the relevant 
category, given a specific term, is equivalent to the 
probability (p2) that the text is in the relevant 
category, given any other term (h1: p1=p2).  The 
second hypothesis (h2) is that these two 
probabilities are not equivalent, and that p1 is much 
greater than p2 (h2: p1>>p2).  The calculation of 
this likelihood ratio [-2logL(h1)/L(h2)] for each 
feature and for each category gives a list of all the 
terms in a document set with scores indicating how 
much the presence of that term in a specific 
document indicates that the document is in a 
specific category.  
 
Politician Entertainer 
Word ?-score Word ?-score 
campaign 3457.049 Star 3283.872 
republican 1969.707 Actor 2478.675 
budget 140.292 Budget 17.312 
bigot 2.577 Sexist 3.874 
Figure 2.  Subset of topic signatures generated from training set 
for two categories. 
 
In creating topic signature features for the 
subcategorization of persons, we created a database 
of topic signatures generated from the training set 
(see Figure 2).1  Each sentence from the training set 
was treated as a unique document, and the 
classification of the instance contained in that 
sentence was treated as the relevant topic.  We 
implemented the algorithm described in (Lin and 
Hovy, 2000) with the addition of a cutoff, such that 
the topic signatures for a term are only included if 
the p1/p2 for that term is greater than the mean 
p1/p2 over all terms.  This modification was made 
to ensure the assumption that p1 is much greater 
than p2.  A weighted sum was then computed for 
each of the eight person subcategories according to 
the formula below: 
                                                 
1 To avoid noise, we used only those sentences in which 
each person instance was of the same category. 
 
Topic Sig ScoreType= ?N [ ?-score of wordn,Type 
          /(distance from instance)2] 
 
where N is the number of words in the sentence, 
?-score of wordn,Type is the topic signature score of 
word n for topic Type, and distance from instance 
is the number of words away from the instance 
that word n is.  These topic signature scores are 
calculated for each of the eight subcategories.   
These eight topic signature features convey 
semantic information about the overall context 
in which each instance exists.  The topic 
signature scores are weighted according to the 
inverse square of their distance under the (not 
always true) assumption that the farther away a 
word is from an instance, the less information it 
bears on classification.  This weighting is 
particularly important when instances of 
different categories occur in the same sentence 
(e.g., ??of those donating to Bush?s campaign 
was actor Arnold Schwarzenegger??). 
 
3.3 WordNet Features 
 
A natural limitation of the topic signature 
features is their inability to give weight to 
related and synonymous terms that do not 
appear in the training data.  To address this 
limitation, we took advantage of the online 
resource WordNet (Fellbaum, 1998).  The 
WordNet hypernym tree was expanded for each 
word surrounding the instance and each word in 
the tree was given a score based on the topic 
signature database generated from the training 
data.  The scores were then weighted by the 
inverse of their height in the tree and then 
summed together, similarly to the procedure in 
(Resnik, 1993).  These sums are computed for 
each word surrounding the instance, and are 
summed according to the weighting process 
described above.  This produces a distinct 
WordNet feature for each of the eight classes 
and is described by the equation below: 
 
WordNet Score Type=  
?N[?M ?-score of wordm,Type/(depth of wordm in WordNet)] 
/(distance from instance) 2 
 
where the variables are as above and M is the 
number of words in the WordNet hypernym 
tree.  These WordNet features supplement the 
coverage of the topic signatures generated from 
the training data by including synonyms that 
 may not have existed in that data set.  Further, 
the features include information gained from the 
hypernyms themselves (e.g., the hypernym of 
?Congress? is ?legislature?).  These final 
hypernym scores are weighted by the inverse of 
their height in the tree to reduce the effect of 
concepts that may be too general (e.g., at the top 
of the hypernym tree for ?Congress? is 
?group?).  In order to avoid noise due to 
inappropriate word senses, we only used data 
from senses that matched the part of speech.  
These eight WordNet features add to the above 
features for a total of 96 features. 
 
4. Methods 
 
4.1 Experiment 1: Held out data 
 
 
To examine the generalizability of classifiers 
trained on the automatically generated data, a C4.5 
decision tree classifier (Quinlan, 1993) was trained 
and tested on the held out test set described above.   
Initial results revealed that, due to differing 
contexts, instances of the same name in a single 
text would often be classified into different 
subcategories.  To deal with this problem, we 
augmented the classifier with another program, 
MemRun, which standardizes the subcategorization 
of instances based on their most frequent 
classification.  Developed and tested in 
(Fleischman, 2001), MemRun is based upon the 
hypothesis that by looking at all the classifications 
an instance has received throughout the test set, an 
?average? sub-categorization can be computed that 
offers a better guess than a low confidence 
individual classification. 
MemRun operates in two rounds.  In the first 
round, each instance of the test set is evaluated 
using the decision tree, and a classification 
hypothesis is generated.  If the confidence level of 
this hypothesis is above a certain threshold 
(THRESH 1), then the hypothesis is entered into 
the temporary database (see Figure 3) along with 
the degree of confidence of that hypothesis, and the 
number of times that hypothesis has been received. 
Because subsequent occurrences of person 
instances frequently differ orthographically from 
their initial occurrence (e.g., ?George Bush? 
followed by ?Bush?) a simple algorithm was 
devised for surface reference disambiguation.  The 
algorithm keeps a record of initial full name usages 
of all person instances in a text.  When partial 
references to the instance are later encountered in 
the text, as determined by simple regular 
expression matching, they are entered into the 
MemRun database as further occurrences of the 
original instance.  This record of full name 
references is cleared after a text is examined to 
avoid possible instance confusions (e.g., ?George 
W. Bush? and ?George Bush Sr.?).  This simple 
algorithm operates on the assumption that partial 
references to individuals with the same last name in 
the same text will not occur due to human authors? 
desire to avoid any possible confusion.2  When all 
of the instances in the data set are examined, the 
round is complete.  
In MemRun?s second round, the data set is 
reexamined, and hypothesis classifications are 
again produced.  If the confidence of one of these 
hypotheses is below a second threshold (THRESH 
2), then the hypothesis is ignored and the database 
value is used.3  In this experiment, the entries in the 
database are compared and the most frequent entry 
(i.e., the max classification based on confidence 
level multiplied by the increment) is returned.  
When all instances have been again examined, the 
round is complete.   
 
Figure 3. MemRun database for Decision Tree classifier 
 
4.2 Experiment 2: Learning Algorithms 
 
Having examined the generalizability when 
using automatically generated training data, we turn 
to the question of appropriate learning algorithms 
for the task.  We chose to examine five different 
learning algorithms.  Along with C4.5, we 
examined a feed-forward neural network with 50 
hidden units, a k-Nearest Neighbors 
implementation (k=1) (Witten & Frank, 1999), a 
Support Vector Machine implementation using a 
linear kernel (Witten & Frank, 1999), and a na?ve 
Bayes classifier using discretized attributes and 
                                                 
2 This algorithm does not address definite descriptions and 
pronominal references because they are not classified by 
IdentiFinder as people names, and thus are not marked for 
fine-grained classification in the test set. 
3 The ability of the algorithm to ignore the database?s 
suggestion in the second round allows instances with the 
same name (e.g., ?Paul Simon?) to receive different 
classifications in different contexts. 
Instance Class Confidence Occur 
George Bush Politician 97.5% 4 
 Business 83.4% 1 
Dana Carvey Entertainer 92.4% 7 
 Politician 72.1% 2 
 with feature subset selection (Kohavi & 
Sommerfield, 1996).  For each classifier, 
comparisons were based on results from the 
validation set (~1000 instances) described above. 
 
4.3 Experiment 3: Feature sets 
 
To examine the effectiveness of the individual 
types of features, a C4.5 decision tree classifier 
(Quinlan, 1993) was trained on the 25,000 instance 
data set described above using all possible 
combinations of the three feature sets.  The 
performance was ascertained on the validation set 
described above. 
 
5. Results 
 
5.1 Experiment 1: Held out data 
 
47.3
70.4
83.5
57
60.9
70.1
40
50
60
70
80
% 
Co
rre
ct
Validation Held out
Baseline No MemRun MemRun
 
Figure 4.  Results of classifier on validation set and held 
out data.  Results compare baseline of always choosing 
most probable class with C4.5 classifier both with and 
without MemRun.   
 
The results of the classifier on both the 
validation set and the held out test set can be seen 
in Figure 4.  The results are presented for a 
classifier trained using the C4.5 algorithm both 
with and without MemRun (THRESH1=85, 
THRESH2=98).  Also shown is the baseline score 
for each test set computed by always choosing the 
most frequent classification (Politician for both).   
It is clear from the figure that the classifiers for 
both test sets and for both conditions performed 
better than baseline.  Also clear is that the MemRun 
algorithm significantly improves performance on 
both the validation and held out test sets.   
Figure 4 further shows a large discrepancy 
between the performance of the classifier on the 
two data sets.  Expectedly, the validation set is 
classified more easily both with and without 
MemRun.  The size of the discrepancy is a function 
of how different the distribution of the training set 
is from the true distribution of person instances in 
the world.  While this discrepancy is undeniable, it 
is interesting to note how well the classifier 
generalizes given the very biased sample upon 
which it was trained. 
 
5.2 Experiment 2: Learning Algorithms 
 
47.3
57.7
64
68.1 69.5
70.4
40
45
50
55
60
65
70
75
%
 C
or
re
ct
Baseline k-NN Na?ve
Bayes
SVM Neural Net C4.5
 Figure 5. Comparison of different learning algorithms on 
a validation set.  Learners include: k-Nearest Neighbors, 
Na?ve Bayes, support vector machine, neural network, and 
C4.5 decision tree. 
 
Figure 5 shows the results of comparing 
different machine learning strategies.  It is clear 
from the figure that all the algorithms perform 
better than the baseline score, while the C4.5 
algorithm performs the best.  This is not 
surprising as decision trees combine powerful 
aspects of non-linear separation and feature 
selection.   
Interestingly, however, there is no clear 
relationship between performance and the 
theoretical foundations of the classifier.    
Although the two top performers (decision tree 
and the neural network) are both non-linear 
classifiers, the linear SVM outperforms the non-
linear k-Nearest Neighbors.  This must, 
however, be taken with a grain of salt, as little 
was done to optimize either the k-NN or SVM 
implementation. 
Another interesting finding in recent work 
is an apparent relationship between classifier 
type and performance on held out data.  While 
the non-parametric learners, i.e. C4.5 and k-NN, 
are fairly robust to generalization, the 
parametric learners, i.e. Na?ve Bayes and SVM, 
perform significantly worse on the new 
distribution.  In future work, we intend to 
examine further this possible relationship. 
 
 5.3 Experiment 3: Feature sets 
 
The results of the feature set experiment can 
be seen in figure 6.  Results are shown for the 
validation set using all combinations of the 
three feature sets.  A baseline measure of 
always classifying the most frequent category 
(Politician) is also displayed.  
It is clear that each of the single feature sets 
(frequency features, topic signature features, 
and WordNet features) is sufficient to 
outperform the baseline.  Interestingly, topic 
signature features outperform WordNet 
features, even though they are similar in form.  
This suggests that the WordNet features are 
noisy and may contain too much generality.  It 
may be more appropriate to use a cutoff, such 
that only the concepts two levels above the term 
are examined.  Another source of noise comes 
from words with multiple senses.  Although our 
method uses only word senses of the 
appropriate part of speech, WordNet still often 
provides many different possible senses.  
 
47.3
59.4
63.6
67.1
60.7
68 68.1
70.4
40
45
50
55
60
65
70
75
%
 C
or
re
ct
Baseline Freq WN Sig Freq & WN Freq & Sig Sig & WN All
 Figure 6. Results of using different combinations of 
feature sets.  Results shown on validation set using C4.5 
classifier without MemRun. 
 
Also of interest is the effect of combining 
any two feature sets.  While using topic 
signatures and either word frequencies or 
WordNet features improves performance by a 
small amount, combining frequency and 
WordNet scores results in performance worse 
than WordNet alne.  This suggests over fitting 
of the training data and may be due to the noise 
in the WordNet features. 
It is clear, however, that the combination of 
all three features provides considerable 
improvement in performance over any of the 
individual features.  In future work we will 
examine how ensemble learning (Hastie, 2001) 
might be used to capitalize further on these 
qualitatively different feature sets. 
 
6. Related Work 
 
While much research has gone into the coarse 
categorization of named entities, we are not aware 
of much previous work using learning algorithms 
to perform more fine-grained classification.   
Wacholder et al (1997) use hand-written rules 
and knowledge bases to classify proper names into 
broad categories.  They employ an aggregation 
method similar to MemRun, but do not use 
multiple thresholds to increase accuracy. 
MacDonald (1993) also uses hand-written rules 
for coarse named entity categorization.  However, 
where Wacholder et al use evidence internal to the 
entity name, MacDonald employs local context to 
aid in classification.  Such hand-written heuristic 
rules resemble those we automatically generate. 
Bechet et al (2000) use a decision tree 
algorithm to classify unknown proper names into 
the categories: first name, last name, country, town, 
and organization.  This is still a much coarser 
distinction than that focused on in this research.  
Further, Bechet et al focused only on those proper 
names embedded in complex noun phrases (NPs), 
using only elements in the NP as its feature set.   
 
7. Conclusions 
 
The results of these experiments, though 
preliminary, are very promising.  Our research 
makes clear that positive results are possible 
with relatively simple statistical techniques.  
This research has shown that training data 
construction is critical.  The failure of our 
automatic data generation algorithm to produce 
a good sample of training data is evident in the 
large disparity between performances on 
validation and held out test sets.  There are at 
least two reasons for the algorithm?s poor 
sampling. 
First, by using only high confidence guesses 
from the seed trained classifier, the training data 
may have a disproportionate number of 
instances that are easy to classify.  This is 
evident in the number of partial names that are 
present in the held out test set versus the 
training set.  Partial names, such as ?Simon? 
instead of ?Paul Simon,? usually occur with 
weaker evidence for classification than full 
 names.  In the training set only 45.1% of the 
instances are partial names, whereas in the more 
realistic distribution of the held out set, 58.4% 
are partial names. 
The second reason for the poor sampling 
stems from the use of lists of person names.  
Because the training set is derived from 
individuals in these lists, the coverage of 
individuals included in the training set is 
inherently limited.  For example, in the 
businessperson category, lists of individuals 
were taken from such resources as Forbes? 
annual ranking of the nation?s wealthiest 
people, under the assumption that wealthy 
people are often in the news.  However, the list 
fails to mention the countless vice presidents 
and analysts that frequent the pages of the Wall 
Street Journal.  This failure to include such 
lower level businesspersons means that a large 
space of the classification domain is not 
covered by the training set, which in turn leads 
to poor results on the held out test set. 
The results of these experiments suggest 
that better fine-grained classification of named 
entities will require not only more sophisticated 
feature selection, but also a better data 
generation procedure.  In future work, we will 
investigate more sophisticated bootstrapping 
methods, as (Collins & Singer, 1999) as well as 
co-training and co-testing (Muslea et al, 2000).   
In future work we will also examine 
adapting the hierarchical decision list algorithm 
from (Yarowsky, 2000) to our task.  Treating 
fine-grained classification of named entities as a 
word sense disambiguation problem (where 
categories are treated as different senses of a 
generic ?person name?) allows these methods to 
be directly applicable.  The algorithm is 
particularly relevant in that it provides an 
intuitive way to take advantage of the 
similarities of certain categories (e.g., Athlete 
and Entertainer). 
Of more theoretical concern are the 
problems of miscellaneous classifications that 
do not fit easily into any category, as well as, 
instances that may fit into more than one 
category (e.g., Ronald Reagan can be either a 
Politician or an Entertainer).  We plan to 
address these issues as well as problems that 
may arise with extending this system for use 
with other classes, such as organizations.   
 
8. References 
 
Bechet, F., Nasr, A., Genet, F.  2000.  Tagging unknown proper 
names using decision trees.  Proc. of ACL, Hong Kong. 
 
Bikel, D., Schwartz, R., Weischedel, R.  1999.  An algorithm 
that learns what?s in a name.  Machine Learning: Special 
Issue on NL Learning, 34, 1-3. 
 
Brill E. 1994. Some advances in rule based part of speech 
tagging.  Proc. of AAAI, Los Angeles. 
 
Collins, M. and Singer, Y. 1999. Unsupervised models for 
named entity classification. Proc. of the Joint SIGDAT 
Conference on Empirical Methods in Natural Language 
Processing and Very Large Corpora.  
 
Dunning, T., 1993. Accurate methods for statistics of surprise 
and coincidence. Computational Linguistics, 19:61--74. 
 
Fellbaum, C. (ed.), 1998. An electronic lexical database. 
Cambridge, MA: MIT Press.  
 
Fleischman, M. 2001.  Automated Subcategorization of Named 
Entities.  Proc. of the ACL Student Workshop.  
 
Hastie, T., Tibshirani, R., and J. Friedman. 2001.  The 
Elements of Statistical Learning. Springer. 
 
Jurafsky, D. and Martin, J..  2000.  Speech and Language 
Processing.  Upper Saddle River, NJ: Prentice Hall. 
 
Kohavi,, R., Sommerfield, D., and Dougherty, J, 1996.  
Data mining using MLC++ : A machine learning library 
in C ++. Tools with Artificial Intelligence, pp. 234-245. 
 
Lin, C.-Y. and E.H. Hovy, 2000. The Automated 
Acquisition of Topic Signatures for Text 
Summarization. Proc. of the COLING Conference. 
Strasbourg, France.  
 
MacDonald D.D., 1993.  Internal and external evidence in the 
identification and semantic categorization of proper names. 
In B.Boguraev and J. Pustejovsky, eds., Corpus Processing 
for Lexical Acquisition, pp. 61-76, Cambridge: MIT Press. 
 
Muslea, I., Minton, S., Knoblock, C.  Selective sampling 
with redundant views.  Proc. of the 15th National 
Conference on Artificial Intelligence, AAAI-2000.  
 
Porter, M. F. 1980.  An algorithm for suffix stripping. 
Program, 14 (no. 3), 130-137. 
 
Quinlin, J.R., 1993.  C4.5: Programs for Machine Learning.  
San Mateo, CA: Morgan Kaufmann Publishers. 
 
Resnik, P. 1993. Selection and Information. PhD thesis, 
University of Pennsylvania. 
 
Wacholder, N., Ravin, Y., Choi, M. 1997.  Disambiguation of 
Proper Names in Text.  Proc. of the Fifth Conference on 
Applied Natural Language Processing, Washington, D.C. 
Witten, I. & Frank, E. 1999.  Data Mining: Practical 
Machine Learning Tools and Techniques with JAVA 
implementations.  Morgan Kaufmann, October. 
Yarowsky D. 2000. Hierarchical Decision List for Word 
Sense Disambiguation. Computers and the Humanities. 
34: 179-186. 
Yang, Y., Pedersen, J.O., 1997.  A Comparative Study on 
Feature Selection in Text Categorization, Proc. of the 
14th International Conference on Machine Learning 
ICML97, pp. 412-420 
 
Towards Terascale Knowledge Acquisition 
Patrick Pantel, Deepak Ravichandran and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA  90292 
{pantel,ravichan,hovy}@isi.edu 
 
Abstract 
Although vast amounts of textual data are freely 
available, many NLP algorithms exploit only a 
minute percentage of it. In this paper, we study the 
challenges of working at the terascale. We present 
an algorithm, designed for the terascale, for mining  
is-a relations that achieves similar performance to a 
state-of-the-art linguistically-rich method. We fo-
cus on the accuracy of these two systems as a func-
tion of processing time and corpus size. 
1 Introduction 
The Natural Language Processing (NLP) com-
munity has recently seen a growth in corpus-based 
methods. Algorithms light in linguistic theories but 
rich in available training data have been success-
fully applied to several applications such as ma-
chine translation (Och and Ney 2002), information 
extraction (Etzioni et al 2004), and question an-
swering (Brill et al 2001). 
In the last decade, we have seen an explosion in 
the amount of available digital text resources. It is 
estimated that the Internet contains hundreds of 
terabytes of text data, most of which is in an 
unstructured format. Yet, many NLP algorithms 
tap into only megabytes or gigabytes of this 
information. 
In this paper, we make a step towards acquiring 
semantic knowledge from terabytes of data. We 
present an algorithm for extracting is-a relations, 
designed for the terascale, and compare it to a state 
of the art method that employs deep analysis of 
text (Pantel and Ravichandran 2004). We show 
that by simply utilizing more data on this task, we 
can achieve similar performance to a linguistically-
rich approach. The current state of the art co-
occurrence model requires an estimated 10 years 
just to parse a 1TB corpus (see Table 1). Instead of 
using a syntactically motivated co-occurrence ap-
proach as above, our system uses lexico-syntactic 
rules. In particular, it finds lexico-POS patterns by 
making modifications to the basic edit distance 
algorithm. Once these patterns have been learnt, 
the algorithm for finding new is-a relations runs in 
O(n), where n is the number of sentences. 
In semantic hierarchies such as WordNet (Miller 
1990), an is-a relation between two words x and y 
represents a subordinate relationship (i.e. x is more 
specific than y). Many algorithms have recently 
been proposed to automatically mine is-a (hypo-
nym/hypernym) relations between words. Here, we 
focus on is-a relations that are characterized by the 
questions ?What/Who is X?? For example, Table 2 
shows a sample of 10 is-a relations discovered by 
the algorithms presented in this paper. In this table, 
we call azalea, tiramisu, and Winona Ryder in-
stances of the respective concepts flower, dessert 
and actress. These kinds of is-a relations would be 
useful for various purposes such as ontology con-
struction, semantic information retrieval, question 
answering, etc. 
The main contribution of this paper is a compari-
son of the quality of our pattern-based and co-
occurrence models as a function of processing time 
and corpus size. Also, the paper lays a foundation 
for terascale acquisition of knowledge. We will 
show that, for very small or very large corpora or 
for situations where recall is valued over precision, 
the pattern-based approach is best. 
2 Relevant Work 
Previous approaches to extracting is-a relations 
fall under two categories: pattern-based and co-
occurrence-based approaches. 
2.1 Pattern-based approaches 
Marti Hearst (1992) was the first to use a pat-
tern-based approach to extract hyponym relations 
from a raw corpus. She used an iterative process to 
semi-automatically learn patterns. However, a 
corpus of 20MB words yielded only 400 examples. 
Our pattern-based algorithm is very similar to the 
one used by Hearst. She uses seed examples to 
manually discover her patterns whearas we use a 
minimal edit distance algorithm to automatically 
discover the patterns. 
771
Riloff and Shepherd (1997) used a semi-
automatic method for discovering similar words 
using a few seed examples by using pattern-based 
techniques and human supervision. Berland and 
Charniak (1999) used similar pattern-based tech-
niques and other heuristics to extract meronymy 
(part-whole) relations. They reported an accuracy 
of about 55% precision on a corpus of 100,000 
words. Girju et al (2003) improved upon Berland 
and Charniak's work using a machine learning 
filter. Mann (2002) and Fleischman et al (2003) 
used part of speech patterns to extract a subset of 
hyponym relations involving proper nouns. 
Our pattern-based algorithm differs from these 
approaches in two ways. We learn lexico-POS 
patterns in an automatic way. Also, the patterns are 
learned with the specific goal of scaling to the 
terascale (see Table 2). 
2.2 Co-occurrence-based approaches 
The second class of algorithms uses co-
occurrence statistics (Hindle 1990, Lin 1998). 
These systems mostly employ clustering algo-
rithms to group words according to their meanings 
in text. Assuming the distributional hypothesis 
(Harris 1985), words that occur in similar gram-
matical contexts are similar in meaning. Curran 
and Moens (2002) experimented with corpus size 
and complexity of proximity features in building 
automatic thesauri. CBC (Clustering by Commit-
tee) proposed by Pantel and Lin (2002) achieves 
high recall and precision in generating similarity 
lists of words discriminated by their meaning and 
senses. However, such clustering algorithms fail to 
name their classes. 
Caraballo (1999) was the first to use clustering 
for labeling is-a relations using conjunction and 
apposition features to build noun clusters. Re-
cently, Pantel and Ravichandran (2004) extended 
this approach by making use of all syntactic de-
pendency features for each noun. 
3 Syntactical co-occurrence approach 
Much of the research discussed above takes a 
similar approach of searching text for simple sur-
face or lexico-syntactic patterns in a bottom-up 
approach. Our co-occurrence model (Pantel and 
Ravichandran 2004) makes use of semantic classes 
like those generated by CBC. Hyponyms are gen-
erated in a top-down approach by naming each 
group of words and assigning that name as a hypo-
nym of each word in the group (i.e., one hyponym 
per instance/group label pair). 
The input to the extraction algorithm is a list of 
semantic classes, in the form of clusters of words, 
which may be generated from any source. For ex-
ample, following are two semantic classes discov-
ered by CBC: 
(A) peach, pear, pineapple, apricot, 
mango, raspberry, lemon, cherry, 
strawberry, melon, blueberry, fig, apple, 
plum, nectarine, avocado, grapefruit, 
papaya, banana, cantaloupe, cranberry, 
blackberry, lime, orange, tangerine, ... 
(B) Phil Donahue, Pat Sajak, Arsenio 
Hall, Geraldo Rivera, Don Imus, Larry King, 
David Letterman, Conan O'Brien, Rosie 
O'Donnell, Jenny Jones, Sally Jessy Raph-
ael, Oprah Winfrey, Jerry Springer, Howard 
Stern, Jay Leno, Johnny Carson, ... 
The extraction algorithm first labels concepts 
(A) and (B) with fruit and host respectively. Then, 
is-a relationships are extracted, such as: apple is a 
fruit, pear is a fruit, and David Letterman is a host. 
An instance such as pear is assigned a hypernym 
fruit not because it necessarily occurs in any par-
ticular syntactic relationship with the word fruit, 
but because it belongs to the class of instances that 
does. The labeling of semantic classes is performed 
in three phases, as outlined below. 
3.1 Phase I 
In the first phase of the algorithm, feature vec-
tors are extracted for each word that occurs in a 
semantic class. Each feature corresponds to a 
grammatical context in which the word occurs. For 
example, ?catch __? is a verb-object context. If the 
word wave occurred in this context, then the con-
text is a feature of wave. 
We then construct a mutual information vector 
MI(e) = (mie1, mie2, ?, miem) for each word e, 
where mief is the pointwise mutual information 
between word e and context f, which is defined as: 
 
N
c
N
c
N
c
ef m
j
ej
n
i
if
ef
mi
??
== ?
=
11
log  
Table 2. Sample of 10 is-a relationships discovered by 
our co-occurrence and pattern-based systems.  
CO-OCCURRENCE SYSTEM PATTERN-BASED SYSTEM 
Word Hypernym Word Hypernym 
azalea flower American airline 
bipolar disorder disease Bobby Bonds coach 
Bordeaux wine radiation therapy cancer 
treatment 
Flintstones television show tiramisu dessert 
salmon fish Winona Ryder actress 
 
Table 1. Approximate processing time on a single 
Pentium-4 2.5 GHz machine. 
TOOL 15 GB ORPUS 1 TB CORPUS 
POS Tagger 2 days 125 days 
NP Chunker 3 days 214 days 
Dependency Parser 56 days 10.2 years 
Syntactic Parser 5.8 years 388.4 years 
 
772
where n is the number of elements to be clustered, 
cef is the frequency count of word e in grammatical 
context f, and N is the total frequency count of all 
features of all words. 
3.2 Phase II 
Following (Pantel and Lin 2002), a committee 
for each semantic class is constructed. A commit-
tee is a set of representative elements that unambi-
guously describe the members of a possible class. 
For example, in one of our experiments, the com-
mittees for semantic classes (A) and (B) from Sec-
tion 3 were: 
A) peach, pear, pineapple, apricot, mango, 
raspberry, lemon, blueberry 
B) Phil Donahue, Pat Sajak, Arsenio Hall, 
Geraldo Rivera, Don Imus, Larry King, 
David Letterman 
3.3 Phase III 
By averaging the feature vectors of the commit-
tee members of a particular semantic class, we 
obtain a grammatical template, or signature, for 
that class. For example, Figure 1 shows an excerpt 
of the grammatical signature for semantic class 
(B). The vector is obtained by averaging the fea-
ture vectors of the words in the committee of this 
class. The ?V:subj:N:joke? feature indicates a sub-
ject-verb relationship between the class and the 
verb joke while ?N:appo:N:host? indicates an ap-
position relationship between the class and the 
noun host. The two columns of numbers indicate 
the frequency and mutual information scores. 
To name a class, we search its signature for cer-
tain relationships known to identify class labels. 
These relationships, automatically learned in 
(Pantel and Ravichandran 2004), include apposi-
tions, nominal subjects, such as relationships, and 
like relationships. We sum up the mutual informa-
tion scores for each term that occurs in these rela-
tionships with a committee of a class. The highest 
scoring term is the name of the class. 
The syntactical co-occurrence approach has 
worst-case time complexity O(n2k), where n is the 
number of words in the corpus and k is the feature-
space (Pantel and Ravichandran 2004). Just to 
parse a 1 TB corpus, this approach requires ap-
proximately 10.2 years (see Table 2). 
4 Scalable pattern-based approach 
We propose an algorithm for learning highly 
scalable lexico-POS patterns. Given two sentences 
with their surface form and part of speech tags, the 
algorithm finds the optimal lexico-POS alignment. 
For example, consider the following 2 sentences: 
1) Platinum is a precious metal. 
2) Molybdenum is a metal. 
Applying a POS tagger (Brill 1995) gives the 
following output: 
 
Surface Platinum is a precious metal . 
POS NNP VBZ DT JJ NN . 
 
Surface Molybdenum is a metal . 
POS NNP VBZ DT NN . 
 
A very good pattern to generalize from the 
alignment of these two strings would be 
 
Surface  is a  metal . 
POS NNP     . 
 
We use the following notation to denote this 
alignment: ?_NNP is a (*s*) metal.?, where  
?_NNP represents the POS tag NNP?. 
To perform such alignments we introduce two 
wildcard operators, skip (*s*) and wildcard (*g*). 
The skip operator represents 0 or 1 instance of any 
word (similar to the \w* pattern in Perl), while the 
wildcard operator represents exactly 1 instance of 
any word (similar to the \w+ pattern in Perl). 
4.1 Algorithm 
We present an algorithm for learning patterns at 
multiple levels. Multilevel representation is de-
fined as the different levels of a sentence such as 
the lexical level and POS level. Consider two 
strings a(1, n) and b(1, m) of lengths n and m re-
spectively. Let a1(1, n) and a2(1, n) be the level 1 
(lexical level) and level 2 (POS level) representa-
tions for the string a(1, n). Similarly, let b1(1, m) 
and b2(1, m) be the level 1 and level 2 representa-
tions for the string b(1, m). The algorithm consists 
of two parts: calculation of the minimal edit dis-
tance and retrieval of an optimal pattern. The 
minimal edit distance algorithm calculates the 
number of edit operations (insertions, deletions and 
replacements) required to change one string to 
another string. The optimal pattern is retrieved by 
{Phil Donahue,Pat Sajak,Arsenio Hall} 
 N:gen:N  
  talk show 93 11.77 
  television show 24 11.30 
  TV show 25 10.45 
  show 255 9.98 
  audience 23 7.80 
  joke 5 7.37 
 V:subj:N  
  joke 39 7.11 
  tape 10 7.09 
  poke 15 6.87 
  host 40 6.47 
  co-host 4 6.14 
  banter 3 6.00 
  interview 20 5.89 
 N:appo:N  
  host 127 12.46 
  comedian 12 11.02 
  King 13 9.49 
  star 6 7.47 
Figure 1. Excerpt of the grammatical signature for the 
television host class. 
 
773
keeping track of the edit operations (which is the 
second part of the algorithm). 
Algorithm for calculating the minimal edit distance 
between two strings 
D[0,0]=0 
for i = 1 to n do  D[i,0] = D[i-1,0] + cost(insertion) 
for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) 
for i = 1 to n do 
 for j = 1 to m do 
  D[i,j] = min( D[i-1,j-1] + cost(substitution), 
        D[i-1,j] + cost(insertion), 
        D[i,j-1] + cost(deletion)) 
Print (D[n,m]) 
Algorithm for optimal pattern retrieval 
i = n, j = m; 
while i ? 0 and j ? 0 
 if D[i,j] = D[i-1,j] + cost(insertion) 
  print (*s*), i = i-1 
 else if D[i,j] = D[i,j-1] + cost(deletion) 
  print(*s*), j = j-1 
 else if a1i = b1j 
  print (a1i), i = i -1, j = j =1 
 else if a2i = b2j 
  print (a2i), i = i -1, j = j =1 
 else 
  print (*g*), i = i -1, j = j =1 
We experimentally set (by trial and error): 
cost(insertion)  = 3 
cost(deletion)  = 3 
cost(substitution) = 0 if a1i=b1j 
  = 1 if a1i?b1j, a2i=b2j 
  = 2 if a1i?b1j, a2i?b2j 
4.2 Implementation and filtering 
The above algorithm takes O(y2) time for every 
pair of strings of length at most y. Hence, if there 
are x strings in the collection, each string having at 
most length y, the algorithm has time complexity 
O(x2y2) to extract all the patterns in the collection. 
Applying the above algorithm on a corpus of 
3GB  with 50 is-a relationship seeds, we obtain a 
set of 600 lexico-POS. Following are two of them: 
1) X_JJ#NN|JJ#NN#NN|NN _CC Y_JJ#JJ#NN|JJ 
|NNS|NN|JJ#NNS|NN#NN|JJ#NN|JJ#NN#NN 
e.g. ?caldera or lava lake? 
2) X_NNP#NNP|NNP#NNP#NNP#NNP#NNP#CC#NNP 
|NNP|VBN|NN#NN|VBG#NN|NN ,_, _DT 
Y_NN#IN#NN|JJ#JJ#NN|JJ|NN|NN#IN#NNP 
|NNP#NNP|NN#NN|JJ#NN|JJ#NN#NN 
e.g. ?leukemia, the cancer of ... 
Note that we store different POS variations of 
the anchors X and Y. As shown in example 1, the 
POS variations of the anchor X are (JJ NN, JJ NN 
NN, NN). The variations for anchor Y are (JJ JJ 
NN, JJ, etc.). The reason is quite straightforward: 
we need to determine the boundary of the anchors 
X and Y and a reasonable way to delimit them 
would be to use POS information.  All the patterns 
produced by the multi-level pattern learning algo-
rithm were generated from positive examples. 
From amongst these patterns, we need to find the 
most important ones. This is a critical step because 
frequently occurring patterns have low precision 
whereas rarely occurring patterns have high preci-
sion. From the Information Extraction point of 
view neither of these patterns is very useful. We 
need to find patterns with relatively high occur-
rence and high precision. We apply the log likeli-
hood principle (Dunning 1993) to compute this 
score. The top 15 patterns according to this metric 
are listed in Table 3 (we omit the POS variations 
for visibility). Some of these patterns are similar to 
the ones discovered by Hearst (1992) while other 
patterns are similar to the ones used by Fleischman 
et al (2003). 
4.3 Time complexity 
To extract hyponym relations, we use a fixed 
number of patterns across a corpus. Since we treat 
each sentences independently from others, the 
algorithm runs in linear time O(n) over the corpus 
size, where n is number of sentences in the corpus. 
5 Experimental Results 
In this section, we empirically compare the pat-
tern-based and co-occurrence-based models pre-
sented in Section 3 and Section 4. The focus is on 
the precision and recall of the systems as a func-
tion of the corpus size. 
5.1 Experimental Setup 
We use a 15GB newspaper corpus consisting of 
TREC9, TREC 2002, Yahoo! News ~0.5GB, AP 
newswire ~2GB, New York Times ~2GB, Reuters 
~0.8GB, Wall Street Journal ~1.2GB, and various 
online news website ~1.5GB. For our experiments, 
we extract from this corpus six data sets of differ-
ent sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB 
and 15GB. 
For the co-occurrence model, we used Minipar 
(Lin 1994), a broad coverage parser, to parse each 
data set. We collected the frequency counts of the 
grammatical relationships (contexts) output by 
Minipar and used them to compute the pointwise 
mutual information vectors described in Section 
3.1. For the pattern-based approach, we use Brill?s 
POS tagger (1995) to tag each data set. 
5.2 Precision 
We performed a manual evaluation to estimate 
the precision of both systems on each dataset. For 
each dataset, both systems extracted a set of is-a 
Table 3. Top 15 lexico-syntactic patterns discovered 
by our system. 
X, or Y X, _DT Y _(WDT|IN) Y like X and 
X, (a|an) Y X, _RB known as Y _NN, X and other Y 
X, Y X ( Y ) Y, including X, 
Y, or X Y such as X Y, such as X 
X is a Y X, _RB called Y Y, especially X 
 
774
relationships. Six sets were extracted for the pat-
tern-based approach and five sets for the co-
occurrence approach (the 15GB corpus was too 
large to process using the co-occurrence model ? 
see dependency parsing time estimates in Table 2). 
From each resulting set, we then randomly se-
lected 50 words along with their top 3 highest 
ranking is-a relationships. For example, Table 4 
shows three randomly selected names for the pat-
tern-based system on the 15GB dataset. For each 
word, we added to the list of hypernyms a human 
generated hypernym (obtained from an annotator 
looking at the word without any system or Word-
Net hyponym). We also appended the WordNet 
hypernyms for each word (only for the top 3 
senses). Each of the 11 random samples contained 
a maximum of 350 is-a relationships to manually 
evaluate (50 random words with top 3 system, top 
3 WordNet, and human generated relationship). 
We presented each of the 11 random samples to 
two human judges. The 50 randomly selected 
words, together with the system, human, and 
WordNet generated is-a relationships, were ran-
domly ordered. That way, there was no way for a 
judge to know the source of a relationship nor each 
system?s ranking of the relationships. For each 
relationship, we asked the judges to assign a score 
of correct, partially correct, or incorrect. We then 
computed the average precision of the system, 
human, and WordNet on each dataset. We also 
computed the percentage of times a correct rela-
tionship was found in the top 3 is-a relationships of 
a word and the mean reciprocal rank (MRR). For 
each word, a system receives an MRR score of 1 / 
M, where M is the rank of the first name judged 
correct. Table 5 shows the results comparing the 
two automatic systems. Table 6 shows similar 
results for a more lenient evaluation where both 
correct and partially correct are judged correct. 
For small datasets (below 150MB), the pattern-
based method achieves higher precision since the 
co-occurrence method requires a certain critical 
mass of statistics before it can extract useful class 
signatures (see Section 3). On the other hand, the 
pattern-based approach has relatively constant 
precision since most of the is-a relationships se-
lected by it are fired by a single pattern. Once the 
co-occurrence system reaches its critical mass (at 
150MB), it generates much more precise hypo-
nyms. The Kappa statistics for our experiments 
were all in the range 0.78 ? 0.85. 
Table 7 and Table 8 compare the precision of the 
pattern-based and co-occurrence-based methods 
with the human and WordNet hyponyms. The 
variation between the human and WordNet scores 
across both systems is mostly due to the relative 
cleanliness of the tokens in the co-occurrence-
based system (due to the parser used in the ap-
proach). WordNet consistently generated higher 
precision relationships although both algorithms 
approach WordNet quality on 6GB (the pattern-
based algorithm even surpasses WordNet precision 
on 15GB). Furthermore, WordNet only generated a 
hyponym 40% of the time. This is mostly due to 
the lack of proper noun coverage in WordNet. 
On the 6 GB corpus, the co-occurrence approach 
took approximately 47 single Pentium-4 2.5 GHz 
processor days to complete, whereas it took the 
pattern-based approach only four days to complete 
on 6 GB and 10 days on 15 GB. 
5.3 Recall 
The co-occurrence model has higher precision 
than the pattern-based algorithm on most datasets. 
Table 4. Is-a relationships assigned to three randomly selected words (using pattern-based system on 15GB dataset). 
RANDOM WORD HUMAN WORDNET PATTERN-BASED SYSTEM (RANKED) 
Sanwa Bank bank none subsidiary / lender / bank 
MCI Worldcom Inc. telecommunications company none phone company / competitor / company 
cappuccino beverage none item / food / beverage 
 
Table 5. Average precision, top-3 precision, and MRR 
for both systems on each dataset. 
 PATTERN SYSTEM CO-OCCURRENCE SYSTEM 
 
Prec Top-3 MRR Prec Top-3 MRR 
1.5MB
 
38.7% 41.0% 41.0% 4.3% 8.0% 7.3% 
15MB 39.1% 43.0% 41.5% 14.6% 32.0% 24.3% 
150MB 40.6% 46.0% 45.5% 51.1% 73.0% 67.0% 
1.5GB 40.4% 39.0% 39.0% 56.7% 88.0% 77.7% 
6GB 46.3% 52.0% 49.7% 64.9% 90.0% 78.8% 
15GB 55.9% 54.0% 52.0% Too large to process 
 
Table 6. Lenient average precision, top-3 precision, 
and MRR for both systems on each dataset. 
 PATTERN SYSTEM CO-OCCURRENCE SYSTEM 
 
Prec Top-3 MRR Prec Top-3 MRR 
1.5MB
 
56.6% 60.0% 60.0% 12.4% 20.0% 15.2% 
15MB 57.3% 63.0% 61.0% 23.2% 50.0% 37.3% 
150MB 50.7% 56.0% 55.0% 60.6% 78.0% 73.2% 
1.5GB 52.6% 51.0% 51.0% 69.7% 93.0% 85.8% 
6GB 61.8% 69.0% 67.5% 78.7% 92.0% 86.2% 
15GB 67.8% 67.0% 65.0% Too large to process 
 
775
However, Figure 2 shows that the pattern-based 
approach extracts many more relationships. 
Semantic extraction tasks are notoriously diffi-
cult to evaluate for recall. To approximate recall, 
we defined a relative recall measure and conducted 
a question answering (QA) task of answering defi-
nition questions. 
5.3.1 Relative recall 
Although it is impossible to know the number of 
is-a relationships in any non-trivial corpus, it is 
possible to compute the recall of a system relative 
to another system?s recall. The recall of a system 
A, RA, is given by the following formula: 
 
C
C
R AA =  
where CA is the number of correct is-a relation-
ships extracted by A and C is the total number of 
correct is-a relationships in the corpus. We define 
relative recall of system A given system B, RA,B, as: 
 
B
A
B
A
BA C
C
R
R
R ==
,
 
Using the precision estimates, PA, from the pre-
vious section, we can estimate CA ? PA ? |A|, where 
A is the total number of is-a relationships discov-
ered by system A. Hence, 
 
BP
AP
R
B
A
BA
?
?
=
,
 
Figure 3 shows the relative recall of A = pattern-
based approach relative to B = co-occurrence 
model. Because of sparse data, the pattern-based 
approach has much higher precision and recall (six 
times) than the co-occurrence approach on the 
small 15MB dataset. In fact, only on the 150MB 
dataset did the co-occurrence system have higher 
recall. With datasets larger than 150MB, the co-
occurrence algorithm reduces its running time by 
filtering out grammatical relationships for words 
that occurred fewer than k = 40 times and hence 
recall is affected (in contrast, the pattern-based 
approach may generate a hyponym for a word that 
it only sees once). 
5.3.2 Definition questions 
Following Fleischman et al (2003), we select 
the 50 definition questions from the TREC2003 
(Voorhees 2003) question set. These questions are 
of the form ?Who is X?? and ?What is X?? For 
each question (e.g., ?Who is Niels Bohr??, ?What 
is feng shui??) we extract its respective instance 
(e.g., ?Neils Bohr? and ?feng shui?), look up their 
corresponding hyponyms from our is-a table, and 
present the corresponding hyponym as the answer. 
We compare the results of both our systems with 
WordNet. We extract at most the top 5 hyponyms 
provided by each system. We manually evaluate 
the three systems and assign 3 classes ?Correct 
(C)?, ?Partially Correct (P)? or ?Incorrect (I)? to 
each answer. 
This evaluation is different from the evaluation 
performed by the TREC organizers for definition 
questions. However, by being consistent across all 
Total Number of Is-A Relationships vs. Dataset
0
200000
400000
600000
800000
1000000
1200000
1400000
1.5MB 15MB 150MB 1.5GB 6GB 15GB
Datasets
To
ta
l I
s-
A 
Re
la
tio
n
s
hi
ps
s
Pattern-based System
Co-occurrence-based System
Figure 2. Number of is-a relationships extracted by 
the pattern-based and co-occurrence-based approaches. 
 
Table 7. Average precision of the pattern-based sys-
tem vs. WordNet and human hyponyms. 
 PRECISION MRR 
 
Pat. WNet Human Pat. WNet Human 
1.5MB
 
38.7% 45.8% 83.0% 41.0% 84.4% 83.0% 
15MB 39.1% 52.4% 81.0% 41.5% 95.0% 91.0% 
150MB 40.6% 49.4% 84.0% 45.5% 88.9% 94.0% 
1.5GB 40.4% 43.4% 79.0% 39.0% 93.3% 89.0% 
6GB 46.3% 46.5% 76.0% 49.7% 75.0% 76.0% 
15GB 55.9% 45.6% 79.0% 52.0% 78.0% 79.0% 
 
Table 8. Average precision of the co-occurrence-
based system vs. WordNet and human hyponyms. 
 PRECISION MRR 
 
Co-occ WNet Human Co-occ WNet Human 
1.5MB
 
4.3% 42.7% 52.7% 7.3% 87.7% 95.0% 
15MB 14.6% 38.1% 48.7% 24.3% 86.6% 95.0% 
150MB 51.1% 57.5% 65.8% 67.0% 85.1% 98.0% 
1.5GB 56.7% 62.8% 70.3% 77.7% 93.0% 98.0% 
6GB 64.9% 68.9% 75.2% 78.8% 94.3% 98.0% 
 
Relative Recall (Pattern-based vs. Co-occurrence-based)
0.00
1.00
2.00
3.00
4.00
5.00
6.00
7.00
1.5MB 15MB 150MB 1.5GB 6GB 15GB
(projected)
Datesets
Re
la
tiv
e 
Re
ca
ll
Figure 3. Relative recall of the pattern-based approach 
relative to the co-occurrence approach. 
 
 
776
systems during the process, these evaluations give 
an indication of the recall of the knowledge base. 
We measure the performance on the top 1 and the 
top 5 answers returned by each system. Table 9 
and Table 10 show the results. 
The corresponding scores for WordNet are 38% 
accuracy in both the top-1 and top-5 categories (for 
both strict and lenient). As seen in this experiment, 
the results for both the pattern-based and co-
occurrence-based systems report very poor per-
formance for data sets up to 150 MB. However, 
there is an increase in performance for both sys-
tems on the 1.5 GB and larger datasets. The per-
formance of the system in the top 5 category is 
much better than that of WordNet (38%). There is 
promise for increasing our system accuracy by re-
ranking the outputs of the top-5 hypernyms. 
6 Conclusions 
There is a long standing need for higher quality 
performance in NLP systems. It is possible that 
semantic resources richer than WordNet will en-
able them to break the current quality ceilings. 
Both statistical and symbolic NLP systems can 
make use of such semantic knowledge. With the 
increased size of the Web, more and more training 
data is becoming available, and as Banko and Brill 
(2001) showed, even rather simple learning algo-
rithms can perform well when given enough data. 
In this light, we see an interesting need to de-
velop fast, robust, and scalable methods to mine 
semantic information from the Web. This paper 
compares and contrasts two methods for extracting 
is-a relations from corpora. We presented a novel 
pattern-based algorithm, scalable to the terascale, 
which outperforms its more informed syntactical 
co-occurrence counterpart on very small and very 
large data. 
Albeit possible to successfully apply linguisti-
cally-light but data-rich approaches to some NLP 
applications, merely reporting these results often 
fails to yield insights into the underlying theories 
of language at play. Our biggest challenge as we 
venture to the terascale is to use our new found 
wealth not only to build better systems, but to im-
prove our understanding of language. 
References  
Banko, M. and Brill, E. 2001. Mitigating the paucity of data problem.  
In Proceedings of HLT-2001. San Diego, CA. 
Berland, M. and E. Charniak, 1999. Finding parts in very large 
corpora. In ACL-1999. pp. 57?64. College Park, MD. 
Brill, E., 1995. Transformation-based error-driven learning and 
natural language processing: A case study in part of speech 
tagging. Computational Linguistics, 21(4):543?566. 
Brill, E.; Lin, J.; Banko, M.; Dumais, S.; and Ng, A. 2001. Data-
intensive question answering. In Proceedings of the TREC-10 
Conference, pp 183?189. Gaithersburg, MD. 
Caraballo, S. 1999. Automatic acquisition of a hypernym-labeled 
noun hierarchy from text. In Proceedings of ACL-99. pp 120?126, 
Baltimore, MD. 
Curran, J. and Moens, M. 2002. Scaling context space. In Proceedings 
of ACL-02. pp 231?238, Philadelphia, PA. 
Dunning, T. 1993. Accurate methods for the statistics of surprise and 
coincidence. Computational Linguistics 191 (1993), 61?74. 
Etzioni, O.; Cafarella, M.; Downey, D.; Kok, S.; Popescu, A.M.; 
Shaked, T.; Soderland, S.; Weld, D. S.; and Yates, A. 2004. Web-
scale information extraction in Know-It All (Preliminary Results). 
To appear in the Conference on WWW. 
Fleischman, M.; Hovy, E.; and Echihabi, A. 2003. Offline strategies 
for online question answering: Answering questions before they are 
asked. In Proceedings of ACL-03. pp. 1?7. Sapporo, Japan. 
Girju, R.; Badulescu, A.; and Moldovan, D. 2003. Learning semantic 
constraints for the automatic discovery of part-whole relations. In 
Proceedings of HLT/NAACL-03. pp. 80?87. Edmonton, Canada. 
Harris, Z. 1985. Distributional structure. In: Katz, J. J. (ed.) The 
Philosophy of Linguistics. New York: Oxford University Press. pp. 
26?47. 
Hearst, M. 1992. Automatic acquisition of hyponyms from large text 
corpora. In COLING-92. pp. 539?545. Nantes, France. 
Hindle, D. 1990. Noun classification from predicate-argument 
structures. In Proceedings of ACL-90. pp. 268?275. Pittsburgh, PA. 
Lin, D. 1994. Principar - an efficient, broad-coverage, principle-based 
parser. Proceedings of COLING-94. pp. 42?48. Kyoto, Japan. 
Lin, D. 1998. Automatic retrieval and  clustering of similar words. In 
Proceedings of COLING/ACL-98. pp. 768?774. Montreal, Canada. 
Mann, G. S. 2002. Fine-Grained Proper Noun Ontologies for Question 
Answering. SemaNet? 02: Building and Using Semantic Networks, 
Taipei, Taiwan. 
Miller, G. 1990. WordNet: An online lexical database. International 
Journal of Lexicography, 3(4). 
Och, F.J. and Ney, H. 2002. Discriminative training and maximum 
entropy models for statistical machine translation. In Proceedings 
of ACL. pp. 295?302. Philadelphia, PA. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of SIGKDD-02. pp. 613?619. Edmonton, Canada. 
Pantel, P. and Ravichandran, D. 2004. Automatically labeling seman-
tic classes. In Proceedings of HLT/NAACL-04. pp. 321?328. Bos-
ton, MA. 
Riloff, E. and Shepherd, J. 1997. A corpus-based approach for 
building semantic lexicons. In Proceedings of EMNLP-1997. 
Voorhees, E. 2003. Overview of the question answering track. In 
Proceedings of TREC-12 Conference. NIST, Gaithersburg, MD. 
Table 9. QA definitional evaluations for pattern-based 
system. 
 TOP-1 TOP5 
 
Strict Lenient Strict Lenient 
1.5MB
 
0% 0% 0% 0% 
15MB 0% 0% 0% 0% 
150MB 2.0% 2.0% 2.0% 2.0% 
1.5GB 16.0% 22.0% 20.0% 22.0% 
6GB 38.0% 52.0% 56.0% 62.0% 
15GB 38.0% 52.0% 70.0% 74.0% 
 
Table 10. QA definitional evaluations for co-
occurrence-based system. 
 TOP-1 TOP5 
 
Strict Lenient Strict Lenient 
1.5MB
 
0% 0% 0% 0% 
15MB 0% 0% 0% 0% 
150MB 0% 0% 0% 0% 
1.5GB 6.0% 8.0% 6.0% 8.0% 
6GB 36.0% 44.0% 60.0% 62.0% 
 
777
FrameNet-based Semantic Parsing using Maximum Entropy Models 
Namhee Kwon 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
nkwon@isi.edu 
Michael Fleischman 
Messachusetts Institute of 
Technology,  
77 Massachusetts Ave 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
Information Sciences Institute 
University of Southern California
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
As part of its description of lexico-semantic 
predicate frames or conceptual structures, the 
FrameNet project defines a set of semantic 
roles specific to the core predicate of a 
sentence.  Recently, researchers have tried to 
automatically produce semantic interpretations 
of sentences using this information.  Building 
on prior work, we describe a new method to 
perform such interpretations.  We define 
sentence segmentation first and show how 
Maximum Entropy re-ranking helps achieve a 
level of 76.2% F-score (answer among top-
five candidates) or 61.5% (correct answer). 
1 Introduction 
To produce a semantic analysis has long been a 
goal of Computational Linguistics.  To do so, 
however, requires a representation of the semantics 
of each predicate.  Since each predicate may have a 
particular collection of semantic roles (agent, 
theme, etc.) the first priority is to build a collection 
of predicate senses with their associated role 
frames.  This task is being performed in the 
FrameNet project based on frame semantics 
(Fillmore, 1976). 
Each frame contains a principal lexical item as 
the target predicate and associated frame-specific 
roles, such as offender and buyer, called frame 
elements.  FrameNet I contains 1,462 distinct 
predicates (927 verbs, 339 nouns, 175 adjectives) 
in 49,000 annotated sentences with 99,000 
annotated frame elements.  Given these, it would 
be interesting to attempt an automatic sentence 
interpretation. 
We build semantic parsing based on FrameNet, 
treating it as a classification problem.  We split the 
problem into three parts: sentence segmentation, 
frame element identification for each segment, and 
semantic role tagging for each frame element.  In 
this paper, we provide a pipeline framework of 
these three phases, followed by a step of re-ranking 
from n-best lists of every phase for the final output.  
All classification and re-ranking are performed by 
Maximum Entropy. 
The top-five final outputs provide an F-score of 
76.2% for the correct frame element identification 
and semantic role tagging.  The performance of the 
single best output is 61.5% F-score. 
The rest of the paper is organized as follows: we 
review related work in Section 2, explain 
Maximum Entropy in Section 3, describe the 
detailed method in Section 4, show the re-ranking 
process in Section 5, and conclude in Section 6.   
2 Related Work 
The first work using FrameNet for semantic 
parsing was done by Gildea and Jurafsky (G & J, 
2002) using conditional probabilistic models.  
They divide the problem into two sub-tasks: frame 
element identification and frame element 
classification.  Frame element identification 
identifies the frame element boundaries in a 
sentence, and frame element classification 
classifies each frame element into its appropriate 
semantic role.  The basic assumption is that the 
frame element (FE) boundaries match the parse 
constituents, and both identification and 
classification are then done for each constituent1. 
In addition to the separate two phase model of 
frame element identification and role classification, 
they provide an integrated model that exhibits 
improved performance.  They define a frame 
element group (FEG) as a set of frame element 
roles present in a particular sentence.  By 
integrating FE identification with role labeling, 
allowing FEG priors and role labeling decision to 
affect the determination of next FE identification, 
they accomplish F-score of 71.9% for FE 
identification and 62.8% for both of FE 
identification and role labeling.  However, since 
this integrated approach has an exponential 
complexity in the number of constituents, they 
apply a pruning scheme of using only the top m 
                                                     
1 The final output performance measurement is limited 
to the number of parse constituents matching the 
frame element boundaries. 
hypotheses on the role for each constituent (m = 
10). 
Fleischman et al(FKH, 2003) extend G & J?s 
work and achieve better performance in role 
classification for correct frame element boundaries.  
Their work improves accuracy from 78.5% to 
84.7%.  The main reasons for improvement are 
first the use of Maximum Entropy and second the 
use of sentence-wide features such as Syntactic 
patterns and previously identified frame element 
roles.  It is not surprising that there is a 
dependency between each constituent?s role in a 
sentence and sentence level features reflecting this 
dependency improve the performance. 
In this paper, we extend our previous work 
(KFH) by adopting sentence level features even for 
frame element identification. 
3 Maximum Entropy 
ME models implement the intuition that the best 
model is the one that is consistent with the set of 
constraints imposed by the evidence, but otherwise 
is as uniform as possible (Berger et al 1996).  We 
model the probability of a class c given a vector of 
features x according to the ME formulation below: 
        )],(exp[1)|(
0
xcfZxcp ii
n
ix
?=?=  
Here xZ  is normalization constant, ),( xcfi  is a 
feature function which maps each class and vector 
element to a binary value, n is the total number of 
feature functions, and i?  is a weight for the 
feature function.  The final classification is just the 
class with the highest probability given its feature 
vector and the model.   
It is important to note that the feature functions 
described here are not equivalent to the subset 
conditional distributions that are used in G & J?s 
model.  ME models are log-linear models in which 
feature functions map specific instances of features 
and classes to binary values.  Thus, ME is not here 
being used as another way to find weights for an 
interpolated model.  Rather, the ME approach 
provides an overarching framework in which the 
full distribution of classes (semantic roles) given 
features can be modeled. 
4 Model 
We define the problem into three subsequent 
processes (see Figure 1): 1) sentence segmentation 
2) frame element identification, and 3) semantic 
role tagging for the identified frame elements.  In 
order to use sentence-wide features for the FE 
identification, a sentence should have a single non-
overlapping constituent sequence instead of all the 
independent constituents.  Sentence segmentation 
is applied before FE identification for this purpose.  
For each segment the classification into FE or not 
is performed in the FE identification phase, and 
from the FE-tagged constituents the semantic role 
classification is applied in the role tagging phase. 
He got up, bent briefly over her hand.
(He) (got up) (bent) (briefly) (over her hand)
FE NO T FE FE
(He) (briefly) (over her hand)
Agent Manner Path
Input sentence
1) Sentence Segmentation: 
choose the highest constituents 
while separating target word 
2) Frame Element Boundary Identification:
apply ME classification to classify each segment 
into classes of FE (frame element), T (target), NO (none)
Extract the identified FEs:
choose segments that are identified as FEs
3) Semantic Role Tagging:
apply ME classification to classify each FE
Into classes of 120 semantic roles
Output role: Agent (He), Manner (briefly), Path (over her hand) 
for the target ?bent?
Fig. 1. The sequence of steps on a sample sentence. 
4.1 Sentence Segmentation 
The advantages of applying sentence 
segmentation before FE identification are 
considered in two ways.  First we can utilize 
sentence-wide features, and second the number of 
constituents as FE candidates is reduced, which 
reduces the convergence time in training. 
We segment a sentence with parse constituents2.  
During training, we split a sentence into true frame 
elements and the remainder.  After choosing frame 
elements as segments, we choose the highest level 
constituents in parse tree for other parts, and then 
make a complete sentence composed of a sequence 
of constituent segments.  During testing, we need 
to consider all combinations of various level 
constituents.  We know the given target word 
should be a separate segment because a target word 
is not a part of other FEs.  Since most frame 
elements tend to be among the higher levels of a 
parse tree, we decide to use the highest 
constituents while separating the target word.  
Figure 2 shows an example of the segmentation for 
                                                     
2 We use Michael Collins?s parser :  
http://www.cis.upenn.edu/~mcollins/ 
an actual sentence in FrameNet with the target 
word ?bent?.  
He got up bent briefly over her hand
PRP VBD RP
PRT
VBD RP IN PRP$ NN
VP
VP
VP
ADVP
NP
PP
NP
S
Fig. 2. A sample sentence segmentation: ?bent? is 
a target predicate in a sentence and the shaded 
constituent represents each segment. 
However, this segmentation for testing reduces 
the FE coverage of constituents, which means our 
FE classification performance is limited.  Table 1 
shows the FE coverage and the number of 
constituents for our development set.  The FE 
coverage of individual constituents (86.36%) 
means the accuracy of the parser.  This limitation 
and will be discussed in detail in Section 4.4. 
Method Number of constituents 
FE coverage 
(%) 
Individual 
constituents  115,380 86.36 
Sentence 
segmentation 29,688 77.25 
Table 1.  The number of constituents and FE 
coverage for development set. 
4.2 Frame Element Identification 
Frame element identification is executed for the 
sequence of segments. For the example sentence in 
Figure 2, ?(He) (got up) (bent) (briefly) (over her 
hand)?, there are five segments and each segment 
has its own feature vector.  Maximum Entropy 
classification into the classes of FE, Target, or 
None is conducted for each.  Since the target 
predicate is given we don?t need to classify a target 
word into a class, but we do not exclude it from the 
segments because we want to get benefit of using 
previous segment?s features. 
The initial features are adopted from G & J and 
FKH, and most features are common to both of 
frame element identification and semantic role 
classification.  The features are: 
? Target predicate (target): The target 
predicate, the principal word in a sentence, is 
the feature that is provided by the user.  
Although there can be many predicates in a 
sentence, only one predicate is defined at a 
time. 
? Target identification (tar): The target 
identification is a binary value, indicating 
whether the given constituent is a target or not.  
Because we have a target word in a sequence 
of segments, we provide this information 
explicitly. 
? Constituent path (path): From the syntactic 
parse tree of a sentence, we extract the path 
from each constituent to the target predicate.   
The path is represented by the nodes through 
which one passes while traveling up the tree 
from the constituent and then down through 
the governing category to the target word.  For 
example, ?over her hand? in a sentence of 
Figure 2 has a path PP?VP?VBD. 
? Phrase Type (pt): The syntactic phrase type 
(e.g., NP, PP) of each constituent is also 
extracted from the parse tree. 
? Syntactic Head (head): The syntactic head of 
each constituent is obtained based on Michael 
Collins?s heuristic method3.  When the head is 
a proper noun, ?proper-noun? substitutes for 
the real head.  The decision if the head is 
proper noun is done by the part of speech tag 
in a parse tree.  
? Logical Function (lf): The logical functions of 
constituents in a sentence are simplified into 
three values: external argument, object 
argument, other.  We follow the links in the 
parse tree from the constituent to the ancestors 
until we meet either S or VP.  If the S is found 
first, we assign external argument to the 
constituent, and if the VP is found, we assign 
object argument. Otherwise, other is assigned.  
Generally, a grammatical function of external 
argument is a subject, and that of object 
argument is an object.  This feature is applied 
only to constituents whose phrase type is NP.  
? Position (pos): The position indicates whether 
a constituent appears before or after the target 
predicate and whether the constituent has the 
same parent as the target predicate or not. 
? Voice (voice): The voice of a sentence (active, 
passive) is determined by a simple regular 
expression over the surface form of the 
sentence. 
? Previous class (c_n): The class information of 
the nth-previous constituent (target, frame 
element, or none) is used to exploit the 
dependency between constituents.  During 
training, this information is provided by simply 
                                                     
3 http://www.ai.mit.edu/people/mcollins/papers/heads 
looking at the true classes of the frame element 
occurring n-positions before the current 
element.  During testing, hypothesized classes 
of the n elements are used and Viterbi search is 
performed to find the most probable tag 
sequence for a sentence. 
The combination of these features is used in ME 
classification as feature sets.  The feature sets are 
optimized by previous work and trial and error 
experiments.  Table 2 shows the lists of feature sets 
for ?briefly? in a sentence of ?He got up, bent 
briefly over her hand?.  These feature sets contain 
the previous or next constituent?s features, for 
example, pt_-1 represents the previous 
constituent?s phrase type and lf_1 represents the 
next constituent?s logical function. 
Feature Set Example Functions 
f(c, target) f(c, ?bent?) = 1 
f(c, target, pt) f(c, ?bent?,ADVP) = 1 
f(c, target, pt, lf) f(c, ?bent?,ADVP,other) = 1 
f(c, pt, pos, voice) f(c, ADVP,after_yes,active) = 1 
f(c, pt, lf) f(c, ADVP,other) = 1 
f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1 
f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1 
f(c, pt_-1, pos_-1,voice) f(c, VBD_-1,t_-1,active) = 1 
f(c, pt_1, pos_1, voice) f(c, PP_1,after_yes_1, active) = 1 
f(c, head) f(c, ?briefly?) = 1 
f(c, head, target) f(c, ?briefly?, ?bent?) = 1 
f(c, path) f(c, ADVP?VP?VBD) = 1 
f(c, path_-1) f(c, VBD_-1) = 1 
f(c, path_1) f(c, PP?VP?VBD_1) = 1 
f(c, tar) f(c, 0) = 1 
f(c, c_-1) f(c, ?target?_-1) = 1 
f(c, c_-1,c_-2) f(c, ?target?_-1,?NO FE?_-2) = 1 
Table 2. Feature sets used in ME frame element 
identification.  Example functions of ?briefly? 
from the sample sentence in Fig.2 are shown. 
4.3 Semantic Role Classification 
The semantic role classification is executed only 
for the constituents that are classified into FEs in 
the previous FE identification phase.  Maximum 
Entropy classification is performed to classify each 
FE into classes of semantic roles. 
Most features from the frame element 
identification in Section 4.2 are still used, and two 
additional features are applied.  The feature sets 
are in Table 3. 
? Order (order): The relative position of a 
frame element in a sentence is given.  For 
example, in the sentence from Figure 2, there 
are three frame elements, and the element 
?He? has order 0, while ?over her hand? has 
order 2. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern is generated from the parse 
tree by looking at the phrase type and logical 
functions of each frame element in a sentence.  
For example, in the sentence from Figure 2, 
?He? is an external argument Noun Phrase, 
?bent? is a target predicate, and ?over her 
hand? is an external argument Prepositional 
Phrase.  Thus, the syntactic pattern associated 
with the sentence is [NP-ext, target, PP-ext]. 
Feature Sets 
f(c, target) f(r, head) 
f(r, target, pt) f(r, head, target) 
f(r, target, pt, lf) f(r, head, target, pt) 
f(r, pt, pos, voice) f(r, order, syn) 
f(r, pt, pos, voice, target) f(r,target, order, syn) 
f(r, r_-1) f(r,r_-1,r_-2) 
Table 3. Feature sets used in ME semantic role 
classification. 
4.4 Experiments and Results 
Since FrameNet II was published during our 
research, we continued using FrameNet I (120 
semantic role categories).  We can, therefore, 
compare our results with previous research by 
matching exactly the same data as used in G & J 
and FKH.  We thank Dan Gildea for providing the 
following data set: training (36,993 sentences / 
75,548 frame elements), development (4,000 
sentences / 8,167 frame elements), and held our 
test sets (3,865 sentences / 7,899 frame elements). 
We train the ME models using the GIS 
algorithm (Darroch and Ratcliff, 1972) as 
implemented in the YASMET ME package (Och, 
2002).  For testing, we use the YASMET 
MEtagger (Bender et al 2003) to perform the 
Viterbi search for choosing the most probable tag 
sequence for a sentence using the probabilities 
from training.  Feature weights are smoothed using 
Gaussian priors with mean 0 (Chen and Rosenfeld, 
1999).  The standard deviation of this distribution 
and the number of GIS iterations for training are 
optimized on development set for each experiment.  
Table 4 shows the performance for test set. The 
evaluation is done for individual frame elements. 
To segment a sentence before FE identification 
or role tagging improves the overall performance 
(from 57.6% to 60.0% in Table 4).  Since the 
segmentation reduces the FE coverage of segments, 
we conduct the experiment with the manually 
chosen segmentation to see how much the 
segmentation helps the performance.  Here, we 
extract segments from the parse tree constituents, 
so the FE coverage is 86% for test set, which 
maches the parsing accuracy.  Table 5 shows the 
performance of the frame element identification for 
test set:  F-score is 77.2% that is much better than 
71.7% of our automatic segmentation. 
FE identification FE identification & Role tagging Method 
Prec Rec F Prec Rec F 
G & J 
separated 
model 
73.6 63.1 67.5 67.0 46.8 55.1 
FKH 
ME model 73.6 67.9 70.6 60.0 55.4 57.6 
Our model 
(segmentation 
+ ME 
classification) 
75.5 68.2 71.7 62.9 56.8 60.0 
Table 4. Performance comparison for test set. 
Precision Recall F-score 
82.1 72.9 77.2 
Table 5.  Result of frame element identification on 
manual segmentation of test set 
5 n-best Lists and Re-ranking 
As stated, the sentence segmentation improves 
the performance by using sentence-wide features, 
but it drops the FE coverage of constituents.  In 
order to determine a good segmentation for a 
sentence that does not reduce the FE coverage, we 
perform another experiment by using re-ranking.  
We obtain all possible segmentations for a given 
sentence, and conduct frame element identification 
and semantic role classification for all 
segmentations.  During both phases, we get n-best 
lists with Viterbi search, and finally choose the 
best output with re-ranking method.  Figure 3 
shows the overall framework of this task.  
5.1 Maximum Entropy Re-ranking 
We model the probability of output r given 
candidates? feature sets {x1 .. xt} where t is the total 
number of candidates and xj is a feature set of the 
jth candidate according to the following ME 
formulation: 
]})..{,(exp[1})..{|(
0
1 1?
=
=
n
i
tiixt
xxrfZxxrp ?
 
where Zx is a normalization factor, fi(r,{x1..xt}) is a 
feature function which maps each output and all 
candidates? feature sets to a binary value, n is the 
total number of feature functions, and ?i is the 
weight for a given feature function.  The weight ?i 
is associated with only each feature function while 
the weight in the ME classifier is associated with 
all possible classes as well as feature functions.  
The final decision is r having the highest 
probability of p(r|{x1..xt}) from t number of 
candidates. 
As a feature set for each candidate, we use the 
ME classification probability that is calculated 
during Viterbi search.  These probabilities are 
conditional probabilities given feature sets and 
these feature sets depend on the previous output, 
for example, semantic role tagging is done for the 
identified FEs in the previous phase.  For this 
reason, the product of these conditional 
probabilities is used as a feature set. 
    )|(*)|(*)|()|( ferpsegfepssegpsrp =  
where s is a given sentence, seg is a segmentation, 
fe is a frame element identification, and r is the 
final semantic role tagging.  p(fe|seg) and p(r|fe) 
are produced from the ME classification but 
p(seg|s) is computed by a heuristic method and a 
development set optimization experiment.  The 
adopted p(seg|s) is composed of p(each segment?s 
part of speech tag | target?s part of speech tag), 
p(the number of total segments in a sentence | total 
number of words in a sentence), and the average of 
each segment?s p(head word of FE | target). 
Two additional feature sets other than p(r|s) are 
applied to get slight improvement for re-ranking 
performance, which are average of p(parse tree 
depth of FE | target) and average of p(head word 
of FE | target). 
5.2 Experiments and Results 
We apply ME re-ranking in YASMET-ME 
package.  We train re-ranking model with 
development set after obtaining candidate lists for 
the set.  For a simple cross validation, the 
development set is divided into a sub-training set 
(3,200 sentences) and a sub-development set (800 
sentences) by selecting every fifth sentence.  
Training for re-ranking is executed with the sub- 
training set and optimization is done with the sub-
development set.  The final test is applied to test 
set. 
The possible number of segmentations is different 
depending on sentences, but the average number of 
segmentation lists is 15.24 for the development  set.  
For these segmentations, we compute 10-best5 lists 
for the FE identification and 10-best lists for the 
semantic role classification. 
                                                     
4  To reduce the number of different segmenations 
while not dropping the FE coverage, the segmentations 
having too many segments for a long sentence are 
excluded. 
5 The experiment showed 10-best lists outperformed 
other n-best lists where n is less than 10.  The bigger 
number was not tested because of  huge number of lists. 
 He craned over the balcony again but finally he seemed to sigh.
1. (He) (craned) (over) (the) (balcony) (again) (but) (finally) (he) (seemed) (to) (sigh).
?
6. (He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed) (to sigh).
7. (He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed to sigh).
?
11. (He) (craned) (over the balcony) (again) (but) (finally) (he) (seemed to sigh).
12. (He) (craned) (over the balcony) (again) (but) (finally he seemed to sigh).
Input sentence
Sentence Segmentation: segment a sentence into all possible combinations of constituents of a                                        
parse tree while separating target word (In this example, target word is ?craned?.)
Frame Element Identification:  apply ME classification to all segmentations and get n-best output 
classifying each segment into FE (frame element), T (target), or NO (none), then extract segments that are 
identified as frame elements
(1)
(2)
(4)
1.1 (He)
?
6.1 (He) (the balcony)
?
11.1 (He) (over the balcony)
?
12.1 (He) (over the balcony)
12.2 (He) 
12.3 (He) (over the balcony) (again)
..
(3)
Semantic Role Classification: apply ME classification into 120 semantic roles and get n-best output for each
1.1.1 Agent (He)
?.
6.1.1 Agent (He), BodyPart (the balcony)
?.
12.1.1 Agent (He), Goal (over the balcony)
12.1.2 Agent (He), Path (over the balcony)
12.1.3 Self-mover (He), Goal (over the balcony)
?.
Re-ranking : apply ME re-ranking and choose the best one from long lists
Final Output Agent (He), Path (over the balcony)
 
Fig. 3.  The framework of the re-ranking method with an actual system output.  (1) contains different number 
of segmentations depending on each sentence, (2) has mn number of lists when we obtain m possible 
segmentations in (1) and we get n-best FE identifications, (3) has mnn number of lists when we get n-best 
role classifications given mn lists (4) shows finally chosen output. 
Table 6 shows the performance of re-ranking.  
To evaluate the performance of top-n, the best 
tagging output for a sentence is chosen among n-
lists and the performance is computed for that list.  
The top-5 lists show two interesting points: one is 
that precision is very high, and the other is that F-
score including role tagging is not much different 
from F-score of only FE identification.  In other 
words, there are a few (not 120) confusing roles for 
a given frame element, and we have many frame 
elements that are not identified even in n-best lists. 
FE identification FE identification  & Role tagging Re-rank 
Prec Rec F Prec Rec F 
Top-1 77.4 66.0 71.2 66.7 57.0 61.5 
Top-2 81.8 69.2 75.0 75.6 64.0 69.4 
Top-5 86.8 72.4 78.0 83.7 69.9 76.2 
Table 6. Re-ranking performance for test set 
To improve our re-ranker, more features 
regarding these problems should be added, and a 
more principled method to obtain the probability of 
segmenations, p(seg) in Sectioin 5.1, needs to be 
investigated. 
Table 7 compares the final output with G & J?s 
best result.  Our model is slightly worse than their 
integrated model, but it supports much further 
experimentation in segmentation and re-ranking. 
FE identification FE indetification & Role tagging Method 
Prec Rec F Prec Rec F 
G & J 
integrated 
model 
74.0 70.1 72.0 64.6 61.2 62.9 
Our 
model w/ 
re-ranking 
77.4 66.0 71.2 66.7 57.0 61.5 
Table 7. The final output for test set.  
6 Conclusion 
We describe a pipeline framework to analyze 
sentences into frame elements and semantic roles 
based on the FrameNet corpus.  The process 
includes four steps: sentence segmentation, FE 
identification, role classification, and final re-
ranking of the n-best outputs. 
In future work, we will investigate ways to 
reduce the gap between the five-best output 
performance and the single best output.  More 
features should be extracted to improve re-ranking 
accuracy.  Although the segmentation improves the 
performance, since the final output is dominated by 
the initial segmentation, we will explore a smart 
segmentation method, possibly one not even 
limited to constituents. 
In addition to the provided syntactic features, we 
will apply semantic features using ontology.  
Finally, the challenge is to apply this type of work 
to new predicates, ones not yet treated in 
FrameNet.  We are searching for methods to 
achieve this. 
References  
O. Bender, K. Macherey, F.J. Och, and H. Ney. 
2003. Comparison of Alignment Templates and 
Maximum Entropy Models for Natural Language 
Processing. Proc. of EACL-2003. Budapest, 
Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natural 
Language Proc. of Computational Linguistics, 
vol. 22, no. 1. 
S.F. Chen and R. Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
M. Collins. 1997. Three Generative, Lexicalized 
Models for Statistical Parsing. Proc. of the 35th 
Annual Meeting of the ACL. pages 16-23, 
Madrid, Spain. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models.  Annals 
of Mathematical Statistics, 43:1470-1480. 
C.Fillmore 1976. Frame Semantics and the Nature 
of Language.  Annals of the New York Academy 
of Science Conference on the Origin and 
Development of Language and Speech, Volume 
280 (pp. 20-32). 
M. Fleischman, N. Kwon, and E. Hovy. 2003. 
Maximum Entropy Models for FrameNet 
Classification. Proc. of Empirical Methods in 
Natural Language Processing conference 
(EMNLP) 2003. Sapporo, Japan.  
D. Gildea and D. Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3) 245-288 14. 
K. Hacioglu, W. Ward. 2003. Target word 
detection and semantic role chunking using 
support vector machines. Proc. of HLT-NAACL 
2003, Edmonton, Canada. 
F.J. Och. 2002. Yet Another Maxent Toolkit: 
YASMET www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/. 
S. Pradhan, K. Haciolgu, W. Ward, J. Martin, D. 
Jurafsky. 2003. Semantic Role Parsing: Adding 
Semantic Structure to Unstructured Text. Proc of 
of the International Conference on Data Mining 
(ICDM-2003), Melbourne, FL  
C. Thompson, R. Levy, and C. Manning. 2003. A 
Generative Model for FrameNet Semantic Role 
Labeling. Proc. of the Fourteenth European 
Conference on Machine Learning, Croatia 
Determining the Sentiment of Opinions 
Soo-Min Kim 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
skim@isi.edu 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
Abstract 
Identifying sentiments (the affective parts 
of opinions) is a challenging problem.  We 
present a system that, given a topic, 
automatically finds the people who hold 
opinions about that topic and the sentiment 
of each opinion.  The system contains a 
module for determining word sentiment 
and another for combining sentiments 
within a sentence.  We experiment with 
various models of classifying and 
combining sentiment at word and sentence 
levels, with promising results.   
1 Introduction 
What is an opinion?   
The many opinions on opinions are reflected 
in a considerable literature (Aristotle 1954; 
Perelman 1970; Toulmin et al 1979; Wallace 
1975; Toulmin 2003).  Recent computational 
work either focuses on sentence ?subjectivity? 
(Wiebe et al 2002; Riloff et al 2003), 
concentrates just on explicit statements of 
evaluation, such as of films (Turney 2002; Pang 
et al 2002),  or focuses on just one aspect of 
opinion, e.g., (Hatzivassiloglou and McKeown 
1997) on adjectives.  We wish to study opinion 
in general; our work most closely resembles 
that of (Yu and Hatzivassiloglou 2003).   
Since an analytic definition of opinion is 
probably impossible anyway, we will not 
summarize past discussion or try to define 
formally what is and what is not an opinion.  
For our purposes, we describe an opinion as a 
quadruple [Topic, Holder, Claim, Sentiment] in 
which the Holder believes a Claim about the 
Topic, and in many cases associates a 
Sentiment, such as good or bad, with the belief.  
For example, the following opinions contain 
Claims but no Sentiments:  
?I believe the world is flat?  
?The Gap is likely to go bankrupt? 
?Bin Laden is hiding in Pakistan?  
?Water always flushes anti-clockwise in 
the southern hemisphere?  
Like Yu and Hatzivassiloglou (2003), we 
want to automatically identify Sentiments, 
which in this work we define as an explicit or 
implicit expression in text of the Holder?s 
positive, negative, or neutral regard toward the 
Claim about the Topic.  (Other sentiments we 
plan to study later.)  Sentiments always involve 
the Holder?s emotions or desires, and may be 
present explicitly or only implicitly:  
 ?I think that attacking Iraq would put the 
US in a difficult position? (implicit)  
?The US attack on Iraq is wrong? 
(explicit)  
?I like Ike? (explicit) 
?We should decrease our dependence on 
oil? (implicit)  
 ?Reps. Tom Petri and William F. 
Goodling asserted that counting illegal aliens 
violates citizens? basic right to equal 
representation?  (implicit)  
In this paper we address the following 
challenge problem.  Given a Topic (e.g., 
?Should abortion be banned??) and a set of 
texts about the topic, find the Sentiments 
expressed about (claims about) the Topic (but 
not its supporting subtopics) in each text, and 
identify the people who hold each sentiment.  
To avoid the problem of differentiating 
between shades of sentiments, we simplify the 
problem to: identify just expressions of 
positive, negative, or neutral sentiments, 
together with their holders.  In addition, for 
sentences that do not express a sentiment but 
simply state that some sentiment(s) exist(s), 
return these sentences in a separate set.  For 
example, given the topic ?What should be done 
with Medicare?? the sentence ?After years of 
empty promises, Congress has rolled out two 
Medicare prescription plans, one from House 
Republicans and the other from the Democratic  
Sentence
POS Tagger
verbs nounsAdjectives
Adjective Senti ment
classifier 
sentiment sentiment
Sentence sentiment classifier
Opinion region + polarity + holder
Holder finder
Named Entity 
Tagger
Sentence
Sentence
texts + topic
sentiment sentiment sentiment
V rbs
Verb Senti ment
classifier 
Nouns
Noun Senti ment
classifier 
WordNet
Sentence :
Figure 1: System architecture.  
Sens. Bob Graham of Florida and Zell Miller of 
Georgia? should be returned in the separate set.   
We approach the problem in stages, starting 
with words and moving on to sentences.  We 
take as unit sentiment carrier a single word, and 
first classify each adjective, verb, and noun by 
its sentiment.  We experimented with several 
classifier models.  But combining sentiments 
requires additional care, as Table 1 shows.   
California Supreme Court agreed that the state?s 
new term-limit law was constitutional. 
California Supreme Court disagreed that the 
state?s new term-limit law was constitutional. 
California Supreme Court agreed that the state?s 
new term-limit law was unconstitutional. 
California Supreme Court disagreed that the 
state?s new term-limit law was unconstitutional. 
Table 1: Combining sentiments.  
A sentence might even express opinions of 
different people.  When combining word-level 
sentiments, we therefore first determine for 
each Holder a relevant region within the 
sentence and then experiment with various 
models for combining word sentiments.     
We describe our models and algorithm in 
Section 2, system experiments and discussion 
in Section 3, and conclude in Section 4.   
2 Algorithm  
Given a topic and a set of texts, the system 
operates in four steps.  First it selects sentences 
that contain both the topic phrase and holder 
candidates.  Next, the holder-based regions of 
opinion are delimited.  Then the sentence 
sentiment classifier calculates the polarity of all 
sentiment-bearing words individually. Finally, 
the system combines them to produce the 
holder?s sentiment for the whole sentence.  
Figure 1 shows the overall system architecture.  
Section 2.1 describes the word sentiment 
classifier and Section 2.2 describes the sentence 
sentiment classifier.   
2.1 Word Sentiment Classifier 
2.1.1 Word Classification Models 
For word sentiment classification we 
developed two models.  The basic approach is 
to assemble a small amount of seed words by 
hand, sorted by polarity into two lists?positive 
and negative?and then to grow this by adding 
words obtained from WordNet (Miller et al 
1993; Fellbaum et al 1993).  We assume 
synonyms of positive words are mostly positive 
and antonyms mostly negative, e.g., the 
positive word ?good? has synonyms ?virtuous, 
honorable, righteous? and antonyms ?evil, 
disreputable, unrighteous?.  Antonyms of 
negative words are added to the positive list, 
and synonyms to the negative one.   
To start the seed lists we selected verbs (23 
positive and 21 negative) and adjectives (15 
positive and 19 negative), adding nouns later.   
Since adjectives and verbs are structured 
differently in WordNet, we obtained from it 
synonyms and antonyms for adjectives but only 
synonyms for verbs.  For each seed word, we 
extracted from WordNet its expansions and 
added them back into the appropriate seed lists.  
Using these expanded lists, we extracted an 
additional cycle of words from WordNet, to 
obtain finally 5880 positive adjectives, 6233 
negative adjectives, 2840 positive verbs, and 
3239 negative verbs.   
However, not all synonyms and antonyms 
could be used: some had opposite sentiment or 
were neutral.  In addition, some common words 
such as ?great?, ?strong?, ?take?, and ?get? 
occurred many times in both positive and 
negative categories.  This indicated the need to 
develop a measure of strength of sentiment 
polarity (the alternative was simply to discard 
such ambiguous words)?to determine how 
strongly a word is positive and also how 
strongly it is negative.  This would enable us to 
discard sentiment-ambiguous words but retain 
those with strengths over some threshold.   
Armed with such a measure, we can also 
assign strength of sentiment polarity to as yet 
unseen words.  Given a new word, we use 
WordNet again to obtain a synonym set of the 
unseen word to determine how it interacts with 
our sentiment seed lists.  That is, we compute  
(1)                 ).....,|(maxarg
)|(maxarg
21 n
c
c
synsynsyncP
wcP
?
where c is a sentiment category (positive or 
negative), w is the unseen word, and synn are the 
WordNet synonyms of w.  To compute 
Equation (1), we tried two different models:  
(2)   )|()(maxarg
)|()(maxarg
)|()(maxarg)|(maxarg
1
))(,(
 ...3 2 1
?
=
=
=
=
m
k
wsynsetfcount
k
c
n
c
cc
kcfPcP
csynsynsynsynPcP
cwPcPwcP
where fk is the kth feature (list word) of 
sentiment class c which is also a member of the 
synonym set of w, and count(fk,synset(w)) is the 
total number of occurrences of fk in the 
synonym set of w.  P(c) is the number of words 
in class c divided by the total number of words 
considered.  This model derives from document 
classification.  We used the synonym and 
antonym lists obtained from Wordnet instead of 
learning word sets from a corpus, since the 
former is simpler and does not require 
manually annotated data for training.   
Equation (3) shows the second model for a 
word sentiment classifier.   
(3)        
)(
),(
)(maxarg
)|()(maxarg)|(maxarg
1
ccount
csyncount
cP
cwPcPwcP
n
i
i
c
cc
?
==
=
 
To compute the probability P(w|c) of word w 
given a sentiment class c, we count the 
occurrence of w?s synonyms in the list of c.  
The intuition is that the more synonyms 
occuring in c, the more likely the word belongs.   
We computed both positive and negative 
sentiment strengths for each word and 
compared their relative magnitudes.  Table 2 
shows several examples of the system output, 
computed with Equation (2), in which ?+? 
represents positive category strength and ?-? 
negative.  The word ?amusing?, for example, 
was classified as carrying primarily positive 
sentiment, and ?blame? as primarily negative.  
The absolute value of each category represents 
the strength of its sentiment polarity.  For 
instance, ?afraid? with strength -0.99 represents 
strong negavitity while ?abysmal? with strength 
-0.61 represents weaker negativity.   
abysmal : NEGATIVE   
[+ : 0.3811][- : 0.6188] 
adequate : POSITIVE    
[+ : 0.9999][- : 0.0484e-11] 
afraid : NEGATIVE     
[+ : 0.0212e-04][- : 0.9999] 
ailing : NEGATIVE     
[+ : 0.0467e-8][- : 0.9999] 
amusing : POSITIVE    
[+ : 0.9999][- : 0.0593e-07] 
answerable : POSITIVE   
[+ : 0.8655][- : 0.1344] 
apprehensible: POSITIVE   
[+ : 0.9999][- : 0.0227e-07] 
averse : NEGATIVE      
[+ : 0.0454e-05][- : 0.9999] 
blame : NEGATIVE      
[+ : 0.2530][- : 0.7469] 
Table 2: Sample output of word sentiment 
classifier.  
2.2 Sentence Sentiment Classifier 
As shows in Table 1, combining sentiments 
in a sentence can be tricky.  We are interested 
in the sentiments of the Holder about the 
Claim.  Manual analysis showed that such 
sentiments can be found most reliably close to 
the Holder; without either Holder or 
Topic/Claim nearby as anchor points, even 
humans sometimes have trouble reliably 
determining the source of a sentiment.  We 
therefore included in the algorithm steps to 
identify the Topic (through direct matching, 
since we took it as given) and any likely 
opinion Holders (see Section 2.2.1).  Near each 
Holder we then identified a region in which 
sentiments would be considered; any 
sentiments outside such a region we take to be 
of undetermined origin and ignore (Section 
2.2.2).  We then defined several models for 
combining the sentiments expressed within a 
region (Section 2.2.3).   
2.2.1 Holder Identification 
We used BBN?s named entity tagger 
IdentiFinder to identify potential holders of an 
opinion.  We considered PERSON and 
ORGANIZATION as the only possible opinion 
holders.  For sentences with more than one 
Holder, we chose the one closest to the Topic 
phrase, for simplicity.  This is a very crude step.  
A more sophisticated approach would employ a 
parser to identify syntactic relationships 
between each Holder and all dependent 
expressions of sentiment.   
2.2.2 Sentiment Region 
Lacking a parse of the sentence, we were 
faced with a dilemma: How large should a 
region be?  We therefore defined the sentiment 
region in various ways (see Table 3) and 
experimented with their effectiveness, as 
reported in Section 3.   
Window1: full sentence 
Window2: words between Holder and Topic 
Window3: window2 ? 2 words 
Window4: window2 to the end of sentence 
Table 3: Four variations of region size. 
2.2.3 Classification Models 
We built three models to assign a sentiment 
category to a given sentence, each combining 
the individual sentiments of sentiment-bearing 
words, as described above, in a different way.   
Model 0 simply considers the polarities of 
the sentiments, not the strengths:  
Model 0: ? (signs in region) 
The intuition here is something like 
?negatives cancel one another out?.  Here the 
system assigns the same sentiment to both ?the 
California Supreme Court agreed that the 
state?s new term-limit law was constitutional? 
and ?the California Supreme Court disagreed 
that the state?s new term-limit law was 
unconstitutional?.  For this model, we also 
included negation words such as not and never 
to reverse the sentiment polarity.   
Model 1 is the harmonic mean (average) of 
the sentiment strengths in the region:  
Model 1: 
cwcp
wcp
cn
scP
ij
n
i
i
=
= ?
=
)|(argmax if
 ,)|(
)(
1)|(
j
1
 
Here n(c) is the number of words in the region 
whose sentiment category is c.  If a region 
contains more and stronger positive than 
negative words, the sentiment will be positive.   
Model 2 is the geometric mean:  
Model 2: 
cwcpif
wcpscP
ij
n
i
i
cn
=
?= ?
=
?
)|(argmax 
,)|(10)|(
j
1
1)(
 
2.2.4 Examples 
The following are two example outputs.   
 
Public officials throughout California have 
condemned a U.S. Senate vote Thursday to 
exclude illegal aliens from the 1990 census, 
saying the action will shortchange California in 
Congress and possibly deprive the state of 
millions of dollars of federal aid for medical 
emergency services and other programs for poor 
people. 
TOPIC : illegal alien 
HOLDER : U.S. Senate 
OPINION REGION: vote/NN Thursday/NNP     
to/TO exclude/VB illegal/JJ aliens/NNS from/IN 
the/DT 1990/CD census,/NN  
SENTIMENT_POLARITY: negative  
For that reason and others, the Constitutional 
Convention unanimously rejected term limits 
and the First Congress soundly defeated two 
subsequent term-limit proposals. 
TOPIC : term limit 
HOLDER : First Congress 
OPINION REGION: soundly/RB defeated/VBD 
two/CD subsequent/JJ term-limit/JJ 
proposals./NN 
SENTIMENT_POLARITY: negative 
3 Experiments 
The first experiment examines the two word 
sentiment classifier models and the second the 
three sentence sentiment classifier models.   
3.1 Word Sentiment Classifier 
For test material, we asked three humans to 
classify data.  We started with a basic English 
word list for foreign students preparing for the 
TOEFL test and intersected it with an adjective 
list containing 19748 English adjectives and a 
verb list of 8011 verbs to obtain common 
adjectives and verbs.  From this we randomly 
selected 462 adjectives and 502 verbs for 
human classification.  Human1 and human2 
each classified 462 adjectives, and human2 and 
human3 502 verbs.   
The classification task is defined as assigning 
each word to one of three categories: positive, 
negative, and neutral.  
3.1.1 Human?Human Agreement  
 Adjectives Verbs 
 Human1 : Human2 Human1 : Human3
Strict 76.19% 62.35% 
Lenient 88.96% 85.06% 
Table 4: Inter-human classification 
agreement. 
Table 4 shows inter-human agreement.  The 
strict measure is defined over all three 
categories, whereas the lenient measure is taken 
over only two categories, where positive and 
neutral have been merged, should we choose to 
focus only on differentiating words of negative 
sentiment.   
3.1.2 Human?Machine Agreement 
Table 5 shows results, using Equation (2) of 
Section 2.1.1, compared against a baseline that 
randomly assigns a sentiment category to each 
word (averaged over 10 iterations).  The system 
achieves lower agreement than humans but 
higher than the random process.   
Of the test data, the algorithm classified 
93.07% of adjectives and 83.27% of verbs as 
either positive and negative.  The remainder of 
adjectives and verbs failed to be classified, 
since they did not overlap with the synonym set 
of adjectives and verbs.   
In Table 5, the seed list included just a few 
manually selected seed words (23 positive and 
21 negative verbs and 15 and 19 adjectives, 
repectively).  We decided to investigate the 
effect of more seed words.   After collecting the 
annotated data, we added half of it (231 
adjectives and 251 verbs) to the training set, 
retaining the other half for the test.  As Table 6 
shows, agreement of both adjectives and verbs 
with humans improves.  Recall is also 
improved.  
Adjective 
(Train: 231  Test : 231) 
Verb 
(Train: 251  Test : 251) 
Lenient agreement Lenient agreement 
H1:M H2:M 
recall 
H1:M H3:M 
recall 
75.66% 77.88% 97.84% 81.20% 79.06% 93.23%
Table 6: Results including manual data.   
3.2 Sentence Sentiment Classifier 
3.2.1 Data 
100 sentences were selected from the DUC 
2001 corpus with the topics ?illegal alien?, 
?term limits?, ?gun control?, and ?NAFTA?.  
Two humans annotated the 100 sentences with 
three categories (positive, negative, and N/A).  
To measure the agreement between humans, we 
used the Kappa statistic (Siegel and Castellan 
Jr. 1988).  The Kappa value for the annotation 
task of 100 sentences was 0.91, which is 
considered to be reliable.   
3.2.2 Test on Human Annotated Data 
We experimented on Section 2.2.3?s 3 
models of sentiment classifiers, using the 4 
different window definitions and 4 variations of 
word-level classifiers (the two word sentiment 
equations introduced in Section 2.1.1, first with 
and then without normalization, to compare 
performance).   
Since Model 0 considers not probabilities of 
words but only their polarities, the two word- 
level classifier equations yield the same results. 
Consequently, Model 0 has 8 combinations and 
Models 1 and 2 have 16 each.    
To test the identification of opinion Holder, 
we first ran models with holders that were 
annotated by humans then ran the same models 
with the automatic holder finding strategies.  
The results appear in Figures 2 and 3. The 
models are numbered as follows: m0 through 
m4 represent 4 sentence classifier models,
Table 5. Agreement between humans and system.  
 Adjective  (test: 231 adjectives) Verb (test : 251 verbs) 
Lenient agreement Lenient agreement 
 
H1:M H2:M 
recall 
H1:M H3:M 
recall  
Random selection 
(average of 10 iterations) 59.35% 57.81% 100% 59.02% 56.59% 100% 
Basic method 68.37% 68.60% 93.07% 75.84% 72.72% 83.27% 
p1/p2 and p3/p4 represent the word classifier 
models in Equation (2) and Equation (3) with 
normalization and without normalization 
respectively. 
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
ra
cy
Window 1 Window 2 Window 3 Window 4
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
rac
y
Window 1 Window 2 Window 3 Window 4
Human 1 : Machine
Human 2 : Machine
Figure 2: Results with manually annotated 
Holder. 
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
rac
y
Window 1 Window 2 Window 3 Window 4
0.3
0.4
0.5
0.6
0.7
0.8
0.9
m0p1 m0p3 m1p1 m1p2 m1p3 m1p4 m2p1 m2p2 m2p3 m2p4
ac
cu
rac
y
Window 1 Window 2 Window 3 Window 4
Human 1 : Machine
Human 2 : Machine
Figure 3: Results with automatic Holder 
detection. 
Correctness of an opinion is determined 
when the system finds both a correct holder and 
the appropriate sentiment within the sentence.  
Since human1 classified 33 sentences positive 
and 33 negative, random classification gives 33 
out of 66 sentences.  Similarly, since human2 
classified 29 positive and 34 negative, random 
classification gives 34 out of 63 when the 
system blindly marks all sentences as negative 
and 29 out of 63 when it marks all as positive.  
The system?s best model performed at 81% 
accuracy with the manually provided holder 
and at 67% accuracy with automatic holder 
detection.   
3.3 Problems 
3.3.1 Word Sentiment Classification 
As mentioned, some words have both strong 
positive and negative sentiment.  For these 
words, it is difficult to pick one sentiment 
category without considering context.  Second, 
a unigram model is not sufficient: common 
words without much sentiment alone can 
combine to produce reliable sentiment.  For 
example, in ??Term limits really hit at 
democracy,? says Prof. Fenno?, the common 
and multi-meaning word ?hit? was used to 
express a negative point of view about term 
limits.  If such combinations occur adjacently, 
we can use bigrams or trigrams in the seed 
word list.  When they occur at a distance, 
however, it is more difficult to identify the 
sentiment correctly, especially if one of the 
words falls outside the sentiment region.   
3.3.2 Sentence Sentiment Classification 
Even in a single sentence, a holder might 
express two different opinions. Our system 
only detects the closest one.   
Another difficult problem is that the models 
cannot infer sentiments from facts in a 
sentence.  ?She thinks term limits will give 
women more opportunities in politics? 
expresses a positive opinion about term limits 
but the absence of adjective, verb, and noun 
sentiment-words prevents a classification.   
Although relatively easy task for people, 
detecting an opinion holder is not simple either.  
As a result, our system sometimes picks a 
wrong holder when there are multiple plausible 
opinion holder candidates present.   Employing 
a parser to delimit opinion regions and more 
accurately associate them with potential holders 
should help.   
3.4 Discussion 
Which combination of models is best? 
The best overall performance is provided by 
Model 0.  Apparently, the mere presence of 
negative words is more important than 
sentiment strength.  For manually tagged holder 
and topic, Model 0 has the highest single 
performance, though Model 1 averages best.   
Which is better, a sentence or a region?  
With manually identified topic and holder, 
the region window4 (from Holder to sentence 
end) performs better than other regions.   
How do scores differ from manual to 
automatic holder identification? 
Table 7 compares the average results with 
automatic holder identification to manually 
annotated holders in 40 different models.  
Around 7 more sentences (around 11%) were 
misclassified by the automatic detection 
method.    
 positive negative total 
Human1 5.394 1.667 7.060 
Human2 4.984 1.714 6.698 
Table 7: Average difference between 
manual and automatic holder detection. 
How does adding the neutral sentiment as a 
separate category affect the score? 
It is very confusing even for humans to 
distinguish between a neutral opinion and non-
opinion bearing sentences.  In previous 
research, we built a sentence subjectivity 
classifier.  Unfortunately, in most cases it 
classifies neutral and weak sentiment sentences 
as non-opinion bearing sentences.   
4 Conclusion 
Sentiment recognition is a challenging and 
difficult part of understanding opinions.  We 
plan to extend our work to more difficult cases 
such as sentences with weak-opinion-bearing 
words or sentences with multiple opinions 
about a topic.  To improve identification of the 
Holder, we plan to use a parser to associate 
regions more reliably with holders.  We plan to 
explore other learning techniques, such as 
decision lists or SVMs.   
Nonetheless, as the experiments show, 
encouraging results can be obtained even with 
relatively simple models and only a small 
amount of manual seeding effort.     
References  
Aristotle. The Rhetorics and Poetics (trans. W. 
Rhys Roberts), Modern Library, 1954. 
Fellbaum, C., D. Gross, and K. Miller. 1993. 
Adjectives in WordNet.  http://www.cosgi. 
princeton.edu/~wn. 
Hatzivassiloglou, V. and K. McKeown 1997. 
Predicting the Semantic Orientation of 
Adjectives. Proceedings of the 35th ACL 
conference, 174?181. 
Miller, G.A., R. Beckwith, C. Fellbaum, D. 
Gross, and K. Miller. 1993. Introduction to 
WordNet: An On-Line Lexical Database.  
http://www.cosgi.princeton.edu/~wn. 
Pang, B. L. Lee, and S. Vaithyanathan, 2002.  
Thumbs up? Sentiment classification using 
Machine Learning Techniques. Proceedings 
of the EMNLP conference.  
Perelman, C. 1970. The New Rhetoric: A 
Theory of Practical Reasoning. In The Great 
Ideas Today. Chicago: Encyclopedia 
Britannica.    
Riloff, E., J. Wiebe, and T. Wilson 2003. 
Learning Subjective Nouns Using Extraction 
Pattern Bootstrapping. Proceedings of the 
CoNLL-03 conference.   
Siegel, S. and N.J. Castellan Jr. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill.  
Toulmin, S.E., R. Rieke, and A. Janik. 1979. 
An Introduction to Reasoning. Macmillan, 
New York.   
Toulmin, S.E. 2003. The Uses of Argument.  
Cambridge University Press.  
Turney, P. 2002. Thumbs Up or Thumbs 
Down? Semantic Orientation Applied to 
Unsupervised Classification of Reviews. 
Proceedings of the 40th Annual Meeting of 
the ACL, Philadelphia, 417?424. 
Wallace, K. 1975. Topoi and the Problem of 
Invention. In W. Ross Winterowd (ed), 
Contemporary Rhetoric. Harcourt Brace 
Jovanovich.  
Wiebe, J. et al 2002. NRRC summer study Jan 
Wiebe and group (University of Pittsburgh) 
on ?subjective? statements.  
Yu, H. and V. Hatzivassiloglou. 2003. Towards 
Answering Opinion Questions: Separating 
Facts from Opinions and Identifying the 
Polarity of Opinion Sentences. Proceedings 
of the EMNLP conference. 
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1057?1064
Manchester, August 2008
OntoNotes: Corpus Cleanup of Mistaken Agreement Using  
Word Sense Disambiguation 
Liang-Chih Yu and Chung-Hsien Wu 
Dept. of Computer Science and Information Engineering
National Cheng Kung University 
Tainan, Taiwan, R.O.C. 
{lcyu,chwu}@csie.ncku.edu.tw 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 90292, USA 
hovy@isi.edu 
 
 
 
 
Abstract 
Annotated corpora are only useful if their 
annotations are consistent.  Most large-scale 
annotation efforts take special measures to 
reconcile inter-annotator disagreement. To 
date, however, no-one has investigated how 
to automatically determine exemplars in 
which the annotators agree but are wrong. In 
this paper, we use OntoNotes, a large-scale 
corpus of semantic annotations, including 
word senses, predicate-argument structure, 
ontology linking, and coreference. To de-
termine the mistaken agreements in word 
sense annotation, we employ word sense 
disambiguation (WSD) to select a set of 
suspicious candidates for human evaluation. 
Experiments are conducted from three as-
pects (precision, cost-effectiveness ratio, and 
entropy) to examine the performance of 
WSD. The experimental results show that 
WSD is most effective on identifying erro-
neous annotations for highly-ambiguous 
words, while a baseline is better for other 
cases. The two methods can be combined to 
improve the cleanup process. This procedure 
allows us to find approximately 2% remain-
ing erroneous agreements in the OntoNotes 
corpus. A similar procedure can be easily 
defined to check other annotated corpora. 
1 Introduction 
Word sense annotated corpora are useful re-
sources for many natural language applications. 
Various machine learning algorithms can then be 
trained on these corpora to improve the applica-
tions? effectiveness. Lately, many such corpora 
have been developed in different languages, in-
cluding SemCor (Miller et al, 1993), LDC-DSO 
(Ng and Lee, 1996), Hinoki (Kasahara et al, 
2004), and the sense annotated corpora with the 
help of Web users (Chklovski and Mihalcea, 
2002). The SENSEVAL1 (Kilgarriff and Palmer, 
2000; Kilgarriff, 2001; Mihalcea and Edmonds, 
2004) and SemEval-20072 evaluations have also 
created large amounts of sense tagged data for 
word sense disambiguation (WSD) competitions.  
The OntoNotes (Pradhan et al, 2007a; Hovy et 
al., 2006) project has created a multilingual cor-
pus of large-scale semantic annotations, includ-
ing word senses, predicate-argument structure, 
ontology linking, and coreference3. In word sense 
creation, sense creators generate sense definitions 
by grouping fine-grained sense distinctions ob-
tained from WordNet and dictionaries into more 
coarse-grained senses. There are two reasons for 
this grouping instead of using WordNet senses 
directly. First, people have trouble distinguishing 
many of the WordNet-level distinctions in real 
text, and make inconsistent choices; thus the use 
of coarse-grained senses can improve inter-
annotator agreement (ITA) (Palmer et al, 2004; 
2006). Second, improved ITA enables machines 
to more accurately learn to perform sense tagging 
automatically. Sense grouping in OntoNotes has 
been calibrated to ensure that ITA averages at 
least 90%. Table 1 shows the OntoNotes sense 
                                                          
1 http://www.senseval.org 
2 http://nlp.cs.swarthmore.edu/semeval 
3 Year 1 of the OntoNotes corpus has been re-
leased by Linguistic Data Consortium (LDC) 
(http://www.ldc.upenn.edu) in early 2007. The 
Year 2 corpus will be released in early 2008. 
 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Un-ported li-
cense (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
1057
tags and definitions for the word arm (noun 
sense). The OntoNotes sense tags have been used 
for many applications, including the SemEval-
2007 evaluation (Pradhan et al, 2007b), sense 
merging (Snow et al, 2007), sense pool verifica-
tion (Yu et al, 2007), and class imbalance prob-
lems (Zhu and Hovy, 2007). 
In creating OntoNotes, each word sense anno-
tation involves two annotators and an adjudicator. 
First, all sentences containing the target word 
along with its sense distinctions are presented 
independently to two annotators for sense annota-
tion. If the two annotators agree on the same 
sense for the target word in a given sentence, 
then their selection is stored in the corpus. Oth-
erwise, this sentence is double-checked by the 
adjudicator for the final decision. The major 
problem of the above annotation scheme is that 
only the instances where the two annotators dis-
agreed are double-checked, while those showing 
agreement are stored directly without any adjudi-
cation. Therefore, if the annotators happen to 
agree but are both wrong, then the corpus be-
comes polluted by the erroneous annotations. 
Table 2 shows an actual occurrence of an errone-
ous instance (sentence) for the target word man-
agement. In this example sentence, the actual 
sense of the target word is management.01, but 
both of our annotators made a decision of man-
agement.02. (Note that there is no difficulty in 
making this decision; the joint error might have 
occurred due to annotator fatigue, habituation 
after a long sequence of management.02 deci-
sions, etc.)   
Although most annotations in OntoNotes are 
correct, there is still a small (but unknown) frac-
tion of erroneous annotations in the corpus. 
Therefore, some cleanup procedure is necessary 
to produce a high-quality corpus. However, it is 
impractical for human experts to evaluate the 
whole corpus for cleanup. Given that we are fo-
cusing on word senses, this study proposes the 
use of WSD to facilitate the corpus cleanup proc-
ess. WSD has shown promising accuracy in re-
cent SENSEVAL and SemEval-2007 evaluations.  
The rest of this work is organized as follows. 
Section 2 describes the corpus cleanup procedure. 
Section 3 presents the features for WSD. Section 
4 summarizes the experimental results. Conclu-
sions are drawn in Section 5. 
2 Corpus Cleanup Procedure 
Figure 1 shows the cleanup procedure (dashed 
lines) for the OntoNotes corpus. As mentioned 
earlier, each word along with its sentence in-
stances is annotated by two annotators. The anno-
Sense Tag Sense Definition WordNet sense 
arm.01 The forelimb of an animal WN.1 
arm.02 A weapon WN.2 
arm.03 A subdivision or branch of an organization WN.3 
arm.04 A projection, a narrow extension of a structure WN.4 WN.5 
Table 1. OntoNotes sense tags and definitions. The WordNet version is 2.1. 
Example sentence: 
The 45-year-old Mr. Kuehn, who has a background in crisis management, succeeds Alan D. Rubendall, 45.
management.01: Overseeing or directing. Refers to the act of managing something. 
 
He was given overall management of the program. 
I'm a specialist in risk management. 
The economy crashed because of poor management. 
management.02: The people in charge. The ones actually doing the managing. 
 
Management wants to start downsizing. 
John was promoted to Management. 
I spoke to their management, and they're ready to make a deal. 
Table 2. Example sentence for the target word management along with its sense definitions. 
1058
tated corpus can thus be divided into two parts 
according to the annotation results. The first part 
includes the annotation with disagreement among 
the two annotators, which is then double-checked 
by the adjudicator. The final decisions made by 
the adjudicator are stored into the corpus. Since 
this part is double-checked by the adjudicator, it 
will not be evaluated by the cleanup procedure.  
The second part of the corpus is the focus of 
the cleanup procedure. The WSD system evalu-
ates each instance in the second part. If the output 
of the WSD system disagrees with the two anno-
tators, the instance is considered to be a suspi-
cious candidate, otherwise it is considered to be 
clean and stored into the corpus.  The set of sus-
picious candidates is collected and subsequently 
evaluated by the adjudicator to identify erroneous 
annotations. 
3 Word Sense Disambiguation 
This study takes a supervised learning approach 
to build a WSD system from the OntoNotes cor-
pus. The feature set used herein is similar to sev-
eral state-of-the-art WSD systems (Lee and Ng., 
2002; Ando, 2006; Tratz et al, 2007; Cai et al, 
2007; Agirre and Lopez de Lacalle, 2007; Specia 
et al, 2007), which is further integrated into a 
Na?ve Bayes classifier (Lee and Ng., 2002; Mi-
halcea, 2007). In addition, a new feature, predi-
cate-argument structure, provided by the 
OntoNotes corpus is also integrated. The feature 
set includes: 
Part-of-Speech (POS) tags: This feature in-
cludes the POS tags in the positions (P-3, P-2, P-1, 
P0, P1, P2, P3), relative to the POS tag of the tar-
get word.  
Local Collocations: This feature includes single 
words and multi-word n-grams. The single words 
include (W-3, W-2, W-1, W0, W1, W2, W3), relative 
to the target word W0. Similarly, the multi-word 
n-grams include (W-2,-1, W-1,1, W1,2, W-3,-2,-1, W-2,-1,1, 
W-1,1,2, W1,2,3). 
Bag-of-Words: This feature can be considered as 
a global feature, consisting of 5 words prior to 
and after the target word, without regard to posi-
tion. 
Predicate-Argument Structure: The predicate-
argument structure captures the semantic rela-
tions between the predicates and their arguments 
within a sentence, as shown in Figure 2. These 
relations can be either direct or indirect. A direct 
relation is used to model a verb-noun (VN) or 
noun-verb (NV) relation, whereas an indirect re-
lation is used to model a noun-noun (NN) rela-
tion. Additionally, an NN-relation can be built 
from the combination of an NV-relation and VN-
relation. For instance, in Figure 2, the NN-
relation (R3) can be built by combining the NV-
relation (R1) the VN-relation (R2). Therefore, the 
two features, R1 and R3, can be used to disam-
biguate the noun arm 4. 
4 Experimental Results 
4.1 Experiment setup  
The experiment data used herein was the 35 
nouns from the SemEval-2007 English Lexical 
Sample Task (Pradhan et al, 2007b). All sen-
tences containing the 35 nouns were selected 
from the OntoNotes corpus, resulting in a set of 
16,329 sentences. This data set was randomly 
split into training and test sets using different 
proportions (1:9 to 9:1, 10% increments). The 
WSD systems (described in Section 3) were then 
                                                          
4 Our WSD system does not include the sense 
identifier (except for the target word) for word-
level training and testing. 
The New York arm.03  ...  auctioned.01 off the estate.01
ARG0-INV ARG1
ARG0-INV-ARG1
NV-relation: (arm.03, ARG0-INV, auction.01)
VN-relation: (auction.01, ARG1, estate.01)
NN-relation: (arm.03, ARG0-INV-ARG1, estate.01)
Figure 2. Example of predicate-argument struc-
ture. The label ?-INV? denotes an inverse direc-
tion (i.e.,  from a noun to a verb). 
final decision
Annotation with 
agreement
Annotation with 
disagreement
AdjudicatorWSD
agree with 
annotators
disagree with annotators
Annotated
Corpus
 
Figure 1. Corpus cleanup procedure. 
1059
built from the different portions of the training 
set, called WSD_1 to WSD_9, respectively, and 
applied to their corresponding test sets. In each 
test set, the instances with disagreement among 
the annotators were excluded, since they have 
already been double-checked by the adjudicator. 
A baseline system was also implemented using 
the principle of most frequent sense (MFS), 
where each word sense distribution was retrieved 
from the OntoNotes corpus. Table 3 shows the 
accuracy of the baseline and WSD systems. 
The output of WSD may agree or disagree 
with the annotators. The instances with dis-
agreement were selected from each WSD system 
as suspicious candidates. This experiment ran-
domly selected at most 20 suspicious instances 
for each noun to form a suspicious set of 687 in-
stances. An adjudicator who is a linguistic expert 
then evaluated the suspicious set, and agreed in 
42 instances with the WSD systems, indicating 
about 6% (42/687) truly erroneous annotations. 
This corresponds to 2.6% (42/16329) erroneous 
annotations in the corpus as a whole, which we 
verified by an independent random spot check.  
In the following sections, we examine the per-
formance of WSD from three aspects: precision, 
cost-effectiveness ratio, and entropy, and finally 
summarize a general cleanup procedure for other 
sense annotated corpora. 
4.2 Cleanup precision analysis 
The cleanup precision for a single WSD system 
can be defined as the number of erroneous in-
stances identified by the WSD system, divided by 
the number of suspicious candidates selected by 
the WSD system. An erroneous instance refers to 
an instance where the annotators agree with each 
other but disagree with the adjudicator. Table 4 
lists the cleanup precision of the baseline and 
WSD systems. The experimental results show 
that WSD_7 (trained on 70% training data) iden-
tified 17 erroneous instances, out of 120 selected 
suspicious candidates, thus yielding the highest 
precision of 0.142. Another observation is that 
the upper bound of WSD_7 was 0.35 (42/120) 
under the assumption that it identified all errone-
ous instances. This low precision discourages the 
use of WSD to automatically correct erroneous 
annotations.  
4.3 Cleanup cost-effectiveness analysis 
The cleanup procedure used herein is a semi-
automatic process; that is, WSD is applied in the 
first stage to select suspicious candidates for hu-
man evaluation in the later stage. Obviously, we 
would like to minimize the number of candidates 
the adjudicator has to examine.  Thus we define a 
metric, the cost-effectiveness (CE) ratio, to 
measure the performance of WSD. The cost rate 
is defined as the number of suspicious instances 
selected by a single WSD system, divided by the 
total number of suspicious instances in the suspi-
cious set. The effectiveness rate is defined as the 
number of erroneous instances identified by a 
single WSD system, divided by the total number 
of erroneous instances in the suspicious set. In 
this experiment, the baseline value of the cost-
effectiveness ratio is 1, which means that human 
expert needs to evaluate all 687 instances in the 
suspicious set to identify 42 erroneous instances. 
Figure 3 illustrates the CE ratio of the WSD sys-
tems. The most cost-effective WSD system was 
WSD_7. The CE ratios of the baseline and 
WSD_7 are listed in Table 5. The experimental 
results indicate that 17.5% of suspicious in-
stances were required to be evaluated to identify 
about 40% erroneous annotations when using 
WSD_7. 
WSD  Baseline
(MFS) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
Accuracy 0.696 0.751 0.798 0.809 0.819 0.822 0.824 0.831 0.836 0.832
Table 3. Accuracy of the baseline and WSD systems with different training portions. 
WSD  Baseline (MFS) 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 
Prec 0.090 (17/188) 
0.113 
(20/177)
0.112 
(16/143) 
0.113 
(17/150)
0.124 
(16/129)
0.123 
(15/122)
0.127 
(16/126)
0.142 
(17/120) 
0.130 
(14/108) 
0.125 
(14/112)
Table 4. Cleanup precision of the baseline and WSD systems with different training portions. 
1060
4.4 Entropy analysis 
So far, the experimental results show that the best 
WSD system can help human experts identify 
about 40% erroneous annotations, but it still 
missed the other 60%. To improve performance, 
we conducted experiments to analyze the effect 
of word entropy with respect to WSD perform-
ance on identifying erroneous annotations. 
For the SemEval 35 nouns used in this experi-
ment, some words are very ambiguous and some 
words are not. This property of ambiguity may 
affect the performance of WSD systems on iden-
tifying erroneous annotation. To this end, this 
experiment used entropy to measure the ambigu-
ity of words (Melamed, 1997). The entropy of 
word can be computed by the word sense distri-
bution, defined as 
2( ) ( )log ( ),
i
i i
ws W
H W P ws P ws
?
= ? ?             (1) 
where ( )H W  denotes the entropy of a word W,  
and iws  denotes a word sense. A high entropy 
value indicates a high ambiguity level. For in-
stance, the noun defense has 7 senses (see Table 
7) in the OntoNotes corpus, occurring with the 
distribution {.14, .18, .19, .08, .04, .28, .09}, thus 
yielding a relative high entropy value (2.599). 
Conversely, the entropy of the noun rate is low 
(0.388), since it has only two senses with very 
skewed distribution {.92, .08}.   
Consider the two groups of the SemEval nouns: 
the nouns for which at least one (Group 1) or 
none (Group 2) of their erroneous instances can 
be identified by the machine. The average en-
tropy of these two groups of nouns was computed, 
as shown in Table 6. An independent t-test was 
then used to determine whether or not the differ-
ence of the average entropy among these two 
groups was statistically significant. The experi-
mental results show that WSD_7 was more effec-
tive on identifying erroneous annotations 
occurring in highly-ambiguous words (p<0.05), 
while the baseline system has no such tendency 
(p=0.368). 
Table 7 shows the detail analysis of WSD per-
formance on different words. As indicated, 
WSD_7 identified the erroneous instances (7/7) 
occurring in the two top-ranked highly-
ambiguous nouns, i.e., defense and position, but 
missed all those (0/12) occurring in the two most 
unambiguous words, i.e., move and rate. The ma-
jor reason is that the sense distribution of unam-
biguous words is often skew, thus WSD systems 
built from such imbalanced data tend to suffer 
from the over-fitting problem; that is, tend to 
over-fit the predominant sense class and ignore 
small sense classes (Zhu and Hovy, 2007). Fortu-
nately, the over-fitting problem can be greatly 
reduced when the entropy of words exceeds a 
certain threshold (e.g., the dashed line in Table 7), 
since the word sense has become more evenly 
distributed.  
4.5 Combination of WSD and MFS 
Another observation from Table 7 is that WSD_7 
identified more erroneous instances when the 
word entropy exceeded the cut-point, since the 
over-fitting problem was reduced. Conversely, 
MFS identified more ones when the word entropy 
is below the cut-point. This finding encourages 
the use of a combination of WSD_7 and MFS for 
corpus cleanup; that is, different strategies can be 
used with different entropy intervals. For this 
experiment data, MFS and WSD_7 can be ap-
plied below and above the cut-point, respectively, 
to select the suspicious instances for human 
evaluation. As illustrated in Figure 4, when the 
entropy of words increased, the accumulated ef-
fectiveness rates of both WSD_7 and MFS in-
creased accordingly, since more erroneous 
instances were identified. Additionally, the dif-
ference of the accumulated effect rate of MFS 
 Cost Effect CE Ratio
Baseline
(MFS) 
0.274 
(188/687)
0.405 
(17/42) 1.48 
WSD_7 0.175 (120/687)
0.405 
(17/42) 2.31 
Table 5. CE ratio of the baseline and WSD_7. 
 
Figure 3. CE ratio of WSD systems with differ-
ent training portions. 
1061
and WSD_7 increased gradually from the begin-
ning until the cut-point, since MFS identified 
more erroneous instances than WSD_7 did in this 
stage. When the entropy exceeded the cut-point, 
WSD_7 was more effective and thus its effec-
tiveness rate kept increasing, while that of MFS 
increased slowly, thus their difference was de-
creased with the rise of the entropy. For the com-
bination of MFS and WSD_7, its effectiveness 
rate before the cut-point was the same as that of 
MFS, since MFS was used in this stage to select 
the suspicious set. When WSD was used after the 
cut-point, the effectiveness rate of the combina-
tion system increased continuously, and finally 
reached 0.5 (21/42).  
Based on the above experimental results, the 
most cost-effective way for corpus cleanup is to 
use the combination method and begin with the 
most ambiguous words, since the WSD system in 
the combination method is more effective on 
identifying erroneous instances occurring in 
highly-ambiguous words and these words are 
also more important for many applications. Fig-
ure 5 shows the curve of the CE ratios of the 
combination method by starting with the most 
ambiguous word. The results indicate that the CE 
ratio of the combination method decreased 
gradually after more words with lower entropy 
were involved in the cleanup procedure. Addi-
tionally, the CE ratio of the combination method 
was improved by using MFS after the cut-point 
and finally reached 2.50, indicating that 50% 
(21/42) erroneous instances can be identified by 
double-checking 20% (137/687) of the suspicious 
set. This CE ratio was better than 2.31 and 1.48, 
reached by WSD_7 and MFS respectively. 
The proposed cleanup procedure can be ap-
plied to other sense annotated corpora by the fol-
lowing steps: 
Noun #sense Major Sense Entropy
#err. 
instances WSD_7 MFS 
WSD_7+ 
MFS 
defense 7 0.28 2.599 5 5 4 5 
position 7 0.30 2.264 2 2 2 2 
base 6 0.35 2.023 1 1 0 1 
system 6 0.54 1.525 2 1 0 1 
chance 4 0.49 1.361 1 1 1 1 
order 8 0.72 1.348 4 1 0 1 
part 5 0.70 1.288 1 1 1 1 
power 3 0.51 1.233 3 1 3 3 
area 3 0.72 1.008 2 1 2 2 
management 2 0.62 0.959 2 1 0 0 
condition 3 0.71 0.906 1 0 1 1 
job 3 0.78 0.888 1 0 0 0 
state 4 0.83 0.822 1 0 0 0 
hour 4 0.85 0.652 1 1 1 1 
value 3 0.90 0.571 2 1 1 1 
plant 3 0.88 0.556 1 0 0 0 
move 4 0.93 0.447 6 0 0 0 
rate 2 0.92 0.388 6 0 1 1 
Total ? ? ? 42 17 17 21 
Nouns without erroneous instances: authority, bill, capital, carrier, development, drug, 
effect, exchange, future, network, people, point, policy, president, share, source, space 
Table 7. Entropy of words versus WSD performance. The dashed line denotes a cut-point for the com-
bination of the baseline and WSD_7. 
 Group 1 Group 2 Difference p-value 
Baseline (MFS) 1.226 1.040 0.186 0.368 
WSD_7 1.401 0.932 0.469* 0.013 
  *p<0.05 
Table 6. Average entropy of two groups of nouns for the baseline and WSD_7. 
1062
z Build the baseline (MFS) and WSD systems 
from the corpus. 
z Create a suspicious set from the WSD systems. 
z Calculate the entropy for each word in terms 
of it sense distribution in the corpus. 
z Choose a cut-point value. Select a small por-
tion of words with entropy within a certain in-
terval (e.g., 1.0 ~ 1.5 in Table 7) for human 
evaluation to decide an appropriate cut-point 
value. The cut-point value should not be too 
low or too high, since WSD systems may suf-
fer from the over-fitting problem if it is too 
low, and the performance would be dominated 
by the baseline system if it is too high.  
z Combine the baseline and best single WSD 
system through the cut-point. 
z Start the cleanup procedure in the descending 
order of word entropy until the CE ratio is be-
low a predefined threshold. 
5 Conclusion 
This study has presented a cleanup procedure to 
identify incorrect sense annotation in a corpus. 
The cleanup procedure incorporates WSD sys-
tems to select a set of suspicious instances for 
human evaluation. The experiments are con-
ducted from three aspects: precision, cost-
effectiveness ratio, and entropy, to examine the 
performance of WSD. The experimental results 
show that the WSD systems are more effective 
on highly-ambiguous words. Additionally, the 
most cost-effective cleanup strategy is to use the 
combination method and begin with the most 
ambiguous words. The incorrect sense annota-
tions found in this study can be used for SemE-
val-2007 to improve the accuracy of WSD 
evaluation.  
The absence of related work on (semi-) auto-
matically determining cases of erroneous agree-
ment among annotators in a corpus is rather 
surprising. Variants of the method described here, 
replacing WSD for whatever procedure is appro-
priate for the phenomenon annotated in the cor-
pus (sentiment recognition for a sentiment corpus, 
etc.), are easy to implement and may produce 
useful results for corpora in current use. Future 
work will focus on devising an algorithm to per-
form the cleanup procedure iteratively on the 
whole corpus. 
References  
E. Agirre and O. Lopez de Lacalle. 2007. UBC-ALM: 
Combining k-NN with SVD for WSD. In Proc. of 
the 4th International Workshop on Semantic 
Evaluations (SemEval-2007) at ACL-07, pages 342-
345. 
R.K. Ando. 2006. Applying Alternating Structure Op-
timization to Word Sense Disambiguation. In Proc. 
of CoNLL, pages 77-84. 
J.F. Cai, W.S. Lee, and Y.W. Teh. 2007. Improving 
Word Sense Disambiguation Using Topic Features. 
In Proc. of EMNLP-CoNLL, pages 1015-1023. 
T. Chklovski and R. Mihalcea. 2002. Building a Sense 
Tagged Corpus with Open Mind Word Expert. In 
Proc. of the Workshop on Word Sense Disambigua-
tion: Recent Successes and Future Directions at 
ACL-02, pages 116-122. 
C. Fellbaum. 1998. WordNet: An Electronic Lexical 
Database. Cambridge, MA: MIT Press. 
E.H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and 
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proc. of HLT/NAACL-06, pages 57-60. 
K. Kasahara, H. Sato, F. Bond, T. Tanaka, S. Fujita, T. 
Kanasugi, and S. Amano. 2004. Construction of a 
Figure 4. Effectiveness rate against word entropy.
 
Figure 5. CE ratio against word entropy. 
1063
apanese Semantic Lexicon: Lexeed. In IPSG SIG: 
2004-NLC-159, Tokyo, pages 75-82. 
A. Kilgarriff. 2001. English Lexical Sample Task De-
scription. In Proc. of the SENSEVAL-2 Workshop, 
pages 17-20. 
A. Kilgarriff and M. Palmer, editors. 2000. 
SENSEVAL: Evaluating Word Sense Disambigua-
tion Programs, Computer and the Humanities, 
34(1-2):1-13. 
Y.K. Lee and H.T. Ng. 2002. An Empirical Evaluation 
of Knowledge Sources and Learning Algorithms 
for Word Sense Disambiguation. In Proc. of 
EMNLP, pages 41-48. 
I.D. Melamed. 1997. Measuring Semantic Entropy. In 
Proc. of ACL-SIGLEX Workshop, pages 41-46. 
R. Mihalcea. 2007. Using Wikipedia for Automatic-
Word Sense Disambiguation. In Proc. of 
NAACL/HLT-07, pages 196-203. 
R. Mihalcea and P. Edmonds, editors. 2004. In Proc. 
of SENSEVAL-3. 
G. Miller, C. Leacock, R. Tengi, and R. Bunker. 1993. 
A Semantic Concordance. In Proc. of the 3rd 
DARPA Workshop on Human Language Technol-
ogy, pages 303?308. 
H.T. Ng and H.B. Lee. 1996. Integrating Multiple 
Knowledge Sources to Disambiguate Word Sense: 
An Exemplar-based Approach. In Proc. of the 34th 
Meeting of the Association for Computational Lin-
guistics (ACL-96), pages 40-47. 
M. Palmer, O. Babko-Malaya, and H.T. Dang. 2004. 
Different Sense Granularities for Different Applica-
tions. In Proc. of the 2nd International Workshop 
on Scalable Natural Language Understanding at 
HLT/NAACL-04. 
M. Palmer, H.T. Dang, and C. Fellbaum. 2006. Mak-
ing Fine-grained and Coarse-grained Sense Distinc-
tions, Both Manually and Automatically. Journal of 
Natural Language Engineering, 13:137?163. 
S. Pradhan, E.H. Hovy, M. Marcus, M. Palmer, L. 
Ramshaw, and R. Weischedel. 2007a. OntoNotes: 
A Unified Relational Semantic Representation. In 
Proc. of the First IEEE International Conference 
on Semantic Computing (ICSC-07), pages 517-524.   
S. Pradhan, E. Loper, D. Dligach, and M. Palmer. 
2007b. SemEval-2007 Task 17: English Lexical 
Sample, SRL and All Words. In Proc. of the 4th In-
ternational Workshop on Semantic Evaluations 
(SemEval-2007) at ACL-07, pages 87-92. 
R. Snow, S. Prakash, D. Jurafsky, and A.Y. Ng. 2007. 
Learning to Merge Word Senses. In Proc. of 
EMNLP-CoNLL, pages 1005-1014. 
L. Specia, M. Stevenson, and M. das Gracas V. Nunes. 
2007. Learning Expressive Models for Word Sense 
Disambiguation. In Proc. of the 45th Annual Meet-
ing of the Association of Computational Linguistics 
(ACL-07), pages 41?48. 
S. Tratz, A. Sanfilippo, M. Gregory, A. Chappell, C. 
Posse, and P. Whitney. 2007. PNNL: A Supervised 
Maximum Entropy Approach to Word Sense Dis-
ambiguation. In Proc. of the 4th International 
Workshop on Semantic Evaluations (SemEval-2007) 
at ACL-07, pages 264-267. 
L.C. Yu, C.H. Wu, A. Philpot, and E.H. Hovy. 2007. 
OntoNotes: Sense Pool Verification Using Google 
N-gram and Statistical Tests. In Proc. of the On-
toLex Workshop at the 6th International Semantic 
Web Conference (ISWC 2007). 
J. Zhu and E.H. Hovy. 2007. Active Learning for 
Word Sense Disambiguation with Methods for Ad-
dressing the Class Imbalance Problem, In Proc. of 
EMNLP-CoNLL, pages 783-790. 
 
1064
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1129?1136
Manchester, August 2008
Multi-Criteria-based Strategy to Stop Active Learning for Data An-
notation 
Jingbo Zhu   Huizhen Wang 
Natural Language Processing Laboratory 
Northeastern University 
Shenyang, Liaoning, P.R.China 110004 
zhujingbo@mail.neu.edu.cn 
wanghuizhen@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
Abstract 
In this paper, we address the issue of de-
ciding when to stop active learning for 
building a labeled training corpus. Firstly, 
this paper presents a new stopping crite-
rion, classification-change, which con-
siders the potential ability of each unla-
beled example on changing decision 
boundaries. Secondly, a multi-criteria-
based combination strategy is proposed 
to solve the problem of predefining an 
appropriate threshold for each confi-
dence-based stopping criterion, such as 
max-confidence, min-error, and overall-
uncertainty. Finally, we examine the ef-
fectiveness of these stopping criteria on 
uncertainty sampling and heterogeneous 
uncertainty sampling for active learning. 
Experimental results show that these 
stopping criteria work well on evaluation 
data sets, and the combination strategies 
outperform individual criteria. 
1 Introduction 
Creating a large labeled training corpus is very 
expensive and time-consuming in some real-
world applications. For example, it is a crucial 
issue for automated word sense disambiguation 
task, because validations of sense definitions and 
sense-tagged data annotation have to be done by 
human experts, e.g. OntoNotes project (Hovy et 
al., 2006).  
Active learning aims to minimize the amount 
of human labeling effort by automatically select-
ing the most informative unlabeled example for 
human annotation. In recent years active learning 
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
has been widely studied in natural language 
processing (NLP) applications, such as word 
sense disambiguation (WSD) (Chen et al, 2006; 
Zhu and Hovy, 2007), text classification (TC) 
(Lewis and Gale, 1994; McCallum and Nigam, 
1998a), named entity recognition (Shen et al, 
2004), chunking (Ngai and Yarowsky, 2000), 
and statistical parsing (Tang et al, 2002). 
However, deciding when to stop active learn-
ing is still an unsolved problem and seldom men-
tioned issue in previous studies. Actually it is a 
very important practical issue in real-world ap-
plications, because it obviously makes no sense 
to continue the active learning procedure until 
the whole unlabeled corpus has been labeled. 
The active learning process can be ended when 
the current classifier reaches the maximum effec-
tiveness. In principle, how to learn a stopping 
criterion is a problem of estimation of classifier 
(i.e. learner) effectiveness during active learning 
(Lewis and Gale, 1994).  
In this paper, we address the issue of a stop-
ping criterion for pool-based active learning with 
uncertainty sampling (Lewis and Gale, 1994), 
and propose a multi-criteria-based approach to 
determining when to stop active learning process. 
Firstly, this paper makes a comprehensive analy-
sis on some confidence-based stopping criteria 
(Zhu and Hovy, 2007), including max-
confidence, min-error and overall-uncertainty, 
then proposes a new stopping criterion, classifi-
cation-change, which considers the potential 
ability of each unlabeled example on changing 
decision boundaries. Secondly, a combination 
strategy is proposed to solve the problem of pre-
defining an appropriate threshold for each confi-
dence-based stopping criterion in a specific task.  
In uncertainty sampling scheme, the most un-
certain unlabeled example is considered as the 
most informative case selected by active learner 
at each learning cycle. However, an uncertain 
example for one classifier may be not an uncer-
1129
tain example for other classifiers. When using 
active learning for real-world applications such 
as WSD, it is possible that a classifier of one type 
selects samples for training a classifier of another 
type, called the heterogeneous approach (Lewis 
and Catlett, 1994). For example, the final trained 
classifier for WSD is often different from the 
classifier used in active learning for constructing 
the training corpus.  
To date, no one has studied the stopping crite-
rion issue for the heterogeneous approach. In this 
paper, we examine the effectiveness of each 
stopping criterion on both traditional uncertainty 
sampling and heterogeneous uncertainty sam-
pling for active learning. Experimental results of 
active learning for WSD and TC tasks show that 
these proposed stopping criteria work well on 
evaluation data sets, and the combination strate-
gies outperform individual criteria. 
2 Active Learning Process 
In this paper, we are interested in uncertainty 
sampling for pool-based active learning (Lewis 
and Gale, 1994), in which an unlabeled example 
x with maximum uncertainty is selected to aug-
ment the training data at each learning cycle. The 
maximum uncertainty implies that the current 
classifier has the least confidence on its classifi-
cation of this unlabeled example.  
Actually active learning is a two-stage process 
in which a small number of labeled samples and 
a large number of unlabeled examples are first 
collected in the initialization stage, and a closed-
loop stage of query and retraining is adopted.  
Procedure: Active Learning Process 
Input: initial small training set L, and pool of unla-
beled data set U 
Use L to train the initial classifier C  
Repeat 
1. Use the current classifier C to label all unla-
beled examples in U 
2. Use uncertainty sampling technique to select m 
most informative unlabeled examples, and ask 
oracle H for labeling 
3. Augment L with these m new examples, and 
remove them from U 
4. Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning with uncertainty sam-
pling technique 
3 Stopping Criteria for Active Learning  
In this section, we mainly address the problem of 
general stopping criteria for active leanring, and 
study how to define a reasonable and appropriate 
stopping criterion SC shown in Fig. 1. 
3.1 Effectiveness Estimation and Confi-
dence Estimation 
To examine whether the classifier has reached 
the maximum effectiveness during active learn-
ing procedure, it seems an appealing solution 
when repeated learning cycles show no signifi-
cant performance improvement. However, this is 
often not feasible. To investigate the impact of 
performance change on defining a stopping crite-
rion for active learning, we first give an example 
of active learning for WSD shown in Fig. 2. 
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD task
interest
 
Figure 2. An example of active learning for WSD 
on word ?interest?. 
 
Fig. 2 shows that the accuracy performance 
generally increases, but apparently degrades at 
iterations 30, 90 and 190, and does not change 
anymore during iterations 220-260 in the active 
learning process. Actually the first time of the 
highest performance of 91.5% is achieved at 900 
which is not shown in Fig. 2. Although the accu-
racy performance curve shows an increasing 
trend, it is not monotonically increasing. It is not 
easy to automatically determine the point of no 
significant performance improvement on the 
validation set, because points such as 30 or 90 
would mislead a final judgment.  
Besides, there is a problem of performance es-
timation of the current classifier during active 
learning process, because a separate validation 
set should be prepared in advance, a procedure 
that causes additional (high) cost since it is often 
done manually. Besides, how many samples are 
required for the pregiven separate validation set 
is an open question. Too few samples may not be 
adequate for a reasonable estimate and may re-
sult in an incorrect result. Too many samples 
would increase the building cost.  
To define a stopping criterion for active learn-
ing, Zhu and hovy (2007) considered the estima-
tion of the classifier?s effectiveness as the second 
1130
task of confidence estimation of the classifier on 
its classification of all remaining unlabeled data. 
In the following section, we first introduce two 
confidence-based criteria, max-confidence and 
min-error, proposed by Zhu and Hovy (2007). 
3.2 Max-Confidence 
In uncertainty sampling scheme, if the uncer-
tainty value of the most informative unlabeled 
example is sufficiently small, we can assume that 
the current classifier has sufficient confidence on 
its classification of the remaining unlabeled data. 
So the active learning process can be ended. 
Based on such assumption, Zhu and Hovy (2007) 
proposed max-confidence criterion based on the 
uncertainty estimation of the most informative 
unlabeled example. Its strategy is to consider 
whether the uncertainty value of the most infor-
mative unlabeled example is less than a very 
small predefined threshold. 
3.3 Min-Error 
As shown in Fig. 1, in uncertainty sampling 
scheme, the current classifier has the least confi-
dence on its classification of these top-m selected 
unlabeled examples. If the current classifier can 
correctly classify these most informative exam-
ples, we can assume that the current classifier 
have sufficient confidence on its classification of 
the remaining unlabeled data. Based on such as-
sumption, Zhu and Hovy (2007) proposed min-
error criterion based on feedback from the oracle. 
Its strategy is to consider whether the current 
classifier can correctly predict the labels on these 
selected unlabeled examples, or the accuracy 
performance of the current classifier on these 
most informative examples is larger than a prede-
fined threshold.  
3.4 Overall-Uncertainty 
The motivation behind the overall-uncertainty 
method is similar to that of the max-confidence 
method. However, the max-confidence method 
only considers the most informative example at 
each learning cycle. The overall-uncertainty 
method considers the overall uncertainty on all 
unlabeled examples. If the overall uncertainty of 
all unlabeled examples becomes very small, we 
can assume that the current classifier has suffi-
cient confidence on its classification of the re-
maining unlabeled data. Based on such assump-
tion, we propose overall-uncertainty method 
which is to consider whether the average uncer-
tainty value of all remaining unlabeled examples 
is less than a very small predefined threshold. 
3.5 Classification-Change 
There is another problem of estimating classifier 
performance during active learning process. 
Cross-validation on the training set is almost im-
practical during the active learning procedure, 
because the alternative of requiring a held-out 
validation set for active learning is counterpro-
ductive. Hence we should look for a self-
contained method. 
Actually the motivation behind uncertainty 
sampling is to find some unlabeled examples 
near decision boundaries, and use them to clarify 
the position of decision boundaries. The current 
classifier considers such unlabeled examples near 
decision boundaries as the most informative ex-
amples in uncertainty sampling scheme for active 
learning. In other words, we assume that an 
unlabeled example with maximum uncertainty 
has the highest chance to change the decision 
boundaries. 
Based on the above analysis, we think the ac-
tive learning process can stop if there is no unla-
beled example that can potentially change the 
decision boundaries. However, in practice, it is 
almost impossible to exactly recognize which 
unlabeled example can truly change the decision 
boundaries in the next learning cycle, because 
the true label of each unlabeled example is un-
known. 
To solve this problem, we make an assump-
tion that labeling an unlabeled example may shift 
the decision boundaries if this example was pre-
viously ?outside? and is now ?inside?. In other 
words, if an unlabeled example is automatically 
assigned to two different labels during two recent 
learning cycles 2 , we think that the labeling of 
this unlabeled example has a good chance to 
change the decision boundaries.  
Based on such assumption, we propose a new 
approach based on classification change of each 
unlabeled example during two recent consecutive 
learning cycles (?previous? and ?current?), called 
the classification-change method. Its strategy is 
to stop the active learning process by considering 
whether no classification change happens to the 
remaining unlabeled examples during two recent 
consecutive learning cycles. If true, we assume 
that the current classifier has sufficient confi-
dence on its classification of the remaining unla-
                                                 
2 For example, an unlabeled example x was classified into 
class A at ith iteration, and class B at i+1th iteration. 
1131
beled data, because all unlabeled examples near 
decision boundaries have been exhausted, and no 
further labeling will affect active learner. 
4 Combination Strategy  
As for the above three confidence-based stopping 
criteria such as max-confidence, min-error and 
overall-uncertainty, how to automatically deter-
mine an appropriate threshold in a specific task is 
a crucial problem. We think that different appro-
priate thresholds are needed for various active 
learning applications.  
To solve this problem, in this section we pro-
pose a general combination strategy by consider-
ing the best of both classification-change and a 
confidence-based criterion, in which the prede-
fined threshold of the confidence-based stopping 
criterion can be automatically updated during 
active learning.  
The motivation behind the general combina-
tion strategy is to check whether the active learn-
ing becomes stable (i.e. check whether the classi-
fication-change method is met) when the current 
confidence-based stopping criterion is satisfied. 
If not, we think there are some remaining unla-
beled examples that can potentially shift the de-
cision boundaries, even if they are considered as 
certain cases from the current classifier?s view-
points. In this case, the threshold of the current 
confidence-based stopping criterion should be 
automatically revised to keep continuing the ac-
tive learning process. The general combination 
strategy can be summarized as follows. 
Procedure: General combination strategy 
Given: 
z stopping criterion 1: max-confidence or min-
error or overall-uncertainty 
z Stopping criterion 2: classification-change 
z The predefined threshold for stopping criterion 1 
is initially set to ? 
Steps(during active learning process): 
1. First check whether stopping criterion 1 is satis-
fied. If yes, go to 2; 
2. Then check whether stopping criterion 2 is satis-
fied. If yes, goto 4), otherwise goto 3; 
3. Automatically update the current threshold to be 
a new smaller value for max-confidence and 
overall-uncertainty, or to be a new larger value 
for min-error, and then goto 1. 
4. Stop active learning process.  
Figure 3. General combination strategy 
 
? Strategy 1: This strategy combines the max-
confidence and classification-change meth-
ods simultaneously.  
? Strategy 2: This strategy combines the min-
error and classification-change methods si-
multaneously. 
? Strategy 3: This strategy combines the over-
all-uncertainty and classification-change 
methods simultaneously. 
5 Evaluation 
5.1 Experimental Settings 
In the following sections, we evaluate the 
effectiveness of seven stopping criteria for active 
learning for WSD and TC tasks, including max-
confidence (MC), min-error (ME), overall-
uncertainty (OU), classification-change (CC), 
strategy 1 (CC-MC), strategy 2 (CC-ME), and 
strategy 3 (CC-OU). Following previous studies 
(Zhu and Hovy, 2007), the predefined thresh-
olds3 used for MC, ME and OU are set to 0.01, 
0.9 and 0.01, respectively. 
To evaluate the effectiveness of each stopping 
criterion, we first construct two types of baseline 
methods called ?All? and ?First? methods. ?All? 
method is defined as when all unlabeled exam-
ples in the pool are learned. ?First? method is 
defined as when the current classifier reaches the 
same performance of the ?All? method at the 
first time during the active learning process.  
A better stopping criterion can not only 
achieve almost the same performance given by 
the ?All? baseline method (i.e. accuracy 
performance), but also learn almost the same 
number of unlabeled examples by the ?First? 
baseline method (i.e. percentage performance).  
In uncertainty sampling scheme, the well-
known entropy-based uncertainty measurement 
(Chen et al, 2006; Schein and Ungar, 2007) is 
used in our active learning study as follows: 
( ) ( | ) log ( | )
y Y
UM x P y x P y x
?
= ??         (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. UM 
is the uncertainty measurement function based on 
the entropy estimation of the classifier?s 
posterior distribution. 
We utilize maximum entropy (MaxEnt) model 
(Berger et al, 1996) to design the basic classifier 
used in active learning for WSD and TC tasks. 
The advantage of the MaxEnt model is the ability 
to freely incorporate features from diverse 
sources into a single, well-grounded statistical 
                                                 
3 In the following experiments, these thresholds are also 
used as initial values of ? for individual criteria in the gen-
eral combination strategy shown in Fig. 3. 
1132
model. A publicly available MaxEnt toolkit4 was 
used in our experiments. To build the MaxEnt-
based classifier for WSD, three knowledge 
sources are used to capture contextual informa-
tion: unordered single words in topical context, 
POS of neighboring words with position infor-
mation, and local collocations, which are the 
same as the knowledge sources used in (Lee and 
Ng, 2002). In the design of text classifier, the 
maximum entropy model is also utilized, and no 
feature selection technique is used. 
In the following active learning comparison 
experiments, the algorithm starts with a 
randomly chosen initial training set of 10 labeled 
examples, and makes 10 queries after each 
learning iteration. A 10 by 10-fold cross-
validation was performed. All results reported 
are the average of 10 trials in each active 
learning process. In the following comparison 
experiments, the performance reported on 
Ontonotes data set is the macro-average on ten 
nouns, and the performance on TWA data set is 
the macro-average on six words. 
5.2 Data Sets 
Six publicly available natural data sets have been 
used in the following active learning comparison 
experiments. Three data sets are used for TC 
tasks: WebKB, Comp2a and Comp2b. The other 
three data sets are used for WSD tasks: 
OntoNotes, Interest and TWA.  
The WebKB dataset was widely used in TC 
research. Following previous studies (McCallum 
and Nigam, 1998b), we use the four most popu-
lous categories: student, faculty, course and pro-
ject, altogether containing 4199 web pages. In 
the preprocessing step, we only remove those 
words that occur merely once without using 
stemming. The resulting vocabulary has 23803 
words. 
The Comp2a data set consists of comp.os.ms-
windows.misc and comp.sys.ibm.pc.hardware 
subset of NewsGroups. The Comp2b data set 
consists of comp.graphics and comp.windows.x 
categories from NewsGroups. Both two data sets 
have been previously used in active learning for 
TC (Roy and McCallum, 2001; Schein and Un-
gar, 2007). 
The OntoNotes project (Hovy et al, 2006) 
uses the WSJ part of the Penn Treebank. The 
senses of noun words occurring in OntoNotes are 
linked to the Omega ontology. Ontonotes has 
                                                 
                                                4See  http://homepages.inf.ed.ac.uk/s0450736/maxent_ 
toolkit.html 
been used previously in active learning for WSD 
tasks (Zhu and Hovy, 2007). In the following 
comparison experiments, we focus on 10 most 
frequent nouns 5  previously used in (Zhu and 
Hovy, 2007): rate, president, people, part, point, 
director, revenue, bill, future, and order.  
The Interest data set developed by Bruce and 
Wiebe (1994) has been previously used for WSD 
(Ng and Lee, 1996). This data set consists of 
2369 sentences of the noun ?interest? with its 
correct sense manually labeled. The noun 
?interest? has six different senses in this data set.  
TWA developed by Mihalcea and Yang on 2003, 
is sense tagged data for six words with two-way 
ambiguities, previously used in WSD research. 
These six words are bass, crane, motion, palm, 
plant and tank. All instances were drawn from 
the British National Corpus. 
5.3 Stopping Criteria for Uncertainty Sam-
pling 
In order to evaluate the effectiveness of our stop-
ping criteria, we first apply them to uncertainty 
sampling for active learning for WSD and TC 
tasks. Table 1 shows that ?First? method gener-
ally achieves higher performance than that of the 
?All? method.  We can see from the ?Average? 
row that stopping criteria MC, ME, CC-MC, CC-
ME and CC-OU achieve close average accuracy 
performance to the ?All? method whereas OU 
and CC achieve lower average accuracy 
performance. OU method achieves the lowest 
average accuracy performance. CC-ME achieves 
the highest average accuracy of 89.6%, followed 
by CC-MC. 
Compared to the ?First? method, CC-OU 
achieves the best average percentage 
performance of 37.03% (i.e. the closest one to 
the ?First? method), followed by ME method. On 
six evaluation data sets, Table 1 shows that CC-
ME method achieves 4 out of 6 highest accuracy 
performances, followed by CC-MC and MC 
methods. And CC-ME method also achieves 3 
out of 6 best percentage performance, followed 
by CC, CC-OU and ME methods.  
Among these four individual stopping criteria, 
ME outperforms MC, OU and CC. However, ME 
method can only be applied to batch-based 
selection because ME criterion is based on the 
feedback from Oracle. Too few informative 
candidates may not be adequate for obtaining a 
reasonable feedback for ME criterion.  
 
5 See http://www.nlplab.com/ontonotes-10-nouns.rar 
1133
Data set All First MC ME OU CC CC-MC CC-ME CC-OU
0.910 0.911 0.910 0.910 0.837 0.912 0.912 0.913 0.912 WebKB 
100% 31.50% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53%
0.880 0.884 0.877 0.879 0.868 0.876 0.879 0.880 0.876 Comp2a 
100% 35.12% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35% 
0.900 0.901 0.887 0.888 0.880 0.879 0.891 0.893 0.882 Comp2b 
100% 41.66% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81% 
0.939 0.942 0.929 0.934 0.928 0.936 0.940 0.939 0.939 Ontonotes 
100% 22.81% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75% 
0.908 0.910 0.910 0.906 0.906 0.901 0.910 0.906 0.906 Interest 
100% 29.83% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62% 
0.846 0.858 0.843 0.844 0.837 0.820 0.841 0.845 0.838 TWA 
100% 59.67% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12% 
0.897 0.901 0.892 0.893 0.876 0.887 0.895 0.896 0.892 Average 
100% 37.43% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03%
Table 1. Effectiveness of seven stopping criteria for uncertainty sampling for active learning. For each 
data set, Table 1 shows the accuracy of the classifier and percentage of learned instances over all 
unlabeled data when each stopping criterion is met. The boldface numbers indicate the best corre-
sponding performances. 
Data set All MC ME OU CC CC-MC CC-ME CC-OU 
0.858 0.808 0.818 0.601 0.820 0.820 0.824 0.820 WebKB 
100% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53% 
0.894 0.838 0.839 0.825 0.837 0.838 0.846 0.837 Comp2a 
100% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35% 
0.922 0.884 0.882 0.878 0.874 0.885 0.883 0.879 Comp2b 
100% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81% 
0.925 0.923 0.924 0.921 0.921 0.932 0.927 0.929 Ontonotes 
100% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75% 
0.899 0.906 0.890 0.890 0.885 0.906 0.891 0.890 Interest 
100% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62% 
0.812 0.784 0.793 0.765 0.775 0.799 0.810 0.794 TWA 
100% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12% 
0.885 0.857 0.857 0.813 0.852 0.863 0.863 0.858 Average 
100% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03% 
Table 2. Effectiveness of seven stopping criteria for heterogeneous uncertainty sampling for active 
learning. Table 2 shows the accuracy of the classifier and percentage of learned instances over all 
unlabeled data when each stopping criterion is met. The boldface numbers indicate the best corre-
sponding performances. 
 
Interestingly, our proposed CC method 
acheves the best macro-average percentage 
performance on the TWA data set, however, 
other criteria work poorly, compared to the 
?First? method. Actually the sense distribution of 
each noun in TWA set is very skewed. From 
WSD experimental results on TWA, we found 
that only few learned instances can train the 
MaxEnt-based classifier with the highest 
accuracy performance.  
In Table 1, the boldface numbers indicate the 
best performances. Three combination strategies 
achieve 12 out of 16 best performances 6 . We 
                                                                                                                          
6 CC and CC-OU methods achieve the same best percentage 
performance of 31.53% on WebKB data set. MC and CC-
think the general combination strategy 
outperform individual stopping criteria for 
uncertainty sampling for active learning, because 
four individual stopping criteria only totally 
achieve 4 out of 16 best performances. 
5.4 Stopping Criteria for Heterogeneous 
Uncertainty Sampling 
In the following comparison experiments on het-
erogeneous uncertainty sampling, a MaxEnt-
based classifier is used to select the most infor-
mative examples for training an another type of 
classifier based on multinomial na?ve Bayes (NB) 
model (McCallum and Nigam, 1998b).  
 
MC methods achieve the same highest accuracy perform-
ance of 91% on Interest data set.  
1134
Table 2 shows that the NB-based classifier 
trained on all data (i.e. ?All method?) achieves 
only 1.2% lower average accuracy performance 
than that of MaxEnt-based classifier. However, 
we can see from Table 2 that accuracy perform-
ances of each stopping criterion for heterogene-
ous uncertainty sampling are apparently lower 
than that for uncertainty sampling shown in Ta-
ble 1. The main reason is that an uncertain ex-
ample for one classifier (i.e. MaxEnt) may not be 
an uncertain example for other classifiers (i.e. 
NB). This comparison experiments aim to ana-
lyze the accuracy effectiveness of stopping crite-
ria for heterogeneous uncertainty sampling, 
compared to that for uncertainty sampling shown 
in Table 1. Therefore we do not provide the re-
sults of the ?First? method for heterogeneous 
uncertainty sampling. The ?Average? row shows 
that CC-MC and CC-ME achieve the highest 
average accuracy performance of 86.3%, fol-
lowed by CC-OU. On six data sets, CC-ME 
achieves 3 out of 6 highest accuracy perform-
ances.  
Interestingly, these stopping criteria work very 
well on the Ontonotes and Interest data sets. 
Three combination strategies achieve higher ac-
curacy performance than the ?All? method on 
Ontonotes. However, the accuracy performances 
of these seven stopping criteria for heterogene-
ous uncertainty sampling on WebKB, Comp2a, 
Comp2b, and TWA degrade, compared to the 
?All? method.  
The general combination strategy achieves 7 
out of 9 boldface accuracy performances7. And 
only MC method achieves other 2 boldface accu-
racy performances. Experimental results show 
that the general combination strategy outper-
forms individual stopping criteria in overall for 
heterogeneous uncertainty sampling.   
6 Related Work 
 Zhu and Hovy (2007) proposed a confidence-
based framework to predict the upper bound and 
the lower bound for a stopping criterion in active 
learning. Actually this framework is a very 
coarse solution that simply uses max-confidence 
method to predict the upper bound, and uses min-
error method to predict the lower bound. Zhu et. 
al. (2008) proposed a minimum expected error 
strategy to learn a stopping criterion through es-
                                                 
7 MC and CC-MC methods achieve the same highest accu-
racy performance of 90.6% on Interest data set. CC-MC and 
CC-CA methods achieve the same highest average accuracy 
performance of 86.3%. 
timation of the classifier?s expected error on fu-
ture unlabeled examples. However, both two 
studies did not give an answer to the problem of 
how to define an appropriate threshold for the 
stopping criterion in a specific task.   
Vlachos (2008) also studied a stopping crite-
rion of active learning based on the estimate of 
the classifier?s confidence, in which a separate 
and large dataset is prepared in advance to esti-
mate the classifier?s confidence. However, there 
is a risk to be misleading because how many 
examples are required for this pregiven separate 
dataset is an open question in real-world 
applications, and it can not guarantee that the 
classifier shows a rise-peak-drop confidence 
pattern during active learning process.  
Schohn and Cohn (2000) proposed a stopping 
criterion for active learning with support vector 
machines based on an assumption that the data 
used is linearly separable. However, in most real-
world cases this assumption seems to be 
unreasonable and difficult to satisfy. And their 
stopping criterion cannot be applied for active 
learning with other type of classifier such as NB, 
MaxEnt models.  
7 Discussion 
We believe that a classifier?s performance 
change is a good signal of stopping the active 
learning process. It is worth studying further how 
to combine the factor of performance change 
with our proposed stopping criteria. 
Among these stopping criteria, ME, CC, CC-
ME can be used directly for committee-based 
sampling (Engelson and Dagan, 1999) for active 
learning. However, to use MC, OU, CC-MC and 
CC-OU for committee-based sampling, we 
should adopt a new uncertainty measurement 
such as vote entropy to measure the uncertainty 
of each unlabled example in the pool. 
In the above active learning comparison 
experiments, the confidence estimation for each 
confidence-based stopping criterion is done 
within the unlabeled pool U. We think that for 
these confidence-based stopping criteria except 
SA method, confidence estimation on a large-
scale outside unlabeled data set is worth studying 
in the future work. 
8 Conclusion and Future Work 
In this paper, we address the stopping criterion 
issue of active learning, and propose a new stop-
ping criterion, classification-change, which con-
siders the potential ability of each unlabeled ex-
1135
ample on changing decision boundaries. To solve 
the problem of predefining an appropriate 
threshold for each confidence-based stopping 
criterion, a multi-criteria-based general combina-
tion strategy is proposed. Experimental results on 
uncertainty sampling and heterogeneous uncer-
tainty sampling show that these stopping criteria 
work well on evaluation data sets, and combina-
tion strategies can achieve better performance 
than individual criteria. Some interesting future 
work is to investigate further how to combine the 
best of these criteria, and how to consider per-
formance change to define an appropriate stop-
ping criterion for active learning.  
Acknowledgments 
This work was supported in part by the National 
863 High-tech Project (2006AA01Z154) and the 
Program for New Century Excellent Talents in 
University (NCET-05-0287). 
References 
Berger Adam L., Vincent J. Della Pietra, Stephen A. 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing. Computational 
Linguistics 22(1):39?71. 
Bruce Rebecca and Janyce Wiebe. 1994. Word sense 
disambiguation using decomposable models. Pro-
ceedings of the 32nd annual meeting on Associa-
tion for Computational Linguistics, pp. 139-146. 
Chen Jinying, Andrew Schein, Lyle Ungar and Mar-
tha Palmer. 2006. An empirical study of the behav-
ior of active learning for word sense disambigua-
tion. Proceedings of the main conference on Hu-
man Language Technology Conference of the 
North American Chapter of the Association of 
Computational Linguistics, pp. 120-127 
Engelson S. Argamon and I. Dagan. 1999. Commit-
tee-based sample selection for probabilistic classi-
fiers. Journal of Artificial Intelligence Research 
(11):335-360. 
Hovy Eduard, Mitchell Marcus, Martha Palmer, 
Lance Ramshaw and Ralph Weischedel. 2006. 
Ontonotes: The 90% Solution. In Proceedings of 
the Human Language Technology Conference of 
the NAACL, pp. 57-60. 
Lee Yoong Keok and Hwee Tou Ng. 2002. An em-
pirical evaluation of knowledge sources and learn-
ing algorithm for word sense disambiguation. In 
Proceedings of the ACL conference on Empirical 
methods in natural language processing, pp. 41-48 
Lewis David D. and Jason Catlett. 1994. Heterogene-
ous uncertainty sampling for supervised learning. 
In Proceedings of 11th International Conference on 
Machine Learning, pp. 148-156 
Lewis David D. and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In 
Proceedings of the 17th annual international ACM 
SIGIR conference on Research and development in 
information retrieval, pp. 3-12 
McCallum Andrew and Kamal Nigam. 1998a. Em-
ploying EM in pool-based active learning for text 
classification. In Proceedings of the 15th Interna-
tional Conference on Machine Learning, pp.350-
358 
McCallum Andrew and Kamal Nigam. 1998b. A 
comparison of event models for na?ve bayes text 
classification. In AAAI-98 workshop on learning 
for text categorization. 
Ng Hwee Tou and Hian Beng Lee. 1996. Integrating 
multiple knowledge sources to disambiguate word 
sense: an exemplar-based approach. In Proceed-
ings of the 34th Annual Meeting of the Association 
for Computational Linguistics, pp.40-47 
Ngai Grace and David Yarowsky. 2000. Rule writing 
or annotation: cost-efficient resource usage for 
based noun phrase chunking. In Proceedings of the 
38th Annual Meeting of the Association for Com-
putational Linguistics, pp. 117-125 
Roy Nicholas and Andrew McCallum. 2001. Toward 
optimal active learning through sampling estima-
tion of error reduction. In Proceedings of the 
Eighteenth International Conference on Machine 
Learning, pp. 441-448 
Schein Andrew I. and Lyle H. Ungar. 2007. Active 
learning for logistic regression: an evaluation. 
Machine Learning 68(3): 235-265 
Schohn Greg and David Cohn. 2000. Less is more: 
Active learning with support vector machines. In 
Proceedings of the Seventeenth International Con-
ference on Machine Learning, pp. 839-846 
Shen Dan, Jie Zhang, Jian Su, Guodong Zhou and 
Chew-Lim Tan. 2004. Multi-criteria-based active 
learning for named entity recognition. In Proceed-
ings of the 42nd Annual Meeting on Association 
for Computational Linguistics. 
Tang Min, Xiaoqiang Luo and Salim Roukos. 2002. 
Active learning for statistical natural language 
parsing. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguistics, 
pp. 120-127 
Vlachos Andreas. 2008. A stopping criterion for ac-
tive learning. Computer Speech and Language. 
22(3): 295-312 
Zhu Jingbo and Eduard Hovy. 2007. Active learning 
for word sense disambiguation with methods for 
addressing the class imbalance problem. In Pro-
ceedings of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and 
Computational Natural Language Learning, pp. 
783-790 
Zhu Jingbo, Huizhen Wang and Eduard Hovy. 2008. 
Learning a stopping criterion for active learning 
for word sense disambiguation and text classifica-
tion. In Proceedings of the Third International Joint 
Conference on Natural Language Processing, pp. 
366-372 
1136
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 161?170, Prague, June 2007. c?2007 Association for Computational Linguistics
LEDIR: An Unsupervised Algorithm for Learning Directionality of Inference Rules 
Rahul Bhagat, Patrick Pantel, Eduard Hovy Information Sciences Institute University of Southern California Marina del Rey, CA {rahul,pantel,hovy}@isi.edu   Abstract Semantic inference is a core component of many natural language applications. In re-sponse, several researchers have developed algorithms for automatically learning infer-ence rules from textual corpora. However, these rules are often either imprecise or un-derspecified in directionality. In this paper we propose an algorithm called LEDIR that filters incorrect inference rules and identi-fies the directionality of correct ones. Based on an extension to Harris?s distribu-tional hypothesis, we use selectional pref-erences to gather evidence of inference di-rectionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines. 1 Introduction Paraphrases are textual expressions that convey the same meaning using different surface forms. Tex-tual entailment is a similar phenomenon, in which the presence of one expression licenses the validity of another. Paraphrases and inference rules are known to improve performance in various NLP applications like Question Answering (Harabagiu and Hickl 2006), summarization (Barzilay et al 1999) and Information Retrieval (Anick and Tipir-neni 1999).  Paraphrase and entailment involve inference rules that license a conclusion when a premise is given.  Deciding whether a proposed inference rule is fully valid is difficult, however, and most NL systems instead focus on plausible inference.  In this case, one statement has some likelihood of 
being identical in meaning to, or derivable from, the other.  In the rest of this paper we discuss plau-sible inference only.   Given the importance of inference, several re-searchers have developed inference rule collec-tions. While manually built resources like Word-Net (Fellbaum 1998) and Cyc (Lenat 1995) have been around for years, for coverage and domain adaptability reasons many recent approaches have focused on automatic acquisition of paraphrases (Barzilay and McKeown 2001) and inference rules (Lin and Pantel 2001; Szpektor et al 2004). The downside of these approaches is that they often result in incorrect inference rules or in inference rules that are underspecified in directionality (i.e. asymmetric but are wrongly considered symmet-ric). For example, consider an inference rule from DIRT (Lin and Pantel 2001): X eats Y ? X likes Y  (1)   All rules in DIRT are considered symmetric. Though here, one is most likely to infer that ?X eats Y? ? ?X likes Y?, because if someone eats something, he most probably likes it1, but if he likes something he might not necessarily be able to eat it. So for example, given the sentence ?I eat spicy food?, one is mostly likely to infer that ?I like spicy food?. On the other hand, given the sentence ?I like rollerblading?, one cannot infer that ?I eat rollerblading?. In this paper, we propose an algorithm called LEDIR (pronounced ?leader?) for LEarning Di-rectionality of Inference Rules. Our algorithm fil-ters incorrect inference rules and identifies the di-rectionality of the correct ones. Our algorithm                                                 1 There could be certain usages of ?X eats Y? where, one might not be able to infer ?X likes Y? (for example meta-phorical). But, in most cases, this inference holds. 
161
works with any resource that produces inference rules of the form shown in example (1). We use both the distributional hypothesis and selectional preferences as the basis for our algorithm. We pro-vide empirical evidence to validate the following main contribution:  Claim: Relational selectional preferences can be used to automatically determine the plausibility and directionality of an inference rule. 2 Related Work In this section, we describe applications that can benefit by using inference rules and their direc-tionality.  We then talk about some previous work in this area. 2.1 Applications Open domain question answering approaches often cast QA as the problem of finding some kind of semantic inference between a question and its an-swer(s) (Moldovan et al 2003; Echiabi and Marcu 2003). Harabagiu and Hickl (2006) recently dem-onstrated that textual entailment inference informa-tion, which in this system is a set of directional inference relations, improves the performance of a QA system significantly even without using any other form of semantic inference. This evidence supports the idea that learning the directionality of other sets of inference rules may improve QA per-formance.  In Multi-Document Summarization (MDS), paraphrasing is useful for determining sentences that have similar meanings (Barzilay et al 1999). Knowing the directionality between the inference rules here could allow the MDS system to choose either the more specific or general sentence de-pending on the purpose of the summary. In IR, paraphrases have been used for query ex-pansion, which is known to promote effective re-trieval (Anick and Tipirneni 1999). Knowing the directionality of rules here could help in making a query more general or specific depending on the user needs. 2.2 Learning Inference Rules Automatically learning paraphrases and inference rules from text is a topic that has received much attention lately. Barzilay and McKeown (2001) for paraphrases, DIRT (Lin and Pantel 2001) and TEASE (Szpektor et al 2004) for inference rules, 
are recent approaches that have achieved promis-ing results. While all these approaches produce collections of inference rules that have good recall, they suffer from the complementary problem of low precision. They also make no attempt to dis-tinguish between symmetric and asymmetric infer-ence rules. Given the potential positive impact shown in Section 2.1 of learning the directionality of inference rules, there is a need for methods, such as the one we present, to improve existing automatically created resources. 2.3 Learning Directionality There have been a few approaches at learning the directionality of restricted sets of semantic rela-tions, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmet-ric relations between verbs. They manually exam-ined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candi-date verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selec-tional preferences of a single verb, i.e. the semantic types of a verb?s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrences between verbs and nouns to learn the verb-verb inference rules. Like the previous two methods, their approach too deals only with verbs and is lim-ited to learning inference rules that are temporal in nature. Geffet and Dagan (2005) proposed an extension to the distributional hypothesis to discover entail-ment relation between words. They model the con-text of a word using its syntactic features and com-pare the contexts of two words for strict inclusion to infer lexical entailment. In principle, their work is the most similar to ours. Their method however 
162
is limited to lexical entailment and they show its effectiveness for nouns. Our method on the other hand deals with inference rules between binary relations and includes inference rules between ver-bal relations, non-verbal relations and multi-word relations. Our definition of context and the meth-odology for obtaining context similarity and over-lap is also much different from theirs. 3 Learning Directionality of Inference Rules The aim of this paper is to filter out incorrect infer-ence rules and to identify the directionality of the correct ones. Let pi ? pj be an inference rule where each p is a binary semantic relation between two entities x and y. Let <x, p, y> be an instance of relation p. Formal problem definition: Given the inference rule pi ? pj, we want to conclude which one of the following is more appropriate: 1. pi ? pj 2. pi ? pj 3. pi ? pj 4. No plausible inference Consider the example (1) from section 1. There, it is most plausible to conclude  ?X eats Y? ? ?X likes Y?.  Our algorithm LEDIR uses selectional prefer-ences along the lines of Resnik (1996) and Pantel et al (2007) to determine the plausibility and di-rectionality of inference rules. 3.1 Underlying Assumption Many approaches to modeling lexical semantics have relied on the distributional hypothesis (Harris 1954), which states that words that appear in the same contexts tend to have similar meanings. The idea is that context is a good indicator of a word meaning. Lin and Pantel (2001) proposed an exten-sion to the distributional hypothesis and applied it to paths in dependency trees, where if two paths tend to occur in similar contexts it is hypothesized that the meanings of the paths tend to be similar. In this paper, we assume and propose a further extension to the distributional hypothesis and call it the ?Directionality Hypothesis?. Directionality Hypothesis: If two binary semantic relations tend to occur in similar contexts and the first one occurs in significantly more contexts than 
the second, then the second most likely implies the first and not vice versa. The intuition here is that of generality. The more general a relation, more the types (and number) of contexts in which it is likely to appear. Consider the example (1) from section 1. The fact is that there are many more things that someone might like than those that someone might eat. Hence, by applying the directionality hypothesis, one can in-fer that ?X eats Y? ? ?X likes Y?. The key to applying the distributional hypothe-sis to the problem at hand is to model the contexts appropriately and to introduce a measure for calcu-lating context similarity. Concepts in semantic space, due to their abstractive power, are much richer for reasoning about inferences than simple surface words. Hence, we model the context of a relation p of the form <x, p, y> by using the seman-tic classes C(x) and C(y) of words that can be in-stantiated for x and y respectively. To measure context similarity of two relations, we calculate the overlap coefficient (Manning and Sch?tze, 1999) between their contexts. 3.2 Selectional Preferences The selectional preferences of a predicate is the set of semantic classes that its arguments can belong to (Wilks 1975). Resnik (1996) gave an informa-tion theoretical formulation of the idea. Pantel et al (2007) extended this idea to non-verbal rela-tions by defining the relational selectional prefer-ences (RSPs) of a binary relation p as the set of semantic classes C(x) and C(y) of words that can occur in positions x and y respectively. The set of semantic classes C(x) and C(y) can be obtained either from a manually created taxonomy like WordNet as proposed in the above previous approaches or by using automatically generated classes from the output of a word clustering algo-rithm as proposed in Pantel et al (2007). For ex-ample given a relation like ?X likes Y?, its RSPs from WordNet could be {individual, so-cial_group?} for X and {individual, food, activ-ity?} for Y. In this paper, we deployed both the Joint Rela-tional Model (JRM) and Independent Relational Model (IRM) proposed by Pantel et al (2007) to obtain the selectional preferences for a relation p.   
163
Model 1: Joint Relational Model (JRM) The JRM uses a large corpus to learn the selec-tional preferences of a binary semantic relation by considering its arguments jointly. Given a relation p and large corpus of English text, we first find all occurrences of relation p in the corpus. For every instance <x, p, y> in the cor-pus, we obtain the sets C(x) and C(y) of the seman-tic classes that x and y belong to. We then accumu-late the frequencies of the triples <c(x), p, c(y)> by assuming that every c(x) ? C(x) can co-occur with every  c(y) ? C(y) and vice versa. Every triple <c(x), p, c(y)> obtained in this manner is a candi-date selectional preference for p. Following Pantel et al (2007), we rank these candidates using Pointwise mutual information (Cover and Thomas 1991). The ranking function is defined as the strength of association between two semantic classes, cx and cy2, given the relation p: 
? 
pmi c
x
p; c
y
p
( )
= log
P c
x
,c
y
p
( )
P c
x
p
( )
P c
y
p
( )
                   (3.1) 
Let |cx, p, cy| denote the frequency of observing the instance <c(x), p, c(y)>. We estimate the prob-abilities of Equation 3.1 using maximum likeli-hood estimates over our corpus: 
? 
P c
x
p
( )
=
c
x
, p,?
?, p,?
P c
y
p
( )
=
?, p,c
y
?, p,?
P c
x
,c
y
p
( )
=
c
x
, p,c
y
?, p,?
                 (3.2) 
We estimate the above frequencies using: 
  
? 
c
x
, p,? =
w, p,?
C w
( )
w?c
x
?
?, p,c
y
=
?, p,w
C w
( )
w?c
y
?
c
x
, p,c
y
=
w
1
, p,w
2
C w
1
( )
? C w
2
( )
w
1
?c
x
,w
2
?c
y
?
       (3.3) 
where |x, p, y| denotes the frequency of observing the instance <x, p, y> and |C(w)| denotes the num-ber of classes to which word w belongs. |C(w)| dis-tributes w?s mass equally among all of its senses C(w). Model 2: Independent Relational Model (IRM) Due to sparse data, the JRM is likely to miss some pair(s) of valid relational selectional preferences. Hence we use the IRM, which models the argu-ments of a binary semantic relation independently.                                                 2 cx and cy are shorthand for c(x) and c(y) in our equations. 
Similar to JRM, we find all instances of the form <x, p, y> for a relation p. We then find the sets C(x) and C(y) of the semantic classes that x and y belong to and accumulate the frequencies of the triples <c(x), p, *> and <*, p, c(y)> where c(x) ? C(x) and  c(y) ? C(y). All the tuples <c(x), p, *> and <*, p, c(y)> are the independent candidate RSPs for a relation p and we rank them according to equation 3.3. Once we have the independently learnt RSPs, we need to convert them into a joint representation for use by the inference plausibility and direction-ality model. To do this, we obtain the Cartesian product between the sets <C(x), p, *>  and <*, p, C(y)> for a relation p. The Cartesian product be-tween two sets A and B is given by: 
?
A ? B = a,b
( )
:?a? A and ?b? B
{ }
        (3.4) Similarly we obtain: 
? 
C
x
, p,? ? ?, p,C
y
=
c
x
, p,c
y
: ? c
x
, p,? ? C
x
, p,? and
? ?, p,c
y
? ?, p,C
y
? 
? 
? 
? 
? 
? 
? 
? 
? 
? 
 (3.5) 
The Cartesian product in equation 3.5 gives the joint representation of the RSPs of the relation p learnt using IRM. In the joint representation, the IRM RSPs have the form <c(x), p, c(y)>  which is the same form as the JRM RSPs. 3.3 Inference plausibility and directionality model Our model for determining inference plausibility and directionality is based on the intuition that for an inference to hold between two semantic rela-tions there must exist sufficient overlap between their contexts and the directionality of the infer-ence depends on the quantitative comparison be-tween their contexts. Here we model the context of a relation by the selectional preferences of that relation. We deter-mine the plausibility of an inference based on the overlap coefficient (Manning and Sch?tze, 1999) between the selectional preferences of the two paths. We determine the directionality based on the difference in the number of selectional preferences of the relations when the inference seems plausi-ble.  Given a candidate inference rule pi ? pj, we first obtain the RSPs <C(x), pi, C(y)>  for pi and <C(x), pj, C(y)> for pj.  We then calculate the over-lap coefficient between their respective RSPs. Overlap coefficient is one of the many distribu-
164
tional similarity measures used to calculate the similarity between two vectors A and B: 
? 
sim A,B
( )
=
A? B
min A , B
( )
           (3.6) 
The overlap coefficient between the selectional preferences of pi and pj is calculated as: 
? 
sim p
i
, p
j
( )
=
C
x
, p
i
,C
y
? C
x
, p
j
,C
y
min C
x
, p
i
,C
y
,C
x
, p
j
,C
y
( )
          (3.7) 
If sim(pi,pj) is above a certain empirically de-termined threshold ? (?1), we conclude that the inference is plausible, i.e.: If  
? 
sim p
i
,p
j
( )
??  we conclude the inference is plausible else  we conclude the inference is not plausible For a plausible inference, we then compute the ratio between the number of selectional prefer-ences |C(x), pi, C(y)|  for pi and |C(x), pj, C(y)| for pj and compare it against an empirically determined threshold ? (?1) to determine the direction of in-ference. So the algorithm is: If   
? 
C
x
, p
i
,C
y
C
x
, p
j
,C
y
? ?       we conclude pi ? pj 
else if  
? 
C
x
, p
i
,C
y
C
x
, p
j
,C
y
?
1?     we conclude pi ? pj else                 we conclude pi ? pj 4 Experimental Setup In this section, we describe our experimental setup to validate our claim that LEDIR can be used to determine plausibility and directionality of an in-ference rule. Given an inference rule of the form pi ? pj, we want to use automatically learned relational selec-tional preferences to determine whether the infer-ence rule is valid and if it is valid then what its di-rectionality is.  4.1 Inference Rules LEDIR can work with any set of binary semantic inference rules. For the purpose of this paper, we chose the inference rules from the DIRT resource (Lin and Pantel 2001). DIRT consists of 12 million rules extracted from 1GB of newspaper text (AP Newswire, San Jose Mercury and Wall Street 
Journal). For example, ?X eats Y? ? ?X likes Y? is an inference rule from DIRT. 4.2 Semantic Classes Appropriate choice of semantic classes is crucial for learning relational selectional preferences. The ideal set should have semantic classes that have the right balance between abstraction and discrimina-tion, the two important characteristics that are of-ten conflicting. A very general class has limited discriminative power, while a very specific class has limited abstractive power. Finding the right balance here is a separate research problem of its own. Since the ideal set of universally acceptable se-mantic classes in unavailable, we decided to use the Pantel et al (2007) approach of using two sets of semantic classes. This approach gave us the ad-vantage of being able to experiment with sets of classes that vary a lot in the way they are generated but try to maintain the granularity by obtaining approximately the same number of classes. The first set of semantic classes was obtained by running the CBC clustering algorithm (Pantel and Lin, 2002) on TREC-9 and TREC-2002 newswire collections consisting of over 600 million words. This resulted in 1628 clusters, each representing a semantic class. The second set of semantic classes was obtained by using WordNet 2.1 (Fellbaum 1998). We ob-tained a cut in the WordNet noun hierarchy3 by manual inspection and used all the synsets below a cut point as the semantic class at that node. Our inspection showed that the synsets at depth four formed the most natural semantic classes4. A cut at depth four resulted in a set of 1287 semantic classes, a set that is much coarser grained than WordNet which has an average depth of 12. This seems to be a depth that gives a reasonable abstrac-tion while maintaining good discriminative power. It would however be interesting to experiment with more sophisticated algorithms for extracting se-mantic classes from WordNet and see their effect 
                                                3 Since we are dealing with only noun binary relations, we use only WordNet noun Hierarchy. 4 By natural, here, we simply mean that a manual inspection by the authors showed that, at depth four, the resulting clus-ters had struck a better granularity balance than other cutoff points. We acknowledge that this is a very coarse way of ex-tracting concepts from WordNet. 
165
on the relational selectional preferences, something we do not address this in this paper. 4.3 Implementation We implemented LEDIR with both the JRM and IRM models using inference rules from DIRT and semantic classes from both CBC and WordNet. We parsed the 1999 AP newswire collection consisting of 31 million words with Minipar (Lin 1993) and used this to obtain the probability statistics for the models (as described in section 3.2).  We performed both system-wide evaluations and intrinsic evaluations with different values of ? and ? parameters. Section 5 presents these results and our error analysis. 4.4 Gold Standard Construction In order to evaluate the performance of the differ-ent systems, we compare their outputs against a manually annotated gold standard. To create this gold standard, we randomly sampled 160 inference rules of the form pi ? pj from DIRT. We discarded three rules since they contained nominalizations5.  For every inference rule of the form pi ? pj, the annotation guideline asked annotators (in this pa-per we used two annotators) to choose the most appropriate of the four options: 1. pi ? pj 2. pi ? pj 3. pi ? pj 4. No plausible inference To help the annotators with their decisions, the annotators were provided with 10 randomly chosen instances for each inference rule. These instances, extracted from DIRT, provided the annotators with context where the inference could hold. So for ex-ample, for the inference rule ?X eats Y? ? ?X likes Y?, an example instance would be ?I eat spicy food? ? ?I like spicy food?. The annotation guide-line however gave the annotators the freedom to think of examples other than the ones provided to make their decisions. The annotators found that while some decisions were quite easy to make, the more complex ones                                                 5 For the purpose of simplicity, we in our experiments did not use DIRT rules containing nominalizations. The algo-rithm however can be applied without change to inference rules containing nominalization. In fact, in the resource that we plan to release soon, we have applied the algorithm without change to DIRT rules containing nominalizations. 
often involved the choice between bi-directionality and one of the directions. To minimize disagree-ments and to get a better understanding of the task, the annotators trained themselves by annotating several samples together. We divided the set of 157 inference rules, into a development set of 57 inference rules and a blind test set of 100 inference rules. Our two annotators annotated the development test set together to train themselves. The blind test set was then annotated individually to test whether the task is well de-fined. We used the kappa statistic (Siegel and Castellan Jr. 1988) to calculate the inter-annotator agreement, resulting in ?=0.63. The annotators then looked at the disagreements together to build the final gold standard. All this resulted in a final gold standard of 100 annotated DIRT rules. 4.5 Baselines To get an objective assessment of the quality of the results obtained by using our models, we compared the output of our systems against three baselines: B-random: Randomly assigns one of the four pos-sible tags to each candidate inference rule.  B-frequent: Assigns the most frequently occurring tag in the gold standard to each candidate infer-ence rule. B-DIRT: Assumes each inference rule is bidirec-tional and assigns the bidirectional tag to each candidate inference rule. 5 Experimental Results In this section, we provide empirical evidence to validate our claim that the plausibility and direc-tionality of an inference rule can be determined using LEDIR. 5.1 Evaluation Criterion We want to measure the effectiveness of LEDIR for the task of determining the validity and direc-tionality of a set of inference rules. We follow the standard approach of reporting system accuracy by comparing system outputs on a test set with a manually created gold standard. Using the gold standard described in Section 4.4, we measure the accuracy of our systems using the following for-mula: 
166
erencesinput
erencestaggedcorrectly
Accuracy
inf
inf
=
 
5.2 Result Summary We ran all our algorithms with different parameter combinations on the development set (the 57 DIRT rules described in Section 4.4). This resulted in a total of 420 experiments on the development set. Based on these experiments, we used the accuracy statistic to obtain the best parameter combination for each of our four systems. We then used these parameter values to obtain the corresponding per-centage accuracies on the test set for each of the four systems. Model ?  ? Accuracy (%) B-random - - 25 B-frequent - - 34 B-DIRT - - 25 CBC 0.15 2 38 JRM WN 0.55 2 38 CBC 0.15 3 48 IRM WN 0.45 2 43 Table 1: Summary of results on the test set Table 1 summarizes the results obtained on the test set for the three baselines and for each of the four systems using the best parameter combina-tions obtained as described above. The overall best performing system uses the IRM algorithm with RSPs form CBC. Its performance is found to be significantly better than all the three baselines us-ing the Student?s paired t-test (Manning and Sch?tze, 1999) at p<0.05. However, this system is not statistically significant when compared with the other LEDIR implementations (JRM and IRM with WordNet). 5.3 Performance and Error Analysis The best performing system selected using the de-velopment set is the IRM system using CBC with the parameters ?=0.15 and ?=3. In general, the results obtained on the test set show that the IRM tends to perform better than the JRM. This obser-vation points at the sparseness of data available for learning RSPs for the more restrictive JRM, the reason why we introduced the IRM in the first place. A much larger corpus would be needed to obtain good enough coverage for the JRM. 
GOLD STANDARD  ? ? ? NO ? 16 1 3 7 ? 0 3 1 3 ? 7 4 22 15 SYST
EM 
NO 2 3 4 9 Table 2: Confusion Matrix for the best performing system, IRM using CBC with ?=0.15 and ?=3. Table 2 shows the confusion matrix for the overall best performing system as selected using the development set (results are taken from the test set). The confusion matrix indicates that the system does a very good job of identifying the directional-ity of the correct inference rules, but gets a big per-formance hit from its inability to identify the incor-rect inference rules accurately. We will analyze this observation in more detail below. Figure 1 plots the variation in accuracy of IRM with different RSPs and different values of ? and ?. The figure shows a very interesting trend.  It is clear that for all values of ?, systems for IRM us-ing CBC tend to reach their peak in the range 0.15 ? ? ? 0.25, whereas the systems for IRM using WordNet (WN), tend to reach their peak in the range 0.4 ? ? ? 0.6. This variation indicates the kind of impact the selection of semantic classes could have on the overall performance of the sys-tem. This is not hard evidence, but it does suggest that finding the right set of semantic classes could be one big step towards improving system accu-racy. 
 Figure 1: Accuracy variation for IRM with differ-ent values of ? and ?. Two other factors that have a big impact on the performance of our systems are the values of the system parameters ? and ?, which decide the plau-
167
sibility and directionality of an inference rule, re-spectively. To better study their effect on the sys-tem performances, we studied the two parameters independently. 
 Figure 2: Accuracy variation in predicting correct versus incorrect inference rules for different values of ?. 
 Figure 3: Accuracy variation in predicting direc-tionality of correct inference rules for different values of ?. Figure 2 shows the variation in the accuracy for the task of predicting the correct and incorrect in-ference rules for the different systems when vary-ing the value of ?. To obtain this graph, we classi-fied the inference rules in the test set only as cor-rect and incorrect without further classification based on directionality. All of our four systems obtained accuracy scores in the range of 68-70% showing a good performance on the task of deter-mining plausibility. This however is only a small improvement over the baseline score of 66% ob-tained by assuming every inference to be plausible (as will be shown below, our system has most im-pact not on determining plausibility but on deter-
mining directionality). Manual inspection of some system errors showed that the most common errors were due to the well-known ?problem of an-tonymy? when applying the distributional hypothe-sis. In DIRT, one can learn rules like ?X loves Y? ? ?X hates Y?. Since the plausibility of inference rules is determined by applying the distributional hypothesis and the antonym paths tend to take the same set of classes for X and Y, our models find it difficult to filter out the incorrect inference rules which DIRT ends up learning for this very same reason. To improve our system, one avenue of re-search is to focus specifically on filtering incorrect inference rules involving antonyms (perhaps using methods similar to (Lin et al 2003)). Figure 3 shows the variation in the accuracy for the task of predicting the directionality of the cor-rect inference rules for the different systems when varying the value of ?.  To obtain this graph, we separated the correct inference rules form the in-correct ones and ran all the systems on only the correct ones, predicting only the directionality of each rule for different values of ?. Too low a value of ? means that the algorithms tend to predict most things as unidirectional and too high a value means that the algorithms tend to predict everything as bidirectional. It is clear from the figure that the performance of all the systems reach their peak performance in the range 2 ? ? ? 4, which agrees with our intuition of obtaining the best system ac-curacy in a medium range. It is also seen that the best accuracy for each of the models goes up as compared to the corresponding values obtained in the general framework. The best performing sys-tem, IRM using CBC RSPs, reaches a peak accu-racy of 63.64%, a much higher score than its accu-racy score of 48% under the general framework and also a significant improvement over the base-line score of 48.48% for this task. Paired t-test shows that the difference is statistically significant at p<0.05. The baseline score for this task is ob-tained by assigning the most frequently occurring direction to all the correct inference rules. This paints a very encouraging picture about the ability of the algorithm to identify the directionality much more accurately if it can be provided with a cleaner set of inference rules. 
168
6 Conclusion Semantic inferences are fundamental to under-standing natural language and are an integral part of many natural language applications such as question answering, summarization and textual entailment. Given the availability of large amounts of text and with the increase in computation power, learning them automatically from large text cor-pora has become increasingly feasible and popular. We introduced the Directionality Hypothesis, which states that if two paths share a significant number of relational selectional preferences (RSPs) and if the first has many more RSPs than the second, then the second path implies the first. Our experiments show empirical evidence that the Directionality Hypothesis with RSPs can indeed be used to filter incorrect inference rules and find the directionality of correct ones. We believe that this result is one step in the direction of solving the basic problem of semantic inference. Several questions must still be addressed. The models need to be improved in order to address the problem of incorrect inference rules. The distribu-tional hypothesis does not provide a framework to address the issue with antonymy relations like ?X loves Y? ? ?X hates Y? and hence other ideas need to be investigated. Ultimately, our goal is to improve the perform-ance of NLP applications with better inferencing capabilities. Several recent data points, such as  (Harabagiu and Hickl 2006), and others discussed in Section 2.1, give promise that refined inference rules for directionality may indeed improve ques-tion answering, textual entailment and multi-document summarization accuracies. It is our hope that methods such as the one proposed in this paper may one day be used to harness the richness of automatically created inference rule resources within large-scale NLP applications. References Anick, P.G. and Tipirneni, S. 1999. The Paraphrase Search Assistant: Terminology Feedback for Iterative Information Seeking. In Proceedings of SIGIR 1999. pp. 53-159. Berkeley, CA Barzilay, R. and McKeown, K.R. 2001.Extracting Para-phrases from a Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, France. 
Barzilay, R.; McKeown, K.R. and Elhadad, M. 1999. Information Fusion in the Context of Multi-Document Summarization. In Proceedings of ACL 1999. College Park, Maryland. Chklovski, T. and Pantel, P. 2004. VerbOCEAN: Min-ing the Web for Fine-Grained Semantic Verb Rela-tions. In Proceedings of EMNLP 2004. Barcellona Spain. Cover, T.M. and Thomas, J.A. 1991. Elements of Infor-mation Theory. John Wiley & Sons. Echihabi, A. and Marcu. D. 2003. A Noisy-Channel Approach to Question Answering. In Proceedings of ACL 2003. Sapporo, Japan. Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT Press. Geffet, M.; Dagan, I. 2005. The Distributional Inclusion Hypothesis and Lexical Entailment. In Proceedings of ACL 2005. pp. 107-114. Ann Arbor, Michigan. Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual Entailment in Open-Domain Question An-swering. In Proceedings of ACL 2006.  pp. 905-912. Sydney, Australia. Harris, Z. 1954. Distributional structure. Word. 10(23): 146-162. Lenat, D. 1995. CYC: A large-scale investment in knowledge infrastructure. Communications of the ACM, 38(11):33?38. Lin, D. 1993. Parsing Without OverGeneration. In Pro-ceedings of  ACL 1993. pp. 112-120. Columbus, OH. Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for Question Answering. Natural Language Engineering 7(4):343-360. Lin, D.; Zhao, S.; Qin, L. and Zhou, M. 2003. Identify-ing Synonyms among Distributionally Similar Words. In Proceedings of IJCAI 2003, pp. 1492-1493. Acapulco, Mexico. Manning, C.D. and Sch?tze, H. 1999. Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, MA. Moldovan, D.; Clark, C.; Harabagiu, S. and Maiorano S.  2003. COGEX: A Logic Prover for Question An-swering. In Proceedings of HLT/NAACL 2003. Ed-monton, Canada. Pantel, P.; Bhagat, R.; Coppola, B.; Chklovski, T. and Hovy, E. 2007. ISP: Learning Inferential Selectional Preferences. In Proceedings of HLT/NAACL 2007. Rochester, NY. 
169
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In Proceedings of KDD 2002. pp. 613-619. Edmonton, Canada. Resnik, P. 1996. Selectional Constraints: An Informa-tion-Theoretic Model and its Computational Realiza-tion. Cognition, 61:127?159. Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for the Behavioral Sciences. McGraw-Hill. Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling web-based acquisition of entailment relations. In Proceedings of EMNLP 2004. pp. 41-48. Barce-lona, Spain. Torisawa, K. 2006. Acquiring Inference Rules with Temporal Constraints by Using Japanese Coordi-nated Sentences and Noun-Verb Co-occurances. In Proceedings of HLT/NAACL 2006. pp. 57-64. New York, New York. Wilks, Y. 1975. Preference Semantics.  In E.L. Keenan (ed.), Formal Semantics of Natural Language. Cam-bridge: Cambridge University Press. Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering Asymmetric Entailment Relations between Verbs using Selectional Preferences. In Pro-ceedings of COLING/ACL 2006. pp. 849-856. Syd-ney, Australia.   
170
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 783?790, Prague, June 2007. c?2007 Association for Computational Linguistics
Active Learning for Word Sense Disambiguation with Methods for    
Addressing the Class Imbalance Problem 
Jingbo Zhu 
University of Southern California 
Information Sciences Institute 
Northeastern University, P.R.China 
Natural Language Processing Lab 
Zhujingbo@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu 
 
 
Abstract 
In this paper, we analyze the effect of 
resampling techniques, including under-
sampling and over-sampling used in active 
learning for word sense disambiguation 
(WSD). Experimental results show that un-
der-sampling causes negative effects on ac-
tive learning, but over-sampling is a rela-
tively good choice. To alleviate the within-
class imbalance problem of over-sampling, 
we propose a bootstrap-based over-
sampling (BootOS) method that works bet-
ter than ordinary over-sampling in active 
learning for WSD. Finally, we investigate 
when to stop active learning, and adopt two 
strategies, max-confidence and min-error, 
as stopping conditions for active learning. 
According to experimental results, we sug-
gest a prediction solution by considering 
max-confidence as the upper bound and 
min-error as the lower bound for stopping 
conditions. 
1 Introduction 
Word sense ambiguity is a major obstacle to accu-
rate information extraction, summarization, and 
machine translation (Ide and Veronis, 1998). In 
recent years, a variety of techniques for machine 
learning algorithms have demonstrated remarkable 
performance for automated word sense disam-
biguation (WSD) (Chan and Ng, 2006; Dagan et. 
al., 2006; Xue et. al., 2006; Kohomban and Lee. 
2005; Dang and Palmer, 2005), when enough la-
beled training data is available. However, creating 
a large sense-tagged corpus is very expensive and 
time-consuming, because these data have to be an-
notated by human experts.  
Among the techniques to solve the knowledge 
bottleneck problem, active learning is a promising 
way (Lewis and Gale, 1994; McCallum and Ni-
gram, 1998). The purpose of active learning is to 
minimize the amount of human labeling effort by 
having the system automatically select for human 
annotation the most informative unannotated case.  
In real-world data, the distribution of the senses 
of a word is often very skewed. Some studies re-
ported that simply selecting the predominant sense 
provides superior performance, when a highly 
skewed sense distribution and insufficient context 
exist (Hoste et al, 2001; McCarthy et. al., 2004). 
The data set is imbalanced when at least one of the 
senses is heavily underrepresented compared to the 
other senses. In general, a WSD classifier is de-
signed to optimize overall accuracy without taking 
into account the class imbalance distribution in a 
real-world data set. The result is that the classifier 
induced from imbalanced data tends to over-fit the 
predominant class and to ignore small classes 
(Japkowicz and Stephen, 2002). Recently, much 
work has been done in addressing the class 
imbalance problem, reporting that resampling 
methods such as over-sampling and under-
sampling are useful in supervised learning with 
imbalanced data sets to induce more effective 
classifiers (Estabrooks et al, 2004; Zhou and Liu, 
2006).  
In general framework of active learning, the 
learner (i.e. supervised classifier) is formed by us-
ing supervised learning algorithms. To date, how-
ever, no-one has studied the effects of over-
sampling and under-sampling on active learning 
783
methods. In this paper, we study active learning 
with resampling methods addressing the class im-
balance problem for WSD. It is noteworthy that 
neither of these techniques need modify the 
architecture or learning algorithm, making them 
very easy to use and extend to other domains. 
Another problem in active learning is  knowing 
when to stop the process. We address this problem 
in this paper, and discuss how to form the final 
classifier for use. This is a problem of estimation 
of classifier effectiveness (Lewis and Gale, 1994). 
Because it is difficult to know when the classifier 
reaches maximum effectiveness, previous work 
used a simple stopping condition when the training 
set reaches desirable size. However, in fact it is 
almost impossible to predefine an appropriate size 
of desirable training data for inducing the most 
effective classifier. To solve the problem, we 
consider the problem of estimation of classifier 
effectiveness as a second task of estimating 
classifier confidence. This paper adopts two 
strategies: max-confidence and min-error, and sug-
gests a prediction solution by considering max-
confidence as the upper bound and min-error as the 
lower bound for the stopping conditions. 
2 Related Work 
The ability of the active learner can be referred to 
as selective sampling, of which two major schemes 
exist: uncertainty sampling and committee-based 
sampling. The former method, for example pro-
posed by Lewis and Gale (1994), is to use only one 
classifier to identify unlabeled examples on which 
the classifier is least confident. The latter method 
(McCallum and Nigam, 1998) generates a commit-
tee of classifiers (always more than two classifiers) 
and selects the next unlabeled example by the prin-
ciple of maximal disagreement among these classi-
fiers. With selective sampling, the size of the train-
ing data can be significantly reduced for text 
classification (Lewis and Gale, 1994; McCallum 
and Nigam, 1998), and word sense disambiguation 
(Chen, et al 2006).  
A method similar to committee-based sampling 
is co-testing proposed by Muslea et al (2000), 
which trains two learners individually on two 
compatible and uncorrelated views that should be 
able to reach the same classification accuracy. In 
practice, however, these conditions of view selec-
tion are difficult to meet in real-world word sense 
disambiguation tasks.  
Recently, much work has been done on the class 
imbalance problem. The well-known approach is 
resampling, in which some training material is du-
plicated. Two types of popular resampling methods 
exist for addressing the class imbalance problem: 
over-sampling and under-sampling. The basic idea 
of resampling methods is to change the training 
data distribution and make the data more balanced. 
It works ok in supervised learning, but has not 
been tested in active learning. Previous work re-
ports that cost-sensitive learning is a good solution 
to the class imbalance problem (Weiss, 2004). In 
practice, for WSD, the costs of various senses of a 
disambiguated word are unequal and unknown, 
and they are difficult to evaluate in the process of 
learning.   
In recent years, there have been attempts to ap-
ply active learning for word sense disambiguation 
(Chen et al, 2006). However, to our best knowl-
edge, there has been no such attempt to consider 
the class imbalance problem in the process of ac-
tive learning for WSD tasks. 
3 Resampling Methods 
3.1 Under-sampling 
Under-sampling is a popular method in addressing 
the class imbalance problem by changing the train-
ing data distribution by removing some exemplars 
of the majority class at random. Some previous 
work reported that under-sampling is effective in 
learning on large imbalanced data sets (Japkowicz 
and Stephen, 2002). However, as under-sampling 
removes some potentially useful training samples, 
it could cause negative effects on the classifier per-
formance.  
One-sided sampling is a method similar to un-
der-sampling, in which redundant and borderline 
training examples are identified and removed from 
training data (Kubat and Matwin, 1997). Kuban 
and Matwin reported that one-sided sampling is 
effective in learning with two-class large imbal-
anced data sets. However, the relative computa-
tional cost of one-sided sampling in active learning 
is very high, because sampling computations must 
be implemented for each learning iteration. Our 
primitive experimental results show that, in the 
multi-class problem of WSD, one-sided sampling 
degrades the performance of active learning. And 
784
due to the high computation complexity of one-
sided sampling, we use random under-sampling in 
our comparison experiments instead.  
To control the degree of change of the training 
data distribution, the ratio of examples from the 
majority and the minority class after removal from 
the majority class is called the removal rate (Jo 
and Japkowicz, 2004). If the removal rate is 1.0, 
then under-sampling methods build data sets with 
complete class balance. However, it was reported 
previously that perfect balance is not always the 
optimal rate (Estabrooks et al, 2004). In our com-
parison experiments, we set the removal rate for 
under-sampling to 0.8, since some cases have 0.8 
as the optimal rate reported in (Estabrooks et al, 
2004). 
3.2 Over-sampling 
Over-sampling is also a popular method in ad-
dressing the class imbalance problem by resam-
pling the small class until it contains as many ex-
amples as the large one. In contrast to under-
sampling, over-sampling is the process of adding 
examples to the minority class, and is accom-
plished by random sampling and duplication. Be-
cause the process of over-sampling involves 
making exact copies of examples, it usually in-
creases the training cost and may lead to overfit-
ting. There is a recent variant of over-sampling 
named SMOTE (Chawla et al, 2002) which is a 
synthetic minority over-sampling technique. The 
authors reported that a combination of SMOTE 
and under-sampling can achieve better classifier 
performance in ROC space than only under-
sampling the majority class. 
In our comparison experiments, we use over-
sampling, measured by a resampling rate called the 
addition rate (Jo and Japkowicz, 2004) that indi-
cates the number of examples that should be added 
into the minority class. The addition rate for over-
sampling is also set to 0.8 in our experiments. 
3.3 Bootstrap-based Over-sampling 
While over-sampling decreases the between-class 
imbalance, it increases the within-class imbalance 
(Jo and Japkowicz, 2004) because of the increase 
of exact copies of examples at random. To allevi-
ate this within-class imbalance problem, we pro-
pose a bootstrap-based over-sampling method 
(BootOS) that uses a bootstrap resampling tech-
nique in the process of over-sampling.  Bootstrap-
ping, explained below, is a resampling technique 
similar to jackknifing.  
There are two reasons for choosing a bootstrap 
method as resampling technique in the process of 
over-sampling. First, using a bootstrap set can 
avoid exactly copying samples in the minority 
class. Second, the bootstrap method may give a 
smoothing of the distribution of the training sam-
ples (Hamamoto et al, 1997), which can alleviate 
the within-class imbalance problem cased by over-
sampling.  
To generate the bootstrap set, we use a well-
known bootstrap technique proposed by Hama-
moto et al (1997) that does not select samples ran-
domly, allowing all samples in the minority 
class(es) an equal chance to be selected.  
Algorithm BootOS(X, N, r, k) 
Input: Minority class sample set X={x1, x2, ?, xn} of 
size n; Difference in number of examples between the 
majority and the minority class = N; Addition rate = r 
(< 1.0); Number of nearest neighbors = k. 
Output: bootstrap sample set XB of size N*r 
=X (xB1, xB2, ?, xB(N*r)). ?
1. For i = 1 To N*r 
2.       If i == n then (*all samples in minority class 
sample set have been used*) 
3.             j = 1; //the first sample is selected again  
4.       Else     
5.             j = i; // the i-th sample is selected 
6.       Endif 
7.       Select j-th sample xj (also as xj,0) from X 
8.       Find the k nearest neighbor samples xj,1, xj,2, 
?, xj,k using similarity functions. 
9.       Compute a bootstrap sample xBi: 
,01
k
Bi j ll
1x x
k =
= + ?  
10. Endfor 
11. return 
Figure 1. The BootOS algorithm 
4 Active Learning with Resampling 
In this work, we are interested in selective sam-
pling for pool-based active learning, and focus on 
uncertainty sampling (Lewis and Gale, 1994). The 
key point is how to measure the uncertainty of an 
unlabeled exemplar, and select a new exemplar 
with maximum uncertainty to augment the training 
data. The maximum uncertainty implies that the 
current classifier has the least confidence in its 
classification of this exemplar. The well-known 
entropy is a good uncertainty measurement widely 
785
used in active learning (zhang and Chen, 2002; 
Chen et al, 2006): 
1
( ) ( ) ( | ) log ( | )i j i j
j
U i H P p s w p s w
=
= = ??in i    (1) 
where U is the uncertainty measurement function 
H represents the entropy function. In the WSD 
task, p(sj|wi) is the predicted probability of sense sj 
outputted by the current classifier, when given a 
sample i containing a disambiguated word wi.  
Algorithm Active-Learning-with-Resampling(L,U,m) 
Input: Let L be initial small training data set; U the 
pool of unlabeled exemplars 
Output: labeled training data set L 
1. Resample L to generate new training data set L* 
using resampling techniques such as under-
sampling, over-sampling or BootOS, and then use 
L* to train the initial classifier 
2. Loop while adding new instances into L 
a. use the current classifier to probabilistically la-
bel all unlabeled exemplars in U 
b. Based on active learning rules, present m top-
ranked exemplars to oracle for labeling 
c. Augment L with the m new exemplars, and re-
move them from U 
d. Resample L to generate new training data set 
L* using resampling techniques such as under-
sampling, over-sampling, or BootOS, and use 
L* to retrain the current classifier         
3. Until the predefined stopping condition is met. 
4. return 
Figure 2. Active learning with resampling 
 
In step 1 and 2(d) in Fig. 2, if we do not gener-
ate L*, and L is used directly to train the current 
classifier, we call it ordinary active learning. In the 
process of active learning, we used the entropy-
based uncertainty measurement for all active learn-
ing frameworks in our comparison experiments. 
Actually our active learning with resampling is a 
heterogeneous approach in which the classifier 
used to select new instances is different from the 
resulting classifier (Lewis and Catlett, 1994).  
We utilize a maximum entropy (ME) model 
(Berger et al, 1996) to design the basic classifier 
used in active learning for WSD. The advantage of 
the ME model is the ability to freely incorporate 
features from diverse sources into a single, well-
grounded statistical model. A publicly available 
ME toolkit (Zhang et. al., 2004) was used in our 
experiments. In order to extract the linguistic fea-
tures necessary for the ME model, all sentences 
containing the target word were automatically part-
of-speech (POS) tagged using the Brill POS tagger 
(Brill, 1992). Three knowledge sources were used 
to capture contextual information: unordered single 
words in topical context, POS of neighboring 
words with position information, and local colloca-
tions.  These are same as three of the four knowl-
edge sources used in (Lee and Ng, 2002). Their 
fourth knowledge source (named syntactic rela-
tions) was not used in our work. 
5 Stopping Conditions 
In active learning algorithm, defining the stopping 
condition for active learning is a critical problem, 
because it is almost impossible for the human an-
notator to label all unlabeled samples. This is a 
problem of estimation of classifier effectiveness 
(Lewis and Gale 1994). In fact, it is difficult to 
know when the classifier reaches maximum 
effectiveness. In previous work some researchers 
used a simple stopping condition when the training 
set reached a predefined desired size. It is almost 
impossible to predefine an appropriate size of 
desirable training data for inducing the most 
effective classifier.  
To solve the problem, we consider the problem 
of estimating  classifier effectiveness as the 
problem of confidence estimation of classifier on 
the remaining unlabeled samples. Concretely, if we 
find that the current classifier already has 
acceptably strong confidence on its classification 
results for all remained unlabeled data, we assume 
the current training data is sufficient to train the 
classifier with maximum effectiveness. In other 
words, if a classifier induced from the current 
training data has strong classification confidence 
on an unlabeled example, we could consider it as a 
redundant example. 
Based on above analyses, we adopt here two 
stopping conditions for active learning: 
? Max-confidence: This strategy is based on 
uncertainty measurement, considering whether 
the entropy of each selected unlabeled example 
is less than a very small predefined threshold 
close to zero, such as 0.001.  
? Min-error: This strategy is based on feedback 
from the oracle when the active learner asks 
for true labels for selected unlabeled examples, 
considering whether the current trained 
classifier could correctly predict the labels or 
the accuracy performance of predictions on 
786
selected unlabeled examples is already larger 
than a predefined accuacy threshold. 
Once max-confidence and min-error conditions 
are met, the current classifier is assumed to have 
strong enough confidence on the classification 
results of all remained unlabeled data. 
6 Evaluation 
6.1 Data 
The data used for our comparison experiments 
were developed as part of the OntoNotes project 
(Hovy et al, 2006), which uses the WSJ part of the 
Penn Treebank (Marcus et al, 1993). The senses 
of noun words occurring in OntoNotes are linked 
to the Omega ontology. In OntoNotes, at least two 
humans manually annotate the coarse-grained 
senses of selected nouns and verbs in their natural 
sentence context. To date, OntoNotes has 
annotated several tens of thousands of examples, 
covering several hundred nouns and verbs, with an 
inter-annotator agreement rate of at least 90%.   
Those 38 random chosen ambiguous nouns used 
in all following experiments are shown in Table 1. 
It is apparent that the sense distributions of most 
nouns are very skewed (frequencies shown in the 
table, separated by /). 
Words sense distribution  words sense distribution 
Rate 1025/182 president 936/157/17 
People 815/67/7/5 part 456/102/75/16 
Point 471/88/37/19/9/6 director 517/23 
Revenue 517/23 bill 348/130/40 
Future 413/82/23 order 354/61/54/6/6 
Plant 376/51 board 369/15 
Today 238/149 policy 308/74 
Capital 325/21/8 term 147/137/52/13 
management 210/130 move 302/13/5 
Position 97/75/67/61/10/7 amount 236/57/16 
Home 267/17/16 power 154/134/15 
Leader 244/38 return 191/35/29/12/9 
administration 266/11 payment 201/69 
Account 233/18/13 control 90/66/64/21/12/5 
Lot 221/20 activity 218/23 
Drug 160/74 building 177/48/5 
Estate 214/11 house 112/71/25 
development 165/46/6 network 127/53/29 
Strategy 198/11 place 69/63/50/18/5 
Table 1. Data set used in experiments 
6.2 Results 
In the following active learning comparison 
experiments, we tested with five resampling 
methods including random sampling (Random), 
uncertainty sampling (Ordinary), under-sampling, 
over-sampling, and BootOS. The 1-NN technique 
was used for bootstrap-based resampling of 
BootOS in our experiments. A 5 by 5-fold cross-
validation was performed on each noun?s data.  
We used 20% randomly chosen data for held-out 
evaluation  and the other 80% as the pool of 
unlabeled data for each round of the active 
learning.  For all words, we started with a 
randomly chosen initial training set of 10 
examples, and we made 10 queries after each 
learning iteration.  
In the evaluation, average accuracy and recall 
are used as measures of performances for each 
active learning method. Note that the macro-
average way is adopted for recall evaluation in 
each noun WSD task. The accuracy measure 
indicates the percentage of testing instances 
correctly identified by the system. The macro-
average recall measure indicates how well the 
system performs on each sense.   
 
Experiment 1: Performance comparison ex-
periments on active learning 
 0.78
 0.8
 0.82
 0.84
 0.86
 0.88
 10  30  50  70  90  110  130  150  170  190  210  230  250  270  290
A
ve
ra
ge
 A
cc
ur
ac
y
Number of learned samples
Active learning for WSD
Random
Ordinary
Under-sampling
Over-sampling
BootOS
 
Figure 3. Average accuracy performance com-
parison experiments 
 0.36
 0.38
 0.4
 0.42
 0.44
 0.46
 0.48
 0.5
 0.52
 10  30  50  70  90  110  130  150  170  190  210  230  250  270  290
A
ve
ra
ge
 R
ec
al
l
Number of learned samples
Active learning for WSD
Random
Ordinary
Under-sampling
Over-sampling
BootOS
 
Figure 4. Average recall performance comparison 
experiments 
787
As shown in Fig. 3 and Fig. 4, when the number of 
learned samples for each noun is smaller than 120, 
the BootOS has the best performance, followed by 
over-sampling and ordinary method. As the num-
ber of learned samples increases, ordinary, over-
sampling and BootOS have similar performances 
on accuracy and recall. Our experiments also ex-
hibit that random sampling method is the worst on 
both accuracy and recall.  
Previous work (Estabrooks et al, 2004) reported 
that under-sampling of the majority class (pre-
dominant sense) has been proposed as a good 
means of increasing the sensitivity of a classifier to 
the minority class (infrequent sense). However, in 
our active learning experiments, under-sampling is 
apparently worse than ordinary, over-sampling and 
our BootOS. The reason is that in highly imbal-
anced data, too many useful training samples of 
majority class are discarded in under-sampling, 
causing the performance of active learning to de-
grade.  
 
Experiment 2: Effectiveness of learning in-
stances for infrequent senses 
It is important to enrich the corpora by learning 
more instances for infrequent senses using active 
learning with less human labeling. This procedure 
not only makes the corpora ?richer?, but also 
alleviates  the domain dependence problem faced 
by corpus-based supervised approaches to WSD.  
The objective of this experiment is to evaluate 
the performance of active learning in learning 
samples of infrequent senses from an unlabeled 
corpus. Due to highly skewed word sense 
distributions in our data set, we consider all senses 
other than the predominant sense as infrequent 
senses in this experiment.  
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300pe
rc
en
ta
ge
 o
f l
ea
rn
ed
 in
st
an
ce
s 
fo
r 
in
fr
eq
ue
nt
 s
en
se
s
Number of learned samples
Active learning for WSD
Random
Ordinary
Under-sampling
Over-sampling
BootOS
 
Figure 5. Comparison experiments on learning in-
stances for infrequent senses 
Fig. 5 shows that random sampling is the worst 
in active learning for infrequent senses. The reason 
is very obvious: the sense distribution of the 
learned sample set by random sampling is almost 
identical to that of the original data set. 
 Under-sampling is apparently worse than ordi-
nary active learning, over-sampling and BootOS 
methods. When the number of learned samples for 
each noun is smaller than 80, BootOS achieves 
slight better performance than ordinary active 
learning and over-sampling.  
When the number of learned samples is larger 
than 80 and smaller than 160, these three methods 
exhibit similar performance. As the number of it-
erations increases, ordinary active learning is 
slightly better than over-sampling and BootOS. In 
fact, after the 16th iteration (10 samples chosen in 
each iteration), results indicate that most instances 
for infrequent senses have been learned.  
 
Experiment 3: Effectiveness of Stopping Condi-
tions for active learning 
To evaluate the effectiveness of two strategies 
max-confidence and min-error as stopping condi-
tions of active learning, we first construct an ideal 
stopping condition when the classifier could reach 
the highest accuracy performance at the first time 
in the procedure of active learning. When the ideal 
stopping condition is met, it means that the current 
classifier has reached maximum effectiveness. In 
practice, it is impossible to exactly know when the 
ideal stopping condition is met before all unlabeled 
data are labeled by a human annotator. We only 
use this ideal method in our comparison experi-
ments to analyze the effectiveness of our two pro-
posed stopping conditions. 
For general purpose, we focus on the ordinary 
active learning to design the basic system, and to 
evaluate the effectiveness of three stop conditions. 
In the following experiments, the entropy threshold 
used in max-confidence strategy is set to 0.001, and 
the accuracy threshold used in min-error strategy 
is set to 0.9.   
In Table 2, the column ?Size? stands for the size 
of unlabeled data set of corresponding noun word 
used in active learning. There are two columns for 
each stopping condition: the left column ?num? 
presents number of learned instances and the right 
column ?%? presents its percentage over all data 
when the corresponding stopping condition is met. 
 
788
Ideal Max-confidence Min-error Words Size 
num % num % num % 
Rate 966 200 .23 410 .41 290 .29 
People 715 140 .20 290 .41 200 .28 
Point 504 90 .18 220 .44 120 .24 
Revenue 432 70 .16 110 .25 80 .19 
Future 414 120 .29 140 .34 60 .14 
Plant 342 210 .61 180 .53 110 .32 
Today 382 250 .65 240 .63 230 .60 
Capital 283 70 .25 180 .64 90 .32 
Management 272 200 .74 210 .77 210 .77 
Position 254 210 .83 230 .91 220 .87 
Home 240 60 .25 160 .67 60 .25 
Leader 226 60 .27 120 .53 70 .31 
administration 222 30 .14 90 .41 50 .23 
Account 211 50 .24 130 .62 70 .33 
Lot 185 30 .16 60 .32 40 .22 
Drug 187 130 .70 140 .75 120 .64 
Estate 180 20 .11 50 .28 30 .17 
Development 174 40 .23 150 .86 80 .46 
Strategy 167 10 .06 100 .60 10 .06 
President 888 120 .14 220 .25 120 .14 
Part 519 110 .21 240 .46 130 .25 
Director 432 110 .25 130 .30 90 .21 
Bill 414 120 .29 280 .68 150 .36 
Order 385 130 .34 220 .57 140 .36 
Board 307 40 .13 190 .62 40 .13 
Policy 306 90 .29 200 .65 150 .49 
Term 279 120 .43 190 .68 130 .47 
Move 256 50 .20 140 .55 50 .20 
Amount 247 210 .85 200 .81 140 .57 
Power 242 190 .78 190 .78 190 .78 
Return 221 90 .41 160 .72 100 .45 
Payment 216 120 .56 160 .74 150 .69 
Control 206 160 .78 200 .97 200 .97 
Activity 193 30 .16 130 .67 70 .36 
Building 184 90 .49 130 .71 110 .60 
House 166 100 .60 150 .90 110 .66 
Network 167 110 .66 130 .78 100 .60 
Place 164 120 .73 150 .91 120 .73 
Table 2 Effectiveness of three stopping conditions 
 
As shown in Table 2, the min-error strategy 
based on feedback of human annotator is very 
close to the ideal method. Therefore, when com-
paring to ideal stopping condition, min-error strat-
egy is a good choice as stopping condition for ac-
tive learning. It is important to note that the min-
error method does not need more additional 
computational costs, it only depends upon the 
feedback of human annotator when labeling the 
chosen unlabeled samples.   
From experimental results, we can see that max-
confidence strategy is worse than min-error 
method. However, we believe that the entropy of 
each unlabeled sample is a good signal to stop ac-
tive learning. So we suggest that there may be a 
good prediction solution in which the min-error 
strategy is used as the lower-bound of stopping 
condition, and max-confidence strategy as the up-
per-bound of stopping condition for active learning. 
7 Discussion 
As discussed above, finding more instances for 
infrequent senses at the earlier stages of active 
learning is very significant in making the corpus 
richer, meaning less effort for human labeling. In 
practice, another way to learn more instances for 
infrequent senses is to first build a training data set 
by active learning or by human efforts, and then 
build a supervised classifier to find more instances 
for infrequent sense. However, it is interesting to 
know how much initial training data is enough for 
this task, and how much human labeling efforts 
could be saved.  
From experimental results, we found that among 
these chosen unlabeled instances by active learner, 
some instances are informative samples helpful for 
improving classification performance, and other 
instances are borderline samples which are unreli-
able because even a small amount of noise can lead 
the sample to the wrong side of the decision 
boundary. The removal of these borderline samples 
might improve the performance of active learning. 
The proposed prediction solution based on max-
confidence and min-error strategies is a coarse 
framework. To predict when to stop active learning 
procedure, it is logical to consider the changes of 
accuracy performance of the classifier as a signal 
to stop the learning iteration. In other words, dur-
ing the range predicted by the proposed solution, if 
the change of accuracy performance of the learner 
(classifier) is very small, we could assume that the 
current classifier has reached maximum effective-
ness. 
8 Conclusion and Future Work 
In this paper, we consider the class imbalance 
problem in WSD tasks, and analyze the effect of 
resampling techniques including over-sampling 
and under-sampling in active learning. Experimen-
tal results show that over-sampling is a relatively 
good choice in active learning for WSD in highly 
imbalanced data. Under-sampling causes negative 
effect on active learning. A new over-sampling 
method named BootOS based on bootstrap tech-
nique is proposed to alleviate the within-class im-
balance problem of over-sampling, and works bet-
ter than ordinary over-sampling in active learning 
for WSD. It is noteworthy that none of these 
techniques require to modify the architecture or 
789
learning algorithm; therefore, they are very easy to 
use and extend to other applications. To predict 
when to stop active learning, we adopt two 
strategies including max-confidence and min-error 
as stopping conditions. According to our 
experimental results, we suggest a prediction 
solution by considering max-confidence as the 
upper bound and min-error as the lower bound of 
stopping conditions for active learning.  
In the future work, we will study how to exactly 
identify these borderline samples thus they are not 
firstly selected in active learning procedure. The 
borderline samples have the higher entropy values 
meaning least confident for the current classifier. 
The borderline instances can be detected using the 
concept of Tomek links (Tomek 1976). It is also 
worth studying cost-sensitive learning for active 
learning with imbalanced data, and using such 
techniques for WSD. 
References 
A. L. Berger, S. A. Della, and V. J  Della. 1996. A maximum 
entropy approach to natural language processing. Compu-
tational Linguistics 22(1):39?71. 
E Brill. 1992. A simple rule-based part of speech tagger. In 
the Proceedings of the Third Conference on Applied Natu-
ral Language Processing. 
Y. S. Chan and H. T. Ng. 2006. Estimating class priors in 
domain adaptation. In Proc. of ACL06.  
N. Chawla, K. Bowyer, L. Hall, W. Kegelmeyer. 2002. 
SMOTE: synthetic minority over-sampling technique. Jour-
nal of Artificial Intelligence Research, 2002(16): 321-357 
J. Chen, A. Schein, L. Ungar, M. Palmer. 2006. An empirical 
study of the behavior of active learning for word sense dis-
ambiguation. In Proc. of HLT-NAACL06 
I. Dagan, O. Glickman, A. Gliozzo, E. Marmorshtein, and C. 
Strapparava. 2006. Direct Word Sense Matching for Lexi-
cal Substitution. In Proc. of ACL'06 
H. T. Dang and M. Palmer. 2005. The Role of Semantic Roles 
in Disambiguating Verb Senses. In Proc. of ACL'05. 
A. Estabrooks, T. Jo and N. Japkowicz. 2004. A multiple re-
sampling method for learning from imbalanced data set. 
Computational Intelligence, 20(1):18-36 
Y. Hamamoto, S. Uchimura and S. Tomita. 1997. A bootstrap 
technique for nearest neighbor classifier design. IEEE 
Transactions on Pattern Analysis and Machine Intelligence, 
19(1):73-79 
V. Hoste, A. Kool, and W. Daelemans. 2001. Classifier opti-
mization and combination in the English all words task. In 
Proc. of the SENSEVAL-2 workshop 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R. 
Weischedel. 2006. Ontonotes: The 90% Solution. In Proc. 
of HLT-NAACL06. 
N. Ide and J. Veronis. 1998. Introduction to the special issue 
on word sense disambiguation: the state of the art. Compu-
tational Linguistics, 24(1):1-37 
N. Japkowicz and S. Stephen. 2002. The class imbalance 
problem: a systematic study. Intelligent Data Analysis, 
6(5):429-450 
T. Jo and N. Japkowicz. 2004. Class imbalances versus small 
disjuncts. SIGKSS Explorations, 6(1):40-49 
U. S. Kohomban and W. S. Lee. 2005. Learning Semantic 
Classes for Word Sense Disambiguation. In Proc. of 
ACL'05 
M. Kubat and S. Matwin. 1997. Addressing the curse of im-
balanced training sets: one-sided selection. In Proc. of 
ICML97 
Y.K. Lee and. H.T. Ng. 2002. An empirical evaluation of 
knowledge sources and learning algorithm for word sense 
disambiguation. In Proc. of EMNLP-2002 
D. D. Lewis and W. A. Gale. 1994. A sequential algorithm for 
training text classifiers. In Proc. of SIGIR-94 
D.D. Lewis and J. Catlett. 1994. Heterogeneous uncertainty 
sampling for supervised learning. In Proc. of ICML94 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. 
Building a large annotated corpus of English: the Penn 
Treebank. Computational Linguistics,  19(2):313-330 
A. McCallum and K. Nigram. 1998. Employing EM in pool-
based active learning for text classification. In Proc. 15th 
ICML 
D. McCarthy, R. Koeling, J. Weeds, and J. Carroll. 2004. 
Finding predominant senses in untagged text. In Proc. of 
ACL04 
I. Muslea, S. Minton, and C. A. Knoblock. 2000. Selective 
sampling with redundant views. In Proc. of National Con-
ference on Artificial Intelligence 
I. Tomek. 1976. Two modifications of CNN. IEEE Transac-
tions on Systems, Man and Cybernetics, 6(6):769-772 
G. M. Weiss. 2004. Mining with rarity ? problems and solu-
tions: a unifying framework. SIGKDD Explorations, 
6(1):7-19 
N. Xue, J. Chen and M. Palmer. 2006. Aligning Features with 
Sense Distinction Dimensions. In Proc. of ACL'06 
Z. Zhou, X. Liu. 2006. Training cost-sensitive neural networks 
with methods addressing the class imbalance problem. 
IEEE Transactions on Knowledge and Data Engineering, 
18(1):63-77 
L. Zhang, J. Zhu, and T. Yao. 2004. An evaluation of statisti-
cal spam filtering techniques. ACM Transactions on Asian 
Language Information Processing, 3(4):243?269. 
C. Zhang and T. Chen. 2002. An active learning frame-
work for content-based information retrieval. IEEE 
Transactions on Multimedia, 4(2):260-268 
790
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 837?846, Prague, June 2007. c?2007 Association for Computational Linguistics 
Extracting Data Records from Unstructured Biomedical Full Text 
Donghui Feng       Gully Burns       Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
 
 
Abstract 
In this paper, we address the problem of 
extracting data records and their attributes 
from unstructured biomedical full text. 
There has been little effort reported on this 
in the research community. We argue that 
semantics is important for record extraction 
or finer-grained language processing tasks. 
We derive a data record template including 
semantic language models from unstruc-
tured text and represent them with a dis-
course level Conditional Random Fields 
(CRF) model. We evaluate the approach 
from the perspective of Information Extrac-
tion and achieve significant improvements 
on system performance compared with 
other baseline systems. 
1 Introduction 
The discovery and extraction of specific types of 
information, and its (re)structuring and storage into 
databases, are critical tasks for data mining, 
knowledge acquisition, and information integration 
from large corpora or heterogeneous resources 
(e.g., Muslea et al, 2001; Arasu and Garcia-
Molina, 2003). For example, webpages of products 
on Amazon may contain a list of data records such 
as books, watches, and electronics. Automatic 
extraction of individual records will facilitate the 
access and management of data resources. 
Most current approaches address this problem 
for structured or semi-structured text, for instance, 
from XML format files or lists and/or tabular data 
records on webpages (e.g., Liu et al, 2003; Zhu et 
al., 2006). The techniques applied rely strongly on 
the analysis of document structure derived from 
the webpage?s html tags (e.g., the DOM tree 
model). 
Regarding unstructured text, most Information 
Extraction (IE) work has focused on named entities 
(people, organizations, places, etc.). Such IE treats 
each extracted element as a separate record. Much 
less work has focused on the case where several 
related pieces of information have to be extracted 
to jointly comprise a single data record. In this 
work, it is usually assumed that there is only one 
record for each document (e.g., Kristjannson et al, 
2004). Almost no work tries to extract multiple 
data records from a single document. Multiple data 
records can be scattered across the narrative in free 
text. The problem becomes much harder as there 
are no explicit boundaries between data records 
and no heavily indicative format features (like html 
tags) to utilize. 
With the exponential increase of unstructured 
text resources (e.g., digitalized publications, papers 
and/or technical reports), knowledge needs have 
made it a necessity to explore this problem. For 
example, biomedical papers contain numerous ex-
periments and findings. But the large volume and 
rate of publication have made it infeasible to read 
through the articles and manually identify data re-
cords and attributes. 
We present a study to extract data records and 
attributes from the biomedical research literature. 
This is part of an effort to develop a Knowledge 
Base Management System to benefit neuroscience 
research. Specifically we are interested in knowl-
edge of various aspects (attributes) of Tract-tracing 
Experiments (TTE) (data records) in neuroscience. 
The goal of TTE experiments is to chart the inter-
connectivity of the brain by injecting tracer chemi-
cals into a region of the brain and identifying cor-
responding labeled regions where the tracer is 
837
  
Figure 1. An example of data records and attributes in a research article. 
taken up and transported to (Burns et al, 2007). 
To extract data records from the research litera-
ture, we need to solve two sub-problems: discover-
ing individual attributes of records and grouping 
them into one or more individual records, each re-
cord representing one TTE experiment. Each at-
tribute may contain a list of words or phrases and 
each record may contain a list of attributes.  
Listing each sentence from top to bottom, we 
call the first problem the Horizontal Problem (HP) 
and the second the Vertical Problem (VP). Figure 
1 provides an example of a TTE research article 
with colored fragments representing attributes and 
dashed frames representing data records. For in-
stance, the third dashed frame represents one ex-
periment record having three attributes with corre-
sponding biological interpretations: ?no labeled 
cells?, ?the DCN?, and ?the contralateral AVCN?. 
We view the HP and VP problems as two se-
quential labeling problems and describe our ap-
proach using two-level Conditional Random Fields 
(CRF) (Lafferty et al, 2001) models to extract data 
records and their attributes.  
The HP problem (finding individual attribute 
values) is solved using a sentence-level CRF label-
ing model that integrates a rich set of linguistic 
features. For the VP problem, we apply a dis-
course-level CRF model to identify individual ex-
periments (data records). This model utilizes deep 
semantic knowledge from the HP results (attribute 
labels within sentences) together with semantic 
language models and achieves significant im-
provements over baseline systems.  
This paper mainly focuses on the VP problem, 
since linguistic features for the HP problem is the 
general IE topic of much past research (e.g., Peng 
and McCallum, 2004). We apply various feature 
combinations to learn the most suitable and indica-
tive linguistic features. 
The remainder of this paper is organized as fol-
lows: in the next section we discuss related work. 
Following that, we present the approach to extract 
data records in Section 3. We give extensive ex-
perimental evaluations in Section 4 and conclude 
in Section 5. 
2 Related Work 
As mentioned, data record extraction has been 
extensively studied for structured and semi-
structured resources (e.g., Muslea et al, 2001; 
Arasu and Garcia-Molina, 2003; Liu et al, 2003; 
Zhu et al, 2006). Most of those approaches rely on 
the analysis of document structure (reflected in, for 
example, html tags), from which record templates 
are derived. However, this approach does not apply 
to unstructured text. The reason lies in the 
difficulty of representing a data record template in 
free text without formatting tags and integrating it 
838
 into a learning system. We show how to address 
this problem by deriving data record templates 
through language analysis and representing them 
with a discourse level CRF model. 
Given the problem of identifying one or more 
records in free text, it is natural to turn toward text 
segmentation. The Natural Language Processing 
(NLP) community has come up with various 
solutions towards topic-based text segmentation 
(e.g., Hearst, 1994; Choi, 2000; Malioutov and 
Barzilay, 2006). Most unsupervised text 
segmentation approaches work under optimization 
criteria to maximize the intra-segment similarity 
and minimize the inter-segment similarity based on 
word distribution statistics. However, this 
approach cannot be applied directly to data record 
extraction. A careful study of our corpus shows 
that data records share many words and phrases 
and are not distinguishable based on word 
similairties. In other words, different experiments 
(records) always belong to the same topic and there 
is no way to segment them using standard topic 
segmentation techniques (even if one views the 
problem as a finer-level segmentation than 
traditional text segmentation). In addition, most 
text segmentation approaches require a 
prespecified number of segments, which in our 
domain cannot be provided. 
(Wick et al, 2006) report extracting database re-
cords by learning record field compatibility. How-
ever, in our case, the field compatibility is hard to 
distinguish even by a human expert. Cluster-based 
or pairwise field similarity measures do not apply 
to our corpora without complex knowledge reason-
ing. Most of Wick et al?s data (faculty and stu-
dent?s homepages) contains one record. 
In addition, as explained below, we have found 
that surface word statistics alone are not sufficient 
to derive data record templates for extraction. 
Some (limited) form of semantic understanding of 
text is necessary. We therefore first perform some  
sentence level extraction (following the HP 
problem) and then integrate semantic labels and 
semantic language model features into a discourse 
level CRF model to represent the template for 
extracting data records in the future. 
Recently an increasing number of research ef-
forts on text mining and IE have used CRF models 
(e.g., Peng and McCallum, 2004). The CRF model 
provides a compact way to integrate different types 
of features when sequential labeling is important. 
Recent work includes improved model variants 
(e.g., Jiao et al, 2006; Okanohara et al, 2006) and 
applications such as web data extraction (Pinto et 
al., 2003), scientific citation extraction (Peng and 
McCallum, 2004), and word alignment (Blunsom 
and Cohn, 2006). But none of them have used 
CRFs for discourse level data record extraction. 
We use a CRF model to represent a data record 
template and integrate various knowledge as CRF 
features. Instead of traditional work on the sen-
tence level, our focus here is on the discourse level. 
As this has not been carefully explored, we ex-
periment with various selected features. 
For the biomedical domain, our work will facili-
tate biomedical research by supporting the con-
struction of Knowledge Base Management Sys-
tems (e.g., Stephan et al, 2001; Hahn et al, 2002; 
Burns and Cheng, 2006). Unlike the well-studied 
problem of relation extraction from biomedical 
text, our work focuses on grouping extracted at-
tributes across sentences into meaningful data re-
cords. TTE experiment is only one of many ex-
perimental types in biology. Our work can be gen-
eralized to many different types of data records to 
facilitate biology research. 
In the next section, we present our approach to 
extracting data records. 
3 Extracting Data Records 
Inspired by the idea of Noun Phrase (NP) chunking 
in a single sentence, we view the data records 
extraction problem as discourse chunking from a 
sequence of sentences using a sequential labeling 
CRF model. 
3.1 Sequential Labeling Model: CRF 
The CRF model addresses the problem of labeling 
sequential tokens while relaxing the strong 
independence assumptions of Hidden Markov 
Models (HMMs) and avoiding the presence of 
label bias from having few successor states. For 
each current state, we obtain the conditional 
probability of its output states given previously 
assigned values of input states. For most language 
processing tasks, this model is simply a linear-
chain Markov Random Fields model. 
In typical labeling processes using CRFs each 
token is viewed as a labeling unit. For our prob-
lem, we process each input document 
),...,,( 21 nsssD =  as a sequence of individual sen-
839
 tences, with a corresponding labeling sequence of 
labels, ),...,,( 21 nlllL = , so that each sentence corre-
sponds to only one label. In our problem, each data 
record corresponds to a distinct TTE experiment. 
Similar to NP chunking, we define three labels for 
sentences, ?B_REC? (beginning of record), 
?I_REC? (inside record), and ?O? (other). The de-
fault label ?O? indicates that this sentence is be-
yond our concern. 
The CRF model is trained to maximize the 
probability of )|( DLP , that is, given an input 
document D, we find the most probable labeling 
sequence L. The decision rule for this procedure is: 
)|(maxarg? DLPL
L
=                                        (1) 
A CRF model of the two sequences is character-
ized by a set of feature functions kf and their corre-
sponding weights k? . As in Markov fields, the 
conditional probability )|( DLP  can be computed 
using Equation 2. 
??
???
???=
= ?
T
t k
ttkk
S
tDllf
Z
DLP
1
1 ),,,(*exp
1
)|( ?        (2) 
where ),,,( 1 tDllf ttk ? is a feature function, represent-
ing either the state transition feature ),,( 1 Dllf ttk ?  or 
the feature of output state ),( Dlf tk given the input 
sequence. All these feature functions are user-
defined boolean functions. 
CRF works under the framework of supervised 
learning, which requires a pre-labeled training set 
to learn and optimize system parameters to maxi-
mize the probability or its log format. Equipped 
with this model, we investigate how to apply it and 
prepare features accordingly. 
3.2 Feature Preparation 
The CRF model provides a compact, unified 
framework to integrate features. However, unlike 
sentence-level processing, where features are very 
intuitive and circumscribed, it is not obvious what 
features are most indicative for our problem. We 
therefore explore three categories of features for 
discourse level chunking. 
3.2.1 Semantic Attribute Labels 
Most text segmentation approaches compute 
surface word similarity scores in given corpora 
without semantic analysis. However, in our case, 
data records have very similar characteristics and 
share most of the words. They are not 
distinguishable just from an analysis of surface 
word statistics. We have to understand the 
semantics before we can make decisions about data 
record extraction.  
In our case, we care about the four types of at-
tributes of each data record (one TTE experiment). 
Table 1 gives the definitions of the four attributes 
for each data record. 
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription 
a description of labeling, in-
cluding label density or label 
type. 
Table 1. Attributes of data records (a TTE experiment). 
To obtain this semantic attributes information of 
individual sentences (the HP problem), we first 
apply another sentence-level CRF model to label 
each sentence. We consider five categories of fea-
tures based on language analysis. Table 2 shows 
the features for each category. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a den-
sity term? 
Lexicon 
Knowledge 
LABELING_TYPE Does word de-
note a labeling 
type? 
Surface 
Word 
Word Current word 
Context    
Window 
CONT-INJ If current word 
is within a win-
dow of injection 
context 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of 
the word if dif-
ferent 
Gov-verb The governing 
verb 
Subject The sentence 
subject  
Dependency 
Features 
Object The sentence 
object 
Table 2. The features for labeling words. 
840
 a. Lexicon knowledge. We used names of brain 
structures taken from brain atlases (Swanson, 
2004), standard terms to denote neuro-
anatomical topographical relationships (e.g., 
?rostral?), the name or abbreviation of the 
tracer chemical used (e.g., ?PHAL?), and 
commonsense descriptions for descriptions of 
the labeling (e.g., ?dense?, ?light?).  
b. Surface and window word. The current 
word and the words around are important in-
dicators of the most probable label. 
c. Context window. The TTE is a description of 
the inject-label-findings process. Whenever a 
word having a root form of ?injection? or 
?deposit? appears, we generate a context 
window and all the words falling into this 
window are assigned a feature of ?CONT-
INJ?.  
d. Dependency features. We apply a depend-
ency parser MiniPar (Lin, 1998) to parse each 
sentence, and then derive four types of fea-
tures from the parsing result. These features 
are (a) root form of every word, (b) the sub-
ject within the sentence, (c) the object within 
the sentence, and (d) the governing verbs. 
The labeling system assigns a label for every to-
ken in each sentence. We achieved the best per-
formance with an F-score of 0.79 (based on a pre-
cision of 0.80 and a recall of 0.78). This is not the 
focus of this paper. Please refer to our previous 
work (Burns et al, 2007) for details. 
 
 
 
 
 
 
 
Figure 2. An example of semantic attribute labels. 
With the sentence-level understanding of each 
sentence, we obtain the semantic attribute labels 
for the data records. Figure 2 gives an example 
sentence with semantic attribute labels. Here 
<tracerChemical>, <labelingLocation>, and <la-
belingDescription> are recognized by the system, 
and the attribute names will be used as features for 
this sentence. 
3.2.2 Semantic Language Model 
Since text narratives might adhere to logical ways 
of expressing facts, language models for each sen-
tence will also provide good features to extract 
data records. However, in biomedical research arti-
cles many of the technical words/phrases used in 
the narrative are repeated across experiments, mak-
ing the surface word language model of little use in 
deriving generalized data record templates. Con-
sidering this, we replace in each sentence the la-
beled fragments with their attribute labels and then 
derive semantic language models from that format. 
By ?semantic language model? we therefore mean 
a combination of semantic labels and surface 
words.  
For example, in the sentence shown in Figure 2, 
we have the semantic language model trigrams 
location-of-<tracerChemical>, sites-in-
<injectionLocation>, and <labelingDescription>-
followed-the. In addition, we also query WordNet 
for the root form of each word to generalize the 
semantic language models. This for example pro-
duces the semantic language model trigrams site-
in-<injectionLocation> and <labelingDescription>-
follow-the. 
We believe the collected semantic language 
models represent an inherent structure of unstruc-
tured data records. By integrating them as features 
with a CRF model, we expect to represent data re-
cord templates and use the learned model to extract 
new data records.  
However, it is not clear what semantic language 
models are most indicative and useful. A bag-of-
words (language models) approach may bring 
much noise in. We show below a comparison of 
regular language models and semantic language 
models in evaluations.  
3.2.3 Layout and Word Heuristics 
The previous two categories of features come from 
the discovery of semantic components of sentences 
and their narrative form word analysis. When in-
terviewing the neuroscience expert annotator, we 
learned that some layout and word level heuristics 
may also help to delineate individual data records. 
Table 3 gives the two types of heuristic features. 
When a sentence contains heuristic words, it 
will be assigned to a word heuristic feature. If the 
sentence is at the boundary of a paragraph, it will 
be assigned a layout heuristic feature, namely the 
first or the last sentence in the paragraph.  
<SENT FILE="1995-360-213-ns.xml" INDEX= "63"> 
Regardless of the precise location of <tracerChemical> 
PHAL </tracerChemical> injection sites in <injectionLo-
cation> the MEA </injectionLocation> , <labelingDe-
scription> labeled axons </labelingDescription> followed 
the same basic routes . 
</SENT> 
841
 Name Feature Descrip-tion 
EXP_B_WORD 
INJECT 
CASE 
EXPERIMENT 
APPLICATION 
DEPOSIT 
PLACEMENT 
INTRODUCTION 
Heuristic 
words for 
beginning 
of an ex-
periment 
descrip-
tion 
POS_IN_PARA FIRST_IN_PARA 
LAST_IN_PARA 
Position of 
the sen-
tence in 
the para-
graph 
Table 3. The heuristic features. 
4 Empirical Evaluation 
To evaluate the effectiveness and performance of 
our technique, we conducted extensive experi-
ments to measure the data record extraction ap-
proach. 
4.1 Experimental Setup 
We used the machine learning package MALLET 
(McCallum, 2002) to conduct the CRF model 
training and labeling. 
We have obtained the digital publications of 
9474 Journal of Comparative Neurology (JCN)1 
articles from 1982 to 2005. We have converted the 
PDF format into plain text, maintaining paragraph 
breaks (some errors still occur though).  A simple 
heuristic based approach identifies semantic sec-
tions of the paper (e.g, Introduction, Results, Dis-
cussion). As most experimental descriptions appear 
in the Results section, we only process the Results 
section. A neuroscience expert manually annotated 
the data records in the Results section of 58 re-
search articles. The total number of sentences in 
the Results section of the 58 files is 6630 (averag-
ing 114.3 sentences per article). 
 Training Set Testing Set 
Docs 39 19 
Data Records 249 133 
Table 4. Experiment configuration. 
We randomly divided this material into training 
and testing sets under a 2:1 ratio, giving 39 docu-
ments in the training set and 19 in the testing set. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
Table 4 gives the numbers of documents and data 
records in the training and the testing set. 
4.2 Evaluation Metrics 
To evaluate data record extraction, we notice it is 
not fair to strictly evaluate the boundaries of data 
records because this does not penalize the near-
miss and false positive of data records in a reason-
able way; sentences near a boundary that contain 
no relevant record information can be included or 
omitted without affecting the results. Hence the 
standard Pk (Beeferman et al, 1997) and WinDiff 
(Pevzner and Hearst, 2002) measures for text seg-
mentation are not so suitable for our task. 
As we are concerned with the usefulness of 
knowledge in extracted data records, we instead 
evaluate from the perspective of IE. We measure 
system performance on the quality of the extracted 
data records. For each extracted data record, it will 
be aligned to one of the data records in the gold 
standard using the ?dominance rule? (if the data 
record can be aligned to multiple records in the 
gold standard, it will be aligned to the one with 
highest overlap). Then we evaluate the precision, 
recall, and F1 scores of extracted units of the data 
record. The units are the attributes in data records. 
system by the units extracted  theof #
unitscorrect   # of
precision =   (3) 
standard gold in the units  theof #
 unitscorrect   # of
recall =                (4) 
ecallrprecision
recall*precision
F +=
*2
1                                    (5) 
These measures provide an indication of the 
completeness and correctness of each extracted 
record (experiment). We also measure the number 
of distinct records extracted, compared with the 
gold standard as appearing in the document. 
4.3 Experiment Results 
To fully compare the effectiveness of our semantic 
analysis functionality, we evaluated system per-
formance for all the following systems:  
TextTiling (TT): To compare with text segmen-
tation techniques, we use TextTiling (Hearst, 1994) 
with default parameters as the first baseline sys-
tem. 
Random Guess (RG): In order to demonstrate 
the data balance of all the possible labels in the 
testing set, we also use another baseline system 
with random decisions for each sentence.  
842
 Domain Heuristics (DH): In a regular TTE ex-
periment, only one tracer chemical will typically 
be used. Given this heuristic, we assume each data 
record contains one tracer chemical. In this system, 
we first locate sentences with identified trace 
chemicals, and then we greedily expand backward 
and forward until another new tracer chemical ap-
pears or no other attribute is included. 
Surface Text (ST): To measure the effective-
ness of the semantic analysis (attribute labels and 
semantic language models), the ST system utilizes 
only standard surface word language models and 
heuristic features. 
Semantic Analysis (SEM): The SEM system 
uses all the semantic features available (including 
identified attributes and semantic language models) 
and two heuristic features. 
Table 5 shows the final performance of these 
different systems. The second column provides the 
numbers of extracted data records. In this task, a 
larger number does not necessarily mean a better 
system, as a system might produce too many false 
positives. The remaining three columns represent 
the precision, recall, and F1 scores, averaged over 
all data records. With our approach, the system 
performance is significantly improved compared 
with other systems. System TT fails in this task as 
it only outputs the full document as one single re-
cord. 
 # of     
Records 
Prec. Rec. F1 
TT 19 0.3861 1.0 0.5571 
RG 758 0.6331 0.0913 0.1595 
DH 162 0.6703 0.4902 0.5663 
ST 82 0.8182 0.8339 0.8260 
SEM 72 0.8505 0.9258 0.8865 
Table 5. System performance. 
To investigate how plain text language models 
and semantic language models affect system per-
formance, we also experimented with all the lan-
guage models. Table 6 shows comparisons of three 
types of language models. Systems with semantic 
analysis always work better than those with only 
surface text analysis. Without semantic analysis, 
unigram features work better than bigram and tri-
gram features. This matches our intuition: without 
generalizing to semantic language models, higher 
order language models will be relatively sparse and 
contain much noise. However, when taking into 
account the semantic features, we found that bi-
gram and trigram semantic language model fea-
tures outperformed unigrams. They are especially 
important in boosting the recall scores as they cap-
ture more generalized information when derived. 
Unigram (%) Bigram (%) Trigram (%)  
Prec/Rec/F1 Prec/Rec/F1 Prec/Rec/F1 
ST 81.8/83.4/82.6 69.1/88.4/77.6 57.9/88.8/70.1 
SEM 85.1/86.6/85.6 85.1/92.6/88.7 82.2/92.7/87.1 
Table 6. Language model comparisons. 
As an example, Table 7 gives a list of high qual-
ity bigram semantic language models ranked by 
their information gains based on the training data. 
through_<labelingLocation> rat_no 
<labelingDescription>_be of_<tracerChemical> 
<labelingLocation>_( <tracerChemical>_be 
<tracerChemical>_injection be_inject 
into_<injectionLocation> be_center 
<labelingDescription>_from inject_with 
<tracerChemical>_in injection_of 
in_<labelingLocation> in_experiment 
Table 7. An example list of top-ranked bigrams. 
The main difficulty for data record extraction 
from unstructured text lies in deriving and repre-
senting a template for future extraction. We actu-
ally take advantage of CRF and represent the tem-
plate with a CRF model.  
Each data record is measured with precision, re-
call, and F1 scores. Figure 3 depicts the distribu-
tion of extracted data records according to these 
measures in the best system. 
Distribution
0
5
10
15
20
25
30
35
40
45
50
55
60
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Performance
# 
of
 e
xt
ra
ct
ed
 r
ec
or
ds
Prec
Rec
F1
 
Figure 3. Data records performance distribution. 
The results are encouraging, especially given the 
complexity and flexibility of data record descrip-
tions in the unstructured text. In Figure 3, Axis X 
843
 represents the value interval for precision, recall, 
and F1, and Axis Y represents the number of ex-
tracted records with their corresponding values. 
For example, 57 records have recall scores falling 
into [0.9, 1.0].  
Figure 4 gives an example alignment between 
system result and the gold standard. Each record is 
represented by a range of sentences. The numbers 
following each record in the system result are indi-
vidual data record?s precision and recall scores. 
          System                                   Gold 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 4. An example of record extraction in one doc. 
This is a real example from the testing set. For 
records R1, R3, and R6, the system can extract the 
exact sentences contained. For record R2 and R5, 
although they do not exactly match at the sentence 
level, the extracted record contains the entire re-
quired set of attributes as in the gold standard.  
4.4 Error Analysis and Discussion 
When we investigated the errors, we found that 
sometimes the extracted data records combined 
two or more smaller gold standard records, or vice 
versa. As shown in Figure 4, extracted records R4 
and R7 are both combinations of records in the 
gold standard. This is partially due to the granular-
ity definition problem. Authors may mention sev-
eral approaches/symptoms to one type of experi-
ment for a single purpose. In this case, it is almost 
infeasible to have annotators strictly agree on 
granularity and thus to teach the system to acquire 
this knowledge. For example, in the gold standard, 
the annotator annotated three successive sentences 
as three separate records but the system output 
those as only one data record. In this extreme case, 
it is too hard to expect the system to perform well. 
In our approach, the semantic attribute labels 
and semantic language models require the result of 
the initial sentence-level labeling, which has an F-
score of 0.79. The error may propagate into the 
data record extraction procedure and lower overall 
system performance. 
In our current experiments, we also assume all 
the attributes within one segment belong to one 
record. However, the situation of embedded data 
records will make this problem harder. For exam-
ple, authors sometimes compare the current ex-
periment with other approaches in referenced pa-
pers. In this case, those attributes should be ex-
cluded from the records. We need to invent rules or 
constraints to filter them out. When such reference 
occurs at experiment boundaries, it brings higher 
risk for correct results.  
It is a very hard problem to extract from unstruc-
tured text neat structured records. The annotators 
sometimes employ background knowledge or rea-
soning when performing manual extraction; such 
knowledge cannot today be easily modeled and 
integrated into learning systems.  
In our study, we also compared some feature se-
lection approaches. Similar to (Yang and Pedersen, 
1997), we tried Feature Instance Frequency, Mu-
tual Information, Information Gain, and CHI-
square test. But we eventually found that the sys-
tem including all the features worked best, and 
with all the other configurations unchanged, fea-
ture instance frequency worked at almost the same 
level as other complex measures such as mutual 
information and information gain.  
5 Conclusion and Future Work 
In this paper, we explored the problem of extract-
ing data records from unstructured text. The lack 
of structure makes it difficult to derive meaningful 
objects and their values without resorting to deeper 
language analysis techniques. We derived indica-
tive linguistic features to represent data record 
templates in free text, using a two-pass approach in 
which the second pass used the IE labels derived 
from the first to compose attributes into coherent 
data records. We evaluated the results from an IE 
perspective and reported potential problems of er-
ror generation. 
? 
R1:S12~S29 (1.0/1.0) 
? 
R2: S31~S41 (1.0/1.0) 
 
R3: S42~S52 (1.0/1.0) 
? 
R4: S56~S73 
(0.517/1.0) 
? 
R5: S75~S88 (1.0/1.0) 
? 
R6: S91~S106(1.0/1.0) 
? 
R7: S108~S118 
(0.523/1.0)  
? 
? 
R1': S12~S29 
? 
R2': S31~S40  
? 
R3': S42~S52 
? 
R4': S56~S63 
? 
R5': S65~S73 
R6': S74~S88 
.. 
R7': S91~S106 
? 
R8': S108~S114 
R9': S115~S118 
? 
844
 For the future, we plan to explore additional fea-
ture types and feature selection strategies to deter-
mine what is ?good? for unstructured record tem-
plates to improve our results. More effort will also 
be put into the sentence-level analysis to reduce 
error propagations. In addition, ontology based 
knowledge inference strategies might be useful to 
validate attributes in single record and in turn help 
data record extraction. The last thing under our 
direction is to explore new models if applicable.  
We hope this thought-provoking problem will 
attract more attention from the community. In the 
future, we plan to make our corpus available to the 
community. The solution to this problem will 
highly affect the access of knowledge in large scale 
unstructured text corpora. 
Acknowledgements 
The work was supported in part by an ISI seed 
funding, and in part by a grant from the National 
Library of Medicine (RO1 LM07061). The authors 
want to thank Feng Pan for his helpful suggestions 
with the manuscript. We would also like to thank 
the anonymous reviewers for their valuable com-
ments. 
References 
Arasu, A., and Garcia-Molina, H. 2003. Extracting 
structured data from web pages. In Proc. of SIMOD-
2003.  
Beeferman, D., Berger, A., and Lafferty, J. 1997. Text 
segmentation using exponential models. In Proc. of 
EMNLP-1997.  
Blunsom, P. and Cohn, T. 2006. Discriminative word 
alignment with conditional random fields. In Proc. of 
ACL-2006.  
Brazma, A., et al, 2001. Minimum information about a 
microarray experiment (MIAME)-toward standards 
for microarray data. Nat Genet, 29(4): p. 365-71.  
Burns, G.A. and Cheng, W.-C. 2006. Tools for knowl-
edge acquisition within the NeuroScholar system and 
their application to anatomical tract-tracing data. In 
Journal of Biomedical Discovery and Collaboration.  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent 
Approaches to Mining the Primary Research Litera-
ture: Techniques, Systems, and Examples. Book 
Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Choi, F. Y. Y. 2000. Advances in domain independent 
linear text segmentation. In Proc. of NAACL-2000.  
Hahn, U., Romacher, M., and Schulz, S. 2002. Creating 
knowledge repositories from biomedical reports the 
MEDSYNDIKATE text mining system. In Proc. of 
PSB-2002. 
Hearst, M. 1994. Multi-paragraph segmentation of ex-
pository text. In Proc. of ACL-1994.  
Jiao, F., Wang, S., Lee, C., Greiner, R., and 
Schuurmans, D. 2006. Semi-supervised conditional 
random fields for improved sequence segmentation 
and labeling. In Proc. of ACL-2006.  
Kristjannson, T., Culotta, A. Viola, P., and McCallum, 
2004. A. Interactive information extraction with con-
strained conditional random fields. In Proc. of AAAI-
2004. 
Lafferty, J., McCallum, A. and Pereira, F. 2001 Condi-
tional Random Fields: probabilistic models for seg-
menting and labeling Sequence Data. In Proc. of 
ICML-2001. 
Lin, D. 1998. Dependency-based evaluation of MINI-
PAR. In Proc. of Workshop on the Evaluation of 
Parsing Systems.  
Liu, B., Grossman, R., and Zhai, Y. 2003. Mining data 
records in web pages. In Proc. of SIGKDD-2003.  
Malioutov, I. and Barzilay, R. 2006. Minimum cut 
model for spoken lecture segmentation. In Proc. of 
ACL-2006.  
McCallum, A.K. 2002. MALLET: A Machine Learning 
for Language Toolkit. http://mallet.cs.umass.edu. 
Muslea, I., Minton, S., and Knoblock, C.A. 2001. 
Hierarchical wrapper induction for semistructured 
information sources. Autonomous Agents and Multi-
Agent Systems 4:93-114. 
Okanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 
2006. Improving the scalability of semi-markov con-
ditional random fields for named entity recognition. 
In Proc. of ACL-2006.  
Peng, F. and McCallum, A. 2004. Accurate information 
extraction from research papers using conditional 
random fields. In Proc. of HLT-NAACL-2004.  
Pevzner, L., and Hearst, M. 2002. A Critique and Im-
provement of an Evaluation Metric for Text Segmen-
tation. Computational Linguistics. 
Pinto, D., A. McCallum, X. Wei, and W.B. Croft. 2003. 
Table Extraction Using Conditional Random Fields. 
In Proc. of SIGIR-2003.  
845
 Stephan, K.E. et al, 2001. Advanced database method-
ology for the Collation of Connectivity data on the 
Macaque brain (CoCoMac). Philos Trans R Soc Lond 
B Biol Sci, 356(1412).  
Swanson, L.W. 2004. Brain Maps: Structure of the Rat 
Brain. 3rd edition, Elsevier Academic Press.  
Wick, M., Culotta, A., and McCallum, A. 2006. Learn-
ing field compatibilities to extract database records 
from unstructured text. In Proc. of EMNLP-2006. 
Yang, Y., and Pedersen, J. 1997. A comparative study 
on feature selection in text categorization. In Proc. of 
ICML-1997, pp. 412-420.  
Zhu, J., Nie, Z., Wen, J., Zhang, B., and Ma, W. 2006. 
Simultaneous record detection and attribute labeling 
in web data extraction. In Proc. of KDD-2006. 
846
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 1056?1064, Prague, June 2007. c?2007 Association for Computational Linguistics
Crystal: Analyzing Predictive Opinions on the Web 
 
 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
{skim,hovy}@ISI.EDU 
 
  
 
Abstract 
In this paper, we present an election predic-
tion system (Crystal) based on web users? 
opinions posted on an election prediction 
website. Given a prediction message, Crys-
tal first identifies which party the message 
predicts to win and then aggregates predic-
tion analysis results of a large amount of 
opinions to project the election results. We 
collect past election prediction messages 
from the Web and automatically build a 
gold standard. We focus on capturing lexi-
cal patterns that people frequently use 
when they express their predictive opinions 
about a coming election. To predict elec-
tion results, we apply SVM-based super-
vised learning. To improve performance, 
we propose a novel technique which gener-
alizes n-gram feature patterns. Experimen-
tal results show that Crystal significantly 
outperforms several baselines as well as a 
non-generalized n-gram approach. Crystal 
predicts future elections with 81.68% accu-
racy. 
1 Introduction 
As a growing number of people use the Web as a 
medium for expressing their opinions, the Web is 
becoming a rich source of various opinions in the 
form of product reviews, travel advice, social issue 
discussions, consumer complaints, stock market 
predictions, real estate market predictions, etc. 
At least two categories of opinions can be iden-
tified. One consists of opinions such as ?I 
like/dislike it?, and the other consists of opinions 
like ?It is likely/unlikely to happen.? We call the 
first category Judgment Opinions and the second 
(those discussing the future) Predictive Opinions. 
Judgment opinions express positive or negative 
sentiment about a topic such as, for example, re-
views about cameras, movies, books, or hotels, and 
discussions about topics like abortion and war. In 
contrast, predictive opinions express a person's 
opinion about the future of a topic or event such as 
the housing market, a popular sports match, and 
national election, based on his or her belief and 
knowledge. 
Due to the different nature of these two catego-
ries of opinion, each has different valences. Judg-
ment opinions have core valences of positive and 
negative. For example, ?liking a product? and 
?supporting abortion? have the valence ?positive? 
toward each topic (namely ?a product? and ?abor-
tion?). Predictive opinions have the core valence of 
likely or unlikely predicated on the event. For ex-
ample, a sentence ?Housing prices will go down 
soon? carries the valence of ?likely? for the event 
of ?housing prices go down?.  
The two types of opinions can co-appear. The 
sentence ?I like Democrats but I think they are not 
likely to win considering the war issue? contains 
both types of opinion: ?positive? valence towards 
Democrats and ?unlikely? valence towards the 
event of ?Democrats wins?. In order to accurately 
identify and analyze each type of opinion, different 
approaches are desirable. 
Note that our work is different from predictive 
data mining which models a data mining system 
using statistical approaches in order to forecast the 
future or trace a pattern of interest (Rickel and Por-
ter, 1997; Rodionov and Martin, 1996). Example 
domains of predictive data mining include earth-
quake prediction, air temperature prediction, for-
eign exchange prediction, and energy price predic-
1056
tion. However, predictive data mining is only fea-
sible when a large amount of structured numerical 
data (e.g., in a database) is available. Unlike this 
research area which analyzes numeric values, our 
study mines unstructured text using NLP tech-
niques and it can potentially extend the reach of 
numeric techniques.  
Despite the vast amount of predictive opinions 
and their potential applications such as identifica-
tion and analysis of people's opinions about the 
real estate market or a specific country's economic 
future, studies on predictive opinions have been 
neglected in Computational Linguistics, where 
most previous work focuses on judgment opinions 
(see Section 2). In this paper, we concentrate on 
identifying predictive opinion with its valence.  
Among many prediction domains on the Web, 
we focus on election prediction and introduce 
Crystal, a system to predict election results using 
the public's written viewpoints. To build our sys-
tem, we collect opinions about past elections 
posted on an election prediction project website 
before the election day, and build a corpus1. We 
then use this corpus to train our system for analyz-
ing predictive opinion messages and, using this, to 
predict the election outcome. Due to the availabil-
ity of actual results of the past elections, we can 
not only evaluate how accurately Crystal analyzes 
prediction messages (by checking agreement with 
the gold standard), but also objectively measure the 
prediction accuracy of our system. 
The main contributions of this work are as fol-
lows: 
? an NLP technique for analyzing predictive 
opinions in the electoral domain; 
? a method of automatically building a corpus 
of predictive opinions for a supervised 
learning approach; and 
? a feature generalization technique that out-
performs all the baselines on the task of 
identifying a predicted winning party given 
a predictive opinion. 
The rest of this paper is structured as follows. 
Section 2 surveys previous work. Section 3 for-
mally defines our task and describes our data set. 
Section 4 describes our system Crystal with pro-
posed feature generalization algorithm. Section 5 
                                                 
1 The resulting corpus is available at  
http://www.isi.edu/ ~skim/Download/Data/predictive.htm 
reports empirical evidence that Crystal outper-
forms several baseline systems. Finally, Section 6 
concludes with a description of the impact of this 
work. 
2 Related Work 
This work is closely related to opinion analysis and 
text classification. Most research on opinion analy-
sis in computational linguistics has focused on sen-
timent analysis, subjectivity detection, and review 
mining. Pang et al (2002) and Turney (2002) clas-
sified sentiment polarity of reviews at the docu-
ment level. Wiebe et al (1999) classified sentence 
level subjectivity using syntactic classes such as 
adjectives, pronouns and modal verbs as features. 
Riloff and Wiebe (2003) extracted subjective ex-
pressions from sentences using a bootstrapping 
pattern learning process. Wiebe et. al (2004) and 
Riloff et. al (2005) adopted pattern learning with 
lexical feature generalization for subjective expres-
sion detection. Dave et. al (2003) and Jindal and 
Liu (2006) also learned patterns of opinion expres-
sion in product reviews. Yu and Hatzivassiloglou 
(2003) identified the polarity of opinion sentences 
using semantically oriented words. These tech-
niques were applied and examined in different do-
mains, such as customer reviews (Hu and Liu 
2004; Popescu et al, 2005) and news articles (Kim 
and Hovy, 2004; Wilson et al, 2005). 
In text classification, systems typically use bag-
of-words models, mostly with supervised learning 
algorithms using Naive Bayes or Support Vector 
Machines (Joachims, 1998) to classify documents 
into several categories such as sports, art, politics, 
and religion. Liu et al (2004) and Gliozzo et al 
(2005) address the difficulty of obtaining training 
corpora for supervised learning and propose unsu-
pervised learning approaches. Another recent re-
lated classification task focuses on academic and 
commercial efforts to detect email spam messages. 
For an SVM-based approach, see (Drucker et al, 
1999). In our study, we explore the use of general-
ized lexical features for predictive opinion analysis 
and compare it with the bag-of-words approach. 
3 Modeling Prediction 
In this section, we define the task of analyzing pre-
dictive opinions in the electoral domain. 
1057
3.1 Task Definition 
We model predictive opinions in an election as 
follows: 
Valence) (Party,i ni onedicti onOpElecti onPr =  
where Party is a political party running for an elec-
tion (e.g., Democrats and Republicans) and Va-
lence is the valence of a predictive opinion which 
can be either ?likely to win? (WIN) or ?unlikely to 
win? (LOSE). Values for Party vary depending on 
in which year (e.g., 1996 and 2006) and where an 
election takes place (e.g., United States, France, or 
Japan). The unit of a predictive opinion is an un-
structured textual document such as an article in a 
personal blog or a message posted on a news group 
discussion board about the topic of ?Which party 
do you think will win/lose in this election??. 
Figure 1 illustrates an overview of our election 
prediction system Crystal in action. Given each 
document posted on blogs or message boards (e.g., 
www.election prediction.org) as seen in Figure 1.a, 
a system can determine a Party that the author of a 
document thinks to win or lose (Valence), Figure 
1.b. For the example document starting with the 
sentence ?I think this riding will stay NDP as it has 
for the past 11 years.? in Figure 1.a, our predictive 
opinion analysis system aims to recognize NDP as 
Party and WIN as Valence. After aggregating the 
predictive opinion analysis results of all docu-
ments, we project the election results in Figure 1.c. 
The following section describes how we obtain our 
data set and the subsequent sections describe Crys-
tal. 
3.2 Automatically Labeled Data 
We collected messages posted on an election pre-
diction project page, www.electionprediction. org. 
The website contains various election prediction 
projects (e.g., provincial election, federal election, 
and general election) of different countries (e.g., 
Canada and United Kingdom) from 1999 to 2006. 
For our data set, we downloaded Canadian federal 
election prediction data for 2004 and 2006. The 
Canadian federal electoral system is based on 308 
Figure 1. Our election prediction system. Public
opinions are collected from message boards (a)
and our system determines for each the election
prediction ?Party? and ?Valence? (b). The output
of the system is a prediction of the election out-
come (c). 
Message text Predicted  winning party Riding Year
??? ??? ??? ??? 
Message_1457 Party_3 Riding_206 2004
Message_1458 Party_2 Riding_206 2004
Message_1459 Party_2 Riding_189 2006
Message_1460 Party_1 Riding_189 2006
Message_1461 Party_2 Riding_189 2006
Message_1462 Party_1 Riding_46 2006
??? ??? ??? ??? 
Table 1. A snapshot of the processed data 
Riding name Party Candidate name 
 NDP Noreen Johns 
Blackstrap Liberal J. Wayne Zimmer
 PC Lynne Yelich 
Table 2. An example of our Party-Candidate 
listing for a riding (PC: Progressive Conserva-
tive) 
1058
ridings (electoral districts). The website contains 
308 separate html files of messages corresponding 
to the 308 ridings for different years. In total, we 
collected 4858 and 4680 messages for the 2004 
and 2006 federal elections respectively. On aver-
age, a message consists of 98.8 words. 
To train and evaluate our system, we require a 
gold standard for each message (i.e., which party 
does an author of a message predict to win?). One 
option is to hire human annotators to build the gold 
standard. Instead, we used an online party logo 
image file that the author of each message already 
labeled for the message. Note that authors only 
select parties they think will win, which means our 
gold standard only contains a party with WIN va-
lence of each message. However, we leverage this 
information to build a system which is able to de-
termine a party even with LOSE valence. We de-
scribe this idea in detail in Section 4. 
Finally, we pre-processed the data by converting 
the downloaded html source files into a structured 
format with the following fields: message, party, 
riding, and year, where message is a text, party is a 
winning party predicted in the text, riding is one of 
the 308 ridings, and year is either 2004 or 2006. 
Table 1 shows a snapshot of the processed data set 
that we used for our system training and evalua-
tion. An additional piece of information consisting 
of a candidate's name for each party for each riding 
was also stored in our data set. With this informa-
tion, the system can infer opinions about a party 
based on opinions about candidates who run for the 
party. Table 2 shows an example of a riding. 
4 Analyzing Predictions 
In this section we describe Crystal. One simple 
approach could be a system (see NGR system in 
Section 5) trained by a machine learning technique 
using n-gram features and classifying a message 
into multiple classes (e.g., NDP, Liberal, or Pro-
gressive). However, we develop a more sophisti-
cated algorithm and compare its result with several 
baselines, including the simple n-gram method2. 
Experimental results in Section 5 show that Crystal 
outperforms all the baselines. 
Our approach consists of three steps: feature 
generalization, classification using SVMs, and 
                                                 
2 N-gram approach is often unbeatable (and therefore great) in 
many text classification tasks. 
SVM result integration3. Crystal generates general-
ized sentences in the feature generalization step. 
Then it classifies each sentence using generalized 
lexical features in order to determine Valence of 
Party in a sentence. Finally, it combines results of 
sentences to determine Valence and Party of a 
message. Note that the classification using SVM is 
an intermediate step conducting a binary classifica-
tion (i.e., WIN or LOSE) for the final multi-class 
classification in result integration. The following 
sections describe each step. 
4.1 Feature Generalization 
In the feature generalization step, we generalize 
patterns of words used in predictive opinions. For 
example, instead of using three different trigrams 
like ?Liberals will win?, ?NDP will win?, and 
?Conservatives will win?, we generalize these to 
?PARTY will win?. The assumption is that the 
generalized patterns can represent better the rela-
tionship among Party, Valence, and words sur-
rounding Party (e.g., will win) than pure lexical 
patterns. For this algorithm, we first substitute a 
candidate's name (both the first name and the last 
name) with the political party name that the candi-
date belongs to (see Table 2). We then break each 
message into sentences4.   
Table 3 outlines the feature generalization algo-
rithm. Here, our approach is that if a message pre-
                                                 
3 ?feature? indicates n-grams in our corpus that we use in the 
SVM classification step. 
4 The sentence breaker that we used is available at 
http://search.cpan.org/ ~shlomoy/Lingua-EN-sentence -
0.25/lib/Lingua/EN/Sentence.pm. 
1 for each message M with a party that M predicts to win, Pw 
2   for each sentence Si in a message M 
3      for each party Pj in Si 
4         valence Vj = +1 if Pj = Pw 
5         valence Vj = -1  Otherwise 
6         Generate S'ij by substituting Pj with  PARTY 
7         and all other parties in Si with OTHER
8          Return (Pj, Vj, S'ij) 
Table 3. Feature generalization algorithm 
1059
dicts a particular party to win, sentences which 
mention that party in the message also imply that it 
will win. Conversely all other parties are assumed 
to be in sentences that imply they will lose. As 
shown in Section 3.2, a message (M) in our corpus 
has a label of a party (Pw) that the author of M pre-
dicts to win. After breaking sentences in M, we 
duplicate a sentence by the number of unique par-
ties in the sentence and modify the duplicated sen-
tences by substituting the party names with 
PARTY and OTHER in order to generalize fea-
tures. 
Consider the following sentence: 
 ?Dockrill will barely take this riding from 
Rodger Cuzner?  
which gets re-written as: 
?NDP will barely take this riding from Liberal?  
because Dockrill is an NDP candidate and Rodger 
Cuzner is a Liberal candidate. Since the sentence 
contains two parties (i.e., NDP and Liberal), the 
algorithm duplicates the sentence twice, once for 
each party (see Lines 4?8 in Table 3)5. For NDP, 
the algorithm determines its Valence as -1 because 
NDP is not equal to the predicted winning party 
(i.e., Liberal) of the message (see Lines 4?5 in Ta-
                                                 
5 In the feature generalization algorithm, we represent 
WIN and LOSE valence as +1 and -1. 
ble 3). Then it generates a generalized sentence by 
substituting NDP with PARTY and Liberal with 
OTHER (Lines 6?7). It returns (NDP, -1, ?PARTY 
will barely take this riding from OTHER?). For 
Liberal, on the other hand, the algorithm deter-
mines its Valence as +1 since Liberal is the same 
as the predicted winning party of the message. Af-
ter similar generalization, it returns (Liberal, +1, 
?OTHER will barely take this riding from 
PARTY?).  
Note that the final result of the feature generali-
zation algorithm is a set of triplets: (Party, Va-
lence, Generalized Sentence). Among a triplet, we 
use (Valence, Generalized Sentence) to produce 
feature vectors for a machine learning algorithm 
(see Section 4.2) and (Party, Valence) to integrate 
system results of each sentence for the final deci-
sion of Party and Valence of a message (see Sec-
tion 4.3). Figure 2 shows an example of the algo-
rithm. 
4.2 Classification Using SVMs 
In this step, we use Support Vector Machines 
(SVMs) to train our system using the generalized 
features described in Section 4.1. After we ob-
tained examples of (Valence, Generalized Sen-
tence) in the feature generalization step, we mod-
eled a subtask of classifying a Generalized Sen-
tence into Valence towards our final goal of deter-
mining (Valence, Party) of a message. This subtask 
is a binary classification since Valence has only 2 
classes: +1 and -16. Given a generalized sentence 
?OTHER will barely take this riding from 
PARTY? in Figure 2, for example, the goal of our 
system is to learn WIN valence for PARTY. Fea-
tures for SVMs are extracted from generalized sen-
tences. We implemented our SVM learning model 
using the SVMlight package7. 
4.3 SVM Result Integration 
In this step, we combine the valence of each sen-
tence predicted by SVMs to determine the final 
valence and predicted party of a message. For each 
party mentioned in a message, we calculate the 
sum of the party's valences of each sentence and 
                                                 
6 However, the final evaluation of the system and all the base-
lines is equally performed on the multi-classification results of 
messages. 
7 SVMlight is available from http://svmlight.joachims. 
org/ 
Figure 2. An example of feature generalization 
of a message 
1060
pick a party that has the maximum value. This in-
tegration algorithm can be represented as follows: 
?
=
m
k
k
p
pValence
0
)(max arg
 
where p is one of parties mentioned in a message, 
m is the number of sentences that contains party p 
in a message, and Valencek(p) is the valence of p in 
the kth sentence that contains p. Given the example 
in Figure 2, the Liberal party appears twice in sen-
tence S0 and S1 and its total valence score is +2, 
whereas the NDP party appears once in sentence 
S1 and its valence sum is -1. As a result, our algo-
rithm picks liberal as the winning party that the 
message predicts. 
5 Experiments and Results 
This section reports our experimental results show-
ing empirical evidence that Crystal outperforms 
several baseline systems. 
5.1 Experimental Setup 
Our corpus consists of 4858 and 4680 messages 
from 2004 and 2006 Canadian federal election pre-
diction data respectively described in detail in Sec-
tion 3.2. We split our pre-processed corpus into 10 
folds for cross-validation. We implemented the 
following five systems to compare with Crystal 8. 
? NGR: In this algorithm, we train the system us-
ing SVM with n-gram features without the gener-
alization step described in Section 4.19. The re-
placement of each candidate's first and last name 
by his or her party name was still applied. 
? FRQ: This system picks the most frequently 
mentioned party in a message as the predicted 
winning party. Party name substitution is also ap-
plied. For example, given a message ?This riding 
will go liberal. Dockrill will barely take this riding 
from Rodger Cuzner.?, all candidates' names are 
replaced by party names (i.e., ?This riding will go 
Liberal. NDP will barely take this riding from Lib-
eral.?). After name replacement, the system picks 
Liberal as an answer because Liberal appears twice 
whereas NDP appears only once. Note that, unlike 
Crystal, this system does not consider the valence 
of each party (as done in our sentence duplication 
                                                 
8 In our experiments using SVM, we used the linear kernel for 
all Crystal, NGR, and JDG. 
9 This system is exactly like Crystal without the feature gener-
alization and result integration steps. 
step of the feature generalization algorithm). In-
stead, it blindly picks the party that appeared most 
in a message. 
? MJR: This system marks all messages with the 
most dominant predicted party in the entire data 
set. In our corpus, Conservatives was the majority 
party (3480 messages) followed closely by Liberal 
(3473 messages). 
? INC: This system chooses the incumbent party 
as the predicted winning party of a message. (This 
is a strong baseline since incumbents often win in 
Canadian politics). For example, since the incum-
bent party of the riding ?Blackstrap? in 2004 was 
Conservative, all the messages about Blackstrap in 
2004 were marked Conservative as their predicted 
winning party by this system.  
? JDG: This system uses judgment opinion words 
as its features for SVM. For our list of judgment 
opinion words, we use General Inquirer which is a 
publicly available list of 1635 positive and nega-
tive sentiment words (e.g., love, hate, wise, dumb, 
etc.)10. 
5.2 Experimental Results 
We measure the system performance with its accu-
racy in two different ways: accuracy per message 
(Accmessage) and accuracy per riding (Accriding). Both 
accuracies are represented as follows: 
set test ain  messages of # Total
labledcorrectly  system  themessages of #=messageAcc  
set test ain  ridings of # Total
predictedcorrectly  system  theridings of #=ridingAcc  
We first report the results with Accmessage in 
Evaluation1 and then report with Accriding in 
Evaluation2. 
Evaluation1: Table 4 shows accuracies of base-
lines and Crystal. We calculated accuracy for each 
test set in 10-fold data sets and averaged it. Among 
the baselines, MJR performed worst (36.48%). 
Both FRQ and INC performed around 50% 
(54.82% and 53.29% respectively). NGR achieved 
its best score (62.02%) when using unigram, bi-
gram, and trigram features together (uni+bi+tri). 
We also experimented with other feature combina-
tions (see Table 5). Our system achieved 73.07% 
which is 11% higher than NGR and around 20% 
                                                 
10 Available at http://www.wjh.harvard.edu/~inquirer 
/homecat.htm 
1061
higher than FRQ and INC. The best accuracy of 
our system was also obtained with the combination 
of unigram, bigram, and trigram features. 
The JDG system, which uses positive and nega-
tive sentiment word features, had 66.23% accu-
racy. This is about 7% lower than Crystal. Since 
the lower performance of JDG might be related to 
the number of features it uses, we also experi-
mented with the reduced number of features of 
Crystal based on the tfidf scores11. With the same 
number of features (i.e., 1635), Crystal performed 
70.62% which is 4.4% higher than JDG. An inter-
esting finding was that NGR with 1635 features 
performed only 54.60% which is significantly 
                                                 
11 The total number of all features of Crystal is 689,642. 
lower than both systems. This indicates that the 
1635 pure n-gram features are not as good as the 
same number of sentiment words carefully chosen 
from a dictionary but the generalized features of 
Crystal represent the predictive opinions better 
than JDG features. 
Table 5 illustrates the comparison of NGR 
(without feature generalization) and Crystal (with 
feature generalization) in different feature combi-
nations. uni, bi, tri, and four correspond to uni-
gram, bigram, trigram, and fourgram. Our pro-
posed technique Crystal performed always better 
than the pure n-gram system (NGR). Both systems 
performed best (62.02% and 73.07%) with the 
combination of unigram, bigram, and trigram 
(uni+bi+tri). The second best scores (61.96% and 
73.01%) are achieved with the combinations of all 
grams (uni+bi+tri+four) in both systems. Using 
fourgrams alone performed worst since the system 
overfitted to the training examples. 
Table 6 presents several examples of frequent n-
gram features in both WIN and LOSE classes. As 
shown in Table 6, lexical patterns in the WIN class 
express optimistic sentiments about PARTY (e.g., 
PARTY_will_win and go_ PARTY_again) 
whereas patterns in the LOSE class express pessi-
mistic sentiments (e.g., PARTY_don't_have) and 
optimistic ones about OTHER (e.g., 
want_OTHER). 
Evaluation2: In this evaluation, we use Accriding 
computed as the number of ridings that a system 
correctly predicted, divided by the total number of 
ridings. For each riding R, systems pick a party 
that obtains the majority prediction votes from 
messages in R as the winning party of R. For ex-
Patterns in WIN class Patterns in LOSE class 
PARTY_will_win want_OTHER 
PARTY_hold PARTY_don?t_have 
PARTY_will_win_this OTHER_and 
PARTY_win the_PARTY 
will_go_PARTY OTHER_will_win 
PARTY_will_take OTHER_is 
PARTY_will_take_this to_the_OTHER 
PARTY_is and_OTHER 
safest_PARTY results_OTHER 
PARTY_has OTHER_has 
go_PARTY_again to_OTHER 
Table 6. Examples of frequent features in 
WIN and LOSE classes. 
system Accmessage (%) Accriding (%) 
FRQ 54.82 63.14 
MJR 36.48 36.63 
INC 53.29 78.03 
NGR (uni+bi+tri) 62.02 79.65 
JDG 66.23 78.68 
Crystal (uni+bi+tri) 73.07 81.68 
Table 4. System performance with accuracy 
per message (Accmessage ) and accuracy per 
riding (Accriding): FRQ, MJR, INC, NGR, 
JDG, and Crystal. 
Accmessage (%) Features 
NGR Crystal 
uni 60.49 72.03 
bi 58.79 71.81 
tri 54.04 69.57 
four 47.25 67.64 
uni + bi 61.54 72.93 
uni + tri 61.36 72.20 
uni + four 60.70 72.84 
bi + tri 58.68 72.26 
bi + four 58.54 72.17 
uni + bi + tri 62.02 73.07 
uni + bi + four 61.75 72.30 
uni + tri + four 61.34 72.30 
bi + tri + four 58.42 72.62 
uni + bi + tri + four 61.96 73.01 
Table 5. System performance with different 
features: Pure n-gram (NGR) and General-
ized n-gram Crystal. 
1062
ample, if Crystal identified 9 messages predicting 
for Conservative Party, 3 messages for NDP, and 1 
message for Liberal among 13 messages in the rid-
ing ?Blackstrap?, the system will predict that the 
Conservative Party would win in ?Blackstrap?. 
Table 4 shows the system performance with Ac-
criding. Note that people who write messages on a 
particular web site are not a random sample for 
prediction. So we introduce a measure of confi-
dence (ConfidenceScore) of each system and use 
the prediction results when the ConfidenceScore is 
higher than a threshold. Otherwise, we use a de-
fault party (i.e., the incumbent party) as the win-
ning party. ConfidenceScore of a riding R is calcu-
lated as follows: 
ConfidenceScore =  countmessage(Pfirst) ?  countmes-
sage(Psecond) 
where countmessage(Px) is the number of messages 
that predict a party Px to win, Pfirst is the party that 
the most number of messages predict to win, and 
Psecond is the party that the second most number of 
messages predict to win. 
We used 62 ridings to tune the ConfidenceScore 
parameter arriving at the value of 4. As shown in 
Table 4, the system which just considers the in-
cumbent party (INC) performed fairly well 
(78.03% accuracy) because incumbents are often 
re-elected in Canadian elections. The upper bound 
of this prediction task is 88.85% accuracy which is 
the prediction result using numerical values of a 
prediction survey. FRQ and MJR performed 
63.14% and 36.63% respectively. Similarly to 
Evaluation1, JDG which only uses judgment word 
features performed worse than both Crystal and 
NGR. Also, Crystal with our feature generalization 
algorithm performed better than NGR with non-
generalized n-gram features. The accuracy of Crys-
tal (81.68%) is comparable to the upper bound 
88.85%. 
6 Discussion 
In this section, we discuss possible extensions and 
improvements of this work. 
Our experiment focuses on investigating aspects 
of predictive opinions by learning lexical patterns 
and comparing them with judgment opinions. 
However, this work can be extended to investigat-
ing how those two types of opinions are related to 
each other and whether lexical features of one 
(e.g., judgment opinion) can help identify the other 
(e.g., predictive opinion). Combining two types of 
opinion features and testing on each domain can 
examine this issue. 
In our experiment, we used General Inquirer 
words as judgment opinion indicators for JDG 
baseline system. It might be interesting to employ 
different resources for judgment words such as the 
polarity lexicon by Wilson et al (2005) and the 
recently released SentiWordNet12. 
 Our work is an initial step towards analyzing a 
new type of opinion. In the future, we plan to in-
corporate more features such as priors like incum-
bent party in addition to the lexical features to im-
prove the system performance. 
7 Conclusions 
In this paper, we proposed a framework for work-
ing with predictive opinion. Previously, research-
ers in opinion analysis mostly focused on judgment 
opinions which express positive or negative senti-
ment about a topic, as in product reviews and pol-
icy discussions. Unlike judgment opinions, predic-
tive opinions express a person's opinion about the 
future of a topic or event such as the housing mar-
ket, a popular sports match, and election results, 
based on his or her belief and knowledge. Among 
these many kinds of predictive opinions, we fo-
cused on election prediction. 
We collected past election prediction data from 
an election prediction project site and automati-
cally built a gold standard. Using this data, we 
modeled the election prediction task using a super-
vised learning approach, SVM. We proposed a 
novel technique which generalized n-gram feature 
patterns. Experimental results showed that this ap-
proach outperforms several baselines as well as a 
non-generalized n-gram approach. This is signifi-
cant because an n-gram model without generaliza-
tion is often extremely competitive in many text 
classification tasks.  
This work adopts NLP techniques for predictive 
opinions and it sets the foundation for exploring a 
whole new subclass of the opinion analysis prob-
lems. Potential applications of this work are sys-
tems that analyze various kinds of election predic-
tions by monitoring texts in discussion boards and 
personal blogs. In the future, we would like to 
                                                 
12 http://sentiwordnet.isti.cnr.it/ 
1063
model predictive opinions in other domains such as 
the real estate market and the stock market which 
would require further exploration of system design 
and data collection.  
Reference 
Engelmore, R., and Morgan, A. eds. 1986. Blackboard 
Systems. Reading, Mass.: Addison-Wesley. 
Dave, K., Lawrence, S. and Pennock, D. M.  2003. Min-
ing the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. Proc. of 
World Wide Web Conference 2003 
Drucker, H., Wu, D. and Vapnik, V. 1999. Support vec-
tor machines for spam categorization. IEEE Trans. 
Neural Netw., 10, pp 1048?1054. 
Gliozzo, A., Strapparava C. and Dagan, I. 2005. Investi-
gating Unsupervised Learning for Text Categoriza-
tion Bootstrapping, Proc. of EMNLP 2005. Vancou-
ver, B.C., Canada 
Hu, M. and Liu, B. 2004. Mining and summarizing cus-
tomer reviews. Proc. Of KDD-2004, Seattle, Wash-
ington, USA. 
Jindal, N. and Liu, B. 2006. Mining Comprative Sen-
tences and Relations. Proc. of 21st National Confer-
ence on Artificial Intellgience (AAAI-2006). 2006. 
Boston, Massachusetts, USA 
Joachims, T. 1998. Text categorization with support 
vector machines: Learning with many relevant fea-
tures, Proc. of ECML, p. 137?142.  
Kim, S-M. and Hovy, E. 2004. Determining the Senti-
ment of Opinions. Proc. of COLING 2004. 
Liu, B., Li, X., Lee, W. S. and Yu, P. S. Text Classifica-
tion by Labeling Words Proc. of AAAI-2004, San 
Jose, USA. 
Pang, B, Lee, L. and Vaithyanathan, S. 2002. Thumbs 
up? Sentiment Classification using Machine Learning 
Techniques. Proc. of EMNLP 2002. 
Popescu, A-M. and Etzioni, O. 2005. Extracting Product 
Features and Opinions from Reviews, Proc. of HLT-
EMNLP 2005. 
Rickel, J. and Porter, B. 1997. Automated Modeling of 
Complex Systems to Answer Prediction Questions, 
Artificial Intelligence Journal, volume 93, numbers 
1-2, pp. 201?260 
Riloff, E., Wiebe, J., and Phillips, W. 2005. Exploiting 
Subjectivity Classification to Improve Information 
Extraction, Proc. of the 20th National Conference on 
Artificial Intelligence (AAAI-05) . 
Riloff, E., Wiebe, J. and Wilson, T. 2003. Learning Sub-
jective Nouns Using Extraction Pattern Bootstrap-
ping. Proc. of CoNLL 2003. pp 25?32. 
Rodionov, S. and Martin, J. H. 1996. A Knowledge-
Based System for the Diagnosis and Prediction of 
Short-Term Climatic Changes in the North Atlantic, 
Journal of Climate, 9(8)  
Turney, P. 2002. Thumbs Up or Thumbs Down? Se-
mantic Orientation Applied to Unsupervised Classifi-
cation of Reviews. Proc. of ACL 2002, pp 417?424. 
Wiebe, J., Bruce, R. and O?Hara, T. 1999. Development 
and use of a gold standard data set for subjectivity 
classifications. Proc. of ACL 1999, pp 246?253. 
Wiebe, J., Wilson, T. , Bruce, R. , Bell , M. and Martin, 
M. Learning Subjective Language. 2004. Computa-
tional Linguistics 
Wilson, T., Wiebe, J. and Hoffmann, P. 2005. Recog-
nizing Contextual Polarity in Phrase-Level Sentiment 
Analysis. Proc. of HLT/EMNLP 2005. 
Yu, H. and Hatzivassiloglou, V. 2003. Towards An-
swering Opinion Questions: Separating Facts from 
Opinions and Identifying the Polarity of Opinion 
Sentences. Proc. of EMNLP 2003. 
1064
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 948?957,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Toward Completeness in Concept Extraction and Classification
Eduard Hovy and Zornitsa Kozareva
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292
hovy@isi.edu, zkozareva@gmail.com
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Abstract
Many algorithms extract terms from text to-
gether with some kind of taxonomic clas-
sification (is-a) link. However, the general
approaches used today, and specifically the
methods of evaluating results, exhibit serious
shortcomings. Harvesting without focusing on
a specific conceptual area may deliver large
numbers of terms, but they are scattered over
an immense concept space, making Recall
judgments impossible. Regarding Precision,
simply judging the correctness of terms and
their individual classification links may pro-
vide high scores, but this doesn?t help with the
eventual assembly of terms into a single coher-
ent taxonomy. Furthermore, since there is no
correct and complete gold standard to measure
against, most work invents some ad hoc evalu-
ation measure. We present an algorithm that is
more precise and complete than previous ones
for identifying from web text just those con-
cepts ?below? a given seed term. Comparing
the results to WordNet, we find that the algo-
rithm misses terms, but also that it learns many
new terms not in WordNet, and that it clas-
sifies them in ways acceptable to humans but
different from WordNet.
1 Collecting Information with Care
Over the past few years, many algorithms have been
published on automatically harvesting terms and
their conceptual types from the web and/or other
large corpora (Etzioni et al, 2005; Pasca, 2007;
Banko et al, 2007; Yi and Niblack, 2005; Snow et
al., 2005). But several basic problems limit the even-
tual utility of the results.
First, there is no standard collection of facts
against which results can be measured. As we show
in this paper, WordNet (Fellbaum, 1998), the most
obvious contender because of its size and popularity,
is deficient in various ways: it is neither complete
nor is its taxonomic structure inarguably perfect. As
a result, alternative ad hoc measures are invented
that are not comparable. Second, simply harvesting
facts about an entity without regard to its actual sub-
sequent organization inflates Recall and Precision
evaluation scores: while it is correct that a jaguar
is a animal, mammal, toy, sports-team, car-make,
and operating-system, this information doesn?t help
to create a taxonomy that, for example, places mam-
mal and animal closer to one another than to some
of the others. ((Snow et al, 2005) is an exception
to this.) As a result, this work may give a mislead-
ing sense of progress. Third, entities are of differ-
ent formal types, and their taxonomic treatment is
consequently different: some are at the level of in-
stances (e.g., Michelangelo was a painter) and some
at the level of concepts (e.g., a painter is a human).
The goal of our research is to learn terms for en-
tities (objects) and their taxonomic organization si-
multaneously, from text. Our method is to use a
single surface-level pattern with several open posi-
tions. Filling them in different ways harvests differ-
ent kinds of information, and/or confirms this infor-
mation. We evaluate in two ways: against WordNet,
since that is a commonly available and popular re-
source, and also by asking humans to judge the re-
sults since WordNet is neither complete nor exhaus-
tively taxonomized.
In this paper, we describe experiments with two
rich and common portions of an entity taxonomy:
Animals and People. The claim of this paper is: It is
possible to learn terms automatically to populate a
targeted portion of a taxonomy (such as below An-
948
imals or People) both at high precision compared
to WordNet and including additional correct ones as
well. We would like to also report on Recall rela-
tive to WordNet, but given the problems described
in Section 4, this turns out to be much harder than
would seem.
First, we need to define some basic terminology:
term: An English word (for our current purposes, a
noun or a proper name).
seed term: A word we use to initiate the algorithm.
concept: An item in the classification taxonomy we
are building. A concept may correspond to several
terms (singular form, plural form, the term?s syn-
onyms, etc.).
root concept: A concept at a fairly general (high)
level in the taxonomy, to which many others are
eventually learned to be subtypes/instances of.
basic-level concept: A concept at the ?basic level?,
corresponding approximately to the Basic Level cat-
egories defined in Prototype Theory in Psychology
(Rosch, 1978). For our purposes, a concept corre-
sponding to the (proto)typical level of generality of
its type; that is, a dog, not a mammal or a dachshund;
a singer, not a human or an opera diva.
instance: An item in the classification taxonomy
that is more specific than a concept; only one exam-
ple of the instance exists in ?the real world? at any
time. For example, Michelangelo is an instance, as
well as Mazda Miata with license plate 3HCY687,
while Mazda Miata is not.
classification link: We use a single relation, that,
depending on its arguments, is either is a type of
(when both arguments are concepts), or is an in-
stance of or is an example of (when the first argu-
ment is an instance/example of the second).
Section 2 describes our method for harvesting;
Section 3 discusses related work; and Section 4 de-
scribes the experiments and the results.
2 Term and Relation Extraction using the
Doubly-Anchored Pattern
Our goal is to develop a technique that automatically
?fills in? the concept space in the taxonomy below
any root concept, by harvesting terms through re-
peated web queries. We perform this in two alter-
nating stages.
Stage 1: Basic-level/Instance concept collec-
tion: We use the Doubly-Anchored Pattern DAP de-
veloped in (Kozareva et al, 2008):
DAP: [SeedTerm1] such as [SeedTerm2] and <X>
which learns a list of basic-level concepts or in-
stances (depending on whether SeedTerm2 ex-
presses a basic-level concept or an instance).1 DAP
is very reliable because it is instantiated with ex-
amples at both ?ends? of the space to be filled (the
higher-level (root) concept SeedTerm1 and a basic-
level term or instance (SeedTerm2)), which mutu-
ally disambiguate each other. For example, ?pres-
idents? for SeedTerm1 can refer to the leader of a
country, corporation, or university, and ?Ford? for
SeedTerm2 can refer to a car company, an automo-
bile pioneer, or a U.S. president. But when the two
terms co-occur in a text that matches the pattern
?Presidents such as Ford and <X>?, the text will
almost certainly refer to country presidents.
The first stage involves a series of repeated re-
placements of SeedTerm2 by newly-learned terms
in order to generate even more seed terms. That is,
each new basic-level concept or instance is rotated
into the pattern (becoming a new SeedTerm2) in a
bootstrapping cycle that Kozareva et al called reck-
less bootstrapping. This procedure is implemented
as exhaustive breadth-first search, and iterates until
no new terms are harvested. The harvested terms are
incorporated in a Hyponym Pattern Linkage Graph
(HPLG) G = (V,E), where each vertex v ? V is
a candidate term and each edge (u, v) ? E indi-
cates that term v was generated by term u. A term
u is ranked by Out-Degree(u) =
P
?(u,v)?E
w(u,v)
|V |?1
,
which represents the weighted sum of u?s outgoing
edges normalized by the total number of other nodes
in the graph. Intuitively, a term ranks highly if it
is frequently discovering many different terms dur-
ing the reckless bootstrapping cycle. This method is
very productive, harvesting a constant stream of new
terms for basic-level concepts or instances when the
taxonomy below the initial root concept SeedTerm1
is extensive (such as for Animals or People).
1Strictly speaking, our lowest-level concepts can be in-
stances, basic-level concepts, or concepts below the basic level
(e.g., dachsund). But for the sake of simplicity we will refer to
our lowest-level terms as basic-level concepts and instances.
949
Stage 2: Intermediate level concept collection:
Going beyond (Kozareva et al, 2008), we next apply
the Doubly-Anchored Pattern in the ?backward? di-
rection (DAP?1), for any two seed terms represent-
ing basic-level concepts or instances:
DAP?1: <X> such as [SeedTerm1] and [SeedTerm2]
which harvests a set of concepts, most of them inter-
mediate between the basic level or instance and the
initial higher-level seed.
This second stage (DAP?1) has not yet been de-
scribed in the literature. It proceeds analogously.
For pairs of basic-level concepts or instances be-
low the root concept that were found during the first
stage, we instantiate DAP?1 and issue a new web
query. For example, if the term ?cats? was harvested
by DAP in ?Animals such as dogs and <X>?, then
the pair < dogs, cats > forms the new Web query
?<X> such as dogs and cats?. We extract up to 2
consecutive nouns from the <X> position.
This procedure yields a large number of discov-
ered concepts, but they cannot all be used for fur-
ther bootstrapping. In addition to practical limita-
tions (such as limits on web querying), many of them
are too general?more general than the initial root
concept?and could derail the bootstrapping process
by introducing terms that stray every further away
from the initial root concept. We therefore rank the
harvested terms based on the likelihood that they
will be productive if they are expanded in the next
cycle. Ranking is based on two criteria: (1) the con-
cept should be prolific (i.e., produce many lower-
level concepts) in order to keep the bootstrapping
process energized, and (2) the concept should be
subordinate to the root concept, so that the process
stays within the targeted part of the search space.
To perform ranking, we incorporate both the har-
vested concepts and the basic-level/instance pairs
into a Hypernym Relation Graph (HRG), which we
define as a bipartite graph HRG = (V,E) with two
types of vertices. One set of vertices represents the
concepts (the category vertices (V
c
), and a second
set of vertices represents the basic-level/instance
pairs that produced the concepts (the member pair
vertices (V
mp
)). We create an edge e(u, v) ? E
between u ? V
c
and v ? V
mp
when the con-
cept represented by u was harvested by the basic-
level/instance pair represented by v, with the weight
of the edge defined as the number of times that the
lower pair found the concept on the web.
We use the Hypernym Relation Graph to rank
the intermediate concepts based on each node?s In-
Degree, which is the sum of the weights on the
node?s incoming edges. Formally, In-Degree(u) =
?
?(u,v)?E
w(u, v). Intuitively, a concept will be
ranked highly if it was harvested by many different
combinations of basic-level/instance terms.
However, this scoring function does not deter-
mine whether a concept is more or less general than
the initial root concept. For example, when har-
vesting animal categories, the system may learn the
word ?species?, which is a very common term asso-
ciated with animals, but also applies to non-animals
such as plants. To prevent the inclusion of over-
general terms and constrain the search to remain
?below? the root concept, we apply a Concept Posi-
tioning Test (CPT): We issue the following two web
queries:
(a) Concept such as RootConcept and <X>
(b) RootConcept such as Concept and <X>
If (b) returns more web hits than (a), then the con-
cept passes the test, otherwise it fails. The first (most
highly ranked) concept that passes CPT becomes the
new seed concept for the next bootstrapping cycle.
In principle, we could use all the concepts that pass
the CPT for bootstrapping2. However, for practical
reasons (primarily limitations on web querying), we
run the algorithm for 10 iterations.
3 Related Work
Many algorithms have been developed to automat-
ically acquire semantic class members using a va-
riety of techniques, including co-occurrence statis-
tics (Riloff and Shepherd, 1997; Roark and Char-
niak, 1998), syntactic dependencies (Pantel and
Ravichandran, 2004), and lexico-syntactic patterns
(Riloff and Jones, 1999; Fleischman and Hovy,
2002; Thelen and Riloff, 2002).
The work most closely related to ours is that of
(Hearst, 1992) who introduced the idea of apply-
ing hyponym patterns to text, which explicitly iden-
tify a hyponym relation between two terms (e.g.,
2The number of ranked concepts that pass CPT changes in
each iteration. Also, the wildcard * is important for counts, as
can be verified with a quick experiment using Google.
950
?such authors as <X>?). In recent years, sev-
eral researchers have followed up on this idea using
the web as a corpus. (Pasca, 2004) applies lexico-
syntactic hyponym patterns to the Web and use the
contexts around them for learning. KnowItAll (Et-
zioni et al, 2005) applies the hyponym patterns to
extract instances from the Web and ranks them by
relevance using mutual information. (Kozareva et
al., 2008) introduced a bootstrapping scheme using
the doubly-anchored pattern (DAP) that is guided
through graph ranking. This approach reported a
significant improvement from 5% to 18% over ap-
proaches using singly-anchored patterns like those
of (Pasca, 2004) and (Etzioni et al, 2005).
(Snow et al, 2005) describe a dependency path
based approach that generates a large number of
weak hypernym patterns using pairs of noun phrases
present in WordNet. They build a classifier using
the different hypernym patterns and find among the
highest precision patterns those of (Hearst, 1992).
Snow et al report performance of 85% precision
at 10% recall and 25% precision at 30% recall for
5300 hand-tagged noun phrase pairs. (McNamee et
al., 2008) use the technique of (Snow et al, 2005)
to harvest the hypernyms of the proper names. The
average precision on 75 automatically detected cat-
egories is 53%. The discovered hypernyms were
intergrated in a Question Answering system which
showed an improvement of 9% when evaluated on a
TREC Question Answering data set.
Recently, (Ritter et al, 2009) reported hypernym
learning using (Hearst, 1992) patterns and manually
tagged common and proper nouns. All hypernym
candidates matching the pattern are acquired, and
the candidate terms are ranked by mutual informa-
tion. However, they evaluate the performance of
their hypernym algorithm by considering only the
top 5 hypernyms given a basic-level concept or in-
stance. They report 100% precision at 18% recall,
and 66% precision at 72% recall, considering only
the top-5 list. Necessarily, using all the results re-
turned will result in lower precision scores. In con-
trast to their approach, our aim is to first acquire au-
tomatically with minimal supervision the basic-level
concepts for given root concept. Thus, we almost
entirely eliminate the need for humans to provide
hyponym seeds. Second, we evaluate the perfor-
mance of our approach not by measuring the top-
ranked 5 hypernyms given a basic-level concept, but
considering all harvested hypernyms of the concept.
Unlike (Etzioni et al, 2005), (Pasca, 2007) and
(Snow et al, 2005), we learn both instances and con-
cepts simultaneously.
Some researchers have also worked on reorga-
nizing, augmenting, or extending semantic concepts
that already exist in manually built resources such
as WordNet (Widdows and Dorow, 2002; Snow et
al., 2005) or Wikipedia (Ponzetto and Strube, 2007).
Work in automated ontology construction has cre-
ated lexical hierarchies (Caraballo, 1999; Cimiano
and Volker, 2005; Mann, 2002), and learned seman-
tic relations such as meronymy (Berland and Char-
niak, 1999; Girju et al, 2003).
4 Evaluation
The root concepts discussed in this paper are An-
imals and People, because they head large taxo-
nomic structures that are well-represented in Word-
Net. Throughout these experiments, we used as the
initial SeedTerm2 lions for Animals and Madonna
for People (by specifically choosing a proper name
for People we force harvesting down to the level of
individual instances). To collect data, we submitted
the DAP patterns as web queries to Google, retrieved
the top 1000 web snippets per query, and kept only
the unique ones. In total, we collected 1.1 GB of
snippets for Animals and 1.5 GB for People. The
algorithm was allowed to run for 10 iterations.
The algorithm learns a staggering variety of terms
that is much more diverse than we had antici-
pated. In addition to many basic-level concepts or
instances, such as dog and Madonna respectively,
and many intermediate concepts, such as mammals,
pets, and predators, it also harvested categories that
clearly seemed useful, such as laboratory animals,
forest dwellers, and endangered species. Many other
harvested terms were more difficult to judge, includ-
ing bait, allergens, seafood, vectors, protein, and
pests. While these terms have an obvious relation-
ship to Animals, we have to determine whether they
are legitimate and valuable subconcepts of Animals.
A second issue involves relative terms that are
hard to define in an absolute sense, such as native
animals and large mammals.
A complete evaluation should answer the following
three questions:
951
? Precision: What is the correctness of the har-
vested concepts? (How many of them are sim-
ply wrong, given the root concept?)
? Recall: What is the coverage of the harvested
concepts? (How many are missing, below a
given root concept?)
? How correct is the taxonomic structure
learned?
Given the number and variety of terms obtained,
we initially decided that an automatic evaluation
against existing resources (such as WordNet or
something similar) would be inadequate because
they do not contain many of our harvested terms,
even though many of these terms are clearly sensi-
ble and potentially valuable. Indeed, the whole point
of our work is to learn concepts and taxonomies that
go above and beyond what is currently available.
However, it is necessary to compare with
something, and it is important not to skirt the issue
by conducting evaluations that measure subsets of
results, or that perhaps may mislead. We therefore
decided to compare our results against WordNet and
to have human annotators judge as many results as
we could afford (to obtain a measure of Precision
and the legitimate extensions beyond WordNet).
Unfortunately, it proved impossible to measure
Recall against WordNet, because this requires as-
certaining the number of synsets in WordNet be-
tween the root and its basic-level categories. This
requires human judgment, which we could not af-
ford. We plan to address this question in future
work. Also, assessing the correctness of the learned
taxonomy structure requires the manual assessment
of each classification link proposed by the system
that is not already in WordNet, a task also beyond
our budget to complete in full. Some results?for
just basic-level terms and intermediate concepts, but
not among intermediate-level concepts?are shown in
Section 4.3.
We provide Precision scores using the following
measures, where terms refers to the harvested terms:
Pr
WN
=
#terms found in WordNet
#terms harvested by system
Pr
H
=
#terms judged correct by human
#terms harvested by system
NotInWN = #terms judged correct by human but
not in WordNet
We conducted three sets of experiments. Ex-
periment 1 evaluates the results of using DAP to
learn basic-level concepts for Animals and instances
for People. Experiment 2 evaluates the results of
using DAP?1 to harvest intermediate concepts be-
tween each root concept and its basic-level concepts
or instances. Experiment 3 evaluates the taxonomy
structure that is produced via the links between the
instances and intermediate concepts.
4.1 Experiment 1: Basic-Level Concepts and
Instances
In this section we discuss the results of harvest-
ing the basic-level Animal concepts and People in-
stances. The bootstrapping algorithm ranks the har-
vested terms by their Out-Degree score and consid-
ers as correct only those with Out-Degree > 0. In
ten iterations, the bootstrapping algorithm produced
913 Animal basic-level concepts and 1, 344 People
instances that passed this Out-Degree criterion.
4.1.1 Human Evaluation
The harvested terms were labeled by human
judges as either correct or incorrect with respect to
the root concept. Table 1 shows the Precision of the
top-ranked N terms, with N shown in increments
of 100. Overall, the Animal terms yielded 71%
(649/913) Precision and the People terms yielded
95% Precision (1,271/1,344). Figure 1 shows that
higher-ranked Animal terms are more accurate than
lower-ranked terms, which indicates that the scor-
ing function did its job. For People terms, accuracy
was very high throughout the ranked list. Overall,
these results show that the bootstrapping algorithm
generates a large number of correct instances of high
quality.
4.1.2 WordNet Evaluation
Table 1 shows a comparison of the harvested
terms against the terms present in WordNet.
Note that the Precision measured against WordNet
(Pr
WN
) for People is dramatically different from
the Precision based on human judgments (Pr
H
).
This can be explained by looking at the NotInWN
column, which shows that 48 correct Animal terms
952
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 100  200  300  400  500  600  700  800  900
Pr
ec
is
io
n
Rank
Animal Basic-level Concepts
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 200  400  600  800  1000  1200
Pr
ec
is
io
n
Rank
People Instances
Figure 1: Ranked Basic-Concepts and Instances.
and 986 correct People instances are not present in
WordNet (primarily, for people, because WordNet
contains relatively few proper names). These results
show that there is substantial room for improvement
in WordNet?s coverage of these categories. For Ani-
mals, the precision measured against WordNet is ac-
tually higher than the precision measured by human
judges, which may indicate that the judges failed to
recognize some correct animal terms.
Pr
WN
Pr
H
NotInWN
Animal .79 .71 48
People .23 .95 986
Table 1: Instance Evaluation.
4.1.3 Evaluation against Prior Work
To assess how well our algorithm compares with
previous semantic class learning methods, we com-
pared our results to those of (Kozareva et al, 2008).
Our work was inspired by that approach?in fact, we
use that previous algorithm as the first step of our
bootstrapping process. The novelty of our approach
is the insertion of an additional bootstrapping stage
that iteratively learns new intermediate concepts us-
ing DAP?1 and the Concept Positioning Test, fol-
lowed by the subsequent use of the newly learned
intermediate concepts in DAP to expand the search
space beyond the original root concept. This leads
to the discovery of additional basic-level terms or in-
stances, which are then recycled in turn to discover
new intermediate concepts, and so on.
Consequently, we can compare the results pro-
duced by the first iteration of our algorithm (be-
fore intermediate concepts are learned) to those of
(Kozareva et al, 2008) for the Animal and People
categories, and then compare again after 10 boot-
strapping iterations of intermediate concept learn-
ing. Figure 2 shows the number of harvested con-
cepts for Animals and People after each bootstrap-
ping iteration. Bootstrapping with intermediate con-
cepts produces nearly 5 times as many basic-level
concepts and instances than (Kozareva et al, 2008)
obtain, while maintaining similar levels of precision.
The intermediate concepts help so much because
they steer the learning process into new (yet still cor-
rect) regions of the search space after each iteration.
For instance, in the first iteration, the pattern ?ani-
mals such as lions and *? harvests about 350 basic-
level concepts, but only animals that are mentioned
in conjunction with lions are learned. Of these, an-
imals typically quite different from lions, such as
grass-eating kudu, are often not discovered.
However, in the second iteration, the intermediate
concept Herbivore is chosen for expansion. The pat-
tern ?herbivore such as antelope and *? discovers
many additional animals, including kudu, that co-
occur with antelope but do not co-occur with lions.
Table 2 shows examples of the 10 top-ranked
basic-level concepts and instances that were learned
for 3 randomly-selected intermediate Animal and
People concepts (IConcepts) that were acquired dur-
ing bootstrapping. In the next section, we present an
953
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 1  2  3  4  5  6  7  8  9  10
#I
te
m
s 
Le
ar
ne
d
Iterations
Animal Intermediate Concepts
Animal Basic-level Concepts
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 1  2  3  4  5  6  7  8  9  10
#I
te
m
s 
Le
ar
ne
d
Iterations
People Intermediate Concepts
People Instances
Figure 2: Learning Curves.
evaluation of the intermediate concept terms.
4.2 Experiment 2: Intermediate Concepts
In this section we discuss the results of harvesting
the intermediate-level concepts. Given the variety of
the harvested results, manual judgment of correct-
ness required an in-depth human annotation study.
We also compare our harvested results against the
concept terms in WordNet.
4.2.1 Human Evaluation
We hired 4 annotators (undergraduates at a dif-
ferent institution) to judge the correctness of the in-
termediate concepts. We created detailed annota-
tion guidelines that define 14 annotation labels for
each of the Animal and People classes, as shown
in Table 3. The labels are clustered into 4 major
PEOPLE
IConcept Instances
Dictators: Adolf Hitler, Joseph Stalin, Benito Mussolini, Lenin,
Fidel Castro, Idi Amin, Slobodan Milosevic,
Hugo Chavez, Mao Zedong, Saddam Hussein
Celebrities: Madonna, Paris Hilton, Angelina Jolie, Britney ,
Spears, Tom Cruise, Cameron Diaz, Bono,
Oprah Winfrey, Jennifer Aniston, Kate Moss
Writers: William Shakespeare, James Joyce, Charles Dickens,
Leo Tolstoy, Goethe, Ralph Waldo Emerson,
Daniel Defoe, Jane Austen, Ernest Hemingway,
Franz Kafka
ANIMAL
IConcept Basic-level Terms
Crustacean: shrimp, crabs, prawns, lobsters, crayfish, mysids,
decapods, marron, ostracods, yabbies
Primates: baboons, monkeys, chimpanzees, apes, marmosets,
chimps, orangutans, gibbons, tamarins, bonobos
Mammal: mice, whales, seals, dolphins, rats, deer, rabbits,
dogs, elephants, squirrels
Table 2: Learned People and Animals Terms.
types: Correct, Borderline, BasicConcept, and Not-
Concept. The details of our annotation guidelines,
the reasons for the intermediate labels, and the anno-
tation study can be found in (Kozareva et al, 2009).
ANIMAL
TYPE LABEL EXAMPLES
Correct GeneticAnimal reptile,mammal
BehavioralByFeeding predator, grazer
BehaviorByHabitat saltwater mammal
BehaviorSocialIndiv herding animal
BehaviorSocialGroup herd, pack
MorphologicalType cloven-hoofed animal
RoleOrFunction pet, parasite
Borderline NonRealAnimal dragons
EvaluativeTerm varmint, fox
OtherAnimal critter, fossil
BasicConcept BasicAnimal dog, hummingbird
NotConcept GeneralTerm model, catalyst
NotAnimal topic, favorite
GarbageTerm brates, mals
PEOPLE
TYPE LABEL EXAMPLES
Correct GeneticPerson Caucasian, Saxon
NonTransientEventRole stutterer, gourmand
TransientEventRole passenger, visitor
PersonState dwarf, schizophrenic
FamilyRelation aunt, mother
SocialRole fugitive, hero
NationOrTribe Bulgarian, Zulu
ReligiousAffiliation Catholic, atheist
Borderline NonRealPerson biblical figures
OtherPerson colleagues, couples
BasicConcept BasicPerson child, woman
RealPerson Barack Obama
NotConcept GeneralTerm image, figure
NotPerson books, events
Table 3: Intermediate Concept Annotation Labels
We measured pairwise inter-annotator agreement
across the four labels using the Fleiss kappa (Fleiss,
1971). The ? scores ranged from 0.61?0.71 for
Animals (average ?=0.66) and from 0.51?0.70 for
People (average ?=0.60). These agreement scores
seemed good enough to warrant using these human
judgments to estimate the accuracy of the algorithm.
The bootstrapping algorithm harvested 3, 549 An-
imal and 4, 094 People intermediate concepts in ten
iterations. After In-Degree ranking was applied,
954
we chose a random sample of intermediate concepts
with frequency over 1, which was given to four hu-
man judges for annotation. Table 4 summarizes the
labels assigned by the four annotators (A
1
? A
4
).
The top portion of Table 4 shows the results for all
the intermediate concepts (437 Animal terms and
296 People terms), and the bottom portion shows the
results only for the concepts that passed the Concept
Positioning Test (187 Animal terms and 139 People
terms). Accuracy is computed in two ways: Acc1 is
the percent of intermediate concepts labeled as Cor-
rect; Acc2 is the percent of intermediate concepts
labeled as either Correct or Borderline.
Without the CPT, accuracies range from 53?66%
for Animals and 75?85% for People. After ap-
plying the CPT, the accuracies increase to 71?84%
for animals and 82?94% for people. These results
confirm that the Concept Positioning Test is effec-
tive at removing many of the undesirable terms.
Overall, these results demonstrate that our algorithm
produced many high-quality intermediate concepts,
with good precision.
Figure 3 shows accuracy curves based on the
rankings of the intermediate concepts (based on In-
Degree scores). The CPT clearly improves accu-
racy even among the most highly ranked concepts.
For example, the Acc1 curves for animals show that
nearly 90% of the top 100 intermediate concepts
were correct after applying the CPT, whereas only
70% of the top 100 intermediate concepts were cor-
rect before. However, the CPT also eliminates many
desirable terms. For People, the accuracies are still
relatively high even without the CPT, and a much
larger set of intermediate concepts is learned.
Animals People
A
1
A
2
A
3
A
4
A
1
A
2
A
3
A
4
Correct 246 243 251 230 239 231 225 221
Borderline 42 26 22 29 12 10 6 4
BasicConcept 2 8 9 2 6 2 9 10
NotConcept 147 160 155 176 39 53 56 61
Acc1 .56 .56 .57 .53 .81 .78 .76 .75
Acc2 .66 .62 .62 .59 .85 .81 .78 .76
Animals after CPT People after CPT
A
1
A
2
A
3
A
4
A
1
A
2
A
3
A
4
Correct 146 133 144 141 126 126 114 116
Borderline 11 15 9 13 6 2 2 0
BasicConcept 2 8 9 2 0 1 7 7
NotConcept 28 31 25 31 7 10 16 16
Acc1 .78 .71 .77 .75 .91 .91 .82 .83
Acc2 .84 .79 .82 .82 .95 .92 .83 .83
Table 4: Human Intermediate Concept Evaluation.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300  350  400
Pr
ec
is
io
n
Rank
Animal Intermediate Concepts
noCPTC
noCPTCB
withCPTC
withCPTCB
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 50  100  150  200  250  300
Pr
ec
is
io
n
Rank
People Intermediate Concepts
noCPTC
noCPTCB
withCPTC
withCPTCB
Figure 3: Intermediate Concept Precision at Rank N.
4.2.2 WordNet Evaluation
We also compared the intermediate concepts har-
vested by the algorithm to the contents of WordNet.
The results are shown in Table 5. WordNet contains
20% of the Animal concepts and 51% of the People
concepts learned by our algorithm, which confirms
that many of these concepts were considered to be
valuable taxonomic terms by the WordNet develop-
ers. However, our human annotators judged 57%
of the Animal and 84% of the People concepts to
be correct, which suggests that our algorithm gen-
erates a substantial number of additional concepts
that could be used to enrich taxonomic structure in
WordNet.
955
Pr
WN
Pr
H
NotInWN
Animal .20 (88/437) .57 (248/437) 204
People .51 (152/296) .85 (251/296) 108
Table 5: WordNet Intermediate Concept Evaluation.
4.3 Experiment 3: Taxonomic Links
In this section we evaluate the classification (taxon-
omy) that is learned by evaluating the links between
the intermediate concepts and the basic-level con-
cept/instance terms. That is, when our algorithm
claims that isa(X,Y), how often is X truly a subcon-
cept of Y? For example, isa(goat, herbivore) would
be correct, but isa(goat, bird) would not. Again,
since WordNet does not contain all the harvested
concepts, we conduct both a manual evaluation and
a comparison against WordNet.
4.3.1 Manual and WordNet Evaluations
Creating and evaluating the full taxonomic struc-
ture between the root and the basic-level or instance
terms is future work. Here we evaluate simply the
accuracy of the taxonomic links between basic-level
concepts/instances and intermediate concepts as har-
vested, but not between intermediate concepts. For
each pair, we extracted all harvested links and deter-
mined whether the same links appear in WordNet.
The links were also given to human judges. Table 6
shows the results.
ISA Pr
WN
Pr
H
NotInWN
Animal .47(912/1940) .88 (1716/1940) 804
People .23 (318/908) .94 (857/908) 539
Table 6: WordNet Taxonomic Evaluation.
The results show that WordNet lacks nearly half
of the taxonomic relations that were generated by
the algorithm: 804 Animal and 539 People links.
5 Conclusion
We describe a novel extension to the DAP approach
for discovering basic-level concepts or instances and
their superconcepts given an initial root concept. By
appropriate filling of different positions in DAP, the
algorithm alternates between ?downward? and ?up-
ward? learning. A key resulting benefit is that each
new intermediate-level term acquired restarts har-
vesting in a new region of the concept space, which
allows previously unseen concepts to be discovered
with each bootstrapping cycle.
We also introduce the Concept Positioning Test,
which serves to confirm that a harvested concept
falls into the desired part of the search space rela-
tive to either a superordinate or subordinate concept
in the growing taxonomy, before it is selected for
further harvesting using the DAP.
These algorithms can augment other term harvest-
ing algorithms recently reported. But in order to
compare different algorithms, it is important to com-
pare results to a standard. WordNet is our best can-
didate at present. But WordNet is incomplete. Our
results include a significantly large number of in-
stances of People (which WordNet does not claim
to cover), a number comparable to the results of (Et-
zioni et al, 2005; Pasca, 2007; Ritter et al, 2009).
Rather surprisingly, our results also include a large
number of basic-level and intermediate concepts for
Animals that are not present in WordNet, a category
WordNet is actually fairly complete about. These
numbers show clearly that it is important to conduct
manual evaluation of term harvesting algorithms in
addition to comparing to a standard resource.
Acknowledgments
This research was supported in part by grants from
the National Science Foundation (NSF grant no. IIS-
0429360), and the Department of Homeland Se-
curity, ONR Grant numbers N0014-07-1-0152 and
N00014-07-1-0149. We are grateful to the anno-
tators at the University of Pittsburgh who helped
us evaluate this work: Jay Fischer, David Halpern,
Amir Hussain, and Taichi Nakatani.
References
M. Banko, M. Cafarella, S. Soderland, M. Broadhead,
and O.Etzioni. 2007. Open information extraction
from the web. In Proceedings of International Joint
Conference on Artificial Itelligence, pages 2670?2676.
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proceeding of RANLP-05, pages 166?172.
956
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database (Language, Speech, and Communication).
May.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proceedings of the
COLING conference, August.
J.L. Fleiss. 1971. Measuring nominal scale agree-
ment among many raters. Psychological Bulletin,
76(5):378?382.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In HLT-NAACL.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In COLING, pages 539?545.
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Seman-
tic class learning from the web with hyponym pat-
tern linkage graphs. In Proceedings of ACL-08: HLT,
pages 1048?1056. Association for Computational Lin-
guistics.
Z. Kozareva, E. Hovy, and E. Riloff. 2009. Learning and
evaluating the content and structure of a term taxon-
omy. In AAAI-09 Spring Symposium on Learning by
Reading and Learning to Read.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In COLING-02 on SEMANET,
pages 1?7.
P. McNamee, R. Snow, P. Schone, and J. Mayfield. 2008.
Learning named entity hyponyms for question answer-
ing. In Proceedings of the Third International Joint
Conference on Natural Language Processing.
P. Pantel and D. Ravichandran. 2004. Automatically la-
beling semantic classes. In HLT-NAACL, pages 321?
328.
M. Pasca. 2004. Acquisition of categorized named en-
tities for web search. In Proceedings of CIKM, pages
137?145.
M. Pasca. 2007. Weakly-supervised discovery of named
entities using web search queries. In CIKM, pages
683?690.
S. Ponzetto and M. Strube. 2007. Deriving a large scale
taxonomy from wikipedia. In Proceedings of the 22nd
National COnference on Artificial Intelligence (AAAI-
07), pages 1440?1447.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117?124.
A. Ritter, S. Soderland, and O. Etzioni. 2009. What is
this, anyway: Automatic hypernym discovery. In Pro-
ceedings of AAAI-09 Spring Symposium on Learning
by Reading and Learning to Read, pages 88?93.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110?1116.
E. Rosch, 1978. Principles of Categorization, pages 27?
48.
R. Snow, D. Jurafsky, and A. Y. Ng. 2005. Learning
syntactic patterns for automatic hypernym discovery.
In NIPS.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214?221.
D. Widdows and B. Dorow. 2002. A graph model for un-
supervised lexical acquisition. In Proceedings of the
19th international conference on Computational lin-
guistics, pages 1?7.
J. Yi and W. Niblack. 2005. Sentiment mining in web-
fountain. In ICDE ?05: Proceedings of the 21st In-
ternational Conference on Data Engineering, pages
1073?1083.
957
Toward Semantics-Based Answer Pinpointing
Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, Deepak Ravichandran
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292-6695
USA
tel: +1-310-448-8731
{hovy,gerber,ulf,cyl,ravichan}@isi.edu
ABSTRACT
We describe the treatment of questions (Question-Answer
Typology, question parsing, and results) in the Weblcopedia
question answering system.  
1. INTRODUCTION
Several research projects have recently investigated the
problem of automatically answering simple questions that have
brief phrasal answers (?factoids?), by identifying and extracting
the answer from a large collection of text.  
The systems built in these projects exhibit a fairly standard
structure: they create a query from the user?s question, perform
IR with the query to locate (segments of) documents likely to
contain an answer, and then pinpoint the most likely answer
passage within the candidate documents.  The most common
difference lies in the pinpointing. Many projects employ a
window-based word scoring method that rewards desirable
words in the window.  They move the window across the
candidate answers texts/segments and return the window at the
position giving the highest total score.  A word is desirable if
it is a content word and it is either contained in the question, or
is a variant of a word contained in the question, or if it matches
the words of the expected answer.  Many variations of this
method are possible?of the scores, of the treatment of multi-
word phrases and gaps between desirable words, of the range of
variations allowed, and of the computation of the expected
answer words.  
Although it works to some degree (giving results of up to 30%
in independent evaluations), the window-based method has
several quite serious limitations:
? it cannot pinpoint answer boundaries precisely (e.g., an
exact name or noun phrase),
? it relies solely on information at the word level, and
hence cannot recognize information of the desired type
(such as Person or Location),
? it cannot locate and compose parts of answers that are
distributed over areas wider than the window.
Window-based pinpointing is therefore not satisfactory in the
long run, even for factoid QA.  In this paper we describe work
in our Webclopedia project on semantics-based answer
pinpointing. Initially, though, recognizing the simplicity and
power of the window-based technique for getting started, we
implemented a version of it as a fallback method.  We then
implemented two more sophisticated methods: syntactic-
semantic question analysis and QA pattern matching.  This
involves classification of QA types to facilitate recognition of
desired answer types, a robust syntactic-semantic parser to
analyze the question and candidate answers, and a matcher that
combines word- and parse-tree-level information to identify
answer passages more precisely.  We expect that the two
methods will really show their power when more complex non-
factoid answers are sought.  In this paper we describe how well
the three methods did relative to each other.  Section 2 outlines
the Webclopedia system.  Sections 3, 4, and 5 describe the
semantics-based components: a QA Typology, question and
answer parsing, and matching.  Finally, we outline current
work on automatically learning QA patterns using the Noisy
Channel Model.  
2. WEBCLOPEDIA
Webclopedia?s architecture (Figure 1) follows the pattern
outlined above:
Question parsing: Using BBN?s IdentiFinder [1], our
parser CONTEX (Section 4) produces a syntactic-semantic
analysis of the question and determines the QA type (Section
3).  
Query formation : Single- and multi-word units (content
words) are extracted from the analysis, and WordNet synsets are
used for query expansion.  A Boolean query is formed. See [9].
IR: The IR engine MG [12] returns the top-ranked 1000
documents.
Segmentat ion : To decrease the amount of text to be
processed, the documents are broken into semantically
coherent segments.  Two text segmenter?TexTiling [5] and
C99 [2]?were tried; the first is used; see [9].
Ranking segments : For each segment, each sentence i s
scored using a formula that rewards word and phrase overlap
with the question and its expanded query words.  Segments are
ranked.  See [9]
Parsing segments : CONTEX parses each sentence of the
top-ranked 100 segments (Section 4).  
Pinpointing: For each sentence, three steps of matching are
performed (Section 5); two compare the analyses of the
question and the sentence; the third uses the window method to
compute a goodness score.  
Ranking of answers : The candidate answers? scores are
compared and the winner(s) are output.
3. THE QA TYPOLOGY
In order to perform pinpointing deeper than the word level, the
system has to produce a representation of what the user i s
asking.  Some previous work in automated question answering
has categorized questions by question word or by a mixture of
question word and the semantic class of the answer [11, 10].  To
ensure full coverage of all forms of simple question and answer,
and to be able to factor in deviations and special requirements,
we are developing a QA Typology.  
We motivate the Typology (a taxonomy of QA types) as
follows.  
There are many ways to ask the same thing: What is the age o f
the Queen of Holland?  How old is the Netherlands? queen?  How
long has the ruler of Holland been alive?  Likewise, there are
many ways of delivering the same answer: about 60; 63 years
old; since January 1938.  Such variations form a sort of
semantic equivalence class of both questions and answers.
Since the user may employ any version of his or her question,
and the source documents may contain any version(s) of the
answer, an efficient system should group together equivalent
question types and answer types.  Any specific question can
then be indexed into its type, from which all equivalent forms
of the answer can be ascertained.  These QA equivalence types
can help with both query expansion and answer pinpointing.
However, the equivalence is fuzzy; even slight variations
introduce exceptions: who invented the gas laser? can be
answered by both Ali Javan and a scientist at MIT, while what
is the name of the person who invented the gas laser? requires
the former only.  This inexactness suggests that the QA types
be organized in an inheritance hierarchy, allowing the answer
requirements satisfying more general questions to be
overridden by more specific ones ?lower down?.  
These considerations help structure the Webclopedia QA
Typology.  Instead of focusing on question word or semantic
type of the answer, our classes attempt to represent the user?s
intention, including for example the classes Why-Famous (for
Who was Christopher Columbus? but not Who discovered
IR
? Steps: create query from question (WordNet-expand)
             retrieve top 1000 documents
? Engines: MG (Sydney)?(Lin)
                  AT&T (TREC)?(Lin)
Segmentation
? Steps:segment each document into topical segments
? Engines: fixed-length (not used)
                 TexTiling (Hearst 94)?(Lin)
                 C99 (Choi 00)?(Lin)
                 MAXNET (Lin 00, not used)
Ranking
? Steps: score each sentence in each segment,
                              using WordNet expansion
             rank segments
? Engines: FastFinder (Junk)
Matching
? Steps: match general constraint patterns against parse trees
            match desired semantic type against parse tree elements
            match desired words against words in sentences
? Engines: matcher (Junk)
Ranking and answer extraction
? Steps: rank candidate answers
            extract and format them
? Engines: part of matcher (Junk)
Question parsing
? Steps: parse question
            find desired semantic type
? Engines: IdentiFinder (BBN)
                CONTEX (Hermjakob)
QA typology
? Categorize QA types in taxonomy (Gerber)
Constraint patterns
? Identify likely answers in relation to other
   parts of the sentence (Gerber)
Retrieve documents
Segment documents
Rank segments
Parse top segments
Parse question
Input question
Match segments against question
Rank and prepare answers
Create query
Output answers
Segment Parsing
? Steps: parse segment sentences
? Engines: CONTEX (Hermjakob)
Figure 1. Webclopedia architecture.
America?, which is the QA type Proper-Person) and
Abbreviation-Expansion (for What does HLT stand for?).  In
addition, the QA Typology becomes increasingly specific as
one moves from the root downward.
To create the QA Typology, we analyzed 17,384 questions and
their answers (downloaded from answers.com); see (Gerber, in
prep.).  The Typology (Figure 2) contains 72 nodes, whose leaf
nodes capture QA variations that can in many cases be further
differentiated.
Each Typology node has been annotated with examples and
typical patterns of expression of both Question and Answer,
using a simple template notation that expressed configurations
of words and parse tree annotations (Figure 3).  Question
pattern information (specifically, the semantic type of the
answer required, which we call a Qtarget) is produced by the
CONTEX parser (Section 4) when analyzing the question,
enabling it to output its guess(s) for the QA type.  Answer
pattern information is used by the Matcher (Section 5) to
pinpoint likely answer(s) in the parse trees of candidate answer
sentences.
Question examples and question templates
Who was Johnny Mathis' high school track coach?
Who was Lincoln's Secretary of State?
who be <entity>'s <role>
Who was President of Turkmenistan in 1994?
Who is the composer of Eugene Onegin?
Who is the chairman of GE?
who be <role> of <entity>
Answer templates and actual answers
<person>, <role> of  <entity>
Lou Vasquez, track coach of?and Johnny Mathis
<person> <role-title*> of <entity>
Signed Saparmurad Turkmenbachy [Niyazov],
president of Turkmenistan
<entity>?s <role> <person>
...Turkmenistan?s President Saparmurad Niyazov
<person>'s <entity>
...in Tchaikovsky's Eugene Onegin...
<role-title> <person> ... <entity> <role>
Mr. Jack Welch, GE chairman...
<subject>|<psv object> of related role-verb
       ...Chairman John Welch said ...GE's
Figure 3. Some QA Typology node annotations for
Proper-Person.
At the time of the TREC-9 Q&A evaluation, we had produced
approx. 500 patterns by simply cross-combining approx. 20
Question patterns with approx. 25 Answer patterns.  To our
disappointment (Section 6), these patterns were both too
specific and too few to identify answers frequently?when they
applied, they were quite accurate, but they applied too seldom.
We therefore started work on automatically learning QA
patterns in parse trees (Section 7).  On the other hand, the
semantic class of the answer (the Qtarget) is used to good effect
(Sections 4 and 6).
4. PARSING
CONTEX is a deterministic machine-learning based grammar
learner/parser that was originally built for MT [6].  For
English, parses of unseen sentences measured 87.6% labeled
precision and 88.4% labeled recall, trained on 2048 sentences
from the Penn Treebank. Over the past few years it has been
extended to Japanese and Korean [7].
4.1 Parsing Questions
Accuracy is particularly important for question parsing,
because for only one question there may be several answers in a
large document collection.  In particular, it is important to
identify as specific a Qtarget as possible.  But grammar rules
ERACITY YES:NO
TRUE:FALSE
NTIT Y A GENT NAME LAST-NAME
FIRST-NAME
ORGANIZATION
GROUP-OF-PEOPLE
A NIMAL
PERSON OCCUPATION-PERSON
GEOGRAPHICA L-PERSON
PROPER-NAMED-ENTITY PROPER-PERSON
PROPER-ORGANIZATION
PROPER-PLACE CITY
COUNTRY
STATE-DISTRICT
QUANTITY NU MERICAL-QUANTI TY
MONETARY-QUANTITY
TEMPORAL-QUANTITY
MASS-QUANTI TY
SPATIAL-QUANTITY DISTANCE-QUANTITY
AREA-QUANTITY
VOLUME-QUANTI TY
TEMP-LOC DATE
DATE-RANGE
LOCATOR ADDRESS
EMAIL-ADDRESS
PHONE-NUMBER
URL
TANGIBLE-OBJECT HU MAN-FOOD
SUBS TANCE LIQUID
BODY-PART
INSTRUMENT
GARMENT
TITLED-WORK
ABSTRACT SHAPE
ADJECTIVE COLOR
DISEASE
TEXT
NARRATIVE GENERAL-INFO DEFINITION USE
EXPRESSION-ORIGIN
HISTORY WHY-FAMOUS BIO
ANTECEDENT
INFLUENCE CONSEQUENT
CAUSE-EFFECT METHOD-MEANS
CIRCUMSTANCE-MEANS REASON
EVALUATION PRO-CON
CONTRAST
RATING
COUNSEL-ADVICE
Figure 2. Portion of Webclopedia QA Typology.
for declarative sentences do not apply well to questions, which
although typically shorter than declaratives, exhibit markedly
different word order, preposition stranding (?What university
was Woodrow Wilson President of??), etc.  
Unfortunately for CONTEX, questions to train on were not
initially easily available; the Wall Street Journal sentences
contain a few questions, often from quotes, but not enough and
not representative enough to result in an acceptable level of
question parse accuracy.  By collecting and treebanking,
however, we increased the number of questions in the training
data from 250 (for our TREC-9 evaluation version of
Webclopedia) to 400 on Oct 16 to 975 on Dec 9.  The effect i s
shown in Table 1.  In the first test run (?[trained] without
[additional questions]?), CONTEX was trained mostly on
declarative sentences (2000 Wall Street Journal sentences,
namely the enriched Penn Treebank, plus a few other non-
question sentences such as imperatives and short phrases).  In
later runs (?[trained] with [add. questions]?), the system was
trained on the same examples plus a subset of the 1153
questions we have treebanked at ISI (38 questions from the pre-
TREC-8 test set, all 200 from TREC-8 and 693 TREC-9, and
222 others).
The TREC-8 and TREC-9 questions were divided into 5 subsets,
used in a five-fold cross validation test in which the system was
trained on all but the test questions, and then evaluated on the
test questions.   
Reasons for the improvement include (1) significantly more
training data; (2) a few additional features, some more treebank
cleaning, a bit more background knowledge etc.; and (3) the
251 test questions on Oct. 16 were probably a little bit harder
on average, because a few of the TREC-9 questions initially
treebanked (and included in the October figures) were selected
for early treebanking because they represented particular
challenges, hurting subsequent Qtarget processing.
4.2 Parsing Potential Answers
The semantic type ontology in CONTEX was extended to
include 115 Qtarget types, plus some combined types; more
details in [8].  Beside the Qtargets that refer to concepts in
CONTEX?s concept ontology (see first example below),
Qtargets can also refer to part of speech labels (first example),
to constituent roles or slots of parse trees (second and third
examples), and to more abstract nodes in the QA Typology
(later examples). For questions with the Qtargets Q-WHY-
FAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and
others, the parser also provides Qargs?information helpful for
matching (final examples).
Semantic ontology types (I-EN-CITY)
and part of speech labels (S-PROPER-NAME):
What is the capital of Uganda?
QTARGET: (((I-EN-CITY S-PROPER-NAME))
((EQ I-EN-PROPER-PLACE)))
Parse tree roles:
Why can't ostriches fly?
      QTARGET: (((ROLE REASON)))
Name a film in which Jude Law acted.
      QTARGET: (((SLOT TITLE-P TRUE)))
QA Typology nodes:
What are the Black Hills known for?
     Q-WHY-FAMOUS
What is Occam's Razor?
     Q-DEFINITION
What is another name for nearsightedness?
     Q-SYNONYM
Should you exercise when you're sick?
     Q-YES-NO-QUESTION
Qargs for additional information:
Who was Betsy Ross?
     QTARGET: (((Q-WHY-FAMOUS-PERSON)))  
     QARGS: (("Betsy Ross"))
How is "Pacific Bell" abbreviated?
     QTARGET: (((Q-ABBREVIATION)))
     QARGS: (("Pacific Bell"))
What are geckos?
     QTARGET: (((Q-DEFINITION)))
     QARGS: (("geckos" "gecko") ("animal"))
These Qtargets are determined during parsing using 276 hand-
written rules.  Still, for approx. 10% of the TREC-8&9
questions there is no easily determinable Qtarget (?What does
the Peugeot company manufacture??; ?What is caliente in
English??).  Strategies for dealing with this are under
investigation.  More details appear in (Hermjakob, 2001).  The
current accuracy of the parser on questions and resulting
Qtargets sentences is shown in Table 2.
5. ANSWER MATCHING
The Matcher performs three independent matches, in order:
? match QA patterns in the parse tree,
? match Qtargets and Qwords in the parse tree,
? match over the answer text using a word window.
Details appear in [9].
Table 1. Improvement in parsing of questions.
Labeled Labeled Tagging Crossing
Precision Recall Precision Recall Accuracy Brackets
Without, Oct 16 90.74% 90.72% 84.62% 83.48% 94.95% 0.6
With, Oct 16 94.19% 94.86% 91.63% 91.91% 98.00% 0.48
With, Dec 9 97.33% 97.13% 95.40% 95.13% 98.64% 0.19
Table 1.  Improvement in parsing of questions.
6. RESULTS
We entered the TREC-9 short form QA track, and received an
overall Mean Reciprocal Rank score of 0.318, which put
Webclopedia in essentially tied second place with two others.
(The best system far outperformed those in second place.)  
In order to determine the relative performance of the modules,
we counted how many correct answers their output contained,
working on our training corpus.  Table 3 shows the evolution
of the system over a sample one-month period, reflecting the
amount of work put into different modules.  The modules QA
pattern, Qtarget, Qword, and Window were all run in parallel
from the same Ranker output.  
The same pattern, albeit with lower scores, occurred in the
TREC test (Table 4).  The QA patterns made only a small
contribution, the Qtarget made by far the largest contribution,
and, interestingly, the word-level window match lay
somewhere in between.
Table 4. TREC-9 test: correct answers
attributable to each module.
IR hits QA pattern Qtarget Window Total
78.1 5.5 26.2 10.4 30.3
We are pleased with the performance of the Qtarget match.  This
shows that CONTEX is able to identify to some degree the
semantic type of the desired answer, and able to pinpoint these
types also in candidate answers.  The fact that it outperforms
the window match indicates the desirability of looking deeper
than the surface level.  As discussed in Section 4, we are
strengthening the parser?s ability to identify Qtargets.  
We are disappointed in the performance of the 500 QA patterns.
Analysis suggests that we had too few patterns, and the ones we
had were too specific.  When patterns matched, they were rather
accurate, both in finding correct answers and more precisely
pinpointing the boundaries of answers.  However, they were
too sensitive to variations in phrasing.  Furthermore, it was
difficult to construct robust and accurate question and answer
phraseology patterns manually, for several reasons.  First,
manual construction relies on the inventiveness of the pattern
builder to foresee variations of phrasing, for both question and
answer.  It is however nearly impossible to think of all
possible variations when building patterns.  
Second, it is not always clear at what level of representation to
formulate the pattern: when should one specify using words?
Parts of speech? Other parse tree nodes? Semantic classes?  The
patterns in Figure 3 include only a few of these alternatives.
Specifying the wrong elements can result in non-optimal
coverage.  Third, the work is simply tedious.  We therefore
decided to try to learn QA patterns automatically.  
7. TOWARD LEARNING QA PATTERNS
AUTOMATICALLY
To learn corresponding question and answer expressions, we
pair up the parse trees of a question and (each one of) its
answer(s).  We then apply a set of matching criteria to identify
potential corresponding portions of the trees.  We then use the
EM algorithm to learn the strengths of correspondence
combinations at various levels of representation.  This work i s
still in progress.  
In order to learn this information we observe the truism that
there are many more answers than questions. This holds for the
two QA corpora we have access to?TREC and an FAQ website
(since discontinued).  We therefore use the familiar version of
the Noisy Channel Model and Bayes? Rule.   For each basic QA
type (Location, Why-Famous, etc.):
Table 2. Question parse tree and Qtarget accuracies.
# Penn # Question Crossing Qtarget Qtarget
Treebank sentences Labeled Labele d Tagging brackets accuracy accuracy
sentences added Precision Recall Accuracy (/ sent) (strict) (lenient)
2000 0 83.47% 82.49% 94.65% 0.34 63.00% 65.50%
3000 0 84.74% 84.16% 94.51% 0.35 65.30% 67.40%
2000 38 91.20% 89.37% 97.63% 0.26 85.90% 87.20%
3000 38 91.52% 90.09% 97.29% 0.26 86.40% 87.80%
2000 975 95.71% 95.45% 98.83% 0.17 96.10% 97.30%
Date Number
Qs
IR
hits
Ranker
hits
QA
pattern
Qtgt
match
Qword
fallback
Window
fallback
Total
2-Jul 52 1.00 0.61 0.12 0.49 0.15 0.19 0.62
8-Jul 38 0.89 0.40 0.28 0.40 0.12 n/a 0.53
13-Jul 52 1.00 0.61 0.04 0.48 0.15 0.22 0.53
3-Aug 55 n/a n/a 0.04 0.32 0.15 0.19 0.41
Table 3. Relative performance of Webclopedia modules on training corpus.
P(A|Q)  =  argmax P(Q|A) . P(A)
P(A)  =   ?all trees (# nodes that may express a true A) 
/  (number of nodes in tree)
P(Q|A)  =  ?all QA tree pairs (number of covarying nodes 
in Q and A trees)
/ (number of nodes in A tree)
As usual, many variations are possible, including how to
determine likelihood of expressing a true answer; whether to
consider all nodes or just certain major syntactic ones (N, NP,
VP, etc.); which information within each node to consider
(syntactic? semantic? lexical?); how to define ?covarying
information??node identity? individual slot value equality?;
what to do about the actual answer node in the A trees; if (and
how) to represent the relationships among A nodes that have
been found to be important; etc.  Figure 4 provides an answer
parse tree that indicates likely Location nodes, determined by
appropriate syntactic class, semantic type, and syntactic role
in the sentence.  
Our initial model focuses on bags of corresponding QA parse
tree nodes, and will help to indicate for a given question what
type of node(s) will contain the answer.  We plan to extend this
model to capture structured configurations of nodes that, when
matched to a question, will help indicate where in the parse tree
of a potential answer sentence the answer actually lies.  Such
bags or structures of nodes correspond, at the surface level, to
important phrases or words.  However, by using CONTEX
output we abstract away from the surface level, and learn to
include whatever syntactic and/or semantic information is best
suited for predicting likely answers.
8. REFERENCES
[1] Bikel, D., R. Schwartz, and R. Weischedel.  1999.  An
Algorithm that Learns What s in a Name.  Machine
Learning Special Issue on NL Learning, 34, 1?3.
[2] Choi, F.Y.Y. 2000. Advances in independent linear text
segmentation. Proceedings of the 1st Conference of the
North American Chapter of the Association for
Computational Linguistics (NAACL-00), 26?33.
[3] Fellbaum, Ch. (ed). 1998. WordNet: An Electronic Lexical
Database. Cambridge: MIT Press.
[4] Gerber, L. 2001.  A QA Typology for Webclopedia. In prep.
[5] Hearst, M.A. 1994. Multi-Paragraph Segmentation of
Expository Text.  Proceedings of the Annual Conference
of the Association for Computational Linguistics (ACL-
94).
[6] Hermjakob, U. 1997. Learning Parse and Translation
Decisions from Examples with Rich Context.  Ph.D.
dissertation, University of Texas at Austin.
file://ftp.cs.utexas.edu/pub/ mooney/papers/hermjakob-
dissertation-97.ps.gz.
[7] Hermjakob, U.  2000. Rapid Parser Development: A
Machine Learning Approach for Korean. Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-2000).
http://www.isi.edu/~ulf/papers/kor_naacl00.ps.gz.
[8] Hermjakob, U. 2001. Parsing and Question Classification
for Question Answering. In prep.
[9] Hovy, E.H., L. Gerber, U. Hermjakob, M. Junk, and C.-Y.
Lin. 2000. Question Answering in Webclopedia.
Proceedings of the TREC-9 Conference.  NIST.
Gaithersburg, MD.
[10] Moldovan, D., S. Harabagiu, M. Pasca, R. Mihalcea,, R.
Girju, R. Goodrum, and V. Rus. 2000. The Structure and
Performance of an Open-Domain Question Answering
System. Proceedings of the Conference of the Association
for Computational Linguistics (ACL-2000), 563?570.
[11] Srihari, R. and W. Li. 2000. A Question Answering System
Supported by Information Extraction. In Proceedings of
the 1st Conference of the North American Chapter of the
Association for Computational Linguistics (ANLP-
NAACL-00), 166?172.
[12] Witten, I.H., A. Moffat, and T.C. Bell. 1994. Managing
Gigabytes: Compressing and Indexing Documents and
Images. New York: Van Nostrand Reinhold.
SU
RF
  L
ux
or
 is
 fa
m
ed
 fo
r i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
.  
CA
T 
S-
SN
T 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(S
UB
J) 
SC
O
RE
 4
 
SU
RF
  i
s  
CA
T 
S-
A
U
X
 
CL
A
SS
 I-
EV
-B
E 
CL
A
SS
ES
 (I
-E
V-
BE
) 
LE
X
  b
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
JP
 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(C
OM
PL
) 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 0
 
SU
RF
  f
or
 it
s V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  .
  
CA
T 
D
-P
ER
IO
D
 
LE
X
  .
  
R
O
LE
S 
(D
UM
M
Y)
 
SC
O
RE
 0
 
SU
RF
  L
ux
or
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-L
U
X
O
R 
CL
A
SS
ES
 (I
-E
N-
LU
XO
R 
I-E
N-
CI
TY
 I-
EN
-P
LA
CE
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  L
ux
or
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  f
am
ed
  
CA
T 
S-
A
D
J 
CL
A
SS
 I-
EA
D
J-
FA
M
ED
 
CL
A
SS
ES
 (I
-E
AD
J-F
AM
ED
) 
LE
X
  f
am
ed
  
R
O
LE
S 
(P
RE
D)
 
G
RA
D
E 
U
N
G
RA
D
ED
 
SC
O
RE
 1
 
SU
RF
  f
or
  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-F
O
R 
CL
A
SS
ES
 (I
-E
P-
FO
R)
 
LE
X
  f
or
  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
 a
nd
 th
e 
K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  i
ts 
V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  a
nd
  
CA
T 
S-
CO
O
RD
-C
O
N
J 
CL
A
SS
 I-
EC
-A
N
D
 
CL
A
SS
ES
 (I
-E
C-
AN
D)
 
LE
X
  a
nd
  
R
O
LE
S 
(C
ON
J) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(C
OO
RD
) 
SC
O
RE
 2
 
SU
RF
  i
ts 
 
CA
T 
S-
PO
SS
-P
RO
N
 
CL
A
SS
 I-
EN
-P
O
SS
-P
RO
N
O
U
N
 
CL
A
SS
ES
 (I
-E
N-
PO
SS
-P
RO
NO
UN
) 
LE
X
  P
O
SS
-P
RO
N
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
 P
ha
ra
on
ic
 n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
of
 th
e 
K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
RO
PE
R-
O
RG
A
N
IZ
A
TI
O
N
 
CL
A
SS
ES
 (I
-E
N-
PR
OP
ER
-O
RG
AN
IZ
AT
IO
N 
I-E
N-
OR
GA
NI
ZA
TI
ON
 I-
EN
-A
GE
NT
 I-
EN
-P
RO
PE
R-
NA
M
ED
-E
NT
IT
Y)
 
LE
X
  V
al
le
y 
of
 th
e 
K
in
gs
  
R
O
LE
S 
(M
OD
) 
N
A
M
ED
-E
N
TI
TY
-U
N
IT
-P
 T
RU
E 
SC
O
RE
 4
 
SU
RF
  P
ha
ra
on
ic
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-P
H
A
RA
O
N
IC
 
CL
A
SS
ES
 (I
-E
N-
PH
AR
AO
NI
C)
 
LE
X
  P
ha
ra
on
ic
  
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  n
ec
ro
po
lis
  
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-N
EC
RO
PO
LI
S 
CL
A
SS
ES
 (I
-E
N-
NE
CR
OP
OL
IS
) 
LE
X
  n
ec
ro
po
lis
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 5
 
SU
RF
  o
f t
he
 K
in
gs
  
CA
T 
S-
PP
 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 3
 
SU
RF
  V
al
le
y 
 
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-V
A
LL
EY
 
CL
A
SS
ES
 (I
-E
N-
VA
LL
EY
 I-
EN
-P
LA
CE
) 
LE
X
  v
al
le
y 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  o
f  
CA
T 
S-
PR
EP
 
CL
A
SS
 I-
EP
-O
F 
CL
A
SS
ES
 (I
-E
P-
OF
) 
LE
X
  o
f  
R
O
LE
S 
(P
) 
SC
O
RE
 0
 
SU
RF
  t
he
 K
in
gs
  
CA
T 
S-
N
P 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
in
gs
  
CA
T 
S-
PR
O
PE
R-
N
A
M
E 
CL
A
SS
 I-
EN
-K
IN
G
-N
A
M
E 
CL
A
SS
ES
 (I
-E
N-
KI
NG
-N
AM
E 
I-E
N-
AG
EN
T)
 
LE
X
  K
in
g 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 3
 
SU
RF
  t
he
  
CA
T 
S-
D
EF
-A
RT
 
CL
A
SS
 I-
EA
RT
-D
EF
-A
RT
 
CL
A
SS
ES
 (I
-E
AR
T-
DE
F-
AR
T)
 
LE
X
  t
he
  
R
O
LE
S 
(D
ET
) 
SC
O
RE
 0
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
co
m
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
te
m
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  c
om
pl
ex
  
CA
T 
S-
CO
U
N
T-
N
O
U
N
 
CL
A
SS
 I-
EN
-C
O
M
PL
EX
 
CL
A
SS
ES
 (I
-E
N-
CO
M
PL
EX
) 
LE
X
  c
om
pl
ex
  
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
SU
RF
  K
ar
na
k 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-K
A
RN
A
K
 
CL
A
SS
ES
 (I
-E
N-
KA
RN
AK
) 
LE
X
  k
ar
na
k 
 
R
O
LE
S 
(M
OD
) 
SC
O
RE
 1
 
SU
RF
  t
em
pl
e 
 
CA
T 
S-
N
O
U
N
 
CL
A
SS
 I-
EN
-T
EM
PL
E 
CL
A
SS
ES
 (I
-E
N-
TE
M
PL
E)
 
LE
X
  t
em
pl
e 
 
R
O
LE
S 
(P
RE
D)
 
SC
O
RE
 1
 
Figure 4. Candidate answer tree showing likely Location answers.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 596?603, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Handling Biographical Questions with Implicature 
 
 
Donghui Feng Eduard Hovy 
Information Sciences Institute Information Sciences Institute 
University of Southern California University of Southern California 
Marina del Rey, CA, 90292 Marina del Rey, CA, 90292 
donghui@isi.edu hovy@isi.edu 
 
 
 
 
Abstract 
Traditional question answering systems 
adopt the following framework: parsing 
questions, searching for relevant docu-
ments, and identifying/generating an-
swers. However, this framework does not 
work well for questions with hidden as-
sumptions and implicatures. In this paper, 
we describe a novel idea, a cascading 
guidance strategy, which can not only 
identify potential traps in questions but 
further guide the answer extraction pro-
cedure by recognizing whether there are 
multiple answers for a question. This is 
the first attempt to solve implicature prob-
lem for complex QA in a cascading fash-
ion using N-gram language models as 
features. We here investigate questions 
with implicatures related to biography 
facts in a web-based QA system, Power-
Bio. We compare the performances of 
Decision Tree, Na?ve Bayes, SVM (Sup-
port Vector Machine), and ME (Maxi-
mum Entropy) classification methods. 
The integration of the cascading guidance 
strategy can help extract answers for 
questions with implicatures and produce 
satisfactory results in our experiments. 
1 Motivation 
Question Answering has emerged as a key area in 
natural language processing (NLP) to apply ques-
tion parsing, information extraction, summariza-
tion, and language generation techniques (Clark et 
al., 2004; Fleischman et al, 2003; Echihabi et al, 
2003; Yang et al, 2003; Hermjakob et al, 2002; 
Dumais et al, 2002). Traditional question answer-
ing systems adopt the framework of parsing ques-
tions, searching for relevant documents, and then 
pinpointing and generating answers. However, this 
framework includes potential dangers. For exam-
ple, to answer the question ?when did Beethoven 
get married??, a typical QA system would identify 
the question target to be a ?Date? and would apply 
techniques to identify the date Beethoven got mar-
ried. Since Beethoven never married, this direct 
approach is likely to deliver wrong answers. The 
trick in the question is the implicature that Beetho-
ven got married. In the main task of QA track of 
TREC 2003, the performances of most systems on 
providing ?NIL? when no answer is possible range 
from only 10% to 30% (Voorhees, 2003). 
Just as some questions have no answer, others 
may have multiple answers. For instance, with 
?who was Ronald Reagan?s wife??, a QA system 
may give only ?Nancy Davis? as the answer. How-
ever, there is another correct answer: Jane Wyman. 
The problem here is the implicature in the question 
that Reagan only got married once. 
An implicature is anything that is inferred from 
an utterance but that is not a condition for the truth 
of the utterance (Gazdar, 1979; Levinson, 1983). 
Implicatures in questions either waste computa-
tional effort or impair the performance of a QA 
system or both. Therefore, when answering ques-
tions, it is prudent to identify the questions with 
implicatures before processing starts.  
In this paper, we describe a novel idea to solve 
the problem: a strategy of cascading guidance. This 
is the first attempt to solve implicature problem for 
complex QA in a cascading fashion using N-gram 
596
language models as features. The cascading guid-
ance part is designed to be inserted immediately 
before the search procedure to handle questions 
with implicatures. It can not only first identify the 
potential ?no answer? traps but also identify 
whether multiple answers for this question are 
likely.  
To investigate the performance of the cascading 
guidance strategy, we here study two types of 
questions related to biography facts in a web-based 
biography QA system, PowerBio. This web-based 
QA system extracts biographical facts from the 
web obtained by querying a web search engine 
(Google in our case).  Figure 1 provides the two 
types of questions we selected, which we refer to 
as SPOUSE_QUESTION and CHIL-
D_QUESTION.  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1. SPOUSE_QUESTION and 
CHILD_QUESTION 
 
Both types of questions have implicatures to jus-
tify the use of the cascading guidance strategy. In-
tuitively, to answer these questions, we have two 
issues related to implicatures to clarify:  
 
? Does the person have a spouse/child?  
? What's the number of answers for this ques-
tion? (One or many?) 
 
We therefore create two successive classifica-
tion engines in the cascading classifier.  
For learning, our approach queries the search 
engine with every person listed in the training set, 
extracts related features from the documents, and 
trains the cascading classifiers. For application, 
when a new question is given, the cascading classi-
fier is applied before activation of the search sub-
system. We compare the performances of four 
popular classification approaches in the cascading 
classifier, namely Decision Tree, Na?ve Bayes, 
SVM (Support Vector Machine), and ME (Maxi-
mum Entropy) classifications. 
The paper is structured as follows: related work 
is discussed in Section 2. We introduce our cascad-
ing guidance technique in Section 3, including De-
cision Tree, Na?ve Bayes and SVM (Support 
Vector Machine) and ME (Maximum Entropy) 
classifications. The experimental results are pre-
sented in Section 4. We discuss related issues and 
future work in Section 5.  
2 Related Work 
Question Answering has attracted much attention 
from the areas of Natural Language Processing, 
Information Retrieval and Data Mining (Fleisch-
man et al, 2003; Echihabi et al, 2003; Yang et al, 
2003; Hermjakob et al, 2002; Dumais et al, 2002; 
Hermjakob et al, 2000). It is tested in several ven-
ues, including the TREC and CLEF Question An-
swering tracks (Voorhees, 2003; Magnini et al, 
2003). Most research efforts in the Question An-
swering community have focused on factoid ques-
tions and successful Question Answering systems 
tend to have similar underlying pipelines structures 
(Prager et al, 2004; Xu et al, 2003; Hovy et al, 
2000; Moldovan et al, 2000). 
Recently more techniques for answer extraction, 
answer selection, and answer validation have been 
proposed (Lita et al, 2004; Soricut and Brill, 2004; 
Clark et al, 2004).  
Prager et al (2004) proposed applying constraint 
satisfaction obtained by asking auxiliary questions 
to improve system performance. This approach 
requires the creation of auxiliary questions, which 
may be complex to automate. 
Ravichandran and Hovy (2002) proposed auto-
matically learning surface text patterns for answer 
extraction. However, this approach will not work if 
no explicit answers exist in the source. The first 
reason is that in that situation the anchors to learn 
the patterns cannot be determined. Secondly, most 
of the facts without explicit values are not ex-
pressed with long patterns including anchors. For 
example, the phrase ?the childless marriage? gives 
enough information that a person has no child. But 
it is almost impossible to learn such surface text 
patterns following (Ravichandran and Hovy, 2002). 
Reported work on question processing focuses 
mainly on the problems of parsing questions, de-
termining the question target for search subsystem 
I. SPOUSE_QUESTION 
    E.g. Who is <PERSON>?s wife?      
           Who is <PERSON>?s husband? 
           Whom did <PERSON> marry? 
            ? 
II. CHILD_QUESTION 
    E.g. Who is <PERSON>?s son?      
           Who is <PERSON>?s daughter? 
           Who is <PERSON>?s child? 
?
597
(Pasca and Harabagiu, 2001; Hermjakob et al, 
2000). Saquete et al (2004) decompose complex 
temporal questions into simpler ones based on the 
temporal relationships in the question. 
To date, there has been little published work on 
handling implicatures in questions. Just-In-Time 
Information Seeking Agents (JITISA) was pro-
posed by Harabagiu (2001) to process questions in 
dialogue and implicatures. The agents are created 
based on pragmatic knowledge. Traditional answer 
extraction and answer fusion approaches assume 
the question is always correct and explicit answers 
do exist in the corpus. Reported work attempts to 
rank the candidate answer list to boost the correct 
one into top position. This is not enough when 
there may not be an answer for the question posed.  
For biographical fact extraction and generation, 
Zhou et al (2004) and Schiffman et al (2001) use 
summarization techniques to generate human biog-
raphies. Mann and Yarowsky (2005) propose fus-
ing the extracted information across documents to 
return a consensus answer. In their approach, they 
did not consider multiple values or no values for 
biography facts, although multiple facts are com-
mon for some biography attributes, such as multi-
ple occupations, children, books, places of 
residence, etc. In these cases a consensus answer is 
not adequate. 
Our work differs from theirs because we are not 
only working on information/answer extraction; 
the focus in this paper is the guidance for answer 
extraction of questions (or IE task for values) with 
implicatures. This work can be of great help for 
immediate biographical information extraction. 
We describe details of the cascading guidance 
technique and investigate how it will help for ques-
tion answering in Section 3.  
3 Cascading Guidance Technique 
We turn to the Web by querying a web search en-
gine (Google in our case) to find evidence to create 
guidance for answer extraction. 
3.1 Classification Procedure 
The cascading classifier is applied after the name 
of the person and the answer types are identified. 
Figure 2 gives the pipeline of the classification 
procedure. 
With the identified person name, we query the 
search engine (Google) to obtain the top N web 
pages/documents. A simple data cleaning program 
only keeps the content texts in the web page, which 
is broken up into separate sentences. Following 
that, topic sentences are identified with the key-
word topic identification technique. For each topic 
we provide a list of possible related keywords and 
any sentences containing both the person?s name 
(or reference) and at least one of the keywords will 
be selected. The required features are extracted 
from the topic sentences and passed to the cascad-
ing classifier as supporting evidence to generate 
guidance for answer extraction. 
 
Figure 2. Procedure of Cascading Classifier 
3.2 Feature Extraction 
Intuitively, sentences elaborating a biographical 
fact in a given topic should have similar styles 
(short patterns) of organizing words and phrases. 
Here, topic means an aspect of biographical facts, 
e.g., marriage, children, birthplace, and so on. In-
spired by this, we consider taking N-grams in sen-
tences as our features. However, N-gram features 
not closely related to the topic will bring more 
noise into the system. Therefore, we only take the 
N-grams within a fixed-length window around the 
topic keywords for features calculation, and pass 
them as evidence to cascading classifier.  
Classification Results 
Search EnginePerson 
Name 
Web 
Pages 
Data Cleaner
Sentence breaker 
Cascading 
Classifier  
Clean 
Topic  
Sentences
Topic  
Identification
Feature  
Extraction 
598
For N-grams, instead of using the multiplication 
of conditional probabilities of each word in the N-
gram, we only consider the last conditional prob-
ability (see below). The reason is that the last con-
ditional probability is a strong sign of the pattern?s 
importance and how this sequence of words is or-
ganized. Simply multiplying all the conditional 
probabilities will decrease the value and require 
normalization. Realizing that in a set of documents 
the frequency of each N-gram is very important 
information, we combine the last conditional prob-
ability with the frequency. 
The computation for each feature of unigram, 
bigram and trigram are defined as the following 
formulas:  
)(*)( iiunigram wfreqwpf =                             (1) 
),(*)|( 11 iiiibigram wwfreqwwpf ??=             (2) 
),,(*),|( 1212 iiiiiitrigram wwwfreqwwwpf ????=     
                                                                           (3) 
We here investigate four kinds of classifiers, 
namely Decision Tree, Na?ve Bayes, Support Vec-
tor Machine (SVM), and Maximum Entropy (ME).  
3.3 Classification Approaches 
The cascading classifier is composed of two suc-
cessive parts. Given the set of extracted features, 
the classification result could lead to different re-
sponses to the question, either answering with ?no 
value? with strong confidence or directing the an-
swer extraction model how many answers should 
be sought. 
For text classification, there are several well-
studied classifiers in the machine learning and 
natural language processing communities.  
 
Decision Tree Classification 
The Decision Tree classifier is simple and matches 
human intuitions perfectly while it has been proved 
efficient in many application systems. The basic 
idea is to break up the classification decision into a 
union of a set of simpler decisions based on N-
gram features. Due to the large feature set, we use 
C5.0, the decision tree software package developed 
by RuleQuest Research (Quinlan, 1993), instead of 
C4.5. 
 
Na?ve Bayes Classification 
The Na?ve Bayes classifier utilizes Bayes' rule as 
follows. Supposing we have the feature 
set { }nfffF ,...,, 21= , the probability that person 
p belongs to a class c is given as: 
)|'(maxarg
'
FcPc
c
=     (4) 
Based on Bayes? rule, we have 
)'()'|(maxarg
)(
)'()'|(
maxarg
)|'(maxarg
'
'
'
cPcFP
FP
cPcFP
FcPc
c
c
c
=
=
=
    (5) 
This was used for both successive classifiers of the 
cascading engine. 
 
SVM Classification 
SVM (Support Vector Machines) has attracted 
much attention since it was introduced in (Boser et 
al., 1992). As a special and effective approach for 
kernel based methods, SVM creates non-linear 
classifiers by applying the kernel trick to maxi-
mum-margin hyperplanes.  
Suppose nipi ,...,1, =  represent the training set 
of persons, and the classes for classifications are 
},{ 21 ccC = (for simplicity, we represent the 
classes with { }1,1?=C ). Then the classification 
task requires the solution of the following optimi-
zation problem (Hsu et al, 2003): 
0
1))((
2
1
min
1
,,
?
??+
+ ?
=
i
ii
T
i
n
i
i
T
b
bpctosubject
M
?
???
?????
    (6) 
We use the SVM classification package 
LIBSVM (Chang and Lin, 2001) in our problem. 
 
ME Classification 
ME (Maximum Entropy) classification is used here 
to directly estimate the posterior probability for 
classification. 
Suppose p represents the person and the classes 
for classifications are { }21,ccC = , we have M fea-
ture functions Mmpchm ,...,1),,( = . For each fea-
ture function, we have a model 
parameter Mmm ,...,1, =? . The classification with 
599
maximum likelihood estimation can be defined as 
follows (Och and Ney, 2002): 
? ?
?
=
==
=
'
1
]),(exp[
]),(exp[
)|()|(
1
'
1
c
M
m
mm
M
m
mm
pch
pch
pcppcP M
?
?
?
    (7) 
The decision rule to choose the most probable 
class is (Och and Ney, 2002): { }
??
?
??
?=
=
?
=
M
m
mm
c
c
pch
pcPc
1
),(maxarg
)|(maxarg?
?            (8) 
We use the published package YASMET 1  to 
conduct parameters training and classification. 
YASMET requires supervised learning for the 
training of maximum entropy model. 
The four classification approaches are assem-
bled in a cascading fashion. We discuss their per-
formance next. 
4 Experiments and Results 
4.1 Experimental Setup 
We download from infoplease.com 2  and biogra-
phy.com 3  two corpora of people?s biographies, 
which include 24,975 and 24,345 bios respectively. 
We scan each whole corpus and extract people 
having spouse information. To create the data set, 
we manually check and categorize each person as 
having multiple spouses, only one spouse, or no 
spouse. Similarly, we obtained another list of per-
sons having multiple children, only one child, and 
no child. The sizes of data extracted are given in 
Table 1.  
 
Type Child Spouse 
No_value 25 20 
One_value 35 32 
Multiple_values 107 43 
Table 1. Extracted experimental data 
 
For the cascading classification, in the first step, 
when classifying whether a person has a 
spouse/child or not, we merge the last two subsets 
                                                          
1 http://www.fjoch.com/YASMET.html 
2 http://www.infoplease.com/people.html 
3 http://www.biography.com/search/index.jsp 
with one value and multiple values into one. Table 
2 presents the data used for each level of classifica-
tion. 
 
 class Child Spouse
No_value 25 20 First-level 
Classification With_value 142 75 
One_value 35 32 Second-level 
Classification Multiple_value 107 43 
Table 2. Data set used for classification 
To investigate the performances of our cascad-
ing classifiers, we divided the two sets into training 
set and testing set, with half of them in the training 
set and half in the testing set. 
4.2 Empirical Results 
For each situation of the two questions, when the 
answer type has been determined to be the child or 
spouse of a person, we send the person?s name to 
Google and collect the top N documents. As de-
scribed in Figure 2, topic sentences in each docu-
ment are selected by keyword matching. A window 
with the length of w is applied to the sentence. All 
word sequences in the window are selected for fea-
ture calculation. We take all the three N-gram lan-
guage models (unigram, bigram, and trigram) in 
the window for feature computation. Table 3 gives 
the sizes of the bigram feature sets for first-level 
classification as we take more and more documents 
into the system. 
 
Top N Docs Child Spouse 
1 3468 1958 
10 27733 12325 
20 46431 27331 
30 61057 36637 
40 76687 43771 
50 87020 50868 
60 96393 61632 
70 108053 67712 
80 118947 73306 
90 130526 77370 
100 139722 82339 
Table 3. Sizes of feature sets 
 
As described in Section 3, the feature values are 
applied in the classifiers. Tables 4 and 5 give the 
best performances of the 4 classifiers in the two 
situations when we select the top N articles using 
N-gram probability for feature computation. 
Due to the large size of the feature set, C5.0, 
SVM, and ME packages will not work at some 
600
point as more documents are encountered. The Na-
?ve Bayes classification is more scalable as we use 
intermediate file to store probability tables. 
 
Precision First-level 
Classification 
Second-level 
Classification
C5.0 82.90% 65.70% 
Na?ve 
Bayes 
87.80% 72.86% 
SVM 84.15% 75.71% 
ME 86.59% 75.71% 
Table 4. Precision scores for child classification 
 
 
Precision First-level 
Classification 
Second-level 
Classification
C5.0 80.90% 56.80% 
Na?ve 
Bayes 
83.00% 
 
59.46% 
SVM 78.72% 54.05% 
ME 78.72% 51.35% 
Table 5. Precision scores for spouse classification 
 
 
Feature # of times  
identified  
(out of 75) 
p(wi|wi-2,wi-1) 
and his wife 35  0.6786 
her husband , 33 0.3082 
and her husband 26   0.5476 
was married to 20 0.8621 
with his wife 14   0.875 
her second husband   13 0.6667 
her marriage to 13 0.5 
ex - wife           12 0.3333 
ex - husband 11    0.6667 
her first husband     10  0.75 
second husband ,     10       1 
his first wife 8 0.3333 
first husband ,  7        0.6667 
second wife ,   7 0.3333 
his first marriage     5        0.1667 
s second wife 5 0.75 
Table 6. Example trigram features for second-level 
classification for Spouse (one or multiple values) 
 
The feature set has a large number of features. 
However, not all of them will be used for each per-
son. We studied the number of times features are 
identified/used in the training and testing sets and 
their probabilities. Table 6 presents a list of some 
trigram features for second-level classification 
(one or multiple values) for Spouse. Obviously, 
indicating features have a large probability as ex-
pected. The second column gives the number of 
times the feature is used out of the training and 
testing set (75 persons in total). 
 
Will more complex N-gram features work bet-
ter? 
Intuitively, being less ambiguous, more complex 
N-gram features carry more precise information 
and therefore should work better than simple ones. 
We studied the performances for different N-gram 
language model features. Below are the results of 
Na?ve Bayes first-level classification for Child, 
using different N-gram features. 
 
 
Top N 
Docs 
Unigram Bigram Trigram 
1 34.78% 54.35% 67.39% 
10 30.48% 79.27% 86.59% 
20 26.83% 82.93% 85.37% 
30 24.39% 81.71% 86.59% 
Table 7. Comparisons of classification precisions 
using different N-gram features for child 
 
From Table 7, we can infer that bigram features 
work better than unigram features, and trigram fea-
tures work better than bigrams when we select dif-
ferent numbers of top N documents. Trigram 
features actually bring enough evidence in classifi-
cation. However, when we investigated 4-grams 
language features in the collected data, most of 
them are very sparse in the feature space of all the 
cases. Applying 4-grams or higher may not help in 
our task. 
 
Will more data/documents help? 
The performance of corpus-based statistical ap-
proaches usually depends on the size of corpus. A 
traditional view for most NLP problems is that 
more data will help to improve the system?s per-
formance. However, for data collected from a 
search engine, this may not be the case, since web 
data is usually ambiguous and noisy. We therefore 
investigate the data size?s effect on system per-
formance. Figure 3 gives the precision curves of 
the Na?ve Bayes classifier for the first-level classi-
fication for Child. 
Except for the case of top 1, where the top 
document alone may not contain too much useful 
information on selected topics, precision scores 
only have slight variations for increasing numbers 
of documents. For bigram features, over the top 50 
601
through top 70 documents, the precision scores 
even get a little worse.  
 
Performances on Top N Docs
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 10 20 30 40 50 60 70 80 90 100
Top N
Precision
Bigram
Trigram
 
Figure 3. Performance on top N documents 
4.3 Examples 
Equipped with the cascading guiding strategy, we 
are able to handle questions containing implica-
tures. In our system, when we can determine the 
answer type is child or spouse, the cascading guid-
ing system will help the answer extraction part to 
extract answers from the designated corpus. Figure 
4 gives two examples of the strategy.  
 
Figure 4. Classification Example for question 
 
For the first question, the classifier recognizes 
there is no spouse for the target person and returns 
information for the answer generation. The fea-
tures used here are the first-level classification re-
sult for SPOUSE_QUESTION. For the second 
question, the classifier recognizes the target person 
has a child first, followed by recognizing that the 
answer has multiple values. In this way, the strat-
egy integrated to the question answering system 
can improve the system?s performance by handling 
questions with implicatures. 
5 Discussion and Future Work 
Questions may have implicatures due to the flexi-
bility of human language and conversation. In real 
question-answering systems, failure to handle them 
may either waste huge computation cost or impair 
system?s performance. The traditional QA frame-
work does not work well for questions containing 
implicatures. We describe a novel idea in this pa-
per to identify potential traps in biographical ques-
tions and recognize whether there are multiple 
answers for a question. 
Question-Answering systems, even when fo-
cused upon biographies, have to handle many facts, 
such as birth date, birth place, parents, training, 
accomplishments, etc. These values can be ex-
tracted using typical text harvesting approaches. 
However, when there are no values for some bio-
graphical information, the task becomes much 
more difficult because text seldom explicitly states 
a negative. For example, the following two ques-
tions require schools attended:  
 
? Where did <person> graduate from? 
? What university did <person> attend? 
 
Our program scanned the two corpora of bios 
and found only 2 out 49320 bios explicitly stating 
that the subject never attended any school. There-
fore, for some types of information, it will be much 
harder to identify null values through evidence 
from text. Some more complicated reasoning and 
inference may be required. Classifiers for some 
biographical facts may need to incorporate extra 
knowledge from other resources. The inherent rela-
tions between biography facts can also be used to 
validate each other. For example, the relations of 
marriage and child, birth place and childhood 
home, etc. may provide clues for cross-validation. 
We plan to investigate these problems in the future.  
Acknowledgements 
We wish to thank the anonymous reviewers for 
their helpful feedback and corrections. Also we 
thank Lei Ding, Feng Pan, and Deepak Ravi-
chandran for their valuable comments on this work.  
 
References  
Boser, B.E., Guyon, I. and Vapnik, V. 1992. A training 
algorithm for optimal margin classifiers. Proceedings 
of the ACM COLT 1992. 
Chang, C. and Lin, C. 2001. LIBSVM -- A library for 
support vector machines. Software available at 
http://www.csie.ntu.edu.tw/~cjlin/libsvm/ 
Q1: Who is Sophia Smith?s spouse? 
 Classified: <NO_SPOUSE> 
 Answer: She did not marry. 
 
Q2: Who is John Ritter?s child?  
 Classified: <HAVING_CHILD> 
 Classified: <MULTIPLE_VALUES> 
 ? 
602
Chu-Carroll, J., Czuba, K., Prager, J., and Ittycheriah, 
A. 2003. In question answering, two heads are better 
than one. Proceedings of HLT-NAACL-2003. 
Clark, S., Steedman, M. and Curran, J.R. 2004. Object-
extraction and question-parsing using CCG. Proceed-
ings EMNLP-2004, pages 111-118, Barcelona, Spain. 
Dumais, S., Banko, M., Brill, E., Lin, J., and Ng, A. 
2002. Web question answering: is more always bet-
ter? Proceedings of SIGIR-2002.  
Echihabi, A. and Marcu, D. 2003. A noisy channel ap-
proach to question answering. Proceedings of ACL-
2003. 
Fleischman, M., Hovy, E.H., and Echihabi, A. 2003. 
Offline strategies for online question answering: an-
swering questions before they are asked. Proceedings 
of ACL-2003. 
Gazdar, G. 1979. Pragmatics: Implicature, presupposi-
tion, and logical form. New York: Academic Press. 
Harabagiu, S. 2001. Just-In-Time Question Answering. 
Invited talk in Proceedings of the Sixth Natural Lan-
guage Processing Pacific Rim Symposium 2001. 
Hermjakob, U., Echihabi, A., and Marcu, D. 2002. 
Natural language based reformulation resource and 
web exploitation for question answering. Proceed-
ings of TREC-2002. 
Hermjakob, U., Hovy, E.H., and Lin, C. 2000. Knowl-
edge-based question answering. TREC-2000. 
Hovy, E.H., Gerber, L., Hermjakob, U., Junk, M., and 
Lin, C. 2000. Question answering in Webclopedia. 
Proceedings of TREC-2000. 
Hovy, E.H., Hermjakob, U., Lin, C., and Ravichandran, 
D. 2002. Using knowledge to facilitate factoid an-
swer pinpointing. Proceedings of COLING-2002. 
Hsu, C.-W., Chang, C.-C., and Lin, C.-J. 2003. A Prac-
tical Guide to Support Vector Classification. Avail-
able at: 
http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.
pdf.  
Levinson, S. 1983. Pragmatics. Cambridge University 
Press. 
Lita L.V. and Carbonell, J. 2004. Instance-based ques-
tion answering: a data driven approach. Proceedings 
of EMNLP 2004. 
Magnini, B., Romagnoli, S., Vallin, A., Herrera, J., Pe-
?as, A., Peinado, V., Verdejo, F., Rijke, M. 2003. 
The Multiple Language Question Answering Track at 
CLEF 2003. CLEF 2003: 471-486. 
Mann, G. and Yarowsky, D. 2005. Multi-field informa-
tion extraction and cross-document fusion. Proceed-
ings of ACL-2005. 
Moldovan, D., Clark, D., Harabagiu, S., and Maiorano, 
S. 2003. Cogex: A logic prover for question answer-
ing. Proceedings of ACL-2003. 
Moldovan, D., Harabagiu, S., Pasca, M., Mihalcea, R., 
Girju, R., Goodrum, R., and Rus, V. 2000. The struc-
ture and performance of an open-domain question 
answering system. Proceedings of ACL-2000. 
Nyberg, E. et al 2003. A multi strategy approach with 
dynamic planning. Proceedings of TREC-2003. 
Och, F. J.and Ney, H. 2002. Discriminative training and 
maximum entropy models for statistical machine 
translation. Proceedings of ACL 2002 pp. 295-302. 
Pasca, M. and Harabagiu, S. 2001. High Performance 
Question/Answering. Proceedings of SIGIR-2001.  
Prager, J. M., Chu-Carroll, J., and Czuba, K.W.. 2004. 
Question answering using constraint satisfaction. 
Proceedings of the 42nd Meeting of the Association 
for Computational Linguistics (ACL'04). 
Quinlan, J. R. 1993. C4.5: Programs for machine learn-
ing. Morgan Kaufmann, San Mateo, CA, 1993. 
Ravichandran, D. and Hovy, E.H. 2002. Learning Sur-
face Text Patterns for a Question Answering System. 
Proceedings of ACL-2002. 
Saquete, E., Mart?nez-Barco, P., Mu?oz, R., and Vicedo, 
J.L. 2004. Splitting complex temporal questions for 
question answering systems. Proceedings of ACL'04. 
Schiffman, B., Mani, I., and Concepcion, K.J. 2001. 
Producing biographical summaries: combining lin-
guistic knowledge with corpus statistics. Proceedings 
of ACL/EACL-2001. 
Soricut, R. and Brill, E. 2004. Automatic question an-
swering: beyond the factoid. Proceedings of 
HLT/NAACL-2004, Boston, MA. 
Voorhees, E.M. 2003. Overview of the trec 2003 ques-
tion answering track. Proceedings of TREC-2003. 
Xu, J., Licuanan, A., Weischedel, R. 2003. TREC 2003 
QA at BBN: Answering Definitional Questions. Pro-
ceedings of TREC 2003. 
Yang, H., Chua, T.S., Wang, S., and Koh, C.K. 2003. 
Structured use of external knowledge for eventbased 
open domain question answering. Proceedings of 
SIGIR-2003. 
Zhou, L., Ticrea, M., and Hovy, E.H. 2004. Multi-
document biography summarization. Proceedings of 
EMNLP-2004. 
603
Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 4?5,
Vancouver, October 2005.
Classummary:
Introducing Discussion Summarization to Online Classrooms
Liang Zhou, Erin Shaw, Chin-Yew Lin, and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, shaw, hovy}@isi.edu
Abstract
This paper describes a novel summariza-
tion system, Classummary, for interactive
online classroom discussions. This system
is originally designed for Open Source
Software (OSS) development forums.
However, this new application provides
valuable feedback on designing summari-
zation systems and applying them to eve-
ryday use, in addition to the traditional
natural language processing evaluation
methods. In our demonstration at HLT,
new users will be able to direct this sum-
marizer themselves.
1 Introduction
The availability of many chat forums reflects the
formation of globally dispersed virtual communi-
ties, one of which is the very active and growing
movement of Open Source Software (OSS) devel-
opment. Working together in a virtual community
in non-collocated environments, OSS developers
communicate and collaborate using a wide range
of web-based tools including Internet Relay Chat
(IRC), electronic mailing lists, and more.
Another similarly active virtual community is
the distributed education community. Whether
courses are held entirely online or mostly on-
campus, online asynchronous discussion boards
play an increasingly important role, enabling class-
room-like communication and collaboration
amongst students, tutors and instructors. The Uni-
versity of Southern California, like many other
universities, employs a commercial online course
management system (CMS).  In an effort to bridge
research and practice in education, researchers at
ISI replaced the native CMS discussion board with
an open source board that is currently used by se-
lected classes. The board provides a platform for
evaluating new teaching and learning technologies.
Within the discussion board teachers and students
post messages about course-related topics. The
discussions are organized chronologically within
topics and higher-level forums. These ?live? dis-
cussions are now enabling a new opportunity, the
opportunity to apply and evaluate advanced natural
language processing (NLP) technology.
Recently we designed a summarization system
for technical chats and emails on the Linux kernel
(Zhou and Hovy, 2005). It clusters discussions ac-
cording to subtopic structures on the sub-message
level, identifies immediate responding pairs using
machine-learning methods, and generates subtopic-
based mini-summaries for each chat log. Incorpo-
ration of this system into the ISI Discussion Board
framework, called Classummary, benefits both
distance learning and NLP communities. Summa-
ries are created periodically and sent to students
and teachers via their preferred medium (emails,
text messages on mobiles, web, etc). This relieves
users of the burden of reading through a large vol-
ume of messages before participating in a particu-
lar discussion. It also enables users to keep track of
all ongoing discussions without much effort. At the
same time, the discussion summarization system
can be measured beyond the typical NLP evalua-
4
tion methodologies, i.e. measures on content cov-
erage. Teachers and students? willingness and con-
tinuing interest in using the software will be a
concrete acknowledgement and vindication of such
research-based NLP tools. We anticipate a highly
informative survey to be returned by users at the
end of the service.
2  Summarization Framework
In this section, we will give a brief description of
the discussion summarization framework that is
applied to online classroom discussions.
One important component in the original system
(Zhou and Hovy, 2005) is the sub-message clus-
tering. The original chat logs are in-depth technical
discussions that often involve multiple sub-topics,
clustering is used to model this behavior. In Clas-
summary, the discussions are presented in an orga-
nized fashion where users only respond to and
comment on specific topics. Thus, it eliminates the
need for clustering.
 All messages in a discussion are related to the
central topic, but to varying degrees. Some are an-
swers to previously asked questions, some make
suggestions and give advice where they are re-
quested, etc. We can safely assume that for this
type of conversational interactions, the goal of the
participants is to seek help or advice and advance
their current knowledge on various course-related
subjects. This kind of interaction can be modeled
as one problem-initiating message and one or more
corresponding problem-solving messages, formally
defined as Adjacent Pairs (AP). A support vector
machine, pre-trained on lexical and structural fea-
tures for OSS discussions, is used to identify the
most relevant responding messages to the initial
post within a topic.
Having obtained all relevant responses, we
adopt the typical summarization paradigm to ex-
tract informative sentences to produce concise
summaries. This component is modeled after the
BE-based multi-document summarizer (Hovy et
al., 2005). It consists of three steps. First, impor-
tant basic elements (BEs) are identified according
to their likelihood ratio (LR). BEs are automati-
cally created minimal semantic units of the form
head-modifier-relation (for example, ?Libyans |
two | nn?, ?indicted | Libyans | obj?, and ?indicted
| bombing | for?). Next, each sentence is given a
score which is the sum of its BE scores, computed
in the first step, normalized by its length. Lastly,
taking into consideration the interactions among
summary sentences, a MMR (Maximum Marginal
Relevancy) model (Goldstein et al, 1999) is used
to extract sentences from the list of top-ranked
sentences computed from the second step.
3 Accessibility
Classummary is accessible to students and teachers
while classes are in session. At HLT, we will dem-
onstrate an equivalent web-based version. Discus-
sions are displayed on a per-topic basis; and
messages belonging to a specific discussion are
arranged in ascending order according to their
timestamps. While viewing a new message on a
topic, the user can choose to receive a summary of
the discussion so far or an overall summary on the
topic. Upon receiving the summary (for students,
at the end of an academic term), a list of questions
is presented to the user to gather comments on
whether Classummary is useful. We will show the
survey results from the classes (which will have
concluded by then) at the conference.
References
Hovy, E., C.Y. Lin, and L. Zhou. 2005. A BE-based
multi-document summarizer with sentence compres-
sion. To appear in Proceedings of Multilingual Sum-
marization Evaluation (ACL 2005), Ann Arbor, MI.
Goldstein, J., M. Kantrowitz, V. Mittal, and J. Car-
bonell. Summarizing Text Documents: Sentence Se-
lection and Evaluation Metrics. Proceedings of the
22nd International ACM Conference on Research and
Development in Information Retrieval (SIGIR-99),
Berkeley, CA, 121-128.
Zhou, L. and E. Hovy. 2005. Digesting virtual ?geek?
culture: The summarization of technical internet re-
lay chats. To appear in Proceedings of Association of
Computational Linguistics (ACL 2005), Ann Arbor,
MI.
5
Automatic Detection of Opinion Bearing Words and Sentences 
Soo-Min Kim and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{skim, hovy}@isi.edu 
Abstract 
We describe a sentence-level opinion 
detection system. We first define what 
an opinion means in our research and 
introduce an effective method for ob-
taining opinion-bearing and non-
opinion-bearing words. Then we de-
scribe recognizing opinion-bearing sen-
tences using these words We test the 
system on 3 different test sets: MPQA 
data, an internal corpus, and the TREC-
2003 Novelty track data. We show that 
our automatic method for obtaining 
opinion-bearing words can be used ef-
fectively to identify opinion-bearing 
sentences. 
1 Introduction 
Sophisticated language processing in recent 
years has made possible increasingly complex 
challenges for text analysis. One such challenge 
is recognizing, classifying, and understanding 
opinionated text. This ability is desirable for 
various tasks, including filtering advertisements, 
separating the arguments in online debate or 
discussions, and ranking web documents cited as 
authorities on contentious topics.   
The challenge is made very difficult by a 
general inability to define opinion. Our prelimi-
nary reading of a small selection of the available 
literature (Aristotle, 1954; Toulmin et al, 1979; 
Perelman, 1970; Wallace, 1975), as well as our 
own text analysis, indicates that a profitable ap-
proach to opinion requires a system to know 
and/or identify at least the following elements: 
the topic (T), the opinion holder (H), the belief 
(B), and the opinion valence (V). For the pur-
poses of the various interested communities, 
neutral-valence opinions (such as we believe the 
sun will rise tomorrow; Susan believes that John 
has three children) is of less interest; more rele-
vant are opinions in which the valence is posi-
tive or negative. Such valence often falls 
together with the actual belief, as in ?going to 
Mars is a waste of money?; in which the word 
waste signifies both the belief a lot [of money] 
and the valence bad/undesirable, but need not 
always do so: ?Smith[the holder] believes that 
abortion should be permissible[the topic] al-
though he thinks that is a bad thing[the va-
lence]?.   
As the core first step of our research, we 
would like an automated system to identify, 
given an opinionated text, all instances of the 
[Holder/Topic/Valence] opinion triads it con-
tains1. Exploratory manual work has shown this 
to be a difficult task. We therefore simplify the 
task as follows.  We build a classifier that sim-
ply identifies in a text all the sentences express-
ing a valence. Such a two-way classification is 
simple to set up and evaluate, since enough test-
ing data has been created.   
As primary indicators, we note from newspa-
per editorials and online exhortatory text that 
certain modal verbs (should, must) and adjec-
tives and adverbs (better, best, unfair, ugly, nice, 
desirable, nicely, luckily) are strong markers of 
opinion. Section 3 describes our construction of 
a series of increasingly large collections of such 
marker words. Section 4 describes our methods 
for organizing and combining them and using 
them to identify valence-bearing sentences.  The 
evaluation is reported in Section 5.   
2 Past Computational Studies 
There has been a spate of research on identify-
ing sentence-level subjectivity in general and 
opinion in particular. The Novelty track 
                                                          
1 In the remainder of the paper, we will mostly use ?opin-
ion? in place of ?valence?.  We will no longer discuss Be-
lief, Holder, or Topic. 
61
(Soboroff and Harman, 2003) of the TREC-2003 
competition included a task of recognizing opin-
ion-bearing sentences (see Section 5.2).   
Wilson and Wiebe (2003) developed an anno-
tation scheme for so-called subjective sentences 
(opinions and other private states) as part of a 
U.S. government-sponsored project (ARDA 
AQUAINT NRRC) in 2002. They created a cor-
pus, MPQA, containing news articles manually 
annotated. Several other approaches have been 
applied for learning words and phrases that sig-
nal subjectivity. Turney (2002) and Wiebe 
(2000) focused on learning adjectives and adjec-
tival phrases and Wiebe et al (2001) focused on 
nouns. Riloff et al (2003) extracted nouns and 
Riloff and Wiebe (2003) extracted patterns for 
subjective expressions using a bootstrapping 
process.  
3 Data Sources 
We developed several collections of opinion-
bearing and non-opinion-bearing words. One is 
accurate but small; another is large but relatively 
inaccurate. We combined them to obtain a more 
reliable list. We obtained an additional list from 
Columbia University. 
3.1 Collection 1: Using WordNet 
In pursuit of accuracy, we first manually col-
lected a set of opinion-bearing words (34 adjec-
tives and 44 verbs). Early classification trials 
showed that precision was very high (the system 
found only opinion-bearing sentences), but since 
the list was so small, recall was very low (it 
missed many). We therefore used this list as 
seed words for expansion using WordNet. Our 
assumption was that synonyms and antonyms of 
an opinion-bearing word could be opinion-
bearing as well, as for example ?nice, virtuous, 
pleasing, well-behaved, gracious, honorable, 
righteous? as synonyms for ?good?, or ?bad, evil, 
disreputable, unrighteous? as antonyms. How-
ever, not all synonyms and antonyms could be 
used: some such words seemed to exhibit both 
opinion-bearing and non-opinion-bearing senses, 
such as ?solid, hot, full, ample? for ?good?.  
This indicated the need for a scale of valence 
strength. If we can measure the ?opinion-based 
closeness? of a synonym or antonym to a known 
opinion bearer, then we can determine whether 
to include it in the expanded set.  
To develop such a scale, we first created a 
non-opinion-bearing word list manually and 
produced related words for it using WordNet.  
To avoid collecting uncommon words, we 
started with a basic/common English word list 
compiled for foreign students preparing for the 
TOEFL test. From this we randomly selected 
462 adjectives and 502 verbs for human annota-
tion. Human1 and human2 annotated 462 adjec-
tives and human3 and human2 annotated 502 
verbs, labeling each word as either opinion-
bearing or non-opinion-bearing. 
O P N o n O P
w o r d
S y n o n y m  se t  o f  O P S y n o n y m  s e t  o f  N o n O P
S y n o n y m  s e t  o f  
a  g iv e n  w o r d
O P         :  O p in io n -b e a r in g  w o r d s
N o n O P : N o n -O p in io n -b e a r in g  
w o r d s  
Figure 1. Automatic word expansion using WordNet 
Now, to obtain a measure of opinion/non-
opinion strength, we measured the WordNet 
distance of a target (synonym or antonym) word 
to the two sets of manually selected seed words 
plus their current expansion words (see Figure 
1). We assigned the new word to the closer 
category. The following equation represents this 
approach: 
(1)          ).....,|(maxarg
)|(maxarg
21 n
c
c
synsynsyncP
wcP
?  
where c is a category (opinion-bearing or non-
opinion-bearing), w is the target word, and synn 
is the synonyms or antonyms of the given word 
by WordNet. To compute equation (1), we built 
a classification model, equation (2): 
(2)   )|()(maxarg
)|()(maxarg
)|()(maxarg)|(maxarg
1
))(,(
 ...3 2 1
?
=
=
=
=
m
k
wsynsetfcount
k
c
n
c
cc
kcfPcP
csynsynsynsynPcP
cwPcPwcP
where kf  is the kth feature of category c which is 
also a member of the synonym set of the target 
word w, and count(fk, synset(w)) means the total 
number of occurrences of fk in the synonym set 
of w. The motivation for this model is document 
classification.  (Although we used the synonym 
set of seed words achieved by WordNet, we 
could instead have obtained word features from 
a corpus.) After expansion, we obtained 2682 
62
opinion-bearing and 2548 non-opinion-bearing 
adjectives, and 1329 opinion-bearing and 1760 
non-opinion-bearing verbs, with strength values. 
By using these words as features we built a Na-
ive bayesian classifier and we finally classified 
32373 words. 
3.2 Collection 2: WSJ Data 
Experiments with the above set did not provide 
very satisfactory results on arbitrary text. For 
one reason, WordNet?s synonym connections 
are simply not extensive enough. However, if 
we know the relative frequency of a word in 
opinion-bearing texts compared to non-opinion-
bearing text, we can use the statistical informa-
tion instead of lexical information. For this, we 
collected a huge amount of data in order to make 
up for the limitations of collection 1. 
Following the insight of Yu and Hatzivassi-
loglou (2003), we made the basic and rough as-
sumption that words that appear more often in 
newspaper editorials and letters to the editor 
than in non-editorial news articles could be po-
tential opinion-bearing words (even though edi-
torials contain sentences about factual events as 
well). We used the TREC collection to collect 
data, extracting and classifying all Wall Street 
Journal documents from it either as Editorial or 
nonEditorial based on the occurrence of the 
keywords ?Letters to the Editor?, ?Letter to the 
Editor? or ?Editorial? present in its headline.  
This produced in total 7053 editorial documents 
and 166025 non-editorial documents.   
We separated out opinion from non-opinion 
words by considering their relative frequency in 
the two collections, expressed as a probability, 
using SRILM, SRI?s language modeling toolkit 
(http://www.speech.sri.com/projects/srilm/). For 
every word W occurring in either of the docu-
ment sets, we computed the followings: 
documents Editorialin   wordstotal
documents Editorialin W #)(Pr =WobEditorial
docs alnonEditoriin   wordstotal
docs alnonEditoriin W #)(Pr =WobalnonEditori
  
We used Kneser-Ney smoothing (Kneser and 
Ney, 1995) to handle unknown/rare words.  
Having obtained the above probabilities we cal-
culated the score of W as the following ratio: 
alProb(W)nonEditori
rob(W)EditorialP )( =WScore
 
Score(W) gives an indication of the bias of 
each word towards editorial or non-editorial 
texts. We computed scores for 86,674,738 word 
tokens. Naturally, words with scores close to 1 
were untrustworthy markers of valence.  To 
eliminate these words we applied a simple filter 
as follows. We divided the Editorial and the 
non-Editorial collections each into 3 subsets. For 
each word in each {Editorial, non-Editorial} 
subset pair we calculated Score(W). We retained 
only those words for which the scores in all 
three subset pairs were all greater than 1 or all 
less than 1.  In other words, we only kept words 
with a repeated bias towards Editorial or non-
Editorial. This procedure helped eliminate some 
of the noisy words, resulting in 15568 words.  
3.3 Collection 3: With Columbia Wordlist 
Simply partitioning WSJ articles into Edito-
rial/non-Editorial is a very crude differentiation.  
In order to compare the effectiveness of our im-
plementation of this idea with the implementa-
tion by Yu and Hatzivassiloglou of Columbia 
University, we requested their word list, which 
they kindly provided. Their list contained 
167020 adjectives, 72352 verbs, 168614 nouns, 
and 9884 adverbs. However, this figure is sig-
nificantly inflated due to redundant counting of 
words with variations in capitalization and a 
punctuation.We merged this list and ours to ob-
tain collection 4. Among these words, we only 
took top 2000 opinion bearing words and top 
2000 non-opinion-bearing words for the final 
word list. 
3.4 Collection 4: Final Merger  
So far, we have classified words as either opin-
ion-bearing or non-opinion-bearing by two dif-
ferent methods.  The first method calculates the 
degrees of closeness to manually chosen sets of 
opinion-bearing and non-opinion-bearing words 
in WordNet and decides its class and strength. 
When the word is equally close to both classes, 
it is hard to decide its subjectivity, and when 
WordNet doesn?t contain a word or its syno-
nyms, such as the word ?antihomosexsual?, we 
fail to classify it.   
The second method, classification of words 
using WSJ texts, is less reliable than the lexical 
method.  However, it does for example success-
fully handle ?antihomosexual?.  Therefore, we 
combined the results of the two methods (collec-
tions 1 and 2), since their different characteris-
63
tics compensate for each other. Later we also 
combine 4000 words from the Columbia word 
list to our final 43700 word list. Since all three 
lists include a strength between 0 and 1, we 
simply averaged them, and normalized the va-
lence strengths to the range from -1 to +1, with 
greater opinion valence closer to 1 (see Table 1).  
Obviously, words that had a high valence 
strength in all three collections had a high over-
all positive strength. When there was a conflict 
vote among three for a word, it aotomatically 
got weak strength. Table 2 shows the distribu-
tion of words according to their sources: Collec-
tion1(C1), Collection2(C2) and Collection3(C3). 
4 Measuring Sentence Valence 
4.1   Two Models 
We are now ready to automatically identify 
opinion-bearing sentences.  We defined several 
models, combining valence scores in different 
ways, and eventually kept two:  
Model 1: Total valence score of all words in a 
sentence  
Model 2: Presence of a single strong valence 
word  
The intuition underlying Model 1 is that sen-
tences in which opinion-bearing words dominate 
tend to be opinion-bearing, while Model 2 re-
flects the idea that even one strong valence word 
is enough.  After experimenting with these mod-
els, we decided to use Model 2.   
How strong is ?strong enough??  To deter-
mine the cutoff threshold (?) on the opinion-
bearing valence strength of words, we experi-
mented on human annotated data.  
4.2   Gold Standard Annotation  
We built two sets of human annotated sentence 
subjectivity data.  Test set A contains 50 sen-
tences about welfare reform, of which 24 sen-
tences are opinion-bearing.  Test set B contains 
124 sentences on two topics (illegal aliens and 
term limits), of which 53 sentences are opinion-
bearing. Three humans classified the sentences 
as either opinion or non-opinion bearing.  We 
calculated agreement for each pair of humans 
and for all three together.  Simple pairwise 
agreement averaged at 0.73, but the kappa score 
was only 0.49.  
Table 3 shows the results of experimenting 
with different combinations of Model 1, Model 
2, and several cutoff values. Recall, precision, F-
score, and accuracy are defined in the normal 
way. Generally, as the cutoff threshold in-
creases, fewer opinion markers are included in 
the lists, and precision increases while recall 
drops. The best F-core is obtained on Test set A, 
Model 2, with ?=0.1 or 0.2 (i.e., being rather 
liberal). 
Table 1. Examples of opinion-bearing/non-opinion-
bearing words  
Adjectives Final score Verbs Final score 
Careless 0.63749 Harm 0.61715 
wasteful 0.49999 Hate 0.53847 
Unpleasant 0.15263 Yearn 0.50000 
Southern -0.2746 Enter -0.4870 
Vertical -0.4999 Crack -0.4999 
Scored -0.5874 combine -0.5852 
Table 2. Distribution of words 
 C1 C2 C3 # words % 
 ?   25605 58.60 
  ?  8202 18.77 
   ? 2291 5.24 
 ? ?  5893 13.49 
  ? ? 834 1.90 
 ?  ? 236 0.54 
 ? ? ? 639 1.46 
Total # 32373 15568 4000 43700 100 
Table 3. Determining ? and performance for various models on gold standard data  
[?: cutoff parameter, R: recall, P: precision, F: F-score, A: accuracy] 
Development Test set A Development Test set B 
 
Model1 Model2 Model1 Model2 
? R P F A R P F A R P F A R P F A 
0.1 0.54 0.61 0.57 0.62 0.91 0.55 0.69 0.6 0.43 0.36 0.39 0.43 0.94 0.45 0.61 0.48 
0.2 0.54 0.61 0.57 0.62 0.91 0.56 0.69 0.62 0.39 0.35 0.37 0.42 0.86 0.45 0.59 0.49 
0.3 0.58 0.6 0.59 0.62 0.83 0.55 0.66 0.6 0.43 0.39 0.41 0.47 0.77 0.45 0.57 0.05 
0.4 0.33 0.8 0.47 0.64 0.33 0.8 0.47 0.64 0.45 0.36 0.4 0.42 0.45 0.36 0.4 0.42 
0.5 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.32 0.3 0.31 0.4 0.32 0.3 0.31 0.4 
0.6 0.16 0.8 0.27 0.58 0.16 0.8 0.27 0.58 0.2 0.22 0.21 0.35 0.2 0.22 0.21 0.35 
64
Table 4. Test on MPQA data 
 Accuracy Precision Recall 
 C Ours All C Ours All C Ours All
t=1 0.55 0.63 0.59 0.55 0.61 0.58 0.97 0.85 0.91
t=2 0.57 0.65 0.63 0.56 0.70 0.63 0.92 0.62 0.75
t=3 0.58 0.61 0.62 0.58 0.77 0.69 0.84 0.40 0.56
t=4 0.59 0.55 0.60 0.60 0.83 0.74 0.74 0.22 0.39
t=5 0.59 0.51 0.55 0.62 0.87 0.78 0.63 0.12 0.25
t=6 0.58 0.48 0.52 0.64 0.91 0.82 0.53 0.06 0.15
random 0.50 0.54 0.50 
C: Columbia word list(top 10682 words),  Ours : C1+C2 (top 
10682 words), All: C+Ours (top 19947 words) 
5 Results 
We tested our system on three different data sets.  
First, we ran the system on MPQA data pro-
vided by ARDA. Second, we participated in the 
novelty track of TREC 2003. Third, we ran it on 
our own test data described in Section 4.2. 
5.1  MPQA Test 
The MPQA corpus contains news articles manu-
ally annotated using an annotation scheme for 
subjectivity (opinions and other private states 
that cannot be directly observed or verified. 
(Quirk et al, 1985), such as beliefs, emotions, 
sentiment, speculation, etc.). This corpus was 
collected and annotated as part of the summer 
2002 NRRC Workshop on Multi-Perspective 
Question Answering (MPQA) (Wiebe et al, 
2003) sponsored by ARDA. It contains 535 
documents and 10,657 sentences.   
The annotation scheme contains two main 
components: a type of explicit private state and 
speech event, and a type of expressive subjec-
tive element. Several detailed attributes and 
strengths are annotated as well. More details are 
provided in (Riloff et al, 2003).   
Subjective sentences are defined according to 
their attributes and strength. In order to apply 
our system at the sentence level, we followed 
their definition of subjective sentences. The an-
notation GATE_on is used to mark speech 
events and direct expressions of private states.  
The onlyfactive attribute is used to indicate 
whether the source of the private state or speech 
event is indeed expressing an emotion, opinion 
or other private state. GATE_expressive-
subjectivity annotation marks words and phrases 
that indirectly express a private state. 
In our experiments, our system performed 
relatively well in both precision and recall. We 
interpret our opinion markers as coinciding with 
(enough of) the ?subjective? words of MPQA.  
In order to see the relationship between the 
number of opinion-bearing words in a sentence 
and its classification by MPQA as subjective, 
we varied the threshold number of opinion-
bearing words required for subjectivity. Table 4 
shows accuracy, precision, and recall according 
to the list used and the threshold value t. 
The random row shows the average of ten 
runs of randomly assigning sentences as either 
subjective or objective. As we can see from Ta-
ble 4, our word list which is the combination of 
the Collection1 and Collection2, achieved 
higher accuracy and precision than the Colum-
bia list. However, the Columbia list achieved 
higher recall than ours. For a fair comparison, 
we took top 10682 opinion-bearing words from 
each side and ran the same sentence classifier 
system.2  
5.2 TREC data 
Opinion sentence recognition was a part of the 
novelty track of TREC 2003 (Soboroff and Har-
man, 2003). The task was as follows.  Given a 
TREC topic and an ordered list of 25 documents 
relevant to the topic, find all the opinion-bearing 
sentences. No definition of opinion was pro-
vided by TREC; their assessor?s intuitions were 
considered final. In 2003, there were 22 opinion 
topics containing 21115 sentences in total.  The 
opinion topics generally related to the pros and 
cons of some controversial subject, such as, 
?partial birth abortion ban?, ?Microsoft antitrust 
charges?, ?Cuban child refugee Elian Gonzalez?, 
?marijuana legalization?, ?Clinton relationship 
with Lewinsky?, ?death penalty?, ?adoption 
same-sex partners, and etc. For the opinion top-
ics, a sentence is relevant if it contains an opin-
ion about that subject, as decided by the assessor.  
There was no categorizing of polarity of opinion 
or ranking of sentences by likelihood that they 
contain an opinion. F-score was used to measure 
system performance. 
We submitted 5 separate runs, using different 
models. Our best model among the five was 
Model 2. It performed the second best of the 55 
runs in the task, submitted by 14 participating 
                                                          
2 In comparison, the HP-Subj (height precision subjectivity 
classifier) (Riloff, 2003) produced recall 40.1 and precision 
90.2 on test data using text patterns, and recall 32.9 and 
precision 91.3 without patterns.  These figures are compa-
rable with ours. 
65
institutions. (Interestingly, and perhaps disturb-
ingly, RUN3, which simply returned every sen-
tence as opinion-bearing, fared extremely well, 
coming in 11th.  This model now provides a 
baseline for future research.) After the TREC 
evaluation data was made available, we tested 
Model 1 and Model 2 further. Table 5 shows the 
performance of each model with the two best-
performing cutoff values. 
Table 5. System performance with different models and 
cutoff values on TREC 2003 data 
Model System Parameter ? F-score 
0.2 0.398 Model1 
0.3 0.425 
0.2 0.514 
Model2 
0.3 0.464 
5.3 Test with Our Data 
Section 4.2 described our manual data annota-
tion by 3 humans. Here we used the work of one 
human as development test data for parameter 
tuning.  The other set with 62 sentences on the 
topic of gun control we used as blind test data. 
Although the TREC and MPQA data sets are lar-
ger and provide comparisons with others? work, 
and despite the low kappa agreement values, we 
decided to obtain cutoff values on this data too. 
The graphs in Figure 3 show the performance of 
Models 1 and 2 with different values. 
6 Conclusions and Future Work 
In this paper, we described an efficient auto-
matic algorithm to produce opinion-bearing 
words by combining two methods. The first 
method used only a small set of human-
annotated data. We showed that one can find 
productive synonyms and antonyms of an opin-
ion-bearing word through automatic expansion 
in WordNet and use them as feature sets of a 
classifier. To determine a word?s closeness to 
opinion-bearing or non-opinion-bearing synoym 
set, we also used all synonyms of a given word 
as well as the word itself. An additional method, 
harvesting words from WSJ, can compensate the 
first method.   
Using the resulting list, we experimented with 
different cutoff thresholds in the opinion/non-
opinion sentence classification on 3 different 
test data sets.  Especially on the TREC 2003 
Novelty Track, the system performed well. We 
plan in future work to pursue the automated 
analysis of exhortatory text in order to produce 
detailed argument graphs reflecting their au-
thors? argumentation. 
References 
Aristotle. The Rhetorics and Poetics (trans. W. Rhys Rob-
erts, Modern Library, 1954). 
Fellbaum, C., D. Gross, and K. Miller. 1993. Adjectives  in 
WordNet.  http://www.cosgi.princeton.edu/~wn. 
Kneser, R. and H. Ney. 1995. Improved Backing-off for n-
gram Language Modeling. Proceedings of ICASSP, vol. 
1, 181?184.  
Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. 
Miller. 1993. Introduction to WordNet: An On-Line 
Lexical Database. http://www.cosgi.princeton. edu/~wn. 
Pang, B. L. Lee, and S. Vaithyanathan, 2002.  Thumbs up? 
Sentiment classification using Machine Learning Tech-
niques. Proceedings of the EMNLP conference. 
Perelman, C. 1970. The New Rhetoric: A Theory of Practi-
cal Reasoning. In The Great Ideas Today. Chicago: En-
cyclopedia Britannica.    
Riloff , E. and J. Wiebe. 2003. Learning Extraction Pat-
terns for Opinion-bearing Expressions. Proceedings of 
the EMNLP-03. 
Riloff, E., J. Wiebe, and T. Wilson 2003. Learning Subjec-
tive Nouns Using Extraction Pattern Bootstrapping. 
Proceedings of CoNLL-03 
Soboroff, I. and D. Harman. 2003. Overview of the TREC 
2003 Novelty Track. Proceedings of TREC-2003. 
Toulmin, S.E., R. Rieke, and A. Janik. 1979. An Introduc-
tion to Reasoning. Macmillan, New York 
Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic 
Orientation Applied to Unsupervised Classification of 
Reviews. Proceedings of the 40th Annual Meeting of the 
ACL, Philadelphia, 417?424. 
Wallace, K. 1975. Topoi and the Problem of Invention. In 
W. Ross Winterowd (ed), Contemporary Rhetoric. Har-
court Brace Jovanovich.  
Wilson, T. and J. Wiebe. 2003. Annotating Opinions in the 
World Press. Proceedings of the ACL SIGDIAL-03. 
Yu, H. and V. Hatzivassiloglou. 2003. Towards Answering 
Opinion Questions: Separating Facts from Opinions and 
Identifying the Polarity of Opinion Sentences. Proceed-
ings of EMNLP-2003. 
Model1
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6
cutoff parameter
Recall
Precision
fscore
Accuracy
Model2
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6
cutoff parameter
Recall
Precision
fscore
Accuracy
Figure 3. Test on human-annotated sentences 
66
59
60
61
62
63
64
65
66
Learning a Stopping Criterion for Active Learning for Word Sense 
Disambiguation and Text Classification 
Jingbo Zhu   Huizhen Wang 
Natural Language Processing Lab  
Northeastern University 
Shenyang, Liaoning, P.R.China, 110004 
Zhujingbo@mail.neu.edu.cn
wanghuizhen@mail.neu.edu.cn 
Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
hovy@isi.edu
 
Abstract 
In this paper, we address the problem of 
knowing when to stop the process of active 
learning. We propose a new statistical 
learning approach, called minimum 
expected error strategy, to defining a 
stopping criterion through estimation of the 
classifier?s expected error on future 
unlabeled examples in the active learning 
process. In experiments on active learning 
for word sense disambiguation and text 
classification tasks, experimental results 
show that the new proposed stopping 
criterion can reduce approximately 50% 
human labeling costs in word sense 
disambiguation with degradation of 0.5% 
average accuracy, and approximately 90% 
costs in text classification with degradation 
of 2% average accuracy. 
1 Introduction 
Supervised learning models set their parameters 
using given labeled training data, and generally 
outperform unsupervised learning methods when 
trained on equal amount of training data. However, 
creating a large labeled training corpus is very 
expensive and time-consuming in some real-world 
cases such as word sense disambiguation (WSD).  
Active learning is a promising way to minimize 
the amount of human labeling effort by building an 
system that automatically selects the most informa-
tive unlabeled example for human annotation at 
each annotation cycle. In recent years active learn-
ing  has attracted a lot of research interest, and has 
been studied in many natural language processing 
(NLP) tasks, such as text classification (TC) 
(Lewis and Gale, 1994; McCallum and Nigam, 
1998), chunking (Ngai and Yarowsky, 2000), 
named entity recognition (NER) (Shen et al, 2004; 
Tomanek et al, 2007), part-of-speech tagging 
(Engelson and Dagan, 1999), information 
extraction (Thompson et  al., 1999), statistical 
parsing (Steedman et al, 2003), and word sense 
disambiguation (Zhu and Hovy, 2007).  
Previous studies reported that active learning 
can help in reducing human labeling effort. With 
selective sampling techniques such as uncertainty 
sampling (Lewis and Gale, 1994) and committee-
based sampling (McCallum and Nigam, 1998), the 
size of the training data can be significantly re-
duced for text classification (Lewis and Gale, 
1994; McCallum and Nigam, 1998), word sense 
disambiguation (Chen, et al 2006; Zhu and Hovy, 
2007), and named entity recognition (Shen et al, 
2004; Tomanek et al, 2007) tasks.  
Interestingly, deciding when to stop active 
learning is an issue seldom mentioned issue in 
these studies. However, it is an important practical 
topic, since it obviously makes no sense to 
continue the active learning procedure until the 
whole corpus has been labeled. How to define an 
adequate stopping criterion remains an unsolved 
problem in active learning. In principle, this is a 
problem of estimation of classifier effectiveness 
(Lewis and Gale, 1994). However, in real-world 
applications, it is difficult to know when the 
classifier reaches its maximum effectiveness 
before all unlabeled examples have been 
annotated. And when the unlabeled data set 
becomes very large, full annotation is almost 
impossible for human annotator.  
In this paper, we address the issue of a stopping 
criterion for active learning, and propose a new 
statistical learning approach, called minimum ex-
366
pected error strategy, that defines a stopping crite-
rion through estimation of the classifier?s expected 
error on future unlabeled examples. The intuition is 
that the classifier reaches maximum effectiveness 
when it results in the lowest expected error on 
remaining unlabeled examples. This proposed 
method is easy to implement, involves small 
additional computation costs, and can be applied to 
several different learners, such as Naive Bayes 
(NB), Maximum Entropy (ME), and Support 
Vector Machines (SVMs) models. Comparing with 
the confidence-based stopping criteria proposed by 
Zhu and Hovy (2007), experimental results show 
that the new proposed stopping criterion achieves 
better performance in active learning for both the 
WSD and TC tasks. 
2 Active Learning Process and Problem 
of General Stopping Criterion 
2.1 Active Learning Process 
Active learning is a two-step semi-supervised 
learning process in which a small number of la-
beled samples and a large number of unlabeled 
examples are first collected in the initialization 
stage, and a close-loop stage of query and retrain-
ing is adopted. The purpose of active learning is to 
minimize the amount of human labeling effort by 
having the system in each cycle automatically se-
lect for human annotation the most informative 
unannotated case.   
Procedure: Active Learning Process 
Input: initial small training set L, and pool of 
unlabeled data set U 
Use L to train the initial classifier C (i.e. a classi-
fier for uncertainty sampling or a set of classifiers 
for committee-based sampling) 
Repeat 
? Use the current classifier C  to label all 
unlabeled examples in U 
? Based on active learning rules R such as un-
certainty sampling or committee-based sam-
pling, present m top-ranked unlabeled ex-
amples to oracle H for labeling 
? Augment L with the m new examples, and 
remove them from U 
? Use L to retrain the current classifier C 
Until the predefined stopping criterion SC is met. 
Figure 1. Active learning process 
In this work, we are interested in selective sam-
pling for pool-based active learning, and focus on 
uncertainty sampling (Lewis and Gale, 1994). The 
key point is how to measure the uncertainty of an 
unlabeled example, in order to select a new exam-
ple with maximum uncertainty to augment the 
training data. The maximum uncertainty implies 
that the current classifier has the least confidence 
in its classification of this unlabeled example x. 
The well-known entropy is a good uncertainty 
measurement widely used in active learning: 
( ) ( | ) log ( | )
y Y
UM x P y x P y x
?
= ??         (1) 
where P(y|x) is the a posteriori probability. We 
denote the output class y?Y={y1, y2, ?, yk}. UM is 
the uncertainty measurement function based on the 
entropy estimation of the classifier?s posterior 
distribution. 
2.2 General Stopping Criteria 
As shown in Fig. 1, the active learning process 
repeatedly provides the most informative unlabeled 
examples to an oracle for annotation, and update 
the training set, until the predefined stopping 
criterion SC is met. In practice, it is not clear how 
much annotation is sufficient for inducing a 
classifier with maximum effectiveness (Lewis and 
Gale, 1994). This procedure can be implemented 
by defining an appropriate stopping criterion for 
active learning.  
In active learning process, a general stopping 
criterion SC can be defined as: 
1 (
0 ,AL
effectiveness C
SC
otherwise
) ???= ??         (2) 
where ? is a user predefined constant and the func-
tion effectiveness(C) evaluates the effectiveness of 
the current classifier. The learning process ends 
only if the stopping criterion function SCAL is equal 
to 1. The value of constant ? represents a tradeoff 
between the cost of annotation and the effective-
ness of the resulting classifier. A larger ? would 
cause more unlabeled examples to be selected for 
human annotation, and the resulting classifier 
would be more robust. A smaller ? means the re-
sulting classifier would be less robust, and less 
unlabeled examples would be selected to annotate.  
In previous work (Shen et al, 2004; Chen et al, 
2006; Li and Sethi, 2006; Tomanek et al, 2007), 
there are several common ways to define the func-
367
tion effectiveness(C). First, previous work always 
used a simple stopping condition, namely, when 
the training set reaches desirable size. However, it 
is almost impossible to predefine an appropriate 
size of desirable training data guaranteed to induce 
the most effective classifier. Secondly, the learning 
loop can end if no uncertain unlabeled examples 
can be found in the pool. That is, all informative 
examples have been selected for annotation. 
However, this situation seldom occurs in real-
world applications. Thirdly, the active learning 
process can stop if the targeted performance level 
is achieved. However, it is difficult to predefine an 
appropriate and achievable performance, since it 
should depend on the problem at hand and the 
users? requirements.  
2.3 Problem of Performance Estimation 
An appealing solution has the active learning 
process end when repeated cycles show no 
significant performance improvement on the test 
set. However, there are two open problems. The 
first question is how to measure the performance of 
a classifier in active learning. The second one is 
how to know when the resulting classifier reaches 
the highest or adequate performance. It seems 
feasible that a separate validation set can solve 
both problems. That is, the active learning process 
can end if there is no significant performance 
improvement on the validation set. But how many 
samples are required for the pregiven separate 
validation set is an open question. Too few 
samples may not be adequate for a reasonable 
estimation and may result in an incorrect result. 
Too many samples would cause additional high 
cost because the separate validation set is generally 
constructed manually in advance.  
3 Statistical Learning Approach 
3.1 Confidence-based Strategy 
To avoid the problem of performance estimation 
mentioned above, Zhu and Hovy (2007) proposed 
a confidence-based framework to predict the upper 
bound and the lower bound for a stopping criterion 
in active learning. The motivation is to assume that 
the current training data is sufficient to train the 
classifier with maximum effectiveness if the cur-
rent classifier already has acceptably strong confi-
dence on its classification results for all remained 
unlabeled data.  
The first method to estimate the confidence of 
the classifier is based on uncertainty measurement, 
considering whether the entropy of each selected 
unlabeled example is less than a small predefined 
threshold. Here we call it Entropy-MCS. The 
stopping criterion SC Entropy-MCS can be defined as: 
 
1 , ( )
0 ,
E
Entropy MCS
x U UM x
SC
otherwise
?
?
? ? ??= ??
    (3) 
where ?E is a user predefined entropy threshold and 
the function UM(x) evaluates the uncertainty of 
each unlabeled example x.  
The second method to estimate the confidence 
of the classifier is based on feedback from the ora-
cle when the active learner asks for true labels for 
selected unlabeled examples, by considering 
whether the current trained classifier could 
correctly predict the labels or the accuracy 
performance of predictions on selected unlabeled 
examples is already larger than a predefined 
accuracy threshold. Here we call it OracleAcc-
MCS. The stopping criterion SCOracleAcc-MCS can be 
defined as: 
1 (
0 ,
) A
OracleAcc MCS
OracleAcc C
SC
otherwise
?
?
??= ??
    (4) 
where ?A is a user predefined accuracy threshold 
and function OracleAcc(C) evaluates accuracy per-
formance of the classifier on these selected unla-
beled examples through feedback of the Oracle.  
3.2 Minimum Expected Error Strategy 
In fact, these above two confidence-based methods 
do not directly estimate classifier performance that 
closely reflects the classifier effectiveness, because 
they only consider entropy of each unlabeled 
example and accuracy on selected informative 
examples at each iteration step. In this section we 
therefore propose a new statistical learning ap-
proach to defining a stopping criterion through es-
timation of the classifier?s expected error on all 
future unlabeled examples, which we call minimum 
expected error strategy (MES). The motivation 
behind MES is that the classifier C (a classifier for 
uncertainty sampling or set of classifiers for com-
mittee-based sampling) with maximum effective-
ness is the one that results in the lowest expected 
368
error on whole test set in the learning process. The 
stopping criterion SC MES is defined as: 
1 ( )
0 ,
err
MES
Error C
SC
otherwise
???= ??           (5) 
where ?err is a user predefined expected error 
threshold and the function Error(C) evaluates the 
expected error of the classifier C that closely re-
flects the classifier effectiveness. So the key point 
of defining MES-based stopping criterion SC MES is 
how to calculate the function Error(C) that denotes 
the expected error of the classifier C.  
Suppose given a training set L and an input 
sample x, we can write the expected error of the 
classifier C as follows: 
( ) ( ( ) | ) ( )Error C R C x x P x dx= ?           (6) 
where P(x) represents the known marginal distribu-
tion of x. C(x) represents the classifier?s decision 
that is one of k classes: y?Y={y1, y2, ?, yk}. R(yi|x) 
denotes a conditional loss for classifying the input 
sample x into a class yi that can be defined as 
1
( | ) [ , ] ( | )
k
i j
j
R y x i j P y x?
=
=?             (7) 
where P(yj|x) is the a posteriori probability pro-
duced by the classifier C. ?[i,j] represents a zero-
one loss function for every class pair {i,j} that as-
signs no loss to a correct classification, and assigns 
a unit loss to any error. 
In this paper, we focus on pool-based active 
learning in which a large unlabeled data pool U is 
available, as described Fig. 1. In active learning 
process, our interest is to estimate the classifier?s 
expected error on future unlabeled examples in the 
pool U. That is, we can stop the active learning 
process when the active learner results in the low-
est expected error over the unlabeled examples in 
U. The pool U can provide an estimate of P(x). So 
for minimum error rate classification (Duda and 
Hart. 1973) on unlabeled examples, the expected 
error of the classifier C can be rewritten as 
1
( ) (1 max ( | ))
y Y
x U
Error C P y x
U ??
= ??        (8) 
Assuming N unlabeled examples in the pool U, 
the total time is O(N) for automatically determin-
ing whether the proposed stopping criterion SCMES 
is satisfied in the active learning.  
If the pool U is very large (e.g. more than 
100000 examples), it would still cause high com-
putation cost at each iteration of active learning. A 
good approximation is to estimate the expected 
error of the classifier using a subset of the pool, not 
using all unlabeled examples in U. In practice, a 
good estimation of expected error can be formed 
with few thousand examples. 
4 Evaluation 
In this section, we evaluate the effectiveness of 
three stopping criteria for active learning for word 
sense disambiguation and text classification as 
follows: 
? Entropy-MCS ? stopping active learning 
process when the stopping criterion function 
SCEntropy-MCS defined in (3) is equal to 1, where 
?E=0.01, 0.001,  0.0001.  
? OracleAcc-MCS ? stopping active learning 
process when the stopping criterion function 
SCOracleAcc-MCS defined in (4) is equal to 1, 
where ?A=0.9, 1.0.  
? MES ? stopping active learning process when 
the stopping criterion function SCMES defined 
in (5) is equal to 1, where ?err=0.01, 0.001, 
0.0001.  
The purpose of defining stopping criterion of 
active learning is to study how much annotation is 
sufficient for a specific task. To comparatively 
analyze the effectiveness of each stopping criterion, 
a baseline stopping criterion is predefined as when 
all unlabeled examples in the pool U are learned. 
Comparing with the baseline stopping criterion, a 
better stopping criterion not only achieves almost 
the same performance, but also has needed to learn 
fewer unlabeled examples when the active learning 
process is ended. In other words, for a stopping 
criterion of active learning, the fewer unlabeled 
examples that have been leaned when it is met, the 
bigger reduction in human labeling cost is made. 
In the following active learning experiments, a 
10 by 10-fold cross-validation was performed. All 
results reported are the average of 10 trials in each 
active learning process.  
4.1 Word Sense Disambiguation 
The first comparison experiment is active learning 
for word sense disambiguation. We utilize a 
maximum entropy (ME) model (Berger et al, 
1996) to design the basic classifier used in active 
learning for WSD. The advantage of the ME model 
is the ability to freely incorporate features from 
369
diverse sources into a single, well-grounded statis-
tical model. A publicly available ME toolkit 
(Zhang et. al., 2004) was used in our experiments. 
In order to extract the linguistic features necessary 
for the ME model in WSD tasks, all sentences con-
taining the target word are automatically part-of-
speech (POS) tagged using the Brill POS tagger 
(Brill, 1992). Three knowledge sources are used to 
capture contextual information: unordered single 
words in topical context, POS of neighboring 
words with position information, and local colloca-
tions. These are same as the knowledge sources 
used in (Lee and Ng, 2002) for supervised auto-
mated WSD tasks.  
The data used for comparison experiments was 
developed as part of the OntoNotes project (Hovy 
et al, 2006), which uses the WSJ part of the Penn 
Treebank (Marcus et al, 1993). The senses of 
noun words occurring in OntoNotes are linked to 
the Omega ontology (philpot et al, 2005). In 
OntoNotes, at least two human annotators 
manually annotate the coarse-grained senses of 
selected nouns and verbs in their natural sentence 
context. In this experiment, we used several tens of 
thousands of annotated OntoNotes examples, 
covering in total 421 nouns with an inter-annotator 
agreement rate of at least 90%. We find that 302 
out of 421 nouns occurring in OntoNotes are 
ambiguous, and thus are used in the following 
WSD experiments. For these 302 ambiguous 
nouns, there are 3.2 senses per noun, and 172 
instances per noun.  
The active learning algorithms start with a 
randomly chosen initial training set of 10 labeled 
samples for each noun, and make 10 queries after 
each learning iteration. Table 1 shows the 
effectiveness of each stopping criterion tested on 
active learning for WSD on these ambiguous 
nouns? WSD tasks. We analyze average accuracy 
performance of the classifier and average 
percentage of unlabeled examples learned when 
each stopping criterion is satisfied in active 
learning for WSD tasks. All accuracies and 
percentages reported in Table 1 are macro-
averages over these 302 ambiguous nouns. 
 
 
 
 
 
 
Stopping Criterion Average accuracy 
Average 
percentage 
all unlabeled examples learned 87.3% 100% 
Entropy-MCS method (0.0001) 86.8% 81.8% 
Entropy-MCS method (0.001) 86.8% 75.8% 
Entropy-MCS method (0.01) 86.8% 68.6% 
OracleAcc-MCS method (0.9) 86.8% 56.5% 
OracleAcc-MCS method (1.0) 86.8% 62.4% 
MES method (0.0001) 86.8% 67.1% 
MES method (0.001) 86.8% 58.8% 
MES method (0.01) 86.8% 52.7% 
Table 1. Effectiveness of each stopping criterion of 
active learning for WSD on OnteNotes. 
 
Table 1 shows that these stopping criteria 
achieve the same accuracy of 86.8% which is 
within 0.5% of the accuracy of the baseline method 
(all unlabeled examples are labeled). It is obvious 
that these stopping criteria can help reduce the hu-
man labeling costs, comparing with the baseline 
method. The best criterion is MES method 
(?err=0.01), following by OracleAcc-MCS method 
(?A=0.9). MES method (?err=0.01) and OracleAcc-
MCS method (?A=0.9) can make 47.3% and 44.5% 
reductions in labeling costs, respectively. Entropy-
MCS method is apparently worse than MES and 
OracleAcc-MCS methods. The best of the 
Entropy-MCS method is the one with ?E=0.01 
which makes approximately 1/3 reduction in 
labeling costs. We also can see from Table 1 that 
for Entropy-MCS and MES methods, reduction 
rate becomes smaller as the ? becomes smaller. 
4.2 Text Classification 
The second data set is for active learning for text 
classification using the WebKB corpus 1  
(McCallum et al, 1998). The WebKB dataset was 
formed by web pages gathered from various uni-
versity computer science departments. In the fol-
lowing active learning experiment, we use four 
most populous categories: student, faculty, course 
and project, altogether containing 4,199 web pages. 
Following previous studies (McCallum et al, 
1998), we only remove those words that occur 
merely once without using stemming or stop-list. 
The resulting vocabulary has 23,803 words. In the 
design of the text classifier, the maximum entropy 
model is also utilized, and no feature selection 
technique is used. 
                                                 
1 See http://www.cs.cmu.edu/~textlearning 
370
The algorithm is initially given 20 labeled ex-
amples, 5 from each class. Table 2 shows the 
effectiveness of each stopping criterion of active 
learning for text classification on WebKB corpus. 
All results reported are the average of 10 trials. 
Stopping Criterion Average accuracy 
Average 
percentage 
all unlabeled examples learned 93.5% 100% 
Entropy-MCS method (0.0001) 92.5% 23.8% 
Entropy-MCS method (0.001) 92.4% 22.3% 
Entropy-MCS method (0.01) 92.5% 21.8% 
OracleAcc-MCS method (0.9) 91.5% 13.1% 
OracleAcc-MCS method (1.0) 92.5% 24.5% 
MES method (0.0001) 92.1% 17.9% 
MES method (0.001) 92.0% 15.6% 
MES method (0.01) 91.5% 10.9% 
Table 2. Effectiveness of each stopping criterion of 
active learning for TC on WebKB corpus. 
 
From results shown in Table 2, we can see that 
MES method (?err=0.01) already achieves 91.5% 
accuracy in 10.9% unlabeled examples learned. 
The accuracy of all unlabeled examples learned is 
93.5%. This situation means the approximately 
90% remaining unlabeled examples only make 
only 2% performance improvement. Like the 
results of WSD shown in Table 1, for Entropy-
MCS and MES methods used in active learning for 
text classification tasks, the corresponding 
reduction rate becomes smaller as the value of ? 
becomes smaller. MES method (?err=0.01) can 
make approximately 90% reduction in human la-
beling costs and results in 2% accuracy perform-
ance degradation. The Entropy-MCS method 
(?E=0.01) can make approximate 80% reduction in 
costs and results in 1% accuracy performance 
degradation. Unlike the results of WSD shown in 
Table 1, the OracleAcc-MCS method (?A=1.0) 
makes the smallest reduction rate of 75.5%. 
Actually in real-world applications, the selection of 
a stopping criterion is a tradeoff issue between 
labeling cost and effectiveness of the classifier.  
5 Discussion 
It is interesting to investigate the impact of per-
formance change on defining a stopping criterion, 
so we show an example of active learning for 
WSD task in Fig. 2.  
 0.8
 0.82
 0.84
 0.86
 0.88
 0.9
 0.92
 0.94
 0  20  40  60  80  100  120  140  160  180  200  220
A
cc
ur
ac
y
Number of Learned Examples
Active Learning for WSD task
rate-n
 
Figure 2. An example of active learning for WSD 
on noun ?rate? in OntoNotes. 
 
Fig. 2 shows that the accuracy performance gen-
erally increases, but apparently degrades at the it-
erations ?20?, ?80?, ?170?, ?190?, and ?200?, and 
does not change anymore during the iterations 
[?130?-?150?] or [?200?-?220?] in the active learn-
ing process. Actually the first time of the highest 
performance of 95% achieved is at ?450?, which is 
not shown in Fig. 2. In other words, although the 
accuracy performance curve shows an increasing 
trend, it is not monotonously increasing. From Fig. 
2 we can see that it is not easy to automatically 
determine the point of no significant performance 
improvement on the validation set, because points 
such as ?20? or ?80? would mislead final judgment. 
However, we do believe that the change of per-
formance is a good signal to stop active learning 
process. So it is worth studying further how to 
combine the factor of performance change with our 
proposed stopping criteria of active learning.  
The OracleAcc-MCS method would not work if 
only one or too few informative examples are 
queried at the each iteration step in the active 
learning. There is an open issue how many selected 
unlabeled examples at each iteration are adequate 
for the batch-based sample selection.  
For these stopping crieria, there is no general 
method to automatically determine the best 
threshold for any given task. It may therefore be 
necessary to use a dynamic threshold change tech-
nique in which the predefined threshold can be 
automatically modified if the performance is still 
significantly improving when the stopping crite-
rion is met during active learning process.  
371
6 Conclusion and Future Work 
In this paper, we address the stopping criterion is-
sue of active learning, and analyze the problems 
faced by some common ways to stop the active 
learning process. In essence, defining a stopping 
criterion of active learning is a problem of estimat-
ing classifier effectiveness. The purpose of defin-
ing stopping criterion of active learning is to know 
how much annotation is sufficient for a special task. 
To determine this, this paper proposes a new statis-
tical learning approach, called minimum expected 
error strategy, for defining a stopping criterion 
through estimation of the classifier?s expected er-
ror on future unlabeled examples during the active 
learning process. Experimental results on word 
sense disambiguation and text classification tasks 
show that new proposed minimum expected error 
strategy outperforms the confidence-based strategy, 
and achieves promising results. The interesting 
future work is to study how to combine the best of 
both strategies, and how to consider performance 
change to define an appropriate stopping criterion 
for active learning.  
Acknowledgments 
This work was supported in part by the National 
Natural Science Foundation of China under Grant 
(60473140), the National 863 High-tech Project 
(2006AA01Z154); the Program for New Century 
Excellent Talents in University(NCET-05-0287).  
References 
A. L. Berger, S. A. Della, and V. J  Della. 1996. A 
maximum entropy approach to natural language 
processing. Computational Linguistics 22(1):39?71. 
E Brill. 1992. A simple rule-based part of speech tag-
ger. In the Proceedings of the Third Conference on 
Applied Natural Language Processing. 
J. Chen, A. Schein, L. Ungar, M. Palmer. 2006. An 
empirical study of the behavior of active learning for 
word sense disambiguation. In Proc. of HLT-
NAACL06 
R. O. Duda and P. E. Hart. 1973. Pattern classification 
and scene analysis. New York: Wiley.  
S. A. Engelson and I. Dagan. 1999. Committee-based 
sample selection for probabilistic classifiers. Journal 
of Artificial Intelligence Research. 
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw and R. 
Weischedel. 2006. Ontonotes: The 90% Solution. In 
Proc. of HLT-NAACL06. 
Y.K. Lee and. H.T. Ng. 2002. An empirical evaluation 
of knowledge sources and learning algorithm for 
word sense disambiguation. In Proc. of EMNLP02 
D. D. Lewis and W. A. Gale. 1994. A sequential algo-
rithm for training text classifiers. In Proc. of SIGIR-
94 
M. Li, I. K. Sethi. 2006. Confidence-based active learn-
ing. IEEE transaction on pattern analysis and ma-
chine intelligence, 28(8):1251-1261. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 
1993. Building a large annotated corpus of English: 
the Penn Treebank. Computational Linguistics, 
19(2):313-330 
A. McCallum and K. Nigram. 1998. Employing EM in 
pool-based active learning for text classification. In 
Proc. of 15th ICML 
G. Ngai and D. Yarowsky. 2000. Rule writing or anno-
tation: cost-efficient resource usage for based noun 
phrase chunking. In Proc. of ACL-02  
A. Philpot, E. Hovy and P. Pantel. 2005. The Omega 
Ontology. In Proc. of ONTOLEX Workshop at 
IJCNLP. 
D. Shen, J. Zhang, J. Su, G. Zhou and C. Tan. 2004. 
Multi-criteria-based active learning for named entity 
recognition. In Prof. of ACL-04. 
M. Steedman, R. Hwa, S. Clark, M. Osborne, A. Sakar, 
J. Hockenmaier, P. Ruhlen, S. Baker and J. Crim. 
2003. Example selection for bootstrapping statistical 
parsers. In Proc. of HLT-NAACL-03 
C. A. Thompson, M. E. Califf and R. J. Mooney. 1999. 
Active learning for natural language parsing and in-
formation extraction. In Proc. of ICML-99. 
K. Tomanek, J. Wermter and U. Hahn. 2007. An ap-
proach to text corpus construction which cuts anno-
tation costs and maintains reusability of annotated 
data. In Proc. of EMNLP/CoNLL07 
L. Zhang, J. Zhu, and T. Yao. 2004. An evaluation of 
statistical spam filtering techniques. ACM Transac-
tions on Asian Language Information Processing, 
3(4):243?269. 
J. Zhu, E. Hovy. 2007. Active learning for word sense 
disambiguation with methods for addressing the 
class imbalance problem. In Proc. of 
EMNLP/CoNLL07 
372
Towards Automated Semantic Analysis on Biomedical Research Articles
 
Donghui Feng         Gully Burns         Jingbo Zhu         Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, jingboz, hovy}@isi.edu 
 
Abstract 
In this paper, we present an empirical 
study on adapting Conditional Random 
Fields (CRF) models to conduct semantic 
analysis on biomedical articles using ac-
tive learning. We explore uncertainty-
based active learning with the CRF model 
to dynamically select the most informa-
tive training examples. This abridges the 
power of the supervised methods and ex-
pensive human annotation cost. 
1 Introduction 
Researchers have experienced an increasing need 
for automated/semi-automated knowledge acquisi-
tion from the research literature. This situation is 
especially serious in the biomedical domain where 
the number of individual facts that need to be 
memorized is very high. 
Many successful information extraction (IE) 
systems, work in a supervised fashion, requiring 
human annotations for training. However, human 
annotations are either too expensive or not always 
available and this has become a bottleneck to de-
veloping supervised IE methods to new domains. 
Fortunately, active learning systems design 
strategies to select the most informative training 
examples. This process can achieve certain levels 
of performance faster and reduce human annota-
tion (e.g., Thompson et al, 1999; Shen et al, 2004). 
In this paper, we present an empirical study on 
adapting CRF model to conduct semantic analysis 
on biomedical research literature. We integrate an 
uncertainty-based active learning framework with 
the CRF model to dynamically select the most in-
formative training examples and reduce human 
annotation cost. A systematic study with exhaus-
tive experimental evaluations shows that it can 
achieve satisfactory performance on biomedical 
data while requiring less human annotation. 
Unlike direct estimation on target individuals in 
traditional active learning, we use two heuristic 
certainty scores, peer comparison certainty and set 
comparison certainty, to indirectly estimate se-
quences labeling quality in CRF models. 
We partition biomedical research literature by 
experimental types. In this paper, our goal is to 
analyze various aspects of useful knowledge about 
tract-tracing experiments (TTE). This type of ex-
periments has prompted the development of sev-
eral curated databases but they have only partial 
coverage of the available literature (e.g., Stephan et 
al., 2001). 
2 Related Work 
Knowledge Base Management Systems allow 
individual users to construct personalized 
repositories of knowledge statements based on 
their own interaction with the research literature 
(Stephan et al, 2001; Burns and Cheng, 2006). But 
this process of data entry and curation is manual. 
Current approaches on biomedical text mining (e.g., 
Srinivas et al, 2005; OKanohara et al, 2006) tend 
to address the tasks of named entity recognition or 
relation extraction, and our goal is more complex: 
to extract computational representations of the 
minimum information in a given experiment type. 
Pattern-based IE approaches employ seed data 
to learn useful patterns to pinpoint required fields 
values (e.g. Ravichandran and Hovy, 2002; Mann 
and Yarowsky, 2005; Feng et al, 2006). However, 
this only works if the data corpus is rich enough to 
learn variant surface patterns and does not neces-
sarily generalize to more complex situations, such 
as our domain problem. Within biomedical articles, 
sentences tend to be long and the prose structure 
tends to be more complex than newsprint. 
871
The CRF model (Lafferty et al, 2001) provides 
a compact way to integrate different types of fea-
tures for sequential labeling problems. Reported 
work includes improved model variants (e.g., Jiao 
et al, 2006) and applications such as web data ex-
traction (Pinto et al, 2003), scientific citation ex-
traction (Peng and McCallum, 2004), word align-
ment (Blunsom and Cohn, 2006), and discourse-
level chunking (Feng et al, 2007). 
Pool-based active learning was first successfully 
applied to language processing on text classifica-
tion (Lewis and Gale, 1994; McCallum and Nigam, 
1998; Tong and Koller, 2000). It was also gradu-
ally applied to NLP tasks, such as information ex-
traction (Thompson et al, 1999); semantic parsing 
(Thompson et al, 1999); statistical parsing (Tang 
et al, 2002); NER (Shen et al, 2004); and Word 
Sense Disambiguation (Chen et al, 2006). In this 
paper, we use CRF models to perform a more com-
plex task on the primary TTE experimental results 
and adapt it to process new biomedical data. 
3 Semantic Analysis with CRF Model 
3.1 What knowledge is of interest? 
The goal of TTE is to chart the interconnectivity of 
the brain by injecting tracer chemicals into a region 
of the brain and then identifying corresponding 
labeled regions where the tracer is transported to. 
A typical TTE paper may report experiments about 
one or many labeled regions.  
Name Description 
injectionLocation the named brain region where the injection was made. 
tracerChemical the tracer chemical used. 
labelingLocation the region/location where the labeling was found. 
labelingDescription a description of labeling, den-sity or label type. 
Table 1. Minimum knowledge schema for a TTE. 
 
 
 
 
 
 
 
 
 
 
Figure 1. An extraction example of TTE description. 
In order to construct the minimum information 
required to interpret a TTE, we consider a set of 
specific components as shown in Table 1. 
Figure 1 gives an example of description of a 
complete TTE in a single sentence. In the research 
articles, this information is usually spread over 
many such sentences.  
3.2 CRF Labeling 
We use a plain text sentence for input and attempt 
to label each token with a field label. In addition to 
the four pre-defined fields, a default label, ?O?, is 
used to denote tokens beyond our concern.  
In this task, we consider five types of features 
based on language analysis as shown in Table 2. 
Name Feature Description 
TOPOGRAPHY Is word topog-
raphic? 
BRAIN_REGION Is word a region 
name? 
TRACER Is word a tracer 
chemical? 
DENSITY Is word a density 
term? 
Lexical 
Knowledge 
LABELING_TYPE Does word denote 
a labeling type? 
Surface Word Word Current word 
Context    
Window 
CONT_INJ If current word if 
within a window 
of injection con-
text 
Prev-word Previous word Window 
Words Next-word Next word 
Root-form Root form of the 
word if different 
Gov-verb The governing 
verb 
Subj The sentence 
subject  
Dependency 
Features 
Obj The sentence 
object 
Table 2. The features for system labeling. 
Lexical Knowledge. We define lexical items rep-
resenting different aspects of prior knowledge. To 
this end we use names of brain structures taken 
from brain atlases, standard terms to denote neuro-
anatomical topographical spatial relationships, and 
common sense words for labeling descriptions. We 
collect five separate lexicons as shown in Table 3. 
Lexicons # of terms # of words 
BRAIN_REGION 1123 5536 
DENSITY 8 10 
LABELING_TYPE 9 13 
TRACER 30 30 
TOPOGRAPHY 9 36 
Total 1179 5625 
Table 3. The five lexicons. 
The NY injection ( Fig . 9B ) encompassed  
 
tracerChemical 
most of the pons and was very dense in  
 
injectionLocation 
the region of the MLF. 
 
labelingLocation 
872
Surface word. The word token is an important 
indicator of the probable label for itself.  
Context Window. The TTE is a description of the 
inject-label-findings context. Whenever we find a 
word with a root form of ?injection? or ?deposit?, 
we generate a context window around this word 
and all the words falling into this window are as-
signed a feature of ?CON_INJ?. This means when 
labeling these words the system should consider 
the very current context. 
Window Words. We also use all the words occur-
ring in the window around the current word. We 
set the window size to only include the previous 
and following words (window size = 1).  
Dependency Features. To untangle word relation-
ships within each sentence, we apply the depend-
ency parser MiniPar (Lin, 1998) to parse each sen-
tence, and then derive four types of features. These 
features are (a) root form of word, (b) the subject 
in the sentence, (c) the object in the sentence, and 
(d) the governing verb for each word. 
4 Uncertainty-based Active Learning 
Active learning was initially introduced for 
classification tasks. The intuition is to always add 
the most informative examples to the training set to 
improve the system as much as possible.  
We apply an uncertainty/certainty score-based 
approach. Unlike traditional classification tasks, 
where disagreement or uncertainty is easy to obtain 
on target individuals, information extraction tasks 
in our problem take a whole sequence of tokens 
that might include several slots as processing units. 
We therefore need to make decisions on whether a 
full sequence should be returned for labeling. 
Estimations on confidence for single segments 
in the CRF model have been proposed by (Culotta 
and McCallum, 2004; Kristjannson et al, 2004). 
However as every processing unit in the data set is 
at the sentence level and we make decisions at the 
sentence level to train better sequential labeling 
models, we define heuristic scores at the sentence 
level.  
Symons et al (2006) presents multi-criterion for 
active learning with CRF models, but our motiva-
tion is from a different perspective. The labeling 
result for every sentence corresponds to a decoding 
path in the state transition network. Inspired by the 
decoding and re-ranking approaches in statistical 
machine translation, we use two heuristic scores to 
measure the degree of correctness of the top label-
ing path, namely, peer comparison certainty and 
set comparison certainty. 
Suppose a sentence S includes n words/tokens 
and a labeling path at position m in the ranked N-
best list is represented by ),...,,( 110 ?= nm lllL . Then 
the probability of this labeling path is represented 
by )( mLP , and we have the following two equa-
tions to define the peer comparison certainty 
score, )(SScore peer  and set comparison certainty 
score, )(SScoreset : 
)(
)(
)(
2
1
LP
LP
SScorepeer =                                      (1) 
?
=
=
N
k
k
set
LP
LP
SScore
1
1
)(
)(
)(                                    (2) 
For peer comparison certainty (Eq. 1), we calcu-
late the ratio of the top-scoring labeling path prob-
ability to the second labeling path probability. A 
high ratio means there is a big jump from the top 
labeling path to the second one. The higher the ra-
tio score, the higher the relative degree of correct-
ness for the top labeling path, giving system higher 
confidence for those with higher peer comparison 
certainty scores. Sentences with lowest certainty 
score will be sent to the oracle for manual labeling. 
In the labeling path space, if a labeling path is 
strong enough, its probability score should domi-
nate all the other path scores. In Equation 2, we 
compute the set comparison certainty score by con-
sidering the portion of the probability of the path in 
the overall N-best labeling path space. A large 
value means the top path dominates all the other 
labeling paths together giving the system a higher 
confidence on the current path over others. 
We start with a seed training set including k la-
beled sentences. We then train a CRF model with 
the training data and use it to label unlabeled data. 
The results are compared based on the certainty 
scores and those sentences with the lowest cer-
tainty scores are sent to an oracle for human label-
ing. The new labeled sentences are then added to 
the training set for next iteration.  
5 Experimental Results 
We first investigated how the active learning steps 
could help for the task. Second, we evaluated how 
the CRF labeling system worked with different sets 
of features. We finally applied the model to new 
873
biomedical articles and examined its performance 
on one of its subsets. 
5.1 Experimental Setup 
We have obtained 9474 Journal of Comparative 
Neurology (JCN)1 articles from 1982 to 2005. For 
sentence labeling, we collected 21 TTE articles 
from the JCN corpus. They were converted from 
PDF files to XML files, and all of the article sec-
tions were identified using a simple rule-based ap-
proach. As most of the meaningful descriptions of 
TTEs appear in the Results section, we only proc-
essed the Results section. The 21 files in total in-
clude 2009 sentences, in which 1029 sentences are 
meaningful descriptions for TTEs and 980 sen-
tences are not related to TTEs.  
We randomly split the sentences into a training 
pool and a testing pool, under a ratio 2:1. The 
training pool includes 1338 sentences, with 685 of 
them related to TTEs, while 653 not. Testing was 
based on meaningful sentences in the testing pool. 
Table 4 gives the configurations in the data pools. 
 # of        
Related 
Sentences  
# of        
Unrelated 
Sentences 
Sum 
Training Pool 685 653 1338 
Testing Pool 344 327 671 
Sum 1029 980 2009 
Table 4. Training and testing pool configurations. 
5.2 Evaluation Metrics 
As the label ?O? dominates the data set (70% out 
of all tokens), a simple accuracy score would pro-
vide an inappropriate high score for a baseline sys-
tem that always chooses ?O?. We used Precision, 
Recall, and F_Score to evaluate only meaningful 
labels. 
5.3 How well does active learning work? 
For the active learning procedure, we initially se-
lected a set of seed sentences related to TTEs from 
the training pool. At every step we trained a CRF 
model and labeled sentences in the rest of the train-
ing pool. As described in section 4, those with the 
lowest rank on certainty scores were selected. If 
they are related to a TTE, human annotation will 
be added to the training set. Otherwise, the system 
will keep on selecting sentences until it finds 
enough related sentences. 
                                                 
1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 
People have found active learning in batch mode 
is more efficient, as in some cases a single addi-
tional training example will not improve a classi-
fier/system that much. In our task, we chose the 
bottom k related sentences with the lowest cer-
tainty scores. We conducted various experiments 
for k = 2, 5, and 10. We also compared experi-
ments with passive learning, where at every step 
the new k related sentences were randomly se-
lected from the corpus. Figures 2, 3, and 4 give the 
learning curves for precision, recall, and F_Scores 
when k = 10. 
 
Figure 2. Learning curve for Precision. 
 
Figure 3. Learning curve for Recall. 
 
Figure 4. Learning curve for F_Score. 
From these figures, we can see active learning 
approaches required fewer training examples to 
achieve the same level of performance. As we it-
eratively added new labeled sentences into the 
training set, the precision scores of active learning 
were steadily better than that of passive learning as 
the uncertain examples were added to strengthen 
874
existing labels. However, the recall curve is 
slightly different. Before some point, the recall 
score of passive learning was a little better than 
active learning. The reason is that examples se-
lected by active learning are mainly used to foster 
existing labels but have relatively weaker im-
provements for new labels, while passive learning 
has the freedom to add new knowledge for new 
labels and improve recall scores faster. As we keep 
on using more examples, the active learning 
catches up with and overtakes passive learning on 
recall score. 
These experiments demonstrate that under the 
framework of active learning, examples needed to 
train a CRF model can be greatly reduced and 
therefore make it feasible to adapt to other domains. 
5.4 How well does CRF labeling work? 
As we added selected annotated sentences, the sys-
tem performance kept improving. We investigated 
system performance at the final step when all the 
related sentences in the training pool are selected 
into the training set. The testing set alo only in-
cludes the related sentences. This results in 685 
training sentences and 344 testing sentences. 
To establish a baseline for our labeling task, we 
simply scanned every sentence for words or 
phrases from each lexicon. If the term was present, 
then we labeled the word based on the lexicon in 
which it appeared. If words appeared in multiple 
lexicons, we assigned labels randomly. 
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 5. Precision, Recall, and F_Score for labeling. 
We tried exhaustive feature combinations. Table 
5 shows system performance with different feature 
combinations. All systems performed significantly 
higher than the baseline. The sole use of lexicon 
knowledge produced poor performance, and the 
inclusion of surface words produced significant 
improvement. The use of window words boosted 
precision and recall. The performance with all the 
features generated an F_score of 0.7909. 
We explored how system performance reflects 
different labels. Figure 5 and 6 depict the detailed 
distribution of system labeling from the perspec-
tive of precision and recall respectively for the sys-
tem with the best performance. Most errors oc-
curred in the confusion of injectionLocation and 
labelingLocation, or of the meaningful labels and 
?O?. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 5. Precision confusion matrix distribution. 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
injLoc labelDesp labelLoc tracer
O
injLoc
labelDesp
labelLoc
tracer
 
Figure 6. Recall confusion matrix distribution. 
The worst performance occurred for files that 
distinguish themselves from others by using fairly 
different writing styles. We believe given more 
training data with different writing styles, the sys-
tem could achieve a better overall performance. 
5.5 On New Biomedical Data 
Under this active learning framework, we have 
shown a CRF model can be trained with less anno-
tation cost than using traditional passive learning. 
We adapted the trained CRF model to new bio-
medical research articles. 
Out of the 9474 collected JCN articles, more 
than 230 research articles are on TTEs. The whole 
processing time for each document varies from 20 
seconds to 90 seconds. We sent the new system-
labeled files back to a biomedical knowledge ex-
pert for manual annotation. The time to correct one 
automatically labeled document is dramatically 
reduced, around 1/3 of that spent on raw text. 
We processed 214 new research articles and ex-
amined a subset including 16 articles. We evalu-
875
ated it in two aspects: the overall performance and 
the performance averaged at the document level. 
Table 6 gives the performance on the whole new 
subset and that averaged on 16 documents. The 
performance is a little bit lower than reported in 
the previous section as the new document set might 
include different styles of documents. We exam-
ined system performance at each document. Figure 
7 gives the detailed evaluation for each of the 16 
documents. The average F_Score of the document 
level is around 74%. For those documents with 
reasonable TTE description, the system can 
achieve an F_Score of 87%. The bad documents 
had a different description style and usually mixed 
the TTE descriptions with general discussion.  
 Prec. Recall F_Score 
Overall 0.7683 0.7155 0.7410 
Averaged per Doc. 0.7686 0.7209 0.7418 
Table 6. Performance on the whole new subset and                 
the averaged performance per document. 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
11 4 12 0 6 2 14 7 8 15 3 1 13 10 9 5 Doc No.
Precision 
Recall 
F-score
 
Figure 7. System performance per document. 
6 Conclusions and Future Work 
In this paper, we explored adapting a supervised 
CRF model for semantic analysis on biomedical 
articles using an active learning framework. It 
abridges the power of the supervised approach and 
expensive human costs. We are also investigating 
the use of other certainty measures, such as aver-
aged field confidence scores over each sentence. 
In the long run we wish to generalize the frame-
work to be able to mine other types of experiments 
within the biomedical research literature and im-
pact research in those domains. 
References 
Blunsom, P. and Cohn, T. 2006. Discriminative word align-
ment with conditional random fields. In ACL-2006. 
Burns, G.A. and Cheng, W.C. 2006. Tools for knowledge 
acquisition within the NeuroScholar system and their ap-
plication to anatomical tract-tracing data. In Journal of 
Biomedical Discovery and Collaboration. 
Chen, J., Schein, A., Ungar, L., and Palmer, M. 2006. An em-
pirical study of the behavior of active learning for word 
sense disambiguation. In Proc. of HLT-NAACL 2006.  
Culotta, A. and McCallum, A. 2004. Confidence estimation 
for information extraction. In HLT-NAACL-2004, short pa-
pers. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting data 
records from unstructured biomedical full text. In 
Proc. of EMNLP-CONLL-2007. 
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and 
re-ranking for answering biographical queries on the web. 
In Proc. of AAAI-2006. 
Jiao, F., Wang, S., Lee, C., Greiner, R., and Schuurmans, D. 
2006. Semi-supervised conditional random fields for im-
proved sequence segmentation and labeling. In Proc. of 
ACL-2006. 
Kristjannson, T., Culotta, A., Viola, P., and McCallum, A. 
2004. Interactive information extraction with constrained 
conditional random fields. In Proc. of AAAI-2004.  
Lafferty, J., McCallum, A., and Pereira, F. 2001. Conditional 
random fields: probabilistic models for segmenting and la-
beling sequence data. In Proc. of ICML-2001. 
Lewis, D.D. and Gale, W.A. 1994. A sequential algorithm for 
training text classifiers. In Proc. of SIGIR-1994. 
Lin, D. 1998. Dependency-based evaluation of MINIPAR. In 
Workshop on the Evaluation of Parsing Systems. 
Mann, G.S. and Yarowsky, D. 2005. Multi-field information 
extraction and cross-document fusion. In Proc. of ACL-
2005. 
McCallum, A.K. 2002. MALLET: a machine Learning for 
language toolkit. http://mallet.cs.umass.edu.  
McCallum, A. and Nigam, K. 1998. Employing EM in pool-
based active learning for text classification. In Proc. of 
ICML-98.  
OKanohara, D., Miyao, Y., Tsuruoka, Y., and Tsujii, J. 2006. 
Improving the scalability of semi-markov conditional ran-
dom fields for named entity recognition. In ACL-2006. 
Peng, F. and McCallum, A. 2004. Accurate information ex-
traction from research papers using conditional random 
fields. In Proc. of HLT-NAACL-2004. 
Pinto, D., McCallum, A., Wei, X., and Croft, W.B. 2003. Ta-
ble extraction using conditional random fields. In SIGIR-
2003. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text 
patterns for a question answering system. In ACL-2002.  
Shen, D., Zhang, J., Su, J., Zhou, G., and Tan, C.L. 2004. 
Multi-criteria-based active learning for named entity rec-
ognition. In Proc. of ACL-2004. 
Srinivas, et al, 2005. Comparison of vector space model 
methodologies to reconcile cross-species neuroanatomical 
concepts. Neuroinformatics, 3(2). 
Stephan, K.E., et al, 2001. Advanced database methodology 
for the Collation of Connectivity data on the Macaque 
brain (CoCoMac). Philos Trans R Soc Lond B Biol Sci. 
Symons et al, 2006. Multi-Criterion Active Learning in Con-
ditional Random Fields.  In ICTAI-2006. 
Tang, M., Luo, X., and Roukos, S. 2002. Active learning for 
statistical natural language parsing. In ACL-2002. 
Thompson, C.A., Califf, M.E., and Mooney, R.J. 1999. Active 
learning for natural language parsing and information ex-
traction. In Proc. of ICML-99. 
Tong, S. and Koller, D. 2000. Support vector machine active 
learning with applications to text classification. In Proc. of 
ICML-2000. 
876
c? 2002 Association for Computational Linguistics
Introduction to the Special Issue on
Summarization
Dragomir R. Radev? Eduard Hovy?
University of Michigan USC/ISI
Kathleen McKeown?
Columbia University
1. Introduction and Definitions
As the amount of on-line information increases, systems that can automatically sum-
marize one or more documents become increasingly desirable. Recent research has
investigated types of summaries, methods to create them, and methods to evaluate
them. Several evaluation competitions (in the style of the National Institute of Stan-
dards and Technology?s [NIST?s] Text Retrieval Conference [TREC]) have helped de-
termine baseline performance levels and provide a limited set of training material.
Frequent workshops and symposia reflect the ongoing interest of researchers around
the world. The volume of papers edited by Mani and Maybury (1999) and a book
(Mani 2001) provide good introductions to the state of the art in this rapidly evolving
subfield.
A summary can be loosely defined as a text that is produced from one or more
texts, that conveys important information in the original text(s), and that is no longer
than half of the original text(s) and usually significantly less than that. Text here is
used rather loosely and can refer to speech, multimedia documents, hypertext, etc.
The main goal of a summary is to present the main ideas in a document in less
space. If all sentences in a text document were of equal importance, producing a sum-
mary would not be very effective, as any reduction in the size of a document would
carry a proportional decrease in its informativeness. Luckily, information content in a
document appears in bursts, and one can therefore distinguish between more and less
informative segments. Identifying the informative segments at the expense of the rest
is the main challenge in summarization.
Of the many types of summary that have been identified (Borko and Bernier 1975;
Cremmins 1996; Sparck Jones 1999; Hovy and Lin 1999), indicative summaries provide
an idea of what the text is about without conveying specific content, and informative
ones provide some shortened version of the content. Topic-oriented summaries con-
centrate on the reader?s desired topic(s) of interest, whereas generic summaries reflect
the author?s point of view. Extracts are summaries created by reusing portions (words,
sentences, etc.) of the input text verbatim, while abstracts are created by regenerating
? Assistant Professor, School of Information, Department of Electrical Engineering and Computer Science
and Department of Linguistics, University of Michigan, Ann Arbor. E-mail: radev@umich.edu.
? ISI Fellow and Senior Project Leader, Information Sciences Institute of the University of Southern
California, Marina del Rey, CA. E-mail: hovy@isi.edu.
? Professor, Department of Computer Science, New York University, New York, NY. E-mail:
kathy@cs.columbia.edu.
400
Computational Linguistics Volume 28, Number 4
the extracted content. Extraction is the process of identifying important material in
the text, abstraction the process of reformulating it in novel terms, fusion the process
of combining extracted portions, and compression the process of squeezing out unim-
portant material. The need to maintain some degree of grammaticality and coherence
plays a role in all four processes.
The obvious overlap of text summarization with information extraction, and con-
nections from summarization to both automated question answering and natural lan-
guage generation, suggest that summarization is actually a part of a larger picture.
In fact, whereas early approaches drew more from information retrieval, more re-
cent approaches draw from the natural language field. Natural language generation
techniques have been adapted to work with typed textual phrases, in place of se-
mantics, as input, and this allows researchers to experiment with approaches to ab-
straction. Techniques that have been developed for topic-oriented summaries are now
being pushed further so that they can be applied to the production of long answers
for the question-answering task. However, as the articles in this special issue show,
domain-independent summarization has several specific, difficult aspects that make it
a research topic in its own right.
2. Major Approaches
We provide a sketch of the current state of the art of summarization by describing
the general areas of research, including single-document summarization through ex-
traction, the beginnings of abstractive approaches to single-document summarization,
and a variety of approaches to multidocument summarization.
2.1 Single-Document Summarization through Extraction
Despite the beginnings of research on alternatives to extraction, most work today
still relies on extraction of sentences from the original document to form a summary.
The majority of early extraction research focused on the development of relatively
simple surface-level techniques that tend to signal important passages in the source
text. Although most systems use sentences as units, some work with larger passages,
typically paragraphs. Typically, a set of features is computed for each passage, and
ultimately these features are normalized and summed. The passages with the highest
resulting scores are sorted and returned as the extract.
Early techniques for sentence extraction computed a score for each sentence based
on features such as position in the text (Baxendale 1958; Edmundson 1969), word
and phrase frequency (Luhn 1958), key phrases (e.g., ?it is important to note?) (Ed-
mundson 1969). Recent extraction approaches use more sophisticated techniques for
deciding which sentences to extract; these techniques often rely on machine learning
to identify important features, on natural language analysis to identify key passages,
or on relations between words rather than bags of words.
The application of machine learning to summarization was pioneered by Kupiec,
Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier
to combine features from a corpus of scientific articles and their abstracts. Aone et
al. (1999) and Lin (1999) experimented with other forms of machine learning and its
effectiveness. Machine learning has also been applied to learning individual features;
for example, Lin and Hovy (1997) applied machine learning to the problem of de-
termining how sentence position affects the selection of sentences, and Witbrock and
Mittal (1999) used statistical approaches to choose important words and phrases and
their syntactic context.
401
Radev, Hovy, and McKeown Summarization: Introduction
Approaches involving more sophisticated natural language analysis to identify key
passages rely on analysis either of word relatedness or of discourse structure. Some
research uses the degree of lexical connectedness between potential passages and the
remainder of the text; connectedness may be measured by the number of shared words,
synonyms, or anaphora (e.g., Salton et al 1997; Mani and Bloedorn 1997; Barzilay
and Elhadad 1999). Other research rewards passages that include topic words, that is,
words that have been determined to correlate well with the topic of interest to the user
(for topic-oriented summaries) or with the general theme of the source text (Buckley
and Cardie 1997; Strzalkowski et al 1999; Radev, Jing, and Budzikowska 2000).
Alternatively, a summarizer may reward passages that occupy important positions
in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This
method requires a system to compute discourse structure reliably, which is not possible
in all genres. This technique is the focus of one of the articles in this special issue (Teufel
and Moens 2002), which shows how particular types of rhetorical relations in the genre
of scientific journal articles can be reliably identified through the use of classification.
An open-source summarization environment, MEAD, was recently developed at the
Johns Hopkins summer workshop (Radev et al 2002). MEAD allows researchers to
experiment with different features and methods for combination.
Some recent work (Conroy and O?Leary 2001) has turned to the use of hidden
Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the
probability of inclusion of a sentence in an extract depends on whether the previous
sentence has been included as well.
2.2 Single-Document Summarization through Abstraction
At this early stage in research on summarization, we categorize any approach that
does not use extraction as an abstractive approach. Abstractive approaches have used
information extraction, ontological information, information fusion, and compression.
Information extraction approaches can be characterized as ?top-down,? since they
look for a set of predefined information types to include in the summary (in con-
trast, extractive approaches are more data-driven). For each topic, the user predefines
frames of expected information types, together with recognition criteria. For example,
an earthquake frame may contain slots for location, earthquake magnitude, number of
casualties, etc. The summarization engine must then locate the desired pieces of infor-
mation, fill them in, and generate a summary with the results (DeJong 1978; Rau and
Jacobs 1991). This method can produce high-quality and accurate summaries, albeit in
restricted domains only.
Compressive summarization results from approaching the problem from the point
of view of language generation. Using the smallest units from the original document,
Witbrock and Mittal (1999) extract a set of words from the input document and then
order the words into sentences using a bigram language model. Jing and McKeown
(1999) point out that human summaries are often constructed from the source docu-
ment by a process of cutting and pasting document fragments that are then combined
and regenerated as summary sentences. Hence a summarizer can be developed to
extract sentences, reduce them by dropping unimportant fragments, and then use in-
formation fusion and generation to combine the remaining fragments. In this special
issue, Jing (2002) reports on automated techniques to build a corpus representing the
cut-and-paste process used by humans; such a corpus can then be used to train an
automated summarizer.
Other researchers focus on the reduction process. In an attempt to learn rules for
reduction, Knight and Marcu (2000) use expectation maximization to train a system
to compress the syntactic parse tree of a sentence in order to produce a shorter but
402
Computational Linguistics Volume 28, Number 4
still maximally grammatical version. Ultimately, this approach can likely be used for
shortening two sentences into one, three into two (or one), and so on.
Of course, true abstraction involves taking the process one step further. Abstraction
involves recognizing that a set of extracted passages together constitute something
new, something that is not explicitly mentioned in the source, and then replacing them
in the summary with the (ideally more concise) new concept(s). The requirement that
the new material not be in the text explicitly means that the system must have access
to external information of some kind, such as an ontology or a knowledge base, and be
able to perform combinatory inference (Hahn and Reimer 1997). Since no large-scale
resources of this kind yet exist, abstractive summarization has not progressed beyond
the proof-of-concept stage (although top-down information extraction can be seen as
one variant).
2.3 Multidocument Summarization
Multidocument summarization, the process of producing a single summary of a set
of related source documents, is relatively new. The three major problems introduced
by having to handle multiple input documents are (1) recognizing and coping with
redundancy, (2) identifying important differences among documents, and (3) ensuring
summary coherence, even when material stems from different source documents.
In an early approach to multidocument summarization, information extraction
was used to facilitate the identification of similarities and differences (McKeown and
Radev 1995). As for single-document summarization, this approach produces more of a
briefing than a summary, as it contains only preidentified information types. Identity of
slot values are used to determine when information is reliable enough to include in the
summary. Later work merged information extraction approaches with regeneration of
extracted text to improve summary generation (Radev and McKeown 1998). Important
differences (e.g., updates, trends, direct contradictions) are identified through a set of
discourse rules. Recent work also follows this approach, using enhanced information
extraction and additional forms of contrasts (White and Cardie 2002).
To identify redundancy in text documents, various similarity measures are used.
A common approach is to measure similarity between all pairs of sentences and then
use clustering to identify themes of common information (McKeown et al 1999; Radev,
Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure
the similarity of a candidate passage to that of already-selected passages and retain
it only if it contains enough new (dissimilar) information. A popular such measure is
maximal marginal relevance (MMR) (Carbonell, Geng, and Goldstein 1997; Carbonell
and Goldstein 1998).
Once similar passages in the input documents have been identified, the infor-
mation they contain must be included in the summary. Rather than simply listing
all similar sentences (a lengthy solution), some approaches will select a representa-
tive passage to convey information in each cluster (Radev, Jing, and Budzikowska
2000), whereas other approaches use information fusion techniques to identify repet-
itive phrases from the clusters and combine the phrases into the summary (Barzilay,
McKeown, and Elhadad 1999). Mani, Gates, and Bloedorn (1999) describe the use of
human-generated compression and reformulation rules.
Ensuring coherence is difficult, because this in principle requires some understand-
ing of the content of each passage and knowledge about the structure of discourse.
In practice, most systems simply follow time order and text order (passages from
the oldest text appear first, sorted in the order in which they appear in the input).
To avoid misleading the reader when juxtaposed passages from different dates all
say ?yesterday,? some systems add explicit time stamps (Lin and Hovy 2002a). Other
403
Radev, Hovy, and McKeown Summarization: Introduction
systems use a combination of temporal and coherence constraints to order sentences
(Barzilay, Elhadad, and McKeown 2001). Recently, Otterbacher, Radev, and Luo (2002)
have focused on discourse-based revisions of multidocument clusters as a means for
improving summary coherence.
Although multidocument summarization is new and the approaches described
here are only the beginning, current research also branches out in other directions. Re-
search is beginning on the generation of updates on new information (Allan, Gupta,
and Khandelwal 2001). Researchers are currently studying the production of longer
answers (i.e., multidocument summaries) from retrieved documents, focusing on such
types as biographies of people, descriptions of multiple events of the same type
(e.g., multiple hurricanes), opinion pieces (e.g., editorials and letters discussing a con-
tentious topic), and causes of events. Another challenging ongoing topic is the gener-
ation of titles for either a single document or set of documents. This challenge will be
explored in an evaluation planned by NIST in 2003.
2.4 Evaluation
Evaluating the quality of a summary has proven to be a difficult problem, principally
because there is no obvious ?ideal? summary. Even for relatively straightforward news
articles, human summarizers tend to agree only approximately 60% of the time, mea-
suring sentence content overlap. The use of multiple models for system evaluation
could help alleviate this problem, but researchers also need to look at other methods
that can yield more acceptable models, perhaps using a task as motivation.
Two broad classes of metrics have been developed: form metrics and content met-
rics. Form metrics focus on grammaticality, overall text coherence, and organization
and are usually measured on a point scale (Brandow, Mitze, and Rau 1995). Content is
more difficult to measure. Typically, system output is compared sentence by sentence
or fragment by fragment to one or more human-made ideal abstracts, and as in in-
formation retrieval, the percentage of extraneous information present in the system?s
summary (precision) and the percentage of important information omitted from the
summary (recall) are recorded. Other commonly used measures include kappa (Car-
letta 1996) and relative utility (Radev, Jing, and Budzikowska 2000), both of which take
into account the performance of a summarizer that randomly picks passages from the
original document to produce an extract. In the Document Understanding Conference
(DUC)-01 and DUC-02 summarization competitions (Harman and Marcu 2001; Hahn
and Harman 2002), NIST used the Summary Evaluation Environment (SEE) interface
(Lin 2001) to record values for precision and recall. These two competitions, run along
the lines of TREC, have served to establish overall baselines for single-document and
multidocument summarization and have provided several hundred human abstracts
as training material. (Another popular source of training material is the Ziff-Davis cor-
pus of computer product announcements.) Despite low interjudge agreement, DUC
has shown that humans are better summary producers than machines and that, for
the news article genre, certain algorithms do in fact do better than the simple baseline
of picking the lead material.
The largest task-oriented evaluation to date, the Summarization Evaluation Con-
ference (SUMMAC) (Mani et al 1998; Firmin and Chrzanowski 1999) included three
tests: the categorization task (how well can humans categorize a summary compared
to its full text?), the ad hoc task (how well can humans determine whether a full text is
relevant to a query just from reading the summary?) and the question task (how well
can humans answer questions about the main thrust of the source text from reading
just the summary?). But the interpretation of the results is not simple; studies (Jing et
al. 1998; Donaway, Drummey, and Mather 2000; Radev, Jing, and Budzikowska 2000)
404
Computational Linguistics Volume 28, Number 4
show how the same summaries receive different scores under different measures or
when compared to different (but presumably equivalent) ideal summaries created by
humans. With regard to interhuman agreement, Jing et al find fairly high consistency
in the news genre only when the summary (extract) length is fixed relatively short.
Marcu (1997a) provides some evidence that other genres will deliver less consistency.
With regard to the lengths of the summaries produced by humans when not con-
strained by a particular compression rate, both Jing and Marcu find great variation.
Nonetheless, it is now generally accepted that for single news articles, systems produce
generic summaries indistinguishable from those of humans.
Automated summary evaluation is a gleam in everyone?s eye. Clearly, when an
ideal extract has been created by human(s), extractive summaries are easy to evalu-
ate. Marcu (1999) and Goldstein et al (1999) independently developed an automated
method to create extracts corresponding to abstracts. But when the number of available
extracts is not sufficient, it is not clear how to overcome the problems of low inter-
human agreement. Simply using a variant of the Bilingual Evaluation Understudy
(BLEU) scoring method (based on a linear combination of matching n-grams between
the system output and the ideal summary) developed for machine translation (Pap-
ineni et al 2001) is promising but not sufficient (Lin and Hovy 2002b).
3. The Articles in this Issue
The articles in this issue move beyond the current state of the art in various ways.
Whereas most research to date has focused on the use of sentence extraction for sum-
marization, we are beginning to see techniques that allow a system to extract, merge,
and edit phrases, as opposed to full sentences, to generate a summary. Whereas many
summarization systems are designed for summarization of news, new algorithms are
summarizing much longer and more complex documents, such as scientific journal
articles, medical journal articles, or patents. Whereas most research to date has fo-
cused on text summarization, we are beginning to see a move toward summarization
of speech, a medium that places additional demands on the summarization process.
Finally, in addition to providing full summarization systems, the articles in this issue
also focus on tools that can aid in the process of developing summarization systems,
on computational efficiency of algorithms, and on techniques needed for preprocessing
speech.
The four articles that focus on summarization of text share a common theme:
Each views the summarization process as consisting of two phases. In the first, mate-
rial within the original document that is important is identified and extracted. In the
second, this extracted material may be modified, merged, and edited using genera-
tion techniques. Two of the articles focus on the extraction stage (Teufel and Moens
2002; Silber and McCoy 2002), whereas Jing (2002) examines tools for automatically
constructing resources that can be used for the second stage.
Teufel and Moens propose significantly different techniques for sentence extraction
than have been used in the past. Noting the difference in both length and structure
between scientific articles and news, they claim that both the context of sentences and
a more focused search for sentences is needed in order to produce a good summary
that is only 2.5% of the original document. Their approach is to provide a summary
that focuses on the new contribution of the paper and its relation to previous work.
They rely on rhetorical relations to provide information about context and to identify
sentences relating to, for example, the aim of the paper, its basis in previous work,
or contrasts with other work. Their approach features the use of corpora annotated
both with rhetorical relations and with relevance; it uses text categorization to extract
405
Radev, Hovy, and McKeown Summarization: Introduction
sentences corresponding to any of seven rhetorical categories. The result is a set of
sentences that situate the article in respect to its original claims and in relation to other
research.
Silber and McCoy focus on computationally efficient algorithms for sentence ex-
traction. They present a linear time algorithm to extract lexical chains from a source
document (the lexical-chain approach was originally developed by Barzilay and El-
hadad [1997] but used an exponential time algorithm). This approach facilitates the
use of lexical chains as an intermediate representation for summarization. Barzilay and
Elhadad present an evaluation of the approach for summarization with both scientific
documents and university textbooks.
Jing advocates the use of a cut-and-paste approach to summarization in which
phrases, rather than sentences, are extracted from the original document. She shows
that such an approach is often used by human abstractors. She then presents an auto-
mated tool that is used to analyze a corpus of paired documents and abstracts written
by humans, in order to identify the phrases within the documents that are used in
the abstracts. She has developed an HMM solution to the matching problem. The
decomposition program is a tool that can produce training and testing corpora for
summarization, and its results have been used for her own summarization program.
Saggion and Lapalme (2002) describe a system, SumUM, that generates indicative-
informative summaries from technical documents. To build their system, Saggion and
Lapalme have studied a corpus of professionally written (short) abstracts. They have
manually aligned the abstracts and the original documents. Given the structured form
of technical papers, most of the information in the abstracts was also found in either the
author abstract (20%) or in the first section of the paper (40%) or the headlines or cap-
tions (23%). Based on their observations, the authors have developed an approach to
summarization, called selective analysis, which mimics the human abstractors? routine.
The four components of selective analysis are indicative selection, informative selection,
indicative generation, and informative generation.
The final article in the issue (Zechner 2002) is distinct from the other articles in
that it addresses problems in summarization of speech. As in text summarization,
Zechner also uses sentence extraction to determine the content of the summary. Given
the informal nature of speech, however, a number of significant steps must be taken
in order to identify useful segments for extraction. Zechner develops techniques for
removing disfluencies from speech, for identifying units for extraction that are in
some sense equivalent to sentences, and for identifying relations such as question-
answer across turns in order to determine when units from two separate turns should
be extracted as a whole. This preprocessing yields a transcript on which standard
techniques for extraction in text (here the use of MMR [Carbonell and Goldstein 1998]
to identify relevant units) can operate successfully.
Though true abstractive summarization remains a researcher?s dream, the success
of extractive summarizers and the rapid development of compressive and similar
techniques testifies to the effectiveness with which the research community can address
new problems and find workable solutions to them.
References
Allan, James, Rahul Gupta, and Vikas
Khandelwal. 2001. Temporal summaries
of news topics. In Proceedings of the 24th
Annual International ACM SIGIR Conference
on Research and Development in Information
Retrieval, pages 10?18.
Aone, Chinatsu, Mary Ellen Okurowski,
James Gorlinsky, and Bjornar Larsen.
1999. A trainable summarizer with
knowledge acquired from robust NLP
techniques. In I. Mani and M. T. Maybury,
editors, Advances in Automatic Text
Summarization. MIT Press, Cambridge,
pages 71?80.
Barzilay, Regina and Michael Elhadad. 1997.
406
Computational Linguistics Volume 28, Number 4
Using lexical chains for text
summarization. In Proceedings of the
ACL/EACL?97 Workshop on Intelligent
Scalable Text Summarization, pages 10?17,
Madrid, July.
Barzilay, Regina and Michael Elhadad. 1999.
Using lexical chains for text
summarization. In I. Mani and M. T.
Maybury, editors, Advances in Automatic
Text Summarization. MIT Press,
Cambridge, pages 111?121.
Barzilay, Regina, Noe?mie Elhadad, and
Kathy McKeown. 2001. Sentence ordering
in multidocument summarization. In
Proceedings of the Human Language
Technology Conference.
Barzilay, Regina, Kathleen McKeown, and
Michael Elhadad. 1999. Information
fusion in the context of multi-document
summarization. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics, College Park,
MD, 20?26 June, pages 550?557.
Baxendale, P. B. 1958. Man-made index for
technical literature?An experiment. IBM
Journal of Research and Development,
2(4):354?361.
Borko, H. and C. Bernier. 1975. Abstracting
Concepts and Methods. Academic Press,
New York.
Brandow, Ron, Karl Mitze, and Lisa F. Rau.
1995. Automatic condensation of
electronic publications by sentence
selection. Information Processing and
Management, 31(5):675?685.
Buckley, Chris and Claire Cardie. 1997.
Using empire and smart for
high-precision IR and summarization. In
Proceedings of the TIPSTER Text Phase III
12-Month Workshop, San Diego, CA,
October.
Carbonell, Jaime, Y. Geng, and Jade
Goldstein. 1997. Automated
query-relevant summarization and
diversity-based reranking. In Proceedings
of the IJCAI-97 Workshop on AI in Digital
Libraries, pages 12?19.
Carbonell, Jaime G. and Jade Goldstein.
1998. The use of MMR, diversity-based
reranking for reordering documents and
producing summaries. In Alistair Moffat
and Justin Zobel, editors, Proceedings of the
21st Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Melbourne,
Australia, pages 335?336.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Conroy, John and Dianne O?Leary. 2001.
Text summarization via hidden Markov
models. In Proceedings of the 24th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 406?407.
Cremmins, Edward T. 1996. The Art of
Abstracting. Information Resources Press,
Arlington, VA, second edition.
DeJong, Gerald Francis. 1978. Fast Skimming
of News Stories: The FRUMP System. Ph.D.
thesis, Yale University, New Haven, CT.
Donaway, R. L., K. W. Drummey, and
L. A. Mather. 2000. A comparison of
rankings produced by summarization
evaluation measures. In Proceedings of the
Workshop on Automatic Summarization,
ANLP-NAACL2000, Association for
Computational Linguistics, 30 April,
pages 69?78.
Edmundson, H. P. 1969. New methods in
automatic extracting. Journal of the
Association for Computing Machinery,
16(2):264?285.
Firmin, T. and M. J. Chrzanowski. 1999. An
evaluation of automatic text
summarization systems. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 325?336.
Goldstein, Jade, Mark Kantrowitz, Vibhu O.
Mittal, and Jaime G. Carbonell. 1999.
Summarizing text documents: Sentence
selection and evaluation metrics. In
Research and Development in Information
Retrieval, pages 121?128, Berkeley, CA.
Hahn, Udo and Donna Harman, editors.
2002. Proceedings of the Document
Understanding Conference (DUC-02).
Philadelphia, July.
Hahn, Udo and Ulrich Reimer. 1997.
Knowledge-based text summarization:
Salience and generalization operators for
knowledge base abstraction. In I. Mani
and M. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 215?232.
Harman, Donna and Daniel Marcu, editors.
2001. Proceedings of the Document
Understanding Conference (DUC-01). New
Orleans, September.
Hovy, E. and C.-Y. Lin. 1999. Automated
text summarization in SUMMARIST. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 81?94.
Jing, Hongyan. 2002. Using hidden Markov
modeling to decompose human-written
summaries. Computational Linguistics,
28(4), 527?543.
Jing, Hongyan and Kathleen McKeown.
1999. The decomposition of
human-written summary sentences. In
407
Radev, Hovy, and McKeown Summarization: Introduction
M. Hearst, F. Gey, and R. Tong, editors,
Proceedings of SIGIR?99: 22nd International
Conference on Research and Development in
Information Retrieval, University of
California, Berkeley, August,
pages 129?136.
Jing, Hongyan, Kathleen McKeown, Regina
Barzilay, and Michael Elhadad. 1998.
Summarization evaluation methods:
Experiments and analysis. In Intelligent
Text Summarization: Papers from the 1998
AAAI Spring Symposium, Stanford, CA,
23?25 March. Technical Report SS-98-06.
AAAI Press, pages 60?68.
Knight, Kevin and Daniel Marcu. 2000.
Statistics-based summarization?Step one:
Sentence compression. In Proceedings of the
17th National Conference of the American
Association for Artificial Intelligence
(AAAI-2000), pages 703?710.
Kupiec, Julian, Jan O. Pedersen, and
Francine Chen. 1995. A trainable
document summarizer. In Research and
Development in Information Retrieval,
pages 68?73.
Lin, C. and E. Hovy. 1997. Identifying topics
by position. In Fifth Conference on Applied
Natural Language Processing, Association
for Computational Linguistics, 31
March?3 April, pages 283?290.
Lin, Chin-Yew. 1999. Training a selection
function for extraction. In Proceedings of
the Eighteenth Annual International ACM
Conference on Information and Knowledge
Management (CIKM), Kansas City, 6
November. ACM, pages 55?62.
Lin, Chin-Yew. 2001. Summary evaluation
environment.
http://www.isi.edu/cyl/SEE.
Lin, Chin-Yew and Eduard Hovy. 2002a.
From single to multi-document
summarization: A prototype system and
its evaluation. In Proceedings of the 40th
Conference of the Association of
Computational Linguistics, Philadelphia,
July, pages 457?464.
Lin, Chin-Yew and Eduard Hovy. 2002b.
Manual and automatic evaluation of
summaries. In Proceedings of the Document
Understanding Conference (DUC-02)
Workshop on Multi-Document Summarization
Evaluation at the ACL Conference,
Philadelphia, July, pages 45?51.
Luhn, H. P. 1958. The automatic creation of
literature abstracts. IBM Journal of Research
Development, 2(2):159?165.
Mani, Inderjeet. 2001. Automatic
Summarization. John Benjamins,
Amsterdam/Philadelphia.
Mani, Inderjeet and Eric Bloedorn. 1997.
Multi-document summarization by graph
search and matching. In Proceedings of the
Fourteenth National Conference on Artificial
Intelligence (AAAI-97), Providence, RI.
American Association for Artificial
Intelligence, pages 622?628.
Mani, Inderjeet, Barbara Gates, and Eric
Bloedorn. 1999. Improving summaries by
revising them. In Proceedings of the 37th
Annual Meeting of the Association for
Computational Linguistics (ACL 99), College
Park, MD, June, pages 558?565.
Mani, Inderjeet, David House, G. Klein,
Lynette Hirshman, Leo Obrst, The?re`se
Firmin, Michael Chrzanowski, and Beth
Sundheim. 1998. The TIPSTER SUMMAC
text summarization evaluation. Technical
Report MTR 98W0000138, The Mitre
Corporation, McLean, VA.
Mani, Inderjeet and Mark Maybury, editors.
1999. Advances in Automatic Text
Summarization. MIT Press, Cambridge.
Marcu, Daniel. 1997a. From discourse
structures to text summaries. In
Proceedings of the ACL?97/EACL?97 Workshop
on Intelligent Scalable Text Summarization,
Madrid, July 11, pages 82?88.
Marcu, Daniel. 1997b. The Rhetorical Parsing,
Summarization, and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto, Toronto.
Marcu, Daniel. 1999. The automatic
construction of large-scale corpora for
summarization research. In M. Hearst,
F. Gey, and R. Tong, editors, Proceedings of
SIGIR?99: 22nd International Conference on
Research and Development in Information
Retrieval, University of California,
Berkeley, August, pages 137?144.
Marcu, Daniel and Laurie Gerber. 2001. An
inquiry into the nature of multidocument
abstracts, extracts, and their evaluation. In
Proceedings of the NAACL-2001 Workshop on
Automatic Summarization, Pittsburgh, June.
NAACL, pages 1?8.
McKeown, Kathleen, Judith Klavans,
Vasileios Hatzivassiloglou, Regina
Barzilay, and Eleazar Eskin. 1999.
Towards multidocument summarization
by reformulation: Progress and prospects.
In Proceedings of the 16th National
Conference of the American Association for
Artificial Intelligence (AAAI-1999), 18?22
July, pages 453?460.
McKeown, Kathleen R. and Dragomir R.
Radev. 1995. Generating summaries of
multiple news articles. In Proceedings of the
18th Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Seattle, July,
pages 74?82.
Ono, K., K. Sumita, and S. Miike. 1994.
408
Computational Linguistics Volume 28, Number 4
Abstract generation based on rhetorical
structure extraction. In Proceedings of the
International Conference on Computational
Linguistics, Kyoto, Japan, pages 344?348.
Otterbacher, Jahna, Dragomir R. Radev, and
Airong Luo. 2002. Revisions that improve
cohesion in multi-document summaries:
A preliminary study. In ACL Workshop on
Text Summarization, Philadelphia.
Papineni, K., S. Roukos, T. Ward, and W-J.
Zhu. 2001. BLEU: A method for automatic
evaluation of machine translation.
Research Report RC22176, IBM.
Radev, Dragomir, Simone Teufel, Horacio
Saggion, Wai Lam, John Blitzer, Arda
C?elebi, Hong Qi, Elliott Drabek, and
Danyu Liu. 2002. Evaluation of text
summarization in a cross-lingual
information retrieval framework.
Technical Report, Center for Language
and Speech Processing, Johns Hopkins
University, Baltimore, June.
Radev, Dragomir R., Hongyan Jing, and
Malgorzata Budzikowska. 2000.
Centroid-based summarization of
multiple documents: Sentence extraction,
utility-based evaluation, and user studies.
In ANLP/NAACL Workshop on
Summarization, Seattle, April.
Radev, Dragomir R. and Kathleen R.
McKeown. 1998. Generating natural
language summaries from multiple
on-line sources. Computational Linguistics,
24(3):469?500.
Rau, Lisa and Paul Jacobs. 1991. Creating
segmented databases from free text for
text retrieval. In Proceedings of the 14th
Annual International ACM-SIGIR Conference
on Research and Development in Information
Retrieval, New York, pages 337?346.
Saggion, Horacio and Guy Lapalme. 2002.
Generating indicative-informative
summaries with SumUM. Computational
Linguistics, 28(4), 497?526.
Salton, G., A. Singhal, M. Mitra, and
C. Buckley. 1997. Automatic text
structuring and summarization.
Information Processing & Management,
33(2):193?207.
Silber, H. Gregory and Kathleen McCoy.
2002. Efficiently computed lexical chains
as an intermediate representation for
automatic text summarization.
Computational Linguistics, 28(4), 487?496.
Sparck Jones, Karen. 1999. Automatic
summarizing: Factors and directions. In
I. Mani and M. T. Maybury, editors,
Advances in Automatic Text Summarization.
MIT Press, Cambridge, pages 1?13.
Strzalkowski, Tomek, Gees Stein, J. Wang,
and Bowden Wise. 1999. A robust
practical text summarizer. In I. Mani and
M. T. Maybury, editors, Advances in
Automatic Text Summarization. MIT Press,
Cambridge, pages 137?154.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and rhetorical
status. Computational Linguistics, 28(4),
409?445.
White, Michael and Claire Cardie. 2002.
Selecting sentences for multidocument
summaries using randomized local
search. In Proceedings of the Workshop on
Automatic Summarization (including DUC
2002), Philadelphia, July. Association for
Computational Linguistics, New
Brunswick, NJ, pages 9?18.
Witbrock, Michael and Vibhu Mittal. 1999.
Ultra-summarization: A statistical
approach to generating highly condensed
non-extractive summaries. In Proceedings
of the 22nd Annual International ACM SIGIR
Conference on Research and Development in
Information Retrieval, Berkeley,
pages 315?316.
Zechner, Klaus. 2002. Automatic
summarization of open-domain
multiparty dialogues in diverse genres.
Computational Linguistics, 28(4), 447?485.
 Automatic Evaluation of Summaries Using N-gram  
Co-Occurrence Statistics 
Chin-Yew Lin and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{cyl,hovy}@isi.edu 
 
Abstract 
Following the recent adoption by the machine 
translation community of automatic evalua-
tion using the BLEU/NIST scoring process, 
we conduct an in-depth study of a similar idea 
for evaluating summaries. The results show 
that automatic evaluation using unigram co-
occurrences between summary pairs correlates 
surprising well with human evaluations, based 
on various statistical metrics; while direct ap-
plication of the BLEU evaluation procedure 
does not always give good results. 
1 Introduction 
Automated text summarization has drawn a lot of inter-
est in the natural language processing and information 
retrieval communities in the recent years. A series of 
workshops on automatic text summarization (WAS 
2000, 2001, 2002), special topic sessions in ACL, 
COLING, and SIGIR, and government sponsored 
evaluation efforts in the United States (DUC 2002) and 
Japan (Fukusima and Okumura 2001) have advanced 
the technology and produced a couple of experimental 
online systems (Radev et al 2001, McKeown et al 
2002). Despite these efforts, however, there are no 
common, convenient, and repeatable evaluation meth-
ods that can be easily applied to support system devel-
opment and just-in-time comparison among different 
summarization methods. 
The Document Understanding Conference (DUC 2002) 
run by the National Institute of Standards and Technol-
ogy (NIST) sets out to address this problem by provid-
ing annual large scale common evaluations in text 
summarization. However, these evaluations involve 
human judges and hence are subject to variability (Rath 
et al 1961). For example, Lin and Hovy (2002) pointed 
out that 18% of the data contained multiple judgments 
in the DUC 2001 single document evaluation1.  
To further progress in automatic summarization, in this 
paper we conduct an in-depth study of automatic 
evaluation methods based on n-gram co-occurrence in 
the context of DUC. Due to the setup in DUC, the 
evaluations we discussed here are intrinsic evaluations 
(Sp?rck Jones and Galliers 1996). Section 2 gives an 
overview of the evaluation procedure used in DUC. 
Section 3 discusses the IBM BLEU (Papineni et al 
2001) and NIST (2002) n-gram co-occurrence scoring 
procedures and the application of a similar idea in 
evaluating summaries. Section 4 compares n-gram co-
occurrence scoring procedures in terms of their correla-
tion to human results and on the recall and precision of 
statistical significance prediction. Section 5 concludes 
this paper and discusses future directions. 
2 Document Understanding Conference 
The 2002 Document Understanding Conference2 in-
cluded the follow two main tasks: 
? Fully automatic single-document summarization: 
given a document, participants were required to 
create a generic 100-word summary.  The training 
set comprised 30 sets of approximately 10 docu-
ments each, together with their 100-word human 
written summaries.  The test set comprised 30 un-
seen documents. 
? Fully automatic multi-document summarization: 
given a set of documents about a single subject, 
participants were required to create 4 generic sum-
maries of the entire set, containing 50, 100, 200, 
and 400 words respectively.  The document sets 
were of four types: a single natural disaster event; a 
                                                           
1 Multiple judgments occur when more than one performance 
score is given to the same system (or human) and human sum-
mary pairs by the same human judge. 
2 DUC 2001 and DUC 2002 have similar tasks, but summaries 
of 10, 50, 100, and 200 words are requested in the multi-
document task in DUC 2002. 
                                                               Edmonton, May-June 2003
                                                               Main Papers , pp. 71-78
                                                         Proceedings of HLT-NAACL 2003
 single event; multiple instances of a type of event; 
and information about an individual.  The training 
set comprised 30 sets of approximately 10 docu-
ments, each provided with their 50, 100, 200, and 
400-word human written summaries.  The test set 
comprised 30 unseen sets. 
A total of 11 systems participated in the single-
document summarization task and 12 systems partici-
pated in the multi-document task.   
2.1 Evaluation Materials 
For each document or document set, one human sum-
mary was created as the ?ideal? model summary at each 
specified length.  Two other human summaries were 
also created at each length.  In addition, baseline sum-
maries were created automatically for each length as 
reference points.  For the multi-document summariza-
tion task, one baseline, lead baseline, took the first 50, 
100, 200, and 400 words in the last document in the 
collection.  A second baseline, coverage baseline, took 
the first sentence in the first document, the first sentence 
in the second document and so on until it had a sum-
mary of 50, 100, 200, or 400 words. Only one baseline 
(baseline1) was created for the single document summa-
rization task. 
2.2 Summary Evaluation Environment 
To evaluate system performance NIST assessors who 
created the ?ideal? written summaries did pairwise com-
parisons of their summaries to the system-generated 
summaries, other assessors? summaries, and baseline 
summaries.  They used the Summary Evaluation Envi-
ronment (SEE) 2.0 developed by (Lin 2001) to support 
the process.  Using SEE, the assessors compared the 
system?s text (the peer text) to the ideal (the model 
text).  As shown in Figure 1, each text was decomposed 
into a list of units and displayed in separate windows. 
SEE 2.0 provides interfaces for assessors to judge both 
the content and the quality of summaries.  To measure 
content, assessors step through each model unit, mark 
all system units sharing content with the current model 
unit (green/dark gray highlight in the model summary 
window), and specify that the marked system units ex-
press all, most, some, or hardly any of the content of the 
Figure 1. SEE in an evaluation session. 
 current model unit.  To measure quality, assessors rate 
grammaticality3, cohesion4, and coherence5 at five dif-
ferent levels: all, most, some, hardly any, or none6.  For 
example, as shown in Figure 1, an assessor marked sys-
tem units 1.1 and 10.4 (red/dark underlines in the left 
pane) as sharing some content with the current model 
unit 2.2 (highlighted green/dark gray in the right). 
2.3 Evaluation Metrics 
Recall at different compression ratios has been used in 
summarization research to measure how well an auto-
matic system retains important content of original 
documents (Mani et al 1998). However, the simple sen-
tence recall measure cannot differentiate system per-
formance appropriately, as is pointed out by Donaway 
et al (2000). Therefore, instead of pure sentence recall 
score, we use coverage score C. We define it as fol-
lows7: 
)1(
summary model in the MUs ofnumber  Total
  marked) MUs of(Number EC ?=
E, the ratio of completeness, ranges from 1 to 0: 1 for 
all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 
for none.  If we ignore E (set it to 1), we obtain simple 
sentence recall score.  We use average coverage scores 
derived from human judgments as the references to 
evaluate various automatic scoring methods in the fol-
lowing sections.  
3 BLEU and N-gram Co-Occurrence 
To automatically evaluate machine translations the ma-
chine translation community recently adopted an n-gram 
co-occurrence scoring procedure BLEU (Papineni et al 
2001). The NIST (NIST 2002) scoring metric is based 
on BLEU. The main idea of BLEU is to measure the 
translation closeness between a candidate translation 
and a set of reference translations with a numerical met-
ric. To achieve this goal, they used a weighted average 
of variable length n-gram matches between system 
translations and a set of human reference translations 
and showed that a weighted average metric, i.e. BLEU, 
correlating highly with human assessments.  
Similarly, following the BLEU idea, we assume that the 
closer an automatic summary to a professional human 
                                                           
3 Does the summary observe English grammatical rules inde-
pendent of its content? 
4 Do sentences in the summary fit in with their surrounding 
sentences?  
5 Is the content of the summary expressed and organized in an 
effective way? 
6 These category labels are changed to numerical values of 
100%, 80%, 60%, 40%, 20%, and 0% in DUC 2002. 
7 DUC 2002 uses a length adjusted version of coverage metric 
C?, where C? = ?*C + (1-?)*B. B is the brevity and ? is a pa-
rameter reflecting relative importance (DUC 2002). 
summary, the better it is. The question is: ?Can we ap-
ply BLEU directly without any modifications to evalu-
ate summaries as well??. We first ran IBM?s BLEU 
evaluation script unmodified over the DUC 2001 model 
and peer summary set. The resulting Spearman rank 
order correlation coefficient (?) between BLEU and the 
human assessment for the single document task is 0.66 
using one reference summary and 0.82 using three ref-
erence summaries; while Spearman ? for the multi-
document task is 0.67 using one reference and 0.70 us-
ing three. These numbers indicate that they positively 
correlate at ? = 0.018. Therefore, BLEU seems a prom-
ising automatic scoring metric for summary evaluation. 
According to Papineni et al (2001), BLEU is essentially 
a precision metric. It measures how well a machine 
translation overlaps with multiple human translations 
using n-gram co-occurrence statistics. N-gram precision 
in BLEU is computed as follows: 
? ?
? ?
? ??
? ??
?
?
=
}{
}{
)(
)(
CandidatesC Cgramn
CandidatesC Cgramn
clip
n gramnCount
gramnCount
p   (2) 
Where Countclip(n-gram) is the maximum number of n-
grams co-occurring in a candidate translation and a ref-
erence translation, and Count(n-gram) is the number of 
n-grams in the candidate translation. To prevent very 
short translations that try to maximize their precision 
scores, BLEU adds a brevity penalty, BP, to the for-
mula: 
)3(
1
|)|/||1( ??
?
??
?
?
>
=
? rcife
rcif
BP cr  
Where |c| is the length of the candidate translation and 
|r| is the length of the reference translation. The BLEU 
formula is then written as follows: 
)4(logexp
1
??
???
?
?= ?
=
N
n
nn pwBPBLEU  
N is set at 4 and wn, the weighting factor, is set at 1/N. 
For summaries by analogy, we can express equation (1) 
in terms of n-gram matches following equation (2): 
)5(
)(
)(
}{
}{ ? ?
? ?
? ??
? ??
?
?
=
UnitsModelC Cgramn
UnitsModelC Cgramn
match
n gramnCount
gramnCount
C  
Where Countmatch(n-gram) is the maximum number of 
n-grams co-occurring in a peer summary and a model 
unit and Count(n-gram) is the number of n-grams in the 
model unit. Notice that the average n-gram coverage 
score, Cn, as shown in equation 5 is a recall metric 
                                                           
8 The number of instances is 14 (11 systems, 2 humans, and 1 
baseline) for the single document task and is 16 (12 systems, 2 
humans, and 2 baselines) for the multi-document task. 
 instead of a precision one as pn. Since the denominator 
of equation 5 is the total sum of the number of n-grams 
occurring at the model summary side instead of the peer 
side and only one model summary is used for each 
evaluation; while there could be multiple references 
used in BLEU and Countclip(n-gram) could come from 
matching different reference translations. Furthermore, 
instead of a brevity penalty that punishes overly short 
translations, a brevity bonus, BB, should be awarded to 
shorter summaries that contain equivalent content. In 
fact, a length adjusted average coverage score was used 
as an alternative performance metric in DUC 2002. 
However, we set the brevity bonus (or penalty) to 1 for 
all our experiments in this paper. In summary, the n-
gram co-occurrence statistics we use in the following 
sections are based on the following formula: 
)6(logexp),( ???
?
???
?
?= ?
=
j
in
nn CwBBjiNgram  
Where j ? i, i and j range from 1 to 4, and wn is 1/(j-
i+1). Ngram(1, 4) is a weighted variable length n-gram 
match score similar to the IBM BLEU score; while 
Ngram(k, k), i.e. i = j = k, is simply the average k-gram 
coverage score Ck.  
With these formulas, we describe how to evaluate them 
in the next section. 
4 Evaluations of N-gram Co-Occurrence 
Metrics 
In order to evaluate the effectiveness of automatic 
evaluation metrics, we propose two criteria: 
1. Automatic evaluations should correlate highly, 
positively, and consistently with human assess-
ments. 
2. The statistical significance of automatic evaluations 
should be a good predictor of the statistical signifi-
cance of human assessments with high reliability. 
The first criterion ensures whenever a human recognizes 
a good summary/translation/system, an automatic 
evaluation will do the same with high probability. This 
enables us to use an automatic evaluation procedure in 
place of human assessments to compare system per-
formance, as in the NIST MT evaluations (NIST 2002). 
The second criterion is critical in interpreting the sig-
nificance of automatic evaluation results. For example, 
if an automatic evaluation shows there is a significant 
difference between run A and run B at ? = 0.05 using 
the z-test (t-test or bootstrap resampling), how does this 
translate to ?real? significance, i.e. the statistical signifi-
cance in a human assessment of run A and run B? Ide-
ally, we would like there to be a positive correlation 
between them. If this can be asserted with strong reli-
ability (high recall and precision), then we can use the 
automatic evaluation to assist system development and 
to be reasonably sure that we have made progress. 
4.1 Correlation with Human Assessments 
As stated in Section 3, direct application of BLEU on 
the DUC 2001 data showed promising results. However, 
BLEU is a precision-based metric while the human 
evaluation protocol in DUC is essentially recall-based. 
We therefore prefer the metric given by equation 6 and 
use it in all our experiments. Using DUC 2001 data, we 
compute average Ngram(1,4) scores for each  peer sys-
tem at different summary sizes and rank systems ac-
cording to their scores. We then compare the 
Ngram(1,4) ranking with the human ranking. Figure 2 
shows the result of DUC 2001 multi-document data. 
Stopwords are ignored during the computation of 
Ngram(1,4) scores and words are stemmed using a Por-
ter stemmer (Porter 1980). The x-axis is the human 
ranking and the y-axis gives the corresponding 
Ngram(1,4) rankings for summaries of difference sizes. 
The straight line marked by AvgC is the ranking given 
by human assessment. For example, a system at (5,8) 
Table 1. Spearman rank order correlation coeffi-
cients of different DUC 2001 data between 
Ngram(1, 4)n rankings and human rankings includ-
ing (S) and excluding (SX) stopwords. SD-100 is 
for single document summaries of 100 words and 
MD-50, 100, 200, and 400 are for multi-document 
summaries of 50, 100, 200, and 400 words. MD-All 
averages results from summaries of all sizes.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Human Ranking
N
g
ra
m
(1
, 4
)n
 R
an
ki
n
g
AvgC
Ngram(1, 4)50
Ngram(1, 4)100
Ngram(1, 4)200
Ngram(1, 4)400
Ngram(1, 4)all
Figure 2. Scatter plot of Ngram(1,4)n score rank-
ings versus human ranking for the multi-
document task data from DUC 2001. The same 
system is at each vertical line with ranking given 
by different Ngram(1,4)n scores. The straight line 
(AvgC) is the human ranking and n marks sum-
maries of different sizes. Ngram(1,4)all combines 
results from all sizes. 
SD-100 MD-All MD-50 MD-100 MD-200 MD-400
SX 0.604 0.875 0.546 0.575 0.775 0.861
S 0.615 0.832 0.646 0.529 0.814 0.843
 means that human ranks its performance at the 5th rank 
while Ngram(1,4)400 ranks it at the 8th. If an automatic 
ranking fully matches the human ranking, its plot will 
coincide with the heavy diagonal. A line with less de-
viation from the heavy diagonal line indicates better 
correlation with the human assessment. 
To quantify the correlation, we compute the Spearman 
rank order correlation coefficient (?) for each N-
gram(1,4)n run at different summary sizes (n). We also 
test the effect of inclusion or exclusion of stopwords. 
The results are summarized in Table 1. 
Although these results are statistically significant (? = 
0.025) and are comparable to IBM BLEU?s correlation 
figures shown in Section 3, they are not consistent 
across summary sizes and tasks. For example, the corre-
lations of the single document task are at the 60% level; 
while they range from 50% to 80% for the multi-
document task. The inclusion or exclusion of stopwords 
also shows mixed results. In order to meet the require-
ment of the first criterion stated in Section 3, we need 
better results. 
The Ngram(1,4)n score is a weighted average of variable 
length n-gram matches. By taking a log sum of the n-
gram matches, the Ngram(1,4)n favors match of longer 
n-grams. For example, if ?United States of America? 
occurs in a reference summary, while one peer sum-
mary, A, uses ?United States? and another summary, B, 
uses the full phrase ?United States of America?, sum-
mary B gets more contribution to its overall score sim-
ply due to the longer version of the name. However, 
intuitively one should prefer a short version of the name 
in summarization. Therefore, we need to change the 
weighting scheme to not penalize or even reward shorter 
equivalents. We conduct experiments to understand the 
effect of individual n-gram co-occurrence scores in ap-
proximating human assessments. Tables 2 and 3 show 
the results of these runs without and with stopwords 
respectively. 
For each set of DUC 2001 data, single document 100-
word summarization task, multi-document 50, 100, 200, 
and 400 -word summarization tasks, we compute 4 dif-
ferent correlation statistics: Spearman rank order corre-
lation coefficient (Spearman ?),  linear regression t-test 
(LRt, 11 degree of freedom for single document task and 
13 degree of freedom for multi-document task), Pearson 
product moment coefficient of correlation (Pearson ?), 
and coefficient of determination (CD) for each 
Ngram(i,j) evaluation metric. Among them Spearman ? 
is a nonparametric test, a higher number indicates 
higher correlation; while the other three tests are para-
metric tests. Higher LRt, Pearson ?, and CD also sug-
gests higher linear correlation.  
Analyzing all runs according to Tables 2 and 3, we 
make the following observations: 
(1) Simple unigram, Ngram(1,1), and bi-gram, 
Ngram(2,2), co-occurrence statistics consistently 
outperform (0.99 ? Spearman ? ? 0.75) the 
weighted average of n-gram of variable length 
Ngram(1, 4) (0.88 ? Spearman ? ? 0.55) in single 
and multiple document tasks when stopwords are 
ignored. Importantly, unigram performs especially 
well with Spearman ? ranging from 0.88 to 0.99 
that is better than the best case in which weighted 
average of variable length n-gram matches is used 
and is consistent across different data sets. 
(2) The performance of weighted average n-gram 
scores is in the range between bi-gram and tri-gram 
co-occurrence scores. This might suggest some 
summaries are over-penalized by the weighted av-
erage metric due to the lack of longer n-gram 
matches. For example, given a model string 
?United States, Japan, and Taiwan?, a candidate 
Table 3. Various Ngram(i, j) rank/score correlations 
for 4 different statistics (with stopwords). 
Table 2. Various Ngram(i,j) rank/score correlations 
for 4 different statistics (without stopwords): Spear-
man rank order coefficient correlation (Spearman ?), 
linear regression t-test (LRt), Pearson product mo-
ment coefficient of correlation (Pearson ?), and co-
efficient of determination (CD).  
Ngram (1,4) Ngram (1,1) Ngram (2,2) Ngram (3,3) Ngram (4,4)
Single Doc Spearman ? 0.604 0.989 0.868 0.527 0.505
100 LRt 1.025 7.130 2.444 0.704 0.053
Pearson ? 0.295 0.907 0.593 0.208 0.016
CD 0.087 0.822 0.352 0.043 0.000
Multi-Doc Spearman ? 0.875 0.993 0.950 0.782 0.736
All LRt 3.910 13.230 5.830 3.356 2.480
Pearson ? 0.735 0.965 0.851 0.681 0.567
CD 0.540 0.931 0.723 0.464 0.321
Multi-Doc Spearman ? 0.546 0.879 0.746 0.496 0.343
50 LRt 2.142 5.681 3.350 2.846 2.664
Pearson ? 0.511 0.844 0.681 0.620 0.594
CD 0.261 0.713 0.463 0.384 0.353
Multi-Doc Spearman ? 0.575 0.896 0.761 0.543 0.468
100 LRt 2.369 7.873 3.641 1.828 1.385
Pearson ? 0.549 0.909 0.711 0.452 0.359
CD 0.301 0.827 0.505 0.204 0.129
Multi-Doc Spearman ? 0.775 0.979 0.904 0.782 0.754
200 LRt 3.243 15.648 4.929 2.772 2.126
Pearson ? 0.669 0.974 0.807 0.609 0.508
CD 0.447 0.950 0.651 0.371 0.258
Multi-Doc Spearman ? 0.861 0.982 0.961 0.854 0.661
400 LRt 4.390 10.569 6.409 3.907 2.755
Pearson ? 0.773 0.946 0.872 0.735 0.607
CD 0.597 0.896 0.760 0.540 0.369
Ngram (1,4) Ngram (1,1) Ngram (2,2) Ngram (3,3) Ngram (4,4)
Single Doc Spearman ? 0.615 0.951 0.863 0.615 0.533
100 LRt 1.076 4.873 2.228 0.942 0.246
Pearson ? 0.309 0.827 0.558 0.273 0.074
CD 0.095 0.683 0.311 0.075 0.005
Multi-Doc Spearman ? 0.832 0.918 0.936 0.832 0.732
All LRt 3.752 6.489 5.451 3.745 2.640
Pearson ? 0.721 0.874 0.834 0.720 0.591
CD 0.520 0.764 0.696 0.519 0.349
Multi-Doc Spearman ? 0.646 0.586 0.650 0.589 0.600
50 LRt 2.611 2.527 2.805 2.314 1.691
Pearson ? 0.587 0.574 0.614 0.540 0.425
CD 0.344 0.329 0.377 0.292 0.180
Multi-Doc Spearman ? 0.529 0.636 0.625 0.571 0.468
100 LRt 2.015 3.338 2.890 2.039 1.310
Pearson ? 0.488 0.679 0.625 0.492 0.342
CD 0.238 0.462 0.391 0.242 0.117
Multi-Doc Spearman ? 0.814 0.964 0.879 0.814 0.746
200 LRt 3.204 10.134 4.926 3.328 2.173
Pearson ? 0.664 0.942 0.807 0.678 0.516
CD 0.441 0.888 0.651 0.460 0.266
Multi-Doc Spearman ? 0.843 0.914 0.946 0.857 0.721
400 LRt 4.344 5.358 6.344 4.328 3.066
Pearson ? 0.769 0.830 0.869 0.768 0.648
CD 0.592 0.688 0.756 0.590 0.420
 string ?United States, Taiwan, and Japan? has a 
unigram score of 1, bi-gram score of 0.5, and tri-
gram and 4-gram scores of 0 when the stopword 
?and? is ignored. The weighted average n-gram 
score for the candidate string is 0.  
(3) Excluding stopwords in computing n-gram co-
occurrence statistics generally achieves better cor-
relation than including stopwords. 
4.2 Statistical Significance of N-gram Co-
Occurrence Scores versus Human As-
sessments 
We have shown that simple unigram, Ngram(1,1), or bi-
gram, Ngram(2,2), co-occurrence statistics based on 
equation 6 outperform the weighted average of n-gram 
matches, Ngram(1,4), in the previous section. To exam-
ine how well the statistical significance in the automatic 
Ngram(i,j) metrics translates to real significance when 
human assessments are involved, we set up the follow-
ing test procedures: 
(1) Compute pairwise statistical significance test such 
as z-test or t-test for a system pair (X,Y) at certain ? 
level, for example ? = 0.05, using automatic met-
rics and human assigned scores. 
(2) Count the number of cases a z-test indicates there is 
a significant difference between X and Y based on 
the automatic metric. Call this number NAs. 
(3) Count the number of cases a z-test indicates there is 
a significant difference between X and Y based on 
the human assessment. Call this number NHs. 
(4) Count the cases when an automatic metric predicts 
a significant difference and the human assessment 
also does. Call this Nhit. For example, if a z-test in-
dicates system X is significantly different from Y 
with ? = 0.05 based on the automatic metric scores 
and the corresponding z-test also suggests the same 
based on the human agreement, then we have a hit. 
(5) Compute the recall and precision using the follow-
ing formulas: 
recall = 
 Hs
hit
N
N
 
precision = 
 As
hit
N
N
 
A good automatic metric should have high recall and 
precision. This implies that if a statistical test indicates a 
significant difference between two runs using the auto-
matic metric then very probably there is also a signifi-
cant difference in the manual evaluation. This would be 
very useful during the system development cycle to 
gauge if an improvement is really significant or not. 
Figure 3 shows the recall and precision curves for the 
DUC 2001 single document task at different ? levels 
and Figure 4 is for the multi-document task with differ-
ent summary sizes. Both of them exclude stopwords. 
We use z-test in all the significance tests with ? level at 
0.10, 0.05, 0.25, 0.01, and 0.005. 
From Figures 3 and 4, we can see Ngram(1,1) and 
Ngram(2,2) reside on the upper right corner of the recall 
and precision graphs. Ngram(1,1) has the best overall 
behavior. These graphs confirm Ngram(1,1) (simple 
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Significance Predication Recall and Precision Curve
Ngram(1,4)
Ngram(1,1)
Ngram(2,2)
Ngram(3,3)
Ngram(4,4)
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Recall
Pr
ec
is
io
n
Significance Predication Recall and Precision Curve
Ngram(1,4)
Ngram(1,1)
Ngram(2,2)
Ngram(3,3)
Ngram(4,4)
Figure 3. Recall and precision curves of N-
gram co-occurrence statistics versus human 
assessment for DUC 2001 single document 
task. The 5 points on each curve represent val-
ues for the 5 ? levels. 
Figure 4. Recall and precision curves of N-gram 
co-occurrence statistics versus human assessment 
for DUC 2001 multi-document task. Dark (black) 
solid lines are for average of all summary sizes, 
light (red) solid lines are for 50-word summaries, 
dashed (green) lines are for 100-word summaries, 
dash-dot lines (blue) are for 200-word summaries, 
and dotted (magenta) lines are for 400-word 
summaries.
 unigram) is a good automatic scoring metric with good 
statistical significance prediction power. 
5 Conclusions 
In this paper, we gave a brief introduction of the manual 
summary evaluation protocol used in the Document 
Understanding Conference. We then discussed the IBM 
BLEU MT evaluation metric, its application to sum-
mary evaluation, and the difference between precision-
based BLEU translation evaluation and recall-based 
DUC summary evaluation. The discrepancy led us to 
examine the effectiveness of individual n-gram co-
occurrence statistics as a substitute for expensive and 
error-prone manual evaluation of summaries. To evalu-
ate the performance of automatic scoring metrics, we 
proposed two test criteria. One was to make sure system 
rankings produced by automatic scoring metrics were 
similar to human rankings. This was quantified by 
Spearman?s rank order correlation coefficient and three 
other parametric correlation coefficients. Another was 
to compare the statistical significance test results be-
tween automatic scoring metrics and human assess-
ments. We used recall and precision of the agreement 
between the test statistics results to identify good auto-
matic scoring metrics. 
According to our experiments, we found that unigram 
co-occurrence statistics is a good automatic scoring 
metric. It consistently correlated highly with human 
assessments and had high recall and precision in signifi-
cance test with manual evaluation results. In contrast, 
the weighted average of variable length n-gram matches 
derived from IBM BLEU did not always give good cor-
relation and high recall and precision. We surmise that a 
reason for the difference between summarization and 
machine translation might be that extraction-based 
summaries do not really suffer from grammar problems, 
while translations do. Longer n-grams tend to score for 
grammaticality rather than content. 
It is encouraging to know that the simple unigram co-
occurrence metric works in the DUC 2001 setup. The 
reason for this might be that most of the systems par-
ticipating in DUC generate summaries by sentence ex-
traction. We plan to run similar experiments on DUC 
2002 data to see if unigram does as well. If it does, we 
will make available our code available via a website to 
the summarization community. 
Although this study shows that unigram co-occurrence 
statistics exhibit some good properties in summary 
evaluation, it still does not correlate to human assess-
ment 100% of the time. There is more to be desired in 
the recall and precision of significance test agreement 
with manual evaluation. We are starting to explore vari-
ous metrics suggested in Donaway et al (2000). For 
example, weight n-gram matches differently according 
to their information content measured by tf, tfidf, or 
SVD. In fact, NIST MT automatic scoring metric (NIST 
2002) already integrates such modifications. 
One future direction includes using an automatic ques-
tion answer test as demonstrated in the pilot study in 
SUMMAC (Mani et al 1998). In that study, an auto-
matic scoring script developed by Chris Buckley 
showed high correlation with human evaluations, al-
though the experiment was only tested on a small set of 
3 topics. 
According to Over (2003), NIST spent about 3,000 man 
hours each in DUC 2001 and 2002 for topic and docu-
ment selection, summary creation, and manual evalua-
tion. Therefore, it would be wise to use these valuable 
resources, i.e. manual summaries and evaluation results, 
not only in the formal evaluation every year but also in 
developing systems and designing automatic evaluation 
metrics. We would like to propose an annual automatic 
evaluation track in DUC that encourages participants to 
invent new automated evaluation metrics. Each year the 
human evaluation results can be used to evaluate the 
effectiveness of the various automatic evaluation met-
rics. The best automatic metric will be posted at the 
DUC website and used as an alternative in-house and 
repeatable evaluation mechanism during the next year. 
In this way the evaluation technologies can advance at 
the same pace as the summarization technologies im-
prove. 
References 
Donaway, R.L., Drummey, K.W., and Mather, L.A. 
2000. A Comparison of Rankings Produced by 
Summarization Evaluation Measures. In Proceeding 
of the Workshop on Automatic Summarization, post-
conference workshop of ANLP-NAACL-2000, pp. 
69-78, Seattle, WA, 2000. 
DUC. 2002. The Document Understanding Conference.  
http://duc.nist.gov.  
Fukusima, T. and Okumura, M. 2001. Text Summariza-
tion Challenge: Text Summarization Evaluation at 
NTCIR Workshop2. In Proceedings of the Second 
NTCIR Workshop on Research in Chinese & Japa-
nese Text Retrieval and Text Summarization, NII, 
Tokyo, Japan, 2001. 
Lin, C.-Y. 2001. Summary Evaluation Environment.  
http://www.isi.edu/~cyl/SEE. 
Lin, C.-Y. and E. Hovy. 2002. Manual and Automatic 
Evaluations of Summaries. In Proceedings of the 
Workshop on Automatic Summarization, post-
conference workshop of ACL-2002, pp. 45-51, Phila-
delphia, PA, 2002. 
McKeown, K., R. Barzilay, D. Evans, V. Hatzivassi-
loglou, J. L. Klavans, A. Nenkova, C. Sable, B. 
Schiffman, S. Sigelman. Tracking and Summarizing 
 News on a Daily Basis with Columbia?s Newsblaster. 
In Proceedings of Human Language Technology 
Conference 2002 (HLT 2002). San Diego, CA, 2002. 
Mani, I., D. House, G. Klein, L. Hirschman, L. Obrst, T. 
Firmin, M. Chrzanowski, and B. Sundheim. 1998. 
The TIPSTER SUMMAC Text Summarization 
Evaluation: Final Report.  MITRE Corp. Tech. Re-
port. 
NIST. 2002. Automatic Evaluation of Machine Transla-
tion Quality using N-gram Co-Occurrence Statistics. 
Over, P. 2003. Personal Communication.  
Papineni, K., S. Roukos, T. Ward, W.-J. Zhu. 2001. 
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. IBM Research Report RC22176 
(W0109-022). 
Porter, M. F. 1980. An Algorithm for Suffix Stripping. 
Program, 14, pp. 130-137. 
Radev, D. R., S. Blair-Goldensohn, Z. Zhang, and R. 
S. Raghavan. Newsinessence: A System for Domain-
Independent, Real-Time News Clustering and Multi-
Document Summarization. In Proceedings of human 
Language Technology Conference (HLT 2001), San 
Diego, CA, 2001. 
Sp?rck Jones, K. and J. R. Galliers. 1996. Evaluating 
Natural Language Processing Systems: An Analysis 
and Review. New York: Springer. 
Rath, G.J., Resnick, A., and Savage, T.R. 1961. The 
Formation of Abstracts by the Selection of Sen-
tences. American Documentation, 12(2), pp. 139-
143. Reprinted in Mani, I., and Maybury, M., eds, 
Advances in Automatic Text Summarization, MIT 
Press, pp. 287-292. 
WAS. 2000. Workshop on Automatic Summarization, 
post-conference workshop of ANLP-NAACL-2000, 
Seattle, WA, 2000. 
WAS. 2001. Workshop on Automatic Summarization, 
pre-conference workshop of NAACL-2001, Pitts-
burgh, PA, 2001. 
WAS. 2002. Workshop on Automatic Summarization, 
post-conference workshop of ACL-2002, Philadel-
phia, PA, 2002. 
A Web-Trained Extraction Summarization System 
 
Liang Zhou and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, hovy}@isi.edu 
 
 
 
 
Abstract 
 
A serious bottleneck in the development of 
trainable text summarization systems is the 
shortage of training data. Constructing such 
data is a very tedious task, especially because 
there are in general many different correct 
ways to summarize a text. Fortunately we can 
utilize the Internet as a source of suitable 
training data. In this paper, we present a 
summarization system that uses the web as the 
source of training data. The procedure involves 
structuring the articles downloaded from 
various websites, building adequate corpora of 
(summary, text) and (extract, text) pairs, 
training on positive and negative data, and 
automatically learning to perform the task of 
extraction-based summarization at a level 
comparable to the best DUC systems.  
 
 
1    Introduction 
 
    The task of an extraction-based text summarizer is to 
select from a text the most important sentences that are 
in size a small percentage of the original text yet still as 
informative as the full text (Kupiec et al, 1995). 
Typically, trainable summarization systems characterize 
each sentence according to a set of predefined features 
and then learn from training material which feature 
combinations are indicative of good extract sentences. 
In order to learn the characteristics of indicative 
summarizing sentences, a large enough collection of 
(summary, text) pairs must be provided to the system.  
    Research in automated text summarization is 
constantly troubled by the difficulty of finding or 
constructing large collections of (extract, text) pairs. 
Usually, (abstract, text) pairs are available and can be 
easily obtained (though not in sufficient quantity to 
support fully automated learning for large domains). But 
abstract sentences are not identical to summary 
sentences and hence make direct comparison difficult. 
Therefore, some algorithms have been introduced to 
generate (extract, text) pairs expanded from (abstract, 
text) inputs (Marcu, 1999).  
    The explosion of the World Wide Web has made 
accessible billions of documents and newspaper articles. 
If one could automatically find short forms of longer 
documents, one could build large training sets over 
time. However, one cannot today retrieve short and long 
texts on the same topic directly.  
    News published on the Internet is an exception. 
Although it is not ideally organized, the topic 
orientation and temporal nature of news makes it 
possible to impose an organization and thereby obtain a 
training corpus on the same topic. We hypothesize that 
weekly articles are sophisticated summaries of daily 
ones, and monthly articles are summaries of weekly 
ones, as shown in Figure 1. Under this hypothesis, how 
accurate an extract summarizer can one train? In this 
paper we first describe the corpus reorganization, then 
in Section 3 the training data formulation and the 
system, the system evaluation in Section 4, and finally 
future work in Section 5.  
 
 daily 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
weekly 
monthly 
Figure 1. Corpus structure. 
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 205-211
                                                         Proceedings of HLT-NAACL 2003
2    Corpus Construct
    
2.1 Download Initial Collection 
 
    The Yahoo Full Coverage Collection (YFCC) was 
downloaded from http://fullcoverage.yahoo.com during 
December 2001. The full coverage texts were 
downloaded based on a snapshot of the links
in Yahoo Full Coverage at that time. A spid
the top eight categories: U.S., World, Business, 
Technolo tertainment, 
Sports. A y were saved i
index page that contained the headline and its full text 
URL. A page fetcher then nloaded all the pag
listed in the snapshot index
    Under the eight ca
subcategories, 216590 new
 
2.2 Preprocessing 
 
    All the articles in the 
following. Each article is i
with actual contents buried
and markings. Identifying t
process (Finn et al, 2001
main body of the article
templates, and then f
information embedded in th
each opening and closing ta
name indicates the conten
closing tags are images or j
discarded.  
    The clean texts are the
breaker, Lovin?s stemmer, 
converted into standard XM
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
organization 
    The news articles posted under Yahoo Full Coverage 
are from 125 different web publishers. Except for some 
well-known sites, the publishing frequencies for the rest 
of the sites are not known. But Yahoo tends to use those 
publishers over and over again, leaving for each 
r a tra ublishing habit. Our system records 
ishing or each article from each publisher 
chronologically, and then calculates the publishing 
frequency for each publisher. Over all the articles from 
blisher,  comp the minimum 
publishing gap (MPG) between two articles. If the MPG 
is less than 3 days or the MPG is unknown in the case of 
publishers seen  once in the YFCC, then this 
3           6        7  dow
 file.  
tegories, there are 46
s articles.  
YFCC are preprocessed 
n the original raw html for
 in layers of irrelevant ta
he text body is a challengin
). The system identifies t
 using a set of retriev
urther eliminates usele
e main body by considerin
g set. For example, if the t
ts between the opening an
ust meta-info, the contents
n processed by a senten
a part-of-speech tagger, an
L form.  
A
1999 
       8     10 1
1       2 
Figure 2: The hierarchicaes 3 
as 
m 
gs 
g 
he 
al 
ss 
g 
ag 
d 
 is 
ce 
d 
publisher is labe
greater than 3 d
weekly publishe
are labeled as mo
    For each artic
it as a daily, w
domain under e
restructured into
year, weeks of 
week. The vis
hierarchical stru
category World i
 
3 System 
 
    Recognizing 
from the web re
constant shortag
taking a closer e
one notices that f
 
 
 
 
 
 
 
frica 
2000
 
 
 
 
 
 
 
 
 
 
 
 
 
    2    3    ? 1
        3            4 W
Day
Articles 
l structure for domain onlygy, Science, Health, En
ll news links in each categorand 
n an a pu the systemled as a daily pub
ays but less than 
r. Publishers with 
nthly publishers. 
le in the collectio
eekly, or month
ach category in t
 a hierarchy by ye
each month, and 
ualization of a
cture of the d
s shown in Figure
(summary, text) 
pository is the ke
e of summarizatio
xamination of th
or each day, there
2001
    2    3    ? 
eek
Africa.  utes  contained 
er crawled 
publishe
the publil of p
 date fion 2.3 Chronological Re
 lisher. If the MPG is 
15, it is labeled as a 
all other MPG values 
 
n, the system relabels 
ly publication. Each 
he collection is then 
ar, months within the 
finally days of each 
n example of the 
omain Africa under 
 2.  
pairs automatically 
y to overcoming the 
n training data. After 
e reorganized YFCC, 
 are a number of       
Year 
Month 
articles published that update the progress of a particular 
news topic. Daily articles are published by identified 
daily publishers. Then at the end of each week, there are 
several weekly articles published by weekly publishers 
on the same topic. At the end of each month, again there 
are articles on the same topic posted by publishers 
labeled as monthly publishers. There is a common 
thematic connection between the daily articles and the 
weekly articles, and between the weekly articles and the 
monthly articles. The daily articles on a particular event 
are more detailed, and are written step-by-step as it was 
happening. The weekly articles review the daily articles 
and recite important snippets from the daily news. The 
monthly articles are written in a more condensed 
fashion quoting from the weeklies. 
    Instead of asking human judges to identify 
informative sentences in documents, and since 
beautifully written ?summaries? are already available, 
we need to align the sentences from the daily articles 
with each weekly article sentence, and align weekly 
article sentences with each monthly article sentence, in 
order to collect the (summary, text) pairs and eventually 
generate (extract, text) pairs. The pairs are constructed 
at both sentence and document levels. 
 
3.1 Alignment 
 
    In our system, three methods for sentence-level and 
document-level alignment are investigated: 
? extraction-based: Marcu (1999) introduces an 
algorithm that produces corresponding extracts 
given (abstract, text) tuples with maximal 
semantic similarity. We duplicated this algorithm 
but replaced inputs with (summary, text), parents 
and their respective children in the hierarchical 
domain tree. Thus for example, the summary is a 
monthly article when the text is a weekly article or 
a weekly article when the text is a daily one. We 
start with the cosine-similarity metric stated in 
(Marcu 1999) and keep deleting sentences that are 
not related to the summary document until any 
more deletion would result in a drop in similarity 
with the summary. The resulting set of sentences is 
the extract concerning the topic discussed in the 
summary. It forms the pair (extract, text). If there 
is more than one summary for a particular text 
(nonsummary article), the resulting extracts will 
vary if the summary articles are written on the 
same event, but are focused on different 
perspectives. Thus, a summary article may be 
aligned with several extracts and extracts 
generated from a single text may align with many 
summaries. The relationship amongst summaries, 
extracts, and texts forms a network topology.  
    To generate sentence level alignment, we 
replaced the input with (summary sentence, text) 
pairs. Starting  with  a  nonsummary text, 
the sentences that are irrelevant to the summary 
sentence are deleted repeatedly, resulting in the 
preservation of sentences similar in meaning to the 
summary sentence. For each sentence in the 
summary, it is aligned with a number of 
nonsummary sentences to form (summary 
sentence, nonsummary sentences) pairs.  This 
alignment is done for each sentence of the 
summary articles. Finally for each nonsummary 
we group together all the aligned sentences to form 
the pair (extract, text).  
? similarity-based: inspired by sentence alignment 
for multilingual parallel corpora in Machine 
Translation (Church, 1993; Fung and Church, 
1994; Melamed, 1999), we view the alignment 
between sentences from summaries and sentences 
from nonsummaries as the alignment of 
monolingual parallel texts at the sentence level. In 
every domain of the YFCC, each article is 
represented as a vector in a vector space where 
each dimension is a distinct non-stop word 
appearing in this domain. Measuring the cosine-
similarity between two articles, we can decide 
whether they are close semantically. This method 
has been widely used in Information Retrieval 
(Salton, 1975). To extend this idea, we measure 
the cosine-similarity between two sentences, one 
from a summary (weekly or monthly article) and 
the other one from a nonsummary (daily or weekly 
article). If the similarity score between the two 
crosses a predetermined threshold, the two 
sentences are aligned to form the pair (summary 
sentence, text sentence). The relationship between 
sentences is many-to-many. With any particular 
nonsummary article, sentences that are aligned 
with summary sentences form the extract and the 
pair (extract, text).  
? summary-based: concerned with the noise that 
may accompany similarity calculations from 
extraction-based and similarity-based alignments, 
we align an entire summary article with all its 
nonsummary articles published in the same time 
period, as determined from the previously 
described chronological reorganization. The 
alignment results are pairs of the format (summary, 
texts). One summary can only be aligned with a 
certain group of nonsummaries. Each nonsummary 
can be aligned with many summaries. No sentence 
level alignment is done with this method. 
 
 
 
 
3.2 Training Data 
 
    The main goal of a leaning-based extraction 
summarization system is to learn the ability to judge 
whether a particular sentence in a text appear in the 
extract or not. Therefore, two sets of training data are 
needed, one indicative enough for the system to select a 
sentence to be in the extract (labeled as positive data), 
the other indicative enough for the system to keep the 
sentence from being added to the extract (labeled as 
negative data). For each of the alignment methods, we 
produce summary training data and nonsummary 
training data for each domain in the YFCC. 
    From extraction-based and similarity-based alignment 
methods, for each nonsummary article, there are two 
sets of sentences, the set of sentences that compose the 
extract with the respect to some summary article or 
align with summary sentences, and the rest of the 
sentences that are not related to the summary or aligned. 
The two sets of sentences over all articles in the domain 
form the positive and negative training data sets.  
    Using summary-based alignment, all the summary 
articles are in the positive training set, and all the 
nonsummary material is in the negative set. Full texts 
are used.  
 
3.3 Bigram Estimates Extract Desirability 
 
    We treat each domain independently. Using a bigram 
model, we estimate the desirability of a sentence 
appearing in the extract P(S) from the summary training 
data as: 
 
    P(S) = P(w1 | start) P(w2 | w1)?P(wn | wn-1) 
 
  We estimate the desirability of a sentence not 
appearing in the extract P?(S) from the nonsummary 
training data as: 
     
    P?(S) = P?(w1 | start) P?(w2 | w1)?P?(wn | wn-1) 
 
    For each domain in the YFCC, a summary bigram 
table and a nonsummary bigram table are created.  
 
3.4 Extraction Process 
 
    Zajic et al (2002) used a Hidden Markov Model as 
part of their headline generation system. In our system, 
we started with a similar idea of a lattice for summary 
extraction. In Figure 3, E states emit sentences that are 
going to be in the extract, and N states emit all other 
sentences. Given an input sentence, if P(S) is greater 
than P?(S), it means that the sentence has a higher 
desirability of being an extraction sentence; otherwise, 
the sentence will not be included in the resulting extract.  
 
 
 
 
 
 
 
 
 
 
After reading in the last sentence from the input, the 
extract is created by traversing the path from start state 
to end state and only outputting the sentences emitted 
by the E states.  
    The extracts generated are in size shorter than the 
original texts. However, the number of sentences that E 
states emit cannot be predetermined. This results in 
unpredictable extract length. Most frequently, longer 
extracts are produced. The system needs more control 
over how long extracts will be in order for meaningful 
evaluation to be conducted.  
    To follow up on the lattice idea, we used the 
following scoring mechanism: 
 
R = P(S) / P?(S) 
 
R indicates the desirability ratio of the sentence being in 
the extract over it being left out. For each sentence from 
the input, it is assigned an R score. Then all the 
sentences with their R scores are sorted in descending 
order. With respect to the length restriction, we choose 
only the top n R-scored sentences.  
 
3.5 Selecting the Training Domain 
 
    There are 463 domains under the 8 categories of 
YFCC, meaning 463 paired summary-bigram and 
nonsummary-bigram tables. On average for each 
domain, the summary-bigram table contains 20000 
entries; the nonsummary-bigram table contains 173000 
entries. When an unknown text or a set of unknown 
texts come in to be summarized, the system needs to 
select the most appropriate pair of bigram tables to 
create the extract. The most desirable domain for an 
unknown text or texts contains articles focusing on the 
same issues as the unknown ones. Two methods are 
used: 
? topic signature (Lin and Hovy, 2000): a topic 
signature is a family of related terms {topic, 
signature}, where topic is the target concept and 
signature is a vecto  related ms. The topic in 
e formula is assigned with the domain ame. To 
nstruct the set of related words, w consider 
N2 N3 N1
es
E2 E3 E1 
Figure 3. Lattice. th
co
only nou because
major issues discus
those issues evolver of are on
sed in the d
d. Each n terinterested in the ns  we ly 
omain, n
oun in th n
e ot in how 
e domain 
receives a tf.idf score. 30 top-scoring nouns are 
selected to be the signature representing the 
domain. For each test text, its signature is 
computed with the same tf.idf method against each 
domain. The domain that has the highest number 
of overlaps in signature words is selected and its 
bigram tables are used to construct the extract of 
the test text. The following table illustrates. Inputs 
are three sets of 10 documents each from the 
DUC01 training corpus concerning the topics on 
Africa, earthquake, and Iraq, respectively. The 
scores are the total overlaps between a domain and 
each individual test set. The Three sets are all 
correctly classified.  
 
domain/input Africa Earthquake Iraq 
Africa 24 10 9 
Earthquake 7 20 8 
Iraq 48 8 97 
 
? hierarchical signature: each domain is given a 
name when it was downloaded. The name gives a 
description of the domain at the highest level. 
Since the name is the most informative word, if we 
gather the words that most frequently co-occur 
within the sentence(s) that contain the name itself, 
a list of less informative but still important words 
can become part of the domain signature. Using 
this list of words, we find another list of words that 
most frequently co-occur with each of them 
individually. Therefore, a three-layer hierarchical 
domain signature can be created: level one, the 
domain name; level two, 10 words with the highest 
co-occurrences with the domain name; level three, 
10 words that most frequently co-occur with level 
two signatures. Again only nouns are considered. 
For example, for domain on Iraq, the level one 
signature is ?Iraq?; level two signatures are 
?Saddam?, ?sanction?, ?weapon?, ?Baghdad?, and 
etc.; third level signatures are ?Gulf?, ?UN?, 
?Arab?, ?security?, etc. The document signature 
for the test text is computed the same way as in the 
topic signature method. Overlap between the 
domain signature and the document signature is 
computed with a different scoring system, in 
which the weights are chosen by hand. If level one 
is matched, add 10 points; for each match at level 
two, add 2 points; for each match at level three, 
add 1 point. The domain that receives the highest 
points will be selected. A much deeper signature 
hierarchy can be created recursively. Through 
experiment, we see that a three-level signature 
suffices. The following table shows the effects of 
this method: 
 
domain/input Africa Earthquake Iraq 
Africa 86 7 41 
Earthquake 7 74 0 
Iraq 15 26 202 
 
    Since it worked well for our test domains, we 
employed the topic-signature method in selecting 
training domains.  
 
4 Evaluation 
 
4.1 Alignment Choice 
 
    To determine which of the alignment methods of 
Section 3.1 is best, we need true summaries, not 
monthly or weekly articles from the web. We tested the 
equivalencies of the three methods on three sets of 
articles from the DUC01 training corpus, which 
includes human-generated ?gold standard? summaries. 
They are on the topics of Africa, earthquake, and Iraq. 
The following table shows the results of this 
experiment. Each entry demonstrates the cosine 
similarity, using the tf.idf score, of the extracts 
generated by the system using training data created from 
the alignment method in the column, compare to the 
summaries generated by human.  
 
 extraction similarity summary 
Africa 0.273 0.304 0.293 
Earthquake 0.318 0.332 0.342 
Iraq 0.234 0.246 0.247 
 
    We see that all three methods produce roughly equal 
extracts, when compared with the gold standard 
summaries. The summary-based alignment method is 
the least time consuming and the most straightforward 
method to use in practice.  
 
4.2 System Performance 
 
    There are 30 directories in the DUC01 testing corpus. 
All articles in each directory are used to make the 
selection of its corresponding training domain, as 
described in Section 3.5. Even if no domain completely 
covers the event, the best one is selected by the system.  
    To evaluate system performance on summary 
creation, we randomly selected one article from each 
directory from the DUC01 testing corpus, for each 
article, there are three human produced summaries. Our 
system summarizes each article three times with the 
length restriction respectively set to the lengths of the 
three human summaries. We also evaluated the DUC01 
single-document summarization baseline system results 
(first 100 words from each document) to set a lower 
bound. To see the upper bound, each human generated 
summary is judged against the other two human 
summaries on the same article. DUC01 top performer, 
system from SMU, in single-document summarization, 
was also evaluated. In all, 30 * 3! human summary 
judgments, 30 * 3 baseline summary judgments, 30 
SMU system judgments, and 30 * 3 system summary 
judgments are made. The following table is the 
evaluation results using the SEE system version 1.0 (Lin 
2002), with visualization in Figure 4. Summary model 
units are graded as full, partial, or none in completeness 
in coverage with the peer model units. And Figure 5 
shows an example of the comparison between the 
human-created summary and the system-generated 
extract.  
 
 SRECALL SPRECISON LRECALL LPRECISION 
Baseline 0.246 0.306 0.301 0.396 
System 0.452 0.341 0.577 0.509 
SMU 0.499 0.482 0.583 0.672 
Human 0.542 0.500 0.611 0.585 
 
Performance Evaluation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
SRECALL SPRECI LRECALL LPRECI
baseline
system
SMU
human
 
 
 
 
    The performance is reported on four metrics. Recall 
measures how well a summarizer retains original 
content. Precision measures how well a system 
generates summaries. SRECALL and SPRECISION are 
the strict recall and strict precision that take into 
consideration only units with full completeness in unit 
coverage. LRECALL and LPRECISION are the lenient 
recall and lenient precision that count units with partial 
and full completeness in unit coverage. Extract 
summaries that are produced by our system has 
comparable performance in recall with SMU, meaning 
that the coverage of important information   is   good.   
But    our   system   shows weakness in precision due to 
the fact that each sentence in the system-generated 
extract is not compressed in any way. Each sentence in 
the extract has high coverage over the human summary. 
But sentences that have no value have also been 
included in the result. This causes long extracts on 
average, hence, the low average in precision measure. 
Since our sentence ranking mechanism is based on 
desirability, sentences at the end of the extract are less 
desirable and can be removed. This needs further 
investigation. Clearly there is the need to reduce the size 
of the generated summaries. In order to produce simple 
and concise extracts, sentence compression needs to be 
performed (Knight and Marcu, 2000).  
    Despite the problems, however, our system?s 
performance places it at equal level to the top-scoring 
systems in DUC01. Now that the DUC02 material is 
also available, we will compare our results to their top-
scoring system as well.  
 
4.3 Conclusion 
 
    One important stage in developing a learning-based 
extraction summarization system is to find sufficient 
and relevant collections of (extract, text) pairs. This task 
is also the most difficult one since resources of 
constructing the pairs are scarce. To solve this 
bottleneck, one wonders whether the  web can be seen 
as a vast repository that is waiting to be tailored in order 
to fulfill our quest in finding summarization training 
data. We have discovered a way to find short forms of 
longer documents and have built an extraction-based 
summarizer learning from reorganizing news articles 
from the World Wide Web and performing at a level 
comparable to DUC01 systems. We are excited about 
the power of how reorganization of the web news 
articles has brought us and will explore this idea in other 
tasks of natural language processing.  
 
5 Future Work 
 
Figure 4. System performance.         Multi-document summarization naturally comes 
into picture for future development. Our corpus 
organization itself is in the form of multiple articles 
being summarized into one (monthly or weekly). How 
do we learn and use this structure to summarize a new 
set of articles? 
    Headline generation is another task that we can 
approach equipped with our large restructured web 
corpus.  
    We believe that the answers to these questions are 
embedded in the characteristics of the corpus, namely 
the WWW, and are eager to discover them in the near 
future.  
 
Acknowledgement 
 
    We want to thank Dr. Chin-Yew Lin for making the 
Yahoo Full Coverage Collection download.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
References 
 
Kenneth W. Church. 1993. Char align: A program for 
aligning parallel texts at the character level. In 
Proceedings of the Workshop on Very Large 
Corpora: Academic and Industrial Perspectives, 
ACL. Association for Computational Linguistics, 
1993. 
 
Aidan Finn, Nicholas Kushmerick, and Barry Smyth. 
2001. Fact or fiction: Content classification for 
digital libraries. In Proceedings of NSF/DELOS 
Workshop on Personalization and Recommender.  
 
Pascale Fung and Kenneth W. Church. 1994. K-vec: A 
new approach for aligning parallel texts. In 
Proceedings from the 15th International Conference 
on Computational Linguistics, Kyoto. 
 
Kevin Knight and Daniel Marcu (2000). Statistics-
Based Summarization--Step One: Sentence 
Compression. The 17th National Conference of the 
American Association for Artificial Intelligence 
AAAI'2000, Outstanding Paper Award, Austin, 
Texas, July 30-August 3, 2000. 
 
Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. 
A trainable document summarizer. In SIGIR ?95, 
Proceedings of the 18th Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval. Seattle, Washington, USA., 
pages 68-73. ACM Press. 
 
 Human-produced summary System-produced summary 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
A major earthquake registering 7.2 on the Richter 
scale shook the Solomon Islands in the South 
Pacific today .  
 
It was the largest earthquake in the Solomons since 
a 7.4 quake on Nov . 5 , 1978 and the strongest in 
the world in the five months .  
 
An 8.3 quake hit the Macquarie Islands south of 
Australia on May 23 .  
 
The preliminary reading of 7.2 is slightly stronger 
than the 7.1 magnitude earthquake that hit the San 
Francisco Bay area Oct . 17 .  
 
Major earthquakes in the Solomons usually don't 
cause much damage or many casualties because 
the area is sparsely populated and not extensive
developed .
ly 
 
A major earthquake registering 7.2 on the Richter scale 
shook the Solomon Islands in the South Pacific today, 
the U.S. Geological Survey says. 
 
The preliminary reading of 7.2 is slightly stronger than 
the 7.1 magnitude earthquake that hit the San Francisco 
Bay area Oct. 17. 
 
It was the largest earthquake in the Solomons since a 7.4 
quake on Nov. 5, 1978. 
 
There were no immediate reports of injury or damage. 
 
An 8.3 quake hit the Macquarie Islands south of 
Australia on May 23. 
 
The Richter scale is a measure of ground motion as 
recorded on seismographs. 
 
Thus a reading of 7.5 reflects an earthquake 10 times 
stronger than one of 6.5. 
 
 Figure 5. Comparison of summaries generated by human and system.  
 
Chin-Yew Lin. 2001. Summary evaluation environment. 
http://www.isi.edu/~cyl/SEE. 
 
Chin-Yew Lin and Eduard Hovy. 2000. The automated 
acquisition of topic signatures for text 
summarization. In Proceedings of the 18th 
International Conference on Computational 
Linguistics (COLING 2000), Saarbr?cken, Germany, 
July 31- August 4, 2000. 
 
   Daniel Marcu. 1999. The automatic construction of 
large-scale corpora for summarization research. The 
22nd International ACM SIGIR Conference on 
Research and Development in Information Retrieval 
(SIGIR'99), pages 137-144, Berkeley, CA, August 
1999.  
 
I. Dan Melamed. 1999. Bitext maps and alignment via 
pattern recognition. Computational Linguistics, 
25(1):107-130. 
 
Gerard Salton. 1975. A vector space model for 
information retrieval. Communications of the ACM, 
18(11):613-620, November 1975.  
 
David Zajic, Bonnie Dorr, and Richard Schwartz. 2002. 
Automatic headline generation for newspaper 
stories. In Proceedings of the ACL-2002 Workshop 
on Text Summarization, Philadelphia, PA, 2002. 
 
 
 
 
A Maximum Entropy Approach to FrameNet Tagging 
 
Michael Fleischman and Eduard Hovy 
USC Information Science Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, hovy }@ISI.edu 
  
Abstract 
The development of FrameNet, a large 
database of semantically annotated sentences, 
has primed research into statistical methods 
for semantic tagging.  We advance previous 
work by adopting a Maximum Entropy 
approach and by using Viterbi search to find 
the highest probability tag sequence for a 
given sentence.  Further we examine the use 
of syntactic pattern based re-ranking to further 
increase performance.  We analyze our 
strategy using both extracted and human 
generated syntactic features.  Experiments 
indicate 85.7% accuracy using human 
annotations on a held out test set. 
1 Introduction 
The ability to develop automatic methods for semantic 
classification has been hampered by the lack of large 
semantically annotated corpora.  Recent work in the 
development of FrameNet, a large database of 
semantically annotated sentences, has laid the 
foundation for the use of statistical approaches to 
automatic semantic classification.   
The FrameNet project seeks to annotate a large 
subset of the British National Corpus with semantic 
information.  Annotations are based on Frame 
Semantics (Fillmore, 1976), in which frames are defined 
as schematic representations of situations involving 
various Frame Elements such as participants, props, and 
other conceptual roles.   
In each FrameNet sentence, a single target 
predicate is identified and all of its relevant Frame 
Elements are tagged with their element-type (e.g., 
Agent, Judge), their syntactic Phrase Type (e.g., NP, 
PP), and their Grammatical Function (e.g., External 
Argument, Object Argument).  Figure 1 shows an 
example of an annotated sentence and its appropriate 
semantic frame. 
 
To our knowledge, Gildea and Jurafsky (2000) is 
the only work that uses FrameNet to build a statistical 
semantic classifier.  They split the problem into two 
distinct sub-tasks: Frame Element identification and 
Frame Element classification.  In the identification 
phase, they use syntactic information extracted from a 
parse tree to learn the boundaries of Frame Elements in 
sentences.  The work presented here, focuses only on 
the second phase: classification. 
Gildea and Jurafsky (2000) describe a system that 
uses completely syntactic features to classify the Frame 
Elements in a sentence.  They extract features from a 
parse tree and model the conditional probability of a 
semantic role given those features.  They report an 
accuracy of 76.9% on a held out test set. 
 
 
 
 
 
 
She  clapped  her hands  in inspiration. 
Frame:        Body-Movement 
Frame Elements:   
Agent     Body Part Cause 
     -NP             -NP   -PP 
     -Ext.            -Obj.   -Comp. 
 
Figure 1.  Frame for lemma ?clap? shown with three core Frame 
Elements and a sentence annotated with element type, phrase 
type, and grammatical function. 
 
We extend Gildea and Jurafsky (2000)?s initial 
effort in three ways.  First, we adopt a Maximum 
Entropy (ME) framework to better learn the feature 
weights associated with the classification model.  
Second, we recast the classification task as a tagging 
problem in which an n-gram model of Frame Elements 
is applied to find the most probable tag sequence (as 
opposed to the most probable individual tags).  Finally, 
we implement a re-ranking system that takes advantage 
of the sentence-level syntactic patterns of each 
sequence.  We analyze our results using syntactic 
features extracted from a parse tree generated by Collins 
parser (Collins, 1997) and compare those to models 
built using features extracted from FrameNet?s human 
annotations.   
2 Method 
2.1 
2.2 
                                                          
Training (32,251 sentences), development (3,491 
sentences), and held out test sets (3,398 sentences) were 
generated from the June 2002 FrameNet release 
following the divisions used in Gildea and Jurafsky 
(2000) 1 .  Because human-annotated syntactic 
information could only be obtained for a subset of their 
data, the training, development, and test sets used here 
are approximately 10% smaller than those used in 
Gildea and Jurafsky (2000).2  There are on average 2.2 
Frame Elements per sentence, falling into one of 126 
unique classes.   
Maximum Entropy 
ME models implement the intuition that the best model 
will be the one that is consistent with all the evidence, 
but otherwise, is as uniform as possible.  (Berger et al, 
1996).  Following recent successes using it for many 
NLP tasks (Och and Ney, 2002; Koeling, 2000), we use 
ME to implement a Frame Element classifier. 
We use the YASMET ME package (Och, 
2002) to train an approximation of the model below: 
 
P(r| pt, voice, position, target, gf, h) 
 
Here r indicates the element type, pt the phrase type, gf 
the grammatical function, h the head word, and target 
the target predicate.  Due to data sparsity issues, we do 
not calculate this model directly, but rather, model 
various feature combinations as described in Gildea and 
Jurafsky (2000).   
The classifier was trained, using only features that 
had a frequency in training of one or more, and until 
performance on the development set ceased to improve.  
Feature weights were smoothed using a Bayesian 
method, such that weight limits are Gaussian distributed 
with mean 0 and standard deviation 1.  
Tagging 
Frame Elements do not occur in isolation, but rather, 
depend very much on what other Elements occur in a 
sentence.  For example, if a Frame Element is tagged as 
an Agent it is highly unlikely that the next Element will 
also be an Agent.  We exploit this dependency by 
treating the Frame Element classification task as a 
tagging problem. 
The YASMET MEtagger was used to apply an n-
gram tag model to the classification task (Bender et al, 
2003).  The feature set for the training data was 
2.3 
3 Results 
                                                          
1 Divisions given by Dan Gildea via personal communication. 
2  Gildea and Jurafsky (2000) use 36995 training, 4000 
development, and 3865 test sentences.  They do not report 
results using hand annotated syntactic information. 
augmented to include information about the tags of the 
previous one and two Frame Elements in the sentence: 
 
P(r| pt, voice, position, target, gf, h, r -1,r -1+r -2) 
 
Viterbi search was then used to find the most probable 
tag sequence through all possible sequences. 
Pattern Features 
A great deal of information useful for classification can 
be found in the syntactic patterns associated with each 
sequence of Frame Elements.  A typical syntactic 
pattern is exhibited by the sentence ?Alexandra bent her 
head.?  Here ?Alexandra? is an external argument Noun 
Phrase, ?bent? is the target, and ?her head? is an object 
argument Noun Phrase.  In the training data, a syntactic 
pattern of NP-ext, target, NP-obj, given the predicate 
bend, was associated 100% of the time with the Frame 
Element pattern: ?Agent target BodyPart?, thus, 
providing powerful evidence as to the classification of 
those Frame Elements.   
We exploit these sentence-level patterns by 
implementing a re-ranking system that chooses among 
the n-best tagger outputs.  The re-ranker was trained on 
a development corpus, which was first tagged using the 
MEtagger described above.  For each sentence in the 
development corpus, the 10 best tag sequences are 
output by the classifier and described by three 
probabilities: 3  1) the sequence?s probability given by 
the ME classifier (ME); 2) the conditional probability of 
that sequence given the syntactic pattern and the target 
predicate (pat+target); 3) a back off conditional 
probability of the tag sequence given just the syntactic 
pattern (pat).  A ME model is then used to combine the 
log of these probabilities to give a model of the form: 
 
P(tag-seq| ME, pat+target, pat) 
 
Figure 2 shows the performance of the base ME model, 
the base model within a tagging framework, and the 
base model within a tagging framework plus the re-
ranker.  Results are shown for data sets trained and 
tested using human annotated syntactic features and 
trained and tested using automatically extracted 
syntactic features.  In both cases the training and test 
sets are identical.   
For both the extracted and human conditions, 
adopting a tagging framework improves results by over 
1%.  However, while the syntactic pattern based re-
ranker increases performance using human annotations 
by nearly 2%, the effect when using automatically 
extracted information is only 0.5%.  This is reasonable 
3  Using n-best lists of 50 and 100 showed no significant 
difference in performance. 
considering that the re-ranker?s effectiveness is 
correlated with the level of noise in the syntactic 
patterns upon which it is based. 
The difference in performance between the models 
under both human and extracted conditions was 
relatively consistent: averaging 8.7% with a standard 
deviation of 0.7. 
As a further analysis, we have examined the 
performance of our base ME model on the same test set 
as that used in Gildea and Jurafsky (2000).  Using only 
extracted information, we achieve an accuracy of 
74.9%, two percent lower than their reported results.  
This result is not unreasonable, however, because, due 
to limited time, very little effort was spent tuning the 
parameters of the model.  
 
 
 
 
 
 
 
 
 
 
 
Figure 2.  Performance of models on held out test data.  ME refers 
to results of the base Maximum Entropy model, Tagger to a 
combined ME and Viterbi search model, Re-Rank to the Tagger 
augmented with a re-ranker.  Extracted refers to models trained 
using features extracted from parse trees, Human to models using 
features from FrameNet?s human annotations. 
4 Conclusion 
It is clear that using a tagging framework and syntactic 
patterns improves performance of the semantic classifier 
when features are extracted from either automatically 
generated parse trees or human annotations.  The most 
striking result of these experiments, however, is the 
dramatic decrease in performance associated with using 
features extracted from a parse tree.   
This decrease in performance can be traced to at 
least two aspects of the automatic extraction process: 
noisy parser output and limited grammatical 
information.   
To compensate for noisy parser output, our current 
work is focusing on two strategies.  First, we are 
looking at using shallower but more reliable methods 
for syntactic feature generation, such as part of speech 
tagging and text chunking, to either replace or augment 
the syntactic parser.  Second, we are using ontological 
information, such as word classes and synonyms, in the 
hopes that semantic information may supplement the 
noisy syntactic information. 
The models trained on features extracted from parse 
trees do not have access to rich grammatical 
information.  Following Gildea and Jurafsky (2000), 
automatic extraction of grammatical information here is 
limited to the governing category of a Noun Phrase.  
The FrameNet annotations, however, are much richer 
and include information about complements, modifiers, 
etc.  We are looking at ways to include such information 
either by using alternative parsers (Hermjakob, 1997) or 
as a post processing task (Blaheta and Charniak, 2000).  
In future work, we will extend the strategies 
outlined here to incorporate Frame Element 
identification into our model.  By treating semantic 
classification as a single tagging problem, we hope to 
create a unified, practical, and high performance system 
for Frame Element tagging. 
76.375.8
74
85.7
83.8
82.6
68
70
72
74
76
78
80
82
84
86
88
ME Tagger Re-Rank
% 
Co
rre
ct
Extracted Human
Acknowledgments 
The authors would like to thank Dan Gildea who 
generously allowed us access to his data files and Oliver 
Bender for making the MEtagger software publicly 
available.  Finally, we thank Franz Och whose help and 
expertise were invaluable. 
References 
O. Bender, K. Macherey, F. J. Och, and H. Ney. 2003. 
Comparison of Alignment Templates and Maximum 
Entropy Models for Natural Language Processing. EACL-
2003.  Budapest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, vol. 22, no. 1. 
D. Blaheta and E. Charniak. 2000. Assigning Function Tags to 
Parsed Text, In Proc. of the 1st NAACL, Seattle, WA. 
M. Collins. 1997. Three generative, lexicalized models for 
statistical parsing.  In Proc. of the 35th Annual Meeting of 
the ACL. 
C. Fillmore 1976.  Frame semantics and the nature of 
language. In Annals of the New York Academy of Sciences: 
Conference on the Origin and Development of Language 
and Speech, Volume 280 (pp. 20-32).  
D. Gildea and D. Jurafsky. 2000.  Automatic Labeling of 
Semantic Roles, ACL-2000, Hong Kong.  
U. Hermjakob, 1997. Learning Parse and Translation 
Decisions from Examples with Rich Context. Ph.D. 
Dissertation, University of Texas at Austin, Austin, TX. 
R. Koeling. 2000. Chunking with maximum entropy models. 
CoNLL-2000. Lisbon, Portugal. 
F.J. Och, H. Ney. 2002. Discriminative Training and 
Maximum Entropy Models for Statistical Machine 
Translation. ACL-2002. Philadelphia, PA. 
F.J. Och. 2002. Yet another maxent toolkit: YASMET. www-
i6.informatik.rwth-aachen.de/Colleagues/och/. 
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 200?207,
New York, June 2006. c?2006 Association for Computational Linguistics
Identifying and Analyzing Judgment Opinions 
 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way, Marina del Rey, CA 90292 
{skim, hovy}@ISI.EDU 
Abstract 
In this paper, we introduce a methodology 
for analyzing judgment opinions. We de-
fine a judgment opinion as consisting of a 
valence, a holder, and a topic. We decom-
pose the task of opinion analysis into four 
parts: 1) recognizing the opinion; 2) iden-
tifying the valence; 3) identifying the 
holder; and 4) identifying the topic. In this 
paper, we address the first three parts and 
evaluate our methodology using both in-
trinsic and extrinsic measures. 
1 Introduction 
Recently, many researchers and companies have 
explored the area of opinion detection and analysis. 
With the increased immersion of Internet users has 
come a proliferation of opinions available on the 
web. Not only do we read more opinions from the 
web, such as in daily news editorials, but also we 
post more opinions through mechanisms such as 
governmental web sites, product review sites, news 
group message boards and personal blogs. This 
phenomenon has opened the door for massive 
opinion collection, which has potential impact on 
various applications such as public opinion moni-
toring and product review summary systems. 
   Although in its infancy, many researchers have 
worked in various facets of opinion analysis. Pang 
et al (2002) and Turney (2002) classified senti-
ment polarity of reviews at the document level.  
Wiebe et al (1999) classified sentence level sub-
jectivity using syntactic classes such as adjectives, 
pronouns and modal verbs as features. Riloff and 
Wiebe (2003) extracted subjective expressions 
from sentences using a bootstrapping pattern learn-
ing process. Yu and Hatzivassiloglou (2003) iden-
tified the polarity of opinion sentences using 
semantically oriented words. These techniques 
were applied and examined in different domains, 
such as customer reviews (Hu and Liu 2004) and 
news articles1. These researchers use lists of opin-
ion-bearing clue words and phrases, and then apply 
various additional techniques and refinements. 
Along with many opinion researchers, we par-
ticipated in a large pilot study, sponsored by NIST, 
which concluded that it is very difficult to define 
what an opinion is in general. Moreover, an ex-
pression that is considered as an opinion in one 
domain might not be an opinion in another. For 
example, the statement ?The screen is very big? 
might be a positive review for a wide screen desk-
top review, but it could be a mere fact in general 
newspaper text. This implies that it is hard to apply 
opinion bearing words collected from one domain 
to an application for another domain. One might 
therefore need to collect opinion clues within indi-
vidual domains. In case we cannot simply find 
training data from existing sources, such as news 
article analysis, we need to manually annotate data 
first. 
Most opinions are of two kinds: 1) beliefs about 
the world, with values such as true, false, possible, 
unlikely, etc.; and 2) judgments about the world, 
with values such as good, bad, neutral, wise, fool-
ish, virtuous, etc. Statements like ?I believe that he 
is smart? and ?Stock prices will rise soon? are ex-
amples of beliefs whereas ?I like the new policy on 
social security? and ?Unfortunately this really was 
his year: despite a stagnant economy, he still won 
his re-election? are examples of judgment opinions. 
However, judgment opinions and beliefs are not 
necessarily mutually exclusive. For example, ?I 
think it is an outrage? or ?I believe that he is 
smart? carry both a belief and a judgment. 
In the NIST pilot study, it was apparent that 
human annotators often disagreed on whether a 
belief statement was or was not an opinion. How-
ever, high annotator agreement was seen on judg-
                                                          
1 TREC novelty track 2003 and 2004 
200
ment opinions. In this paper, we therefore focus 
our analysis on judgment opinions only. We hope 
that future work yields a more precise definition of 
belief opinions on which human annotators can 
agree. 
We define a judgment opinion as consisting of 
three elements: a valence, a holder, and a topic. 
The valence, which applies specifically to judg-
ment opinions and not beliefs, is the value of the 
judgment. In our framework, we consider the fol-
lowing valences: positive, negative, and neutral. 
The holder of an opinion is the person, organiza-
tion or group whose opinion is expressed. Finally, 
the topic is the event or entity about which the 
opinion is held. 
In previous work, Choi et al (2005) identify 
opinion holders (sources) using Conditional Ran-
dom Fields (CRF) and extraction patterns. They 
define the opinion holder identification problem as 
a sequence tagging task: given a sequence of words 
( nxxx L21 ) in a sentence, they generate a se-
quence of labels ( nyyy L21 ) indicating whether 
the word is a holder or not. However, there are 
many cases where multiple opinions are expressed 
in a sentence each with its own holder. In those 
cases, finding opinion holders for each individual 
expression is necessary. In the corpus they used, 
48.5% of the sentences which contain an opinion 
have more than one opinion expression with multi-
ple opinion holders. This implies that multiple 
opinion expressions in a sentence occur signifi-
cantly often. A major challenge of our work is 
therefore not only to focus on sentence with only 
one opinion, but also to identify opinion holders 
when there is more than one opinion expressed in a 
sentence. For example, consider the sentence ?In 
relation to Bush?s axis of evil remarks, the German 
Foreign Minister also said, Allies are not satellites, 
and the French Foreign Minister caustically criti-
cized that the United States? unilateral, simplistic 
worldview poses a new threat to the world?. Here, 
?the German Foreign Minister? should be the 
holder for the opinion ?Allies are not satellites? 
and ?the French Foreign Minister? should be the 
holder for ?caustically criticized?. 
In this paper, we introduce a methodology for 
analyzing judgment opinions. We decompose the 
task into four parts: 1) recognizing the opinion; 2) 
identifying the valence; 3) identifying the holder; 
and 4) identifying the topic. For the purposes of 
this paper, we address the first three parts and 
leave the last for future work. Opinions can be ex-
tracted from various granularities such as a word, a 
sentence, a text, or even multiple texts. Each is 
important, but we focus our attention on word-
level opinion detection (Section 2.1) and the detec-
tion of opinions in short emails (Section 3). We 
evaluate our methodology using intrinsic and ex-
trinsic measures. 
The remainder of the paper is organized as fol-
lows. In the next section, we describe our method-
ology addressing the three steps described above, 
and in Section 4 we present our experimental re-
sults. We conclude with a discussion of future 
work. 
2 Analysis of Judgment Opinions  
In this section, we first describe our methodology 
for detecting opinion bearing words and for identi-
fying their valence, which is described in Section 
2.1. Then, in Section 2.2, we describe our algo-
rithm for identifying opinion holders. In Section 3, 
we show how to use our methodology for detecting 
opinions in short emails. 
2.1 Detecting Opinion-Bearing Words 
and Identifying Valence 
We introduce an algorithm to classify a word as 
being positive, negative, or neutral classes. This 
classifier can be used for any set of words of inter-
est and the resulting words with their valence tags 
can help in developing new applications such as a 
public opinion monitoring system. We define an 
opinion-bearing word as a word that carries a posi-
tive or negative sentiment directly such as ?good?, 
?bad?, ?foolish?, ?virtuous?, etc. In other words, 
this is the smallest unit of opinion that can thereaf-
ter be used as a clue for sentence-level or text-level 
opinion detection.  
We treat word sentiment classification into Posi-
tive, Negative, and Neutral as a three-way classifi-
cation problem instead of a two-way classification 
problem of Positive and Negative. By adding the 
third class, Neutral, we can prevent the classifier 
from assigning either positive or negative senti-
ment to weak opinion-bearing words. For example, 
the word ?central? that Hatzivassiloglou and 
McKeown (1997) included as a positive adjective 
is not classified as positive in our system. Instead 
201
we mark it as ?neutral? since it is a weak clue for 
an opinion. If an unknown word has a strong rela-
tionship with the neutral class, we can therefore 
classify it as neutral even if it has some small con-
notation of Positive or Negative as well.  
Approach: We built a word sentiment classifier 
using WordNet and three sets of positive, negative, 
and neutral words tagged by hand. Our insight is 
that synonyms of positive words tend to have posi-
tive sentiment. We expanded those manually se-
lected seed words of each sentiment class by 
collecting synonyms from WordNet. However, we 
cannot simply assume that all the synonyms of 
positive words are positive since most words could 
have synonym relationships with all three senti-
ment classes. This requires us to calculate the 
closeness of a given word to each category and 
determine the most probable class. The following 
formula describes our model for determining the 
category of a word: 
(1)            ).....,|(maxarg)|(maxarg 21 n
cc
synsynsyncPwcP ?  
where c is a category (Positive, Negative, or Neu-
tral) and w is a given word; synn is a WordNet 
synonym of the word w. We calculate this close-
ness as follows; 
(2)   )|()(maxarg
)|()(maxarg
)|()(maxarg)|(maxarg
1
))(,(
 ...3 2 1
?
=
=
=
=
m
k
wsynsetfcount
k
c
n
c
cc
kcfPcP
csynsynsynsynPcP
cwPcPwcP  
where kf  is the kth feature of class c which is also a 
member of the synonym set of the given word w. 
count(fk ,synset(w)) is the total number of occur-
rences of the word feature fk in the synonym set of 
word w. In section 4.1, we describe our manually 
annotated dataset which we used for seed words 
and for our evaluation. 
2.2 Identifying Opinion Holders 
Despite successes in identifying opinion expres-
sions and subjective words/phrases (See Section 
1), there has been less achievement on the factors 
closely related to subjectivity and polarity, such as 
identifying the opinion holder. However, our re-
search indicates that without this information, it is 
difficult, if not impossible, to define ?opinion? ac-
curately enough to obtain reasonable inter-
annotator agreement. Since these factors co-occur 
and mutually reinforce each other, the question 
?Who is the holder of this opinion?? is as impor-
tant as ?Is this an opinion?? or ?What kind of opin-
ion is expressed here??. 
In this section, we describe the automated iden-
tification for opinion holders. We define an opin-
ion holder as an entity (person, organization, 
country, or special group of people) who expresses 
explicitly or implicitly the opinion contained in the 
sentence.  
Previous work that is related to opinion holder 
identification is (Bethard et al 2004) who identify 
opinion propositions and holders. However, their 
opinion is restricted to propositional opinion and 
mostly to verbs. Another related work is (Choi et al 
2005) who use the MPQA corpus2 to learn patterns 
of opinion sources using a graphical model and 
extraction pattern learning. However, they have a 
different task definition from ours. They define the 
task as identifying opinion sources (holders) given 
a sentence, whereas we define it as identifying 
opinion sources given an opinion expression in a 
sentence. We discussed their work in Section 1. 
Data: As training data, we used the MPQA cor-
pus (Wilson and Wiebe, 2003), which contains 
news articles manually annotated by 5 trained an-
notators. They annotated 10657 sentences from 
535 documents, in four different aspects: agent, 
expressive-subjectivity, on, and inside. Expressive-
subjectivity marks words and phrases that indi-
rectly express a private state that is defined as a 
term for opinions, evaluations, emotions, and 
speculations. The on annotation is used to mark 
speech events and direct expressions of private 
states. As for the holder, we use the agent of the 
selected private states or speech events. While 
there are many possible ways to define what opin-
ion means, intuitively, given an opinion, it is clear 
what the opinion holder means. Table 1 shows an 
example of the annotation. In this example, we 
consider the expression ?the U.S. government ?is 
the source of evil? in the world? with an expres-
                                                          
2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 
Sentence 
Iraqi Vice President Taha Yassin Rama-
dan, responding to Bush?s ?axis of evil? 
remark, said the U.S. government ?is the 
source of evil? in the world. 
Expressive 
subjectivity
the U.S. government ?is the source of evil? 
in the world 
Strength Extreme 
Source Iraqi Vice President Taha Yassin Ramadan
Table 1: Annotation example 
202
sive-subjectivity tag as an opinion of the holder 
?Iraqi Vice President Taha Yassin Ramadan?.  
Approach: Since more than one opinion may be 
expressed in a sentence, we have to find an opinion 
holder for each opinion expression. For example, 
in a sentence ?A thinks B?s criticism of T is 
wrong?, B is the holder of ?the criticism of T?, 
whereas A is the person who has an opinion that 
B?s criticism is wrong. Therefore, we define our 
task as finding an opinion holder, given an opinion 
expression. Our earlier work (ref suppressed) fo-
cused on identifying opinion expressions within 
text. We employ that system in tandem with the 
one described here. 
To learn opinion holders automatically, we use a 
Maximum Entropy model. Maximum Entropy 
models implement the intuition that the best model 
is the one that is consistent with the set of con-
straints imposed by the evidence but otherwise is 
as uniform as possible (Berger et al 1996). There 
are two ways to model the problem with ME: clas-
sification and ranking. Classification allocates each 
holder candidate to one of a set of predefined 
classes while ranking selects a single candidate as 
answer. This means that classification modeling3 
can select many candidates as answers as long as 
they are marked as true, and does not select any 
candidate if every one is marked as false. In con-
trast, ranking always selects the most probable 
candidate as an answer, which suits our task better. 
Our earlier experiments showed poor performance 
with classification modeling, an experience also 
reported for Question Answering (Ravichandran et 
al. 2003).   
We modeled the problem to choose the most 
probable candidate that maximizes a given condi-
tional probability distribution, given a set of holder 
candidates h
1
h
2
. . . h
N
and opinion expression e. 
The conditional probability P h h
1
h
2
. . . h
N
, e  
can be calculated based on K feature func-
tions f
k
h , h
1
h
2
. ..h
N
, e . We write a decision rule 
for the ranking as follows: 
{ }
{ } ]e),hhh(h,f?[=
e)],hhh|[P(hh
K
=k
Nkk
h
N
h
?
=
1
21
21
...argmax  
...argmax
 
Each k?  is a model parameter indicating the 
weight of its feature function. 
                                                          
3 In our task, there are two classes: holder and non-holder. 
Figure 1 illustrates our holder identification sys-
tem. First, the system generates all possible holder 
candidates, given a sentence and an opinion ex-
pression <E>. After parsing the sentence, it ex-
tracts features such as the syntactic path 
information between each candidate <H> and the 
expression <E> and a distance between <H> and 
<E>. Then it ranks holder candidates according to 
the score obtained by the ME ranking model. Fi-
nally the system picks the candidate with the high-
est score. Below, we describe in turn how to select 
holder candidates and how to select features for the 
training model. 
Holder Candidate Selection: Intuitively, one 
would expect most opinion holders to be named 
entities (PERSON or ORGANIZATION)4. However, 
other common noun phrases can often be opinion 
holders, such as ?the leader?, ?three nations?, and 
?the Arab and Islamic world?. Sometimes, pro-
nouns like he, she, and they that refer to a PERSON, 
or it that refers to an ORGANIZATION or country, 
can be an opinion holder. In our study, we consider 
all noun phrases, including common noun phrases, 
named entities, and pronouns, as holder candidates. 
Feature Selection: Our hypothesis is that there 
exists a structural relation between a holder <H> 
and an expression <E> that can help to identify 
opinion holders. This relation may be represented 
by lexical-level patterns between <H> and <E>, 
but anchoring on surface words might run into the 
data sparseness problem. For example, if we see 
the lexical pattern ?<H> recently criticized <E>? in 
the training data, it is impossible to match the ex-
pression ?<H> yesterday condemned <E>?. These, 
however, have the same syntactic features in our 
                                                          
4 We use BBN?s named entity tagger IdentiFinder to collect 
named entities. 
Sentence             :   w1 w2 w3 w4 w5 w6 w7 w8 w9 ? wn
Opinion expression  <E>  :                  w6 w7 w8
? w2 ... w4 w5 w6 w7 w8 ? w11 w12 w13 ? w18 ? w23 w24 w25 ..
      C1         C2            <E>                       C3                 C4                  C5 
given
Candidate 
holder 
selection
Feature 
extraction:
Parsing
C1            C2    <E>          C3          C4          C5
Rank the candidates by 
ME model 1.C1   2. C5   3.C3  4.C2  5.C4  
Pick the best candidate as a holder C1    
 
Figure 1: Overall system architecture 
203
model. We therefore selected structural features 
from a deep parse, using the Charniak parser. 
After parsing the sentence, we search for the 
lowest common parent node of the words in <H> 
and <E> respectively (<H> and <E> are mostly 
expressed with multiple words). A lowest common 
parent node is a non-terminal node in a parse tree 
that covers all the words in <H> and <E>. Figure 2 
shows a parsed example of a sentence with the 
holder ?China?s official Xinhua news agency? and 
the opinion expression ?accusing?. In this example, 
the lowest common parent of words in <H> is the 
bold NP and the lowest common parent of <E> is 
the bold VBG. We name these nodes Hhead and 
Ehead respectively. After finding these nodes, we 
label them by subscript (e.g., NPH and VBGE) to 
indicate they cover <H> and <E>. In order to see 
how Hhead and Ehead are related to each other in 
the parse tree, we define another node, HEhead, 
which covers both Hhead and Ehead. In the exam-
ple, HEhead is S at the top of the parse tree since it 
covers both NPH and VBGE.  We also label S by 
subscript as SHE. 
To express tree structure for ME training, we 
extract path information between <H> and <E>.  In 
the example, the complete path from Hhead to 
Ehead is ?<H> NP S VP S S VP VBG <E>?.  How-
ever, representing each complete path as a single 
feature produces so many different paths with low 
frequencies that the ME system would learn 
poorly. Therefore, we split the path into three 
parts: HEpath, Hpath an Epath. HEpath is defined 
as a path from HEhead to its left and right child 
nodes that are also parents of Hhead and Ehead. 
Hpath is a path from Hhead and one of its ancestor 
nodes that is a child of HEhead. Similarly, Epath is 
defined as a path from Ehead to one of its ances-
tors that is also a child of HEhead. With this split-
ting, the system can work when any of HEpath, 
Hpath or Epath appeared in the training data, even 
if the entire path from <H> to <E> is unseen. Table 
2 summarizes these concepts with two holder can-
didate examples in the parse tree of Figure 2. 
We also include two non-structural features. The 
first is the type of the candidate, with values NP, 
PERSON, ORGANIZATION, and LOCATION. The 
second feature is the distance between <H> and 
<E>, counted in parse tree words. This is moti-
vated by the intuition that holder candidates tend to 
lie closer to their opinion expression. All features 
are listed in Table 3. We describe the performance 
of the system in Section 4. 
Candidate 1 Candidate 2 
 China?s official Xinu-
hua news agency Bush 
Hhead NPH  NNPH 
Ehead VBGE VBGE 
HEhead SHE VPHE 
Hpath NPH NNPH NPH NPH 
NPH PPH 
Epath VBGE VPE SE SE VPE VBGE VPE SE SE  
HEpath SHE NPH VPE VPHE  PPH SE 
Table 2: Heads and paths for the Figure 2 example  
Features Description 
F1 Type of <H> 
F2 HEpath 
F3 Hpath 
F4 Epath 
F5 Distance between <H> and <E> 
Table 3: Features for ME training 
NP ADVP VP
S
.
NP JJ
NNP NNNN
NNP POS
RB
VBD PP
PP
,
NPIN
NNP
NPIN
PPNP
NNNP NPIN
S
S
official
China ?s
Xinhua news agency
also
weighed
in
sunday
on
NNP POS
choice
Bush ?s
of NNS
words
VP
VBG
PPNP
accusing
the
DT NN
IN S
president
of VP
VBG NP PP
orchestrating
public opinion
JJ NN In advance of possible 
strikes against the three 
countries in an expansion of 
the war against terrorism  
Figure 2: A parsing example 
 
204
Model 1 
? Translate a German email to English  
? Apply English opinion-bearing words 
Model 2 
? Translate English opinion-bearing words to 
German 
 
? Analyze a German email using the German 
opinion-bearing words. 
Table 4: Two models of German Email opinion 
analysis system 
3 Applying our Methodology to German 
Emails 
In this section, we describe a German email analy-
sis system into which we included the opinion-
bearing words from Section 2.1 to detect opinions 
expressed in emails. This system is part of a col-
laboration with the EU-funded project QUALEG 
(Quality of Service and Legitimacy in eGovern-
ment) which aims at enabling local governments to 
manage their policies in a transparent and trustable 
way5. For this purpose, local governments should 
be able to measure the performance of the services 
they offer, by assessing the satisfaction of its citi-
zens. This need makes a system that can monitor 
and analyze citizens? emails essential. The goal of 
our system is to classify emails as neutral or as 
bearing a positive or negative opinion. 
To generate opinion bearing words, we ran the 
word sentiment classifier from Section 2.1 on 8011 
verbs to classify them into 807 positive, 785 nega-
tive, and 6149 neutral. For 19748 adjectives, the 
system classified them into 3254 positive, 303 
negative, and 16191 neutral. Since our opinion-
bearing words are in English and our target system 
is in German, we also applied a statistical word 
alignment technique, GIZA++ 6  (Och and Ney 
2000). Running it on version two of the European 
Parliament corpus, we obtained statistics for 
678,340 German-English word pairs and 577,362 
English-German word pairs. Obtaining these two 
lists of translation pairs allows us to convert Eng-
lish words to German, and German to English, 
without a full document translation system. To util-
ize our English opinion-bearing words in a German 
opinion analysis system, we developed two models, 
                                                          
5 http://www.qualeg.eupm.net/my_spip/index.php 
6 http://www.fjoch.com/GIZA++.html 
outlined in Table 4, each of which is triggered at 
different points in the system.  
In both models, however, we still need to decide 
how to apply opinion-bearing words as clues to 
determine the sentiment of a whole email. Our 
previous work on sentence level sentiment classifi-
cation (ref suppressed) shows that the presence of 
any negative words is a reasonable indication of a 
negative sentence. Since our emails are mostly 
short (the average number of words in each email 
is 19.2) and we avoided collecting weak negative 
opinion clue words, we hypothesize that our previ-
ous sentence sentiment classification study works 
on the email sentiment analysis. This implies that 
an email is negative if it contains more than certain 
number of strong negative words. We tune this 
parameter using our training data. Conversely, if 
an email contains mostly positive opinion-bearing 
words, we classify it as a positive email. We assign 
neutral if an email does not contain any strong 
opinion-bearing words.   
Manually annotated email data was provided by 
our joint research site. This data contains 71 emails 
from citizens regarding a German festival. 26 of 
them contained negative complaints, for example, 
the lack of parking space, and 24 of them were 
positive with complimentary comments to the or-
ganization. The rest of them were marked as 
?questions? such as how to buy festival tickets, 
?only text? of simple comments, ?fuzzy?, and ?dif-
ficult?. So, we carried system experiments on posi-
tive and negative emails with precision and recall. 
We report system results in Section 4. 
4 Experiment Results  
In this section, we evaluate the three systems de-
scribed in Sections 2 and 3: detecting opinion-
bearing words and identifying valence, identifying 
opinion holders, and the German email opinion 
analysis system. 
4.1 Detecting Opinion-bearing Words   
We described a word classification system to de-
tect opinion-bearing words in Section 2.1. To ex-
amine its effectiveness, we annotated 2011 verbs 
and 1860 adjectives, which served as a gold stan-
dard7. These words were randomly selected from a  
                                                          
7 Although nouns and adverbs may also be opinion-bearing, 
we focus only on verbs and adjectives for this study. 
205
collection of 8011 English verbs and 19748 Eng-
lish adjectives. We use training data as seed words 
for the WordNet expansion part of our algorithm 
(described in Section 2.1). Table 5 shows the dis-
tribution of each semantic class. In both verb and 
adjective annotation, neutral class has much more 
words than the positive or negative classes. 
We measured the precision, recall, and F-score 
of our system using 10-fold cross validation. Table 
6 shows the results with 95% confidence bounds. 
Overall (combining positive, neutral and negative), 
our system achieved 77.7% ? 1.2% accuracy on 
verbs and 69.1% ? 2.1% accuracy on adjectives. 
The system has very high precision in the neutral 
category for both verbs (97.2%) and adjectives 
(89.5%), which we interpret to mean that our sys-
tem is really good at filtering non-opinion bearing 
words. Recall is high in all cases but precision var-
ies; very high for neutral and relatively high for 
negative but low for positive. 
4.2 Opinion Holder Identification  
We conducted experiments on 2822 <sentence; 
opinion expression; holder> triples and divided the 
data set into 10 <training; test> sets for cross vali-
dation. For evaluation, we consider to match either 
fully or partially with the holder marked in the test 
data. The holder matches fully if it is a single entity 
(e.g., ?Bush?). The holder matches partially when 
it is part of the multiple entities that make up the 
marked holder. For example, given a marked 
holder ?Michel Sidibe, Director of the Country and  
Regional Support Department of UNAIDS?, we  
consider both ?Michel Sidibe? and ?Director of the 
Country and Regional Support Department of 
UNAIDS? as acceptable answers. 
Our experiments consist of two parts based on 
the candidate selection method. Besides the selec-
tion method we described in Section 2.2, we also 
conducted a separate experiment by excluding pro-
nouns from the candidate list. With the second 
method, the system always produces a non-
pronoun holder as an answer. This selection 
method is useful in some Information Extraction 
application that only cares non-pronoun holders. 
We report accuracy (the percentage of correct 
answers the system found in the test set) to evalu-
ate our system. We also report how many correct 
answers were found within the top2 and top3 sys-
tem answers. Tables 7 and 8 show the system accu-
racy with and without considering pronouns as 
alias candidates, respectively. Table 8 mostly 
shows lower accuracies than Table 7 because test 
data often has only a non-pronoun entity as a 
holder and the system picks a pronoun as its an-
swer. Even if the pronoun refers the same entity 
marked in the test data, the evaluation system 
counts it as wrong because it does not match the 
hand annotated holder. 
To evaluate the effectiveness of our system, we 
set the baseline as a system choosing the closest 
candidate to the expression as a holder without the 
Maximum Entropy decision. The baseline system 
had an accuracy of only 21.3% for candidate selec-
tion over all noun phrases and 23.2% for candidate 
selection excluding pronouns.  
The results show that detecting opinion holders 
is a hard problem, but adopting syntactic features 
(F2, F3, and F4) helps to improve the system. A 
promising avenue of future work is to investigate 
the use of semantic features to eliminate noun 
 Positive Negative Neutral Total 
Verb 69 151 1791 2011 
Adjective 199 304 1357 1860 
Table 5: Word distribution in our gold standard
 Precision Recall F-score 
V 20.5% ? 3.5% 82.4% ? 7.5% 32.3% ? 4.6% 
P 
A 32.4% ? 3.8% 75.5% ? 6.1% 45.1% ? 4.4% 
V 97.2% ? 0.6% 77.6% ? 1.4% 86.3% ? 0.7% 
X 
A 89.5% ? 1.7% 67.1% ? 2.7% 76.6% ? 2.1% 
V 37.8% ? 4.9% 76.2% ? 8.0% 50.1% ? 5.6% 
N 
A 60.0% ? 4.1% 78.5% ? 4.9% 67.8% ? 3.8% 
Table 6: Precision, recall, and F-score on word va-
lence categorization for Positive (P), Negative (N) 
and Neutral (X) verbs (V) and adjectives (A) (with 
95% confidence intervals) 
 Baseline F5 F15 F234 F12345 
Top1 23.2% 21.8% 41.6% 50.8% 52.7%
Top2 39.7% 61.9% 66.3% 67.9%
Top3 52.2% 72.5% 77.1% 77.8%
Table 7: Opinion holder identification results 
(excluding pronouns from candidates) 
 Baseline F5 F15 F234 F12345 
Top1 21.3% 18.9% 41.8% 47.9% 50.6%
Top2 37.9% 61.6% 64.8% 66.7%
Top3 51.2% 72.3% 75.3% 76.0%
Table 8: Opinion holder identification results (All 
noun phrases as candidates) 
206
phrases such as ?cheap energy subsidies? or ?pos-
sible strikes? from the candidate set before we run 
our ME model, since they are less likely to be an 
opinion holder than noun phrases like ?three na-
tions? or ?Palestine people.?  
4.3 German Emails  
For our experiment, we performed 7-fold cross 
validation on a set of 71 emails. Table 9 shows the 
average precision, recall, and F-score. Results 
show that our system identifies negative emails 
(complaints) better than praise. When we chose a 
system parameter for the focus, we intended to find 
negative emails rather than positive emails because 
officials who receive these emails need to act to 
solve problems when people complain but they 
have less need to react to compliments. By high-
lighting high recall of negative emails, we may 
misclassify a neutral email as negative but there is 
also less chance to neglect complaints.  
Category  Model1 Model2 
Precision 0.72 0.55 
Recall 0.40 0.65 
Positive 
(P) 
F-score 0.51 0.60 
Precision 0.55 0.61 
Recall 0.80 0.42 
Negative 
(N) 
F-score 0.65 0.50 
Table 9: German email opinion analysis system results 
5 Conclusion and Future Work 
In this paper, we presented a methodology for ana-
lyzing judgment opinions, which we define as 
opinions consisting of a valence, a holder, and a 
topic. We presented models for recognizing sen-
tences containing judgment opinions, identifying 
the valence of the opinion, and identifying the 
holder of the opinion. Remaining is to also finally 
identify the topic of the opinion. Past tests with 
human annotators indicate that the accuracy of 
identifying valence, holder and topic is much in-
creased when all three are being done simultane-
ously. We plan to investigate a joint model to 
verify this intuition. 
Our past work indicated that, for newspaper 
texts, it is feasible for annotators to identify judg-
ment opinion sentences and for them to identify 
their holders and judgment valences. It is encour-
aging to see that we achieved good results on a 
new genre ? emails sent from citizens to a city co- 
unsel ? and in a new language, German. 
This paper presents a computational framework 
for analyzing judgment opinions. Even though 
these are the most common opinions, it is a pity 
that the research community remains unable to de-
fine belief opinions (i.e., those opinions that have 
values such as true, false, possible, unlikely, etc.) 
with high enough inter-annotator agreement. Only 
once we properly define belief opinion will we be 
capable of building a complete opinion analysis 
system.  
References  
Berger, A, S. Della Pietra, and V. Della Pietra. 1996. A Maximum 
Entropy Approach to Natural Language. Computational Linguis-
tics 22(1). 
Bethard, S., H. Yu, A. Thornton, V. Hatzivassiloglou, and D. Jurafsky. 
2004. Automatic Extraction of Opinion Propositions and their 
Holders. AAAI Spring Symposium on Exploring Attitude and Affect 
in Text. 
Charniak, E. 2000. A Maximum-Entropy-Inspired Parser.  Proc. of 
NAACL-2000.  
Choi, Y., C. Cardie, E. Riloff, and S. Patwardhan. 2005. Identifying 
Sources of Opinions with Conditional Random Fields and Extrac-
tion Patterns. Proc. of Human Language Technology Confer-
ence/Conference on Empirical Methods in Natural Language 
Processing (HLT-EMNLP 2005). 
Esuli, A.  and F. Sebastiani. 2005. Determining the semantic orienta-
tion of terms through gloss classification. Proc. of CIKM-05, 14th 
ACM International Conference on Information and Knowledge 
Management. 
Hatzivassiloglou, V. and McKeown, K. (1997). Predicting the seman-
tic orientation of adjectives. Proc. 35th Annual Meeting of the 
Assoc. for Computational Linguistics (ACL-EACL 97. 
Hu, M. and Liu, B. 2004. Mining and summarizing customer reviews. 
Proc. of KDD?04. pp.168 - 177 
Och, F.J. 2002. Yet Another MaxEnt Toolkit: YASMET 
http://wasserstoff.informatik.rwth-aachen.de/Colleag ues/och/ 
Och, F.J and  Ney, H. 2000. Improved statistical alignment models. 
Proc. of ACL-2000, pp. 440?447, Hong Kong, China. 
Pang, B, L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment 
Classification using Machine Learning Techniques.  Proc. of 
EMNLP 2002. 
Ravichandran, D., E. Hovy, and F.J. Och. 2003. Statistical QA - clas-
sifier vs re-ranker: What?s the difference? Proc. of the ACL Work-
shop on Multilingual Summarization and Question Answering. 
Riloff, E. and J. Wiebe. 2003. Learning Extraction Patterns for Sub-
jective Expressions. Proc. of EMNLP-03. 
Turney, P. 2002. Thumbs Up or Thumbs Down? Semantic Orientation 
Applied to Unsupervised Classification of Reviews. Proc. of the 
40th Annual Meeting of the ACL, 417?424. 
Wiebe, J, R. Bruce, and T. O?Hara. 1999. Development and use of a 
gold standard data set for subjectivity classifications. Proc. of the 
37th Annual Meeting of the Association for Computational Linguis-
tics (ACL-99), 246?253. 
Wilson, T. and J. Wiebe. 2003. Annotating Opinions in the World 
Press. Proc. of  ACL SIGDIAL-03. 
Yu, H. and V. Hatzivassiloglou. 2003. Towards Answering Opinion 
Questions: Separating Facts from Opinions and Identifying the Po-
larity of Opinion Sentences. Proc. of EMNLP. 
207
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 208?215,
New York, June 2006. c?2006 Association for Computational Linguistics
Learning to Detect Conversation Focus of Threaded Discussions 
 
 
Donghui Feng        Erin Shaw        Jihie Kim        Eduard Hovy 
Information Sciences Institute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, shaw, jihie, hovy}@isi.edu 
 
 
Abstract 
In this paper we present a novel feature-
enriched approach that learns to detect the 
conversation focus of threaded discus-
sions by combining NLP analysis and IR 
techniques. Using the graph-based algo-
rithm HITS, we integrate different fea-
tures such as lexical similarity, poster 
trustworthiness, and speech act analysis of 
human conversations with feature-
oriented link generation functions. It is 
the first quantitative study to analyze hu-
man conversation focus in the context of 
online discussions that takes into account 
heterogeneous sources of evidence. Ex-
perimental results using a threaded dis-
cussion corpus from an undergraduate 
class show that it achieves significant per-
formance improvements compared with 
the baseline system. 
1 Introduction 
Threaded discussion is popular in virtual cyber 
communities and has applications in areas such as 
customer support, community development, inter-
active reporting (blogging) and education. Discus-
sion threads can be considered a special case of 
human conversation, and since we have huge re-
positories of such discussion, automatic and/or 
semi-automatic analysis would greatly improve the 
navigation and processing of the information.  
A discussion thread consists of a set of messages 
arranged in chronological order. One of the main 
challenges in the Question Answering domain is 
how to extract the most informative or important 
message in the sequence for the purpose of answer-
ing the initial question, which we refer to as the 
conversation focus in this paper. For example, 
people may repeatedly discuss similar questions in 
a discussion forum and so it is highly desirable to 
detect previous conversation focuses in order to 
automatically answer queries (Feng et al, 2006).  
Human conversation focus is a hard NLP (Natu-
ral Language Processing) problem in general be-
cause people may frequently switch topics in a real 
conversation. The threaded discussions make the 
problem manageable because people typically fo-
cus on a limited set of issues within a thread of a 
discussion. Current IR (Information Retrieval) 
techniques are based on keyword similarity meas-
ures and do not consider some features that are 
important for analyzing threaded discussions. As a 
result, a typical IR system may return a ranked list 
of messages based on keyword queries even if, 
within the context of a discussion, this may not be 
useful or correct. 
Threaded discussion is a special case of human 
conversation, where people may express their 
ideas, elaborate arguments, and answer others? 
questions; many of these aspects are unexplored by 
traditional IR techniques. First, messages in 
threaded discussions are not a flat document set, 
which is a common assumption for most IR sys-
tems. Due to the flexibility and special characteris-
tics involved in human conversations, messages 
within a thread are not necessarily of equal impor-
tance. The real relationships may differ from the 
analysis based on keyword similarity measures, 
e.g., if a 2nd message ?corrects? a 1st one, the 2nd 
message is probably more important than the 1st. 
IR systems may give different results. Second, 
messages posted by different users may have dif-
ferent degrees of correctness and trustworthiness, 
which we refer to as poster trustworthiness in this 
paper. For instance, a domain expert is likely to be 
more reliable than a layman on the domain topic.  
208
In this paper we present a novel feature-enriched 
approach that learns to detect conversation focus of 
threaded discussions by combining NLP analysis 
and IR techniques. Using the graph-based algo-
rithm HITS (Hyperlink Induced Topic Search, 
Kleinberg, 1999), we conduct discussion analysis 
taking into account different features, such as lexi-
cal similarity, poster trustworthiness, and speech 
act relations in human conversations. We generate 
a weighted threaded discussion graph by applying 
feature-oriented link generation functions. All the 
features are quantified and integrated as part of the 
weight of graph edges. In this way, both quantita-
tive features and qualitative features are combined 
to analyze human conversations, specifically in the 
format of online discussions. 
To date, it is the first quantitative study to ana-
lyze human conversation that focuses on threaded 
discussions by taking into account heterogeneous 
evidence from different sources. The study de-
scribed here addresses the problem of conversation 
focus, especially for extracting the best answer to a 
particular question, in the context of an online dis-
cussion board used by students in an undergraduate 
computer science course. Different features are 
studied and compared when applying our approach 
to discussion analysis. Experimental results show 
that performance improvements are significant 
compared with the baseline system. 
The remainder of this paper is organized as fol-
lows: We discuss related work in Section 2. Sec-
tion 3 presents thread representation and the 
weighted HITS algorithm. Section 4 details fea-
ture-oriented link generation functions. Compara-
tive experimental results and analysis are given in 
Section 5. We discuss future work in Section 6. 
2 Related Work 
Human conversation refers to situations where two 
or more participants freely alternate in speaking 
(Levinson, 1983). What makes threaded discus-
sions unique is that users participate asynchro-
nously and in writing. We model human 
conversation as a set of messages in a threaded 
discussion using a graph-based algorithm. 
Graph-based algorithms are widely applied in 
link analysis and for web searching in the IR com-
munity. Two of the most prominent algorithms are 
Page-Rank (Brin and Page, 1998) and the HITS 
algorithm (Kleinberg, 1999). Although they were 
initially proposed for analyzing web pages, they 
proved useful for investigating and ranking struc-
tured objects. Inspired by the idea of graph based 
algorithms to collectively rank and select the best 
candidate, research efforts in the natural language 
community have applied graph-based approaches 
on keyword selection (Mihalcea and Tarau, 2004), 
text summarization (Erkan and Radev, 2004; Mi-
halcea, 2004), word sense disambiguation (Mihal-
cea et al, 2004; Mihalcea, 2005), sentiment 
analysis (Pang and Lee, 2004), and sentence re-
trieval for question answering (Otterbacher et al, 
2005). However, until now there has not been any 
published work on its application to human con-
versation analysis specifically in the format of 
threaded discussions. In this paper, we focus on 
using HITS to detect conversation focus of 
threaded discussions.  
Rhetorical Structure Theory (Mann and Thom-
son, 1988) based discourse processing has attracted 
much attention with successful applications in sen-
tence compression and summarization. Most of the 
current work on discourse processing focuses on 
sentence-level text organization (Soricut and 
Marcu, 2003) or the intermediate step (Sporleder 
and Lapata, 2005). Analyzing and utilizing dis-
course information at a higher level, e.g., at the 
paragraph level, still remains a challenge to the 
natural language community. In our work, we util-
ize the discourse information at a message level. 
Zhou and Hovy (2005) proposed summarizing 
threaded discussions in a similar fashion to multi-
document summarization; but then their work does 
not take into account the relative importance of 
different messages in a thread. Marom and Zuker-
man (2005) generated help-desk responses using 
clustering techniques, but their corpus is composed 
of only two-party, two-turn, conversation pairs, 
which precludes the need to determine relative im-
portance as in a multi-ply conversation. 
In our previous work (Feng et al, 2006), we im-
plemented a discussion-bot to automatically an-
swer student queries in a threaded discussion but 
extract potential answers (the most informative 
message) using a rule-based traverse algorithm that 
is not optimal for selecting a best answer; thus, the 
result may contain redundant or incorrect informa-
tion. We argue that pragmatic knowledge like 
speech acts is important in conversation focus 
analysis. However, estimated speech act labeling 
between messages is not sufficient for detecting 
209
human conversation focus without considering 
other features like author information. Carvalho 
and Cohen (2005) describe a dependency-network 
based collective classification method to classify 
email speech acts. Our work on conversation focus 
detection can be viewed as an immediate step fol-
lowing automatic speech act labeling on discussion 
threads using similar collective classification ap-
proaches. 
We next discuss our approach to detect conver-
sation focus using the graph-based algorithm HITS 
by taking into account heterogeneous features. 
3 Conversation Focus Detection 
In threaded discussions, people participate in a 
conversation by posting messages. Our goal is to 
be able to detect which message in a thread con-
tains the most important information, i.e., the focus 
of the conversation. Unlike traditional IR systems, 
which return a ranked list of messages from a flat 
document set, our task must take into account 
characteristics of threaded discussions.  
First, messages play certain roles and are related 
to each other by a conversation context. Second, 
messages written by different authors may vary in 
value. Finally, since postings occur in parallel, by 
various people, message threads are not necessarily 
coherent so the lexical similarity among the mes-
sages should be analyzed. To detect the focus of 
conversation, we integrate a pragmatics study of 
conversational speech acts, an analysis of message 
values based on poster trustworthiness and an 
analysis of lexical similarity. The subsystems that 
determine these three sources of evidence comprise 
the features of our feature-based system. 
Because each discussion thread is naturally rep-
resented by a directed graph, where each message 
is represented by a node in the graph, we can apply 
a graph-based algorithm to integrate these sources 
and detect the focus of conversation. 
3.1 Thread Representation 
A discussion thread consists of a set of messages 
posted in chronological order. Suppose that each 
message is represented by mi, i =1,2,?, n. Then 
the entire thread is a directed graph that can be rep-
resented by G= (V, E), where V is the set of nodes 
(messages), V= {mi,i=1,...,n}, and E is the set of 
directed edges. In our approach, the set V is auto-
matically constructed as each message joins in the 
discussion. E is a subset of VxV. We will discuss 
the feature-oriented link generation functions that 
construct the set E in Section 4. 
We make use of speech act relations in generat-
ing the links. Once a speech act relation is identi-
fied between two messages, links will be generated 
using generation functions described in next sec-
tion. When mi is a message node in the thread 
graph, VmF i ?)( represents the set of nodes that 
node mi points to (i.e., children of mi), and 
VmB i ?)( represents the set of nodes that point to 
mi (i.e., parents of mi). 
3.2 Graph-Based Ranking Algorithm: HITS 
Graph-based algorithms can rank a set of objects in 
a collective way and the affect between each pair 
can be propagated into the whole graph iteratively. 
Here, we use a weighted HITS (Kleinberg, 1999) 
algorithm to conduct message ranking. 
Kleinberg (1999) initially proposed the graph-
based algorithm HITS for ranking a set of web 
pages. Here, we adjust the algorithm for the task of 
ranking a set of messages in a threaded discussion. 
In this algorithm, each message in the graph can be 
represented by two identity scores, hub score and 
authority score. The hub score represents the qual-
ity of the message as a pointer to valuable or useful 
messages (or resources, in general). The authority 
score measures the quality of the message as a re-
source itself. The weighted iterative updating com-
putations are shown in Equations 1 and 2. ?
?
+ =
)(
1 )(*)(
ij mFm
j
r
iji
r mauthoritywmhub           (1) 
?
?
+ =
)(
1 )(*)(
ij mBm
j
r
jii
r mhubwmauthority           (2) 
where r and r+1 are the numbers of iterations. 
The number of iterations required for HITS to 
converge depends on the initialization value for 
each message node and the complexity of the 
graph. Graph links can be induced with extra 
knowledge (e.g. Kurland and Lee, 2005). To help 
integrate our heterogeneous sources of evidence 
with our graph-based HITS algorithm, we intro-
duce link generation functions for each of the three 
features, (gi, i=1, 2, 3), to add links between mes-
sages. 
4 Feature-Oriented Link Generation 
210
Conversation structures have received a lot of at-
tention in the linguistic research community (Lev-
inson, 1983). In order to integrate conversational 
features into our computational model, we must 
convert a qualitative analysis into quantitative 
scores. For conversation analysis, we adopted the 
theory of Speech Acts proposed by (Austin, 1962; 
Searle, 1969) and defined a set of speech acts (SAs) 
that relate every pair of messages in the corpus. 
Though a pair of messages may only be labeled 
with one speech act, a message can have multiple 
SAs with other messages. 
We group speech acts by function into three 
categories, as shown in Figure 1. Messages may 
involve a request (REQ), provide information 
(INF), or fall into the category of interpersonal 
(INTP) relationship. Categories can be further di-
vided into several single speech acts.  
 
Figure 1. Categories of Message Speech Act. 
The SA set for our corpus is given in Table 1. A 
speech act may a represent a positive, negative or 
neutral response to a previous message depending 
on its attitude and recommendation. We classify 
each speech act as a direction as POSITIVE (+), 
NEGATIVE (?) or NEUTRAL, referred to as SA 
Direction, as shown in the right column of Table 1. 
The features we wish to include in our approach 
are lexical similarity between messages, poster 
trustworthiness, and speech act labels between 
message pairs in our discussion corpus.  
The feature-oriented link generation is con-
ducted in two steps. First, our approach examines 
in turn all the speech act relations in each thread 
and generates two types of links based on lexical 
similarity and SA strength scores. Second, the sys-
tem iterates over all the message nodes and assigns 
each node a self-pointing link associated with its 
poster trustworthiness score. The three features are 
integrated into the thread graph accordingly by the 
feature-oriented link generation functions. Multiple 
links with the same start and end points are com-
bined into one. 
Speech
Act Name Description Dir. 
ACK Acknowl-edge 
Confirm or             
acknowledge + 
CANS Complex Answer 
Give answer requiring a 
full description of pro-
cedures, reasons, etc. 
 
COMM Command Command or            announce  
COMP Compli-ment 
Praise an argument or 
suggestion + 
CORR Correct Correct a wrong answer or solution ? 
CRT Criticize Criticize an argument ? 
DESC Describe Describe a fact or    situation  
ELAB Elaborate Elaborate on a previous argument or question  
OBJ Object Object to an argument or suggestion ? 
QUES Question Ask question about a specific problem  
SANS Simple Answer 
Answer with a short 
phrase or few words      
(e.g. factoid, yes/no) 
 
SUG Suggest Give advice or suggest a solution  
SUP Support Support an argument or suggestion + 
Table 1. Types of message speech acts in corpus. 
4.1 Lexical Similarity 
Discussions are constructed as people express 
ideas, opinions, and thoughts, so that the text itself 
contains information about what is being dis-
cussed. Lexical similarity is an important measure 
for distinguishing relationships between message 
pairs. In our approach, we do not compute the lexi-
cal similarity of any arbitrary pair of messages, 
instead, we consider only message pairs that are 
present in the speech act set. The cosine similarity 
between each message pair is computed using the 
TF*IDF technique (Salton, 1989). 
Messages with similar words are more likely to 
be semantically-related. This information is repre-
sented by term frequency (TF). However, those 
Inform:    
INF 
Interpersonal: 
INTP 
COMM  
QUES  
Speech 
Act Request: 
REQ 
ACK 
COMP 
CRT  
OBJ  
SUP  
CANS 
CORR 
DESC 
ELAB 
SANS 
SUG 
211
with more general terms may be unintentionally 
biased when only TF is considered so Inverse 
Document Frequency (IDF) is introduced to miti-
gate the bias. The lexical similarity score can be 
calculated using their cosine similarity. 
),(cos_ ji
l mmsimW =                      (3) 
For a given a speech act, SAij(mi?mj), connect-
ing message mi and mj, the link generation function 
g1 is defined as follows:  
)()(1
l
ijij WarcSAg =                          (4) 
The new generated link is added to the thread 
graph connecting message node mi and mj with a 
weight of Wl. 
4.2 Poster Trustworthiness 
Messages posted by different people may have dif-
ferent degrees of trustworthiness. For example, 
students who contributed to our corpus did not 
seem to provide messages of equal value. To de-
termine the trustworthiness of a person, we studied 
the responses to their messages throughout the en-
tire corpus. We used the percentage of POSITIVE 
responses to a person?s messages to measure that 
person?s trustworthiness. In our case, POSITIVE 
responses, which are defined above, included SUP, 
COMP, and ACK. In addition, if a person?s mes-
sage closed a discussion, we rated it POSITIVE. 
Suppose the poster is represented by kperson , 
the poster score, pW , is a weight calculated by 
))((
))(_(
)(
k
k
k
p
personfeedbackcount
personfeedbackpositivecount
personW =  
                                                                        (5) 
For a given single speech act, SAij(mi?mj), the 
poster score indicates the importance of message 
mi by itself and the generation function is given by  
)()(2
p
iiij WarcSAg =                               (6) 
The generated link is self-pointing, and contains 
the strength of the poster information. 
4.3 Speech Act Analysis 
We compute the strength of each speech act in a 
generative way, based on the author and trustwor-
thiness of the author. The strength of a speech act 
is a weighted average over all authors. 
)(
)(
)(
)()( k
P
person
persons personW
SAcount
SAcount
dirsignSAW
k
k?= (7) 
where the sign function of direction is defined with 
Equation 8. 
??
??=
             Otherwise     1
NEGATIVE isdir  if     1
)(dirsign                      (8) 
All SA scores are computed using Equation 7 
and projected to [0, 1]. For a given speech act, 
SAij(mi?mj), the generation function will generate 
a weighted link in the thread graph as expressed in 
Equation 9. 
??
??
?=
               Otherwise      )(
NEUTRAL is  if      )(
)(3 s
ij
ij
s
ii
ij Warc
SAWarc
SAg     (9) 
The SA scores represent the strength of the rela-
tionship between the messages. Depending on the 
direction of the SA, the generated link will either 
go from message mi to mj or from message mi to mi 
(i.e., to itself). If the SA is NEUTRAL, the link will 
point to itself and the score is a recommendation to 
itself. Otherwise, the link connects two different 
messages and represents the recommendation de-
gree of the parent to the child message. 
5 Experiments 
5.1 Experimental Setup  
We tested our conversation-focus detection ap-
proach using a corpus of threaded discussions from 
three semesters of a USC undergraduate course in 
computer science. The corpus includes a total of 
640 threads consisting of 2214 messages, where a 
thread is defined as an exchange containing at least 
two messages. 
Length of thread Number of threads 
3 139 
4 74 
5 47 
6 30 
7 13 
8 11 
Table 2. Thread length distribution. 
From the complete corpus, we selected only 
threads with lengths of greater than two and less 
than nine (messages). Discussion threads with 
lengths of only two would bias the random guess 
of our baseline system, while discussion threads 
with lengths greater than eight make up only 3.7% 
of the total number of threads (640), and are the 
least coherent of the threads due to topic-switching 
and off-topic remarks. Thus, our evaluation corpus 
included 314 threads, consisting of 1307 messages, 
with an average thread length of 4.16 messages per 
212
thread. Table 2 gives the distribution of the lengths 
of the threads. 
The input of our system requires the identifica-
tion of speech act relations between messages. Col-
lective classification approaches, similar to the 
dependency-network based approach that Carvalho 
and Cohen (2005) used to classify email speech 
acts, might also be applied to discussion threads. 
However, as the paper is about investigating how 
an SA analysis, along with other features, can 
benefit conversation focus detection, so as to avoid 
error propagation from speech act labeling to sub-
sequent processing, we used manually-annotated 
SA relationships for our analysis. 
Code Frequency Percentage (%) 
ACK 53 3.96 
CANS 224 16.73 
COMM 8 0.6 
COMP 7 0.52 
CORR 20 1.49 
CRT 23 1.72 
DESC 71 5.3 
ELAB 105 7.84 
OBJ 21 1.57 
QUES 450 33.61 
SANS 23 1.72 
SUG 264 19.72 
SUP 70 5.23 
Table 3. Frequency of speech acts. 
The corpus contains 1339 speech acts. Table 3 
gives the frequencies and percentages of speech 
acts found in the data set. Each SA generates fea-
ture-oriented weighted links in the threaded graph 
accordingly as discussed previously. 
 
Number of best     
answers 
Number of threads 
1 250 
2 56 
3 5 
4 3 
Table 4. Gold standard length distribution. 
We then read each thread and choose the mes-
sage that contained the best answer to the initial 
query as the gold standard. If there are multiple 
best-answer messages, all of them will be ranked 
as best, i.e., chosen for the top position. For exam-
ple, different authors may have provided sugges-
tions that were each correct for a specified 
situation. Table 4 gives the statistics of the num-
bers of correct messages of our gold standard. 
We experimented with further segmenting the 
messages so as to narrow down the best-answer 
text, under the assumption that long messages 
probably include some less-than-useful informa-
tion. We applied TextTiling (Hearst, 1994) to seg-
ment the messages, which is the technique used by 
Zhou and Hovy (2005) to summarize discussions. 
For our corpus, though, the ratio of segments to 
messages was only 1.03, which indicates that our 
messages are relatively short and coherent, and that 
segmenting them would not provide additional 
benefits. 
5.2 Baseline System 
To compare the effectiveness of our approach with 
different features, we designed a baseline system 
that uses a random guess approach. Given a dis-
cussion thread, the baseline system randomly se-
lects the most important message. The result was 
evaluated against the gold standard. The perform-
ance comparisons of the baseline system and other 
feature-induced approaches are presented next. 
5.3 Result Analysis and Discussion 
We conducted extensive experiments to investigate 
the performance of our approach with different 
combinations of features. As we discussed in Sec-
tion 4.2, each poster acquires a trustworthiness 
score based on their behavior via an analysis of the 
whole corpus. Table 5 is a sample list of some 
posters with their poster id, the total number of 
responses (to their messages), the total number of 
positive responses, and their poster scores pW . 
 
Poster 
ID 
    Total 
Response 
  Positive 
Response  
pW  
193 1 1 1 
93 20 18 0.9 
38 15 12 0.8 
80 8 6 0.75 
47 253 182 0.719 
22 3 2 0.667 
44 9 6 0.667 
91 6 4 0.667 
147 12 8 0.667 
32 10 6 0.6 
190 9 5 0.556 
97 20 11 0.55 
12 2 1 0.5 
Table 5. Sample poster scores. 
213
Based on the poster scores, we computed the 
strength score of each SA with Equation 7 and pro-
jected them to [0, 1]. Table 6 shows the strength 
scores for all of the SAs. Each SA has a different 
strength score and those in the NEGATIVE cate-
gory have smaller ones (weaker recommendation). 
 
SA )(SAWs  SA )(SAWs  
CANS 0.8134 COMM 0.6534 
DESC 0.7166 ELAB 0.7202 
SANS 0.8281 SUG 0.8032 
QUES 0.6230   
ACK 0.6844 COMP 0.8081 
SUP 0.8057   
CORR 0.2543 CRT 0.1339 
OBJ 0.2405   
Table 6. SA strength scores. 
We tested the graph-based HITS algorithm with 
different feature combinations and set the error rate 
to be 0.0001 to get the algorithm to converge. In 
our experiments, we computed the precision score 
and the MRR (Mean Reciprocal Rank) score 
(Voorhees, 2001) of the most informative message 
chosen (the first, if there was more than one). Ta-
ble 7 shows the performance scores for the system 
with different feature combinations. The perform-
ance of the baseline system is shown at the top. 
The HITS algorithm assigns both a hub score 
and an authority score to each message node, re-
sulting in two sets of results. Scores in the HITS_ 
AUTHORITY rows of Table 7 represent the re-
sults using authority scores, while HITS_HUB 
rows represent the results using hub scores.  
Due to the limitation of thread length, the lower 
bound of the MRR score is 0.263. As shown in the 
table, a random guess baseline system can get a 
precision of 27.71% and a MRR score of 0.539.  
When we consider only lexical similarity, the 
result is not so good, which supports the notion 
that in human conversation context is often more 
important than text at a surface level. When we 
consider poster and lexical score together, the per-
formance improves. As expected, the best per-
formances use speech act analysis. More features 
do not always improve the performance, for exam-
ple, the lexical feature will sometimes decrease 
performance. Our best performance produced a 
precision score of 70.38% and an MRR score of 
0.825, which is a significant improvement over the 
baseline?s precision score of 27.71% and its MRR 
score of 0.539. 
Algorithm  & 
Features 
Correct   
(out of 314) 
Precision 
(%) MRR 
Baseline 87 27.71 0.539 
Lexical 65 20.70 0.524 
Poster 90 28.66 0.569 
SA 215 68.47 0.819 
Lexical +  
Poster 91 28.98 0.565 
Lexical +      
SA 194 61.78 0.765 
Poster +        
SA 221 70.38 0.825 H
IT
S_
A
U
T
H
O
R
IT
Y
 
Lexical +  
Poster + 
SA 
212 67.52 0.793 
Lexical 153 48.73 0.682 
Poster 79 25.16 0.527 
SA 195 62.10 0.771 
Lexical +  
Poster 158 50.32 0.693 
Lexical +      
SA 177 56.37 0.724 
Poster +       
SA 207 65.92 0.793 
H
IT
S_
H
U
B
 
Lexical + 
Poster + 
SA 
196 62.42 0.762 
Table 7. System Performance Comparison. 
Another widely-used graph algorithm in IR is 
PageRank (Brin and Page, 1998). It is used to in-
vestigate the connections between hyperlinks in 
web page retrieval. PageRank uses a ?random 
walk? model of a web surfer?s behavior. The surfer 
begins from a random node mi and at each step 
either follows a hyperlink with the probability of d, 
or jumps to a random node with the probability of 
(1-d). A weighted PageRank algorithm is used to 
model weighted relationships of a set of objects. 
The iterative updating expression is 
? ??
?
+ +?=
)(
)(
1 )(*)1()(
ij
jk
mBm
j
r
mFm
jk
ji
i
r mPR
w
w
ddmPR  (10) 
where r and r+1 are the numbers of iterations. 
 
We also tested this algorithm in our situation, 
but the best performance had a precision score of 
only 47.45% and an MRR score of 0.669. It may 
be that PageRank?s definition and modeling ap-
proach does not fit our situation as well as the 
HITS approach. In HITS, the authority and hub- 
214
based approach is better suited to human conversa-
tion analysis than PageRank, which only considers 
the contributions from backward links of each 
node in the graph. 
6 Conclusions and Future Work 
We have presented a novel feature-enriched ap-
proach for detecting conversation focus of threaded 
discussions for the purpose of answering student 
queries. Using feature-oriented link generation and 
a graph-based algorithm, we derived a unified 
framework that integrates heterogeneous sources 
of evidence. We explored the use of speech act 
analysis, lexical similarity and poster trustworthi-
ness to analyze discussions. 
From the perspective of question answering, this 
is the first attempt to automatically answer com-
plex and contextual discussion queries beyond fac-
toid or definition questions. To fully automate 
discussion analysis, we must integrate automatic 
SA labeling together with our conversation focus 
detection approach. An automatic system will help 
users navigate threaded archives and researchers 
analyze human discussion. 
Supervised learning is another approach to de-
tecting conversation focus that might be explored. 
The tradeoff and balance between system perform-
ance and human cost for different learning algo-
rithms is of great interest. We are also exploring 
the application of graph-based algorithms to other 
structured-objects ranking problems in NLP so as 
to improve system performance while relieving 
human costs. 
Acknowledgements 
The work was supported in part by DARPA grant DOI-
NBC Contract No. NBCHC050051, Learning by Read-
ing, and in part by a grant from the Lord Corporation 
Foundation to the USC Distance Education Network. 
The authors want to thank Deepak Ravichandran, Feng 
Pan, and Rahul Bhagat for their helpful suggestions 
with the manuscript. We would also like to thank the 
HLT-NAACL reviewers for their valuable comments. 
References 
Austin, J. 1962. How to do things with words. Cam-
bridge, Massachusetts: Harvard Univ. Press. 
Brin, S. and Page, L. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer 
Networks and ISDN Systems, 30(1-7):107--117. 
Carvalho, V.R. and Cohen, W.W. 2005. On the collec-
tive classification of email speech acts. In Proceed-
ings of SIGIR-2005, pp. 345-352. 
Erkan, G. and Radev, D. 2004. Lexrank: graph-based 
centrality as salience in text summarization. Journal 
of Artificial Intelligence Research (JAIR). 
Feng, D., Shaw, E., Kim, J., and Hovy, E.H. 2006. An 
intelligent discussion-bot for answering student que-
ries in threaded discussions. In Proceedings of Intel-
ligent User Interface (IUI-2006), pp. 171-177.  
Hearst, M.A. 1994. Multi-paragraph segmentation of 
expository text. In Proceedings of ACL-1994.  
Kleinberg, J. 1999. Authoritative sources in a hyper-
linked environment. Journal of the ACM, 46(5). 
Kurland, O. and Lee L. 2005. PageRank without hyper-
links: Structural re-ranking using links induced by 
language models. In Proceedings of SIGIR-2005.  
Levinson, S. 1983. Pragmatics. Cambridge Univ. Press. 
Mann, W.C. and Thompson, S.A. 1988. Rhetorical 
structure theory: towards a functional theory of text 
organization. Text, 8 (3), pp. 243-281. 
Marom, Y. and Zukerman, I. 2005. Corpus-based gen-
eration of easy help-desk responses. Technical Re-
port, Monash University. Available at: 
http://www.csse.monash.edu.au/publications/2005/tr-
2005-166-full.pdf. 
Mihalcea, R. 2004. Graph-based ranking algorithms for 
sentence extraction, applied to text summarization. In 
Companion Volume to ACL-2004. 
Mihalcea, R. 2005. unsupervised large-vocabulary word 
sense disambiguation with graph-based algorithms 
for sequence data labeling. In HLT/EMNLP 2005. 
Mihalcea, R. and Tarau, P. 2004. TextRank: bringing 
order into texts. In Proceedings of EMNLP 2004. 
Mihalcea, R., Tarau, P. and Figa, E. 2004. PageRank on 
semantic networks, with application to word sense 
disambiguation. In Proceedings of COLING 2004. 
Otterbacher, J., Erkan, G., and  Radev, D. 2005. Using 
random walks for question-focused sentence re-
trieval. In Proceedings of HLT/EMNLP 2005.  
Pang, B. and Lee, L. 2004. A sentimental education: 
sentiment analysis using subjectivity summarization 
based on minimum cuts. In ACL-2004. 
Salton, G. 1989. Automatic Text Processing, The Trans-
formation, Analysis, and Retrieval of Information by 
Computer. Addison-Wesley, Reading, MA, 1989. 
Searle, J. 1969. Speech Acts. Cambridge: Cambridge 
Univ. Press. 
Soricut, R. and Marcu, D. 2003. Sentence level dis-
course parsing using syntactic and lexical informa-
tion. In Proceedings of HLT/NAACL-2003. 
Sporleder, C. and Lapata, M. 2005. Discourse chunking 
and its application to sentence compression. In Pro-
ceedings of HLT/EMNLP 2005. 
Voorhees, E.M. 2001. Overview of the TREC 2001 
question answering track. In TREC 2001. 
Zhou, L. and Hovy, E.H. 2005. Digesting virtual ?geek
?culture: the summarization of technical internet re-
lay chats. In Proceedings of ACL 2005. 
215
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 447?454,
New York, June 2006. c?2006 Association for Computational Linguistics
ParaEval: Using Paraphrases to Evaluate Summaries Automatically
Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, cyl, dragos, hovy} @isi.edu
Abstract
ParaEval is an automated evaluation
method for comparing reference and peer
summaries. It facilitates a tiered-
comparison strategy where recall-oriented
global optimal and local greedy searches
for paraphrase matching are enabled in
the top tiers. We utilize a domain-
independent paraphrase table extracted
from a large bilingual parallel corpus us-
ing methods from Machine Translation
(MT). We show that the quality of ParaE-
val?s evaluations, measured by correlating
with human judgments, closely resembles
that of ROUGE?s.
1 Introduction
Content coverage is commonly measured in sum-
mary comparison to assess how much information
from the reference summary is included in a peer
summary. Both manual and automatic methodolo-
gies have been used. Naturally, there is a great
amount of confidence in manual evaluation since
humans can infer, paraphrase, and use world
knowledge to relate text units with similar mean-
ings, but which are worded differently. Human
efforts are preferred if the evaluation task is easily
conducted and managed, and does not need to be
performed repeatedly. However, when resources
are limited, automated evaluation methods become
more desirable.
For years, the summarization community has
been actively seeking an automatic evaluation
methodology that can be readily applied to various
summarization tasks. ROUGE (Lin and Hovy,
2003) has gained popularity due to its simplicity
and high correlation with human judgments. Even
though validated by high correlations with human
judgments gathered from previous Document Un-
derstanding Conference (DUC) experiments, cur-
rent automatic procedures (Lin and Hovy, 2003;
Hovy et al, 2005) only employ lexical n-gram
matching. The lack of support for word or phrase
matching that stretches beyond strict lexical
matches has limited the expressiveness and utility
of these methods. We need a mechanism that sup-
plements literal matching?i.e. paraphrase and
synonym?and approximates semantic closeness.
In this paper we present ParaEval, an automatic
summarization evaluation method, which facili-
tates paraphrase matching in an overall three-level
comparison strategy. At the top level, favoring
higher coverage in reference, we perform an opti-
mal search via dynamic programming to find
multi-word to multi-word paraphrase matches be-
tween phrases in the reference summary (usually
human-written) and those in the peer summary
(system-generated). The non-matching fragments
from the previous level are then searched by a
greedy algorithm to find single-word para-
phrase/synonym matches. At the third and the low-
est level, we perform literal lexical unigram
matching on the remaining texts. This tiered design
for summary comparison guarantees at least a
ROUGE-1 level of summary content matching if
no paraphrases are found.
The first two levels employ a paraphrase table.
Since manually created multi-word paraphrases-
?phrases determined by humans to be paraphrases
of one another?are not available in sufficient
quantities, we automatically build a paraphrase
447
table using methods from the Machine Translation
(MT) field. The assumption made in creating this
table is that if two English phrases are translated
into the same foreign phrase with high probability
(shown in the alignment results from a statistically
trained alignment algorithm), then the two English
phrases are paraphrases of each other.
This paper is organized in the following way:
Section 2 introduces previous work in summariza-
tion evaluation; Section 3 describes the motivation
behind this work; paraphrase acquisition is dis-
cussed in Section 4; Section 5 explains in detail
our summary comparison mechanism; Section 6
validates ParaEval with human summary judg-
ments; and we conclude and discuss future work in
Section 7.
2 Previous Work
There has been considerable work in both manual
and automatic summarization evaluations. Three
most noticeable efforts in manual evaluation are
SEE (Lin and Hovy, 2001), Factoid (Van Halteren
and Teufel, 2003), and the Pyramid method
(Nenkova and Passonneau, 2004).
SEE provides a user-friendly environment in
which human assessors evaluate the quality of
system-produced peer summary by comparing it to
a reference summary. Summaries are represented
by a list of summary units (sentences, clauses,
etc.). Assessors can assign full or partial content
coverage score to peer summary units in compari-
son to the corresponding reference summary units.
Grammaticality can also be graded unit-wise.
The goal of the Factoid work is to compare the
information content of different summaries of the
same text and determine the minimum number of
summaries, which was shown through experimen-
tation to be 20-30, needed to achieve stable con-
sensus among 50 human-written summaries.
The Pyramid method uses identified consen-
sus?a pyramid of phrases created by annota-
tors?from multiple reference summaries as the
gold-standard reference summary. Summary com-
parisons are performed on Summarization Content
Units (SCUs) that are approximately of clause
length.
To facilitate fast summarization system design-
evaluation cycles, ROUGE was created (Lin and
Hovy, 2003). It is an automatic evaluation package
that measures a number of n-gram co-occurrence
statistics between peer and reference summary
pairs. ROUGE was inspired by BLEU (Papineni et
al., 2001) which was adopted by the machine
translation (MT) community for automatic MT
evaluation. A problem with ROUGE is that the
summary units used in automatic comparison are
of fixed length. A more desirable design is to have
summary units of variable size. This idea was im-
plemented in the Basic Elements (BE) framework
(Hovy et al, 2005) which has not been completed
due to its lack of support for paraphrase matching.
Both ROUGE and BE have been shown to corre-
late well with past DUC human summary judg-
ments, despite incorporating only lexical matching
on summary units (Lin and Hovy, 2003; Hovy et
al., 2005).
3 Motivation
3.1 Paraphrase Matching
An important difference that separates current
manual evaluation methods from their automatic
counterparts is that semantic matching of content
units is performed by human summary assessors.
An essential part of the semantic matching in-
volves paraphrase matching?determining whether
phrases worded differently carry the same semantic
information. This paraphrase matching process is
observed in the Pyramid annotation procedure
shown in (Nenkova and Passonneau, 2004) over
three summary sets (10 summaries each). In the
example shown in Figure 1 (reproduced from
Pyramid results), each of the 10 phrases (numbered
1 to 10) extracted from summary sentences carries
the same semantic content as the overall summary
content unit labeled SCU1 does. Each extracted
phrase is identified as a summary content unit
(SCU). In our work in building an automatic
evaluation procedure that enables paraphrase
SCU1: the crime in question was the Lockerbie {Scotland} bombing
1 [for the Lockerbie bombing]
2 [for blowing up] [over Lockerbie, Scotland]
3 [of bombing] [over Lockerbie, Scotland]
4 [was blown up over Lockerbie, Scotland, ]
5 [the bombing of Pan Am Flight 103]
6 [bombing over Lockerbie, Scotland, ]
7 [for Lockerbie bombing]
8 [bombing of Pan Am flight 103 over Lockerbie. ]
9 [linked to the Lockerbie bombing]
10 [in the Lockerbie bombing case. ]
Figure 1. Paraphrases created by Pyramid annotation.
448
matching, we aim to automatically identify these
10 phrases as paraphrases of one another.
3.2 Synonymy Relations
Synonym matching and paraphrase matching are
often mentioned in the same context in discussions
of extending current automated summarization
evaluation methods to incorporate the matching of
semantic units. While evaluating automatically
extracted paraphrases via WordNet (Miller et al,
1990), Barzilay and McKeown (2001) quantita-
tively validated that synonymy is not the only
source of paraphrasing. We envisage that this
claim is also valid for summary comparisons.
From an in-depth analysis on the manually cre-
ated SCUs of the DUC2003 summary set D30042
(Nenkova and Passonneau, 2004), we find that
54.48% of 1746 cases where a non-stop word from
one SCU did not match with its supposedly hu-
man-aligned pairing SCUs are in need of some
level of paraphrase matching support. For example,
in the first two extracted SCUs (labeled as 1 and 2)
in Figure 1??for the Lockerbie bombing? and ?for
blowing up ? over Lockerbie, Scotland??no
non-stop word other than the word ?Lockerbie?
occurs in both phrases. But these two phrases were
judged to carry the same semantic meaning be-
cause human annotators think the word ?bombing?
and the phrase ?blowing up? refer to the same ac-
tion, namely the one associated with ?explosion.?
However, ?bombing? and ?blowing up? cannot be
matched through synonymy relations by using
WordNet, since one is a noun and the other is a
verb phrase (if tagged within context). Even when
the search is extended to finding synonyms and
hypernyms for their categorical variants and/or
using other parts of speech (verb for ?bombing?
and noun phrase for ?blowing up?), a match still
cannot be found.
To include paraphrase matching in summary
evaluation, a collection of less-strict paraphrases
must be created and a matching strategy needs to
be investigated.
4 Paraphrase Acquisition
Paraphrases are alternative verbalizations for con-
veying the same information and are required by
many Natural Language Processing (NLP) appli-
cations. In particular, summary creation and
evaluation methods need to recognize paraphrases
and their semantic equivalence. Unfortunately, we
have yet to incorporate into the evaluation frame-
work previous findings in paraphrase identification
and extraction (Barzilay and McKeown, 2001;
Pang et al, 2003; Bannard and Callison-Burch,
2005).
4.1 Related Work on Paraphrasing
Three major approaches in paraphrase collection
are manual collection (domain-specific), collection
utilizing existing lexical resources (i.e. WordNet),
and derivation from corpora. Hermjakob et al
(2002) view paraphrase recognition as
reformulation by pattern recognition. Pang et al
(2003) use word lattices as paraphrase representa-
tions from semantically equivalent translations
sets. Using parallel corpora, Barzilay and McKe-
own (2001) identify paraphrases from multiple
translations of classical novels, where as Bannard
and Callison-Burch (2005) develop a probabilistic
representation for paraphrases extracted from large
Machine Translation (MT) data sets.
4.2 Extracting Paraphrases
Our method to automatically construct a large do-
main-independent paraphrase collection is based
on the assumption that two different English
phrases of the same meaning may have the same
translation in a foreign language.
Phrase-based Statistical Machine Translation
(SMT) systems analyze large quantities of bilin-
gual parallel texts in order to learn translational
alignments between pairs of words and phrases in
two languages (Och and Ney, 2004). The sentence-
based translation model makes word/phrase align-
ment decisions probabilistically by computing the
optimal model parameters with application of the
statistical estimation theory. This alignment proc-
ess results in a corpus of word/phrase-aligned par-
allel sentences from which we can extract phrase
pairs that are translations of each other. We ran the
alignment algorithm from (Och and Ney, 2003) on
a Chinese-English parallel corpus of 218 million
English words. Phrase pairs are extracted by fol-
lowing the method described in (Och and Ney,
2004) where all contiguous phrase pairs having
consistent alignments are extraction candidates.
The resulting phrase table is of high quality; both
the alignment models and phrase extraction meth-
449
ods have been shown to produce very good results
for SMT. Using these pairs we build paraphrase
sets by joining together all English phrases with
the same Chinese translation. Figure 2 shows an
example word/phrase alignment for two parallel
sentence pairs from our corpus where the phrases
?blowing up? and ?bombing? have the same Chi-
nese translation. On the right side of the figure we
show the paraphrase set which contains these two
phrases, which is typical in our collection of ex-
tracted paraphrases.
5 Summary Comparison in ParaEval
This section describes the process of comparing a
peer summary against a reference summary and the
summary grading mechanism.
5.1 Description
We adopt a three-tier matching strategy for sum-
mary comparison. The score received by a peer
summary is the ratio of the number of reference
words matched to the total number of words in the
reference summary. The total number of matched
reference words is the sum of matched words in
reference throughout all three tiers. At the top
level, favoring high recall coverage, we perform an
optimal search to find multi-word paraphrase
matches between phrases in the reference summary
and those in the peer. Then a greedy search is per-
formed to find single-word paraphrase/synonym
matches among the remaining text. Operations
conducted in these two top levels are marked as
linked rounded rectangles in Figure 3. At the bot-
tom level, we find lexical identity matches, as
marked in rectangles in the example. If no para-
phrases are found, this last level provides a guar-
antee of lexical comparison that is equivalent to
what other automated systems give. In our system,
the bottom level currently performs unigram
matching. Thus, we are ensured with at least a
ROUGE-1 type of summary comparison. Alterna-
tively, equivalence of other ROUGE configura-
tions can replace the ROUGE-1 implementation.
There is no theoretical reason why the first two
levels should not merge. But due to high computa-
tional cost in modeling an optimal search, the sepa-
ration is needed. We explain this in detail below.
5.2 Multi-Word Paraphrase Matching
In this section we describe the algorithm that per-
forms the multi-word paraphrase matching be-
tween phrases from reference and peer summaries.
Using the example in Figure 3, this algorithm cre-
ates the phrases shown in the rounded rectangles
and establishes the appropriate links indicating
corresponding paraphrase matches.
Problem Description
Measuring content coverage of a peer summary
using a single reference summary requires com-
puting the recall score of how much information
from the reference summary is included in the
peer. A summary unit, either from reference or
peer, cannot be matched for more than once. For
Figure 2. An example of paraphrase extraction.
Figure 3. Comparison of summaries.
450
example, the phrase ?imposed sanctions on Libya?
(r1) in Figure 3?s reference summary was matched
with the peer summary?s ?voted sanctions against
Libya? (p1). If later in the peer summary there is
another phrase p2 that is also a paraphrase of r1, the
match of r1 cannot be counted twice. Conversely,
double counting is not permissible for
phrase/words in the peer summary, either.
We conceptualize the comparison of peer
against reference as a task that is to complete over
several time intervals. If the reference summary
contains n sentences, there will be n time intervals,
where at time ti, phrases from a particular sentence
i of the reference summary are being considered
with all possible phrases from the peer summary
for paraphrase matches. A decision needs to be
made at each time interval:
? Do we employ a local greedy match algo-
rithm that is recall generous (preferring more
matched words from reference) towards only the
reference sentence currently being analyzed,
? Or do we need to explore globally, in-
specting all reference sentences and find the best
overall matching combinations?
Consider the scenario in Figure 4:
1) at t0: L(p1 = r2) > L(p2 = r1) and r2 contains r1.
A local search algorithm leads to match(p1, r2). L() indi-
cates the number of words in reference matched by the
peer phrase through paraphrase matching and match()
indicates a paraphrase match has occurred (more in the
figure).
2) at t1: L(p1 = r3) > L(p1 = r2). A global algo-
rithm reverses the decision match(p1, r2) made at t0 and
concludes match(p1, r3) and match(p2, r1) . A local
search algorithm would have returned no match.
Clearly, the global search algorithm achieves
higher overall recall (in words). The matching of
paraphrases between a reference and its peer be-
comes a global optimization problem, maximizing
the content coverage of the peer compared in refer-
ence.
Solution Model
We use dynamic programming to derive the solu-
tion of finding the best paraphrase-matching com-
binations. The optimization problem is as follows:
Sentences from a reference summary and a peer
summary can be broken into phrases of various
lengths. A paraphrase lookup table is used to find
whether a reference phrase and a peer phrase are
paraphrases of each other. What is the optimal
paraphrase matching combination of phrases from
reference and peer that gives the highest recall
score (in number of matched reference words) for
this given peer? The solution should be recall ori-
ented (favoring a peer phrase that matches more
reference words than those match less).
Following (Trick, 1997), the solution can be
characterized as:
1) This problem can be divided into n stages
corresponding to the n sentences of the reference
summary. At each stage, a decision is required to
determine the best combination of matched para-
phrases between the reference sentence and the
entire peer summary that results in no double
counting of phrases on the peer side. There is no
double counting of reference phrases across stages
since we are processing one reference sentence at a
time and are finding the best paraphrase matches
using the entire peer summary. As long as there is
no double counting in peers, we are guaranteed to
have none in reference, either.
2) At each stage, we define a number of pos-
sible states as follows.  If, out of all possible
phrases of any length extracted from the reference
sentence, m phrases were found to have matching
paraphrases in the peer summary, then a state is
any subset of the m phrases.
3) Since no double counting in matched
phrases/words is allowed in either the reference
summary or the peer summary, the decision of
which phrases (leftover text segments in reference
Pj and ri represent phrases chosen for   paraphrase
matching from peer and reference respectively.
Pj = ri indicates that the phrase Pj from peer is
found to be a paraphrase to the phrase ri from
reference.
L(Pj = ri) indicates the number of words matched
by Pj in ri when they are found to be paraphrases of
each other.
L(Pj = ri) and L(Pj = ri+1) may not be equal if the
number of words in ri, indicated by L(ri), does not
equal to the number of words in ri+1, indicated by
L(ri+1).
Figure 4. Local vs. global paraphrase matching.
451
and in peer) are allowed to match for the next stage
is made in the current stage.
4) Principle of optimality: at a given state, it
is not necessary to know what matches occurred at
previous stages, only on the accumulated recall
score (matched reference words) from previous
stages and what text segments (phrases) in peer
have not been taken/matched in previous stages.
5) There exists a recursive relationship that
identifies the optimal decision for stage s (out of n
total stages), given that stage s+1 has already been
solved.
6) The final stage, n (last sentence in refer-
ence), is solved by choosing the state that has the
highest accumulated recall score and yet resulted
no double counting in any phrase/word in peer the
summary.
Figure 5 demonstrates the optimal solution (12
reference words matched) for the example shown
in Figure 4. We can express the calculations in the
following formulas:
where fy(xb) denotes the optimal recall coverage
(number of words in the reference summary
matched by the phrases from the peer summary) at
state xb in stage y. r(xb) is the recall coverage given
state xb. And c(xb) records the phrases matched in
peer with no double counting, given state xb.
5.3 Synonym Matching
All paraphrases whose pairings do not involve
multi-word to multi-word matching are called
synonyms in our experiment. Since these phrases
have either a n-to-1 or 1-to-n matching ratio (such
as the phrases ?blowing up? and ?bombing?), a
greedy algorithm favoring higher recall coverage
reduces the state creation and stage comparison
costs associated with the optimal procedure
(O(m6): O(m3) for state creation, and for 2 stages at
any time)). The paraphrase table described in Sec-
tion 4 is used.
 Synonym matching is performed only on parts
of the reference and peer summaries that were not
matched from the multi-word paraphrase-matching
phase.
5.4 Lexical Matching
This matching phase performs straightforward
lexical matching, as exemplified by the text frag-
ments marked in rectangles in Figure 3. Unigrams
are used as the units for counting matches in ac-
cordance with the previous two matching phases.
 During all three matching phases, we employed
a ROUGE-1 style of counting. Other alternatives,
such as ROUGE-2, ROUGE-SU4, etc., can easily
be adapted to each phase.
6  Evaluation of ParaEval
To evaluate and validate the effectiveness of an
automatic evaluation metric, it is necessary to
show that automatic evaluations correlate with
human assessments highly, positively, and consis-
tently (Lin and Hovy, 2003). In other words, an
automatic evaluation procedure should be able to
distinguish good and bad summarization systems
by assigning scores with close resemblance to hu-
mans? assessments.
6.1 Document Understanding Conference
The Document Understanding Conference has
provided large-scale evaluations on both human-
created and system-generated summaries annually.
Research teams are invited to participate in solving
summarization problems with their systems. Sys-
tem-generated summaries are then assessed by
humans and/or automatic evaluation procedures.
The collection of human judgments on systems and
their summaries has provided a test-bed for devel-
oping and validating automated summary grading
methods (Lin and Hovy, 2003; Hovy et al, 2005).
The correlations reported by ROUGE and BE
show that the evaluation correlations between these
two systems and DUC human evaluations are
much higher on single-document summarization
tasks. One possible explanation is that when sum-
Figure 5. Solution for the example in Figure 4.
452
marizing from only one source (text), both human-
and system-generated summaries are mostly ex-
tractive. The reason for humans to take phrases (or
maybe even sentences) verbatim is that there is less
motivation to abstract when the input is not highly
redundant, in contrast to input for multi-document
summarization tasks, which we speculate allows
more abstracting. ROUGE and BE both facilitate
lexical n-gram matching, hence, achieving amaz-
ing correlations. Since our baseline matching strat-
egy is lexically based when paraphrase matching is
not activated, validation on single-doc summariza-
tion results is not repeated in our experiment.
6.2 Validation and Discussion
We use summary judgments from DUC2003?s
multi-document summarization (MDS) task to
evaluate ParaEval. During DUC2003, participating
systems created short summaries (~100 words) for
30 document sets. For each set, one assessor-
written summary was used as the reference to
compare peer summaries created by 18 automatic
systems (including baselines) and 3 other human-
written summaries. A system ranking was pro-
duced by taking the averaged performance on all
summaries created by systems. This evaluation
process is replicated in our validation setup for
ParaEval. In all, 630 summary pairs were com-
pared. Pearson?s correlation coefficient is com-
puted for the validation tests, using DUC2003
assessors? results as the gold standard.
Table 1 illustrates the correlation figures from
the DUC2003 test set. ParaEval-para_only shows
the correlation result when using only paraphrase
and synonym matching, without the baseline uni-
gram matching. ParaEval-2 uses multi-word para-
phrase matching and unigram matching, omitting
the greedy synonym-matching phrase. ParaEval-3
incorporates matching at all three granularity lev-
els.
We see that the current implementation of
ParaEval closely resembles the way ROUGE-1
differentiates system-generated summaries. We
believe this is due to the identical calculations of
recall scores. The score that a peer summary re-
ceives from ParaEval depends on the number of
words matched in the reference summary from its
paraphrase, synonym, and unigram matches. The
counting of individual words in reference indicates
a ROUGE-1 design in grading. However, a de-
tailed examination on individual reference-peer
comparisons shows that paraphrase and synonym
comparisons and matches, in addition to lexical n-
gram matching, do measure a higher level of con-
tent coverage. This is demonstrated in Figure 6a
and b. Strict unigram matching reflects the content
retained by a peer summary mostly in the 0.2-0.4
ranges in recall, shown as dark-colored dots in the
graphs. Allowing paraphrase and synonym match-
ing increases the detection of peer coverage to the
range of 0.3-0.5, shown as light-colored dots.
We conducted a manual evaluation to further
examine the paraphrases being matched. Using 10
summaries from the Pyramid data, we asked three
human subjects to judge the validity of 128 (ran-
domly selected) paraphrase pairs extracted and
identified by ParaEval. Each pair of paraphrases
was coupled with its respective sentences as con-
texts. All paraphrases judged were multi-word.
ParaEval received an average precision of 68.0%.
The complete agreement between judges is 0.582
according to the Kappa coefficient (Cohen, 1960).
In Figure 7, we show two examples that the human
judges consider to be good paraphrases produced
and matched by ParaEval. Judges voiced difficul-
DUC-2003 Pearson
ROUGE-1 0.622
ParaEval-para_only 0.41
ParaEval-2 0.651
ParaEval-3 0.657
Table 1. Correlation with DUC 2003 MDS results.
Human Summaries: ParaEval vs. ROUGE-1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
DUC2003 Summary Writers
Re
ca
ll 
(%
 w
or
d 
m
at
ch
)
ROUGE-1 Scores
ParaEval-3 Scores
System Summaries: ParaEval vs. ROUGE-1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
DUC2003 Systems
Re
ca
ll 
(%
 w
or
d 
m
at
ch
)
ROUGE-1 Scores
ParaEval-3 Scores
a).
Human-
written
summaries.
b).
System-
generated
summaries.
Figure 6. A detailed look at the scores assigned by
lexical and paraphrase/synonym comparisons.
453
Figure 7. Paraphrases matched by ParaEval.
ties in determining ?semantic equivalence.? There
were cases where paraphrases would be generally
interchangeable but could not be matched because
of non-semantic equivalence in their contexts. And
there were paraphrases that were determined as
matches, but if taken out of context, would not be
direct replacements of each other. These two situa-
tions are where the judges mostly disagreed.
7 Conclusion and Future Work
In this paper, we have described an automatic
summarization evaluation method, ParaEval, that
facilitates paraphrase matching using a large do-
main-independent paraphrase table extracted from
a bilingual parallel corpus. The three-layer match-
ing strategy guarantees a ROUGE-like baseline
comparison if paraphrase matching fails.
The paraphrase extraction module from the cur-
rent implementation of ParaEval does not dis-
criminate among the phrases that are found to be
paraphrases of one another. We wish to incorporate
the probabilistic paraphrase extraction model from
(Bannard and Callison-Burch, 2005) to better ap-
proximate the relations between paraphrases. This
adaptation will also lead to a stochastic model for
the low-level lexical matching and scoring.
We chose English-Chinese MT parallel data be-
cause they are news-oriented which coincides with
the task genre from DUC. However, it is unknown
how large a parallel corpus is sufficient in provid-
ing a paraphrase collection good enough to help
the evaluation process. The quality of the para-
phrase table is also affected by changes in the do-
main and language pair of the MT parallel data.
We plan to use ParaEval to investigate the impact
of these changes on paraphrase quality under the
assumption that better paraphrase collections lead
to better summary evaluation results.
The immediate impact and continuation of the
described work would be to incorporate paraphrase
matching and extraction into the summary creation
process. And with ParaEval, it is possible for us to
evaluate systems that do incorporate some level of
abstraction, especially paraphrasing.
References
Bannard, C. and C. Callison-Burch. 2005. Paraphrasing with bilingual
parallel corpora. Proceedings of ACL-2005.
Barzilay, R. and K. McKeown. 2001. Extracting paraphrases from a
parallel corpus. Proceedings of ACL/EACL-2001.
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer.
1993. The mathematics of machine translation: Parameter estima-
tion. Computational Linguistics, 19(2): 263?311, 1993.
Cohen, J. 1960. A coefficient of agreement for nominal scales. Edu-
cation and Psychological Measurement, 43(6):37?46.
Diab, M. and P. Resnik. 2002. An unsupervised method for word
sense tagging using parallel corpora. Proceedings of ACL-2002.
DUC. 2001?2005. Document Understanding Conferences.
Hermjakob, U., A. Echihabi, and D. Marcu. 2002. Natural language
based reformulation resource and web exploitation for question
answering. Proceedings of TREC-2002.
Hovy, E, C.Y. Lin, and L. Zhou. 2005. Evaluating DUC 2005 using
basic elements. Proceedings of DUC-2005.
Hovy, E., C.Y. Lin, L. Zhou, and J. Fukumoto. 2005a. Basic Ele-
ments. http://www.isi.edu/~cyl/BE.
Lin, C.Y.  2001. http://www.isi.edu/~cyl/SEE.
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of summaries
using n-gram co-occurrence statistics. Proceedings of the HLT-
2003.
Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller.
1990. Introduction to WordNet: An on-line lexical database. Inter-
national Journal of Lexicography, 3(4): 235?245.
Nenkova, A. and R. Passonneau. 2004. Evaluating content selection in
summarization: the pyramid method. Proceedings of the HLT-
NAACL 2004.
Och, F. J. and H. Ney. 2003. A systematic comparison of various
statistical alignment models. Computational Linguistics, 29(1): 19-
?51, 2003.
Och, F. J. and H. Ney. 2004. The alignment template approach to
statistical machine translation. Computational Linguistics, 30(4),
2004.
Pang, B. , K. Knight and D. Marcu. 2003. Syntax-based alignment of
multiple translations: extracting paraphrases and generating new
sentences. Proceedings of HLT/NAACL-2003.
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. IBM research report
Bleu: a method for automatic evaluation of machine translation
IBM Research Division Technical Report, RC22176, 2001.
Trick, M. A. 1997. A tutorial on dynamic program-
ming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.
Van Halteren, H. and S. Teufel. 2003. Examining the consensus be-
tween human summaries: initial experiments with factoid analysis.
Proceedings of HLT-2003.
454
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 57?60,
New York, June 2006. c?2006 Association for Computational Linguistics
OntoNotes: The 90% Solution 
 
 
Eduard Hovy Mitchell Marcus Martha Palmer Lance Ramshaw Ralph Weischedel 
USC/ICI Comp & Info Science ICS and Linguistics BBN Technologies BBN Technologies 
4676 Admiralty U. of Pennsylvania U. of Colorado 10 Moulton St. 10 Moulton St. 
Marina d. R., CA Philadelphia, PA Boulder, CO Cambridge, MA Cambridge, MA 
hovy 
@isi.edu 
mitch  
@cis.upenn.edu 
martha.palmer 
@colorado.edu 
lance.ramshaw 
@bbn.com 
weischedel 
@bbn.com 
 
 
 
 
Abstract* 
We describe the OntoNotes methodology and its 
result, a large multilingual richly-annotated corpus 
constructed at 90% interannotator agreement. An 
initial portion (300K words of English newswire 
and 250K words of Chinese newswire) will be 
made available to the community during 2007. 
1 Introduction 
Many natural language processing applications 
could benefit from a richer model of text meaning 
than the bag-of-words and n-gram models that cur-
rently predominate. Until now, however, no such 
model has been identified that can be annotated 
dependably and rapidly. We have developed a 
methodology for producing such a corpus at 90% 
inter-annotator agreement, and will release com-
pleted segments beginning in early 2007. 
The OntoNotes project focuses on a domain in-
dependent representation of literal meaning that 
includes predicate structure, word sense, ontology 
linking, and coreference. Pilot studies have shown 
that these can all be annotated rapidly and with 
better than 90% consistency. Once a substantial 
and accurate training corpus is available, trained 
algorithms can be developed to predict these struc-
tures in new documents. 
                                                        
*
 This work was supported under the GALE program of the 
Defense Advanced Research Projects Agency, Contract No. 
HR0011-06-C-0022. 
This process begins with parse (TreeBank) and 
propositional (PropBank) structures, which provide 
normalization over predicates and their arguments.  
Word sense ambiguities are then resolved, with 
each word sense also linked to the appropriate 
node in the Omega ontology. Coreference is also 
annotated, allowing the entity mentions that are 
propositional arguments to be resolved in context. 
Annotation will cover multiple languages (Eng-
lish, Chinese, and Arabic) and multiple genres 
(newswire, broadcast news, news groups, weblogs, 
etc.), to create a resource that is broadly applicable. 
2 Treebanking 
The Penn Treebank (Marcus et al, 1993) is anno-
tated with information to make predicate-argument 
structure easy to decode, including function tags 
and markers of ?empty? categories that represent 
displaced constituents.  To expedite later stages of 
annotation, we have developed a parsing system 
(Gabbard et al, 2006) that recovers both of these 
latter annotations, the first we know of.  A first-
stage parser matches the Collins (2003) parser on 
which it is based on the Parseval metric, while si-
multaneously achieving near state-of-the-art per-
formance on recovering function tags (F-measure 
89.0). A second stage, a seven stage pipeline of 
maximum entropy learners and voted perceptrons, 
achieves state-of-the-art performance (F-measure 
74.7) on the recovery of empty categories by com-
bining a linguistically-informed architecture and a 
rich feature set with the power of modern machine 
learning methods. 
57
3 PropBanking  
The Penn Proposition Bank, funded by ACE 
(DOD), focuses on the argument structure of verbs, 
and provides a corpus annotated with semantic 
roles, including participants traditionally viewed as 
arguments and adjuncts.  The 1M word Penn Tree-
bank II Wall Street Journal corpus has been suc-
cessfully annotated with semantic argument 
structures for verbs and is now available via the 
Penn Linguistic Data Consortium as PropBank I 
(Palmer et al, 2005).   Links from the argument 
labels in the Frames Files to FrameNet frame ele-
ments and VerbNet thematic roles are being added.  
This style of annotation has also been successfully 
applied to other genres and languages. 
4 Word Sense  
Word sense ambiguity is a continuing major ob-
stacle to accurate information extraction, summari-
zation and machine translation.  The subtle fine-
grained sense distinctions in WordNet have not 
lent themselves to high agreement between human 
annotators or high automatic tagging performance. 
Building on results in grouping fine-grained 
WordNet senses into more coarse-grained senses 
that led to improved inter-annotator agreement 
(ITA) and system performance (Palmer et al, 
2004; Palmer et al, 2006), we have  developed a 
process for rapid sense inventory creation and an-
notation that includes critical links between the 
grouped word senses and the Omega ontology 
(Philpot et al, 2005; see Section 5 below). 
This process is based on recognizing that sense 
distinctions can be represented by linguists in an 
hierarchical structure, similar to a decision tree, 
that is rooted in very coarse-grained distinctions 
which become increasingly fine-grained until 
reaching WordNet senses at the leaves.  Sets of 
senses under specific nodes of the tree are grouped 
together into single entries, along with the syntac-
tic and semantic criteria for their groupings, to be 
presented to the annotators.   
As shown in Figure 1, a 50-sentence sample of 
instances is annotated and immediately checked for 
inter-annotator agreement.  ITA scores below 90% 
lead to a revision and clarification of the groupings 
by the linguist. It is only after the groupings have 
passed the ITA hurdle that each individual group is 
linked to a conceptual node in the ontology. In ad-
dition to higher accuracy, we find at least a three-
fold increase in annotator productivity. 
 
Figure 1. Annotation Procedure 
As part of OntoNotes we are annotating the 
most frequent noun and verb senses in a 300K 
subset of the PropBank, and will have this data 
available for release in early 2007.  
4.1 Verbs 
Our initial goal is to annotate the 700 most fre-
quently occurring verbs in our data, which are 
typically also the most polysemous; so far 300 
verbs have been grouped and 150 double anno-
tated. Subcategorization frames and semantic 
classes of arguments play major roles in determin-
ing the groupings, as illustrated by the grouping for 
the 22 WN 2.1 senses for drive in Figure 2.  In ad-
word
Check against ontology (1 person)
not OK
Annotate test (2 people)
Results: agreement 
and confusion matrix
Sense partitioning, creating definitions, 
commentary, etc. (2 or 3 people)
Adjudication (1 person)
OK 
not OK
Sa
ve
 
fo
r 
fu
ll
a
n
n
o
ta
tio
n
GI: operating or traveling via a vehi-
cle 
NP (Agent) drive NP, NP drive PP 
WN1: ?Can you drive a truck??, WN2: ?drive to school,?, WN3: ?drive her to 
school,?, WN12: ?this truck drives well,? WN13: ?he drives a taxi,?,WN14: ?The car 
drove around the corner,?, WN:16: ?drive the turnpike to work,?  
G2: force to a position or stance 
NP drive NP/PP/infinitival 
WN4: ?He drives me mad.,? WN6: ?drive back the invaders,? WN7: ?She finally 
drove him to change jobs,? WN8: ?drive a nail,? WN15: ?drive the herd,? WN22: 
?drive the game.? 
G3:  to exert energy on behalf of 
something NP drive NP/infinitival 
WN5: ?Her passion drives her,? WN10: ?He is driving away at his thesis.? 
G4: cause object to move rapidly by 
striking it NP drive NP 
WN9: ?drive the ball into the outfield ,? WN17 ?drive a golf ball,? WN18 ?drive a 
ball? 
Figure 2. A Portion of the Grouping of WordNet Senses for "drive? 
58
dition to improved annotator productivity and ac-
curacy, we predict a corresponding improvement 
in word sense disambiguation performance.  Train-
ing on this new data, Chen and Palmer (2005) re-
port 86.3% accuracy for verbs using a smoothed 
maximum entropy model and rich linguistic fea-
tures, which is 10% higher than their earlier, state-
of-the art performance on ungrouped, fine-grained 
senses. 
4.2 Nouns 
We follow a similar procedure for the annotation 
of nouns.  The same individual who groups Word-
Net verb senses also creates noun senses, starting 
with WordNet and other dictionaries.  We aim to 
double-annotate the 1100 most frequent polyse-
mous nouns in the initial corpus by the end of 
2006, while maximizing overlap with the sentences 
containing annotated verbs.   
Certain nouns carry predicate structure; these 
include nominalizations (whose structure obvi-
ously is derived from their verbal form) and vari-
ous types of relational nouns (like father, 
President, and believer, that express relations be-
tween entities, often stated using of).  We have 
identified a limited set of these whose structural 
relations can be semi-automatically annotated with 
high accuracy.   
5 Ontology  
In standard dictionaries, the senses for each word 
are simply listed.   In order to allow access to addi-
tional useful information, such as subsumption, 
property inheritance, predicate frames from other 
sources, links to instances, and so on, our goal is to 
link the senses to an ontology.  This requires de-
composing the hierarchical structure into subtrees 
which can then be inserted at the appropriate con-
ceptual node in the ontology. 
The OntoNotes terms are represented in the 
110,000-node Omega ontology (Philpot et al, 
2005), under continued construction and extension 
at ISI.  Omega, which has been used for MT, 
summarization, and database alignment, has been 
assembled semi-automatically by merging a vari-
ety of sources, including Princeton?s WordNet, 
New Mexico State University?s Mikrokosmos, and 
a variety of Upper Models, including DOLCE 
(Gangemi et al, 2002), SUMO (Niles and Pease, 
2001), and ISI?s Upper Model, which are in the 
process of being reconciled.  The verb frames from 
PropBank, FrameNet, WordNet, and Lexical Con-
ceptual Structures (Dorr and Habash, 2001) have 
all been included and cross-linked.   
In work planned for later this year, verb and 
noun sense groupings will be manually inserted 
into Omega, replacing the current (primarily 
WordNet-derived) contents. For example, of the 
verb groups for drive in the table above, G1 and 
G4 will be placed into the area of ?controlled mo-
tion?, while G2 will then sort with ?attitudes?.   
6 Coreference  
The coreference annotation in OntoNotes connects 
coreferring instances of specific referring expres-
sions, meaning primarily NPs that introduce or 
access a discourse entity. For example, ?Elco In-
dustries, Inc.?, ?the Rockford, Ill. Maker of fasten-
ers?, and ?it? could all corefer. (Non-specific 
references like ?officials? in ?Later, officials re-
ported?? are not included, since coreference for 
them is frequently unclear.) In addition, proper 
premodifiers and verb phrases can be marked when 
coreferent with an NP, such as linking, ?when the 
company withdrew from the bidding? to ?the with-
drawal of New England Electric?.  
Unlike the coreference task as defined in the 
ACE program, attributives are not generally 
marked. For example, the ?veterinarian? NP would 
not be marked in ?Baxter Black is a large animal 
veterinarian?. Adjectival modifiers like ?Ameri-
can? in ?the American embassy? are also not sub-
ject to coreference. 
Appositives are annotated as a special kind of 
coreference, so that later processing will be able to 
supply and interpret the implicit copula link. 
All of the coreference annotation is being dou-
bly annotated and adjudicated. In our initial Eng-
lish batch, the average agreement scores between 
each annotator and the adjudicated results were 
91.8% for normal coreference and 94.2% for ap-
positives. 
7 Related and Future Work  
PropBank I (Palmer et al, 2005), developed at 
UPenn, captures predicate argument structure for 
verbs; NomBank provides predicate argument 
structure for nominalizations and other noun predi-
cates (Meyers et al, 2004).  PropBank II annota-
59
tion (eventuality ID?s, coarse-grained sense tags, 
nominal coreference and selected discourse con-
nectives) is being applied to a small (100K) paral-
lel Chinese/English corpus (Babko-Malaya et al, 
2004).  The OntoNotes representation extends 
these annotations, and allows eventual inclusion of 
additional shallow semantic representations for 
other phenomena, including temporal and spatial 
relations, numerical expressions, deixis, etc. One 
of the principal aims of OntoNotes is to enable 
automated semantic analysis.  The best current al-
gorithm for semantic role labeling for PropBank 
style annotation (Pradhan et al, 2005) achieves an 
F-measure of 81.0 using an SVM. OntoNotes will 
provide a large amount of new training data for 
similar efforts.   
Existing work in the same realm falls into two 
classes: the development of resources for specific 
phenomena or the annotation of corpora. An ex-
ample of the former is Berkeley?s FrameNet pro-
ject (Baker et al, 1998), which produces rich 
semantic frames, annotating a set of examples for 
each predicator (including verbs, nouns and adjec-
tives), and describing the network of relations 
among the semantic frames.  An example of the 
latter type is the Salsa project (Burchardt et al, 
2004), which produced a German lexicon based on 
the FrameNet semantic frames and annotated a 
large German newswire corpus.  A second exam-
ple, the Prague Dependency Treebank (Hajic et al, 
2001), has annotated a large Czech corpus with 
several levels of (tectogrammatical) representation, 
including parts of speech, syntax, and topic/focus 
information structure. Finally, the IL-Annotation 
project (Reeder et al, 2004) focused on the repre-
sentations required to support a series of increas-
ingly semantic phenomena across seven languages 
(Arabic, Hindi, English, Spanish, Korean, Japanese  
and French). In intent and in many details, 
OntoNotes is compatible with all these efforts, 
which may one day all participate in a larger multi-
lingual corpus integration effort.   
References  
O. Babko-Malaya, M. Palmer, N. Xue, A. Joshi, and S. Ku-
lick. 2004. Proposition Bank II: Delving Deeper, Frontiers 
in Corpus Annotation, Workshop, HLT/NAACL  
C. F. Baker, C. J. Fillmore, and J. B. Lowe. 1998. The Berke-
ley FrameNet Project. In Proceedings of COLING/ACL, 
pages 86-90. 
J. Chen and M. Palmer.  2005.  Towards Robust High Per-
formance Word Sense Disambiguation of English Verbs 
Using Rich Linguistic Features. In Proceedings of 
IJCNLP2005, pp. 933-944. 
B. Dorr and N. Habash.  2001.  Lexical Conceptual Structure 
Lexicons. In Calzolari et al ISLE-IST-1999-10647-WP2-
WP3, Survey of Major Approaches Towards Bilin-
gual/Multilingual Lexicons.  
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado, and M. 
Pinkal. 2006. Consistency and Coverage: Challenges for 
exhaustive semantic annotation. In Proceedings of DGfS-
06. 
C. Fellbaum (ed.). 1998. WordNet: An On-line Lexical Data-
base and Some of its Applications. MIT Press. 
R. Gabbard, M. Marcus, and S. Kulick. Fully Parsing the Penn 
Treebank. In Proceedings of HLT/NAACL 2006.  
A. Gangemi, N. Guarino, C. Masolo, A. Oltramari, and L. 
Schneider. 2002. Sweetening Ontologies with DOLCE. In 
Proceedings of EKAW  pp. 166-181. 
J. Hajic, B. Vidov?-Hladk?, and P. Pajas.  2001: The Prague 
Dependency Treebank: Annotation Structure and Support. 
Proceeding of the IRCS Workshop on Linguistic Data-
bases, pp. 105?114. 
M. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. 
Building a Large Annotated Corpus of English: The Penn 
Treebank. Computational Linguistics 19: 313-330. 
A. Meyers, R. Reeves, C Macleod, R. Szekely, V. Zielinska, 
B. Young, and R. Grishman. 2004. The NomBank Project: 
An Interim Report. Frontiers in Corpus Annotation, Work-
shop in conjunction with HLT/NAACL. 
I. Niles and A. Pease.  2001.  Towards a Standard Upper On-
tology.  Proceedings of the International Conference on 
Formal Ontology in Information Systems (FOIS-2001). 
M. Palmer, O. Babko-Malaya, and H. T. Dang. 2004. Differ-
ent Sense Granularities for Different Applications, 2nd 
Workshop on Scalable Natural Language Understanding 
Systems, at HLT/NAACL-04,  
M. Palmer, H. Dang and C. Fellbaum. 2006. Making Fine-
grained and Coarse-grained Sense Distinctions, Both 
Manually and Automatically, Journal of Natural Language 
Engineering, to appear. 
M. Palmer, D. Gildea, and P. Kingsbury. 2005. The Proposi-
tion Bank: A Corpus Annotated with Semantic Roles, 
Computational Linguistics, 31(1). 
A. Philpot, E.. Hovy, and P. Pantel. 2005. The Omega Ontol-
ogy. Proceedings of the ONTOLEX Workshop at IJCNLP 
 S. Pradhan, W. Ward, K. Hacioglu, J. Martin, D. Jurafsky.  
2005.  Semantic Role Labeling Using Different Syntactic 
Views.  Proceedings of the ACL.  
F. Reeder, B. Dorr, D. Farwell, N. Habash, S. Helmreich, E.H. 
Hovy, L. Levin, T. Mitamura, K. Miller, O. Rambow, A. 
Siddharthan. 2004.  Interlingual Annotation for MT Devel-
opment. Proceedings of AMTA.  
60
Proceedings of NAACL HLT 2007, pages 564?571,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
ISP: Learning Inferential Selectional Preferences 
 
Patrick Pantel?, Rahul Bhagat?, Bonaventura Coppola?, 
Timothy Chklovski?, Eduard Hovy? 
?Information Sciences Institute 
University of Southern California 
Marina del Rey, CA 
{pantel,rahul,timc,hovy}@isi.edu
?ITC-Irst and University of Trento 
Via Sommarive, 18 ? Povo 38050  
Trento, Italy 
coppolab@itc.it 
  
Abstract 
Semantic inference is a key component 
for advanced natural language under-
standing. However, existing collections of 
automatically acquired inference rules 
have shown disappointing results when 
used in applications such as textual en-
tailment and question answering. This pa-
per presents ISP, a collection of methods 
for automatically learning admissible ar-
gument values to which an inference rule 
can be applied, which we call inferential 
selectional preferences, and methods for 
filtering out incorrect inferences. We 
evaluate ISP and present empirical evi-
dence of its effectiveness. 
1 Introduction 
Semantic inference is a key component for ad-
vanced natural language understanding. Several 
important applications are already relying heavily 
on inference, including question answering 
(Moldovan et al 2003; Harabagiu and Hickl 2006), 
information extraction (Romano et al 2006), and 
textual entailment (Szpektor et al 2004). 
In response, several researchers have created re-
sources for enabling semantic inference. Among 
manual resources used for this task are WordNet 
(Fellbaum 1998) and Cyc (Lenat 1995). Although 
important and useful, these resources primarily 
contain prescriptive inference rules such as ?X di-
vorces Y ? X married Y?. In practical NLP appli-
cations, however, plausible inference rules such as 
?X married Y? ? ?X dated Y? are very useful. This, 
along with the difficulty and labor-intensiveness of 
generating exhaustive lists of rules, has led re-
searchers to focus on automatic methods for build-
ing inference resources such as inference rule 
collections (Lin and Pantel 2001; Szpektor et al 
2004) and paraphrase collections (Barzilay and 
McKeown 2001). 
Using these resources in applications has been 
hindered by the large amount of incorrect infer-
ences they generate, either because of altogether 
incorrect rules or because of blind application of 
plausible rules without considering the context of 
the relations or the senses of the words. For exam-
ple, consider the following sentence: 
Terry Nichols was charged by federal prosecutors for murder 
and conspiracy in the Oklahoma City bombing. 
and an inference rule such as: 
 X is charged by Y ? Y announced the arrest of X (1) 
Using this rule, we can infer that ?federal prosecu-
tors announced the arrest of Terry Nichols?. How-
ever, given the sentence: 
Fraud was suspected when accounts were charged by CCM 
telemarketers without obtaining consumer authorization. 
the plausible inference rule (1) would incorrectly 
infer that ?CCM telemarketers announced the ar-
rest of accounts?. 
This example depicts a major obstacle to the ef-
fective use of automatically learned inference 
rules. What is missing is knowledge about the ad-
missible argument values for which an inference 
rule holds, which we call Inferential Selectional 
Preferences. For example, inference rule (1) 
should only be applied if X is a Person and Y is a 
Law Enforcement Agent or a Law Enforcement 
Agency. This knowledge does not guarantee that 
the inference rule will hold, but, as we show in this 
paper, goes a long way toward filtering out errone-
ous applications of rules. 
In this paper, we propose ISP, a collection of 
methods for learning inferential selectional prefer-
ences and filtering out incorrect inferences. The 
564
presented algorithms apply to any collection of 
inference rules between binary semantic relations, 
such as example (1). ISP derives inferential selec-
tional preferences by aggregating statistics of in-
ference rule instantiations over a large corpus of 
text. Within ISP, we explore different probabilistic 
models of selectional preference to accept or reject 
specific inferences. We present empirical evidence 
to support the following main contribution: 
Claim: Inferential selectional preferences can be 
automatically learned and used for effectively fil-
tering out incorrect inferences. 
2 Previous Work 
Selectional preference (SP) as a foundation for 
computational semantics is one of the earliest top-
ics in AI and NLP, and has its roots in (Katz and 
Fodor 1963).  Overviews of NLP research on this 
theme are (Wilks and Fass 1992), which includes 
the influential theory of Preference Semantics by 
Wilks, and more recently (Light and Greiff 2002). 
Rather than venture into learning inferential 
SPs, much previous work has focused on learning 
SPs for simpler structures. Resnik (1996), the 
seminal paper on this topic, introduced a statistical 
model for learning SPs for predicates using an un-
supervised method. 
Learning SPs often relies on an underlying set of 
semantic classes, as in both Resnik?s and our ap-
proach. Semantic classes can be specified manu-
ally or derived automatically. Manual collections 
of semantic classes include the hierarchies of 
WordNet (Fellbaum 1998), Levin verb classes 
(Levin 1993), and FrameNet (Baker et al 1998). 
Automatic derivation of semantic classes can take 
a variety of approaches, but often uses corpus 
methods and the Distributional Hypothesis (Harris 
1964) to automatically cluster similar entities into 
classes, e.g. CBC (Pantel and Lin 2002). In this 
paper, we experiment with two sets of semantic 
classes, one from WordNet and one from CBC. 
Another thread related to our work includes ex-
tracting from text corpora paraphrases (Barzilay 
and McKeown 2001) and inference rules, e.g. 
TEASE1 (Szpektor et al 2004) and DIRT (Lin and 
Pantel 2001). While these systems differ in their 
approaches, neither provides for the extracted in-
                                                     
1 Some systems refer to inferences they extract as entail-
ments; the two terms are sometimes used interchangeably. 
ference rules to hold or fail based on SPs. Zanzotto 
et al (2006) recently explored a different interplay 
between SPs and inferences. Rather than examine 
the role of SPs in inferences, they use SPs of a par-
ticular type to derive inferences.  For instance the 
preference of win for the subject player, a nomi-
nalization of play, is used to derive that ?win ? 
play?. Our work can be viewed as complementary 
to the work on extracting semantic inferences and 
paraphrases, since we seek to refine when a given 
inference applies, filtering out incorrect inferences. 
3 Selectional Preference Models 
The aim of this paper is to learn inferential selec-
tional preferences for filtering inference rules. 
Let pi ? pj be an inference rule where p is a bi-
nary semantic relation between two entities x and 
y. Let ?x, p, y? be an instance of relation p. 
Formal task definition: Given an inference rule 
 pi ? pj and the instance ?x, pi, y?, our task is to 
determine if ?x, pj, y? is valid. 
Consider the example in Section 1 where we 
have the inference rule ?X is charged by Y? ? ?Y 
announced the arrest of X?. Our task is to auto-
matically determine that ?federal prosecutors an-
nounced the arrest of Terry Nichols? (i.e., 
?Terry Nichols, pj, federal prosecutors?) is valid 
but that ?CCM telemarketers announced the arrest 
of accounts? is invalid. 
Because the semantic relations p are binary, the 
selectional preferences on their two arguments may 
be either considered jointly or independently. For 
example, the relation p = ?X is charged by Y? 
could have joint SPs: 
 ?Person, Law Enforcement Agent? 
 ?Person, Law Enforcement Agency?  (2) 
 ?Bank Account, Organization? 
or independent SPs: 
 ?Person, *? 
 ?*, Organization? (3) 
 ?*, Law Enforcement Agent? 
This distinction between joint and independent 
selectional preferences constitutes the difference 
between the two models we present in this section. 
The remainder of this section describes the ISP 
approach. In Section 3.1, we describe methods for 
automatically determining the semantic contexts of 
each single relation?s selectional preferences. Sec-
tion 3.2 uses these for developing our inferential 
565
selectional preference models. Finally, we propose 
inference filtering algorithms in Section 3.3. 
3.1 Relational Selectional Preferences 
Resnik (1996) defined the selectional preferences 
of a predicate as the semantic classes of the words 
that appear as its arguments. Similarly, we define 
the relational selectional preferences of a binary 
semantic relation pi as the semantic classes C(x) of 
the words that can be instantiated for x and as the 
semantic classes C(y) of the words that can be in-
stantiated for y. 
The semantic classes C(x) and C(y) can be ob-
tained from a conceptual taxonomy as proposed in 
(Resnik 1996), such as WordNet, or from the 
classes extracted from a word clustering algorithm 
such as CBC (Pantel and Lin 2002). For example, 
given the relation ?X is charged by Y?, its rela-
tional selection preferences from WordNet could 
be {social_group, organism, state?} for X and 
{authority, state, section?} for Y. 
Below we propose joint and independent mod-
els, based on a corpus analysis, for automatically 
determining relational selectional preferences. 
Model 1: Joint Relational Model (JRM) 
Our joint model uses a corpus analysis to learn SPs 
for binary semantic relations by considering their 
arguments jointly, as in example (2). 
Given a large corpus of English text, we first 
find the occurrences of each semantic relation p. 
For each instance ?x, p, y?, we retrieve the sets C(x) 
and C(y) of the semantic classes that x and y be-
long to and accumulate the frequencies of the tri-
ples ?c(x), p, c(y)?, where c(x) ? C(x) and  
c(y) ? C(y)2. 
Each triple ?c(x), p, c(y)? is a candidate selec-
tional preference for p. Candidates can be incorrect 
when: a) they were generated from the incorrect 
sense of a polysemous word; or b) p does not hold 
for the other words in the semantic class. 
Intuitively, we have more confidence in a par-
ticular candidate if its semantic classes are closely 
associated given the relation p. Pointwise mutual 
information (Cover and Thomas 1991) is a com-
monly used metric for measuring this association 
strength between two events e1 and e2: 
                                                     
2 In this paper, the semantic classes C(x) and C(y) are ex-
tracted from WordNet and CBC (described in Section 4.2).  
 ( )( ) ( )21
21
21
,
log);(
ePeP
eeP
eepmi =  (3.1) 
We define our ranking function as the strength 
of association between two semantic classes, cx and 
cy3, given the relation p: 
 ( ) ( )( ) ( )pcPpcP pccPpcpcpmi yx yxyx
,
log; =  (3.2) 
Let |cx, p, cy| denote the frequency of observing 
the instance ?c(x), p, c(y)?. We estimate the prob-
abilities of Equation 3.2 using maximum likeli-
hood estimates over our corpus: 
( ) ?? ?= ,, ,,ppcpcP xx
 ( ) ???= ,, ,, pcppcP yy  ( ) ??= ,, ,,, p cpcpccP yxyx   (3.3) 
Similarly to (Resnik 1996), we estimate the 
above frequencies using: 
( )??
?=?
xcw
x wC
pw
pc
,,
,,
 
( )??
?=?
ycw
y wC
wp
cp
,,
,,
 
( ) ( )??? ?= yx cwcwyx wCwC
wpw
cpc
21 , 21
21 ,,,,
 
where |x, p, y| denotes the frequency of observing 
the instance ?x, p, y? and |C(w)| denotes the number 
of classes to which word w belongs. |C(w)| distrib-
utes w?s mass equally to all of its senses cw. 
Model 2: Independent Relational Model (IRM) 
Because of sparse data, our joint model can miss 
some correct selectional preference pairs. For ex-
ample, given the relation  
 Y announced the arrest of X 
we may find occurrences from our corpus of the 
particular class ?Money Handler? for X and ?Law-
yer? for Y, however we may never see both of 
these classes co-occurring even though they would 
form a valid relational selectional preference. 
To alleviate this problem, we propose a second 
model that is less strict by considering the argu-
ments of the binary semantic relations independ-
ently, as in example (3). 
Similarly to JRM, we extract each instance  
?x, p, y? of each semantic relation p and retrieve the 
set of semantic classes C(x) and C(y) that x and y 
belong to, accumulating the frequencies of the tri-
ples ?c(x), p, *? and ?*, p, c(y)?, where  
c(x) ? C(x) and c(y) ? C(y). 
All tuples ?c(x), p, *? and ?*, p, c(y)? are candi-
date selectional preferences for p. We rank candi-
dates by the probability of the semantic class given 
the relation p, according to Equations 3.3. 
                                                     
3 cx and cy are shorthand for c(x) and c(y) in our equations. 
566
3.2 Inferential Selectional Preferences 
Whereas in Section 3.1 we learned selectional 
preferences for the arguments of a relation p, in 
this section we learn selectional preferences for the 
arguments of an inference rule pi ? pj. 
Model 1: Joint Inferential Model (JIM) 
Given an inference rule pi ? pj, our joint model 
defines the set of inferential SPs as the intersection 
of the relational SPs for pi and pj, as defined in the 
Joint Relational Model (JRM). For example, sup-
pose relation pi = ?X is charged by Y? gives the 
following SP scores under the JRM: 
 ?Person, pi, Law Enforcement Agent? = 1.45 
 ?Person, pi, Law Enforcement Agency? = 1.21  
 ?Bank Account, pi, Organization? = 0.97 
and that pj = ?Y announced the arrest of X? gives 
the following SP scores under the JRM: 
 ?Law Enforcement Agent, pj, Person? = 2.01 
 ?Reporter, pj, Person? = 1.98  
 ?Law Enforcement Agency, pj, Person? = 1.61 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, Person? 
 ?Law Enforcement Agency, Person? 
We rank the candidate inferential SPs according 
to three ways to combine their relational SP scores, 
using the minimum, maximum, and average of the 
SPs. For example, for ?Law Enforcement Agent, 
Person?, the respective scores would be 1.45, 2.01, 
and 1.73. These different ranking strategies pro-
duced nearly identical results in our experiments, 
as discussed in Section 5. 
Model 2: Independent Inferential Model (IIM) 
Our independent model is the same as the joint 
model above except that it computes candidate in-
ferential SPs using the Independent Relational 
Model (IRM) instead of the JRM. Consider the 
same example relations pi and pj from the joint 
model and suppose that the IRM gives the follow-
ing relational SP scores for pi: 
 ?Law Enforcement Agent, pi, *? = 3.43 
 ?*, pi, Person? = 2.17  
 ?*, pi, Organization? = 1.24 
and the following relational SP scores for pj: 
 ?*, pj, Person? = 2.87 
 ?Law Enforcement Agent, pj, *? = 1.92  
 ?Reporter, pj, *? = 0.89 
The intersection of the two sets of SPs forms the 
candidate inferential SPs for the inference pi ? pj: 
 ?Law Enforcement Agent, *? 
 ?*, Person?  
We use the same minimum, maximum, and av-
erage ranking strategies as in JIM. 
3.3 Filtering Inferences 
Given an inference rule pi ? pj and the instance  
?x, pi, y?, the system?s task is to determine whether 
?x, pj, y? is valid. Let C(w) be the set of semantic 
classes c(w) to which word w belongs. Below we 
present three filtering algorithms which range from 
the least to the most permissive: 
? ISP.JIM, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, c(y)? was admitted by the 
Joint Inferential Model for some c(x) ? C(x) and 
c(y) ? C(y). 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SPs ?c(x), pj, *? AND ?*, pj, c(y)? were 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
? ISP.IIM.?, accepts the inference ?x, pj, y? if the 
inferential SP ?c(x), pj, *? OR ?*, pj, c(y)? was 
admitted by the Independent Inferential Model 
for some c(x) ? C(x) and c(y) ? C(y) . 
Since both JIM and IIM use a ranking score in 
their inferential SPs, each filtering algorithm can 
be tuned to be more or less strict by setting an ac-
ceptance threshold on the ranking scores or by se-
lecting only the top ? percent highest ranking SPs. 
In our experiments, reported in Section 5, we 
tested each model using various values of ?. 
4 Experimental Methodology 
This section describes the methodology for testing 
our claim that inferential selectional preferences 
can be learned to filter incorrect inferences. 
Given a collection of inference rules of the form 
pi ? pj, our task is to determine whether a particu-
lar instance ?x, pj, y? holds given that ?x, pi, y? 
holds4. In the next sections, we describe our collec-
tion of inference rules, the semantic classes used 
for forming selectional preferences, and evaluation 
criteria for measuring the filtering quality. 
                                                     
4 Recall that the inference rules we consider in this paper are 
not necessary strict logical inference rules, but plausible in-
ference rules; see Section 3. 
567
4.1 Inference Rules 
Our models for learning inferential selectional 
preferences can be applied to any collection of in-
ference rules between binary semantic relations. In 
this paper, we focus on the inference rules con-
tained in the DIRT resource (Lin and Pantel 2001). 
DIRT consists of over 12 million rules which were 
extracted from a 1GB newspaper corpus (San Jose 
Mercury, Wall Street Journal and AP Newswire 
from the TREC-9 collection). For example, here 
are DIRT?s top 3 inference rules for ?X solves Y?: 
 ?Y is solved by X?, ?X resolves Y?, ?X finds a solution to Y? 
4.2 Semantic Classes 
The choice of semantic classes is of great impor-
tance for selectional preference. One important 
aspect is the granularity of the classes. Too general 
a class will provide no discriminatory power while 
too fine-grained a class will offer little generaliza-
tion and apply in only extremely few cases. 
The absence of an attested high-quality set of 
semantic classes for this task makes discovering 
preferences difficult. Since many of the criteria for 
developing such a set are not even known, we de-
cided to experiment with two very different sets of 
semantic classes, in the hope that in addition to 
learning semantic preferences, we might also un-
cover some clues for the eventual decisions about 
what makes good semantic classes in general. 
Our first set of semantic classes was directly ex-
tracted from the output of the CBC clustering algo-
rithm (Pantel and Lin 2002). We applied CBC to 
the TREC-9 and TREC-2002 (Aquaint) newswire 
collections consisting of over 600 million words. 
CBC generated 1628 noun concepts and these were 
used as our semantic classes for SPs. 
Secondly, we extracted semantic classes from 
WordNet 2.1 (Fellbaum 1998). In the absence of 
any externally motivated distinguishing features 
(for example, the Basic Level categories from Pro-
totype Theory, developed by Eleanor Rosch 
(1978)), we used the simple but effective method 
of manually truncating the noun synset hierarchy5 
and considering all synsets below each cut point as 
part of the semantic class at that node. To select 
the cut points, we inspected several different hier-
archy levels and found the synsets at a depth of 4 
                                                     
5 Only nouns are considered since DIRT semantic relations 
connect only nouns. 
to form the most natural semantic classes. Since 
the noun hierarchy in WordNet has an average 
depth of 12, our truncation created a set of con-
cepts considerably coarser-grained than WordNet 
itself. The cut produced 1287 semantic classes, a 
number similar to the classes in CBC. To properly 
test WordNet as a source of semantic classes for 
our selectional preferences, we would need to ex-
periment with different extraction algorithms. 
4.3 Evaluation Criteria 
The goal of the filtering task is to minimize false 
positives (incorrectly accepted inferences) and 
false negatives (incorrectly rejected inferences). A 
standard methodology for evaluating such tasks is 
to compare system filtering results with a gold 
standard using a confusion matrix. A confusion 
matrix captures the filtering performance on both 
correct and incorrect inferences: 
  
where A represents the number of correct instances 
correctly identified by the system, D represents the 
number of incorrect instances correctly identified 
by the system, B represents the number of false 
positives and C represents the number of false 
negatives. To compare systems, three key meas-
ures are used to summarize confusion matrices: 
? Sensitivity, defined as CA
A
+ , captures a filter?s 
probability of accepting correct inferences; 
? Specificity, defined as DB
D
+ , captures a filter?s 
probability of rejecting incorrect inferences; 
? Accuracy, defined as DCBA
DA
+++
+ , captures the 
probability of a filter being correct. 
5 Experimental Results 
In this section, we provide empirical evidence to 
support the main claim of this paper. 
Given a collection of DIRT inference rules of 
the form pi ? pj, our experiments, using the meth-
odology of Section 4, evaluate the capability of our 
ISP models for determining if ?x, pj, y? holds given 
that ?x, pi, y? holds. 
GOLD STANDARD   
1 0 
1 A B 
SY
ST
E
M
 
0 C D 
568
5.1 Experimental Setup 
Model Implementation 
For each filtering algorithm in Section 3.3, ISP.JIM, 
ISP.IIM.?, and ISP.IIM.?, we trained their probabil-
istic models using corpus statistics extracted from 
the 1999 AP newswire collection (part of the 
TREC-2002 Aquaint collection) consisting of ap-
proximately 31 million words. We used the Mini-
par parser (Lin 1993) to match DIRT patterns in 
the text. This permits exact matches since DIRT 
inference rules are built from Minipar parse trees. 
For each system, we experimented with the dif-
ferent ways of combining relational SP scores: 
minimum, maximum, and average (see Section 
3.2). Also, we experimented with various values 
for the ? parameter described in Section 3.3. 
Gold Standard Construction 
In order to compute the confusion matrices de-
scribed in Section 4.3, we must first construct a 
representative set of inferences and manually anno-
tate them as correct or incorrect. 
We randomly selected 100 inference rules of the 
form pi ? pj from DIRT. For each pattern pi, we 
then extracted its instances from the Aquaint 1999 
AP newswire collection (approximately 22 million 
words), and randomly selected 10 distinct in-
stances, resulting in a total of 1000 instances. For 
each instance of pi, applying DIRT?s inference rule 
would assert the instance ?x, pj, y?. Our evaluation 
tests how well our models can filter these so that 
only correct inferences are made. 
To form the gold standard, two human judges 
were asked to tag each instance ?x, pj, y? as correct 
or incorrect. For example, given a randomly se-
lected inference rule ?X is charged by Y ? Y an-
nounced the arrest of X? and the instance ?Terry 
Nichols was charged by federal prosecutors?, the 
judges must determine if the instance ?federal 
prosecutors, Y announced the arrest of X, Terry 
Nichols? is correct. The judges were asked to con-
sider the following two criteria for their decision: 
? ?x, pj, y? is a semantically meaningful instance; 
? The inference pi ? pj holds for this instance. 
Judges found that annotation decisions can range 
from trivial to difficult. The differences often were 
in the instances for which one of the judges fails to 
see the right context under which the inference 
could hold. To minimize disagreements, the judges 
went through an extensive round of training. 
To that end, the 1000 instances ?x, pj, y? were 
split into DEV and TEST sets, 500 in each. The 
two judges trained themselves by annotating DEV 
together. The TEST set was then annotated sepa-
rately to verify the inter-annotator agreement and 
to verify whether the task is well-defined. The 
kappa statistic (Siegel and Castellan Jr. 1988) was 
? = 0.72. For the 70 disagreements between the 
judges, a third judge acted as an adjudicator. 
Baselines 
We compare our ISP algorithms to the following 
baselines: 
? B0: Rejects all inferences; 
? B1: Accepts all inferences; 
? Rand: Randomly accepts or rejects inferences. 
One alternative to our approach is admit instances 
on the Web using literal search queries. We inves-
tigated this technique but discarded it due to subtle 
yet critical issues with pattern canonicalization that 
resulted in rejecting nearly all inferences. How-
ever, we are investigating other ways of using Web 
corpora for this task. 
Table 1. Filtering quality of best performing systems according to the evaluation criteria defined in Section 4.3 on 
the TEST set ? the reported systems were selected based on the Accuracy criterion on the DEV set. 
PARAMETERS SELECTED FROM DEV SET 
SYSTEM 
RANKING STRATEGY ? (%) 
SENSITIVITY 
(95% CONF) 
SPECIFICITY 
(95% CONF) 
ACCURACY 
(95% CONF) 
B0 - - 0.00?0.00 1.00?0.00 0.50?0.04 
B1 - - 1.00?0.00 0.00?0.00 0.49?0.04 
Random - - 0.50?0.06 0.47?0.07 0.50?0.04 
ISP.JIM maximum 100 0.17?0.04 0.88?0.04 0.53?0.04 
ISP.IIM.? maximum 100 0.24?0.05 0.84?0.04 0.54?0.04 CBC 
ISP.IIM.? maximum 90 0.73?0.05 0.45?0.06 0.59?0.04? 
ISP.JIM minimum 40 0.20?0.06 0.75?0.06 0.47?0.04 
ISP.IIM.? minimum 10 0.33?0.07 0.77?0.06 0.55?0.04 WordNet 
ISP.IIM.? minimum 20 0.87?0.04 0.17?0.05 0.51?0.05 
? Indicates statistically significant results (with 95% confidence) when compared with all baseline systems using pairwise t-test. 
569
5.2 Filtering Quality 
For each ISP algorithm and parameter combina-
tion, we constructed a confusion matrix on the de-
velopment set and computed the system sensitivity, 
specificity and accuracy as described in Section 
4.3. This resulted in 180 experiments on the devel-
opment set. For each ISP algorithm and semantic 
class source, we selected the best parameter com-
binations according to the following criteria: 
? Accuracy: This system has the best overall abil-
ity to correctly accept and reject inferences. 
? 90%-Specificity: Several formal semantics and 
textual entailment researchers have commented 
that inference rule collections like DIRT are dif-
ficult to use due to low precision. Many have 
asked for filtered versions that remove incorrect 
inferences even at the cost of removing correct 
inferences. In response, we show results for the 
system achieving the best sensitivity while main-
taining at least 90% specificity on the DEV set. 
We evaluated the selected systems on the TEST 
set. Table 1 summarizes the quality of the systems 
selected according to the Accuracy criterion. The 
best performing system, ISP.IIM.?, performed  sta-
tistically significantly better than all three base-
lines. The best system according to the 90%-
Specificity criteria was ISP.JIM, which coinciden-
tally has the highest accuracy for that model as 
shown in Table 16. This result is very promising 
for researchers that require highly accurate infer-
ence rules since they can use ISP.JIM and expect to 
recall 17% of the correct inferences by only ac-
cepting false positives 12% of the time. 
Performance and Error Analysis 
Figures 1a) and 1b) present the full confusion ma-
trices for the most accurate and highly specific sys-
tems, with both systems selected on the DEV set. 
The most accurate system was ISP.IIM.?, which is 
the most permissive of the algorithms. This sug-
                                                     
6 The reported sensitivity of ISP.Joint in Table 1 is below 
90%, however it achieved 90.7% on the DEV set. 
gests that a larger corpus for learning SPs may be 
needed to support stronger performance on the 
more restrictive methods. The system in Figure 
1b), selected for maximizing sensitivity while 
maintaining high specificity, was 70% correct in 
predicting correct inferences. 
Figure 2 illustrates the ROC curve for all our 
systems and parameter combinations on the TEST 
set. ROC curves plot the true positive rate against 
the false positive rate. The near-diagonal line plots 
the three baseline systems. 
Several trends can be observed from this figure. 
First, systems using the semantic classes from 
WordNet tend to perform less well than systems 
using CBC classes. As discussed in Section 4.2, we 
used a very simplistic extraction of semantic 
classes from WordNet. The results in Figure 2 
serve as a lower bound on what could be achieved 
with a better extraction from WordNet. Upon in-
spection of instances that WordNet got incorrect 
but CBC got correct, it seemed that CBC had a 
much higher lexical coverage than WordNet. For 
example, several of the instances contained proper 
names as either the X or Y argument (WordNet has 
poor proper name coverage). When an argument is 
not covered by any class, the inference is rejected. 
Figure 2 also illustrates how our three different 
ISP algorithms behave. The strictest filters, ISP.JIM 
and ISP.IIM.?, have the poorest overall perform-
ance but, as expected, have a generally very low 
rate of false positives. ISP.IIM.?, which is a much 
more permissive filter because it does not require 
ROC on the TEST Set
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
1-Specificity
S
en
si
tiv
ity
Baselines WordNet CBC ISP.JIM ISP.IIM.AND ISP.IIM.OR
Figure 2. ROC curves for our systems on TEST. 
GOLD STANDARD a)  
1 0 
1 184 139 
SY
ST
E
M
 
0 63 114 
GOLD STANDARD b)  
1 0 
1 42 28 
SY
ST
E
M
 
0 205 225 
Figure 1. Confusion matrices for a) ISP.IIM.? ? best 
Accuracy; and b) ISP.JIM ? best 90%-Specificity.
570
both arguments of a relation to match, has gener-
ally many more false positives but has an overall 
better performance. 
We did not include in Figure 2 an analysis of the 
minimum, maximum, and average ranking strate-
gies presented in Section 3.2 since they generally 
produced nearly identical results. 
For the most accurate system, ISP.IIM.?, we ex-
plored the impact of the cutoff threshold ? on the 
sensitivity, specificity, and accuracy, as shown in 
Figure 3. Rather than step the values by 10% as we 
did on the DEV set, here we stepped the threshold 
value by 2% on the TEST set. The more permis-
sive values of ? increase sensitivity at the expense 
of specificity. Interestingly, the overall accuracy 
remained fairly constant across the entire range of 
?, staying within 0.05 of the maximum of 0.62 
achieved at ?=30%. 
Finally, we manually inspected several incorrect 
inferences that were missed by our filters. A com-
mon source of errors was due to the many incorrect 
?antonymy? inference rules generated by DIRT, 
such as ?X is rejected in Y???X is accepted in Y?. 
This recognized problem in DIRT occurs because 
of the distributional hypothesis assumption used to 
form the inference rules. Our ISP algorithms suffer 
from a similar quandary since, typically, antony-
mous relations take the same sets of arguments for 
X (and Y). For these cases, ISP algorithms learn 
many selectional preferences that accept the same 
types of entities as those that made DIRT learn the 
inference rule in the first place, hence ISP will not 
filter out many incorrect inferences. 
6 Conclusion 
We presented algorithms for learning what we call 
inferential selectional preferences, and presented 
evidence that learning selectional preferences can 
be useful in filtering out incorrect inferences. Fu-
ture work in this direction includes further explora-
tion of the appropriate inventory of semantic 
classes used as SP?s. This work constitutes a step 
towards better understanding of the interaction of 
selectional preferences and inferences, bridging 
these two aspects of semantics. 
References 
Barzilay, R.; and McKeown, K.R. 2001.Extracting Paraphrases from a 
Parallel Corpus. In Proceedings of ACL 2001. pp. 50?57. Toulose, 
France. 
Baker, C.F.; Fillmore, C.J.; and Lowe, J.B. 1998. The Berkeley 
FrameNet Project. In Proceedings of COLING/ACL 1998.  pp. 86-
90. Montreal, Canada. 
Cover, T.M. and Thomas, J.A. 1991. Elements of Information Theory. 
John Wiley & Sons. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical Database. MIT 
Press. 
Harabagiu, S.; and Hickl, A. 2006. Methods for Using Textual 
Entailment in Open-Domain Question Answering. In Proceedings 
of ACL 2006.  pp. 905-912. Sydney, Australia. 
Katz, J.; and Fodor, J.A. 1963. The Structure of a Semantic Theory. 
Language, vol 39. pp.170?210.  
Lenat, D. 1995. CYC: A large-scale investment in knowledge 
infrastructure. Communications of the ACM, 38(11):33?38. 
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary 
Investigation. University of Chicago Press, Chicago, IL. 
Light, M. and Greiff, W.R. 2002. Statistical Models for the Induction 
and Use of Selectional Preferences. Cognitive Science,26:269?281. 
Lin, D. 1993. Parsing Without OverGeneration. In Proceedings of  
ACL-93. pp. 112-120. Columbus, OH. 
Lin, D. and Pantel, P. 2001. Discovery of Inference Rules for 
Question Answering. Natural Language Engineering 7(4):343-360. 
Moldovan, D.I.; Clark, C.; Harabagiu, S.M.; Maiorano, S.J. 2003. 
COGEX: A Logic Prover for Question Answering. In Proceedings 
of HLT-NAACL-03. pp. 87-93. Edmonton, Canada. 
Pantel, P. and Lin, D. 2002. Discovering Word Senses from Text. In 
Proceedings of KDD-02. pp. 613-619. Edmonton, Canada. 
Resnik, P. 1996. Selectional Constraints: An Information-Theoretic 
Model and its Computational Realization. Cognition, 61:127?159. 
Romano, L.; Kouylekov, M.; Szpektor, I.; Dagan, I.; Lavelli, A. 2006. 
Investigating a Generic Paraphrase-Based Approach for Relation 
Extraction. In EACL-2006. pp. 409-416. Trento, Italy. 
Rosch, E. 1978. Human Categorization. In E. Rosch and B.B. Lloyd 
(eds.) Cognition and Categorization. Hillsdale, NJ: Erlbaum.  
Siegel, S. and Castellan Jr., N. J. 1988. Nonparametric Statistics for 
the Behavioral Sciences. McGraw-Hill. 
Szpektor, I.; Tanev, H.; Dagan, I.; and Coppola, B. 2004. Scaling 
web-based acquisition of entailment relations. In Proceedings of 
EMNLP 2004. pp. 41-48. Barcelona,Spain. 
Wilks, Y.; and Fass, D. 1992. Preference Semantics: a family history. 
Computing and Mathematics with Applications, 23(2). A shorter 
version in the second edition of the Encyclopedia of Artificial 
Intelligence, (ed.) S. Shapiro. 
Zanzotto, F.M.; Pennacchiotti, M.; Pazienza, M.T. 2006. Discovering 
Asymmetric Entailment Relations between Verbs using Selectional 
Preferences. In COLING/ACL-06. pp. 849-856. Sydney, Australia. 
Figure 3. ISP.IIM.? (Best System)?s performance 
variation over different values for the ? threshold. 
ISP.IIM.OR (Best System)'s Performance vs. Tau-Thresholds
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70 80 90 100
Tau-Thresholds
Sensitivity Specificity Accuracy
571
Proceedings of NAACL HLT 2007, Companion Volume, pages 217?220,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
A Semi-Automatic Evaluation Scheme: Automated Nuggetization for 
Manual Annotation 
Liang Zhou, Namhee Kwon, and Eduard Hovy 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{liangz, nkwon, hovy}@isi.edu 
 
Abstract 
In this paper we describe automatic in-
formation nuggetization and its applica-
tion to text comparison. More 
specifically, we take a close look at how 
machine-generated nuggets can be used to 
create evaluation material. A semi-
automatic annotation scheme is designed 
to produce gold-standard data with excep-
tionally high inter-human agreement.  
1 Introduction 
In many natural language processing (NLP) tasks, 
we are faced with the problem of determining the 
appropriate granularity level for information units. 
Most commonly, we use sentences to model indi-
vidual pieces of information. However, more NLP 
applications require us to define text units smaller 
than sentences, essentially decomposing sentences 
into a collection of phrases. Each phrase carries an 
independent piece of information that can be used 
as a standalone unit. These finer-grained informa-
tion units are usually referred to as nuggets.  
When performing within-sentence comparison 
for redundancy and/or relevancy judgments, with-
out a precise and consistent breakdown of nuggets 
we can only rely on rudimentary n-gram segmenta-
tions of sentences to form nuggets and perform 
subsequent n-gram-wise text comparison. This is 
not satisfactory for a variety of reasons. For exam-
ple, one n-gram window may contain several sepa-
rate pieces of information, while another of the 
same length may not contain even one complete 
piece of information.  
Previous work shows that humans can create 
nuggets in a relatively straightforward fashion. In 
the PYRAMID scheme for manual evaluation of 
summaries (Nenkova and Passonneau, 2004), ma-
chine-generated summaries were compared with 
human-written ones at the nugget level. However, 
automatic creation of the nuggets is not trivial. 
Hamly et al (2005) explore the enumeration and 
combination of all words in a sentence to create the 
set of all possible nuggets. Their automation proc-
ess still requires nuggets to be manually created a 
priori for reference summaries before any sum-
mary comparison takes place. This human in-
volvement allows a much smaller subset of phrase 
segments, resulting from word enumeration, to be 
matched in summary comparisons. Without the 
human-created nuggets, text comparison falls back 
to its dependency on n-grams. Similarly, in ques-
tion-answering (QA) evaluations, gold-standard 
answers use manually created nuggets and com-
pare them against system-produced answers bro-
ken down into n-gram pieces, as shown in 
POURPRE (Lin and Demner-Fushman, 2005) and 
NUGGETEER (Marton and Radul, 2006).  
A serious problem in manual nugget creation is 
the inconsistency in human decisions (Lin and 
Hovy, 2003). The same nugget will not be marked 
consistently with the same words when sentences 
containing multiple instances of it are presented to 
human annotators. And if the annotation is per-
formed over an extended period of time, the con-
sistency is even lower. In recent exercises of the 
PYRAMID evaluation, inconsistent nuggets are 
flagged by a tracking program and returned back to 
the annotators, and resolved manually.  
Given these issues, we address two questions in 
this paper: First, how do we define nuggets so that 
they are consistent in definition? Secondly, how do 
217
we utilize automatically extracted nuggets for vari-
ous evaluation purposes?  
2  Nugget Definition  
 
Based on our manual analysis and computational 
modeling of nuggets, we define them as follows:  
Definition:  
? A nugget is predicated on either an event  or 
an entity .  
? Each nugget consists of two parts: the an-
chor and the content.  
The anchor is either:  
? the head noun of the entity, or 
? the head verb of the event, plus the head 
noun of its associated entity (if more than 
one entity is attached to the verb, then its 
subject).  
The content is a coherent single piece of infor-
mation associated with the anchor. Each anchor 
may have several separate contents.  
When a nugget contains nested sentences, this 
definition is applied recursively. Figure 1 shows an 
example. Anchors are marked with square brack-
ets. If the anchor is a verb, then its entity attach-
ment is marked with curly brackets. If the sentence 
in question is a compound and/or complex sen-
tence, then this definition is applied recursively to 
allow decomposition. For example, in Figure 1, 
without recursive decomposition, only two nuggets 
are formed: 1) ?[girl] working at the bookstore in 
Hollywood?, and 2) ?{girl} [talked] to the diplo-
mat living in Britain?. In this example, recursive 
decomposition produces nuggets with labels 1-a, 1-
b, 2-a, and 2-b.  
2.1  Nugget Extraction 
We use syntactic parse trees produced by the 
Collins parser (Collins, 1999) to obtain the struc-
tural representation of sentences. Nuggets are ex-
tracted by identifying subtrees that are descriptions 
for entities and events. For entity nuggets, we ex-
amine subtrees headed by ?NP?; for event nuggets, 
subtrees headed by ?VP? are examined and their 
corresponding subjects (siblings headed by ?NP?) 
are treated as entity attachments for the verb 
phrases.  
3  Utilizing Nuggets in Evaluations 
In recent QA and summarization evaluation exer-
cises, manually created nuggets play a determinate 
role in judging system qualities. Although the two 
task evaluations are similar, the text comparison 
task in summarization evaluation is more complex 
because systems are required to produce long re-
sponses and thus it is hard to yield high agreement 
if manual annotations are performed. The follow-
ing experiments are conducted in the realm of 
summarization evaluation.  
3.1  Manually Created Nuggets 
During the recent two Document Understanding 
Confereces (DUC-05 and DUC-06) (NIST, 2002?
2007), the PYRAMID framework (Nenkova and 
Passonneau, 2004) was used for manual summary 
evaluations. In this framework, human annotators 
select and highlight portions of reference summa-
ries to form a pyramid of summary content units 
(SCUs) for each docset. A pyramid is constructed 
from SCUs and their corresponding popularity 
scores?the number of reference summaries they 
appeared in individually. SCUs carrying the same 
information do not necessarily have the same sur-
face-level words. Annotators need to make the de-
cisions based on semantic equivalence among 
Figure 1. Nugget definition examples.  
Sentence:  
The girl working at the bookstore in Hollywood 
talked to the diplomat living in Britain.  
 
Nuggets are: 
1)  [girl] working at the bookstore in Holly-
wood 
a. [girl] working at the bookstore  
b. [bookstore] in Hollywood 
2) {girl} [talked] to the diplomat living in 
Britain 
a. {girl} [talked] to the diplomat 
b. [diplomat] living in Britian 
 
Anchors: 
1)  [girl] 
a. [girl] 
b. [bookstore] 
2) {girl} [talked]: talked is the anchor verb 
and girl is its entity attachment.  
a. {girl} [talked] 
b. [diplomat]  
218
various SCUs. To evaluate a peer summary from a 
particular docset, annotators highlight portions of 
text in the peer summary that convey the same in-
formation as those SCUs in previously constructed 
pyramids.  
3. 2  Automatically Created Nuggets 
We envisage the nuggetization process being 
automated and nugget comparison and aggregation 
being performed by humans. It is crucial to involve 
humans in the evaluation process because recog-
nizing semantically equivalent units is not a trivial 
task computationally. In addition, since nuggets are 
system-produced and can be imperfect, annotators 
are allowed to reject and re-create them. We per-
form record-keeping in the background on which 
nugget or nugget groups are edited so that further 
improvements can be made for nuggetization.  
The evaluation scheme is designed as follows: 
 
For reference summaries  (per docset):  
? Nuggets are created for all sentences;  
? Annotators will group equivalent nuggets.  
? Popularity scores are automatically assigned 
to nugget groups.  
For peer summaries :  
? Nuggets are created for all sentences;  
? Annotators will match/align peer?s nuggets 
with reference nugget groups.  
? Recall scores are to be computed.  
3. 3  Consistency in Human Involvement 
The process of creating nuggets has been auto-
mated and we can assume a certain level of consis-
tency based on the usage of the syntactic parser. 
However, a more important issue emerges. When 
given the same set of nuggets, would human anno-
tators agree on nugget group selections and their 
corresponding contributing nuggets? What levels 
of agreement and disagreement should be ex-
pected? Two annotators, one familiar with the no-
tion of nuggetization (C1) and one not (C2), 
participated in the following experiments.  
Figure 2 shows the annotation procedure for 
reference summaries. After two rounds of individ-
ual annotations and consolidations and one final 
round of conflict resolution, a set of gold-standard 
nugget groups is created for each docset and will 
be subsequently used in peer summary annotations. 
The first round of annotation is needed since one 
of the annotators, C2, is not familiar with nuggeti-
zation. After the initial introduction of the task, 
concerns and questions arisen can be addressed. 
Then the annotators proceed to the second round of 
annotation. Naturally, some differences and con-
flicts remain. Annotators must resolve these prob-
lems during the final round of conflict resolution 
and create the agreed-upon gold-standard data.   
Previous manual nugget annotation has used one 
annotator as the primary nugget creator and an-
other annotator as an inspector (Nenkova and Pas-
sonneau, 2004). In our annotation experiment, we 
encourage both annotators to play equally active 
roles. Conflicts between annotators resulting from 
ideology, comprehension, and interpretation differ-
ences helped us to understand that complete 
agreement between annotators is not realistic and 
not achievable, unless one annotator is dominant 
over the other. We should expect a 5-10% annota-
tion variation.  
In Figure 3, we show annotation comparisons 
from first to second round. The x -axis shows the 
nugget groups that C1 and C2 have agreed on. The 
y -axis shows the popularity score a particular nug-
get group received. Selecting from three reference 
summaries, a score of three for a nugget group in-
dicates it was created from nuggets in all three 
 
Figure 2. Reference annotation and gold-standard 
data creation.  
219
summaries. The first round initially appears suc-
cessful because the two annotators had 100% 
agreement on nugget groups and their correspond-
ing scores. However, C2, the novice nuggetizer, 
was much more conservative than C1, because 
only 10 nugget groups were created. The geometric 
mean of agreement on all nugget group assignment 
is merely 0.4786. During the second round, differ-
ences in group-score allocations emerge, 0.9192, 
because C2 is creating more nugget groups. The 
geometric mean of agreement on all nugget group 
assignment has been improved to 0.7465.  
After the final round of conflict resolution, 
gold-standard data was created. Since all conflicts 
must be resolved, annotators have to either con-
vince or be convinced by the other. How much 
change is there between an annotator?s second-
round annotation and the gold-standard? Geomet-
ric mean of agreement on all nugget group assign-
ment for C1 is 0.7543 and for C2 is 0.8099. 
Agreement on nugget group score allocation for 
C1 is 0.9681 and for C2 is 0.9333. From these fig-
ures, we see that while C2 contributed more to the 
gold-standard?s nugget group creations, C1 had 
more accuracy in finding the correct number of 
nugget occurrences in reference summaries. This 
confirms that both annotators played an active role. 
Using the gold-standard nugget groups, the annota-
tors performed 4 peer summary annotations. The 
agreement among peer summary annotations is 
quite high, at approximately 0.95. Among the four, 
annotations on one peer summary from the two 
annotators are completely identical.  
4  Conclusion 
In this paper we have given a concrete definition 
for information nuggets and provided a systematic 
implementation of them. Our main goal is to use 
these machine-generated nuggets in a semi-
automatic evaluation environment for various NLP 
applications. We took a close look at how this can 
be accomplished for summary evaluation, using 
nuggets created from reference summaries to grade 
peer summaries. Inter-annotator agreements are 
measured to insure the quality of the gold-standard 
data created. And the agreements are very high by 
following a meticulous procedure. We are cur-
rently preparing to deploy our design into full-
scale evaluation exercises.  
References 
Collins, M. 1999. Head-driven statistical models for 
natural language processing. Ph D Dissertation , Uni-
versity of Pennsylvania.  
Hamly, A., A. Nenkova, R. Passonneau, and O. Ram-
bow. 2005. Automation of summary evaluation by 
the pyramid method. In Proceedings of RANLP.  
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. In 
Proceedings of NAACL-HLT. 
Lin, J. and D. Demner-Fushman. 2005. Automatically 
evaluating answers to definition questions. In Pro-
ceedings of HLT-EMNLP.  
Marton, G. and A. Radul. 2006. Nuggeteer: automatic 
nugget-based evaluation using description and judg-
ments. In Proceedings NAACL-HLT.  
Nenkova, A. and R. Passonneau. 2004. Evaluating con-
tent selection in summarization: the pyramid method. 
In Proceedings NAACL-HLT.  
NIST. 2001?2007. Document Understanding Confer-
ence. www-nlpir.nist.gov/projects/duc/index.html.  
 
 
Figure 3. Annotation comparisons from 1 st to 
2nd round.  
220
 Learning Surface Text Patterns 
for a Question Answering System 
Deepak Ravichandran and Eduard Hovy  
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
USA 
{ravichan,hovy}@isi.edu  
 
 
Abstract 
In this paper we explore the power of 
surface text patterns for open-d main 
question answering systems.  In order to 
obtain an optimal set of patterns, we have 
developed a method for learning such 
patterns automatically. A tagged corpus 
is built from the Internet in a 
bootstrapping process by providing a few 
hand-crafted examples of each question 
type to Altavista. Patterns are then 
automatically extracted from the returned 
documents and standardized. We 
calculate the precision of each pattern, 
and the average precision for each 
question type. These patterns are then 
applied to find answers to new questions.  
Using the TREC-10 question set, we 
report results for two cases: answers 
determined from the TREC-10 corpus 
and from the web. 
 
1 Introduction 
Most of the recent open domain question-
answering systems use external knowledge 
and tools for answer pinpointing. These may 
include named entity taggers, WordNet, 
parsers, hand-tagged corpora, and ontology 
lists (Srihari and Li, 00; Harabagiu et al, 01; 
Hovy et al, 01; Prager et al, 01). However, at 
the recent TREC-10 QA evaluation 
(Voorhees, 01), the winning system used just 
one resource: a fairly extensive list of surface 
patterns (Soubbotin and Soubbotin, 01). The 
apparent power of such patterns surprised 
many. We therefore decided to investigate 
their potential by acquiring patterns 
automatically and to measure their accuracy. 
It has been noted in several QA systems 
that certain types of answer are expressed 
using characteristic phrases (Lee et al, 01; 
Wang et al, 01). For example, for 
BIRTHDATEs (with questions like ?When 
was X born??), typical answers are  
?Mozart was born in 1756.? 
?Gandhi (1869? 1948)??  
These examples suggest that phrases like  
?<NAME> was born in <BIRTHDATE>? 
?<NAME> (<BIRTHDATE>? ?  
when formulated as regular expressions, can 
be used to locate the correct answer.  
In this paper we present an approach for 
automatically learning such regular 
expressions (along with determining their 
precision) from the web, for given types of 
questions.  Our method uses the machine 
learning technique of bootstrapping to build a 
large tagged corpus starting with only a few 
examples of QA pairs.  Similar techniques 
have been investigated extensively in the field 
of information extraction (Riloff, 96).  These 
techniques are greatly aided by the fact that 
there is no need to hand-tag a corpus, while 
the abundance of data on the web makes it 
easier to determine reliable statistical 
estimates. 
Our system assumes each sentence to be a 
simple sequence of words and searche  for 
repeated word orderings as evidence for 
                  Computational Linguistics (ACL), Philadelphia, July 2002, pp. 41-47.
                         Proceedings of the 40th Annual Meeting of the Association for
 useful answer phrases.  We use suffix trees 
for extracting substrings of optimal length.  
We borrow the idea of suffix trees from 
computational biology (Gusfield, 97) where it 
is primarily used for detecting DNA 
sequences. Suffix trees can be processed in 
time linear on the size of the corpus and, more 
importantly, they do not restrict the length of 
substrings.  We then test the patterns learned 
by our system on new unseen questions from 
the TREC-10 set and evaluate their results to 
determine the precision of the patterns. 
 
2 Learning of Patterns 
We describe the pattern-lear ing algorithm 
with an example.  A table of patterns is 
constructed for each individual question type 
by the following procedure (Algorithm 1).  
1. Select an example for a given question 
type. Thus for BIRTHYEAR questions we 
select ?Mozart 1756? (we refer to 
?Mozart? as the question term and ?1756? 
as the answer term). 
2. Submit the question and the answer term 
as queries to a search engine.  Thus, we 
give the query +?Mozart? +?1756? to 
AltaVista (http://www.altavista.com). 
3. Download the top 1000 web documents 
provided by the search engine. 
4. Apply a sentence breaker to the 
documents. 
5. Retain only those sentences that contain 
both the question and the answer term.  
Tokenize the input text, smooth variations 
in white space characters, and remove html 
and other extraneous tags, to allow simple 
regular expression matching tools such as 
egrep to be used. 
6. Pass each retained sentence through a 
suffix tree construc or.  This finds all 
substrings, of all lengths, along with their 
counts. For example consider the  
sentences ?The great composer Mozart 
(1756? 1791) achieved fame at a young 
age? ?Mozart (1756? 1791) was a genius?, 
and ?The whole world would always be 
indebted to the great music of Mozart 
(1756? 1791)?. The longest matching 
substring for all 3 sentences is ?Mozart 
(1756? 1791)?, which the suffix tree would 
extract as one of the outputs along with the 
score of 3. 
7. Pass each phrase in the suffix tree through 
a filter to retain only those phrases that 
co tain both the question and the answer 
term. For the example, we extract only 
those phrases from the suffix tree that 
contain the words ?Mozart? and ?1756?.  
8. Replace the word for the question term by 
the tag ?<NAME>? and the word for the 
answer term by the term ?<ANSWER>?.   
 
This procedure is repeated for different 
examples of the same question type.  For 
BIRTHDATE we also use ?Gandhi 1869?, 
?Newton 1642?, etc. 
For BIRTHDATE, the above steps 
produce the following output: 
a. born in <ANSWER> , <NAME>   
b. <NAME> was born on <ANSWER> ,  
c. <NAME> ( <ANSWER> -   
d. <NAME> ( <ANSWER -  )  
...  
These are some of the most common 
substrings of the extracted sentences that 
contain both <NAME> and <ANSWER>.  
Since the suffix tree records all substrings, 
partly overlapping strings such as c and d are 
separately saved, which allows us to obtain 
separate counts of their occurrence 
fr quencies.  As will be seen later, this allows 
us to differentiate patterns such as d (which 
records a still living person, and is quite 
precise) from its more general substring c 
(which is less precise).   
 
Algorithm 2: Calculating the precision of each 
pattern. 
1. Query the search engine by using only the 
question term (in the example, only 
?Mozart?). 
2. Download the top 1000 web documents 
provided by the search engine. 
3. As before, segment these documents into 
individual sentences. 
4. Retain only those sentences that contain 
the question term. 
5. For each pattern obtained from Algorithm 
1, check the presence of each pattern in the 
 sentence obtained from above for two 
instances: 
i) Presence of the pattern with 
<ANSWER> tag matched by any 
word. 
ii) Presence of the pattern in the sentence 
with <ANSWER> tag matched by the 
correct answer term. 
In our example, for the pattern ?<NAME> 
was born in <ANSWER>? we check the 
presence of the following strings in the 
answer sentence 
i) Mozart was born in <ANY_WORD> 
ii) Mozart was born in 1756 
Calculate the precision of each pattern by 
the formula P = Ca / o where  
Ca = total number of patterns with the 
answer term present  
Co = total number of patterns present 
with answer term replaced by any word 
6. Retain only the patterns matching a 
sufficient number of examples (we choose 
the number of examples > 5). 
 
We obtain a table of regular expression 
patterns for a given question type, along with 
the precision of each pattern.  This precision 
is the probability of each pattern containing 
the answer and follows directly from the 
principle of maximum likelihood estimation. 
For BIRTHDATE the following table is 
obtained: 
1.0  <NAME>( <ANSWER> -  )  
0.85  <NAME> was born on <ANSWER>, 
0.6  <NAME> was born in <ANSWER> 
0.59  <NAME> was born <ANSWER>  
0.53  <ANSWER> <NAME> was born 
0.50  ?  <NAME> ( <ANSWER> 
0.36 <NAME> ( <ANSWER> -  
For a given question type a good range of 
patterns was obtained by giving the system as 
few as 10 examples.  The rather long list of 
patterns obtained would have been very 
difficult for any human to come up with 
manually.   
The question term could appear in the 
documents obtained from the web in various
ways.  Thus ?Mozart? could be written as 
?Wolfgang Amadeus Mozart?, ?Mozart, 
Wolfgang Amadeus?, ?Amadeus Mozart? or 
?Mozart?.  To learn from such variations, in 
step 1 of Algorithm 1 we specify the various 
ways in which the question term could be 
specified in the text.  The presence of any of 
these names would cause it to be tagged as the 
original question term ?Mozart?.  
The same arrangement is also done for the 
answer term so that presence of any variant of 
the answer term would cause it to be treated 
exactly like the original answer term.  While 
easy to do for BIRTHDATE, this step can be 
problematic for question types such as 
DEFINITION, which may contain various 
acceptable answers.  In general the input 
example terms have to be carefully selected 
so that the questions they represent do not 
have a long list of possible answers, as this 
would affect the confidence of the precision 
scores for each pattern.  All the answers need 
to be enlisted to ensure a high confidence in 
the precision score of each pttern, in the 
present framework.   
 The precision of the patterns obtained 
from one QA-pair example in algorithm 1 is 
calculated from the documents obtained in 
algorithm 2 for other examples of the same 
question type.  In other words, the precision 
scores are calculated by cross-checking the 
patterns across various examples of the same 
typ .  This step proves to be very significant 
as it helps to eliminate dubious patterns, 
which may appear because the contents of 
two or more websites may be the same, or the 
same web document reappears in the search 
engine output for algorithms 1 and 2. 
Algorithm 1 does not explicitly specify 
any particular question type.  Judicious choice 
of the QA example pair therefore allows it to 
be used for many question types without
change.    
 
3 Finding Answers 
Using the patterns to answer a new question 
we employ the following algorithm:  
1. Determine the question type of the new 
question.  We use our existing QA system 
(Hovy et al, 2002b; 2001) to do so.   
2. The question term in the question is 
identified, also using our existing system.
 3. Create a query from the question term and 
perform IR (by using a given answer 
document corpus such as the TREC-10 
collection or web search otherwise).   
4. Segment the documents obtained into 
sentences and smooth out white space 
variations and html and other tags, as 
before. 
5. Replace the question term in each sentence 
by the question tag (?<NAME>?, in the 
case of BIRTHYEAR).  
6. Using the pattern table developed for that 
particular question type, search for the 
presence of each pattern.  Select words 
matching the tag ?<ANSWER>? as the 
answer. 
7. Sort these answers by their pattern?s 
precision scores.  Discard duplicates (by 
elementary string comparisons).  Return 
the top 5 answers. 
 
4 Experiments 
From our Webclopedia QA Typology 
(Hovy et al, 2002a) we selected 6 different 
question types: BIRTHDATE, LOCATION, 
INVENTOR, DISCOVERER, DEFINITION, 
WHY-FAMOUS.  The pattern table for each 
of these question types was constructed using 
Algorithm 1.  
Some of the patterns obtained long with 
their precision are as follows 
 
BIRTHYEAR  
1.0 <NAME> ( <ANSWER> - ) 
0.85 <NAME> was born on <ANSWER> , 
0.6 <NAME> was born in <ANSWER> 
0.59 <NAME> was born <ANSWER> 
0.53 <ANSWER> <NAME> was born 
0.5 - <NAME> ( <ANSWER> 
0.36 <NAME> ( <ANSWER> - 
0.32 <NAME> ( <ANSWER> ) , 
0.28 born in <ANSWER> , <NAME> 
0.2 of <NAME> ( <ANSWER> 
 
INVENTOR 
1.0 <ANSWER> invents <NAME> 
1.0 the <NAME> was invented by 
<ANSWER> 
1.0 <ANSWER> invented the <NAME> in 
1.0 <ANSWER> ' s invention of the 
<NAME> 
1.0 <ANSWER> invents the <NAME> . 
1.0 <ANSWER> ' s <NAME> was 
1.0 <NAME> , invented by <ANSWER> 
1.0 <ANSWER> ' s <NAME> and
1.0 that <ANSWER> ' s <NAME> 
1.0 <NAME> was invented by <ANSWER> , 
 
DISCOVERER 
1.0 when <ANSWER> discovered 
<NAME> 
1.0 <ANSWER> ' s discovery of <NAME> 
1.0 <ANSWER> , the discoverer of 
<NAME> 
1.0 <ANSWER> discovers <NAME> . 
1.0 <ANSWER> discover <NAME> 
1.0 <ANSWER> discovered <NAME> , the 
1.0 discovery of <NAME> by <ANSWER>. 
0.95 <NAME> was discovered by 
<ANSWER> 
0.91 of <ANSWER> ' s <NAME> 
0.9 <NAME> was discovered by 
<ANSWER> in 
 
DEFINITION 
1.0 <NAME> and related <ANSWER>s 
1.0 <ANSWER> ( <NAME> , 
1.0 <ANSWER> , <NAME> . 
1.0 , a <NAME> <ANSWER> , 
1.0 ( <NAME> <ANSWER> ) , 
1.0 form of <ANSWER> , <NAME> 
1.0 for <NAME> , <ANSWER> and 
1.0 cell <ANSWER> , <NAME> 
1.0 and <ANSWER> > <ANSWER> > 
<NAME> 
0.94 as <NAME> , <ANSWER> and
 
WHY-FAMOUS 
1.0 <ANSWER> <NAME> called 
1.0 laureate <ANSWER> <NAME> 
1.0 by the <ANSWER> , <NAME> , 
1.0 <NAME> - the <ANSWER> of 
1.0 <NAME> was the <ANSWER> of 
0.84 by the <ANSWER> <NAME> , 
0.8 the famous <ANSWER> <NAME> , 
0.73 the famous <ANSWER> <NAME> 
0.72 <ANSWER> > <NAME> 
0.71 <NAME> is the <ANSWER> of 
 
LOCATION 
1.0 <ANSWER> ' s <NAME> . 
 1.0 regional : <ANSWER> : <NAME> 
1.0 to <ANSWER> ' s <NAME> , 
1.0 <ANSWER> ' s <NAME> in 
1.0 in <ANSWER> ' s <NAME> , 
1.0 of <ANSWER> ' s <NAME> , 
1.0 at the <NAME> in <ANSWER> 
0.96 the <NAME> in <ANSWER> , 
0.92 from <ANSWER> ' s <NAME> 
0.92 near <NAME> in <ANSWER> 
 
For each question type, we extracted the 
corresponding questions from the TREC-10 
set.  These questions were run through the 
testing phase of the algorithm.  Two sets of 
experiments were performed.  In the first 
case, the TREC corpus was used as the input 
source and IR was performed by the IR 
component of our QA system (Lin, 2002).  In 
the second case, the web was the input source 
and the IR was performed by the AltaVista 
search engine.  
Results of the experiments, measured by 
Mean Reciprocal Rank (MRR) score 
(Voorhees, 01), are:  
 
TREC Corpus 
Question type Number of 
questions 
MRR on 
TREC docs 
BIRTHYEAR 8 0.48 
INVENTOR 6 0.17 
DISCOVERER 4 0.13 
DEFINITION 102 0.34 
WHY-FAMOUS 3 0.33 
LOCATION 16 0.75 
 
Web 
Question type Number of 
questions 
MRR on the 
Web 
BIRTHYEAR 8 0.69 
INVENTOR 6 0.58 
DISCOVERER 4 0.88 
DEFINITION 102 0.39 
WHY-FAMOUS 3 0.00 
LOCATION 16 0.86 
 
The results indicate that the system 
performs better on the Web data than on the 
TREC corpus.  The abundance of data on the 
web makes it easier for the system to locate 
answers with high precision scores (the 
system finds many examples of correct 
answers among the top 20 when using the 
Web as the input source).  A similar result for 
QA was obtained by Brill et al (2001).  The 
TREC corpus does not have enough candidate 
answers with high precision score and has to 
settle for answers extracted from sentences 
matched by low precision patterns.  The 
WHY-FAMOUS question type is an 
exception and may be due to the fact that the 
system was tested on a small number of 
questions.   
 
5 Shortcoming and Extensions 
No external knowledge has been added to 
these patterns.  We frequently observe the 
need for matching part of speech and/or 
semantic types, however.  For example, the 
question: ?Where are the Rocky Mountains 
located?? is answered by ?Denver?s new 
airport, topped with white fiberglass cones in 
imitation of the Rocky Mountains in the 
background, continues to lie empty?, because 
the system picked the answer ?the 
background? using the pattern ?the <NAME> 
in <ANSWER>,?. Using a named entity 
tagger and/or an ontology would enable the 
system to use the knowledge that 
?background? is not a location. 
DEFINITION questions pose a related 
problem.  Frequently the system?s patterns 
match a term that is too general, though 
correct technically.  For ?what is nepotism?? 
the pattern ?<ANSWER>, <NAME>? 
matches ??in the form of widespread 
bureaucratic abuses: graft, nepotism??; for 
?what is sonar?? the pattern ?<NAME> and 
related <ANSWER>s? matches ??while its 
sonar and related underseas systems are 
built??.  
The patterns cannot handle long-distance 
dependencies.  For example, for ?Where is 
London?? the system cannot locate the answer 
in ?London, which has one of the most busiest 
airports in the world, lies on the banks of the 
river Thames? due to the explosive danger of 
unrestricted wildcard matching, as would be 
required in the pattern ?<QUESTION>, 
(<any_word>)*, lies on <ANSWER>?.  This 
is one of the reasons why the system performs 
 very well on certain types of questions from 
the web but performs poorly with documents 
obtained from the TREC corpus.  The 
abundance and variation of data on the 
Internet alows the system to find an instance 
of its patterns without losing answers to long-
term dependencies.  The TREC corpus, on the 
other hand, typically contains fewer candidate 
answers for a given question and many of the 
answers present may match only long-term 
dependency patterns. 
More information needs to be added to the 
text patterns regarding the length of the 
answer phrase to be expected.  The system 
searches in the range of 50 bytes of the 
answer phrase to capture the pattern. It fails to 
perform under certain conditions as 
exemplified by the question ?When was 
Lyndon B. Johnson born??.  The system 
selects the sentence ?Tower gained national 
attention in 1960 when he lost to democratic 
Sen. Lyndon B. Johnson, who ran for both re-
election and the vice presidency? using the 
pattern ?<NAME> <ANSWER> ? ?.  The 
system lacks the information that the 
<ANSWER> tag should be replaced exactly 
by one word.  Simple extensions could be 
made to the system so that instead of 
searching in the range of 50 bytes for the 
answer phrase it could search for the answer 
in the range of 1? 2 chunks (basic phrases in 
English such as simple NP, VP, PP, etc.). 
A more serious limitat on is that the 
present framework can handle only one 
anchor point (the question term) in the 
candidate answer sentence.  It cannot work for 
types of question that require multiple words 
from the question to be in the answer 
sentence, possibly apart from each other.  For 
example, in ?Which county does the city of 
Long Beach lie??, the answer ?Long Beach is 
situated in Los Angeles County? requires the 
pattern. ?<QUESTION_TERM_1> situated in 
<ANSWER> <QUESTION_TERM_2>?, 
where <QUESTION_TERM_1> and 
<QUESTION_TERM_2> represent the terms 
?Long Beach? and ?county? respectively.  
The performance of the system depends 
significantly on there being only one anchor 
word, which allows a single word match 
between the question and the candidate 
answer sentence.  The presence of multiple 
anchor words would help to eliminate many 
of the candidate answers by simply using the 
condition that all the anchor words from the 
question must be present in the candidate 
answer sentence. 
The system does not classify or make any 
distinction between upper and lower case 
let ers.  For example, ?What is micron?? is 
answered by ?In Boise, Idaho, a spokesman 
for Micron, a maker of semiconductors, said 
Simms are ? a very high volume product for 
us ?? ?.  The answer returned by the system 
would have been perfect if the word ?micron? 
had been capitalized in the question. 
Canonicalization of words is also an issue.  
While giving examples in the bootstrapping 
procedure, say, for BIRTHDATE questions, 
the answer term could be written in many 
ways (for example, Gandhi?s birth date can be 
wr tten as ?1869?, ?Oct. 2, 1869?, ?2nd 
October 1869?,  ?October 2 1869?, and so 
on). Instead of enlisting all the possibilities a 
date tagger could be used to cluster all the 
variations and tag them with the same term.  
The same idea could also be extended for 
smoothing out the variations in the question 
term for names of persons (Gandhi could be 
written as ?Mahatma Gandhi?, ?Mohandas 
Karamchand Gandhi?, etc.). 
 
6 Conclusion 
The web results easily outperform the 
TREC results.  This suggests that there is a 
need to integrate the outputs of the Web and 
the TREC corpus.  Since the output from the 
Web contains many correct answers among 
the top ones, a simple word count could help 
in eliminating many unlikely answers.  This 
would work well for question types like 
BIRTHDATE or LOCATION but is not clear 
for question types like DEFINITION. 
The simplicity of this method makes it 
perfect for multilingual QA.  Many tools 
required by sophisticated QA systems (named 
entity taggers, parsers, ontologies, etc.) are 
language specific and require significant 
effort to adapt to a new language.  Since the 
answer patterns used in this method are 
 learned using only a small number of manual 
training terms, one can rapidly learn patterns 
for new languages, assuming the web search 
engine is appropriately switched. 
Acknowledgements 
This work was supported by the Advanced 
Research and Development Activity 
(ARDA)'s Advanced Question Answering for 
Intelligence (AQUAINT) Program under 
contract number MDA908-02-C-0007. 
 
References  
Brill, E., J. Lin, M. Banko, S. Dumais, and A. Ng. 
2001. Data-Intensive Question Answering. 
Proceedings of the TREC-10 Conference. 
NIST, Gaithersburg, MD, 183? 9. 
Gusfield, D. 1997. Algorithms on Strings, Trees 
and Sequences: Computer Science and 
Computational Biology. Chapter 6: Linear 
Time construction of Suffix trees, 94? 121. 
Harabagiu, S., D. Moldovan, M. Pasca, R. 
Mihalcea, M. Surdeanu, R. Buneascu, R. G?rju, 
V. Rus and P. Morarescu. 2001. FALCON: 
Boosting Knowledge for Answer Engines. 
Proceedings of the 9th Text Retrieval 
Conference (TREC-9), NIST, 479? 488.  
Hovy, E.H., U. Hermjakob, and C.-Y. Lin. 2001. 
The Use of External Knowledge in Factoid 
QA.   Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 166?
174.  
Hovy, E.H., U. Hermjakob, and D. Ravichandran. 
2002a. A Question/Answer Typology with 
Surface Text Patterns. Proceedings of the 
Human Language Technology (HLT) 
conference.  San Diego, CA.  
Hovy, E.H., U. Hermjakob, C.-Y. Lin, and D. 
Ravichandran. 2002b. Using Knowledge to 
Facilitate Pinpointing of Factoid Answers. 
Proceedings of the COLING-2002 conference. 
Taipei, Taiwan.  
Lee, G.G., J. Seo, S. Lee, H. Jung, B-H. Cho, C. 
Lee, B-K. Kwak, J, Cha, D. Kim, J-H. An, H. 
Kim, and K. Kim. 2001. SiteQ: Engineering 
High Performance QA System Using Lexico-
Semantic Pattern Matching and Shallow NLP. 
Proceedings of the TREC-10 Conference. 
NIST, Gaithersburg, MD, 437? 46.  
Lin, C-Y. 2002. The Effectiveness of Dictionary 
and Web-Based Answer Reranking.  
Proceedings of the COLING-2002 conference. 
Taipei, Taiwan.  
Prager, J. and J. Chu-Carroll. 2001. Use of 
WordNet Hypernyms for Answering What-Is 
Questions. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 309?
316. 
Riloff, E. 1996. Automatically Generating 
Extraction Patterns from Untagged Text.  
Proceedings of the Thirteenth National 
Conference on Artificial Intelligence (AAAI-
96), 1044? 1049. 
Soubbotin, M.M. and S.M. Soubbotin. 2001. 
Patterns of Potential Answer Expressions as 
Clues to the Right Answer. Proceedings of the 
TREC-10 Conference. NIST, Gaithersburg, 
MD, 175? 182.  
Srihari, R. and W. Li. 2000. A Question 
Answering System Supported by Information 
Extraction. Proceedings of the 1st Meeting of 
the North American Chapter of the Association 
for Computational Linguistics (ANLP-
NAACL-00), Seattle, WA, 166? 172. 
Voorhees, E. 2001. Overview of the Question 
Answering Track. Proceedings of the TREC-10 
Conference. NIST, Gaithersburg, MD, 157?
165.  
Wang, B., H. Xu, Z. Yang, Y. Liu, X. Cheng, D. 
Bu, and S. Bai. 2001. TREC-10 Experiments at 
CAS-ICT: Filtering, Web, and QA. 
Proceedings of the TREC-10 Conference. 
NIST, Gaithersburg, MD, 229? 41.  
From Single to Multi-document Summarization:  
A Prototype System and its Evaluation 
Chin-Yew Lin and Eduard Hovy  
University of Southern California / Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{cyl,hovy}@isi.edu 
 
Abstract 
NeATS is a multi-document 
summarization system that attempts 
to extract relevant or interesting 
portions from a set of documents 
about some topic and present them 
in coherent order. NeATS is among 
the best performers in the large scale 
summarization evaluat ion DUC 
2001. 
1 Introduction 
In recent years, text summarization has been 
enjoying a period of revival.  Two workshops 
on Automatic Summarization were held in 
2000 and 2001.  However, the area is still 
being fleshed out: most past efforts have 
focused only on single-document 
summarization (Mani 2000), and no standard 
test sets and large scale evaluations have been 
reported or made available to the English-
speaking research community except the 
TIPSTER SUMMAC Text Summarization 
evaluation (Mani et al 1998). 
To address these issues, the Document 
Understanding Conference (DUC) sponsored 
by the National Institute of Standards and 
Technology (NIST) started in 2001 in the 
United States.  The Text Summarization 
Challenge (TSC) task under the NTCIR (NII-
NACSIS Test Collection for IR Systems) 
project started in 2000 in Japan.  DUC and 
TSC both aim to compile standard training and 
test collections that can be shared among 
researchers and to provide common and large 
scale evaluations in single and multiple 
document summarization for their participants. 
In this paper we describe a multi-document 
summarization system NeATS.  It attempts to 
extract relevant or interesting portions from a 
set of documents about some topic and present 
them in coherent order.  We outline the 
NeATS system and describe how it performs 
content selection, filtering, and presentation in 
Section 2.  Section 3 gives a brief overview of 
the evaluation procedure used in DUC -2001 
(DUC 2001).  Section 4 discusses evaluation 
metrics, and Section 5 the results.  We 
conclude with future directions. 
2 NeATS 
NeATS is an extraction-based multi-document 
summarization system.  It leverages techniques 
proved effective in single document 
summarization such as: term frequency (Luhn 
1969), sentence position (Lin and Hovy 1997), 
stigma words (Edmundson 1969), and a 
simplified version of MMR (Goldstein et al 
1999) to select and filter content.  To improve 
topic coverage and readability, it uses term 
clustering, a ?buddy system? of paired 
sentences, and explicit time annotation. 
Most of the techniques adopted by NeATS are 
not new.  However, applying them in the 
proper places to summarize multiple 
documents and evaluating the results on large 
scale common tasks are new. 
Given an input of a collection of sets of 
newspaper articles, NeATS generates 
summaries in three stages: content selection, 
filtering, and presentation. We describe each 
stage in the following sections. 
2.1 Content Selection 
The goal of content selection is to identify 
important concepts mentioned in a document 
collection.  For example, AA flight 11, AA 
flight 77, UA flight 173, UA flight 93, New 
York, World Trade Center, Twin Towers, 
Osama bin Laden, and al-Qaida are key 
concepts for a document collection about the 
September 11 terrorist attacks in the US. 
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 457-464.
                         Proceedings of the 40th Annual Meeting of the Association for
In a key step for locating important sentences, 
NeATS computes the likelihood ratio l  
(Dunning, 1993) to identify key concepts in 
unigrams, bigrams, and trigrams1, using the 
on- topic document collection as the relevant 
set and the off-topic document collection as the 
irrelevant set.  Figure 1 shows the top 5 
concepts with their relevancy scores (-2l) for 
the topic ?Slovenia Secession from 
Yugoslavia? in the DUC-2001 test collection.  
This is similar to the idea of topic signature 
introduced in (Lin and Hovy 2000). 
With the individual key concepts available, we 
proceed to cluster these concepts in order to 
identify major subtopics within the main topic. 
Clusters are formed through strict lexical 
connection.  For example, Milan and Kucan 
are grouped as ?Milan Kucan? since ?Milan 
Kucan? is a key bigram concept; while 
Croatia, Yugoslavia, Slovenia, republic, and 
are joined due to the connections as follows: 
? Slovenia Croatia 
? Croatia Slovenia 
? Yugoslavia Slovenia 
? republic Slovenia 
                                              
1 Closed class words (of, in, and, are, and so on) 
were ignored in constructing unigrams, bigrams and 
trigrams. 
? Croatia republic 
Each sentence in the document set is then 
ranked, using the key concept structures. An 
example is shown in Figure 2.  The ranking 
algorithm rewards most specific concepts first; 
for example, a sentence containing ?Milan 
Kucan? has a higher score than a sentence 
contains only either Milan or Kucan.  A 
sentence containing both Milan and Kucan  but 
not in consecutive order gets a lower score too.  
This ranking algorithm performs relatively 
well, but it also results in many ties.  
Therefore, it is necessary to apply some 
filtering mechanism to maintain a reasonably 
sized sentence pool for final presentation. 
2.2 Content Filtering 
NeATS uses three different filters: sentence 
position, stigma words, and maximum 
marginal relevancy. 
2.2.1  Sentence Position  
Sentence position has been used as a good 
important content filter since the late 60s 
(Edmundson 1969).  It was also used as a 
baseline in a preliminary multi-document 
summarization study by Marcu and Gerber 
(2001) with relatively good results.  We apply 
a simple sentence filter that only retains the 
lead 10 sentences. 
2.2.2  Stigma Words  
Some sentences start with 
? conjunctions (e.g., but, although, however), 
? the verb say and its derivatives, 
? quotation marks, 
? pronouns such as he, she, and they, 
and usually cause discontinuity in summaries. 
Since we do not use discourse level selection 
criteria ? la (Marcu 1999), we simply reduce 
the scores of these sentences to avoid including 
them in short summaries. 
2.2.3   Maximum Marginal Relevancy  
Figure 2. Top 5 unigram, bigram, and trigram concepts for topic "Slovenia Secession from Yugoslavia".  
Rank Unigram (-2l) Bigram (-2l) Trigram (-2l)
1 Slovenia 319.48 federal army 21.27 Slovenia central bank 5.80
2 Yugoslavia 159.55 Slovenia Croatia 19.33 minister foreign affairs 5.80
3 Slovene 87.27 Milan Kucan 17.40 unallocated federal debt 5.80
4 Croatia 79.48 European Community 13.53 Drnovsek prime minister 3.86
5 Slovenian 67.82 foreign exchange 13.53 European Community countries 3.86
Figure 1. Sample key concept structure. 
n1
(:S URF " WEBCL -SUMM MARIZ ER-KU CAN"
 :C AT S- NP
 :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
 :L EX  0 .6363 63636 36363 6
 :S UBS
 ( ((KUC AN-0)
   (:S URF " Milan  Ku can"
    :C AT S- NP
    :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
    :L EX 0. 63636 36363 63636
    :S UBS
    ((( KUCAN -1)
      (:S URF " Ku can"
       :C AT S- NP
       :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
       :L EX 0. 63636 36363 63636 ))
     (( KUCAN -2)
      (:S URF " Milan "
       :C AT S- NP
       :C LASS I-EN- WEBCL -SIGN ATURE -KUCAN
       :L EX 0. 63636 36363 63636 ))))) ))
The content selection and filtering methods 
described in the previous section only  concern 
individual sentences.  They do not consider the 
redundancy issue when two top ranked 
sentences refer to similar things.  To address 
the problem, we use a simplified version of 
CMU?s MMR (Goldstein et al 1999) 
algorithm.  A sentence is added to the 
summary if and only if its content has less than 
X percent overlap with the summary.  The 
overlap ratio is computed using simple 
stemmed word overlap and the threshold X is 
set empirically. 
2.3 Content Presentation 
NeATS so far only considers features 
pertaining to individual sentences.  As we 
mentioned in Section 2.2.2, we can demote 
some sentences containing stigma words to 
improve the cohesion and coherence of 
summaries.  However, we still face two 
problems: definite noun phrases and events 
spread along an extended timeline.  We 
describe these problems and our solutions in 
the following sections. 
2.3.1  A Buddy System of Paired Sentences  
The problem of definite noun phrases can be 
illustrated in Figure 3.  These sentences are 
from documents of the DUC -2001 topic US 
Drought of 1988.  According to pure sentence 
scores, sentence 3 of document AP891210-
0079 has a higher score (34.60) than sentence 
1 (32.20) and should be included in the shorter 
summary (size=?50?).  However, if we select 
sentence 3 without also including sentence 1, 
the definite noun phrase ?The record $3.9 
billion drought relief program of 1988? seems 
to come without any context.  To remedy this 
problem, we introduce a buddy system to 
improve cohesion and coherence.  Each 
sentence is paired with a suitable introductory 
sentence unless it is already an introductory 
sentence.  In DUC -2001 we simply used the 
first sentence of its document.  This assumes 
lead sentences provide introduction and 
context information about what is coming next.  
2.3.2  Time Annotation and Sequence 
One main problem in multi-document 
summarization is that documents in a 
collection might span an extended time period. 
For example, the DUC-2001 topic ?Slovenia 
Secession from Yugoslavia? contains 11 
documents dated from 1988 to 1994, from 5 
different sources 2.  Although a source 
document for single-document summarization 
might contain information collected across an 
extended time frame and from multiple 
sources, the author at least would synchronize 
them and present them in a coherent order.  In 
multi-document summarization, a date 
expression such as Monday occurring in two 
different documents might mean the same date 
or different dates.  For example, sentences in 
the 100 word summary shown in Figure 4 
come from 3 main time periods, 1990, 1991, 
and 1994.  If no absolute time references are 
given, the summary might mislead the reader 
to think that all the events mentioned in the 
four summary sentences occurred in a single 
week.  Therefore, time disambiguation and 
normalization are very important in multi-
document summarization.  As the first attempt, 
we use publication dates as reference points 
and compute actual dates for the following 
date expressions: 
? weekdays (Sunday, Monday, etc); 
? (past | next | coming) + weekdays; 
? today, yesterday, last night.  
We then order the summary sentences in their 
chronological order. Figure 4 shows an 
                                              
2 Sources include Associated Press, Foreign 
Broadcast Information Service, Financial Times, 
San Jose Mercury News, and Wall Street Journal. 
<multi size="50" docset="d50i">  
AP891210-0079 1 (32.20) (12/10/89) America's 1988 drought captured attention everywhere, but especially in 
Washington where politicians pushed through the largest disaster relief measure in U.S. history.  
AP891213-0004 1 (34.60) (12/13/89) The drought of 1988 hit ? 
</multi> 
<multi size="100" docset="d50i"> 
AP891210-0079 1 (32.20) (12/10/89) America's 1988 drought captured attention everywhere, but especially in 
Washington where politicians pushed through the largest disaster relief measure in U.S. history.  
AP891210-0079 3 (41.18) (12/10/89) The record $3.9 billion drought relief program of 1988, hailed as 
salvation for small farmers devastated by a brutal dry spell, became much more _ an unexpected, election-
year windfall for thousands of farmers who collected millions of dollars for nature's normal quirks.  
AP891213-0004 1 (34.60) (12/13/89) The drought of 1988 hit ?  
</multi> 
Figure 3. 50 and 100 word summaries for topic "US Drought of 1988". 
example 100 words summary with time 
annotations. Each sentence is marked with its 
publication date and a reference date 
(MM/DD/YY) is inserted after every date 
expression. 
3 DUC 2001 
Before we present our results, we describe the 
corpus and evaluation procedures of the 
Document Understanding Conference 2001 
(DUC 2001). 
DUC is a new evaluation series supported by 
NIST under TIDES, to further progress in 
summarization and enable researchers to 
participate in large-scale experiments.  There 
were three tasks in 2001: 
(1) Fully automatic summarization of a single 
document. 
(2) Fully automatic summarization of multiple 
documents: given a set of document on a 
single subject, participants were required to 
create 4 generic summaries of the entire set 
with approximately 50, 100, 200, and 400 
words. 30 document sets of approximately 10 
documents each were provided with their 50, 
100, 200, and 400 human written summaries 
for training (training set) and another 30 
unseen sets were used for testing (test set). 
(3) Exploratory summarization: participants 
were encouraged to investigate alternative 
approaches in summarization and report their 
results. 
NeATS participated only in the fully automatic 
multi-document summarization task.  A total 
of 12 systems participated in that task.   
The training data were distributed in early 
March of 2001 and the test data were 
distributed in mid-June of 2001.  Results were 
submitted to NIST for evaluation by July 1st. 
3.1 Evaluation Procedures 
NIST assessors who created the ?ideal? written 
summaries did pairwise comparisons of their 
summaries to the system-generated summaries, 
other assessors? summaries, and baseline 
summaries.  In addition, two baseline 
summaries were created automatically as 
reference points.  The first baseline, lead 
baseline, took the first 50, 100, 200, and 400 
words in the last document in the collection.  
The second baseline, coverage baseline, took 
the first sentence in the first document, the first 
sentence in the second document and so on 
until it had a summary of 50, 100, 200, or 400 
words. 
3.2 Summary Evaluation 
Environment 
NIST used the Summary Evaluation 
Environment (SEE) 2.0 developed by one of 
the authors (Lin 2001) to support its human 
evaluation process.  Using SEE, the assessors 
evaluated the quality of the system?s text (the 
peer text) as compared to an ideal (the model 
text).  The two texts were broken into lists of 
units and displayed in separate windows.  In 
DUC-2001 the sentence was used as the 
smallest unit of evaluation.  
SEE 2.0 provides interfaces for assessors to 
judge the quality of summaries in 
grammatically3, cohesion4, and coherence5 at 
five different levels: all, most, some, hardly 
any, or none.  It also allow s assessors to step 
through each model unit, mark all system units 
sharing content with the current model unit, 
and specify that the marked system units 
                                              
3 Does a summary follow the rule of English 
grammatical rules independent of its content? 
4 Do sentences in a summary fit in with their 
surrounding sentences?  
5 Is the content of a summary expressed and 
organized in an effectiv e way? 
Figure 4. 100 word summary with explicit time annotation. 
<multi size="100" docset="d45h"> 
AP900625-0160  1 (26.60) (06/25/90) The republic of Slovenia plans to begin work on a constitution 
that will give it full sovereignty within a new Yugoslav confederation, the state Tanjug news agency 
reported Monday (06/25/90).  
WSJ910628-0109 3 (9.48)  (06/28/91) On Wednesday (06/26/91), the Slovene soldiers manning this border 
post raised a new flag to mark Slovenia's independence from Yugoslavia.  
WSJ910628-0109 5 (53.77) (06/28/91) Less than two days after Slovenia and Croatia, two of Yugoslavia's 
six republics, unilaterally seceded from the nation, the federal government in Belgrade mobilized 
troops to regain control.  
FBIS3-30788    2 (49.14) (02/09/94) In the view of Yugoslav diplomats, the normalization of relations 
between Slovenia and the Federal Republic of Yugoslavia will certainly be a strenuous and long-term 
project.  
</multi> 
express all, most, some or hardly any of the 
content of the current model unit. 
4 Evaluation Metrics  
One goal of DUC-2001 was to debug the 
evaluation procedures and identify stable 
metrics that could serve as common reference 
points. NIST did not define any official 
performance metric in DUC-2001.  It released 
the raw evaluation results to DUC -2001 
participants and encouraged them to propose 
metrics that would help progress the field. 
4.1.1  Recall, Coverage, Retention and 
Weighted Retention  
Recall at different compression ratios has been 
used in summarization research (Mani 2001) to 
measure how well an automatic system retains 
important content of original documents.  
Assume we have a system summary Ss and a 
model summary Sm. The number of sentences 
occurring in both Ss and Sm is Na, the number 
of sentences in Ss is Ns, and the number of 
sentences in Sm is Nm. Recall is defined as 
Na/Nm.  The Compression Ratio is defined as 
the length of a summary (by words or 
sentences) divided by the length of its original 
document. DUC-2001 set the compression 
lengths to 50, 100, 200, and 400 words for the 
multi-document summarization task.  
However, applying recall in DUC-2001 
without modification is not appropriate 
because: 
1. Multiple system units contribute to 
multiple model units. 
2. Ss and Sm do not exactly overlap.  
3. Overlap judgment is not binary.  
For example, in an evaluation session an 
assessor judged system units S1.1 and S10.4 as 
sharing some content with model unit M2.2.  
Unit S1.1 says ?Thousands of people are 
feared dead? and unit M2.2 says ?3,000 and 
perhaps ? 5,000 people have been killed?.  
Are ?thousands? equivalent to ?3,000 to 
5,000? or not?  Unit S10.4 indicates it was an 
?earthquake of magnitude 6.9? and unit M2.2 
says it was ?an earthquake measuring 6.9 on 
the Richter scale?.  Both of them report a ?6.9? 
earthquake.   But the second part of system 
unit S10.4, ?in an area so isolated??, seems 
to share some content with model unit M4.4 
?the quake was centered in a remote 
mountainous area?. Are these two equivalent? 
This example highlights the difficulty of 
judging the content coverage of system 
summaries against model summaries and the 
inadequacy of using recall as defined.  
As we mentioned earlier, NIST assessors not 
only marked the sharing relations among 
system units (SU) and model units (MU), they 
also indicated the degree of match, i.e., all, 
most , some, hardly any,  or none.  This enables 
us to compute weighted recall.  
Different versions of weighted recall were 
proposed by DUC-2001 participants. 
McKeown et al (2001) treated the 
completeness of coverage as threshold: 4 for 
all, 3 for most  and above, 2 for some and 
above, and 1 for hardly any and above.  They 
then proceeded to compare system 
performances at different threshold levels.  
They defined recall at threshold t, Recallt, as 
follows:  
summary model in the MUs ofnumber  Total
 aboveor at  marked MUs ofNumber t  
We used the completeness of coverage as 
coverage score, C, instead of threshold: 1 for 
all, 3/4 for most, 1/2 for some, and 1/4 for 
hardly any, 0 for none.  To avoid confusion 
with the recall used in information retrieval, 
we call our metric weighted retention, 
Retentionw, and define it as follows: 
summary model in the MUs ofnumber  Total
  marked) MUs of(Number C?  
if we ignore C and set it always to 1, we obtain 
an unweighted retention, Retention1.  We used 
Retention1 in our evaluation to illustrate that 
relative system performance changes when 
different evaluation metrics are chosen.  
Therefore, it is important to have common and 
agreed upon metrics to facilitate large scale 
evaluation efforts.  
4.1.2  Precision and Pseudo Precision 
Precision is also a common measure.  
Borrowed from information retrieval research, 
precision is used to measure how effectively a 
system generates good summary sentences.  It 
is defined as Na/ Ns. Precision in a fixed length 
summary output is equal to recall since N s =  
Nm.  However, due to the three reasons stated 
at the beginning of the previous section, no 
straightforward computation of the traditional 
precision is available in DUC-2001. 
If we count the number of model units that are 
marked as good summary units and are 
selected by systems, and use the number of 
model units in various summary lengths as the 
sample space, we obtain a precision metric 
equal to Retention1.  Alternatively, we can 
count how many unique system units share 
content with model units and use the total 
number of system units as the sample space.  
We define this as pseudo precision, Precisionp, 
as follows: 
summary system in the SUs ofnumber  Total
marked SUs ofNumber  
Most of the participants in DUC-2001 reported 
their pseudo precision figures. 
5 Results and Discussion 
We present the performance of NeATS in 
DUC-2001 in content and quality measures.  
5.1 Content 
With respect to content, we computed 
Retention1, Retention w, and Precisionp using 
the formulas defined in the previous section.  
The scores are shown in Table 1 (overall 
average and per size).  Analyzing all systems? 
results according to these, we made the 
following observations. 
 (1) NeATS (system N) is consistently ranked 
among the top 3 in average and per size 
Retention1 and Retention w. 
(2) NeATS?s performance for averaged pseudo 
precision equals human?s at about 58% (Pp all). 
(3) The performance in weighted retention is 
really low.  Even humans6 score only 29% (Rw 
all). This indicates low inter-human agreement 
(which we take to reflect the undefinedness of 
the ?generic summary? task).  However, the 
unweighted retention of humans is 53%.  This 
suggests assessors did write something similar 
in their summaries but not exactly the same; 
once again illustrating the difficulty of 
summarization evaluation.  
(4) Despite the low inter -human agreement, 
humans score better than any system.  They 
outscore the nearest system by about 11% in 
averaged unweighted retention (R1 all : 53% vs. 
42%) and weighted retention (Rw all : 29% vs. 
18%).  There is obviously still considerable 
room for systems to improve.  
(5) System performances are separated into 
two major groups by baseline 2 (B2: coverage 
baseline) in averaged weighted retention.  This 
confirms that lead sentences are good 
summary sentence candidates and that one 
does need to cover all documents in a topic to 
achieve reasonable performance in multi-
document summarization. NeATS?s strategies 
of filtering sentences by position and adding 
lead sentences to set context are proved 
effective. 
(6) Different metrics result in different 
performance rankings.  This is demonstrated 
by the top 3 systems T, N, and Y.  If we use 
the averaged unweighted retention (R1 all), Y is 
                                              
6 NIST assessors wrote two separate summaries per 
topic.  One was used to judge all system summaries 
and the two baselines. The other was used to 
determine the (potential) upper bound. 
Table 1.  Pseudo precision, unweighted retention, and weighted retention for all summary lengths: overall 
average, 400, 200, 100, and 50 words. 
SYS Pp All R1 All Rw Al l Pp  4 0 0 R1  4 0 0 Rw  4 0 0 Pp  2 0 0 R1 200 Rw  2 0 0 Pp 100 R1 100 Rw 100 Pp  50 R1 50 Rw 50
HM 58.71% 53.00% 28.81% 59.33% 52.95% 33.23% 59.91% 57.23% 33.82% 58.73% 54.67% 27.54% 56.87% 47.16% 21.62%
T 48.96% 35.53% (3) 18.48% (1) 56.51% (3) 38.50% (3) 25.12% (1) 53.85% (3) 35.62% 21.37% (1) 43.53% 32.82% (3) 14.28% (3) 41.95% 35.17% (2) 13.89% (2)
N* 58.72% (1) 37.52% (2) 17.92% (2) 61.01% (1) 41.21% (1) 23.90% (2) 63.34% (1) 38.21% (3) 21.30% (2) 58.79% (1) 36.34% (2) 16.44% (2) 51.72% (1) 34.31% (3) 10.98% (3)
Y 41.51% 41.58% (1) 17.78% (3) 49.78% 38.72% (2) 20.04% 43.63% 39.90% (1) 16.86% 34.75% 43.27% (1) 18.39% (1) 37.88% 44.43% (1) 15.55% (1)
P 49.56% 33.94% 15.78% 57.21% (2) 37.76% 22.18% (3) 51.45% 37.49% 19.40% 46.47% 31.64% 13.92% 43.10% 28.85% 9.09%
L 51.47% (3) 33.67% 15.49% 52.62% 36.34% 21.80% 53.51% 36.87% 18.34% 48.62% (3) 29.00% 12.54% 51.15% (2) 32.47% 9.90%
B2 47.27% 30.98% 14.56% 60.99% 33.51% 18.35% 49.89% 33.27% 17.72% 47.18% 29.48% 14.96% 31.03% 27.64% 8.02%
S 52.53% (2) 30.52% 12.89% 55.55% 36.83% 20.35% 58.12% (2) 38.70% (2) 19.93% (3) 49.70% (2) 26.81% 10.72% 46.43% (3) 19.23% 4.04%
M 43.39% 27.27% 11.32% 54.78% 33.81% 19.86% 45.59% 27.80% 13.27% 41.89% 23.40% 9.13% 31.30% 24.07% 5.05%
R 41.86% 27.63% 11.19% 48.63% 24.80% 12.15% 43.96% 31.28% 15.17% 38.35% 27.61% 11.46% 36.49% 26.84% 6.17%
O 43.76% 25.87% 11.19% 50.73% 27.53% 15.76% 42.94% 26.80% 13.07% 40.55% 25.13% 9.36% 40.80% 24.02% 7.03%
Z 37.98% 23.21% 8.99% 47.51% 31.17% 17.38% 46.76% 25.65% 12.83% 28.91% 17.29% 5.45% 28.74% 18.74% 3.23%
B1 32.92% 18.86% 7.45% 33.48% 17.58% 9.98% 43.13% 18.60% 8.65% 30.23% 17.42% 6.05% 24.83% 21.84% 4.20%
W 30.08% 20.38% 6.78% 38.14% 25.89% 12.10% 26.86% 21.01% 7.93% 28.31% 19.15% 5.36% 27.01% 15.46% 3.21%
U 23.88% 21.38% 6.57% 31.49% 29.76% 13.17% 24.20% 22.64% 8.49% 19.13% 17.54% 3.77% 20.69% 15.57% 3.04%
the best, followed by N, and then T; if we 
choose averaged weighted retention (Rw all), T 
is the best, followed by N, and then Y.  The 
reversal of T and Y due to different metrics 
demonstrates the importance of common 
agreed upon metrics.  We believe that metrics 
have to take coverage score (C, Section 4.1.1) 
into consideration to be reasonable since most 
of the content sharing among system units and 
model units is partial.  The recall at threshold t, 
Recallt (Section 4.1.1), proposed by 
(McKeown et al 2001), is a good example.  In 
their evaluation, NeATS ranked second at t=1, 
3, 4 and first at t=2.   
(7) According to Table 1, NeATS performed 
better on longer summaries (400 and 200 
words) based on weighted retention than it did 
on shorter ones.  This is the result of the 
sentence extraction-based nature of NeATS.  
We expect that systems that use syntax-based 
algorithms to compress their output will 
thereby gain more space to include additional 
important material. For example, System Y 
was the best in shorter summaries.  Its 100- 
and 50-word summaries contain only 
important headlines.  The results confirm this 
is a very effective strategy in composing short 
summaries.  However, the quality of the 
summaries suffered because of the 
unconventional syntactic structure of news 
headlines (Table 2).  
5.2 Quality 
Table 2 shows the macro-averaged scores for 
the humans, two baselines, and 12 systems.  
We assign a score of 4 to all, 3 to most, 2 to 
some, 1 to hardly any, and 0 to none.  The 
value assignment is for convenience of 
computing averages, since it is more 
appropriate to treat these measures as stepped 
values instead of continuous ones.  With this in 
mind, we have the following observations. 
(1) Most systems scored well in 
grammaticality.  This is not a surprise since 
most of the participants extracted sentences as 
summaries.  
But no system or human scored perfect in 
grammaticality. This might be due to the 
artifact of cutting sentences at the 50, 100, 200, 
and 400 words boundaries.  Only system Y 
scored lower than 3, which reflects its headline 
inclusion strategy.  
(2) When it came to the measure for cohesion 
the results are confusing.  If even the human-
made summaries score only 2.74 out of 4, it is 
unclear what this category means, or how the 
assessors arrived at these scores.  However, the 
humans and baseline 1 (lead baseline) did 
score in the upper range of 2 to 3 and all others 
had scores lower than 2.5.  Some of the 
systems (including B2) fell into the range of 1 
to 2 meaning some or hardly any cohesion.  
The lead baseline (B1), taking the first 50, 100, 
200, 400 words from the last document of a 
topic, did well.  On the contrary, the coverage 
baseline (B2) did poorly.  This indicates the 
difficulty of fitting sentences from different 
documents together.  Even selecting 
continuous sentences from the same document 
(B1) seems not to work well.  We need to 
define this metric more clearly and improve 
the capabilities of systems in this respect. 
(3) Coherence scores roughly track cohesion 
scores.  Most systems did better in coherence 
than in cohesion.   The human is the only one 
scoring above 3.  Again the room for 
improvement is abundant. 
(4) NeATS did not fare badly in quality 
measures.  It was in the same categories as 
other top performers: grammaticality is 
between most and all, cohesion, some and 
most , and coherence, some and most.  This 
indicates the strategies employed by NeATS 
(stigma word filtering, adding lead sentence, 
and time annotation) worked to some extent 
but left room for improvement. 
6 Conclusions  
Table 2. Averaged grammaticality, cohesion, and 
coherence over all summary sizes. 
SYS Grammar Cohesion Coherence
Human 3.74 2.74 3.19
B1 3.18 2.63 2.8
B2 3.26 1.71 1.65
L 3.72 1.83 1.9
M 3.54 2.18 2.4
N* 3.65 2 2.22
O 3.78 2.15 2.33
P 3.67 1.93 2.17
R 3.6 2.16 2.45
S 3.67 1.93 2.04
T 3.51 2.34 2.61
U 3.28 1.31 1.11
W 3.13 1.48 1.28
Y 2.45 1.73 1.77
Z 3.28 1.8 1.94
We described a multi-document 
summarization system, NeATS, and its 
evaluation in DUC-2001. We were encouraged 
by the content and readability of the results.  
As a prototype system, NeATS deliberately 
used simple methods guided by a few 
principles: 
? Extracting important concepts based on 
reliable statistics. 
? Filtering sentences by their positions and 
stigma words. 
? Reducing redundancy using MMR. 
? Presenting summary sentences in their 
chronological order with time annotations. 
These simple principles worked effectively.  
However, the simplicity of the system also 
lends itself to further improvements.  We 
would like to apply some compression 
techniques or use linguistic units smaller than 
sentences to improve our retention score.  The 
fact that NeATS performed as well as the 
human in pseudo precision but did less well in 
retention indicates its summaries might include 
good but duplicated information.  Working 
with sub-sentence units should help.  
To improve NeATS?s capability in content 
selection, we have started to parse sentences 
containing key unigram, bigram, and trigram 
concepts to identify their relations within their 
concept clusters. 
To enhance cohesion and coherence, we are 
looking into incorporating discourse 
processing techniques (Marcu 1999) or Radev 
and McKeown?s (1998) summary operators. 
We are analyzing the DUC evaluation scores 
in the hope of suggesting improved and more 
stable metrics. 
References 
DUC. 2001. The Document Understanding 
Workshop 2001. http://www-nlpir.nist.gov/ 
projects/duc/2001.html. 
Dunning, T. 1993. Accurate Methods for the 
Statistics of Surprise and Coincidence.  
Computational Linguistics 19, 61?74. 
Edmundson, H.P. 1969. New Methods in 
Automatic Abstracting.  Journal of the 
Association for Computing Machinery.  
16(2). 
Goldstein, J., M. Kantrowitz, V. Mittal, and J. 
Carbonell. 1999. Summarizing Text 
Documents: Sentence Selection and 
Evaluation Metrics. Proceedings of the 22nd 
International ACM Conference on 
Research and Development in Information 
Retrieval (SIGIR-99), Berkeley, CA, 121?
128. 
Lin, C.-Y. and E.H. Hovy. 2000. The 
Automated Acquisition of Topic 
Signatures for Text Summarization. 
Proceedings of the COLING 
Conference.  Saarbr?cken , Germany. 
Lin, C.-Y. 2001.  Summary Evaluation 
Environment. http://www.isi.edu/~cyl/SEE. 
Luhn, H. P. 1969. The Automatic Creation of 
Literature Abstracts. IBM Journal of 
Research and Development 2(2), 1969. 
Mani, I., D. House, G. Klein, L. Hirschman, L. 
Obrst, T. Firmin, M. Chrzanow ski, and B. 
Sundheim. 1998. The TIPSTER SUMMAC 
Text Summarization Evaluation: Final 
Report. MITRE Corp. Tech. Report. 
Mani, I. 2001. Automatic Summarization. John 
Benjamins Pub Co. 
Marcu, D. 1999. Discourse trees are good 
indicators of importance in text. In I. Mani 
and M. Maybury (eds), Advances in 
Automatic Text Summarization, 123?136. 
MIT Press. 
Marcu, D. and L. Gerber. 2001. An Inquiry 
into the Nature of Multidocument 
Abstracts, Extracts, and their Evaluation. 
Proceedings of the NAACL -2001 Workshop 
on Automatic Summarization.  Pittsburgh, 
PA. 
McKeown, K., R. Barzilay, D. Evans, V. 
Hatzivassiloglou, M-Y Kan, B, Schiffman, 
and S. Teufel 2001. Columbia Multi-
Document Summarization: Approach and 
Evaluation.  DUC-01 Workshop on Text 
Summarization.  New Orleans, LA.  
Radev, D.R. and K.R. McKeown. 1998. 
Generating Natural Language Summaries 
from Multiple On-line Sources. 
Computational Linguistics, 24(3):469?500.  
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked 
 
Michael Fleischman, Eduard Hovy,  
Abdessamad Echihabi 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, hovy, echihabi} @ISI.edu 
  
Abstract 
Recent work in Question Answering has 
focused on web-based systems that 
extract answers using simple lexico-
syntactic patterns.  We present an 
alternative strategy in which patterns are 
used to extract highly precise relational 
information offline, creating a data 
repository that is used to efficiently 
answer questions.  We evaluate our 
strategy on a challenging subset of 
questions, i.e. ?Who is ?? questions, 
against a state of the art web-based 
Question Answering system.  Results 
indicate that the extracted relations 
answer 25% more questions correctly and 
do so three orders of magnitude faster 
than the state of the art system. 
1 Introduction 
Many of the recent advances in Question 
Answering have followed from the insight that 
systems can benefit by exploiting the redundancy 
of information in large corpora.  Brill et al (2001) 
describe using the vast amount of data available on 
the World Wide Web to achieve impressive 
performance with relatively simple techniques.  
While the Web is a powerful resource, its 
usefulness in Question Answering is not without 
limits.   
The Web, while nearly infinite in content, is 
not a complete repository of useful information.  
Most newspaper texts, for example, do not remain 
accessible on the Web for more than a few weeks.  
Further, while Information Retrieval techniques are 
relatively successful at managing the vast quantity 
of text available on the Web, the exactness 
required of Question Answering systems makes 
them too slow and impractical for ordinary users. 
In order to combat these inadequacies, we 
propose a strategy in which information is 
extracted automatically from electronic texts 
offline, and stored for quick and easy access.  We 
borrow techniques from Text Mining in order to 
extract semantic relations (e.g., concept-instance 
relations) between lexical items.  We enhance 
these techniques by increasing the yield and 
precision of the relations that we extract.   
Our strategy is to collect a large sample of 
newspaper text (15GB) and use multiple part of 
speech patterns to extract the semantic relations.  
We then filter out the noise from these extracted 
relations using a machine-learned classifier.  This 
process generates a high precision repository of 
information that can be accessed quickly and 
easily. 
We test the feasibility of this strategy on one 
semantic relation and a challenging subset of 
questions, i.e., ?Who is ?? questions, in which 
either a concept is presented and an instance is 
requested (e.g., ?Who is the mayor of Boston??), 
or an instance is presented and a concept is 
requested (e.g., ?Who is Jennifer Capriati??).  By 
choosing this subset of questions we are able to 
focus only on answers given by concept-instance 
relationships.  While this paper examines only this 
type of relation, the techniques we propose are 
easily extensible to other question types. 
Evaluations are conducted using a set of ?Who 
is ?? questions collected over the period of a few 
months from the commercial question-based 
search engine www.askJeeves.com.  We extract 
approximately 2,000,000 concept-instance 
relations from newspaper text using syntactic 
patterns and machine-learned filters (e.g., 
?president Bill Clinton? and ?Bill Clinton, 
president of the USA,?).  We then compare 
answers based on these relations to answers given 
by TextMap (Hermjakob et al, 2002), a state of the 
art web-based question answering system.  Finally, 
we discuss the results of this evaluation and the 
implications and limitations of our strategy. 
3.1 
2 
3 
3.2 
Related Work 
A great deal of work has examined the problem of 
extracting semantic relations from unstructured 
text.  Hearst (1992) examined extracting hyponym 
data by taking advantage of lexical patterns in text.  
Using patterns involving the phrase ?such as?, she 
reports finding only 46 relations in 20M of New 
York Times text.  Berland and Charniak (1999) 
extract ?part-of? relations between lexical items in 
text, achieving only 55% accuracy with their 
method.  Finally, Mann (2002) describes a method 
for extracting instances from text that takes 
advantage of part of speech patterns involving 
proper nouns.  Mann reports extracting 200,000 
concept-instance pairs from 1GB of Associated 
Press text, only 60% of which were found to be 
legitimate descriptions.   
These studies indicate two distinct problems 
associated with using patterns to extract semantic 
information from text.  First, the patterns yield 
only a small amount of the information that may be 
present in a text (the Recall problem).  Second, 
only a small fraction of the information that the 
patterns yield is reliable (the Precision problem).   
Relation Extraction 
Our approach follows closely from Mann (2002).  
However, we extend this work by directly 
addressing the two problems stated above.  In 
order to address the Recall problem, we extend the 
list of patterns used for extraction to take 
advantage of appositions.  Further, following 
Banko and Brill (2001), we increase our yield by 
increasing the amount of data used by an order of 
magnitude over previously published work.  
Finally, in order to address the Precision problem, 
we use machine learning techniques to filter the 
output of the part of speech patterns, thus purifying 
the extracted instances. 
Data Collection and Preprocessing 
Approximately 15GB of newspaper text was 
collected from: the TREC 9 corpus (~3.5GB), the 
TREC 2002 corpus (~3.5GB), Yahoo! News 
(.5GB), the AP newswire (~2GB), the Los Angeles 
Times (~.5GB), the New York Times (~2GB), 
Reuters (~.8GB), the Wall Street Journal 
(~1.2GB), and various online news websites 
(~.7GB).  The text was cleaned of HTML (when 
necessary), word and sentence segmented, and part 
of speech tagged using Brill?s tagger (Brill, 1994). 
Extraction Patterns 
Part of speech patterns were generated to take 
advantage of two syntactic constructions that often 
indicate concept-instance relationships: common 
noun/proper noun constructions (CN/PN) and 
appositions (APOS).  Mann (2002) notes that 
concept-instance relationships are often expressed 
by a syntactic pattern in which a proper noun 
follows immediately after a common noun.  Such 
patterns (e.g. ?president George Bush?) are very 
productive and occur 40 times more often than 
patterns employed by Hearst (1992).  Table 1 
shows the regular expression used to extract such 
patterns along with examples of extracted patterns. 
 
${NNP}*${VBG}*${JJ}*${NN}+${NNP}+ 
trainer/NN Victor/NNP Valle/NNP  
ABC/NN spokesman/NN Tom/NNP Mackin/NNP 
official/NN Radio/NNP Vilnius/NNP  
German/NNP expert/NN Rriedhart/NNP 
Dumez/NN Investment/NNP 
Table 1.  The regular expression used to extract CN/PN 
patterns (common noun followed by proper noun).  
Examples of extracted text are presented below.  Text in 
bold indicates that the example is judged illegitimate.  
 
${NNP}+\s*,\/,\s*${DT}*${JJ}*${NN}+(?:of\/IN)* 
          \s*${NNP}*${NN}*${IN}*${DT}*${NNP}* 
          ${NN}*${IN}*${NN}*${NNP}*,\/,  
Stevens/NNP  ,/, president/NN of/IN the/DT firm/NN  ,/, 
Elliott/NNP Hirst/NNP  ,/, md/NN of/IN Oldham/NNP Signs/NNP  ,/, 
George/NNP McPeck/NNP,/, an/DT engineer/NN from/IN Peru/NN,/, 
Marc/NNP Jonson/NNP,/, police/NN chief/NN of/IN Chamblee/NN ,/, 
David/NNP Werner/NNP ,/, a/DT real/JJ estate/NN investor/NN ,/, 
Table 2.  The regular expression used to extract APOS 
patterns (syntactic appositions).  Examples of extracted 
text are presented below.  Text in bold indicates that the 
example is judged illegitimate.  
In addition to the CN/PN pattern of Mann 
(2002), we extracted syntactic appositions (APOS).  
This pattern detects phrases such as ?Bill Gates, 
chairman of Microsoft,?.  Table 2 shows the 
regular expression used to extract appositions and 
examples of extracted patterns.  These regular 
expressions are not meant to be exhaustive of all 
possible varieties of patterns construed as CN/PN 
or APOS.  They are ?quick and dirty? 
implementations meant to extract a large 
proportion of the patterns in a text, acknowledging 
that some bad examples may leak through. 
3.3 Filtering 
The concept-instance pairs extracted using the 
above patterns are very noisy.  In samples of 
approximately 5000 pairs, 79% of the APOS 
extracted relations were legitimate, and only 45% 
of the CN/PN extracted relations were legitimate.  
This noise is primarily due to overgeneralization of 
the patterns (e.g., ?Berlin Wall, the end of the Cold 
War,?) and to errors in the part of speech tagger 
(e.g., ?Winnebago/CN Industries/PN?).  Further, 
some extracted relations were considered either 
incomplete (e.g., ?political commentator Mr. 
Bruce?) or too general (e.g., ?meeting site Bourbon 
Street?) to be useful.  For the purposes of learning 
a filter, these patterns were treated as illegitimate. 
In order to filter out these noisy concept-
instance pairs, 5000 outputs from each pattern 
were hand tagged as either legitimate or 
illegitimate, and used to train a binary classifier.  
The annotated examples were split into a training 
set (4000 examples), a validation set (500 
examples); and a held out test set (500 examples).  
The WEKA machine learning package (Witten and 
Frank, 1999) was used to test the performance of 
various learning and meta-learning algorithms, 
including Na?ve Bayes, Decision Tree, Decision 
List, Support Vector Machines, Boosting, and 
Bagging.   
Table 4 shows the list of features used to 
describe each concept-instance pair for training the 
CN/PN filter.  Features are split between those that 
deal with the entire pattern, only the concept, only 
the instance, and the pattern?s overall orthography.  
The most powerful of these features examines an 
Ontology in order to exploit semantic information 
about the concept?s head.  This semantic 
information is found by examining the super-
concept relations of the concept head in the 
110,000 node Omega Ontology (Hovy et al, in 
prep.).   
 
 
Feature 
Type  
Pattern  
Features 
Binary ${JJ}+${NN}+${NNP}+ 
Binary ${NNP}+${JJ}+${NN}+${NNP}+ 
Binary ${NNP}+${NN}+${NNP}+ 
Binary ${NNP}+${VBG}+${JJ}+${NN}+${NNP}+ 
Binary ${NNP}+${VBG}+${NN}+${NNP}+ 
Binary ${NN}+${NNP}+ 
Binary ${VBG}+${JJ}+${NN}+${NNP}+ 
Binary ${VBG}+${NN}+${NNP}+ 
  Concept Features 
Binary Concept head ends in "er" 
Binary Concept head ends in "or" 
Binary Concept head ends in "ess" 
Binary Concept head ends in "ist" 
Binary Concept head ends in "man" 
Binary Concept head ends in "person" 
Binary Concept head ends in "ant" 
Binary Concept head ends in "ial" 
Binary Concept head ends in "ate" 
Binary Concept head ends in "ary" 
Binary Concept head ends in "iot" 
Binary Concept head ends in "ing" 
Binary Concept head is-a occupation 
Binary Concept head is-a person 
Binary Concept head is-a organization 
Binary Concept head is-a company 
Binary Concept includes digits 
Binary Concept has non-word 
Binary Concept head in general list 
Integer Frequency of concept head in CN/PN 
Integer Frequency of concept head in APOS 
  Instance Features 
Integer Number of lexical items in instance 
Binary Instance contains honorific 
Binary Instance contains common name 
Binary Instance ends in honorific 
Binary Instance ends in common name 
Binary Instance ends in determiner 
  Case Features 
Integer Instance: # of lexical items all Caps 
Integer Instance: # of lexical items start w/ Caps 
Binary Instance: All lexical items start w/ Caps 
Binary Instance: All lexical items all Caps 
Integer Concept: # of lexical items all Caps 
Integer Concept: # of lexical items start w/ Caps 
Binary Concept: All lexical items start w/ Caps 
Binary Concept: All lexical items all Caps 
Integer Total # of lexical items all Caps 
Integer Total # of lexical items start w/ Caps 
Table 4.  Features used to train CN/PN pattern filter.  
Pattern features address aspects of the entire pattern, 
Concept features look only at the concept, Instance 
features examine elements of the instance, and Case 
features deal only with the orthography of the lexical 
items. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1.  Performance of machine learning algorithms 
on a validation set of 500 examples extracted using the 
CN/PN pattern.  Algorithms are compared to a baseline 
in which only concepts that inherit from ?Human? or 
?Occupation? in Omega pass through the filter.   
4 
4.1 
                                                          
Extraction Results 
Machine Learning Results 
Figure 1 shows the performance of different 
machine learning algorithms, trained on 4000 
extracted CN/PN concept-instance pairs, and tested 
on a validation set of 500.  Na?ve Bayes, Support 
Vector Machine, Decision List and Decision Tree 
algorithms were all evaluated and the Decision 
Tree algorithm (which scored highest of all the 
algorithms) was further tested with Boosting and 
Bagging meta-learning techniques.  The algorithms 
are compared to a baseline filter that accepts 
concept-instance pairs if and only if the concept 
head is a descendent of either the concept 
?Human? or the concept ?Occupation? in Omega.  
It is clear from the figure that the Decision Tree 
algorithm plus Bagging gives the highest precision 
and overall F-score.  All subsequent experiments 
are run using this technique.1 
Since high precision is the most important 
criterion for the filter, we also examine the 
performance of the classifier as it is applied with a 
threshold.  Thus, a probability cutoff is set such 
that only positive classifications that exceed this 
cutoff are actually classified as legitimate.  Figure 
2 shows a plot of the precision/recall tradeoff as 
this threshold is changed.  As the threshold is 
raised, precision increases while recall decreases.  
Based on this graph we choose to set the threshold 
at 0.9.  
Learning Algorithm Performance
0.5
0.6
0.7
0.8
0.9
1
Baseline Na?ve Bayes SVM Decision
List
Decision
Tree
DT +
Boosting
DT +
Bagging
Recall Precision F-Score
4.2 
                                                          
1 Precision and Recall here refer only to the output of the 
extraction patterns.  Thus, 100% recall indicates that all 
legitimate concept-instance pairs that were extracted using the 
patterns, were classified as legitimate by the filter.  It does not 
indicate that all concept-instance information in the text was 
extracted.  Precision is to be understood similarly. 
Applying the Decision Tree algorithm with 
Bagging, using the pre-determined threshold, to the 
held out test set of 500 examples extracted with the 
CN/PN pattern yields a precision of .95 and a 
recall of .718.  Under these same conditions, but 
applied to a held out test set of 500 examples 
extracted with the APOS pattern, the filter has a 
precision of .95 and a recall of .92.   
 
 
 
 
 
 
 
 
Precision vs. Recall
as a Function of Threshold
0.955
96
0.965
97
0.975
98
0.985
99
0.995
0.4 0.5 0.6 0.7 0.8 0.9
Recall
P
re
ci
si
o
n
 0.
 0.
 0.
 0.
 
Figure 2.  Plot of precision and recall on a 500 example 
validation set as a threshold cutoff for positive 
classification is changed.  As the threshold is increased, 
precision increases while recall decreases.  At the 0.9 
threshold value, precision/recall on the validation set is 
0.98/0.7, on a held out test set it is 0.95/0.72.   
Final Extraction Results 
The CN/PN and APOS filters were used to extract 
concept-instance pairs from unstructured text.  The 
approximately 15GB of newspaper text (described 
above) was passed through the regular expression 
patterns and filtered through their appropriate 
learned classifier.  The output of this process is 
approximately 2,000,000 concept-instance pairs.  
Approximately 930,000 of these are unique pairs, 
comprised of nearly 500,000 unique instances 2 , 
paired with over 450,000 unique concepts3 (e.g., 
2 Uniqueness of instances is judged here solely on the basis of 
surface orthography.  Thus, ?Bill Clinton? and ?William 
Clinton? are considered two distinct instances.  The effects of 
collapsing such cases will be considered in future work. 
3 As with instances, concept uniqueness is judged solely on the 
basis of orthography.  Thus, ?Steven Spielberg? and ?J. Edgar 
Hoover? are both considered instances of the single concept 
Threshold=0.90
Threshold=0.80
?sultry screen actress?), which can be categorized 
based on nearly 100,000 unique complex concept 
heads (e.g., ?screen actress?) and about 14,000 
unique simple concept heads (e.g., ?actress?).  
Table 3 shows examples of this output. 
A sample of 100 concept-instance pairs was 
randomly selected from the 2,000,000 extracted 
pairs and hand annotated.  93% of these were 
judged legitimate concept-instance pairs. 
 
Concept head Concept Instance 
Producer Executive producer Av Westin 
Newspaper Military newspaper Red Star 
Expert Menopause expert Morris Notwlovitz 
Flutist Flutist James Galway 
Table 3.  Example of concept-instance repository.  
Table shows extracted relations indexed by concept 
head, complete concept, and instance. 
5 
                                                                                          
Question Answering Evaluation 
A large number of questions were collected over 
the period of a few months from 
www.askJeeves.com.  100 questions of the form 
?Who is x? were randomly selected from this set.  
The questions queried concept-instance relations 
through both instance centered queries (e.g., ?Who 
is Jennifer Capriati??) and concept centered 
queries (e.g., ?Who is the mayor of Boston??).  
Answers to these questions were then 
automatically generated both by look-up in the 
2,000,000 extracted concept-instance pairs and by 
TextMap, a state of the art web-based Question 
Answering system which ranked among the top 10 
systems in the TREC 11 Question Answering track 
(Hermjakob et al, 2002). 
Although both systems supply multiple 
possible answers for a question, evaluations were 
conducted on only one answer.4  For TextMap, this 
answer is just the output with highest confidence, 
i.e., the system?s first answer.  For the extracted 
instances, the answer was that concept-instance 
pair that appeared most frequently in the list of 
extracted examples.  If all pairs appear with equal 
frequency, a selection is made at random. 
Answers for both systems are then classified 
by hand into three categories based upon their 
 
?director.?  See Fleischman and Hovy (2002) for techniques 
useful in disambiguating such instances. 
4 Integration of multiple answers is an open research question 
and is not addressed in this work. 
information content. 5  Answers that unequivocally 
identify an instance?s celebrity (e.g., ?Jennifer 
Capriati is a tennis star?) are marked correct.  
Answers that provide some, but insufficient, 
evidence to identify the instance?s celebrity (e.g., 
?Jennifer Capriati is a defending champion?) are 
marked partially correct.  Answers that provide no 
information to identify the instance?s celebrity 
(e.g., ?Jennifer Capriati is a daughter?) are marked 
incorrect.6  Table 5 shows example answers and 
judgments for both systems. 
 
State of the Art  Extraction  
Answer Mark Answer Mark 
Who is Nadia 
Comaneci? 
U.S. 
citizen 
P Romanian 
Gymnast 
C 
Who is Lilian 
Thuram? 
News 
page 
I French 
defender 
P 
Who is the mayor 
of Wash., D.C.? 
Anthony 
Williams 
C no answer 
found 
I 
Table 5.  Example answers and judgments of a state of 
the art system and look-up method using extracted 
concept-instance pairs on questions collected online.  
Ratings were judged as either correct (C), partially 
correct (P), or incorrect (I). 
6 
                                                          
Question Answering Results 
Results of this comparison are presented in Figure 
3.  The simple look-up of extracted concept-
instance pairs generated 8% more partially correct 
answers and 25% more entirely correct answers 
than TextMap.  Also, 21% of the questions that 
TextMap answered incorrectly, were answered 
partially correctly using the extracted pairs; and 
36% of the questions that TextMap answered 
incorrectly, were answered entirely correctly using 
the extracted pairs.  This suggests that over half of 
the questions that TextMap got wrong could have 
benefited from information in the concept-instance 
pairs.  Finally, while the look-up of extracted pairs 
took approximately ten seconds for all 100 
questions, TextMap took approximately 9 hours.  
5  Evaluation of such ?definition questions? is an active 
research challenge and the subject of a recent TREC pilot 
study.  While the criteria presented here are not ideal, they are 
consistent, and sufficient for a system comparison. 
6  While TextMap is guaranteed to return some answer for 
every question posed, there is no guarantee that an answer will 
be found amongst the extracted concept-instance pairs.  When 
such a case arises, the look-up method?s answer is counted as 
incorrect. 
This difference represents a time speed up of three 
orders of magnitude. 
There are a number of reasons why the state of 
the art system performed poorly compared to the 
simple extraction method.  First, as mentioned 
above, the lack of newspaper text on the web 
means that TextMap did not have access to the 
same information-rich resources that the extraction 
method exploited.  Further, the simplicity of the 
extraction method makes it more resilient to the 
noise (such as parser error) that is introduced by 
the many modules employed by TextMap.  And 
finally, because it is designed to answer any type 
of question, not just ?Who is?? questions, 
TextMap is not as precise as the extraction 
technique.  This is due to both its lack of tailor 
made patterns for specific question types, as well 
as, its inability to filter those patterns with high 
precision. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
7 
                                                          
Figure 3.  Evaluation results for the state of the art 
system and look-up method using extracted concept-
instance pairs on 100 ?Who is ?? questions collected 
online.  Results are grouped by category: partially 
correct, entirely correct, and entirely incorrect. 
Discussion and Future Work 
The information repository approach to Question 
Answering offers possibilities of increased speed 
and accuracy for current systems.  By collecting 
information offline, on text not readily available to 
search engines, and storing it to be accessible 
quickly and easily, Question Answering systems 
will be able to operate more efficiently and more 
effectively.   
In order to achieve real-time, accurate 
Question Answering, repositories of data much 
larger than that described here must be generated.  
We imagine huge data warehouses where each 
repository contains relations, such as birthplace-of, 
location-of, creator-of, etc.  These repositories 
would be automatically filled by a system that 
continuously watches various online news sources, 
scouring them for useful information.   
Such a system would have a large library of 
extraction patterns for many different types of 
relations.  These patterns could be manually 
generated, such as the ones described here, or 
learned from text, as described in Ravichandran 
and Hovy (2002).  Each pattern would have a 
machine-learned filter in order to insure high 
precision output relations.  These relations would 
then be stored in repositories that could be quickly 
and easily searched to answer user queries. 7   
In this way, we envision a system similar to 
(Lin et al, 2002).  However, instead of relying on 
costly structured databases and pain stakingly 
generated wrappers, repositories are automatically 
filled with information from many different 
patterns.  Access to these repositories does not 
require wrapper generation, because all 
information is stored in easily accessible natural 
language text.  The key here is the use of learned 
filters which insure that the information in the 
repository is clean and reliable. 
Performance on a Question 
Answering Task
10
15
20
25
30
35
40
45
50
Partial Correct Incorrect
%
 C
or
re
ct
State of the Art System Extraction System
Such a system is not meant to be complete by 
itself, however.  Many aspects of Question 
Answering remain to be addressed.  For example, 
question classification is necessary in order to 
determine which repositories (i.e., which relations) 
are associated with which questions.   
Further, many question types require post 
processing.  Even for ?Who is ?? questions 
multiple answers need to be integrated before final 
output is presented.  An interesting corollary to 
using this offline strategy is that each extracted 
instance has with it a frequency distribution of 
associated concepts (e.g., for ?Bill Clinton?: 105 
?US president?; 52 ?candidate?; 4 ?nominee?).  
This distribution can be used in conjunction with 
time/stamp information to formulate mini 
biographies as answers to ?Who is ?? questions. 
We believe that generating and maintaining 
information repositories will advance many aspects 
of Natural Language Processing.  Their uses in 
7 An important addition to this system would be the inclusion 
of time/date stamp and data source information.  For, while 
?George Bush? is ?president? today, he will not be forever. 
data driven Question Answering are clear.  In 
addition, concept-instance pairs could be useful in 
disambiguating references in text, which is a 
challenge in Machine Translation and Text 
Summarization.   
In order to facilitate further research, we have 
made the extracted pairs described here publicly 
available at www.isi.edu/~fleisch/instances.txt.gz.  
In order to maximize the utility of these pairs, we 
are integrating them into an Ontology, where they 
can be more efficiently stored, cross-correlated, 
and shared.   
Acknowledgments 
 
The authors would like to thank Miruna Ticrea for 
her valuable help with training the classifier.  We 
would also like to thank Andrew Philpot for his work 
on integrating instances into the Omega Ontology, 
and Daniel Marcu whose comments and ideas were 
invaluable. 
References 
 
Michelle Banko, Eric Brill. 2001. Scaling to Very Very 
Large Corpora for Natural Language Disambiguation.  
Proceedings of the Association for Computational 
Linguistics, Toulouse, France.  
 
Matthew Berland and Eugene Charniak. 1999. Finding 
Parts in Very Large Corpora.  Proceedings of the 37th 
Annual Meeting of the Association for Computational 
Linguistics. College Park, Maryland. 
 
Eric Brill. 1994. Some advances in rule based part of speech 
tagging.  Proc. of AAAI.  Seattle, Washington. 
 
Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais, 
and Andrew Ng. 2001. Data-Intensive Question 
Answering. Proceedings of the 2001 Text REtrieval 
Conference (TREC 2001), Gaithersburg, MD. 
 
Michael Fleischman and Eduard Hovy. 2002.  Fine 
Grained Classification of Named Entities.  19th 
International Conference on Computational 
Linguistics (COLING). Taipei, Taiwan. 
 
Ulf Hermjakob, Abdessamad Echihabi, and Daniel 
Marcu. 2002.  Natural Language Based 
Reformulation Resource and Web Exploitation for 
Question Answering.  In Proceedings of the TREC-
2002 Conference, NIST. Gaithersburg, MD. 
 
Marti Hearst. 1992. Automatic Acquisition of 
Hyponyms from Large Text Corpora.  Proceedings of 
the Fourteenth International Conference on 
Computational Linguistics, Nantes, France. 
 
Jimmy Lin, Aaron Fernandes, Boris Katz, Gregory 
Marton, and Stefanie Tellex. 2002. Extracting 
Answers from the Web Using Data Annotation and 
Data Mining Techniques.  Proceedings of the 2002 
Text REtrieval Conference (TREC 2002) 
Gaithersburg, MD.  
 
Gideon S. Mann. 2002.  Fine-Grained Proper Noun 
Ontologies for Question Answering.  SemaNet'02: 
Building and Using Semantic Networks, Taipei, 
Taiwan. 
 
Deepak Ravichandran and Eduard Hovy. 2002.  
Learning surface text patterns for a Question 
Answering system.  Proceedings of the 40th ACL 
conference. Philadelphia, PA.  
 
I. Witten and E. Frank. 1999.  Data Mining: Practical 
Machine Learning Tools and Techniques with JAVA 
implementations.  Morgan Kaufmann, San Francisco, 
CA. 
 
 
 
 
 
 
iNeATS: Interactive Multi-Document Summarization
Anton Leuski, Chin-Yew Lin, Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way, Suite 1001
Marina Del Rey, CA 90292-6695
{leuski,cyl,hovy}@isi.edu
Abstract
We describe iNeATS ? an interactive
multi-document summarization system
that integrates a state-of-the-art summa-
rization engine with an advanced user in-
terface. Three main goals of the sys-
tem are: (1) provide a user with control
over the summarization process, (2) sup-
port exploration of the document set with
the summary as the staring point, and (3)
combine text summaries with alternative
presentations such as a map-based visual-
ization of documents.
1 Introduction
The goal of a good document summary is to provide
a user with a presentation of the substance of a body
of material in a coherent and concise form. Ideally, a
summary would contain only the ?right? amount of
the interesting information and it would omit all the
redundant and ?uninteresting? material. The quality
of the summary depends strongly on users? present
need ? a summary that focuses on one of several top-
ics contained in the material may prove to be either
very useful or completely useless depending on what
users? interests are.
An automatic multi-document summarization
system generally works by extracting relevant sen-
tences from the documents and arranging them in a
coherent order (McKeown et al, 2001; Over, 2001).
The system has to make decisions on the summary?s
size, redundancy, and focus. Any of these deci-
sions may have a significant impact on the quality
of the output. We believe a system that directly in-
volves the user in the summary generation process
and adapts to her input will produce better sum-
maries. Additionally, it has been shown that users
are more satisfied with systems that visualize their
decisions and give the user a sense of control over
the process (Koenemann and Belkin, 1996).
We see three ways in which interactivity and
visualization can be incorporated into the multi-
document summarization process:
1. give the user direct control over the summariza-
tion parameters such as size, redundancy, and
focus of the summaries.
2. support rapid browsing of the document set us-
ing the summary as the starting point and com-
bining the multi-document summary with sum-
maries for individual documents.
3. incorporate alternative formats for organizing
and displaying the summary, e.g., a set of news
stories can be summarized by placing the sto-
ries on a world map based on the locations of
the events described in the stories.
In this paper we describe iNeATS (Interactive
NExt generation Text Summarization) which ad-
dresses these three directions. The iNeATS system
is built on top of the NeATS multi-document sum-
marization system. In the following section we give
a brief overview of the NeATS system and in Sec-
tion 3 describe the interactive version.
2 NeATS
NeATS (Lin and Hovy, 2002) is an extraction-
based multi-document summarization system. It is
among the top two performers in DUC 2001 and
2002 (Over, 2001). It consists of three main com-
ponents:
Content Selection The goal of content selection is
to identify important concepts mentioned in
a document collection. NeATS computes the
likelihood ratio (Dunning, 1993) to identify key
concepts in unigrams, bigrams, and trigrams
and clusters these concepts in order to identify
major subtopics within the main topic. Each
sentence in the document set is then ranked, us-
ing the key concept structures. These n-gram
key concepts are called topic signatures.
Content Filtering NeATS uses three different fil-
ters: sentence position, stigma words, and re-
dundancy filter. Sentence position has been
used as a good important content filter since
the late 60s (Edmundson, 1969). NeATS ap-
plies a simple sentence filter that only retains
the N lead sentences. Some sentences start
with conjunctions, quotation marks, pronouns,
and the verb ?say? and its derivatives. These
stigma words usually cause discontinuities in
summaries. The system reduces the scores of
these sentences to demote their ranks and avoid
including them in summaries of small sizes. To
address the redundancy problem, NeATS uses a
simplified version of CMU?s MMR (Goldstein
et al, 1999) algorithm. A sentence is added to
the summary if and only if its content has less
than X percent overlap with the summary.
Content Presentation To ensure coherence of the
summary, NeATS pairs each sentence with an
introduction sentence. It then outputs the final
sentences in their chronological order.
3 Interactive Summarization
Figure 1 shows a screenshot of the iNeATS system.
We divide the screen into three parts corresponding
to the three directions outlined in Section 1. The
control panel displays the summarization parame-
ters on the left side of the screen. The document
panel shows the document text on the right side. The
summary panel presents the summaries in the mid-
dle of the screen.
3.1 Controlling Summarization Process
The top of the control panel provides the user with
control over the summarization process. The first set
of widgets contains controls for the summary size,
sentence position, and redundancy filters. The sec-
ond row of parameters displays the set of topic sig-
natures identified by the iNeATS engine. The se-
lected subset of the topic signatures defines the con-
tent focus for the summary. If the user enters a new
value for one of the parameters or selects a different
subset of the topic signatures, iNeATS immediately
regenerates and redisplays the summary text in the
top portion of the summary panel.
3.2 Browsing Document Set
iNeATS facilitates browsing of the document set by
providing (1) an overview of the documents, (2)
linking the sentences in the summary to the original
documents, and (3) using sentence zooming to high-
light the most relevant sentences in the documents.
The bottom part of the control panel is occupied
by the document thumbnails. The documents are ar-
ranged in chronological order and each document is
assigned a unique color to paint the text background
for the document. The same color is used to draw
the document thumbnail in the control panel, to fill
up the text background in the document panel, and to
paint the background of those sentences in the sum-
mary that were collected from the document. For
example, the screenshot shows that a user selected
the second document which was assigned the or-
ange color. The document panel displays the doc-
ument text on orange background. iNeATS selected
the first two summary sentences from this document,
so both sentences are shown in the summary panel
with orange background.
The sentences in the summary are linked to the
original documents in two ways. First, the docu-
ment can be identified by the color of the sentence.
Second, each sentence is a hyperlink to the docu-
ment ? if the user moves the mouse over a sentence,
the sentence is underlined in the summary and high-
lighted in the document text. For example, the first
sentence of the summary is the document sentence
Figure 1: Screenshot of the iNeATS system.
highlighted in the document panel. If the user clicks
on the sentence, iNeATS brings the source document
into the document panel and scrolls the window to
make the sentence visible.
The relevant parts of the documents are illumi-
nated using the technique that we call sentence
zooming. We make the text color intensity of each
sentence proportional to the relevance score com-
puted by the iNeATS engine and a zooming parame-
ter which can be controlled by the user with a slider
widget at the top of the document panel. The higher
the sentence score, the darker the text is. Conversely,
sentences that blend into the background have a very
low sentence score. The zooming parameter con-
trols the proportion of the top ranked sentences vis-
ible on the screen at each moment. This zooming
affects both the full-text and the thumbnail docu-
ment presentations. Combining the sentence zoom-
ing with the document set overview, the user can
quickly see which document contains most of the
relevant material and where approximately in the
document this material is placed.
The document panel in Figure 1 shows sentences
that achieve 50% on the sentence score scale. We see
that the first half of the document contains two black
sentences: the first sentence that starts with ?US In-
surers...?, the other starts with ?President George...?.
Both sentences have a very high score and they were
selected for the summary. Note, that the very first
sentence in the document is the headline and it is not
used for summarization. Note also that the sentence
that starts with ?However,...? scored much lower
than the selected two ? its color is approximately
half diluted into the background.
There are quite a few sentences in the second part
of the document that scored relatively high. How-
ever, these sentences are below the sentence position
cutoff so they do not appear in the summary. We il-
lustrate this by rendering such sentences in slanted
style.
3.3 Alternative Summaries
The bottom part of the summary panel is occupied
by the map-based visualization. We use BBN?s
IdentiFinder (Bikel et al, 1997) to detect the names
of geographic locations in the document set. We
then select the most frequently used location names
and place them on world map. Each location is iden-
tified by a black dot followed by a frequency chart
and the location name. The frequency chart is a bar
chart where each bar corresponds to a document.
The bar is painted using the document color and the
length of the bar is proportional to the number of
times the location name is used in the document.
The document set we used in our example de-
scribes the progress of the hurricane Andrew and its
effect on Florida, Louisiana, and Texas. Note that
the source documents and therefore the bars in the
chart are arranged in the chronological order. The
name ?Miami? appears first in the second document,
?New Orleans? in the third document, and ?Texas? is
prominent in the last two documents. We can make
some conclusions on the hurricane?s path through
the region ? it traveled from south-east and made its
landing somewhere in Louisiana and Texas.
4 Discussion
The iNeATS system is implemented in Java. It uses
the NeATS engine implemented in Perl and C. It
runs on any platform that supports these environ-
ments. We are currently working on making the sys-
tem available on our web site.
We plan to extend the system by adding temporal
visualization that places the documents on a timeline
based on the date and time values extracted from the
text.
We plan to conduct a user-based evaluation of the
system to compare users? satisfaction with both the
automatically generated summaries and summaries
produced by iNeATS.
References
Daniel M. Bikel, Scott Miller, Richard Schwartz, and
Ralph Weischedel. 1997. Nymble: a high-
performance learning name-finder. In Proceedings of
ANLP-97, pages 194?201.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61?74.
H. P. Edmundson. 1969. New methods in automatic ex-
traction. Journal of the ACM, 16(2):264?285.
Jade Goldstein, Mark Kantrowitz, Vibhu O. Mittal, and
Jaime G. Carbonell. 1999. Summarizing text docu-
ments: Sentence selection and evaluation metrics. In
Research and Development in Information Retrieval,
pages 121?128.
Jurgen Koenemann and Nicholas J. Belkin. 1996. A case
for interaction: A study of interactive information re-
trieval behavior and effectivness. In Proceedings of
ACM SIGCHI Conference on Human Factors in Com-
puting Systems, pages 205?212, Vancouver, British
Columbia, Canada.
Chin-Yew Lin and Eduard Hovy. 2002. From single
to multi-document summarization: a prototype sys-
tem and it evaluation. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics (ACL-02), Philadelphia, PA, USA.
Kathleen R. McKeown, Regina Barzilay, David Evans,
Vasileios Hatzivassiloglou, Barry Schiffman, and Si-
mone Teufel. 2001. Columbia multi-document sum-
marization: Approach and evaluation. In Proceed-
ings of the Workshop on Text Summarization, ACM SI-
GIR Conference 2001. DARPA/NIST, Document Un-
derstanding Conference.
Paul Over. 2001. Introduction to duc-2001: an intrin-
sic evaluation of generic news text summarization sys-
tems. In Proceedings of the Workshop on Text Summa-
rization, ACM SIGIR Conference 2001. DARPA/NIST,
Document Understanding Conference.
Proceedings of the 43rd Annual Meeting of the ACL, pages 298?305,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Digesting Virtual ?Geek? Culture:
The Summarization of Technical Internet Relay Chats
Liang Zhou and Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{liangz, hovy} @isi.edu
Abstract
This paper describes a summarization
system for technical chats and emails on
the Linux kernel. To reflect the complex-
ity and sophistication of the discussions,
they are clustered according to subtopic
structure on the sub-message level, and
immediate responding pairs are identified
through machine learning methods. A re-
sulting summary consists of one or more
mini-summaries, each on a subtopic from
the discussion.
1 Introduction
The availability of many chat forums reflects the
formation of globally dispersed virtual communi-
ties. From them we select the very active and
growing movement of Open Source Software
(OSS) development. Working together in a virtual
community in non-collocated environments, OSS
developers communicate and collaborate using a
wide range of web-based tools including Internet
Relay Chat (IRC), electronic mailing lists, and
more (Elliott and Scacchi, 2004). In contrast to
conventional instant message chats, IRCs convey
engaging and focused discussions on collaborative
software development. Even though all OSS par-
ticipants are technically savvy individually, sum-
maries of IRC content are necessary within a
virtual organization both as a resource and an or-
ganizational memory of activities (Ackerman and
Halverson, 2000). They are regularly produced
manually by volunteers. These summaries can be
used for analyzing the impact of virtual social in-
teractions and virtual organizational culture on
software/product development.
The emergence of email thread discussions and
chat logs as a major information source has
prompted increased interest in thread summariza-
tion within the Natural Language Processing
(NLP) community. One might assume a smooth
transition from text-based summarization to email
and chat-based summarizations. However, chat
falls in the genre of correspondence, which re-
quires dialogue and conversation analysis. This
property makes summarization in this area even
more difficult than traditional summarization. In
particular, topic ?drift? occurs more radically than
in written genres, and interpersonal and pragmatic
content appears more frequently. Questions about
the content and overall organization of the sum-
mary must be addressed in a more thorough way
for chat and other dialogue summarization sys-
tems.
In this paper we present a new system that clus-
ters sub-message segments from correspondences
according to topic, identifies the sub-message
segment containing the leading issue within the
topic, finds immediate responses from other par-
ticipants, and consequently produces a summary
for the entire IRC. Other constructions are possi-
ble. One of the two baseline systems described in
this paper uses the timeline and dialogue structure
to select summary content, and is quite effective.
We use the term chat loosely in this paper. Input
IRCs for our system is a mixture of chats and
298
emails that are indistinguishable in format ob-
served from the downloaded corpus (Section 3).
In the following sections, we summarize previ-
ous work, describe the email/chat data, intra-
message clustering and summary extraction proc-
ess, and discuss the results and future work.
2 Previous and Related Work
There are at least two ways of organizing dialogue
summaries: by dialogue structure and by topic.
Newman and Blitzer (2002) describe methods
for summarizing archived newsgroup conversa-
tions by clustering messages into subtopic groups
and extracting top-ranked sentences per subtopic
group based on the intrinsic scores of position in
the cluster and lexical centrality. Due to the techni-
cal nature of our working corpus, we had to handle
intra-message topic shifts, in which the author of a
message raises or responds to multiple issues in the
same message. This requires that our clustering
component be not message-based but sub-
message-based.
Lam et al (2002) employ an existing summar-
izer for single documents using preprocessed email
messages and context information from previous
emails in the thread.
Rambow et al (2004) show that sentence ex-
traction techniques are applicable to summarizing
email threads, but only with added email-specific
features. Wan and McKeown (2004) introduce a
system that creates overview summaries for ongo-
ing decision-making email exchanges by first de-
tecting the issue being discussed and then
extracting the response to the issue. Both systems
use a corpus that, on average, contains 190 words
and 3.25 messages per thread, much shorter than
the ones in our collection.
Galley et al (2004) describe a system that iden-
tifies agreement and disagreement occurring in
human-to-human multi-party conversations. They
utilize an important concept from conversational
analysis, adjacent pairs (AP), which consists of
initiating and responding utterances from different
speakers. Identifying APs is also required by our
research to find correspondences from different
chat participants.
In automatic summarization of spoken dia-
logues, Zechner (2001) presents an approach to
obtain extractive summaries for multi-party dia-
logues in unrestricted domains by addressing in-
trinsic issues specific to speech transcripts. Auto-
matic question detection is also deemed important
in this work. A decision-tree classifier was trained
on question-triggering words to detect questions
among speech acts (sentences). A search heuristic
procedure then finds the corresponding answers.
Ries (2001) shows how to use keyword repetition,
speaker initiative and speaking style to achieve
topical segmentation of spontaneous dialogues.
3    Technical Internet Relay Chats
GNUe, a meta-project of the GNU project
1
?one of
the most famous free/open source software pro-
jects?is the case study used in (Elliott and Scacchi,
2004) in support of the claim that, even in virtual
organizations, there is still the need for successful
conflict management in order to maintain order
and stability.
The GNUe IRC archive is uniquely suited for
our experimental purpose because each IRC chat
log has a companion summary digest written by
project participants as part of their contribution to
the community. This manual summary constitutes
gold-standard data for evaluation.
3.1 Kernel Traffic
2
Kernel Traffic is a collection of summary digests
of discussions on GNUe development. Each digest
summarizes IRC logs and/or email messages (later
referred to as chat logs) for a period of up to two
weeks. A nice feature is that direct quotes and
hyperlinks are part of the summary. Each digest is
an extractive overview of facts, plus the author?s
dramatic and humorous interpretations.
3.2 Corpus Download
The complete Linux Kernel Archive (LKA) con-
sists of two separate downloads. The Kernel Traf-
fic (summary digests) are in XML format and were
downloaded by crawling the Kernel Traffic site.
The Linux Kernel Archives (individual IRC chat
logs) are downloaded from the archive site. We
matched the summaries with their respective chat
logs based on subject line and publication dates.
3.3 Observation on Chat Logs
                                                           
1
 http://www.gnu.org
2
 http://kt.hoser.ca/kernel-traffic/index.html
299
Upon initial examination of the chat logs, we
found that many conventional assumptions about
chats in general do not apply. For example, in most
instant-message chats, each exchange usually con-
sists of a small number of words in several sen-
tences. Due to the technical nature of GNUe, half
of the chat logs contain in-depth discussions with
lengthy messages. One message might ask and an-
swer several questions, discuss many topics in de-
tail, and make further comments. This property,
which we call subtopic structure, is an important
difference from informal chat/interpersonal banter.
Figure 1 shows the subtopic structure and relation
of the first 4 messages from a chat log, produced
manually. Each message is represented horizon-
tally; the vertical arrows show where participants
responded to each other. Visual inspection reveals
in this example there are three distinctive clusters
(a more complex cluster and two smaller satellite
clusters) of discussions between participants at
sub-message level.
3.4 Observation on Summary Digests
To measure the goodness of system-produced
summaries, gold standards are used as references.
Human-written summaries usually make up the
gold standards. The Kernel Traffic (summary di-
gests) are written by Linux experts who actively
contribute to the production and discussion of the
open source projects. However, participant-
produced digests cannot be used as reference
summaries verbatim. Due to the complex structure
of the dialogue, the summary itself exhibits some
discourse structure, necessitating such reader guid-
ance phrases such as ?for the ? question,? ?on the
? subject,? ?regarding ?,? ?later in the same
thread,? etc., to direct and refocus the reader?s at-
tention. Therefore, further manual editing and par-
titioning is needed to transform a multi-topic digest
into several smaller subtopic-based gold-standard
reference summaries (see Section 6.1 for the trans-
formation).
4 Fine-grained Clustering
To model the subtopic structure of each chat mes-
sage, we apply clustering at the sub-message level.
4.1 Message Segmentation
First, we look at each message and assume that
each participant responds to an ongoing discussion
by stating his/her opinion on several topics or is-
sues that have been discussed in the current chat
log, but not necessarily in the order they were dis-
cussed. Thus, topic shifts can occur sequentially
within a message. Messages are partitioned into
multi-paragraph segments using TextTiling, which
reportedly has an overall precision of 83% and re-
call of 78% (Hearst, 1994).
4.2 Clustering
After distinguishing a set of message segments, we
cluster them. When choosing an appropriate clus-
tering method, because the number of subtopics
under discussion is unknown, we cannot make an
assumption about the total number of resulting
clusters. Thus, nonhierarchical partitioning meth-
ods cannot be used, and we must use a hierarchical
method.  These methods can be either agglomera-
tive, which begin with an unclustered data set and
perform N ? 1 pairwise joins, or divisive, which
add all objects to a single cluster, and then perform
N ? 1 divisions to create a hierarchy of smaller
clusters, where N is the total number of items to be
clustered (Frakes and Baeza-Yates, 1992).
Ward?s Method
Hierarchical agglomerative clustering methods are
commonly used and we employ Ward?s method
(Ward and Hook, 1963), in which the text segment
pair merged at each stage is the one that minimizes
the increase in total within-cluster variance.
Each cluster is represented by an L-dimensional
vector (x
i1
, x
i2
, ?, x
iL
) where each x
ik
 is the word?s
tf ? idf score. If m
i
 is the number of objects in the
cluster, the squared Euclidean distance between
two segments i and j is:
? 
d
ij
2
= (x
ik
K=1
L
?
? x
jk
)
2
Figure 1. An example of chat subtopic structure
and relation between correspondences.
300
When two segments are joined, the increase in
variance I
ij
 is expressed as:
? 
I
ij
=
m
i
m
j
m
i
+ m
j
d
ij
2
Number of Clusters
The process of joining clusters continues until the
combination of any two clusters would destabilize
the entire array of currently existing clusters pro-
duced from previous stages. At each stage, the two
clusters x
ik
 and x
jk
 are chosen whose combination
would cause the minimum increase in variance I
ij
,
expressed as a percentage of the variance change
from the last round. If this percentage reaches a
preset threshold, it means that the nearest two
clusters are much further from each other com-
pared to the previous round; therefore, joining of
the two represents a destabilizing change, and
should not take place.
Sub-message segments from resulting clusters
are arranged according to the sequence the original
messages were posted and the resulting subtopic
structures are similar to the one shown in Figure 1.
5 Summary Extraction
Having obtained clusters of message segments fo-
cused on subtopics, we adopt the typical summari-
zation paradigm to extract informative sentences
and segments from each cluster to produce sub-
topic-based summaries. If a chat log has n clusters,
then the corresponding summary will contain n
mini-summaries.
All message segments in a cluster are related to
the central topic, but to various degrees. Some are
answers to questions asked previously, plus further
elaborative explanations; some make suggestions
and give advice where they are requested, etc.
From careful analysis of the LKA data, we can
safely assume that for this type of conversational
interaction, the goal of the participants is to seek
help or advice and advance their current knowl-
edge on various technical subjects. This kind of
interaction can be modeled as one problem-
initiating segment and one or more corresponding
problem-solving segments. We envisage that iden-
tifying corresponding message segment pairs will
produce adequate summaries. This analysis follows
the structural organization of summaries from Ker-
nel Traffic. Other types of discussions, at least in
part, require different discourse/summary organi-
zation.
These corresponding pairs are formally intro-
duced below, and the methods we experimented
with for identifying them are described.
5.1 Adjacent Response Pairs
An important conversational analysis concept, ad-
jacent pairs (AP), is applied in our system to iden-
tify initiating and responding correspondences
from different participants in one chat log. Adja-
cent pairs are considered fundamental units of
conversational organization (Schegloff and Sacks,
1973). An adjacent pair is said to consist of two
parts that are ordered, adjacent, and produced by
different speakers (Galley et al, 2004). In our
email/chat (LKA) corpus a physically adjacent
message, following the timeline, may not directly
respond to its immediate predecessor. Discussion
participants read the current live thread and decide
what he/she would like to correspond to, not nec-
essarily in a serial fashion. With the added compli-
cation of subtopic structure (see Figure 1) the
definition of adjacency is further violated. Due to
its problematic nature, a relaxation on the adja-
cency requirement is used in extensive research in
conversational analysis (Levinson, 1983). This re-
laxed requirement is adopted in our research.
Information produced by adjacent correspon-
dences can be used to produce the subtopic-based
summary of the chat log.  As described in Section
4, each chat log is partitioned, at sub-message
level, into several subtopic clusters. We take the
message segment that appears first chronologically
in the cluster as the topic-initiating segment in an
adjacent pair. Given the initiating segment, we
need to identify one or more segments from the
same cluster that are the most direct and relevant
responses. This process can be viewed equivalently
as the informative sentence extraction process in
conventional text-based summarization.
5.2 AP Corpus and Baseline
We manually tagged 100 chat logs for adjacent
pairs. There are, on average, 11 messages per chat
log and 3 segments per message (This is consid-
erably larger than threads used in previous re-
search). Each chat log has been clustered into one
or more bags of message segments. The message
segment that appears earliest in time in a cluster
301
was marked as the initiating segment. The annota-
tors were provided with this segment and one other
segment at a time, and were asked to decide
whether the current message segment is a direct
answer to the question asked, the suggestion that
was requested, etc. in the initiating segment. There
are 1521 adjacent response pairs; 1000 were used
for training and 521 for testing.
Our baseline system selects the message seg-
ment (from a different author) immediately fol-
lowing the initiating segment. It is quite effective,
with an accuracy of 64.67%. This is reasonable
because not all adjacent responses are interrupted
by messages responding to different earlier initiat-
ing messages.
In the following sections, we describe two ma-
chine learning methods that were used to identify
the second element in an adjacent response pair
and the features used for training. We view the
problem as a binary classification problem, distin-
guishing less relevant responses from direct re-
sponses. Our approach is to assign a candidate
message segment c an appropriate response class r.
5.3 Features
Structural and durational features have been dem-
onstrated to improve performance significantly in
conversational text analysis tasks. Using them,
Galley et al (2004) report an 8% increase in
speaker identification. Zechner (2001) reports ex-
cellent results (F > .94) for inter-turn sentence
boundary detection when recording the length of
pause between utterances.  In our corpus, dura-
tional information is nonexistent because chats and
emails were mixed and no exact time recordings
beside dates were reported. So we rely solely on
structural and lexical features.
For structural features, we count the number of
messages between the initiating message segment
and the responding message segment. Lexical fea-
tures are listed in Table 1. The tech words are the
words that are uncommon in conventional litera-
ture and unique to Linux discussions.
5.4 Maximum Entropy
Maximum entropy has been proven to be an ef-
fective method in various natural language proc-
essing applications (Berger et al, 1996). For
training and testing, we used YASMET
3
.  To est i-
mate P(r | c) in the exponential form, we have:
? 
P?(r | c) = 1
Z?(c)  exp( ?i,r
i
?
f
i,r
(c,r))
where Z?(c) is a normalizing constant and the fea-
ture function for feature f
i
 and response class r is
defined as:
? 
f
i,r
(c,
? 
r ) =
1, if f
i
> 0 and 
? 
r = r
0, otherwise            
  
? 
? 
? 
.?
i,r
 is the feature-weight parameter for feature f
i 
and
response class r. Then, to determine the best class r
for the candidate message segment c, we have:
? 
r
*
= arg max
r
P(r | c)   .
5.5 Support Vector Machine
Support vector machines (SVMs) have been shown
to outperform other existing methods (na?ve Bayes,
k-NN, and decision trees) in text categorization
(Joachims, 1998). Their advantages are robustness
and the elimination of the need for feature selec-
tion and parameter tuning. SVMs find the hyper-
plane that separates the positive and negative
training examples with maximum margin. Finding
this hyperplane can be translated into an optimiza-
tion problem of finding a set of coefficients ?
i
*
 of
the weight vector 
  
? 
r 
w for document d
i
 of class y
i
 ?
{+1 , ?1}:
  
? 
r 
w = ?
i
*
i
?
y
i
r 
d 
i
,    ?
i
> 0     .
Testing data are classified depending on the side
of the hyperplane they fall on. We used the
LIBSVM
4
 package for training and testing.
                                                           
3
 http://www.fjoch.com/YASMET.html
4
 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
Feature sets baseline MaxEnt SVM
64.67%
Structural 61.22% 71.79%
Lexical 62.24% 72.22%
Structural + Lexical 72.61% 72.79%
? number of overlapping words
? number of overlapping content words
? ratio of overlapping words
? ratio of overlapping content words
? number of overlapping tech words
Table 1. Lexical features.
Table 2. Accuracy on identifying APs.
302
5.6 Results
Entries in Table 2 show the accuracies achieved
using machine learning models and feature sets.
5.7 Summary Generation
After responding message segments are identified,
we couple them with their respective initiating
segment to form a mini-summary based on their
subtopic. Each initializing segment has zero or
more responding segments. We also observed zero
response in human-written summaries where par-
ticipants initiated some question or concern, but
others failed to follow up on the discussion. The
AP process is repeated for each cluster created
previously. One or more subtopic-based mini-
summaries make up one final summary for each
chat log. Figure 2 shows an example. For longer
chat logs, the length of the final summary is arbi-
trarily averaged at 35% of the original.
6 Summary Evaluation
To evaluate the goodness of the system-produced
summaries, a set of reference summaries is used
for comparison. In this section, we describe the
manual procedure used to produce the reference
summaries, and the performances of our system
and two baseline systems.
6.1 Reference Summaries
Kernel Traffic digests are participant-written
summaries of the chat logs. Each digest mixes the
summary writer?s own narrative comments with
direct quotes (citing the authors) from the chat log.
As observed in Section 3.4, subtopics are inter-
mingled in each digest. Authors use key phrases to
link the contents of each subtopic throughout texts.
In Figure 3, we show an example of such a digest.
Discussion participants? names are in italics and
subtopics are in bold. In this example, the conver-
sation was started by Benjamin Reed with two
questions: 1) asking for conventions for writing
/proc drivers, and 2) asking about the status of
sysctl. The summary writer indicated that Linus
Torvalds replied to both questions and used the
phrase ?for the ? question, he added?? to high-
light the answer to the second question. As the di-
Subtopic 1:
Benjamin Reed: I wrote a wireless ethernet driver a
while ago... Are driver writers recommended to use
that over extending /proc or is it deprecated?
Linus Torvalds: Syscyl is deprecated. It?s useful in one
way only ...
Subtopic 2:
Benjamin Reed: I am a bit uncomfortable ... wondering
for a while if there are guidelines on ?
Linus Torvalds: The thing to do is to create ...
Subtopic 3:
Marcin Dalecki: Are you just blind to the never-ending
format/ compatibility/ ? problems the whole idea
behind /proc induces inherently?
Figure 2. A system-produced summary.
Benjamin Reed wrote a wireless Ethernet driver that
used /proc as its interface. But he was a little uncom-
fortable ? asked if there were any conventions he
should follow. He added, ?and finally, what?s up with
sysctl? ??
Linus Torvalds replied with: ?the thing to do is to cre-
ate a ?[program code]. The /proc/drivers/ directory is
already there, so you?d basically do something like ?
[program code].? For the sysctl question, he added
?sysctl is deprecated. ...?
Marcin Dalecki flamed Linus: ?Are you just blind to
the never-ending format/compatibility/? problems the
whole idea behind /proc  induces inherently?
?[example]?
Figure 3. An original Kernel Traffic digest.
Mini 1:
Benjamin Reed wrote a wireless Ethernet driver that
used /proc as its interface. But he was a little uncom-
fortable ? and asked if there were any conventions he
should follow.
Linus Torvalds replied with: the thing to do is to create
a ?[program code]. The /proc/drivers/ directory is
already there, so you?d basically do something like ?
[program code].
Marcin Dalecki flamed Linus: Are you just blind to the
never-ending format/ compatibility/ ? problems the
whole idea behind /proc  induces inherently?
?[example]
Mini 2:
Benjamin Reed: and finally, what?s up with sysctl? ...
Linus Torvalds replied: sysctl is deprecated. ...
Figure 4. A reference summary reproduced
from a summary digest.
303
gest goes on, Marcin Dalecki only responded to the
first question with his excited commentary.
Since our system-produced summaries are sub-
topic-based and partitioned accordingly, if we use
unprocessed Kernel Traffic as references, the com-
parison would be rather complicated and would
increase the level of inconsistency in future as-
sessments.  We manually reorganized each sum-
mary digest into one or more mini-summaries by
subtopic (see Figure 4.) Examples (usually kernel
stats) and programs are reduced to ?[example]?
and ?[program code].? Quotes (originally in sepa-
rate messages but merged by the summary writer)
that contain multiple topics are segmented and the
participant?s name is inserted for each segment.
We follow clues like ?to answer ? question? to
pair up the main topics and their responses.
6.2 Summarization Results
We evaluated 10 chat logs. On average, each con-
tains approximately 50 multi-paragraph tiles (par-
titioned by TextTile) and 5 subtopics (clustered by
the method from Section 4).
A simple baseline system takes the first sentence
from each email in the sequence that they were
posted, based on the assumption that people tend to
put important information in the beginning of texts
(Position Hypothesis).
A second baseline system was built based on
constructing and analyzing the dialogue structure
of each chat log. Participants often quote portions
of previously posted messages in their responses.
These quotes link most of the messages from a
chat log. The message segment that immediately
follows the quote is automatically paired with the
quote itself and added to the summary and sorted
according to the timeline. Segments that are not
quoted in later messages are labeled as less rele-
vant and discarded. A resulting baseline summary
is an inter-connected structure of segments that
quoted and responded to one another. Figure 5 is a
shortened summary produced by this baseline for
the ongoing example.
The summary digests from Kernel Traffic
mostly consist of direct snippets from original
messages, thus making the reference summaries
extractive even after rewriting. This makes it pos-
sible to conduct an automatic evaluation. A com-
puterized procedure calculates the overlap between
reference and system-produced summary units.
Since each system-produced summary is a set of
mini-summaries based on subtopics, we also com-
pared the subtopics against those appearing in ref-
erence summaries (precision = 77.00%, recall =
74.33 %, F = 0.7566).
Recall Precision F-measure
Baseline1
30.79% 16.81% .2175
Baseline2
63.14% 36.54% .4629
Summary
52.57% 52.14% .5235
System
Topic-summ
52.57% 63.66% .5758
Table 3 shows the recall, precision, and F -
measure from the evaluation. From manual analy-
sis on the results, we notice that the original digest
writers often leave large portions of the discussion
out and focus on a few topics. We think this is be-
cause among the participants, some are Linux vet-
erans and others are novice programmers. Digest
writers recognize this difference and reflect it in
their writings, whereas our system does not. The
entry ?Topic-summ? in the table shows system-
produced summaries being compared only against
the topics discussed in the reference summaries.
6.3 Discussion
A recall of 30.79% from the simple baseline reas-
sures us the Position Hypothesis still applies in
conversational discussions. The second baseline
performs extremely well on recall, 63.14%. It
shows that quoted message segments, and thereby
derived dialogue structure, are quite indicative of
where the important information resides. Systems
built on these properties are good summarization
systems and hard-to-beat baselines. The system
described in this paper (Summary) shows an F-
measure of .5235, an improvement from .4629 of
the smart baseline. It gains from a high precision
because less relevant message segments are identi-
fied and excluded from the adjacent response pairs,
[0|0] Benjamin Reed:  ?I wrote an ? driver ? /proc
??
[0|1] Benjamin Reed: ?? /proc/ guideline ??
[0|2] Benjamin Reed: ?? syscyl ??
[1|0] Linus Torvalds responds to [0|0, 0|1, 0|2]: ?the
thing to do is ?? ?sysctl is deprecated ? ?
Figure 5. A short example from Baseline 2.
Table 3. Summary of results.
304
leaving mostly topic-oriented segments in summa-
ries. There is a slight improvement when assessing
against only those subtopics appeared in the refer-
ence summaries (Topic-summ). This shows that we
only identified clusters on their information con-
tent, not on their respective writers? experience and
reliability of knowledge.
In the original summary digests, interactions and
reactions between participants are sometimes de-
scribed. Digest writers insert terms like ?flamed?,
?surprised?, ?felt sorry?, ?excited?, etc. To analyze
social and organizational culture in a virtual envi-
ronment, we need not only information extracts
(implemented so far) but also passages that reveal
the personal aspect of the communications. We
plan to incorporate opinion identification into the
current system in the future.
7 Conclusion and Future Work
In this paper we have described a system that per-
forms intra-message topic-based summarization by
clustering message segments and classifying topic-
initiating and responding pairs. Our approach is an
initial step in developing a framework that can
eventually reflect the human interactions in virtual
environments. In future work, we need to prioritize
information according to the perceived knowl-
edgeability of each participant in the discussion, in
addition to identifying informative content and
recognizing dialogue structure. While the approach
to the detection of initiating-responding pairs is
quite effective, differentiating important and non-
important topic clusters is still unresolved and
must be explored.
References
M. S. Ackerman and C. Halverson. 2000. Reexaming
organizational memory. Communications of the
ACM, 43(1), 59?64.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?71.
M. Elliott and W. Scacchi. 2004. Free software devel-
opment: cooperation and conflict in a virtual organi-
zational culture. S. Koch (ed.), Free/Open Source
Software Development, IDEA publishing, 2004.
W. B. Frakes and R. Baeza-Yates. 1992. Information
retrieval: data structures & algorithms. Prentice Hall.
M. Galley, K. McKeown, J. Hirschberg, and E.
Shriberg. 2004. Identifying agreement and disagree-
ment in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In the
Proceedings of ACL-04.
M. A. Hearst. 1994. Multi-paragraph segmentation of
expository text. In the Proceedings of ACL 1994.
T. Joachims. 1998. Text categorization with support
vector machines: Learning with many relevant fea-
tures. In Proceedings of the ECML, pages 137?142.
D. Lam and S. L. Rohall. 2002. Exploiting e-mail
structure to improve summarization. Technical Paper
at IBM Watson Research Center #20?02.
S. Levinson. 1983. Pragmatics. Cambridge University
Press.
P. Newman and J. Blitzer. 2002. Summarizing archived
discussions: a beginning. In Proceedings of Intelli-
gent User Interfaces.
O. Rambow, L. Shrestha, J. Chen and C. Laurdisen.
2004. Summarizing email threads. In Proceedings of
HLT-NAACL 2004: Short Papers.
K. Ries. 2001. Segmenting conversations by topic, ini-
tiative, and style. In Proceedings of SIGIR Work-
shop: Information Retrieval Techniques for Speech
Applications 2001: 51?66.
E. A. Schegloff and H. Sacks. 1973. Opening up clos-
ings. Semiotica, 7-4:289?327.
S. Wan and K. McKeown. 2004. Generating overview
summaries of ongoing email thread discussions. In
Proceedings of COLING 2004.
J. H. Ward Jr. and M. E. Hook. 1963. Application of an
hierarchical grouping procedure to a problem of
grouping profiles. Educational and Psychological
Measurement, 23, 69?81.
K. Zechner. 2001. Automatic generation of concise
summaries of spoken dialogues in unrestricted do-
mains. In Proceedings of SIGIR 2001.
305
Proceedings of the 43rd Annual Meeting of the ACL, pages 622?629,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Randomized Algorithms and NLP: Using Locality Sensitive Hash Function
for High Speed Noun Clustering
Deepak Ravichandran, Patrick Pantel, and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way
Marina del Rey, CA 90292.
{ravichan, pantel, hovy}@ISI.EDU
Abstract
In this paper, we explore the power of
randomized algorithm to address the chal-
lenge of working with very large amounts
of data. We apply these algorithms to gen-
erate noun similarity lists from 70 million
pages. We reduce the running time from
quadratic to practically linear in the num-
ber of elements to be computed.
1 Introduction
In the last decade, the field of Natural Language Pro-
cessing (NLP), has seen a surge in the use of cor-
pus motivated techniques. Several NLP systems are
modeled based on empirical data and have had vary-
ing degrees of success. Of late, however, corpus-
based techniques seem to have reached a plateau
in performance. Three possible areas for future re-
search investigation to overcoming this plateau in-
clude:
1. Working with large amounts of data (Banko and
Brill, 2001)
2. Improving semi-supervised and unsupervised al-
gorithms.
3. Using more sophisticated feature functions.
The above listing may not be exhaustive, but it is
probably not a bad bet to work in one of the above
directions. In this paper, we investigate the first two
avenues. Handling terabytes of data requires more
efficient algorithms than are currently used in NLP.
We propose a web scalable solution to clustering
nouns, which employs randomized algorithms. In
doing so, we are going to explore the literature and
techniques of randomized algorithms. All cluster-
ing algorithms make use of some distance similar-
ity (e.g., cosine similarity) to measure pair wise dis-
tance between sets of vectors. Assume that we are
given n points to cluster with a maximum of k fea-
tures. Calculating the full similarity matrix would
take time complexity n2k. With large amounts of
data, say n in the order of millions or even billions,
having an n2k algorithm would be very infeasible.
To be scalable, we ideally want our algorithm to be
proportional to nk.
Fortunately, we can borrow some ideas from the
Math and Theoretical Computer Science community
to tackle this problem. The crux of our solution lies
in defining Locality Sensitive Hash (LSH) functions.
LSH functions involve the creation of short signa-
tures (fingerprints) for each vector in space such that
those vectors that are closer to each other are more
likely to have similar fingerprints. LSH functions
are generally based on randomized algorithms and
are probabilistic. We present LSH algorithms that
can help reduce the time complexity of calculating
our distance similarity atrix to nk.
Rabin (1981) proposed the use of hash func-
tions from random irreducible polynomials to cre-
ate short fingerprint representations for very large
strings. These hash function had the nice property
that the fingerprint of two identical strings had the
same fingerprints, while dissimilar strings had dif-
ferent fingerprints with a very small probability of
collision. Broder (1997) first introduced LSH. He
proposed the use of Min-wise independent functions
to create fingerprints that preserved the Jaccard sim-
622
ilarity between every pair of vectors. These tech-
niques are used today, for example, to eliminate du-
plicate web pages. Charikar (2002) proposed the
use of random hyperplanes to generate an LSH func-
tion that preserves the cosine similarity between ev-
ery pair of vectors. Interestingly, cosine similarity is
widely used in NLP for various applications such as
clustering.
In this paper, we perform high speed similarity
list creation for nouns collected from a huge web
corpus. We linearize this step by using the LSH
proposed by Charikar (2002). This reduction in
complexity of similarity computation makes it pos-
sible to address vastly larger datasets, at the cost,
as shown in Section 5, of only little reduction in
accuracy. In our experiments, we generate a simi-
larity list for each noun extracted from 70 million
page web corpus. Although the NLP community
has begun experimenting with the web, we know
of no work in published literature that has applied
complex language analysis beyond IR and simple
surface-level pattern matching.
2 Theory
The core theory behind the implementation of fast
cosine similarity calculation can be divided into two
parts: 1. Developing LSH functions to create sig-
natures; 2. Using fast search algorithm to find near-
est neighbors. We describe these two components in
greater detail in the next subsections.
2.1 LSH Function Preserving Cosine Similarity
We first begin with the formal definition of cosine
similarity.
Definition: Let u and v be two vectors in a k
dimensional hyperplane. Cosine similarity is de-
fined as the cosine of the angle between them:
cos(?(u, v)). We can calculate cos(?(u, v)) by the
following formula:
cos(?(u, v)) =
|u.v|
|u||v|
(1)
Here ?(u, v) is the angle between the vectors u
and v measured in radians. |u.v| is the scalar (dot)
product of u and v, and |u| and |v| represent the
length of vectors u and v respectively.
The LSH function for cosine similarity as pro-
posed by Charikar (2002) is given by the following
theorem:
Theorem: Suppose we are given a collection of
vectors in a k dimensional vector space (as written as
Rk). Choose a family of hash functions as follows:
Generate a spherically symmetric random vector r
of unit length from this k dimensional space. We
define a hash function, hr, as:
hr(u) =
{
1 : r.u ? 0
0 : r.u < 0
(2)
Then for vectors u and v,
Pr[hr(u) = hr(v)] = 1?
?(u, v)
pi
(3)
Proof of the above theorem is given by Goemans
and Williamson (1995). We rewrite the proof here
for clarity. The above theorem states that the prob-
ability that a random hyperplane separates two vec-
tors is directly proportional to the angle between the
two vectors (i,e., ?(u, v)). By symmetry, we have
Pr[hr(u) 6= hr(v)] = 2Pr[u.r ? 0, v.r < 0]. This
corresponds to the intersection of two half spaces,
the dihedral angle between which is ?. Thus, we
have Pr[u.r ? 0, v.r < 0] = ?(u, v)/2pi. Proceed-
ing we have Pr[hr(u) 6= hr(v)] = ?(u, v)/pi and
Pr[hr(u) = hr(v)] = 1 ? ?(u, v)/pi. This com-
pletes the proof.
Hence from equation 3 we have,
cos(?(u, v)) = cos((1? Pr[hr(u) = hr(v)])pi)
(4)
This equation gives us an alternate method for
finding cosine similarity. Note that the above equa-
tion is probabilistic in nature. Hence, we generate a
large (d) number of random vectors to achieve the
process. Having calculated hr(u) with d random
vectors for each of the vectors u, we apply equation
4 to find the cosine distance between two vectors.
As we generate more number of random vectors, we
can estimate the cosine similarity between two vec-
tors more accurately. However, in practice, the num-
ber (d) of random vectors required is highly domain
dependent, i.e., it depends on the value of the total
number of vectors (n), features (k) and the way the
vectors are distributed. Using d random vectors, we
623
can represent each vector by a bit stream of length
d.
Carefully looking at equation 4, we can ob-
serve that Pr[hr(u) = hr(v)] = 1 ?
(hamming distance)/d1 . Thus, the above theo-
rem, converts the problem of finding cosine distance
between two vectors to the problem of finding ham-
ming distance between their bit streams (as given by
equation 4). Finding hamming distance between two
bit streams is faster and highly memory efficient.
Also worth noting is that this step could be consid-
ered as dimensionality reduction wherein we reduce
a vector in k dimensions to that of d bits while still
preserving the cosine distance between them.
2.2 Fast Search Algorithm
To calculate the fast hamming distance, we use the
search algorithm PLEB (Point Location in Equal
Balls) first proposed by Indyk and Motwani (1998).
This algorithm was further improved by Charikar
(2002). This algorithm involves random permuta-
tions of the bit streams and their sorting to find the
vector with the closest hamming distance. The algo-
rithm given in Charikar (2002) is described to find
the nearest neighbor for a given vector. We mod-
ify it so that we are able to find the top B closest
neighbor for each vector. We omit the math of this
algorithm but we sketch its procedural details in the
next section. Interested readers are further encour-
aged to read Theorem 2 from Charikar (2002) and
Section 3 from Indyk and Motwani (1998).
3 Algorithmic Implementation
In the previous section, we introduced the theory for
calculation of fast cosine similarity. We implement
it as follows:
1. Initially we are given n vectors in a huge k di-
mensional space. Our goal is to find all pairs of
vectors whose cosine similarity is greater than
a particular threshold.
2. Choose d number of (d << k) unit random
vectors {r0, r1, ......, rd} each of k dimensions.
A k dimensional unit random vector, in gen-
eral, is generated by independently sampling a
1Hamming distance is the number of bits which differ be-
tween two binary strings.
Gaussian function with mean 0 and variance 1,
k number of times. Each of the k samples is
used to assign one dimension to the random
vector. We generate a random number from
a Gaussian distribution by using Box-Muller
transformation (Box and Muller, 1958).
3. For every vector u, we determine its signature
by using the function hr(u) (as given by equa-
tion 4). We can represent the signature of vec-
tor u as: u? = {hr1(u), hr2(u), ......., hrd(u)}.
Each vector is thus represented by a set of a bit
streams of length d. Steps 2 and 3 takes O(nk)
time (We can assume d to be a constant since
d << k).
4. The previous step gives n vectors, each of them
represented by d bits. For calculation of fast
hamming distance, we take the original bit in-
dex of all vectors and randomly permute them
(see Appendix A for more details on random
permutation functions). A random permutation
can be considered as random jumbling of the
bits of each vector2. A random permutation
function can be approximated by the following
function:
pi(x) = (ax + b)mod p (5)
where, p is prime and 0 < a < p , 0 ? b < p,
and a and b are chosen at random.
We apply q different random permutation for
every vector (by choosing random values for a
and b, q number of times). Thus for every vec-
tor we have q different bit permutations for the
original bit stream.
5. For each permutation function pi, we lexico-
graphically sort the list of n vectors (whose bit
streams are permuted by the function pi) to ob-
tain a sorted list. This step takes O(nlogn)
time. (We can assume q to be a constant).
6. For each sorted list (performed after applying
the random permutation function pi), we calcu-
late the hamming distance of every vector with
2The jumbling is performed by a mapping of the bit index
as directed by the random permutation function. For a given
permutation, we reorder the bit indexes of all vectors in similar
fashion. This process could be considered as column reording
of bit vectors.
624
B of its closest neighbors in the sorted list. If
the hamming distance is below a certain prede-
termined threshold, we output the pair of vec-
tors with their cosine similarity (as calculated
by equation 4). Thus, B is the beam parameter
of the search. This step takes O(n), since we
can assume B, q, d to be a constant.
Why does the fast hamming distance algorithm
work? The intuition is that the number of bit
streams, d, for each vector is generally smaller than
the number of vectors n (ie. d << n). Thus, sort-
ing the vectors lexicographically after jumbling the
bits will likely bring vectors with lower hamming
distance closer to each other in the sorted lists.
Overall, the algorithm takes O(nk+nlogn) time.
However, for noun clustering, we generally have the
number of nouns, n, smaller than the number of fea-
tures, k. (i.e., n < k). This implies logn << k and
nlogn << nk. Hence the time complexity of our
algorithm is O(nk + nlogn) ? O(nk). This is a
huge saving from the original O(n2k) algorithm. In
the next section, we proceed to apply this technique
for generating noun similarity lists.
4 Building Noun Similarity Lists
A lot of work has been done in the NLP community
on clustering words according to their meaning in
text (Hindle, 1990; Lin, 1998). The basic intuition
is that words that are similar to each other tend to
occur in similar contexts, thus linking the semantics
of words with their lexical usage in text. One may
ask why is clustering of words necessary in the first
place? There may be several reasons for clustering,
but generally it boils down to one basic reason: if the
words that occur rarely in a corpus are found to be
distributionally similar to more frequently occurring
words, then one may be able to make better infer-
ences on rare words.
However, to unleash the real power of clustering
one has to work with large amounts of text. The
NLP community has started working on noun clus-
tering on a few gigabytes of newspaper text. But
with the rapidly growing amount of raw text avail-
able on the web, one could improve clustering per-
formance by carefully harnessing its power. A core
component of most clustering algorithms used in the
NLP community is the creation of a similarity ma-
trix. These algorithms are of complexity O(n2k),
where n is the number of unique nouns and k is the
feature set length. These algorithms are thus not
readily scalable, and limit the size of corpus man-
ageable in practice to a few gigabytes. Clustering al-
gorithms for words generally use the cosine distance
for their similarity calculation (Salton and McGill,
1983). Hence instead of using the usual naive cosine
distance calculation between every pair of words we
can use the algorithm described in Section 3 to make
noun clustering web scalable.
To test our algorithm we conduct similarity based
experiments on 2 different types of corpus: 1. Web
Corpus (70 million web pages, 138GB), 2. Newspa-
per Corpus (6 GB newspaper corpus)
4.1 Web Corpus
We set up a spider to download roughly 70 million
web pages from the Internet. Initially, we use the
links from Open Directory project3 as seed links for
our spider. Each webpage is stripped of HTML tags,
tokenized, and sentence segmented. Each docu-
ment is language identified by the software TextCat4
which implements the paper by Cavnar and Trenkle
(1994). We retain only English documents. The web
contains a lot of duplicate or near-duplicate docu-
ments. Eliminating them is critical for obtaining bet-
ter representation statistics from our collection. The
problem of identifying near duplicate documents in
linear time is not trivial. We eliminate duplicate and
near duplicate documents by using the algorithm de-
scribed by Kolcz et al (2004). This process of dupli-
cate elimination is carried out in linear time and in-
volves the creation of signatures for each document.
Signatures are designed so that duplicate and near
duplicate documents have the same signature. This
algorithm is remarkably fast and has high accuracy.
This entire process of removing non English docu-
ments and duplicate (and near-duplicate) documents
reduces our document set from 70 million web pages
to roughly 31 million web pages. This represents
roughly 138GB of uncompressed text.
We identify all the nouns in the corpus by us-
ing a noun phrase identifier. For each noun phrase,
we identify the context words surrounding it. Our
context window length is restricted to two words to
3http://www.dmoz.org/
4http://odur.let.rug.nl/?vannoord/TextCat/
625
Table 1: Corpus description
Corpus Newspaper Web
Corpus Size 6GB 138GB
Unique Nouns 65,547 655,495
Feature size 940,154 1,306,482
the left and right of each noun. We use the context
words as features of the noun vector.
4.2 Newspaper Corpus
We parse a 6 GB newspaper (TREC9 and
TREC2002 collection) corpus using the dependency
parser Minipar (Lin, 1994). We identify all nouns.
For each noun we take the grammatical context of
the noun as identified by Minipar5. We do not use
grammatical features in the web corpus since pars-
ing is generally not easily web scalable. This kind of
feature set does not seem to affect our results. Cur-
ran and Moens (2002) also report comparable results
for Minipar features and simple word based proxim-
ity features. Table 1 gives the characteristics of both
corpora. Since we use grammatical context, the fea-
ture set is considerably larger than the simple word
based proximity feature set for the newspaper cor-
pus.
4.3 Calculating Feature Vectors
Having collected all nouns and their features, we
now proceed to construct feature vectors (and
values) for nouns from both corpora using mu-
tual information (Church and Hanks, 1989). We
first construct a frequency count vector C(e) =
(ce1, ce2, ..., cek), where k is the total number of
features and cef is the frequency count of feature
f occurring in word e. Here, cef is the number
of times word e occurred in context f . We then
construct a mutual information vector MI(e) =
(mie1,mie2, ...,miek) for each word e, where mief
is the pointwise mutual information between word e
and feature f , which is defined as:
mief = log
cef
N
?n
i=1
cif
N ?
?k
j=1
cej
N
(6)
where n is the number of words and N =
5We perform this operation so that we can compare the per-
formance of our system to that of Pantel and Lin (2002).
?n
i=1
?m
j=1 cij is the total frequency count of all
features of all words.
Having thus obtained the feature representation of
each noun we can apply the algorithm described in
Section 3 to discover similarity lists. We report re-
sults in the next section for both corpora.
5 Evaluation
Evaluating clustering systems is generally consid-
ered to be quite difficult. However, we are mainly
concerned with evaluating the quality and speed of
our high speed randomized algorithm. The web cor-
pus is used to show that our framework is web-
scalable, while the newspaper corpus is used to com-
pare the output of our system with the similarity lists
output by an existing system, which are calculated
using the traditional formula as given in equation
1. For this base comparison system we use the one
built by Pantel and Lin (2002). We perform 3 kinds
of evaluation: 1. Performance of Locality Sensitive
Hash Function; 2. Performance of fast Hamming
distance search algorithm; 3. Quality of final simi-
larity lists.
5.1 Evaluation of Locality sensitive Hash
function
To perform this evaluation, we randomly choose 100
nouns (vectors) from the web collection. For each
noun, we calculate the cosine distance using the
traditional slow method (as given by equation 1),
with all other nouns in the collection. This process
creates similarity lists for each of the 100 vectors.
These similarity lists are cut off at a threshold of
0.15. These lists are considered to be the gold stan-
dard test set for our evaluation.
For the above 100 chosen vectors, we also calcu-
late the cosine similarity using the randomized ap-
proach as given by equation 4 and calculate the mean
squared error with the gold standard test set using
the following formula:
errorav =
?
?
i
(CSreal,i ? CScalc,i)
2/total
(7)
where CSreal,i and CScalc,i are the cosine simi-
larity scores calculated using the traditional (equa-
tion 1) and randomized (equation 4) technique re-
626
Table 2: Error in cosine similarity
Number of ran-
dom vectors d
Average error in
cosine similarity
Time (in hours)
1 1.0000 0.4
10 0.4432 0.5
100 0.1516 3
1000 0.0493 24
3000 0.0273 72
10000 0.0156 241
spectively. i is the index over all pairs of elements
that have CSreal,i >= 0.15
We calculate the error (errorav) for various val-
ues of d, the total number of unit random vectors r
used in the process. The results are reported in Table
26. As we generate more random vectors, the error
rate decreases. For example, generating 10 random
vectors gives us a cosine error of 0.4432 (which is a
large number since cosine similarity ranges from 0
to 1.) However, generation of more random vectors
leads to reduction in error rate as seen by the val-
ues for 1000 (0.0493) and 10000 (0.0156). But as
we generate more random vectors the time taken by
the algorithm also increases. We choose d = 3000
random vectors as our optimal (time-accuracy) cut
off. It is also very interesting to note that by using
only 3000 bits for each of the 655,495 nouns, we
are able to measure cosine similarity between every
pair of them to within an average error margin of
0.027. This algorithm is also highly memory effi-
cient since we can represent every vector by only a
few thousand bits. Also the randomization process
makes the the algorithm easily parallelizable since
each processor can independently contribute a few
bits for every vector.
5.2 Evaluation of Fast Hamming Distance
Search Algorithm
We initially obtain a list of bit streams for all the
vectors (nouns) from our web corpus using the ran-
domized algorithm described in Section 3 (Steps 1
to 3). The next step involves the calculation of ham-
ming distance. To evaluate the quality of this search
algorithm we again randomly choose 100 vectors
(nouns) from our collection. For each of these 100
vectors we manually calculate the hamming distance
6The time is calculated for running the algorithm on a single
Pentium IV processor with 4GB of memory
with all other vectors in the collection. We only re-
tain those pairs of vectors whose cosine distance (as
manually calculated) is above 0.15. This similarity
list is used as the gold standard test set for evaluating
our fast hamming search.
We then apply the fast hamming distance search
algorithm as described in Section 3. In particular, it
involves steps 3 to 6 of the algorithm. We evaluate
the hamming distance with respect to two criteria: 1.
Number of bit index random permutations functions
q; 2. Beam search parameter B.
For each vector in the test collection, we take the
top N elements from the gold standard similarity list
and calculate how many of these elements are actu-
ally discovered by the fast hamming distance algo-
rithm. We report the results in Table 3 and Table 4
with beam parameters of (B = 25) and (B = 100)
respectively. For each beam, we experiment with
various values for q, the number of random permu-
tation function used. In general, by increasing the
value for beam B and number of random permu-
tation q , the accuracy of the search algorithm in-
creases. For example in Table 4 by using a beam
B = 100 and using 1000 random bit permutations,
we are able to discover 72.8% of the elements of the
Top 100 list. However, increasing the values of q and
B also increases search time. With a beam (B) of
100 and the number of random permutations equal
to 100 (i.e., q = 1000) it takes 570 hours of process-
ing time on a single Pentium IV machine, whereas
with B = 25 and q = 1000, reduces processing time
by more than 50% to 240 hours.
We could not calculate the total time taken to
build noun similarity list using the traditional tech-
nique on the entire corpus. However, we estimate
that its time taken would be at least 50,000 hours
(and perhaps even more) with a few of Terabytes of
disk space needed. This is a very rough estimate.
The experiment was infeasible. This estimate as-
sumes the widely used reverse indexing technique,
where in one compares only those vector pairs that
have at least one feature in common.
5.3 Quality of Final Similarity Lists
For evaluating the quality of our final similarity lists,
we use the system developed by Pantel and Lin
(2002) as gold standard on a much smaller data set.
We use the same 6GB corpus that was used for train-
627
Table 3: Hamming search accuracy (Beam B = 25)
Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
25 6.1% 4.9% 4.2% 3.1% 2.4% 1.9%
50 6.1% 5.1% 4.3% 3.2% 2.5% 1.9%
100 11.3% 9.7% 8.2% 6.2% 5.7% 5.1%
500 44.3% 33.5% 30.4% 25.8% 23.0% 20.4%
1000 58.7% 50.6% 48.8% 45.0% 41.0% 37.2%
Table 4: Hamming search accuracy (Beam B = 100)
Random permutations q Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
25 9.2% 9.5% 7.9% 6.4% 5.8% 4.7%
50 15.4% 17.7% 14.6% 12.0% 10.9% 9.0%
100 27.8% 27.2% 23.5% 19.4% 17.9% 16.3%
500 73.1% 67.0% 60.7% 55.2% 53.0% 50.5%
1000 87.6% 84.4% 82.1% 78.9% 75.8% 72.8%
ing by Pantel and Lin (2002) so that the results are
comparable. We randomly choose 100 nouns and
calculate the top N elements closest to each noun in
the similarity lists using the randomized algorithm
described in Section 3. We then compare this output
to the one provided by the system of Pantel and Lin
(2002). For every noun in the top N list generated
by our system we calculate the percentage overlap
with the gold standard list. Results are reported in
Table 5. The results shows that we are able to re-
trieve roughly 70% of the gold standard similarity
list. In Table 6, we list the top 10 most similar words
for some nouns, as examples, from the web corpus.
6 Conclusion
NLP researchers have just begun leveraging the vast
amount of knowledge available on the web. By
searching IR engines for simple surface patterns,
many applications ranging from word sense disam-
biguation, question answering, and mining seman-
tic resources have already benefited. However, most
language analysis tools are too infeasible to run on
the scale of the web. A case in point is generat-
ing noun similarity lists using co-occurrence statis-
tics, which has quadratic running time on the input
size. In this paper, we solve this problem by pre-
senting a randomized algorithm that linearizes this
task and limits memory requirements. Experiments
show that our method generates cosine similarities
between pairs of nouns within a score of 0.03.
In many applications, researchers have shown that
more data equals better performance (Banko and
Brill, 2001; Curran and Moens, 2002). Moreover,
at the web-scale, we are no longer limited to a snap-
shot in time, which allows broader knowledge to be
learned and processed. Randomized algorithms pro-
vide the necessary speed and memory requirements
to tap into terascale text sources. We hope that ran-
domized algorithms will make other NLP tools fea-
sible at the terascale and we believe that many al-
gorithms will benefit from the vast coverage of our
newly created noun similarity list.
Acknowledgement
We wish to thank USC Center for High Performance
Computing and Communications (HPCC) for help-
ing us use their cluster computers.
References
Banko, M. and Brill, E. 2001. Mitigating the paucity of dat-
aproblem. In Proceedings of HLT. 2001. San Diego, CA.
Box, G. E. P. and M. E. Muller 1958. Ann. Math. Stat. 29,
610?611.
Broder, Andrei 1997. On the Resemblance and Containment of
Documents. Proceedings of the Compression and Complex-
ity of Sequences.
Cavnar, W. B. and J. M. Trenkle 1994. N-Gram-Based Text
Categorization. In Proceedings of Third Annual Symposium
on Document Analysis and Information Retrieval, Las Ve-
gas, NV, UNLV Publications/Reprographics, 161?175.
628
Table 5: Final Quality of Similarity Lists
Top 1 Top 5 Top 10 Top 25 Top 50 Top 100
Accuracy 70.7% 71.9% 72.2% 71.7% 71.2% 71.1%
Table 6: Sample Top 10 Similarity Lists
JUST DO IT computer science TSUNAMI Louis Vuitton PILATES
HAVE A NICE DAY mechanical engineering tidal wave PRADA Tai Chi
FAIR AND BALANCED electrical engineering LANDSLIDE Fendi Cardio
POWER TO THE PEOPLE chemical engineering EARTHQUAKE Kate Spade SHIATSU
NEVER AGAIN Civil Engineering volcanic eruption VUITTON Calisthenics
NO BLOOD FOR OIL ECONOMICS HAILSTORM BURBERRY Ayurveda
KINGDOM OF HEAVEN ENGINEERING Typhoon GUCCI Acupressure
If Texas Wasn?t Biology Mudslide Chanel Qigong
BODY OF CHRIST environmental science windstorm Dior FELDENKRAIS
WE CAN PHYSICS HURRICANE Ferragamo THERAPEUTIC TOUCH
Weld with your mouse information science DISASTER Ralph Lauren Reflexology
Charikar, Moses 2002. Similarity Estimation Techniques from
Rounding Algorithms In Proceedings of the 34th Annual
ACM Symposium on Theory of Computing.
Church, K. and Hanks, P. 1989. Word association norms, mu-
tual information, and lexicography. In Proceedings of ACL-
89. pp. 76?83. Vancouver, Canada.
Curran, J. and Moens, M. 2002. Scaling context space. In
Proceedings of ACL-02 pp 231?238, Philadelphia, PA.
Goemans, M. X. and D. P. Williamson 1995. Improved Ap-
proximation Algorithms for Maximum Cut and Satisfiability
Problems Using Semidefinite Programming. JACM 42(6):
1115?1145.
Hindle, D. 1990. Noun classification from predicate-argument
structures. In Proceedings of ACL-90. pp. 268?275. Pitts-
burgh, PA.
Lin, D. 1998. Automatic retrieval and clustering of similar
words. In Proceedings of COLING/ACL-98. pp. 768?774.
Montreal, Canada.
Indyk, P., Motwani, R. 1998. Approximate nearest neighbors:
towards removing the curse of dimensionality Proceedings
of 30th STOC, 604?613.
A. Kolcz, A. Chowdhury, J. Alspector 2004. Improved ro-
bustness of signature-based near-replica detection via lexi-
con randomization. Proceedings of ACM-SIGKDD (2004).
Lin, D. 1994 Principar - an efficient, broad-coverage,
principle-based parser. Proceedings of COLING-94, pp. 42?
48. Kyoto, Japan.
Pantel, Patrick and Dekang Lin 2002. Discovering Word
Senses from Text. In Proceedings of SIGKDD-02, pp. 613?
619. Edmonton, Canada
Rabin, M. O. 1981. Fingerprinting by random polynomials.
Center for research in Computing technology , Harvard Uni-
versity, Report TR-15-81.
Salton, G. and McGill, M. J. 1983. Introduction to Modern
Information Retrieval. McGraw Hill.
Appendix A. Random Permutation
Functions
We define [n] = {0, 1, 2, ..., n? 1}.
[n] can thus be considered as a set of integers from
0 to n? 1.
Let pi : [n] ? [n] be a permutation function chosen
at random from the set of all such permutation func-
tions.
Consider pi : [4] ? [4].
A permutation function pi is a one to one mapping
from the set of [4] to the set of [4].
Thus, one possible mapping is:
pi : {0, 1, 2, 3} ? {3, 2, 1, 0}
Here it means: pi(0) = 3, pi(1) = 2, pi(2) = 1,
pi(3) = 0
Another possible mapping would be:
pi : {0, 1, 2, 3} ? {3, 0, 1, 2}
Here it means: pi(0) = 3, pi(1) = 0, pi(2) = 1,
pi(3) = 2
Thus for the set [4] there would be 4! = 4?3?2 =
24 possibilities. In general, for a set [n] there would
be n! unique permutation functions. Choosing a ran-
dom permutation function amounts to choosing one
of n! such functions at random.
629
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 483?490,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Automatic Identification of Pro and Con Reasons in Online Reviews 
 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{skim, hovy}@ISI.EDU 
 
  
 
Abstract 
In this paper, we present a system that 
automatically extracts the pros and cons 
from online reviews. Although many ap-
proaches have been developed for ex-
tracting opinions from text, our focus 
here is on extracting the reasons of the 
opinions, which may themselves be in the 
form of either fact or opinion. Leveraging 
online review sites with author-generated 
pros and cons, we propose a system for 
aligning the pros and cons to their sen-
tences in review texts. A maximum en-
tropy model is then trained on the result-
ing labeled set to subsequently extract 
pros and cons from online review sites 
that do not explicitly provide them. Our 
experimental results show that our result-
ing system identifies pros and cons with 
66% precision and 76% recall. 
1 Introduction  
Many opinions are being expressed on the Web 
in such settings as product reviews, personal 
blogs, and news group message boards. People 
increasingly participate to express their opinions 
online. This trend has raised many interesting 
and challenging research topics such as subjec-
tivity detection, semantic orientation classifica-
tion, and review classification. 
Subjectivity detection is the task of identifying 
subjective words, expressions, and sentences. 
(Wiebe et al, 1999; Hatzivassiloglou and Wiebe, 
2000; Riloff et al 2003). Identifying subjectivity 
helps separate opinions from fact, which may be 
useful in question answering, summarization, etc. 
Semantic orientation classification is a task of 
determining positive or negative sentiment of 
words (Hatzivassiloglou and McKeown, 1997; 
Turney, 2002; Esuli and Sebastiani, 2005). Sen-
timent of phrases and sentences has also been 
studied in (Kim and Hovy, 2004; Wilson et al, 
2005). Document level sentiment classification is 
mostly applied to reviews, where systems assign 
a positive or negative sentiment for a whole re-
view document (Pang et al, 2002; Turney, 
2002).  
Building on this work, more sophisticated 
problems in the opinion domain have been stud-
ied by many researchers. (Bethard et al, 2004; 
Choi et al, 2005; Kim and Hovy, 2006) identi-
fied the holder (source) of opinions expressed in 
sentences using various techniques. (Wilson et 
al., 2004) focused on the strength of opinion 
clauses, finding strong and weak opinions. 
(Chklovski, 2006) presented a system that aggre-
gates and quantifies degree assessment of opin-
ions scattered throughout web pages. 
 Beyond document level sentiment classifica-
tion in online product reviews, (Hu and Liu, 
2004; Popescu and Etzioni, 2005) concentrated 
on mining and summarizing reviews by extract-
ing opinion sentences regarding product features. 
In this paper, we focus on another challenging 
yet critical problem of opinion analysis, identify-
ing reasons for opinions, especially for opinions 
in online product reviews. The opinion reason 
identification problem in online reviews seeks to 
answer the question ?What are the reasons that 
the author of this review likes or dislikes the 
product?? For example, in hotel reviews, infor-
mation such as ?found 189 positive reviews and 
65 negative reviews? may not fully satisfy the 
information needs of different users. More useful 
information would be ?This hotel is great for 
families with young infants? or ?Elevators are 
grouped according to floors, which makes the 
wait short?. 
This work differs in important ways from 
studies in (Hu and Liu, 2004) and (Popescu and 
Etzioni, 2005). These approaches extract features 
483
of products and identify sentences that contain 
opinions about those features by using opinion 
words and phrases. Here, we focus on extracting 
pros and cons which include not only sentences 
that contain opinion-bearing expressions about 
products and features but also sentences with 
reasons why an author of a review writes the re-
view. Following are examples identified by our 
system. 
 
It creates duplicate files. 
Video drains battery. 
It won't play music from all 
music stores 
 
 Even though finding reasons in opinion-
bearing texts is a critical part of in-depth opinion 
assessment, no study has been done in this par-
ticular vein partly because there is no annotated 
data. Labeling each sentence is a time-
consuming and costly task. In this paper, we pro-
pose a framework for automatically identifying 
reasons in online reviews and introduce a novel 
technique to automatically label training data for 
this task. We assume reasons in an online review 
document are closely related to pros and cons 
represented in the text. We leverage the fact that 
reviews on some websites such as epinions.com 
already contain pros and cons written by the 
same author as the reviews. We use those pros 
and cons to automatically label sentences in the 
reviews on which we subsequently train our clas-
sification system. We then apply the resulting 
system to extract pros and cons from reviews in 
other websites which do not have specified pros 
and cons. 
This paper is organized as follows: Section 2 
describes a definition of reasons in online re-
views in terms of pros and cons. Section 3 pre-
sents our approach to identify them and Section 4 
explains our automatic data labeling process. 
Section 5 describes experimental and results and 
finally, in Section 6, we conclude with future 
work. 
2 Pros and Cons in Online Reviews 
This section describes how we define reasons in 
online reviews for our study. First, we take a 
look at how researchers in Computational Lin-
guistics define an opinion for their studies. It is 
difficult to define what an opinion means in a 
computational model because of the difficulty of 
determining the unit of an opinion. In general, 
researchers study opinion at three different lev-
els: word level, sentence level, and document 
level.  
Word level opinion analysis includes word 
sentiment classification, which views single lexi-
cal items (such as good or bad) as sentiment car-
riers, allowing one to classify words into positive 
and negative semantic categories. Studies in sen-
tence level opinion regard the sentence as a mini-
mum unit of opinion. Researchers try to identify 
opinion-bearing sentences, classify their senti-
ment, and identify opinion holders and topics of 
opinion sentences. Document level opinion 
analysis has been mostly applied to review clas-
sification, in which a whole document written for 
a review is judged as carrying either positive or 
negative sentiment. Many researchers, however, 
consider a whole document as the unit of an 
opinion to be too coarse. 
In our study, we take the approach that a re-
view text has a main opinion (recommendation 
or not) about a given product, but also includes 
various reasons for recommendation or non-
recommendation, which are valuable to identify. 
Therefore, we focus on detecting those reasons in 
online product review. We also assume that rea-
sons in a review are closely related to pros and 
cons expressed in the review. Pros in a product 
review are sentences that describe reasons why 
an author of the review likes the product. Cons 
are reasons why the author doesn?t like the prod-
uct. Based on our observation in online reviews, 
most reviews have both pros and cons even if 
sometimes one of them dominates. 
3 Finding Pros and Cons 
This section describes our approach for find-
ing pro and con sentences given a review text. 
We first collect data from epinions.com and 
automatically label each sentences in the data set. 
We then model our system using one of the ma-
chine learning techniques that have been success-
fully applied to various problems in Natural 
Language Processing. This section also describes 
features we used for our model.   
3.1 Automatically Labeling Pro and Con 
Sentences 
Among many web sites that have product re-
views such as amazon.com and epinions.com, 
some of them (e.g. epinions.com) explicitly state 
pros and cons phrases in their respective catego-
ries by each review?s author along with the re-
view text. First, we collected a large set of <re-
view text, pros, cons> triplets from epin-
484
ions.com.  A review document in epinions.com 
consists of a topic (a product model, restaurant 
name, travel destination, etc.), pros and cons 
(mostly a few keywords but sometimes complete 
sentences), and the review text. Our automatic 
labeling system first collects phrases in pro and 
con fields and then searches the main review text 
in order to collect sentences corresponding to 
those phrases. Figure 1 illustrates the automatic 
labeling process. 
 
Figure 1. The automatic labeling process of 
pros and cons sentences in a review. 
The system first extracts comma-delimited 
phrases from each pro and con field, generating 
two sets of phrases: {P1, P2, ?, Pn} for pros 
and {C1, C2, ?, Cm} for cons. In the example in 
Figure 1, ?beautiful display? can be Pi and ?not 
something you want to drop? can be Cj. Then the 
system compares these phrases to the sentences 
in the text in the ?Full Review?. For each phrase 
in {P1, P2, ?, Pn} and {C1, C2, ?, Cm}, the 
system checks each sentence to find a sentence 
that covers most of the words in the phrase. Then 
the system annotates this sentence with the ap-
propriate ?pro? or ?con? label. All remaining 
sentences with neither label are marked as ?nei-
ther?. After labeling all the epinion data, we use 
it to train our pro and con sentence recognition 
system. 
3.2 Modeling with Maximum Entropy 
Classification 
We use Maximum Entropy classification for the 
task of finding pro and con sentences in a given 
review. Maximum Entropy classification has 
been successfully applied in many tasks in natu-
ral language processing, such as Semantic Role 
labeling, Question Answering, and Information 
Extraction. 
Maximum Entropy models implement the in-
tuition that the best model is the one that is con-
sistent with the set of constraints imposed by the 
evidence but otherwise is as uniform as possible 
(Berger et al, 1996). We modeled the condi-
tional probability of a class c  given a feature 
vector x  as follows: 
)),(exp(
1
)|( ?=
i
ii
x
xcf
Z
xcp ?  
where xZ  is a normalization factor which can be 
calculated by the following: 
 ? ?=
c i
iix xcfZ )),(exp( ?  
In the first equation, ),( xcfi  is a feature func-
tion which has a binary value, 0 or 1. i?  is a 
weight parameter for the feature function 
),( xcfi  and higher value of the weight indicates 
that ),( xcfi  is an important feature for a class 
c . For our system development, we used 
MegaM toolkit 1  which implements the above 
intuition.  
In order to build an efficient model, we sepa-
rated the task of finding pro and con sentences 
into two phases, each being a binary classifica-
tion. The first is an identification phase and the 
second is a classification phase. For this 2-phase 
model, we defined the 3 classes of c  listed in 
Table 1. The identification task separates pro and 
con candidate sentences (CR and PR in Table 1) 
from sentences irrelevant to either of them (NR). 
The classification task then classifies candidates 
into pros (PR) and cons (CR). Section 5 reports 
system results of both phases. 
                                                 
1 http://www.isi.edu/~hdaume/megam/index.html 
Table 1: Classes defined for the classification 
tasks. 
Class 
symbol Description 
PR Sentences related to pros in a review 
CR Sentences related to cons in a review 
NR Sentences related to neither PR nor CR 
 
485
3.3 Features 
The classification uses three types of features: 
lexical features, positional features, and opinion-
bearing word features.  
For lexical features, we use unigrams, bi-
grams, and trigrams collected from the training 
set. They investigate the intuition that there are 
certain words that are frequently used in pro and 
con sentences which are likely to represent rea-
sons why an author writes a review. Examples of 
such words and phrases are: ?because? and 
?that?s why?. 
 For positional features, we first find para-
graph boundaries in review texts using html tags 
such as <br> and <p>. After finding paragraph 
boundaries, we add features indicating the first, 
the second, the last, and the second last sentence 
in a paragraph. These features test the intuition 
used in document summarization that important 
sentences that contain topics in a text have cer-
tain positional patterns in a paragraph (Lin and 
Hovy, 1997), which may apply because reasons 
like pros and cons in a review document are most 
important sentences that summarize the whole 
point of the review.   
For opinion-bearing word features, we used 
pre-selected opinion-bearing words produced by 
a combination of two methods. The first method 
derived a list of opinion-bearing words from a 
large news corpus by separating opinion articles 
such as letters or editorials from news articles 
which simply reported news or events. The sec-
ond method calculated semantic orientations of 
words based on WordNet2 synonyms. In our pre-
vious work (Kim and Hovy, 2005), we demon-
strated that the list of words produced by a com-
bination of those two methods performed very 
well in detecting opinion bearing sentences. Both 
algorithms are described in that paper.  
The motivation for including the list of opin-
ion-bearing words as one of our features is that 
pro and con sentences are quite likely to contain 
opinion-bearing expressions (even though some 
of them are only facts), such as ?The waiting 
time was horrible? and ?Their portion size of 
food was extremely generous!? in restaurant re-
views. We presumed pro and con sentences con-
taining only facts, such as ?The battery lasted 3 
hours, not 5 hours like they advertised?, would 
be captured by lexical or positional features. 
In Section 5, we report experimental results 
with different combinations of these features. 
                                                 
2 http://wordnet.princeton.edu/ 
Table 2 summarizes the features we used for our 
model and the symbols we will use in the rest of 
this paper. 
4 Data 
We collected data from two different sources: 
epinions.com and complaints.com3 (see Section 
3.1 for details about review data in epinion.com). 
Data from epinions.com is mostly used to train 
the system whereas data from complaints.com is 
to test how the trained model performs on new 
data. 
Complaints.com includes a large database of 
publicized consumer complaints about diverse 
products, services, and companies collected for 
over 6 years. Interestingly, reviews in com-
plaint.com are somewhat different from many 
other web sites which are directly or indirectly 
linked to Internet shopping malls such as ama-
zon.com and epinions.com. The purpose of re-
views in complaints.com is to share consumers? 
mostly negative experiences and alert businesses 
to customers feedback. However, many reviews 
in Internet shopping mall related reviews are 
positive and sometimes encourage people to buy 
more products or to use more services.  
Despite its significance, however, there is no 
hand-annotated data that we can use to build a 
system to identify reasons of complaints.com. In 
order to solve this problem, we assume that rea-
sons in complaints reviews are similar to cons in 
other reviews and therefore if we are, somehow, 
able to build a system that can identify cons from 
                                                 
3 http://www.complaints.com/ 
Table 2: Feature summary. 
Feature 
category Description Symbol
Lexical 
Features 
unigrams  
bigrams 
trigrams  
Lex 
Positional 
Features 
the first, the second, 
the last, the second 
to last sentence in a 
paragraph 
Pos 
Opinion-
bearing 
word  
features 
pre-selected opin-
ion-bearing words Op 
 
486
reviews, we can apply it to identify reasons in 
complaints reviews. Based on this assumption, 
we learn a system using the data from epin-
ions.com, to which we can apply our automatic 
data labeling technique, and employ the resulting 
system to identify reasons from reviews in com-
plaint.com. The following sections describe each 
data set. 
4.1 Dataset 1: Automatically Labeled Data 
We collected two different domains of reviews 
from epinions.com: product reviews and restau-
rant reviews. As for the product reviews, we col-
lected 3241 reviews (115029 sentences) about 
mp3 players made by various manufacturers such 
as Apple, iRiver, Creative Lab, and Samsung. 
We also collected 7524 reviews (194393 sen-
tences) about various types of restaurants such as 
family restaurants, Mexican restaurants, fast food 
chains, steak houses, and Asian restaurants. The 
average numbers of sentences in a review docu-
ment are 35.49 and 25.89 respectively.     
The purpose of selecting one of electronics 
products and restaurants as topics of reviews for 
our study is to test our approach in two ex-
tremely different situations. Reasons why con-
sumers like or dislike a product in electronics? 
reviews are mostly about specific and tangible 
features. Also, there are somewhat a fixed set of 
features of a specific type of product, for exam-
ple, ease of use, durability, battery life, photo 
quality, and shutter lag for digital cameras. Con-
sequently, we can expect that reasons in electron-
ics? reviews may share those product feature 
words and words that describe aspects of features 
such as short or long for battery life. This fact 
might make the reason identification task easy.  
 On the other hand, restaurant reviewers talk 
about very diverse aspects and abstract features 
as reasons. For example, reasons such as ?You 
feel like you are in a train station or a busy 
amusement park that is ill-staffed to meet de-
mand!?, ?preferential treatment given to large 
groups?, and ?they don't offer salads of any 
kind? are hard to predict. Also, they seem rarely 
share common keyword features. 
We first automatically labeled each sentence 
in those reviews collected from each domain 
with the features described in Section 3.1. We 
divided the data for training and testing. We then 
trained our model using the training set and 
tested it to see if the system can successfully la-
bel sentences in the test set. 
4.2 Dataset 2: Complaints.com Data 
From the database 4  in complaints.com, we 
searched for the same topics of reviews as Data-
set 1: 59 complaints reviews about mp3 players 
and 322 reviews about restaurants5. We tested 
our system on this dataset and compare the re-
sults against human judges? annotation results. 
Subsection 5.2 reports the evaluation results. 
5 Experiments and Results 
We describe two goals in our experiments in this 
section. The first is to investigate how well our 
pro and con detection model with different fea-
ture combinations performs on the data we col-
lected from epinions.com. The second is to see 
how well the trained model performs on new 
data from a different source, complaint.com.  
For both datasets, we carried out two separate 
sets of experiments, for the domains of mp3 
players and restaurant reviews. We divided data 
into 80% for training, 10% for development, and 
10% for test for our experiments. 
5.1 Experiments on Dataset 1 
Identification step: Table 3 and 4 show pros and 
cons sentences identification results of our sys-
tem for mp3 player and restaurant reviews re-
spectively. The first column indicates which 
combination of features was used for our model 
(see Table 2 for the meaning of Op, Lex, and Pos 
feature categories). We measure the performance 
with accuracy (Acc), precision (Prec), recall 
(Recl), and F-score 6. 
The baseline system assigned all sentences as 
reason and achieved 57.75% and 54.82% of ac-
curacy. The system performed well when it only 
used lexical features in mp3 player reviews 
(76.27% of accuracy in Lex), whereas it per-
formed well with the combination of lexical and 
opinion features in restaurant reviews (Lex+Op 
row in Table 4). 
It was very interesting to see that the system 
achieved a very low score when it only used 
opinion word features. We can interpret this phe-
nomenon as supporting our hypothesis that pro 
and con sentences in reviews are often purely 
                                                 
4 At the time (December 2005), there were total 42593 
complaint reviews available in the database. 
5 Average numbers of sentences in a complaint is 
19.57 for mp3 player reviews and 21.38 for restaurant 
reviews. 
6 We calculated F-score by 
Recall Precision 
Recall Precision   2
+
??  
487
factual. However, opinion features improved 
both precision and recall when combined with 
lexical features in restaurant reviews. It was also 
interesting that experiments on mp3 players re-
views achieved mostly higher scores than restau-
rants. Like the observation we described in Sub-
section 4.1, frequently mentioned keywords of 
product features (e.g. durability) may have 
helped performance, especially with lexical fea-
tures. Another interesting observation is that the 
positional features that helped in topic sentence 
identification did not help much for our task.        
Classification step: Tables 5 and 6 show the 
system results of the pro and con classification 
task. The baseline system marked all sentences 
as pros and achieved 53.87% and 50.71% accu-
racy for each domain. All features performed 
better than the baseline but the results are not as 
good as in the identification task. Unlike the 
identification task, opinion words by themselves 
achieved the best accuracy in both mp3 player 
and restaurant domains. We think opinion words 
played more important roles in classifying pros 
and cons than identifying them. Position features 
helped recognizing con sentences in mp3 player 
reviews.  
5.2 Experiments on Dataset 2 
This subsection reports the evaluation results of 
our system on Dataset 2. Since Dataset 2 from 
complaints.com has no training data, we trained 
a system on Dataset 1 and applied it to Dataset 2. 
Table 3: Pros and cons sentences identification 
results on mp3 player reviews. 
Features 
used 
Acc 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score
(%) 
Op 60.15 65.84 57.31 61.28 
Lex 76.27 66.18 76.42 70.93 
Lex+Pos 63.10 71.14 60.72 65.52 
Lex+Op 62.75 70.64 60.07 64.93 
Lex+Pos+Op 62.23 70.58 59.35 64.48 
Baseline 57.75    
 
Table 4: Reason sentence identification results 
on restaurant reviews. 
Features 
used 
Acc 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score
(%) 
Op 61.64 60.76 47.48 53.31 
Lex 63.77 67.10 51.20 58.08 
Lex+Pos 63.89 67.62 51.70 58.60 
Lex+Op 61.66 69.13 54.30 60.83 
Lex+Pos+Op 63.13 66.80 50.41 57.46 
Baseline 54.82    
 
Table 5: Pros and cons sentences classification results for mp3 player reviews. 
Cons  Pros Features 
used 
Acc 
(%) Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Op 57.18 54.43 67.10 60.10 61.18 48.00 53.80 
Lex 55.88 55.49 67.45 60.89 56.52 43.88 49.40 
Lex+Pos 55.62 55.26 68.12 61.02 56.24 42.62 48.49 
Lex+Op 55.60 55.46 64.63 59.70 55.81 46.26 50.59 
Lex+Pos+Op 56.68 56.70 62.45 59.44 56.65 50.71 53.52 
baseline 53.87      (mark all as pros) 
 
Table 6: Pros and cons sentences classification results for restaurant reviews. 
Cons Pros Features 
used 
Acc 
(%) Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Prec 
(%) 
Recl 
(%) 
F-score 
(%) 
Op 57.32 54.78 51.62 53.15 59.32 62.35 60.80 
Lex 55.76 55.94 52.52 54.18 55.60 58.97 57.24 
Lex+Pos 56.07 56.20 53.33 54.73 55.94 58.78 57.33 
Lex+Op 55.88 56.10 52.39 54.18 55.68 59.34 57.45 
Lex+Pos+Op 55.79 55.89 53.17 54.50 55.70 58.38 57.01 
baseline 50.71      (mark all as pros) 
488
A tough question, however, is how to evaluate 
the system results. Since it seemed impossible to 
evaluate the system without involving a human 
judge, we annotated a small set of data manually 
for evaluation purposes. 
Gold Standard Annotation: Four humans 
annotated 3 sets of test sets: Testset 1 with 5 
complaints (73 sentences), Testset 2 with 7 com-
plaints (105 sentences), and Testset 3 with 6 
complaints (85 sentences). Testset 1 and 2 are 
from mp3 player complaints and Testset 3 is 
from restaurant reviews. Annotators marked sen-
tences if they describe specific reasons of the 
complaint. Each test set was annotated by 2 hu-
mans. The average pair-wise human agreement 
was 82.1%7. 
System Performance: Like the human anno-
tators, our system also labeled reason sentences. 
Since our goal is to identify reason sentences in 
complaints, we applied a system modeled as in 
the identification phase described in Subsection 
3.2 instead of the classification phase8. Table 7 
reports the accuracy, precision, and recall of the 
system on each test set. We calculated numbers 
in each A and B column by assuming each anno-
tator?s answers separately as a gold standard.  
 
    
In Table 7, accuracies indicate the agreement 
between the system and human annotators. The 
average accuracy 68.0% is comparable with the 
pair-wise human agreement 82.1% even if there 
is still a lot of room for improvement9. It was 
interesting to see that Testset 3, which was from 
restaurant complaints, achieved higher accuracy 
and recall than the other test sets from mp3 
player complaints, suggesting that it would be 
interesting to further investigate the performance 
                                                 
7 The kappa value was 0.63. 
8 In complaints reviews, we believe that it is more 
important to identify reason sentences than to classify 
because most reasons in complaints are likely to be 
cons. 
9 The baseline system which assigned the majority 
class to each sentence achieved 59.9% of average 
accuracy. 
of reason identification in various other review 
domains such as travel and beauty products in 
future work. Also, even though we were some-
what able to measure reason sentence identifica-
tion in complaint reviews, we agree that we need 
more data annotation for more precise evalua-
tion. 
Finally, the followings are examples of sen-
tences that our system identified as reasons of 
complaints. 
(1) Unfortunately, I find that 
I am no longer comfortable in 
your establishment because of 
the unprofessional, rude, ob-
noxious, and unsanitary treat-
ment from the employees.  
(2) They never get my order 
right the first time and what 
really disgusts me is how they 
handle the food. 
(3) The kids play area at 
Braum's in The Colony, Texas is 
very dirty. 
(4) The only complaint that I 
have is that the French fries 
are usually cold. 
(5) The cashier there had short 
changed me on the payment of my 
bill. 
 
As we can see from the examples, our system 
was able to detect con sentences which contained 
opinion-bearing expressions such as in (1), (2), 
and (3) as well as reason sentences that mostly 
described mere facts as in (4) and (5).      
6 Conclusions and Future work 
This paper proposes a framework for identifying 
one of the critical elements of online product re-
views to answer the question, ?What are reasons 
that the author of a review likes or dislikes the 
product?? We believe that pro and con sentences 
in reviews can be answers for this question. We 
present a novel technique that automatically la-
bels a large set of pro and con sentences in online 
reviews using clue phrases for pros and cons in 
epinions.com in order to train our system. We 
applied it to label sentences both on epin-
ions.com and complaints.com. To investigate the 
reliability of our system, we tested it on two ex-
tremely different review domains, mp3 player 
reviews and restaurant reviews. Our system with 
the best feature selection performs 71% F-score 
in the reason identification task and 61% F-score 
in the reason classification task. 
Table 7: System results on Complaint.com 
reviews (A, B: The first and the second anno-
tator of each set) 
 Testset 1 Testset 2 Testset 3 
 A B A B A B 
Avg 
Acc(%) 65.8 63.0 67.6 61.0 77.6 72.9 68.0 
Prec(%) 50.0 60.7 68.6 62.9 67.9 60.7 61.8 
Recl(%) 56.0 51.5 51.1 44.0 65.5 58.6 54.5 
 
489
The experimental results further show that pro 
and con sentences are a mixture of opinions and 
facts, making identifying them in online reviews 
a distinct problem from opinion sentence identi-
fication. Finally, we also apply the resulting sys-
tem to another review data in complaints.com in 
order to analyze reasons of consumers? com-
plaints.  
In the future, we plan to extend our pro and 
con identification system on other sorts of opin-
ion texts, such as debates about political and so-
cial agenda that we can find on blogs or news 
group discussions, to analyze why people sup-
port a specific agenda and why people are 
against it. 
Reference  
Berger, Adam L., Stephen Della Pietra, and Vin-
cent Della Pietra. 1996. A maximum entropy ap-
proach to natural language processing, Computa-
tional Linguistics, (22-1).  
Bethard, Steven, Hong Yu, Ashley Thornton, Va-
sileios Hatzivassiloglou, and Dan Jurafsky. 
2004. Automatic Extraction of Opinion Proposi-
tions and their Holders, AAAI Spring Symposium 
on Exploring Attitude and Affect in Text: Theo-
ries and Applications. 
Chklovski, Timothy. 2006. Deriving Quantitative 
Overviews of Free Text Assessments on the 
Web. Proceedings of 2006 International Confer-
ence on Intelligent User Interfaces (IUI06). 
Sydney, Australia. 
Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S. 
2005. Identifying Sources of Opinions with 
Conditional Random Fields and Extraction Pat-
terns. Proceedings of HLT/EMNLP-05. 
Esuli, Andrea and Fabrizio Sebastiani. 2005. De-
termining the semantic orientation of terms 
through gloss classification. Proceedings of 
CIKM-05, 14th ACM International Conference 
on Information and Knowledge Management, 
Bremen, DE, pp. 617-624.  
Hatzivassiloglou, Vasileios and Kathleen McKe-
own. 1997. Predicting the Semantic Orientation 
of Adjectives. Proceedings of 35th Annual Meet-
ing of the Assoc. for Computational Linguistics 
(ACL-97): 174-181 
Hatzivassiloglou, Vasileios and Janyce Wiebe. 
2000. Effects of Adjective Orientation and 
Gradability on Sentence Subjectivity. Proceed-
ings of International Conference on Computa-
tional Linguistics (COLING-2000). Saarbr?cken, 
Germany. 
Hu, Minqing and Bing Liu. 2004. Mining and 
summarizing customer reviews". Proceedings of 
the ACM SIGKDD International Conference on 
Knowledge Discovery & Data Mining (KDD-
2004), Seattle, Washington, USA. 
Kim, Soo-Min and Eduard Hovy. 2004. Determin-
ing the Sentiment of Opinions. Proceedings of 
COLING-04. pp. 1367-1373. Geneva, Switzer-
land. 
Kim, Soo-Min and Eduard Hovy. 2005. Automatic 
Detection of Opinion Bearing Words and Sen-
tences. In the Companion Volume of the Pro-
ceedings of IJCNLP-05, Jeju Island, Republic of 
Korea. 
Kim, Soo-Min and Eduard Hovy. 2006. Identifying 
and Analyzing Judgment Opinions. Proceedings 
of HLT/NAACL-2006, New York City, NY. 
Lin, Chin-Yew and Eduard Hovy. 1997. 
Identifying Topics by Position. Proceedings of 
the 5th Conference on Applied Natural Lan-
guage Processing (ANLP97). Washington, D.C. 
Pang, Bo, Lillian Lee, and Shivakumar Vaithyana-
than. 2002. Thumbs up? Sentiment Classifica-
tion using Machine Learning Techniques, Pro-
ceedings of EMNLP 2002. 
Popescu, Ana-Maria, and Oren Etzioni. 2005. 
Extracting Product Features and Opinions from 
Reviews , Proceedings of HLT-EMNLP 2005. 
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 
2003. Learning Subjective Nouns Using Extrac-
tion Pattern Bootstrapping. Proceedings of Sev-
enth Conference on Natural Language Learning 
(CoNLL-03). ACL SIGNLL. Pages 25-32. 
Turney, Peter D. 2002. Thumbs up or thumbs 
down? Semantic orientation applied to unsuper-
vised classification of reviews, Proceedings of 
ACL-02, Philadelphia, Pennsylvania, 417-424 
Wiebe, Janyce M., Bruce, Rebecca F., and O'Hara, 
Thomas P. 1999. Development and use of a gold 
standard data set for subjectivity classifications. 
Proceedings of ACL-99. University of Maryland, 
June, pp. 246-253. 
Wilson, Theresa, Janyce Wiebe, and Paul Hoff-
mann. 2005. Recognizing Contextual Polarity in 
Phrase-Level Sentiment Analysis. Proceedings 
of HLT/EMNLP 2005, Vancouver, Canada 
Wilson, Theresa, Janyce Wiebe, and Rebecca Hwa. 
2004. Just how mad are you? Finding strong and 
weak opinion clauses. Proceedings of 19th Na-
tional Conference on Artificial Intelligence 
(AAAI-2004). 
490
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024?1031,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Topic Analysis for Psychiatric Document Retrieval 
Liang-Chih Yu*?, Chung-Hsien Wu*, Chin-Yew Lin?, Eduard Hovy? and Chia-Ling Lin*
*Department of CSIE, National Cheng Kung University, Tainan, Taiwan, R.O.C. 
?Microsoft Research Asia, Beijing, China 
?Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA 
liangchi@isi.edu,{chwu,totalwhite}@csie.ncku.edu.tw,cyl@microsoft.com,hovy@isi.edu 
 
 
Abstract 
Psychiatric document retrieval attempts to 
help people to efficiently and effectively 
locate the consultation documents relevant 
to their depressive problems. Individuals 
can understand how to alleviate their symp-
toms according to recommendations in the 
relevant documents. This work proposes 
the use of high-level topic information ex-
tracted from consultation documents to im-
prove the precision of retrieval results. The 
topic information adopted herein includes 
negative life events, depressive symptoms 
and semantic relations between symptoms, 
which are beneficial for better understand-
ing of users' queries. Experimental results 
show that the proposed approach achieves 
higher precision than the word-based re-
trieval models, namely the vector space 
model (VSM) and Okapi model, adopting 
word-level information alone. 
1 Introduction 
Individuals may suffer from negative or stressful 
life events, such as death of a family member, ar-
gument with a spouse and loss of a job. Such 
events play an important role in triggering depres-
sive symptoms, such as depressed moods, suicide 
attempts and anxiety. Individuals under these cir-
cumstances can consult health professionals using 
message boards and other services. Health profes-
sionals respond with suggestions as soon as possi-
ble. However, the response time is generally sev-
eral days, depending on both the processing time 
required by health professionals and the number of 
problems to be processed. Such a long response 
time is unacceptable, especially for patients suffer-
ing from psychiatric emergencies such as suicide 
attempts. A potential solution considers the prob-
lems that have been processed and the correspond-
ing suggestions, called consultation documents, as 
the psychiatry web resources. These resources gen-
erally contain thousands of consultation documents 
(problem-response pairs), making them a useful 
information resource for mental health care and 
prevention. By referring to the relevant documents, 
individuals can become aware that they are not 
alone because many people have suffered from the 
same or similar problems. Additionally, they can 
understand how to alleviate their symptoms ac-
cording to recommendations. However, browsing 
and searching all consultation documents to iden-
tify the relevant documents is time consuming and 
tends to become overwhelming. Individuals need 
to be able to retrieve the relevant consultation 
documents efficiently and effectively. Therefore, 
this work presents a novel mechanism to automati-
cally retrieve the relevant consultation documents 
with respect to users' problems. 
Traditional information retrieval systems repre-
sent queries and documents using a bag-of-words 
approach. Retrieval models, such as the vector 
space model (VSM) (Baeza-Yates and Ribeiro-
Neto, 1999) and Okapi model (Robertson et al, 
1995; Robertson et al, 1996; Okabe et al, 2005), 
are then adopted to estimate the relevance between 
queries and documents. The VSM represents each 
query and document as a vector of words, and 
adopts the cosine measure to estimate their rele-
vance. The Okapi model, which has been used on 
the Text REtrieval Conference (TREC) collections, 
developed a family of word-weighting functions 
1024
for relevance estimation. These functions consider 
word frequencies and document lengths for word 
weighting. Both the VSM and Okapi models esti-
mate the relevance by matching the words in a 
query with the words in a document. Additionally, 
query words can further be expanded by the con-
cept hierarchy within general-purpose ontologies 
such as WordNet (Fellbaum, 1998), or automati-
cally constructed ontologies (Yeh et al, 2004). 
However, such word-based approaches only 
consider the word-level information in queries and 
documents, ignoring the high-level topic informa-
tion that can help improve understanding of users' 
queries. Consider the example consultation docu-
ment in Figure 1. A consultation document com-
prises two parts: the query part and recommenda-
tion part. The query part is a natural language text, 
containing rich topic information related to users' 
depressive problems. The topic information in-
cludes negative life events, depressive symptoms, 
and semantic relations between symptoms. As in-
dicated in Figure 1, the subject suffered from a 
love-related event, and several depressive symp-
toms, such as <Depressed>, <Suicide>, <Insom-
nia> and <Anxiety>. Moreover, there is a cause-
effect relation holding between <Depressed> and 
<Suicide>, and a temporal relation holding be-
tween <Depressed> and <Insomnia>. Different 
topics may lead to different suggestions decided by 
experts. Therefore, an ideal retrieval system for 
consultation documents should consider such topic 
information so as to improve the retrieval precision. 
Natural language processing (NLP) techniques 
can be used to extract more precise information 
from natural language texts (Wu et al, 2005a; Wu 
et al, 2005b; Wu et al, 2006; Yu et al, 2007). 
This work adopts the methodology presented in 
(Wu et al 2005a) to extract depressive symptoms 
and their relations, and adopts the pattern-based 
method presented in (Yu et al, 2007) to extract 
negative life events from both queries and consul-
tation documents. This work also proposes a re-
trieval model to calculate the similarity between a 
query and a document by combining the similari-
ties of the extracted topic information. 
The rest of this work is organized as follows. 
Section 2 briefly describes the extraction of topic 
information. Section 3 presents the retrieval model. 
Section 4 summarizes the experimental results. 
Conclusions are finally drawn in Section 5. 
2 Framework of Consultation Document 
Retrieval 
Figure 2 shows the framework of consultation 
document retrieval. The retrieval process begins 
with receiving a user?s query about his depressive 
problems in natural language. The example query 
is shown in Figure 1. The topic information is then 
extracted from the query, as shown in the center of 
Figure 2. The extracted topic information is repre-
Consultation DocumentQuery:
It's normal to feel this way when going through these kinds of struggles, but over 
time your emotions should level out. Suicide doesn't solve anything; think about 
how it would affect your family........ There are a few things you can try to help 
you get to sleep at night, like drinking warm milk, listening to relaxing music....... 
Recommendation:
After that, it took me a long time to fall asleep at night.  
<Depressed>
<Suicide>
<Insomnia>
<Anxiety>
cause-effect temporal
I broke up with my boyfriend.
I often felt like crying and felt pain every day. 
So, I tried to kill myself several times. 
In recent months, I often lose my temper for no reason.
 
Figure 1.  Example of a consultation document. The bold arrowed lines denote cause-effect relations; ar-
rowed lines denote temporal relations; dashed lines denote temporal boundaries, and angle brackets de-
note depressive symptoms 
1025
sented by the sets of negative life events, depres-
sive symptoms, and semantic relations. Each ele-
ment in the event set and symptom set denotes an 
individual event and symptom, respectively, while 
each element in the relation set denotes a symptom 
chain to retain the order of symptoms. Similarly, 
the query parts of consultation documents are rep-
resented in the same manner. The relevance esti-
mation then calculates the similarity between the 
input query and the query part of each consultation 
document by combining the similarities of the sets 
of events, symptoms, and relations within them. 
Finally, a list of consultation documents ranked in 
the descending order of similarities is returned to 
the user. 
In the following, the extraction of topic informa-
tion is described briefly. The detailed process is 
described in (Wu et al 2005a) for symptom and 
relation identification, and in (Yu et al, 2007) for 
event identification. 
1) Symptom identification: A total of 17 symp-
toms are defined based on the Hamilton De-
pression Rating Scale (HDRS) (Hamilton, 
1960). The identification of symptoms is sen-
tence-based. For each sentence, its structure is 
first analyzed by a probabilistic context free 
grammar (PCFG), built from the Sinica Tree-
bank corpus developed by Academia Sinica, 
Taiwan (http://treebank.sinica.edu.tw), to gen-
erate a set of dependencies between word to-
kens. Each dependency has the format (modi-
fier, head, relmodifier,head). For instance, the de-
pendency (matters, worry about, goal) means 
that "matters" is the goal to the head of the sen-
tence "worry about". Each sentence can then 
be associated with a symptom based on the 
probabilities that dependencies occur in all 
symptoms, which are obtained from a set of 
training sentences. 
2) Relation Identification: The semantic rela-
tions of interest include cause-effect and tem-
poral relations. After the symptoms are ob-
tained, the relations holding between symp-
toms (sentences) are identified by a set of dis-
course markers. For instance, the discourse 
markers "because" and "therefore" may signal 
cause-effect relations, and "before" and "after" 
may signal temporal relations. 
3) Negative life event identification: A total of 5 
types of events, namely <Family>, <Love>, 
<School>, <Work> and <Social> are defined 
based on Pagano et als (2004) research. The 
identification of events is a pattern-based ap-
proach. A pattern denotes a semantically plau-
sible combination of words, such as <parents, 
divorce> and <exam, fail>. First, a set of pat-
terns is acquired from psychiatry web corpora 
by using an evolutionary inference algorithm. 
The event of each sentence is then identified 
by using an SVM classifier with the acquired 
patterns as features. 
3 Retrieval Model 
The similarity between a query and a document, 
( , )Sim q d , is calculated by combining the similari-
ties of the sets of events, symptoms and relations 
within them, as shown in (1). 
Consultation
Documents
Ranking
Relevance
Estimation
Query
(Figure 1)
Topic Information
Symptom 
Identification
Negative Life Event
Identification
Relation
Identification
D S A
D S Cause-Effect
D I A
Temporal
I
S I A
<Love>
Topic Analysis
 
Figure 2.  Framework of consultation document retrieval. The rectangle denotes a negative life event re-
lated to love relation. Each circle denotes a symptom. D: Depressed, S: Suicide, I: Insomnia, A: Anxiety. 
1026
( , )
( , ) ( , ) (1 ) ( , ),Evn Sym Rel
Sim q d
Sim q d Sim q d Sim q d? ? ? ?
=
+ + ? ?   (1) 
where ( , )EvnSim q d , ( , )SymSim q d  and ( , )RelSim q d , 
denote the similarities of the sets of events, symp-
toms and relations, respectively, between a query 
and a document, and ?  and ? denote the combi-
nation factors. 
3.1 Similarity of events and symptoms 
The similarities of the sets of events and symptoms 
are calculated in the same method. The similarity 
of the event set (or symptom set) is calculated by 
comparing the events (or symptoms) in a query 
with those in a document. Additionally, only the 
events (or symptoms) with the same type are 
considered. The events (or symptoms) with 
different types are considered as irrelevant, i.e., no 
similarity. For instance, the event <Love> is 
considered as irrelevant to <Work>. The similarity 
of the event set is calculated by 
( , )
1
( , ) cos( , ) .,
( )
Evn
q d q d
q d e q d
Sim q d
Type e e e e const
N Evn Evn ? ?
= +? ?  (2) 
where qEvn  and dEvn  denote the event set in a 
query and a document, respectively; qe  and de  
denote the events; ( )q dN Evn Evn?  denotes the 
cardinality of the union of qEvn  and dEvn  as a 
normalization factor, and ( , )q dType e e  denotes an 
identity function to check whether two events have 
the same type, defined as 
1     ( ) ( )
( , ) .
0    otherwise
q d
q d
Type e Type e
Type e e
=??= ???
  (3) 
The cos( , )q de e  denotes the cosine angle between 
two vectors of words representing qe  and de , as 
shown below. 
( ) ( )
1
2 2
1 1
cos( , ) ,q d
q d
T i i
e ei
q d
T Ti i
e ei i
w w
e e
w w
=
= =
= ?
? ?
  (4) 
where w denotes a word in a vector, and T denotes 
the dimensionality of vectors. Accordingly, when 
two events have the same type, their similarity is 
given as cos( , )q de e  plus a constant, const.. Addi-
tionally, cos( , )q de e  and const. can be considered 
as the word-level and topic-level similarities, re-
spectively. The optimal setting of const. is deter-
mined empirically. 
3.2 Similarity of relations 
When calculating the similarity of relations, only 
the relations with the same type are considered. 
That is, the cause-effect (or temporal) relations in a 
query are only compared with the cause-effect (or 
temporal) relations in a document. Therefore, the 
similarity of relation sets can be calculated as 
,
1
( , ) ( , ) ( , ),
q d
Rel q d q d
r r
Sim q d Type r r Sim r r
Z
= ?  (5) 
( ) ( ) ( ) ( ),C q C d T q T dZ N r N r N r N r= +   (6) 
where qr and dr denote the relations in a query and 
a document, respectively; Z denotes the normaliza-
tion factor for the number of relations; ( , )q dType e e  
denotes an identity function similar to (3), and 
( )CN i   and ( )TN i  denote the numbers of cause-
effect and temporal relations. 
Both cause-effect and temporal relations are rep-
resented by symptom chains. Hence, the similarity 
of relations is measured by the similarity of symp-
tom chains. The main characteristic of a symptom 
chain is that it retains the cause-effect or temporal 
order of the symptoms within it. Therefore, the 
order of the symptoms must be considered when 
calculating the similarity of two symptom chains. 
Accordingly, a sequence kernel function (Lodhi et 
al., 2002; Cancedda et al, 2003) is adopted to cal-
culate the similarity of two symptom chains. A 
sequence kernel compares two sequences of sym-
bols (e.g., characters, words) based on the subse-
quences within them, but not individual symbols. 
Thereby, the order of the symptoms can be incor-
porated into the comparison process. 
The sequence kernel calculates the similarity of 
two symptom chains by comparing their sub-
symptom chains at different lengths. An increasing 
number of common sub-symptom chains indicates 
a greater similarity between two symptom chains. 
For instance, both the two symptom chains 
1 2 3 4s s s s  and 3 2 1s s s  contain the same symptoms 1s , 
2s  and 3s , but in different orders. To calculate the 
similarity between these two symptom chains, the 
sequence kernel first calculates their similarities at 
length 2 and 3, and then averages the similarities at 
the two lengths. To calculate the similarity at 
1027
length 2, the sequence kernel compares their sub-
symptom chains of length 2, i.e., 
1 2 1 3 1 4 2 3 2 4 3 4{ , , , , , }s s s s s s s s s s s s  and 3 2 3 1 2 1{ , , }s s s s s s . 
Similarly, their similarity at length 3 is calculated 
by comparing their sub-symptom chains of length 
3, i.e., 1 2 3 1 2 4 1 3 4 2 3 4{ ,  ,  ,  }s s s s s s s s s s s s  and 3 2 1{ }s s s . 
Obviously, no similarity exists between 1 2 3 4s s s s  
and 3 2 1s s s , since no sub-symptom chains are 
matched at both lengths. In this example, the sub-
symptom chains of length 1, i.e., individual symp-
toms, do not have to be compared because they 
contain no information about the order of symp-
toms. Additionally, the sub-symptom chains of 
length 4 do not have to be compared, because the 
two symptom chains share no sub-symptom chains 
at this length. Hence, for any two symptom chains, 
the length of the sub-symptom chains to be com-
pared ranges from two to the minimum length of 
the two symptom chains. The similarity of two 
symptom chains can be formally denoted as 
1 2
1 2
1 2
2
( , ) ( , )
                ( , )
1
                ( , ),
1
N N
q d q d
N N
q d
N
N N
n q d
n
Sim r r Sim sc sc
K sc sc
K sc sc
N =
?
=
= ? ?
  (7) 
where 1Nqsc  and 2
N
dsc  denote the symptom chains 
corresponding to qr  and dr , respectively; 1N  and 
2N  denote the length of 1
N
qsc  and 2
N
dsc , respec-
tively; (  ,   )K i i  denotes the sequence kernel for 
calculating the similarity between two symptom 
chains; (  ,   )nK i i  denotes the sequence kernel for 
calculating the similarity between two symptom 
chains at length n, and N is the minimum length of 
the two symptom chains, i.e., 1 2min( , )N N N= . 
The sequence kernel 1 2( , )N Nn i jK sc sc  is defined as 
21
1 2
1 2
1 2
1 1 2 2
( )( )
( , )   
( ) ( )
( ) ( )
         ,
( ) ( ) ( ) ( )
n
n n
NN
n jN N n i
n i j N N
n i n j
N N
u i u j
u SC
N N N N
u i u j u i u j
u SC u SC
scsc
K sc sc
sc sc
sc sc
sc sc sc sc
? ?
? ? ? ?
?
? ?
??=
? ?
=
?
? ?
i
(8) 
where 1 2( , )N Nn i jK sc sc  is the normalized inner 
product of vectors 1( )Nn isc?  and 2( )Nn jsc? ; ( )n? i  
denotes a mapping that transforms a given symp-
tom chain into a vector of the sub-symptom chains 
of length n; ( )u? i  denotes an element of the vector, 
representing the weight of a sub-symptom chain u , 
and nSC  denotes the set of all possible sub-
symptom chains of length n. The weight of a sub-
symptom chain, i.e., ( )u? i , is defined as 
1
1
1
1       is a contiguous sub-symptom chain of 
    is a non-contiguous sub-symptom chain
( )  
        with  skipped symptoms
0       does not appear in ,
N
i
N
u i
N
i
u sc
u
sc
u sc
??? ?
???= ????
(9) 
where [0,1]??  denotes a decay factor that is 
adopted to penalize the non-contiguous sub-
symptom chains occurred in a symptom chain 
based on the skipped symptoms. For instance, 
1 2 2 31 2 3 1 2 3
( ) ( ) 1s s s ss s s s s s? ?= =  since 1 2s s  and 2 3s s  
are considered as contiguous in 1 2 3s s s , and 
1 3
1
1 2 3( )s s s s s? ?=  since 1 3s s  is a non-contiguous 
sub-symptom chain with one skipped symptom. 
The decay factor is adopted because a contiguous 
sub-symptom chain is preferable to a non-
contiguous chain when comparing two symptom 
chains. The setting of the decay factor is domain 
dependent. If 1? = , then no penalty is applied for 
skipping symptoms, and the cause-effect and tem-
poral relations are transitive. The optimal setting of 
Figure 3.  Illustrative example of relevance com-
putation using the sequence kernel function. 
1028
?  is determined empirically. Figure 3 presents an 
example to summarize the computation of the 
similarity between two symptom chains. 
4 Experimental Results 
4.1 Experiment setup 
1) Corpus: The consultation documents were 
collected from the mental health website of the 
John Tung Foundation (http://www.jtf.org.tw) 
and the PsychPark (http://www.psychpark.org), 
a virtual psychiatric clinic, maintained by a 
group of volunteer professionals of Taiwan 
Association of Mental Health Informatics (Bai 
et al 2001). Both of the web sites provide 
various kinds of free psychiatric services and 
update the consultation documents periodically. 
For privacy consideration, all personal infor-
mation has been removed. A total of 3,650 
consultation documents were collected for 
evaluating the retrieval model, of which 20 
documents were randomly selected as the test 
query set, 100 documents were randomly se-
lected as the tuning set to obtain the optimal 
parameter settings of involved retrieval models, 
and the remaining 3,530 documents were the 
reference set to be retrieved. Table 1 shows the 
average number of events, symptoms and rela-
tions in the test query set. 
2) Baselines: The proposed method, denoted as 
Topic, was compared to two word-based re-
trieval models: the VSM and Okapi BM25 
models. The VSM was implemented in terms 
of the standard TF-IDF weight. The Okapi 
BM25 model is defined as 
(1) 31
2
3
( 1)( 1)
| | ,
t Q
k qtfk tf avdl dl
w k Q
K tf k qtf avdl dl?
++ ?++ + +?  (10) 
where t denotes a word in a query Q; qtf and tf 
denote the word frequencies occurring in a 
query and a document, respectively, and  (1)w  
denotes the Robertson-Sparck Jones weight of 
t (without relevance feedback), defined as 
(1) 0.5log ,
0.5
N n
w
n
? += +             (11) 
where N denotes the total number of docu-
ments, and n denotes the number of documents 
containing t. In (10), K is defined as 
1((1 ) / ),K k b b dl avdl= ? + ?             (12) 
where dl and avdl denote the length and aver-
age length of a document, respectively. The 
default values of 1k , 2k , 3k  and b are describe 
in (Robertson et al, 1996), where 1k  ranges 
from 1.0 to 2.0; 2k  is set to 0; 3k  is set to 8, 
and b ranges from 0.6 to 0.75. Additionally, 
BM25 can be considered as BM15 and BM11 
when b is set to 1 and 0, respectively. 
3) Evaluation metric: To evaluate the retrieval 
models, a multi-level relevance criterion was 
adopted. The relevance criterion was divided 
into four levels, as described below. 
z Level 0: No topics are matched between a 
query and a document. 
z Level 1: At least one topic is partially 
matched between a query and a document. 
z Level 2: All of the three topics are partially 
matched between a query and a document. 
z Level 3: All of the three topics are partially 
matched, and at least one topic is exactly 
matched between a query and a document. 
To deal with the multi-level relevance, the dis-
counted cumulative gain (DCG) (Jarvelin and 
Kekalainen, 2000) was adopted as the evalua-
tion metric, defined as 
[1],                                   1     [ ]
[ 1] [ ]/ log , otherwisec
G if i
DCG i
DCG i G i i
=??= ? ? +??
(13) 
where i denotes the i-th document in the re-
trieved list; G[i] denotes the gain value, i.e., 
relevance levels, of the i-th document, and c 
denotes the parameter to penalize a retrieved 
document in a lower rank. That is, the DCG 
simultaneously considers the relevance levels, 
and the ranks in the retrieved list to measure 
the retrieval precision. For instance, let 
<3,2,3,0,0> denotes the retrieved list of five 
documents with their relevance levels. If no 
penalization is used, then the DCG values for 
Topic Avg. Number
Negative Life Event 1.45 
Depressive Symptom 4.40 
Semantic Relation 3.35 
Table 1. Characteristics of the test query set. 
1029
the retrieved list are <3,5,8,8,8>, and thus 
DCG[5]=8. Conversely, if c=2, then the docu-
ments retrieved at ranks lower than two are pe-
nalized. Hence, the DCG values for the re-
trieved list are <3,5,6.89,6.89,6.89>, and 
DCG[5]=6.89. 
The relevance judgment was performed by 
three experienced physicians. First, the pooling 
method (Voorhees, 2000) was adopted to gen-
erate the candidate relevant documents for 
each test query by taking the top 50 ranked 
documents retrieved by each of the involved 
retrieval models, namely the VSM, BM25 and 
Topic. Two physicians then judged each can-
didate document based on the multilevel rele-
vance criterion. Finally, the documents with 
disagreements between the two physicians 
were judged by the third physician. Table 2 
shows the average number of relevant docu-
ments for the test query set. 
4) Optimal parameter setting: The parameter 
settings of BM25 and Topic were evaluated us-
ing the tuning set. The optimal setting of 
BM25 were k1 =1 and b=0.6. The other two pa-
rameters were set to the default values, i.e., 
2 0k =  and 3 8k = . For the Topic model, the 
parameters required to be evaluated include the 
combination factors, ?  and ? , described in 
(1); the constant const. described in (2), and 
the decay factor, ? , described in (9). The op-
timal settings were 0.3? = ; 0.5? = ; 
const.=0.6 and 0.8? = . 
4.2 Retrieval results 
The results are divided into two groups: the preci-
sion and efficiency. The retrieval precision was 
measured by DCG values. Additionally, a paired, 
two-tailed t-test was used to determine whether the 
performance difference was statistically significant. 
The retrieval efficiency was measure by the query 
processing time, i.e., the time for processing all the 
queries in the test query set. 
Table 3 shows the comparative results of re-
trieval precision. The two variants of BM25, 
namely BM11 and BM15, are also considered in 
comparison. For the word-based retrieval models, 
both BM25 and BM11 outperformed the VSM, and 
BM15 performed worst. The Topic model 
achieved higher DCG values than both the BM-
series models and VSM. The reasons are three-fold. 
First, a negative life event and a symptom can each 
be expressed by different words with the same or 
similar meaning. Therefore, the word-based mod-
els often failed to retrieve the relevant documents 
when different words were used in the input query. 
Second, a word may relate to different events and 
symptoms. For instance, the term "worry about" is 
Relevance Level Avg. Number
Level 1 18.50 
Level 2 9.15 
Level 3 2.20 
Table 2. Average number of relevant documents 
for the test query set. 
 DCG(5) DCG(10) DCG(20) DCG(50) DCG(100) 
Topic 4.7516* 6.9298 7.6040* 8.3606* 9.3974* 
BM25 4.4624 6.7023 7.1156 7.8129 8.6597 
BM11 3.8877 4.9328 5.9589 6.9703 7.7057 
VSM 2.3454 3.3195 4.4609 5.8179 6.6945 
BM15 2.1362 2.6120 3.4487 4.5452 5.7020 
Table 3. DCG values of different retrieval models.  * Topic vs BM25 significantly different (p<0.05) 
Retrieval Model Avg. Time (seconds)
Topic 17.13 
VSM 0.68 
BM25 0.48 
Table 4. Average query processing time of differ-
ent retrieval models. 
1030
a good indicator for both the symptoms <Anxiety> 
and <Hypochondriasis>. This may result in ambi-
guity for the word-based models. Third, the word-
based models cannot capture semantic relations 
between symptoms. The Topic model incorporates 
not only the word-level information, but also more 
useful topic information about depressive problems, 
thus improving the retrieval results. 
The query processing time was measured using 
a personal computer with Windows XP operating 
system, a 2.4GHz Pentium IV processor and 
512MB RAM. Table 4 shows the results. The topic 
model required more processing time than both 
VSM and BM25, since identification of topics in-
volves more detailed analysis, such as semantic 
parsing of sentences and symptom chain construc-
tion. This finding indicates that although the topic 
information can improve the retrieval precision, 
incorporating such high-precision features reduces 
the retrieval efficiency. 
5 Conclusion 
This work has presented the use of topic informa-
tion for retrieving psychiatric consultation docu-
ments. The topic information can provide more 
precise information about users' depressive prob-
lems, thus improving the retrieval precision. The 
proposed framework can also be applied to differ-
ent domains as long as the domain-specific topic 
information is identified. Future work will focus on 
more detailed experiments, including the contribu-
tion of each topic to retrieval precision, the effect 
of using different methods to combine topic infor-
mation, and the evaluation on real users. 
References 
Baeza-Yates, R. and B. Ribeiro-Neto. 1999. Modern 
Information Retrieval. Addison-Wesley, Reading, 
MA. 
Cancedda, N., E. Gaussier, C. Goutte, and J. M. Renders. 
2003. Word-Sequence Kernels. Journal of Machine 
Learning Research, 3(6):1059-1082. 
Fellbaum, C. 1998. WordNet: An Electronic Lexical 
Database. MIT Press, Cambridge, MA. 
Hamilton, M. 1960. A Rating Scale for Depression. 
Journal of Neurology, Neurosurgery and Psychiatry, 
23:56-62 
Jarvelin, K. and J. Kekalainen. 2000. IR Evaluation 
Methods for Retrieving Highly Relevant Documents. 
In Proc. of the 23rd Annual International ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, pages 41-48. 
Lodhi, H., C. Saunders, J. Shawe-Taylor, N. Cristianini, 
and C. Watkins. 2002. Text Classification Using 
String Kernels. Journal of Machine Learning Re-
search, 2(3):419-444. 
Okabe, M., K. Umemura and S. Yamada. 2005. Query 
Expansion with the Minimum User Feedback by 
Transductive Learning. In Proc. of HLT/EMNLP, 
Vancouver, Canada, pages 963-970. 
Pagano, M.E., A.E. Skodol, R.L. Stout, M.T. Shea, S. 
Yen, C.M. Grilo, C.A. Sanislow, D.S. Bender, T.H. 
McGlashan, M.C. Zanarini, and J.G. Gunderson. 
2004. Stressful Life Events as Predictors of Function-
ing: Findings from the Collaborative Longitudinal 
Personality Disorders Study. Acta Psychiatrica Scan-
dinavica, 110: 421-429. 
Robertson, S. E., S. Walker, S. Jones, M. M. Hancock-
Beaulieu, and M.Gatford. 1995. Okapi at TREC-3. In 
Proc. of the Third Text REtrieval Conference (TREC-
3), NIST. 
Robertson, S. E., S. Walker, M. M. Beaulieu, and 
M.Gatford. 1996. Okapi at TREC-4. In Proc. of the 
fourth Text REtrieval Conference (TREC-4), NIST. 
Voorhees, E. M. and D. K. Harman. 2000. Overview of 
the Sixth Text REtrieval Conference (TREC-6). In-
formation Processing and Management, 36(1):3-35. 
Wu, C. H., L. C. Yu, and F. L. Jang. 2005a. Using Se-
mantic Dependencies to Mine Depressive Symptoms 
from Consultation Records. IEEE Intelligent System, 
20(6):50-58. 
Wu, C. H., J. F. Yeh, and M. J. Chen. 2005b. Domain-
Specific FAQ Retrieval Using Independent Aspects. 
ACM Trans. Asian Language Information Processing, 
4(1):1-17. 
Wu, C. H., J. F. Yeh, and Y. S. Lai. 2006. Semantic 
Segment Extraction and Matching for Internet FAQ 
Retrieval. IEEE Trans. Knowledge and Data Engi-
neering, 18(7):930-940. 
Yeh, J. F., C. H. Wu, M. J. Chen, and L. C. Yu. 2004. 
Automated Alignment and Extraction of Bilingual 
Domain Ontology for Cross-Language Domain-
Specific Applications. In Proc. of the 20th COLING, 
Geneva, Switzerland, pages 1140-1146. 
Yu, L. C., C. H. Wu, Yeh, J. F., and F. L. Jang. 2007. 
HAL-based Evolutionary Inference for Pattern Induc-
tion from Psychiatry Web Resources. Accepted by 
IEEE Trans. Evolutionary Computation. 
1031
Proceedings of ACL-08: HLT, pages 1048?1056,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Semantic Class Learning from the Web with Hyponym Pattern Linkage
Graphs
Zornitsa Kozareva
DLSI, University of Alicante
Campus de San Vicente
Alicante, Spain 03080
zkozareva@dlsi.ua.es
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT 84112
riloff@cs.utah.edu
Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
hovy@isi.edu
Abstract
We present a novel approach to weakly super-
vised semantic class learning from the web,
using a single powerful hyponym pattern com-
bined with graph structures, which capture
two properties associated with pattern-based
extractions: popularity and productivity. In-
tuitively, a candidate is popular if it was dis-
covered many times by other instances in the
hyponym pattern. A candidate is productive
if it frequently leads to the discovery of other
instances. Together, these two measures cap-
ture not only frequency of occurrence, but also
cross-checking that the candidate occurs both
near the class name and near other class mem-
bers. We developed two algorithms that begin
with just a class name and one seed instance
and then automatically generate a ranked list
of new class instances. We conducted exper-
iments on four semantic classes and consis-
tently achieved high accuracies.
1 Introduction
Knowing the semantic classes of words (e.g., ?trout?
is a kind of FISH) can be extremely valuable for
many natural language processing tasks. Although
some semantic dictionaries do exist (e.g., Word-
Net (Miller, 1990)), they are rarely complete, espe-
cially for large open classes (e.g., classes of people
and objects) and rapidly changing categories (e.g.,
computer technology). (Roark and Charniak, 1998)
reported that 3 of every 5 terms generated by their
semantic lexicon learner were not present in Word-
Net. Automatic semantic lexicon acquisition could
be used to enhance existing resources such as Word-
Net, or to produce semantic lexicons for specialized
categories or domains.
A variety of methods have been developed for
automatic semantic class identification, under the
rubrics of lexical acquisition, hyponym acquisition,
semantic lexicon induction, semantic class learn-
ing, and web-based information extraction. Many
of these approaches employ surface-level patterns to
identify words and their associated semantic classes.
However, such patterns tend to overgenerate (i.e.,
deliver incorrect results) and hence require addi-
tional filtering mechanisms.
To overcome this problem, we employed one sin-
gle powerful doubly-anchored hyponym pattern to
query the web and extract semantic class instances:
CLASS NAME such as CLASS MEMBER and *.
We hypothesized that a doubly-anchored pattern,
which includes both the class name and a class
member, would achieve high accuracy because of
its specificity. To address concerns about coverage,
we embedded the search in a bootstrapping process.
This method produced many correct instances, but
despite the highly restrictive nature of the pattern,
still produced many incorrect instances. This re-
sult led us to explore new ways to improve the ac-
curacy of hyponym patterns without requiring addi-
tional training resources.
The main contribution of this work is a novel
method for combining hyponym patterns with graph
structures that capture two properties associated
with pattern extraction: popularity and productivity.
Intuitively, a candidate word (or phrase) is popular
if it was discovered many times by other words (or
1048
phrases) in a hyponym pattern. A candidate word is
productive if it frequently leads to the discovery of
other words. Together, these two measures capture
not only frequency of occurrence, but also cross-
checking that the word occurs both near the class
name and near other class members.
We present two algorithms that use hyponym pat-
tern linkage graphs (HPLGs) to represent popularity
and productivity information. The first method uses
a dynamically constructed HPLG to assess the pop-
ularity of each candidate and steer the bootstrapping
process. This approach produces an efficient boot-
strapping process that performs reasonably well, but
it cannot take advantage of productivity information
because of the dynamic nature of the process.
The second method is a two-step procedure that
begins with an exhaustive pattern search that ac-
quires popularity and productivity information about
candidate instances. The candidates are then ranked
based on properties of the HPLG. We conducted ex-
periments with four semantic classes, achieving high
accuracies and outperforming the results reported by
others who have worked on the same classes.
2 Related Work
A substantial amount of research has been done in
the area of semantic class learning, under a variety
of different names and with a variety of different
goals. Given the great deal of similar work in infor-
mation extraction and ontology learning, we focus
here only on techniques for weakly supervised or
unsupervised semantic class (i.e., supertype-based)
learning, since that is most related to the work in
this paper.
Fully unsupervised semantic clustering (e.g.,
(Lin, 1998; Lin and Pantel, 2002; Davidov and Rap-
poport, 2006)) has the disadvantage that it may or
may not produce the types and granularities of se-
mantic classes desired by a user. Another related
line of work is automated ontology construction,
which aims to create lexical hierarchies based on se-
mantic classes (e.g., (Caraballo, 1999; Cimiano and
Volker, 2005; Mann, 2002)), and learning semantic
relations such as meronymy (Berland and Charniak,
1999; Girju et al, 2003).
Our research focuses on semantic lexicon induc-
tion, which aims to generate lists of words that be-
long to a given semantic class (e.g., lists of FISH
or VEHICLE words). Weakly supervised learning
methods for semantic lexicon generation have uti-
lized co-occurrence statistics (Riloff and Shepherd,
1997; Roark and Charniak, 1998), syntactic in-
formation (Tanev and Magnini, 2006; Pantel and
Ravichandran, 2004; Phillips and Riloff, 2002),
lexico-syntactic contextual patterns (e.g., ?resides
in <location>? or ?moved to <location>?) (Riloff
and Jones, 1999; Thelen and Riloff, 2002), and
local and global contexts (Fleischman and Hovy,
2002). These methods have been evaluated only on
fixed corpora1, although (Pantel et al, 2004) demon-
strated how to scale up their algorithms for the web.
Several techniques for semantic class induction
have also been developed specifically for learning
from the web. (Pas?ca, 2004) uses Hearst?s pat-
terns (Hearst, 1992) to learn semantic class instances
and class groups by acquiring contexts around the
pattern. Pasca also developed a second technique
(Pas?ca, 2007b) that creates context vectors for a
group of seed instances by searching web query
logs, and uses them to learn similar instances.
The work most closely related to ours is Hearst?s
early work on hyponym learning (Hearst, 1992)
and more recent work that has followed up on her
idea. Hearst?s system exploited patterns that explic-
itly identify a hyponym relation between a seman-
tic class and a word (e.g., ?such authors as Shake-
speare?). We will refer to these as hyponym pat-
terns. Pasca?s previously mentioned system (Pas?ca,
2004) applies hyponym patterns to the web and ac-
quires contexts around them. The KnowItAll system
(Etzioni et al, 2005) also uses hyponym patterns to
extract class instances from the web and then evalu-
ates them further by computing mutual information
scores based on web queries.
The work by (Widdows and Dorow, 2002) on lex-
ical acquisition is similar to ours because they also
use graph structures to learn semantic classes. How-
ever, their graph is based entirely on syntactic rela-
tions between words, while our graph captures the
ability of instances to find each other in a hyponym
pattern based on web querying, without any part-of-
speech tagging or parsing.
1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated
on web pages, but used a precompiled corpus of downloaded
web pages.
1049
3 Semantic Class Learning with Hyponym
Pattern Linkage Graphs
3.1 A Doubly-Anchored Hyponym Pattern
Our work was motivated by early research on hy-
ponym learning (Hearst, 1992), which applied pat-
terns to a corpus to associate words with semantic
classes. Hearst?s system exploited patterns that ex-
plicitly link a class name with a class member, such
as ?X and other Ys? and ?Ys such as X?. Relying
on surface-level patterns, however, is risky because
incorrect items are frequently extracted due to poly-
semy, idiomatic expressions, parsing errors, etc.
Our work began with the simple idea of using an
extremely specific pattern to extract semantic class
members with high accuracy. Our expectation was
that a very specific pattern would virtually eliminate
the most common types of false hits that are caused
by phenomena such as polysemy and idiomatic ex-
pressions. A concern, however, was that an ex-
tremely specific pattern would suffer from sparse
data and not extract many new instances. By using
the web as a corpus, we hoped that the pattern could
extract at least a few instances for virtually any class,
and then we could gain additional traction by boot-
strapping these instances.
All of the work presented in this paper uses just
one doubly-anchored pattern to identify candidate
instances for a semantic class:
<class name> such as <class member> and *
This pattern has two variables: the name of the se-
mantic class to be learned (class name) and a mem-
ber of the semantic class (class member). The aster-
isk (*) indicates the location of the extracted words.
We describe this pattern as being doubly-anchored
because it is instantiated with both the name of the
semantic class as well as a class member.
For example, the pattern ?CARS such as FORD
and *? will extract automobiles, and the pattern
?PRESIDENTS such as FORD and *? will extract
presidents. The doubly-anchored nature of the pat-
tern serves two purposes. First, it increases the like-
lihood of finding a true list construction for the class.
Our system does not use part-of-speech tagging or
parsing, so the pattern itself is the only guide for
finding an appropriate linguistic context.
Second, the doubly-anchored pattern virtually
Members = {Seed};
P0= ?Class such as Seed and *?;
P = {P0};
iter = 0;
While ((iter < Max Iters) and (P 6= {}))
iter++;
For each Pi ? P
Snippets = web query(Pi);
Candidates = extract words(Snippets,Pi);
Pnew = {};
For each Candidatek ? Candidates
If (Candidatek /? Members);
Members = Members ? {Candidatek};
Pk= ?Class such as Candidatek and *?;
Pnew = Pnew ? { Pk };
P = Pnew;
Figure 1: Reckless Bootstrapping
eliminates ambiguity because the class name and
class member mutually disambiguate each other.
For example, the word FORD could refer to an auto-
mobile or a person, but in the pattern ?CARS such as
FORD and *? it will almost certainly refer to an au-
tomobile. Similarly, the class ?PRESIDENT? could
refer to country presidents or corporate presidents,
and ?BUSH? could refer to a plant or a person. But
in the pattern ?PRESIDENTS such as BUSH?, both
words will surely refer to country presidents.
Another advantage of the doubly-anchored pat-
tern is that an ambiguous or underspecified class
name will be constrained by the presence of the class
member. For example, to generate a list of com-
pany presidents, someone might naively define the
class name as PRESIDENTS. A singly-anchored pat-
tern (e.g., ?PRESIDENTS such as *?) might gener-
ate lists of other types of presidents (e.g., country
presidents, university presidents, etc.). Because the
doubly-anchored pattern also requires a class mem-
ber (e.g., ?PRESIDENTS such as BILL GATES and
*?), it is likely to generate only the desired types of
instances.
3.2 Reckless Bootstrapping
To evaluate the performance of the doubly-anchored
pattern, we began by using the pattern to search the
web and embedded this process in a simple boot-
strapping loop, which is presented in Figure 1. As
input, the user must provide the name of the desired
1050
semantic class (Class) and a seed example (Seed),
which are used to instantiate the pattern. On the
first iteration, the pattern is given to Google as a
web query, and new class members are extracted
from the retrieved text snippets. We wanted the
system to be as language-independent as possible,
so we refrained from using any taggers or parsing
tools. As a result, instances are extracted using only
word boundaries and orthographic information. For
proper name classes, we extract all capitalized words
that immediately follow the pattern. For common
noun classes, we extract just one word, if it is not
capitalized. Examples are shown below, with the ex-
tracted items underlined:
countries such as China and Sri Lanka are ...
fishes such as trout and bass can ...
One limitation is that our system cannot learn
multi-word instances of common noun categories,
or proper names that include uncapitalized words
(e.g., ?United States of America?). These limita-
tions could be easily overcome by incorporating a
noun phrase (NP) chunker and extracting NPs.
Each new class member is then used as a seed in-
stance in the bootstrapping loop. We implemented
this process as breadth-first search, where each ?ply?
of the search process is the result of bootstrapping
the class members learned during the previous it-
eration as seed instances for the next one. During
each iteration, we issue a new web query and add
the newly extracted class members to the queue for
the next cycle. We run this bootstrapping process for
a fixed number of iterations (search ply), or until no
new class members are produced. We will refer to
this process as reckless bootstrapping because there
are no checks of any kind. Every term extracted by
the pattern is assumed to be a class member.
3.2.1 Results
Table 1 shows the results for 4 iterations of reck-
less bootstrapping for four semantic categories: U.S.
states, countries, singers, and fish. The first two
categories are relatively small, closed sets (our gold
standard contains 50 U.S. states and 194 countries).
The singers and fish categories are much larger, open
sets (see Section 4 for details).
Table 1 reveals that the doubly-anchored pattern
achieves high accuracy during the first iteration, but
Iter. countries states singers fish
1 .80 .79 .91 .76
2 .57 .21 .87 .64
3 .21 .18 .86 .54
4 .16 ? .83 .54
Table 1: Reckless Bootstrapping Accuracies
quality deteriorates rapidly as bootstrapping pro-
gresses. Figure 2 shows the recall and precision
curves for countries and states. High precision is
achieved only with low levels of recall for countries.
Our initial hypothesis was that such a specific pat-
tern would be able to maintain high precision be-
cause non-class members would be unlikely to co-
occur with the pattern. But we were surprised to find
that many incorrect entries were generated for rea-
sons such as broken expressions like ?Merce -dez?,
misidentified list constructions (e.g., ?In countries
such as China U.S. Policy is failing...?), and incom-
plete proper names due to insufficient length of the
retrieved text snippet.
Incorporating a noun phrase chunker would elim-
inate some of these cases, but far from all of them.
We concluded that even such a restrictive pattern is
not sufficient for semantic class learning on its own.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1
Pr
ec
is
io
n
Recall
Country/State
Country
State
Figure 2: Recall/precision for reckless bootstrapping
In the next section, we present a new approach
that creates a Hyponym Pattern Linkage Graph to
steer bootstrapping and improve accuracy.
3.3 Using Dynamic Graphs to Steer
Bootstrapping
Intuitively, we expect true class members to occur
frequently in pattern contexts with other class mem-
1051
bers. To operationalize this intuition, we create a hy-
ponym pattern linkage graph, which represents the
frequencies with which candidate instances generate
each other in the pattern contexts.
We define a hyponym pattern linkage graph
(HPLG) as a G = (V,E), where each vertex v ? V
is a candidate instance and each edge (u, v) ? E
means that instance v was generated by instance u.
The weight w of an edge is the frequency with which
u generated v. For example, consider the following
sentence, where the pattern is italicized and the ex-
tracted instance is underlined:
Countries such as China and Laos have been...
In the HPLG, an edge e = (China, Laos) would
be created because the pattern anchored by China
extracted Laos as a new candidate instance. If this
pattern extracted Laos from 15 different snippets,
then the edge?s weight would be 15. The in-degree
of a node represents its popularity, i.e., the number
of instance occurrences that generated it.
The graph is constructed dynamically as boot-
strapping progresses. Initially, the seed is the only
trusted class member and the only vertex in the
graph. The bootstrapping process begins by instan-
tiating the doubly-anchored pattern with the seed
class member, issuing a web query to generate new
candidate instances, and adding these new instances
to the graph. A score is then assigned to every node
in the graph, using one of several different metrics
defined below. The highest-scoring unexplored node
is then added to the set of trusted class members, and
used as the seed for the next bootstrapping iteration.
We experimented with three scoring functions for
selecting nodes. The In-Degree (inD) score for ver-
tex v is the sum of the weights of all incoming edges
(u, v), where u is a trusted class member. Intuitively,
this captures the popularity of v among instances
that have already been identified as good instances.
The Best Edge (BE) score for vertex v is the maxi-
mum edge weight among the incoming edges (u, v),
where u is a trusted class member.
The Key Player Problem (KPP) measure is used in
social network analysis (Borgatti and Everett, 2006)
to identify nodes whose removal would result in a
residual network of minimum cohesion. A node re-
ceives a high value if it is highly connected and rel-
atively close to most other nodes in the graph. The
KPP score for vertex v is computed as:
KPP (v) =
?
u?V
1
d(u, v)
|V |?1
where d(u, v) is the shortest path between two ver-
tices, where u is a trusted node. For tie-breaking, the
distances are multiplied by the weight of the edge.
Note that all of these measures rely only on in-
coming edges because a node does not acquire out-
going edges until it has already been selected as a
trusted class member and used to acquire new in-
stances. In the next section, we describe a two-step
process for creating graphs that can take advantage
of both incoming and outgoing edges.
3.4 Re-Ranking with Precompiled Graphs
One way to try to confirm (or disconfirm) whether
a candidate instance is a true class member is to see
whether it can produce new candidate instances. If
we instantiate our pattern with the candidate (i.e.,
?CLASS NAME such as CANDIDATE and *?) and
successfully extract many new instances, then this
is evidence that the candidate frequently occurs with
the CLASS NAME in list constructions. We will re-
fer to the ability of a candidate to generate new in-
stances as its productivity.
The previous bootstrapping algorithm uses a dy-
namically constructed graph that is constantly evolv-
ing as new nodes are selected and explored. Each
node is scored based only on the set of instances
that have been generated and identified as ?trusted?
at that point in the bootstrapping process. To use
productivity information, we must adopt a different
procedure because we need to know not only who
generated each candidate, but also the complete set
of instances that the candidate itself can generate.
We adopted a two-step process that can use both
popularity and productivity information in a hy-
ponym pattern linkage graph to assess the quality of
candidate instances. First, we perform reckless boot-
strapping for a class name and seed until no new
instances are generated. Second, we assign a score
to each node in the graph using a scoring function
that takes into account both the in-degree (popular-
ity) and out-degree (productivity) of each node. We
experimented with four different scoring functions,
some of which were motivated by work on word
1052
sense disambiguation to identify the most ?impor-
tant? node in a graph containing its possible senses
(Navigli and Lapata, 2007).
The Out-degree (outD) score for vertex v is the
weighted sum of v?s outgoing edges, normalized by
the number of other nodes in the graph.
outD(v) =
?
?(v,p)?E
w(v, p)
|V |?1
This measure captures only productivity, while the
next three measures consider both productivity and
popularity. The Total-degree (totD) score for ver-
tex v is the weighted sum of both incoming and
outgoing edges, normalized by the number of other
nodes in the graph. The Betweenness (BT) score
(Freeman, 1979) considers a vertex to be important
if it occurs on many shortest paths between other
vertices.
BT (v) =
?
s,t?V :s 6=v 6=t
?st(v)
?st
where ?st is the number of shortest paths from s to t,
and ?st(v) is the number of shortest paths from s to
t that pass through vertex v. PageRank (Page et al,
1998) establishes the relative importance of a ver-
tex v through an iterative Markov chain model. The
PageRank (PR) score of a vertex v is determined
on the basis of the nodes it is connected to.
PR(v) = (1??)|V | + ?
?
u,v?E
PR(u)
outdegree(u)
? is a damping factor that we set to 0.85. We dis-
carded all instances that produced zero productivity
links, meaning that they did not generate any other
candidates when used in web queries.
4 Experimental evaluation
4.1 Data
We evaluated our algorithms on four semantic cat-
egories: U.S. states, countries, singers, and fish.
The states and countries categories are relatively
small, closed sets: our gold standards consist of 50
U.S. states and 194 countries (based on a list found
on Wikipedia). The singers and fish categories are
much larger, open classes. As our gold standard for
fish, we used a list of common fish names found on
Wikipedia.2 All the singer names generated by our
2We also counted as correct plural versions of items found
on the list. The total size of our fish list is 1102.
States
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
25 1.0 1.0 1.0 1.0 1.0 .88 .88
50 .96 .98 .98 1.0 1.0 .86 .82
64 .77 .78 .77 .78 .78 .77 .67
Countries
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
50 .98 .97 .98 1.0 1.0 .98 .97
100 .96 .97 .94 1.0 .99 .97 .95
150 .90 .92 .91 1.0 .95 .94 .92
200 .83 .81 .83 .90 .87 .82 .80
300 .60 .59 .61 .61 .62 .56 .60
323 .57 .55 .57 .57 .58 .52 .57
Singers
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
10 .92 .96 .92 1.0 1.0 1.0 1.0
25 .89 .90 .91 1.0 1.0 1.0 .99
50 .92 .85 .92 .97 .98 .95 .97
75 .89 .83 .91 .96 .95 .93 .95
100 .86 .81 .89 .96 .93 .94 .94
150 .86 .79 .88 .95 .92 .93 .87
180 .86 .80 .87 .91 .91 .91 .88
Fish
Popularity Prd Pop&Prd
N BE KPP inD outD totD BT PR
10 .90 .90 .90 1.0 1.0 .90 .70
25 .80 .88 .76 1.0 .96 .96 .72
50 .82 .80 .78 1.0 .94 .88 .66
75 .72 .69 .72 .93 .87 .79 .64
100 .63 .68 .66 .84 .80 .74 .62
116 .60 .65 .66 .80 .78 .71 .59
Table 2: Accuracies for each semantic class
algorithms were manually reviewed for correctness.
We evaluated performance in terms of accuracy (the
percentage of instances that were correct).3
4.2 Performance
Table 2 shows the accuracy results of the two al-
gorithms that use hyponym pattern linkage graphs.
We display results for the top-ranked N candidates,
for all instances that have a productivity value >
zero.4 The Popularity columns show results for the
3We never generated duplicates so the instances are distinct.
4Obviously, this cutoff is not available to the popularity-
based bootstrapping algorithm, but here we are just comparing
the top N results for both algorithms.
1053
bootstrapping algorithm described in Section 3.3,
using three different scoring functions. The re-
sults for the ranking algorithm described in Sec-
tion 3.4 are shown in the Productivity (Prd) and
Popularity&Productivity (Pop&Prd) columns. For
the states, countries, and singers categories, we ran-
domly selected 5 different initial seeds and then av-
eraged the results. For the fish category we ran each
algorithm using just the seed ?salmon?.
The popularity-based metrics produced good ac-
curacies on the states, countries, and singers cate-
gories under all 3 scoring functions. For fish, KPP
performed better than the others.
The Out-degree (outD) scoring function, which
uses only Productivity information, obtained the
best results across all 4 categories. OutD achieved
100% accuracy for the first 50 states and fish, 100%
accuracy for the top 150 countries, and 97% accu-
racy for the top 50 singers. The three scoring met-
rics that use both popularity and productivity also
performed well, but productivity information by it-
self seems to perform better in some cases.
It can be difficult to compare the results of differ-
ent semantic class learners because there is no stan-
dard set of benchmark categories, so researchers re-
port results for different classes. For the state and
country categories, however, we can compare our
results with that of other web-based semantic class
learners such as Pasca (Pas?ca, 2007a) and the Know-
ItAll system (Etzioni et al, 2005). For the U.S.
states category, our system achieved 100% recall
and 100% precision for the first 50 items generated,
and KnowItAll performed similarly achieving 98%
recall with 100% precision. Pasca did not evaluate
his system on states.
For the countries category, our system achieved
100% precision for the first 150 generated instances
(77% recall). (Pas?ca, 2007a) reports results of 100%
precision for the first 25 instances generated, and
82% precision for the first 150 instances gener-
ated. The KnowItAll system (Etzioni et al, 2005)
achieved 97% precision with 58% recall, and 79%
precision with 87% recall.5 To the best of our
knowledge, other researchers have not reported re-
sults for the singer and fish categories.
5(Etzioni et al, 2005) do not report exactly how many coun-
tries were in their gold standard.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0  50  100  150  200  250  300  350  400
Ac
cu
ra
cy
Iterations
outD
inD
cutoff, t
Figure 3: Learning curve for Placido Domingo
Figure 3 shows the learning curve for both al-
gorithms using their best scoring functions on the
singer category with Placido Domingo as the initial
seed. In total, 400 candidate words were generated.
The Out-degree scoring function ranked the candi-
dates well. Figure 3 also includes a vertical line
indicating where the candidate list was cut (at 180
instances) based on the zero productivity cutoff.
One observation is that the rankings do a good
job of identifying borderline cases, which typically
are ranked just below most correct instances but just
above the obviously bad entries. For example, for
states, the 50 U.S. states are ranked first, followed
by 14 more entries (in order):
Russia, Ukraine, Uzbekistan, Azerbaijan,
Moldova, Tajikistan, Armenia, Chicago,
Boston, Atlanta, Detroit, Philadelphia, Tampa,
Moldavia
The first 7 entries are all former states of the So-
viet Union. In retrospect, we realized that we
should have searched for ?U.S. states? instead of just
?states?. This example illustrates the power of the
doubly-anchored hyponym pattern to correctly iden-
tify our intended semantic class by disambiguating
our class name based on the seed class member.
The algorithms also seem to be robust with re-
spect to initial seed choice. For the states, coun-
tries, and singers categories, we ran experiments
with 5 different initial seeds, which were randomly
selected. The 5 country seeds represented a diverse
set of nations, some of which are rarely mentioned in
the news: Brazil, France, Guinea-Bissau, Uganda,
1054
and Zimbabwe. All of these seeds obtained ? 92%
recall with ? 90% precision.
4.3 Error Analysis
We examined the incorrect instances produced by
our algorithms and found that most of them fell into
five categories.
Type 1 errors were caused by incorrect proper
name extraction. For example, in the sentence
?states such as Georgia and English speaking coun-
tries like Canada...?, ?English? was extracted as
a state. These errors resulted from complex noun
phrases and conjunctions, as well as unusual syn-
tactic constructions. An NP chunker might prevent
some of these cases, but we suspect that many of
them would have been misparsed regardless.
Type 2 errors were caused by instances that for-
merly belonged to the semantic class (e.g., Serbia-
Montenegro and Czechoslovakia are no longer coun-
tries). In this error type, we also include border-
line cases that could arguably belong to the semantic
class (e.g., Wales as a country).
Type 3 errors were spelling variants (e.g., Kyrgys-
tan vs. Kyrgyzhstan) and name variants (e.g., Bey-
once vs. Beyonce Knowles). Officially, every entity
has one official spelling and one complete name, but
in practice there are often variations that may occur
nearly as frequently as the official name. For exam-
ple, it is most common to refer to the singer Beyonce
by just her first name.
Type 4 errors were caused by sentences that were
just flat out wrong in their factual assertions. For ex-
ample, some sentences referred to ?North America?
as a country.
Type 5 errors were caused by broken expressions
found in the retrieved snippets (e.g. Michi -gan).
These errors may be fixable by cleaning up the web
pages or applying heuristics to prevent or recognize
partial words.
It is worth noting that incorrect instances of Types
2 and 3 may not be problematic to encounter in a
dictionary or ontology. Name variants and former
class members may in fact be useful to have.
5 Conclusions
Combining hyponym patterns with pattern linkage
graphs is an effective way to produce a highly ac-
curate semantic class learner that requires truly min-
imal supervision: just the class name and one class
member as a seed. Our results consistently produced
high accuracy and for the states and countries cate-
gories produced very high recall.
The singers and fish categories, which are much
larger open classes, also achieved high accuracy and
generated many instances, but the resulting lists are
far from complete. Even on the web, the doubly-
anchored hyponym pattern eventually ran out of
steam and could not produce more instances. How-
ever, all of our experiments were conducted using
just a single hyponym pattern. Other researchers
have successfully used sets of hyponym patterns
(e.g., (Hearst, 1992; Etzioni et al, 2005; Pas?ca,
2004)), and multiple patterns could be used with
our algorithms as well. Incorporating additional hy-
ponym patterns will almost certainly improve cover-
age, and could potentially improve the quality of the
graphs as well.
Our popularity-based algorithm was very effec-
tive and is practical to use. Our best-performing al-
gorithm, however, was the 2-step process that be-
gins with an exhaustive search (reckless bootstrap-
ping) and then ranks the candidates using the Out-
degree scoring function, which represents produc-
tivity. The first step is expensive, however, because
it exhaustively applies the pattern to the web until
no more extractions are found. In our evaluation, we
ran this process on a single PC and it usually finished
overnight, and we were able to learn a substantial
number of new class instances. If more hyponym
patterns are used, then this could get considerably
more expensive, but the process could be easily par-
allelized to perform queries across a cluster of ma-
chines. With access to a cluster of ordinary PCs,
this technique could be used to automatically create
extremely large, high-quality semantic lexicons, for
virtually any categories, without external training re-
sources.
Acknowledgments
This research was supported in part by the Department
of Homeland Security under ONR Grants N00014-07-1-014
and N0014-07-1-0152, the European Union Sixth Framework
project QALLME FP6 IST-033860, and the Spanish Ministry
of Science and Technology TEXT-MESS TIN2006-15265-C06-
01.
1055
References
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proc. of the 37th Annual Meeting of
the Association for Computational Linguistics.
S. Borgatti and M. Everett. 2006. A graph-theoretic per-
spective on centrality. Social Networks, 28(4).
S. Caraballo. 1999. Automatic Acquisition of a
Hypernym-Labeled Noun Hierarchy from Text. In
Proc. of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 120?126.
P. Cimiano and J. Volker. 2005. Towards large-scale,
open-domain and ontology-based named entity classi-
fication. In Proc. of Recent Advances in Natural Lan-
guage Processing, pages 166?172.
D. Davidov and A. Rappoport. 2006. Efficient unsu-
pervised discovery of word categories using symmet-
ric patterns and high frequency words. In Proc. of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the ACL.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91?134, June.
M.B. Fleischman and E.H. Hovy. 2002. Fine grained
classification of named entities. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
C. Freeman. 1979. Centrality in social networks: Con-
ceptual clarification. Social Networks, 1:215?239.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing semantic constraints for the automatic discovery of
part-whole relations. In Proc. of Conference of HLT /
North American Chapter of the Association for Com-
putational Linguistics.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th confer-
ence on Computational linguistics, pages 539?545.
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proc. of the 19th International Conference on Com-
putational linguistics, pages 1?7.
D. Lin. 1998. Automatic retrieval and clustering of sim-
ilar words. In Proc. of the 17th international confer-
ence on Computational linguistics, pages 768?774.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In Proc. of the 19th International
Conference on Computational Linguistics, pages 1?7.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal of Lexicography, 3(4).
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In Proc. of the 20th International Joint Confer-
ence on Artificial Intelligence, pages 1683?1688.
M. Pas?ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137?145.
M. Pas?ca. 2007a. Organizing and searching the world
wide web of facts ? step two: harnessing the wisdom
of the crowds. In Proc. of the 16th International Con-
ference on World Wide Web, pages 101?110.
M. Pas?ca. 2007b. Weakly-supervised discovery of
named entities using web search queries. In Proc. of
the sixteenth ACM conference on Conference on infor-
mation and knowledge management, pages 683?690.
L. Page, S. Brin, R. Motwani, and T. Winograd. 1998.
The pagerank citation ranking: Bringing order to the
web. Technical report, Stanford Digital Library Tech-
nologies Project.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proc. of Conference of
HLT / North American Chapter of the Association for
Computational Linguistics, pages 321?328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. To-
wards terascale knowledge acquisition. In Proc. of the
20th international conference on Computational Lin-
guistics, page 771.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proc. of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proc. of the Sixteenth National Conference on Arti-
ficial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proc. of
the Second Conference on Empirical Methods in Nat-
ural Language Processing, pages 117?124.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proc. of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1110?1116.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proc. of 11st
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pat-
tern Contexts. In Proc. of the 2002 Conference on Em-
pirical Methods in Natural Language Processing.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1?7.
1056
Assigning Time-Stamps to Event-Clauses
Elena Filatova
Information Sciences Institute
University of Southern
California
elena@isi.edu
Eduard Hovy
Information Sciences Institute
University of Southern
California
hovy@isi.edu
Abstract
We describe a procedure for arranging into a
time-line the contents of news stories
describing the development of some situation.
We describe the parts of the system that deal
with 1. breaking sentences into event-clauses
and 2. resolving both explicit and implicit
temporal references. Evaluations show a
performance of  52%, compared to humans.
1 Introduction
Linguists who have analyzed news stories
(Schokkenbroek,1999; Bell,1997; Ohtsuka and
Brewer,1992, etc.) noticed that ?narratives1 are
about more than one event and these events are
temporally ordered. Though it seems most
logical to recapitulate events in the order in
which they happened, i.e. in chronological order,
the events are often presented in a different
sequence?.  The  same  paper  states that ?it is
important to reconstruct the underlying event
order2 for narrative analysis to assign meaning to
the sequence in which the events are narrated at
the level of discourse structure?.If the
underlying event structure cannot be
reconstructed, it may well be impossible to
understand the narrative at all, let alne assign
meaning to its structure?.
Several psycholinguistic experiments show
the influence of event-arrangement in news
stories on the ease of comprehension by readers.
Duszak (1991) had readers reconstruct a news
story from the randomized sentences. According
to his experiments readers have a default strategy
by which?in the absence of cues to the
contrary?they re-impose chronological order on
events in the discourse.
                                                
1 Schokkenbroek (1999) uses the term narrative for news
stories that relate more than one event.
2 i.e., chronological order.
The problem of reconstructing the
chronological order of events becomes more
complicated if we have to deal with separate
news stories, written at different times and
describing the development of some situation, as
is the case for multidocument summarization.
By judicious definition, one can make this
problem easy or hard.  Selecting only specific
items to assign time-points to, and then
measuring correctness on them alone, may give
high performance but leave much of the text
unassigned.  We address the problem of
assigning a time-point to every clause in the text.
Our approach is to break the news stories into
their constituent events and to assign time-
stamps?either time-points or time-intervals?to
these events. When assigning time-stamps we
analyze both implicit time references (mainly
through the tense system) and explicit ones
(temporal adverbials) such as ?on Monday?, ?in
1998?, etc. The result of the work is a prototype
program which takes as input set of news stories
broken into separate sentences and produces as
output a text that combines all the events from all
the articles, organized in chronological order.
2 Data
As data we used a set of news stories about an
earthquake in Afghanistan that occurred at the
end of May in 1998. These news stories were
taken from CNN, ABC, and APW websites for
the DUC-2000 meeting. The stories were all
written within one week. Some of the texts were
written on the same day. In addition to a
description of the May earthquake, these texts
contain references to another earthquake that
occurred in the same region in February 1998.
3 Identifying Events
To divide sentences into event-clauses we use
CONTEX (Hermjakob, 1997), a parser that
produces a syntactic parse tree augmented with
semantic labels.  CONTEX uses machine
learning techniques to induce a grammar from a
given treebanks.  A sample output of CONTEX
is given in Appendix 1.
To divide a sentence into event-clauses the
parse tree output by CONTEX is analyzed from
left to right (root to leaf). The ::CAT field for
each node provides the necessary information
about whether the node under consideration
forms a part of its upper level event or whether it
introduces a new event.  ::CAT features that
indicate new events are: S-CLAUSE, S-SNT, S-
SUB-CLAUSE, S-PART-CLAUSE, S-REL-
CLAUSE.  These features mark clauses which
contain both subject (one or several NPs) and
predicate (VP containing one or several verbs).
The above procedure classifies a clause
containing more than one verb as a simple
clause.  Such clauses are treated as one event and
only one time-point will be assigned to them.
This is fine when the second verb is used in the
same tense as the first, but may be wrong in
some cases, as in He lives in this house now and
will stay here for one more year. There are no
such clauses in the analyzed data, so we ignore
this complication for the present.
The parse tree also gives information about
the tense of verbs, used later for time assignment.
In order to facilitate subsequent processing,
we wish to rephrase relative clauses as full
independent sentences.  We therefore have to
replace pronouns where it is possible by their
antecedents. Very often the parser gives
information about the referential antecedents (in
the example below, Russia). Therefore we
introduced the rule: if it is possible to identify
the referent, put it into the event-clause:
1.Russia <..> said;
2.which <Russia> has loaned helicopters in
previous disasters;
3.it <Russia> would consider sending aid.
But sometimes the antecedent is identified
incorrectly.
Qulle charged that the United Nations and
non-governmental organizations involved
in the relief were poorly coordinated,
which was costing lives.
Here the antecedent for which is identified as the
relief, and gives which <the relief> was costing
lives instead of which <poor coordination> was
costing lives. Fortunately, in most cases our rule
works correctly.
Although the event-identifier works
reasonably well, breaking text into event-clauses
needs further investigation. Table 1 shows the
performance of the system. Two kinds of
mistakes are made by the event identifier: those
caused by CONTEX (it does not identify clauses
with omitted predicate, etc.) and those caused by
the fact that our clause identifier does too
shallow analysis of the parse tree.
4 Time-stamper
According to (Bell, 1997) ?time is expressed at
different levels?in the morphology and syntax of
the verb phrase, in time adverbials whether lexical
or phrasal, and in the discourse structure of the
stories above the sentence?.
4.1 Representation of Time-points and -intervals
For the present work we use slightly modified
time representations suggested in (Allen, 1991).
Formats used for time representation:
? {YYYY:DDD:W}3 Used when it is possible to
point out the particular day the event occurred.
{YYYY1:DDD1:W1},{YYYY2:DDD2:W2}...
Used when it is possible to point out several
concrete days when the events occurred.
? {YYYY1:DDD1:W1}---{YYYY2:DDD2:W2}
Used when it is possible to point out a range of days
when the event occurred.
? <<<{YYYY:DDD:W} Used when it is possible to
say the event occurred {YYYY:DDD:W} or earlier.
? >>>{YYYY:DDD:W} Used when it is possible to
say the event occurred {YYYY:DDD:W} or later.
4.2 Time-points Used for the Time-stamper
We use two anchoring time points:
1. Time of the article
We require that the first sentence for each article
contains time information. For example:
T1 (05/30/1998:Saturday 18:35:42.49)
PAKINSTAN MAY BE PREPARING FOR
ANOTHER TEST.
The date information is in bold. We denote by Ti
the reference time-point for the article, where i
                                                
3 YYYY?year number, DDD?absolute number of the day
within the year (1?366), W-?umber of the day in a week
(1- Monday, ? 7- Saturday). If it is impossible to point out
the day of the week then W is assigned 0.
 Recall = (# of event-clauses correctly identified by system) / ( # of event-clauses identified manually)
 Precision = (# of event-clauses correctly identified by system) / ( # of event-clauses identified by system)
Text
number
# of clauses
by human
# of clauses
by system
#
correct
recall precision
Text 1 7 6 5 5/7 = 71.42% 5/6 = 83.33%
Text 2 27 31 15 15/27 = 55.55% 15/31 = 48.38%
Text 3 5 8 3 3/5 = 60% 3/8 = 37.5%
Text 4 28 28 18 18/28 = 64.28% 18/28 = 64.28%
Text 5 33 36 19 19/33 = 57.57% 19/36= 52.77%
Text 6 58 63 36 36/58=62.07% 36/63 = 57.14%
total 158 172 96 96/158 = 60.76% 96/172 = 55.81%
Table 1. Recall and precision scores for event identifier.
means that it is the time point of article i.  The
symbol Ti is used as a comparative time-point if the
time the article was written is unknown. The
information in brackets gives the exact date the
article was written, which is the main anchor point
for the time-stamper. The information about hours,
minutes and seconds is ignored for the present.
2. Last time point assigned in the same sentence
While analyzing different event-clauses within the
same sentence we keep track of what time-point was
most recently assigned within this sentence. If
needed, we can refer to this time-point. In case the
most recent time information assigned is not a date
but an interval we record information about both
time boundaries. When the program proceeds to the
next sentence, the variable for the most recently
assigned date becomes undefined. In most cases this
assumption works correctly (example 5.2?5.3):
5.2.1 In the village of Kol, hundreds of people
swarmed a United Nations helicopter
5.2.2 that <a United Nations helicopter>
touched down three days after Saturday?s
earthquake
5.2.3 <after Saturday?s earthquake> struck a
remote mountainous area rocked three months
earlier by another massive quake
5.2.4 that <another massive quake> claimed
some 2,300 victims.
5.3.1 On Monday and Tuesday, U.N. helicopters
evacuated 50 of the most seriously injured to
emergency medical centers.
The last time interval assigned for sentence 5.2 is
{1998:53:0}---{1998:71:0}, which gives an
approximate range of days when the previous
earthquake happened. But the information in
sentence 5.3 is about the recent earthquake and not
about the previous one of 3 months earlier, which is
why it would be a mistake to point Monday and
Tuesday within that range.
Mani and Wilson (2000) point out ?over half of
the errors [made by his time-stamper] were due to
propagation of spreading of an incorrect event time
to neighboring events?. The rule of dropping the
most recently assigned date as an anchor point when
proceeding to the next sentence very often helps us
to avoid this problem.
There are however cases where dropping the
most recent time as an anchor when proceeding to
the next sentence causes errors:
4.8.1 But in February a devastating earthquake
in the same region killed 2,300 people and left
thousands of people homeless.
4.9.1 At the time international aid workers
suffered through a logistical nightmare to reach
the snow-bound region with assistance.
It is clear that sentence 4.9 is the continuation of
sentence 4.8 and refers to the same time point
(February earthquake). In this case our rule assigns
the wrong time to 4.9.1. Still we retain this rule
because it is more frequently correct than incorrect.
4.3 Preprocessing
First, the text divided into event-clauses is run
through a program that extracts all the date-stamps
(made available by Kevin Knight, ISI). In most
cases this program does not miss any date-stamps
and extracts only the correct ones. The only cases in
which it did not work properly for the texts were:
? sentence 1.h1
PAKISTAN MAY BE PREPARING FOR
ANOTHER TEST.
Here the modal verb MAY was assumed to be the
month, given that it started with a capital letter.
? sentence 6.24
Tuberculosis is already common in the area
where people live in close quarters and have
poor hygiene
here the noun quarters, which in this case is used in
the sense immediate contact or close range
(Merriam-Webster dictionary), was assumed to be
used in the sense the fourth part of a measure of
time (Merriam-Webster dictionary).
After extracting all the date-phrases we proceed
to time assignment.
4.4 Rules of Time Assignment
When assigning a time to an event, we select the
time to be either the most recently assigned date or,
if the value of the most recently assigned date is
undefined, to the date of the article.  We use a set of
rules to perform this selection. These rules can be
divided into two main categories: those that work
for sentences containing explicit date information,
and those that work for sentences that do not.
4.4.1 Assigning Time-Stamps to the Clauses
with Explicit Date Information
? Day of the Week
If the day-of-the-week used in the event-
clause is the same as that of the article (or the
most recently assigned date, if it is defined), and
there no words before it could signal that the
described event happened earlier or will happen
later, then the time-point of the article (or the
most recently assigned date, if it is defined) is
assigned to this event. If before or after a day-of-
the-week there is a word/words signaling that the
event happened earlier of will happen later then
the time-point is assigned in accordance with this
signal-word and the most recently assigned date,
if it is defined.
If the day-of-the-week used in the event-
clause is not the same as that of the article (or the
most recently assigned date, if it is defined), then
if there are words pointing out that the event
happened before the article was written or the
tense used in the clause is past, then the time for
the event-clause is assigned in accordance with
this word (such words we call signal-words), or
the most recent day corresponding to the current
day-of-the-week is chosen. If the signal-word
points out that the event will happen after the
article was written or the tense used in the clause
is future, then the time for the event-clause is
assigned in accordance with the signal word or
the closest subsequent day corresponding to the
current day-of-the-week.
5.3.1 On Monday and Tuesday, U.N.
helicopters evacuated 50 of the most
seriously injured to emergency medical
centers.
The time for article 5 is (06/06/1998:Tuesday
15:17:00). So, the time assigned to this event-
clause is: 5.3.1 {1998:151:1}, {1998:152:2}.
? Name of Month
The rules are the same as for a day-of-the-
week, but in this case a time-range is assigned to
the event-clause. The left boundary of the range
is the first day of the month, the right boundary
is the last day of the month, and though it is
possible to figure out the days of weeks for these
boundaries, this aspect is ignored for the present.
4.8.1 But in February a devastating
earthquake in the same region killed 2,300
people and left thousands of people
homeless.
The time for article 4 is (05/30/1998:Saturday
14:41:00). So, the time assigned to this event-
clause is 4.8.1 {1998:32:0}---{1998:60:0}.
In the analyzed corpus there is a case where
the presence of a name of month leads to a
wrong time-stamping:
6.3.1 Estimates say
6.3.2 up to 5,000 people died from the May
30 quake,
6.3.3 more than twice as many fatalities as in
the February disaster.
Because of February, a wrong time-interval is
assigned to clause 6.3.3, namely {1998:32:0}---
{1998:60:0}. As this event-clause is a description
of the latest news as compared to some figures it
should have the time-point of the article. Such
cases present a good possibility for the use of
machine learning techniques to disambiguate
between the cases where we should take into
account date-phrase information and where not.
? Weeks, Days, Months, Years
We might have date-stamps where the words
weeks, days, months, years are used with
modifiers. For example
5.2.1 In the village of Kol, hundreds of
people swarmed a United Nations helicopter
5.2.2 that <a United Nations helicopter>
touched down three days after Saturday?s
earthquake
5.2.3 after Saturday?s earthquake struck a
remote mountainous area rocked three
months earlier by another massive quake
5.2.4 that <another massive quake> claimed
some 2,300 victims.
In event-clause 5.2.3 the expression three months
earlier is used. It is clear that to get the time for
the event it is not enough to subtract 3 months
from the time of the article because the above
expression gives an approximate range within
which this event could happen and not a
particular date. For such cases we invented the
following rule:
Time=multiplier*length4; (in this case 3*30);
Day=DDD-Time; (for years Year=YYYY-Time)
Left boundary of the range=
Day-round (10%(Day));
(for years = Year - round(10%(Year)))
Right boundary of the range =
       Day + round (10%(Day));
(for years = Year + round (10%(Year)))
For event 5.2.3 the time range will be
{1998:53:0}---{1998:71:0} (the exact date of the
article is {1998:152:2}).
If the modifier used with weeks, days, months
or years is several, then the multiplier used in (1)
is equal to 2.
? When, Since, After, Before, etc.
If an event-clause does not contain any date-
phrase but contains one of the words ?when?,
?since?, ?after?, ?before?, etc., it might mean that
this clause refers to an event, the time of which
can be used as a reference point for the event
under analysis. In this case we ask the user to
insert the time for this reference event manually.
This rule can cause problems in cases where
?after? or ?before? are used not as temporal
connectors but as spatial ones, though in the
analyzed texts we did not face this problem.
4.4.2 Assigning Time-Stamps to the Clauses
without Explicit Date Information
                                                
4 For days, length is equal to 1, weeks?7, months?30.
? Present/Past Perfect
If the current event-clause refers to a time-
point in Present/Past Perfect tense, then an open-
ended time-interval is assigned to this event. The
starting point is unknown; the end-point is either
the most recently assigned date or the time-point
of the article.
? Future Tense
      If the current event-clause contains a verb in
future tense (one of the verbs ?shall?, ?will?,
?should?, ?would?, ?might? is present in the
clause) then the open-ended time-interval
assigned to this event-clause has the starting
point at either the most recently assigned date or
the date of the article.
? Other Tenses
Other tenses that can be identified with the
help of CONTEX are Present and Past
Indefinite. In the analyzed data all the verbs in
Present Indefinite are given the most recently
assigned date (or the date of the article). The
situation with Past Indefinite is much more
complicated and requires further investigation of
more data. News stories usually describe the
events that already took place at some time in the
past, which is why even if the day when the
event happened is not over, past tense is very
often used for the description (this is especially
noticeable for US news of European, Asian,
African and Australian events). This means that
very often an event-clause containing a verb in
Past Indefinite Tense can be assigned the most
recently assigned date (or the date of the article).
It might prove useful to use machine learned
rules for such cases.
? No verb in the event-clause
If there is no verb in the event-clause then the
most recently assigned date (or the date of the
article) is assigned to the event-clause.
4.5 Sources of Errors for Time-stamper
We ran the time-stamper program on two
types of data: list of event-clauses extracted by
the event identifier and list of event-clauses
created manually. Tables 2 and 3 show the
results.  In the former case we analyzed only the
correctly identified clauses. One can see that
even on manually created data the performance
of the time-stamper is not 100%. Why?
Some errors are caused by assigning the time
based on the date-phrase present in the event-
clause, when this date-phrase is not an adverbial
time modifier but an attribute. For example,
 1. Estimates say
2. up to 5,000 people died from the May 30
earthquake,
3. more than twice as many fatalities as in
the February disaster.
The third event describes the May 30 earthquake
but the time interval given for this event is
{1998:32:0}---{1998:60:0} (i.e., the event
happened in February). It might be possible to
use machine learned rules to correct such cases.
One more significant source of errors is the
writing style:
1. ?When I left early this morning,
2. everything was fine.
3. After the earthquake, I came back,
4. and the house had collapsed.
5. I looked for two days and gave up.
6. Everybody gave up
When the reader sees early this morning he or
she tends to assign to this clause the time of the
article, but later as seeing looked for two days,
realizes that the time of the clause containing
early this morning is two days earlier than the
time of the article. It seems that the errors caused
by the writing style can hardly be avoided.
If an event happened at some time-point but
according to the information in the sentence we
can assign only a time-interval to this event (for
example, February Earthquake) then we say that
the time-interval is assigned correctly if the
necessary time-point is within this time-interval
5 Time-line for Several News Stories and its
Applications
After stamping all the news stories from the
analyzed set, we arrange the event-clauses from
all the articles into a chronological order. After
doing that we obtain a new set of event-clauses
which can easily be divided into two subsets?the
first one containing all the references to the
February earthquake, the second one containing
the list of event-clauses in chronological order,
describing what happened in May.
Such a text where all the events are organized
in a chronological order might be very helpful in
multidocument summarization, where it is
important to include into the final summary not
only the most important information but also the
most recent one. The output of the presented
system gives the information about the time-
order of the events described in several
documents.
6 Related work
Several linguistic and psycholinguistic studies
deal with the problem of time-arrangement of
different texts. The research presented in these
studies highlights many problems but does not
solve them.
As for computational applications of time
theories, most work was done on temporal
expressions that appear in scheduling dialogues
(Busemann et al, 1997; Alexandresson et al,
1997). There are many constraints on temporal
expressions in this domain. The most relevant
prior work is (Mani and Wilson, 2000), who
implemented their system on news stories,
introduced rules spreading time-stamps obtained
with the help of explicit temporal expressions
throughout the whole article, and invented
machine learning rules for disambiguating
between specific and generic use of temporal
expressions (for example, whether Christmas is
used to denote the 25th of December or to denote
some period of time around the 25th of
December). They also mention a problem of
disambiguating between temporal expression and
proper name, as in ?USA Today?.
7 Conclusion
Bell (1997) notices ?more research is needed
on the effects of time structure on news
comprehension. The hypothesis that the non-
canonical news format does adversely affect
understanding is a reasonable one on the basis of
comprehension research into other narrative
genres, but the degree to which familiarity with
news models may mitigate these problems is
unclear?. This research can greatly improve the
performance of time-stamper and might lead to a
list of machine learning rules for time detection.
In this paper we made an attempt to not just
analyze and decode temporal expressions but to
apply this analysis throughout the whole text and
assign time-stamps to such type of clauses,
which later could be used as separate sentences
in various natural language applications, for
example in multidocument summarization.
Text number Number of event-
clauses identified
correctly
Number of time point
correctly assigned to
correctly identified clauses
percentage of
correct
assignment
text 1 5 4 80.00
text 2 15 15 100
text 3 3 2 66.67
text 4 18 17 94.44
text 5 19 17 89.47
text 6 36 24 66.66
Total 96 79 82.29
Table 2. Time-stamper performance on automatically claused texts
(only correctly identified clauses are analyzed).
text
number
number of manually
created event-clauses
number of time point
correctly assigned to
manually created clauses
percentage of
correct
assignment
target 1 7 6 85.71
target 2 27 20 74.07
target 3 5 4 80.00
target 4 28 26 92.85
target 5 33 30 90.91
target 6 58 37 63.79
Total 158 123 77.85
Table 3. Time-stamper performance on manually (correct) claused texts.
References:
J. Alexandresson, N. Reithinger, and E. Maier,
1997. Insights into the Dialogue Processing of
VERBMOBIL. Proceedings of the Fifth
Conference on Applied Natural Language
Processing, 33?40
J. Allen, 1991. Time and Time Again: The
Many Ways to Represent Time. Int'l. Journal of
Intelligent Systems 4, 341?356.
J. Allen and G. Ferguson, 1994. Actions and
Events in Interval Temporal Logic. The University
of Rochester, Technical Report 521
A. Bell, 1997. The Discourse Structure of News
Structure. Approaches to Media Discourse, ed. A.
Bell, 64?104.
S. Busemann, T. Decleck, A.K. Diagne, L. Dini,
J. Klein, and S. Schmeier, 1997. Natural Language
Dialogue Service for Appointment Scheduling
Agents. Proceedings of the Fifth Conference on
Applied Natural Language Processing, 25?32.
I. Mani and G. Wilson, 2000. Robust Temporal
Processing of News. Proceedings of the ACL-2000
Conference.
C. Schokkenbroek, 1999. News Stories:
Structure, Time and Evaluation. Time & Society, vol.
8(1), 59?98.
Appendix 1
CONTEX output for the sentence ?Russia, which has loaned helicopters in previous disasters, said it would
consider sending aid? (sentence 5.11) . The surface nodes (lexemes) are underlined.
::NODE 1 ::SURF "Russia , which has loaned helicopters in previous disasters , said it would consider sending aid ." ::CAT S-SNT ::SUBS (2
20 21 29) ::LEX "say" ::FORMS (((MODE F-DECL) (PERSON F-THIRD-P) (NUMBER F-SING) (CASE F-NOM) (TENSE F-PAST-TENSE)))
::NODE 2 ::SURF "Russia , which has loaned helicopters in previous disasters ," ::CAT S-NP ::SUBS (3 7) ::PARENTS (1) ::LEX "Russia"
::ROLES (SUBJ) ::FORMS (((NUMBER F-SING) (PERSON F-THIRD-P))) ::INDEX 2
::NODE 3 ::SURF "Russia ," ::CAT S-NP ::SUBS (4 6) ::PARENTS (2)::LEX "Russia" ::ROLES (PRED) ::FORMS (((NUMBER F-SING)
(PERSON F-THIRD-P))) ::INDEX 1
::NODE 4 ::SURF "Russia" ::CAT S-NP ::SUBS (5) ::PARENTS (3)::LEX "Russia" ::ROLES (PRED) ::FORMS (((NUMBER F-SING)
(PERSON F-THIRD-P)))
::NODE 5 ::SURF "Russia" ::CAT S-PROPER-NAME ::PARENTS (4) ::LEX "Russia" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 6 ::SURF "," ::CAT D-COMMA ::PARENTS (3) ::LEX "," ::ROLES (DUMMY)
::NODE 7 ::SURF "which has loaned helicopters in previous disasters ," ::CAT S-REL-CLAUSE ::SUBS (8 10 11 13 19) ::PARENTS (2) ::LEX
"loan" ::ROLES (MOD) ::FORMS (((MODE F-DECL) (TENSE F-PERF-TENSE) (PERSON F-THIRD-P) (NUMBER F-SING)))
::NODE 8 ::SURF "which" ::CAT S-INTERR-NP ::SUBS (9) ::PARENTS (7) ::LEX "which" ::ROLES (SUBJ) ::FORMS (((NUMBER F-
SING) (PERSON F-THIRD-P))) ::PRON-REF 1
::NODE 9 ::SURF "which" ::CAT S-INTERR-PRON ::PARENTS (8)::LEX "which" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 10 ::SURF "has loaned" ::CAT S-VERB ::PARENTS (7) ::LEX "loan" ::ROLES (PRED) ::FORMS (((NUMBER F-SING) (PERSON
F-THIRD-P) (TENSE F-PERF-TENSE)))
::NODE 11 ::SURF "helicopters" ::CAT S-NP ::SUBS (12) ::PARENTS (7)::LEX "helicopter" ::ROLES (OBJ) ::FORMS (((PERSON F-
THIRD-P) (NUMBER F-PLURAL)))
::NODE 12 ::SURF "helicopters" ::CAT S-NOUN ::PARENTS (11)::LEX "helicopter" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-PLURAL)))
::NODE 13 ::SURF "in previous disasters" ::CAT S-PP ::SUBS (14 15) ::PARENTS (7) ::LEX "disaster" ::ROLES (LOCATION) ::FORMS
(((NUMBER F-PLURAL) (PERSON F-THIRD-P)))
::NODE 14 ::SURF "in" ::CAT S-PREP ::PARENTS (13)::LEX "in" ::ROLES (P)
::NODE 15 ::SURF "previous disasters" ::CAT S-NP ::SUBS (16 18) ::PARENTS (13) ::LEX "disaster" ::ROLES (PRED) ::FORMS
(((PERSON F-THIRD-P) (NUMBER F-PLURAL)))
::NODE 16 ::SURF "previous" ::CAT S-ADJP ::SUBS (17) ::PARENTS (15) ::LEX "previous" ::ROLES (MOD)
::NODE 17 ::SURF "previous" ::CAT S-ADJ ::PARENTS (16)::LEX "previous" ::ROLES (PRED)
::NODE 18 ::SURF "disasters" ::CAT S-NOUN ::PARENTS (15) ::LEX "disaster" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-PLURAL)))
::NODE 19 ::SURF "," ::CAT D-COMMA ::PARENTS (7) ::LEX "," ::ROLES (DUMMY)
::NODE 20 ::SURF "said" ::CAT S-VERB ::PARENTS (1)::LEX "say" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P) (NUMBER F-
SING) (TENSE F-PAST-TENSE)))
::NODE 21 ::SURF "it would consider sending aid" ::CAT S-SNT ::SUBS (22 24 25) ::PARENTS (1)::LEX "consider" ::ROLES (COMPL)
::FORMS (((MODE F-DECL) (GENDER F-NEUT) (PERSON F-THIRD-P) (NUMBER F-SING) (CASE F-NOM) (TENSE F-PRES-TENSE)
(MODALS F-WOULD)))
::NODE 22 ::SURF "it" ::CAT S-NP ::SUBS (23) ::PARENTS (21)::LEX "PRON" ::ROLES (SUBJ) ::FORMS (((NUMBER F-SING)
(PERSON F-THIRD-P) (GENDER F-NEUT)))::PRON-REF 2
::NODE 23 ::SURF "it" ::CAT S-REG-PRON ::PARENTS (22)::LEX "PRON" ::ROLES (PRED) ::FORMS (((GENDER F-NEUT) (PERSON
F-THIRD-P) (NUMBER F-SING)))
::NODE 24 ::SURF "would consider" ::CAT S-TR-VERB ::PARENTS (21)::LEX "consider" ::ROLES (PRED) ::FORMS (((GENDER F-
NEUT) (PERSON F-THIRD-P) (NUMBER F-SING) (TENSE F-PRES-TENSE) (MODALS F-WOULD)))
::NODE 25 ::SURF "sending aid" ::CAT S-INF-CLAUSE ::SUBS (26 27) ::PARENTS (21) ::LEX "send" ::ROLES (COMPL) ::FORMS
(((TENSE F-PRES-PART) (MODE F-DECL)))
::NODE 26 ::SURF "sending" ::CAT S-DITR-VERB ::PARENTS (25)::LEX "send" ::ROLES (PRED) ::FORMS (((TENSE F-PRES-PART)))
::NODE 27 ::SURF "aid" ::CAT S-NP ::SUBS (28) ::PARENTS (25)::LEX "aid" ::ROLES (OBJ) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 28 ::SURF "aid" ::CAT S-COUNT-NOUN ::PARENTS (27)::LEX "aid" ::ROLES (PRED) ::FORMS (((PERSON F-THIRD-P)
(NUMBER F-SING)))
::NODE 29 ::SURF "." ::CAT D-PERIOD ::PARENTS
 Manual and Automatic Evaluation of Summaries 
Chin-Yew Lin and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292  
+1-310-448-8711/8731 
{cyl,hovy}@isi.edu 
 
Abstract 
In this paper we discuss manual and 
automatic evaluations of summaries using 
data from the Document Understanding 
Conference 2001 (DUC-2001).  We first 
show the instability of the manual 
evaluation. Specifically, the low inter-
human agreement indicates that more 
reference summaries are needed. To 
investigate the feasibility of automated 
summary evaluation based on the recent 
BLEU method from machine translation, we 
use accumulative n-gram overlap scores 
between system and human summaries.  The 
initial results provide encouraging 
correlations with human judgments, based 
on the Spearman rank-order correlation 
coefficient.  However, relative ranking of 
systems needs to take into account the 
instability.   
1 Introduction 
Previous efforts in large-scale evaluation of text 
summarization include TIPSTER SUMMAC 
(Mani et al 1998) and the Document 
Understanding Conference (DUC) sponsored by 
the National Institute of Standards and 
Technology (NIST).  DUC aims to compile 
standard training and test collections that can be 
shared among researchers and to provide 
common and large scale evaluations in single 
and multiple document summarization for their 
participants. 
In this paper we discuss manual and automatic 
evaluations of summaries using data from the 
Document Understanding Conference 2001 
(DUC-2001).  Section 2 gives a brief overview 
of the evaluation procedure used in DUC-2001 
and the Summary Evaluation Environment 
(SEE) interface used to support the DUC-2001 
human evaluation protocol.  Section 3 discusses 
evaluation metrics.  Section 4 shows the 
instability of manual evaluations.  Section 5 
outlines a method of automatic summary 
evaluation using accumulative n-gram matching 
score (NAMS) and proposes a view that casts 
summary evaluation as a decision making 
process.  It shows that the NAMS method is 
bounded and in most cases not usable, given 
only a single reference summary to compare 
with.  Section 6 discusses why this is so, 
illustrating various forms of mismatching 
between human and system summaries.  We 
conclude with lessons learned and future 
directions. 
2 Document Understanding 
Conference (DUC) 
DUC2001 included three tasks: 
? Fully automatic single-document 
summarization: given a document, 
participants were required to create a 
generic 100-word summary.  The training 
set comprised 30 sets of approximately 10 
documents each, together with their 100-
word human written summaries.  The test 
set comprised 30 unseen documents. 
? Fully automatic multi-document 
summarization: given a set of documents 
about a single subject, participants were 
required to create 4 generic summaries of 
the entire set, containing 50, 100, 200, and 
400 words respectively.  The document sets 
were of four types: a single natural disaster 
event; a single event; multiple instances of a 
type of event; and information about an 
individual.  The training set comprised 30 
sets of approximately 10 documents, each 
provided with their 50, 100, 200, and 400-
word human written summaries.  The test 
set comprised 30 unseen sets. 
       Philadelphia, July 2002, pp. 45-51.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
 ? Exploratory summarization: participants 
were encouraged to investigate alternative 
approaches to evaluating summarization and 
report their results. 
A total of 11 systems participated in the single-
document summarization task and 12 systems 
participated in the multi-document task.   
The training data were distributed in early 
March of 2001 and the test data were distributed 
in mid-June of 2001.  Results were submitted to 
NIST for evaluation by July 1st 2001. 
2.1 Evaluation Materials 
For each document or document set, one human 
summary was created as the ?ideal? model 
summary at each specified length.  Two other 
human summaries were also created at each 
length.  In addition, baseline summaries were 
created automatically for each length as 
reference points.  For the multi-document 
summarization task, one baseline, lead baseline, 
took the first 50, 100, 200, and 400 words in the 
last document in the collection.  A second 
baseline, coverage baseline, took the first 
sentence in the first document, the first sentence 
in the second document and so on until it had a 
summary of 50, 100, 200, or 400 words. Only 
one baseline (baseline1) was created for the 
single document summarization task. 
2.2 Summary Evaluation Environment 
NIST assessors who created the ?ideal? written 
summaries did pairwise comparisons of their 
summaries to the system-generated summaries, 
other assessors? summaries, and baseline 
summaries.  They used the Summary Evaluation 
Environment (SEE) 2.0 developed by one of the 
authors (Lin 2001) to support the process.  
Using SEE, the assessors compared the system?s 
text (the peer text) to the ideal (the model text).  
As shown in Figure 1, each text was 
decomposed into a list of units and displayed in 
separate windows.  In DUC-2001 the sentence 
was used as the smallest unit of evaluation. 
Figure 1. SEE in an evaluation session. 
 SEE 2.0 provides interfaces for assessors to 
judge both the content and the quality of 
summaries.  To measure content, assessors step 
through each model unit, mark all system units 
sharing content with the current model unit 
(shown in green highlight in the model summary 
window), and specify that the marked system 
units express all, most, some or hardly any of 
the content of the current model unit.  To 
measure quality, assessors rate grammaticality1, 
cohesion2, and coherence3 at five different 
levels: all, most, some, hardly any, or none.   
For example, as shown in Figure 1, an assessor 
marked system units 1.1 and 10.4 (shown in red 
underlines) as sharing some content with the 
current model unit 2.2 (highlighted green). 
3 Evaluation Metrics 
One goal of DUC-2001 was to debug the 
evaluation procedures and identify stable 
metrics that could serve as common reference 
points. NIST did not define any official 
performance metric in DUC-2001.  It released 
the raw evaluation results to DUC-2001 
participants and encouraged them to propose 
metrics that would help progress the field. 
3.1 Recall, Coverage, Retention and 
Weighted Retention 
Recall at different compression ratios has been 
used in summarization research to measure how 
well an automatic system retains important 
content of original documents (Mani and 
Maybury 1999).  Assume we have a system 
summary Ss and a model summary Sm.  The 
number of sentences occurring in Ss is Ns, the 
number of sentences in Sm is Nm, and the number 
in both Ss and Sm is Na.  Recall is defined as 
Na/Nm.  The Compression Ratio is defined as the 
length of a summary (by words or sentences) 
divided by the length of its original document.   
Applying this direct all-or-nothing recall in 
DUC-2001 without modification is not 
appropriate because: 
                                                     
1 Does the summary observe English grammatical 
rules independent of its content? 
2 Do sentences in the summary fit in with their 
surrounding sentences?  
3 Is the content of the summary expressed and 
organized in an effective way? 
1. Multiple system units contribute to multiple 
model units.  
2. Exact overlap between Ss and Sm rarely 
occurs.  
3. Overlap judgment is not binary.  
For example in Figure 1, an assessor judged 
system units 1.1 and 10.4 sharing some content 
with model unit 2.2.  Unit 1.1 says ?Thousands 
of people are feared dead? and unit 2.2 says 
?3,000 and perhaps ? 5,000 people have been 
killed?.  Are ?thousands? equivalent to ?3,000 to 
5,000? or not?  Unit 10.4 indicates it was an 
?earthquake of magnitude 6.9? and unit 2.2 says 
it was ?an earthquake measuring 6.9 on the 
Richter scale?.  Both of them report a ?6.9? 
earthquake.   But the second part of system unit 
10.4, ?in an area so isolated??, seems to share 
some content with model unit 4.4 ?the quake 
was centered in a remote mountainous area?.  
Are these two equivalent?  This example 
highlights the difficulty of judging the content 
coverage of system summaries against model 
summaries and the inadequacy of using simple 
recall as defined. 
For this reason, NIST assessors not only marked 
the segments shared between system units (SU) 
and model units (MU), they also indicated the 
degree of match, i.e., all, most, some, hardly 
any, or none.  This enables us to compute 
weighted recall.  
Different versions of weighted recall were 
proposed by DUC-2001 participants. (McKeown 
et al 2001) treated the completeness of coverage 
as a threshold: 4 for all, 3 for most and above, 2 
for some and above, and 1 for hardly any and 
above.  They then proceeded to compare system 
performances at different threshold levels.  They 
defined recall at threshold t, Recallt, as follows:  
summary model in the MUs ofnumber  Total
 aboveor at  marked MUs ofNumber t  
Instead of thresholds, we use here as coverage 
score the ratio of completeness of coverage C: 1 
for all, 3/4 for most, 1/2 for some, 1/4 for hardly 
any, and 0 for none.  To avoid confusion with 
the recall used in information retrieval, we call 
our metric weighted retention, Retentionw, and 
define it as follows: 
summary model in the MUs ofnumber  Total
  marked) MUs of(Number C?  
 If we ignore C (set it to 1), we obtain an 
unweighted retention, Retention1.  We used 
Retention1 in our evaluation to illustrate that 
relative system performance (i.e., system 
ranking) changes when different evaluation 
metrics are chosen.  Therefore, it is important to 
have common and agreed upon metrics to 
facilitate large scale evaluation efforts.  
4 Instability of Manual Judgments 
In the human evaluation protocol described in 
Section 2, nothing prevents an assessor from 
assigning different coverage scores to the same 
system units produced by different systems 
against the same model unit.  (Since most 
systems produce extracts, the same sentence 
may appear in many summaries, especially for 
single-document summaries.)  Analyzing the 
DUC-2001 results, we found the following: 
? Single document task  
o A total of 5,921 judgments  
o Among them, 1,076 (18%) contain 
multiple judgments for the same units  
o 143 (2.4%) of them have three different 
coverage scores  
? Multi-document task  
o A total of 6,963 judgments  
o Among them 528 (7.6%) contain multiple 
judgments  
o 27 (0.4%) of them have three different 
coverage scores  
Intuitively this is disturbing; the same phrase 
compared to the same model unit should always 
have the same score regardless of which system 
produced it.  The large percentage of multiple 
judgments found in the single document 
evaluation are test-retest errors that need to be 
addressed in computing performance metrics.   
Figure 2 and Figure 3 show the retention scores 
for systems participating in the single- and 
multi-document tasks respectively.  The error 
bars are bounded at the top by choosing the 
maximum coverage score (MAX) assigned by 
an assessor in the case of multiple judgment 
scores and at the bottom by taking the minimum 
assignment (MIN).  We also compute system 
39
.75
40
.95
33
.98
31
.32 31
.72
31
.15
36
.29
27
.93
34
.44
29
.90 30
.21
28
.02
30
.71
26
.03
39.57
40.88
30.12
28.09
28.26 27.71
33.01
24.46
34.44
26.02
27.40
25.54
28.66
23.90
39.67
40.90
32.24
29.89 30.21 29.61
34.78
26.38
34.44
28.25
28.99
27.02
29.76
25.12
20.00
25.00
30.00
35.00
40.00
45.00
Hu
ma
n1
Hu
ma
n2
Ba
se
line
1 O P Q R S T V W X Y Z
Systems
Re
te
nt
io
n
MAJORITY
ORIGINAL
MAX
MIN
AVG
Figure 2. DUC 2001 single document retention score distribution. 
 retentions using the majority (MAJORITY) and 
average (AVG) of assigned coverage scores.  
The original (ORIGINAL) does not consider the 
instability in the data.   
Analyzing all systems? results, we made the 
following observations.   
(1) Inter-human agreement is low in the single-
document task (~40%) and even lower in 
multi-documents task (~29%). This 
indicates that using a single model as 
reference summary is not adequate. 
(2) Despite the low inter-human agreement, 
human summaries are still much better than 
the best performing systems. 
(3) The relative performance (rankings) of 
systems changes when the instability of 
human judgment is considered.  However, 
the rerankings remain local; systems remain 
within performance groups. For example, 
we have the following groups in the multi-
document summarization task (Figure 3, 
considering 0.5% error): 
a. {Human1, Human2} 
b. {N, T, Y} 
c. {Baseline2, L, P} 
d. {S} 
e. {M, O, R} 
f. {Z} 
g. {Baseline1, U, W} 
The existence of stable performance regions is 
encouraging.  Still, given the large error bars, 
one can produce 162 different rankings of these 
16 systems.  Groups are less obvious in the 
single document summarization task due to 
close performance among systems.  
Table 1 shows relative performance between 
systems x and y in the single document 
Table 1. Pairwise relative system performance 
(single document summarization task). 
28
.55 29
.03
7.4
9
15
.42 15
.71
11
.56
17
.92
11
.39
16
.49
11
.56
13
.60
18
.63
6.6
1 6.9
0
17
.94
9.1
6
28.55
29.02
7.30
14.02
15.00
10.96
17.92
10.93
15.06
10.87
12.40
18.29
6.47
6.74
17.60
8.87
28.55
29.03
7.38
14.76
15.38
11.26
17.92
11.19
15.80
11.22
13.02
18.47
6.54 6.81
17.80
9.02
5.00
10.00
15.00
20.00
25.00
30.00
Hu
ma
n1
Hu
ma
n2
Ba
se
line
1
Ba
se
line
2 L M N O P R S T U W Y Z
Systems
Re
te
nt
io
n
MAJORITY
ORIGINAL
MAX
MIN
AVG
Figure 3. DUC 2001 multi-document retention score distribution. 
H1 H2 B1 O P Q R S T V W X Y Z
H1 = - + + + + + + + + + + + +
H2 + = + + + + + + + + + + + +
B1 - - = ~ ~ ~ ~ + - + ~ + ~ +
O - - ~ = ~ ~ - + - ~ ~ + ~ +
P - - ~ ~ = ~ - + - ~ ~ + ~ +
Q - - ~ ~ ~ = - ~ - ~ ~ ~ ~ +
R - - ~ + + + = + ~ + + + + +
S - - - - - ~ - = - ~ ~ ~ - ~
T - - + + + + ~ + = + + + + +
V - - - ~ ~ ~ - ~ - = ~ ~ ~ ~
W - - ~ ~ ~ ~ - ~ - ~ = ~ ~ +
X - - - - - ~ - ~ - ~ ~ = - ~
Y - - ~ ~ ~ ~ - + - ~ ~ + = +
Z - - - - - - - ~ - ~ - ~ - =
 summarization task. A ?+? indicates the 
minimum retention score of x (row) is higher 
than the maximum retention score of y 
(column), a ?-? indicates the maximum retention 
score of x is lower than the minimum retention 
score of y, and a ?~? means x and y are 
indistinguishable.  Table 2 shows relative 
system performance in the multi-document 
summarization task.  
Despite the instability of the manual evaluation, 
we discuss automatic summary evaluation in an 
attempt to approximate the human evaluation 
results in the next section. 
5 Automatic Summary Evaluation 
Inspired by recent progress in automatic 
evaluation of machine translation (BLEU; 
Papineni et al 2001), we would like to apply the 
same idea in the evaluation of summaries.  
Following BLEU, we used the automatically 
computed accumulative n-gram matching scores 
(NAMS) between a model unit (MU) and a 
system summary (S)4 as performance indicator, 
considering multi-document summaries.  Only 
content words were used in forming n-grams. 
NAMS is defined as follows:  
a1?NAM1 + a2?NAM2 + a3?NAM3 + a4?NAM4 
NAMn is n-gram hit ratio defined as: 
MUin  grams-n of # total
S and MUbetween  grams-n matched of #  
We tested three different configurations of ai: 
                                                     
4 The whole system summary was used to compute 
NAMS against a model unit. 
C1: a1 = 1 and a2 = a3 = a4 = 0; 
C2: a1 = 1/3, a2 = 2/3, and a3 = a4 = 0; 
C3: a1 = 1/6, a2 = 2/6, a3 = 3/6, and a4 = 0; 
C1 is simply unigram matching.  C2 and C3 
give more credit to longer n-gram matches.  To 
examine the effect of stemmers in helping the n-
gram matching, we also tested all configurations 
with two different stemmers (Lovin?s and 
Porter?s).  Figure 4 shows the results with and 
without using stemmers and their Spearman 
rank-order correlation coefficients (rho) 
compared against the original retention ranking 
from Figure 4.  X-nG is configuration n without 
using any stemmer, L-nG with the Lovin 
stemmer, and P-nG with the Porter stemmer. 
The results in Figure 4 indicate that unigram 
matching provides a good approximation, but 
the best correlation is achieved using C2 with 
the Porter stemmer.  Using stemmers did 
improve correlation.  Notice that rank inversion 
remains within the performance groups 
identified in Section 4.  For example, the 
retention ranking of Baseline1, U, and W is 14, 
16, and 15 respectively.  The P-2G ranking of 
these three systems is 15, 14, and 16.  The only 
system crossing performance groups is Y.  Y 
should be grouped with N and T but the 
automatic evaluations place it lower, in the 
group with Baseline2, L, and P.  The primary 
reason for Y?s behavior may be that its 
summaries consist mainly of headlines, whose 
abbreviated style differs from the language 
models derived from normal newspaper text.   
For comparison, we also ran IBM?s BLEU 
evaluation script5 over the same model and 
system summary set. The Spearman rank-order 
correlation coefficient (?) for the single 
document task is 0.66 using one reference 
summary and 0.82 using three reference 
summaries; while Spearman ? for the multi-
document task is 0.67 using one reference and 
0.70 using three.  
6 Conclusions 
We described manual and automatic evaluation 
of single and multi-document summarization in 
DUC-2001.  We showed the instability of 
                                                     
5 We thank Kishore Papineni for sending us BLEU 
1.0. 
Table 2. Pairwise relative system performance 
(multi-document summarization task). 
H1 H2 B1 B2 L M N O P R S T U W Y Z
H1 = - + + + + + + + + + + + + + +
H2 + = + + + + + + + + + + + + + +
B1 - - = - - - - - - - - - + + - -
B2 - - + = ~ + - + ~ + + - + + - +
L - - + ~ = + - + ~ + + - + + - +
M - - + - - = - ~ - ~ - - + + - +
N - - + + + + = + + + + - + + ~ +
O - - + - - ~ - = - ~ - - + + - +
P - - - ~ ~ + - + = + + - + + - +
R - - + - - ~ - ~ - = - - + + - +
S - - + - - + - + - + = - + + - +
T - - + + + + + + + + + = + + + +
U - - - - - - - - - - - - = - - -
W - - - - - - - - - - - - + = - -
Y - - + + + + ~ + + + + - + + = +
Z - - + - - - - - - - - - + + - =
 human evaluations and the need to consider this 
factor when comparing system performances.  
As we factored in the instability, systems tended 
to form separate performance groups.  One 
should treat with caution any interpretation of 
performance figures that ignores this instability.   
Automatic evaluation of summaries using 
accumulative n-gram matching scores seems 
promising.  System rankings using NAMS and 
retention ranking had a Spearman rank-order 
correlation coefficient above 97%.  Using 
stemmers improved the correlation. However, 
satisfactory correlation is still elusive. The main 
problem we ascribe to automated summary 
evaluation is the large expressive range of 
English since human summarizers tend to create 
fresh text.  No n-gram matching evaluation 
procedure can overcome the paraphrase or 
synonym problem unless (many) model 
summaries are available.   
We conclude the following:  
(1) We need more than one model summary 
although we cannot estimate how many 
model summaries are required to achieve 
reliable automated summary evaluation. 
(2) We need more than one evaluation for each 
summary against each model summary.  
(3) We need to ensure a single rating for each 
system unit.  
References 
DUC. 2001. The Document Understanding 
Conference 2001. http://www-nlpir.nist.gov/ 
projects/duc/2001.html.  
Lin, C.-Y. 2001. Summary Evaluation 
Environment.  http://www.isi.edu/~cyl/SEE. 
Mani, I., D. House, G. Klein, L. Hirschman, L. 
Obrst, T. Firmin, M. Chrzanowski, and B. 
Sundheim. 1998. The TIPSTER SUMMAC 
Text Summarization Evaluation: Final 
Report.  MITRE Corp. Tech. Report. 
Papineni K., S. Roukos, T. Ward, W.-J. Zhu. 
2001. BLEU: a Method for Automatic 
Evaluation of Machine Translation. IBM 
Research Report RC22176(W0109-022). 
Original 
SYSCODE Retention X-1G X-2G X-3G L-1G L-2G L-3G P-1G P-2G P-3G
ranking (unigram) (unigram) (unigram)
Human1 1 2 1 1 1 1 1 1 1 1
Human2 2 1 2 2 2 2 2 2 2 2
Baseline1 14 15 15 15 16 15 14 16 15 14
Baseline2 8 8 7 6 8 8 6 8 8 6
L 7 7 6 7 7 7 7 7 7 7
M 10 10 10 10 10 11 11 9 10 11
N 4 4 4 4 4 4 4 4 4 4
O 11 12 12 12 12 12 12 11 12 12
P 6 5 5 5 5 5 5 5 5 5
R 11 11 11 11 11 10 10 12 11 10
S 9 9 9 9 9 9 9 9 9 9
T 3 3 3 3 3 3 3 3 3 3
U 16 14 14 14 14 14 15 14 14 15
W 15 16 16 16 15 16 16 15 16 16
Y 5 6 8 8 6 6 8 6 6 8
Z 13 13 13 13 13 13 13 13 13 13
Spearman ? 1.00000 0.98382 0.97206 0.96912 0.98382 0.98382 0.97206 0.98235 0.98676 0.97206
No stemmer Lovin stemmer Porter stemmer
Figure 4.  Manual and automatic ranking comparisons. 
 	

?The?Potential?and?Limitations?of?Automatic?????????????????????????????????????????
Sentence?Extraction?for?Summarization?
Chin-Yew?Lin?and?Eduard?Hovy?
University?of?Southern?California/Information?Sciences?Institute?
4676?Admiralty?Way?
Marina?del?Rey,?CA?90292,?USA?
{cyl,hovy}@isi.edu 
?
?
Abstract?
In?this?paper?we?present?an?empirical?study?of?
the?potential?and?limitation?of?sentence?extrac-
tion? in? text? summarization.?Our? results? show?
that? the? single? document? generic? summariza-
tion?task?as?defined?in?DUC?2001?needs?to?be?
carefully?refocused?as?reflected?in?the? low?in-
ter-human? agreement? at? 100-word 1 ?(0.40?
score)? and? high? upper? bound? at? full? text 2?
(0.88)? summaries.? For? 100-word? summaries,?
the?performance?upper?bound,?0.65,?achieved?
oracle?extracts3.?Such?oracle?extracts?show?the?
promise? of? sentence? extraction? algorithms;?
however,? we? first? need? to? raise? inter-human?
agreement?to?be?able?to?achieve?this?perform-
ance? level.?We? show? that? compression? is? a?
promising?direction? and? that? the? compression?
ratio?of?summaries?affects?average?human?and?
system?performance.?
1? Introduction?
Most? automatic? text? summarization? systems? existing?
today?are?extraction?systems? that?extract?parts?of?origi-
nal? documents? and? output? the? results? as? summaries.?
Among? them,? sentence? extraction? is? by? far? the? most?
???????????????????????????????????????????????????????????
1?We?compute?unigram?co-occurrence?score?of?a?pair?of?man-
ual? summaries,? one? as? candidate? summary? and? the? other? as?
reference.?
2?We?compute?unigram?co-occurrence?scores?of?a?full?text?and?
its?manual?summaries?of?100?words.?These?scores?are?the?best?
achievable? using? the? unigram? co-occurrence? scoring? metric?
since? all?possible?words? are? contained? in? the? full? text.?Three?
manual?summaries?are?used.?
3 ?Oracle? extracts? are? the? best? scoring? extracts? generated? by?
exhaustive? search? of? all? possible? sentence? combinations? of?
100?5?words.??
popular? (Edmundson? 1969,? Luhn? 1969,? Kupiec? et? al.?
1995,?Goldstein? et? al.?1999,?Hovy? and?Lin?1999).?The?
majority?of?systems?participating? in? the?past?Document?
Understanding? Conference? (DUC? 2002),? a? large? scale?
summarization? evaluation? effort? sponsored? by? the? US?
government,? are? extraction? based.? Although? systems?
based?on? information?extraction? (Radev?and?McKeown?
1998,?White?et?al.?2001,?McKeown?et?al.?2002)?and?dis-
course?analysis?(Marcu?1999b,?Strzalkowski?et?al.?1999)?
also?exist,?we?focus?our?study?on?the?potential?and?limi-
tations?of?sentence?extraction?systems?with?the?hope?that?
our?results?will?further?progress?in?most?of?the?automatic?
text?summarization?systems?and?evaluation?setup.?
The?evaluation?results?of?the?single?document?summari-
zation?task?in?DUC?2001?and?2002?(DUC?2002,?Paul?&?
Liggett?2002)?indicate?that?most?systems?are?as?good?as?
the?baseline?lead-based?system?and?that?humans?are?sig-
nificantly?better,?though?not?by?much.?This?leads?to?the?
belief?that?lead-based?summaries?are?as?good?as?we?can?
get? for? single? document? summarization? in? the? news?
genre,? implying? that? the? research? community? should?
invest?future?efforts? in?other?areas.? In?fact,?a?very?short?
summary? of? about? 10? words? (headline-like)? task? has?
replaced? the? single? document? 100-word? summary? task?
in?DUC?2003.?The?goal?of?this?study?is?to?renew?interest?
in? sentence? extraction-based? summarization? and? its?
evaluation?by? estimating? the?performance?upper?bound?
using?oracle?extracts,?and?to?highlight?the?importance?of?
taking? into? account? the? compression? ratio? when? we?
evaluate?extracts?or?summaries.??
Section? 2? gives? an? overview? of?DUC? relevant? to? this?
study.?Section?3? introduces? a? recall-based?unigram? co-
occurrence? automatic?evaluation?metric.?Section?4?pre-
sents?the?experimental?design.?Section?5?shows?the?em-
pirical? results.? Section? 6? concludes? this? paper? and?
discusses?future?directions.?
?2? Document?Understanding?Conference?
Fully? automatic? single-document? summarization? was?
one? of? two?main? tasks? in? the? 2001?Document?Under-
standing?Conference.?Participants?were? required? to?cre-
ate? a? generic? 100-word? summary.? ?There?were? 30? test?
sets? in?DUC?2001?and?each? test?set?contained?about?10?
documents.?For?each?document,?one?summary?was?cre-
ated? manually? as? the? ?ideal?? model? summary? at? ap-
proximately? 100?words.? ?We?will? refer? to? this?manual?
summary? as? H1.? Two? other? manual? summaries? were?
also?created?at?about?that?length.??We?will?refer?to?these?
two?additional?human?summaries?as?H2?and?H3.?In?addi-
tion,?baseline?summaries?were?created?automatically?by?
taking? the? first? n? sentences? up? to? 100?words.?We?will?
refer?this?baseline?extract?as?B1.?
3? Unigram?Co-Occurrence?Metric?
In?a?recent?study?(Lin?and?Hovy?2003),?we?showed?that?
the?recall-based?unigram?co-occurrence?automatic?scor-
ing?metric?correlated?highly?with?human?evaluation?and?
has? high? recall? and? precision? in? predicting? statistical?
significance?of? results?comparing?with? its?human?coun-
terpart.? The? idea? is? to? measure? the? content? similarity?
between?a?system?extract?and?a?manual?summary?using?
simple? n-gram? overlap.? A? similar? idea? called? IBM?
BLEU? score? has? proved? successful? in? automatic? ma-
chine?translation?evaluation?(Papineni?et?al.?2001,?NIST?
2002).?For?summarization,?we?can?express?the?degree?of?
content?overlap? in? terms?of?n-gram?matches?as? the? fol-
lowing?equation:?
)1(
)(
)(
}{
}{
? ?
? ?
? ??
? ??
?
?
=
UnitsModelC Cgramn
UnitsModelC Cgramn
match
n gramnCount
gramnCount
C ?
Model?units? are? segments?of?manual? summaries.?They?
are? typically? either? sentences? or? elementary? discourse?
units?as?defined?by?Marcu? (1999b).?Countmatch(n-gram)?
is? the?maximum? number? of? n-grams? co-occurring? in? a?
system?extract? and? a?model?unit.?Count(n-gram)? is? the?
number? of? n-grams? in? the?model? unit.?Notice? that? the?
average?n-gram? coverage? score,?Cn,? as? shown? in?equa-
tion?1,?is?a?recall-based?metric,?since?the?denominator?of?
equation? 1? is? the? sum? total? of? the? number? of? n-grams?
occurring? in? the?model? summary? instead?of? the? system?
summary?and?only?one?model?summary?is?used?for?each?
evaluation.? In? summary,? the? unigram? co-occurrence?
statistics?we?use? in? the?following?sections?are?based?on?
the?following?formula:?
)2(logexp),( ???
?
???
?
= ?
=
j
in
nn CwjiNgram ?
Where? j??? i,? i? and? j? range? from?1? to?4,?and?wn? is?1/(j-
i+1).?Ngram(1,?4)?is?a?weighted?variable? length?n-gram?
match? score? similar? to? the? IBM? BLEU? score;? while?
Ngram(k,?k),?i.e.?i?=?j?=?k,?is?simply?the?average?k-gram?
co-occurrence?score?Ck.? In? this?study,?we?set? i?=? j?=?1,?
i.e.?unigram?co-occurrence?score.???
With?a?test?collection?available?and?an?automatic?scoring?
metric? defined,?we? describe? the? experimental? setup? in?
the?next?section.?
4? Experimental?Designs?
As? stated? in? the? introduction,?we? aim? to? find? the? per-
formance?upper?bound?of?a? sentence? extraction? system?
and? the?effect?of?compression?ratio?on? its?performance.?
We? present? our? experimental? designs? to? address? these?
questions?in?the?following?sections.?
4.1? Performance? Upper? Bound? Estimation?
Using?Oracle?Extract?
In?order?to?estimate?the?potential?of?sentence?extraction?
systems,?it?is?important?to?know?the?upper?bound?that?an?
ideal? sentence? extraction? method? might? achieve? and?
how? far? the? state-of-the-art? systems? are? away? from? the?
bound.? If? the? upper? bound? is? close? to? state-of-the-art?
systems?? performance? then?we? need? to? look? for? other?
summarization?methods? to? improve?performance.? If? the?
upper? bound? is?much? higher? than? any? current? systems?
can?achieve,?then?it?is?reasonable?to?invest?more?effort?in?
sentence? extraction? methods.? The? question? is? how? to?
estimate? the?performance?upper?bound.?Our? solution? is?
to?cast?this?estimation?problem?as?an?optimization?prob-
lem.? We? exhaustively? generate? all? possible? sentence?
combinations? that?satisfy?given? length?constraints? for?a?
summary,? for? example,? all? the? sentence? combinations?
totaling? 100?5? words.?We? then? compute? the? unigram?
co-occurrence? score? for? each? sentence? combination,?
against? the? ideal.? The? best? combinations? are? the? ones?
with? the?highest?unigram? co-occurrence? score.?We?call?
this? sentence? combination? the? oracle? extract.? Figure? 1?
shows?an?oracle?extract? for?document?AP900424-0035.?
One?of?its?human?summaries?is?shown?in?Figure?2.?The?
oracle? extract? covers? almost? all? aspects? of? the? human?
summary?except?sentences?5?and?6?and?part?of?sentence?
4.?However,?if?we?allow?the?automatic?extract?to?contain?
more?words,?for?example,?150?words?shown?in?Figure?3,?
the? longer?oracle? extract? then? covers? everything? in? the?
human?summary.?This?indicates?that?lower?compression?
can? boost? system? performance.? The? ultimate? effect? of?
compression?can?be?computed?using?the?full?text?as?the?
oracle? extract,? since? the? full? text? should?contain?every-
thing? included? in? the? human? summary.? That? situation?
provides? the? best? achievable? unigram? co-occurrence?
score.?A?near?optimal?score?also?confirms?the?validity?of?
using? the?unigram?co-occurrence? scoring?method?as?an?
automatic?evaluation?method.?
?4.2? Compression?Ratio? and? Its?Effect? on? System?
Performance?
One? important? factor? that? affects? the? average?perform-
ance? of? sentence? extraction? system? is? the? number? of?
sentences? contained? in? the? original? documents.? This?
factor?is?often?overlooked?and?has?never?been?addressed?
systematically.? For? example,? if? a? document? contains?
only?one?sentence?then?this?document?will?not?be?useful?
in?differentiating? summarization? system?performance???
there? is? only? one? choice.?However,? for? a? document? of?
100?sentences?and?assuming?each?sentence? is?20?words?
long,? there? are? C(100,5)? =? 75,287,520? different? 100-
word?extracts.?This?huge?search?space?lowers?the?chance?
of? agreement? between? humans? on? what? constitutes? a?
good? summary.? It? also?makes? system? and? human? per-
formance? approach? average? since? it? is?more? likely? to?
include? some?good? sentences?but?not? all?of? them.?Em-
pirical?results?shown? in?Section?5?confirm? this?and? that?
leads?us?to?the?question?of?how?to?construct?a?corpus?to?
evaluate? summarization? systems.?We?discuss? this? issue?
in?the?conclusion?section.??
4.3? Inter-Human?Agreement?and?Its?Effect?on?
System?Performance?
In? this? section? we? study? how? inter-human? agreement?
affects? system? performance.? Lin? and?Hovy? (2002)? re-
ported? that,? compared? to? a?manually? created? ideal,?hu-
mans? scored?about?0.40? in?average?coverage? score? and?
the? best? system? scored? about? 0.35.?According? to? these?
numbers,?we?might?assume?that?humans?cannot?agree?to?
each?other?on?what? is? important?and? the?best?system? is?
almost?as?good?as?humans.?If?this?is?true?then?estimating?
an?upper?bound?using?oracle?extracts?is?meaningless.?No?
matter?how?high?the?estimated?upper?bounds?may?be,?we?
probably?would?never?be?able? to?achieve? that?perform-
ance? due? to? lack? of? agreement? between? humans:? the?
oracle? approximating? one? human?would? fail?miserably?
with?another.??Therefore?we?set?up?experiments?to?inves-
tigate?the?following:?
1.? What? is? the?distribution?of? inter-human?agree-
ment??
Figure?3.?A?150-word?oracle?extract? for?docu-
ment?AP900424-0035.?
Figure? 2.? A? manual? summary? for? document?
AP900424-0035.?
Figure?1.?A?100-word?oracle?extract? for?docu-
ment?AP900424-0035.?
<DOC>?
<DOCNO>AP900424-0035</DOCNO>?
<DATE>04/24/90</DATE>?
<HEADLINE>?
<S?HSNTNO="1">Elizabeth?Taylor?in?Intensive?Care?Unit</S>?
<S?HSNTNO="2">By?JEFF?WILSON</S>?
<S?HSNTNO="3">Associated?Press?Writer</S>?
<S?HSNTNO="4">SANTA?MONICA,?Calif.?(AP)</S>?
</HEADLINE>?
<TEXT>?
<S?SNTNO="1">A?seriously?ill?Elizabeth?Taylor?battled?pneumonia?at?her?
hospital,?her?breathing?assisted?by?a?ventilator,?doctors?say.</S>?
<S?SNTNO="2">Hospital?officials?described?her?condition?late?Monday?
as?stabilizing?after?a?lung?biopsy?to?determine?the?cause?of?the?pneumo-
nia.</S>?
<S?SNTNO="3">Analysis?of?the?tissue?sample?was?expected?to?take?until?
Thursday,?said?her?spokeswoman,?Chen?Sam.</S>?
<S?SNTNO="9">Another?spokewoman?for?the?actress,?Lisa?Del?Favaro,?
said?Miss?Taylor's?family?was?at?her?bedside.</S>?
<S?SNTNO="13">``It?is?serious,?but?they?are?really?pleased?with?her?
progress.</S>?
<S?SNTNO="22">During?a?nearly?fatal?bout?with?pneumonia?in?1961,?
Miss?Taylor?underwent?a?tracheotomy,?an?incision?into?her?windpipe?to?
help?her?breathe.</S>?
</TEXT>?
</DOC>
<DOC>?
<TEXT>?
<S?SNTNO="1">Elizabeth?Taylor?battled?pneumonia?at?her?hospital,?
assisted?by?a?ventilator,?doctors?say.</S>?
<S?SNTNO="2">Hospital?officials?described?her?condition?late?Monday?
as?stabilizing?after?a?lung?biopsy?to?determine?the?cause?of?the?pneumo-
nia.</S>?
<S?SNTNO="3">Analysis?of?the?tissue?sample?was?expected?to?be?com-
plete?by?Thursday.</S>?
<S?SNTNO="4">Ms.?Sam,?spokeswoman?said?"it?is?serious,?but?they?are?
really?pleased?with?her?progress.</S>?
<S?SNTNO="5">She's?not?well.</S>?
<S?SNTNO="6">She's?not?on?her?deathbed?or?anything.</S>?
<S?SNTNO="7">Another?spokeswoman,?Lisa?Del?Favaro,?said?Miss?
Taylor's?family?was?at?her?bedside.</S>?
<S?SNTNO="8">During?a?nearly?fatal?bout?with?pneumonia?in?1961,?Miss?
Taylor?underwent?a?tracheotomy?to?help?her?breathe.</S>?
</TEXT>?
</DOC>?
<DOC>?
<DOCNO>AP900424-0035</DOCNO>?
<DATE>04/24/90</DATE>?
<HEADLINE>?
<S?HSNTNO="1">Elizabeth?Taylor?in?Intensive?Care?Unit</S>?
<S?HSNTNO="2">By?JEFF?WILSON</S>?
<S?HSNTNO="3">Associated?Press?Writer</S>?
<S?HSNTNO="4">SANTA?MONICA,?Calif.?(AP)</S>?
</HEADLINE>?
<TEXT>?
<S?SNTNO="1">A?seriously?ill?Elizabeth?Taylor?battled?pneumonia?at?her?
hospital,?her?breathing?assisted?by?a?ventilator,?doctors?say.</S>?
<S?SNTNO="2">Hospital?officials?described?her?condition?late?Monday?
as?stabilizing?after?a?lung?biopsy?to?determine?the?cause?of?the?pneumo-
nia.</S>?
<S?SNTNO="3">Analysis?of?the?tissue?sample?was?expected?to?take?until?
Thursday,?said?her?spokeswoman,?Chen?Sam.</S>?
<S?SNTNO="4">The?58-year-old?actress,?who?won?best-actress?Oscars?
for?``Butterfield?8''?and?``Who's?Afraid?of?Virginia?Woolf,''?has?been?
hospitalized?more?than?two?weeks.</S>?
<S?SNTNO="8">Her?condition?is?presently?stabilizing?and?her?physicians?
are?pleased?with?her?progress.''</S>?
<S?SNTNO="9">Another?spokewoman?for?the?actress,?Lisa?Del?Favaro,?
said?Miss?Taylor's?family?was?at?her?bedside.</S>?
<S?SNTNO="13">``It?is?serious,?but?they?are?really?pleased?with?her?
progress.</S>?
<S?SNTNO="14">She's?not?well.</S>?
<S?SNTNO="15">She's?not?on?her?deathbed?or?anything,''?Ms.?Sam?said?
late?Monday.</S>?
<S?SNTNO="22">During?a?nearly?fatal?bout?with?pneumonia?in?1961,?
Miss?Taylor?underwent?a?tracheotomy,?an?incision?into?her?windpipe?to?
help?her?breathe.</S>?
</TEXT>?
</DOC>
?2.? How?does?a?state-of-the-art?system?differ?from?
average?human?performance?at?different? inter-
human?agreement?levels???
We? present? our? results? in? the? next? section? using? 303?
newspaper?articles?from?the?DUC?2001?single?document?
summarization?task.?Besides?the?original?documents,?we?
also? have? three? human? summaries,? one? lead? summary?
(B1),? and? one? automatic? summary? from? one? top? per-
forming?system?(T)?for?each?document.?
5? Results?
In? order? to? determine? the? empirical? upper? and? lower?
bounds? of? inter-human? agreement,?we? first? ran? cross-
human?evaluation?using?unigram?co-occurrence?scoring?
through? six? human? summary? pairs,? i.e.? (H1,H2),?
(H1,H3),?(H2,H1),?(H2,H3),?(H3,H1),?and?(H3,H2).?For?
a? summary? pair? (X,Y),?we? used?X? as? the?model? sum-
mary?and?Y?as?the?system?summary.?Figure?4?shows?the?
distributions?of?four?different?scenarios.?The?MaxH?dis-
tribution? picks? the? best? inter-human? agreement? scores?
for?each?document,?the?MinH?distribution?the?minimum?
one,? the?MedH?distribution? the?median,? and? the?AvgH?
distribution? the? average.?The? average?of? the?best? inter-
human? agreement? and? the? average? of? average? inter-
human?agreement?differ?by?about?10?percent?in?unigram?
co-occurrence?score?and?18?percent?between?MaxH?and?
MinH.? These? big? differences? might? come? from? two?
sources.?The? first? one? is? the? limitation? of? the? unigram?
0
10
20
30
40
50
60
70
80
0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Unigram?Co-occurrence?Scores
#?
o
f?I
n
st
an
ce
s
AvgH MaxH MedH MinH
Average?MAX?=?0.50
Average?AVG?=?0.40
Average?MED?=?0.39
Average?MIN?=?0.32
Figure? 4.? DUC? 2001? single? document? inter-
human? unigram? co-occurrence? score? distribu-
tions? for? maximum,? minimum,? average,? and?
median.?
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D4
1.A
P8
81
21
1-0
02
7
D5
3.F
BI
S3
-2
29
42
D3
1.L
A0
21
68
9-
02
27
D3
4.A
P8
80
91
4-0
07
9
D5
3.A
P8
80
81
6-0
23
4
D2
8.L
A1
10
59
0-
00
38
D1
9.A
P8
80
33
0-0
11
9
D1
4.A
P9
01
01
0-0
03
6
D2
2.L
A0
70
18
9-
00
80
D2
7.L
A1
00
78
9-
00
07
D1
2.F
T9
34
-1
10
14
D2
2.A
P8
81
21
6-0
01
7
D3
7.F
BI
S3
-1
19
19
D1
9.L
A1
02
18
9-
01
51
D0
5.F
T9
41
-1
54
7
D5
7.L
A1
10
58
9-
00
82
D3
4.A
P8
80
91
3-0
20
4
D4
5.A
P9
00
62
5-0
16
0
D5
0.A
P8
81
22
2-0
11
9
D1
4.A
P9
01
01
2-0
03
2
D4
1.S
JM
N9
1-
06
07
10
22
D4
3.F
T9
23
-5
85
9
D0
8.A
P8
90
31
6-0
01
8
D1
9.A
P8
80
62
3-0
13
5
D4
3.F
T9
33
-8
94
1
D4
4.F
T9
34
-9
11
6
D1
2.W
SJ
87
02
27
-0
14
9
D0
4.F
T9
23
-5
08
9
D1
5.A
P8
90
30
2-0
06
3
D0
4.F
T9
23
-6
03
8
D3
7.A
P8
90
70
4-0
04
3
D1
2.W
SJ
87
01
23
-0
10
1
D1
5.A
P8
90
51
1-0
12
6
D1
5.A
P9
00
52
1-0
06
3
D0
6.F
T9
22
-1
02
00
D3
4.A
P9
00
60
1-0
04
0
Document?IDs
U
n
ig
ra
m
?C
o-
oc
cu
rr
en
ce
?S
co
re
s
MaxH B1 T E100 E150 FT Avg?MaxH Avg?B1 Avg?T Avg?E100 Avg?E150 Avg?FT
Figure?5.?DUC?2001?single?document? inter-human,?baseline,?system,?100-word,?150-word,?and?full? text?
oracle?extracts?unigram?co-occurrence?score?distributions?(#?of?sentences<=30).?Document?IDs?are?sorted?
by?decreasing?MaxH.?
?co-occurrence?scoring?applied?to?manual?summaries?that?
it?cannot? recognize?synonyms?or?paraphrases.?The?sec-
ond?one?is?the?true? lack?of?agreement?between?humans.?
We?would? like? to?conduct?an? in-depth?study? to?address?
this? question,? and?would? just? assume? the? unigram? co-
occurrence?scoring?is?reliable.?
In? other? experiments,? we? used? the? best? inter-human?
agreement?results?as?the?reference?point?for?human?per-
formance?upper?bound.?This?also? implied? that?we?used?
the? human? summary? achieving? the? best? inter-human?
agreement?score?as?our?reference?summary.?
Figure? 5? shows? the? unigram? co-occurrence? scores? of?
human,?baseline,? system?T,? and? three?oracle? extraction?
systems?at?different?extraction?lengths.?We?generated?all?
possible? sentence? combinations? that? satisfied? 100?5?
words?constraints.?Due? to?computation-intensive?nature?
of?this?task,?we?only?used?documents?with?fewer?than?30?
sentences.? We? then? computed? the? unigram? co-
occurrence?score?for?each?combination,?selected?the?best?
one?as?the?oracle?extraction,?and?plotted?the?score?in?the?
figure.?The?curve?for?100?5?words?oracle?extractions?is?
the?upper?bound? that? a? sentence? extraction? system? can?
achieve? within? the? given? word? limit.? If? an? automatic?
system?is?allowed?to?extract?more?words,?we?can?expect?
that? longer? extracts? would? boost? system? performance.?
The?question? is?how?much?better? and?what? is? the?ulti-
mate? limit??To? address? these? questions,?we? also? com-
puted? unigram? co-occurrence? scores? for? oracle?
extractions?of?150?5?words?and?full?text4.?The?perform-
ance?of? full? text? is? the?ultimate?performance?an?extrac-
tion?system?can? reach?using? the?unigram?co-occurrence?
scoring? method.?We? also? computed? the? scores? of? the?
lead?baseline?system?(B1)?and?an?automatic?system?(T).??
The? average? unigram? co-occurrence? score? for? full? text?
(FT)?was?0.833,?150?5?words?(E150)?was?0.796,?100?5?
words? (E100)? was? 0.650,? the? best? inter-human? agree-
ment?(MaxH)?was?0.546,?system?T?was?0.465,?and?base-
line?was?0.456.?It?is?interesting?to?note?that?the?state-of-
the-art?system?performed?at? the?same? level?as? the?base-
line?system?but?was?still?about?10%?away?from?human.?
The?10%?difference?between?E100?and?MaxH?(0.650?vs.?
0.546)? implies?we?might? need? to? constraint?humans? to?
focus? their?summaries? in?certain?aspects? to?boost? inter-
human? agreement? to? the? level?of?E100;?while? the?15%?
and?24%?improvements?from?E100?to?E150?and?FT?in-
dicate?compression?would?help?push?overall?system?per-
formance? to?a?much?higher? level,? if?a?system? is?able? to?
compress? longer? summaries? into? a? shorter?without? los-
ing?important?content.?
To? investigate? relative? performance? of? humans,? sys-
tems,? and? oracle? extracts? at? different? inter-human?
agreement? levels,?we? created? three? separate? document?
sets? based? on? their? maximum? inter-human? agreement?
(MaxH)?scores.?Set?Set?A?had?MaxH?score?greater?than?
or?equal?to?0.70,?set?B?was?between?0.70?and?0.60,?and?
???????????????????????????????????????????????????????????
4?We? used? full? text? as? extract? and? computed? its? unigram? co-
occurrence?score?against?a?reference?summary.?
Figure? 7.? DUC? 2001? single? document? inter-
human,?baseline,?system,?and?full? text?unigram?
co-occurrence?score?distributions?(Set?B).?
Figure? 6.? DUC? 2001? single? document? inter-
human,?baseline,?system,?and?full? text?unigram?
co-occurrence?score?distributions?(Set?A).?
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D4
1.A
P8
81
21
1-0
02
7
D1
1.A
P8
90
40
3-0
12
3
D1
4.A
P8
80
90
2-0
06
2
D4
1.A
P8
90
80
1-0
02
5
D0
6.S
JM
N9
1-
06
19
10
81
D4
1.L
A0
51
59
0-
00
65
D0
6.W
SJ
91
07
10
-0
14
8
D5
3.F
BI
S3
-2
29
42
D2
8.L
A1
10
49
0-
01
84
D3
1.L
A0
30
88
9-
01
63
D1
3.S
JM
N9
1-
06
25
54
34
D2
4.L
A0
51
19
0-
01
85
D3
1.L
A0
21
68
9-
02
27
D0
5.F
T9
31
-3
88
3
D0
6.S
JM
N9
1-
06
28
30
83
D3
1.A
P8
91
00
6-0
02
9
D5
0.A
P8
80
71
4-0
14
2
D3
4.A
P8
80
91
4-0
07
9
D1
4.A
P8
80
62
9-0
15
9
D1
3.A
P9
00
30
6-0
10
5
D3
1.L
A0
30
78
9-
00
47
D1
4.L
A1
03
08
9-
00
70
Document?IDs
U
ni
g
ra
m
?C
o
-o
cc
u
rr
en
ce
?S
co
re
s
MaxH B1 T AvgMaxH Avg?B1 Avg?T
Avg?FT FT Avg?E100 Avg?E150
Avg?E150?=?0.863
Avg?FT?=?0.924
Avg?E100?=?0.705
Avg?MaxH?=?0.741
Avg?T?=?0.525
Avg?B1?=?0.516
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D5
3.A
P8
80
81
6-0
23
4
D0
5.F
T9
21
-9
31
0
D4
1.L
A0
81
49
0-
00
30
D2
4.A
P9
00
42
4-0
03
5
D3
7.F
BI
S4
-2
76
02
D0
6.L
A0
71
59
0-
00
68
D2
8.L
A1
10
59
0-
00
38
D3
1.L
A0
61
58
9-
01
43
D3
2.A
P9
00
32
3-0
03
6
D3
7.A
P9
01
01
3-0
04
6
D5
6.A
P8
81
12
6-0
00
7
D0
6.A
P8
90
32
2-0
01
0
D1
9.A
P8
80
33
0-0
11
9
D1
4.A
P8
80
91
3-0
12
9
D0
4.F
T9
23
-5
79
7
D4
4.F
T9
32
-5
85
5
D1
4.A
P9
01
01
0-0
03
6
D3
7.A
P8
80
51
0-0
17
8
D4
1.A
P8
90
80
5-0
12
6
D2
2.A
P8
80
70
5-0
10
9
D5
0.A
P9
00
91
0-0
02
0
D3
1.S
JM
N9
1-
06
08
42
28
D3
4.A
P9
00
52
9-0
00
5
D2
2.L
A0
70
18
9-
00
80
D0
4.F
T9
23
-5
83
5
D1
4.A
P8
81
22
2-0
12
6
D2
4.A
P9
00
51
2-0
03
8
D2
7.L
A1
00
78
9-
00
07
D3
1.A
P8
81
00
9-0
07
2
D4
5.A
P8
80
52
0-0
26
4
D0
8.A
P8
80
31
8-0
05
1
D1
5.F
BI
S4
-6
77
21
D1
2.F
T9
34
-1
10
14
D3
7.A
P9
00
42
8-0
10
8
D4
5.
FT
92
1-
30
5
D5
4.L
A0
92
79
0-
00
10
D5
6.S
JM
N9
1-
06
13
63
05
Dcoument?IDs
U
ni
g
ra
m
?C
o
-o
cc
u
rr
en
ce
?S
co
re
s
MaxH B1 T FT AvgMaxH Avg?B1
Avg?T Avg?FT Avg?E100 Avg?E150
Avg?E150?=?0.840
Avg?E100?=?0.698
Avg?FT?=?0.917
Avg?MaxH?=?0.645
Avg?B1?=?0.509
Avg?T?=?0.490
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D2
2.A
P8
81
21
6-0
01
7
D4
4.F
T9
33
-1
08
81
D4
3.A
P8
90
13
1-0
28
0
D0
6.L
A0
11
88
9-
00
67
D2
7.A
P8
90
72
2-0
08
1
D4
1.S
JM
N9
1-
06
14
21
26
D5
7.A
P9
01
20
3-0
16
6
D3
1.S
JM
N9
1-
06
01
22
24
D2
8.S
JM
N9
1-
06
31
21
20
D3
0.A
P9
00
41
6-0
18
8
D2
7.W
SJ
91
11
21
-0
13
6
D3
4.A
P8
80
91
3-0
20
4
D2
7.W
SJ
91
12
12
-0
08
0
D0
6.W
SJ
91
04
05
-0
15
4
D1
5.L
A1
01
69
0-
00
40
D5
0.A
P8
81
22
2-0
11
9
D0
8.A
P8
90
30
7-0
15
0
D4
4.F
T9
33
-5
70
9
D3
7.A
P9
00
42
8-0
00
5
D5
0.A
P8
91
21
3-0
00
4
D5
4.W
SJ
91
10
31
-0
01
2
D1
9.L
A0
71
58
9-
00
76
D1
4.A
P9
00
82
9-0
12
0
D4
3.F
T9
11
-3
46
3
D3
1.A
P8
80
92
7-0
11
7
D2
8.L
A1
21
18
9-
00
17
D0
8.A
P8
90
31
6-0
01
8
D4
4.F
T9
34
-8
62
8
D0
8.A
P9
00
72
1-0
11
0
D1
9.A
P8
80
62
3-0
13
5
D2
2.A
P8
80
70
5-0
01
8
D3
2.A
P8
90
32
6-0
08
1
D4
3.F
T9
33
-8
94
1
D5
3.A
P8
80
61
3-0
16
1
Document?IDs
U
ni
g
ra
m
?C
o
-o
cc
u
rr
en
ce
?S
co
re
s
MaxH B1 T AvgMaxH Avg?B1 Avg?T
Avg?FT FT Avg?E100 Avg?E150
Avg?E150?=?0.790
Avg?E100?=?0.645
Avg?FT?=?0.897
Avg?MaxH?=?0.536
Avg?T?=?0.435
Avg?B1?=?0.423
Figure? 8.? DUC? 2001? single? document? inter-
human,?baseline,?system,?and?full? text?unigram?
co-occurrence?score?distributions?(Set?C).?
?set?C?between?0.60?and?0.50.?A?had?22?documents,?set?B?
37,?and?set?C?100.?Total?was?about?52%?(=159/303)?of?
the? test? collection.?The? 100?5? and? 150?5?words? aver-
ages?were? computed? over? documents?which? contain? at?
most?30?sentences.?The?results?are?shown? in?Figures?6,?
7,?and?8.?In?the?highest?inter-human?agreement?set?(A),?
we? found? that? average?MaxH,? 0.741,?was? higher? than?
average? 100?5? words? oracle? extract,? 0.705;? while? the?
average? automatic? system? performance? was? around?
0.525.? This? is? good? news? since? the? high? inter-human?
agreement?and?the?big?difference?(0.18)?between?100?5?
words? oracle? and? automatic? system? performance? pre-
sents? a? research? opportunity? for? improving? sentence?
extraction? algorithms.?The? scores? of?MaxH? (0.645? for?
set?B?and?0.536?for?set?C)?in?the?other?two?sets?are?both?
lower?than?100?5?words?oracles?(0.698?for?set?B,?5.3%?
lower,? and? 0.645? for? set? C,? 9.9%? lower).? This? result?
suggests? that?optimizing?sentence?extraction?algorithms?
at? the? Set? C? level?might? not? be?worthwhile? since? the?
algorithms? are? likely? to? overfit? the? training? data.? The?
reason? is? that? the? average? run? time? performance? of? a?
sentence?extraction?algorithm?depends?on?the?maximum?
inter-human? agreement.? For? example,? given? a? training?
reference? summary?TSUM1? and? its? full?document?TDOC1,?
we?optimize?our?sentence?extraction?algorithm?to?gener-
ate?an?oracle?extract?based?on?TSUM1?from?TDOC1.?In?the?
run?time,?we?test?on?a?reference?summary?RSUM1?and?its?
full?document?RDOC1.? In? the?unlikely?case? that?RDOC1? is?
the?same?as?TDOC1?and?RSUM1? is? the?same?as?TSUM1,? i.e.?
TSUM1?and?RSUM1?have?unigram?co-occurrence?score?of?1?
(perfect? inter-human? agreement? for? two? summaries? of?
one?document),?the?optimized?algorithm?will?generate?a?
perfect?extract?for?RDOC1?and?achieve? the?best?perform-
ance? since? it? is?optimized?on?TSUM1.?However,?usually?
TSUM1?and?RSUM1?are?different.?Then?the?performance?of?
the?algorithm?will?not?exceed?the?maximum?unigram?co-
occurrence?score?between?TSUM1?and?RSUM1.?Therefore?it?
is? important? to? ensure? high? inter-human? agreement? to?
allow? researchers? room? to?optimize?sentence?extraction?
algorithms?using?oracle?extracts.???
Finally,?we? present? the? effect? of? compression? ratio? on?
inter-human? agreement? (MaxH)? and? performance? of?
baseline?(B1),?automatic?system?T?(T),?and?full?text?ora-
cle?(FT)? in?Figure?9.?Compression?ratio? is?computed? in?
terms?of?words?instead?of?sentences.?For?example,?a?100?
words? summary?of? a? 500?words?document?has? a? com-
pression? ratio?of?0.80? (=1?100/500).?The? figure? shows?
that?three?human?summaries?(H1,?H2,?and?H3)?had?dif-
ferent? compression? ratios? (CMPR?H1,?CMPR?H2,? and?
CMPR?H3)? for? different? documents? but? did? not? differ?
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
D2
4.L
A0
51
19
0-
01
85
D1
9.A
P8
80
21
7-0
17
5
D2
4.A
P9
00
51
1-0
15
9
D5
6.A
P8
81
12
6-0
00
7
D5
3.F
BI
S3
-2
29
42
D4
4.F
T9
34
-1
33
50
D5
7.A
P8
91
01
7-0
20
4
D0
4.F
T9
23
-6
03
8
D3
7.A
P8
80
51
0-0
17
8
D2
2.L
A0
70
18
9-
00
80
D4
4.F
T9
22
-3
17
1
D3
1.A
P8
81
00
9-0
07
2
D3
1.S
JM
N9
1-
06
01
22
24
D0
6.A
P9
01
02
9-0
03
5
D0
8.A
P9
01
23
1-0
01
2
D1
5.L
A1
01
69
0-
00
40
D4
4.F
T9
33
-2
76
0
D0
8.S
JM
N9
1-
06
19
30
81
D0
8.A
P8
80
31
8-0
05
1
D1
5.A
P9
00
52
1-0
06
3
D0
6.A
P8
90
32
2-0
01
0
D3
1.S
JM
N9
1-
06
08
42
28
D3
2.A
P8
90
50
2-0
20
5
D4
5.S
JM
N9
1-
06
18
20
91
D3
2.A
P9
00
31
3-0
19
1
D3
9.A
P8
81
01
7-0
23
5
D3
2.L
A0
40
78
9-
00
51
D5
3.A
P8
81
22
7-0
18
5
D5
0.A
P8
91
21
0-0
07
9
D0
6.L
A0
80
79
0-
01
11
D5
4.L
A1
02
19
0-
00
45
D3
7.S
JM
N9
1-
06
14
30
70
D1
3.W
SJ
91
07
02
-0
07
8
D2
2.W
SJ
88
09
23
-0
16
3
Document?IDs
U
ni
g
ra
m
?C
o-
oc
cu
rr
en
ce
?S
co
re
s
B1 MaxH T CMPR?H1 CMPR?H2 CMPR?H3
FT Linear?(B1) Linear?(MaxH) Linear?(T) Linear?(FT)
Figure?9.?DUC?2001?single?doc?inter-human,?baseline,?and?system?unigram?co-occurrence?score?versus?
compression?ratio.?Document?IDs?are?sorted?by?increasing?compression?ratio?CMPR?H1.?
?much.?The?unigram?co-occurrence?scores?for?B1,?T,?and?
MaxH?were?noisy?but?had? a?general? trend? (Linear?B1,?
Linear? T,? and? Linear? MaxH)? of? drifting? into? lower?
performance? when? compression? ratio? increased? (i.e.?
when? summaries? became? shorter);? while? the? per-
formance? of? FT? did? not? exhibit? a? similar? trend.? This?
confirms? our? earlier? hypothesis? that? humans? are? less?
likely? to? agree? at? high? compression? ratio? and? system?
performance?will?also?suffer?at?high?compression? ratio.?
The?constancy?of?FT?across?different?compression?ratios?
is? reasonable? since? FT? scores? should? only? depend? on?
how? well? the? unigram? co-occurrence? scoring? method?
captures?content?overlap?between?a?full?text?and?its?ref-
erence? summaries? and? how? likely? humans? use?
vocabulary?outside?the?original?document.?
6? Conclusions?
In? this? paper? we? presented? an? empirical? study? of? the?
potential? and? limitations? of? sentence? extraction? as? a?
method? of? automatic? text? summarization.?We? showed?
the?following:?
(1)? How?to?use?oracle?extracts?to?estimate?the?per-
formance? upper? bound? of? sentence? extraction?
methods?at?different?extract?lengths.?We?under-
stand?that?summaries?optimized?using?unigram?
co-occurrence? score? do? not? guarantee? good?
quality? in? terms? of? coherence,? cohesion,? and?
overall?organization.?However,?we?would?argue?
that?a?good?summary?does?require?good?content?
and?we?will?leave?how?to?make?the?content?co-
hesive,? coherent,? and? organized? to? future? re-
search.??
(2)? Inter-human?agreement?varied?a?lot?and?the?dif-
ference?between?maximum?agreement? (MaxH)?
and? minimum? agreement? (MinH)? was? about?
18%?on? the?DUC?2001?data.?To?minimize? the?
gap,?we?need?to?define?the?summarization?task?
better.? This? has? been? addressed? by? providing?
guided? summarization? tasks? in? DUC? 2003?
(DUC? 2002).?We? guesstimate? the? gap? should?
be?smaller?in?DUC?2003?data.?
(3)? State-of-the-art?systems?performed?at?the?same?
level?as?the?baseline?system?but?were?still?about?
10%? away? from? the? average? human? perform-
ance.??
(4)? The? potential? performance? gains? (15%? from?
E100? to? E150? and? 24%? to? FT)? estimated? by?
oracle?extracts?of?different?sizes? indicated? that?
sentence? compression? or? sub-sentence? extrac-
tion?are?promising?future?directions.?
(5)? The?relative?performance?of?humans?and?oracle?
extracts? at? three? inter-human? agreement? inter-
vals?showed?that?it?was?only?meaningful?to?op-
timize? sentence? extraction? algorithms? if? inter-
human? agreement?was? high.?Although? overall?
high? inter-human?agreement?was? low?but?sub-
sets? of? high? inter-human? agreement? did? exist.?
For? example,? about? human? achieved? at? least?
60%? agreement? in? 59? out? of? 303? (~19%)?
documents?of?30?sentences?or?less.?
(6)? We? also? studied? how? compression? ratio? af-
fected? inter-human?agreement?and?system?per-
formance,? and? the? results? supported? our?
hypothesis? that? humans? tend? to? agree? less? at?
high? compression? ratio,? and? similar? between?
humans?and?systems.?How?to?take?into?account?
this?factor?in?future?summarization?evaluations?
is?an?interesting?topic?to?pursue?further.?
Using?exhaustive?search?to?identify?oracle?extraction?has?
been? studied?by?other? researchers?but? in?different? con-
texts.?Marcu?(1999a)?suggested?using?exhaustive?search?
to?create?training?extracts?from?abstracts.?Donaway?et?al.?
(2000)?used?exhaustive?search?to?generate?all?three?sen-
tences?extracts? to?evaluate?different?evaluation?metrics.?
The?main?difference?between?their?work?and?ours?is?that?
we? searched? for? extracts? of? a? fixed? number? of? words?
while?they?looked?for?extracts?of?a?fixed?number?of?sen-
tences.??
In?the?future,?we?would? like?to?apply?a?similar?method-
ology? to?different? text?units,? for?example,?sub-sentence?
units?such?as?elementary?discourse?unit?(Marcu?1999b).?
We?want? to? study?how? to? constrain? the? summarization?
task? to? achieve? higher? inter-human? agreement,? train?
sentence? extraction? algorithms? using? oracle? extracts? at?
different? compression? sizes,? and? explore? compression?
techniques?to?go?beyond?simple?sentence?extraction.?
References?
Donaway,? R.L.,? Drummey,? K.W.,? and? Mather,? L.A.?
2000.? A? Comparison? of? Rankings? Produced? by?
Summarization?Evaluation?Measures.? In? Proceeding?
of?the?Workshop?on?Automatic?Summarization,?post-
conference?workshop?of?ANLP-NAACL-2000,?Seat-
tle,?WA,?USA,?69?78.?
DUC.?2002.?The?Document?Understanding?Conference.??
http://duc.nist.gov.??
Edmundson,? H.P.? 1969.? New? Methods? in? Automatic?
Abstracting.??Journal?of?the?Association?for?Comput-
ing?Machinery.??16(2).?
Goldstein,? J.,? M.? Kantrowitz,? V.? Mittal,? and? J.? Car-
bonell.? 1999.? Summarizing? Text? Documents:? Sen-
tence? Selection? and? Evaluation? Metrics.? In?
Proceedings?of? the?22nd? International?ACM?Confer-
ence? on?Research? and?Development? in? Information?
Retrieval?(SIGIR-99),?Berkeley,?CA,?USA,?121?128.?
Hovy,?E.?and?C.-Y.?Lin.?1999.?Automatic?Text?Summa-
rization? in? SUMMARIST.? In? I.? Mani? and? M.?
?Maybury? (eds),? Advances? in? Automatic? Text? Sum-
marization,?81?94.?MIT?Press.?
Kupiec,?J.,?J.?Pederson,?and?F.?Chen.?1995.?A?Trainable?
Document? Summarizer.? In? Proceedings? of? the? 18th?
International?ACM?Conference?on?Research?and?De-
velopment? in? Information?Retrieval? (SIGIR-95),?Se-
attle,?WA,?USA,?68?73.?
Lin,?C.-Y.? and?E.?Hovy.?2002.?Manual? and?Automatic?
Evaluations? of? Summaries.? In? Proceedings? of? the?
Workshop? on? Automatic? Summarization,? post-
conference? workshop? of? ACL-2002,? pp.? 45-51,?
Philadelphia,?PA,?2002.?
Lin,?C.-Y.?and?E.H.?Hovy.?2003.?Automatic?Evaluation?
of? Summaries?Using?N-gram? Co-occurrence? Statis-
tics.? In? Proceedings? of? the? 2003?Human? Language?
Technology? Conference? (HLT-NAACL? 2003),? Ed-
monton,?Canada,?May?27???June?1,?2003.?
Luhn,?H.?P.?1969.?The?Automatic?Creation?of?Literature?
Abstracts.? IBM? Journal? of? Research? and? Develop-
ment.?2(2),?1969.?
Marcu,?D.?1999a.?The?automatic?construction?of? large-
scale? corpora? for? summarization? research.? Proceed-
ings? of? the? 22nd? International?ACM?Conference? on?
Research?and?Development? in? Information?Retrieval?
(SIGIR-99),?Berkeley,?CA,?USA,?137?144.?
Marcu,?D.?1999b.?Discourse?trees?are?good?indicators?of?
importance?in?text.?In?I.?Mani?and?M.?Maybury?(eds),?
Advances? in? Automatic? Text? Summarization,? 123?
136.?MIT?Press.?
McKeown,? K.,? R.? Barzilay,? D.? Evans,? V.? Hatzivassi-
loglou,? J.? L.? Klavans,? A.? Nenkova,? C.? Sable,? B.?
Schiffman,?S.?Sigelman.?2002.?Tracking?and?Summa-
rizing?News?on?a?Daily?Basis?with?Columbia?s?News-
blaster.? In? Proceedings? of? Human? Language?
Technology? Conference? 2002? (HLT? 2002).? San?
Diego,?CA,?USA.?
NIST.?2002.?Automatic?Evaluation?of?Machine?Transla-
tion?Quality?using?N-gram?Co-Occurrence?Statistics.?
Over,? P.? and?W.? Liggett.? 2002.? Introduction? to?DUC-
2002:?an? Intrinsic?Evaluation?of?Generic?News?Text?
Summarization? Systems.? In? Proceedings? of? Work-
shop? on? Automatic? Summarization? (DUC? 2002),?
Philadelphia,?PA,?USA.?
http://www-nlpir.nist.gov/projects/duc/pubs/?
2002slides/overview.02.pdf?
Papineni,? K.,? S.? Roukos,? T.?Ward,?W.-J.? Zhu.? 2001.?
Bleu:?a?Method?for?Automatic?Evaluation?of?Machine?
Translation.? IBM? Research? Report? RC22176?
(W0109-022).?
Radev,? D.R.? and? K.R.? McKeown.? 1998.? Generating?
Natural?Language?Summaries?from?Multiple?On-line?
Sources.?Computational?Linguistics,?24(3):469?500.?
Strzalkowski,?T,?G.?Stein,?J.?Wang,?and?B,?Wise.?A?Ro-
bust?Practical?Text?Summarizer.?1999.?In?I.?Mani?and?
M.? Maybury? (eds),? Advances? in? Automatic? Text?
Summarization,?137?154.?MIT?Press.?
White,?M.,?T.?Korelsky,?C.?Cardie,?V.?Ng,?D.? Pierce,?
and?K.?Wagstaff.?2001.?Multidocument?Summariza-
tion? via? Information? Extraction.? In? Proceedings? of?
Human? Language? Technology? Conference? 2001?
(HLT?2001),?San?Diego,?CA,?USA.?
?
?
Maximum Entropy Models for FrameNet Classification  
 
Michael Fleischman, Namhee Kwon and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{fleisch, nkwon, hovy }@ISI.edu 
  
Abstract 
The development of FrameNet, a large 
database of semantically annotated sen-
tences, has primed research into statistical 
methods for semantic tagging.  We ad-
vance previous work by adopting a 
Maximum Entropy approach and by using 
previous tag information to find the high-
est probability tag sequence for a given 
sentence.  Further we examine the use of 
sentence level syntactic pattern features to 
increase performance.  We analyze our 
strategy on both human annotated and 
automatically identified frame elements, 
and compare performance to previous 
work on identical test data.  Experiments 
indicate a statistically significant im-
provement (p<0.01) of over 6%. 
1 Introduction 
Recent work in the development of FrameNet, a 
large database of semantically annotated sentences, 
has laid the foundation for statistical approaches to 
the task of automatic semantic classification.   
The FrameNet project seeks to annotate a large 
subset of the British National Corpus with seman-
tic information.  Annotations are based on Frame 
Semantics (Fillmore, 1976), in which frames are 
defined as schematic representations of situations 
involving various frame elements such as partici-
pants, props, and other conceptual roles.   
In each FrameNet sentence, a single target 
predicate is identified and all of its relevant frame 
elements are tagged with their semantic role (e.g., 
Agent, Judge), their syntactic phrase type (e.g., 
NP, PP), and their grammatical function (e.g., ex-
ternal argument, object argument).  Figure 1 shows 
an example of an annotated sentence and its appro-
priate semantic frame.   
 
 
 
 
 
 
 
          She  clapped  her hands  in inspiration. 
Frame:        Body-Movement 
 
Frame Elements:   
Agent     Body Part Cause 
       -NP            -NP  -PP 
       -Ext             -Obj -Comp 
 
Figure 1.  Frame for lemma ?clap? shown with three 
core frame elements and a sentence annotated with ele-
ment type, phrase type, and grammatical function. 
 
As of its first release in June 2002, FrameNet 
has made available 49,000 annotated sentences.  
The release contains 99,000 annotated frame ele-
ments for 1462 distinct lexical predicates (927 
verbs, 339 nouns, and 175 adjectives). 
While considerable in scale, the FrameNet da-
tabase does not yet approach the magnitude of re-
sources available for other NLP tasks.  Each target 
predicate, for example, has on average only 30 sen-
tences tagged.  This data sparsity makes the task of 
learning a semantic classifier formidable, and in-
creases the importance of the modeling framework 
that is employed. 
2 Related Work 
To our knowledge, Gildea and Jurafsky (2002) 
is the only work to use FrameNet to build a statis-
tically based semantic classifier.  They split the 
problem into two distinct sub-tasks: frame element 
identification and frame element classification.  In 
the identification phase, syntactic information is 
extracted from a parse tree to learn the boundaries 
of the frame elements in a sentence.  In the classi-
fication phase, similar syntactic information is 
used to classify those elements into their semantic 
roles.   
In both phases Gildea and Jurafsky (2002) 
build a model of the conditional probabilities of the 
classification given a vector of syntactic features.  
The full conditional probability is decomposed into 
simpler conditional probabilities that are then in-
terpolated to make the classification.  Their best 
performance on held out test data is achieved using 
a linear interpolation model: 
where r is the class to be predicted, x is the vector 
of syntactic features, xi is a subset of those fea-
tures, ?i is the weight given to that subset condi-
tional probability (as determined using the EM 
algorithm), and m is the total number of subsets 
used.  Using this method, they report a test set ac-
curacy of 78.5% on classifying semantic roles and 
precision/recall scores of .726/.631 on frame ele-
ment identification.  
We extend Gildea and Jurafsky (2002)?s initial 
effort in three ways.  First, we adopt a maximum 
entropy (ME) framework in order to learn a more 
accurate classification model.  Second, we include 
features that look at previous tags and use previous 
tag information to find the highest probability se-
mantic role sequence for a given sentence.  Finally, 
we examine sentence-level patterns that exploit 
more global information in order to classify frame 
elements.  We compare the results of our classifier 
to that of Gildea and Jurafsky (2002) on matched 
test sets of both human annotated and automati-
cally identified frame elements.  
3 
                                                          
Semantic Role Classification 
Training (36,993 sentences / 75,548 frame ele-
ments), development (4,000 sentences / 8,167 
frame elements), and held out test sets (3,865 sen-
tences / 7,899 frame elements) were obtained in 
order to exactly match those used in Gildea and 
Jurafsky (2002)1 .  In the experiments presented 
below, features are extracted for each frame ele-
ment in a sentence and used to classify that ele-
ment into one of 120 semantic role categories.  The 
boundaries of each frame element are given based 
on the human annotations in FrameNet.  In Section 
4, experiments are performed using automatically 
identified frame elements. 
3.1 
                                                          
1 Data sets (including parse trees) were obtained from Dan 
Gildea via personal communication. 
Features 
For each frame element, features are extracted 
from the surface text of the sentence and from an 
automatically generated syntactic parse tree 
(Collins, 1997).  The features used are described 
below: 
 
)|()|(
0
i
m
i
i xrpxrp ?
=
= ? ? Target predicate (tar): Although there may 
be many predicates in a sentence with associ-
ated frame elements, classification operates on 
only one target predicate at a time.  The target 
predicate is the only feature that is not ex-
tracted from the sentence itself and must be 
given by the user.  Note that the frame which 
the target predicate instantiates is not given, 
leaving any word sense ambiguities to be han-
dled implicitly by the classifier.2 
? Phrase type (pt):  The syntactic phrase type of 
the frame element (e.g. NP, PP) is extracted 
from the parse tree of the sentence by finding 
the constituent in the tree whose boundaries 
match the human annotated boundaries of the 
element.  In cases where there exists no con-
stituent that perfectly matches the element, the 
constituent is chosen which matches the largest 
text span of the element and has the same left-
most boundary.  
? Syntactic head (head): The syntactic heads of 
the frame elements are extracted from the 
frame element?s matching constituent (as de-
scribed above) using a heuristic method de-
scribed by Michael Collins. 3   This method 
extracts the syntactic heads of constituents; 
thus, for example, the second frame element in 
Figure 1 has head ?hands,? while the third 
frame element has head ?in.? 
? Logical Function (lf): A simplification of the 
grammatical function annotation (see section 
1) is extracted from the parse tree.  Unlike the 
2 Because of the interaction of head word features with the 
target predicate, we suspect that ambiguous lexical items do 
not account for much error.  This question, however, will be 
addressed explicitly in future work. 
3 http://www.ai.mit.edu/people/mcollins/papers/heads 
Table 1.  Feature sets used in ME frame element classifier.  Shows individual feature sets, example feature 
function from that set, and total number of feature functions in the set.  Examples taken from frame element 
?in inspiration,? shown in Figure 1. 
 Number Feature Set Example function Number of Functions 
in Feature Set 
0 f(r, tar) f(CAUSE, ?clap?)=1 6,518 
1 f(r, tar, pt) f(CAUSE, ?clap?, PP)=1 12,030 
2 f(r, tar, pt, lf) f(CAUSE, ?clap?, PP, other)=1 14,615 
3 f(r, pt, pos, voice) f(CAUSE, NP, ?clap?, active)=1 1,215 
4 f(r, pt, pos, voice ,tar) f(CAUSE, PP, after, active, ?clap?)=1 15,602 
5 f(r ,head) f(CAUSE, ?in?)=1 18,504 
6 f(r, head, tar) f(CAUSE, ?in?, ?clap?)=1 38,223 
7 f(r, head, tar, pt) f(CAUSE, ?in?, ?clap?, PP)=1 39,740 
8 f(r, order, syn) f(CAUSE, 2, 
[NP-Ext,Target,NP-Obj,PP-other])=1 
13,228 
9 f(r, tar, order, syn) f(CAUSE, ?clap?, 2, 
[NP-Ext,Target,NP-Obj,PP-other])=1 
40,580 
10 f(r,r_-1) f(CAUSE, BODYPART)=1 1,158 
11 f(r,r_-1,r_-2) f(CAUSE, BODYPART, AGENT)=1 2,030 
Total Number of Features:  203,443 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
full grammatical function, the lf can have only 
one of three values: external argument, object 
argument, other.  A node is considered an ex-
ternal argument if it is an ancestor of an S 
node, an object argument if it is an ancestor of 
a VP node, and other for all other cases.  This 
feature is only applied to frame elements 
whose phrase type is NP.  
? Position (pos): The position of the frame ele-
ment relative to the target (before, after) is ex-
tracted based on the surface text of the 
sentence. 
? Voice (voice): The voice of the sentence (ac-
tive, passive) is determined using a simple 
regular expression passed over the surface text 
of the sentence. 
? Order (order): The position of the frame ele-
ment relative to the other frame elements in the 
sentence.  For example, in the sentence from 
Figure 1, the element ?She? has order=0, while 
?in inspiration? has order=2. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern of the sentence is generated 
by looking at the phrase types and logical 
functions of each frame element in the sen-
tence.  For example, in the sentence: ?Alexan-
dra bent her head;? ?Alexandra? is an external 
argument Noun Phrase, ?bent? is a target 
predicate, and ?her head? is an object argu-
ment Noun Phrase.  Thus, the syntactic pattern 
associated with the sentence is [NP-ext, target, 
NP-obj].   
These syntactic patterns can be highly in-
formative for classification.  For example, in 
the training data, a syntactic pattern of [NP-
ext, target, NP-obj] given the predicate bend 
was associated 100% of the time with the 
Frame Element pattern: ?AGENT TARGET 
BODYPART.? 
? Previous role (r_n): Frame elements do not 
occur in isolation, but rather, depend very 
much on the other elements in a sentence.  
This dependency can be exploited in classifica-
tion by using the semantic roles of previously 
classified frame elements as features in the 
classification of a current element.  This strat-
egy takes advantage of the fact that, for exam-
ple, if a frame element is tagged as an AGENT 
it is highly unlikely that the next element will 
also be an AGENT. 
The previous role feature indicates the 
classification that the n-previous frame ele-
ment received.  During training, this informa-
tion is provided by simply looking at the true 
classes of the frame element occurring n posi-
tions before the target element.  During testing, 
hypothesized classes of the n elements are used 
and Viterbi search is performed to find the 
most probable tag sequence for a sentence. 
3.2 Maximum Entropy 
ME models implement the intuition that the best 
model will be the one that is consistent with the set 
of constrains imposed by the evidence, but other-
wise is as uniform as possible (Berger et al, 1996).  
We model the probability of a semantic role r 
given a vector of features x according to the ME 
formulation below: 
3.3 
3.4 
Experiments 
We present three experiments in which different 
feature sets are used to train the ME classifier.  The 
first experiment uses only those feature combina-
tions described in Gildea and Jurafsky (2002) (fea-
ture sets 0-7 from Table 1).  The second 
experiment uses a super set of the first and incor-
porates the syntactic pattern features described 
above (feature sets 0-9).  The final experiment uses 
the previous tags and implements Viterbi search to 
find the best tag sequence (feature sets 0-11). 
?
=
=
n
i
i
x
xrfZxrp
0
i )],(exp[1)|( ?  
Here Zx is a normalization constant, fi(r,x) is a fea-
ture function which maps each role and vector 
element (or combination of elements) to a binary 
value, n is the total number of feature functions, 
and ?i is the weight for a given feature function.  
The final classification is just the role with highest 
probability given its feature vector and the model. 
We further investigate the effect of varying two 
aspects of classifier training: the standard deviation 
of the Gaussian priors used for smoothing, and the 
number of sentences used for training.  To examine 
the effect of optimizing the standard deviation, a 
range of values was chosen and a classifier was 
trained using each value until performance on a 
development set ceased to improve.   
The feature functions that we employ can be 
divided into feature sets based upon the types and 
combinations of features on which they operate.  
Table 1 lists the feature sets that we use, as well as 
the number of individual feature functions they 
contain.  The feature combinations were chosen 
based both on previous work and trial and error.  In 
future work we will examine more principled fea-
ture selection techniques. 
To examine the effect of training set size on 
performance, five data sets were generated from 
the original set with 36, 367, 3674, 7349, and 
24496 sentences, respectively.  These data sets 
were created by going through the original set and 
selecting every thousandth, hundredth, tenth, fifth, 
and every second and third sentence, respectively.  
It is important to note that the feature functions 
described here are not equivalent to the subset 
conditional distributions that are used in the Gildea 
and Jurafsky model.  ME models are log-linear 
models in which feature functions map specific 
instances of syntactic features and classes to binary 
values (e.g., if a training element has head=?in? 
and role=CAUSE, then, for that element, the feature 
function f(CAUSE, ?in?) will equal 1).  Thus, ME is 
not here being used as another way to find weights 
for an interpolated model.  Rather, the ME ap-
proach provides an overarching framework in 
which the full distribution of semantic roles given 
syntactic features can be modeled. 
 
 
 
 
 
 
 
 
 
 
 We train the ME models using the GIS algo-
rithm (Darroch and Ratcliff, 1972) as implemented 
in the YASMET ME package (Och, 2002).  We 
use the YASMET MEtagger (Bender et al, 2003) 
to perform the Viterbi search.  The classifier was 
trained until performance on the development set 
ceased to improve.  Feature weights were 
smoothed using Gaussian priors with mean 0 
(Chen and Rosenfeld, 1999).  The standard devia-
tion of this distribution was optimized on the de-
velopment set for each experiment. 
 
Classifier Performance on Test Set
81.7
83.6
84.7
78.5
76
78
80
82
84
86
G&J Exp 1 Exp 2 Exp 3
%
 C
or
re
ct
Figure 2.  Performance of models on test data using 
hand annotated frame element boundaries.  G&J refers 
to the results of Gildea and Jurafsky (2002).  Exp 1 in-
corporates feature sets 0-7 from Table 1; Exp 2 feature 
sets 0-9; Exp 3 features 0-11.  
Results  
Figure 2 shows the results of our experiments 
alongside those of (Gildea and Jurafsky, 2002) on 
identical held out test sets.  The difference in per-
formance between each classifier is statistically 
significant at (p<0.01) (Mitchell, 1997), with the 
exception of Exp 2 and Exp 3, whose difference is 
statistically significant at (p<0.05).   
 
Table 2.  Effect of different smoothing parameter (std. 
dev.) values on classification performance. 
Std. Dev. % Correct 
1 79.9 
2 82.1 
4 81.9 
 
Table 2 shows the effect of varying the stan-
dard deviation of the Gaussian priors used for 
smoothing in Experiment 1.  The difference in per-
formance between the classifiers trained using 
standard deviation 1 and 2 is statistically signifi-
cant at (p<0.01). 
 
10%
20%
30%
40%
50%
60%
70%
80%
90%
10 100 1000 10000 100000
# Sentences in Training
%
 C
or
re
ct
 
Figure 3.  Effect of training set size on semantic role 
classification. 
 
Figure 3 shows the change in performance as a 
function of training set size.  Classifiers were 
trained using the full set of features described for 
Experiment 3. 
Table 3 shows the confusion matrix for a subset 
of semantic roles.  Five roles were chosen for pres-
entation based upon their high contribution to clas-
sifier error.  Confusion between these five account 
for 27% of all errors made amongst the 120 possi-
ble roles.  The tenth role, other, represents the sum 
of the remaining 115 roles.  Table 4 presents ex-
ample errors for five of the most confused roles.   
3.5 Discussion 
It is clear that the ME models improve perform-
ance on frame element classification.  There are a 
number of reasons for this improvement. 
First, for this task the log-linear model employed 
in the ME framework is better than the linear 
interpolation model used by Gildea and Jurafsky.  
One possible reason for this is that semantic role 
classification benefits from the ME model?s bias 
for more uniform probability distributions that sat-
isfy the constraints placed on the model by the 
training data.   
Another reason for improved performance comes 
from ME?s simpler design.  Instead of having to 
worry about finding proper backoff strategies 
amongst distributions of features subsets, ME al-
lows one to include many features in a single 
model and automatically adjusts the weights of 
these features appropriately. 
 
Table 3.  Confusion matrix for five roles which contrib-
ute most to overall system error. Columns refer to ac-
tual role.  Rows refer to the model?s hypothesis.  Other 
refers to combination of all other roles. 
  Area Spkr Goal Msg Path Other Prec. 
Area 98  6  18 16 0.710 
Spkr  373  23  41 0.853 
Goal 11  431  28 50 0.828 
Msg  18 1 315  33 0.858 
Path 32  36  415 41 0.791 
Other 15 21 26 24 33 5784 0.979 
Recall 0.628 0.905 0.862 0.87 0.84 0.969   
 
Also, because the ME models find weights for 
many thousands of features, they have many more 
degrees of freedom than the linear interpolated 
models of Gildea and Jurafsky.  Although many 
degrees of freedom can lead to overfitting of the 
training data, the smoothing procedure employed 
in our experiments helps to counteract this prob-
lem.  As evidenced in Table 2, by optimizing the 
standard deviation used in smoothing the ME 
models are able to show significant increases in 
performance on held out test data. 
Finally, by including in our model sentence-
level pattern features and information about previ-
ous classes, global information can be exploited for 
improved classification.  The accuracy gained by 
including such global information confirms the 
intuition that the semantic role of an element is 
much related to the entire sentence of which it is a 
part. 
Having discussed the advantages of the models 
presented here, it is interesting to look at the errors 
that the system makes.  It is clear from the confu-
sion matrix in Table 3 that a great deal of the sys-
tem error comes from relatively few semantic 
roles.4  Table 4 offers some insight into why these 
errors occur.  For example, the confusions exem-
plified in 1 and 2 are both due to the fact that the 
particular phrases employed can be used in multi-
ple roles (including the roles hypothesized by the 
system).  Thus, while ?across the counter? may be 
considered a goal when one is talking about a per-
son and their head, the same phrase would be con-
sidered a path if one were talking about a mouse 
who is running.   
 
Table 4.  Example errors for five of the most often con-
fused semantic roles 
 Actual Proposed Example Sentence 
1 Goal Path The barman craned his head 
across the counter. 
2 Area Path Mr. Glass began hallucinating, 
throwing books around the 
classroom. 
3 Message Speaker Debate lasted until 20 Septem-
ber, opposition being voiced 
by a number of Italian and 
Spanish prelates. 
4 Addressee Speaker Furious staff claim they were 
even called in from holiday to 
be grilled by a specialist secu-
rity firm 
5 Reason Evaluee We cannot but admire the 
efficiency with which she 
took control of her own life. 
 
Examples 3 and 4, while showing phrases with 
similar confusions, stand out as being errors caused 
by an inability to deal with passive sentences.  
Such errors are not unexpected; for, even though 
the voice of the sentence is an explicit feature, the 
system suffers from the paucity of passive sen-
tences in the data (approximately 5%). 
Finally, example 5 shows an error that is based 
on the difficult nature of the decision itself (i.e., it 
is unclear whether ?the efficiency? is the reason for 
admiration, or what is being admired).  Often 
times, phrases are assigned semantic roles that are 
not obvious even to human evaluators.  In such 
cases it is difficult to determine what information 
might be useful for the system. 
Having looked at the types of errors that are 
common for the system, it becomes interesting to 
examine what strategy may be best to overcome 
such errors.  Aside from new features, one solution 
is obvious: more data.  The curve in Figure 2 
shows that there is still a great deal of performance 
to be gained by training the current ME models on 
more data.  The slope of the curve indicates that 
we are far from a plateau, and that even constant 
increases in the amount of available training data 
may push classifier performance above 90% accu-
racy.   
Having demonstrated the effectiveness of the 
ME approach on frame element classification 
given hand annotated frame element boundaries, 
we next examine the value of the approach given 
automatically identified boundaries. 
                                                          
4 
4.1 
Frame Element Identification 
Gildea and Jurafsky equate the task of locating 
frame element boundaries to one of identifying 
frame elements amongst the parse tree constituents 
of a given sentence.  Because not all frame element 
boundaries exactly match constituent boundaries, 
this approach can perform no better than 86.9% 
(i.e. the number of elements that match constitu-
ents (6864) divided by the total number of ele-
ments (7899)) on the test set.   
Features 
Frame element identification is a binary classifica-
tion problem in which each constituent in a parse 
tree is described by a feature vector and, based on 
that vector, tagged as either a frame element or not.  
In generating feature vectors we use a subset of the 
features described for role tagging as well as an 
additional path feature. 
 
 
Figure 4.  Generation of path features used in frame 
element tagging.  The path from the constituent ?in in-
spiration? to the target predicate ?clapped? is repre-
sented as the string PP?VP?VBD.  
 
Gildea and Jurafsky introduce the path feature 
in order to capture the structural relationship be-
tween a constituent and the target predicate.  The  4 44% of all error is due to confusion between only nine roles. 
Table 5.  Results of frame element identification.  G&J represents results reported in (Gildea and Jurafsky, 2002), 
ME results for the experiments reported here.  The second column shows precision, recall, and F-scores for the task 
of frame element identification, the third column for the combined task of identification and classification.   
FE ID only FE ID + FE Classification Method 
Precision Recall F-Score Precision Recall F-Score 
G&J Boundary id + baseline role labeler .726 .631 .675 .67 .468 .551 
ME Boundary id + ME role labeler .736 .679 .706 .6 .554 .576 
 
path of a constituent is represented by the nodes 
through which one passes while traveling up the 
tree from the constituent and then down through 
the governing category to the target.  Figure 4 
shows an example of this feature for a frame ele-
ment from the sentence presented in Figure 1. 
4.2 
4.3 
Experiments 
We use the ME formulation described in Section 
3.2 to build a binary classifier.  The classifier fea-
tures follow closely those used in Gildea and Juraf-
sky.  We model the data using the feature sets: f(fe, 
path), f(fe, path, tar), and f(fe, head, tar), where fe 
represents the binary classification of the constitu-
ent.  While this experiment only uses three feature 
sets, the heterogeneity of the path feature is so 
great that the classifier itself uses 1,119,331 unique 
binary features. 
With the constituents having been labeled, we 
apply the ME frame element classifier described 
above.  Results are presented using the classifier of 
Experiment 1, described in section 3.3. We then 
investigate the effect of varying the number of 
constituents used for training on identification per-
formance.  Five data sets of approximately 100,000 
10,000, 1,000, and 100 constituents were generated 
from the original set by random selection and used 
to train ME models as described above. 
Results 
Table 5 compares the results of Gildea and Juraf-
sky (2002) and the ME frame element identifier on 
both the task of frame element identification alone, 
and the combined task of frame element identifica-
tion and classification.  In order to be counted cor-
rect on the combined task, the constituent must 
have been correctly identified as a frame element, 
and then must have been correctly classified into 
one of the 120 semantic categories.   
Recall is calculated based on the total number 
of frame elements in the test set, not on the total 
number of elements that have matching parse con-
stituents.  Thus, the upper limit is 86.9%, not 
100%.  Precision is calculated as the number of 
correct positive classifications divided by the num-
ber of total positive classifications.  
The difference in the F-scores on the identifica-
tion task alone and on the combined task are statis-
tically significant at the (p<0.01) level 5 .  The 
accuracy of the ME semantic classifier on the 
automatically identified frame elements is 81.5%, 
not a statistically significant difference from its 
performance on hand labeled elements, but a statis-
tically significant difference from the classifier of 
Gildea and Jurafsky (2002) (p<0.01). 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
100 1000 10000 100000 1000000
# Constituents in Training
F-
Sc
or
e
 
Figure 5.  Effect of training set size on frame element 
boundary identification. 
 
Figure 5 shows the results of varying the train-
ing set size on identification performance.  For 
each data set, thresholds were chosen to maximize 
F-Score. 
4.4 
                                                          
Discussion 
It is clear from the results above that the perform-
ance of the ME model for frame element classifica-
tion is robust to the use of automatically identified 
frame element boundaries.  Further, the ME 
5 G&J?s results for the combined task were generated with a 
threshold applied to the FE classifier (Dan Gildea, personal 
communication).  This is why their precision/recall scores are 
dissimilar to their accuracy scores, as reported in section 3.  
Because the ME classifier does not employ a threshold, com-
parisons must be based on F-score. 
framework yields better results on the frame ele-
ment identification task than the simple linear in-
terpolation model of Gildea and Jurafsky.  This 
result is not surprising given the discussion in Sec-
tion 3.   
What is striking, however, is the drastic overall 
reduction in performance on the combined 
identification and classification task.  The 
bottleneck here is the identification of frame 
element boundaries.  Unlike with classification 
though, Figure 5 indicates that a plateau in the 
learning curve has been reached, and thus, more 
data will not yield as dramatic an improvement for 
the given feature set and model.   
5 Conclusion 
The results reported here show that ME models 
provide higher performance on frame element clas-
sification tasks, given both human and automati-
cally identified frame element boundaries, than the 
linear interpolation models examined in previous 
work.  We attribute this increase to the benefits of 
the ME framework itself, the incorporation of sen-
tence-level syntactic patterns into our feature set, 
and the use of previous tag information to find the 
most probable sequence of roles for a sentence.   
But perhaps most striking in our results are the 
effects of varying training set size on the perform-
ance of the classification and identification models.  
While for classification, the learning curve appears 
to be still increasing with training set size, the 
learning curve for identification appears to have 
already begun to plateau.  This suggests that while 
classification will continue to improve as the Fra-
meNet database gets larger, increased performance 
on identification will rely on the development of 
more sophisticated models. 
In future work, we intend to apply the lessons 
learned here to the problem of frame element iden-
tification.  Gildea and Jurafsky have shown that 
improvements in identification can be had by more 
closely integrating the task with classification (they 
report an F-Score of .719 using an integrated 
model).  We are currently exploring a ME ap-
proach which integrates these two tasks under a 
tagging framework.  Initial results show that sig-
nificant improvements can be had using techniques 
similar to those described above. 
Acknowledgments 
The authors would like to thank Dan Gildea who 
generously allowed us access to his data files and 
Oliver Bender for making the MEtagger software 
available.  Finally, we thank Franz Och whose help 
and expertise was invaluable. 
References 
O. Bender, K. Macherey, F. J. Och, and H. Ney. 
2003. Comparison of Alignment Templates and 
Maximum Entropy Models for Natural Lan-
guage Processing. Proc. of EACL-2003.  Buda-
pest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natu-
ral Language Processing. Computational Lin-
guistics, vol. 22, no. 1. 
S. F. Chen and R. Rosenfeld. 1999. A Gaussian 
prior for smoothing maximum entropy models. 
Technical Report CMUCS -99-108, Carnegie 
Mellon University 
M. Collins. 1997. Three generative, lexicalized 
models for statistical parsing.  Proc. of the 35th 
Annual Meeting of the ACL.  pages 16-23, Ma-
drid, Spain. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
iterative scaling for log-linear models. Annals 
of Mathematical Statistics, 43:1470-1480. 
C. Fillmore 1976.  Frame semantics and the nature 
of language. Annals of the New York Academy 
of Sciences: Conference on the Origin and De-
velopment of Language and Speech, Volume 
280 (pp. 20-32).  
D. Gildea and D. Jurafsky. 2002.  Automatic La-
beling of Semantic Roles, Computational Lin-
guistics, 28(3) 245-288 14.  
T. Mitchell. 1997.  Machine Learning.  McGraw-
Hill International Editions, New York, NY. 
Pages 143-145. 
F.J. Och. 2002. Yet another maxent toolkit: 
YASMET. www-i6.informatik.rwth-
aachen.de/Colleagues/och/. 
 
 
Statistical QA - Classifier vs. Re-ranker: What?s the difference? 
Deepak Ravichandran, Eduard Hovy, and Franz Josef Och 
Information Sciences Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
{ravichan,hovy,och}@isi.edu 
Abstract 
In this paper, we show that we can ob-
tain a good baseline performance for 
Question Answering (QA) by using 
only 4 simple features. Using these fea-
tures, we contrast two approaches used 
for a Maximum Entropy based QA sys-
tem. We view the QA problem as a 
classification problem and as a re-
ranking problem. Our results indicate 
that the QA system viewed as a re-
ranker clearly outperforms the QA sys-
tem used as a classifier. Both systems 
are trained using the same data.  
1 Introduction 
Open-Domain factoid Question Answering (QA) 
is defined as the task of answering fact-based 
questions phrased in Natural Language. Exam-
ples of some question and answers that fall in 
the fact-based category are: 
 
1. What is the capital of Japan? - Tokyo 
2. What is acetaminophen? - Non-aspirin pain killer 
3. Where is the Eiffel Tower? - Paris 
 
The architecture of most of QA systems consists 
of two basic modules: the information retrieval 
(IR) module and the answer pinpointing module. 
These two modules are used in a typical pipeline 
architecture. 
For a given question, the IR module finds a 
set of relevant segments. Each segment typically 
consists of at most R sentences1 . The answer 
pinpointing module processes each of these seg-
ments and finds the appropriate answer phrase. 
                                                          
1
 In our experiments we use R=1  
phrase. Evaluation of a QA system is judged on 
the basis on the final output answer and the cor-
responding evidence provided by the segment. 
This paper focuses on the answer pinpointing 
module. Typical QA systems perform re-ranking 
of candidate answers as an important step in 
pinpointing. The goal is to rank the most likely 
answer first by using either symbolic or statisti-
cal methods. Some QA systems make use of 
statistical answer pinpointing (Xu et. al, 2002; 
Ittycheriah, 2001; Ittycheriah and Salim, 2002) 
by treating it as a classification problem. In this 
paper, we cast the pinpointing problem in a sta-
tistical framework and compare two approaches, 
classification and re-ranking. 
2 Statistical Answer Pinpointing 
2.1 Answer Modeling 
The answer-pinpointing module gets as input a 
question q and a set of possible answer candi-
dates }...{ 21 Aaaa . It outputs one of the answer 
}...{ 21 Aaaaa ?  from the candidate answer set. 
We consider two ways of modeling this prob-
lem.  
One approach is the traditional classification 
view (Ittycheriah, 2001) where we present each 
Question-Answer pair to the classifier which 
classifies it as either correct answer (true) or in-
correct answer (false), based on some evidence 
(features).  
In this case, we model )},...{,|( 21 qaaaacP A . 
Here, 
 false}{true,c = signifies the correctness 
of the answer a  with respect to the question q . 
The probability )},...{,|( 21 qaaaacP A  for each QA 
pair is modeled independently of other such 
pairs. Thus, for the same question, many QA 
pairs are presented to the classifier as independ-
ent events (histories). If the training corpus con-
tains Q questions with A answers for each ques-
tion, the total number of events (histories) would 
be equal to Q?A with two classes (futures) (cor-
rect or incorrect answer) for each event. Once 
the probabilities )},...{,|( 21 qaaaacP A  have been 
computed, the system has to return the best an-
swer. The following decision rule is used: 
)]},...{,|([maxarg 21 qaaaatruePa A
a
=
?
 
Another way of viewing the problem is as a 
re-ranking task. This is possible because the QA 
task requires the identification of only one cor-
rect answer, instead of identifying all the correct 
answer in the collection. In this case, we model 
)},...{|( 21 qaaaaP A . If the training corpus contains 
Q questions with A answers for each question, 
the total number of events (histories) would be 
equal to Q, with A classes (futures). This view 
requires the following decision-rule to identify 
the answer that seems most promising: 
)]},...{|([maxarg 21 qaaaaPa A
a
=
?
 
In summary, 
 Classifier Re-ranker 
#Events (Histo-
ries) 
Q?A Q 
#Classes (Futures) 
per event 
2 A 
 
where, 
Q = total number of questions. 
A = total number of answer chunks considered 
for each question. 
2.2 Maximum Entropy formulation 
We use Maximum Entropy to model the given 
problem both as a classifier and a re-ranker. We 
define M feature functions, 
Mmqaaaaf Am ,....,1),},...{,( 21 = , that may be useful 
in characterizing the task. Della Pietra et. al 
(1995) contains good description of Maximum 
Entropy models. 
We model the classifier as follows: 
? ?
?
=
=
=
?
1
21?,
1
21,
21
)]},...{,(exp[
)]},...{,(exp[
)},...{,|(
c
M
m
Amcm
M
m
Amcm
A
qaaaaf
qaaaaf
qaaaacP
?
?
 
where, 
},{;,....,1;
,
falsetruecMmcm ==? are the model 
parameters. 
The decision rule for choosing the best an-
swer is: 
])},...{,([maxarg
)]},...{,|([maxarg
1
21,
21
?
=
?
=
=
M
m
Amtruem
a
A
a
qaaaaf
qaaaatruePa
?
 
The above decision rule requires comparison of 
different probabilities of the 
form )},...{,|( 21 qaaaatrueP A . However, these 
probabilities are modeled as independent events 
(histories) in the classifier and hence the training 
criterion does not make them directly compara-
ble. 
For the re-ranker, we model the probability 
as: 
( )
? ?
?
=
=
=
?
1
21
1
21
21
)]},...{,?(exp[
)]},...{,(exp[
},...{|
a
M
m
Amm
M
m
Amm
A
qaaaaf
qaaaaf
qaaaaP
?
?
 
where, 
Mmm ,....,1; =?  are the model parameters. 
Note that for the classifier the model parameters 
are cm,?  , whereas for the re-ranker they are m? . 
This is because for the classifier, each feature 
function has different weights associated with 
each class (future). Hence, the classifier has 
twice the model parameters as compared to the 
re-ranker. 
The decision rule for the re-ranker is given by: 
.
?
=
?
=
=
M
m
Amm
a
A
a
qaaaaf
qaaaaPa
1
21
21
)]},...{,([maxarg
)]},...{|([maxarg
?
  
The re-ranker makes the probabilities 
)},...{|( 21 qaaaaP A , considered for the decision 
rule, directly comparable against each other, by 
incorporating them into the training criterion 
itself. Table 1 summarizes the differences of the 
two models. 
  
2.3 Feature Functions 
Using above formulation to model the probabil-
ity distribution we need to come up with features 
fj. We use only four basic feature functions for 
our system. 
1. Frequency: It has been observed that the 
correct answer has a higher frequency 
(Magnini et al; 2002) in the collection of 
answer chunks (C). Hence we count the 
number of time a potential answer occurs in 
the IR output and use its logarithm as a fea-
ture. This is a positive continuous valued 
feature. 
2. Expected Answer Class: Most of the current 
QA systems employ some type of Answer 
Class Identification module. Thus questions 
like ?When did Bill Clinton go to college??, 
would be identified as a question asking 
about a time (or a time period), ?Where is 
the sea of tranquility?? would be identified 
as a question asking for a location. If the an-
swer class matches the expected answer 
class (derived from the question by the an-
swer identification module) this feature fires 
(i.e., it has a value of 1). Details of this mod-
ule are explained in Hovy et al (2002). This 
is a binary-valued feature. 
3. Question Word Absent: Usually a correct 
answer sentence contains a few of the ques-
tion words. This feature fires if the candidate 
answer does not contain any of the question 
words. This is also a binary valued feature. 
4. Word Match: It is the sum of ITF2 values for 
the words in the question that matches iden-
tically with the words in the answer sen-
tence. This is a positive continuous valued 
feature. 
2.4 Training 
We train our Maximum Entropy model using 
Generalized Iterative scaling (Darroch and 
Ratcliff, 1972) approach by using YASMET3 . 
3 Evaluation Metric 
The performance of the QA system is highly 
dependent on the performance of the two indi-
vidual modules IR and answer-pinpointing. The 
system would have excellent performance if 
both have good accuracy. Hence, we need a 
good evaluation metric to evaluate each of these 
components individually. One standard metric 
for IR is recall and precision. We can modify 
this metric for QA as follows: 
                                                          
2
 ITF = Inverse Term Frequency. We take a large inde-
pendent corpus & estimate ITF(W) =1/(count(W)), where 
W = Word. 
3
 YASMET. (Yet Another Small Maximum Entropy 
Toolkit) http://www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/software/YASMET.html 
 
 Classifier Re-Ranker 
Mode
ling 
Equa-
tion ? ?
?
=
=
=
?
1
21?,
1
21,
21
)]},...{,(exp[
)]},...{,(exp[
)},...{,|(
c
M
m
Amcm
M
m
Amcm
A
qaaaaf
qaaaaf
qaaaacP
?
?
 
? ?
?
=
=
=
?
1
21
1
21
21
)]},...{,?(exp[
)]},...{,(exp[
)},...{|(
a
M
m
Amm
M
m
Amm
A
qaaaaf
qaaaaf
qaaaaP
?
?
 
 
Deci-
sion 
Rule  
])},...{,([maxarg
)}},...{,|({maxarg
1
21,
21
?
=
?
=
=
M
m
Atruemm
a
A
a
qaaaaf
qaaaatruePa
?
 
 
.
?
=
?
=
=
M
m
Amm
a
A
a
qaaaaf
qaaaaPa
1
21
21
)]},...{,([maxarg
)]},...{|([maxarg
?
 
 
Table 1 : Model comparison between a Classifier and Re-ranker 
 segmentsanswer relevant  Total #
returnedsegment  answer relevant  #
  Recall =  
returned segments Total #
returned segmentsanswer relevant  #
 Precision =  
It is almost impossible to measure recall be-
cause the IR collection is typically large and in-
volves several hundreds of thousands of 
documents. Hence, we evaluate our IR by only 
the precision measure at top N segments. This 
method is actually a rather sloppy approximation 
to the original recall and precision measure. 
Questions with fewer correct answers in the col-
lection would have a lower precision score as 
compared to questions with many answers. 
Similarly, it is unclear how one would evaluate 
answer questions with No Answer (NIL) in the 
collection using this metric. All these questions 
would have zero precision from the IR collec-
tion. 
The answer-pinpointing module is evaluated 
by checking if the answer returned by the system 
as the top ranked (#1) answer is correct/incorrect 
with respect to the IR collection and the true 
answer. Hence, if the IR system fails to return 
even a single sentence that contains the correct 
answer for the given question, we do not penal-
ize the answer-pinpointing module. It is again 
unclear how to evaluate questions with No an-
swer (NIL). (Here, for our experiments we at-
tribute the error to the IR module.) 
Finally, the combined system is evaluated by 
using the standard technique, wherein the An-
swer (ranked #1) returned by the system is 
judged to be either correct or incorrect and then 
the average is taken. 
Question: 
1395 Who is Tom Cruise married to ? 
 
IR Output: 
1 Tom Cruise is married to actress Nicole Kidman and they have two adopted children . 
2 Tom Cruise is married to Nicole Kidman . 
. 
. 
 
Output of Chunker: (The number to the left of each chunk records the IR sentence from 
which that particular chunk came) 
1  Tom Cruise 
1  Tom 
1  Cruise 
1  is married 
1  married 
1  actress Nicole Kidman and they 
1  actress Nicole Kidman 
1  actress 
1  Nicole Kidman 
1  Nicole 
1  Kidman 
1  they 
1  two adopted children 
1  two 
1  adopted 
1  children 
2  Tom Cruise 
2  Tom 
2  Cruise 
2  is married 
2  married 
2  Nicole Kidman 
2  Nicole 
2  Kidman 
. 
. 
 
Figure 1 : Candidate answer extraction for a question. 
4 Experiments 
4.1 Framework 
Information Retrieval 
For our experiments, we use the Web search 
engine AltaVista. For every question, we re-
move stop-words and present all other question 
words as query to the Web search engine. The 
top relevant documents are downloaded. We 
apply a sentence segmentor, and identify those 
sentences that have high ITF overlapping words 
with the given question. The sentences are then 
re-ranked accordingly and only the top K sen-
tences (segments) are presented as output of the 
IR system. 
Candidate Answer Extraction 
For a given question, the IR returns top K 
segments. For our experiments a segment con-
sists of one sentence. We parse each of the sen-
tences and obtain a set of chunks, where each 
chunk is a node of the parse tree. Each chunk is 
viewed as a potential answer. For our experi-
ments we restrict the number of potential an-
swers to be at most 5000. We illustrate this 
process in Figure 1. 
Training/Test Data 
Table 2 : Training size and sources. 
 
 Training + 
Validation 
Test 
Question collec-
tion 
TREC 9 + 
TREC 10 
TREC11 
Total questions 1192 500 
 
We use the TREC 9 and TREC 10 data sets 
for training and the TREC 11 data set for testing. 
We initially apply the IR step as described above 
and obtain a set of at most 5000 answers. For 
each such answer we use the pattern file sup-
plied by NIST to tag answer chunks as either 
correct (1) or incorrect (0). This is a very noisy 
way of tagging data. In some cases, even though 
the answer chunk may be tagged as correct it 
may not be supported by the accompanying sen-
tence, while in other cases, a correct chunk may 
be graded as incorrect, since the pattern file list 
did not represent a exhaustive list of answers. 
We set aside 20% of the training data for valida-
tion. 
4.2 Classifier vs. Re-Ranker 
We evaluate the performance of the QA system 
viewed as a classifier (with a post-processing 
step) and as a re-ranker. In order to do a fair 
evaluation of the system we test the performance 
of the QA system under varying conditions of 
the output of the IR system. The results are 
shown in Table 3. 
The results should be read in the following 
way: We use the same IR system. However, dur-
ing each run of the experiment we consider only 
the top K sentences returned by the IR system 
K={1,10,50,100,150,200}. The column ? cor-
rect?  represents the number of questions the en-
tire QA (IR + re-ranker) system answered 
correctly. ? IR Loss?  represents the average 
number of questions for which the IR failed 
completely (i.e., the IR did not return even a sin-
gle sentence that contains the correct answer). 
The IR precision is the precision of the IR sys-
tem for the number of sentences considered. An-
swer-pinpointing performance is based on the 
metric described above. Finally, the overall 
score is the score of the entire QA system. (i.e., 
precision at rank#1). 
The ? Overall Precision" column indicates 
that the re-ranker clearly outperforms the classi-
fier. However, it is also very interesting to com-
pare the performance of the re-ranker ? Overall 
Precision?  with the ? Answer-Pinpointing preci-
sion? . For example, in the last row, for the re-
ranker the ? Answer-Pinpointing Precision?  is 
0.5182 whereas the ? Overall Precision?  is only 
0.34. The difference is due to the performance of 
the poor performance of the IR system (? IR 
Loss?  = 0.344). 
4.3 Oracle IR system 
In order to determine the performance of the 
answer pinpointing module alone, we perform 
the so-called oracle IR experiment. Here, we 
present to the answer pinpointing module only 
those sentences from IR that contain an answer4. 
The task of the answer pinpointing module is to 
pick out of the correct answer from the given 
collection. We report results in Table 4. In these 
results too the re-ranker has better performance 
as compared to the classifier. However, as we 
see from the results, there is a lot of room for 
improvement for the re-ranker system, even with 
a perfect IR system. 
5 Discussion 
Our experiments clearly indicate that the QA 
system viewed as a re-ranker outperforms the 
QA system viewed as a classifier. The difference 
stem from the following reasons: 
1. The classification training criteria work on a 
more difficult objective function of trying to 
find whether each candidate answer answers 
the given question, as opposed to trying to 
find the best answer for the given question. 
Hence, the same feature set that works for 
the re-ranker need not work for the classi-
fier. The feature set used in this problem is 
not good enough to help the classifier dis-
tinguish between correct and incorrect an-
                                                          
4
 This was performed by extracting all the sentences that 
were judged to have the correct answer by human evalua-
tors during the TREC 2002 evaluations. 
swers for the given question (even though it 
is good for the re-ranker to come up with the 
best answer). 
2. The comparison of probabilities across dif-
ferent events (histories) for the classifier, 
during the decision rule process, is problem-
atic. This is because the probabilities, which 
we obtain after the classification approach, 
are only a poor estimate of the true probabil-
ity. The re-ranker, however, directly allows 
these probabilities to be comparable by in-
corporating them into the model itself. 
3. The QA system viewed as a classifier suf-
fers from the problem of a highly unbal-
anced data set. We have less than 1% 
positive examples and more than 99% nega-
tive examples (we had almost 4 million 
training data events) in the problem. Ittyche-
riah (2001), and Ittycheriah and Roukos 
(2002), use a more controlled environment 
for training their system. They have 23% 
positive examples and 77% negative exam-
ples. They prune out most of the incorrect 
answer initially, using a pre-processing step 
by using either a rule-based system (Ittyche-
riah, 2001) or a statistical system (Ittyche-
riah et al, 2002); and hence obtain a much 
more manageable distribution in the training 
phase of the Maximum Entropy model. 
Answer-Pinpointing 
Precision Number Correct Overall Precision IR Sen-
tences 
Total 
ques-
tions IR Precision IR Loss Classifier Re-ranker Classifier Re-ranker Classifier Re-ranker 
1 500 0.266 0.742 0.0027 0.3565 29 46 0.058 0.092 
10 500 0.2018 0.48 0.0016 0.4269 7 111 0.014 0.222 
50 500 0.1155 0.386 0.0015 0.4885 6 150 0.012 0.3 
100 500 0.0878 0.362 0.0015 0.5015 5 160 0.01 0.32 
150 500 0.0763 0.35 0.0015 0.5138 5 167 0.01 0.334 
200 500 0.0703 0.344 0.0015 0.5182 3 170 0.01 0.34 
Table 3 : Results for Classifier and Re-ranker under varying conditions of IR. 
IR Sentences = Total IR sentences considered for every question 
IR Precision = Precision @ (IR Sentences) 
IR Loss = (Number of Questions for which the IR did not produce a single answer)/(Total Questions) 
Overall Precision = (Number Correct)/(Total Questions) 
  
 
 
 
 
6 Conclusion 
The re-ranker system is very robust in handling 
large amounts of data and still produces reason-
able results. There is no need for a major pre-
processing step (for eliminating undesirable in-
correct answers from the training) or the post-
processing step (for selecting the most promis-
ing answer.) 
We also consider it significant that a QA sys-
tem with just 4 features (viz. Frequency, 
Expected Answer Type, Question word absent, 
and ITF word match) is a good baseline system 
and performs better than the median perform-
ance of all the QA systems in the TREC 2002 
evaluations5.  
Ittycheriah (2001), and Ittycheriah and Rou-
kos (2002) have shown good results by using a 
range of features for Maximum Entropy QA sys-
tems. Also, the results indicate that there is 
scope for research in IR for QA systems. The 
QA system has an upper ceiling on performance 
due to the quality of the IR system. The QA 
community has yet to address these problems in 
a principled way, and the IR details of most of 
the system are hidden behind the complicated 
system architecture. 
The re-ranking model basically changes the 
objective function for training and the system is 
directly optimized on the evaluation function 
criteria (though still using Maximum Likelihood 
training). Also this approach seems to be very 
robust to noisy training data and is highly scal-
able. 
Acknowledgements. 
This work was supported by the Advance Re-
search and Development Activity (ARDA)?s 
Advanced Question Answering for Intelligence 
(AQUAINT) Program under contract number 
                                                          
5
 However, since the IR system used here was from the 
Web, our results are not directly comparable with the 
TREC systems. 
MDA908-02-C-007. The authors wish to ex-
press particular gratitude to Dr. Abraham It-
tycheriah, both for his supervision and education 
of the first author during his summer visit to 
IBM TJ Watson Research Center in 2002 and 
for his thoughtful comments on this paper, 
which was inspired by his work. 
References 
Darroch, J. N., and D. Ratcliff. 1972. Generalized 
iterative scaling for log-linear models. Annals of 
Mathematical Statistics, 43:1470?1480. 
Hermjakob, U. 1997. Learning Parse and Translation 
Decisions from Examples with Rich Context. 
Ph.D. Dissertation, University of Texas at Austin, 
Austin, TX.  
Hovy, E.H., U. Hermjakob, D. Ravichandran. 2002. 
A Question/Answer Typology with Surface Text 
Patterns. Proceedings of the DARPA Human Lan-
guage Technology Conferenc,. San Diego, CA, 
247?250. 
Ittycheriah, A. 2001. Trainable Question Answering 
System. Ph.D. Dissertation, Rutgers, The State 
University of New Jersey, New Brunswick, NJ. 
Ittycheriah., A., and S. Roukos. 2002. IBM?S Ques-
tion Answering System-TREC-11. Proceedings of 
TREC 2002, NIST, MD, 394?401. 
Magnini, B, M. Negri, R. Prevete, and H. Tanev. 
2002. Is it the Right Answer? Exploiting Web Re-
dundancy for Answer Validation. Proceedings of 
the 40th Meeting of the Association of Computa-
tional Linguistics, Philadelphia, PA, 425?432. 
Della Pietra, S., V. Della Pietra, and J. Lafferty. 
1995. Inducing Features of Random Fields, Tech-
nical Report Department of Computer Science, 
Carnegie-Mellon University, CMU?CS-95?144. 
Xu, J., A. J. Licuanan, S. May, R. Miller, and R. 
Weischedel. 2002. TREC2002QA at BBN: An-
swer Selection and Confidence Estimation. Pro-
ceedings of TREC 2002. NIST MD. 290?295 
Answer-Pinpointing 
Precision Total ques-
tions IR precision Classifier Re-ranker 
429 1.0 0.156 0.578 
Table 4 : Performance with a perfect IR system 
 
Multi-Document Person Name Resolution 
Michael Ben Fleischman 
Massachusetts Institute of Technology
77 Massachusetts Ave. 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
Multi-document person name resolution fo-
cuses on the problem of determining if two 
instances with the same name and from dif-
ferent documents refer to the same individ-
ual.  We present a two-step approach in 
which a Maximum Entropy model is 
trained to give the probability that two 
names refer to the same individual.  We 
then apply a modified agglomerative clus-
tering technique to partition the instances 
according to their referents.   
1 Intro 
Artists and philosophers have long noted that mul-
tiple distinct entities are often referred to by one 
and the same name (Cohen and Cohen, 1998; Mar-
tinich, 2000).  Recently, this referential ambiguity 
of names has become of increasing concern to 
computational linguists, as well.  As the Internet 
increases in size and coverage, it becomes less and 
less likely that a single name will refer to the same 
individual on two different web sites.  This poses a 
great challenge to information retrieval (IR) and 
question-answering (QA) applications, which often 
rely on little data when responding to user queries.   
Another area in which referential ambiguity is 
problematic involves the automatic population of 
ontologies with instances.  For such tasks, concept-
instance pairs (such as Paul Simon/pop star) are 
extracted from the web, cleaned of noise, and then 
inserted into an already existing ontology.  The 
process of insertion requires that concept-instance 
pairs that have the same referent be merged to-
gether (e.g. Paul Simon/pop star and Paul Simon 
/singer).  Further, instances of the same name, but 
with different referents, must be distinguished (e.g. 
Paul Simon/pop star and Paul Simon /politician).   
We propose a two-step approach: first, we train a 
maximum entropy model to generate the probabil-
ity that any two concept-instance pairs refer to one 
and the same referent.  Then, a modified agglom-
erative clustering technique is used to merge the 
most likely instances together, forming clusters that 
correspond to individual referents.   
2 Related Work 
While there has been a great deal of work on 
coreference resolution within a single document, 
little work has focused on the challenges associated 
with resolving the reference of identical person 
names across multiple documents.   
Mann and Yarowsky (2003) are amongst the few 
who have examined this problem.  They treat it as a 
clustering task, in which, a combination of features 
(such as, a weighted bag of words and biographic 
information extracted from the text) are given to an 
agglomerative clustering algorithm, which outputs 
two clusters representing the two referents of the 
query name. 
Mann and Yarowsky (2003) report results on two 
types of evaluations: using hand-annotated web-
pages returned from truly ambiguous searches, they 
report precision/recall scores of 0.88/0.73; using 
?psuedonames?1 they report an accuracy of 86.4%. 
                                                          
1 Borrowing from techniques in word sense disambigua-
tion, they create a test set of 28 ?pseudonames? by ran-
While Mann and Yarowsky (2003) describe a 
number of useful features for multi-document per-
son name resolution, their technique is limited by 
only allowing a set number of referent clusters.  
Further, as discussed below, their use of artificial 
test data makes it difficult to determine how well it 
generalize to real world problems. 
Bagga and Baldwin (1998) also present an ex-
amination of multi-document person name resolu-
tion.  They first perform within-document 
coreference resolution to form coreference chains 
for each entity in each document.  They then use 
the text surrounding each reference chain to create 
summaries about each entity in each document.  
These summaries are then converted to a bag of 
words feature vector and are clustered using the 
standard vector space model often employed in IR. 
They evaluated their system on 11 entities named 
John Smith taken from a set of 173 New York 
Times articles.  Using an evaluation metric similar 
to a weighted sum of precision and recall they get 
an F-measure of 0.846.   
Although their technique allows for the discovery 
of a variable number of referents, its use of 
simplistic bag of words clustering is an inherently 
limiting aspect of their methodology.  Further, that 
they only evaluate their system, on a single person 
name begs the question of how well such a tech-
nique would fair on a more real-world challenge. 
3 Maximum Entropy Model  
3.1 Data 
Fleischman et al (2003) describe a dataset of con-
cept-instance pairs extracted automatically from a 
very large corpus of newspaper articles.  The data-
set (referred to here as the ACL dataset) contains 
approximately 2 million pairs (of which 93% are 
legitimate) in which the concept is represented by a 
complex noun phrase (e.g. president of the United 
                                                                                            
domly selecting two names from a hand crafted list of 8 
individuals (e.g., Haifa Al-Faisal and Tom Cruise) and 
treat the pair as one name with two referents.   
States) and the instance by a name (e.g. William 
Jefferson Clinton).2   
A set of 2675 legitimate concept-instance pairs 
was randomly selected from the ACL dataset de-
scribed above; each of these was then matched 
with another concept-instance pair that had an 
identical instance name, but a different concept 
name.  This set of matched pairs was hand tagged 
by a human annotator to reflect whether or not the 
identical instance names actually referred to the 
same individual.  The set was then randomly split 
into a training set of 1875 matched pairs (84% re-
ferring to the same individual), a development set 
of 400 matched pairs (85.5% referring to the same 
individual), and a test set of 400 matched pairs 
(83.5% referring to the same individual). 
3.2 Features 
In designing a binary classifier to determine 
whether two concept-instance pairs refer to the 
same individual, we formulate a number of differ-
ent features used to describe each matched pair.  
These features are summarized in Table 1, and de-
scribed in more detail below. 
Name Features 
We use a number of methods meant to express in-
formation available from the orthography of the 
instance name itself.  The first of these features 
(Name-Common) seeks to estimate the commonal-
ity of the instance name.  With this features we 
hope to capture the intuition that more common 
names (such as John Smith) will be more likely to 
refer to different individuals than more uncommon 
names (such as Yasir Arafat).  We calculate this 
feature by splitting the instance name into first, 
middle (if necessary) and last sub-names.  We then 
use a table of name frequencies downloaded from 
the US census website to give each sub-name a 
score; these scores are then multiplied together for 
a final value.   
The second name statistic feature estimates how 
famous the instance name is.  With this features we 
                                                          
2 Although the dataset includes multiple types of named 
entities, we focus here only on person names. 
hope to capture the intuition that names of very 
famous people (such as Michael Jackson) are less 
likely to refer to different individuals than less fa-
mous, yet equally common, names (such as John 
Smith).  We calculate this feature in two ways: 
first, we use the frequency of the instance name as 
it appears in the ACL dataset to give a representa-
tion of how often the name appears in newspaper 
text (Name-Fame); second, we use the number of 
hits reported on google.com for a query consisting 
of the quoted instance name itself (Web-Fame).  
These fame features are used both as is and scaled 
by the commonality feature described above. 
Web Features 
Aside from the fame features described above, we 
use a number of other features derived from web 
search results.  The first of which, called WebInter-
section, is simply the number of hits returned for a 
query using the instance name and the heads of 
each concept noun phrase in the match pair; i.e., 
(name + head1 +head2).   
The second, called WebDifference, is the abso-
lute value of the difference between the hits re-
turned from a query on the instance name and just 
the head of concept 1 vs. the instance name and 
just the head of concept 2; i.e., abs ((name + head1) 
-(name +head2)).   
The third, called WebRatio, is the ratio between 
the WebIntersection score and the sum of the hits 
returned when querying the instance name and just 
the head of concept 1 and the instance name and 
just the head of concept 2; i.e., (name + head1 
+head2) / ((name + head1) +(name +head2)).   
Overlap Features 
In order to capture some aspects of the contex-
tual cues to referent disambiguation, we include 
features representing the similarity between the 
sentential contexts from which each concept-
instance pair was extracted.  The similarity metric 
that we use is a simple word overlap score based 
on the number of words that are shared amongst 
both sentences.  We include scores in which each 
non-stop-word is treated equally (Sentence-Count), 
as well as, in which each non-stop-word is 
weighted according to its term frequency in a large 
corpus (Sentence-TF).  We further include two 
similar features that only examine the overlap in 
the concepts (Concept-Count and Concept-TF).  
 
Name Features 
Feature Name Description 
Name-Common  frequency of name in census data 
Name-Fame frequency of name in ACL dataset 
Web-Fame # of hits from name query 
Web Features 
Web Intersection query(name + head1 +head2) 
Web Difference 
abs( query(name + head1)  
+ query(name +head2)) 
Web Ratio 
query(name + head1 +head2)  
/ ( qry(name + head1) +qry(name +head2))
Overlap Features 
Sentence-Count
# of words common to  
context of both instances 
Sentence-TF 
as above but weighted  
by term frequency 
Concept-Count 
# of words common to  
concept of both instances 
Concept-TF 
as above but weighted  
by term frequency 
Semantic Features 
JCN sem. dist. of Jiang and Conrath 
HSO sem. dist. of Hirst and St. Onge 
LCH sem. dist. of Leacock and Chodrow 
Lin sem. dist. of Lin 
Res sem. dist. of Resnik 
Estimated Statistics 
F1 p(i1=i2 | i1?A, i2?B) 
F2 p(i1?A, i2?B | i1=i2) 
F3 p(i1?A | i2?B) + p(i2?B | i1?A) 
F4 p(i1?A, i2?B) / (p(i1?A) + p(I2?B)) 
Table 1.  Features used in Max. Ent. model split accord-
ing to feature type. 
Semantic Features 
Another important clue in determining the corefer-
ence of instances is the semantic relatedness of the 
concepts with which they are associated.  In order 
to capture this, we employ five metrics described in 
the literature that use the WordNet ontology to de-
termine a semantic distance between two lexical 
items (Budanitsky and Hirst. 2001).  We use the 
implementation described in Pedersen (2004) to 
create features corresponding to the scores on the 
following metrics shown in Table 1. Due to prob-
lems associated with word sense ambiguity, we 
take the maximum score amongst all possible com-
binations of senses for the heads of the concepts in 
the matched pair.  The final output to the model is 
a single similarity measure for each of the eight 
metrics described in Pedersen (2004). 
Estimated Statistics Features 
In developing features useful for referent disambigua-
tion, it is clear that the concept information to which we 
have access is very useful.  For example, given that we 
see John Edwards /politician and John Edwards 
/lawyer, our knowledge that politicians are often lawyers 
is very useful in judging referential identity.3  In order to 
exploit this information, we leverage the strong correla-
tion between orthographic identity of instance names 
and their referential identity.   
As described above, approximately 84% of those 
matched pairs that had identical instance names 
referred to the same referent.  In a separate exami-
nation, we found, not surprisingly, that nearly 
100% of pairs that were matched to instances with 
different names (such as Bill Clinton vs. George 
Clinton) referred to different referents.   
We take advantage of this strong correlation in 
developing features by first making the (admittedly 
wrong) assumption that orthographic identity is 
equivalent to referential identity, and then using 
that assumption to calculate a number of statistics 
over the large ACL dataset.  We postulate that the 
noise introduced by our assumption will be offset 
by the large size of the dataset, yielding a number 
of highly informative features. 
The statistics we calculate are as follows: 
  
P1:  The probability that instance 1 and in-
stance 2 have the same referent given that in-
stance 1 is paired with concept A and instance 2 
with concept B; i.e., p(i1=i2 | i1?A, i2?B) 
P2:  The probability that instance 1 is paired 
with concept A and instance 2 with concept B 
given that instance 1 and instance 2 have the 
same referent; i.e., p(i1?A, i2?B | i1=i2) 
P3:  The probability that instance 1 is paired 
with concept A given that instance 2 is paired 
with concept B plus the probability that instance 
                                                          
3 It should be noted that this feature is attempting to en-
code knowledge about what concepts occur together in 
the real world, which is different than, what is being 
encoded in the semantic features, described above.   
2 is paired with concept B given that instance 1 
is paired with concept A; i.e., p(i1?A | i2?B) 
+ p(i2?B | i1?A) 
P4:  The probability that instance 1 is paired 
with concept A and instance 2 is paired with 
concept B divided by the probability that in-
stance 1 is paired with concept A plus the prob-
ability that instance 2 is paired with concept B; 
i.e., p(i1?A, i2?B) / (p(i1?A) + p(i2?B)) 
 
 
 
 
 
 
 
Figure 1.  Results of Max. Ent. classifier on held out test 
data compared to baseline (i.e., always same referent). 
Aside from the noise introduced by the assump-
tion described above, another problem with these 
features arises when the derived probabilities are 
based on very low frequency counts.  Thus, when 
adding these features to the model, we bin each 
feature according to the number of counts that the 
score was based on.   
3.3 Model 
Maximum Entropy (Max. Ent.) models implement 
the intuition that the best model will be the one that 
is consistent with the set of constrains imposed by 
the evidence, but otherwise is as uniform as possi-
ble (Berger et al, 1996).  We model the probability 
of two instances having the same referent (r=[1,0]) 
given a vector of features x according to the Max. 
Ent. formulation below: 
?
=
=
n
i
i
x
xrfZxrp
0
i )],(exp[1)|( ?  
Here Zx is a normalization constant, fi(r,x) is a 
feature function over values of r and vector ele-
ments, n is the total number of feature functions, 
and ?i is the weight for a given feature function.  
The final output of the model is the probability 
83.50%
90.75%
78%
80%
82%
84%
86%
88%
90%
92%
Baseline Max Ent
%
 C
or
re
ct
given a feature vector that r=1; i.e., the probability 
that the referents are the same. 
We train the Max. Ent. model using the 
YASMET Max. Ent. package (Och, 2002).  Feature 
weights are smoothed using Gaussian priors with 
mean 0.  The standard deviation of this distribution 
is optimized on the development set, as is the num-
ber of training iterations and the probability thresh-
old used to make the hard classifications reported 
in the following experiment. 
3.4 Experimental Results  
Results for the classifier on the held out test set are 
reported in Figure 1.  Baseline here represents al-
ways choosing the most common classification 
(i.e., instance referents are the same).  Figure 2 
represents the learning curve associated with this 
task.  Figure 3 shows the effect on performance of 
incrementally adding the best feature set (as deter-
mined by greedily trying each one) to the model.   
 
 
 
 
 
 
 
Figure 2.  Learning curve of Max. Ent. model.   
3.5 Discussion 
It is clear from the results that this model outper-
forms the baseline for this task (p>0.01) (p<0.01) 
(Mitchell, 1997).  Interestingly, although the num-
ber of labeled examples that were used to train the 
system was by no means extravagant, it appears 
from the learning curve that increasing the size of 
the training set will not have a large effect on clas-
sifier performance.  Also of interest, Figure 3 
shows that the greedy feature selection technique 
found that the most powerful features for this task 
are the estimated statistic features and the web fea-
tures.  While the benefit of such large corpora fea-
tures is not surprising, the relative lack of power 
from the semantic and overlap features (which ex-
ploit ontolological and contextual information) was 
surprising. 4  In future work, we will examine how 
more sophisticated similarity metrics and larger 
windows of context (e.g., the whole document) 
might improve performance. 
4 Clustering 
 
 
 
 
 
 
 
 
Figure 3.  Results of Max. Ent. classifier on held out 
data using different subsets of feature types.  Feature 
types are greedily added one at a time, starting with Es-
timated Statistics and ending with Semantic Features.   
Having generated a model to predict the probability 
that two concept-instance pairs with the same name 
refer to the same individual, we are faced with the 
problem of using such a model to partition all of 
our concept-instance pairs according to the indi-
viduals to which they actually refer.  Although, 
ideally, we should be able to simply apply the 
model to all possible pairs, in reality, such a meth-
odology may lead to a contradiction.   
For example, given that the model predicts in-
stance A is identical to instance B, and in addition, 
that instance B is identical to C, because of the 
transitivity of the identity relation, we must assume 
that A is identical to C.  However, if the model 
predicts that A is not identical to C, (which can and 
does occur) we must assume the model is wrong in 
at least one of its three predictions. 
                                                          
4 Note that for these tests, the model parameters are not 
optimized for each run; thus, the performance is slightly 
worse than in Figure 1. 
11%
12%
13%
14%
15%
16%
10 100 1000
# of Training Examples
%
 E
rr
o
r
88.3%
83.5%
87.0%
89.3% 89.0%
88.3%
80%
81%
82%
83%
84%
85%
86%
87%
88%
89%
90%
baseline stat. +web +name +over +sem   
(all feats)
%
 C
or
re
ct
Following Ng and Cardie (2002), we address this 
problem by clustering each set of concept-instance 
pairs with identical names, using a form of group-
average agglomerative clustering, in which the 
similarity score between instances is just the prob-
ability output by the model.  Because standard ag-
glomerative clustering algorithms are O(n3) if 
cosign similarity metrics are not used (Manning 
and Schutze, 2001), we adapt the method to our 
framework.  Our algorithm operates as follows5: 
 
On input D={concept-instance pairs of same name}, 
build a fully connected graph G with vertex set D: 
 
1) Label each edge (d,d?) in G with a score correspond-
ing to the probability of identity predicted by the 
Max. Ent. model 
2) While the edge with max score in G > threshold: 
a. Merge the two nodes connected by the edge with 
the max score.   
b. For each node in the graph  
a. Merge the two edges connecting it to the 
newly merged node  
b. Assign the new edge a score equal to the avg. 
of the two old edge scores. 
 
The final output of this algorithm is a new graph 
in which each node represents a single referent as-
sociated with a set of concept-instance pairs.  This 
algorithm provides an efficient way, O(n2), to com-
pose the pair-wise information given by the model.  
Further, because the only free parameter is a merg-
ing threshold (which can be determined through 
cross-validation) the algorithm is free to choose a 
different number of referents for each instance 
name it is tested on.  This is critical for the task 
because each instance name can have any number 
of referents associated with it. 
4.1 Test Data 
In order to test clustering, we randomly selected a 
set of 31 instance names from the ACL dataset, 11 
of which referred to multiple individuals and 20 of 
which had only a single referent6.  Each concept-
                                                          
5 This algorithm was developed with Hal Daume (tech-
nical report, in prep.). 
6 In an examination of 113 different randomly selected 
instance names from the ACL dataset we found that 32 
instance pair with that instance name was then ex-
tracted and hand annotated such that each individ-
ual referent was given a unique identifying code.   
We chose not to test on artificially generated test 
examples (such as the pseudo-names described in 
Mann and Yarowsky, 2003) because of our reli-
ance on name orthography in feature generation 
(see section 3.2).  Further, such pseudo-names ig-
nore the fact that names often correlate with other 
features (such as occupation or birthplace), and that 
they do not guarantee clean test data (i.e., the two 
names chosen for artificial identity may themselves 
each refer to multiple individuals). 
4.2 Experimental Design 
In examining the results of the clustering, we chose 
to use a simple clustering accuracy as our perform-
ance metric.  According to this technique, we 
match the output of our system to a gold standard 
clustering (defined by the hand annotations de-
scribed above).7   
We compare our algorithm on the 31 sets of con-
cept-instance pairs described above against two 
baseline systems.  The first (baseline1) is simply a 
single clustering of all pairs into one cluster; i.e., 
all instances have the same referent.  The second 
(baseline2) is a simple greedy clustering algorithm 
that sequentially adds elements to the previous 
cluster whose last-added element is most similar 
(and exceeds some threshold set by cross valida-
tion).  
4.3 Results 
In examining performance, we present a weighted 
average over these 31 instance sets, based on the 
number of nodes (i.e., concept-instance pairs) in 
each set of instances (total nodes = 1256).  Cross-
validation is used to set the threshold for both the 
baseline2 and modified agglomerative algorithm.  
                                                                                            
appeared only once in the dataset, 53 appeared more 
than once but always referred to the same referent, and 
28 had multiple referents. 
7 While this is a relatively simple measure, we believe 
that, if anything, it is overly conservative, and thus, 
valid for the comparisons that we are making.   
These results are presented in Table 2.  Figure 4 
examines performance as a function of the number 
of referents within each of the 31 instance sets.   
4.4 Discussion 
 
 
 
 
 
 
 
 
 
 
 
Figure 4.  Plot of performance for modified agglomera-
tive clustering and Baseline system as a function of the 
number of referents in the test set.   
 
While the algorithm we present clearly outper-
forms the baseline2 method over all 31 instance 
sets (p<0.01), we can see that it only marginally 
outperforms our most simple baseline1 method 
(p<0.10) (Mitchell, 1997).  This is due to the fact 
that for each of the 20 instance sets that only have a 
single referent, the baseline achieves a perfect 
score, while the modified agglomerative method 
only achieves a score of 96.4%.  Given this aspect 
of the baseline, and the distribution of the data, the 
fact that our algorithm outperforms the baseline at 
all speaks to its usefulness for this task.   
A better sense of the usefulness of this algorithm, 
however, can be seen by looking at its performance 
only on instance sets with multiple referents.  As 
seen in Table 3, on multiple referent instance sets, 
modified agglomerative clustering outperforms 
both the baseline1 and baseline2 methods by a sta-
tistically significant margin (p<0.01) (Mitchell, 
1997). 
5 Conclusion 
The problem of cross-document person name dis-
ambiguation is of growing concern in many areas 
of natural language processing.  We have presented 
a two-step methodology for the disambiguation of 
automatically extracted concept-instance pairs.  
Our approach first applies a Maximum Entropy 
model to all concept-instance pairs that share the 
same instance name.  The output probabilities of 
this model are then inputted to a modified agglom-
erative clustering algorithm that partitions the pairs 
according to the individuals to which they refer.  
This algorithm not only allows for a dynamically 
set number of referents, but also, outperforms two 
baseline methods. 
A clear example of the success of this algorithm 
can be seen in the output of the system for the in-
stance set for Michael Jackson (Appendix A, Table 
2).  Here, a name that refers to many individuals is 
fairly well partitioned into appropriate clusters.  
With the instance set for Sonny Bono (Appendix A, 
Table 1), however, we can see why this task is so 
challenging.  Here, although, Sonny Bono only re-
fers to one individual, the system finds (like many 
of the rest of us) that the likelihood of a singer also 
being a politician is so low that the name must re-
fer to two different people.  While this assumption 
is often true (as is the case with Paul Simon), we 
would have hoped that information from our web 
and fame features would have overridden the sys-
tem?s bias in this circumstance. 
In future work we will examine how other fea-
tures may be useful in attacking such hard cases.  
Also, we will examine how this technique can be 
applied more generally to problems that exist be-
tween non-identical, but similar names (e.g. Bill 
Clinton vs. William Jefferson Clinton).   
Acknowledgements 
The authors would like to thank Regina Barzilay for her 
helpful comments and advice, and Hal Daume for his 
useful insights into and discussion of the problem.  We 
also thank Deb Roy for his continued support 
. 
References  
A. Bagga, and B. Baldwin. 1998.  Entity-Based Cross-
Document Coreferencing Using the Vector Space 
Model. . COLING-ACL'98. 
40%
50%
60%
70%
80%
90%
100%
1 2-3 4-5 >6
# of Referents
%
 C
or
re
ct
Baseline Graph Clustering
A. Berger, S. Della Pietra and V. Della Pietra, 1996. A 
Maximum Entropy Approach to Natural Language 
Processing. Computational Linguistics, vol. 22, no. 1. 
A. Budanitsky and G. Hirst . 2001. Semantic distance in 
WordNet: An experimental, application-oriented 
evaluation of five measures. In Workshop on Word-
Net and Other Lexical Resources,  NAACL. 
J. Cohen and E. Cohen.  1998.  The Big Lebowski. Film. 
Columbia TriStar Pictures. 
M. Fleischman, E. Hovy, and A. Echihabi. 2003.  Off-
line Strategies for Online Question Answering: An-
swering Questions Before They Are Asked.  ACL, 
Sapporo, Japan. 
G. S. Mann, David Yarowsky, 2003 Unsupervised Per-
sonal Name Disambiguation, CoNLL, Edmonton, 
Canada. 
C. Manning and H. Schutze.  2001 Foundations of Sta-
tistical Natural Language Processing. MIT Press, 
Cambridge, Ma.  
A.P. Martinich, ed. 2000. The Philosophy of Language, 
Oxford University Press, Oxford, UK. 
T. Mitchell. 1997.  Machine Learning.  McGraw-Hill 
International Editions, New York, NY. Pgs. 143-145. 
V. Ng and C. Cardie Improving Machine Learning Ap-
proaches to Coreference Resolution. ACL, 2002.  
F.J. Och. 2002. Yet another maxent toolkit: YASMET. 
www-i6.informatik.rwth-aachen.de/Colleagues/och/. 
T. Pedersen. 2004. WordNet::Similarity v0.07 . 
http://www.d.umn.edu/~tpederse/similarity.html. 
Appendix A. Sample Cluster Output. 
 
Cluster 1  
time pop star Cluster 1 (cont) 
the singer former rock star 
onetime singer Cluster 2 
former singer Lawmaker 
pop singer crooning lawmaker 
former entertainer mayoral candidate 
former pop star republican politician 
former pop singer congressman 
entertainer Cluster 3 
onetime beau A freshman republican 
Singer  
Table 1.  Output clusters for the name Sonny Bono. 
 
 
 
 
Cluster 1  
platinum recording artist Cluster 2 (cont) 
cbs records artist rocker 
artist american pop superstar 
Cluster 2 visiting idol 
singer idol 
pop idol pop music superstar 
day pop superstar package entertainer 
international pop star another entertainer 
starring singer american pop singer 
american singer Cluster 3 
rock superstar local talk radio personality 
suing pop superstar kabc radio talk show host 
pop superstar los angeles radio personality 
enigmatic pop superstar veteran kabc radio talk show host 
featuring pop star ubiquitous radio commentator 
embattled pop star radio broadcaster 
controversial pop star broadcaster 
including singer Cluster 4 
featuring singer author 
even singer british beer guru 
signing pop performer beer expert 
pop singer Cluster 5 
surrounding entertainer mannequin collector 
joining entertainer Cluster 6 
including entertainer kfor commander 
singing superstar the commander of kfor 
including superstar commander of kfor 
american superstar british commander 
superstar Cluster 7 
ailing superstar the nato commander of    the kosovo liberation force 
reuter pop superstar Cluster 8 
reclusive pop superstar Designer 
quiet pop superstar Cluster 9 
embattled pop superstar deputy secretary    of transportation 
alleging pop superstar deputy secretary of the    department of transportation 
music superstar Cluster 10 
the us pop star historian 
rock star Cluster 11 
pop star education department spokesman 
entertainer company spokesman 
pop recording star dow corning spokesman 
newlywed pop star Cluster 12 
fellow pop star judge 
the singer Cluster 13 
superstar singer receiver 
setting singer career browns receiver 
rock singer trading receiver 
surrounding pop singer ravens receiver 
suing pop singer baltimore wide receiver 
reuter pop singer agent wide receiver 
prague pop singer wide receiver 
pop singer Cluster 14 
rock sensation baylor offensive tackle 
music sensation Cluster 15 
pop sensation beer writer 
Table 2.  Output clusters for the name Michael Jackson 
SENSEVAL Automatic Labeling of Semantic Roles using Maximum Entropy 
Models 
Namhee Kwon  
Information Science Institute 
University of Southern California
4676 Admiralty Way 
Marina del Rey, CA 90292 
nkwon@isi.edu 
Michael Fleischman 
Messachusetts Institute of 
Technology 
77 Massachusetts Ave 
Cambridge, MA 02139 
mbf@mit.edu 
Eduard Hovy 
Information Science Institute 
University of Southern California 
4676 Admiralty Way 
Marina del Rey, CA 90292 
hovy@isi.edu 
 
Abstract 
As a task in SensEval-3, Automatic Labeling 
of Semantic Roles is to identify frame 
elements within a sentence and tag them with 
appropriate semantic roles given a sentence, a 
target word and its frame.  We apply 
Maximum Entropy classification with feature 
sets of syntactic patterns from parse trees and 
officially attain 80.2% precision and 65.4% 
recall.  When the frame element boundaries 
are given, the system performs 86.7% 
precision and 85.8% recall. 
1 Introduction 
The Automatic Labeling of Semantic Roles track 
in SensEval-3 focuses on identifying frame 
elements in sentences and tagging them with their 
appropriate semantic roles based on FrameNet1. 
For this task, we extend our previous work 
(Fleischman et el., 2003) by adding a sentence 
segmentation step and by adopting a few additional 
feature vectors for Maximum Entropy Model.  
Following the task definition, we assume the frame 
and the lexical unit of target word are known 
although we have assumed only the target word is 
known in the previous work. 
2 Model 
We separate the problem of FrameNet tagging 
into three subsequent processes: 1) sentence 
segmentation 2) frame element identification, and 
3) semantic role tagging.  We assume the frame 
element (FE) boundaries match the parse 
constituents, so we segment a sentence based on 
parse constituents.  We consider steps 2) and 3) as 
classification problems.  In frame element 
identification, we use a binary classifier to 
determine if each parse constituent is a FE or not, 
while, in semantic role tagging, we classify each 
                                                     
1 http://www.icsi.berkeley.edu/~framenet 
identified FE into its appropriate semantic role.2  
Figure 1 shows the sequence of steps. 
He fastened the panel from an old radio to the headboard wi th 
sticky tape and tied the driving wheel to Pete 's cardboard box 
wi th st ring
(He) (fastened the panel from an old radio to the headboard 
wi th sticky tape) (and) (t ied) (the driving wheel) ( to Pete 's 
cardboard box) (wi th s tring)
(He) (the driving wheel) (to Pete 's cardboard box) (wi th  string)
Agen t I tem Goal Connecto r
Input sentence
1) Sentence Segmentation: choose the highest 
consti tuen ts while separating targe t word 
2) Frame Element Identification: apply ME 
classification to classify each segment in t o classes of 
FE (frame element), T (target), NO (none) then ext ract 
iden ti fied f rame elements
3) Semantic Role Tagging: apply ME classification to 
classify each FE Into classes of various semantic roles
Output role: Agent (He), I tem ( the driving wheel), 
Goal ( to Pete?s cardboard box), Connector (wi th string)
Fig. 1. The sequence of steps on a sample sentence 
having a target word ?tied?. 
We train the ME models using the GIS 
algorithm (Darroch and Ratcliff, 1972) as 
implemented in the YASMET ME package (Och, 
2002).  We use the YASMET MEtagger (Bender et 
al. 2003) to perform the Viterbi search for 
choosing the most probable tag sequence for a 
sentence using the probabilities computed during 
training. Feature weights are smoothed using 
Gaussian priors with mean 0 (Chen and Rosenfeld, 
1999). 
2.1 Sentence Segmentation 
We segment a sentence into a sequence of non-
overlapping constituents instead of all individual 
constituents. There are a number of advantages to 
applying sentence segmentation before FE 
                                                     
2 We are currently ignoring null instantiations.  
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
boundary identification.  First, it allows us to 
utilize sentence-wide features for FE identification.  
The sentence-wide features, containing dependent 
information between frame element such as the 
previously identified class or the syntactic pattern, 
have previously been shown to be powerful 
features for role classification (Fleischman et al, 
2003).  Further, it allows us to reduce the number 
of candidate constituents for FE, which reduces the 
convergence time in training.  
The constituents are derived from a syntactic 
parse tree3.  Although we need to consider all 
combinations of various level constituents in a 
parse tree, we know the given target word should 
be a separate segment because a target word is not 
a part of other FEs.4  Since most frame elements 
tend to be in higher levels of the parse tree, we 
decide to use the highest constituents (the parse 
constituents having the maximum number of 
words) while separating the target word.  Figure 2 
shows an example of the segmentation for an 
actual sentence in FrameNet with the target word 
?tied?. 
  
He tied the to Pete box
PRP VBD DT TO NP NN
VP
VP
VP
NP
NP
PP
NP
S
?s
CC
and
PP
fastened the panel
from an old radio  
to the headboard  
with sticky tape
?.
VBG
driving wheel
NN
POSNNP
NN
cardboard
IN
with
NP
NN
string
Fig. 2. A sample sentence segmentation: ?tied? is 
the target predicate, and the shaded constituent 
represents each segment. 
However, this segmentation reduces the FE 
coverage of constituents (the number of 
constituents matching frame elements).  In Table 1, 
?individual constituents? means a list of all 
constituents, and ?Sentence segmentation? means a 
sequence of non-overlapping constituents that are 
taken in our work.  We can regard 85.8% as the 
accuracy of the parser. 
                                                     
3 We use Charniak parser :  
   http://www.cs.brown.edu/people/ec/#software 
4 Although 17% of constituents are both a target and a 
frame element, there is no case that a target is a part of a 
frame element. 
Method Number of constituents 
FE coverage 
(%) 
Individual 
constituents  342,245 85.8 
Sentence 
segmentation 66,401 79.5 
Table 1.  FE coverage for the test set. 
2.2 Frame Element Identification 
Frame element identification is executed for 
segments to classify into the classes on FE, Target, 
or None.  When a constituent is both a target and a 
frame element, we set it as a frame element when 
training because we are interested in identifying 
frame elements not a target. 
The initial features are adopted from (Gildea and 
Juraksky 2002) and (Fleischman, Kwon, and Hovy 
2003), and a few additional features are also used.  
The features are: 
? Target predicate (target): The target is the 
principal lexical item in a sentence.  
? Target lexical name (lexunit): The formal 
lexical name of target predicate is the string of 
the original form of target word and 
grammatical type.  For example, when the 
target is ?tied?, the lexical name is ?tie.v?.   
? Target type (ltype): The target type is a part 
of lexunit representing verb, noun, or adjective. 
(e.g. ?v? for a lexunit ?tie.v?) 
? Frame name (frame): The semantic frame is 
defined in FrameNet with corresponding target. 
? Constituent path (path): From the syntactic 
parse tree of a sentence, we extract the path 
from each constituent to the target predicate.   
The path is represented by the nodes through 
which one passes while traveling up the tree 
from the constituent and then down through 
the governing category to the target word.  For 
example, ?the driving wheel? in the sentence 
of Figure 2 has the path, NP?VP?VBD. 
? Partial path (ppath): The partial path is a 
variation of path, and it produces the same path 
as above if the constituent is under the same 
?S? as target word, if not, it gives ?nopath?.  
? Syntactic Head (head): The syntactic head of 
each constituent is obtained based on Michael 
Collins?s heuristic method5.  When the head is 
a proper noun, ?proper-noun? substitutes for 
the real head.  The decision as to whether the 
head is a proper noun is made based on the 
part of speech tags used in the parse tree.  
                                                     
5 http://www.ai.mit.edu/people/mcollins/papers/heads 
? Phrase Type (pt): The syntactic phrase type 
(e.g., NP, PP) of each constituent is also 
extracted from the parse tree.  It is not the 
same as the manually defined PT in FrameNet. 
? Logical Function (lf): The logical functions of 
constituents in a sentence are simplified into 
three values: external argument, object 
argument, other. When the constituent?s 
phrase type is NP, we follow the links in the 
parse tree from the constituent to the ancestors 
until we meet either S or VP.  If the S is found 
first, we assign external argument to the 
constituent, and if the VP is found, we assign 
object argument. Otherwise, other is assigned.   
? Position (pos): The position indicates whether 
a constituent appears before or after the target 
predicate. 
? Voice (voice): The voice of a sentence (active, 
passive) is determined by a simple regular 
expression over the surface form of the 
sentence. 
? Previous class (c_n): The class information of 
the nth-previous constituent (Target, FE, or 
None) is used to exploit the dependency 
between constituents.  During training, this 
information is provided by simply looking at 
the true class of the constituent occurring n-
positions before the target element.  During 
testing, the hypothesized classes are used for 
Viterbi search. 
Feature Set Example Functions 
f(c, lexunit) f(c, tie.v) = 1 
f(c, pt, pos, voice) f(c, NP,after,active) = 1 
f(c, pt, lf) f(c, ADVP,obj) = 1 
f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1 
f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1 
f(c, head) f(c, wheel) = 1 
f(c, head, frame) f(c, wheel, Attaching) = 1 
f(c, path) f(c, NP?VP?VBD) = 1 
f(c, path_-1) f(c, VBD_-1) = 1 
f(c, path_1) f(c, PP?VP?VBD_1) = 1 
f(c, target) f(c, tied) = 1 
f(c, ppath) f(c, NP?VP?VBD) = 1 
f(c, ppath, pos) f(c,NP?VP?VBD, after) = 1 
f(c, ppath_-1, pos_-1) f(c, VBD_-1,after) = 1 
f(c ,ltype,  ppath) f(c, v, NP?VP?VBD) = 1 
f(c ,ltype,  path) f(c, v, NP?VP?VBD) = 1 
f(c ,ltype,  path_-1) f(c, v,VBD_-1) = 1 
f(c  frame) f(c, Attaching) = 1 
f(c, frame, c_-1) f(c, Attaching, T_-1) = 1 
f(c,frame, c_-2,c_-1) f(c, Attaching,NO_-2,T_-1)=1 
Table 2. Feature sets used in ME frame element 
identification.  Example functions of ?the driving 
wheel? from the sample sentence in Fig.2. 
The combinations of these features that are used 
in the ME model are shown in Table 2.  These 
feature sets contain the previous or next 
constituent?s features, for example, pt_-1 
represents the previous constituent?s phrase type 
and lf_1 represents the next constituent?s logical 
function.  
2.3 Semantic Role Classification 
Semantic role classification is executed only for 
the constituents that are classified into FEs in the 
previous FE identification phase by employing 
Maximum Entropy classification.   
In addition to the features in Section 2.2, two 
more features are applied.   
? Order (order): The relative position of a 
frame element in a sentence is given.  For 
example, the sentence from Figure 2 has four 
frame elements, where the element ?He? has 
order 0, while ?with string? has order 3. 
? Syntactic pattern (pat): The sentence level 
syntactic pattern is generated from the parse 
tree by considering the phrase type and logical 
functions of each frame element in the 
sentence.  In the example sentence in Figure 2, 
?He? is an external argument Noun Phrase, 
?tied? is a target predicate, and ?the driving 
wheel? is an object argument Noun Phrase.  
Thus, the syntactic pattern associated with the 
sentence is [NP-ext, target, NP-obj, PP-other, 
PP-other]. 
Table 3 shows the list of feature sets used for the 
ME role classification. 
Feature Set 
f(r, lexunit) f(r, pt, lf) 
f(r, target) f(r, pt_-1, lf_-1) 
f(r, pt, pos, voice) f(r, pt_1, lf_1) 
f(r, head) f(r, order, syn) 
f(r, head, lexunit) f(r, lexunit, order, syn) 
f(r, head, frame) f(r, pt, pos, voice, lexunit) 
f(r, frame, r_-1) f(r, frame, r_-2,r_-1) 
f(r, frame,r_-3, r_-2,r_-1) 
Table 3. Feature sets used in role classification. 
3 Results 
SensEval-3 provides the following data set: 
training set (24,558 sentences/ 51,323 frame 
elements/ 40 frames), and test set (8,002 sentences/ 
16,279 frame elements/ 40 frames). We submit two 
sets to SensEval-3, one (test A) is the output of all 
above processes (identifying frame elements and 
tagging them given a sentence), and the other (test 
B) is to tag semantic roles given frame elements.  
For test B, we attempt the role classification for all 
frame elements including frame elements not 
matching the parse tree constituents.  Although 
there are frame elements that have two different 
semantic roles, we ignore those cases and assign 
one semantic role per frame element.  This 
explains why test B shows 99% attempted frame 
elements.  The attempted number for test A is the 
number of frame elements identified by our system.  
Table 4 shows the official scores for these tests. 
Test Prec. Overlap Recall Attempted 
Test A 80.2 78.4 65.4 81.5 
Test B 86.7 86.6 85.8 99.0 
Table 4. Final SensEval-3 scores for the test set. 
In the official evaluation, the precision and recall 
are calculated by counting correct roles that 
overlap even in only one word with the reference 
set.  Overlap score shows how much of an actual 
FE is identified as an FE not penalizing wrongly 
identified part.  Since this evaluation is so lenient, 
we perform another evaluation to check exact 
matches. 
FE boundary 
Identification 
FE boundary 
Identification & 
Role labeling Method 
Prec Rec Prec Rec 
Test A 80.3 66.1 71.1 58.5 
Test B 100.0 99.0 86.7 85.8 
Table 5.  Exact match scores for the test set. 
4 Discussion and Conclusion 
Due to time limit, we?ve not done many 
experiments with different feature sets or 
thresholds in ME classification.  We expect that 
recall will increase with lower thresholds 
especially in lenient evaluation and the final 
performance will increase by optimizing 
parameters. 
References  
O. Bender, K. Macherey, F.J. Och, and H. Ney. 
2003.      Comparison of Alignment Templates 
and Maximum Entropy Models for Natural 
Language Processing. Proc. of EACL-2003. 
Budapest, Hungary. 
A. Berger, S. Della Pietra and V. Della Pietra, 
1996. A Maximum Entropy Approach to Natural 
Language Proc. of Computational Linguistics, 
vol. 22, no. 1. 
E. Charniak. 2000. A Maximum-Entropy-Inspired 
Parser. Proc. of NAACL-2000, Seattle, USA. 
S.F. Chen and R. Rosenfeld. 1999. A Gaussian 
Prior for Smoothing Maximum Entropy Models. 
Technical Report CMUCS-99-108, Carnegie 
Mellon University. 
J. N. Darroch and D. Ratcliff. 1972. Generalized 
Iterative Scaling for Log-Linear Models.  Annals 
of Mathematical Statistics, 43:1470-1480. 
C.Fillmore 1976. Frame Semantics and the Nature 
of Language.  Annals of the New York Academy 
of Science Conference on the Origin and 
Development of Language and Speech, Volume 
280 (pp. 20-32). 
M. Fleischman, N. Kwon, and E. Hovy. 2003. 
Maximum Entropy Models for FrameNet 
Classification. Proc. of Empirical Methods in 
Natural Language Processing conference 
(EMNLP), 2003. Sapporo, Japan.  
D. Gildea and D. Jurafsky. 2002. Automatic 
Labeling of Semantic Roles. Computational 
Linguistics, 28(3) 245-288 14. 
F.J. Och. 2002. Yet Another Maxent Toolkit: 
YASMET www-i6.informatik.rwth-aachen.de/ 
Colleagues/och/. 
C. Thompson, R. Levy, and C. Manning. 2003. A 
Generative Model for FrameNet Semantic Role 
Labeling. Proc. of the Fourteenth European 
Conference on Machine Learning, 2003. Croatia. 
Template-Filtered Headline Summarization 
 
Liang Zhou and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, hovy}@isi.edu 
 
 
Abstract 
Headline summarization is a difficult task be-
cause it requires maximizing text content in 
short summary length while maintaining gram-
maticality. This paper describes our first attempt 
toward solving this problem with a system that 
generates key headline clusters and fine-tunes 
them using templates. 
1    Introduction 
Producing headline-length summaries is a chal-
lenging summarization problem. Every word be-
comes important. But the need for 
grammaticality?or at least intelligibility? some-
times requires the inclusion of non-content words. 
Forgoing grammaticality, one might compose a 
?headline? summary by simply listing the most 
important noun phrases one after another. At the 
other extreme, one might pick just one fairly in-
dicative sentence of appropriate length, ignoring 
all other material. Ideally, we want to find a bal-
ance between including raw information and sup-
porting intelligibility.  
We experimented with methods that integrate 
content-based and form-based criteria. The proc-
ess consists two phases. The keyword-clustering 
component finds headline phrases in the begin-
ning of the text using a list of globally selected 
keywords. The template filter then uses a collec-
tion of pre-specified headline templates and sub-
sequently populates them with headline phrases to 
produce the resulting headline.  
In this paper, we describe in Section 2 previous 
work. Section 3 describes a study on the use of 
headline templates. A discussion on the process of 
selecting and expanding key headline phrases is in 
Section 4. And Section 5 goes back to the idea of 
templates but with the help of headline phrases. 
Future work is discussed in Section 6.  
 
2    Related Work 
     
Several previous systems were developed to ad-
dress the need for headline-style summaries.  
A lossy summarizer that ?translates? news sto-
ries into target summaries using the ?IBM-style? 
statistical machine translation (MT) model was 
shown in (Banko, et al, 2000). Conditional prob-
abilities for a limited vocabulary and bigram tran-
sition probabilities as headline syntax 
approximation were incorporated into the transla-
tion model. It was shown to have worked surpris-
ingly well with a stand-alone evaluation of 
quantitative analysis on content coverage. The use 
of a noisy-channel model and a Viterbi search was 
shown in another MT-inspired headline summari-
zation system (Zajic, et al, 2002). The method 
was automatically evaluated by BiLingual Evalua-
tion Understudy (Bleu) (Papineni, et al, 2001) 
and scored 0.1886 with its limited length model.    
A nonstatistical system, coupled with linguisti-
cally motivated heuristics, using a parse-and-trim 
approach based on parse trees was reported in 
(Dorr, et al, 2003). It achieved 0.1341 on Bleu 
with an average of 8.5 words.  
Even though human evaluations were con-
ducted in the past, we still do not have sufficient 
material to perform a comprehensive comparative 
evaluation on a large enough scale to claim that 
one method is superior to others. 
 
3    First Look at the Headline Templates 
 
It is difficult to formulate a rule set that defines 
how headlines are written. However, we may dis-
cover how headlines are related to the templates 
derived from them using a training set of 60933 
(headline, text) pairs.  
 
3.1 Template Creation 
 
We view each headline in our training corpus as a 
potential template. For any new text(s), if we can 
select an appropriate template from the set and fill 
it with content words, then we will have a well-
structured headline. An abstract representation of 
the templates suitable for matching against new 
material is required. In our current work, we build  
templates at the part-of-speech (POS) level.  
 
3.2 Sequential Recognition of Templates 
 
We tested how well headline templates overlap 
with the opening sentences of texts by matching 
POS tags sequentially. The second column of Ta-
ble 1 shows the percentage of files whose POS-
level headline words appeared sequentially within 
the context described in the first column.  
 
Text Size Files from corpus (%) 
First sentence 20.01 
First two sentences 32.41 
First three sentences 41.90 
All sentences 75.55 
Table 1: Study on sequential template matching 
of a headline against its text, on training data  
     
3.3 Filling Templates with Key Words  
 
Filling POS templates sequentially using tagging 
information alone is obviously not the most ap-
propriate way to demonstrate the concept of head-
line summarization using template abstraction, 
since it completely ignores the semantic informa-
tion carried by words themselves.  
Therefore, using the same set of POS headline 
templates, we modified the filling procedure. 
Given a new text, each word (not a stop word) is 
categorized by its POS tag and ranked within each 
POS category according to its tf.idf weight. A 
word with the highest tf.dif weight from that POS 
category is chosen to fill each placeholder in a 
template. If the same tag appears more than once 
in the template, a subsequent placeholder is filled 
with a word whose weight is the next highest from 
the same tag category. The score for each filled 
template is calculated as follows: 
score _ t(i) =
W j
j =1
N
?
| desired _ len - template _ len |+1
 
where score_t(i) denotes the final score assigned 
to template i of up to N placeholders and Wj is the 
tf.idf weight of the word assigned to a placeholder 
in the template. This scoring mechanism prefers 
templates with the most desirable length. The 
highest scoring template-filled headline is chosen 
as the result.  
 
4    Key Phrase Selection 
 
The headlines generated in Section 3 are gram-
matical (by virtue of the templates) and reflect 
some content (by virtue of the tf.idf scores). But 
there is no guarantee of semantic accuracy! This 
led us to the search of key phrases as the candi-
dates for filling headline templates. Headline 
phrases should be expanded from single seed 
words that are important and uniquely reflect the 
contents of the text itself. To select the best seed 
words for key phrase expansion, we studied sev-
eral keyword selection models, described below.  
  
4. 1 Model Selection 
 
Bag-of-Words Models  
 
1) Sentence Position Model: Sentence position 
information has long proven useful in identifying 
topics of texts (Edmundson, 1969). We believe 
this idea also applies to the selection of headline 
words. Given a sentence with its position in text, 
what is the likelihood that it would contain the 
first appearance of a headline word: 
 
Count _ Posi = P(Hk |W j)
j =1
N
?
k=1
M
?  
P(Posi) =
Count _ Posi
Count _ PosQ
i =1
Q
?
 
Over all M texts in the collection and over all 
words from the corresponding M headlines (each 
has up to N words), Count_Pos records the num-
ber of times that sentence position i has the first 
appearance of any headline word Wj. P(Hk | Wj) is 
a binary feature. This is computed for all sentence 
positions from 1 to Q. Resulting P(Posi) is a table 
on the tendency of each sentence position contain-
ing one or more headlines words (without indicat-
ing exact words).  
2) Headline Word Position Model: For each 
headline word Wh , it would most likely first ap-
pear at sentence position Posi: 
 
P(Posi |Wh) =
Count(Posi,Wh )
Count(PosQ,Wh )
i=1
Q
?
 
The difference between models 1 and 2 is that 
for the sentence position model, statistics were 
collected for each sentence position i; for the 
headline word position model, information was 
collected for each headline word Wh.  
3) Text Model: This model captures the correla-
tion between words in text and words in headlines 
(Lin and Hauptmann, 2001):  
 
P(Hw |Tw) =
(doc _ tf (w, j) ? title _ tf (w, j))
j =1
M
?
doc _ tf (w, j)
j=1
M
?
 
doc_tf(w,j) denotes the term frequency of word w 
in the j th document of all M documents in the col-
lection. title_tf(w,j) is the term frequency of word 
w in the j th title. Hw and Tw are words that appear in 
both the headline and the text body. For each in-
stance of Hw and Tw pair, Hw = Tw.  
4) Unigram Headline Model: Unigram probabili-
ties on the headline words from the training set. 
5) Bigram Headline Model: Bigram probabilities 
on the headline words from the training set.  
 
Choice on Model Combinations  
 
Having these five models, we needed to determine 
which model or model combination is best suited 
for headline word selection. The blind data was 
the DUC2001 test set of 108 texts. The reference 
headlines are the original headlines with a total of 
808 words (not including stop words). The evalua-
tion was based on the cumulative unigram overlap 
between the n top-scoring words and the reference 
headlines. The models are numbered as in Section 
4.1. Table 2 shows the effectiveness of each 
model/model combination on the top 10, 20, 30, 
40, and 50 scoring words.  
    Clearly, for all lengths greater than 10,  sen-
tence position (model 1) plays the most important 
role in selecting headline words. Selecting the top 
50 words solely based on position information 
means that sentences in the beginning of a text are 
the most informative. However, when we are wor- 
 
Model(s) 10w 20w 30w 40w 50w 
1 2 3 4 5  79 118 147 189 216 
2 3 4 5  74 110 145 178 206 
1 3 4 5  74 116 146 176 208 
1 2 4 5  63 99 144 176 202 
1 2 3 5  87 122 155 187 223 
1 2 3 4  96 149 187 214 230 
3 4 5  61 103 134 170 199 
2 4 5  54 94 137 168 192 
2 3 5  82 117 148 183 212 
2 3 4  67 119 167 192 217 
1 4 5  55  101 126 149 193 
1 3 5  84 113 144 181 216 
1 3 4  97 144 186 212 234 
1 2 5  70 102 146 179 208 
1 4 5  55 101 126 149 193 
1 2 3  131 181 205 230 250 
4 5  46 84 117 140 182 
3 5 72 107 134 166 204 
3 4 58 103 136 165 196 
2 5 62 96 135 172 204 
2 4 38 80 114 144 179 
2 3  100 150 187 215 235 
1 5 72 98 139 158 203 
1 4 69 111 144 169 193 
1 3 154 204 244 271 292 
1 2 74 138 174 199 232 
5 58 84 114 140 171 
4 35 60 87 111 136 
3 86 137 169 208 227 
2 45 94 135 163 197 
1 113 234 275 298 310 
Table 2: Results on model combinations  
 
king with a more restricted length requirement, 
text model (model 3) adds advantage to the posi-
tion model (highlighted, 7th from the bottom of 
Table 2). As a result, the following combination 
of sentence position and text model was used:  
 
P(H |Wi) = P(H | Posi )? P(Hw i |Twi ) 
4.2    Phrase Candidates to Fill Templates 
Section 4.1 explained how we select headline-
worthy words. We now need to expand them into 
phrases as candidates for filling templates. As il-
lustrated in Table 2 and stated in (Zajic et al, 
2002), headlines from newspaper texts mostly use 
words from the beginning of the text. Therefore, 
we search for n-gram phrases comprising key-
words in the first part of the story. Using the 
model combination selected in Section 4.1, 10 
top-scoring words over the whole story are se-
lected and highlighted in the first 50 words of the 
text. The system should have the ability of pulling 
out the largest window of top-scoring words to 
form the headline. To help achieve grammatical-
ity, we produced bigrams surrounding each head-
line-worthy word (underlined), as shown in Figure 
1. From connecting overlapping bigrams in  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
sequence, one sees interpretable clusters of words 
forming. Multiple headline phrases are considered 
as candidates for template filling. Using a set of 
hand-written rules, dangling words were removed 
from the beginning and end of each headline 
phrase.   
 
 
5   Filling Templates with Phrases 
 
5.1    Method 
 
Key phrase clustering preserves text content, but 
lacks the complete and correct representation for 
structuring phrases. The phrases need to go 
through a grammar filter/reconstruction stage to 
gain grammaticality.  
A set of headline-worthy phrases with their cor-
responding POS tags is presented to the template 
filter. All templates in the collection are matched 
against each candidate headline phrase. Strict tag 
matching produces a small number of matching 
templates. To circumvent this problem, a more 
general tag-matching criterion, where tags belong-
ing to the same part-of-speech category can be 
matched interchangeably, was used.  
Headline phrases tend to be longer than most of 
the templates in the collection. This results in only 
partial matches between the phrases and the tem-
plates. A score of fullness on the phrase-template 
match is computed for each candidate template fti: 
 
fti =
length (t i) + matched _ length(hi )
length(t i) + length(h i)
 
 
ti is a candidate template and hi is a headline 
phrase. The top-scoring template is used to filter 
each headline phrase in composing the final multi-
phrase headline. Table 3 shows a random selec-
tion of the results produced by the system.  
 
Generated Headlines 
First Palestinian airlines flight depart Gaza?s airport  
Jerusalem/ suicide bombers targeted market Friday setting blasts 
U.S. Senate outcome apparently rests small undecided voters.  
Brussels April 30 European parliament approved Thursday join 
currency mechanism 
Hong Kong strong winds Sunday killing 150 / Philippines leav-
ing hundreds thousands homeless 
Chileans wish forget years politics repression 
Table 3: System-generated headlines. A headline 
can be concatenated from several phrases, sepa-
rated by ?/?s  
 
5.2   Evaluation 
 
Ideally, the evaluation should show the system?s 
performance on both content selection and gram-
maticality.  However, it is hard to measure the 
level of grammaticality achieved by a system 
computationally.  Similar to (Banko, et al, 2000), 
we restricted the evaluation to a quantitative 
analysis on content only.  
Our system was evaluated on previously unseen 
DUC2003 test data of 615 files. For each file, 
headlines generated at various lengths were com-
pared against i) the original headline, and ii) head-
lines written by four DUC2003 human assessors. 
The performance metric was to count term over-
laps between the generated headlines and the test 
standards.  
Table 4 shows the human agreement and the 
performance of the system comparing with the 
two test standards. P and R are the precision and 
recall scores. 
 
The system-generated headlines were also evalu-
ated using the automatic summarization evalua-
tion tool ROUGE (Recall-Oriented Understudy 
for Gisting Evaluation) (Lin and Hovy,  
Figure 1: Surrounding bigrams for top-scoring 
words 
Allegations  of police  racism  and   brutality  
 
have   shaken this city that for decades has  
 
prided itself on a progressive attitude toward  
 
civil  rights    and a reputation for racial  
 
harmony.  The death of  two blacks   at   a  
 
drug   raid   that went awry, followed 10 days  
 
later by a scuffle between police and? 
 
 Assessors? Generated 
P R Length  
(words) 
P R 
9 0.1167 0.1566 
12 0.1073 0.2092 
Original 
 
0.3429 
 
0.2336 
13 0.1075 0.2298 
9 0.1482 0.1351 
12 0.1365 0.1811 
Assessors?  
0.2186 
 
0.2186 
13 0.1368 0.1992 
Table 4: Results evaluated using unigram over-
lap  
  
  
 
 
2003). The ROUGE score is a measure of n-gram 
recall between candidate headlines and a set of 
reference headlines. Its simplicity and reliability 
are gaining audience and becoming a standard for 
performing automatic comparative summarization 
evaluation. Table 5 shows the ROUGE perform-
ance results for generated headlines with length 12 
against headlines written by human assessors.  
 
6    Conclusion and Future Work 
 
Generating summaries with headline-length re-
striction is hard because of the difficulty of 
squeezing a full text into a few words in a read-
able fashion. In practice, it often happens in order 
to achieve the optimal informativeness, grammati-
cal structure is overlooked, and vice versa. In this 
paper, we have described a system that was de-
signed to use two methods, individually had ex-
hibited exactly one of the two types of unbalances, 
and integrated them to yield content and gram-
maticality. 
Structural abstraction at the POS level is shown 
to be helpful in our current experiment. However, 
part-of-speech tags do not generalize well and fail 
to model issues like subcategorization and other 
lexical semantic effects. This problem was seen 
from the fact that there are half as many templates 
as the original headlines. A more refined pattern 
language, for example taking into account named 
entity types and verb clusters, will further improve 
performance. We intend to incorporate additional 
natural language processing tools to create a more 
sophisticated and richer hierarchical structure for 
headline summarization. 
References 
Michele Banko, Vibhu Mittal, and Michael Wit-
brock. 2000. Headline generation based on sta-
tistical translation. In ACL-2000, pp. 318-325.  
Bonnie Dorr, David Zajic, and Richard Schwartz. 
2003. Hedge trimmer: a parse-and-trim ap-
proach to headline generation. In Proceedings 
of Workshop on Automatic  Summarization, 
2003.  
H. P. Edmundson. 1969. New methods in auto-
matic extracting. Journal of the ACM , 
16(2):264?285. 
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram 
co-occurrence statistics. In HLT-NAACL 2003, 
pp.150?157.  
Rong Lin and Alexander Hauptmann. 2001. Head-
line generation using a training corpus. In 
CICLING 2000.  
Kishore Papineni, Salim Roukos, Todd Ward, and 
Wei-Jin Zhu. 2001. IBM research report Bleu: a 
method for automatic evaluation of machine 
translation. In IBM Research Division Techni-
cal Report, RC22176 (W0109-22). 
David Zajic, Bonnie Dorr, and Richard Schwartz. 
2002. Automatic headline generation for news-
paper stories. In Proceedings of the ACL-2002 
Workshop on Text Summarization. 
 Human Generated 
Unigrams 0.292 0.169 
Bigrams 0.084 0.042 
Trigrams 0.030 0.010 
4-grams 0.012 0.002 
Table 5: Performance on ROUGE  
Interlingual Annotation of Multilingual Text Corpora 
 
Stephen Helmreich 
David Farwell 
Computing Research Laboratory 
New Mexico State University 
david@crl.nmsu.edu
shelmrei@crl.nmsu.edu
Florence Reeder 
Keith Miller 
Information Discovery & Understanding 
MITRE Corporation 
freeder@mitre.org
keith@mitre.org   
Bonnie Dorr 
Nizar Habash 
Institute for Advanced Computer Studies 
University of Maryland 
bonnie@umiacs.umd.edu
habash@umiacs.umd.edu
 
Eduard Hovy 
Information Sciences Institute 
University of Southern California 
hovy@isi.edu 
Lori Levin 
Teruko Mitamura 
Language Technologies Institute 
Carnegie Mellon University 
lsl@cs.cmu.edu
teruko@cs.cmu.edu 
Owen Rambow 
Advaith Siddharthan 
Department of Computer Science 
Columbia University 
rambow@cs.columbia.edu
as372@cs.columbia.edu  
 
 
Abstract 
This paper describes a multi-site project to 
annotate six sizable bilingual parallel corpora 
for interlingual content. After presenting the 
background and objectives of the effort, we 
describe the data set that is being annotated, 
the interlingua representation language used, 
an interface environment that supports the an-
notation task and the annotation process itself. 
We will then present a preliminary version of 
our evaluation methodology and conclude 
with a summary of the current status of the 
project along with a number of issues which 
have arisen.  
1 Introduction 
This paper describes a multi-site National Science 
Foundation project focusing on the annotation of six 
sizable bilingual parallel corpora for interlingual content 
with the goal of providing a significant data set for im-
proving knowledge-based approaches to machine trans-
lation (MT) and a range of other Natural Language 
Processing (NLP) applications. The project participants 
include the Computing Research Laboratory at NMSU, 
the Language Technologies Institute at CMU, the In-
formation Science Institute at USC, UMIACS at the 
University of Maryland, the MITRE Corporation and 
Columbia University. In the remainder of the paper, we 
first present the background and objectives of the pro-
ject. We then describe the data set that is being anno-
tated, the interlingual representation language being 
used, an interface environment that is designed to sup-
port the annotation task, and the process of annotation 
itself. We will then outline a preliminary version of our 
evaluation methodology and conclude with a summary 
of the current status of the project along with a set of 
issues that have arisen since the project began.  
2 Project Goals and Expected Outcomes 
The central goals of the project are: 
? to produce a practical, commonly-shared system 
for representing the information conveyed by a 
text, or ?interlingua?, 
? to develop a methodology for accurately and 
consistently assigning such representations to 
texts across languages and across annotators, 
? to annotate a sizable multilingual parallel corpus 
of source language texts and translations for IL 
content. 
This corpus is expected to serve as a basis for improving 
meaning-based approaches to MT and a range of other 
natural language technologies.  The tools and annotation 
standards will serve to facilitate more rapid annotation 
of texts in the future. 
3 Corpus 
The target data set is modeled on and an extension of 
the DARPA MT Evaluation data set (White and 
O?Connell 1994) and includes data from the Linguistic 
Data Consortium (LDC) Multiple Translation Arabic, 
Part 1 (Walker et al, 2003). The data set consists of 6 
bilingual parallel corpora. Each corpus is made up of 
125 source language news articles along with three 
translations into English, each produced independently 
by different human translators. However, the source 
news articles for each individual language corpus are 
different from the source articles in the other language 
corpora.  Thus, the 6 corpora themselves are comparable 
to each other rather than parallel. The source languages 
are Japanese, Korean, Hindi, Arabic, French and Span-
ish.  Typically, each article is between 300 and 400 
words long (or the equivalent) and thus each corpus has 
between 150,00 and 200,000 words. Consequently, the 
size of the entire data set is around 1,000,000 words. 
Thus, for any given corpus, the annotation effort is 
to assign interlingual content to a set of 4 parallel texts, 
3 of which are in the same language, English, and all of 
which theoretically communicate the same information. 
The following is an example set from the Spanish cor-
pus: 
S: Atribuy? esto en gran parte a
una pol?tica que durante muchos a?os
tuvo un "sesgo concentrador" y repre-
sent? desventajas para las clases me-
nos favorecidas.
T1: He attributed this in great
part to a type of politics that
throughout many years possessed a
"concentrated bias" and represented
disadvantages for the less favored
classes.
T2: To a large extent, he attrib-
uted that fact to a policy which had
for many years had a "bias toward
concentration" and represented disad-
vantages for the less favored
classes.
T3: He attributed this in great
part to a policy that had a "centrist
slant" for many years and represented
disadvantages for the less-favored
classes.
 
The annotation process involves identifying the 
variations between the translations and then assessing 
whether these differences are significant. In this case, 
the translations are, for the most part, the same although 
there are a few interesting variations.  
For instance, where this appears as the translation 
of esto in the first and third translations, that fact 
appears in the second. The translator choice potentially 
represents an elaboration of the semantic content of the 
source expression and the question arises as to whether 
the annotation of the variation in expressions should be 
different or the same.  
More striking perhaps is the variation between 
concentrated bias, bias toward concen-
tration and centrist slant as the translation 
for sesgo concentrador. Here, the third transla-
tion offers a clear interpretation of the source text au-
thor?s intent. The first two attempt to carry over the 
vagueness of the source expression assuming that the 
target text reader will be able to figure it out. But even 
here, the two translators appear to differ as to what the 
source language text author?s intent actually was, the 
former referring to bias of a certain degree of strength 
and the second to a bias  in a certain direction. Seem-
ingly, then, the annotation of each of these expressions 
should differ. 
Furthermore, each source language has different 
methods of encoding meaning linguistically. The resul-
tant differing types of translation mismatch with English 
should provide insight into the appropriate structure and 
content for an interlingual representation. 
The point is that a multilingual parallel data set of 
source language texts and English translations offers a 
unique perspective and unique problem for annotating 
texts for meaning. 
4 Interlingua 
Due to the complexity of an interlingual annotation as 
indicated by the differences described in the previous 
section, the representation has developed through three 
levels and incorporates knowledge from sources such as 
the Omega ontology and theta grids.  Since this is an 
evolving standard, the three levels will be presented in 
order as building on one another. Then the additional 
data components will be described.  
4.1 Three Levels of Representation 
We now describe three levels of representation, referred 
to as IL0, IL1 and IL2. The aim is to perform the annota-
tion process incrementally, with each level of represen-
tation incorporating additional semantic features and 
removing existing syntactic ones. IL2 is intended as the 
interlingua, that abstracts away from (most) syntactic 
idiosyncrasies of the source language. IL0 and IL1 are 
intermediate representations that are useful starting 
points for annotating at the next level. 
4.1.1 IL0 
IL0 is a deep syntactic dependency representation. It 
includes part-of-speech tags for words and a parse tree 
that makes explicit the syntactic predicate-argument 
structure of verbs. The parse tree is labeled with syntac-
tic categories such as Subject or Object , which refer to 
deep-syntactic grammatical function (normalized for 
voice alternations).  IL0 does not contain function words 
(determiners, auxiliaries, and the like): their contribu-
tion is represented as features.  Furthermore, semanti-
cally void punctuation has been removed.  While this 
representation is purely syntactic, many disambiguation 
decisions, relative clause and PP attachment for exam-
ple, have been made, and the presentation abstracts as 
much as possible from surface-syntactic phenomena.  
Thus, our IL0 is intermediate between the analytical and 
tectogrammatical levels of the Prague School (Haji? et 
al 2001). IL0 is constructed by hand-correcting the out-
put of a dependency parser (details in section 6) and is a 
useful starting point for semantic annotation at  IL1, 
since it allows annotators to see how textual units relate 
syntactically when making semantic judgments.  
4.1.2 IL1 
IL1 is an intermediate semantic representation. It asso-
ciates semantic concepts with lexical units like nouns, 
adjectives,  adverbs and verbs (details of the ontology in 
section 4.2). It also replaces the syntactic relations in 
IL0, like subject and object, with thematic roles, like 
agent, theme and goal (details in section 4.3). Thus, like 
PropBank (Kingsbury et al2002), IL1 neutralizes dif-
ferent alternations for argument realization.  However, 
IL1 is not an interlingua; it does not normalize over all 
linguistic realizations of the same semantics. In particu-
lar, it does not address how the meanings of individual 
lexical units combine to form the meaning of a phrase or 
clause. It also does not address idioms, metaphors and 
other non-literal uses of language.  Further, IL1 does not 
assign semantic features to prepositions; these continue 
to be encoded as syntactic heads of their phrases, al-
though these might have been annotated with thematic 
roles such as location or time. 
4.1.3 IL2 
IL2 is intended to be an interlingua, a representation of 
meaning that is reasonably independent of language. IL2 
is intended to capture similarities in meaning across 
languages and across different lexical/syntactic realiza-
tions within a language. For example, IL2 is expected to 
normalize over conversives (e.g. X bought a book from 
Y vs. Y sold a book to X)  (as does FrameNet (Baker et 
al 1998)) and non-literal language usage (e.g. X started 
its business vs. X opened its doors to customers).  The 
exact definition of IL2 will be the major research con-
tribution of this project. 
4.2 The Omega Ontology 
In progressing from IL0 to IL1, annotators have to se-
lect semantic terms (concepts) to represent the nouns, 
verbs, adjectives, and adverbs present in each sentence.  
These terms are represented in the 110,000-node ontol-
ogy Omega (Philpot et al, 2003), under construction at 
ISI.  Omega has been built semi-automatically from a 
variety of sources, including Princeton's WordNet (Fell-
baum, 1998), NMSU?s Mikrokosmos (Mahesh and Ni-
renburg, 1995), ISI's Upper Model (Bateman et al, 
1989) and ISI's SENSUS (Knight and Luk, 1994).  After 
the uppermost region of Omega was created by hand, 
these various resources? contents were incorporated and, 
to some extent, reconciled.  After that, several million 
instances of people, locations, and other facts were 
added (Fleischman et al, 2003).  The ontology, which 
has been used in several projects in recent years (Hovy 
et al, 2001), can be browsed using the DINO browser at 
http://blombos.isi.edu:8000/dino; this browser forms a 
part of the annotation environment.  Omega remains 
under continued development and extension.  
4.3 The Theta Grids 
Each verb in Omega is assigned one or more theta grids 
specifying the arguments associated with a verb and 
their theta roles (or thematic role).  Theta roles are ab-
stractions of deep semantic relations that generalize 
over verb classes.  They are by far the most common 
approach in the field to represent predicate-argument 
structure.  However, there are numerous variations with 
little agreement even on terminology (Fillmore, 1968; 
Stowell, 1981; Jackendoff, 1972; Levin and Rappaport-
Hovav, 1998). 
The theta grids used in our project were extracted 
from the Lexical Conceptual Structure Verb Database 
(LVD) (Dorr, 2001).  The WordNet senses assigned to 
each entry in the LVD were then used to link the theta 
grids to the verbs in the Omega ontology.  In addition to 
the theta roles, the theta grids specify the mapping be-
tween theta roles and their syntactic realization in argu-
ments, such as Subject, Object or Prepositional Phrase, 
and the Obligatory/Optional nature of the argument, 
thus facilitating IL1 annotation.  For example, one of the 
theta grids for the verb ?load? is listed in Table 1 (at the 
end of the paper). 
Although based on research in LCS-based MT 
(Dorr, 1993; Habash et al 2002), the set of theta roles 
used has been simplified for this project.  This list (see 
Table 2 at the end of the paper), was used in the Inter-
lingua Annotation Experiment 2002 (Habash and 
Dorr).1  
4.4 Incremental Annotation 
As described earlier, the development and annota-
tion of the interlingual notation is incremental in nature.  
This necessitates constraining the types and categories 
of attributes included in the annotation during the be-
ginning phases.  Other topics not addressed here, but 
considered for future work include time, aspect, loca-
tion, modality, type of reference, types of speech act, 
causality, etc.  
Thus, IL2 itself is not a final interlingual representa-
tion, but one step along the way. IL0 and IL1 are also 
intermediate representations, and as such are an occa-
sionally awkward mixture of syntactic and semantic 
information. The decisions as to what to annotate, what 
to normalize, what to represent as features at each level 
are semantically and syntactically principled, but also 
governed by expectations about reasonable annotator 
tasks. What is important is that at each stage of trans-
formation, no information is lost, and the original lan-
guage recoverable in principle from the representation. 
5 Annotation Tool 
We have assembled a suite of tools to be used in the 
annotation process.  Some of these tools are previously 
existing resources that were gathered for use in the pro-
ject, and others have been developed specifically with 
the annotation goals of this project in mind.  Since we 
are gathering our corpora from disparate sources, we 
need to standardize the text before presenting it to 
automated procedures.  For English, this involves sen-
tence boundary detection, but for other languages, it 
may involve segmentation, chunking of text, or other 
?text ecology? operations.  The text is then processed 
with a dependency parser, the output of which is viewed 
and corrected in TrED (Haji?, et al, 2001), a graphi-
cally-based tree editing program, written in Perl/Tk2.  
The revised deep dependency structure produced by this 
process is the IL0 representation for that sentence. 
In order to derive IL1 from the IL0 representation, 
annotators use Tiamat, a tool developed specifically for 
                                                           
1 Other contributors to this list are Dan Gildea and Karin 
Kipper Schuler. 
2 http://quest.ms.mff.cuni.cz/pdt/Tools/Tree_Editors/Tre
d/ 
this project.  This tool enables viewing of the IL0 tree 
with easy reference to all of the IL resources described 
in section 4 (the current IL representation, the ontology, 
and the theta grids).  This tool provides the ability to 
annotate text via simple point-and-click selections of 
words, concepts, and theta-roles.  The IL0 is displayed 
in the top left pane, ontological concepts and their asso-
ciated theta grids, if applicable, are located in the top 
right, and the sentence itself is located in the bottom 
right pane.  An annotator may select a lexical item (leaf 
node) to be annotated in the sentence view; this word is 
highlighted, and the relevant portion of the Omega on-
tology is displayed in the pane on the left.  In addition, 
if this word has dependents, they are automatically un-
derlined in red in the sentence view.  Annotators can 
view all information pertinent to the process of deciding 
on appropriate ontological concepts in this view.  Fol-
lowing the procedures described in section 6, selection 
of concepts, theta grids and roles appropriate to that 
lexical item can then be made in the appropriate panes. 
Evaluation of the annotators? output would be daunt-
ing based solely on a visual inspection of the annotated 
IL1 files.  Thus, a tool was also developed to compare 
the output and to generate the evaluation measures that 
are described in section 7.  The reports generated by the 
evaluation tool allow the researchers to look at both 
gross-level phenomena, such as inter-annotator agree-
ment, and at more detailed points of interest, such as 
lexical items on which agreement was particularly low, 
possibly indicating gaps or other inconsistencies in the 
ontology being used. 
6 Annotation Task 
To describe the annotation task, we first present the 
annotation process and tools used with it as well as the 
annotation manuals.  Finally, setup issues relating to 
negotiating multi-site annotations are discussed. 
6.1 Annotation process 
The annotation process was identical for each text. For 
the initial testing period, only English texts were anno-
tated, and the process described here is for English text. 
The process for non-English texts will be, mutatis mu-
tandis, the same. 
Each sentence of the text is parsed into a depend-
ency tree structure. For English texts, these trees were 
first provided by the Connexor parser at UMIACS 
(Tapanainen and Jarvinen, 1997), and then corrected by 
one of the team PIs. For the initial testing period, anno-
tators were not permitted to alter these structures. Al-
ready at this stage, some of the lexical items are 
replaced by features (e.g., tense), morphological forms 
are replaced by features on the citation form, and certain 
constructions are regularized (e.g., passive) and empty 
arguments inserted.  It is this dependency structure that 
is loaded into the annotation tool and which each anno-
tator then marks up. 
The annotator was instructed to annotate all nouns, 
verbs, adjectives, and adverbs. This involves annotating 
each word twice ? once with a concept from Wordnet 
SYNSET and once with a Mikrokosmos concept; these 
two units of information are merged, or at least inter-
twined in Omega. One of the goals and results of this 
annotation process will be a simultaneous coding of 
concepts in both ontologies, facilitating a closer union 
between them.  
In addition, users were instructed to provide a se-
mantic case role for each dependent of a verb. In many 
cases this was ?NONE? since adverbs and conjunctions 
were dependents of verbs in the dependency tree. LCS 
verbs were identified with Wordnet classes and the LCS 
case frames supplied where possible. The user, how-
ever, was often required to determine the set of roles or 
alter them to suit the text. In both cases, the revised or 
new set of case roles was noted and sent to a guru for 
evaluation and possible permanent inclusion. Thus the 
set of event concepts in the ontology supplied with roles 
will grow through the course of the project. 
6.2 The annotation manuals 
Markup instructions are contained in three manuals: a 
users guide for Tiamat (including procedural instruc-
tions), a definitional guide to semantic roles, and a 
manual for creating a dependency structure (IL0). To-
gether these manuals allow the annotator to (1) under-
stand the intention behind aspects of the dependency 
structure; (2) how to use Tiamat to mark up texts; and 
(3) how to determine appropriate semantic roles and 
ontological concepts. In choosing a set of appropriate 
ontological concepts, annotators were encouraged to 
look at the name of the concept and its definition, the 
name and definition of the parent node, example sen-
tences, lexical synonyms attached to the same node, and 
sub- and super-classes of the node. All these manuals 
are available on the IAMTC website3. 
6.3 The multi-site set up 
For the initial testing phase of the project, all annotators 
at all sites worked on the same texts. Two texts were 
provided by each site as were two translations of the 
same source language (non-English) text. To test for the 
effects of coding two texts that are semantically close, 
since they are both translations of the same source 
document, the order in which the texts were annotated 
differed from site to site, with half the sites marking one 
translation first, and the other half of the sites marking 
the second translation first. Another variant tested was 
                                                           
3 http://sparky.umiacs.umd.edu:8000/IAMTC/annotation
_manual.wiki?cmd=get&anchor=Annotation+Manual 
to interleave the two translations, so that two similar 
sentences were coded consecutively. 
During the later production phase, a more complex 
schedule will be followed, making sure that many texts 
are annotated by two annotators, often from different 
sites, and that regularly all annotators will mark the 
same text. This will help ensure continued inter-coder 
reliability. 
In the period leading up to the initial test phase, 
weekly conversations were held at each site by the an-
notators, going over the texts coded. This was followed 
by a weekly conference call among all the annotators. 
During the test phase, no discussion was permitted. 
One of the issues that arose in discussion was how 
certain constructions should be displayed and whether 
each word should have a separate node or whether cer-
tain words should be combined into a single node. In 
view of the fact that the goal was not to tag individual 
words, but entities and relations, in many cases words 
were combined into single nodes to facilitate this proc-
ess. For instance, verb-particle constructions were com-
bined into a single node. In a sentence like ?He threw it 
up?, ?throw? and ?up? were combined into a single 
node ?throw up? since one action is described by the 
combined words. Similarly, proper nouns, compound 
nouns and copular constructions required specialized 
handling.    In addition, issues arose about whether an-
notators should change dependency trees; and in in-
structing the annotators on how best to determine an 
appropriate ontology node.    
7 Evaluation 
The evaluation criteria and metrics continue to evolve 
and are in the early stages of formation and implementa-
tion.  Several possible courses for evaluating the annota-
tions and resulting structures exist.  In the first of these, 
the annotations are measured according to inter-
annotator agreement.  For this purpose, data is collected 
reflecting the annotations selected, the Omega nodes 
selected and the theta roles assigned.  Then, inter-coder 
agreement is measured by a straightforward match, with 
agreement calculated by a Kappa measure (Carletta, 
1993) and a Wood standard similarity (Habash and 
Dorr, 2002).  This is done for three agreement points:  
annotations, Omega selection and theta role selection.  
At this time, the Kappa statistic?s expected agreement is 
defined as 1/(N+1) where N is the number of choices at 
a given data point.  In the case of Omega nodes, this 
means the number of matched Omega nodes (by string 
match) plus one for the possibility of the annotator trav-
ersing up or down the hierarchy. Multiple measures are 
used because it is important to have a mechanism for 
evaluating inter-coder consistency in the use of the IL 
representation language which does not depend on the 
assumption that there is a single correct annotation of a 
given text.  The tools for evaluation have been modified 
from pervious use (Habash and Dorr, 2002). 
Second, the accuracy of the annotation is measured.  
Here accuracy is defined as correspondence to a prede-
fined baseline.  In the initial development phase, all 
sites annotated the same texts and many of the varia-
tions were discussed at that time, permitting the devel-
opment of a baseline annotation.  While not a useful 
long-term strategy, this produced a consensus baseline 
for the purpose of measuring the annotators? task and 
the solidity of the annotation standard.  
The final measurement technique derives from the 
ultimate goal of using the IL representation for MT, 
therefore, we are measuring the ability to generate accu-
rate surface texts from the IL representation as anno-
tated.  At this stage, we are using an available generator, 
Halogen (Knight and Langkilde, 2000).  A tool to con-
vert the representation to meet Halogen requirements is 
being built.  Following the conversion, surface forms 
will be generated and then compared with the originals 
through a variety of standard MT metrics (ISLE, 2003).   
8 Accomplishments and Issues 
In a short amount of time, we have identified languages 
and collected corpora with translations.  We have se-
lected representation elements, from parser outputs to 
ontologies, and have developed an understanding of 
how their component elements fit together.  A core 
markup vocabulary (e.g., entity-types, event-types and 
participant relations) was selected.  An initial version of 
the annotator?s toolkit (Tiamat) has been developed and 
has gone through alpha testing.  The multi-layered ap-
proach to annotation  decided upon reduces the burden 
on the annotators for any given text as annotations build 
upon one another.  In addition to developing individual 
tools, an infrastructure exists for carrying out a multi-
site annotation project.   
In the coming months we will be fleshing out the 
current procedures for evaluating the accuracy of an 
annotation and measuring inter-coder consistency.  
From this, a multi-site evaluation will be produced and  
results reported.  Regression testing, from the interme-
diate stages and representations will be able to be car-
ried out.  Finally, a growing corpus of annotated texts 
will become available.   
In addition to the issues discussed throughout the 
paper, a few others have not yet been identified.  From a 
content standpoint, looking at IL systems for time and 
location should utilize work in personal name, temporal 
and spatial annotation (e.g., Ferro et al, 2001).  Also, an 
ideal IL representation would also account for causality, 
co-reference, aspectual content, modality, speech acts, 
etc.  At the same time, while incorporating these items, 
vagueness and redundancy must be eliminated from the 
annotation language.  Many inter-event relations would 
need to be captured such as entity reference, time refer-
ence, place reference, causal relationships, associative 
relationships, etc.  Finally, to incorporate these, cross-
sentence phenomena remain a challenge.     
From an MT perspective, issues include evaluating 
the consistency in the use of an annotation language 
given that any source text can result in multiple, differ-
ent, legitimate translations (see Farwell and Helmreich, 
2003) for discussion of evaluation in this light.  Along 
these lines, there is the problem of annotating texts for 
translation without including in the annotations infer-
ences from the source text.   
9 Conclusions  
This is a radically different annotation project from 
those that have focused on morphology, syntax or even 
certain types of semantic content (e.g., for word sense 
disambiguation competitions). It is most similar to 
PropBank (Kingsbury et al2002) and FrameNet (Baker 
et al1998).  However, it is novel in its emphasis on:  (1) 
a more abstract level of mark-up (interpretation); (2) the 
assignment of a well-defined meaning representation to 
concrete texts; and (3) issues of a community-wide con-
sistent and accurate annotation of meaning. 
By providing an essential, and heretofore non-
existent, data set for training and evaluating natural lan-
guage processing systems, the resultant annotated multi-
lingual corpus of translations is expected to lead to 
significant research and development opportunities for 
Machine Translation and a host of other Natural Lan-
guage Processing technologies including Question-
Answering and Information Extraction.  
References 
Baker, C., J. Fillmore and J B. Lowe, 1998.  The Berke-
ley FrameNet Project.  Proceedings of ACL. 
Bateman, J.A., R. Kasper, J. Moore, and R. Whitney. 
1989. A General Organization of Knowledge for 
Natural Language Processing: The Penman Upper 
Model. Unpublished research report, USC / Informa-
tion Sciences Institute, Marina del Rey, CA.  
Carletta, J. C. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2), 249-254 
Conceptual Structures and Documentation, UMCP. 
http://www.umiacs.umd.edu/~bonnie/LCS_Database
_Documentation.html  
Dorr, B. J. 2001.  LCS Verb Database, Online Software 
Database of Lexical  
Dorr, B. J., 1993. Machine Translation: A View from the 
Lexicon, MIT Press, Cambridge, MA. 
Farwell, D., and S. Helmreich.  2003.  Pragmatics-based 
Translation and MT Evaluation.  In Proceedings of 
Towards Systematizing MT Evaluation.  MT-Summit 
Workshop, New Orleans, LA. 
Fellbaum, C. (ed.). 1998. WordNet: An On-line Lexical 
Database and Some of its Applications. MIT Press, 
Cambridge, MA. 
Ferro, L., I. Mani, B. Sundheim and G. Wilson.  2001. 
TIDES Temporal Annotation Guidelines. Version 
1.0.2 MITRE Technical Report, MTR 01W0000041 
Fillmore, C..  1968. The Case for Case. In E. Bach and 
R. Harms, editors, Universals in Linguistic Theory, 
pages 1--88. Holt, Rinehart, and Winston.  
Fleischman, M., A. Echihabi, and E.H. Hovy. 2003. 
Offline Strategies for Online Question Answering: 
Answering Questions Before They Are Asked.  Pro-
ceedings of the ACL Conference. Sapporo, Japan. 
Habash, N. and B. Dorr. 2002. Interlingua Annotation 
Experiment Results. AMTA-2002 Interlingua Reli-
ability Workshop. Tiburon, California, USA. 
Habash, N., B. J. Dorr, and D. Traum, 2002. "Efficient 
Language Independent Generation from Lexical 
Conceptual Structures," Machine Translation, 17:4. 
Haji?, J.; B. Vidov?-Hladk?; P. Pajas.  2001: The Pra-
gue Dependency Treebank: Annotation Structure and 
Support. In Proceeding of the IRCS Workshop on 
Linguistic Databases, pp. . University of Pennsyl-
vania, Philadelphia, USA, pp. 105-114. 
Hovy, E., A. Philpot, J. Ambite, Y. Arens, J. Klavans, 
W. Bourne, and D. Saroz.  2001. Data Acquisition 
and Integration in the DGRC's Energy Data Collec-
tion Project, in Proceedings of the NSF's dg.o 2001. 
Los Angeles, CA. 
ISLE 2003.  Framework for Evaluation of Machine 
Translation in ISLE.  
http://www.issco.unige.ch/projects/isle/femti/ 
Jackendoff, R. 1972. Grammatical Relations and Func-
tional Structure. Semantic Interpretation in Genera-
tive Grammar. The MIT Press, Cambridge, MA. 
Kingsbury, P and M Palmer and M Marcus , 2002.  
Adding Semantic Annotation to the Penn TreeBank. 
Proceedings of the Human Language Technology 
Conference (HLT 2002).  
Knight, K., and I. Langkilde. 2000.  Preserving Ambi-
guities in Generation via Automata Intersection. 
American Association for Artificial Intelligence con-
ference (AAAI). 
Knight, K, and S. K. Luk.  1994. Building a Large-Scale 
Knowledge Base for Machine Translation.  Proceed-
ings of AAAI. Seattle, WA. 
Levin, B. and M. Rappaport-Hovav. 1998. From Lexical 
Semantics to Argument Realization. Borer, H. (ed.) 
Handbook of Morphosyntax and Argument Structure. 
Dordrecht: Kluwer Academic Publishers. 
Mahesh, K., and Nirenberg, S.  1995. A Situated Ontol-
ogy for Practical NLP, in Proceedings on the Work-
shop on Basic Ontological Issues in Knowledge 
Sharing at IJCAI-95. Montreal, Canada. 
Philpot, A., M. Fleischman, E.H. Hovy. 2003. Semi-
Automatic Construction of a General Purpose Ontol-
ogy.  Proceedings of the International Lisp Confer-
ence.  New York, NY. Invited. 
Stowell, T. 1981. Origins of Phrase Structure. PhD the-
sis, MIT, Cambridge, MA.  
Tapanainen, P. and T Jarvinen.  1997.  A non-projective 
dependency parser.  In the 5th Conference on Applied 
Natural Language Processing / Association for Com-
putational Linguistics, Washington, DC. 
White, J., and T. O?Connell.  1994.  The ARPA MT 
evaluation methodologies: evolution, lessons, and fu-
ture approaches.  Proceedings of the 1994 Confer-
ence, Association for Machine Translation in the 
Americas 
Walker, K., M. Bamba, D. Miller, X. Ma, C. Cieri, and 
G. Doddington 2003.  Multiple-Translation Arabic 
Corpus, Part 1. Linguistic Data Consortium (LDC) 
catalog num. LDC2003T18 & ISBN 1-58563-276-7. 
 
 
Role Description Grid Syntax Type 
Agent The entity that does the action Agent:  load 
Theme  with possessed 
SUBJ OBLIGATORY 
Theme The entity that is worked on Agent:  load 
Theme with possessed 
OBJ OBLIGATORY 
Possessed The entity controlled or owned Agent:  load 
Theme  with possessed 
PP OPTIONAL 
Table 1 :  A theta grid for the verb "load" 
 
 
Role and Definition Examples 
Agent:  Agents have the features of volition, sentience, causation and 
independent exist 
? Henry pushed/broke the vase. 
Instrument: An instrument should have causation but no volition. Its 
sentience and existence are not relevant. 
? The Hammer broke the vase. 
? She hit him with a baseball bat 
Experiencer: An experiencer has no causation but is sentient and 
exists independently. Typically an experiencer is the subject of verbs 
like feel, hear, see, sense, smell, notice, detect, etc. 
? John heard the vase shatter.   
? John shivered. 
Theme: The theme is typically causally affected or experiences a 
movement and/or change in state. The theme can appear as the infor-
mation in verbs like acquire, learn, memorize, read, study, etc. It can 
also be a thing, event or state (clausal complement). 
? John went to school.  
? John broke the vase.   
? John memorized his lines.  
? She buttered the bread with marga-
rine.   
Perceived: Refers to a perceived entity that isn't required by the verb 
but further characterizes the situation. The perceived is neither caus-
ally affected nor causative. It doesn't experience a movement or 
change in state. Its volition and sentience are irrelevant. Its existence 
is independent of an experiencer. 
? He saw the play.   
? He looked into the room.  
? The cat's fur feels good to John.   
? She imagined the movie to be loud.    
Predicate: Indicates new modifying information about other thematic 
roles. 
? We considered him a fool.   
? She acted happy.   
Source: Indicates where/when the theme started in its motion, or 
what its original state was, or where its original (possibly abstract) 
location/time was. 
? John left the house. 
Goal: Indicates where the theme ends up in its motion, or what its 
final state is, or where/when its final (possibly abstract) location/time 
is. It also can indicate the thing/event resulting from the verb's occur-
rence (the result). 
? John ran home.   
? John ran to the store.  
? John gave a book to Mary.   
? John gave Mary a book. 
Location: Indicates static locations---as opposed to a source or goal, 
i.e., the (possibly abstract) location of the theme or event. 
? He lived in France.   
? The water fills the box.   
? This cabin sleeps five people 
Time Indicates time. ? John sleeps for five hours.   
? Mary ate during the meeting. 
Beneficiary: Indicates the thing that receives the benefit/result of the 
event/state. 
? John baked the cake for Mary.   
? John baked Mary a cake.  
? An accident happened to him.   
Purpose: Indicates the purpose/reason behind an event/state ? He studied for the exam.  
? He searched for rabbits.  
Possessed: Indicates the possessed entity in verbs such as own, have, 
possess, fit, buy, and carry. 
? John has five bucks.  
? He loaded the cart with hay.   
? He bought it for five dollars 
Proposition: Indicates the secondary event/state ? He wanted to study for the exam. 
Modifier: Indicates a property of a thing such as color, taste, size, 
etc. 
? The red book sitting on the table is 
old.  
Null Indicates no thematic contribution. Typical examples are imper-
sonal it and there. 
? It was raining all morning in Miami. 
 
TABLE 2:  List of Theta Roles 
Multi-document Biography Summarization
Liang Zhou, Miruna Ticrea, Eduard Hovy
University of Southern California
Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
                 {liangz, miruna, hovy} @isi.edu
Abstract
In this paper we describe a biography
summarization system using sentence
classification and ideas from information
retrieval. Although the individual techniques
are not new, assembling and applying them to
generate multi-document biographies is new.
Our system was evaluated in DUC2004. It is
among the top performers in task 5?short
summaries focused by person questions.
1 Introduction
Automatic text summarization is one form of
information management. It is described as
selecting a subset of sentences from a document
that is in size a small percentage of the original and
yet is just as informative. Summaries can serve as
surrogates of the full texts in the context of
Information Retrieval (IR). Summaries are created
from two types of text sources, a single document
or a set of documents. Multi-document
summarization (MDS) is a natural and more
elaborative extension of single-document
summarization, and poses additional difficulties on
algorithm design. Various kinds of summaries fall
into two broad categories: generic summaries are
the direct derivatives of the source texts; special-
interest summaries are generated in response to
queries or topic-oriented questions.
One important application of special-interest
MDS systems is creating biographies to answer
questions like ?who is Kofi Annan??. This task
would be tedious for humans to perform in
situations where the information related to the
person is deeply and sparsely buried in large
quantity of news texts that are not obviously
related. This paper describes a MDS biography
system that responds to the ?who is? questions by
identifying information about the person-in-
question using IR and classification techniques,
and creates multi-document biographical
summaries. The overall system design is shown in
Figure 1.
To determine what and how sentences are
selected and ranked, a simple IR method and
experimental classification methods both
contributed. The set of top-scoring sentences, after
redundancy removal, is the resulting biography.
As yet, the system contains no inter-sentence
?smoothing? stage.
In this paper, work in related areas is discussed
in Section 2; a description of our biography corpus
used for training and testing the classification
component is in Section 3; Section 4 explains the
need and the process of classifying sentences
according to their biographical state; the
application of the classification method in
biography extraction/summarization is described in
Section 5; an accompanying evaluation on the
quality of the biography summaries is shown in
Section 6; and future work is outlined in Section 7.
2 Recent Developments
Two trends have dominated automatic
summarization research (Mani, 2001). One is the
work focusing on generating summaries by
extraction, which is finding a subset of the
document that is indicative of its contents (Kupiec
et al, 1995) using ?shallow? linguistic analysis and
statistics.  The other influence is the exploration of
Figure 1. Overall design of the biography
summarization system.
?deeper? knowledge-based methods for
condensing information. Knight and Marcu (2000)
equate summarization with compression at
sentence level to achieve grammaticality and
information capture, and push a step beyond
sentence extraction. Many systems use machine-
learning methods to learn from readily aligned
corpora of scientific articles and their
corresponding abstracts. Zhou and Hovy (2003)
show a summarization system trained from
automatically obtained text-summary alignments
obeying the chronological occurrences of news
events.
MDS poses more challenges in assessing
similarities and differences among the set of
documents. The simple idea of extract-and-
concatenate does not respond to problems arisen
from coherence and cohesion. Barzilay et al
(1999) introduce a combination of extracted
similar phrases and a reformulation through
sentence generation. Lin and Hovy (2002) apply a
collect ion of  known single-document
summarization techniques, cooperating positional
and topical information, clustering, etc., and extend
them to perform MDS.
While many have suggested that conventional
MDS systems can be applied to biography
generation directly, Mani (2001) illustrates that the
added functionality of biographical MDS comes at
the expense of a substantial increase in system
complexity and is somewhat beyond the
capabilities of present day MDS systems. The
discussion was based in part on the only known
MDS biography system (Schiffman et al, 2001)
that uses corpus statistics along with linguistic
knowledge to select and merge description of
people in news. The focus of this work was on
synthesizing succinct descriptions of people by
merging appositives from semantic processing
using WordNet (Miller, 1995).
3 Corpus Description
In order to extract information that is related to a
person from a large set of news texts written not
exclusively about this person, we need to identify
attributes shared among biographies.
Biographies share certain standard components.
We annotated a corpus of 130 biographies of 12
people (activists, artists, leaders, politicians,
scientists, terrorists, etc.).  We found 9 common
elements: bio  (info on birth and death), f a m e
factor, personality, personal, social, education,
nationality, scandal, and w o r k .  Collected
biographies are appropriately marked at clause-
level with one of the nine tags in XML format, for
example:
Martin Luther King <nationality>
was born in Atlanta, Georgia
</nationality>. ? He <bio>was
assassinated on April 4, 1968
</bio>. ? King <education> entered
the Boston University as a
doctoral student </education>. ?
In all, 3579 biography-related phrases were
identified and recorded for the collection, among
them 321 bio , 423 fame , 114 personality, 465
personal, 293 social, 246 education, 95 nationality,
292 scandal, and 1330 work. We then used 100
biographies for training and 30 for testing the
classification module.
4 Sentence Classification
Relating to human practice on summarizing,
three main points are relevant to aid the automation
process (Sp?rck Jones, 1993).  The first is a strong
emphasis on particular purposes, e.g., abstracting
or extracting articles of particular genres.  The
second is the drafting, writing, and revision cycle
in constructing a summary.  Essentially as a
consequence of these first two points, the
summarizing process can be guided by the use of
checklists.  The idea of a checklist is especially
useful for the purpose of generating biographical
summaries because a complete biography should
contain various aspects of a person?s life.  From a
careful analysis conducted while constructing the
biography corpus, we believe that the checklist is
shared and common among all persons in question,
and consists the 9 biographical elements
introduced in Section 3.
The task of fulfilling the biography checklist
becomes a classification problem. Classification is
defined as a task of classifying examples into one
of a discrete set of possible categories (Mitchell,
1997).  Text categorization techniques have been
used extensively to improve the efficiency on
information retrieval and organization.  Here the
problem is that sentences, from a set of documents,
need to be categorized into different biography-
related classes.
4.1 Task Definitions
We designed two classification tasks:
      1) 10 -Class: Given one or more texts about a
person, the module must categorize each
sentence into one of ten classes.  The classes
are the 9 biographical elements plus a class
called none that collects all sentences without
biographical information. This fine-grained
classification task will be beneficial in
generating comprehensive biographies on
people of interest. The classes are:
bio
fame
personality
social
education
nationality
scandal
personal
work
none
       2) 2-Class: The module must make a binary
decision of whether the sentence should be
included in a biography summary. The classes
are:
bio
none
The label bio appears in both task definitions but
bears different meanings. Under 10-Class, class bio
contains information on a person?s birth or death,
and under 2-Class it sums up all 9 biographical
elements from the 10-Class.
4.2 Machine Learning Methods
We experimented with three machine learning
methods for classifying sentences.
Na?ve Bayes
The Na?ve Bayes classifier is among the most
effective algorithms known for learning to classify
text documents (Mitchell, 1997), calculating
explicit probabilities for hypotheses. Using k
features F
j
: j = 1, ?, k, we assign to a given
sentence S the class C:
? 
C = argmax
C
P(C |F
1
,F
2
,...,F
k
)
It can be expressed using Bayes? rule, as (Kupiec
et al, 1995):
? 
P(S ? C |F
1
,F
2
,...F
k
) =
P(F
1
,F
2
,...F
j
| S ? C)?P(S ? C)
P(F
1
,F
2
,...F
k
)
Assuming statistical independence of the
features:
? 
P(S ? C |F
1
,F
2
,...F
k
) =
P(F
j
| S ? C)?P(S ? C)
j=1
k
?
P(F
j
j=1
k
?
)
Since P(F
j
) has no role in selecting C:
? 
P(S ? C |F
1
,F
2
,...F
k
) = P(F
j
| S ? C)?P(S ? C)
j=1
k
?
We trained on the relative frequency of
P(F
j
|S?C) and P(S?C), with add-one smoothing.
This method was used in classifying both the 10-
Class and the 2-Class tasks.
Support Vector Machine
Support Vector Machines (SVMs) have been
shown to be an effective classifier in text
categorization.  We extend the idea of classifying
documents into predefined categories to classifying
sentences into one of the two biography categories
defined by the 2-Class task.  Sentences are
categorized based on their biographical saliency (a
percentage of clearly identified biography words)
and their non-biographical saliency (a percentage
of clearly identified non-biography words).  We
used LIBSVM (Chang and Lin, 2003) for training
and testing.
Decision Tree (4.5)
In addition to SVM, we also used a decision-tree
algorithm, C4.5 (Quinlan, 1993), with the same
training and testing data as SVM.
4.3 Classification Results
The lower performance bound is set by a
baseline system that randomly assigns a
biographical class given a sentence, for both 10-
Class and 2-Class. 2599 testing sentences are from
30 unseen documents.
10-Class Classification
The Na?ve Bayes classifier was used to perform
the 10-Class task.  Table 1 shows its performance
with various features.
Table 1. Performance of 10-Class sentence
classification, using Na?ve Bayes Classifier.
Part-of-speech (POS) information (Brill, 1995)
and word stems (Lovins, 1968) were used in some
feature sets.
We bootstrapped 10395 more biography-
indicating words by recording the immediate
hypernyms, using WordNet (Fellbaum, 1998), of
the words collected from the controlled biography
corpus described in Section 3. These words are
called Expanded Unigrams and their frequency
scores are reduced to a fraction of the original
word?s frequency score.
Some sentences in the testing set were labeled
with multiple biography classes due to the fact that
the original corpus was annotated at clause level.
Since the classification was done at sentence level,
we relaxed the matching/evaluating program
allowing a hit when any of the several classes was
matched. This is shown in Table 1 as the Relaxed
cases.
A closer look at the instances where the false
negatives occur indicates that the classifier
mislabeled instances of class work as instances of
class none.  To correct this error, we created a list
of 5516 work specific words hoping that this would
set a clearer boundary between the two classes.
However performance did not improve.
2-Class Classification
All three machine learning methods were
evaluated in classifying among 2 classes. The
results are shown in Table 2. The testing data is
slightly skewed with 68% of the sentences being
none.
In addition to using marked biographical phrases
as training data, we also expanded the
marking/tagging perimeter to sentence boundaries.
As shown in the table, this creates noise.
5 Biography Extraction
Biographical sentence classification module is
only one of two components that supply the overall
system with usable biographical contents, and is
followed by other stages of processing (see system
design in Figure 1).  We discuss the other modules
next.
5.1 Name-filter
A filter scans through all documents in the set,
eliminating sentences that are direct quotes,
dialogues, and too short (under 5 words).  Person-
oriented sentences containing any variation (first
name only, last name only, and the full name) of
the person?s name are kept for subsequent steps.
Sentences classified as biography-worthy are
merged with the name-filtered sentences with
duplicates eliminated.
5.2 Sentence Ranking
An essential capability of a multi-document
summarizer is to combine text passages in a useful
manner for the reader (Goldstein et al, 2000).
This includes a sentence ordering parameter (Mani,
2001).  Each of the sentences selected by the
name-filter and the biography classifier is either
related to the person-in-question via some news
event or referred to as part of this person?s
biographical profile, or both.  We need a
mechanism that will select sentences that are of
informative significance within the source
document set.  Using inverse-term-frequency
(ITF), i.e. an estimation of information value,
words with high information value (low ITF) are
distinguished from those with low value (high
ITF).  A sorted list of words along with their ITF
scores from a document set?topic ITFs?displays
the important events, persons, etc., from this
particular set of texts.  This allows us to identify
passages that are unusual with respect to the texts
about the person.
However, we also need to identify passages that
are unusual in general.  We have to quantify how
these important words compare to the rest of the
world.  The world is represented by 413307562
w o r d s  f r o m  T R E C - 9  c o r p u s
(http://trec.nist.gov/data.html), with corresponding
ITFs.
The overall informativeness of each word w is:
? 
C
w
=
d
itf
w
W
itf
w
where d
itf
 is the document set ITF of word w and
W
itf
 is the world ITF of w .  A word that occurs
frequently bears a lower C
w
 score compared to a
rarely used word (bearing high information value)
with a higher C
w
 score.
Top scoring sentences are then extracted
according to:
Table 2. Classification results on 2-Class using
Na?ve Bayes, SVM, and C4.5.
? 
C
s
=
C
w
i
i=1
n
?
len(s)
The following is a set of sentences extracted
according to the method described so far.  The
person-in-question is the famed cyclist Lance
Armstrong.
1. Cycling helped him win his
battle with cancer, and
cancer helped him win the
Tour de France.
2. Armstrong underwent four
rounds of intense
chemotherapy.
3. The surgeries and
chemotherapy eliminated the
cancer, and Armstrong began
his cycling comeback.
4. The foundation supports
cancer patients and survivors
through education, awareness
and research.
5. He underwent months of
chemotherapy.
5.3 Redundancy Elimination
Summaries that emphasize the differences across
documents while synthesizing common
information would be the desirable final results.
Removing similar information is part of all MDS
systems. Redundancy is apparent in the Armstrong
example from Section 5.2.  To eliminate repetition
while retaining interesting singletons, we modified
(Marcu, 1999) so that an extract can be
automatically generated by starting with a full text
and systematically removing a sentence at a time
as long as a stable semantic similarity with the
original text is maintained. The original extraction
algorithm was used to automatically create large
volume of (extract, abstract, text) tuples for
training extraction-based summarization systems
with (abstract, text) input pairs.
Top-scoring sentences selected by the ranking
mechanism described in Section 5.2 were the input
to this component. The removal process was
repeated until the desired summary length was
achieved.
Applying this method to the Armstrong example,
the result leaves only one sentence that contains
the topics ?chemotherapy? and ?cancer?.  It
chooses sentence 3, which is not bad, though
sentence 1 might be preferable.
6 Evaluation
6.1 Overview
Extrinsic and intrinsic evaluations are the two
classes of text summarization evaluation methods
(Sparck Jones and Galliers, 1996).  Measuring
content coverage or summary informativeness is an
approach commonly used for intrinsic evaluation.
It measures how much source content was
preserved in the summary.
A complete evaluation should include
evaluations of the accuracy of components
involved in the summarization process (Schiffman
et al, 2001).  Performance of the sentence
classifier was shown in Section 4.  Here we will
show the performance of the resulting summaries.
6.2 Coverage Evaluation
An intrinsic evaluation of biography summary
was recently conducted under the guidance of
Document Understanding Conference (DUC2004)
using the automatic summarization evaluation tool
ROUGE (Recall-Oriented Understudy for Gisting
Evaluation) by Lin and Hovy (2003).  50 TREC
English document clusters, each containing on
average 10 news articles, were the input to the
system. Summary length was restricted to 665
bytes. Brute force truncation was applied on longer
summaries.
The ROUGE-L metric is based on Longest
Common Subsequence (LCS) overlap (Saggion et
al., 2002).  Figure 2 shows that our system (86)
performs at an equivalent level with the best
systems 9 and 10, that is, they both lie within our
system?s 95% upper confidence interval.  The 2-
class classification module was used in generating
the answers. The figure also shows the
performance data evaluated with lower and higher
confidences set at 95%. The performance data are
from official DUC results.
Figure 3 shows the performance results of our
system 86, using 10-class sentence classification,
comparing to other systems from DUC by
replicating the official evaluating process. Only
system 9 performs slightly better with its score
being higher than our system?s 95% upper
confidence interval.
A baseline system (5) that takes the first 665
bytes of the most recent text from the set as the
resulting biography was also evaluated amongst
the peer systems. Clearly, humans still perform at a
level much superior to any system.
Measuring fluency and coherence is also
important in reflecting the true quality of machine-
generated summaries.  There is no automated tool
for this purpose currently.  We plan to incorporate
one for the future development of this work.
6.3 Discussion
N-gram recall scores are computed by ROUGE,
in addition to ROUGE-L shown here. While cosine
similarity and unigram and bigram overlap
demonstrate a sufficient measure on content
coverage, they are not sensitive on how
information is sequenced in the text (Saggion et al,
2002). In evaluating and analyzing MDS results,
metrics, such as ROUGE-L, that consider linguistic
sequence are essential.
Radev and McKeown (1998) point out when
summarizing interesting news events from multiple
sources, one can expect reports with contradictory
and redundant information. An intelligent
summarizer should attain as much information as
possible, combine it, and present it in the most
concise form to the user. When we look at the
different attributes in a person?s life reported in
news articles, a person is described by the job
positions that he/she has held, by education
institutions that he/she has attended, and etc. Those
data are confirmed biographical information and
do not bear the necessary contradiction associated
with evolving news stories. However, we do feel
the need to address and resolve discrepancies if we
were to create comprehensive and detailed
0.25
0.3
0.35
0.4
0.45
0.5
0.55
B E F H G A D C 9 10 11 12 13 86 15 16 17 18 19 20 5 22 23 24 25 26 27 28 29 30 31
ROUGE-L
95% CI Lower
95% CI Higher
Figure 2. Official ROUGE performance results from DUC2004. Peer systems are labeled with numeric IDs.
Humans are numbered A?H. 86 is our system with 2-class biography classification. Baseline is 5.
0.17
0.22
0.27
0.32
0.37
0.42
0.47
0.52
0.57
B F E G H A D C 9 10 11 12 13 86 15 16 17 18 19 20 21 22 23 24 25 5 27 28 29 30 31
ROUGE-L
95% CL Lower
95% CL Higher
Figure 3. Unofficial ROUGE results. Humans are labeled A?H. Peer systems are labeled with numeric IDs.
86 is our system with 10-class biography classification. Baseline is 5.
biographies on people-in-news since miscellaneous
personal facts are often overlooked and told in
conflicting reports. Misrepresented biographical
information may well be controversies and may
never be clarified. The scandal element from our
corpus study (Section 3) is sufficient to identify
information of the disputed kind.
Extraction-based MDS summarizers, such as this
one, present the inherent problem of lacking the
discourse-level fluency. While sentence ordering
for single document summarization can be
determined from the ordering of sentences in the
input article, sentences extracted by a MDS system
may be from different articles and thus need a
strategy on ordering to produce a fluent surface
summary (Barzilay et al, 2002). Previous
summarization systems have used temporal
sequence as the guideline on ordering. This is
especially true in generating biographies where a
person is represented by a sequence of events that
occurred in his/her life. Barzilay et al also
introduced a combinational method with an
alternative strategy that approximates the
information relatedness across the input texts. We
plan to use a fixed-form structure for the majority
of answer construction, fitted for biographies only.
This will be a top-down ordering strategy, contrary
to the bottom-up algorithm shown by Barzilay et
al.
7 Conclusion and Future Work
In this paper, we described a system that uses IR
and text categorization techniques to provide
summary-length answers to biographical questions.
The core problem lies in extracting biography-
related information from large volumes of news
texts and composing them into fluent, concise,
multi-document summaries.  The summaries
generated by the system address the question about
the person, though not listing the chronological
events occurring in this person?s life due to the
lack of background information in the news
articles themselves.  In order to obtain a ?normal?
biography, one should consult other means of
information repositories.
Question: Who is Sir John Gielgud?
Answer: Sir John Gielgud, one of
the great actors of the English
stage who enthralled audiences for
more than 70 years with his
eloquent voice and consummate
artistry, died Sunday at his home
Gielgud?s last major film role was
as a surreal Prospero in Peter
Greenaway?s controversial
Shakespearean rhapsody.
Above summary does not directly explain who
the person-in-question is, but indirectly does so in
explanatory sentences.  We plan to investigate
combining fixed-form and free-form structures in
answer construction. The summary would include
an introductory sentence of the form ?x is
<type/fame-category> ??, possibly through
querying outside online resources.  A main body
would follow the introduction with an assembly of
checklist items generated from the 10-Class
classifier.  A conclusion would contain open-ended
items of special interest.
Furthermore, we would like to investigate
compression strategies in creating summaries,
specifically for biographies.  Our biography corpus
was tailored for this purpose and will be the
starting point for further investigation.
Acknowledgement
We would like to thank Chin-Yew Lin from ISI
for many insightful discussions on MDS,
biography generation, and ROUGE.
References
Regina Barzilay, Kathleen McKeown, and Michael
Elhadad. 1999. Information fusion in the context
of multi-document summarization. In
Proceedings of the 37
th
 Annual Meeting of the
Association of Computational Linguistics (ACL-
99), University of Maryland, 1999, pp. 550?557.
Regina Barzilay, Noemie Elhadad, Kathleen
McKeown. 2002. Inferring strategies for
sentence ordering in multidocument
summarization. JAIR, 17:35?55, 2002.
Eric Brill. 1995. Transformation-based error-
driven learning and natural language processing:
A case study in part of speech tagging.
Computational Linguistics, December 1995.
Chih-Chung Chang and Chih-Jen Lin. 2003.
LIBSVM?A Library for support vector
machines.
http://www.csie.ntu.edu.tw/~cjlin/libsvm/
Christiane Fellbaum, editor. WordNet: An
electronic lexical database. Cambridge, MA:
MIT Press.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and
Mark Kantrowitz. 2000. Multi-document
summarization by sentence extraction. In
Proceedings of the ANLP?2000 Workshop on
Automatic Summarization,  40?48.  New
Brunswick, New Jersey: Association for
Computational Linguistics.
Thorsten Joachims. 1998. Text categorization with
support vector machines: Learning with many
relevant features. In Proceedings of the
European Conference on Machine Learning
(ECML), pages 137?142.
Kevin Knight and Daniel Marcu. 2000. Statistics-
Based summarization step one: sentence
compression. In Proceedings of the 17
th
 National
Conference of the American Association for
Artificial Intelligence (AAAI 2000).
Julian Kupiec, Jan Pedersen, and Francine Chen.
1995. A trainable document summarizer. In
SIGIR?95 , Proceedings of the 18
th
 Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pp. 68?73.
Chin-Yew Lin and Eduard Hovy. 2002. Automated
multi-document summarization in NeATS. In
Proceedings of the Human Language
Technology Conference (HLT2002), San Diego,
CA, U.S.A., March 23-27, 2002.
Chin-Yew Lin and Eduard Hovy. 2003. Automatic
evaluation of summaries using n-gram co-
occurrence statistics. In HLT-NAACL 2003:
Main Proceedings, pp.150?157.
Julie Beth Lovins. 1968. Development of a
stemming algorithm. Mechanical translation and
computational linguistics, 11:22?31, 1968.
Inderjeet Mani. 2001. Automatic summarization
(natural language processing, 3).
Inderjeet Mani. 2001. Recent developments in text
summarization. In CIKM?2001, Proceedings of
the Tenth International Conference on
Information and Knowledge Management,
November 5-10, 2001, 529?531.
Daniel Marcu. 1999. The automatic construction of
large-scale corpora for summarization research.
The 22
nd
 International ACM SIGIR Conference
on Research and Development in Information
Retrieval (SIGIR?99), pages 137-144, Berkeley,
CA, August 1999.
George Miller. 1995. WordNet: a lexical database
for English. Communications of the ACM, pages
39?41.
Tom Mitchell. 1997. Machine Learning. McGraw
Hill, 1997.
Ross J. Quinlan. 1993. C4.5: Programs for
machine learning. San Mateo, CA: Morgan
Kaufmann.
Dragomir R. Radev, Kathleen McKeown. 1998.
Generating natural language summaries from
multiple on-line sources. Computational
Linguistics 24(3): 469?500 (1998).
Horacio Saggion, Dragomir Radev, Simone Teufel,
and Wai Lam. Meta-evaluation of summaries in
a cross-lingual environment using content-based
metrics. In Proceedings of COLING?2002,
Taipei, Taiwan, August 2002.
Barry Schiffman, Inderjeet Mani, and Kristian
Concepcion. 2001. Producing biographical
summaries: combining linguistic knowledge with
corpus statistics. In Proceedings of the 39
th
Annual Meeting of the Association for
Computational Linguistics (ACL?2001),
450?457. New Brunswick, New Jersey:
Association for Computational Linguistics.
Karen Sp?rck Jones and Julia R. Galliers. 1996.
Evaluating Natural Language Processing
Systems: An Analysis and Review. Lecture Notes
in Artificial Intelligence 1083. Berlin: Springer.
Karen Sp?rck Jones. 1993. What might be in a
summary? Information Retrieval 1993: 9?26.
Liang Zhou and Eduard Hovy. A web-trained
extraction summarization system. In
Proceedings of the Human Language
Technology Conference (HLT-NAACL 2003),
pages 284?290.
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 186?187,
Vancouver, October 2005. c?2005 Association for Computational Linguistics
Statistical Shallow Semantic Parsing despite Little Training Data
Rahul Bhagat
Information Sciences
Institute
University of Southern
California
Marina del Rey,
CA, 90292, USA
rahul@isi.edu
Anton Leuski
Institute for Creative
Technologies
University of Southern
California
Marina del Rey,
CA, 90292, USA
leuski@ict.usc.edu
Eduard Hovy
Information Sciences
Institute
University of Southern
California
Marina del Rey,
CA, 90292, USA
hovy@isi.edu
1 Introduction and Related Work
Natural language understanding is an essential mod-
ule in any dialogue system. To obtain satisfac-
tory performance levels, a dialogue system needs
a semantic parser/natural language understanding
system (NLU) that produces accurate and detailed
dialogue oriented semantic output. Recently, a
number of semantic parsers trained using either
the FrameNet (Baker et al, 1998) or the Prop-
Bank (Kingsbury et al, 2002) have been reported.
Despite their reasonable performances on general
tasks, these parsers do not work so well in spe-
cific domains. Also, where these general purpose
parsers tend to provide case-frame structures, that
include the standard core case roles (Agent, Patient,
Instrument, etc.), dialogue oriented domains tend
to require additional information about addressees,
modality, speech acts, etc. Where general-purpose
resources such as PropBank and Framenet provide
invaluable training data for general case, it tends to
be a problem to obtain enough training data in a spe-
cific dialogue oriented domain.
We in this paper propose and compare a num-
ber of approaches for building a statistically trained
domain specific parser/NLU for a dialogue system.
Our NLU is a part of Mission Rehearsal Exercise
(MRE) project (Swartout et al, 2001). MRE is a
large system that is being built to train experts, in
which a trainee interacts with a Virtual Human using
voice input. The purpose of our NLU is to convert
the sentence strings produced by the speech recog-
nizer into internal shallow semantic frames com-
posed of slot-value pairs, for the dialogue module.
2 Parsing Methods
2.1 Voting Model
We use a simple conditional probability model
P (f | W ) for parsing. The model represents the
probability of producing slot-value pair f as an out-
put given that we have seen a particular word or
n-gram W as input. Our two-stage procedure for
generating a frame for a given input sentence is: (1)
Find a set of all slot-value that correspond with each
word/ngram (2) Select the top portion of these can-
didates to form the final frame (Bhagat et al, 2005;
Feng and Hovy, 2003).
2.2 Maximum Entropy
Our next approach is the Maximum Entropy (Berger
et al, 1996) classification approach. Here, we cast
our problem as a problem of ranking using a classi-
fier where each slot-value pair in the training data is
considered a class and feature set consists of the un-
igrams, bigrams and trigrams in the sentences (Bha-
gat et al, 2005).
2.3 Support Vector Machines
We use another commonly used classifier, Support
Vector Machine (Burges, 1998), to perform the
same task (Bhagat et al, 2005). Approach is sim-
ilar to Section 2.2.
2.4 Language Model
As a fourth approach to the problem, we use the Sta-
tistical Language Model (Ponte and Croft, 1997).
We estimate the language model for the slot-value
pairs, then we construct our target interpretation as
186
Method Precison Recall F-score
V oting 0.82 0.78 0.80
ME 0.77 0.80 0.78
SVM 0.79 0.72 0.75
LM1 0.80 0.84 0.82
LM2 0.82 0.84 0.83
Table 1: Performance of different systems on test
data.
a set of the most likely slot-value pairs. We use
unigram-based and trigram-based language mod-
els (Bhagat et al, 2005).
3 Experiments and Results
We train all our systems on a training set of 477
sentence-frame pairs. The systems are then tested on
an unseen test set of 50 sentences. For the test sen-
tences, the system generated frames are compared
against the manually built gold standard frames, and
Precision, Recall and F-scores are calculated for
each frame.
Table 1 shows the average Precision, Recall and
F-scores of the different systems for the 50 test sen-
tences: Voting based (Voting), Maximum Entropy
based (ME), Support Vector Machine based (SVM),
Language Model based with unigrams (LM1) and
Language Model based with trigrams (LM2). The
F-scores show that the LM2 system performs the
best though the system scores in general for all the
systems are very close. To test the statistical signifi-
cance of these scores, we conduct a two-tailed paired
Student?s t test (Manning and Schtze, 1999) on the
F-scores of these systems for the 50 test cases. The
test shows that there is no statistically significant dif-
ference in their performances.
4 Conclusions
This work illustrates that one can achieve fair suc-
cess in building a statistical NLU engine for a re-
stricted domain using relatively little training data
and surprisingly using a rather simple voting model.
The consistently good results obtained from all the
systems on the task clearly indicate the feasibility of
using using only word/ngram level features for pars-
ing.
5 Future Work
Having successfully met the initial challenge of
building a statistical NLU with limited training data,
we have identified multiple avenues for further ex-
ploration. Firstly, we wish to build an hybrid system
that will combine the strengths of all the systems to
produce a much more accurate system. Secondly,
we wish to see the effect that ASR output has on
each of the systems. We want to test the robustness
of systems against an increase in the ASR word er-
ror rate. Thirdly, we want to build a multi-clause
utterance chunker to integrate with our systems. We
have identified that complex multi-clause utterances
have consistently hurt the system performances. To
handle this, we are making efforts along with our
colleagues in the speech community to build a real-
time speech utterance-chunker. We are eager to dis-
cover any performance benefits. Finally, since we
already have a corpus containing sentence and their
corresponding semantic-frames, we want to explore
the possibility of building a Statistical Generator us-
ing the same corpus that would take a frame as input
and produce a sentence as output. This would take
us a step closer to the idea of building a Reversible
System that can act as a parser when used in one
direction and as a generator when used in the other.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The berkeley
framenet project. In Proceedings of COLING/ACL, page 8690, Montreal,
Canada.
Adam L. Berger, Stephen Della Pietra, and Vincent J. Della Pietra. 1996. A
maximum entropy approach to natural language processing. Computational
Linguistics, 22(1):39?71.
Rahul Bhagat, Anton Leuski, and Eduard Hovy. 2005. Statistical shallow
semantic parsing despite little training data. Technical report available at
http://www.isi.edu/?rahul.
Christopher J. C. Burges. 1998. A tutorial on support vector machines for pattern
recognition. Data Mining and Knowledge Discovery, 2(2):121?167.
Donghui Feng and Eduard Hovy. 2003. Semantics-oriented language understand-
ing with automatic adaptability. In Proceedings of Natural Language Process-
ing and Knowledge Engineering.
Paul Kingsbury, Martha Palmer, and Mitch Marcus. 2002. Adding semantic an-
notation to the penn treebank. In Proceedings of HLT Conference.
Christopher D. Manning and Hinrich Schtze. 1999. Foundations of Statistical
Natural Language Processing. The MIT Press, Cambridge, MA.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In Proceed-
ings of the First European Conference on Research and Advanced Technology
for Digital Libraries, pages 120?129.
W. Swartout, R. Hill, J. Gratch, W. Johnson, C. Kyriakakis, C. LaBore, R. Lind-
heim, S. Marsella, D. Miraglia, B. Moore, J. Morie, J. Rickel, M. Thiebaux,
L. Tuch, R. Whitney, and J. Douglas. 2001. Toward the holodeck: Integrating
graphics, sound, character and story. In Proceedings of Autonomous Agents.
187
Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 1?8,
Sydney, July 2006. c?2006 Association for Computational Linguistics
 Extracting Opinions, Opinion Holders, and Topics Expressed in 
Online News Media Text 
Soo-Min Kim and Eduard Hovy 
USC Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{skim, hovy}@ISI.EDU 
 
  
 
Abstract 
This paper presents a method for identi-
fying an opinion with its holder and 
topic, given a sentence from online news 
media texts. We introduce an approach of 
exploiting the semantic structure of a 
sentence, anchored to an opinion bearing 
verb or adjective. This method uses se-
mantic role labeling as an intermediate 
step to label an opinion holder and topic 
using data from FrameNet. We decom-
pose our task into three phases: identify-
ing an opinion-bearing word, labeling 
semantic roles related to the word in the 
sentence, and then finding the holder and 
the topic of the opinion word among the 
labeled semantic roles. For a broader 
coverage, we also employ a clustering 
technique to predict the most probable 
frame for a word which is not defined in 
FrameNet. Our experimental results show 
that our system performs significantly 
better than the baseline. 
1 Introduction   
The challenge of automatically identifying opin-
ions in text automatically has been the focus of 
attention in recent years in many different do-
mains such as news articles and product reviews. 
Various approaches have been adopted in subjec-
tivity detection, semantic orientation detection, 
review classification and review mining. Despite 
the successes in identifying opinion expressions 
and subjective words/phrases, there has been less 
achievement on the factors closely related to sub-
jectivity and polarity, such as opinion holder, 
topic of opinion, and inter-topic/inter-opinion 
relationships. This paper addresses the problem 
of identifying not only opinions in text but also 
holders and topics of opinions from online news 
articles. 
Identifying opinion holders is important espe-
cially in news articles. Unlike product reviews in 
which most opinions expressed in a review are 
likely to be opinions of the author of the review, 
news articles contain different opinions of differ-
ent opinion holders (e.g. people, organizations, 
and countries). By grouping opinion holders of 
different stance on diverse social and political 
issues, we can have a better understanding of the 
relationships among countries or among organi-
zations. 
An opinion topic can be considered as an ob-
ject an opinion is about. In product reviews, for 
example, opinion topics are often the product 
itself or its specific features, such as design and 
quality (e.g. ?I like the design of iPod video?, 
?The sound quality is amazing?). In news arti-
cles, opinion topics can be social issues, gov-
ernment?s acts, new events, or someone?s opin-
ions. (e.g., ?Democrats in Congress accused vice 
president Dick Cheney?s shooting accident.?, 
?Shiite leaders accused Sunnis of a mass killing 
of Shiites in Madaen, south of Baghdad.?)  
As for opinion topic identification, little re-
search has been conducted, and only in a very 
limited domain, product reviews. In most ap-
proaches in product review mining, given a 
product (e.g. mp3 player), its frequently men-
tioned features (e.g. sound, screen, and design) 
are first collected and then used as anchor points. 
In this study, we extract opinion topics from 
news articles. Also, we do not pre-limit topics in 
advance. We first identify an opinion and then 
find its holder and topic. We define holder as an 
entity who holds an opinion, and topic, as what 
the opinion is about.   
In this paper, we propose a novel method that 
employs Semantic Role Labeling, a task of iden-
tifying semantic roles given a sentence. We de-
1
compose the overall task into the following 
steps: 
? Identify opinions. 
? Label semantic roles related to the opin-
ions. 
? Find holders and topics of opinions 
among the identified semantic roles. 
? Store <opinion, holder, topic> triples 
into a database. 
In this paper, we focus on the first three subtasks. 
The main contribution of this paper is to pre-
sent a method that identifies not only opinion 
holders but also opinion topics. To achieve this 
goal, we utilize FrameNet data by mapping target 
words to opinion-bearing words and mapping 
semantic roles to holders and topics, and then use 
them for system training. We demonstrate that 
investigating semantic relations between an opin-
ion and its holder and topic is crucial in opinion 
holder and topic identification. 
This paper is organized as follows: Section 2 
briefly introduces related work both in sentiment 
analysis and semantic role labeling. Section 3 
describes our approach for identifying opinions 
and labeling holders and topics by utilizing Fra-
meNet1 data for our task. Section 4 reports our 
experiments and results with discussions and 
finally Section 5 concludes. 
2 Related Work 
This section reviews previous works in both 
sentiment detection and semantic role labeling.  
2.1 Subjectivity and Sentiment Detection 
Subjectivity detection is the task of identifying 
subjective words, expressions, and sentences 
(Wiebe et al, 1999; Hatzivassiloglou and Wiebe, 
2000; Riloff et al, 2003). Identifying subjectiv-
ity helps separate opinions from fact, which may 
be useful in question answering, summarization, 
etc. Sentiment detection is the task of determin-
ing positive or negative sentiment of words (Hat-
zivassiloglou and McKeown, 1997; Turney, 
2002; Esuli and Sebastiani, 2005), phrases and 
sentences (Kim and Hovy, 2004; Wilson et al, 
2005), or documents (Pang et al, 2002; Turney, 
2002).  
Building on this work, more sophisticated 
problems such as opinion holder identification 
have also been studied. (Bethard et al, 2004) 
identify opinion propositions and holders. Their 
                                                 
1 http://framenet.icsi.berkeley.edu/ 
work is similar to ours but different because their 
opinion is restricted to propositional opinion and 
mostly to verbs. Another related works are (Choi 
et al, 2005; Kim and Hovy, 2005). Both of them 
use the MPQA corpus 2  but they only identify 
opinion holders, not topics. 
As for opinion topic identification, little re-
search has been conducted, and only in a very 
limited domain, product reviews. (Hu and Liu, 
2004; Popescu and Etzioni, 2005) present prod-
uct mining algorithms with extracting certain 
product features given specific product types. 
Our paper aims at extracting topics of opinion in 
general news media text. 
2.2 Semantic Role Labeling 
Semantic role labeling is the task of identifying 
semantic roles such as Agent, Patient, Speaker, 
or Topic, in a sentence. A statistical approach for 
semantic role labeling was introduced by (Gildea 
and Jurafsky, 2002). Their system learned se-
mantic relationship among constituents in a sen-
tence from FrameNet, a large corpus of semanti-
cally hand-annotated data. The FrameNet annota-
tion scheme is based on Frame Semantics (Fill-
more, 1976). Frames are defined as ?schematic 
representations of situations involving various 
frame elements such as participants, props, and 
other conceptual roles.? For example, given a 
sentence ?Jack built a new house out of bricks?, 
a semantic role labeling system should identify 
the roles for the verb built such as ?[Agent Jack] 
built [Created_entity  a new house] [Component out of 
bricks]?3. In our study, we build a semantic role 
labeling system as an intermediate step to label 
opinion holders and topics by training it on opin-
ion-bearing frames and their frame elements in 
FrameNet. 
3 Finding Opinions and Their Holders 
and Topics 
For the goal of this study, extracting opinions 
from news media texts with their holders and 
topics, we utilize FrameNet data. The basic idea 
of our approach is to explore how an opinion 
holder and a topic are semantically related to an 
opinion bearing word in a sentence. Given a sen-
tence and an opinion bearing word, our method 
identifies frame elements in the sentence and 
                                                 
2 http://www.cs.pitt.edu/~wiebe/pubs/ardasummer02/ 
3 The verb ?build? is defined under the frame ?Build-
ing? in which Agent, Created_entity, and Components 
are defined as frame elements. 
2
searches which frame element corresponds to the 
opinion holder and which to the topic. The ex-
ample in Figure 1 shows the intuition of our al-
gorithm. 
We decompose our task in 3 subtasks: (1) col-
lect opinion words and opinion-related frames, 
(2) semantic role labeling for those frames, and 
(3) finally map semantic roles to holder and 
topic. Following subsections describe each sub-
task. 
3.1 Opinion Words and Related Frames 
We describe the subtask of collecting opinion 
words and related frames in 3 phases. 
Phase 1: Collect Opinion Words 
In this study, we consider an opinion-bearing 
(positive/negative) word is a key indicator of an 
opinion. Therefore, we first identify opinion-
bearing word from a given sentence and extract 
its holder and topic. Since previous studies indi-
cate that opinion-bearing verbs and adjectives are 
especially efficient for opinion identification, we 
focus on creating a set of opinion-bearing verbs 
and adjectives. We annotated 1860 adjectives 
and 2011 verbs4 by classifying them into posi-
tive, negative, and neutral classes. Words in the 
positive class carry positive valence whereas 
                                                 
4 These were randomly selected from 8011 English 
verbs and 19748 English adjectives. 
those in negative class carry negative valence. 
Words that are not opinion-bearing are classified 
as neutral.  
Note that in our study we treat word sentiment 
classification as a three-way classification prob-
lem instead of a two-way classification problem 
(i.e. positive and negative). By adding the third 
class, neutral, we can prevent the classifier as-
signing either positive or negative sentiment to 
weak opinion-bearing word. For example, the 
word ?central? that Hatzivassiloglou and McKe-
own (1997) marked as a positive adjective is not 
classified as positive by our system. Instead we 
mark it as ?neutral?, since it is a weak clue for an 
opinion. For the same reason, we did not con-
sider ?able? classified as a positive word by Gen-
eral Inquirer5 , a sentiment word lexicon, as a 
positive opinion indicator. Finally, we collected 
69 positive and 151 negative verbs and 199 posi-
tive and 304 negative adjectives. 
Phase 2: Find Opinion-related Frames 
We collected frames related to opinion words 
from the FrameNet corpus. We used FrameNet II 
(Baker et al, 2003) which contains 450 semantic 
frames and more than 3000 frame elements (FE). 
A frame consists of lexical items, called Lexical 
Unit (LU), and related frame elements. For in-
stance, LUs in ATTACK frame are verbs such as 
assail, assault, and attack, and nouns such as in-
vasion, raid, and strike. FrameNet II contains 
                                                 
5 http://www.wjh.harvard.edu/~inquirer/homecat.htm 
Table 1: Example of opinion related frames 
and lexical units 
Frame 
name Lexical units Frame elements 
Desiring
want, wish, hope, 
eager, desire, 
interested, 
Event, 
Experiencer, 
Location_of_event
Emotion
_directed
agitated, amused, 
anguish, ashamed, 
angry, annoyed, 
Event, Topic 
Experiencer, 
Expressor, 
Mental 
_property
absurd, brilliant, 
careless, crazy, 
cunning, foolish 
Behavior, 
Protagonist, 
Domain, Degree 
Subject 
_stimulus
delightful, amazing, 
annoying, amusing, 
aggravating, 
Stimulus, Degree
Experiencer, 
Circumstances, 
  
Figure 1: An overview of our algorithm 
 
 
3
approximately 7500 lexical units and over 
100,000 annotated sentences. 
For each word in our opinion word set de-
scribed in Phase 1, we find a frame to which the 
word belongs. 49 frames for verbs and 43 frames 
for adjectives are collected. Table 1 shows ex-
amples of selected frames with some of the lexi-
cal units those frames cover. For example, our 
system found the frame Desiring from opinion-
bearing words want, wish, hope, etc. Finally, we 
collected 8256 and 11877 sentences related to 
selected opinion bearing frames for verbs and 
adjectives respectively. 
Phase 3: FrameNet expansion  
Even though Phase 2 searches for a correlated 
frame for each verb and adjective in our opinion-
bearing word list, not all of them are defined in 
FrameNet data. Some words such as criticize and 
harass in our list have associated frames (Case 
1), whereas others such as vilify and maltreat do 
not have those (Case 2). For a word in Case 2, 
we use a clustering algorithms CBC (Clustering 
By Committee) to predict the closest (most rea-
sonable) frame of undefined word from existing 
frames. CBC (Pantel and Lin, 2002) was devel-
oped based on the distributional hypothesis (Har-
ris, 1954) that words which occur in the same 
contexts tend to be similar. Using CBC, for ex-
ample, our clustering module computes lexical 
similarity between the word vilify in Case 2 and 
all words in Case 1. Then it picks criticize as a 
similar word, so that we can use for vilify the 
frame Judgment_communication to which criti-
cize belongs and all frame elements defined un-
der Judgment_ communication. 
3.2 Semantic Role Labeling 
To find a potential holder and topic of an opinion 
word in a sentence, we first label semantic roles 
in a sentence.  
Modeling: We follow the statistical ap-
proaches for semantic role labeling (Gildea and 
Jurafsky, 2002; Fleischman et. al, 2003) which 
separate the task into two steps: identify candi-
dates of frame elements (Step 1) and assign se-
mantic roles for those candidates (Step 2). Like 
their intuition, we treated both steps as classifica-
tion problems. We first collected all constituents 
of the given sentence by parsing it using the 
Charniak parser. Then, in Step 1, we classified 
candidate constituents of frame elements from 
non-candidates. In Step 2, each selected candi-
date was thus classified into one of frame ele-
ment types (e.g. Stimulus, Degree, Experiencer, 
etc.). As a learning algorithm for our classifica-
tion model, we used Maximum Entropy (Berger 
et al, 1996). For system development, we used 
MEGA model optimization package6, an imple-
mentation of ME models. 
Data: We collected 8256 and 11877 sentences 
which were associated to opinion bearing frames 
for verbs and adjectives from FrameNet annota-
tion data. Each sentence in our dataset contained 
a frame name, a target predicate (a word whose 
meaning represents aspects of the frame), and 
frame elements labeled with element types. We 
divided the data into 90% for training and 10% 
for test.  
Features used: Table 2 describes features that 
we used for our classification model. The target 
word is an opinion-bearing verb or adjective 
which is associated to a frame. We used the 
Charniak parser to get a phrase type feature of a 
frame element and the parse tree path feature. 
We determined a head word of a phrase by an 
algorithm using a tree head table7, position fea-
ture by the order of surface words of a frame 
element and the target word, and the voice fea-
ture by a simple pattern. Frame name for a target 
                                                 
6 http://www.isi.edu/~hdaume/megam/index.html 
7 http://people.csail.mit.edu/mcollins/papers/heads 
Table 2: Features used for our semantic role 
labeling model. 
Feature Description 
target word 
A predicate whose meaning 
represents the frame (a verb 
or an adjective in our task) 
phrase type Syntactic type of the frame element (e.g. NP, PP) 
head word Syntactic head of the frame element phrase 
parse tree 
path 
A path between the frame 
element and target word in 
the parse tree 
position 
Whether the element phrase 
occurs before or after the tar-
get word 
voice The voice of the sentence (active or passive) 
frame name one of our opinion-related frames 
 
4
word was selected by methods described in 
Phase 2 and Phase 3 in Subsection 3.1.   
3.3 Map Semantic Roles to Holder and 
Topic 
After identifying frame elements in a sentence, 
our system finally selects holder and topic from 
those frame elements. In the example in Table 1, 
the frame ?Desiring? has frame elements such as 
Event (?The change that the Experiencer would 
like to see?), Experiencer (?the person or sentient 
being who wishes for the Event to occur?), Loca-
tion_of_event (?the place involved in the desired 
Event?), Focal_participant (?entity that the Ex-
periencer wishes to be affected by some Event?). 
Among these FEs, we can consider that Experi-
encer can be a holder and Focal_participant can 
be a topic (if any exists in a sentence). We 
manually built a mapping table to map FEs to 
holder or topic using as support the FE defini-
tions in each opinion related frame and the anno-
tated sample sentences. 
4 Experimental Results 
The goal of our experiment is first, to see how 
our holder and topic labeling system works on 
the FrameNet data, and second, to examine how 
it performs on online news media text. The first 
data set (Testset 1) consists of 10% of data de-
scribed in Subsection 3.2 and the second (Testset 
2) is manually annotated by 2 humans. (see Sub-
section 4.2). We report experimental results for 
both test sets. 
4.1 Experiments on Testset 1 
Gold Standard: In total, Testset 1 contains 2028 
annotated sentences collected from FrameNet 
data set. (834 from frames related to opinion 
verb and 1194 from opinion adjectives) We 
measure the system performance using precision 
(the percentage of correct holders/topics among 
system?s labeling results), recall (the percentage 
of correct holders/topics that system retrieved), 
and F-score.  
Baseline: For the baseline system, we applied 
two different algorithms for sentences which 
have opinion-bearing verbs as target words and 
for those that have opinion-bearing adjectives as 
target words. For verbs, baseline system labeled 
a subject of a verb as a holder and an object as a 
topic. (e.g. ?[holder He] condemned [topic the law-
yer].?) For adjectives, the baseline marked the 
subject of a predicate adjective as a holder (e.g. 
?[holder I] was happy?). For the topics of adjec-
tives, the baseline picks a modified word if the 
target adjective is a modifier (e.g. ?That was a 
stupid [topic mistake]?.) and a subject word if the 
adjective is a predicate. ([topic The view] is 
breathtaking in January.) 
Result: Table 3 and 4 show evaluation results 
of our system and the baseline system respec-
tively. Our system performed much better than 
the baseline system in identifying topic and 
holder for both sets of sentences with verb target 
words and those with adjectives. Especially in 
recognizing topics of target opinion-bearing 
words, our system improved F-score from 30.4% 
to 66.5% for verb target words and from 38.2% 
to 70.3% for adjectives. It was interesting to see 
that the intuition that ?A subject of opinion-
bearing verb is a holder and an object is a topic? 
which we applied for the baseline achieved rela-
tively good F-score (56.9%). However, our sys-
tem obtained much higher F-score (78.7%). 
Holder identification task achieved higher F-
score than topic identification which implies that 
identifying topics of opinion is a harder task. 
We believe that there are many complicated 
semantic relations between opinion-bearing 
words and their holders and topics that simple 
relations such as subject and object relations are 
not able to capture. For example, in a sentence 
?Her letter upset me?, simply looking for the 
subjective and objective of the verb upset is not 
enough to recognize the holder and topic. It is 
necessary to see a deeper level of semantic rela-
Table 3. Precision (P), Recall (R), and F-
score (F) of Topic and Holder identification 
for opinion verbs (V) and adjectives (A) on 
Testset 1. 
 Topic  Holder  
 P (%) R (%) F (%) P (%) R (%) F (%)
V  69.1 64.0 66.5 81.9 75.7 78.7 
A  67.5 73.4 70.3 66.2 77.9 71.6 
 
Table 4. Baseline system on Testset 1. 
 Topic  Holder  
 P (%) R (%) F (%) P (%) R (%) F (%)
V 85.5 18.5 30.4 73.7 46.4 56.9 
A  68.2 26.5 38.2 12.0 49.1 19.3 
 
5
tions: ?Her letter? is a stimulus and ?me? is an 
experiencer of the verb upset.  
4.2 Experiments on Testset 2 
Gold Standard: Two humans 8  annotated 100 
sentences randomly selected from news media 
texts. Those news data is collected from online 
news sources such as The New York Times, UN 
Office for the Coordination of Humanitarian Af-
fairs, and BBC News 9 , which contain articles 
about various international affaires. Annotators 
identified opinion-bearing sentences with mark-
ing opinion word with its holder and topic if they 
existed. The inter-annotator agreement in identi-
fying opinion sentences was 82%.  
Baseline: In order to identify opinion-bearing 
sentences for our baseline system, we used the 
opinion-bearing word set introduced in Phase 1 
in Subsection 3.1. If a sentence contains an opin-
ion-bearing verb or adjective, the baseline sys-
tem started looking for its holder and topic. For 
holder and topic identification, we applied the 
                                                 
8 We refer them as Human1 and Human2 for the rest of this 
paper. 
9 www.nytimes.com, www.irinnews.org, and 
www.bbc.co.uk  
 
same baseline algorithm as described in Subsec-
tion 4.1 to Testset 2.  
Result: Note that Testset 1 was collected from 
sentences of opinion-related frames in FrameNet 
and therefore all sentences in the set contained 
either opinion-bearing verb or adjective. (i.e. All 
sentences are opinion-bearing) However, sen-
tences in Testset 2 were randomly collected from 
online news media pages and therefore not all of 
them are opinion-bearing. We first evaluated the 
task of opinion-bearing sentence identification. 
Table 5 shows the system results. When we mark 
all sentences as opinion-bearing, it achieved 43% 
and 38% of accuracy for the annotation result of 
Human1 and Human2 respectively. Our system 
performance (64% and 55%) is comparable with 
the unique assignment.  
We measured the holder and topic identifica-
tion system with precision, recall, and F-score. 
As we can see from Table 6, our system achieved 
much higher precision than the baseline system 
for both Topic and Holder identification tasks. 
However, we admit that there is still a lot of 
room for improvement. 
The system achieved higher precision for topic 
identification, whereas it achieved higher recall 
for holder identification. In overall, our system 
attained higher F-score in holder identification 
task, including the baseline system. Based on F-
score, we believe that identifying topics of opin-
ion is much more difficult than identifying hold-
ers. It was interesting to see the same phenome-
non that the baseline system mainly assuming 
that subject and object of a sentence are likely to 
be opinion holder and topic, achieved lower 
scores for both holder and topic identification 
tasks in Testset 2 as in Testset 1. This implies 
that more sophisticated analysis of the relation-
ship between opinion words (e.g. verbs and ad-
jectives) and their topics and holders is crucial.  
4.3 Difficulties in evaluation 
We observed several difficulties in evaluating 
holder and topic identification. First, the bound-
ary of an entity of holder or topic can be flexible. 
For example, in sentence ?Senator Titus Olupitan 
who sponsored the bill wants the permission.?, 
not only ?Senator Titus Olupitan? but also 
?Senator Titus Olupitan who sponsored the bill? 
is an eligible answer. Second, some correct hold-
ers and topics which our system found were 
evaluated wrong even if they referred the same 
entities in the gold standard because human an-
notators marked only one of them as an answer.  
Table 5. Opinion-bearing sentence identifica-
tion on Testset 2. (P: precision, R: recall, F: 
F-score, A: Accuracy, H1: Human1, H2: 
Human2) 
 P (%) R (%) F (%) A (%) 
H1 56.9 67.4 61.7 64.0 
H2 43.1 57.9 49.4 55.0 
 
 
Table 6: Results of Topic and Holder identi-
fication on Testset 2. (Sys: our system, BL: 
baseline) 
Topic Holder 
 
P(%) R(%) F(%) P(%) R(%) F(%)
H1 64.7 20.8 31.5 47.9 34.0 39.8
Sys 
H2 58.8 7.1 12.7 36.6 26.2 30.5
H1 12.5 9.4 10.7 20.0 28.3 23.4
BL 
H2 23.2 7.1 10.9 14.0 19.0 16.1
 
6
In the future, we need more annotated data for 
improved evaluation.   
5 Conclusion and Future Work 
This paper presented a methodology to identify 
an opinion with its holder and topic given a sen-
tence in online news media texts. We introduced 
an approach of exploiting semantic structure of a 
sentence, anchored to an opinion bearing verb or 
adjective. This method uses semantic role label-
ing as an intermediate step to label an opinion 
holder and topic using FrameNet data. Our 
method first identifies an opinion-bearing word, 
labels semantic roles related to the word in the 
sentence, and then finds a holder and a topic of 
the opinion word among labeled semantic roles. 
There has been little previous study in identi-
fying opinion holders and topics partly because it 
requires a great amount of annotated data. To 
overcome this barrier, we utilized FrameNet data 
by mapping target words to opinion-bearing 
words and mapping semantic roles to holders and 
topics. However, FrameNet has a limited number 
of words in its annotated corpus. For a broader 
coverage, we used a clustering technique to pre-
dict a most probable frame for an unseen word.  
Our experimental results showed that our sys-
tem performs significantly better than the base-
line. The baseline system results imply that opin-
ion holder and topic identification is a hard task. 
We believe that there are many complicated se-
mantic relations between opinion-bearing words 
and their holders and topics which simple rela-
tions such as subject and object relations are not 
able to capture. 
In the future, we plan to extend our list of 
opinion-bearing verbs and adjectives so that we 
can discover and apply more opinion-related 
frames. Also, it would be interesting to see how 
other types of part of speech such as adverbs and 
nouns affect the performance of the system. 
Reference 
Baker, Collin F. and Hiroaki Sato. 2003. The Frame-
Net Data and Software. Poster and Demonstration 
at Association for Computational Linguistics. Sap-
poro, Japan. 
Berger, Adam, Stephen Della Pietra, and Vincent 
Della Pietra. 1996. A maximum entropy approach 
to natural language processing, Computational Lin-
guistics, (22-1).  
Bethard, Steven, Hong Yu, Ashley Thornton, Va-
sileios Hatzivassiloglou, and Dan Jurafsky. 2004. 
Automatic Extraction of Opinion Propositions and 
their Holders, AAAI Spring Symposium on Explor-
ing Attitude and Affect in Text: Theories and Ap-
plications. 
Choi, Y., Cardie, C., Riloff, E., and Patwardhan, S. 
2005. Identifying Sources of Opinions with Condi-
tional Random Fields and Extraction Patterns. Pro-
ceedings of HLT/EMNLP-05. 
Esuli, Andrea and Fabrizio Sebastiani. 2005. Deter-
mining the semantic orientation of terms through 
gloss classification. Proceedings of CIKM-05, 14th 
ACM International Conference on Information and 
Knowledge Management, Bremen, DE, pp. 617-
624.  
Fillmore, C. Frame semantics and the nature of lan-
guage. 1976. In Annals of the New York Academy 
of Sciences: Conferences on the Origin and Devel-
opment of Language and Speech, Volume 280: 20-
32. 
Fleischman, Michael, Namhee Kwon, and Eduard 
Hovy. 2003. Maximum Entropy Models for Fra-
meNet Classification. Proceedings of EMNLP, 
Sapporo, Japan.  
Gildea, D. and Jurafsky, D. Automatic Labeling of 
semantic roles. 2002. In Computational Linguis-
tics. 28(3), 245-288. 
Harris, Zellig, 1954. Distributional structure. Word, 
10(23) :146--162. 
Hatzivassiloglou, Vasileios and Kathleen McKeown. 
1997. Predicting the Semantic Orientation of Ad-
jectives. Proceedings of 35th Annual Meeting of 
the Assoc. for Computational Linguistics (ACL-
97): 174-181 
Hatzivassiloglou, Vasileios and Wiebe, Janyce. 2000. 
Effects of Adjective Orientation and Gradability on 
Sentence Subjectivity. Proceedings of Interna-
tional Conference on Computational Linguistics 
(COLING-2000). Saarbr?cken, Germany. 
Hu, Minqing and Bing Liu. 2004. Mining and summa-
rizing customer reviews". Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery & Data Mining (KDD-2004), Seattle, 
Washington, USA. 
Kim, Soo-Min and Eduard Hovy. 2004. Determining 
the Sentiment of Opinions. Proceedings of COL-
ING-04. pp. 1367-1373. Geneva, Switzerland. 
Kim, Soo-Min and Eduard Hovy. 2005. Identifying 
Opinion Holders for Question Answering in Opin-
ion Texts. Proceedings of AAAI-05 Workshop on 
Question Answering in Restricted Domains 
Pang, Bo, Lillian Lee, and Shivakumar Vaithyana-
than. 2002. Thumbs up? Sentiment Classification 
using Machine Learning Techniques, Proceedings 
of EMNLP-2002. 
7
Pantel, Patrick and Dekang Lin. 2002. Discovering 
Word Senses from Text. Proceedings of ACM Con-
ference on Knowledge Discovery and Data Mining. 
(KDD-02). pp. 613-619. Edmonton, Canada. 
Popescu, Ana-Maria and Oren Etzioni. 2005. 
Extracting Product Features and Opinions from 
Reviews , Proceedings of HLT-EMNLP 2005. 
Riloff, Ellen, Janyce Wiebe, and Theresa Wilson. 
2003. Learning Subjective Nouns Using Extraction 
Pattern Bootstrapping. Proceedings of Seventh 
Conference on Natural Language Learning 
(CoNLL-03). ACL SIGNLL. Pages 25-32. 
Turney, Peter D. 2002. Thumbs up or thumbs down? 
Semantic orientation applied to unsupervised clas-
sification of reviews, Proceedings of ACL-02, 
Philadelphia, Pennsylvania, 417-424 
Wiebe, Janyce, Bruce M., Rebecca F., and Thomas P. 
O'Hara. 1999. Development and use of a gold stan-
dard data set for subjectivity classifications. Pro-
ceedings of ACL-99. University of Maryland, June, 
pp. 246-253. 
Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann. 
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. Proceedings of 
HLT/EMNLP 2005, Vancouver, Canada 
 
8
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 77?84,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Re-evaluating Machine Translation Results with Paraphrase Support 
Liang Zhou, Chin-Yew Lin, and Eduard Hovy 
University of Southern California 
Information Sciences Institute 
4676 Admiralty Way 
Marina del Rey, CA 90292-6695 
{liangz, cyl, hovy} @isi.edu 
      
 
  
 
Abstract 
In this paper, we present ParaEval, an 
automatic evaluation framework that uses 
paraphrases to improve the quality of 
machine translation evaluations. Previous 
work has focused on fixed n-gram 
evaluation metrics coupled with lexical 
identity matching. ParaEval addresses 
three important issues: support for para-
phrase/synonym matching, recall meas-
urement, and correlation with human 
judgments. We show that ParaEval corre-
lates significantly better than BLEU with 
human assessment in measurements for 
both fluency and adequacy. 
1 Introduction 
The introduction of automated evaluation proce-
dures, such as BLEU (Papineni et al, 2001) for 
machine translation (MT) and ROUGE (Lin and 
Hovy, 2003) for summarization, have prompted 
much progress and development in both of these 
areas of research in Natural Language Processing 
(NLP). Both evaluation tasks employ a compari-
son strategy for comparing textual units from 
machine-generated and gold-standard texts. Ide-
ally, this comparison process would be per-
formed manually, because of humans? abilities to 
infer, paraphrase, and use world knowledge to 
relate differently worded pieces of equivalent 
information. However, manual evaluations are 
time consuming and expensive, thus making 
them a bottleneck in system development cycles.  
BLEU measures how close machine-generated 
translations are to professional human transla-
tions, and ROUGE does the same with respect to 
summaries. Both methods incorporate the com-
parison of a system-produced text to one or more 
corresponding reference texts. The closeness be-
tween texts is measured by the computation of a 
numeric score based on n-gram co-occurrence 
statistics. Although both methods have gained 
mainstream acceptance and have shown good 
correlations with human judgments, their defi-
ciencies have become more evident and serious 
as research in MT and summarization progresses 
(Callison-Burch et al, 2006).   
Text comparisons in MT and summarization 
evaluations are performed at different text granu-
larity levels. Since most of the phrase-based, 
syntax-based, and rule-based MT systems trans-
late one sentence at a time, the text comparison 
in the evaluation process is also performed at the 
single-sentence level. In summarization evalua-
tions, there is no sentence-to-sentence corre-
spondence between summary pairs?essentially 
a multi-sentence-to-multi-sentence comparison, 
making it more difficult and requiring a com-
pletely different implementation for matching 
strategies. In this paper, we focus on the intrica-
cies involved in evaluating MT results and ad-
dress two prominent problems associated with 
the BLEU-esque metrics, namely their lack of 
support for paraphrase matching and the absence 
of recall scoring. Our solution, ParaEval, utilizes 
a large collection of paraphrases acquired 
through an unsupervised process?identifying 
phrase sets that have the same translation in an-
other language?using state-of-the-art statistical 
MT word alignment and phrase extraction meth-
ods. This collection facilitates paraphrase match-
ing, additionally coupled with lexical identity 
matching which is designed for comparing 
text/sentence fragments that are not consumed by 
paraphrase matching. We adopt a unigram count-
ing strategy for contents matched between sen-
tences from peer and reference translations. This 
unweighted scoring scheme, for both precision 
and recall computations, allows us to directly 
examine both the power and limitations of 
77
ParaEval.  We show that ParaEval is a more sta-
ble and reliable comparison mechanism than 
BLEU, in both fluency and adequacy rankings.  
This paper is organized in the following way: 
Section 2 shows an overview on BLEU and lexi-
cal identity n-gram statistics; we describe ParaE-
val?s implementation in detail in Section 3; the 
evaluation of ParaEval is shown in Section 4; 
recall computation is discussed in Section 5; in 
Section 6, we discuss the differences between 
BLEU and ParaEval when the numbers of refer-
ence translations change; and we conclude and 
discuss future work in Section 7.  
2  N-gram Co-occurrence Statistics 
Being an $8 billion industry (Browner, 2006), 
MT calls for rapid development and the ability to 
differentiate good systems from less adequate 
ones. The evaluation process consists of compar-
ing system-generated peer translations  to human 
written reference translations  and assigning a 
numeric score to each system. While human as-
sessments are still the most reliable evaluation 
measurements, it is not practical to solicit manual 
evaluations repeatedly while making incremental 
system design changes that would only result in 
marginal performance gains. To overcome the 
monetary and time constraints associated with 
manual evaluations, automated procedures have 
been successful in delivering benchmarks for 
performance hill-climbing with little or no cost.  
While a variety of automatic evaluation meth-
ods have been introduced, the underlining com-
parison strategy is similar?matching based on 
lexical identity. The most prominent implemen-
tation of this type of matching is demonstrated in 
BLEU (Papineni et al 2002). The remaining part 
of this section is devoted to an overview of 
BLEU, or the BLEU-esque philosophy.  
2 .1  The BL E U-esque Matching Philosophy 
The primary task that a BLEU-esque procedure 
performs is to compare n-grams from the peer 
translation with the n-grams from one or more 
reference translations and count the number of 
matches. The more matches a peer translation 
gets, the better it is.  
BLEU is a precision-based metric, which is 
the ratio of the number of n-grams from the peer 
translation that occurred in reference translations 
to the total number of n-grams in the peer trans-
lation. The notion of Modified n-gram Precisi on  
was introduced to detect and avoid rewarding 
false positives generated by translation systems. 
To gain high precision, systems could potentially 
over-generate ?good? n-grams, which occur mul-
tiple times in multiple references. The solution to 
this problem was to adopt the policy that an n-
gram, from both reference and peer translations, 
is considered exhausted after participating in a 
match. As a result, the maximum number of 
matches an n-gram from a peer translation can 
receive, when comparing to a set of reference 
translations, is the maximum number of times 
this n-gram occurred in any single reference 
translation. Papineni et al (2002) called this cap-
ping technique clipp ing.  Figure 1, taken from the 
original BLEU paper, demonstrates the computa-
tion of the modified unigram precision for a peer 
translation sentence.  
To compute the modified n-gram precision, 
P n, for a whole test set, including all translation 
segments (usually in sentences), the formula is: 
 
2 .2  Lack of Paraphrasing Support 
Humans are very good at finding creative ways 
to convey the same information. There is no one 
definitive reference translation in one language 
for a text written in another. Having acknowl-
edged this phenomenon, however natural it is, 
human evaluations on system-generated transla-
tions are much more preferred and trusted. How-
ever, what humans can do with ease puts ma-
chines at a loss. BLEU-esque procedures recog-
nize equivalence only when two n-grams exhibit 
the same surface-level representations, i.e. the 
same lexical identities. The BLEU implementa-
tion addresses its deficiency in measuring seman-
tic closeness by incorporating the comparison 
with multiple reference translations. The ration-
ale is that multiple references give a higher 
chance that the n-grams, assuming correct trans-
lations, appearing in the peer translation would 
be rewarded by one of the reference?s n-grams. 
The more reference translations used, the better 
Figure 1. Modified n-gram precision from
BLEU.
 
78
the matching and overall evaluation quality. Ide-
ally (and to an extreme), we would need to col-
lect a large set of human-written translations to 
capture all possible combinations of verbalizing 
variations before the translation comparison pro-
cedure reaches its optimal matching ability.  
One can argue that an infinite number of ref-
erences are not needed in practice because any 
matching procedure would stabilize at a certain 
number of references. This is true if precision 
measure is the only metric computed. However, 
using precision scores alone unfairly rewards 
systems that ?under-generate??producing un-
reasonably short translations. Recall measure-
ments would provide more balanced evaluations. 
When using multiple reference translations, if an 
n-gram match is made for the peer, this n-gram 
could appear in any of the references. The com-
putation of recall becomes difficult, if not impos-
sible. This problem can be reversed if there is 
crosschecking for phrases occurring across refer-
ences?paraphrase recognition. BLEU uses the 
calculation of a brevity penalty to compensate 
the lack of recall computation problem. The 
brevity penalty is computed as follows: 
 
Then, the BLEU score for a peer translation is 
computed as: 
 
BLEU?s adoption of the brevity penalty to off-
set the effect of not having a recall computation 
has drawn criticism on its crudeness in measur-
ing translation quality. Callison-Burch et al 
(2006) point out three prominent factors: 
? ``Synonyms and paraphrases are only 
handled if they are in the set of multiple 
reference translations [available].  
? The scores for words are equally 
weighted so missing out on content-
bearing material brings no additional pen-
alty.  
? The brevity penalty is a stop-gap meas-
ure to compensate for the fairly serious 
problem of not being able to calculate re-
call.? 
With the introduction of ParaEval, we will ad-
dress two of these three issues, namely the para-
phrasing problem and providing a recall meas-
ure.  
3  ParaEval for MT Evaluation 
3.1  Overview 
Reference translations are created from the same 
source text (written in the foreign language) to 
the target language. Ideally, they are supposed to 
be semantically equivalent, i.e. overlap com-
pletely. However, as shown in Figure 2, when 
matching based on lexical identity is used (indi-
cated by links), only half (6 from the left and 5 
from the right) of the 12 words from these two 
sentences are matched. Also, ?to? was a mis-
match. In applying paraphrase matching for MT 
evaluation from ParaEval, we aim to match all 
shaded words from both sentences. 
3 .2  Paraphrase Acquisition 
The process of acquiring a large enough collec-
tion of paraphrases is not an easy task. Manual 
corpus analyses produce domain-specific collec-
tions that are used for text generation and are 
application-specific. But operating in multiple 
domains and for multiple tasks translates into 
multiple manual collection efforts, which could 
be very time-consuming and costly. In order to 
facilitate smooth paraphrase utilization across a 
variety of NLP applications, we need an unsu-
pervised paraphrase collection mechanism that 
can be easily conducted, and produces para-
phrases that are of adequate quality and can be 
readily used with minimal amount of adaptation 
effort.  
Our method (Anonymous, 2006), also illus-
trated in (Bannard and Callison-Burch, 2005), to 
automatically construct a large domain-
independent paraphrase collection is based on the 
assumption that two different phrases of the 
same meaning may have the same translation in a 
Figure 2. Two reference translations. Grey
areas are matched by using BLEU.
79
foreign language. Phrase-based Statistical Ma-
chine Translation (SMT) systems analyze large 
quantities of bilingual parallel texts in order to 
learn translational alignments between pairs of 
words and phrases in two languages (Och and 
Ney, 2004). The sentence-based translation 
model makes word/phrase alignment decisions 
probabilistically by computing the optimal model 
parameters with application of the statistical es-
timation theory. This alignment process results in 
a corpus of word/phrase-aligned parallel sen-
tences from which we can extract phrase pairs 
that are translations of each other. We ran the 
alignment algorithm from (Och and Ney, 2003) 
on a Chinese-English parallel corpus of 218 mil-
lion English words, available from the Linguistic 
Data Consortium (LDC). Phrase pairs are ex-
tracted by following the method described in 
(Och and Ney, 2004) where all contiguous 
phrase pairs having consistent alignments are 
extraction candidates. Using these pairs we build 
paraphrase sets by joining together all English 
phrases that have the same Chinese translation. 
Figure 3 shows an example word/phrase align-
ment for two parallel sentence pairs from our 
corpus where the phrases ?blowing up? and 
?bombing? have the same Chinese translation. 
On the right side of the figure we show the para-
phrase set which contains these two phrases, 
which is typical in our collection of extracted 
paraphrases.  
Although our paraphrase extraction method is 
similar to that of (Bannard and Callison-Burch, 
2005), the paraphrases we extracted are for com-
pletely different applications, and have a broader 
definition for what constitutes a paraphrase. In 
(Bannard and Callison-Burch, 2005), a language 
model is used to make sure that the paraphrases 
extracted are direct substitutes, from the same 
syntactic categories, etc. So, using the example 
in Figure 3, the paraphrase table would contain 
only ?bombing? and ?bombing attack?. Para-
phrases that are direct substitutes of one another 
are useful when translating unknown phrases. 
For instance, if a MT system does not have the 
Chinese translation for the word ?bombing?, but 
has seen it in another set of parallel data (not in-
volving Chinese) and has determined it to be a 
direct substitute of the phrase ?bombing attack?, 
then the Chinese translation of ?bombing attack? 
would be used in place of the translation for 
?bombing?. This substitution technique has 
shown some improvement in translation quality 
(Callison-Burch et al, 2006).  
3 .3  The ParaEval Evaluation Procedure 
We adopt a two-tier matching strategy for MT 
evaluation in ParaEval. At the top tier, a para-
phrase match is performed on system-translated 
sentences and corresponding reference sentences. 
Then, unigram matching is performed on the 
words not matched by paraphrases. Precision is 
measured as the ratio of the total number of 
words matched to the total number of words in 
the peer translation.  
Running our system on the example in Figure 
2, the paraphrase-matching phase consumes the 
words marked in grey and aligns ?have been? 
and ?to be?, ?completed? and ?fully?, ?to date? 
and ?up till now?, and ?sequence? and ?se-
quenced?. The subsequent unigram-matching 
aligns words based on lexical identity.  
We maintain the computation of modified uni-
gram precisi on , defined by the BLEU-esque Phi-
losophy, in principle. In addition to clipping in-
dividual candidate words  with their correspond-
ing maximum reference counts (only for words 
not matched by paraphrases), we clip candidate 
paraphrases  by their maximum reference para-
phrase counts. So two completely different 
phrases in a reference sentence can be counted as 
two occurrences of one phrase. For example in 
Figure 4, candidate phrases ?blown up? and 
?bombing? matched with three phrases from the 
references, namely ?bombing? and two instances 
of ?explosion?. Treating these two candidate 
phrases as one (paraphrase match), we can see its 
clip is 2 (from Ref 1, where ?bomb ing? and ?ex-
plosion? are counted as two occurrences of a sin-
gle phrase). The only word that was matched by 
its lexical identity is ?was?. The modified uni-
gram precision calculated by our method is 4/5, 
where as BLEU gives 2/5.  
Figure 3. An example of the paraphrase extraction
process.
80
4  Evaluating ParaEval 
To be effective in MT evaluations, an automated 
procedure should be capable of distinguishing 
good translation systems from bad ones, human 
translations from systems?, and human transla-
tions of differing quality. For a particular evalua-
tion exercise, an evaluation system produces a 
ranking for system and human translations, and 
compares this ranking with one created by hu-
man judges (Turian et al, 2003). The closer a 
system?s ranking is to the human?s, the better the 
evaluation system is. 
4 .1  Validating ParaEval 
To test ParaEval?s ability, NIST 2003 Chinese 
MT evaluation results were used (NIST 2003). 
This collection consists of 100 source documents 
in Chinese, translations from eight individual 
translation systems, reference translations from 
four humans, and human assessments (on flu-
ency and adequacy). The Spearman rank-order 
coefficient is computed as an indicator of how 
close a system ranking is to gold-standard human 
ranking. It should be noted that the 2003 MT 
data is separate from the corpus that we extracted 
paraphrases from.  
For comparison purposes, BLEU 1  was also 
run. Table 1 shows the correlation figures for the 
two automatic systems with the NIST rankings 
on fluency and adequacy. The lower and higher 
95% confidence intervals are labeled as ?L-CI? 
and ?H-CI?. To estimate the significance of the 
rank-order correlation figures, we applied boot-
strap resampling to calculate the confidence in-
tervals.  In each of 1000 runs, systems were 
ranked based on their translations of 100 ran-
domly selected documents.  Each ranking was 
compared with the NIST ranking, producing a 
correlation score for each run. A t-test was then 
                                                
1 Results shown are from BLEU v.11 (NIST).  
performed on the 1000 correlation scores. In both 
fluency and adequacy measurements, ParaEval 
correlates significantly better than BLEU. The 
ParaEval scores used were precision scores. In 
addition to distinguishing the quality of MT sys-
tems, a reliable evaluation procedure must be 
able to distinguish system translations from hu-
mans? (Lin and Och, 2004). Figure 5 shows the 
overall system and human ranking. In the upper 
left corner, human translators are grouped to-
gether, significantly separated from the auto-
matic MT systems clustered into the lower right 
corner.  
4 .2  Implications to Word-alignment 
We experimented with restricting the para-
phrases being matched to various lengths. When 
allowing only paraphrases of three or more 
words to match, the correlation figures become 
stabilized and ParaEval achieves even higher 
correlation with fluency measurement to 0.7619 
on the Spearman ranking coefficient.   
This phenomenon indicates to us that the bi-
gram and unigram paraphrases extracted using 
SMT word-alignment and phrase extraction pro-
grams are not reliable enough to be applied to 
evaluation tasks. We speculate that word pairs 
extracted from (Liang et al, 2006), where a bidi-
rectional discriminative training method was 
used to achieve consensus for word-alignment 
Figure 4. ParaEval?s matching process.
 
BLEU ParaEval
Fluency 0.6978 0.7575
95% L-CI 0.6967 0.7553
95% H-CI 0.6989 0.7596
Adequacy 0.6108 0.6918
95% L-CI 0.6083 0.6895
95% H-CI 0.6133 0.694
Table 1. Ranking correlations with human
assessments.
 
Figure 5. Overall system and human ranking.
 
81
(mostly lower n-grams), would help to elevate 
the level of correlation by ParaEval.  
4 .3  Implications to Evaluating Paraphrase 
Quality 
Utilizing paraphrases in MT evaluations is also a 
realistic way to measure the quality of para-
phrases acquired through unsupervised channels. 
If a comparison strategy, coupled with para-
phrase matching, distinguishes good and bad MT 
and summarization systems in close accordance 
with what human judges do, then this strategy 
and the paraphrases used are of sufficient quality. 
Since our underlining comparison strategy is that 
of BLEU-1 for MT evaluation, and BLEU has 
been proven to be a good metric for their respec-
tive evaluation tasks, the performance of the 
overall comparison is directly and mainly af-
fected by the paraphrase collection.  
5  ParaEval?s Support for Recall Com-
putation 
Due to the use of multiple references and allow-
ing an n-gram from the peer translation to be 
matched with its corresponding n-gram from any 
of the reference translations, BLEU cannot be 
used to compute recall scores, which are conven-
tionally paired with precision to detect length-
related problems from systems under evaluation.  
5 .1  Using Single References for Recall 
The primary goal in using multiple references is 
to overcome the limitation in matching on lexical 
identity. More translation choices give more 
variations in verbalization, which could lead to 
more matches between peer and reference trans-
lations. Since MT results are generated and 
evaluated at a sentence-to-sentence level (or a 
segment level, where each segment may contain 
a small number of sentences) and no text con-
densation is employed, the number of different 
and correct ways to state the same sentence is 
small. This is in comparison to writing generic 
multi-document summaries, each of which con-
tains multiple sentences and requires significant 
amount of ?rewriting?. When using a large col-
lection of paraphrases while evaluating, we are 
provided with the alternative verbalizations 
needed. This property allows us to use single 
references to evaluate MT results and compute 
recall measurements.  
5 .2  Recall and Adequacy Correlations 
When validating the computed recall scores for 
MT systems, we correlate with human assess-
ments on adequacy only. The reason is that ac-
cording to the definition of recall, the content 
coverage in references, and not the fluency re-
flected from the peers, is being measured. Table 
2 shows ParaEval?s recall correlation with NIST 
2003 Chinese MT evaluation results on systems 
ranking. We see that ParaEval?s correlation with 
adequacy has improved significantly when using 
recall scores to rank than using precision scores.  
5 .3  Not All Single References are Created 
Equal 
Human-written translations differ not only in 
word choice, but also in other idiosyncrasies that 
cannot be captured with paraphrase recognition. 
So it would be presumptuous to declare that us-
ing paraphrases from ParaEval is enough to al-
low using just one reference translation to evalu-
ate. Using multiple references allow more para-
phrase sets to be explored in matching.  
In Table 3, we show ParaEval?s correlation 
figures when using single reference translations. 
E01?E04 indicate the sets of human translations 
used correspondingly.  
Notice that the correlation figures vary a great 
deal depending on the set of single references 
used. How do we differentiate human transla-
tions and know which set of references to use? It 
is difficult to quantify the quality that a human 
written translation reflects. We can only define 
?good? human translations as translations that 
are written not very differently from what other 
humans would write, and ?bad? translations as 
the ones that are written in an unconventional 
fashion. Table 4 shows the differences between 
the four sets of reference translations when com-
BLEU ParaEval
Adequacy 0.6108 0.7373
95% L-CI 0.6083 0.7368
95% H-CI 0.6133 0.7377
Table 2. ParaEval?s recall ranking correlation.
 
Table 3. ParaEval?s correlation (precision)
while using only single references.
E01 E02 E03 E04
Fluency 0.683 0.6501 0.7284 0.6192
95% L-CI 0.6795 0.6482 0.7267 0.6172
95% H-CI 0.6864 0.6519 0.73 0.6208
Adequacy 0.6308 0.5741 0.6688 0.5858
95% L-CI 0.6266 0.5705 0.665 0.5821
95% H-CI 0.635 0.5777 0.6727 0.5895
 
82
paring one set of references to the other three. 
The scores here are the raw ParaEval precision 
scores. E01 and E03 are better, which explains 
the higher correlations ParaEval has using these 
two sets of references individually, shown in Ta-
ble 3.  
6  Observation of Change in Number of 
References 
When matching on lexical identity, it is the gen-
eral consensus that using more reference transla-
tions would increase the reliability of the MT 
evaluation (Turian et al, 2003). It is expected 
that we see an improvement in ranking correla-
tions when moving from using one reference 
translation to more. However, when running 
BLEU for the NIST 2003 Chinese MT evalua-
tion, this trend is inverted, and using single refer-
ence translation gave higher correlation than us-
ing all four references, as illustrated in Table 5.  
Turian et al (2003) reports the same peculiar 
behavior from BLEU on Arabic MT evaluations 
in Figure 5b of their paper. When using three 
reference translations, as the number of segments 
(sentences usually) increases, BLEU correlates 
worse than using single references.  
Since the matching and underlining counting 
mechanisms of ParaEval are built upon the 
fundamentals of BLEU, we were keen to find out 
the differences, other than paraphrase matching, 
between the two methods when the number of 
reference translation changes. By following the 
description from the original BLEU paper, three 
incremental steps were set up for duplicating its 
implementation, namely modified unigram preci-
sion (MUP), geometric mean of MUP (GM), and 
multiplying brevity penalty with GM to get the 
final score (BP-BLEU). At each step, correla-
tions were computed for both using single- and 
multi- references, shown in Table 6a, b, and c. 
 Given that many small changes have been 
made to the original BLEU design, our replica-
tion would not produce the same scores from the 
current version of BLEU. Nevertheless, the in-
verted behavior was observed in fluency correla-
tions at the BP-BLEU step, not at MUP and GM. 
This indicates to us that the multiplication of the 
brevity penalty to balance precision scores is 
problematic. According to (Turian et al, 2003), 
correlation scores computed from using fewer 
references are inflated because the comparisons 
exclude the longer n-gram matches that make 
automatic evaluation procedures diverge from 
the human judgments. Using a large collection of 
paraphrases in comparisons allows those longer 
n-gram matches to happen even if single refer-
ences are used. This collection also allows 
ParaEval to directly compute recall scores, 
avoiding an approximation of recall that is 
problematic.  
ParaEval 95% L-CI 95% H- C I
E01 0.8086 0.8 0 .8172
E02 0.7383 0.7268 0.7497
E03 0.7839 0.7754 0.7923
E04 0.7742 0.7617 0.7866
Table 4. Differences among reference
translations (raw ParaEval precision
scores).  
6(a). System-ranking correlation when using modified
unigram precision (MUP) scores.
6(b). System-ranking correlation when using geometric mean
(GM) of MUPs.
6(c). System-ranking correlation when multiplying the
brevity penalty with GM.
Table 6. Incremental implementation of
BLEU and the correlation behavior at the
three steps: MUP, GM, and BP-BLEU.
MUP E01 E02 E03 E04 4 refs
Fluency 0.6597 0.6216 0.6923 0.4912 0.692
95% L-CI 0.6568 0.6189 0.6917 0.4863 0.6915
95% H-CI 0.6626 0.6243 0.6929 0.496 0.6925
Adequacy 0.5818 0.5459 0.6141 0.4602 0.6165
95% L-CI 0.5788 0.5432 0.6132 0.4566 0.6156
95% H-CI 0.5847 0.5486 0.6151 0.4638 0.6174
GM E01 E02 E03 E04 4 refs
Fluency 0.6633 0.6228 0.6925 0.4911 0.6922
95% L-CI 0.6604 0.6201 0.692 0.4862 0.6918
95% H-CI 0.6662 0.6255 0.6931 0.4961 0.6929
Adequacy 0.5817 0.548 0.615 0.4641 0.6159
95% L-CI 0.5813 0.5453 0.614 0.4606 0.615
95% H-CI 0.5871 0.5508 0.616 0.4676 0.6169
BP-BLEU E01 E02 E03 E04 4 refs
Fluency 0.6637 0.6227 0.6921 0.4947 0.5743
95% L-CI 0.6608 0.62 0.6916 0.4899 0.5699
95% H-CI 0.6666 0.6254 0.6927 0.4996 0.5786
Adequacy 0.5812 0.5486 0.5486 0.5486 0.6671
95% L-CI 0.5782 0.5481 0.5458 0.5458 0.6645
95% H-CI 0.5842 0.5514 0.5514 0.5514 0.6697
 
Table 5. BLEU?s correlating behavior with
multi- and single-reference.
BLEU E01 E02 E03 E04 4 refs
Fluency 0.7114 0.701 0.7084 0.7192 0.6978
95% L-CI 0.7099 0.6993 0.7065 0.7177 0.6967
95% H-CI 0.7129 0.7026 0.7102 0.7208 0.6989
Adequacy 0.644 0.6238 0.6535 0.675 0.6108
95% L-CI 0.6404 0.6202 0.6496 0.6714 0.6083
95% H-CI 0.6476 0.6274 0.6574 0.6786 0.6133
83
7  Conclusion and Future Work 
In this paper, we have described ParaEval, an 
automatic evaluation framework for measuring 
machine translation results. A large collection of 
paraphrases, extracted through an unsupervised 
fashion using SMT methods, is used to improve 
the quality of the evaluations. We addressed 
three important issues, the paraphrasing support, 
the computation of recall measurement, and pro-
viding high correlations with human judgments.  
Having seen that using paraphrases helps a 
great deal in evaluation tasks, naturally the next 
task is to explore the possibility in paraphrase 
induction. The question becomes how to use con-
textual information to calculate semantic close-
ness between two phrases. Can we expand the 
identification of paraphrases to longer ones, ide-
ally sentences?  
The problem in which content bearing words 
carry the same weights as the non-content bear-
ing ones is not addressed. From examining the 
paraphrase extraction process, it is unclear how 
to relate translation probabilities and confidences 
with semantic closeness. We plan to explore the 
parallels between the two to enable a weighted 
implementation of ParaEval.  
Reference 
Anonymous. 2006. Complete citation omitted due to 
the blind review process.  
Bannard, C. and C. Callison-Burch. 2005. Paraphras-
ing with bilingual parallel corpora. Proceedings of 
ACL-20 0 5 .  
Browner, J. 2006. The translator?s blues. 
http://www.slate.com/id/2133922/.  
Callison-Burch, P. Koehn, and M. Osborne. 2006. 
Improved statistical machine translation using 
paraphrases. In Proceedings of H L T / NA A CL-20 0 6 .  
Callison-Burch, C., M. Osborne, and P. Koehn. 2006. 
Re-evaluating the role of bleu in machine transla-
tion research. In Proceedings of EA CL-20 0 6 .  
Inkpen, D. Z. and G. Hirst. 2003. Near-synonym 
choice in natural language generation. Proceedings 
of R A NL P-20 0 3 .  
Leusch, G., N. Ueffing, and H. Ney. 2003. A novel 
string-to-string distance measure with applications 
to machine translation evaluation. In Proceedings 
of MT  Summit I X .  
Liang, P., B. Taskar, and D. Klein. Consensus of sim-
ple unsupervised models for word alignment. In 
Proceedings in H LT / N AA CL-2 0 0 6 .  
Lin, C.Y. and E. Hovy. 2003. Automatic evaluation of 
summaries using n-gram co-occurrence statistics. 
Proceedings of the H L T-20 0 3 . 
Lin, C.Y. and F. J. Och. 2004. Automatic evaluation 
of machine translation quality using longest com-
mon subsequence and skip-bigram statistics. Pro-
ceedings of A CL- 20 0 4 .  
Och, F. J. and H. Ney. 2003. A systematic comparison 
of various statistical alignment models. Computa-
tional Linguist ics , 29(1): 19?51, 2003.  
Och, F. J. and H. Ney. 2004. The alignment template 
approach to statistical machine translation. Compu-
tational L inguis tics , 30(4), 2004.  
Papineni, K., S. Roukos, T. Ward, and W. J. Zhu. 
2002. IBM research report Bleu: a method for 
automatic evaluation of machine translation I B M  
Research D iv is ion Technical Report , RC22176, 
2001.  
Turian, J. P., L. Shen, and I. D. Melamed. 2003. 
Evaluation of machine translation and its evalua-
tion. Proceedings of MT  Summit I X .  
 
84
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 120?121,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Adaptive Information Extraction for Complex Biomedical Tasks 
 
Donghui Feng            Gully Burns            Eduard Hovy 
Information Sciences Insitute 
University of Southern California 
Marina del Rey, CA, 90292 
{donghui, burns, hovy}@isi.edu 
Abstract 
Biomedical information extraction tasks are of-
ten more complex and contain uncertainty at 
each step during problem solving processes. We 
present an adaptive information extraction 
framework and demonstrate how to explore un-
certainty using feedback integration. 
1 Adaptive Information Extraction 
Biomedical information extraction (IE) tasks are 
often more complex and contain uncertainty at each 
step during problem solving processes.  
When in the first place the desired information is 
not easy to define and to annotate (even by humans), 
iterative IE cycles are to be expected. There might 
be gaps between the domain knowledge representa-
tion and computer processing ability. Domain 
knowledge might be hard to represent in a clear 
format easy for computers to process. Computer sci-
entists may need time to understand the inherent 
characteristics of domain problems so as to find ef-
fective approaches to solve them. All these issues 
mandate a more expressive IE process.  
In these situations, the traditional, straightfor-
ward, and one-pass problem-solving procedure, con-
sisting of definition-learning-testing, is no longer 
adequate for the solution.  
 
Figure 1. Adaptive information extraction. 
For more complex tasks requiring iterative cycles, 
an adaptive and extended IE framework has not yet 
been fully defined although variants have been ex-
plored. We describe an adaptive IE framework to 
characterize the activities involved in complex IE 
tasks. Figure 1 depicts the adaptive information ex-
traction framework.  
This procedure emphasizes one important adap-
tive step between the learning and application 
phases. If the IE result is not adequate, some adapta-
tions are required:  
Our study focuses on extracting tract-tracing ex-
periments (Swanson, 2004) from neuroscience arti-
cles. The goal of tract-tracing experiment is to chart 
the interconnectivity of the brain by injecting tracer 
chemicals into a region of the brain and then identi-
fying corresponding labeled regions where the tracer 
is transported to (Burns et al, 2007). Our work is 
performed in the context of NeuroScholar1, a project 
that aims to develop a Knowledge Base Manage-
ment System to benefit neuroscience research.  
We show how this new framework evolves to 
meet the demands of the more complex scenario of 
biomedical text mining. 
2 Feedback Integration 
This task requires finding the knowledge describing 
one or more experiments within an article as well as 
identifying desired fields within individual sen-
tences. Significant complexity arises from the pres-
ence of a variable number of records (experiments) 
in a single research article --- anywhere from one to 
many. 
 
Table 1. An example tract-tracing experiment. 
Table 1 provides an example of a tract-tracing ex-
periment. In this experiment, when the tracer was 
injected into the injection location ?the contralateral 
AVCN?, ?no labeled cells? was found in the label-
ing location ?the DCN?. 
For sentence level fields labeling, the perform-
ance of F1 score is around 0.79 (Feng et al, 2008). 
                                                          
1 http://www.neuroscholar.org/ 
120
We here show how the adaptive information extrac-
tion framework is applied to labeling individual sen-
tences. Please see Feng et al (2007) for the details 
of segmenting data records. 
2.1 Choosing Learning Approach via F1 
A natural way to label sentences is to obtain (by 
hand or learning) patterns characterizing each field 
(Feng et al, 2006; Ravichandran and Hovy, 2002). 
We tried to annotate field values for the biomedical 
data, but we found few intuitive clues that rich sur-
face text patterns could be learned with this corpus.  
This insight, Feedback F1, caused us to give up 
the idea of learning surface text patterns as usual, 
and switch to the Conditional Random Fields (CRF) 
(Lafferty et al, 2001) for labeling sentences instead. 
In contrast to fixed-order patterns, the CRF model 
provides a compact way to integrate different types 
of features for sequential labeling problems and can 
reach state-of-the-art level performance. 
2.2 Determining Knowledge Schema via F2 
In the first place, it is not clear what granularity of 
knowledge/information can be extracted from text 
and whether the knowledge representation is suitable 
for computer processing. We tried a series of ap-
proaches, using different levels of granularity and 
description, in order to obtain formulation suitable 
for IE. Figure 2 represents the evolution of the 
knowledge schema in our repeated activities.  
 
Figure 2. Knowledge schema evolution. 
 
Figure 3. System performance at stage 1 and 2. 
We initially started with the schema in the left-
most column but our pilot study showed that some 
fields, for example, ?label_type?, had too many 
variations in text description, making it very hard for 
CRF to learn clues about it. We then switched to the 
second schema but ended up seeing that the field 
?injectionSpread? needed more domain knowledge 
and was therefore not able to be learned by the sys-
tems. The last column is the final schema after those 
pilot studies. Figure 3 shows system performance 
(overall and the worst field) corresponding to the 
first and the second representation schemas. 
2.3 Exploring Features via F3 
To train CRF sentence labeling systems, it is vital to 
decide what features to use and how to prepare those 
features. Through the cycle of Feedback F3, we ex-
plored five categories of features and their combina-
tions to determine the best features for optimal 
system performance. Table 2 shows system per-
formance with different feature combinations.  
System Features Prec. Recall F_Score 
Baseline 0.4067 0.1761 0.2458 
Lexicon 0.5998 0.3734 0.4602 
Lexicon                   
+ Surface Words 
0.7663 0.7302 0.7478 
Lexicon                   
+ Surface Words     
+ Context Window 
0.7717 0.7279 0.7491 
Lexicon + Surface 
Words + Context 
Window + Window 
Words 
0.8076 0.7451 0.7751 
Lexicon + Surface 
Words + Context 
Window + Window 
Words + Depend-
ency Features  
0.7991 0.7828 0.7909 
Table 2. Precision, Recall, and F_Score for labeling. 
Please see Feng et al (2008) for the details of the 
sentence level extraction and feature preparation,  
3 Conclusions 
In this paper, we have shown an adaptive informa-
tion extraction framework for complex biomedical 
tasks. Using the iterative development cycle, we 
have been able to explore uncertainty at different 
levels using feedback integration.  
References  
Burns, G., Feng, D., and Hovy, E.H. 2007. Intelligent Approaches to 
Mining the Primary Research Literature: Techniques, Systems, and 
Examples. Book Chapter in Computational Intelligence in Bioinfor-
matics, Springer-Verlag, Germany. 
Feng, D., Burns, G., and Hovy, E.H. 2007. Extracting Data Records 
from Unstructured Biomedical Full Text. In Proc. of EMNLP 2007.  
Feng, D., Burns, G., Zhu, J., and Hovy, E.H. 2008. Towards Automated 
Semantic Analysis on Biomedical Research Articles. In Proc. of 
IJCNLP-2008. Poster Paper.  
Feng, D., Ravichandran, D., and Hovy, E.H. 2006. Mining and re-
ranking for answering biographical queries on the web. In Proc. of 
AAAI-2006. pp. 1283-1288. 
Lafferty, J., McCallum, A. and Pereira, F. 2001. Conditional random 
fields: probabilistic models for segmenting and labeling sequence 
data. In Proc. of ICML-2001. 
Ravichandran, D. and Hovy, E.H. 2002. Learning surface text patterns 
for a question answering system. In Proceedings of ACL-2002. 
Swanson, L.W. 2004. Brain maps: structure of the rat brain. 3rd edition, 
Elsevier Academic Press. 
121
Coling 2010: Poster Volume, pages 454?462,
Beijing, August 2010
What?s in a Preposition?
Dimensions of Sense Disambiguation for an Interesting Word Class
Dirk Hovy, Stephen Tratz, and Eduard Hovy
Information Sciences Institute
University of Southern California
{dirkh, stratz, hovy}@isi.edu
Abstract
Choosing the right parameters for a word
sense disambiguation task is critical to
the success of the experiments. We ex-
plore this idea for prepositions, an of-
ten overlooked word class. We examine
the parameters that must be considered in
preposition disambiguation, namely con-
text, features, and granularity. Doing
so delivers an increased performance that
significantly improves over two state-of-
the-art systems, and shows potential for
improving other word sense disambigua-
tion tasks. We report accuracies of 91.8%
and 84.8% for coarse and fine-grained
preposition sense disambiguation, respec-
tively.
1 Introduction
Ambiguity is one of the central topics in NLP. A
substantial amount of work has been devoted to
disambiguating prepositional attachment, words,
and names. Prepositions, as with most other word
types, are ambiguous. For example, the word in
can assume both temporal (?in May?) and spatial
(?in the US?) meanings, as well as others, less
easily classifiable (?in that vein?). Prepositions
typically have more senses than nouns or verbs
(Litkowski and Hargraves, 2005), making them
difficult to disambiguate.
Preposition sense disambiguation (PSD) has
many potential uses. For example, due to the
relational nature of prepositions, disambiguating
their senses can help with all-word sense disam-
biguation. In machine translation, different senses
of the same English preposition often correspond
to different translations in the foreign language.
Thus, disambiguating prepositions correctly may
help improve translation quality.1 Coarse-grained
PSD can also be valuable for information extrac-
tion, where the sense acts as a label. In a recent
study, Hwang et al (2010) identified preposition
related features, among them the coarse-grained
PP labels used here, as the most informative fea-
ture in identifying caused-motion constructions.
Understanding the constraints that hold for prepo-
sitional constructions could help improve PP at-
tachment in parsing, one of the most frequent
sources of parse errors.
Several papers have successfully addressed
PSD with a variety of different approaches (Rudz-
icz and Mokhov, 2003; O?Hara and Wiebe, 2003;
Ye and Baldwin, 2007; O?Hara and Wiebe, 2009;
Tratz and Hovy, 2009). However, while it is often
possible to increase accuracy by using a differ-
ent classifier and/or more features, adding more
features creates two problems: a) it can lead to
overfitting, and b) while possibly improving ac-
curacy, it is not always clear where this improve-
ment comes from and which features are actually
informative. While parameter studies exist for
general word sense disambiguation (WSD) tasks
(Yarowsky and Florian, 2002), and PSD accuracy
has been steadily increasing, there has been no
exploration of the parameters of prepositions to
guide engineering decisions.
We go beyond simply improving accuracy to
analyze various parameters in order to determine
which ones are actually informative. We explore
the different options for context and feature se-
1See (Chan et al, 2007) for the relevance of word sense
disambiguation and (Chiang et al, 2009) for the role of
prepositions in MT.
454
lection, the influence of different preprocessing
methods, and different levels of sense granular-
ity. Using the resulting parameters in a Maximum
Entropy classifier, we are able to improve signif-
icantly over existing results. The general outline
we present can potentially be extended to other
word classes and improve WSD in general.
2 Related Work
Rudzicz and Mokhov (2003) use syntactic and
lexical features from the governor and the preposi-
tion itself in coarse-grained PP classification with
decision heuristics. They reach an average F-
measure of 89% for four classes. This shows that
using a very small context can be effective. How-
ever, they did not include the object of the prepo-
sition and used only lexical features for classifi-
cation. Their results vary widely for the different
classes.
O?Hara and Wiebe (2003) made use of a win-
dow size of five words and features from the
Penn Treebank (PTB) (Marcus et al, 1993) and
FrameNet (Baker et al, 1998) to classify prepo-
sitions. They show that using high level fea-
tures, such as semantic roles, significantly aid dis-
ambiguation. They caution that using colloca-
tions and neighboring words indiscriminately may
yield high accuracy, but has the risk of overfit-
ting. O?Hara and Wiebe (2009) show compar-
isons of various semantic repositories as labels for
PSD approaches. They also provide some results
for PTB-based coarse-grained senses, using a five-
word window for lexical and hypernym features in
a decision tree classifier.
SemEval 2007 (Litkowski and Hargraves,
2007) included a task for fine-grained PSD (more
than 290 senses). The best participating system,
that of Ye and Baldwin (2007), extracted part-of-
speech and WordNet (Fellbaum, 1998) features
using a word window of seven words in a Max-
imum Entropy classifier. Tratz and Hovy (2009)
present a higher-performing system using a set of
20 positions that are syntactically related to the
preposition instead of a fixed window size.
Though using a variety of different extraction
methods, contexts, and feature words, none of
these approaches explores the optimal configura-
tions for PSD.
3 Theoretical Background
The following parameters are applicable to other
word classes as well. We will demonstrate their
effectiveness for prepositions.
Analyzing the syntactic elements of preposi-
tional phrases, one discovers three recurring ele-
ments that exhibit syntactic dependencies and de-
fine a prepositional phrase. The first one is the
governing word (usually a noun, verb, or adjec-
tive)2, the preposition itself, and the object of the
preposition.
Prepositional phrases can be fronted (?In May,
prices dropped by 5%?), so that the governor (in
this case the verb ?drop?) occurs later in the sen-
tence. Similarly, the object can be fronted (con-
sider ?a dessert to die for?).
In the simplest version, we can do classification
based only on the preposition and the governor or
object alone.3 Furthermore, directly neighboring
words can influence the preposition, mostly two-
word prepositions such as ?out of? or ?because
of?.
To extract the words discussed above, one can
either employ a fixed window size, (which has
to be large enough to capture the words), or se-
lect them based on heuristics or parsing informa-
tion. The governor and object can be hard to ex-
tract if they are fronted, since they do not occur in
their unusual positions relative to the preposition.
While syntactically related words improve over
fixed-window-size approaches (Tratz and Hovy,
2009), it is not clear which words contribute most.
There should be an optimal context, i.e., the small-
est set of words that achieves the best accuracy. It
has to be large enough to capture all relevant infor-
mation, but small enough to avoid noise words.4
We surmise that earlier approaches were not uti-
lizing that optimal context, but rather include a lot
of noise.
Depending on the task, different levels of sense
granularity may be used. Fewer senses increase
the likelihood of correct classification, but may in-
2We will refer to the governing word, irrespective of
class, as governor.
3Basing classification on the preposition alone is not fea-
sible, because of the very polysemy we try to resolve.
4It is not obvious how much information a sister-PP can
provide, or the subject of the superordinate clause.
455
correctly conflate prepositions. A finer granular-
ity can help distinguish nuances and better fit the
different contexts. However, it might suffer from
sparse data.
4 Experimental Setup
We explore the different context types (fixed win-
dow size vs. selective), the influence of the words
in that context, and the preprocessing method
(heuristics vs. parsing) on both coarse and fine-
grained disambiguation. We use a most-frequent-
sense baseline. In addition, we compare to the
state-of-the-art systems for both types of granu-
larity (O?Hara and Wiebe, 2009; Tratz and Hovy,
2009). Their results show what has been achieved
so far in terms of accuracy, and serve as a second
measure for comparison beyond the baseline.
4.1 Model
We use the MALLET implementation (McCal-
lum, 2002) of a Maximum Entropy classifier
(Berger et al, 1996) to construct our models. This
classifier was also used by two state-of-the-art
systems (Ye and Baldwin, 2007; Tratz and Hovy,
2009). For fine-grained PSD, we train a separate
model for each preposition due to the high num-
ber of possible classes for each individual prepo-
sition. For coarse-grained PSD, we use a single
model for all prepositions, because they all share
the same classes.
4.2 Data
We use two different data sets from existing re-
sources for coarse and fine-grained PSD to make
our results as comparable to previous work as pos-
sible.
For the coarse-grained disambiguation, we use
data from the POS tagged version of the Wall
Street Journal (WSJ) section of the Penn Tree-
Bank. A subset of the prepositional phrases in
this corpus is labelled with a set of seven classes:
beneficial (BNF), direction (DIR), extent (EXT),
location (LOC), manner (MNR), purpose (PRP),
and temporal (TMP). We extract only those prepo-
sitions that head a PP labelled with such a class
(N = 35, 917). The distribution of classes is
highly skewed (cf. Figure 1). We compare the
PTB class distrib
Page 1
LOC TMP DIR MNR PRP EXT BNF
0
2000
4000
6000
8000
10000
12000
14000
16000
18000 16995
10332
5414
1781 1071 280 44
classes
fre
qu
en
cy
Figure 1: Distribution of Class Labels in the WSJ
Section of the Penn TreeBank.
results of this task to the findings of O?Hara and
Wiebe (2009).
For the fine-grained task, we use data from
the SemEval 2007 workshop (Litkowski and Har-
graves, 2007), separate XML files for the 34 most
frequent English prepositions, comprising 16, 557
training and 8096 test sentences, each instance
containing one example of the respective prepo-
sition. Each preposition has between two and 25
senses (9.76 on average) as defined by The Prepo-
sition Project (Litkowski and Hargraves, 2005).
We compare our results directly to the findings
from Tratz and Hovy (2009). As in the original
workshop task, we train and test on separate sets.
5 Results
In this section we show experimental results for
the influence of word extraction method (parsing
vs. POS-based heuristics), context, and feature se-
lection on accuracy. Each section compares the
results for both coarse and fine-grained granular-
ity. Accuracy for the coarse-grained task is in all
experiments higher than for the fine-grained one.
5.1 Word Extraction
In order to analyze the impact of the extraction
method, we compare parsing versus POS-based
heuristics for word extraction.
Both O?Hara and Wiebe (2009) and Tratz and
Hovy (2009) use constituency parsers to prepro-
cess the data. However, parsing accuracy varies,
456
and the problem of PP attachment ambiguity in-
creases the likelihood of wrong extractions. This
is especially troublesome in the present case,
where we focus on prepositions.5 We use the
MALT parser (Nivre et al, 2007), a state-of-the-
art dependency parser, to extract the governor and
object.
The alternative is a POS-based heuristics ap-
proach. The only preprocessing step needed is
POS tagging of the data, for which we used the
system of Shen et al (2007). We then use simple
heuristics to locate the prepositions and their re-
lated words. In order to determine the governor
in the absence of constituent phrases, we consider
the possible governing noun, verb, and adjective.
The object of the preposition is extracted as first
noun phrase head to the right. This approach is
faster than parsing, but has problems with long-
range dependencies and fronting of the PP (e.g.,
the PP appearing earlier in the sentence than its
governor). word selection
Page 1
MALT 84.4 94.0
84.8 90.9
84.8 91.8
extraction method fine coarse
Heuristics
MALT + Heuristics
Table 1: Accuracies (%) for Word-Extraction Us-
ing MALT Parser or Heuristics.
Interestingly, the extraction method does not
significantly affect the final score for fine-grained
PSD (see Table 1). The high score achieved when
using the MALT parse for coarse-grained PSD
can be explained by the fact that the parser was
originally trained on that data set. The good re-
sults we see when using heuristics-based extrac-
tion only, however, means we can achieve high-
accuracy PSD even without parsing.
5.2 Context
We compare the effects of fixed window size ver-
sus syntactically related words as context. Table 2
shows the results for the different types and sizes
of contexts.6
5Rudzicz and Mokhov (2003) actually motivate their
work as a means to achieve better PP attachment resolution.
6See also (Yarowsky and Florian, 2002) for experiments
on the effect of varying window size for WSD.
context
Page 1
91.6 80.4
92.0 81.4
91.6 79.8
91.0 78.7
80.7 78.9
94.2 56.9
94.0 84.8
Context coarse fine
2-word window
3-word window
4-word window
5-word window
Governor, prep
Prep, object
Governor, prep, object
Table 2: Accuracies (%) for Different Context
Types and Sizes
The results show that the approach using both
governor and object is the most accurate one. Of
the fixed-window-size approaches, three words to
either side works best. This does not necessarily
reflect a general property of that window size, but
can be explained by the fact that most governors
and objects occur within this window size.7 This
dista ce can vary from corpus to corpus, so win-
dow size would have to be determined individu-
ally for each task. The difference between using
governor and preposition versus preposition and
object between coarse and fine-grained classifica-
tion might reflect the annotation process: while
Litkowski and Hargraves (2007) selected exam-
ples based on a search for governors8, most anno-
tators in the PTB may have based their decision
of the PP label on the object that occurs in it. We
conclude that syntactically related words present a
better context for classification than fixed window
sizes.
5.3 Features
Having established the context we want to use, we
now turn to the details of extracting the feature
words from that context.9 Using higher-level fea-
tures instead of lexical ones helps accounting for
sparse training data (given an infinite amount of
data, we would not need to take any higher-level
7Based on such statistics, O?Hara and Wiebe (2003) ac-
tually set their window size to 5.
8Personal communication.
9As one reviewer pointed out, these two dimensions are
highly interrelated and influence each other. To examine the
effects, we keep one dimension constant while varying the
other.
457
features into account, since every case would be
covered). Compare O?Hara and Wiebe (2009).
Following the prepocessing, we use a set of
rules to select the feature words, and then gen-
erate feature values from them using a variety
of feature-generating functions.10 The word-
selection rules are listed below.
Word-Selection Rules
? Governor from the MALT parse
? Object from the MALT parse
? Heuristically determined object of the prepo-
sition
? First verb to the left of the preposition
? First verb/noun/adjective to the left of the
preposition
? Union of (First verb to the left, First
verb/noun/adjective to the left)
? First word to the left
The feature-generating functions, many of
which utilize WordNet (Fellbaum, 1998), are
listed below. To conserve space, curly braces are
used to represent multiple functions in a single
line. The name of each feature is the combination
of the word-selection rule and the output from the
feature-generating function.
WordNet-based Features
? {Hypernyms, Synonyms} for {1st, all}
sense(s) of the word
? All terms in the definitions (?glosses?) of the
word
? Lexicographer file names for the word
? Lists of all link types (e.g., meronym links)
associated with the word
? Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
? All sentence frames for the word
? All {part, member, substance}-of holonyms
for the word
? All sentence frames for the word
Other Features
? Indicator that the word-finding rule found a
word
10Some words may be selected by multiple word-selection
rules. For example, the governor of the preposition may
be identified by the Governor from MALT parse rule, first
noun/verb/adjective to left, and the first word to the left rule.
? Capitalization indicator
? {Lemma, surface form} of the word
? Part-of-speech tag for the word
? General POS tag for the word (e.g. NNS?
NN, VBZ? VB)
? The {first, last} {two, three} letters of each
word
? Indicators for suffix types (e.g., de-
adjectival, de-nominal [non]agentive,
de-verbal [non]agentive)
? Indicators for a wide variety of other affixes
including those related to degree, number, or-
der, etc. (e.g., ultra-, poly-, post-)
? Roget?s Thesaurus divisions for the word
To establish the impact of each feature word on
the outcome, we use leave-one-out and only-one
evaluation.11 The results can be found in Table 3.
A word that does not perform well as the only at-
tribute may still be important in conjunction with
others. Conversely, leaving out a word may not
hurt performance, despite being a good single at-
tribute. word selection
Page 1
Word LOO LOO
92.1 80.1 84.3 78.9
93.4 94.2 84.9 56.3
92.0 77.9 85.0 62.1
92.1 78.7 84.3 78.5
92.1 78.4 84.5 81.0
92.0 78.8 84.4 77.2
91.9 93.0 84.9 56.8
91.8 ? 84.8 ?
coarse fine
Only Only
MALT governor
MALT object
Heuristics VB to left
Heur. NN/VB/ADJ to left
Heur. Governor Union
Heuristics word to left
Heuristics object
none
Table 3: Accuracies (%) for Leave-One-
Out (LOO) and Only-One Word-Extraction-Rule
Evaluation. none includes all words and serves for
comparison. Important words reduce accuracy for
LOO, but rank high when used as only rule.
Independent of the extraction method (MALT
parser or POS-based heuristics), the governor is
the most informative word. Combining several
heuristics to locate the governor is the best sin-
gle feature for fine-grained classification. The rule
looking only for a governing verb fails to account
11Since the feature words are not independent of one an-
other, neither of the two measures is decisive on its own.
458
full both
Page 1
Total Total Total Total
? ? 6 100.0 125 90.4 53 47.2
364 94.0 5 80.0 ? ? 74 93.2
23 69.6 78 65.4 ? ? 1 0.0
151 96.7 87 79.3 ? ? 7 71.4
53 79.2 841 92.5 of 1478 87.9 71 64.8
92 92.4 16 43.8 76 84.2 28 75.0
173 96.0 45 71.1 441 81.4 2287 90.8
? ? 5 80.0 58 91.4 15 53.3
? ? 58 70.7 out ? ? 90 68.9
50 80.0 358 93.9 ? ? 62 90.3
? ? 1 0.0 98 79.6 417 89.4
155 69.0 107 86.0 ? ? 6 83.3
84 100.0 232 84.5 per ? ? 3 100.0
? ? 2 50.0 82 65.9 ? ?
367 86.4 3078 92.0 ? ? 449 94.4
? ? 5 100.0 ? ? 2 0.0
? ? 420 91.7 208 48.1 364 69.0
20 90.0 384 83.3 ? ? 62 93.5
68 77.9 65 87.7 ? ? 3 100.0
? ? 94 71.3 to 572 89.7 3166 97.5
28 78.6 11 72.7 ? ? 55 65.5
29 100.0 4 100.0 102 97.1 2 100.0
? ? 1 0.0 ? ? 604 91.4
102 94.1 98 84.7 ? ? 2 50.0
? ? 45 64.4 ? ? 208 94.2
248 88.3 1341 87.5 up ? ? 20 75.0
down 153 81.7 16 56.2 ? ? 23 73.9
39 87.2 547 92.1 via ? ? 22 40.9
? ? 1 0.0 ? ? 1 100.0
478 82.4 1455 84.5 ? ? 3 33.3
578 85.5 1712 90.5 578 84.4 272 69.5
in 688 77.0 15706 95.0 ? ? 213 96.2
38 73.7 24 91.7 ? ? 69 63.8
297 86.2 415 80.0
Overall 8096 84.8 35917 91.8
fine coarse fine coarse
Prep Acc Acc Prep Acc Acc
aboard like
about near
above nearest
across next
after
against off
along on
alongside onto
amid
among outside
amongst over
around past
as
astride round
at since
atop than
because through
before throughout
behind till
below
beneath toward
beside towards
besides under
between underneath
beyond until
by
upon
during
except whether
for while
from with
within
inside without
into
Table 4: Accuracies (%) for Coarse and Fine-Grained PSD, Using MALT and Heuristics. Sorted by
preposition.
for noun governors, which consequently leads to
a slight improvement when left out.
Curiously, the word directly to the left is a bet-
ter single feature than the object (for fine-grained
classification). Leaving either of them out in-
creases accuracy, which implies that their infor-
mation can be covered by other words.
459
coarse both 2009
Page 1
Most Frequent Sense
f1 f1 f1
LOC 71.8 97.4 82.6 90.8 93.2 92.0 94.7 96.4 95.6
TMP 77.5 39.4 52.3 84.5 85.2 84.8 94.6 94.6 94.6
DIR 91.6 94.2 92.8 95.6 96.5 96.1 94.6 94.5 94.5
MNR 69.9 43.2 53.4 82.6 55.8 66.1 83.3 75.0 78.9
PRP 78.2 48.8 60.1 79.3 70.1 74.4 90.6 83.8 87.1
EXT 0.0 0.0 0.0 81.7 84.6 82.9 87.5 82.1 84.7
BNF 0.0 0.0 0.0 ? ? ? 75.0 34.1 46.9
O'Hara/Wiebe 2009 10-fold CV
Class prec rec prec rec prec rec
Table 5: Precision, Recall and F1 Results (%) for Coarse-Grained Classification. Comparison to O?Hara
and Wiebe (2009). Classes ordered by frequency
5.4 Comparison with Related Work
To situate our experimental results within the
body of work on PSD, we compare them to both
a most-frequent-sense baseline and existing work
for both granularities (see Table 6). The results
use a syntactically selective context of preposi-
tion, governor, object, and word to the left as
determined by combined extraction information
(POS tagging and parsing).
accuracies
Page 1
75.8 39.6  
89.3* 78.3**
93.9 84.8  
coarse fine
Baseline
Related Work
Our system
Table 6: Accuracies (%) for Different Classifi-
cations. Comparison with O?Hara and Wiebe
(2009)*, and Tratz and Hovy (2009)**.
Our system easily exceeds the baseline for both
coarse and fine-grained PSD (see Table 6). Com-
parison with related work shows that we achieve
an improvement of 6.5% over Tratz and Hovy
(2009), which is significant at p < .0001, and
of 4.5% over O?Hara and Wiebe (2009), which is
significant at p < .0001.
A detailed overview over all prepositions for
frequencies and accuracies of both coarse and
fine-grained PSD can be found in Table 4.
In addition to overall accuracy, O?Hara and
Wiebe (2009) also measure precision, recall and
F-measure for the different classes. They omitted
BNF because it is so infrequent. Due to different
training data and models, the two systems are not
strictly comparable, yet they provide a sense of
the general task difficulty. See Table 5. We note
that both systems perform better than the most-
frequent-sense baseline. DIR is reliably classified
using the baseline, while EXT and BNF are never
selected for any preposition. Our method adds
considerably to the scores for most classes. The
low score for BNF is mainly due to the low num-
ber of instances in the data, which is why it was
excluded by O?Hara and Wiebe (2009).
6 Conclusion
To get maximal accuracy in disambiguating
prepositions?and also other word classes?one
needs to consider context, features, and granular-
ity. We presented an evaluation of these parame-
ters for preposition sense disambiguation (PSD).
We find that selective context is better than
fixed window size. Within the context for prepo-
sitions, the governor (head of the NP or VP gov-
erning the preposition), the object of the prepo-
sition (i.e., head of the NP to the right), and the
word directly to the left of the preposition have
the highest influence.12 This corroborates the lin-
guistic intuition that close mutual constraints hold
between the elements of the PP. Each word syn-
tactically and semantically restricts the choice of
the other elements. Combining different extrac-
tion methods (POS-based heuristics and depen-
dency parsing) works better than either one in iso-
lation, though high accuracy can be achieved just
using heuristics. The impact of context and fea-
tures varies somewhat for different granularities.
12These will likely differ for other word classes.
460
Not surprisingly, we see higher scores for coarser
granularity than for the more fine-grained one.
We measured success in accuracy, precision, re-
call, and F-measure, and compared our results to
a most-frequent-sense baseline and existing work.
We were able to improve over state-of-the-art sys-
tems in both coarse and fine-grained PSD, achiev-
ing accuracies of 91.8% and 84.8% respectively.
Acknowledgements
The authors would like to thank Steve DeNeefe,
Victoria Fossum, and Zornitsa Kozareva for com-
ments and suggestions. StephenTratz is supported
by a National Defense Science and Engineering
fellowship.
References
Baker, C.F., C.J. Fillmore, and J.B. Lowe. 1998.
The Berkeley FrameNet Project. In Proceedings of
the 17th international conference on Computational
linguistics-Volume 1, pages 86?90. Association for
Computational Linguistics Morristown, NJ, USA.
Berger, A.L., V.J. Della Pietra, and S.A. Della Pietra.
1996. A maximum entropy approach to natural
language processing. Computational Linguistics,
22(1):39?71.
Chan, Y.S., H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine trans-
lation. In Annual Meeting ? Association For Com-
putational Linguistics, volume 45, pages 33?40.
Chiang, D., K. Knight, and W. Wang. 2009. 11,001
new features for statistical machine translation.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 218?226, Boulder, Colorado, June.
Association for Computational Linguistics.
Fellbaum, C. 1998. WordNet: an electronic lexical
database. MIT Press USA.
Hwang, J. D., R. D. Nielsen, and M. Palmer. 2010.
Towards a domain independent semantics: Enhanc-
ing semantic representation with construction gram-
mar. In Proceedings of the NAACL HLT Workshop
on Extracting and Using Constructions in Computa-
tional Linguistics, pages 1?8, Los Angeles, Califor-
nia, June. Association for Computational Linguis-
tics.
Litkowski, K. and O. Hargraves. 2005. The preposi-
tion project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Litkowski, K. and O. Hargraves. 2007. SemEval-2007
Task 06: Word-Sense Disambiguation of Preposi-
tions. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Marcus, M.P., M.A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of En-
glish: the Penn TreeBank. Computational Linguis-
tics, 19(2):313?330.
McCallum, A.K. 2002. MALLET: A Machine Learn-
ing for Language Toolkit. 2002. http://mallet. cs.
umass. edu.
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language En-
gineering, 13(02):95?135.
O?Hara, T. and J. Wiebe. 2003. Preposition semantic
classification via Penn Treebank and FrameNet. In
Proceedings of CoNLL, pages 79?86.
O?Hara, T. and J. Wiebe. 2009. Exploiting seman-
tic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
Rudzicz, F. and S. A. Mokhov. 2003. To-
wards a heuristic categorization of prepo-
sitional phrases in english with word-
net. Technical report, Cornell University,
arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Shen, L., G. Satta, and A. Joshi. 2007. Guided learn-
ing for bidirectional sequence classification. In Pro-
ceedings of the 45th Annual Meeting of the Associa-
tion of Computational Linguistics, volume 45, pages
760?767.
Tratz, S. and D. Hovy. 2009. Disambiguation of
preposition sense using linguistically motivated fea-
tures. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, Companion Volume: Student Re-
search Workshop and Doctoral Consortium, pages
96?100, Boulder, Colorado, June. Association for
Computational Linguistics.
Yarowsky, D. and R. Florian. 2002. Evaluating sense
disambiguation across diverse parameter spaces.
Natural Language Engineering, 8(4):293?310.
461
Ye, P. and T. Baldwin. 2007. MELB-YB: Preposition
Sense Disambiguation Using Rich Semantic Fea-
tures. In Proceedings of the 4th International Work-
shop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
462
Coling 2010: Poster Volume, pages 979?987,
Beijing, August 2010
Filling Knowledge Gaps in Text for Machine Reading 
Anselmo Pe?as Eduard Hovy
UNED NLP & IR Group USC Information Sciences Institute 
anselmo@lsi.uned.es hovy@isi.edu 
Abstract
Texts are replete with gaps, information 
omitted since authors assume a certain 
amount of background knowledge. We de-
fine the process of enrichment that fills 
these gaps.  We describe how enrichment 
can be performed using a Background 
Knowledge Base built from a large corpus.  
We evaluate the effectiveness of various 
openly available background knowledge 
bases and we identify the kind of informa-
tion necessary for enrichment.   
1 Introduction: Knowledge Gaps 
Automated understanding of connected text re-
mains an unsolved challenge in NLP.  In contrast 
to systems that harvest information from large 
collections of text, or that extract only certain 
pre-specified kinds of information from single 
texts, the task of extracting and integrating all 
information from a single text, and building a 
coherent and relatively complete representation 
of its content, is still beyond current capabilities.
A significant obstacle is the fact that text al-
ways omits information that is important, but that 
people recover effortlessly. Authors leave out 
information that they assume is known to their 
readers, since its inclusion (under the Gricean 
maxim of minimality) would carry an additional, 
often pragmatic, import. The problem is that sys-
tems cannot perform the recovery since they lack 
the requisite background knowledge and inferen-
tial machinery to use it.   
In this research we address the problem of au-
tomatically recovering such omitted information 
to ?plug the gaps? in text.  To do so, we describe 
the background knowledge required as well as a 
procedure of enrichment, which recognizes 
where gaps exist and fills them out using appro-
priate background knowledge as needed. We de-
fine enrichment as:
Def: Enrichment is the process of adding ex-
plicitly to a text?s representation the information 
that is either implicit or missing in the text.   
Central to enrichment is the source of the new 
knowledge. The use of Proposition Stores as 
Background Knowledge Bases (BKB) have been 
argued to be useful for improving parsing, co-
reference resolution, and word sense disambigua-
tion (Clark and Harrison 2009). We argue here 
that Proposition Stores are also useful for 
Enrichment and show how in Section 4. Howev-
er, we show in Section 5 that current BKB re-
sources such as TextRunner (Banko et al 2007) 
and DART (Clark and Harrison 2009) are not 
ideal for enrichment purposes. In some cases 
there is a lack in normalization. But the most im-
portant shortcoming is the lack in answering 
about instances, their possible classes, how they 
relate to propositions, and how different proposi-
tions are related through them. We propose easy 
to achieve extensions in this direction. We test 
this hypothesis building our own Proposition 
Store with the proposed extensions, and compare 
it with them for enrichment in the US football 
domain. 
To perform enrichment, we begin with an ini-
tial simple text representation and a Proposition 
Stores as a background knowledge base. We ex-
ecute a simple formalized procedure to select and 
attach appropriate elements from the BKB to the 
entities and implicit relations present in the initial 
text representation. Surprisingly, we find that 
some quite simple processing can be effective if 
we are able to contextualize the text under inter-
pretation.   
We describe in Section 2 our textual represen-
tations and in Section 3 the process of building 
the Proposition Store. Enrichment is described in 
Section 4, and an evaluation and comparison is 
performed in Section 5.   
2 Text Representation 
The initial, shallow, text representation must cap-
ture the first impression of what is going on in 
the text, possibly (unfortunately) losing some 
fragments. After the first impression, in accord 
with the purpose of the reading, we ?contextual-
979
ize? each sentence, expanding its initial represen-
tation with the relevant related background 
knowledge in our base. 
During this process of making explicit the im-
plicit semantic relations it will become apparent 
whether we need to recover some of the missed 
elements, whether we need to expand some oth-
ers, etc. So the process is identified with the 
growing of the context until deeper interpretation 
is possible. This approach resembles some well-
known theories such as the Theory of Relevance 
(Sperber and Wilson, 1995). The particular me-
thod we envisage is related to Interpretation as 
Abduction (Hobbs et al 1993). 
How can the initial information be represented 
so as to enable the context to grow into an inter-
pretation? We hypothesize that: 
1. Behind certain syntactic dependencies there 
are semantic relations. 
2. In the case of dependencies between nouns, 
this semantic relation can be made more ex-
plicit using verbs and/or prepositions. The 
knowledge base (our Proposition Store) must 
help us find them. 
We look for a semantic representation close 
enough to the syntactic representation we can 
obtain from the dependency graph. The main 
syntactic dependencies we want to represent in 
order to enable enrichment are: 
1. Dependencies between nouns such as noun-
noun compounds (nn) or possessive (poss). 
2. Dependencies between nouns and verbs, 
such as subject and object relations. 
3. Prepositions having two nouns as arguments. 
Then the preposition becomes the label for 
the relation, being the object of the preposi-
tion the target of the relation. 
We collapse the syntactic dependencies be-
tween verb, subject, and object into a single se-
mantic relation. Since we are assuming that the 
verb is the more explicit expression of a semantic 
relation, we fix this in the initial representation. 
The subject will be the source of the relation and 
the object will be the target of the relation. When 
the verb has more arguments we consider its ex-
pansion as a new node as referred in Section 4.4. 
Figure 1 shows the initial minimal representa-
tion for the sentence we will use for our discus-
sion: ?San Francisco's Eric Davis intercepted a 
Steve Walsh pass on the next series to set up a 
seven-yard Young touchdown pass to Brent 
Jones?. Notice that some pieces of the text are 
missing in the initial representation of the text, as 
for example ?on the next series? or ?seven-
yard?.
3 Background Knowledge Base  
We will use a Proposition Stores as a Back-
ground Knowledge Base (BKB). We built it from 
a collection of 30,826 New York Times articles 
about US football, similar to the kind of texts we 
want to interpret.  We parsed the collection using 
a standard dependency parser (Marneffe and 
Manning, 2008; Klein and Maning, 2003) and, 
after collapsing some syntactic dependencies, 
obtained 3,022,305 raw elements in the BKB.  
3.1 Types of elements in the BKB 
We distinguish three kinds of elements in our 
Background Knowledge Base: Entities, Proposi-
tions, and Lexical relations. All three have asso-
ciated their frequency in the reference collection. 
Entities: We distinguish between entity classes 
and entity instances: 
1. Entity classes: Entity classes are denoted by 
nouns. We don?t restrict classes to any par-
ticular predefined set. In addition, we intro-
duce two special classes: Person and Group. 
These two classes are related to the use of 
pronouns in text. Pronouns ?I?, ?he? and 
?she? are linked to class Person. Pronouns 
?we? and ?they? are linked to class Group. 
For example, the occurrence of the pronoun 
?he? in ?He threw a pass? would produce an 
additional count of the proposition ?per-
son:throw:pass?. 
set up to 
Young 
Brent
Jones touchdown 
pass2 
Figure 1. Initial text representation.
nn 
Steve
Walsh 
Eric 
Davis
pass1 
intercept
nn nn 
San Francisco 
poss 
980
2. Entity Instances: Entity instances are indi-
cated by proper nouns. Proper nouns are 
identified by the part of speech tagging. 
Some of these instances will participate in 
the ?has-instance? relation (see below).   
When they participate in a proposition they 
produce proposition instances. 
Propositions: Following Clark and Harrison 
(2009) we call propositions the tuples of words 
that have some determined pattern of syntactic 
relations among them. We focus on NVN, 
NVNPN and NPN proposition types. For exam-
ple, a NVNPN proposition is a full instantiation 
of: Subject:Verb:Object:Prep:Complement.
The first three elements are the subject, the 
verb and the direct object. Fourth is the preposi-
tion that attaches the PP complement to the verb. 
For simplicity, indirect objects are considered as 
a Complement with the preposition ?to?. 
The following are the most frequent NVN 
propositions in the BKB ordered by frequency. 
NVN 2322 'NNP':'beat':'NNP' 
NVN 2231 'NNP':'catch':'pass' 
NVN 2093 'NNP':'throw':'pass' 
NVN 1799 'NNP':'score':'touchdown' 
NVN 1792 'NNP':'lead':'NNP' 
NVN 1571 'NNP':'play':'NNP' 
NVN 1534 'NNP':'win':'game' 
NVN 1355 'NNP':'coach':'NNP' 
NVN 1330 'NNP':'replace':'NNP' 
NVN 1322 'NNP':'kick':'goal' 
The ?NNP? tag replaces specific proper nouns 
(instances) found in the proposition.  
When a sentence has more than one comple-
ment, a new occurrence is counted for each com-
plement. For example, given the sentence 
?Steve_Walsh threw a pass to Brent_Jones in the 
first quarter?, we would add a count to each of 
the following propositions: 
Steve_Walsh:throw:pass 
Steve_Walsh:throw:pass:to:Brent_Jones 
Steve_Walsh:throw:pass:in:quarter 
Notice that we include only the heads of the 
noun phrases in the propositions. 
We call proposition classes the propositions 
that only involve instance classes (e.g., ?per-
son:throw:pass?), and proposition instances
those that involve at least one entity instance 
(e.g., ?Steve_Walsh:throw:pass?).
Proposition instances are useful for the track-
ing of a entity instance. For example, 
?'Steve_Walsh':'supplant':'John_Fourcade':'as':'
quarterback'?. When a proposition instance is 
found, it is stored also as a proposition class re-
placing the proper nouns by a special word 
(NNP) to indicate the presence of an entity in-
stance. The enrichment of the text is based on the 
use of most frequent proposition classes. 
Lexical Relations: We make use of very general 
patterns considering appositions and copula 
verbs (detected by the Stanford parser) in order 
to extract ?is?, and ?has-instance? relations: 
1. Is: between two entity classes. They denote a 
kind of identity between both entity classes, 
but not in any specific hierarchical relation 
such as hyponymy. Neither is a relation of 
synonymy. As a result, it is somehow a kind 
of underspecified relation that groups those 
more specific. For example, if we ask the 
BKB what a ?receiver? is, the most frequent 
relations are: 
290 'person':is:'receiver' 
29 'player':is:'receiver' 
16 'pick':is:'receiver' 
15 'one':is:'receiver' 
14 'receiver':is:'target' 
8 'end':is:'receiver' 
7 'back':is:'receiver' 
6 'position':is:'receiver' 
The number indicates the frequency of the 
relation in the collection. 
2. Has-instance: between an entity class and an 
entity instance. For example, if we ask for 
instances of team, the top instances with 
more support in the collection are: 
192 'team':has-instance:'Jets' 
189 'team':has-instance:'Giants' 
43 'team':has-instance:'Eagles' 
40 'team':has-instance:'Bills' 
36 'team':has-instance:'Colts' 
35 'team':has-instance:'Miami' 
But we can ask also for the possible classes of 
an instance. For example, all the entity classes 
for ?Eric_Davis? are: 
12 'cornerback':has-instance:'Eric_Davis' 
1 'hand':has-instance:'Eric_Davis' 
1 'back':has-instance:'Eric_Davis'  
We still work on other lexical relations such 
as ?part-of? and ?is-value-of?. For example, the 
most frequent ?is-value-of? relations are: 
5178 '[0-9]-[0-9]':is-value-of:'lead' 
3996 '[0-9]-[0-9]':is-value-of:'record' 
2824 '[0-9]-[0-9]':is-value-of:'loss' 
1225 '[0-9]-[0-9]':is-value-of:'season' 
981
4 Enrichment operations 
The goal of the following enrichment operations 
is to make explicit what kind of semantic rela-
tions and entity classes are involved in the text. 
4.1 Fusion of nodes 
Sometimes, the syntactic dependency ties two or 
more words that form a single concept. This is 
the case with multiword terms such as ?tight
end?, ?field goal?, ?running back?, etc. In these 
cases, the meaning of the compound is beyond 
the syntactic dependency. Thus, we shouldn?t 
look for its explicit meaning. Instead, we fuse the 
nodes into a single one. 
The question is whether the fusion of the 
words into a single expression allows or not the 
consideration of possible paraphrases. For exam-
ple, in the case of ?field:nn:goal?, we don?t find 
other ways to express the concept in the BKB. 
However, in the case of ?touchdown:nn:pass? we 
can find, for example, ?pass:for:touchdown? a 
significant amount of times, and we want to iden-
tify them as equivalent expressions. 
4.2 Building context for instances 
Suppose we wish to determine what kind of enti-
ty ?Steve Walsh? is in the context of the syntactic 
dependency ?Steve_Walsh:nn:pass?. First, we 
look into the BKB for the possible entity classes 
of Steve_Walsh previously found in the collec-
tion. In this particular case, the most frequent 
class is ?quarterback?: 
40 'quarterback':has-instance:'Steve_Walsh' 
2 'junior':has-instance:'Steve_Walsh' 
But what happens if we see ?Steve_Walsh? for 
the first time? Then we need to take into account 
the classes shared by other instances in the same 
syntactic context. The most frequent are ?Mari-
no?, ?Kelly?, ?Elway?, etc. From them we are 
able to infer the most plausible class for the new 
entity. In our example, quarterback:
20 'quarterback':has-instance:'Marino' 
6 'passer':has-instance:'Marino' 
?
17 'quarterback':has-instance:'Kelly' 
6 'passer':has-instance:'Kelly' 
?
16 'quarterback':has-instance:'Elway' 
9 'player':has-instance:'Elway' 
4.3 Building context for dependencies 
Now we want to determine the meaning behind 
such syntactic dependencies as: 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,
?Young:nn:pass? or ?pass:to:Brent_Jones?. 
We have two ways for adding more meaning 
to these syntactic dependencies: find the most 
appropriate prepositions to describe them, and 
find the most appropriate verbs. Whether one, the 
other, or both is useful has to be determined dur-
ing the reasoning system development. 
Finding the prepositions 
Several types of propositions in the BKB involve 
prepositions. The most relevant are NPN and 
NVNPN. In the case of ?touchdown:nn:pass?,
?for? is clearly the best interpretation: 
NPN 712 'pass':'for':'touchdown' 
NPN 24 'pass':'include':'touchdown' 
In the case of ?Steve_Walsh:nn:pass? and 
?Young:nn:pass?, since we know they are quar-
terbacks, we can ask for all the prepositions be-
tween ?pass? and ?quarterback?:
NPN 23 'pass':'from':'quarterback' 
NPN 14 'pass':'by':'quarterback' 
If we don?t have any evidence on the instance 
class, and we know only that they are instances, 
the pertinent query to the BKB obtains: 
NPN 1305 'pass':'to':'NNP' 
NPN 1085 'pass':'from':'NNP' 
NPN 147 'pass':'by':'NNP' 
In the case of ?Young:nn:pass? (in ?Young
pass to Brent Jones?), there exists already the 
preposition ?to? (?pass:to:Brent_Jones?), so the 
most promising choice becomes the second, 
?pass:from:Young?, which has one order of 
magnitude more occurrences than its successor. 
In the case of ?Steve_Walsh:nn:pass? (in ?Eric 
Davis intercepted a Steve Walsh pass?) we can 
use additional information: we know that ?Er-
ic_Davis:intercept:pass?. So, we can try to find 
the appropriate preposition using NVNPN propo-
sitions in the following way: 
?Eric_Davis:intercept:pass:P:Steve_Walsh? 
Asking the BKB about the propositions that 
involve two instances with ?intercept? and 
?pass?, we obtain: 
NVNPN 48 'NNP':'intercept':'pass':'by':'NNP' 
982
NVNPN 26 'NNP':'intercept':'pass':'at':'NNP' 
NVNPN 12 'NNP':'intercept':'pass':'from':'NNP' 
We could also query the BKB with the classes 
we have already found for ?Eric_Davis? (cor-
nerback, player, person):
NVNPN 11 'person':'intercept':'pass':'by':'NNP' 
NVNPN 4 'person':'intercept':'pass':'at':'NNP' 
NVNPN 2 'person':'intercept':'pass':'in':'NNP' 
NVNPN 2 'person':'intercept':'pass':'against':'NNP' 
NVNPN 1 'cornerback':'intercept':'pass':'by':'NNP' 
All these queries accumulate evidence over the 
preposition ?by? (?pass:by:Steve_Walsh?).
Finding the verbs 
The next exercise is to find a verb able to give 
meaning to syntactic dependencies such as 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?,
?Young:nn:pass? or ?pass:to:Brent_Jones?. 
We can ask the BKB what instances (NNP) do 
with passes. The most frequent propositions are: 
NVN 2241 'NNP':'catch':'pass' 
NVN 2106 'NNP':'throw':'pass' 
NVN 844 'NNP':'complete':'pass' 
NVN 434 'NNP':'intercept':'pass' 
?
NVNPN 758 'NNP':'throw':'pass':'to':'NNP' 
NVNPN 562 'NNP':'catch':'pass':'for':'yard' 
NVNPN 338 'NNP':'complete':'pass':'to':'NNP' 
NVNPN 255 'NNP':'catch':'pass':'from':'NNP' 
Considering the evidence of ?Brent_Jones? being 
instance of ?end? (tight end), if we ask the BKB 
about the most frequent relations between ?end?
and ?pass? we find: 
NVN 28 'end':'catch':'pass' 
NVN 6 'end':'drop':'pass' 
So, in this case, the BKB suggests that the 
syntactic dependency ?pass:to:Brent_Jones?
means ?Brent Jones is an end catching a pass?.
Or in other words, that ?Brent_Jones? has a role 
of ?catch-ER? with respect to ?pass?.
If we want to accumulate more evidence on 
this we can consider NVNPN propositions in-
cluding ?touchdown?. We only find evidence for 
the most general classes (NNP and person):
NVNPN 189 NNP:'catch':'pass':'for':'touchdown' 
NVNPN 26 NNP:'complete':'pass':'for':'touchdown' 
NVNPN 84 person:catch:pass:for:touchdown 
NVNPN 18 person:complete:pass:for:touchdown 
This means that when we have ?touchdown?, we 
don?t have counts for the second option 
?Brent_Jones:drop:pass?, while ?catch? be-
comes stronger. 
In the case of ?Steve_Walsh:nn:pass? we hy-
pothesize that ?Steve_Walsh? is a ?quarterback?. 
Asking the BKB about the most plausible rela-
tion between a quarterback and a pass we find: 
NVN 98 'quarterback':'throw':'pass' 
NVN 27 'quarterback':'complete':'pass' 
Again, if we take into account that it is a 
?touchdown:nn:pass?, then only the second op-
tion ?Steve_Walsh:complete:pass? is consistent 
with the NVNPN propositions. So, in this case, 
the BKB suggests that the syntactic dependency 
?Steve_Walsh:nn:pass? means ?Steve_Walsh is a 
quarterback completing a pass?. 
Finally, with respect to ?touchdown:nn:pass?, 
we can ask about the verbs that relate them: 
NVN 14 'pass':'set_up':'touchdown' 
NVN 6 'pass':'score':'touchdown' 
NVN 5 'pass':'produce':'touchdown' 
Figure 2 shows the resulting enrichment after 
the process described. 
4.4 Expansion of relations 
Sometimes, the sentence shows a verb with more 
than two arguments. In our example, we have 
?Eric_David:intercept:pass:on:series?. In these 
cases, relations can be expanded into new nodes. 
Following our example, the new node is the 
eventuality of ?intercept? (?intercept-ION?), 
?Eric_Davis? is the ?intercept-ER? and ?pass? is 
the ?intercept-ED?. Then, the missing informa-
tion is attached to the new node (see Figure 3).  
In addition, we can proceed with the expan-
sion of the context considering this new node. 
For example, we are working with the hypothesis 
that ?Steve_Walsh? is an instance of ?quarter-
back? and thus, its most plausible relations with 
?pass? are ?throw? and ?complete?. However, 
now we can ask about the most frequent relation 
between ?quarterback? and a nominalization of 
catch (28) 
drop (6)
throw (98)
complete (27) 
by for 
has-instance (12) has-instance (33) 
to
quarterback end
Young Brent Jones touchdown 
pass
Figure 2. Enrichment of the noun phrase: ?Young 
touchdown pass to Brent Jones?
983
?intercept?. The most frequent proposition is 
?quarterback:throw:interception?, supported 35 
times in the collection. In this way, we have in-
ferred that the nominalization for the eventuality 
of intercept is interception (in our documents). 
Two further actions are possible: reinforce the 
hypothesis of ?throw:pass? instead of ?com-
plete:pass? and add the hypothesis that 
?Steve_Walsh:throw:interception?.
Finally, notice that since ?set_up? doesn?t 
need to accommodate more arguments, we can 
maintain the collapsed edge. 
4.5 Constraining the interpretations 
Some of the inferences being performed are local 
in the sense that they involve only an entity and a 
relation. However, these local inferences must be 
coherent both with the sentence and the complete 
document. To ensure this coherence we can use 
additional information as a way to constrain dif-
ferent hypotheses. In section 4.3 we showed the 
use of NVNPN propositions to constrain NVN 
ones. Another example is the case of ?Er-
ic_Davis:intercept:pass?. We can ask the BKB 
for the entity classes that participate in such kind 
of proposition: 
NVN 75 'person':'intercept':'pass' 
NVN 14 'cornerback':'intercept':'pass' 
NVN 11 'defense':'intercept':'pass' 
NVN 8 'safety':'intercept':'pass' 
NVN 7 'group':'intercept':'pass' 
So the local inference for the kind of entity 
?Eric_Davis? is (cornerback) must be coherent 
with the fact that it intercepted a pass. In this 
case ?cornerback? and ?person? are properly 
reinforced. In some sense, we are using these 
additional constrains as selectional preferences. 
5 Evaluation
Properly evaluating the enrichment process is 
very difficult.  Ideally, one would compare the 
output of an enrichment engine?a text graph 
fully fleshed out with additional knowledge?to 
a gold-standard graph containing all relevant in-
formation explicitly, and measure Recall and 
Precision of the links added by enrichment.  But 
since we have no gold standard examples, and it 
is unclear how much knowledge should be in-
cluded manually if one were to try to build some, 
two options remain: extrinsic evaluations and 
measuring the utility of the BKB in providing 
knowledge. We are in the process of performing 
an extrinsic evaluation, by measuring how much 
QA about the text read improves using the 
enriched representation.  We report here the re-
sults of comparing the utility, for enrichment 
purposes, of two other publicly available back-
ground knowledge bases: DART (Clark and Har-
rison, 2009) and TextRunner (Banko et al 2007). 
5.1 Ability to answer about instances 
As shown in our examples, BKBs need the abili-
ty to answer about instances and their classes. 
The BKBs don?t need to be completely popu-
lated, but at least have enough instance-class at-
tachments in order to allow analogy. 
Neither DART nor TextRunner allow asking 
about possible classes for a particular instance. 
This is out of the scope of TextRunner. In 
DART, instances are replaced by one of three 
basic categories (person, place, organization). 
Although storing the original proper nouns at-
tached to the assigned class would be 
straightforward, these three general classes are 
not enough to support inference. This leads us to 
the next ability. 
5.2 Ability to discover new classes and 
relations 
While quarterbacks throw passes, ends usually 
catch or drop them. As we have shown in our 
examples, classifying them as ?person? or even 
?player? is not specific enough for enrichment. 
Using a predefined set of entity classes doesn?t 
seem a good approach for midterm goals. First, 
human abstraction is not correlated with the ap-
propriate granularity level that enable recovering 
intercept-ER 
throw (98) 
complete (27)
on 
has-instance (17) poss 
intercept
quarterback San Francisco 
Steve
Walsh 
Eric 
Davis
intercept-ION 
pass
Figure 3. Expansion of ?intercept? relation 
intercept-ED 
throw (35) 
series 
nn 
984
of relevant background knowledge. Second, an-
notation will be needed for training.   
In our Proposition Stores, we count simply 
what is explicitly said in the texts about our in-
stances. This seems correlated to an appropriate 
level of granularity. Furthermore, an instance can 
be attached to several classes that can be compat-
ible (quarterback, player, person, leader, veteran, 
etc.). Frequencies tell us the classes we have to 
consider in the first place in order to find a cohe-
rent interpretation of the text. 
5.3 Ability to constrain interpretation 
and accumulate evidence 
Enrichment must be guided by the coherence of 
the ensuing interpretation. For this reason BKBs 
must allow different types of queries over the 
same elements. The aim is to constrain as much 
as possible the relations we recover to the ones 
that give a coherent interpretation of the text. 
As shown in our example, we require the abili-
ty to ask different syntactic contexts/structures 
(NN, NVNPN, etc.), not only NVN (subject-
verb-object). Achieving this is very difficult for 
approaches that don?t use parsing.   
5.4 Ability to digest enough knowledge 
adapted to the domain 
None of the abilities discussed above are relevant 
if the BKB doesn?t contain enough knowledge 
about the domain in which we want to enrich 
documents. To evaluate, we ran three simple 
queries related to the US football domain in or-
der to assess the suitability of the BKBs for 
enrichment: What do quarterbacks do with 
passes? What do persons do with passes? Who 
intercepts passes? Table 1 shows the results ob-
tained with DART, TextRunner and our BKB. 
Although DART is a general domain BKB 
built using parsing, its approach doesn?t allow 
one to process enough information to answer the 
first question (first row in Table 1). A web scale 
resource such as TextRunner is better suited for 
this purpose. However, results show its lack of 
normalization. On the other hand, our BKB is 
able to return a clean and relevant answer. 
The second question (second row) shows the 
ability of the three BKBs to deal with a basic 
abstraction needed for inference. Since TextRun-
ner doesn?t perform any kind of processing over 
entities or pronouns, it doesn?t recover relevant 
knowledge for this question in the football do-
main.  In addition, the table shows the need for 
domain adaptation: most of the TextRunner rela-
tions, such as ?person:gets:pass? or ?per-
son:bought:pass?, refer to different domains. 
DART shows the same effect: the first two en-
tries (?person:make:pass?, ?person:take:pass?) 
belong to different domains. 
DART1 TextRunner2 BKB (Football) 
(no results) (~200) threw  
(~100) completed  
(36) to throw  
(26) has thrown  
(19) makes  
(19) has  
(18) fires  
(99) throw 
(25) complete 
(7) have 
(5) attempt 
(5) not-throw 
(4) toss 
(3) release 
(47) make          
(45) take            
(36) complete    
(30) throw         
(25) let              
(23) catch   
(1) make            
(1) expect          
(22) gets  
(17) makes  
(10) has  
(10) receives  
(7) who has  
(7) must have  
(6) acting on  
(6) to catch  
(6) who buys  
(5) bought  
(5) admits  
(5) gives  
(824) catch 
(546) throw 
(256) complete 
(136) have 
(59) intercept 
(56) drop 
(39) not-catch 
(37) not-throw 
(36) snare 
(27) toss 
(23) pick off 
(20) run 
(13) person      
(6) person/ 
place/ organi-
zation 
(2) full-back 
(1) place     
(30) Early  
(26) Two plays  
(24) fumble  
(20) game  
(20) ball  
(17) Defensively  
(75) person 
(14) cornerback 
(11) defense 
(8) safety 
(7) group 
(5) linebacker 
Table 1. Comparison of DART, TextRunner and our 
BKB for the following queries (rows): (1) quarter-
back:X:pass, (2) person:X:pass, (3) X:intercept:pass.
Frequencies are in parentheses. 
Finally, the third question is aimed at recover-
ing possible agents (those that intercept passes in 
our case). Again, as shown in DART, the re-
duced set of classes given by the entity recogniz-
er is not enough for the football domain. But 
having no classes (TextRunner) is even worse, 
showing its orientation to discovering relations 
rather than to generalizing and answering about 
their possible arguments. Our approach is able to 
discover plausible agent-classes for the query. 
Other queries related to the football domain 
show the same behavior. 
                                                          
1 Available at http://userweb.cs.utexas.edu/users/pclark/dart/ 
2 After aggregating partial results for each cluster using 
http://www.cs.washington.edu/research/textrunner/
985
6 Related Work 
Our approach lies between macro-reading and 
Open Information Extraction (OIE). Macro-
reading (Mitchell et al 2009) is a different task 
from ours; it seeks to populate ontologies.  Here 
concepts and relations are predefined by the on-
tology.
OIE (Banko et al 2007) does not use a prede-
fined set of semantic classes and relations and is 
aimed at web scale. For this reason the frame-
work does not include a complete NLP pipeline. 
The resulting lack of term normalization and ab-
sence of domain adaptation (e.g., the query per-
son:X:pass return throw but also buy) makes the 
results less relevant to single-document reading.  
When, as with DART, the complete NLP pipe-
line is applied over a general corpus, the amount 
of information to be processed has to be limited 
due to computational cost. Ultimately, too little 
knowledge remains for working in a specific 
domain. For example, asking DART about 
?quarterback:X:pass? produces no results. 
Our approach takes advantage of both worlds, 
ensuring that enough amounts of documents re-
lated to the domain will be processed with a 
complete NLP pipeline. Doing so provides 
cleaner and canonical representations (our propo-
sitions) and even higher counts than TextRunner 
for our domain. This level of processing will be 
scalable in the midterm; various people including 
(Huang and Sagae, 2010) are working in linear 
time parsers with state-of-the-art performance. 
Another intermediate point between a collec-
tion of domain documents and the general web, 
reached by restricting processing to the results of 
a web query, is explored in IE-on-demand (Se-
kine 2006; Shinyama and Sekine 2006). Howev-
er, they use a predefined set of entity classes, 
preventing from discovering the appropriate gra-
nularity level that enables retrieval of relevant 
background knowledge. We do not predefine the 
concepts/classes and relations, but discover them 
from what it is explicitly said in the collection. 
The process of building the BKB described 
here is closely related to DART (Clark and Har-
rison, 2009) which in turn is related to KNEXT 
(Van Durme and Schubert, 2008). Perhaps the 
most important extension we performed is the 
inclusion of lexical relations (like ?has-
instance?) that activate more powerful uses of 
the Proposition Stores. 
7 Conclusions and Future Work 
In building a BKB, limiting oneself to a specific 
domain provides some powerful benefits. Ambi-
guity is reduced inside the domain, making 
counts in propositions more accurate. Also, fre-
quency distributions of propositions differ from 
one domain to another. For example, the list of 
the most frequent NVN propositions in our BKB 
(see Section 3.1) is, by itself, an indication of the 
most salient and important events specifically in 
the US football domain. Furthermore, the amount 
of text required to build the BKB is reduced sig-
nificantly allowing processing such as parsing. 
The task of inferring omitted but necessary in-
formation is a significant part of automated text 
interpretation. In this paper we show that even 
simple kinds of information, gleaned relatively 
straightforwardly from a parsed corpus, can be 
quite useful.  Though they are still lexical and 
not even starting to be semantic, propositions 
consisting of verbs as relations between nouns 
seem to provide a surprising amount of utility.  It 
remains a research problem to determine what 
kinds and levels of knowledge are most useful in 
the long run.   
In the paper, we discuss only the propositions 
that are grounded in instantial statements about 
players and events.  But for true learning by 
reading, a system has to be able to recognize 
when the input expresses general rules, and to 
formulate such input as axioms or inferences.  In 
addition is the significant challenge of generaliz-
ing certain kinds of instantial propositions to 
produce inferences.  At which point, for exam-
ple, should the system decide that ?all football 
players have teams?, and how should it do so? 
This remains a topic for future work.   
A further topic of investigation is the time at 
which expansion should occur. Doing so at ques-
tion time, in the manner of traditional task-
oriented back-chaining inference, is the obvious 
choice, but some limited amount of forward 
chaining at reading time seems appropriate too, 
especially if it can significantly assist with text 
processing tasks, in the manner of expectation-
driven understanding.    
Finally, as discussed above, the evaluation of 
intrinsic evaluation procedures remains to be de-
veloped.   
986
Acknowledgments 
We are grateful to Hans Chalupsky and David 
Farwell for their comments and input for this 
work. We acknowledge the builders of TextRun-
ner and DART for their willingness to make their 
resources openly available.   
This work has been partially supported by the 
Spanish Government through the "Programa Na-
cional de Movilidad de Recursos Humanos del 
Plan Nacional de I+D+i 2008-2011? (Grant 
PR2009-0020). Research supported in part by 
Air Force Contract FA8750-09-C-0172 under the 
DARPA Machine Reading Program. 
References  
Banko, M., Cafarella, M., Soderland, S., Broadhead, 
M., Etzioni, O. 2007. Open Information Extrac-
tion from the Web. IJCAI 2007. 
Barker, K. 2007. Building Models by Reading Texts. 
Invited talk at the AAAI 2007 Spring Symposium 
on Machine Reading, Stanford University. 
Clark, P. and Harrison, P. 2009. Large-scale extrac-
tion and use of knowledge from text. The Fifth 
International Conference on Knowledge Capture 
(K-CAP 2009). 
http://www.cs.utexas.edu/users/pclark/dart/ 
Hobbs, J.R., Stickel, M., Appelt, D. and Martin, P., 
1993. Interpretation as Abduction. Artificial In-
telligence, Vol. 63, Nos. 1-2, pp. 69-142.  
http://www.isi.edu/~hobbs/interp-abduct-ai.pdf 
Huang, L. and Sagae, K. 2010. Dynamic Program-
ming for Linear-Time Shift-Reduce Parsing. 
ACL 2010. 
Klein, D. and Manning, C.D. 2003. Accurate Unlexi-
calized Parsing. Proceedings of the 41st Meeting 
of the Association for Computational Linguistics, 
pp. 423-430 
Marneffe, M. and Manning, C.D. 2008. The Stanford 
typed dependencies representation. In COLING 
2008 Workshop on Cross-framework and Cross-
domain Parser Evaluation. 
Mitchell, T. M., Betteridge, J., Carlson, A., Hruschka, 
E., and Wang, R. Populating the Semantic Web 
by Macro-reading Internet Text. The Semantic 
Web - ISWC 2009. LNCS Volume 5823. Sprin-
ger-Verlag. 
Sekine, S. 2006. On Demand Information Extraction. 
COLING 2006. 
Shinyama, Y. and Sekine, S. 2006. Preemptive Infor-
mation Extraction using Unrestricted Relation 
Discovery. HLT-NAACL 2006. 
Sperber, D. and Wilson, D. 1995. Relevance: Com-
munication and cognition (2nd ed.) Oxford, 
Blackwell.
Van Durme, B., Schubert, L. 2008. Open Knowledge 
Extraction through Compositional Language 
Processing. Symposium on Semantics in Systems 
for Text Processing, STEP 2008. 
987
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 698?708, Dublin, Ireland, August 23-29 2014.
Inducing Latent Semantic Relations for Structured Distributional
Semantics
Sujay Kumar Jauhar
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
sjauhar@cs.cmu.edu
Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
hovy@cs.cmu.edu
Abstract
Structured distributional semantic models aim to improve upon simple vector space models of
semantics by hypothesizing that the meaning of a word is captured more effectively through its
relational ? rather than its raw distributional ? signature. In accordance, they extend the vector
space paradigm by structuring elements with relational information that decompose distributional
signatures over discrete relation dimensions. However, the number and nature of these relations
remains an open research question, with most previous work in the literature employing syn-
tactic dependencies as surrogates for truly semantic relations. In this paper we propose a novel
structured distributional semantic model with latent relation dimensions, and instantiate it using
latent relational analysis. Evaluation of our model yields results that significantly outperform
several other distributional approaches on two semantic tasks and performs competitively on a
third relation classification task.
1 Introduction
The distributional hypothesis, articulated by Firth (1957) in the popular dictum ?You shall know the
word by the company it keeps?, has established itself as one of the most popular models of modern
computational semantics. With the rise of massive and easily-accessible digital corpora, computation of
co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs)
that have found relevance in many application areas. These include information retrieval (Manning et
al., 2008), question answering (Tellex et al., 2003), word-sense disambiguation (McCarthy et al., 2004)
and selectional preference modelling (Erk, 2007), to name only a few.
The standard DSM framework, which models the semantics of a word by co-occurrence statistics
computed over its neighbouring words, has several known short-comings. One severe short-coming
derives from the fundamental nature of the vector space model, which characterizes the semantics of a
word by a single vector in a high dimensional space (or some lower dimensional embedding thereof).
Such a modelling paradigm goes against the grain of the intuition that the semantics of a word is neither
unique nor constant. Rather, it is composed of many facets of meaning, and similarity (or dissimilarity)
to other words is an outcome of the aggregate harmony (or dissonance) between the individual facets
under consideration. For example, a shirt may be similar along one facet to a balloon in that they are
both coloured blue, at the same time being similar to a shoe along another facet for both being articles
of clothing, while being dissimilar along yet another facet to a t-shirt because one is stitched from linen
while the other is made from polyester.
Structured distributional semantic models (SDSMs) aim to remedy this fault with DSMs by decompos-
ing distributional signatures over discrete relation dimensions, or facets. This leads to a representation
that characterizes the semantics of a word by a distributional tensor, rather than a vector. Previous at-
tempts in the literature include the work of Pad? and Lapata (2007), Baroni and Lenci (2010) and Goyal
et al. (2013). However, all these approaches assume a simplified representation in which truly semantic
relations are substituted by syntactic relations obtained from a dependency parser.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
698
We believe that there are limiting factors to this approximation. Most importantly, the set of syntactic
relations, while relatively uncontroversial, is unable to capture the full extent of semantic nuance encoun-
tered in natural language text. Often, syntax is ambiguous and leads to multiple semantic interpretations.
Conversely, passivization and dative shift are common examples of semantic invariance in which mul-
tiple syntactic realizations are manifested. Additionally, syntax falls utterly short in explaining more
complex phenomena ? such as the description of buying and selling ? in which implicit semantics are
tacit from complex interactions between multiple participants.
While it is useful to consider relations that draw their origins from semantic roles such as Agent,
Patient and Recipient, it remains unclear what this set of semantic roles should be. This problem is
one that has long troubled linguists (Fillmore, 1967; Sowa, 1991), and has been previously noted by
researchers in NLP as well (M?rquez et al., 2008). Proposed solutions range from a small set of generic
Agent-like or Patient-like roles in Propbank (Kingsbury and Palmer, 2002) to an effectively open-ended
set of highly specific and fine-grained roles in Framenet (Baker et al., 1998). In addition to the theoretic
uncertainty of the set of semantic relations there is the very real problem of the lack of high-performance,
robust semantic parsers to annotate corpora. These issues effectively render the use of pre-defined,
linguistically ordained semantic relations intractable for use in SDSM.
In this paper we propose a novel approach to structuring distributional semantic models with latent
relations that are automatically discovered from corpora. This approach effectively solves the conceptual
dilemma of selecting the most expressive set of semantic relations. To the best of our knowledge this
is the first paper to propose latent relation dimensions for SDSMs. The intuition for generating these
latent relation dimensions leads to a generic framework, which ? in this paper ? is instantiated with
embeddings obtained from latent relational analysis (Turney, 2005).
We conduct experiments on three different semantic tasks to evaluate our model. On a similarity
scoring task and another synonym ranking task the model significantly outperforms other distributional
semantic models, including a standard window-based model, a syntactic SDSM based on previous ap-
proaches proposed in the literature, and a state-of-the-art semantic model trained using recursive neural
networks. On a relation classification task, our model performs competitively, outperforming all but one
of the models it is compared against.
2 Related Work
Since the distributional hypothesis was first proposed by Firth (1957), a number of different research
initiatives have attempted to extend and improve the standard distributional vector space model of se-
mantics. Insensitivity to the multi-faceted nature of semantics has been one of the focal points of several
papers. Earlier work in this regard is a paper by Turney (2012), who proposes that the semantics of a word
is not obtained along a single distributional axis but simultaneously in two different spaces. He proposes
a DSM in which co-occurrence statistics are computed for neighbouring nouns and verbs separately to
yield independent domain and function spaces of semantics.
This intuition is taken further by a stance which proposes that a word?s semantics is distributionally
decomposed over many independent spaces ? each of which is a unique relation dimension. Authors
who have endorsed this perspective are Erk and Pad? (2008), Goyal et al. (2013), Reisinger and Mooney
(2010) and Baroni and Lenci (2010). Our work relates to these papers in that we subscribe to the multiple
space semantics view. However, we crucially differ from them by structuring our semantic space with
information obtained from latent semantic relations rather than from a syntactic parser. In this paper the
instantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which
is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words.
From a modelling perspective, SDSMs characterize the semantics of a word by a distributional ten-
sor. Other notable papers on tensor based semantics or semantics of compositional structures are the
simple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural net-
work approach of Socher et al. (2012), the physics inspired quantum view of semantic composition of
Grefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013).
A different, partially overlapping strain of research attempts to induce word embeddings using meth-
699
ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research
papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010).
Other related work to note is the body of research concerned with semantic relation classification,
which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4
(Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2
(Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at
semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard
and Martin, 2007).
3 Structured Distributional Semantics and Latent Semantic Relation Induction
In this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSM
with latent relation dimensions.
A DSM is a vector space V that contains |?| elements in R
n
, where ? = {w
1
, w
2
, ..., w
k
} is a vocab-
ulary of k distinct words. Every vocabulary word w
i
has an associated semantic vector ~v
i
representing its
distributional signature. Each of the n elements of ~v
i
is associated with a single dimension of its distribu-
tion. This dimension may correspond to another word ? that may or may not belong to ? ? or a latent
dimension as might be obtained from an SVD projection or an embedding learned via a deep neural net-
work. Additionally, each element in ~v
i
is typically a normalized co-occurrence frequency count, a PMI
score, or a number obtained from an SVD or RNN transformation. The semantic similarity between two
words w
i
and w
j
in a DSM is the vector distance defined by cos(~v
i
, ~v
j
) on their associated distributional
vectors.
An SDSM is an extension of DSM. Formally, it is a space U that contains |?| elements in R
d?n
, where
? = {w
1
, w
2
, ..., w
k
} is a vocabulary of k distinct words. Every vocabulary word w
i
has an associated
semantic tensor
~
~u
i
, which is itself composed of d vectors ~u
i1
, ~u
i2
, ..., ~u
id
each having n dimensions.
Every vector ~u
il
?
~
~u
i
represents the distributional signature of the word w
i
in a relation (or along a
facet) r
l
. The d relations of the SDSM may be syntactic, semantic, or latent (as in this paper). The n
dimensional relational vector ~u
il
is configurationally the same as a vector ~v
i
of a DSM. This definition
of an SDSM closely relates to an alternate view of Distributional Memories (DMs) (Baroni and Lenci,
2010) where the semantic space is a third-order tensor, whose modes are Word? Link?Word.
The semantic similarity between two wordsw
i
andw
j
in an SDSM is the similarity function defined by
sim(
~
~u
i
,
~
~u
j
) on their associated semantic tensors. We use the following decomposition of the similarity
function:
sim(
~
~u
i
,
~
~u
j
) =
1
d
d
?
l=1
cos( ~u
il
, ~u
jl
) (1)
Mathematically, this corresponds to the ratio of the normalized Frobenius product of the two matrices
representing
~
~u
i
and
~
~u
j
to the number of rows in both matrices. Intuitively it is simply the average
relation-wise similarity between the two words w
i
and w
j
.
3.1 Latent Relation Induction for SDSM
The intuition behind our approach for inducing latent relation dimensions revolves around the simple ob-
servation that SDSMs, while representing semantics as distributional signatures over relation dimensions,
also effectively encode relational vectors between pairs of words. Our method thus works backwards
from this observation ? beginning with a relational embedding for pairs of words, that are subsequently
transformed to yield an SDSM.
Concretely, given a vocabulary ? = {w
1
, w
2
, ..., w
k
} and a list of word pairs of interest from the
vocabulary ?
V
? ? ? ?, we assume that we have some method for inducing a DSM V
?
that has a
vector representation
~
v
?
ij
of length d for every word pair w
i
, w
j
? ?
V
, which intuitively embeds the
distributional signature of the relation binding the two words in d latent dimensions. We then construct
an SDSM U where ?
U
= ?. For every word w
i
? ? a tensor
~
~u
i
? R
d?k
is generated. The tensor
~
~u
i
700
has d unique k dimensional vectors ~u
i1
, ~u
i2
, ..., ~u
id
. For a given relational vector ~u
il
, the value of the
jth element is taken from the lth element of the vector
~
v
?
ij
belonging to the DSM V
?
. If the vector
~
v
?
ij
does not exist in V
?
? as is the case where the pair w
i
, w
j
/? ?
V
? the value of the jth element of ~u
il
is
set to 0. By applying this mapping to generate semantic tensors for every word in ?, we are left with an
SDSM U that effectively embeds latent relation dimensions. From the perspective of DMs we matricize
the third-order tensor and perform truncated SVD, before restoring the resulting matrix to a third-order
tensor.
3.1.1 Latent Relational Analysis
In what follows, we present our instantiation of this model with an implementation that is based on
Latent Relational Analysis (LRA) (Turney, 2005) to generate the DSM V
?
. While other methods (such
as RNNs) are equally applicable in this scenario, we use LRA for its operational simplicity as well as
proven efficacy on semantic tasks such as analogy detection. The parameter values we chose in our
experiments are not fine-tuned and are guided by recommended values from Turney (2005), or scaled
suitably to accommodate the size of ?
V
.
The input to LRA is a vocabulary ? = {w
1
, w
2
, ..., w
k
} and a list of word pairs of interest from the
vocabulary ?
V
? ? ? ?. While one might theoretically consider a large vocabulary with all possi-
ble pairs, for computational reasons we restrict our vocabulary to approximately 4500 frequent English
words and only consider about 2.5% word pairs with high PMI (as computed on the whole of English
Wikipedia) in ?? ?. For each of the word pairs w
i
, w
j
? ?
V
we extract a list of contexts by querying a
search engine indexed over the combined texts of the whole of English Wikipedia and Gigaword corpora
(approximately 5.8? 10
9
tokens). Suitable query expansion is performed by taking the top 4 synonyms
of w
i
and w
j
using Lin?s thesaurus (Lin, 1998). Each of these contexts must contain both w
i
, w
j
(or
appropriate synonyms) and optionally some intervening words, and some words to either side.
Given such contexts, patterns for every word pair are generated by replacing the two target words w
i
and w
j
with placeholder characters X and Y , and replacing none, some or all of the other words by their
associated part-of-speech tag or a wildcard symbol. For example, if w
i
and w
j
are ?eat? and ?pasta?
respectively, and the queried context is ?I eat a bowl of pasta with a fork?, one would generate patterns
such as ?* X * NN * Y IN a *?, ?* X DT bowl IN Y with DT *?, etc. For every word pair, only the 5000
most frequent patterns are stored.
Once the set of all relevant patterns P = p
1
, p
2
, ..., p
n
have been computed a DSM V is constructed.
In particular, the DSM constitutes a ?
V
based on the list of word pairs of interest, and every word pair
w
i
, w
j
of interest has an associated vector ~v
ij
. Each element m of the vector ~v
ij
is a count pertaining to
the number of times that the pattern p
m
was generated by the word pair w
i
, w
j
.
3.1.2 SVD Transformation
The resulting DSM V is noisy and very sparse. Two transformations are thus applied to V . Firstly all
co-occurrence counts between word pairs and patterns are transformed to PPMI scores (Bullinaria and
Levy, 2007). Then given the matrix representation of V ? where rows correspond to word pairs and
columns correspond to patterns ? SVD is applied to yield V = M?N . HereM andN are matrices that
have unit-length orthogonal columns and ? is a matrix of singular values. By selecting the d top singular
values, we approximate V with a lower dimension projection matrix that reduces noise and compensates
for sparseness: V
?
= M
d
?
d
. This DSM V
?
in d latent dimensions is precisely the one we then use to
construct an SDSM, using the transformation described above.
Since the large number of patterns renders it effectively impossible to store the entire matrix V in
memory we use a memory friendly implementation
1
of a multi-pass stochastic algorithm to directly
approximate the projection matrix (Halko et al., 2011; Rehurek, 2010). A detailed analysis to see how
change in the parameter d effects the quality of the model is presented in section 4.
The optimal SDSM embeddings we trained and used in the experiments detailed below are available
for download at http://www.cs.cmu.edu/~sjauhar/Software_files/LR-SDSM.tar.
1
http://radimrehurek.com/gensim/
701
Model Spearman?s ?
Random 0.000
DSM 0.179
synSDSM 0.315
SENNA 0.510
LR-SDSM (300) 0.567
LR-SDSM (130) 0.586
Table 1
Model Acc.
Random 0.25
DSM 0.28
synSDSM 0.27
SENNA 0.38
LR-SDSM (300) 0.47
LR-SDSM (130) 0.51
Table 2
Results on the WS-353 similarity scoring task and the ESL synonym selection task. LRA-SDSM signif-
icantly outperforms other structured and non-structured distributional semantic models.
gz. This SDSM contains a vocabulary of 4546 frequent English words with 130 latent relation dimen-
sions.
4 Evaluation
Section 3 has described a method for embedding latent relation dimensions in SDSMs. We now turn to
the problem of evaluating these relations within the scope of the distributional paradigm in order to ad-
dress two research questions: 1) Are latent relation dimensions a viable and empirically competitive solu-
tion for SDSM? 2) Does structuring lead to a semantically more expressive model than a non-structured
DSM? In order to answer these questions we evaluate our model on two generic semantic tasks and
present comparative results against other structured and non-structured distributional models. We show
that we outperform all of them significantly, thus answering both research questions affirmatively.
While other research efforts have produced better results on these tasks (Jarmasz and Szpakowicz,
2003; Gabrilovich and Markovitch, 2007; Hassan and Mihalcea, 2011), they are either lexicon or knowl-
edge based, or are driven by corpus statistics that tie into auxiliary resources such as multi-lingual in-
formation and structured ontologies like Wikipedia. Hence they are not relevant to our experimental
validation, and are consequently ignored in our comparative evaluation.
4.1 Word-Pair Similarity Scoring Task
The first task consists in using a semantic model to assign similarity scores to pairs of words. The dataset
used in this evaluation setting is the WS-353 dataset from Finkelstein et al. (2002). It consists of 353
pairs of words along with an averaged similarity score on a scale of 1.0 to 10.0 obtained from 13?16
human judges. Word pairs are presented as-is, without any context. For example, an item in this dataset
might be ?book, paper? 7.46?.
System scores are obtained by using the standard cosine similarity measure between distributional
vectors in a non-structured DSM. In the case of a variant of SDSM, these scores can be found by using
the cosine-based similarity functions in Equation 1 of the previous section. System generated output
scores are evaluated against the gold standard using Spearman?s rank correlation coefficient.
4.2 Synonym Selection Task
In the second task, the same set of semantic space representations is used to select the semantically closest
word to a target from a list of candidates. The ESL dataset from Turney (2002) is used for this task, and
was selected over the slightly larger TOEFL dataset (Landauer and Dumais, 1997). The reason for this
choice was because the latter contained more complex vocabulary words ? several of which were not
present in our simple vocabulary model. The ESL dataset consists of 50 target words that appear with 4
candidate lexical substitutes each. While disambiguating context is also given in this dataset, we discard
it in our experiments. An example item in this dataset might be ?rug? sofa, ottoman, carpet, hallway?,
with ?carpet? being the most synonym-like candidate to the target.
702
Figure 1: Evaluation results on WS-353 and ESL with varying number of latent dimensions. Generally
high scores are obtained in the range of 100-150 latent dimensions, with optimal results on both datasets
at 130 latent dimensions.
Similarity scores ? which are obtained in the same manner as for the previous evaluation task ?
are extracted between the target and each of the candidates in turn. These scores are then sorted in
descending order, with the top-ranking score yielding the semantically closest candidate to the target.
Systems are evaluated on the basis of their accuracy at discriminating the top-ranked candidate.
4.3 Results
We compare our model (LR-SDSM) to several other distributional models in these experiments. These
include a standard distributional vector space model (DSM) trained on the combined text of English
Wikipedia and Gigaword with a window-size of 3 words to either side of a target, a syntax-based SDSM
(Goyal et al., 2013; Baroni and Lenci, 2010) (synSDSM) trained on the same corpus parsed with a
dependency parser (Tratz and Hovy, 2011) and the state-of-the-art neural network embeddings from
Collobert et al. (2011) (SENNA). We also give the expected evaluation scores from a random baseline,
for comparison.
An important factor to consider when constructing an SDSM using LRA is the number of latent di-
mensions selected in the SVD projection. In Figure 1 we investigate the effects of selecting different
number of latent relation dimensions on both semantic evaluation tasks, starting with 10 dimensions up
to a maximum of 800 (which was the maximum that was computationally feasible), in increments of 10.
We note that optimal results on both datasets are obtained at 130 latent dimensions. In addition to the
SDSM obtained in this setting we also give results for an SDSM with 300 latent dimensions (which has
been a recommended value for SVD projections in the literature (Landauer and Dumais, 1997)) in our
comparisons against other models. Comparative results on the Finkelstein WS-353 similarity scoring
task are given in Table 1, while those on the ESL synonym selection task are given in Table 2.
4.4 Discussion
The results in Tables 1 and 2 show that LR-SDSM outperforms the other distributional models by a
considerable and statistically significant margin (p-value < 0.05) on both types of semantic evaluation
tasks. It should be noted that we do not tune to the test sets. While the 130 latent dimension SDSM yields
the best results, 300 latent dimensions also gives comparable performance and moreover outperforms all
the other baselines. In fact, it is worth noting that the evaluation results in figure 1 are almost all better
703
Random SENNA-Mik
DSM SENNA
LR-SDSM
AVC MVC AVC MVC
Prec. 0.111 0.273 0.419 0.382 0.489 0.416 0.431
Rec. 0.110 0.343 0.449 0.443 0.516 0.457 0.475
F-1. 0.110 0.288 0.426 0.383 0.499 0.429 0.444
% Acc. 11.03 34.30 44.91 44.26 51.55 45.65 47.48
Table 3: Results on Relation Classification Task. LR-SDSM scores competitively, outperforming all but
the SENNA-AVC model.
than the results of the other models on either datasets.
We conclude that structuring of a semantic model with latent relational information in fact leads to
performance gains over non-structured variants. Also, the latent relation dimensions we propose offer a
viable and empirically competitive alternative to syntactic relations for SDSMs.
Figure 1 shows the evaluation results on both semantic tasks as a function of the number of latent
dimensions. The general trend of both curves on the figure indicate that the expressive power of the
model quickly increases with the number of dimensions until it peaks in the range of 100?150, and then
decreases or evens out after that. Interestingly, this falls roughly in the range of the 166 frequent (those
that appear 50 times or more) frame elements, or fine-grained relations, from FrameNet that O?Hara and
Wiebe (2009) find in their taxonomization and mapping of a number of lexical resources that contain
semantic relations.
5 Semantic Relation Classification and Analysis of the Latent Structure of Dimensions
In this section we conduct experiments on the task of semantic relation classification. We also perform a
more detailed analysis of the induced latent relation dimensions in order to gain insight into our model?s
perception of semantic relations.
5.1 Semantic Relation Classification
In this task, a relational embedding is used as a feature vector to train a classifier for predicting the
semantic relation between previously unseen word pairs. The dataset used in this experiment is from
the SemEval-2012 task 2 on measuring the degree of relational similarity (Jurgens et al., 2012), since
it characterizes a number of very distinct and interesting semantic relations. In particular it consists of
an aggregated set of 3464 word pairs evidencing 10 kinds of semantic relations. We prune this set to
discard pairs that don?t contain words in the vocabularies of the models we consider in our experiments.
This leaves us with a dataset containing 933 word pairs in 9 classes (1 class was discarded altogether
because it contained too few instances). The 9 semantic relation classes are: ?Class Inclusion?, ?Part-
Whole?, ?Similar?, ?Contrast?, ?Attribute?, ?Non-Attribute?, ?Case Relation?, ?Cause-Purpose? and
?Space-Time?. For example, an instance of a word pair that exemplifies the ?Part-Whole? relationship is
?engine:car?. Note that, as with previous experiments, word pairs are given without any context.
5.2 Results
We compare LR-SDSM on the semantic relation classification task to several different models. These
include the additive vector composition (AVC) and multiplicative vector composition methods (MVC)
proposed by Mitchell and Lapata (2009); we present both DSM and SENNA based variants of these
models. We also compare against the vector difference method of Mikolov et al. (2013) (SENNA-
Mik) which sees semantic relations as a meaning preserving vector translation in an RNN embedded
vector space. Finally, we note the performance of random classification as a baseline, for reference. We
attempted to produce results of a syntactic SDSM on the task; however, the hard constraint imposed by
syntactic adjacency meant that effectively all the word pairs in the dataset yielded zero feature vectors.
To avoid overfitting on all 130 original dimensions in our optimal SDSM, and also to render results
comparable, we reduce the number of latent relation dimensions of LR-SDSM to 50. We similarly reduce
704
Figure 2: Correlation distances between semantic relations? classifier weights. The plot shows how our
latent relations seem to perceive humanly interpretable semantic relations. Most points are fairly well
spaced out, with opposites such as ?Attribute? and ?Non-Attribute? as well as ?Similar? and ?Contrast?
being relatively further apart.
the feature vector dimension of DSM-AVC and DSM-MVC to 50 by feature selection. The dimensions
of SENNA-AVC, SENNA-MVC and SENNA-Mik are already 50, and are not reduced further.
For each of the methods we train a logistic regression classifier. We don?t perform any tuning of
parameters and set a constant ridge regression value of 0.2, which seemed to yield roughly the best
results for all models. The performance on the semantic relation classification task in terms of averaged
precision, recall, F-measure and percentage accuracy using 10-fold cross-validation is given in Table 3.
Additionally, to gain further insight into the LR-SDSM?s understanding of semantic relations, we
conduct a secondary analysis. We begin by training 9 one-vs-all logistic regression classifiers for each of
the 9 semantic relations under consideration. Then pairwise correlation distances are measured between
all pairs of weight vectors of the 9 models. Finally, the distance adjacency matrix is projected into 2-d
space using multidimensional scaling. The result of this analysis is presented in Figure 2.
5.3 Discussion
Table 3 shows that LR-SDSM performs competitively on the relation classification task and outperforms
all but one of the other models. The performance differences are statistically significant with a p-value
< 0.5. We believe that some of the expressive power of the model is lost by compressing to 50 latent
relation dimensions, and that a greater number of dimensions might improve performance. However,
testing a model with a 130-length dense feature vector on a dataset containing 933 instances would
likely lead to overfitting and also not be comparable to the SENNA-based models that operate on 50-
length feature vectors.
Other points to note from Table 3 are that the AVC variants of the the DSM and SENNA composition
models tend to perform better than their MVC counterparts. Also, SENNA-Mik performs surprisingly
poorly. It is worth noting, however, that Mikolov et al. (2013) report results on fairly simple lexico-
syntactic relations between words ? such as plural forms, possessives and gender ? while the semantic
relations under consideration in the SemEval-2012 dataset are relatively more complex.
In the analysis of the latent structure of dimensions presented in Figure 2, there are few interesting
points to note. To begin with, all the points (with the exception of one pair) are fairly well spaced out. At
705
the weight vector level, this implies that different latent dimensions need to fire in different combinations
to characterize distinct semantic relations, thus resulting in low correlation between their corresponding
weight vectors. This indicates the fact that the latent relation dimensions seem to capture the intuition
that each of the classes encodes a distinctly different semantic relation. The notable exception is ?Space-
Time?, which is very close to ?Contrast?. This is probably due to the fact that distributional models are
ineffective at capturing spatio-temporal semantics. Moreover, it is interesting to note that ?Attribute"
and ?Non-Attribute? as well as ?Similar? and ?Contrast?, which are intuitively semantic inverses of each
other are also (relatively) distant from each other in the plot.
These general findings indicate an interesting avenue for future research, which involves mapping the
empirically learnt latent relations to hand-built semantic lexicons or frameworks. This could help to vali-
date the empirical models at various levels of linguistic granularity, as well as establish correspondences
between different views of semantic representation.
6 Conclusion and Future Work
In this paper we have proposed a novel paradigm for SDSMs, that allows for structuring via latent
relational information. We have introduced a generic operational framework that allows for building
such SDSMs and outlined an instantiation of the model with LRA. Experimental results of the model
support our claim that the resulting SDSM captures the semantics of words more effectively than a
number of other semantic models, and presents a viable ? and empirically competitive ? alternative
to syntactic SDSMs. Additionally we have conducted experiments on a relation classification task and
shown promising results, as well as performed analyses to investigate the structure of, and interactions
between, the latent relation dimensions.
These findings motivate a number of future directions of research. Since our framework is fairly gen-
eral we hope to explore techniques other than LRA (such as RNNs) to generate relational embeddings
for word pairs. A desiderata for future techniques is scalability so that we can characterize vocabular-
ies that are larger than the one in our current experiments. We also hope to explore mappings between
our empirically learnt latent relations, and semantic lexicons and frameworks that catalog semantic re-
lations. Finally, we hope to test our model on more realistic application task such as event coreference,
recognizing textual entailment, and semantic parsing in future work.
Acknowledgments
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions
to improve the quality of the paper. This work was supported in part by the following grants: NSF grant
IIS-1143703, NSF award IIS-1147810, DARPA grant FA87501220342.
References
Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In Proceedings of the
17th international conference on Computational linguistics-Volume 1, pages 86?90. Association for Computa-
tional Linguistics.
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Comput. Linguist., 36(4):673?721, December.
Steven Bethard and James H Martin. 2007. Cu-tmp: temporal relation classification using syntactic and semantic
features. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 129?132. Associa-
tion for Computational Linguistics.
John A Bullinaria and Joseph P Levy. 2007. Extracting semantic representations from word co-occurrence statis-
tics: A computational study. Behavior Research Methods, 39(3):510?526.
Ronan Collobert, Jason Weston, L?on Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. J. Mach. Learn. Res., 999888:2493?2537, November.
Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman. 1990.
Indexing by latent semantic analysis. JASIS, 41(6):391?407.
706
Katrin Erk and Sebastian Pad?. 2008. A structured vector space model for word meaning in context. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?08, pages 897?906,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model for selectional preferences.
Charles J Fillmore. 1967. The case for case.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Rup-
pin. 2002. Placing search in context: The concept revisited. In ACM Transactions on Information Systems,
volume 20, pages 116?131, January.
John R. Firth. 1957. A Synopsis of Linguistic Theory, 1930-1955. Studies in Linguistic Analysis, pages 1?32.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing semantic relatedness using wikipedia-based ex-
plicit semantic analysis. In IJCAI, volume 7, pages 1606?1611.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Szpakowicz, Peter Turney, and Deniz Yuret. 2007. Semeval-
2007 task 04: Classification of semantic relations between nominals. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 13?18. Association for Computational Linguistics.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrinmaya Sachan, Shashank Srivastava, and Eduard Hovy. 2013.
A structured distributional semantic model for event co-reference. In Proceedings of the 51st Annual Meeting
of the Association for Computational Linguistics (ACL ? 2013).
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of the Conference on Empirical Methods in Natural Language
Processing, EMNLP ?11, pages 1394?1404, Stroudsburg, PA, USA. Association for Computational Linguistics.
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. 2011. Finding structure with randomness: Probabilistic
algorithms for constructing approximate matrix decompositions. SIAM review, 53(2):217?288.
Samer Hassan and Rada Mihalcea. 2011. Semantic relatedness using salient semantic analysis. In AAAI.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov, Diarmuid ? S?aghdha, Sebastian Pad?, Marco
Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification
of semantic relations between pairs of nominals. In Proceedings of the Workshop on Semantic Evaluations:
Recent Achievements and Future Directions, pages 94?99. Association for Computational Linguistics.
Mario Jarmasz and Stan Szpakowicz. 2003. Roget?s thesaurus and semantic similarity. Recent Advances in
Natural Language Processing III: Selected Papers from RANLP.
David A Jurgens, Peter D Turney, Saif M Mohammad, and Keith J Holyoak. 2012. Semeval-2012 task 2: Measur-
ing degrees of relational similarity. In Proceedings of the First Joint Conference on Lexical and Computational
Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings
of the Sixth International Workshop on Semantic Evaluation, pages 356?364. Association for Computational
Linguistics.
Paul Kingsbury and Martha Palmer. 2002. From treebank to propbank. In LREC. Citeseer.
Thomas K Landauer and Susan T. Dumais. 1997. A solution to plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation of knowledge. Psychological review, pages 211?240.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?tze. 2008. Introduction to Information Retrieval.
Cambridge University Press, New York, NY, USA.
Llu?s M?rquez, Xavier Carreras, Kenneth C Litkowski, and Suzanne Stevenson. 2008. Semantic role labeling: an
introduction to the special issue. Computational linguistics, 34(2):145?159.
Diana McCarthy, Rob Koeling, Julie Weeds, and John Carroll. 2004. Finding predominant word senses in un-
tagged text. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL
?04, Stroudsburg, PA, USA. Association for Computational Linguistics.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic regularities in continuous space word repre-
sentations. Proceedings of NAACL-HLT, pages 746?751.
707
Jeff Mitchell and Mirella Lapata. 2009. Language models based on semantic composition. In Proceedings of the
2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP ?09,
pages 430?439, Stroudsburg, PA, USA. Association for Computational Linguistics.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting semantic role resources for preposition disambiguation. Com-
putational Linguistics, 35(2):151?184.
Sebastian Pad? and Mirella Lapata. 2007. Dependency-based construction of semantic space models. Computa-
tional Linguistics, 33(2):161?199.
Radim Rehurek. 2010. Fast and faster: A comparison of two streamed matrix decomposition algorithms. NIPS
2010 Workshop on Low-rank Methods for Large-scale Machine Learning.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117. Association for Computational Linguistics.
Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations
and syntactic parsing with recursive neural networks. Proceedings of the NIPS-2010 Deep Learning and Unsu-
pervised Feature Learning Workshop.
Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL ?12, pages
1201?1211, Stroudsburg, PA, USA. Association for Computational Linguistics.
John F. Sowa. 1991. Principles of semantic networks. Morgan Kaufmann.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fernandes, and Gregory Marton. 2003. Quantitative evaluation
of passage retrieval algorithms for question answering. In SIGIR, pages 41?47.
Stephen Tratz and Eduard Hovy. 2011. A fast, accurate, non-projective, semantically-enriched parser. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages 1257?1268,
Stroudsburg, PA, USA. Association for Computational Linguistics.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for
semi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computational
Linguistics, pages 384?394. Association for Computational Linguistics.
Peter D. Turney. 2002. Mining the web for synonyms: Pmi-ir versus lsa on toefl. CoRR.
Peter Turney. 2005. Measuring semantic similarity by latent relational analysis. In Proceedings of the 19th
international Conference on Aritifical Intelligence, pages 1136?1141.
Peter D Turney. 2012. Domain and function: A dual-space model of semantic relations and compositions. Journal
of Artificial Intelligence Research, 44:533?585.
Tim Van de Cruys, Thierry Poibeau, and Anna Korhonen. 2013. A tensor-based factorization model of semantic
compositionality. In Proceedings of NAACL-HLT, pages 1142?1151.
708
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1302?1310, Dublin, Ireland, August 23-29 2014.
Unsupervised Word Sense Induction using Distributional Statistics
Kartik Goyal
Carnegie Mellon Uniersity
kartikgo@cs.cmu.edu
Eduard Hovy
Carnegie Mellon University
hovy@cmu.edu
Abstract
Word sense induction is an unsupervised task to find and characterize different senses of polyse-
mous words. This work investigates two unsupervised approaches that focus on using distribu-
tional word statistics to cluster the contextual information of the target words using two different
algorithms involving latent dirichlet allocation and spectral clustering. Using a large corpus for
achieving this task, we quantitatively analyze our clusters on the Semeval-2010 dataset and also
perform a qualitative analysis of our induced senses. Our results indicate that our methods suc-
cessfully characterized the senses of the target words and were also able to find unconventional
senses for those words.
1 Introduction
Word Sense Induction (WSI) involves automatically determining the number of senses of a given word
or a phrase and identifying the features which differentiate those senses. This task, although similar
to the Word Sense Disambiguation (WSD) task, is fundamentally different because it does not involve
any supervision or explicit human knowledge about senses of words. WSI has potential to be extremely
useful in downstream applications because, apart from the savings on annotation costs, it also mitigates
several theoretical conflicts associated with supervised WSD tasks, which generally involve deciding on
the granularity of senses. Ideally, a WSI algorithm would be able to adapt to different tasks requiring
different sense granularities. WSI algorithms can also be used to model the evolution of the senses of
a word with time and hence can be much easier to maintain than existing fixed sense inventories like
WordNet(Miller, 1995), Ontonotes(Hovy et al., 2006) etc. Automatic sense identification systems also
have the potential to generalize well to large amounts of diverse data and hence be useful in various
difficult domain independent tasks such as machine translation and information retrieval.
Several factors make the problem of word sense induction very challenging. Most importantly, it is not
clear what should be the ?true? senses of a word. The semantic continuum makes it always possible to
break a sense into finer grained subsenses. Thus, the problem is one of finding the optimal granularity
for any given task. Even in a semi-supervised setting, it is unknown which sense inventories are most
suited as starting points in a sense bootstrapping procedure.
Our unsupervised approach relies heavily on the distributional statistics of words which occur in the
proximity of the target words. Hence, we first obtain the distributional statistics from a very large cor-
pus to facilitate generalization and reliable estimation of different possible senses. Then we use these
statistics in a novel manner to obtain a representation for the senses of the target word. In this paper, we
discus the performance of induced senses on the Semeval 2010 WSD/WSI(Manandhar et al., 2010) task.
2 Related Work
Much of the work on word sense induction has been quite recent following the Semeval tasks on WSI
in 2007(Agirre and Soroa, 2007) and 2010, but the task was recognized much earlier and various semi-
supervised and unsupervised efforts were directed towards the problem.Yarowsky (1995) proposed a
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1302
semi-supervised approach, which required humans to specify seed words for every ambiguous word and
assumed one sense per discourse for an ambiguous word. The unsupervised approaches mainly focus
on clustering the instances of the target words in a corpus, using first-order vectors, second-order vec-
tors (Purandare and Pedersen, 2004)(Sch?utze, 1998) etc. Pantel and Lin (2002) used various syntactic
and surface features for clustering the various occurences of a target word. Co-occurence graph-based
approaches(V?eronis, 2004) have also been used, which represent the words co-occuring with the tar-
get words as nodes and then identify the highly dense subgraphs or ?hubs? within this co-occurence
graph. Brody and Lapata (2009) and Lau et al. (2012) proposed bayesian WSI systems which cluster
the instances by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet
Processes (HDP)(Teh et al., 2006) etc. wherein each occurence of a target word is represented as a ?doc-
ument? and its surrounding context as the ?observable content?. Choe and Charniak (2013) propose a
?naive bayes? model for WSI which assumes one sense per discourse and uses Expectation Maximiza-
tion(EM) to estimate model parameters like the probability of generating an instance feature like a word
in the context, given the sense of the target word in a particular instance. Reisinger and Mooney (2010)
and Huang et al. (2012) have proposed sense dependent multiple prototypes for a word instead of the
conventional one vector representation per word and have shown that this sense differentiation improves
semantic similarity measurements between words.
3 Basic Motivation: Co-occurence graphs
Conventionally, each word is represented as a co-occurence vector which may contain frequency, point
wise mutual information or some lower dimensional representation of context and this representation
conflates all the senses of a word. These vectors can be viewed as a graph where words are nodes
which have an edge between them if a word occurs in the distributional vector of another. Given a target
ambiguous word w, we refer to those words as the ?first order? words(referred to by ?neighbors?) which
are directly connected to w. The ?second order? words are the words directly connected to the first order
words and so on. This graph is cyclic and each node might have multiple senses conflated into it. In this
work, we only consider the first and second order words, eg. a target word like ?bank? will have words
like ?river? ,?money? etc in it?s first order and the second order vectors will be the words from the context
of the first order words like ?river?:?flood?,?plains? etc, ?money?: ?currency?, ?economy? etc. Essentially,
these second order words characterize the first order words and hence are very informative for clustering
the first order words into different senses. Essentially, we use the second order words as features of
the first order words and use them to cluster the first order words into different senses.It must be noted
that the first order words themselves might have multiple senses and ideally, those words should also be
disambiguated but in the current work we only focus on disambiguating the ?target? words.
4 Methodology
For clustering the neighbors of the target words, we implement and compare two methods which differ
significantly in their technical details and employ distribtutional statistics of the neighbors differently,
which we describe in the sections below. For obtaining the distributional statistics on a large scale, we
used the 5-gram data of Google N-gram corpus(Michel et al., 2011) which effectively lets us use as 10
word window. No lemmatization or case normalization was performed because the large corpus size
ameliorated the problem of sparseness. Only nouns, verbs, adjectives and adverbs were employed for the
statistical estimation because our pilot studies suggested that these words were most informative.
4.1 Latent Dirichlet Allocation
LDA(Blei et al., 2003) is a well known bayesian generative topic model which models every ?document?
as a mixture of latent ?topics? and all its ?words? as multinomial draws from those latent topics. In topic
model parlance, a ?corpus? consists of various ?documents?. Each ?document? has a collection of tokens
which is treated like a bag of words, where each word is drawn from a latent ?topic?. The topics are
shared across documents thus giving each document a topic proportion based upon the topic assignment
of the tokens in a document. The priors on topic proportions and the topic multimonial paramers are
1303
dirichlet parameters. An important characteristic of LDA is its clustering property which makes the
model inclined to enforce sparseness with small dirichlet priors.
It is important to note that we employ LDA in a significantly different manner than the previous ap-
proaches which have used LDA or other related topic models for word sense induction. Other topic
modelling based approaches for WSI represent each instance of the target word as a ?document? and the
immediate context as the ?bag of words? for that ?document?. Unlike these approaches, we represented a
target ambiguous word as the ?corpus? in the topic modelling parlance. Then we found out all the ?first
order? words co-occuring with the target word within a 10 word window. Each ?first order? word/type
is considered a ?document? in our LDA based approach. The latent ?topics? for each ?document? are
the latent ?senses? and each first order type comprises of a ?sense distribution? which is indicative of
its tendency to induce a particular sense in the target word. The ?second order? types are all the words
occuring in a 10 word window of every ?first order? word. These types along with their frequency, form
the ?bag of words? for the ?first order? type(LDA document). Hence, in our model, the latent senses are
shared across all the first order neighbors of the target word and the second order tokens play the role
of ?words? in our LDA based model. After getting the sense distributions for each first order type, we
perform k-means over all the sense distribution vectors such that every first order neighbor gets assigned
a cluster.
We posit that the distributional statistics of a large corpus helps in improving the coverage of second or-
Figure 1: Figure1: s is the latent sense variable. ? is the sense distribution of a first order neighbor. w is
a second order neighbor of a first order word. ? is the sense multinomial with a dirichlet prior ?. ? is a
dirichlet prior on the sense proportion of a first order type.
der words which are essential for reliable clustering of the first order words. However, the large number
of occurences and a large vocabulary make it intractable to run LDA using the original frequency of the
second order words. To overcome this computational hurdle, we posit that with a diverse representation
of the second order words, LDA based parameter estimation relies more upon the relative distribution of
the these words across all the first order words rather than their actual distributions. Hence, we decided to
scale down the actual counts for each word so that we could run LDA with the finite resources available.
An important parameter in this model is the number of latent topics/senses to use, which is specified to
be the actual number of senses specified in the Ontonotes sense inventory. This is an idealized case in
which the number of senses are known. The ? hyperparameter is chosen to be small with respect to the
average ?document lengths? we encounter. This has the effect of pushing most of the probabilistic weight
to one topics instead of diluting it among many topics. We also decided to analyze the effect of part of
speech tags of the second order words in clustering the first order words. The various configurations we
experimented with were:
? All: Considered nouns,verbs,adjectives and adverbs in second order bag of words.
? Nouns: Only considered second order words which were nouns to study the effect of Nouns on
clustering.
? Verbs: Only considered second order words which were verbs.
1304
? Nadj: Considered both nouns and adjectives to study the effect of Noun phrases over clustering.
? Vadv: Considered both verbs and adjectives for second order bag of words.
4.2 Spectral Clustering
Spectral Clustering(Ng et al., 2002) is a clustering technique which uses a pairwise similarity matrix,
L, to find out clusters such that the seperation between the entities in two seperate clusters is maximum
while implicitly taking into account the distances between groups of points instead of considering them
individually. The aim is to find the eigenvectors of D
?1
L corresponding to smallest eigenvalues to
minimize the similarity across two clusters. Here D is a diagonal matrix with degree of node i on entry
D
ii
. For k clusters, k eigenvectors ordered by their eigenvalues are found out. These k eigenvectors are
used to form a n? k matrix where n is the number of datapoints. Each row of this matrix is considered
a datapoint with a vector of length k, thus effectively reducing the dimension of the datapoints to k most
prominent dimensions according to the similarity matrix decomposition. Finally, k-means is performed
on the n vectors to assign a cluster to each datapoint.
We cluster the first order neighbors for each target word using spectral clustering. The crux of this
algorithm lies in using appropriate pairwise distance matrices. For constructing the pairwise distance
matrices of first order types, we used two vectorial representations of the first order words:
? Senna embeddings: The word embeddings trained by a neural network by (Weston et al., 2012)
? Distributional vectors comprised of the frequencies of the second order words.
Then we used these vectors to calculate mutual pairwise distance matrices(we experimented with Eu-
clidean and Cosine distances), which were converted into similarity matrices by using Gaussian kernels.
These matrices were used as input to the spectral clustering algorithm.
We chose to ignore very low frequency words for making word vectors. This cutoff was decided by
analyzing the distributional frequency vs. rank curves of the words, which were heavy tailed. Again, we
use the same number of clusters as the number of senses in Ontonotes sense inventory, so that we can
study the correspondence between our clusters and the Ontonotes senses.
5 Quantitative Analysis
In this paper, we discus our systems? performances on the Semeval-2010 word sense induc-
tion/disambiguation dataset, which contains 100 target words: 50 nouns and 50 verbs. The test data
is a part of OntoNotes (Hovy et al., 2006) and contains around 9000 instances of usage of the target
words. For annotating a particular test instance, we first filtered the surrounding context to retain only
salient Nouns, Verbs, Adverbs, and Adjectives. We report a mixture of senses for each instance, where
the weight for each sense was proportional to the number of filtered surrounding words belonging to that
sense/cluster. As mentioned earlier, we experimented with a variety of settings for spectral clustering
and LDA based methods. The performance with different settings was generally similar and hence, we
report our best results here. For a better insight into how our models in different settings performed, we
also report the full tables for paired F-score. The performance trend of various systems is similar for
other measures. We compare our results to three baselines:
? Most Frequent Sense (MFS) baseline: assigns all the test instances to the most frequent sense of the
target word.
? Brown University?s system results (Choe and Charniak, 2013).
? Lau (LDA) (Lau et al., 2012), who provide only the results for one of the three measures. In
particular, we compare our system to their results obtained by a model that was based on LDA and
used the gold standard number of senses as the number of topics to be used.
1305
System V-measure Paired F-score Supervised F-score #cl
all nouns verbs all nouns verbs all nouns verbs
LDA 4.4 5.2 3.2 60.7 53.2 71.7 60.9 55.2 69.2 2.45
Spectral 4.5 4.6 4.2 61.5 54.5 71.6 60.7 55.1 68.8 1.87
MFS 0.0 0.0 0.0 63.5 57.0 72.7 58.7 53.2 66.6 1.00
Brown 18.0 23.7 9.9 52.9 52.5 53.5 65.4 62.6 69.5 3.42
Lau - - - - - - 64.0 60.0 69.0 -
Table 1: Performance on Paired F-score and supervised F-score. LDA and Spectral are the two methods
proposed in this paper. Lau is the baseline in which LDA system of (Lau et al., 2012) is considered. It
should be noted that in their paper, (Lau et al., 2012) did not report their performance on Paired F-score.
The Semeval-2010 task provides us with 3 evaluation metrics: V-measure, Paired F-score and Super-
vised F-score. It was noticed (Manandhar and Klapaftis, 2009) that V-measure tends to favour systems
that produce a higher number of clusters than the gold standard and hence is not a reliable estimate of
the performance of WSI systems. But, we report our results on V-measure too as it gives useful insight
about the nature of data and the WSI algorithms.
It is important to note that all the measures treat Ontonotes sense annotations as the gold standard, which
makes this task unfit for our evaluation purposes. As mentioned earlier, our argument is that several
decisions related to the granularity of senses and definition of senses are a topic of dispute, and hence
we believe that instead of relying upon a pre-annotated sense inventory, it should be more effective to
induce senses automatically in an unsupervised manner using a large and unbiased corpus, and tune the
granularity governing parameters for different downstream tasks which require sense disambiguation.
But our performance on these annotations still provides us with valuable information about the agree-
ment between Ontonotes senses and our systems? senses. In our experiments, we have not tried to tune
the hyperparameters or perform agglomerative clustering to better fit our clusters to the gold standard
clusters by using training/development set at all, because we wanted to analyze the performance of our
algorithms in the most general setting.
5.1 V-Measure
The V-measure defines the quality of a cluster to be the harmonic mean of homogeneity and coverage.
These can be viewed as precision and recall of the element-wise assignment to clusters, where homo-
geneity measures the ?pureness? of the clusters and coverage measures the ?cohesiveness?. It was noticed
(Manandhar and Klapaftis, 2009) that V-measure tends to favour systems producing a higher number of
clusters than the gold standard and hence is not a reliable estimate of the performance of WSI systems.
In addition, the number of induced clusters in our systems is bounded at the top by the Gold Standard
number of senses because of our choice of hyperparameters in both spectral clustering and LDA based
approaches.
From the results, we realized that the number of senses induced in the test set by our system is quite
low compared to the baselines and other systems that participated in Semeval-2010. This hurts our V-
measure. Our systems perform better on nouns than verbs generally according to this measure. Also,
LDA-based approaches with the number of topics equal to the number of gold-standard senses perform
the best. For spectral clustering, euclidean distances seem to perform better.
5.2 Paired F-score
The paired F-score is the harmonic mean of precision and recall on the task of classifying whether the
instances in a pair belong to the same cluster or not. This measure also penalizes the systems if the
number of induced senses is not equal to the number of senses in the gold standard. It must be noted that
in our approach, the induced number of senses on the test dataset is not equal to the original number of
senses although we clustered with the number of clusters specified by Ontonotes, because our clusters
are different from Ontonotes senses. MFS has a recall of 100% which makes it a very hard baseline to
1306
P F-score(%) all nouns verbs #cl
CD20 60.5 53.1 71.3 2.12
CD15 57.9 50.8 68.2 2.26
CD10 58.5 50.7 69.7 2.27
ED20 61.5 54.5 71.6 1.87
ED15 60.6 53.1 71.5 2.12
ED10 60.0 52.3 71.3 2.45
CS15 59.6 52.9 69.4 2.25
CS10 60.1 51.9 72.0 2.07
ES15 59.8 52.9 71.3 2.15
ES10 60.8 53.5 71.4 2.21
MFS 63.5 57.0 72.7 1.00
Brown 52.9 52.5 53.5 3.42
Table 2: General trend for the various settings: Paired F-Score Evaluation: Spectral Clustering:
?C?:cosine distance, ?E?: Euclidean Distance, ?D?: Second order Distributinal counts, ?S?:Senna em-
beddings and the adjacent numbers are the number of nearest neighbors(in 1000s) considered for the
distance matrix.
P F-score(%) all nouns verbs #cl
all 60.7 53.2 71.7 2.47
noun 59.6 52.1 70.7 2.32
verb 60.0 52.4 71.0 2.25
nadj 59.7 52.6 70.1 2.3
vadv 59.3 52.27 69.6 2.25
MFS 63.5 57.0 72.7 1.00
Brown 52.9 52.5 53.5 3.42
Table 3: General trend for the various settings: Paired F-Score Evaluation: LDA: ?all?: All POS tags con-
sidered in the first order neighborhood, ?noun?: Only nouns considere, ?verbs?: Only verbs considered,
?nadj?: nouns and adjectives considered, ?vadv?:verbs and adverbs considered
beat. Semeval-2010 results show that none of the systems outperform the MFS baseline. Both of our
systems perform better than other systems on this measure and are comparable to the performance of the
MFS baseline.
5.3 Supervised F-score
For the supervised task, the test data is split into two parts: one for mapping the system senses to the gold
standard senses, and the other for evaluation based upon the mapped senses. We report our performance
on the 80% mapping and 20% evaluation split. The mapping is done automatically by the program
provided by the organizers which is based upon representing the gold standard clusters as a mixture of
the system senses.
Our different systems perform similarly on the supervised evaluation. We outperform the tough MFS
baseline and perform competitively against other systems. We observe that other systems outperform us
on the target nouns whereas our performance on verbs is similar to that of other systems. This can be
attributed to the fact that our methods induce a small number of senses in general over the test set but
according to the test data based upon Ontonotes, the senses of nouns have a much higher resolution than
verbs.
5.4 Discussion on Quantitative Results
In general, we found our performance to be competetive with the other systems. Also, we perform
significantly better than other Semeval-2010 systems on the paired F-score metric. In our experiments,
1307
Sense Cluster Words
1 Engineers,Presbyterian,Service,Jewish,Police,Ethnicity,Independent,Movements
2 membrane,complicated,surgical,hypothalamic,potassium,lymphatic,electron,tumor
3 Cynthia,Armstrong,Tracy,Marilyn,Stella,Abbot,Gustavus,Clark,Stewart,Monica
4 heels,noses,haze,hand,drooping,galloped,nakedness,pallid,anguish,palms
5 night,burdens,gut,assassins,witness,results,celestial,visual,deep,Hell
6 lifted,hastily,hovering,guiding,sinner,tendency,developing,sacrificed,condemned
Table 4: Example words in the clusters of ?body.n?
we found that for spectral clustering, Euclidean distances tend to perform better than Cosine distances.
Also, the distributional counts of the second order words tend to perform better than Senna vectors which
is not surprising because the Senna vectors are trained with the philosophy of a language model, which
results in words often being clustered according to their POS tags rather than their semantic closeness.
Spectral methods, yield slightly better results on two metrics than LDA based clustering which suggests
that similarity matrices give us a better idea about interactions between groups of words than simple
occurence frequencies of the words. But a bigger advantage of spectral clustering techniques is the
speed of computing SVD which is much better than that of slow inference algorithms of LDA based
models.
For LDA based models, we also note that different settings focusing on different POS tags, performed
very similarly and did not indicate any strong preference for any POS tag for the task of WSI using LDA.
Finally, both our methods tend to induce a small number of senses in the test data, which suggests that
the induced senses are relatively coarse-grained. Further splitting of coarse clusters using hierarchical
clustering methods might be helpful if a task requires finer-grained senses.
6 Qualitative Analysis
In this section, we present some deductions drawn from the qualitative analysis of clusters generated by
our methods which support our hypothesis. In particular, we discus the nature of clusters generated by
the spectral clustering algorithm using the second order distributional vectors for obtaining the similarity
matrix based on Euclidean distance.
A preliminary analysis of cluster sizes revealed that in almost all the cases, one of the clusters was very
large(about 3 times larger than the second largest cluster) and this largest cluster seemed to conflate a
lot of senses. Other clusters were generally similar sized and most of them represented a sense of the
target word on their own. The results in general look very promising and many clusters can be easily
interpreted as different senses of the target word.
In Table 4, we show the top few words for the word ?body.n?. Some senses very clearly represent
themselves : 1. Body as in organization, 2. Biological terms related to body, 4. Body in a more informal
sense. Sense 5 seems like a mixture of two senses of body, one related to celestial bodies and other related
to dead bodies/murder. Interestingly, sense 3 comprises proper nouns i.e. people whose bodies have been
mentioned in the corpus. This is not a conventional sense listed in any of the sense inventories but based
upon the requirements of a task, one might be interested in differentiating between general mentions of
?bodies? and mentions of ?bodies? which appear when mentioning famous people or celebrities. This sort
of clustering can be incredibly useful in tasks like Machine Translation and Information Retreival which
require us to model semantics of rare words such as important proper nouns.
7 Discussion and Future Work
We used a large corpus and its distributional statistics to perform word sense induction for a set of 100
target words. We proposed two algorithms which cluster the salient words surrounding the target word
by using the distributions of surrounding words. Both LDA based algorithm and the spectral clustering
algorithm yielded similar clusters. We believe that these clusters can be employed in downstream tasks
and can be further broken into smaller fine grained clusters automatically if needed by the application.
1308
We also evaluated our clusters arising from the distributional statistics, in the Semeval-2010 tasks with-
out any tuning and showed that they perform competetively with other approaches.
We argue that treating existing sense inventories as gold standards for WSI tasks is not an appropriate
measure for WSI systems because these inventories would not be able to measure two very important
characteristics of WSI systems which make them more advantageous than supervised WSD systems:a)
coverage and b) discovery of new senses.
Hence, the Semeval-2010 experiments are not an accurate reflection of the capabilities of WSI systems
because they rely on the Ontonotes sense inventory for the Gold Standard judgements, which are admitted
even by the OntoNotes builders to be only 85% reliable on average (Hovy et al., 2006). Our competetive
performance on these tasks show that our methods can be compliant with standard word sense disam-
biguation tasks but more importantly, our qualitative analysis showed that our techniques can discover
new unconventional senses too, which might not be present in the sense inventories but could be very
useful in tasks requiring differentiations. Unfortunately, no metrics exist that can help us quantify the
coverage of senses and their novelty. An ideal metric to evaluate the WSI systems in a better manner,
would be their performance on extrinsic tasks like Machine Translation, Information Retreival, Machine
Reading etc., which require differentiation of senses at different granular levels. WSI techniques have
a potential of eliminating sense annotation costs hence enabling wider use of sense differentiation in a
more generalized setting.
Our techniques resulted in coarse-grained senses. A major challenge in this task is to determine the
appropriate number of senses to induce. To overcome this problem, non-parametric methods could be
conceived to identify the ideal number of clusters automatically. In future, the WSI systems like ours can
also be used to analyze the evolution of senses over a period of time or geographical variation of senses.
As mentioned earlier, the co-occurence graph consists of many canonical representation of words which
must be split according to their different senses. In our experiments, we considered a small number of
target words and did not take into account the multiplicity of senses in the representation of ?first? and
?second? order neighbors. A more sophisticated iterative approach involving making several passes over
a co-occurence graph and refining senses of different words in each pass can ameliorate the problem as-
sociated with a single canonical representation of neighboring words. Finally, designing extrinsic tasks
to measure the efficacy of WSI systems will be extremely helpful in development of more robust and
useful WSI systems.
Acknowledgments
This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT pro-
gram.
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task 02: Evaluating word sense induction and discrimination
systems. In Proceedings of the 4th International Workshop on Semantic Evaluations, pages 7?12. Association
for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. the Journal of machine
Learning research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word sense induction. In Proceedings of the 12th Conference
of the European Chapter of the Association for Computational Linguistics, pages 103?111. Association for
Computational Linguistics.
Do Kook Choe and Eugene Charniak. 2013. Naive bayes word sense induction. In EMNLP, pages 1433?1437.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: the
90% solution. In Proceedings of the human language technology conference of the NAACL, Companion Volume:
Short Papers, pages 57?60. Association for Computational Linguistics.
1309
Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2012. Improving word representations
via global context and multiple word prototypes. In Proceedings of the 50th Annual Meeting of the Associa-
tion for Computational Linguistics: Long Papers-Volume 1, pages 873?882. Association for Computational
Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David Newman, and Timothy Baldwin. 2012. Word sense induction
for novel sense detection. In Proceedings of the 13th Conference of the European Chapter of the Association
for Computational Linguistics, pages 591?601. Association for Computational Linguistics.
Suresh Manandhar and Ioannis P Klapaftis. 2009. Semeval-2010 task 14: evaluation setting for word sense induc-
tion & disambiguation systems. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements
and Future Directions, pages 117?122. Association for Computational Linguistics.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dligach, and Sameer S Pradhan. 2010. Semeval-2010 task
14: Word sense induction & disambiguation. In Proceedings of the 5th International Workshop on Semantic
Evaluation, pages 63?68. Association for Computational Linguistics.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K Gray, Joseph P Pickett, Dale
Hoiberg, Dan Clancy, Peter Norvig, Jon Orwant, et al. 2011. Quantitative analysis of culture using millions of
digitized books. science, 331(6014):176?182.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Andrew Y Ng, Michael I Jordan, Yair Weiss, et al. 2002. On spectral clustering: Analysis and an algorithm.
Advances in neural information processing systems, 2:849?856.
Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In Proceedings of the eighth ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 613?619. ACM.
Amruta Purandare and Ted Pedersen. 2004. Word sense discrimination by clustering contexts in vector and
similarity spaces. In Proceedings of the Conference on Computational Natural Language Learning, pages 41?
48. Boston.
Joseph Reisinger and Raymond J Mooney. 2010. Multi-prototype vector-space models of word meaning. In
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 109?117. Association for Computational Linguistics.
Hinrich Sch?utze. 1998. Automatic word sense discrimination. Computational linguistics, 24(1):97?123.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes.
Journal of the american statistical association, 101(476).
Jean V?eronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech & Language,
18(3):223?252.
Jason Weston, Fr?ed?eric Ratle, Hossein Mobahi, and Ronan Collobert. 2012. Deep learning via semi-supervised
embedding. In Neural Networks: Tricks of the Trade, pages 639?655. Springer.
David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Computational Linguistics, pages 189?196. Association for
Computational Linguistics.
1310
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1414?1422, Dublin, Ireland, August 23-29 2014.
Modeling Newswire Events using Neural Networks for Anomaly Detection
Pradeep Dasigi
Language Technologies Institute
5000 Forbes Avenue
Pittsburgh, PA 15213
USA
pdasigi@cs.cmu.edu
Eduard Hovy
Language Technologies Institute
5000 Forbes Avenue
Pittsburgh, PA 15213
USA
hovy@cmu.edu
Abstract
Automatically identifying anomalous newswire events is a hard problem. We discuss the com-
plexity of the problem and introduce a novel technique to model events based on recursive neural
networks to represent events as composition of their semantic arguments. Our model learns to
differentiate between normal and anomalous events. We model anomaly detection as a binary
classification problem and show that the model learns useful features to classify anomaly. We
use headlines from the weird news category publicly available on newswire websites to extract
anomalous training examples and those from Gigaword as normal examples. We evaluate the
classifier on human annotated data and obtain an accuracy of 65.44%. We also show that our
model is at least as competent as the least competent human annotator in anomaly detection.
1 Introduction
Understanding events is a fundamental prerequisite for deeper semantic analysis of language. We intro-
duce the problem of automatic anomalous event detection in this paper and propose a novel event model
that can learn to differentiate between normal and anomalous events. We generally define anomalous
events as those that are unusual compared to the general state of affairs and might invoke surprise when
reported. For example, given the event mention in the following sentence
Man recovering after being shot by his dog.
one might think it is strange because dogs are not expected to shoot men. But the mentions
Man recovering after being shot by cops.
Man recovering after being bitten by a dog.
are not as unusual as the previous one. While all three sentences are equally valid syntactically, and it
is not unclear what any of them means, it is our knowledge about the role fillers ?both individually
and specifically in combination? that enables us to differentiate between normal and anomalous events.
Hence we hypothesize that anomaly is a result of unexpected or unusual combination of semantic role
fillers. Given this idea, an automatic anomaly detection algorithm has to encode the goodness of semantic
role filler coherence.
It has to be noted that event level anomaly is not the same as semantic incoherence. An event con-
structed by randomly choosing words to form each of the semantic arguments is not anomalous since we
cannot argue whether the event is normal or anomalous when it is unclear what the event means. Hence,
we define anomalous events to be the sub class of those that are semantically coherent, but are unusual
only based on real world knowledge.
Automatic anomalous event detection is a hard problem since determining what a good combination
of role fillers requires deep semantic and pragmatic knowledge. Moreover, manual judgment of anomaly
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1414
itself may be difficult and people often may not agree with each other in this regard. We describe the
difficulty in human judgment in greater detail in Section 4.4. Automatic detection of anomaly requires
encoding complex information, which has to be composed from the semantics of the individual words
in the sentence. A fundamental problem in doing so is the sparsity in semantic space due to the discrete
representations of meaning of words.
In this paper, we describe an attempt to model newswire events as a composition of the predicate with
its semantic arguments. Our approach is based on the recent models used for semantic composition using
recursive neural networks (RNN). It has been previously shown by Socher et al. (2010) and Socher et
al. (2013b) among others that RNN can effectively deal with sparsity in semantic space by represent-
ing meaning at a higher level of abstraction than the surface forms of words, and thus being able to
learn more general patterns. These models are very relevant to modeling event semantics because the
sparsity problem ranges from polysemy and synonymy at the lexical semantic level to entity and event
co-reference at the discourse level.
2 Background
2.1 Selectional Preference and Thematic Fit
Selectional preference, a notion introduced by Wilks (1973), refers to the phenomenon of the predicate
and the fillers of its arguments affecting the likelihood of fillers of other arguments. Thus the idea is that
predicate and the role fillers ?prefer? some fillers for other roles. For example, given that the predicate is
writes, the agent author prefers the patient book, while the agent programmer prefers the patient code.
This idea is used by Elman (2009), and is very similar to the role-filler composition that we use for
anomaly detection.
Erk et al. (2010) also model selectional preferences using vector spaces. They measure the goodness
of the fit of a noun with a verb in terms of the similarity between the vector of the noun and some
?exemplar? nouns taken by the verb in the same argument role. Baroni and Lenci (2010) also measure
selectional preference similarly, but instead of exemplar nouns, they calculate a prototype vector for that
role based on the vectors of the most common nouns occurring in that role for the given verb. Lenci
(2011) builds on this work and models the phenomenon that the expectations of the verb or its role-fillers
change dynamically given other role fillers.
2.2 Recursive Neural Networks
Recursive Neural Networks (RNN), first introduced by Goller and Kuchler (1996), are multilayer neural
network models used for efficient processing of structured objects of arbitrary shape. These have been
successfully used for modeling semantics of sentences of arbitrary length by Socher et al. (2010), for
sentiment analysis by Socher et al. (2013b), for syntactic parsing by Socher et al. (2013a) and for learn-
ing morphologically aware word representations by Luong et al. (2013). RNN are attractive because they
can encode compositions of meaning guided by syntax or some other linguistic structure known a priori.
Moreover, they provide flexibility in terms of learning composition weights based on supervised or un-
supervised objectives. Consequently RNN learn feature representations depending on the task. Hence,
this is a good choice for modeling event composition.
In its simplest form, an RNN processes information backed by a Directed Acyclic Graph (DAG),
where each node represents a neural network with the same parameters. The output produced at each
intermediate step of encoding usually has the same dimensionality as each of the inputs, hence RNN
projects the representation of a structure of arbitrary length into the same space as the inputs. This
property is what makes RNN recursive. An example RNN with a binary DAG (tree) structure is shown
in Figure 1. The activation from each neural network node is
c = g(y
1
?y
2
) = Sg(W (y
1
?y
2
) + b)
where ? represents concatenation of vector representations of the inputs, y
1
, y
2
? R
n?1
are the inputs,
W ? R
n?2n
is the composition weight matrix and b ? R
n?1
is the bias. Sg is a element wise sigmoid
1415
Figure 1: Example of a Recursive Neural Network backed by a binary tree
Figure 2: Example of an event tree
function. Apart from encoding the composition, RNN also produce a score of composition
s = S
?
c
where S ? R
n?1
is a scoring operator and s is a score that shows how good the composition is. (Col-
lobert et al., 2011) take an unsupervised approach to training RNN for semantic composition based on
the contrastive estimation technique proposed by (Smith and Eisner, 2005) and assuming that any word
and its context is a positive example and a random word in the same context is a negative training ex-
ample. (Socher et al., 2013b) among others use a supervised objective that is based on the label error at
the topmost node in the RNN. The parameters of the simplest model are W , b and S. For representation
learning, the inputs x
i
are also made parameters. Goller and Kuchler (1996) propose Backpropagation
through structure (BPTS), that respects the underlying DAG structure during backpropagation of gradi-
ents.
3 Neural Event Model
We define an event as the pair (V,A), where V is the predicate or a semantic verb
1
, and A is the set of its
semantic arguments like agent, patient, time, location, so on. Our aim is to obtain a vector representation
of the event that is composed from representations of individual words, while explicitly guided by the
semantic role structure. This representation can be understood as an embedding of the event in an event
space.
Neural Event Model (NEM) is a kind of RNN that is guided by a tree representation of events like the
one shown in Figure 2. The edges connected to the root of the tree correspond to the predicate and its
semantic roles (arguments). All the other edges form binary sub-trees of arguments. NEM is a super-
vised model that learns to differentiate between anomalous and normal events by classifying the event
embeddings. The inputs to NEM are the semantic arguments, and the representations of words in each
argument. We recursively compose the words in each argument to obtain argument level representations,
which are then composed to obtain an event embedding.
Intra-argument composition (called argument composition henceforth) is unsupervised, and we use
contrastive estimation to learn the parameters. The structure of the binary tree backing argument compo-
sition is determined dynamically, composing at each stage the two nodes which give the best composition
1
By semantic verb, we mean an action word whose syntactic category is not necessarily a verb. For example, in Terrorist
attacks on the World Trade Center.., attacks is not a verb but is still an action word.
1416
Figure 3: Neural Event Model: Encoding
score. Inter-argument composition (called event composition henceforth) is supervised and we use label
error to learn the parameters. Figure 3 shows how NEM encodes the event shown in Figure 2. The blue
boxes show argument composition and the red box shows event composition.
3.1 Training
NEM is trained in two phases. The first, argument composition, is unsupervised while the second, event
composition, is supervised.
3.1.1 Argument Composition
An argument composition node takes inputs of dimensionality 2n and produces an composed output
representation of dimensionality n and a composition score. Accordingly, we define the node in terms
of the parameters ?
arg
= {W
arg
? R
n?2n
; b
arg
, S
arg
? R
n?1
;V } where W
arg
, b
arg
and S
arg
are the
composition weight, bias and the scoring operators respectively as described previously, and V is the set
of representations of all the words in the vocabulary. All nodes performing argument composition use
the same parameters. Training is done in contrastive estimation fashion and the objective is
argmin
?
arg
J
arg
= argmin
?
arg
max(0, 1? s+ s
c
)
where s is the score of the composition of the entire argument produced by the root node of the argument,
and s
c
is the score produced by randomly replacing one of the words in the argument at a time. The
structure of the binary tree backing each argument is determined dynamically. This is done by starting
with leaf nodes in the tree for each of the words in the argument, comparing the composition scores of
every pair of adjacent leaf nodes, and actually composing the pair that gives the highest score, which
gives a new node. The process is repeated until we build a complete binary tree for each argument.
3.1.2 Event Composition
Event composition takes argument representations and produces the event representation and label in-
dicating whether the event is normal or anomalous. We define the event composition node in terms of
the parameters ?
event
= {W
event
? R
n?kn
; b
event
, L
event
? R
n?1
} where k is the number of semantic
arguments per event. L
event
is the label operator. The objective of this phase is
argmin
?
event
J
event
= argmin
?
event
(?l log h(e) + (1? l) log(1? h(e)))
where l is the reference binary label indicating whether the event is normal or anomalous, e is the event
representation and h(e) is the output of the logistic function. Concretely,
h(e) =
1
1 + e
?L
?
event
e
We implement the functions and perform stochastic gradient descent using Theano (Bergstra et al., 2010).
1417
4 Experiments
4.1 Event Extraction
We extract events by running the Semantic Role Labeling (SRL) tool in SENNA (Collobert et al., 2011).
SENNA uses PropBank (Palmer et al., 2005) style semantic tags. We consider only the roles A0, A1,
AM-TMP and AM-LOC as the arguments of our events
2
. For example, the event in the tree shown in
Figure 2 is extracted from the sentence
Two Israeli helicopters killed 70 soldiers in Gaza strip.
and SENNA identifies the following as the semantic roles
verb:killed A0:Two Israeli helicopters A1:70 soldiers AM-LOC:in Gaza strip
4.2 Data
Since the second phase of training NEM is supervised, we need newswire events that are normal and
those that are anomalous. We crawl 3684 ?weird news? headlines available publicly on the website of
NBC news
3
, such as the following:
? India weaponizes world?s hottest chili.
? Man recovering after being shot by his dog.
? Thai snake charmer puckers up to 19 cobras.
We assume that the events extracted from this source, called NBC Weird Events (NWE) henceforth, are
anomalous for training. NWE contains 4271 events extracted using SENNA?s SRL. We use 3771 of those
events as our negative training data, and the remaining for testing. Similarly, we extract events also from
headlines in the AFE section of Gigaword, called Gigaword Events (GWE) henceforth. We assume these
events are normal. To use as positive examples for training event composition, we sample roughly the
same number of events from GWE as our negative examples from NWE. It has to be noted that each
headline may contain multiple events and some may not contain events at all.
For argument composition, we use about 100k whole sentences from AFE headlines and the weird
news headlines from which NWE are extracted. Since we are training argument composition, we do not
use the event structure in the first phase. It has to be noted that all our training data are easily available
and do not require any human annotation.
We test the performance of NEM on 1003 events which are not part of the training dataset. These
events are sampled with equal probabilities from NWE and GWE and are human annotated for anomaly.
Section 4.4 has details of the annotation task.
4.3 Word Vector Initialization
We initialize the vector representations of the words in our vocabulary using the embeddings available in
SENNA 3.0 (Collobert et al., 2011) if available, and randomly if not. For event composition, if the event
does not have a specific role filler, we input a zero vector for the role.
4.4 Annotation
We post the annotation of the test set containing 1003 events as Human Intelligence Tasks (HIT) on
Amazon Mechanical Turk (AMT). We break the task into 20 HITs and ask the workers to select one
of the four options - highly unusual, strange, normal and cannot say for each event. We ask them to
select highly unusual when the event seems too strange to be true, strange if it seems unusual but still
plausible, and cannot say only if the information present in the event is not sufficient to make a decision.
We present each event along with the original headline and the semantic arguments. Along with marking
2
These four types cover about 85% of all arguments in our training and test datasets.
3
http://www.nbcnews.com/html/msnbc/3027113/3032524/4429950/4429950_1.html
1418
Total number of annotators 22
Normal annotations 56.3%
Strange annotations 28.6%
Highly unusual annotations 10.3%
Cannot Say annotations 4.8%
Avg. events annotated per worker 344
4-way Inter annotator agreement (?) 0.34
3-way Inter annotator agreement (?) 0.56
Table 1: Annotation Statistics
one of the four options above, if an event is strange or highly unusual, we ask the annotators to select the
parts of the headline that make it so. Since there can be multiple events in the headline, the annotators
decision regarding the parts of the sentence that cause anomaly help us identify which particular event in
the headline is anomalous.
Table 1 shows some statistics of the annotation task. We compute the Inter Annotator Agreement
(IAA) in terms of Kripendorff?s alpha (Krippendorff, 1980). The advantage of using this measure instead
of the more popular Kappa is that the former can deal with missing information, which is the case with
our task since annotators work on different overlapping subsets of the test set. The 4-way IAA shown
in the table corresponds to agreement over the original 4-way decision (including cannot say) while the
3-way IAA is measured after merging the highly unusual and strange decisions.
Additionally we use MACE (Hovy et al., 2013) to assess the quality of annotation. MACE models the
annotation task as a generative process of producing the observed labels conditioned on the true labels
and the competence of the annotators, and predicts both the latent variables. The average of competence
of annotators, a value that ranges from 0 to 1, for our task is 0.49 for the 4-way decision and 0.59 for the
3-way decision.
We generate true label predictions produced by MACE, discard the events for which the prediction
remains to be cannot say, and use the rest as reference for evaluating NEM, which is described in Sec-
tion 4.5. This leaves 949 events as our reference dataset, of which only 41% of the labels are strange or
highly unusual. It has to be noted that even though our test set has equal size samples from both NWE
and GWE, the true distribution is not uniform.
Language Model Separability Given the annotations, we test to see if the sentences corresponding
to anomalous events can be separated from normal events by simpler features. We build a n-gram lan-
guage model from the training data set used for argument composition and measure the perplexity of
the sentences in the test set. Figure 4 shows a comparison of the perplexity scores for different labels.
If the n-gram features are enough to separate different classes of sentences, one would expect the sen-
tences corresponding to strange and highly unusual labels to have higher perplexity ranges than normal
sentences, because the language model is built from a dataset that is expected to have a distribution of
sentences where majority of them contain normal events. As it can be seen in Figure 4, except for a few
outliers, most data points in all the categories are in similar perplexity ranges. Hence, sentences with
different labels cannot be separated based on an n-gram language model features.
4.5 Evaluation
We evaluate the performance of event composition by comparing the predicted labels from the classifier
against the ones given by MACE. We merge the two anomaly classes and calculate accuracy of the binary
classifier, and the precision and recall of anomaly detection.
Baseline We compare the performance of our model against a baseline that is based on how well
the semantic arguments in the event match the selectional preferences of the predicate. We measure
selectional preference using Point-wise Mutual Information (PMI) (Church and Hanks, 1990) of the head
words of each semantic argument with the predicate. The baseline model is built as follows. We perform
1419
Figure 4: Comparison of perplexity scores for different labels
NEM Baseline
Accuracy 65.44% 45.22%
Anomalous
Precision 56.55% 36.30%
Recall 48.22% 59.50%
Normal
Precision 64.62% 42.08%
Recall 77.66% 33.60 %
Table 2: Classification Performance and Comparison with Baseline
dependency parsing using MaltParser (Nivre et al., 2007) on the sentences in the training data used in
the first phase of training to obtain the head words of the semantic arguments. We then calculate the PMI
values of all the pairs < h
A
, p > where h is the head word of argument A and p is the predicate of the
event. For training our baseline classifier, we use the labeled training data from the event composition
phase. The features to this classifier are the PMI measures of the < h
A
, p > pairs estimated from the
larger dataset. The classifier thus trained to distinguish between anomalous and normal events is applied
to the test set.
Table 2 shows the results and a comparison with the PMI based baseline. The accuracy of the baseline
classifier is lower than 50%, which is the expected accuracy of a classifier that assigns labels randomly.
The precision of that random classifier in predicting anomalous events is expected to be 41%, since that is
the percentage of anomaly labels in our reference set as described in Section 4.4. The accuracy of NEM
is higher than the baseline model. One possible reason for the PMI based baseline having higher recall
in predicting anomaly and lower precision is that the statistics estimated from larger training data cannot
be generalized to the test set due to sparsity issues. This indicates the advantage of using continuous
representations at a higher level of abstraction as features for classification.
To further compare NEM with human annotators, we give to MACE, the binary labels produced by
NEM along with the annotations and measure the competence. For the sake of comparison, we also
give to MACE, a list of random binary labels as one of the annotations to measure the competence of a
hypothetical worker that made random choices. These results are reported in Table 3. It can be seen that
the performance of NEM is comparable at least to the least competent human.
5 Discussion and Future Work
The two evaluation experiments show that the neural network does learn to distinguish between normal
and anomalous events. Future improvements to this model will include better event extraction techniques.
Since the current approach is supervised, the training data size for learning event composition is lim-
ited. We plan to develop unsupervised approaches that can learn good models of normal events, and
detect anomalies based on how well new events fit in the model. One possible approach is to do learning
1420
Human average 0.59
Human highest 0.70
Human lowest 0.26
Random 0.02
NEM 0.26
Table 3: Anomaly Detection Competence
based on contrastive estimation in the second phase as well. The assumption behind taking this approach
for learning is that a randomly generated data point is likely to be a negative example, which is not neces-
sarily true for learning event composition. Generating malformed events that are syntactically valid but
anomalous without much human effort can greatly help in developing such an unsupervised algorithm.
One important aspect of anomaly that is currently not handled by NEM is the level of generality of the
concepts the events contain. Usually more general concepts cause events to be more normal since they
convey lesser information. For example, an American soldier shooting another American soldier may be
considered unusual, while a soldier shooting another soldier may not be as unusual, and at the highest
level of generalization, a person shooting another person is normal. This information of generality has
to be incorporated into the event model. This can be achieved by integrating real world knowledge
from knowledge bases like Wordnet (Miller, 1995) or from corpus statistics like the work by Lin (1998)
into the event model. Bordes et al. (2011) learn continuous representations of entities and relations in
knowledge bases. More recently, an alternative approach for doing the same was proposed by Chen et
al. (2013). These representations can greatly help modeling events.
Finally, the idea of modeling event composition can help processing event data in general and can be
applied to other tasks like finding co-referent events.
6 Conclusion
We introduced the problem of anomalous newswire event detection and illustrated its difficulty. Our
approach is similar to the ones successfully used for modeling semantic composition. We showed that
while our event composition model does learn to distinguish between normal and anomalous events,
there is scope for improved models that can effectively incorporate real world information and can be
trained in an unsupervised fashion. We note that in general event composition is more difficult than
traditional semantic composition since the former also deals with pragmatics. Consequently the set of
nonsensical events is different from the set of anomalous sentences, and while meaningless events and
well composed normal events are two ends of the semantic spectrum, semantically valid anomalous
events lie somewhere between them.
Acknowledgements
This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT pro-
gram.
References
Marco Baroni and Alessandro Lenci. 2010. Distributional memory: A general framework for corpus-based
semantics. Computational Linguistics, 36(4):673?721.
James Bergstra, Olivier Breuleux, Fr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu, Guillaume Desjardins,
Joseph Turian, David Warde-Farley, and Yoshua Bengio. 2010. Theano: a cpu and gpu math expression
compiler. In Proceedings of the Python for Scientific Computing Conference (SciPy), volume 4.
Antoine Bordes, Jason Weston, Ronan Collobert, Yoshua Bengio, et al. 2011. Learning structured embeddings of
knowledge bases. In AAAI.
1421
Danqi Chen, Richard Socher, Christopher D Manning, and Andrew Y Ng. 2013. Learning new facts from knowl-
edge bases with neural tensor networks and semantic word vectors. arXiv preprint arXiv:1301.3618.
Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography.
Computational linguistics, 16(1):22?29.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493?2537.
Jeffrey L Elman. 2009. On the meaning of words and dinosaur bones: Lexical knowledge without a lexicon.
Cognitive science, 33(4):547?582.
Katrin Erk, Sebastian Pad?o, and Ulrike Pad?o. 2010. A flexible, corpus-driven model of regular and inverse
selectional preferences. Computational Linguistics, 36(4):723?763.
Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backprop-
agation through structure. In Neural Networks, 1996., IEEE International Conference on, volume 1, pages
347?352. IEEE.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani, and Eduard Hovy. 2013. Learning whom to trust with
mace. In Proceedings of NAACL-HLT, pages 1120?1130.
Klaus Krippendorff. 1980. Content analysis: An introduction to its methodology. Sage Publications (Beverly
Hills).
Alessandro Lenci. 2011. Composing and updating verb argument expectations: A distributional semantic model.
In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 58?66. As-
sociation for Computational Linguistics.
Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In Proceedings of the 17th international
conference on Computational linguistics-Volume 2, pages 768?774. Association for Computational Linguistics.
Minh-Thang Luong, Richard Socher, and Christopher D Manning. 2013. Better word representations with recur-
sive neural networks for morphology. CoNLL-2013, 104.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev, G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov, and
Erwin Marsi. 2007. Maltparser: A language-independent system for data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic
roles. Computational Linguistics, 31(1):71?106.
Noah A Smith and Jason Eisner. 2005. Contrastive estimation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 354?362.
Association for Computational Linguistics.
Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations
and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and
Unsupervised Feature Learning Workshop, pages 1?9.
Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional
vector grammars. In In Proceedings of the ACL conference. Citeseer.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christo-
pher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceed-
ings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1631?1642.
Yorick Wilks. 1973. Preference semantics. Technical report, DTIC Document.
1422
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1110?1118,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A Semi-Supervised Method to Learn and Construct Taxonomies using the
Web
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
Although many algorithms have been devel-
oped to harvest lexical resources, few organize
the mined terms into taxonomies. We pro-
pose (1) a semi-supervised algorithm that uses
a root concept, a basic level concept, and re-
cursive surface patterns to learn automatically
from the Web hyponym-hypernym pairs sub-
ordinated to the root; (2) a Web based concept
positioning procedure to validate the learned
pairs? is-a relations; and (3) a graph algorithm
that derives from scratch the integrated tax-
onomy structure of all the terms. Comparing
results with WordNet, we find that the algo-
rithm misses some concepts and links, but also
that it discovers many additional ones lacking
in WordNet. We evaluate the taxonomization
power of our method on reconstructing parts
of the WordNet taxonomy. Experiments show
that starting from scratch, the algorithm can
reconstruct 62% of the WordNet taxonomy for
the regions tested.
1 Introduction
A variety of NLP tasks, including inference, tex-
tual entailment (Glickman et al, 2005; Szpektor
et al, 2008), and question answering (Moldovan
et al, 1999), rely on semantic knowledge derived
from term taxonomies and thesauri such as Word-
Net. However, the coverage of WordNet is still lim-
ited in many regions (even well-studied ones such as
the concepts and instances below Animals and Peo-
ple), as noted by researchers such as (Pennacchiotti
and Pantel, 2006) and (Hovy et al, 2009) who per-
form automated semantic class learning. This hap-
pens because WordNet and most other existing tax-
onomies are manually created, which makes them
difficult to maintain in rapidly changing domains,
and (in the face of taxonomic complexity) makes
them hard to build with consistency. To surmount
these problems, it would be advantageous to have
an automatic procedure that can not only augment
existing resources but can also produce taxonomies
for existing and new domains and tasks starting from
scratch.
The main stages of automatic taxonomy induc-
tion are term extraction and term organization. In
recent years there has been a substantial amount of
work on term extraction, including semantic class
learning (Hearst, 1992; Riloff and Shepherd, 1997;
Etzioni et al, 2005; Pasca, 2004; Kozareva et al,
2008), relation acquisition between entities (Girju
et al, 2003; Pantel and Pennacchiotti, 2006; Davi-
dov et al, 2007), and creation of concept lists (Katz
and Lin, 2003). Various attempts have been made to
learn the taxonomic organization of concepts (Wid-
dows, 2003; Snow et al, 2006; Yang and Callan,
2009). Among the most common is to start with a
good ontology and then to try to position the miss-
ing concepts into it. (Snow et al, 2006) maximize
the conditional probability of hyponym-hypernym
relations given certain evidence, while (Yang and
Callan, 2009) combines heterogenous features like
context, co-occurrence, and surface patterns to pro-
duce a more-inclusive inclusion ranking formula.
The obtained results are promising, but the problem
of how to organize the gathered knowledge when
there is no initial taxonomy, or when the initial tax-
onomy is grossly impoverished, still remains.
1110
The major problem in performing taxonomy con-
struction from scratch is that overall concept po-
sitioning is not trivial. It is difficult to discover
whether concepts are unrelated, subordinated, or
parallel to each other. In this paper, we address the
following question: How can one induce the taxo-
nomic organization of concepts in a given domain
starting from scratch?
The contributions of this paper are as follows:
? An automatic procedure for harvesting
hyponym-hypernym pairs given a domain of
interest.
? A ranking mechanism for validating the learned
is-a relations between the pairs.
? A graph-based approach for inducing the taxo-
nomic organization of the harvested terms start-
ing from scratch.
? An experiment on reconstructing WordNet?s
taxonomy for given domains.
Before focusing on the harvesting and taxonomy
induction algorithms, we are going to describe some
basic terminology following (Hovy et al, 2009). A
term is an English word (for our current purposes,
a noun or a proper name). A concept is an item in
the classification taxonomy we are building. A root
concept is a fairly general concept which is located
on the high level of the taxonomy. A basic-level
concept corresponds to the Basic Level categories
defined in Prototype Theory in Psychology (Rosch,
1978). For example, a dog, not a mammal or a col-
lie. An instance is an item in the classification tax-
onomy that is more specific than a concept. For ex-
ample, Lassie, not a dog or collie .
The rest of the paper is organized as follows. Sec-
tion 2 reviews related work. Section 3 describes the
taxonomization framework. Section 4 discusses the
experiments. We conclude in Section 5.
2 Related Work
The first stage of automatic taxonomy induction,
term and relation extraction, is relatively well-
understood. Methods have matured to the point of
achieving high accuracy (Girju et al, 2003; Pantel
and Pennacchiotti, 2006; Kozareva et al, 2008). The
produced output typically contains flat lists of terms
and/or ground instance facts (lion is-a mammal)
and general relation types (mammal is-a animal).
Most approaches use either clustering or patterns
to mine knowledge from structured and unstructured
text. Clustering approaches (Lin, 1998; Lin and Pan-
tel, 2002; Davidov and Rappoport, 2006) are fully
unsupervised and discover relations that are not di-
rectly expressed in text. Their main drawback is that
they may or may not produce the term types and
granularities useful to the user. In contrast, pattern-
based approaches harvest information with high ac-
curacy, but they require a set of seeds and surface
patterns to initiate the learning process. These meth-
ods are successfully used to collect semantic lex-
icons (Riloff and Shepherd, 1997; Etzioni et al,
2005; Pasca, 2004; Kozareva et al, 2008), encyclo-
pedic knowledge (Suchanek et al, 2007), concept
lists (Katz and Lin, 2003), and relations between
terms, such as hypernyms (Ritter et al, 2009; Hovy
et al, 2009) and part-of (Girju et al, 2003; Pantel
and Pennacchiotti, 2006).
However, simple term lists are not enough to solve
many problems involving natural language. Terms
may be augmented with information that is required
for knowledge-intensive tasks such as textual entail-
ment (Glickman et al, 2005; Szpektor et al, 2008)
and question answering (Moldovan et al, 1999). To
support inference, (Ritter et al, 2010) learn the se-
lectional restrictions of semantic relations, and (Pen-
nacchiotti and Pantel, 2006) ontologize the learned
arguments using WordNet.
Taxonomizing the terms is a very powerful
method to leverage added information. Subordi-
nated terms (hyponyms) inherit information from
their superordinates (hypernyms), making it unnec-
essary to learn all relevant information over and over
for every term in the language. But despite many at-
tempts, no ?correct? taxonomization has ever been
constructed for the terms of, say, English. Typically,
people build term taxonomies (and/or richer struc-
tures like ontologies) for particular purposes, using
specific taxonomization criteria. Different tasks and
criteria produce different taxonomies, even when us-
ing the same basic level concepts. This is because
most basic level concepts admit to multiple perspec-
tives, while each task focuses on one, or at most two,
perspectives at a time. For example, a dolphin is a
Mammal (and not a Fish) to a biologist, but is a Fish
1111
(and hence not a Mammal) to a fisherman or anyone
building or visiting an aquarium. More confusingly,
a tiger and a puppy are both Mammals and hence
belong close together in a typical taxonomy, but a
tiger is a WildAnimal (in the perspective of Animal-
Function) and a JungleDweller (in the perspective of
Habitat), while a puppy is a Pet (as function) and a
HouseAnimal (as habitat), which would place them
relatively far from one another. Attempts at pro-
ducing a single multi-perspective taxonomy fail due
to the complexity of interaction among perspectives,
and people are notoriously bad at constructing tax-
onomies adherent to a single perspective when given
terms from multiple perspectives. This issue and the
major alternative principles for taxonomization are
discussed in (Hovy, 2002).
It is therefore not surprising that the second
stage of automated taxonomy induction is harder to
achieve. As mentioned, most attempts to learn tax-
onomy structures start with a reasonably complete
taxonomy and then insert the newly learned terms
into it, one term at a time (Widdows, 2003; Pasca,
2004; Snow et al, 2006; Yang and Callan, 2009).
(Snow et al, 2006) guide the incremental approach
by maximizing the conditional probability over a
set of relations. (Yang and Callan, 2009) introduce
a taxonomy induction framework which combines
the power of surface patterns and clustering through
combining numerous heterogeneous features.
Still, one would like a procedure to organize the
harvested terms into a taxonomic structure starting
fresh (i.e., without using an initial taxonomic struc-
ture). We propose an approach that bridges the gap
between the term extraction algorithms that focus
mainly on harvesting but do not taxonomize, and
those that accept a new term and seek to enrich an al-
ready existing taxonomy. Our aim is to perform both
stages: to extract the terms of a given domain and to
induce their taxonomic organization without any ini-
tial taxonomic structure and information. This task
is challenging because it is not trivial to discover
both the hierarchically related and the parallel (per-
spectival) organizations of concepts. Achieving this
goal can provide the research community with the
ability to produce taxonomies for domains for which
currently there are no existing or manually created
ontologies.
3 Building Taxonomies from Scratch
3.1 Problem Formulation
We define our task as:
Task Definition: Given a root concept, a basic level
concept or an instance, and recursive lexico-syntactic
patterns, (1) harvest in bootstrapping fashion hy-
ponyms and hypernyms subordinated to the root; (2)
filter out erroneous information (extracted concepts
and isa relations); (3) organize the harvested con-
cepts into a taxonomy structure.
!"#!$%&
'%$()%*&
+%#!%,#-!%*& ./0#1-!%*&
/%#,(+0#%*&.-#)(+0#%*& 2-22-$*&
-)(2-$&
103&
10)4%5&
%$%6/-)!&
./%%!-/&
1%%#&73%#&
6"2-&
$(0)&
Figure 1: Taxonomy Induction from Scratch.
Figure 1 shows an example of the task. Start-
ing with the root concept animal and the basic
level concept lion, the algorithm learns new
terms like tiger, puma, deer, donkey of class
animal. Next for each basic level concept, the
algorithm harvests hypernyms and learns that a
lion is-a vertebrate, chordate, feline and mammal.
Finally, the taxonomic structure of each basic
level concept and its hypernyms is induced: ani-
mal?chordate?vertebrate?mammal?feline?lion.
3.2 Knowledge Harvesting
The main objective of our work is not the creation
of a new harvesting algorithm, but rather the or-
ganization of the harvested information in a tax-
onomy structure starting from scratch. There are
many algorithms for hyponym and hypernym har-
vesting from the Web. In our experiments, we use
the doubly-anchored lexico-syntactic patterns and
bootstrapping algorithm introduced by (Kozareva et
al., 2008) and (Hovy et al, 2009).
1112
We are interested in using this approach, because
it is: (1) simple and easy to implement; (2) requires
minimal supervision using only one root concept
and a term to learn new hyponyms and hypernyms
associated to the root; (3) reports higher precision
than current semantic class algorithms (Etzioni et
al., 2005; Pasca, 2004); and (4) adapts easily to dif-
ferent domains.
The general framework of the knowledge harvest-
ing algorithm is shown in Figure 2.
1. Given:
a hyponym pattern Pi={concept such as seed
and *}
a hypernym pattern Pc={* such as term1 and
term2}
a root concept root
a term called seed for Pi
2. build a query using Pi
3. submit Pi to Yahoo! or other search engine
4. extract terms occupying the * position
5. take terms from step 4 and go to step 2.
6. repeat steps 2?5 until no new terms are found
7. rank terms by outDegree
8. for ? terms with outDegree>0, build a query
using Pc
9. submit Pc to Yahoo! or other search engine
10. extract concepts (hypernyms) occupying the *
position
11. rank concepts by inDegree
Figure 2: Knowledge Harvesting Framework.
The algorithm starts with a root concept, seed
term1 of type root and a doubly-anchored pattern
(DAP) such as ?<root> such as <seed> and *?
which learns on the * position new terms of type
root. The newly learned terms, which can be either
instances, basic level or intermediate concepts, are
placed into the position of the seed in the DAP pat-
tern, and the bootstrapping process is repeated. The
process ceases when no new terms are found.
To separate the true from incorrect terms, we use
a graph-based algorithm in which each vertex u is
a term, and an each edge (u, v) ? E corresponds
to the direction in which the term u discovered the
term v. The graph is weighted w(u, v) according
1The input term can be an instance, a basic level or an in-
termediate concept. An intermediate concept is the one that is
located between the basic level and root concepts.
to the number of times the term pair u-v is seen
in unique web snippets. The terms are ranked by
outDegree(u)=
?
?(u,v)?E
w(u,v)
|V |?1 which counts the
number of outgoing links of node u normalized by
the total number of nodes in the graph excluding the
current. The algorithm considers as true terms with
outDegree>0.
All harvested terms are automatically fed into the
hypernym extraction phase. We use the natural or-
der in which the terms discovered each other and
place them into an inverse doubly-anchored pattern
(DAP?1) ?* such as <term1> and <term2>? to
learn hypernyms on the * position. Similarly we
build a graph with nodes h denoting the hypernyms
and nodes t1-t2 denoting the term pairs. The edges
(h, t1 ? t2) ? E? show the direction in which the
term pair discovered the hypernym. The hypernyms
are ranked by inDegree(h)=
?
?(t1?t2,h)?E? w(t1?
t2, h) which rewards hypernyms that are frequently
discovered by various term pairs. The output of
the algorithm is a list of is-a relations between the
learned terms (instances, basic level or intermediate
concepts) and their corresponding hypernyms. For
example, deer is-a herbivore, deer is-a ruminant,
deer is-a mammal.
3.3 Graph-Based Taxonomy Induction
In the final stage of our algorithm, we induce the
overall taxonomic structure using information about
the pairwise positioning of the terms. In the knowl-
edge harvesting and filtering phases, the algorithm
learned is-a relations between the root and the terms
(instances, basic level or intermediate concepts), as
well as the harvested hypernyms and the terms. The
only missing information is the positioning of the in-
termediate concepts located between the basic level
and the root such as mammals, vertibrates, felines,
chordates, among others.
We introduce a concept positioning (CP) proce-
dure that uses a set of surface patterns: ?X such as
Y?, ?X are Y that?, ?X including Y?, ?X like Y?,
?such X as Y? to learn the hierarchical relations for
all possible concept pairs. For each concept pair,
say chordates and vertebrates, we issue the two
following queries:
(a) chordates such as vertebrates
(b) vertebrates such as chordates
1113
If (a) returns more web hits than (b), then chordates
subsumes (or is broader than) vertebrates, other-
wise vertebrates subsumes chordates. For this
pair the such as pattern returned 7 hits for (a) and
0 hits for (b), so that the overall magnitude of the
direction of the relation is weak. To accumulate
stronger evidence, we issue web queries with the
remaining patterns. For the same concept pair, the
overall magnitude of ?X including Y? is 5820 hits
for (a) and 0 for (b).
As shown in Figure 3, the concept positioning pat-
terns cannot always determine the direct taxonomic
organization between two concepts as in the case
of felines and chordates, felines and vertebrates.
One reason is that the concepts are located on dis-
tant taxonomic levels. We humans typically exem-
plify concepts using more proximate ones. There-
fore, the concept positioning procedure can find ev-
idence for the relation ?mammals?felines?, but not
for ?chordates?felines?.
!"#$!%&
'()*(+)!*(,&
-(%#"(,&
$!$$!%,&
%#."&
/0.)1!*(,&
!"#$!%&
/0.)1!*(,&
'()*(+)!*(,&
$!$$!%,&
-(%#"(,&
%#."&
Figure 3: Concept Positioning and Induced Taxonomy.
After the concept positioning procedure has ex-
plored all concept pairs, we encounter two phenom-
ena: (1) direct links between some concepts are
missing and (2) multiple paths can be taken to reach
from one concept to another.
To surmount these problems, we employ a
graph based algorithm that finds the longest
path in the graph G??=(V ??, E??). The nodes
V ??={it1, h1, h2, .., hn, r} represent the input term,
its hypernyms, and the root. An edge (tm, tn) ? E??
indicates that there is a path between the terms tm
and tn. The direction tm ? tn indicates the term
subordination discovered during the CP procedure.
The objective is to find the longest path in G?? be-
tween the root and the input term. Intuitively, find-
ing the longest paths is equivalent to finding the tax-
onomic organization of all concepts.
First, if present, we eliminate all cycles from the
graph. Then, we find all nodes that have no prede-
cessor and those that have no successor. Intuitively,
a node with no predecessors p is likely to be posi-
tioned on the top of the taxonomy (e.g. animal),
while a node with no successor s is likely to be lo-
cated at the bottom (e.g. terms like lion, tiger, puma,
or concepts like krill predators that could not be re-
lated to an instance or a basic level concept during
the CP procedure). We represent the directed graph
as an adjacency matrix A = [am,n], where am,n is
1 if (tm, tn) is an edge of G??, and 0 otherwise. For
each (p, s) pair, we find the list of all paths connect-
ing p with s. In the end, from all discovered can-
didate paths, the algorithm returns the longest one.
The same graph-based taxonomization procedure is
repeated for the rest of the basic level concepts and
their hypernyms.
4 Experiments and Results
To evaluate the performance of a taxonomy induc-
tion algorithm, one can compare against a simple
taxonomy composed of 2?3 levels. However, one
cannot guarantee that the algorithm can learn larger
hierarchies completely or correctly.
Animals provide a good example of the true com-
plexity of concept organization: there are many
types, they are of numerous kinds, people take nu-
merous perspectives over them, and they are rela-
tively well-known to human annotators. In addition,
WordNet has a very rich and deep taxonomic struc-
ture for animals that can be used for direct compar-
ison. We further evaluate our algorithm on the do-
mains of Plants and Vehicles, which share some of
these properties.
4.1 Data Collection
We have run the knowledge harvesting algorithm on
the semantic classes Animals, Plants and Vehicles
starting with only one seed example such as lions,
cucumbers and cars respectively.
First, we formed and submitted the DAP pattern
as web queries to Yahoo!Boss. We retrieved the
top 1000 web snippets for each query. We kept
all unique terms and term pairs. Second, we used
the learned term pairs to form and submit new web
1114
queries DAP?1. In this step, the algorithm harvested
the hypernyms associated with each term. We kept
all unique triples composed of a hypernym and the
term pairs that extracted it. The algorithm ran until
complete exhaustion for 8 iterations for Animals, 10
iterations for Plants and 18 iterations of Vehicles.
Table 1 shows the total number of terms extracted
by the Web harvesting algorithm during the first
stage. In addition, we show the number of terms that
passed the outDegree threshold. We found that the
majority of the learned terms for Animals are basic
level concepts, while for Plants and Vehicles they are
a mixture of basic level and intermediate concepts.
Animals Plants Vehicles
#Extracted Terms 1855 2801 1425
#outDegree(Term)> 0 858 1262 581
Table 1: Learned Terms.
Since human based evaluation of all harvested
terms is time consuming and costly, we have se-
lected 90 terms located at the beginning, in the mid-
dle and in the end of the outDegree ranking. Table
2 summarizes the results.
Plants #CorrectByHand #inWN PrecByHand
rank[1-30] 29 28 .97
rank[420-450] 29 21 .97
rank[1232-1262] 27 19 .90
Vehicles #CorrectByHand #inWN PrecByHand
rank[1-30] 29 27 .97
rank[193-223] 22 18 .73
rank[551-581] 25 19 .83
Table 2: Term Evaluation.
Independently, we can say that the precision of the
harvesting algorithm is from 73 to 90%. In the case
of Vehicles, we found that the learned terms in the
middle ranking do not refer to the meaning of vehi-
cle as a transportation devise, but to the meaning of
vehicle as media (i.e. seminar, newspapers), com-
munication and marketing. For the same category,
the algorithm learned many terms which are missing
from WordNet such as BMW, bakkies, two-wheeler,
all-terrain-vehicle among others.
The second stage of the harvesting algorithm con-
cerns hypernym extraction. Table 3 shows the total
number of hypernyms harvested for all term pairs.
The top 20 highly ranked concepts by inDegree are
the most descriptive terms for the domain. However,
if we are interested in learning a larger set of hy-
pernyms, we found that inDegree is not sufficient
by itself. For example, highly frequent but irrele-
vant hypernyms such as meats, others are ranked
at the top of the list, while low frequent but rele-
vant ones such as protochordates, hooved-mammals,
homeotherms are discarded. This shows that we
need to develop additional and more sensitive mea-
sures for hypernym ranking.
Animals Plants Vehicles
#Extracted Hypernyms 1904 8947 2554
#inDegree(Hypernyms)> 10 110 294 100
Table 3: Learned Hypernyms.
Table 4 shows some examples of the learned an-
imal hypernyms which were annotated by humans
as: correct but not present in WordNet; borderline
which depending on the application could be valu-
able to have or exclude; and incorrect.
CorrectNotInWN {colony|social} insects, grazers, monogastrics
camelid, {mammalian|land|areal} predators
{australian|african} wildlife, filter feeders
hard shelled invertebrates, pelagics
bottom dwellers
Borderline prehistoric animals, large herbivores
pocket pets, farm raised fish, roaring cats
endangered mammals, mysterious hunters
top predators, modern-snakes, heavy game
Incorrect frozen foods, native mammals, red meats
furry predators, others, resources, sorts
products, items, protein
Table 4: Examples of Learned Animal Hypernyms.
The annotators found that 9% of the harvested is-
a relations are missing from WordNet. For example,
cartilaginous fish ? shark; colony insects? bees;
filter feeders? tube anemones among others. This
shows that despite its completeness, WordNet has
still room for improvement.
4.2 A Test: Reconstructing WordNet
As previously discussed in (Hovy et al, 2009), it is
extremely difficult even for expert to manually con-
struct and evaluate the correctness of the harvested
taxonomies. Therefore, we decided to evaluate the
performance of our taxonomization approach recon-
structing WordNet Animals, Plants and Vehicles tax-
onomies.
1115
Given a domain, we select from 140 to 170 of
the harvested terms. For each term, we retrieve all
WordNet hypernyms located on the path between the
input term and the root that is animal, plant or ve-
hicle depending on the domain of interest. We have
found that 98% of the WordNet terms are also har-
vested by our knowledge acquisition algorithm. This
means that being able to reconstruct WordNet?s tax-
onomy is equivalent to evaluating the performance
of our taxonomy induction approach.
Table 5 summarizes the characteristics of the tax-
onomies for the regions tested. For each domain,
we show the total number of terms that must be or-
ganized, and the total number of is-a relations that
must be induced.
Animals Plants Vehicles
#terms 684 554 140
#is-a 4327 2294 412
average depth 6.23 4.12 3.91
max depth 12 8 7
min depth 1 1 1
Table 5: Data for WordNet reconstruction.
Among the three domains we have tested, An-
imals is the most complex and richest one. The
maximum number of levels our algorithm must in-
fer is 11, the minimum is 1 and the average taxo-
nomic depth is 6.2. In total there are three basic level
concepts (longhorns, gaur and bullock) with maxi-
mum depth, twenty terms (basic level and intermedi-
ate concepts) with minimum depth and ninety-eight
terms (wombat, viper, rat, limpkin) with depth 6.
Plants is also a very challenging domain, because
it contains a mixture of scientific and general terms
such as magnoliopsida and flowering plant.
4.3 Evaluation
To evaluate the performance of our taxonomy induc-
tion approach, we use the following measures:
Precision = #is?a found in WordNet and by system#is?a found by system
Recall = #is?a found in WordNet and by system#is?a found in WordNet
Table 6 shows results of the taxonomy induction
of the Vehicles domain using different concept po-
sitioning patterns. The most productive ones are:
?X are Y that? and ?X including Y?. However, the
highest yield is obtained when we combine evidence
from all patterns.
Vehicles Precision Recall
X such as Y .99 (174/175) .42 (174/410)
X are Y that .99 (206/208) .50 (206/410)
X including Y .96 (165/171) .40 (165/410)
X like Y .96 (137/142) .33 (137/410)
such X as Y .98 (44/45) .11 (44/410)
AllPatterns .99 (246/249) .60 ( 246/410)
Table 6: Evaluation of the Induced Vehicle Taxonomy.
Table 7 shows results of the taxonomization of
the Animals and Plants domains. Overall, the ob-
tained results are very encouraging given the fact
that we started from scratch without the usage of
any taxonomic structure. Precision is robust, but we
must further improve recall. Our observation for the
lower recall is that some intermediate concepts re-
late mostly to the high level ones, but not to the basic
level concepts.
Precision Recall
Animals .98 (1643/1688) .38 (1643/4327)
Plants .97 (905/931) .39 (905/2294)
Table 7: Evaluation of the Induced Animal and Plant Tax-
onomies.
Figure 4 shows an example of the taxonomy in-
duced by our algorithm for the vipers, rats, wom-
bats, ducks, emus, moths and penguins basic level
concepts and their WordNet hypernyms.
animals
aquatic_vertebrates chordates invertebrates
vertebrates arthropods
aquatic_birds
duckspenguins
insects
moths
birds
emus
mammalsreptiles
marsupials placentalsrodents
wombats rats
metatherians
snakes
vipers
Figure 4: Induced Taxonomy for Animals.
The biggest challenge of the taxonomization pro-
cess is the merging of independent taxonomic per-
1116
spectives (a deer is a grazer in BehaviorByFeeding,
a wildlife in BehaviorByHabitat, a herd in Behavior-
SocialGroup and an even-toed ungulate in Morpho-
logicalType) into a single hierarchy.
5 Conclusions and Future Work
We are encouraged by the ability of the taxonomiza-
tion algorithm to reconstruct WordNet?s Animal hi-
erarchy, which is one of its most complete and elab-
orated. In addition, we have also evaluated the per-
formance of our algorithm with the Plant and Vehi-
cle WordNet hierarchies.
Currently, our automated taxonomization algo-
rithm is able to build some of the quasi-independent
perspectival taxonomies (Hovy et al, 2009). How-
ever, further research is required to develop methods
that reliably (a) identify the number of independent
perspectives a concept can take (or seems to take in
the domain text), and (b) classify any harvested term
into one or more of them. The result would greatly
simplify the task of the taxonomization stage.
We note that despite this richness, WordNet has
many concepts like camelid, filter feeder, mono-
gastrics among others which are missing, but the
harvesting algorithm can provide. Another promis-
ing line of research would investigate the combina-
tion of the two styles of taxonomization algorithms:
first, the one described here to produce an initial (set
of) taxonomies, and second, the term-insertion algo-
rithms developed in prior work.
Acknowledgments
We acknowledge the support of DARPA contract
number FA8750-09-C-3705. We thank Mark John-
son for the valuable discussions on taxonomy evalu-
ation. We thank the reviewers for their useful feed-
back and suggestions.
References
Dmitry Davidov and Ari Rappoport. 2006. Efficient un-
supervised discovery of word categories using sym-
metric patterns and high frequency words. In Proceed-
ings of the 21st International Conference on Compu-
tational Linguistics and the 44th annual meeting of the
ACL, pages 297?304.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proceedings
of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 232?239.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1?8.
Oren Glickman, Ido Dagan, and Moshe Koppel. 2005.
A probabilistic classification approach for lexical tex-
tual entailment. In Proceedings, The Twentieth Na-
tional Conference on Artificial Intelligence and the
Seventeenth Innovative Applications of Artificial Intel-
ligence Conference, pages 1050?1055.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539?
545.
Eduard H. Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction and
classification. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP 2009, pages 948?957.
Eduard Hovy. 2002. Comparing sets of semantic rela-
tions in ontologies. The Semantics of Relationships:
An Interdisciplinary Perspective, pages 91?110.
Boris Katz and Jimmy Lin. 2003. Selectively using rela-
tions to improve precision in question answering. In In
Proceedings of the EACL-2003 Workshop on Natural
Language Processing for Question Answering, pages
43?50.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proceedings of the 19th international
conference on Computational linguistics, pages 1?7.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In Proceedings of the 17th interna-
tional conference on Computational linguistics, pages
768?774.
Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. Lasso: A tool for surfing the answer
net. In TREC.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of 21st Interna-
tional Conference on Computational Linguistics and
1117
44th Annual Meeting of the Association for Computa-
tional Linguistics, ACL 2006.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
Marco Pennacchiotti and Patrick Pantel. 2006. Ontolo-
gizing semantic relations. In ACL-44: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 793?800.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons. In
Proceedings of the Second Conference on Empirical
Methods in Natural Language Processing, pages 117?
124.
Alan Ritter, Stephen Soderland, and Oren Etzioni. 2009.
What is this, anyway: Automatic hypernym discovery.
In Proceedings of AAAI Spring Symposium on Learn-
ing by Reading and Learning to Read.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent dirichlet alocation method for selectional prefer-
ences. In to appear in Proceedings of the Association
for Computational Linguistics ACL2010.
Eleanor Rosch. 1978. Principles of categorization.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2006.
Semantic taxonomy induction from heterogenous ev-
idence. In Proceedings of 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Linguis-
tics, ACL.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Idan Szpektor, Ido Dagan, Roy Bar-Haim, and Jacob
Goldberger. 2008. Contextual preferences. In ACL
2008, Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics, pages
683?691.
Dominic Widdows. 2003. Unsupervised methods for de-
veloping taxonomies by combining syntactic and sta-
tistical information. In Proceedings of HLT-NAACL.
Hui Yang and Jamie Callan. 2009. A metric-based
framework for automatic taxonomy induction. In
ACL-IJCNLP ?09: Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP: Volume 1, pages 271?279.
1118
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257?1268,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Fast, Accurate, Non-Projective, Semantically-Enriched Parser
Stephen Tratz and Eduard Hovy
Information Sciences Institute
University of Southern California
Marina del Rey, California 90292
{stratz,hovy}@isi.edu
Abstract
Dependency parsers are critical components
within many NLP systems. However, cur-
rently available dependency parsers each ex-
hibit at least one of several weaknesses, in-
cluding high running time, limited accuracy,
vague dependency labels, and lack of non-
projectivity support. Furthermore, no com-
monly used parser provides additional shal-
low semantic interpretation, such as prepo-
sition sense disambiguation and noun com-
pound interpretation. In this paper, we present
a new dependency-tree conversion of the Penn
Treebank along with its associated fine-grain
dependency labels and a fast, accurate parser
trained on it. We explain how a non-projective
extension to shift-reduce parsing can be in-
corporated into non-directional easy-first pars-
ing. The parser performs well when evalu-
ated on the standard test section of the Penn
Treebank, outperforming several popular open
source dependency parsers; it is, to the best
of our knowledge, the first dependency parser
capable of parsing more than 75 sentences per
second at over 93% accuracy.
1 Introduction
Parsers are critical components within many natu-
ral language processing (NLP) systems, including
systems for information extraction, question answer-
ing, machine translation, recognition of textual en-
tailment, summarization, and many others. Unfortu-
nately, currently available dependency parsers suf-
fer from at least one of several weaknesses includ-
ing high running time, limited accuracy, vague de-
pendency labels, and lack of non-projectivity sup-
port. Furthermore, few parsers include any sort of
additional semantic interpretation, such as interpre-
tations for prepositions, possessives, or noun com-
pounds.
In this paper, we describe 1) a new dependency
conversion (Section 3) of the Penn Treebank (Mar-
cus, et al, 1993) along with the associated de-
pendency label scheme, which is based upon the
Stanford parser?s popular scheme (de Marneffe and
Manning, 2008), and a fast, accurate dependency
parser with non-projectivity support (Section 4) and
additional integrated semantic annotation modules
for automatic preposition sense disambiguation and
noun compound interpretation (Section 5). We show
how Nivre?s (2009) swap-based reordering tech-
nique for non-projective shift-reduce-style parsing
can be integrated into the non-directional easy-first
framework of Goldberg and Elhadad (2010) to sup-
port non-projectivity, and we report the results of our
parsing experiments on the standard test section of
the PTB, providing comparisons with several freely
available parsers, including Goldberg and Elhadad?s
(2010) implementation, MALTPARSER (Nivre et al,
2006), MSTPARSER (McDonald et al, 2005; Mc-
Donald and Pereira, 2006), the Charniak (2000)
parser, and the Berkeley parser (Petrov et al, 2006;
Petrov and Klein, 2007).
The experimental results show that the parser is
substantially more accurate than Goldberg and El-
hadad?s original implementation, with fairly simi-
lar overall speed. Furthermore, the results prove
that Stanford-granularity dependency labels can be
learned by modern dependency parsing systems
when using our Treebank conversion, unlike the
Stanford conversion, for which Cer et al (2010)
show that this isn?t the case.
The optional semantic annotation modules also
1257
perform well, with the preposition sense disam-
biguation module exceeding the accuracy of the pre-
vious best reported result for fine-grained preposi-
tion sense disambiguation (85.7% vs Hovy et al?s
(2010) 84.8%), the possessives interpretation sys-
tem achieving over 85% accuracy, and the noun
compound interpretation system performing simi-
larly to an earlier version described by Tratz and
Hovy (2010) at just over 79% accuracy.
2 Background
The NLP community has recently seen a surge of
interest in dependency parsing, with several CoNLL
shared tasks focusing on it (Buchholz and Marsi,
2006; Nivre et al, 2007). One of the main advan-
tages of dependency parsing is the relative ease with
which it can handle non-projectivity1. Additionally,
since each word is linked directly to its head via a
link that, ideally, indicates the syntactic dependency
type, there is no difficulty in determining either the
syntactic head of a particular word or the syntactic
relation type, whereas these issues often arise when
dealing with constituent parses2.
Unfortunately, most currently available depen-
dency parsers produce relatively vague labels or, in
many cases, produce no labels at all. While the
Stanford fine-grain dependency scheme (de Marn-
effe and Manning, 2008) has proven to be popular,
recent experiments by Cer et al (2010) using the
Stanford conversion of the Penn Treebank indicate
that it is difficult for current dependency parsers to
learn. Indeed, the highest scoring parsers trained us-
ing the MSTPARSER (McDonald and Pereira, 2006)
and MALTPARSER (Nivre et al, 2006) parsing suites
achieved only 78.8 and 81.1 labeled attachment
F1, respectively. This contrasted with the much
higher performance obtained using a constituent-to-
dependency conversion approach with accurate, but
much slower, constituency parsers such as the Char-
niak and Johnson (2005) and Berkeley (Petrov et
al., 2006; Petrov and Klein, 2007) parsers, which
achieved 89.1 and 87.9 labeled F1 scores, respec-
tively.
1A tree is non-projective if the sequence of words visited in
a left-to-right, depth-first traversal of the sentence?s parse tree is
different than the actual word order of the sentence.
2These latter two issues are not problems for constituent
parses with binarized output and functional tags.
Though there are many syntactic parsers than can
reconstruct the grammatical structure of a text, there
are few, if any, accurate and widely accepted sys-
tems that also produce shallow semantic analysis of
the text. For example, a parser may indicate that,
in the case of ?ice statue?, ?ice? modifies ?statue? but
will not indicate that ?ice? is the substance of the
statue. Similarly, a parser will indicate which words
a preposition connects but will not give any seman-
tic interpretation (e.g., ?the boy with the pirate hat?
? wearing or carrying, ?wash with cold water? ?
means, ?shave with the grain? ? in the same direc-
tion as). While, in some cases, it may be possible to
use the output from a separate system for this pur-
pose, doing so is often difficult in practice due to a
wide variety of complications, including program-
ming language differences, alternative data formats,
and, sometimes, other parsers.
3 Dependency Conversion
3.1 Relations and Structure
Most recent English dependency parsers produce
one of three sets of dependency types: unlabeled,
some variant of the coarse labels used by the
CoNLL dependency parsing shared-tasks (Buchholz
and Marsi, 2006; Nivre et al, 2007) (e.g., ADV,
NMOD, PMOD), or Stanford?s dependency labels
(de Marneffe and Manning, 2008). Unlabeled de-
pendencies are clearly too impoverished for many
tasks. Similarly, the coarse labels of the CoNLL
tasks are not very specific; for example, the same re-
lation, NMOD, is used for determiners, adjectives,
nouns, participle modifiers, relative clauses, etc. that
modify nouns. In contrast, the Stanford relations
provide a more reasonable level of granularity.
Our dependency relation scheme is similar to
Stanford?s basic scheme but has several differ-
ences. It introduces several new relations including
ccinit ?initial coordinating conjunction?, cleft ?cleft
clause?, combo ?combined term?, extr ?extraposed
element?, infmark ?infinitive marker ?to? ?, objcomp
?object complement?, postloc ?post-modifying lo-
cation?, sccomp ?clausal complement of ?so? ?, vch
?verbal chain? and whadvmod ?wh- adverbial mod-
ifier?. The nsubjpass, csubjpass, and auxpass rela-
tions of Stanford?s are left out because adding them
up front makes learning more difficult and the fact
1258
abbrev abbreviation csubjpass clausal subject (passive) pobj prepositional object
acomp adjectival complement det determiner poss possessive
advcl adverbial clause dobj direct object possessive possessive marker
advmod adverbial modifier extr extraposed element postloc post-modifying location
agent ?by? agent expl ?there? expletive preconj pre conjunct
amod adjectival modifier infmark infinitive marker (?to?) predet predeterminer
appos appositive infmod infinite modifier prep preposition
attr attributive iobj indirect object prt particle
aux auxillary mark subordinate clause marker punct punctuation
auxpass auxillary (passive) measure measure modifier purpcl purpose clause
cleft cleft clause neg negative quantmod quantifier modifier
cc coordination nn noun compound rcmod relative clause
ccinit initial CC nsubj nominal subject rel relative
ccomp clausal complement nsubjpass nominal subject (passive) sccomp clausal complement of ?so?
combo combination term num numeric modifier tmod temporal modifier
compl complementizer number compound number vch verbal chain
conj conjunction objcomp object complement whadvmod wh- adverbial
cop copula complement parataxis parataxis xcomp clausal complement w/o subj
csubj clausal subject partmod participle modifier
Table 1: Dependency scheme with differences versus basic Stanford dependencies highlighted. Bold indicates the
relation does not exist in the Stanford scheme. Italics indicate the relation appears in Stanford?s scheme but not ours.
that a nsubj, csubj, or aux is passive can easily be de-
termined from the final tree. Stanford?s aux depen-
dencies are replaced using verbal chain (vch) links;
conversion of these to Stanford-style aux dependen-
cies is also trivial as a post-processing step.3 The attr
dependency is excluded because it is redundant with
the cop relation due to different handling of copula,
and the dependency scheme does not have an abbrev
label because this information is not provided by the
Penn Treebank. The dependency scheme with dif-
ferences with Stanford highlighted is presented in
Table 1.
In addition to using a slightly different set of de-
pendency names, a handful of relations, notably cop,
conj, and cc, are treated in a different manner. These
differences are illustrated by Figure 1. The Stan-
ford scheme?s treatment of copula may be one rea-
son why dependency parsers have trouble learning
and applying it. Normally, the head of the clause
is a verb, but, under Stanford?s scheme, if the verb
happens to be a copula, the complement of the cop-
ula (cop) is treated as the head of the clause instead.
3The parsing system includes an optional script that can con-
vert vch arcs into aux and auxpass and the subject relations into
csubjpass and nsubjpass.
Figure 1: Example comparing Stanford?s (top) handling
of copula and coordinating conjunctions with ours (bot-
tom).
3.2 Conversion Process
A three-step process is used to convert the Penn
Treebank (Marcus, et al, 1993) from constituent
parses into dependency trees labeled according to
the dependency scheme presented in the prior sec-
tion. The first step is to apply the noun phrase
structure patch created by Vadas and Curran (2007),
which adds structure to the otherwise flat noun
phrases (NPs) of the Penn Treebank (e.g., ?(metal
soup pot cover)? would become ?(metal (soup pot)
cover)?). The second step is to apply a version
of Johansson and Nugues? (2007) constituent-to-
dependency converter with some head-finding rule
modifications; these rules, with changes highlighted
1259
(WH)?NP|NX|NML|NAC FW|NML|NN* JJR $|# CD|FW QP JJ|NAC JJS PRP ADJP RB[SR] VBG|DT|WP
RB NP- S|SBAR|UCP|PP SINV|SBARQ|SQ UH VP|NP VB|VBP
ADJP|JJP NNS QP NN $|# JJ VBN VBG (AD|J)JP ADVP JJR NP|NML JJS DT FW RBR RBS SBAR RB
ADVP RB|RBR|JJ|JJR RBS FW ADVP TO CD IN NP|NML JJS NN
PRN S* VP NN*|NX|NML NP W* PP|IN ADJP|JJ ADVP RB NAC VP INTJ
QP $|# NNS NN CD JJ RB DT NCD QP IN CC JJR JJS
SBARQ SQ S SBARQ SINV FRAG
SQ VBZ VBD VBP VB MD *-PRD SQ VP FRAG X
UCP [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X
VP VBD|AUX VBN MD VBZ VB VBG VBP VP POS *-PRD ADJP JJ NN NNS NP|NML
WHADJP CC JJ WRB ADJP
WHADVP CC WRB|RB
X [QNVP]P|S*|UCP|NML|PR[NT]|RRC|NX|NAC|FRAG|INTJ|AD[JV]P|LST|WH*|X|CONJP
LST LS : DT|NN|SYM
Figure 2: Modified head-finding rules. Underline indicates that the search is performed in a left-to-right fashion instead
of the default right-to-left order. NML and JJP are both products of Vadas and Curran?s (2007) patch. Bold indicates
an added or moved element; for the original rules, see the paper by Johansson and Nugues (2007).
in bold, are provided in Figure 2. Finally, an addi-
tional script makes additional changes and converts
the intermediate output into the dependency scheme.
This dependency conversion has several advan-
tages to it. Using the modified head-finding rules for
Johansson and Nugues? (2007) converter results in
fewer buggy trees than were present in the CoNLL
shared tasks, including fewer trees in which words
are headed by punctuation marks. For sections 2?
21, there are far fewer generic dep/DEP relations
(2,765) than with the Stanford conversion (34,134)
or the CoNLL 2008 shared task conversion (23,811).
Also, the additional conversion script contains vari-
ous rules for correcting part-of-speech (POS) errors
using the syntactic structure as well as additional
rules for some specific word forms, mostly common
words with inconsistent taggings. Many of these
changes cover part-of-speech problems discussed by
Manning (2011), including VBD/VBN, VBZ/NNS,
NNP/NNPS, and IN/WDT/DT issues. In total, the
script changes over 9,500 part-of-speech tags, with
the most common change being to change preposi-
tion tags (IN) into adverb tags (RB) for cases where
there is no prepositional complement/object. The
top fifteen of these changes are presented in Table
2. The conversion script contains a variety of ad-
ditional rules for modifying the parse structure and
fixing erroneous trees as well, including cases where
one or more POS tags were incorrect and, as such,
the initial dependency parse was flawed. Quick
manual inspections of the changes suggested that the
vast majority are accurate.
In the final output from the conversion, the num-
ber of sentences with one or more words dependent
on non-projective arcs in sections 2?21 is 3,245?
about 8.1% of the dataset. About 1.3% of this, or
556 of sentences, is due to the secondary conver-
sion script, with sentences containing approximate
currency amounts (e.g., about $ 10) comprising the
bulk of difference. For these, the quantifying text
(e.g., about, over, nearly), is linked to the number
following the currency symbol instead of to the cur-
rency symbol as it was in the CoNLL 2008 task.
Original New # of changes
IN RB 1128
JJ NN 787
VBD VBN 601
RB IN 462
VBN VBD 441
NN JJ 409
NNPS NNP 405
IN WDT 388
VBG NN 223
DT IN 220
RB JJ 214
VB VBP 184
NN NNS 169
RB NN 157
NNS VBZ 148
Table 2: Top 15 part-of-speech tag changes performed by
the conversion script.
1260
4 Parser
4.1 Algorithm
The parsing approach is based upon the non-
directional easy-first algorithm recently presented
by Goldberg and Elhadad (2010). Their original al-
gorithm behaves as follows. For a sentence of length
n, the algorithm performs a total of n steps. In each
step, one of the unattached tokens is added as a child
to one of its current neighbors and is then removed
from the list of unprocessed tokens. When only one
token remains unprocessed, it is designated as the
root. Provided that only a constant number of po-
tential attachments need to be re-evaluated after each
step, which is the case if one restricts the context for
feature generation to a constant number of neigh-
boring tokens, the algorithm can be implemented to
run in O(n log n). However, since only O(n) dot
products must be calculated by the parser and these
have a large constant associated with them, the run-
ning time will rival O(n) parsers for any reasonable
n, and, thus, a naive O(n2) implementation will be
nearly as fast as a priority queue implementation in
practice.4
The algorithm has a couple potential advantages
over standard shift-reduce style parsing algorithms.
The first advantage is that performing easy ac-
tions first may make the originally difficult deci-
sions easier. The second advantage is that perform-
ing parse actions in a more flexible order than left-
to-right/right-to-left shift-reduce parsing reduces the
chance of error propagation.
Unfortunately, the original algorithm does not
support non-projective trees. To extend the algo-
rithm to support non-projective trees, we introduce
move-right and move-left operations similar to the
stack-to-buffer swaps proposed by Nivre (2009) for
shift-reduce style parsing. Thus, instead of attaching
a token to one of its neighbors at each step, the algo-
rithm may instead decide to move a token past one
of its neighbors. Provided that no node is allowed
to be moved past a token in such a way that a previ-
ous move operation is undone, there can be at most
O(n2) moves and the overall worst-case complexity
becomes O(n2 log n). While theoretically slower,
this has a limited impact upon actual parsing times
4See Goldberg and Elhadad (2010) for more explanation.
in practice, especially for languages with relatively
fixed word order such as English.5 Though Gold-
berg and Elhadad?s (2010) original implementation
only supports unlabeled dependencies, the algorithm
itself is in no way limited in this regard, and it is
simple enough to add labeled dependency support
by treating each dependency label as a specific type
of attach operation (e.g., attach_as_nsubj), which
is the method used by this implementation. Pseu-
docode for the non-directional easy-first algorithm
with non-projective support is given in Algorithm 1.
input : w1 ... wn, #the sentence
m, #the model
k, #the context width
actions, #the list of parse actions
?, #the feature generator
output: tree #a collection of dependency arcs
words = copyOf(s);
stale = copyOf (s);
cache; #cache of action scores
while |words| > 1 do
for w ? stale do
for act ? actions do
cache[w,act] = score(act, ?(w,...),
m);
stale.remove(w);
best = argmax
a?actions&valid(a),w?words
cache[w, a]
if isMove(best) then
i =
words.index(getTokenToMove(best));
words.move (i, isMoveLeft(best) ? -1
: 1);
else
arc = createArc(best);
tree.add(arc);
i = words.index(getChild(arc));
words.remove(i);
for x ? -k,...,k do
stale.add(words.get(index+x));return tree
Algorithm 1: Modified version of Goldberg and
Elhadad?s (2010) Easy-First Algorithm with non-
projective support.
5See Nivre (2009) for more information on the effect of re-
ordering operations on parse time.
1261
4.2 Features
One of the key aspects of the parser is the complex
set of features used. The feature set is based off
the features used by Goldberg and Elhadad (2010)
but has a significant number of extensions. Various
feature templates are specifically designed to pro-
duce features that help with several syntactic issues
including preposition attachment, coordination, ad-
verbial clauses, clausal complements, and relative
clauses. Unfortunately, there is insufficient space in
this paper to describe them all here. However, a list
of feature templates will be provided with the parser
download.
Several of the feature templates use unsupervised
word clusters created with the Brown et al (1992)
hierarchical clustering algorithm. The use of this al-
gorithm was inspired by Koo et al (2008), who used
the top branches of the cluster hierarchy as features.
However, unlike Koo et al?s (2008) parser, the fine-
grained cluster identifiers are used instead of just
the top 4-6 branches of the cluster hierarchy. The
175 word clusters utilized by the parser were created
from the New York Times corpus (Sandhaus, 2008).
Some examples from the clusters are presented in
Figure 3. The ideal number of such clusters was not
thoroughly investigated.
while where when although despite unless unlike ...
why what whom whatever whoever whomever whence ...
based died involved runs ended lived charged born ...
them him me us himself themselves herself myself ...
really just almost nearly simply quite fully virtually ...
know think thought feel believe knew felt hope mean ...
into through on onto atop astride Saturday/Early thru ...
Ms. Mr. Dr. Mrs. Judge Miss Professor Officer Colonel ...
John President David J. St. Robert Michael James George ...
wife own husband brother sister grandfather beloved ...
often now once recently sometimes clearly apparently ...
everyone it everybody somebody anybody nobody hers ...
around over under among near behind outside across ...
Clinton Bush Johnson Smith Brown Williams King ...
children companies women people men things students ...
Figure 3: High frequency examples from 15 of the Brown
clusters.
4.3 Training
The parsing model is trained using a variant of the
structured perceptron training algorithm used in the
original Goldberg and Elhadad (2010) implementa-
tion. The general idea of the algorithm is to iterate
over the sentences and, whenever the model predicts
an incorrect action, update the model weights. Fol-
lowing Goldberg and Elhadad, parameter averaging
is used to reduce overfitting.
Our implementation varies slightly from that of
Goldberg and Elhadad (2010). The difference is
that, at any particular step for a given sentence, the
algorithm continues to update the weight vector as
long as any invalid action is scored higher than any
valid action, not just the highest scoring valid ac-
tion; unfortunately, this change significantly slowed
down the training process. In early experiments, this
change produced a slight improvement in accuracy
though it also slowed training significantly. In later
experiments using additional feature templates, this
change ceased to have any notable impact on the
overall accuracy, but it was kept anyway. 6
The oracle used to determine whether a move op-
eration should be considered legal during the train-
ing phase is similar to Nivre et al?s (2009) improved
oracle based upon maximal projective subcompo-
nents. As an additional restriction, during training,
move actions were only considered valid either if no
other action was valid or if the token to be moved
already had all its children attached and moving it
caused it to be adjacent to its parent. This fits with
Nivre et al?s (2009) intuition that it is best to delay
word reordering as long as possible.
4.4 Speed Enhancements
To enhance the speed for practical use, the parser
uses constraints based upon the part-of-speech tags
of the adjacent word pairs to eliminate invalid de-
pendencies from even being evaluated. A rela-
tion is only considered between a pair of words if
such a relation was observed in the training data
between a pair of words with the same parts-of-
speech (with the exception of the generic dep de-
pendency, which is permitted between any POS tag
pair). Early experiments utilizing similar constraints
showed an improvement in parsing speed of about
16% with no significant impact on accuracy, regard-
less of whether the constraints were enforced during
training.
6See Goldberg and Elhadad (2010) for more description of
the general training procedure.
1262
System Arc Accuracy Perfect Sentences Non-Proj ArcsLabeled Unlabeled Labeled Unlabeled Labeled Unlabeled
THIS WORK 92.1 (93.3) 93.7 (94.3) 38.4 (42.5) 46.2 (48.5) 66.5 (69.7) 69.3 (71.7)
THIS WORKno clusters 91.8 (93.1) 93.4 (94.1) 38.2 (42.3) 45.5 (47.3) 67.3 (70.9) 69.3 (72.5)
THIS WORKmoves disabled 91.7 (92.9) 93.3 (93.9) 37.1 (40.8) 44.2 (46.2) 21.1 (21.1) 22.7 (21.9)
NON-DIR EASY FIRST * 91.2 (92.0) * 37.8 (39.4) * 15.1 (16.3)
EISNER?MST 90.9 (92.2) 92.8 (93.5) 32.1 (35.6) 40.6 (42.3) 62.5 (65.3) 63.7 (66.9)
CHU-LIU-EDMONDSMST 90.0 (91.2) 91.8 (92.5) 28.4 (31.3) 35.0 (36.4) 62.9 (65.3) 64.1 (66.5)
ARC-EAGERMalt 89.8 (91.1) 91.3 (92.1) 31.6 (34.2) 37.4 (38.5) 19.5 (19.5) 20.3 (19.9)
ARC-STANDARDMalt 88.3 (89.5) 89.7 (90.4) 31.4 (34.1) 36.1 (37.3) 13.1 (12.0) 13.9 (12.7)
STACK-EAGERMalt 90.0 (91.2) 91.5 (92.3) 34.5 (37.5) 40.4 (41.9) 51.8 (53.8) 53.8 (55.4)
STACK-LAZYMalt 90.4 (91.7) 91.9 (92.8) 34.8 (37.7) 40.6 (42.5) 61.8 (63.3) 63.3 (65.3)
CHARNIAK? * 93.2 * 43.5 * 32.3
BERKELEY? * 93.3 * 43.6 * 34.3
Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were
produced using gold POS tags. ?Eisner (1996) algorithm with non-projective rewriting and second order features.
?Results not directly comparable; see text. ?Labeled dependencies not available/comparable.
4.5 Evaluation
The following split of the Penn Treebank (Marcus,
et al, 1993) was used for the experiments: sections
2?21 for training, 22 for development, and 23 for
testing.
For part-of-speech (POS) tagging, we used an in-
house SVM-based POS tagger modeled after the
work of Gim?nez and M?rquez (2004) 7. The train-
ing data was tagged in a 10-fold fashion; each fold
was tagged using a tagger trained from the nine re-
maining folds. The development and test sections
were tagged by an instance of the tagger trained us-
ing the entire training set. The full details of the
POS tagger are outside the scope of this paper; it is
included with the parser download.
The final parser was trained for 31 iterations,
which is the point at which its performance on the
development set peaked. One test run was per-
formed with non-projectivity support disabled in or-
der to get some idea of the impact of the move opera-
tions on the parser?s overall performance; also, since
the parsers used for comparison had no access to the
unsupervised word clusters, an additional instance
of the parser was trained with every word treated
as belonging to the same cluster so as to facilitate
a more fair comparison.
Seven different dependency parsing models were
797.42% accuracy on traditional POS evaluation (Penn Tree-
bank WSJ sections 22-24).
trained for comparison using the following open
source parsing packages: Goldberg and Elhadad?s
(2010)?s non-directional easy-first parser, MALT-
PARSER (Nivre et al, 2006), and MSTPARSER
(McDonald and Pereira, 2006)8. The model trained
using Goldberg and Elhadad?s (2010) easy-first
parser serves as something of a baseline. The
four MALTPARSER parsing models used the arc-
eager, arc-standard, stack-eager, and stack-lazy al-
gorithms. One of the MSTPARSER models used
the Chu-Liu-Edmonds maximum spanning tree ap-
proach, and the other used the Eisner (1996) al-
gorithm with second order features and a non-
projective rewriting post-processing step.
Unfortunately, it is not possible to directly com-
pare the parser?s accuracy with most popular con-
stituent parsers such as the Charniak (2000) and
Berkeley (Petrov et al, 2006; Petrov and Klein,
2007) parsers9 both because they do not pro-
duce functional tags for subjects, direct objects,
etc., which are required for the final script of the
constituent-to-dependency conversion routine, and
because they determine part-of-speech tags in con-
junction with the parsing. However, it is possible to
compute approximate unlabeled accuracy scores by
training the constituent parsers on the NP-patched
(Vadas and Curran, 2007) version of the data and
then running the test output through just the first
conversion script?that is, the modified version of
Johansson and Nugues? (2007) converter.
1263
The results of the experiment are given in Ta-
ble 3, including accuracy for individual arcs, non-
projective arcs only, and full sentence match. Punc-
tuation is excluded in all the result computations. To
determine whether an arc is non-projective, the fol-
lowing heuristic was used. Traverse the sentence in
a depth-first search, starting from the imaginary root
node and pursuing child arcs in order of increasing
absolute distance from their parent. Whenever an
arc being traversed is found to cross a previously tra-
versed arc, mark it as non-projective and continue.
To evaluate the impact of part-of-speech tagging er-
ror, results for parsing using the gold standard part-
of-speech tags are also included.
We also measured the speed of the parser on the
various sentences in the test collection. For reason-
able sentence lengths, the parser scales quite well.
The scatterplot depicting the relation between sen-
tence length and parsing time is presented in Figure
5.
Figure 4: Parse times for Penn Treebank section 23 for
the parsers on a PC with a 2.4Ghz Q6600 processor and
8GB RAM. MALTPARSER ran substantially slower than
the others, perhaps due to its use of polynomial kernels,
and isn?t shown. (C-L-E - Chu-Liu-Edmonds, G&E -
Goldberg and Elhadad (2010)).
4.5.1 Results Discussion
The parser achieves 92.1% labeled and 93.7% un-
labeled accuracy on the evaluation, a solid result and
about 2.5% higher than the original easy-first imple-
mentation of Goldberg and Elhadad (2010). Further-
more, the parser processed the entire test section in
8Versions 1.4.1, 0.4.3b, and 0.2, respectively
9Versions 1.1 and 05Aug16, respectively
just over 30 seconds?a rate of over 75 sentences per
second, substantially faster than most of the other
parsers.
Not surprisingly, the results for non-projective
arcs are substantially lower than the results for all
arcs, and the systems that are designed to handle
them outperformed the strictly projective parsers in
this regard.
The negative effect of part-of-speech tagging er-
ror appears to impact the different parsers about the
same amount, with a loss of .6% to .8% in unlabeled
accuracy and 1.1% to 1.3% in labeled accuracy.
The 93.2% and 93.3% accuracy scores achieved
by the Charniak and Berkeley parsers are not too
different from the 93.7% result, but, of course, it is
important to remember that these scores are not di-
rectly comparable.
Figure 5: Sentence length versus parse time. Median
times for five runs over section 23.
5 Shallow Semantic Annotation
To create a more informative parse, the parser in-
cludes four optional modules, a preposition sense
disambiguation (PSD) system, a work-in-progress
?s-possessive interpretation system, a noun com-
pound interpretation system, and a PropBank-based
semantic role labeling system10. Taken together,
these integrated modules enable the parsing sys-
tem to produce substantially more informative out-
put than a traditional parser.
Preposition Sense Disambiguation The PSD
system is a newer version of the system described
10Lack of space prohibits a sufficiently thorough discussion
of these individual components and their evaluations, but addi-
tional information will be available with the system download.
1264
by Tratz and Hovy (2009) and Hovy et al (2010); it
achieves 85.7% accuracy on the SemEval-2007 fine-
grain PSD task (Litkowski and Hargraves, 2007),
which is a statistically significant (p<=0.05; upper-
tailed z test) increase over the previous best reported
result for this dataset, Hovy et al?s (2010) 84.8%.
Noun Compound Interpretation The noun com-
pound interpretation system is a newer version of
the system described by Tratz and Hovy (2010) with
similar accuracy (79.6% vs 79.3% using 10-fold
cross-validation11).
Possessives Interpretation The possessive inter-
pretation system assigns interpretations to ?s pos-
sessives (e.g., John?s arm ? PART-OF, Mowgli?s
capture ? PATIENT/THEME). The current system
achieves over 85.0% accuracy, but it is important to
note that the annotation scheme, automatic classifier,
and dataset are all still under active development.
PropBank SRL The PropBank-based semantic
role labeling system achieves 86.8 combined F1
measure for automatically-generated parse trees cal-
culated over both predicate disambiguation and ar-
gument/adjunct classification (89.5 F1 on predicate
disambiguation, 85.6 F1 on argument and adjuncts
corresponding to dependency links, and 86.8 F1);
this score is not directly comparable to any previ-
ous work due to some differences, including differ-
ences in both the parse tree conversion and the Prop-
Bank conversion. The most similar work is that of
the CoNLL shared task work (Surdeanu et al, 2008;
Hajic? et al, 2009).
6 Related Work
Non-projectivity. There are two main approaches
used in recent NLP literature for handling non-
projectivity in parse trees. The first is to use an al-
gorithm, like the one presented in this paper, that
has inherent support for non-projective trees. Ex-
amples of this include the Chu-Liu-Edmonds? ap-
proach for maximum spanning tree (MST) parsing
(McDonald et al, 2005) and Nivre?s (2009) swap-
based reordering method for shift-reduce parsing.
The second approach is to create an initial projec-
tive parse and then apply transformations to intro-
11These accuracy figures are higher than what should be ex-
pected for unseen datasets; see Tratz and Hovy (2010) for more
detail.
duce non-projectivity into it. Examples of this in-
clude McDonald and Pereira?s (2006) rewriting of
projective trees produced by the Eisner (1996) al-
gorithm, and Nivre and Nilsson?s (2005) pseudo-
projective approach that creates projective trees with
specially marked arcs that are later transformed into
non-projective dependencies.
Descriptive dependency labels. While most re-
cent dependency parsing research has used either
vague labels, such as those of the CoNLL shared
tasks, or no labels at all, some descriptive depen-
dency label schemes exist. By far the most promi-
nent of these is the Stanford typed dependency
scheme (de Marneffe and Manning, 2008). An-
other descriptive scheme that exists, but which is
less widely used in the NLP community, is the one
used by Tapanainen and J?rvinen?s parser (1997).
Unfortunately, the Stanford dependency conversion
of the Penn Treebank has proven difficult to learn for
current dependency parsers (Cer et al, 2010), and
there is no publicly available dependency conversion
according to Tapanainen and J?rvinen?s scheme.
Faster parsing. While the fastest reasonable
parsing algorithms are the O(n) shift-reduce algo-
rithms, such as Nivre?s (2003) algorithm and an ex-
pected linear time dynamic programming approach
presented by Huang and Sagae (2010), a few other
fast alternatives exist. Goldberg and Elhadad?s
(2010) easy-first algorithm is one such example. An-
other example, is Roark and Hollingshead?s (2009)
work that uses chart constraints to achieve linear
time complexity for constituency parsing.
Effective features for parsing. A variety of work
has investigated the use of more informative fea-
tures for parsing. This includes work that inte-
grates second and even third order features (McDon-
ald et al, 2006; Carreras, 2007; Koo and Collins,
2010). Also, some work has incorporated unsuper-
vised word clusters as features, including that of Koo
et al (2008) and Suzuki et al (2009), who utilized
unsupervised word clusters created using the Brown
et al (1992) hierarchical clustering algorithm.
Semantically-enriched output. The 2008 and
2009 CoNLL shared tasks (Surdeanu et al, 2008;
Hajic? et al, 2009), which required participants to
build systems capable of both syntactic parsing and
Semantic Role Labeling (SRL) (Gildea and Juraf-
sky, 2002), are the most notable attempts to encour-
1265
age the development of parsers with additional se-
mantic annotation. These tasks relied upon Prop-
Bank (2005) and NomBank (2004) for the seman-
tic roles. A variety of other systems have focused
on FrameNet-based (1998) SRL instead, including
those that participated in the SemEval-2007 Task 19
(Baker et al, 2007) and work by Das et al (2010).
7 Conclusion
In this paper, we have described a new high-quality
dependency tree conversion of the Penn Treebank
(Marcus, et al, 1993) along with its labeled depen-
dency scheme and presented a parser that is fast, ac-
curate, supports non-projective trees and provides
rich output, including not only informative depen-
dency labels similar to Stanford?s but also additional
semantic annotation for prepositions, possessives,
and noun compound relations. We showed how the
easy-first algorithm of Goldberg and Elhadad (Gold-
berg and Elhadad, 2010) can be extended to support
non-projective trees by adding move actions similar
to Nivre?s (2009) swap-based reordering for shift-
reduce parsing and evaluated our parser on the stan-
dard test section of the Penn Treebank, comparing
with several other freely available parsers.
The Penn Treebank conversion process fixes a
number of buggy trees and part-of-speech tags and
produces dependency trees with a relatively small
percentage of generic dep dependencies. The ex-
perimental results show that dependency parsers can
generally produce Stanford-granularity labels with
high accuracy when using the new dependency con-
version of the Penn Treebank, something which, ac-
cording to the findings of Cer et al (2010), does
not appear to be the case when training and testing
dependency parsers on the Stanford conversion.
The parser achieves high labeled and unlabeled
accuracy in the evaluation, 92.1% and 93.7%, re-
spectively. The 93.7% result represents a 2.5% in-
crease over the accuracy of Goldberg and Elhadad?s
(2010) implementation. Also, the parser proves to
be quite fast, processing section 23 of the Penn Tree-
bank in just over 30 seconds (a rate of over 75 sen-
tences per second).
The parsing system is capable of not only produc-
ing fine-grained dependency relations, but can also
produce shallow semantic annotations for preposi-
tions, possessives, and noun compounds by using
several optional integrated modules. The preposi-
tion sense disambiguation (PSD) module achieves
85.7% accuracy on the SemEval-2007 PSD task, ex-
ceeding the previous best published result of 84.8%
by a statistically significant margin, the possessives
module is over 85% accurate, the noun compound
interpretation module achieves 79.6% accuracy on
Tratz and Hovy?s (2010) dataset. The PropBank
SRL module achieves 89.5 F1 on predicate disam-
biguation and 85.6 F1 on argument and adjuncts cor-
responding to dependency links, for an overall F1 of
86.8. Combined with the core parser, these modules
allow the system to produce a substantially more in-
formative textual analysis than a standard parser.
8 Future Work
There are a variety of ways to extend and improve
upon this work. We would like to change our han-
dling of coordinating conjunctions to treat the co-
ordinating conjunction as the head because this has
fewer ambiguities than the current approach and also
add the ability to produce traces for WH- words. It
would also be interesting to examine the impact on
final parsing accuracy of the various differences be-
tween our dependency conversion and Stanford?s.
To aid future NLP research work, the code,
including the treebank converter, part-of-speech
tagger, parser, and semantic annotation add-ons,
will be made publicly available for download via
http://www.isi.edu.
Acknowledgements
We would like to thank Richard Johansson for
providing us with the code for the pennconverter
consituent-to-dependency converter. We would also
like to thank Dirk Hovy and Anselmo Pe?as for
many valuable dicussions and suggestions.
References
Collin Baker, and Michael Ellsworth and Katrin Erk.
2007. SemEval?07 task 19: Frame Semantic Structure
Extraction. In Proc. of the 4th International Workshop
on Semantic Evaluations
Collin Baker, Charles J. Fillmore and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proc. of the
1266
17th international conference on Computational lin-
guistics
Adam L. Berger, Vincent J. Della Pietra, and Stephen A.
Della Pietra. 1996. A maximum entropy approach to
natural language processing. In Computational Lin-
guistics 22(1):39?71
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
Based n-gram Models of Natural Language. Compu-
tational Linguistics 18(4):467?479.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proc. of CoNLL 2006.
Xavier Carreras. 2007. Experiments with a Higher-
Order Projective Dependency Parser. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007.
Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-
sky, and Christopher D. Manning. 2010. Parsing to
Stanford Dependencies: Trade-offs between speed and
accuracy. In Proc. of LREC 2010.
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proc. of NAACL 2000.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
find-grained n-best parsing and discriminative rerank-
ing. In Proc. of ACL 2005.
Michael A. Covington. 2001. A Fundamental Algorithm
for Dependency Parsing. In Proc. of the 39th Annual
ACM Southeast Conference.
Dipanjan Das, Nathan Schneider, Desai Chen, and Noah
A. Smith. 2010. Probabilistic Frame-Semantic Pars-
ing. In Proc. of HLT-NAACL 2010.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies repre-
sentation. In COLING Workshop on Cross-framework
and Cross-domain Parser Evaluation.
Jason Eisner. 1996. Three New Probabilistic Models
for Dependency Parsing: An Exploration. In Proc. of
COLING 1996.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics.
28(3):245?288.
Jes?s Gim?nez and Llu?s M?rquez 2004. SVMTool: A
General POS Tagger Generator Based on Support Vec-
tor Machines. In Proc. of LREC 2004.
Yoav Goldberg and Michael Elhadad. 2010. An Ef-
ficient Algorithm for Easy-First Non-Directional De-
pendency Parsing. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the ACL.
Yoav Goldberg and Michael Elhadad. 2009. The
CoNLL-2009 Shared Task: Syntactic and Semantic
Dependencies in Multiple Languages. In Proc. of
the Thirteenth Conference on Computational Natural
Language Learning: Shared Task.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition??Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Proc. of
COLING 2010.
Liang Huang and Kenji Sagae. 2010. Dynamic Program-
ming for Linear-Time Shift-Reduce Parsing. In Proc.
of ACL 2010.
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for english. In
Proc. of NODALIDA.
Terry Koo, Xavier Carreras, and Michael Collins. 2008.
Simple Semi-supervised Dependency Parsing. In
Proc. of ACL 2008.
Terry Koo and Michael Collins. 2010. Efficient Third-
order Dependency Parsers. In Proc. of ACL 2010.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proc. of the 4th International Workshop on
Semantic Evaluations.
Christopher D. Manning. 2011. Part-of-Speech Tagging
from 97% to 100%: Is It Time for Some Linguistics?
In Proc. of the 12th International Conference on Intel-
ligent Text Processing and Computational Linguistics
(CICLing 2011).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: the Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajic?. 2005. Non-Projective Dependency Parsing
Using Spanning Tree Algorithms. In Proc. of HLT-
EMNLP 2005.
Ryan McDonald and Fernando Pereira. 2006. Online
Learning of Approximate Dependency Parsing Algo-
rithms. In Proc. of EACL 2006.
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young and Ralph
Grishman. 2004. The NomBank Project: An Interim
Report. In Proc. of the NAACL/HLT Workshop on
Frontiers in Corpus Annotation.
Joakim Nivre. 2009. Non-Projective Dependency Pars-
ing in Expected Linear Time. In Proc. of the 47th An-
nual Meeting of the ACL and the 4th IJCNLP of the
AFNLP.
Joakim Nivre. 2003. An Efficient Algorithm for Projec-
tive Dependency Parsing. In Proc. of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT).
Joakim Nivre, Marco Kuhlmann, and Johan Hall. 2009.
An Improved Oracle for Dependency Parsing with On-
line Reordering. In Proc. of the 11th International
Conference on Parsing Technologies (IWPT).
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of EMNLP-CoNLL 2007.
1267
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006. Malt-
Parser: A Data-Driven Parser-Generator for Depen-
dency Parsing. In Proc. of LREC 2006.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proc. of ACL-2005.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. In Computational Linguistics.
31(1):71?106.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized Parsing. In Proc. of HLT-NAACL
2007.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proc. of COLING-ACL
2006.
Brian Roark and Kristy Hollingshead. 2009. Linear
complexity context-free parsing pipelines via chart
constraints. In Proc. of HLT-NAACL.
Evan Sandhaus. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu?s M?rquez, and Joakim Nivre. 2008. The CoNLL-
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In Proc. of the Twelfth Confer-
ence on Computational Natural Language Learning.
Jun Suzuki, Hideki Isozaki, Xavier Carrerras, and
Michael Collins. 2009. An Empirical Study of Semi-
supervised Structured Conditional Models for Depen-
dency Parsing. In Proc. of EMNLP.
Pasi Tapanainen and Timo J?rvinen. 1997. A non-
projective dependency parser. In Proc. of the fifth con-
ference on applied natural language processing.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense using Linguistically Motivated Fea-
tures. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
Companion Volume: Student Research Workshop and
Doctoral Consortium.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proc. of ACL 2010.
David Vadas and James R. Curran. 2007. Adding Noun
Phrase Structure to the Penn Treebank. In Proc. of
ACL 2007.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical Dependency Analysis With Support Vector Ma-
chines. In Proc. of 8th International Workshop on
Parsing Technologies (IWPT).
1268
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411?1416,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
A Walk-based Semantically Enriched Tree Kernel
Over Distributed Word Representations
Shashank Srivastava1 Dirk Hovy2 Eduard Hovy1
(1) Carnegie Mellon University, Pittsburgh
(2) Center for Language Technology, University of Copenhagen, Denmark
{ssrivastava,hovy}@cmu.edu, mail@dirkhovy.com
Abstract
In this paper, we propose a walk-based graph
kernel that generalizes the notion of tree-
kernels to continuous spaces. Our proposed
approach subsumes a general framework for
word-similarity, and in particular, provides a
flexible way to incorporate distributed repre-
sentations. Using vector representations, such
an approach captures both distributional se-
mantic similarities among words as well as the
structural relations between them (encoded as
the structure of the parse tree). We show an ef-
ficient formulation to compute this kernel us-
ing simple matrix operations. We present our
results on three diverse NLP tasks, showing
state-of-the-art results.
1 Introduction
Capturing semantic similarity between sentences
is a fundamental issue in NLP, with applications in
a wide range of tasks. Previously, tree kernels based
on common substructures have been used to model
similarity between parse trees (Collins and Duffy,
2002; Moschitti, 2004; Moschitti, 2006b). These
kernels encode a high number of latent syntactic
features within a concise representation, and com-
pute the similarity between two parse trees based
on the matching of node-labels (words, POS tags,
etc.), as well as the overlap of tree structures. While
this is sufficient to capture syntactic similarity, it
does not capture semantic similarity very well, even
when using discrete semantic types as node labels.
This constrains the utility of many traditional
tree kernels in two ways: i) two sentences that
are syntactically identical, but have no semantic
similarity can receive a high matching score (see
Table 1, top) while ii) two sentences with only local
syntactic overlap, but high semantic similarity can
receive low scores (see Table 1, bottom).
tree pairs semantic syntactic score
?
?
high
?
?
low
love
we toys
crush
they puppies
kissed
she cat
gave
she
her
kiss
a
friend
feline
green little
her
Table 1: Traditional tree kernels do not capture se-
mantic similarity
In contrast, distributional vector representations
of words have been successful in capturing fine-
grained semantics, but lack syntactic knowledge.
Resources such as Wordnet, dictionaries and on-
tologies that encode different semantic perspectives
can also provide additional knowledge infusion.
In this paper, we describe a generic walk-based
graph kernel for dependency parse trees that sub-
sumes general notions of word-similarity, while
focusing on vector representations of words to
capture lexical semantics. Through a convolutional
framework, our approach takes into account the
distributional semantic similarities between words
in a sentence as well as the structure of the parse
tree. Our main contributions are:
1. We present a new graph kernel for NLP that ex-
tends to distributed word representations, and
diverse word similarity measures.
2. Our proposed approach provides a flexible
framework for incorporating both syntax and
semantics of sentence level constructions.
3. Our generic kernel shows state-of-the-art per-
formance on three eclectic NLP tasks.
1411
2 Related Work
Tree kernels in NLP Tree kernels have been ex-
tensively used to capture syntactic information about
parse trees in tasks such as parsing (Collins and
Duffy, 2002), NER (Wang et al, 2010; Cumby and
Roth, 2003), SRL (Moschitti et al, 2008) and rela-
tion extraction (Qian et al, 2008). These kernels are
based on the paradigm that parse trees are similar if
they contain many common substructures, consist-
ing of nodes with identical labels (Vishwanathan and
Smola, 2003; Collins and Duffy, 2002). Moschitti
(2006a) proposed a partial tree kernel that adds flex-
ibility in matching tree substructures. Croce et al
(2011) introduce a lexical semantic tree kernel that
incorporates continuous similarity values between
node labels, albeit with a different focus than ours
and would not match words with different POS. This
would miss the similarity of ?feline friend? and ?cat?
in our examples, as it requires matching the adjective
?feline? with ?cat?, and verb ?kissed? with ?kiss?.
Walk based kernels Kernels for structured data
derive from the seminal Convolution Kernel for-
malism by Haussler (1999) for designing kernels
for structured objects through local decompositions.
Our proposed kernel for parse trees is most closely
associated with the random walk-based kernels de-
fined by Gartner et al (2003) and Kashima et al
(2003). The walk-based graph kernels proposed by
Gartner et al (2003) count the common walks be-
tween two input graphs, using the adjacency matrix
of the product graph. This work extends to graphs
with a finite set of edge and node labels by appro-
priately modifying the adjacency matrix. Our kernel
differs from these kernels in two significant ways: (i)
Our method extends beyond label matching to con-
tinuous similarity metrics (this conforms with the
very general formalism for graph kernels in Vish-
wanathan et al (2010)). (ii) Rather than using the
adjacency matrix to model edge-strengths, we mod-
ify the product graph and the corresponding adja-
cency matrix to model node similarities.
3 Vector Tree Kernels
In this section, we describe our kernel and an al-
gorithm to compute it as a simple matrix multiplica-
tion formulation.
3.1 Kernel description
The similarity kernel K between two dependency
trees can be defined as:
K(T1, T2) =
?
h1?T1,h2?T2
len(h1)=len(h2)
k(h1, h2)
where the summation is over pairs of equal length
walks h1 and h2 on the trees T1 and T2 respec-
tively. The similarity between two n length walks,
k(h1, h2), is in turn given by the pairwise similari-
ties of the corresponding nodes vih in the respective
walks, measured via the node similarity kernel ?:
k(h1, h2) =
n?
i:1
?(vh1i , v
h2
i )
In the context of parse trees, nodes vh1i and v
h2
i cor-
respond to words in the two parse trees, and thus can
often be conveniently represented as vectors over
distributional/dependency contexts. The vector rep-
resentation allows us several choices for the node
kernel function ?. In particular, we consider:
1. Gaussian : ?(v1, v2) = exp
(
? ?v1?v2?
2
2?2
)
2. Positive-Linear: ?(v1, v2) = max(vT1 v2, 0)
3. Sigmoid: ?(v1, v2) =
(
1 + tanh(?vT1 v2)
)
/2
We note that the kernels above take strictly non-
negative values in [0, 1] (assuming word vector rep-
resentations are normalized). Non-negativity is nec-
essary, since we define the walk kernel to be the
product of the individual kernels. As walk kernels
are products of individual node-kernels, bounded-
ness by 1 ensures that the kernel contribution does
not grow arbitrarily for longer length walks.
The kernel function K puts a high similarity
weight between parse trees if they contain com-
mon walks with semantically similar words in corre-
sponding positions. Apart from the Gaussian kernel,
the other two kernels are based on the dot-product
of the word vector representations. We observe that
the positive-linear kernel defined above is not a Mer-
cer kernel, since the max operation makes it non-
positive semidefinite (PSD). However, this formu-
lation has desirable properties, most significant be-
ing that all walks with one or more node-pair mis-
matches are strictly penalized and add no score to
1412
the tree-kernel. This is a more selective condition
than the other two kernels, where mediocre walk
combinations could also add small contributions to
the score. The sigmoid kernel is also non-PSD, but
is known to work well empirically (Boughorbel et
al., 2005). We also observe while the summation in
the kernel is over equal length walks, the formalism
can allow comparisons over different length paths by
including self-loops at nodes in the tree.
With a notion of similarity between words that
defines the local node kernels, we need computa-
tional machinery to enumerate all pairs of walks
between two trees, and compute the summation
over products in the kernel K(T1, T2) efficiently.
We now show a convenient way to compute this as
a matrix geometric series.
3.2 Matrix Formulation for Kernel
Computation
Walk-based kernels compute the number of com-
mon walks using the adjacency matrix of the prod-
uct graph (Gartner et al, 2003). In our case, this
computation is complicated by the fact that instead
of counting common walks, we need to compute a
product of node-similarities for each walk. Since
we compute similarity scores over nodes, rather than
edges, the product for a walk of length n involves
n+ 1 factors.
However, we can still compute the tree kernel K
as a simple sum of matrix products. Given two trees
T (V,E) and T ?(V ?, E?), we define a modified prod-
uct graph G(Vp, Ep) with an additional ghost node
u added to the vertex set. The vertex and edge sets
for the modified product graph are given as:
Vp := {(vi1, vj1
?) : vi1 ? V, vj1
? ? V ?} ? u
Ep := {((vi1, vj1
?), (vi2, vj2
?)) : (vi1, vi2) ? E,
(vj1
?, vj2
?)) ? E?}
?
{(u, (vi1, vj1
?)) : vi1 ? V, vj1
? ? V ?}
The modified product graph thus has additional
edges connecting u to all other nodes. In our for-
mulation, u now serves as a starting location for all
random walks on G, and a k + 1 length walk of G
corresponds to a pair of k length walks on T and T ?.
We now define the weighted adjacency matrixW for
G, which incorporates the local node kernels.
W(vi1,vj1?),(vi2,vj2?) =
{
0 : ((vi1,vj1?),(vi2,vj2?)) /? Ep
?(vi2, vj2?) : otherwise
Wu,(vi1,vj1?) = ?(vi1, vj1
?)
W(v,u) = 0 ? v ? Vp
There is a straightforward bijective mapping from
walks on G starting from u to pairs of walks on T
and T ?. Restricting ourselves to the case when the
first node of a k + 1 length walk is u, the next k
steps allow us to efficiently compute the products of
the node similarities along the k nodes in the corre-
sponding k length walks in T and T ?. Given this ad-
jacency matrix for G, the sum of values of k length
walk kernels is given by the uth row of the (k+1)th
exponent of the weighted adjacency matrix (denoted
asW k+1). This corresponds to k+1 length walks on
G starting from u and ending at any node. Specif-
ically, Wu,(vi,v?j) corresponds to the sum of similar-
ities of all common walks of length n in T and T ?
that end in vi in T and v?j in T
?. The kernel K for
walks upto length N can now be calculated as :
K(T, T ?) =
|Vp|?
i
Su,i
where
S = W +W 2 + ...WN+1
We note that in out formulation, longer walks are
naturally discounted, since they involve products of
more factors (generally all less than unity).
The above kernel provides a similarity measure
between any two pairs of dependency parse-trees.
Depending on whether we consider directional re-
lations in the parse tree, the edge set Ep changes,
while the procedure for the kernel computation re-
mains the same. Finally, to avoid larger trees yield-
ing larger values for the kernel, we normalize the
kernel by the number of edges in the product graph.
4 Experiments
We evaluate the Vector Tree Kernel (VTK) on
three NLP tasks. We create dependency trees using
the FANSE parser (Tratz and Hovy, 2011), and
use distribution-based SENNA word embeddings
by Collobert et al (2011) as word representations.
These embeddings provide low-dimensional vector
1413
representations of words, while encoding distribu-
tional semantic characteristics. We use LibSVM for
classification. For sake of brevity, we only report
results for the best performing kernel.
We first consider the Cornell Sentence Polarity
dataset by Pang and Lee (2005). The task is to
identify the polarity of a given sentence. The
data consists of 5331 sentences from positive and
negative movie reviews. Many phrases denoting
sentiments are lexically ambiguous (cf. ?terribly
entertaining? vs ?terribly written?), so simple lexi-
cal approaches are not expected to work well here,
while syntactic context could help disambiguation.
Next, we try our approach on the MSR paraphrase
corpus. The data contains a training set of 4077
pairs of sentences, annotated as paraphrases and
non-paraphrases, and a test-set of 1726 sentence
pairs. Each instance consists of a pair of sentences,
so the VTK cannot be directly used by a kernel
machine for classification. Instead, we generate
16 kernel values based for each pair on different
parameter settings of the kernel, and feed these as
features to a linear SVM.
We finally look at the annotated Metaphor corpus
by (Hovy et al, 2013). The dataset consists of sen-
tences with specified target phrases. The task here is
to classify the target use as literal or metaphorical.
We focus on target phrases by upweighting walks
that pass through target nodes. This is done by
simply multiplying the corresponding entries in the
adjacency matrix by a constant factor.
5 Results
5.1 Sentence Polarity Dataset
Prec Rec F1 Acc
Albornoz et al0.63 ? ? 0.63
WNA+synsets 0.61 ? ? 0.61
WNA 0.53 ? ? 0.51
DSM 0.54 0.55 0.55 0.54
SSTK 0.49 0.48 0.48 0.49
VTK 0.65 0.58 0.62 0.67
Table 2: Results on Sentence Polarity dataset
On the polarity data set, Vector Tree Kernel
(VTK) significantly outperforms the state-of-the-art
method by Carrillo de Albornoz et al (2010), who
use a hybrid model incorporating databases of af-
fective lexicons, and also explicitly model the ef-
fect of negation and quantifiers (see Table 2). Lex-
ical approaches using pairwise semantic similarity
of SENNA embeddings (DSM), as well as Word-
net Affective Database-based (WNA) labels perform
poorly (Carrillo de Albornoz et al, 2010), showing
the importance of syntax for this particular problem.
On the other hand, a syntactic tree kernel (SSTK)
that ignores distributional semantic similarity be-
tween words, fails as expected.
5.2 MSR Paraphrase Dataset
Prec Rec F1 Acc
BASE 0.72 0.86 0.79 0.69
Zhang et al0.74 0.88 0.81 0.72
Qiu et al0.73 0.93 0.82 0.72
Malakasiotis 0.74 0.94 0.83 0.74
Finch 0.77 0.90 0.83 0.75
VTK 0.72 0.95 0.82 0.72
Table 3: Results on MSR Paraphrase corpus
On the MSR paraphrase corpus, VTK performs
competitively against state-of-the-art-methods. We
expected paraphrasing to be challenging to our
method, since it can involve little syntactic overlap.
However, data analysis reveals that the corpus gener-
ally contains sentence pairs with high syntactic sim-
ilarity. Results for this task are encouraging since
ours is a general approach, while other systems use
multiple task-specific features like semantic role la-
bels, active-passive voice conversion, and synonymy
resolution. In the future, incorporating such features
to VTK should further improve results for this task .
5.3 Metaphor Identification
Acc P R F1
CRF 0.69 0.74 0.50 0.59
SVM+DSM 0.70 0.63 0.80 0.71
SSTK 0.75 0.70 0.80 0.75
VTK 0.76 0.67 0.87 0.76
Table 4: Results on Metaphor dataset
On the Metaphor corpus, VTK improves the pre-
vious score by Hovy et al (2013), whose approach
uses an conjunction of lexical and syntactic tree ker-
nels (Moschitti, 2006b), and distributional vectors.
VTK identified several templates of metaphor usage
such as ?warm heart? and ?cold shoulder?. We look
towards approaches for automatedly mining such
metaphor patterns from a corpus.
6 Conclusion
We present a general formalism for walk-based
kernels to evaluate similarity of dependency trees.
1414
Our method generalizes tree kernels to take dis-
tributed representations of nodes as input, and cap-
ture both lexical semantics and syntactic structures
of parse trees. Our approach has tunable parame-
ters to look for larger or smaller syntactic constructs.
Our experiments shows state-of-the-art performance
on three diverse NLP tasks. The approach can gen-
eralize to any task involving structural and local sim-
ilarity, and arbitrary node similarity measures.
References
Sabri Boughorbel, Jean-Philippe Tarel, and Nozha Bouje-
maa. 2005. Conditionally positive definite kernels for
svm based image recognition. In ICME, pages 113?
116.
Jorge Carrillo de Albornoz, Laura Plaza, and Pablo
Gerva?s. 2010. A hybrid approach to emotional sen-
tence polarity and intensity classification. In Proceed-
ings of the Fourteenth Conference on Computational
Natural Language Learning, pages 153?161. Associa-
tion for Computational Linguistics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association for
computational linguistics, pages 263?270. Association
for Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011. Structured lexical similarity via convolution
kernels on dependency trees. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1034?1046. Association for
Computational Linguistics.
Chad Cumby and Dan Roth. 2003. On kernel methods
for relational learning. In In Proc. of the International
Conference on Machine Learning, pages 107?114.
Andrew Finch. 2005. Using machine translation evalu-
ation techniques to determine sentence-level semantic
equivalence. In In IWP2005.
Thomas Gartner, Peter Flach, and Stefan Wrobel. 2003.
On graph kernels: Hardness results and efficient al-
ternatives. In Proceedings of the Annual Conference
on Computational Learning Theory, pages 129?143.
Springer.
David Haussler. 1999. Convolution kernels on discrete
structures. Technical Report Technical Report UCS-
CRL-99-10, UC Santa Cruz.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. In Proceed-
ings of NAACL HLT, Meta4NLP Workshop.
Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.
2003. Marginalized kernels between labeled graphs.
In Proceedings of the Twentieth International Con-
ference on Machine Learning, pages 321?328. AAAI
Press.
Prodromos Malakasiotis. 2009. Paraphrase recognition
using machine learning to combine similarity mea-
sures. In Proceedings of the ACL-IJCNLP 2009 Stu-
dent Research Workshop, ACLstudent ?09, pages 27?
35, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2004. A study on convolution
kernels for shallow semantic parsing. In Proceedings
of the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 335?es. Association for
Computational Linguistics.
Alessandro Moschitti. 2006a. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
Alessandro Moschitti. 2006b. Making Tree Kernels
Practical for Natural Language Learning. In In Pro-
ceedings of the 11th Conference of the European
Chapter of the Association for Computational Linguis-
tics.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL.
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697?704. Association for Computational Lin-
guistics.
Long Qiu, Min-Yen Kan, and Tat-Seng Chua. 2006.
Paraphrase recognition via dissimilarity significance
classification. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 18?26, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Quirk, Chris Brockett, and William Dolan. 2004.
Monolingual machine translation for paraphrase gen-
eration. In In Proceedings of the 2004 Conference on
Empirical Methods in Natural Language Processing,
pages 142?149.
1415
Stephen Tratz and Eduard Hovy. 2011. A fast, accu-
rate, non-projective, semantically-enriched parser. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ?11, pages
1257?1268, Stroudsburg, PA, USA. Association for
Computational Linguistics.
S. V. N. Vishwanathan and Alexander J. Smola. 2003.
Fast kernels for string and tree matching. In Advances
In Neural Information Processing Systems 15, pages
569?576. MIT Press.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Risi Kon-
dor, and Karsten M. Borgwardt. 2010. Graph kernels.
J. Mach. Learn. Res., 99:1201?1242, August.
Xinglong Wang, Jun?ichi Tsujii, and Sophia Ananiadou.
2010. Disambiguating the species of biomedical
named entities using natural language parsers. Bioin-
formatics, 26(5):661?667.
Yitao Zhang and Jon Patrick. 2005. Paraphrase identi-
fication by text canonicalization. In In Proceedings
of the Australasian Language Technology Workshop
2005.
1416
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 467?476,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Sentiment Analysis on the People?s Daily
Jiwei Li
1
and Eduard Hovy
2
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
Language Technology Institute, Carnegie Mellon University, PA 15213, USA
jiweil@stanford.edu ehovy@andrew.cmu.edu
Abstract
We propose a semi-supervised bootstrap-
ping algorithm for analyzing China?s for-
eign relations from the People?s Daily.
Our approach addresses sentiment tar-
get clustering, subjective lexicons extrac-
tion and sentiment prediction in a unified
framework. Different from existing algo-
rithms in the literature, time information
is considered in our algorithm through a
hierarchical bayesian model to guide the
bootstrapping approach. We are hopeful
that our approach can facilitate quantita-
tive political analysis conducted by social
scientists and politicians.
1 Introduction
?We have no permanent allies, no permanent friends, but only
permanent interests.?
-Lord Palmerston
Newspapers, especially those owned by official
governments, e.g., Pravda from Soviet Union,
or People?s Daily from P.R. China, usually pro-
vide direct information about policies and view-
points of government. As national policies change
over time, the tone that newspapers adopt, es-
pecially sentiment, changes along with the poli-
cies. For example, there is a stark contrast be-
tween the American newspapers? attitudes towards
Afghanistan before and after 911. Similarly, con-
sider the following examples extracted from the
People?s Daily
1
:
? People? Daily, Aug 29th, 1963
All those who are being oppressed and exploited, Unite
!! Beat US Imperialism and its lackeys.
? People?s Daily, Oct, 20th, 2002
A healthy, steady and developmental relationship be-
tween China and US, conforms to the fundamental in-
terests of people in both countries, and the trend of his-
torical development.
1
Due to the space constraints, we only show the translated
version in most of this paper.
Automatic opinion extraction from newspapers
such as people?s daily can facilitate sociologists
?or political scientists? research or help political
pundits in their decision making process. While
our approach applies to any newspaper in princi-
ple, we focus here on the People?s Daily
2
(Renmin
Ribao), a daily official newspaper in the People?s
Republic of China.
While massive number of works have been in-
troduced in sentiment analysis or opinion target
extraction literature (for details, see Section 2), a
few challenges limit previous efforts in this spe-
cific task: First, the heavy use of linguistic phe-
nomenon in the People?s Daily including rhetoric,
metaphor, proverb, or even nicknames, makes ex-
isting approaches less effective for sentiment in-
ference as identifying these expressions is a hard
NLP problem in nature.
Second, as we are more interested in the degree
of sentiment rather than binary classification (i.e.,
positive versus negative) towards an entity (e.g.
country or individual) in the news article, straight-
forward algorithms to apply would be document-
level sentiment analysis approaches such as vector
machine/regression (Pang et al., 2002) or super-
vised LDA (Blei and McAuliffe, 2010). A single
news article, usually contains different attitudes
towards multiple countries or individuals simul-
taneously (say praising ?friends? and criticizing
?enemies?), as shown in the following example
from the People?s Daily of Mar. 17th, 1966:
US imperialism set up a puppet regime in Vietnam and
sent expeditionary force. . . People of Vietnam prevailed over
the modern-equipped US troops with a vengeance. . . The re-
sult of Johnson Government?s intensifying invasion is that. . . .
There will be the day, when people from all over the world ex-
ecute the heinous US imperialism by hanging on a gibbet. . . .
The heroic people of Vietnam, obtained great victory in the
struggle against the USA imperialism. . .
The switching of praising of Vietnam and
criticizing of the USA would make aforemen-
2
paper.people.com.cn/rmrb/
467
tioned document-level machine learning algo-
rithms based on bags of words significantly less
effective if not separating attitudes towards Viet-
nam from toward the USA in the first place. Mean-
while, the separating task is by no means trivial in
news articles. While US imperialism, US troops,
Johnson Government, invaders, Ngo Dinh Diem
3
all point to the USA or its allies, People of Viet-
nam, the Workers? party
4
, Ho Chi Minh
5
, Viet-
nam People?s Army point to North Vietnam side.
Clustering entities according to sentiment, espe-
cially in Chinese, is fundamentally a difficult task.
And our goal, trying to identify entities towards
whom an article holds the same attitudes, is dif-
ferent from standard coreference resolution, since
for us the co-referent group may include several
distinct entities.
To address the aforementioned problems, in this
paper, we propose a sentiment analysis approach
based on the following assumptions:
1. In a single news article, sentiment towards an
entity is consistent.
2. Over a certain period of time, sentiments to-
wards an entity are inter-related.
The assumptions will facilitate opinion analy-
sis: (1) if we can identify the attitude towards an
entity (e.g., Vietnam) in a news article as posi-
tive, then negative attitudes expressed in the arti-
cle are about other entities. (2) The assumption
enables sentiment inference for unseen words in a
bootstrapping way without having to employ so-
phisticated NLP algorithms. For example, from
1950s to 1960s, USA is usually referred to as ?a
tiger made of paper? in translated version. It is
a metaphor indicating things that appear powerful
(tiger) but weak in nature (made of paper). If it is
first identified that during the designated time pe-
riod, China held a pretty negative attitude towards
the USA based on clues such as common nega-
tive expressions (e.g., ?evil? or ?reactionary?), we
can easily induce that ?a tiger made of paper?, is a
negative word.
Based on aforementioned two assumptions,
we formulate our approach as a semi-supervised
model, which simultaneously bootstrap sentiment
target lists, extracts subjective vocabularies and
3
Leader of South Vietnam
4
Ruling political party of Vietnam.
5
One of Founders of Democratic Republic of Vietnam
(North Vietnam) and Vietnam Workers? party.
performs sentiment analysis. Time information is
considered through a hierarchical bayesian model
to guide time-, document-, sentence- and term-
level sentiment inference. A small seed set of sub-
jective words constitutes our only source of super-
vision.
The main contributions of this paper can be
summarized as follows:
1. We propose a semi-supervised bootstrapping algorithm
tailored for sentiment analysis in the People?s daily
where time information is incorporated. We are hope-
ful that sentiment cues can shed insights on other NLP
tasks such as coreference or metaphor recognition.
2. In Analytical Political Science, the quantitative evalu-
ation of diplomatic relations is usually a manual task
(Robinson and Shambaugh, 1995). We are hopeful that
our algorithm can enable automated political analysis
and facilitate political scientists? and historians? work.
2 Related Works
Significant research efforts have been invested into
sentiment analysis and opinion extraction. In one
direction, researchers look into predicting over-
all sentiment polarity at document-level (Pang and
Lee, 2008), aspect-level (Wang et al., 2010; Jo
and Oh, 2011), sentence-level (Yang and Cardie,
2014) or tweet-level (Agarwal et al., 2011; Go
et al., 2009), which can be treated as a clas-
sification/regression problem by employing stan-
dard machine-learning techniques, such as Naive
Bayesian, SVM (Pang et al., 2002) or supervised-
LDA (Blei and McAuliffe, 2010) with different
types of features (i.e., unigram, bigram, POS).
Other efforts are focused on targeted sentiment
extraction (Choi et al., 2006; Kim and Hovy, 2006;
Jin et al., 2009; Kim and Hovy, 2006). Usu-
ally, sequence labeling models such as CRF (Laf-
ferty et al., 2001) or HMM (LIU et al., 2004) are
employed for identifying opinion holders (Choi
et al., 2005), topics of opinions (Stoyanov and
Cardie, 2008) or opinion expressions (e.g. (Breck
et al., 2007; Johansson and Moschitti, 2010; Yang
and Cardie, 2012)). Kim and Hovy (2004; 2006)
identified opinion holders and targets by exploring
their semantics rules related to the opinion words.
Choi et al. (2006) jointly extracted opinion expres-
sions, holders and their is-from relations using an
ILP approach. Yang and Cardie (2013) introduced
a sequence tagging model based on CRF to jointly
identify opinion holders, opinion targets, and ex-
pressions.
Methods that relate to our approach include
semi-supervised approaches such as pipeline or
468
propagation algorithms (Qiu et al., 2011; Qiu et
al., 2009; Zhang et al., 2010; Duyu et al., 2013).
Concretely, Qiu et al. (2011) proposed a rule-
based semi-supervised framework called double
propagation for jointly extracting opinion words
and targets. Compared to existing bootstrapping
approaches, our framework is more general one
with less restrictions
6
. In addition, our approach
harness global information (e.g. document-level,
time-level) to guide the bootstrapping algorithm.
Another related work is the approach introduced
by O?Connor et al. (O?Connor et al., 2013) that
extracts international relations from political con-
texts.
3 the People?s Daily
The People?s Daily
7
(Renmin Ribao), established
on 15 June 1946, is a daily official newspaper in
the People?s Republic of China, with a approxi-
mate circulation of 2.5 million worldwide. It is
widely recognized as the mouthpiece of the Cen-
tral Committee of the Communist Party of China
(CPC) (Wu, 1994). Editorials and commentaries
are usually regarded both by foreign observers and
Chinese readers as authoritative statements of gov-
ernment policy
8
. According to incomplete statis-
tics, there have benn at least 13 major redesigns
(face-liftings) for the People?s Daily in history, the
most recent in 2013.
4 Model
In this section, we present our model in detail.
4.1 Target and Expression extraction
We first extract expressions (attitude or sentiment
related terms or phrases) and target (entities to-
ward whom the opinion holder (e.g., the People?s
Daily) holds an attitude). See the following exam-
ples:
1. [Albania Workers? party][T] is the [glorious][E]
[party][T] of [Marxism and Leninism][E].
2. The [heroic][E] [people of Vietnam][T] obtained
[great][E] [victory][E] against [the U.S. imperial-
ism][T,E].
3. We strongly [warn][E] Soviet Revisionism][E,T].
6
Qiu et al.?s rule base approach makes strong assumptions
that consider opinion word to adjectives and targets to be
nouns/noun, thus only capable of capturing sentences with
simple patterns.
7
paper.people.com.cn/rmrb/
8
http://en.wikipedia.org/wiki/People?
s_Daily
While the majority of subjective sentences omit
the opinion holder, as in Examples 1 and 2, there
are still a few circumstances where opinion hold-
ers (e.g., ?we?, ?Chinese people?, ?Chinese gov-
ernment?) are retained (Example 3). Some words
(i.e. U.S. imperialism) can be both target and ex-
pression, and there can be multiple targets (Exam-
ple 2) within one sentence.
We use a semi-Markov Conditional Random
Fields (semi-CRFs) (Sarawagi and Cohen, 2004;
Okanohara et al., 2006) algorithm for target and
expression extraction. Semi-CRF are CRFs that
relax the Markovian assumptions and allow for se-
quence labeling at the segment level. It has been
demonstrated more powerful that CRFs in multi-
ple sequence labeling applications including NER
(Okanohara et al., 2006), Chinese word segmenta-
tion (Andrew, 2006) and opinion expression iden-
tification (Yang and Cardie, 2012). Our approach
is an extension of Yang and Cardie (2012)?s sys-
tem
9
. Features we adopted included:
? word, part of speech tag, word length.
? left and right context words within a window
of 2 and the correspondent POS tags.
? NER feature.
? subjectivity lexicon features from dictio-
nary
10
. The lexicon consists of a set of Chi-
nese words that can act as strong or weak
cues to subjectivity.
? segment-level syntactic features defined in
(Yang and Cardie, 2012).
Most existing NER systems can barely recog-
nize entities such as [ Vietnamese People?s Army ]
as a unified name entity in that Chinese parser usu-
ally divides them into a series of separate words,
namely [ Vietnamese/People?s Army ]. To han-
dle this problem, we first employ the Stanford
NER engine
11
and then iteratively ?chunk? con-
secutive words, at least one of which is labeled as
a name entity by the NER engine, before check-
ing whether the chunked entity matches a bag of
words contained in Chinese encyclopedia, e.g.,
Baidu Encyclopedia
12
and Chinese Wikipedia
13
.
9
Yang and Cardie?s system focuses on expression extrac-
tion (not target) and identifies direct subjective expression
(DSE) and expressive subjective expression (ESE).
10
http://ir.dlut.edu.cn/NewsShow.aspx?
ID=215
11
http://nlp.stanford.edu/downloads/
CRF-NER.shtml
12
http://baike.baidu.com/
13
http://zh.wikipedia.org/wiki/
Wikipedia
469
4.2 Notation
Here we describe the key variables in our model.
Let C
i
denote the name entity of country i, G
i
denote its corresponding collection of news ar-
ticles. G
i
is divided into 60*4=240 time spans
(one for each quarter of the year, 60 years in to-
tal), G
i
= {G
i,t
}. G
i,t
is composed of a series
of documents {d}, and d is composed of a series
of sentences {S}, which is represented as a tuple
S = {E
S
, T
S
}, where E
S
is the expression and
T
S
is the target of current sentence.
Sentiment Score m: As we are interested in the
degree of positiveness or negativeness, we divided
international relations into 7 categories: Antag-
onism (score 1), Tension (score 2), Disharmony
(score 3), Neutrality (score 4), Goodness (score
5), Friendliness (score 6), Brotherhood (Comrade-
ship) (score 7) based on researches in political sci-
ence literature
14
. Each of G
i,t
, document d, sen-
tence S and expression term w is associated with
a sentiment score m
i,t
, m
d
, m
S
and m
w
, respec-
tively. M denotes the list of subjective terms,
M = {w,m
w
}
Document Target List T
d
i
: We use T
d
i
to denote
the collection of entity targets in document d ? G
i
which the People?s daily holds similar attribute to-
wards. For example, suppose document d belongs
to Vietnam article collection (C
i
= V ietnam), T
d
i
can be {Vietnam, Workers? party, People?s Army,
Ho Chi Minh}. While U.S., U.S. troops and Lyn-
don Johnson are also entity targets found in d, they
are not supposed to be included in T
d
i
since the au-
thor holds opposite attributes.
Sentence List d
i
: We further use d
i
denotes the
subset of sentences in d talking about entities from
target list T
d
i
. Similarly, in a Vietnam related arti-
cle, sentences talking about the U.S. are not sup-
posed to be included in d
i
.
4.3 Hierarchical Bayesian Markov Model
In our approach, time information is incorporated
through a hierarchical Bayesian Markov frame-
work where m
i,t
is modeled as a first-order Pois-
son Process given the coherence assumption in
time-dependent political news streams.
m
i,t
? Poisson(m
i,t
,m
i,t?1
) (1)
14
http://www.imir.tsinghua.edu.cn/
publish/iis/7522/20120522140122561915769
Figure 1: Hierarchical Bayesian Model for Infer-
ence
For each document d ? G
i,t
, m
d
is sampled from
a Poisson distribution with mean value of m
i,t
.
m
d
? Poisson(m
d
,m
i,t
) (2)
For sentence S ? d
i
,m
S
is sampled fromm
d
from
a Poisson distribution based on m
d
.
m
S
? Poisson(m
S
,m
d
) (3)
4.4 Intialization
Given a labeled subjective list M , for article d ?
G
i
, we initialize T
d
i
with the name of entity C
i
, d
i
with sentences satisfying T
S
= C
i
and E
S
? M .
m
S
for S ? d
i
, is initialized as the average score
of its containing expression E
s
based on M . Then
the MCMC algorithm is applied by iteratively up-
dating m
d
and m
i,t
according to the posterior dis-
tribution. Let P (m|?) denotes the probability of
parameter m given all other parameters and the
posterior distributions are given by:
P (m
d
= ?|?) ? Poisson(?,m
i,t
)
?
S?d
i
Poisson(?,m
S
)
P (m
i,t
= ?|?) ? Poisson(?,m
i,t?1
)
? Poisson(m
i,t+1
, ?) ? ?
?
d?G
i,t
Poisson(m
d
, ?)
(4)
4.5 Semi-supervised Bootstrapping
Our semi-supervised learning algorithm updates
M , T
d
i
, d
i
, S
d
and S
d
i
iteratively. A brief inter-
pretation is shown in Figure 2 and the details are
shown in Figure 4. Concretely, for each sentence
S ? d ? d
i
, step 1 means, if its expression E
S
exists in subjective list M , we added its target T
S
to T
d
i
and S to d
i
. step 2 means if the target T
S
ex-
ists in T
d
i
, its expression,E
s
, is added to subjective
list M with score m
d
. As M and T
d
i
change in the
iteration, in step 3, we again go over all unconsid-
ered sentences with new M and T
d
i
. m
d
and m
i,t
are then updated based on new m
S
using MCMC
in Equ. 4. Note that sentences with pronoun target
are not involved in the bootstrapping procedure.
470
Figure 2: A brief demonstration of the adopted semi-supervised algorithm. (a)?(b): Sentence (2) is
added to d
i
due to the presence of already known subjective term ?great? . Target B is added to target list
T
d
i
. (b)?(c): term ?heroic? is added to subjective word list M with score 7 since it modifies target B.
Input: Entity C
i
, G
i
, subjective term list M
? for each entity i, each document d
T
d
i
= {C
i
}, d
i
= {S|S ? d,C
i
= T
S
, E
s
?M}
for each sentence S ? d
i
:
. m
s
=
1
|E
S
?M|
?
m
E
s
? Iteratively update m
i,t
, m
d
using MCMC based on
posterior probability shown in Equ.4.
Output: {d
i
}, {T
d
i
}, {m
i,t
} and {m
d
}
Figure 3: Initialization Algorithm.
4.6 Error Prevention in Bootstrapping
Error propagation is highly influential and damag-
ing in bootstrapping algorithms, especially when
extending very limited data to huge corpora. To
avoid the collapse of the algorithm, we select can-
didates for opinion analysis in a extremely strict
manner, at the sacrifice of many subjective sen-
tences
15
. Concretely, we only consider sentences
with exactly one target and at least one expression.
Sentences with multiple targets (e.g., Example 2
in Section 4.1) or no expressions, or no targets are
discarded.
In addition to the strict sentence selection ap-
proach, we adopt the following methods for self-
correction in the boot-strapping procedure:
1. For T
1
, T
2
? T
d
i
, (E
1
, T
1
) ? S
1
, (E
2
, T
2
) ?
S
2
, E
1
, E
2
?M , if |m
E
1
?m
E
2
| > 1: Expel
E
1
andE
2
fromM , expel T
1
and T
2
from T
d
i
,
with the exception of original labeled data.
Explanation: If sentiment scores for two ex-
pressions, whose correspondent targets both
15
Negative effect of strict sentence selection can be partly
compensated by the consideration of time-level information
Input: Entity {C
i
}, Articles Collections {G
i
}, subjective
term list M, sentiment score {m
d
}, {m
i,t
}, target list for
each document {T
d
i
}
Algorithm:
while not convergence:
? for each entity C
i
, document d:
for each sentence S ? d? d
i
1. if E
S
?M , T
s
6? T
d
i
T
d
i
= T
d
i
?
T
s
, d
i
= d
i
?
S, m
S
= m
d
2. if T
s
? T
t
i
, E
s
6?M
M = M
?
(E
s
, S
d
), d
i
= d
i
?
S, m
s
= m
d
3. if E
S
?M,T
S
? T
d
i
d
i
= d
i
?
S, m
S
= m
E
s
?Iteratively update m
i,t
, m
d
using MCMC based on
posterior probability shown in Equ.4 .
end while:
Output: subjective term list M, score {m
i,t
}
Figure 4: Semi-supervised learning algorithm.
belong to the target list T
d
i
, diverge enough,
we discard both expressions and targets based
according to Assumption 1: sentiments to-
wards one entity (or its allies) in an article
should be consistent.
2. ?S ? d, T
S
? T
d
i
, |m
E
S
? m
d
| > 1, T
S
is
expelled from T
d
i
.
Explanation: If target T
S
for sentence S be-
longs to T
d
i
, but its corresponding expression
E
s
is not consistent with article-level senti-
ment m
d
, T
S
is expelled from T
d
i
.
5 Experiment
5.1 Data and Preprocessing
Our data set is composed of the People?s daily
from 1950 to 2010, across a 60-year time span.
471
antagonism (m=1) ??(extremely cruel),??(enemy)
tension (m=2) ??(indignation),??(offend)
disharmony (m=3) ??(disappointed),??(regret)
neutrality (m=4) ??,??(concern)
goodness (m=5) ???(developmental),??(respect)
friendship (m=6) ??(friendship),??(friend)
brotherhood (m=7) ??(firmly),??(brother)
Table 1: Illustration of subjective list M
News articles are first segmented using ICTCLAS
Chinese segmentation word system
16
(Zhang et
al., 2003). Articles with fewer than 200 Chi-
nese words are discarded. News articles are clus-
tered by the presence of a country?s name more
than 2 times based on a country name list from
Wikipedia
17
. Articles mentioning more than 5 dif-
ferent countries are discarded since they usually
talk about international conferences. Note that one
article can appear in different collections (example
in Section 1 will appear in both Vietnam and the
U.S. collection).
Compound sentences are segmented into
clauses based on dependency parse tree. Then
those containing more than 50 characters or
less than 4 characters are discarded. To avoid
complicated inference, sentences with negation
indicators are discarded.
5.2 Obtaining Subjectivity Word List
Since there are few Chinese subjectivity lexicons
(with degrees) available and those exist may not
serve our specific purpose, we manually label a
small number of Chinese subjective terms as seed
corpus. We divided the labeling process into 2
steps rather than directly labeling vocabularies
18
.
We first selected 100 news articles and assigned
each of them (as well as the appropriate coun-
try entity C
i
) to 2 students majoring in Interna-
tional Studies, asking them to give a label sen-
timent score (1 to 7) according to the rules de-
scribed in Section 4.2. 20 students participated
in the procedure. Since annotators have plenty of
background knowledge, they agreed on 98 out of
100. Second, we selected out subjectivity lexicons
by matching to a comprehensive subjectivity lex-
icons list
19
. and ask 2 students select the candi-
dates that signal the document-level label from the
16
http://ictclas.org/
17
http://zh.wikipedia.org/wiki/????-(????)
18
We tried direct vocabulary labeling in the first place, but
got low score for inter agreement, where value of Cohen
?
s ?
is only 0.43.
19
http://ir.dlut.edu.cn/NewsShow.aspx?
ID=215
P R F
Total
semi-CRF 0.74 0.78 0.76
CRF 0.73 0.66 0.68
Single
semi-CRF 0.87 0.92 0.90
CRF 0.80 0.87 0.83
Table 2: Results for Expressions/Targets extrac-
tion.
first step. According to whether a word a selected
or not, the value of Cohen
?
s ? is 0.78, showing
substantial agreement. For the small amount of la-
bels on which the judges disagree, we recruited an
extra judge and to serve as a tie breaker. Table 1
shows some labeled examples.
5.3 Targets and Expressions Extraction
As the good performance of semi-CRF in opinion
extraction has been demonstrated in previous work
(Yang and Cardie, 2012), we briefly go over model
evaluation in this subsection for brevity. We man-
ually labeled 600 sentences and performed 5-fold
cross validation for evaluation. We compare semi-
CRF to Standard CRF. We report performances on
two settings in Table 2. The first setting, Total,
corresponds to performance on the whole dataset,
while second one Single, denotes the performance
on the set of sentences with only one target, which
we are more interested in because multiple-target
sentences are discarded in our algorithm. It turned
out that semi-CRF significantly outputs standard
CRF, approaching 0.90 F-1 score on Single setting.
5.4 Foreign Relation Evaluation
Gold-standard foreign relations are taken from Po-
litical Science research at the Institute of Modern
International Relations, Tsinghua University, ex-
tracted from monthly quantitative China foreign
relations reports with 7 countries (U.S., Japan,
Russia/Soviet, England, France, India, and Ger-
many) from 1950 to 2012
20
.
We consider several baselines. For fair compar-
ison, we use identical processing techniques for
each approach. Some baselines make article-level
predictions, for which we obtain time-period level
relation prediction by averaging the documents.
Coreference+Bootstrap (CB): We first imple-
mented Ngai and Wang?s Chinese coreference sys-
20
Details found here http://www.imir.tsinghua.
edu.cn/publish/iisen/7523/index.html.
472
Model Ours CB No-time
Pearson 0.895 0.753 0.808
Model SVR-d SLDA SVR-S
Pearson 0.482 0.427 0.688
Table 3: Pearson Correlation with Gold Standard.
tem (2007). We then bootstrap sentiment terms
and score based on entity coreference.
No-time: A simplified version of our approach
where each article is considered as an independent
unit and no time-level information is considered.
m
d
is obtained by averaging its containing sen-
tences and used for later bootstrapping.
SVR-d: Uses SVM
light
(Joachims, 1999) to
train a linear SVR (Pang and Lee, 2008) for
document-level sentiment prediction using the un-
igram feature. The 100 labeled documents are
used as training data.
SLDA: supervised-LDA (Blei and McAuliffe,
2010) for document-level label prediction. Topic
number is set to 10, 20, 50, 100 respectively and
we report the best result.
SVR-S: Sentence-level SVR to sentences with
presence of entityC
i
21
. We obtain document-level
prediction by averaging its containing sentences
and then time-period level prediction by averaging
its containing documents.
We report the Pearson Correlation with gold
standards in table 3. As we can observe, simple
document-level regression models, i.e., SVR and
SLDA do not fit this task. The reason is sim-
ple: one article d can appear in different collec-
tions. Recall the Vietnam example in Section 1,
it appears in both G
V ietnam
and G
the U.S.
. Sen-
timent prediction for d should be totally opposite
in the two document collections: very positive in
G
V ietnam
and very negative in G
USA
. But doc-
ument level prediction would treat them equally.
Our approach outperforms No-Time, illustrating
the meaningfulness of exploiting time-level infor-
mation in our task. Our system approaches around
0.9 correlation with the gold standards. The reason
why No-Time is better than CB is also simple: CB
includes only coreferent entities in the target list
(e.g., America for the USA article collection), and
therefore overlooks rich information provided by
non-coreferent entities (e.g., President Nixon or
21
Features we explore include word entities in current sen-
tence, POS, a window of k ? {1, 2} words from the target
and the expression and corresponding POS, and the depen-
dency path between target and expression.
Nixon Government). No-Time instead groups en-
tities according to attitude, thereby enabling more
information to be harnessed. For SVR-S, as the
regression model trained from limited labeled data
can hardly cover unseen terms during testing, the
performance is just OK. SVR-S also suffers from
overlooking rich sources of information since it
only considers sentences with exact mention of the
name entity of the corresponding country.
Figure 5: Examples of China?s Foreign Relations.
6 Diplomatic Relations
?The enemy of my enemy is my friend?
?Arabic proverb
A central characteristic of post-World War Second
international system with which China had to deal
would be overwhelming preeminence of the USA
and USSR as each of the superpowers stood at
the center of a broad alliance system who was en-
gaged in an intense and protracted global conflict
with the other. We choose 6 countries and report
results in Figure 5. One of interesting things we
can observe from Figure 5 is that foreign attitudes
are usually divergent towards two opposing forces:
Sino-American relation (see Figure 5(a)) began to
improve when the Sino-Soviet relation (see Figure
5(b)) reached its bottom at the beginning of 1970s.
Similar patterns appear for Sino-Pakistan (see Fig-
ure 5(c)), Sino-India relations (see Figure 5(d))
473
Figure 6: Top coreference terms Towards USA and Soviet Union/Russia versus time. Blue denotes words
that are both Target and positive words in M . Red denotes words that are both Target and negative words
in M
in early 1960s
22
, and Sino-Vietnam 5(f)), Sino-
American relations in late 1970s. On the con-
trast, attitudes are usually consistent toward allied
forces: Sino-Japan relations with Sino-USA re-
lations before 1990s, and Sino-Vietnam relations
with Sino-Soviet relations in late 1970s and 1980s.
Figure 6 presents top clustering target (T
d
i
) in
the USA and Soviet Union/Russia article collec-
tion. As some of vocabulary terms can be both
target and expression, we use blue to label terms
with positive sentiment, red to label negative ones.
As we can see from Figure 6, targets(T ) extracted
by our model show a very clear pattern where al-
lies and co-referent entities are grouped. Another
interesting thing is, the subjectivity of target words
from different times is generally in accord with the
relation curves shown in Figure 5.
7 Conclusion and Discussion
In this paper, we propose a sentiment analy-
sis algorithm to track China?s foreign relations
from the People?s Daily. Our semi-supervised al-
gorithm harnesses higher level information (i.e.,
document-level, time-level) by incorporating a hi-
erarchical Bayesian approach into the framework,
to resolve sentiment target clustering, create sub-
jective lexicons, and perform sentiment prediction
simultaneously. While we focus here on the Peo-
ple?s Daily for diplomatic relation extraction, the
idea of our approach is general and can be ex-
tended broadly. Another contribution of this work
is the creation a comprehensive Chinese subjec-
22
A fan of history can trace the crucial influence of the
USSR in Sino-India relation in 1960s
tive lexicon list. We are hopeful that our approach
can not only facilitate quantitative research by po-
litical scientists, but also shed light on NLP appli-
cations such as coreference and metaphor, where
sentiment clues can be helpful.
It is worth noting that, while harnessing time-
level information can indeed facilitate opinion
analysis, especially when labeled data is limited in
our specific task, it is not a permanent-perfect as-
sumption, especially considering the diversity and
treacherous currents at the international political
stage.
At algorithm-level, to avoid error propagation
due to limitations of current sentiment analysis
tools (even though semi-CRF produces state-of-art
performance in target and expression extraction
task, a performance of 0.8 F-value, when applied
to the whole corpus, can by no means satisfy
our requirements), we discard a great number of
sentences, among which is contained much useful
information. How to resolve these problems
and improve opinion extraction performance is
our long-term goal in sentiment analysis/opinion
extraction literature.
Acknowledgements
The authors want to thank Bishan Yang and Claire
Cardie for useful comments and discussions. The
authors are thankful for suggestions offered by
EMNLP reviewers.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-
bow, and Rebecca Passonneau. 2011. Sentiment
474
analysis of twitter data. In Proceedings of the Work-
shop on Languages in Social Media.
Galen Andrew. 2006. A hybrid markov/semi-markov
conditional random field for sequence segmentation.
In EMNLP.
David M Blei and Jon D McAuliffe. 2010. Supervised
topic models. arXiv preprint arXiv:1003.0783.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In IJCAI.
Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth
Patwardhan. 2005. Identifying sources of opinions
with conditional random fields and extraction pat-
terns. In EMNLP.
Yejin Choi, Eric Breck, and Claire Cardie. 2006. Joint
extraction of entities and relations for opinion recog-
nition. In EMNLP.
Tang Duyu, Qin Bing, Zhou LanJun, Wong KamFai,
Zhao Yanyan, and Liu Ting. 2013. Domain-specific
sentiment word extraction by seed expansion and
pattern generation. arXiv preprint arXiv:1309.6722.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford.
Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009.
A novel lexicalized hmm-based learning framework
for web opinion mining. In ICML.
Yohan Jo and Alice H Oh. 2011. Aspect and senti-
ment unification model for online review analysis.
In ICWSM.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion ex-
pression detection. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning.
Soo-Min Kim and Eduard Hovy. 2004. Determin-
ing the sentiment of opinions. In Proceedings of
the 20th international conference on Computational
Linguistics, page 1367. Association for Computa-
tional Linguistics.
Soo-Min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Proceedings of the Work-
shop on Sentiment and Subjectivity in Text.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Yun-zhong LIU, Ya-ping LIN, and Zhi-ping CHEN.
2004. Text information extraction based on hid-
den markov model [j]. Acta Simulata Systematica
Sinica.
Tie-Yan Liu. 2009. Learning to rank for information
retrieval. Foundations and Trends in Information
Retrieval.
Grace Ngai and Chi-Shing Wang. 2007. A knowledge-
based approach for unsupervised chinese corefer-
ence resolution. Computational Linguistics and
Chinese Language Processing, 12(4):459?484.
Daisuke Okanohara, Yusuke Miyao, Yoshimasa Tsu-
ruoka, and Jun?ichi Tsujii. 2006. Improving
the scalability of semi-markov conditional random
fields for named entity recognition. In ACL.
Brendan O?Connor, Brandon M Stewart, and Noah A
Smith. 2013. Learning to extract international rela-
tions from political context. In ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using
machine learning techniques. In EMNLP.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In IJCAI.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011. Opinion word expansion and target extrac-
tion through double propagation. Computational
linguistics.
Thomas W Robinson and David L Shambaugh. 1995.
Chinese foreign policy: theory and practice. Oxford
University Press.
Sunita Sarawagi and William W Cohen. 2004. Semi-
markov conditional random fields for information
extraction. In NIPS.
Veselin Stoyanov and Claire Cardie. 2008. Topic iden-
tification for fine-grained opinion analysis. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a
rating regression approach. In SIGKDD.
Guoguang Wu. 1994. Command communication: The
politics of editorial formulation in the people?s daily.
China Quarterly, 137:194?211.
Bishan Yang and Claire Cardie. 2012. Extracting opin-
ion expressions with semi-markov conditional ran-
dom fields. In EMNLP.
Bishan Yang and Claire Cardie. 2014. Context-aware
learning for sentence-level sentiment analysis with
posterior regularization. ACL.
Hua-Ping Zhang, Hong-Kui Yu, De-Yi Xiong, and Qun
Liu. 2003. Hhmm-based chinese lexical analyzer
ictclas. In Proceedings of the second SIGHAN work-
shop on Chinese language processing-Volume 17.
475
Lei Zhang, Bing Liu, Suk Hwan Lim, and Eamonn
O?Brien-Strain. 2010. Extracting and ranking prod-
uct features in opinion documents. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics: Posters.
476
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1997?2007,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Major Life Event Extraction from Twitter based on
Congratulations/Condolences Speech Acts
Jiwei Li
1
, Alan Ritter
2
, Claire Cardie
3
and Eduard Hovy
4
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
Department of Computer Science and Engineering, the Ohio State University, OH 43210, USA
3
Computer Science Department, Cornell University, Ithaca, NY 14853, USA
4
Language Technology Institute, Carnegie Mellon University, PA 15213, USA
jiweil@stanford.edu ritter.1492@osu.edu
cardie@cs.cornell.edu ehovy@andrew.cmu.edu
Abstract
Social media websites provide a platform
for anyone to describe significant events
taking place in their lives in realtime.
Currently, the majority of personal news
and life events are published in a tex-
tual format, motivating information ex-
traction systems that can provide a struc-
tured representations of major life events
(weddings, graduation, etc. . . ). This pa-
per demonstrates the feasibility of accu-
rately extracting major life events. Our
system extracts a fine-grained description
of users? life events based on their pub-
lished tweets. We are optimistic that our
system can help Twitter users more easily
grasp information from users they take in-
terest in following and also facilitate many
downstream applications, for example re-
altime friend recommendation.
1 Introduction
Social networking websites such as Facebook and
Twitter have recently challenged mainstream me-
dia as the freshest source of information on im-
portant news events. In addition to an important
source for breaking news, social media presents a
unique source of information on private events, for
example a friend?s engagement or college gradua-
tion (examples are presented in Figure 1). While
a significant amount of previous work has inves-
tigated event extraction from Twitter (e.g., (Rit-
ter et al., 2012; Diao et al., 2012)), existing ap-
proaches mostly focus on public bursty event ex-
traction, and little progress has been made towards
the problem of automatically extracting the major
life events of ordinary users.
A system which can automatically extract ma-
jor life events and generate fine-grained descrip-
tions as in Figure 1 will not only help Twitter
users with the problem of information overload by
summarizing important events taking place in their
friends lives, but could also facilitate downstream
applications such as friend recommendation (e.g.,
friend recommendation in realtime to people who
were just admitted into the same university, get
the same jobs or internships), targeted online ad-
vertising (e.g., recommend baby care products to
newly expecting mothers, or wedding services to
new couples), information extraction, etc.
Before getting started, we first identify a num-
ber of key challenges in extracting significant life
events from user-generated text, which account the
reason for the lack of previous work in this area:
Challenge 1: Ambiguous Definition for Ma-
jor Life Events Major life event identification
is an open-domain problem. While many types of
events (e.g., marriage, engagement, finding a new
job, giving birth) are universally agreed to be im-
portant, it is difficult to robustly predefine a list of
characteristics for important life events on which
algorithms can rely for extraction or classification.
Challenge 2: Noisiness of Twitter Data: The
user-generated text found in social media websites
such as Twitter is extremely noisy. The language
used to describe life events is highly varied and
ambiguous and social media users frequently dis-
cuss public news and mundane events from their
daily lives, for instance what they ate for lunch.
Even for a predefined life event category, such
as marriage, it is still difficult to accurately iden-
tify mentions. For instance, a search for the
keyphrase ?get married? using Twitter Search
1
re-
sults in a large number of returned results that do
not correspond to a personal event:
? I want to get married once. No divorce & no
cheating, just us two till the end.
(error: wishes)
1
https://twitter.com/search?q=
get
?
married
1997
Figure 1: Examples of users mentioning personal life events on Twitter.
? Can Adam Sandler and Drew Barrymore just
drop the pretense and get married already?
(error: somebody else)
? I got married and had kids on purpose
(error: past)
Challenge 3: the Lack of Training Data Col-
lecting sufficient training data in this task for ma-
chine learning models is difficult for a number of
reasons: (1) A traditional, supervised learning ap-
proach, requires explicit annotation guidelines for
labeling, though it is difficult to know which cat-
egories are most representative in the data apriori.
(2) Unlike public events which are easily identi-
fied based on message volume, significant private
events are only mentioned by one or several users
directly involved in the event. Many important cat-
egories are relatively infrequent, so even a large
annotated dataset may contain just a few or no ex-
amples of these categories, making classification
difficult.
In this paper, we present a pipelined system that
addresses these challenges and extracts a struc-
tured representation of individual life events based
on users? Twitter feeds. We exploit the insight to
automatically gather large volumes of major life
events which can be used as training examples for
machine learning models. Although personal life
events are difficult to identify using traditional
approaches due to their highly diverse nature, we
noticed that users? followers often directly reply
to such messages with CONGRATULATIONS or
CONDOLENCES speech acts, for example:
User1: I got accepted into Harvard !
User2: Congratulations !
These speech acts are easy to identify with high
precision because the possible ways to express
them are relatively constrained. Instead of directly
inspecting tweets to determine whether they corre-
spond to major life events, we start by identifying
replies corresponding to CONGRATULATIONS or
CONDOLENCES, and then retrieve the message
they are in response to, which we assume refer to
important life events.
The proposed system automatically identifies
major life events and then extracts correspondent
event properties. Through the proposed system,
we demonstrate that it is feasible to automatically
reconstruct a detailed list of individual life events
based on users? Twitter streams. We hope that
work presented in this paper will facilitate down-
stream applications and encourage follow-up work
on this task.
2 System Overview
An overview of the components of the system is
presented in Figure 2. Pipeline1 first identifies
the major life event category the input tweet talks
about and filters out the irrelevant tweets and will
be described in Section 4. Next, Pipeline2, as,
demonstrated in Section 5, identifies whether the
speaker is directly involved in the life event. Fi-
nally, Pipeline3 extracts the property of event and
will be illustrated in Section 6.
Section 3 serves as the preparing step for the
pipelined system, describing how we collect train-
ing data in large-scale. The experimental evalua-
tion regarding each pipeline of the system is pre-
sented in the corresponding section (i.e., Section
4,5,6) and the end-to-end evaluation will be pre-
1998
Figure 2: System Overview. Blue: original input tweets. Red: filtered out tweets. Magenta: life event
category. Green: life event property. Pipeline 1 identifies the life category the input tweet talks about
(e.g., marriage, graduation) and filter out irrelevant tweets (e.g., I had beef stick for lunch). Pipeline 2
identifies whether the speaker is directly involved in the event. It will preserve self-reported information
(i.e. ?I got married?) and filtered out unrelated tweets (e.g., ?my friend Chris got married?). Pipeline
3 extracts the property of event (e.g. to whom the speaker married or the speaker admitted by which
university).
sented in Section 7.
3 Personal Life Event Clustering
In this section, we describe how we identify com-
mon categories of major life events by leverag-
ing large quantities of unlabeled data and obtain
a collection of tweets corresponding to each type
of identified event.
3.1 Response based Life Event Detection
While not all major life events will elicit CON-
GRATULATIONS or CONDOLENCES from a user?s
followers, this technique allows us to collect large
volumes of high-precision personal life events
which can be used to train models to recognize the
diverse categories of major life events discussed
by social media users.
3.2 Life Event Clustering
Based on the above intuition, we develop an ap-
proach to obtain a list of individual life event clus-
ters. We first define a small set of seed responses
which capture common CONGRATULATIONS and
CONDOLENCES, including the phrases: ?Congrat-
ulations?, ?Congrats?, ?Sorry to hear that?, ?Awe-
some?, and gather tweets that were observed with
seed responses. Next, an LDA (Blei et al., 2003)
2
based topic model is used to cluster the gathered
2
Topic Number is set to 120.
tweets to automatically identify important cate-
gories of major life events in an unsupervised way.
In our approach, we model the whole conversation
dialogue as a document
3
with the response seeds
(e.g., congratulation) masked out. We furthermore
associate each sentence with a single topic, fol-
lowing strategies adopted by (Ritter et al., 2010;
Gruber et al., 2007). We limit the words in our
document collection to verbs and nouns which
we found to lead to clearer topic representations,
and used collapsed Gibbs Sampling for inference
(Griffiths and Steyvers, 2004).
Next one of the authors manually inspected the
resulting major life event types inferred by the
model, and manually assigned them labels such
as ?getting a job?, ?graduation? or ?marriage?
and discarded incoherent topics
4
. Our methodol-
ogy is inspired by (Ritter et al., 2012) that uses
a LDA-CLUSTERING+HUMAN-IDENTIFICATION
strategy to identify public events from Twitter.
Similar strategies have been widely used in un-
supervised information extraction (Bejan et al.,
2009; Yao et al., 2011) and selectional preference
3
Each whole conversation usually contains multiple
tweets and users.
4
While we applied manual labeling and coherence eval-
uation in this work, an interesting direction for future work
is automatically labeling major life event categories follow-
ing previous work on labeling topics in traditional document-
based topic models (Mimno et al., 2011; Newman et al.,
2010).
1999
Figure 3: Illustration of bootstrapping process.
Input: Reply seed list E = {e}, Tweet conversation col-
lection T = {t}, Retrieved Tweets Collection D = ?.
Identified topic list L=?
Begin
While not stopping:
1. For unprocessed conversation t ? T
if t contains reply e ? E,
? add t to D: D = D + t.
? remove t from T : T = T ? t
2. Run streaming LDA (Yao et al., 2009) on newly added
tweets in D.
3. Manually Identify meaningful/trash topics, giving label
to meaningful topics.
4. Add newly detected meaningful topic l to L.
5. For conversation t belonging to trash topics
? remove t from D: D = D ? t
6. Harvest more tweets based on topic distribution.
7. Manually identify top 20 responses to tweets harvested
from Step 6.
8. Add meaningful responses to E.
End
Output: Identified topic list L. Tweet collection D.
Figure 4: Bootstrapping Algorithm for Response-
based Life event identification.
modeling (Kozareva and Hovy, 2010a; Roberts
and Harabagiu, 2011).
Conversation data was extracted from the CMU
Twitter Warehouse of 2011 which contains a total
number of 10% of all published tweets in that year.
3.3 Expanding dataset using Bootstrapping
While our seed patterns for identifying mes-
sages expressing CONGRATULATIONS and CON-
DOLENCES are very high precision, they don?t
cover all the possible ways these speech acts
can be expressed. We therefore adopt a semi-
supervised bootstrapping approach to expand our
reply seeds and event-related tweets. Our boot-
strapping approach is related to previous work
on semi-supervised information harvesting (e.g.,
(Kozareva and Hovy, 2010b; Davidov et al.,
2007)). To preserve the labeled topics from the
first iteration, we apply a streaming approach to
inference (Yao et al., 2009) over unlabeled tweets
(those which did not match one of the response
Figure 5: Illustration of data retrieved in each step
of bootstrapping.
congratulations (cong, congrats); (that?s) fantastic; (so) cool;
(I?m) (very) sorry to hear that; (that?s) great (good) new;
awesome; what a pity; have fun; great; that sucks; too
bad; (that?s) unfortunate; how sad; fabulous; (that?s)
terrific; (that?s) (so) wonderful; my deepest condolences;
Table 1: Responses retrieved from Bootstrapping.
seeds). We collect responses to the newly added
tweets, then select the top 20 frequent replies
5
.
Next we manually inspect and filter the top ranked
replies, and use them to harvest more tweets. This
process is then repeated with another round of
inference in LDA including manual labeling of
newly inferred topics, etc... An illustration of our
approach is presented in Figure 3 and the details
are presented in Figure 4. The algorithm outputs
a collection of personal life topics L, and a collec-
tion of retrieved tweets D. Each tweet d ? D is
associated with a life event topic l, l ? L.
We repeat the bootstrapping process for 4 iter-
ations and end up with 30 different CONGRATU-
LATIONS and CONDOLENCES patterns (shown in
Table 1) and 42 coherent event types which refer to
significant life events (statistics for harvested data
from each step is shown in Figure 5). We show
examples of the mined topics with correspondent
human labels in Table 3, grouped according to a
specific kind of resemblance.
3.4 Summary and Discussion
The objective of this section is (1) identifying a
category of life events (2) identifying tweets asso-
ciated with each event type which can be used as
candidates for latter self reported personal infor-
mation and life event category identification.
We understand that the event list retrieved from
our approach based on replies in the conversation
is far from covering all types of personal events
(especially the less frequent life events). But our
5
We only treat the first sentence that responds to the be-
ginning of the conversation as replies.
2000
Life Event Proportion
Birthday 9.78
Job 8.39
Wedding
Engagement
7.24
Award 6.20
Sports 6.08
Anniversary 5.44
Give Birth 4.28
Graduate 3.86
Death 3.80
Admission 3.54
Interview
Internship
3.44
Moving 3.26
Travel 3.24
Illness 2.45
Life Event Proportion
Vacation 2.24
Relationship 2.16
Exams 2.02
Election 1.85
New Car 1.65
Running 1.42
Surgery 1.20
Lawsuit 0.64
Acting 0.50
Research 0.48
Essay 0.35
Lost Weight 0.35
Publishing 0.28
Song 0.22
OTHER 15.31
Table 2: List of automatically discovered life event
types with percentage (%) of data covered.
list is still able to cover a large proportion of IM-
PORTANT and COMMON life events. Our latter
work is focused on given a random tweet, identi-
fying whether it corresponds to one of the 42 types
of life events in our list.
Another thing worth noting here is that, while
current section is not focused on self-reported in-
formation identification, we have already obtained
a relatively clean set of data with a large pro-
portion of non self-reported information related
tweets being screened: people do not usually re-
spond to non self-reported information with com-
monly used replies, or in other words, with replies
that will pass our next step human test
6
. These non
self-reported tweets would therefore be excluded
from training data.
4 Life Event Identification
In this section, we focused on deciding whether a
given tweet corresponds to one of the 42 prede-
fined life events.
Our training dataset consists of approximately
72,000 tweets from 42 different categories of life
events inferred by our topic model as described
in Section 3. We used the top 25% of tweets for
which our model assigned highest probability to
each topic. For sparsely populated topics we used
the top 50% of tweets to ensure sufficient cover-
age.
We further collected a random sample of about
10 million tweets from Twitter API
7
as non-life
6
For example, people don?t normally respond to ?I want
to get married once? (example in Challenge 2, Section 1)
with ?Congratulations?.
7
https://dev.twitter.com/
Human Label Top words
Wedding
&engagement
wedding, love, ring, engagement,
engaged, bride, video, marrying
Relationship
Begin
boyfriend, girlfriend, date, check,
relationship, see, look
Anniversary anniversary, years, year, married,
celebrating, wife, celebrate, love
Relation End/
Devoice
relationship, ended, hurt, hate, de-
voice, blessings, single
Graduation graduation, school, college, gradu-
ate, graduating, year, grad
Admission admitted, university, admission, ac-
cepted, college, offer, school
Exam passed, exam, test, school,
semester, finished, exams,
midterms
Research research, presentation, journalism,
paper, conference, go, writing
Essay & Thesis essay, thesis, reading, statement,
dissertation, complete, project
Job job, accepted, announce, join, join-
ing, offer, starting, announced,
work
Interview& In-
ternship
interview, position, accepted, in-
ternship, offered, start, work
Moving house, moving, move, city, home,
car, place, apartment, town, leaving
Travel leave, leaving, flight, home, miss,
house, airport, packing, morning
Vacation vocation, family, trip, country, go,
flying, visited, holiday, Hawaii
Winning Award won, award, support, awards, win-
ning, honor, scholarship, prize
Election/
Promotion/
Nomination
president, elected, run, nominated,
named, promotion, cel, selected,
business, vote
Publishing book, sold, writing, finished, read,
copy, review, release, books, cover
Contract signed, contract, deal, agreements,
agreed, produce, dollar, meeting
song/ video/ al-
bum release
video, song, album, check, show,
see, making, radio, love
Acting play, role, acting, drama, played,
series, movie, actor, theater
Death dies, passed, cancer, family, hospi-
tal, dad, grandma, mom, grandpa
Give Birth baby, born, boy, pregnant, girl, lbs,
name, son, world, daughter, birth
Illness ill, hospital, feeling, sick, cold, flu,
getting, fever, doctors, cough
Surgery surgery, got, test, emergency, blood,
tumor, stomachs, hospital, pain,
brain
Sports win, game, team, season, fans,
played, winning, football, luck
Running run, race, finished, race, marathon,
ran, miles, running, finish, goal
New Car car, buy, bought, cars, get, drive,
pick, seat, color, dollar, meet
Lost Weight weight, lost, week, pounds, loss,
weeks, gym, exercise, running
Birthday birthday, come, celebrate, party,
friends, dinner, tonight, friend
Lawsuit sue, sued, file, lawsuit, lawyer, dol-
lars, illegal, court, jury.
Table 3: Example event types with top words dis-
covered by our model.
2001
event examples and trained a 43-class maximum
entropy classifier based on the following features:
? Word: The sequence of words in the tweet.
? NER: Named entity Tag.
? Dictionary: Word matching a dictionaries of
the top 40 words for each life event category
(automatically inferred by the topic model).
The feature value is the term?s probability
generated by correspondent event.
? Window: If a dictionary term exists, left and
right context words within a window of 3
words and their part-of-speech tags.
Name entity tag is assigned from Ritter et al?s
Twitter NER system (Ritter et al., 2011). Part-of-
Speech tags are assigned based on Twitter POS
package (Owoputi et al., 2013) developed by
CMU ARK Lab. Dictionary and Window are
constructed based on the topic-term distribution
obtained from the previous section.
The average precision and recall are shown in
Table 4. And as we can observe, the dictionary
(with probability) contributes a lot to the perfor-
mance and by taking into account a more compre-
hensive set of information around the key word,
classifier on All feature setting generate signifi-
cantly better performance, with 0.382 prevision
and 0.48 recall, which is acceptable considering
(1) This is is a 43-way classification with much
more negative data than positive (2) Some types of
events are very close to each other (e.g., Leaving
and Vocation). Note that recall is valued more than
precision here as false-positive examples will be
further screened in self-reported information iden-
tification process in the following section.
Feature Setting Precision Recall
Word+NER 0.204 0.326
Word+NER+Dictionary 0.362 0.433
All 0.382 0.487
Table 4: Average Performance of Multi-Class
Classifier on Different Feature Settings. Negative
examples (non important event type) are not con-
sidered.
5 Self-Reported Information
Identification
Although a message might refer to a topic cor-
responding to a life event such as marriage, the
event still might be one in which the speaker is
not directly involved. In this section we describe
the self reported event identification portion of our
pipeline, which takes output from Section 4 and
further identifies whether each tweet refers to an
event directly involving the user who publishes it.
Direct labeling of randomly sampled Twitter
messages is infeasible for the following reasons:
(1) Class imbalance: self-reported events are rela-
tively rare in randomly sampled Twitter messages.
(2) A large proportion of self-reported information
refers to mundane, everyday topics (e.g., ?I just
finished dinner!?). Fortunately, many of the tweets
retrieved from Section 3 consist of self-reported
information and describe major life events. The
candidates for annotation are therefore largely nar-
rowed down.
We manually annotated 800 positive examples
of self-reported events distributed across the event
categories identified in Section 3. We ensured
good coverage by first randomly sampling 10 ex-
amples from each category, the remainder were
sampled from the class distribution in the data.
Negative examples of self-reported information
consisted of a combination of examples from the
original dataset
8
and randomly sampled messages
gathered by searching for the top terms in each of
the pre-identified topics using the Twitter Search
interface
9
. Due to great varieties of negative sce-
narios, the negative dataset constitutes about 2500
tweets.
5.1 Features
Identifying self-reported tweet requires sophisti-
cated feature engineering. Let u denote the term
within the tweet that gets the highest possibility
generated by the correspondent topic. We experi-
mented with combinations of the following types
of features (results are presented in Table ??):
? Bigram: Bigrams within each tweet (punctu-
ation included).
? Window: A window of k ? {0, 1, 2} words
adjacent to u and their part-of-speech tags.
? Tense: A binary feature indicating past tense
identified in by the presence of past tense
verb (VBD).
? Factuality: Factuality denotes whether one
expression is presented as corresponding to
real situations in the world (Saur?? and Puste-
jovsky, 2007). We use Stanford PragBank
10
,
8
Most tweets in the bootstrapping output are positive.
9
The majority of results returned by Twitter Search are
negative examples.
10
http://compprag.christopherpotts.net/
factbank.html
2002
an extension of FactBank (Saur?? and Puste-
jovsky, 2009) which contains a list of modal
words such as ?might?, ?will?, ?want to?
etc
11
.
? I: Whether the subject of the tweet is first per-
son singular.
? Dependency: If the subject is first person
singular and the u is a verb, the dependency
path between the subject and u (or non-
dependency).
Tweet dependency paths were obtained from
(Kong et al., 2014). As the tweet parser we use
only supports one-to-one dependency path iden-
tification but no dependency properties, Depen-
dency is a binary feature. The subject of each
tweet is determined by the dependency link to the
root of the tweet from the parser.
Among the features we explore, Word encodes
the general information within the tweet. Win-
dow addresses the information around topic key
word. The rest of the features specifically address
each of the negative situations described in Chal-
lenge 2, Section 1: Tense captures past event de-
scription, Factuality filters out wishes or imagi-
nation, I and Dependency correspond to whether
the described event involves the speaker. We built
a linear SVM classifier using SVM
light
package
(Joachims, 1999).
5.2 Evaluation
Feature Setting Acc Pre Rec
Bigram+Window 0.76 0.47 0.44
Bigram+Window
+Tense+Factuality
0.77 0.47 0.46
all 0.82 0.51 0.48
Table 5: Performance for self-report information
identification regarding different feature settings.
We report performance on the task of identi-
fying self-reported information in this subsection.
We employ 5-fold cross validation and report Ac-
curacy (Accu), Prevision (Prec) and Recall (Rec)
regarding different feature settings. The Tense,
Factuality, I and Dependency features positively
contribute to performance respectively and the
best performance is obtained when all types of fea-
tures are included.
11
Due to the colloquial property of tweets, we also intro-
duced terms such as ?gonna?, ?wanna?, ?bona?.
precision recall F1
0.82 0.86 0.84
Table 7: Performance for identifying properties.
6 Event Property Extraction
Thus far we have described how to automatically
identify tweets referring to major life events. In
addition, it is desirable to extract important prop-
erties of the event, for example the name of the
university the speaker was admitted to (See Figure
1). In this section we take a supervised approach to
event property extraction, based on manually an-
notated data for a handfull of the major life event
categories automatically identified by our system.
While this approach is unlikely to scale to the di-
versity of important personal events Twitter users
are discussing, our experiments demonstrate that
event property extraction is indeed feasible.
We cast the problem of event property extrac-
tion as a sequence labeling task, using Conditional
Random Fields (Lafferty et al., 2001) for learning
and inference. To make best use of the labeled
data, we trained a unified CRF model for closely
related event categories which often share proper-
ties; the full list is presented in Table 6 and we
labeled 300 tweets in total. Features we used in-
clude:
? word token, capitalization, POS
? left and right context words within a window
of 3 and the correspondent part-of-speech
tags
? word shape, NER
? a gazetteer of universities and employers bor-
rowed from NELL
12
.
We use 5-fold cross-validation and report results
in Table 7.
7 End-to-End Experiment
The evaluation for each part of our system has
been demonstrated in the corresponding section.
We now present a real-world evaluation: to what
degree can our trained system automatically iden-
tify life events in real world.
7.1 Dataset
We constructed a gold-standard life event dataset
using annotators from Amazon?s Mechanical Turk
(Snow et al., 2008) using 2 approaches:
12
http://rtw.ml.cmu.edu/rtw/kbbrowser/
2003
Life Event Property
(a) Acceptance, Graduation Name of University/College
(b) Wedding, Engagement, Falling love Name of Spouse/ partner/ bf/ gf
(c) Getting a job, interview, internship Name of Enterprise
(d) Moving to New Places, Trip, Vocation, Leaving Place, Origin, Destination
(e) Winning Award Name of Award, Prize
Table 6: Labeling Event Property.
? Ask Twitter users to label their own tweets
(Participants include friends, colleagues of
the authors and Turkers from Amazon Me-
chanical Turk
13
).
? Ask Turkers to label other people?s tweets.
For option 1, we asked participants to directly la-
bel their own published tweets. For option 2, for
each tweet, we employed 2 Turkers. Due to the
ambiguity in defining life events, the value co-
hen?s kappa
14
as a measure of inter-rater agree-
ment is 0.54; this does not show significant inter-
annotator agreement. The authors examined dis-
agreements and also verified all positively labeled
tweets. The resulting dataset contains around 900
positive tweets and about 60,000 negative tweets.
To demonstrate the advantage of leveraging
large quantities of unlabeled data, the first base-
line we investigate is a Supervised model which is
trained on the manually annotated labeled dataset,
and evaluated using 5 fold cross validation. Our
Supervised baseline consists of a linear SVM
classifier using bag of words, NER and POS fea-
tures. We also tested a second baseline that
combines Supervised algorithm with an our self-
reported information classifier, denoted as Super-
vised+Self.
Results are reported in Table 8; as we can ob-
serve, the fully supervised approach is not suitable
for this task with only one digit F1 score. The
explanations are as follows: (1) the labeled data
can only cover a small proportion of life events
(2) supervised learning does not separate impor-
tant event categories and will therefore classify
any tweet with highly weighted features (e.g., the
mention of ?I? or ?marriage?) as positive. By us-
ing an additional self-reported information classi-
fier in Supervised+Self, we get a significant boost
in precision with a minor recall loss.
13
https://www.mturk.com/mturk/welcome
14
http://en.wikipedia.org/wiki/Cohen?s_
kappa
Approach Precision Recall
Our approach 0.62 0.48
Supervised 0.13 0.20
Supervised+Self 0.25 0.18
Table 8: Performance for different approaches for
identifying life events in real world.
Approach Precision Recall
Step 1 0.65 0.36
Step 2 0.64 0.43
Step 3 0.62 0.48
Table 9: Performance for different steps of boot-
strapping for identifying life events in real world.
Another interesting question is to what degree
the bootstrapping contributes to the final results.
We keep the self-reported information classifier
fixed (though it?s based the ultimate identified
data source), and train the personal event classifier
based on topic distributions identified from each
of the three steps of bootstrapping
15
. Precision
and recall at various stages of bootstrapping are
presented in Table 9. As bootstrapping continues,
the precision remains roughly constant, but recall
increases as more life events and CONGRATULA-
TIONS and CONDOLENCES are discovered.
8 Related Work
Our work is related to three lines of NLP re-
searches. (1) user-level information extraction on
social media (2) public event extraction on social
media. (3) Data harvesting in Information Extrac-
tion, each of which contains large amount of re-
lated work, to which we can not do fully justice.
User Information Extraction from Twitter
Some early approaches towards understanding
user level information on social media is focused
on user profile/attribute prediction (e.g.,(Ciot et
al., 2013)) user-specific content extraction (Diao
15
which are 24, 38, 42-class classifiers, where 24, 38, 42
denoted the number of topics discovered in each step of boot-
strapping (see Figure 5).
2004
et al., 2012; Diao and Jiang, 2013; Li et al., 2014)
or user personalization (Low et al., 2011) identifi-
cation.
The problem of user life event extraction was
first studied by Li and Cardie?s (2014). They at-
tempted to construct a chronological timeline for
Twitter users from their published tweets based on
two criterion: a personal event should be personal
and time-specific. Their system does not explic-
itly identify a global category of life events (and
tweets discussing correspondent event) but identi-
fies the topics/events that are personal and time-
specific to a given user using an unsupervised ap-
proach, which helps them avoids the nuisance of
explicit definition for life event characteristics and
acquisition of labeled data. However, their sys-
tem has the short-coming that each personal topic
needs to be adequately discussed by the user and
their followers in order to be detected
16
.
Public Event Extraction from Twitter Twitter
serves as a good source for event detection owing
to its real time nature and large number of users.
These approaches include identifying bursty pub-
lic topics (e.g.,(Diao et al., 2012)), topic evolution
(Becker et al., 2011) or disaster outbreak (Sakaki
et al., 2010; Li and Cardie, 2013) by spotting the
increase/decrease of word frequency. Some other
approaches are focused on generating a structured
representation of events (Ritter et al., 2012; Ben-
son et al., 2011).
Data Acquisition in Information Extraction
Our work is also related with semi-supervised data
harvesting approaches, the key idea of which is
that some patterns are learned based on seeds.
They are then used to find additional terms, which
are subsequently used as new seeds in the patterns
to search for additional new patterns (Kozareva
and Hovy, 2010b; Davidov et al., 2007; Riloff
et al., 1999; Igo and Riloff, 2009; Kozareva et
al., 2008). Also related approaches are distant or
weakly supervision (Mintz et al., 2009; Craven et
al., 1999; Hoffmann et al., 2011) that rely on avail-
able structured data sources as a weak source of
supervision for pattern extraction from related text
corpora.
16
The reason is that topic models use word frequency for
topic modeling.
9 Conclusion and Discussion
In this paper, we propose a pipelined system for
major life event extraction from Twitter. Experi-
mental results show that our model is able to ex-
tract a wide variety of major life events.
The key strategy adopted in this work is to ob-
tain a relatively clean training dataset from large
quantity of Twitter data by relying on minimum
efforts of human supervision, and sometimes is at
the sacrifice of recall. To achieve this goal, we rely
on a couple of restrictions and manual screenings,
such as relying on replies, LDA topic identifica-
tion and seed screening. Each part of system de-
pends on the early steps. For example, topic clus-
tering in Section 3 not only offers training data for
event identification in Section 4, but prepares the
training data for self-information identification in
Section 5. .
We acknowledge that our approach is not
perfect due to the following ways: (1) The system
is only capable of discovering a few categories
of life events with many others left unidentified.
(2) Each step of the system will induce errors and
negatively affected the following parts. (3) Some
parts of evaluations are not comprehensive due
to the lack of gold-standard data. (4) Among all
pipelines, event property identification in Section
6 still requires full supervision in CRF model,
making it hard to scale to every event type
17
.
How to address these aspects and generate a more
accurate, comprehensive and fine-grained life
event list for Twitter users constitute our further
work.
Acknowledgements
A special thanks is owned to Myle Ott for sug-
gestions on bootstrapping procedure in data har-
vesting. The authors want to thank Noah Smith,
Chris Dyer and Alok Kothari for useful com-
ments, discussions and suggestions regarding dif-
ferent steps of the system and evaluations. We
thank Lingpeng Kong and members of Noah?s
ARK group at CMU for providing the tweet de-
pendency parser. All data used in this work is ex-
tracted from CMU Twitter Warehouse maintained
by Brendan O?Connor, to whom we want to ex-
press our gratitude.
17
We view weakly supervised life event property extrac-
tion as an interesting direction for future work.
2005
References
Hila Becker, Mor Naaman, and Luis Gravano. 2011.
Beyond trending topics: Real-world event identifi-
cation on twitter. ICWSM, 11:438?441.
Cosmin Adrian Bejan, Matthew Titsworth, Andrew
Hickl, and Sanda M Harabagiu. 2009. Nonparamet-
ric bayesian models for unsupervised event corefer-
ence resolution. In NIPS, pages 73?81.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent dirichlet allocation. the Journal of ma-
chine Learning research, 3:993?1022.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Mark Craven, Johan Kumlien, et al. 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Dmitry Davidov, Ari Rappoport, and Moshe Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Annual
Meeting-Association For Computational Linguis-
tics, volume 45, page 232.
Qiming Diao and Jing Jiang. 2013. A unified model
for topics, events and users on twitter. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1869?1879.
Qiming Diao, Jing Jiang, Feida Zhu, and Ee-Peng
Lim. 2012. Finding bursty topics from microblogs.
In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Long
Papers-Volume 1, pages 536?544. Association for
Computational Linguistics.
Thomas L Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
academy of Sciences of the United States of Amer-
ica, 101(Suppl 1):5228?5235.
Amit Gruber, Yair Weiss, and Michal Rosen-Zvi.
2007. Hidden topic markov models. In Inter-
national Conference on Artificial Intelligence and
Statistics, pages 163?170.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies-
Volume 1, pages 541?550. Association for Compu-
tational Linguistics.
Sean P Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics, pages 18?26. Association for Computa-
tional Linguistics.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Lingpeng Kong, Nathan Schneider, Swabha
Swayamdipta, Archna Bhatia, Chris Dyer, and
Noah Smith. 2014. A dependency parser for tweets.
In EMNLP.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learn-
ing arguments and supertypes of semantic relations
using recursive patterns. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 1482?1491. Association
for Computational Linguistics.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not
all seeds are equal: Measuring the quality of text
mining seeds. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 618?626. Association for Computa-
tional Linguistics.
Zornitsa Kozareva, Ellen Riloff, and Eduard H Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In ACL, volume 8,
pages 1048?1056.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data.
Jiwei Li and Claire Cardie. 2013. Early stage
influenza detection from twitter. arXiv preprint
arXiv:1309.7340.
Jiwei Li and Claire Cardie. 2014. Timeline generation:
Tracking individuals on twitter. WWW, 2014.
Jiwei Li, Alan Ritter, and Eduard Hovy. 2014.
Weakly supervised user profile extraction from twit-
ter. ACL.
Yucheng Low, Deepak Agarwal, and Alexander J
Smola. 2011. Multiple domain user personaliza-
tion. In Proceedings of the 17th ACM SIGKDD in-
ternational conference on Knowledge discovery and
data mining, pages 123?131. ACM.
David Mimno, Hanna M Wallach, Edmund Talley,
Miriam Leenders, and Andrew McCallum. 2011.
Optimizing semantic coherence in topic models. In
2006
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, pages 262?
272. Association for Computational Linguistics.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
David Newman, Jey Han Lau, Karl Grieser, and Tim-
othy Baldwin. 2010. Automatic evaluation of
topic coherence. In Human Language Technologies:
The 2010 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 100?108. Association for Computa-
tional Linguistics.
Olutobi Owoputi, Brendan O?Connor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Ellen Riloff, Rosie Jones, et al. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI/IAAI, pages 474?479.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-
supervised modeling of twitter conversations.
Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011.
Named entity recognition in tweets: an experimental
study. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, pages
1524?1534. Association for Computational Linguis-
tics.
Alan Ritter, Oren Etzioni, Sam Clark, et al. 2012.
Open domain event extraction from twitter. In Pro-
ceedings of the 18th ACM SIGKDD international
conference on Knowledge discovery and data min-
ing, pages 1104?1112. ACM.
Kirk Roberts and Sanda M Harabagiu. 2011. Unsuper-
vised learning of selectional restrictions and detec-
tion of argument coercions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 980?990. Association for
Computational Linguistics.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes twitter users: real-time
event detection by social sensors. In Proceedings
of the 19th international conference on World wide
web, pages 851?860. ACM.
Roser Saur?? and James Pustejovsky. 2007. Deter-
mining modality and factuality for text entailment.
In Semantic Computing, 2007. ICSC 2007. Interna-
tional Conference on, pages 509?516. IEEE.
Roser Saur?? and James Pustejovsky. 2009. Factbank:
A corpus annotated with event factuality. Language
resources and evaluation, 43(3):227?268.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y Ng. 2008. Cheap and fast?but is it
good?: evaluating non-expert annotations for natu-
ral language tasks. In Proceedings of the conference
on empirical methods in natural language process-
ing, pages 254?263. Association for Computational
Linguistics.
Limin Yao, David Mimno, and Andrew McCallum.
2009. Efficient methods for topic model inference
on streaming document collections.
Limin Yao, Aria Haghighi, Sebastian Riedel, and An-
drew McCallum. 2011. Structured relation discov-
ery using generative models. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1456?1466. Association
for Computational Linguistics.
2007
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039?2048,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
A Model of Coherence Based on Distributed Sentence Representation
Jiwei Li
1
and Eduard Hovy
3
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
3
Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
jiweil@stanford.edu ehovy@andrew.cmu.edu
Abstract
Coherence is what makes a multi-sentence
text meaningful, both logically and syn-
tactically. To solve the challenge of or-
dering a set of sentences into coherent or-
der, existing approaches focus mostly on
defining and using sophisticated features
to capture the cross-sentence argumenta-
tion logic and syntactic relationships. But
both argumentation semantics and cross-
sentence syntax (such as coreference and
tense rules) are very hard to formalize. In
this paper, we introduce a neural network
model for the coherence task based on
distributed sentence representation. The
proposed approach learns a syntactico-
semantic representation for sentences au-
tomatically, using either recurrent or re-
cursive neural networks. The architecture
obviated the need for feature engineering,
and learns sentence representations, which
are to some extent able to capture the
?rules? governing coherent sentence struc-
ture. The proposed approach outperforms
existing baselines and generates the state-
of-art performance in standard coherence
evaluation tasks
1
.
1 Introduction
Coherence is a central aspect in natural language
processing of multi-sentence texts. It is essen-
tial in generating readable text that the text plan-
ner compute which ordering of clauses (or sen-
tences; we use them interchangeably in this paper)
is likely to support understanding and avoid con-
fusion. As Mann and Thompson (1988) define it,
A text is coherent when it can be ex-
plained what role each clause plays with
regard to the whole.
1
Code available at stanford.edu/
?
jiweil/ or by
request from the first author.
Several researchers in the 1980s and 1990s ad-
dressed the problem, the most influential of
which include: Rhetorical Structure Theory (RST;
(Mann and Thompson, 1988)), which defined
about 25 relations that govern clause interde-
pendencies and ordering and give rise to text
tree structures; the stepwise assembly of seman-
tic graphs to support adductive inference toward
the best explanation (Hobbs et al., 1988); Dis-
course Representation Theory (DRT; (Lascarides
and Asher, 1991)), a formal semantic model of
discourse contexts that constrain coreference and
quantification scoping; the model of intention-
oriented conversation blocks and their stack-based
queueing to model attention flow (Grosz and Sid-
ner, 1986), and more recently an inventory of a
hundred or so binary inter-clause relations and as-
sociated annotated corpus (Penn Discourse Tree-
bank. Work in text planning implemented some
of these models, especially operationalized RST
(Hovy, 1988) and explanation relations (Moore
and Paris, 1989) to govern the planning of coher-
ent paragraphs. Other computational work defined
so called schemas (McKeown, 1985), frames with
fixed sequences of clause types to achieve stereo-
typical communicative intentions.
Little of this work survives. Modern research
tries simply to order a collection of clauses or sen-
tences without giving an account of which order(s)
is/are coherent or what the overall text structure
is. The research focuses on identifying and defin-
ing a set of increasingly sophisticated features by
which algorithms can be trained to propose order-
ings. Features being explored include the clause
entities, organized into a grid (Lapata and Barzi-
lay, 2005; Barzilay and Lapata, 2008), coreference
clues to ordering (Elsner and Charniak, 2008),
named-entity categories (Eisner and Charniak,
2011), syntactic features (Louis and Nenkova,
2012), and others. Besides being time-intensive
(feature engineering usually requites considerable
2039
Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples.
effort and can depend greatly on upstream feature
extraction algorithms), it is not immediately ap-
parent which aspects of a clause or a coherent text
to consider when deciding on ordering. More im-
portantly, the features developed to date are still
incapable of fully specifying the acceptable order-
ing(s) within a context, let alone describe why they
are coherent.
Recently, deep architectures, have been applied
to various natural language processing tasks (see
Section 2). Such deep connectionist architectures
learn a dense, low-dimensional representation of
their problem in a hierarchical way that is capa-
ble of capturing both semantic and syntactic as-
pects of tokens (e.g., (Bengio et al., 2006)), en-
tities, N-grams (Wang and Manning, 2012), or
phrases (Socher et al., 2013). More recent re-
searches have begun looking at higher level dis-
tributed representations that transcend the token
level, such as sentence-level (Le and Mikolov,
2014) or even discourse-level (Kalchbrenner and
Blunsom, 2013) aspects. Just as words combine
to form meaningful sentences, can we take advan-
tage of distributional semantic representations to
explore the composition of sentences to form co-
herent meanings in paragraphs?
In this paper, we demonstrate that it is feasi-
ble to discover the coherent structure of a text
using distributed sentence representations learned
in a deep learning framework. Specifically, we
consider a WINDOW approach for sentences, as
shown in Figure 1, where positive examples are
windows of sentences selected from original arti-
cles generated by humans, and negatives examples
are generated by random replacements
2
. The se-
mantic representations for terms and sentences are
obtained through optimizing the neural network
framework based on these positive vs negative ex-
2
Our approach is inspired by Collobert et al.?s idea (2011)
that a word and its context form a positive training sample
while a random word in that same context gives a negative
training sample, when training word embeddings in the deep
learning framework.
amples and the proposed model produces state-of-
art performance in multiple standard evaluations
for coherence models (Barzilay and Lee, 2004).
The rest of this paper is organized as follows:
We describe related work in Section 2, then de-
scribe how to obtain a distributed representation
for sentences in Section 3, and the window compo-
sition in Section 4. Experimental results are shown
in Section 5, followed by a conclusion.
2 Related Work
Coherence In addition to the early computa-
tional work discussed above, local coherence was
extensively studied within the modeling frame-
work of Centering Theory (Grosz et al., 1995;
Walker et al., 1998; Strube and Hahn, 1999; Poe-
sio et al., 2004), which provides principles to form
a coherence metric (Miltsakaki and Kukich, 2000;
Hasler, 2004). Centering approaches suffer from a
severe dependence on manually annotated input.
A recent popular approach is the entity grid
model introduced by Barzilay and Lapata (2008)
, in which sentences are represented by a vec-
tor of discourse entities along with their gram-
matical roles (e.g., subject or object). Proba-
bilities of transitions between adjacent sentences
are derived from entity features and then concate-
nated to a document vector representation, which
is used as input to machine learning classifiers
such as SVM. Many frameworks have extended
the entity approach, for example, by pre-grouping
entities based on semantic relatedness (Filippova
and Strube, 2007) or adding more useful types
of features such as coreference (Elsner and Char-
niak, 2008), named entities (Eisner and Charniak,
2011), and discourse relations (Lin et al., 2011).
Other systems include the global graph model
(Guinaudeau and Strube, 2013) which projects en-
tities into a global graph. Louis and Nenkova
(2012) introduced an HMM system in which the
coherence between adjacent sentences is modeled
by a hidden Markov framework captured by the
2040
Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The
bottom layer represents word vectors in the sentence. The top layer h
s
denotes the resulting sentence
vector.
transition rules of different topics.
Recurrent and Recursive Neural Networks In
the context of NLP, recurrent neural networks
view a sentence as a sequence of tokens and in-
corporate information from the past (i.e., preced-
ing tokens) (Schuster and Paliwal, 1997; Sutskever
et al., 2011) for acquisition of the current output.
At each step, the recurrent network takes as input
both the output of previous steps and the current
token, convolutes the inputs, and forwards the re-
sult to the next step. It has been successfully ap-
plied to tasks such as language modeling (Mikolov
et al., 2010) and spoken language understanding
(Mesnil et al., 2013). The advantage of recur-
rent network is that it does not depend on exter-
nal deeper structure (e.g., parse tree) and is easy to
implement. However, in the recurrent framework,
long-distance dependencies are difficult to capture
due to the vanishing gradient problem (Bengio et
al., 1994); two tokens may be structurally close to
each other, even though they are far away in word
sequence
3
.
Recursive neural networks comprise another
class of architecture, one that relies and operates
on structured inputs (e.g., parse trees). It com-
putes the representation for each parent based on
its children iteratively in a bottom-up fashion. A
series of variations have been proposed, each tai-
lored to different task-specific requirements, such
as Matrix-Vector RNN (Socher et al., 2012) that
represents every word as both a vector and a ma-
trix, or Recursive Neural Tensor Networks (Socher
et al., 2013) that allow the model to have greater
3
For example, a verb and its corresponding direct object
can be far away in terms of tokens if many adjectives lies in
between, but they are adjacent in the parse tree (Irsoy and
Cardie, 2013).
interactions between the input vectors. Many tasks
have benefited from this recursive framework, in-
cluding parsing (Socher et al., 2011b), sentiment
analysis (Socher et al., 2013), and paraphrase de-
tection (Socher et al., 2011a).
2.1 Distributed Representations
Both recurrent and recursive networks require a
vector representation of each input token. Dis-
tributed representations for words were first pro-
posed in (Rumelhart et al., 1988) and have been
successful for statistical language modeling (El-
man, 1990). Various deep learning architectures
have been explored to learn these embeddings in
an unsupervised manner from a large corpus (Ben-
gio et al., 2006; Collobert and Weston, 2008;
Mnih and Hinton, 2007; Mikolov et al., 2013),
which might have different generalization capabil-
ities and are able to capture the semantic mean-
ings depending on the specific task at hand. These
vector representations can to some extent cap-
ture interesting semantic relationships, such as
King?man ? Queue?woman (Mikolov et al.,
2010), and recently have been successfully used
in various NLP applications, including named en-
tity recognition, tagging, segmentation (Wang et
al., 2013), and machine translation (e.g.,(Collobert
and Weston, 2008; Zou et al., 2013)).
3 Sentence Model
In this section, we demonstrate the strategy
adopted to compute a vector for a sentence given
the sequence of its words and their embeddings.
We implemented two approaches, Recurrent and
Recursive neural networks, following the de-
scriptions in for example (Mikolov et al., 2010;
Sutskever et al., 2011; Socher et al., 2013). As
2041
the details of both approaches can be readily found
there, we make this section brief and omit the de-
tails for brevity.
Let s denote a sentence, comprised of a se-
quence of words s = {w
1
, w
2
, ..., w
n
s
}, where n
s
denotes the number of words within sentence s.
Each word w is associated with a specific vector
embedding e
w
= {e
1
w
, e
2
w
, ..., e
K
w
}, where K de-
notes the dimension of the word embedding. We
wish to compute the vector representation for cur-
rent sentence h
s
= {h
1
s
, h
2
s
, ..., h
K
s
}.
Recurrent Sentence Representation (Recur-
rent) The recurrent network captures certain
general considerations regarding sentential com-
positionality. As shown in Figure 2 (a), for sen-
tence s, recurrent network successively takes word
w
i
at step i, combines its vector representation e
t
w
with former input h
i?1
from step i? 1, calculates
the resulting current embedding h
t
, and passes it
to the next step. The standard recurrent network
calculates h
t
as follows:
h
t
= f(V
Recurrent
?h
t?1
+W
Recurrent
?e
t
w
+b
Recurrent
)
(1)
where W
Recurrent
and V
Recurrent
are K ?K ma-
trixes. b
Recurrent
denotes K ? 1 bias vector and
f = tanh is a standard element-wise nonlinearity.
Note that calculation for representation at time
t = 1 is given by:
h
1
= f(V
Recurrent
?h
0
+W
Recurrent
?e
1
w
+b
Recurrent
)
(2)
where h
0
denotes the global sentence starting vec-
tor.
Recursive Sentence Representation (Recursive)
Recursive sentence representation relies on the
structure of parse trees, where each leaf node of
the tree corresponds to a word from the original
sentence. It computes a representation for each
parent node based on its immediate children re-
cursively in a bottom-up fashion until reaching the
root of the tree. Concretely, for a given parent p
in the tree and its two children c
1
(associated with
vector representation h
c
1
) and c
2
(associated with
vector representation h
c
2
), standard recursive net-
works calculates h
p
for p as follows:
h
p
= f(W
Recursive
? [h
c
1
, h
c
2
] + b
Recursive
) (3)
where [h
c
1
, h
c
2
] denotes the concatenating vec-
tor for children vector representation h
c
1
and h
c
2
.
W
Recursive
is a K ? 2K matrix and b
Recursive
is
the 1?K bias vector. f(?) is tanh function.
Recursive neural models compute parent vec-
tors iteratively until the root node?s representation
is obtained, and use the root embedding to repre-
sent the whole sentence, as shown in Figure 2 (b).
4 Coherence Model
The proposed coherence model adopts a window
approach (Collobert et al., 2011), in which we
train a three-layer neural network based on a slid-
ing windows of L sentences.
4.1 Sentence Convolution
We treat a window of sentences as a clique C and
associate each clique with a tag y
C
that takes the
value 1 if coherent, and 0 otherwise
4
. As shown in
Figure 1, cliques taken from original articles are
treated as coherent and those with sentences ran-
domly replaced are used as negative examples. .
The sentence convolution algorithm adopted in
this paper is defined by a three-layer neural net-
work, i.e., sentence-level input layer, hidden layer,
and overall output layer as shown in Figure 3. For-
mally, each clique C takes as input a (L?K)? 1
vector h
C
by concatenating the embeddings of
all its contained sentences, denoted as h
C
=
[h
s
1
, h
s
2
, ..., h
s
L
]. (Note that if we wish to clas-
sify the first and last sentences and include their
context, we require special beginning and ending
sentence vectors, which are defined as h
<S>
for
s
start
and h
</S>
for s
end
respectively.)
Let H denote the number of neurons in the hid-
den (second) layer. Then each of the hidden lay-
ers takes as input h
C
and performs the convolution
using a non-linear tanh function, parametrized by
W
sen
and b
sen
. The concatenating output vector
for hidden layers, defined as q
C
, can therefore be
rewritten as:
q
C
= f(W
sen
? h
C
+ b
sen
) (4)
where W
sen
is a H? (L?K) dimensional matrix
and b
sen
is a H ? 1 dimensional bias vector.
4
instead of a binary classification (correct/incorrect), an-
other commonly used approach is the contrastive approach
that minimizes the score function max(0, 1 ? s + s
c
) (Col-
lobert et al., 2011; Smith and Eisner, 2005). s denotes the
score of a true (coherent) window and s
c
the score of a cor-
rupt (containing incoherence) one) in an attempt to make the
score of true windows larger and corrupt windows smaller.
We tried the contrastive one for both recurrent and recursive
networks but the binary approach constantly outperformed
the contrastive one in this task.
2042
Figure 3: An example of coherence model based on a window of sentences (clique).
The output layer takes as input q
C
and generates
a scalar using linear function U
T
q
C
+b. A sigmod
function is then adopted to project the value to a
[0,1] probability space, which can be interpreted
as the probability of whether one clique is coher-
ent or not. The execution at the output layer can
be summarized as:
p(y
C
= 1) = sigmod(U
T
q
C
+ b) (5)
where U is anH?1 vector and b denotes the bias.
4.2 Training
In the proposed framework, suppose we have M
training samples, the cost function for recurrent
neural network with regularization on the training
set is given by:
J(?) =
1
M
?
C?trainset
{?y
C
log[p(y
C
= 1)]
? (1? y
C
) log[1? p(y
C
= 1)]}+
Q
2M
?
???
?
2
(6)
where
? = [W
Recurrent
,W
sen
, U
sen
]
The regularization part is paralyzed by Q to avoid
overfitting. A similar loss function is applied to
the recursive network with only minor parameter
altering that is excluded for brevity.
To minimize the objective J(?), we use the di-
agonal variant of AdaGrad (Duchi et al., 2011)
with minibatches, which is widely applied in deep
learning literature (e.g.,(Socher et al., 2011a; Pei
et al., 2014)). The learning rate in AdaGrad is
adapting differently for different parameters at dif-
ferent steps. Concretely, for parameter updates, let
g
i
?
denote the subgradient at time step for param-
eter ?
i
, which is obtained from backpropagation
5
,
the parameter update at time step t is given by:
?
?
= ?
??1
?
?
?
?
t=0
?
g
i2
?
g
i
?
(7)
where ? denotes the learning rate and is set to 0.01
in our approach. Optimal performance is achieved
when batch size is set between 20 and 30.
4.3 Initialization
Elements in W
sen
are initialized by randomly
drawing from the uniform distribution [?, ],
where  =
?
6
?
H+K?L
as suggested in (Collobert
et al., 2011). W
recurrent
, V
recurrent
, W
recursive
and h
0
are initialized by randomly sampling from
a uniform distribution U(?0.2, 0.2). All bias vec-
tors are initialized with 0. Hidden layer numberH
is set to 100.
Word embeddings {e} are borrowed from
Senna (Collobert et al., 2011; Collobert, 2011).
The dimension for these embeddings is 50.
5 Experiments
We evaluate the proposed coherence model on two
common evaluation approaches adopted in exist-
ing work (Barzilay and Lapata, 2008; Louis and
Nenkova, 2012; Elsner et al., 2007; Lin et al.,
2011): Sentence Ordering and Readability Assess-
ment.
5.1 Sentence Ordering
We follow (Barzilay and Lapata, 2008; Louis and
Nenkova, 2012; Elsner et al., 2007; Lin et al.,
5
For more details on backpropagation through RNNs, see
Socher et al. (2010).
2043
2011) that all use pairs of articles, one containing
the original document order and the other a ran-
dom permutation of the sentences from the same
document. The pairwise approach is predicated
on the assumption that the original article is al-
ways more coherent than a random permutation;
this assumption has been verified in Lin et al.?s
work (2011).
We need to define the coherence score S
d
for
a given document d, where d is comprised of a
series of sentences, d = {s
1
, s
2
, .., s
N
d
}, and N
d
denotes the number of sentences within d. Based
on our clique definition, document d is comprised
of N
d
cliques. Taking window size L = 3 as ex-
ample, cliques generated from document d appear
as follows:
< s
start
, s
1
, s
2
>,< s
1
, s
2
, s
3
>, ...,
< s
N
d
?2
, s
N
d
?1
, s
N
d
>,< s
N
d
?1
, s
N
d
, s
end
>
The coherence score for a given document S
d
is
the probability that all cliques within d are coher-
ent, which is given by:
S
d
=
?
C?d
p(y
C
= 1) (8)
For document pair < d
1
, d
2
> in our task, we
would say document d
1
is more coherent than d
2
if
S
d
1
> S
d
2
(9)
5.1.1 Dataset
We use two corpora that are widely employed
for coherence prediction (Barzilay and Lee, 2004;
Barzilay and Lapata, 2008; Elsner et al., 2007).
One contains reports on airplane accidents from
the National Transportation Safety Board and the
other contains reports about earthquakes from the
Associated Press. These articles are about 10
sentences long and usually exhibit clear sentence
structure. For preprocessing, we only lowercase
the capital letters to match with tokens in Senna
word embeddings. In the recursive network, sen-
tences are parsed using the Stanford Parser
6
and
then transformed into binary trees. The accident
corpus ends up with a vocabulary size of 4758 and
an average of 10.6 sentences per document. The
earthquake corpus contains 3287 distinct terms
and an average of 11.5 sentences per document.
6
http://nlp.stanford.edu/software/
lex-parser.shtml
For each of the two corpora, we have 100 arti-
cles for training and 100 (accidents) and 99 (earth-
quakes) for testing. A maximum of 20 random
permutations were generated for each test arti-
cle to create the pairwise data (total of 1986 test
pairs for the accident corpus and 1956 for earth-
quakes)
7
.
Positive cliques are taken from original training
documents. For easy training, rather than creating
negative examples by replacing centered sentences
randomly, the negative dataset contains cliques
where centered sentences are replaced only by
other sentences within the same document.
5.1.2 Training and Testing
Despite the numerous parameters in the deep
learning framework, we tune only two principal
ones for each setting: window size L (tried on
{3, 5, 7}) and regularization parameterQ (tried on
{0.01, 0.1, 0.25, 0.5, 1.0, 1.25, 2.0, 2.5, 5.0}). We
trained parameters using 10-fold cross-validation
on the training data. Concretely, in each setting,
90 documents were used for training and evalua-
tion was done on the remaining articles, following
(Louis and Nenkova, 2012). After tuning, the final
model was tested on the testing set.
5.1.3 Model Comparison
We report performance of recursive and recurrent
networks. We also report results from some popu-
lar approaches in the literature, including:
Entity Grid Model : Grid model (Barzilay and
Lapata, 2008) obtains the best performance when
coreference resolution, expressive syntactic infor-
mation, and salience-based features are incorpo-
rated. Entity grid models represent each sentence
as a column of a grid of features and apply ma-
chine learning methods (e.g., SVM) to identify the
coherent transitions based on entity features (for
details of entity models see (Barzilay and Lapata,
2008)). Results are directly taken from Barzilay
and Lapata?s paper (2008).
HMM : Hidden-Markov approach proposed by
Louis and Nenkova (2012) to model the state
(cluster) transition probability in the coherent con-
text using syntactic features. Sentences need to be
clustered in advance where the number of clus-
ters is tuned as a parameter. We directly take
7
Permutations are downloaded from http:
//people.csail.mit.edu/regina/coherence/
CLsubmission/.
2044
Acci Earthquake Average
Recursive 0.864 0.976 0.920
Recurrent 0.840 0.951 0.895
Entity Grid 0.904 0.872 0.888
HMM 0.822 0.938 0.880
HMM+Entity 0.842 0.911 0.877
HMM+Content 0.742 0.953 0.848
Graph 0.846 0.635 0.740
Table 1: Comparison of Different Coherence
Frameworks. Reported baseline results are among
the best performance regarding each approach is
reprinted from prior work from (Barzilay and Lap-
ata, 2008; Louis and Nenkova, 2012; Guinaudeau
and Strube, 2013).
the results from Louis and Nenkova?s paper and
report the best results among different combi-
nations of parameter and feature settings
8
. We
also report performances of models from Louis
and Nenkova?s work that combine HMM and en-
tity/content models in a unified framework.
Graph Based Approach : Guinaudeau and
Strube (2013) extended the entity grid model to
a bipartite graph representing the text, where the
entity transition information needed for local co-
herence computation is embedded in the bipartite
graph. The Graph Based Approach outperforms
the original entity approach in some of feature set-
tings (Guinaudeau and Strube, 2013).
As can be seen in Table 1, the proposed frame-
works (both recurrent and recursive) obtain state-
of-art performance and outperform all existing
baselines by a large margin. One interpretation
is that the abstract sentence vector representations
computed by the deep learning framework is more
powerful in capturing exactly the relevant the se-
mantic/logical/syntactic features in coherent con-
texts than features or other representations devel-
oped by human feature engineering are.
Another good quality of the deep learning
framework is that it can be trained easily and
makes unnecessary the effort required of feature
engineering. In contrast, almost all existing base-
lines and other coherence methods require sophis-
ticated feature selection processes and greatly rely
on external feature extraction algorithm.
The recurrent network is easier to implement
than the recursive network and does not rely on
external resources (i.e., parse trees), but the recur-
sive network obtains better performance by build-
8
The details for information about parameter and feature
of best setting can be found in (Louis and Nenkova, 2012).
ing the convolution on parse trees rather than sim-
ply piling up terms within the sentence, which is
in line with common expectation.
Both recurrent and recursive models obtain bet-
ter performance on the Earthquake than the Acci-
dent dataset. Scrutiny of the corpus reveals that
articles reporting earthquakes exhibit a more con-
sistent structure: earthquake outbreak, describing
the center and intensity of the earthquake, injuries
and rescue operations, etc., while accident articles
usually exhibit more diverse scenarios.
5.2 Readability Assessment
Barzilay and Lapata (2008) proposed a readability
assessment task for stylistic judgments about the
difficulty of reading a document. Their approach
combines a coherence system with Schwarm and
Ostendorf?s (2005) readability features to clas-
sify documents into two categories, more read-
able (coherent) documents and less readable ones.
The evaluation accesses the ability to differentiate
?easy to read? documents from difficult ones of
each model.
5.2.1 Dataset
Barzilay and Lapata?s (2008) data corpus is
from the Encyclopedia Britannica and the
Britannica Elementary, the latter being a new
version targeted at children. Both versions con-
tain 107 articles. The Encyclopedia Britannica
corpus contains an average of 83.1 sentences
per document and the Britannica Elementary
contains 36.6. The encyclopedia lemmas are
written by different authors and consequently
vary considerably in structure and vocabulary
choice. Early researchers assumed that the chil-
dren version (Britannica Elementary) is easier
to read, hence more coherent than documents in
Encyclopedia Britannica. This is a somewhat
questionable assumption that needs further inves-
tigation.
5.2.2 Training and Testing
Existing coherence approaches again apply a pair-
wise ranking strategy and the article associated
with the higher score is considered to be the more
readable. As the replacement strategy for gener-
ating negative example is apparently not well fit-
ted to this task, we adopted the following training
framework: we use all sliding windows of sen-
tences from coherent documents (documents from
Britannica Elementary) as positive examples,
2045
Approach Accuracy
Recurrent 0.803
Recursive 0.828
Graph Approach 0.786
Entity 0.509
S&O 0.786
Entity+S&O 0.888
Table 2: Comparison of Different Coherence
Frameworks on Readability Assessment. Re-
ported baselines results are are taken from (Barzi-
lay and Lapata, 2008; Guinaudeau and Strube,
2013). S&O: Schwarm and Ostendorf (2005).
and cliques from Encyclopedia Britannica as
negative examples, and again apply Eq. 6 for train-
ing and optimization. During testing, we turn to
Equations 8 and 9 for pairwise comparison. We
adopted five-fold cross-validation in the same way
as in (Barzilay and Lapata, 2008; Guinaudeau and
Strube, 2013) for fair comparison. Parameters
were tuned within each training set also using five-
fold cross-validation. Parameters to tune included
window size L and regularization parameter Q.
5.3 Results
We report results of the proposed approaches in
the work along with entity model (Barzilay and
Lapata, 2008) and graph based approach (Elsner
and Charniak, 2008) in Table 2. The tabs shows
that deep learning approaches again significantly
outperform Entry and Global Approach baselines
and are nearly comparable to the combination of
entity and S&O features. Again, the recursive
network outperforms the recurrent network in this
task.
6 Conclusion
In this paper, we apply two neural network
approaches to the sentence-ordering (coherence)
task, using compositional sentence representations
learned by recurrent and recursive composition.
The proposed approach obtains state-of-art per-
formance on the standard coherence evaluation
tasks.
Acknowledgements
The authors want to thank Richard Socher and
Pradeep Dasigi for the clarification of deep learn-
ing techniques. We also thank the three anony-
mous EMNLP reviewers for helpful comments.
References
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In HLT-NAACL,
pages 113?120.
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
1994. Learning long-term dependencies with gra-
dient descent is difficult. Neural Networks, IEEE
Transactions on, 5(2):157?166.
Yoshua Bengio, Holger Schwenk, Jean-S?ebastien
Sen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In
Innovations in Machine Learning, pages 137?186.
Springer.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160?167. ACM.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
Micha Eisner and Eugene Charniak. 2011. Extending
the entity grid with entity-specific features. In Pro-
ceedings of the 49th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies: short papers-Volume 2, pages
125?129. Association for Computational Linguis-
tics.
Jeffrey L Elman. 1990. Finding structure in time.
Cognitive science, 14(2):179?211.
Micha Elsner and Eugene Charniak. 2008.
Coreference-inspired coherence modeling. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics on Hu-
man Language Technologies: Short Papers, pages
41?44. Association for Computational Linguistics.
Micha Elsner, Joseph L Austerweil, and Eugene Char-
niak. 2007. A unified local and global model for
discourse coherence. In HLT-NAACL, pages 436?
443.
2046
Katja Filippova and Michael Strube. 2007. Extend-
ing the entity-grid coherence model to semantically
related entities. In Proceedings of the Eleventh Eu-
ropean Workshop on Natural Language Generation,
pages 139?142. Association for Computational Lin-
guistics.
Barbara J Grosz and Candace L Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational linguistics, 12(3):175?204.
Barbara J Grosz, Scott Weinstein, and Aravind K Joshi.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational linguis-
tics, 21(2):203?225.
Camille Guinaudeau and Michael Strube. 2013.
Graph-based local coherence modeling. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics, pages 93?103.
Laura Hasler. 2004. An investigation into the use of
centering transitions for summarisation. In Proceed-
ings of the 7th Annual CLUK Research Colloquium,
pages 100?107.
Jerry R Hobbs, Mark Stickel, Paul Martin, and Dou-
glas Edwards. 1988. Interpretation as abduction. In
Proceedings of the 26th annual meeting on Associ-
ation for Computational Linguistics, pages 95?103.
Association for Computational Linguistics.
Eduard H Hovy. 1988. Planning coherent multisenten-
tial text. In Proceedings of the 26th annual meet-
ing on Association for Computational Linguistics,
pages 163?169. Association for Computational Lin-
guistics.
Ozan Irsoy and Claire Cardie. 2013. Bidirectional re-
cursive neural networks for token-level labeling with
structure. arXiv preprint arXiv:1312.0493.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
convolutional neural networks for discourse compo-
sitionality. arXiv preprint arXiv:1306.3584.
Mirella Lapata and Regina Barzilay. 2005. Automatic
evaluation of text coherence: Models and represen-
tations. In IJCAI, volume 5, pages 1085?1090.
Alex Lascarides and Nicholas Asher. 1991. Discourse
relations and defeasible knowledge. In Proceedings
of the 29th annual meeting on Association for Com-
putational Linguistics, pages 55?62. Association for
Computational Linguistics.
Quoc Le and Tomas Mikolov. 2014. Distributed repre-
sentations of sentences and documents.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Automatically evaluating text coherence using dis-
course relations. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 997?1006. Association for Computational
Linguistics.
Annie Louis and Ani Nenkova. 2012. A coherence
model based on syntactic patterns. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 1157?1168. As-
sociation for Computational Linguistics.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Kathleen R McKeown. 1985. Discourse strategies for
generating natural-language text. Artificial Intelli-
gence, 27(1):1?41.
Gr?egoire Mesnil, Xiaodong He, Li Deng, and Yoshua
Bengio. 2013. Investigation of recurrent-neural-
network architectures and learning methods for spo-
ken language understanding. Interspeech.
Tomas Mikolov, Martin Karafi?at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045?1048.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Eleni Miltsakaki and Karen Kukich. 2000. The role
of centering theory?s rough-shift in the teaching and
evaluation of writing skills. In Proceedings of the
38th Annual Meeting on Association for Computa-
tional Linguistics, pages 408?415. Association for
Computational Linguistics.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling.
In Proceedings of the 24th international conference
on Machine learning, pages 641?648. ACM.
Johanna D Moore and Cecile L Paris. 1989. Planning
text for advisory dialogues. In Proceedings of the
27th annual meeting on Association for Computa-
tional Linguistics, pages 203?211. Association for
Computational Linguistics.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.
Massimo Poesio, Rosemary Stevenson, Barbara Di Eu-
genio, and Janet Hitzeman. 2004. Centering: A
parametric theory and its instantiations. Computa-
tional linguistics, 30(3):309?363.
David E Rumelhart, Geoffrey E Hinton, and Ronald J
Williams. 1988. Learning representations by back-
propagating errors. MIT Press, Cambridge, MA,
USA.
Mike Schuster and Kuldip K Paliwal. 1997. Bidirec-
tional recurrent neural networks. Signal Processing,
IEEE Transactions on, 45(11):2673?2681.
2047
Sarah E Schwarm and Mari Ostendorf. 2005. Read-
ing level assessment using support vector machines
and statistical language models. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 523?530. Association
for Computational Linguistics.
Noah A Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the 43rd Annual Meeting
on Association for Computational Linguistics, pages
354?362. Association for Computational Linguis-
tics.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1?9.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801?809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129?136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631?1642.
Michael Strube and Udo Hahn. 1999. Functional
centering: Grounding referential coherence in in-
formation structure. Computational linguistics,
25(3):309?344.
Ilya Sutskever, James Martens, and Geoffrey E Hin-
ton. 2011. Generating text with recurrent neural
networks. In Proceedings of the 28th International
Conference on Machine Learning (ICML-11), pages
1017?1024.
Marilyn A Walker, Aravind Krishna Joshi, and
Ellen Friedman Prince. 1998. Centering theory in
discourse. Oxford University Press.
Sida Wang and Christopher D Manning. 2012. Base-
lines and bigrams: Simple, good sentiment and topic
classification. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Short Papers-Volume 2, pages 90?94. As-
sociation for Computational Linguistics.
Houfeng Wang, Longkai Zhang, Li Li, He Zhengyan,
and Ni Sun. 2013. Improving chinese word seg-
mentation on micro-blog using rich punctuations.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embed-
dings for phrase-based machine translation. In Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP 2013).
2048
Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2061?2069,
October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational Linguistics
Recursive Deep Models for Discourse Parsing
Jiwei Li
1
, Rumeng Li
2
and Eduard Hovy
3
1
Computer Science Department, Stanford University, Stanford, CA 94305, USA
2
School of EECS, Peking University, Beijing 100871, P.R. China
3
Language Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA
jiweil@stanford.edu alicerumeng@foxmail.com ehovy@andrew.cmu.edu
Abstract
Text-level discourse parsing remains a
challenge: most approaches employ fea-
tures that fail to capture the intentional, se-
mantic, and syntactic aspects that govern
discourse coherence. In this paper, we pro-
pose a recursive model for discourse pars-
ing that jointly models distributed repre-
sentations for clauses, sentences, and en-
tire discourses. The learned representa-
tions can to some extent learn the seman-
tic and intentional import of words and
larger discourse units automatically,. The
proposed framework obtains comparable
performance regarding standard discours-
ing parsing evaluations when compared
against current state-of-art systems.
1 Introduction
In a coherent text, units (clauses, sentences, and
larger multi-clause groupings) are tightly con-
nected semantically, syntactically, and logically.
Mann and Thompson (1988) define a text to be
coherent when it is possible to describe clearly
the role that each discourse unit (at any level of
grouping) plays with respect to the whole. In a
coherent text, no unit is completely isolated. Dis-
course parsing tries to identify how the units are
connected with each other and thereby uncover the
hierarchical structure of the text, from which mul-
tiple NLP tasks can benefit, including text sum-
marization (Louis et al., 2010), sentence compres-
sion (Sporleder and Lapata, 2005) or question-
answering (Verberne et al., 2007).
Despite recent progress in automatic discourse
segmentation and sentence-level parsing (e.g.,
(Fisher and Roark, 2007; Joty et al., 2012; Sori-
cut and Marcu, 2003), document-level discourse
parsing remains a significant challenge. Recent
attempts (e.g., (Hernault et al., 2010b; Feng and
Hirst, 2012; Joty et al., 2013)) are still consid-
erably inferior when compared to human gold-
standard discourse analysis. The challenge stems
from the fact that compared with sentence-level
dependency parsing, the set of relations between
discourse units is less straightforward to define.
Because there are no clause-level ?parts of dis-
course? analogous to word-level parts of speech,
there is no discourse-level grammar analogous to
sentence-level grammar. To understand how dis-
course units are connected, one has to understand
the communicative function of each unit, and the
role it plays within the context that encapsulates it,
taken recursively all the way up for the entire text.
Manually developed features relating to words and
other syntax-related cues, used in most of the re-
cent prevailing approaches (e.g., (Feng and Hirst,
2012; Hernault et al., 2010b)), are insufficient for
capturing such nested intentionality.
Recently, deep learning architectures have been
applied to various natural language processing
tasks (for details see Section 2) and have shown
the advantages to capture the relevant semantic
and syntactic aspects of units in context. As word
distributions are composed to form the meanings
of clauses, the goal is to extend distributed clause-
level representations to the single- and multi-
sentence (discourse) levels, and produce the hier-
archical structure of entire texts.
Inspired by this idea, we introduce in this pa-
per a deep learning approach for discourse pars-
ing. The proposed parsing algorithm relies on
a recursive neural network to decide (1) whether
two discourse units are connected and if so (2)
by what relation they are connected. Concretely,
the parsing algorithm takes as input a document of
any length, and first obtains the distributed repre-
sentation for each of its sentences using recursive
convolution based on the sentence parse tree. It
then proceeds bottom-up, applying a binary clas-
sifier to determine the probability of two adjacent
2061
discourse units being merged to form a new sub-
tree followed by a multi-class classifier to select
the appropriate discourse relation label, and cal-
culates the distributed representation for the sub-
tree so formed, gradually unifying subtrees un-
til a single overall tree spans the entire sentence.
The compositional distributed representation en-
ables the parser to make accurate parsing decisions
and capture relations between different sentences
and units. The binary and multi-class classifiers,
along with parameters involved in convolution, are
jointly trained from a collection of gold-standard
discourse structures.
The rest of this paper is organized as follows.
We present related work in Section 2 and de-
scribe the RST Discourse Treebank in Section 3.
The sentence convolution approach is illustrated in
Section 4 and the discourse parser model in Sec-
tion 5. We report experimental results in Section 6
and conclude in Section 7.
2 Related Work
2.1 Discourse Analysis and Parsing
The basis of discourse structure lies in the recog-
nition that discourse units (minimally, clauses) are
related to one another in principled ways, and that
the juxtaposition of two units creates a joint mean-
ing larger than either unit?s meaning alone. In a
coherent text this juxtaposition is never random,
but serves the speaker?s communicative goals.
Considerable work on linguistic and computa-
tional discourse processing in the 1970s and 80s
led to the development of several proposals for re-
lations that combine units; for a compilation see
(Hovy and Maier, 1997). Of these the most influ-
ential is Rhetorical Structure Theory RST (Mann
and Thompson, 1988) that defines about 25 rela-
tions, each containing semantic constraints on its
component parts plus a description of the overall
functional/semantic effect produced as a unit when
the parts have been appropriately connected in the
text. For example, the SOLUTIONHOOD relation
connects one unit describing a problem situation
with another describing its solution, using phrases
such as ?the answer is?; in successful communi-
cation the reader will understand that a problem is
described and its solution is given.
Since there is no syntactic definition of a prob-
lem or solution (they can each be stated in a sin-
gle clause, a paragraph, or an entire text), one has
to characterize discourse units by their commu-
nicative (rhetorical) function. The functions are
reflected in text as signals of the author?s inten-
tions, and take various forms (including expres-
sions such as ?therefore?, ?for example?, ?the an-
swer is?, and so on; patterns of tense or pronoun
usage; syntactic forms; etc.). The signals govern
discourse blocks ranging from a clause to an en-
tire text , each one associated with some discourse
relation.
In order to build a text?s hierarchical structure,
a discourse parser needs to recognize these signals
and use them to appropriately compose the rela-
tionship and nesting. Early approaches (Marcu,
2000a; LeThanh et al., 2004) rely mainly on overt
discourse markers (or cue words) and use hand-
coded rules to build text structure trees, bottom-up
from clauses to sentences to paragraphs. . . . Since
a hierarchical discourse tree structure is analo-
gous to a constituency based syntactic tree, mod-
ern research explored syntactic parsing techniques
(e.g., CKY) for discourse parsing based on mul-
tiple text-level or sentence-level features (Soricut
and Marcu, 2003; Reitter, 2003; Baldridge and
Lascarides, 2005; Subba and Di Eugenio, 2009;
Lin et al., 2009; Luong et al., 2014).
A recent prevailing idea for discourse parsing
is to train two classifiers, namely a binary struc-
ture classifier for determining whether two adja-
cent text units should be merged to form a new
subtree, followed by a multi-class relation classi-
fier for determining which discourse relation label
should be assigned to the new subtree. The idea is
proposed by Hernault and his colleagues (Duverle
and Prendinger, 2009; Hernault et al., 2010a) and
followed by other work using more sophisticated
features (Feng and Hirst, 2012; Hernault et al.,
2010b). Current state-of-art performance for re-
lation identification is achieved by the recent rep-
resentation learning approach proposed by (Ji and
Eisenstein, 2014). The proposed framework pre-
sented in this paper is similar to (Ji and Eisenstein,
2014) for transforming the discourse units to the
abstract representations.
2.2 Recursive Deep Learning
Recursive neural networks constitute one type of
deep learning frameworks which was first pro-
posed in (Goller and Kuchler, 1996). The recur-
sive framework relies and operates on structured
inputs (e.g., a parse tree) and computes the rep-
resentation for each parent based on its children
2062
iteratively in a bottom-up fashion. A series of vari-
ations of RNN has been proposed to tailor differ-
ent task-specific requirements, including Matrix-
Vector RNN (Socher et al., 2012) that represents
every word as both a vector and a matrix, or Recur-
sive Neural Tensor Network (Socher et al., 2013)
that allows the model to have greater interactions
between the input vectors. Many tasks have ben-
efited from the recursive framework, including
parsing (Socher et al., 2011b), sentiment analysis
(Socher et al., 2013), textual entailment (Bowman,
2013), segmentation (Wang and Mansur, 2013;
Houfeng et al., 2013), and paraphrase detection
(Socher et al., 2011a).
3 The RST Discourse Treebank
There are today two primary alternative discourse
treebanks suitable for training data: the Rhetor-
ical Structure Theory Discourse Treebank RST-
DT (Carlson et al., 2003) and the Penn Discourse
Treebank (Prasad et al., 2008). In this paper, we
select the former. In RST (Mann and Thompson,
1988), a coherent context or a document is repre-
sented as a hierarchical tree structure, the leaves
of which are clause-sized units called Elementary
Discourse Units (EDUs). Adjacent nodes (siblings
in the tree) are linked with discourse relations that
are either binary (hypotactic) or multi-child (parat-
actic). One child of each hypotactic relation is al-
ways more salient (called the NUCLEUS); its sib-
ling (the SATELLITE) is less salient compared and
may be omitted in summarization. Multi-nuclear
relations (e.g., CONJUNCTION) exhibit no distinc-
tion of salience between the units.
The RST Discourse Treebank contains 385 an-
notated documents (347 for training and 38 for
testing) from the Wall Street Journal. A total
of 110 fine-grained relations defined in (Marcu,
2000b) are used for tagging relations in RST-DT.
They are subtypes of 18 original high-level RST
categories. For fair comparison with existing sys-
tems, we use in this work the 18 coarse-grained re-
lation classes, which with nuclearity attached form
a set of 41 distinct relations. Non-binary relations
are converted into a cascade of right-branching bi-
nary relations.
Conventionally, discourse parsing in RST-DT
involves the following sub-tasks: (1) EDU seg-
mentation to segment the raw text into EDUs, (2)
tree-building. Since the segmentation task is es-
sentially clause delimitation and hence relatively
easy (with state-of-art accuracy at most 95%),
we focus on the latter problem. We assume that
the gold-standard EDU segmentations are already
given, as assumed in other past work (Feng and
Hirst, 2012).
4 EDU Model
In this section, we describe how we compute
the distributed representation for a given sentence
based on its parse tree structure and contained
words. Our implementation is based on (Socher
et al., 2013). As the details can easily be found
there, we omit them for brevity.
Let s denote any given sentence, comprised of a
sequence of tokens s = {w
1
, w
2
, ..., w
n
s
}, where
n
s
denotes the number of tokens in s. Each to-
ken w is associated with a specific vector embed-
ding e
w
= {e
1
w
, e
2
w
, ..., e
K
w
}, where K denotes the
dimension of the word embedding. We wish to
compute the vector representation h
s
for current
sentence, where h
s
= {h
1
s
, h
2
s
, ..., h
K
s
}.
Parse trees are obtained using the Stanford
Parser
1
, and each clause is treated as an EDU. For
a given parent p in the tree and its two children c
1
(associated with vector representation h
c
1
) and c
2
(associated with vector representation h
c
2
), stan-
dard recursive networks calculate the vector for
parent p as follows:
h
p
= f(W ? [h
c
1
, h
c
2
] + b) (1)
where [h
c
1
, h
c
2
] denotes the concatenating vector
for children representations h
c
1
and h
c
2
; W is a
K ? 2K matrix and b is the 1 ? K bias vector;
and f(?) is the function tanh. Recursive neural
models compute parent vectors iteratively until the
root node?s representation is obtained, and use the
root embedding to represent the whole sentence.
5 Discourse Parsing
Since recent work (Feng and Hirst, 2012; Hernault
et al., 2010b) has demonstrated the advantage of
combining the binary structure classifier (deter-
mining whether two adjacent text units should be
merged to form a new subtree) with the multi-class
classifier (determining which discourse relation la-
bel to assign to the new subtree) over the older
single multi-class classifier with the additional la-
bel NO-REL, our approach follows the modern
1
http://nlp.stanford.edu/software/
lex-parser.shtml
2063
Figure 1: RST Discourse Tree Structure.
strategy but trains binary and multi-class classi-
fiers jointly based on the discourse structure tree.
Figure 2 illustrates the structure of a discourse
parse tree. Each node e in the tree is associated
with a distributed vector h
e
. e
1
, e
2
, e
3
and e
6
constitute the leaves of trees, the distributed vec-
tor representations of which are assumed to be al-
ready obtained from convolution in Section 4. Let
N
r
denote the number of relations and we have
N
r
= 41.
5.1 Binary (Structure) Classification
In this subsection, we train a binary (structure)
classifier, which aims to decide whether two EDUs
or spans should be merged during discourse tree
reconstruction.
Let t
binary
(e
i
, e
j
) be the binary valued variable
indicating whether e
i
and e
j
are related, or in other
words, whether a certain type of discourse rela-
tions holds between e
i
and e
j
. According to Fig-
ure 2, the following pairs constitute the training
data for binary classification:
t
binary
(e
1
, e
2
) = 1, t
binary
(e
3
, e
4
) = 1,
t
binary
(e
2
, e
3
) = 0, t
binary
(e
3
, e
6
) = 0,
t
binary
(e
5
, e
6
) = 1
To train the binary classifier, we adopt a three-
layer neural network structure, i.e., input layer,
hidden layer, and output layer. Let H = [h
e
i
, h
e
j
]
denote the concatenating vector for two spans e
i
and e
j
. We first project the concatenating vector
H to the hidden layer withN
binary
hidden neurons.
The hidden layer convolutes the input with non-
linear tanh function as follows:
L
binary
(e
i
,e
j
)
= f(G
binary
? [h
e
i
, h
e
j
] + b
binary
)
where G
binary
is an N
binary
? 2K convolution ma-
trix and b
binary
denotes the bias vector.
The output layer takes as input L
binary
(e
i
,e
j
)
and gen-
erates a scalar using the linear function U
binary
?
L
binary
(e
i
,e
j
)
+ b. A sigmod function is then adopted to
project the value to a [0,1] probability space. The
execution at the output layer can be summarized
as:
p[t
binary
(e
i
, e
j
) = 1] = g(U
binary
?L
binary
(e
i
,e
j
)
+b
?
binary
)
(2)
where U
binary
is an N
binary
? 1 vector and b
?
binary
denotes the bias. g(?) is the sigmod function.
5.2 Multi-class Relation Classification
If t
binary
(e
i
, e
j
) is determined to be 1, we next
use variable r(e
i
, e
j
) to denote the index of rela-
tion that holds between e
i
and e
j
. A multi-class
classifier is train based on a three-layer neural net-
work, in the similar way as binary classification in
Section 5.1. Concretely, a matrix G
Multi
and bias
vector b
Multi
are first adopted to convolute the con-
catenating node vectors to the hidden layer vector
L
multi
(e
i
,e
j
)
:
L
multi
(e
i
,e
j
)
= f(G
multi
? [h
e
i
, h
e
j
] + b
multi
) (3)
We then compute the posterior probability over
labels given the hidden layer vector L using the
softmax and obtain the N
r
dimensional probabil-
ity vector P
(e
1
,e
2
)
for each EDU pair as follows:
S
(e
i
,e
j
)
= U
multi
? L
multi
(e
i
,e
j
)
(4)
P
(e
1
,e
2
)
(i) =
exp(S
(e
1
,e
2
)
(i))
?
k
exp(S
(e
1
,e
2
)
)(k)
(5)
where U
multi
is the N
r
? 2K matrix. The i
th
ele-
ment in P
(e
1
,e
2
)
denotes the probability that i
t
h re-
lation holds between e
i
and e
j
. To note, binary and
multi-class classifiers are trained independently.
5.3 Distributed Vector for Spans
What is missing in the previous two subsections
are the distributed vectors for non-leaf nodes (i.e.,
e
4
and e
5
in Figure 1), which serve as structure and
relation classification. Again, we turn to recursive
deep learning network to obtain the distributed
vector for each node in the tree in a bottom-up
fashion.
Similar as for sentence parse-tree level compo-
sitionally, we extend a standard recursive neural
network by associating each type of relations r
with one specific K?2K convolution matrix W
r
.
2064
Figure 2: System Overview.
The representation for each node within the tree is
calculated based on the representations for its chil-
dren in a bottom-up fashion. Concretely, for a par-
ent node p, given the distributed representation h
e
i
for left child, h
e
j
for right child, and the relation
r(e
1
, e
2
), its distributed vector h
p
is calculated as
follows:
h
p
= f(W
r(e
1
,e
2
)
? [h
e
i
, h
e
j
] + b
r(e
1
,e
2
)
) (6)
where b
r(e
1
,e
2
)
is the bias vector and f(?) is the
non-linear tanh function.
To note, our approach does not make any dis-
tinction between within-sentence text spans and
cross-sentence text spans, different from (Feng
and Hirst, 2012; Joty et al., 2013)
5.4 Cost Function
The parameters to optimize include sentence-
level convolution parameters [W , b],
discourse-level convolution parameters
[{W
r
}, {b
r
}], binary classification parameters
[G
binary
, b
binary
, U
binary
, b
?
binary
], and multi-class
parameters [G
multi
, b
multi
, U
multi
].
Suppose we have M
1
binary training samples
and M
2
multi-class training examples (M
2
equals
the number of positive examples in M
1
, which
is also the non-leaf nodes within the training dis-
course trees). The cost function for our framework
with regularization on the training set is given by:
J(?
binary
) =
?
(e
i
,e
j
)?{binary}
J
binary
(e
i
, e
j
)
+Q
binary
?
?
???
binary
?
2
(7)
J(?
multi
) =
?
(e
i
,e
j
)?{multi}
J
multi
(e
i
, e
j
)
+Q
multi
?
?
???
multi
?
2
(8)
where
J
binary
(e
i
, e
j
) = ?t(e
i
, e
j
) log p(t(e
i
, e
j
) = 1)
? (1? t(e
i
, e
j
)) log[1? p(t(e
i
, e
j
) = 1)]
J
multi
(e
i
, e
j
) = ? log[p(r(e
i
, e
j
) = r)]
(9)
5.5 Backward Propagation
The derivative for parameters involved is com-
puted through backward propagation. Here we
illustrate how we compute the derivative of
J
multi
(e
i
, e
j
) with respect to different parameters.
For each pair of nodes (e
i
, e
j
) ? multi, we
associate it with a N
r
dimensional binary vector
R(e
i
, e
j
), which denotes the ground truth vector
with a 1 at the correct label r(e
i
, e
j
) and all other
entries 0. Integrating softmax error vector, for any
parameter ?, the derivative of J
multi
(e
i
, e
j
) with re-
spect to ? is given by:
?J
multi
(e
i
, e
j
)
??
= [P
(e
i
,e
j
)
?R
(e
i
,e
j
)
]?
?S
(e
i
,e
j
)
??
(10)
where ? denotes the Hadamard product between
the two vectors. Each training pair recursively
backpropagates its error to some node in the dis-
course tree through [{W
r
}, {b
r
}], and then to
nodes in sentence parse tree through [W, b], and
the derivatives can be obtained according to stan-
dard backpropagation (Goller and Kuchler, 1996;
Socher et al., 2010).
2065
5.6 Additional Features
When determining the structure/multi relation be-
tween individual EDUs, additional features are
also considered, the usefulness of which has been
illustrated in a bunch of existing work (Feng and
Hirst, 2012; Hernault et al., 2010b; Joty et al.,
2012). We consider the following simple text-level
features:
? Tokens at the beginning and end of the EDUs.
? POS at the beginning and end of the EDUs.
? Whether two EDUs are in the same sentence.
5.7 Optimization
We use the diagonal variant of AdaGrad (Duchi et
al., 2011) with minibatches, which is widely ap-
plied in deep learning literature (e.g.,(Socher et
al., 2011a; Pei et al., 2014)). The learning rate
in AdaGrad is adapted differently for different pa-
rameters at different steps. Concretely, let g
i
?
de-
note the subgradient at time step t for parameter
?
i
obtained from backpropagation, the parameter
update at time step t is given by:
?
?
= ?
??1
?
?
?
?
t=0
?
g
i2
?
g
i
?
(11)
where ? denotes the learning rate and is set to 0.01
in our approach.
Elements in {W
r
}, W , G
binary
, G
multi
, U
binary
,
U
multi
are initialized by randomly drawing from
the uniform distribution [?, ], where  is calcu-
lated as suggested in (Collobert et al., 2011). All
bias vectors are initialized with 0. Word embed-
dings {e} are borrowed from Senna (Collobert et
al., 2011; Collobert, 2011).
5.8 Inference
For inference, the goal is to find the most proba-
ble discourse tree given the EDUs within the doc-
ument. Existing inference approach basically in-
clude the approach adopted in (Feng and Hirst,
2012; Hernault et al., 2010b) that merges the most
likely spans at each step and SPADE (Fisher and
Roark, 2007) that first finds the tree structure that
is globally optimal, then assigns the most probable
relations to the internal nodes.
In this paper, we implement a probabilistic
CKY-like bottom-up algorithm for computing the
most likely parse tree using dynamic program-
ming as are adopted in (Joty et al., 2012; Joty
et al., 2013; Jurafsky and Martin, 2000) for the
search of global optimum. For a document with
n EDUs, as different relations are characterized
with different compositions (thus leading to dif-
ferent vectors), we use a N
r
?n?n dynamic pro-
gramming table Pr, the cell Pr[r, i, j] of which
represents the span contained EDUs from i to j
and stores the probability that relation r holds be-
tween the two spans within i to j. Pr[r, i, j] is
computed as follows:
Pr[r, i, j] =max
r
1
,r
2
,k
Pr[r
1
, i, k] ? Pr[r
2
, k, j]
?P (t
binary
(e
[i,k]
, e
[k,j]
) = 1)
?P (r(e
[i,k]
, e
[k,j]
) = 1)
(12)
At each merging step, a distributed vector for the
merged point is calculated according to Eq. 13 for
different relations. The CKY-like algorithms finds
the global optimal. To note, the worst-case run-
ning time of our inference algorithm is O(N
2
r
n
3
),
where n denotes the number of sentences within
the document, which is much slower than the
greedy search. In this work, for simplification, we
simplify the framework by maintaining the top 10
options at each step.
6 Experiments
A measure of the performance of the system is
realized by comparing the structure and labeling
of the RS-tree produced by our algorithm to gold-
standard annotations.
Standard evaluation of discourse parsing output
computes the ratio of the number of identical tree
constituents shared in the generated RS-trees and
the gold-standard trees against the total number
of constituents in the generated discourse trees
2
,
which is further divided to three matrices: Span
(on the blank tree structure), nuclearity (on the
tree structure with nuclearity indication), and rela-
tion (on the tree structure with rhetorical relation
indication but no nuclearity indication).
The nuclearity and relation decisions are made
based on the multi-class output labels from the
deep learning framework. As we do not consider
nuclearity when classifying different discourse re-
lations, the two labels attribute[N][S] and at-
tribute[S][N] made by multi-class classifier will
be treated as the same relation label ATTRIBUTE.
2
Conventionally, evaluation matrices involve precision,
recall and F-score in terms of the comparison between tree
structures. But these are the same when manual segmenta-
tion is used (Marcu, 2000b).
2066
Approach Span Nuclearity Relation
HILDA 75.3 60.0 46.8
Joty et al. 82.5 68.4 55.7
Feng and Hirst 85.7 71.0 58.2
Ji and Eisenstein 82.1 71.1 61.6
Unified (with feature) 82.0 70.0 57.1
Ours (no feature) 82.4 69.2 56.8
Ours (with feature) 84.0 70.8 58.6
human 88.7 77.7 65.7
Table 1: Performances for different approaches.
Performances for baselines are reprinted from
(Joty et al., 2013; Feng and Hirst, 2014; Ji and
Eisenstein, 2014).
Also, we do not train a separate classifier for NU-
CLEUS and SATELLITE identification. The nucle-
arity decision is made based on the relation type
produced by the multi-class classifier.
6.1 Parameter Tuning
The regularization parameter Q constitutes the
only parameter to tune in our framework. We tune
it on the 347 training documents. Concretely, we
employ a five-fold cross validation on the RST
dataset and tune Q on 5 different values: 0.01,
0.1, 0.5, 1.5, 2.5. The final model was tested on
the testing set after parameter tuning.
6.2 Baselines
We compare our model against the following
currently prevailing discourse parsing baselines:
HILDA A discourse parser based on support
vector machine classification introduced by Her-
nault et al. (Hernault et al., 2010b). HILDA uses
the binary and multi-class classifier to reconstruct
the tree structure in a greedy way, where the
most likely nodes are merged at each step. The
results for HILDA are obtained by running the
system with default settings on the same inputs
we provided to our system.
Joty et al The discourse parser introduced by
Joty et al. (Joty et al., 2013). It relies on CRF
and combines intra-sentential and multi-sentential
parsers in two different ways. Joty et al. adopt
the global optimal inference as in our work. We
reported the performance from their paper (Joty et
al., 2013).
Feng and Hirst The linear-time discourse
parser introduced in (Feng and Hirst, 2014) which
relies on two linear-chain CRFs to obtain a se-
quence of discourse constituents.
Ji and Eisenstein The shift-reduce discourse
parser introduced in (Ji and Eisenstein, 2014)
which parses document by relying on the dis-
tributed representations obtained from deep learn-
ing framework.
Additionally, we implemented a simplified ver-
sion of our model called unified where we use
a unified convolutional function with unified pa-
rameters [W
sen
, b
sen
] for span vector computation.
Concretely, for a parent node p, given the dis-
tributed representation h
e
i
for left child, h
e
j
for
right child, and the relation r(e
1
, e
2
), rather than
taking the inter relation between two children, its
distributed vector h
p
is calculated:
h
p
= f(W
sen
? [h
e
i
, h
e
j
] + b
sen
) (13)
6.3 Performance
Performances for different models approaches re-
ported in Table 1. And as we can observe, al-
though the proposed framework obtains compa-
rable result compared with existing state-of-state
performances regarding all evaluating parameters
for discourse parsing. Specifically, as for the three
measures, no system achieves top performance on
all three, though some systems outperform all oth-
ers for one of the measures. The proposed system
achieves high overall performance on all three, al-
though it does not achieve top score on any mea-
sure. The system gets a little bit performance
boost by considering text-level features illustrated
in Section 5.6. The simplified version of the orig-
inal model underperforms against the original ap-
proach due to lack of expressive power in convo-
lution. Performance plummets when different re-
lations are uniformly treated, which illustrates the
importance of taking into consideration different
types of relations in the span convolution proce-
dure.
7 Conclusion
In this paper, we describe an RST-style text-level
discourse parser based on a neural network model.
The incorporation of sentence-level distributed
vectors for discourse analysis obtains compara-
ble performance compared with current state-of-
art discourse parsing system.
Our future work will focus on extending
discourse-level distributed presentations to related
2067
tasks, such as implicit discourse relation identifi-
cation or dialogue analysis. Further, once the tree
structure for a document can be determined, the
vector for the entire document can be obtained
in bottom-up fashion, as in this paper. One can
now investigate whether the discourse parse tree
is useful for acquiring a single document-level
vector representation, which would benefit mul-
tiple tasks, such as document classification or
macro-sentiment analysis.
Acknowledgements
The authors want to thank Vanessa Wei Feng and
Shafiq Joty for helpful discussions regarding RST
dataset. We also want to thank Richard Socher,
Zhengyan He and Pradeep Dasigi for the clarifica-
tion of deep learning techniques.
References
Jason Baldridge and Alex Lascarides. 2005. Proba-
bilistic head-driven parsing for discourse structure.
In Proceedings of the Ninth Conference on Compu-
tational Natural Language Learning, pages 96?103.
Association for Computational Linguistics.
Samuel R Bowman. 2013. Can recursive neural tensor
networks learn logical reasoning? arXiv preprint
arXiv:1312.6192.
Lynn Carlson, Daniel Marcu, and Mary Ellen
Okurowski. 2003. Building a discourse-tagged cor-
pus in the framework of rhetorical structure theory.
Springer.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Ronan Collobert. 2011. Deep learning for efficient dis-
criminative parsing. In International Conference on
Artificial Intelligence and Statistics, number EPFL-
CONF-192374.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. The Journal of Ma-
chine Learning Research, 12:2121?2159.
David A Duverle and Helmut Prendinger. 2009. A
novel discourse parser based on support vector ma-
chine classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 665?673. Association for Compu-
tational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2012. Text-
level discourse parsing with rich linguistic fea-
tures. In Proceedings of the 50th Annual Meeting
of the Association for Computational Linguistics:
Long Papers-Volume 1, pages 60?68. Association
for Computational Linguistics.
Vanessa Wei Feng and Graeme Hirst. 2014. A lin-
ear time bottom-up discourse parser with constraints
and post-editing. In ACL.
Seeger Fisher and Brian Roark. 2007. The utility of
parse-derived features for automatic discourse seg-
mentation. In ANNUAL MEETING-ASSOCIATION
FOR COMPUTATIONAL LINGUISTICS, vol-
ume 45, page 488.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In Neural Net-
works, 1996., IEEE International Conference on,
volume 1, pages 347?352. IEEE.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010a. A semi-supervised approach to
improve classification of infrequent discourse rela-
tions using feature vector extension. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, pages 399?409. As-
sociation for Computational Linguistics.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010b. Hilda: a discourse parser using sup-
port vector machine classification. Dialogue & Dis-
course, 1(3).
Wang Houfeng, Longkai Zhang, and Ni Sun. 2013.
Improving chinese word segmentation on micro-
blog using rich punctuations.
Eduard H Hovy and Elisabeth Maier. 1997. Parsimo-
nious or profligate: How many and which discourse
structure relations. Discourse Processes.
Yangfeng Ji and Jacob Eisenstein. 2014. Representa-
tion learning for text-level discourse parsing.
Shafiq Joty, Giuseppe Carenini, and Raymond T
Ng. 2012. A novel discriminative framework for
sentence-level discourse analysis. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 904?915. Asso-
ciation for Computational Linguistics.
Shafiq Joty, Giuseppe Carenini, Raymond Ng, and
Yashar Mehdad. 2013. Combining intra-and multi-
sentential rhetorical parsing for document-level dis-
course analysis. In Proceedings of the 51st annual
meeting of the association for computational lin-
guistics (ACL), pages 486?496.
Dan Jurafsky and James H Martin. 2000. Speech &
Language Processing. Pearson Education India.
2068
Huong LeThanh, Geetha Abeysinghe, and Christian
Huyck. 2004. Generating discourse structures for
written texts. In Proceedings of the 20th inter-
national conference on Computational Linguistics,
page 329. Association for Computational Linguis-
tics.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009.
Recognizing implicit discourse relations in the penn
discourse treebank. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 1-Volume 1, pages 343?351.
Association for Computational Linguistics.
Annie Louis, Aravind Joshi, and Ani Nenkova. 2010.
Discourse indicators for content selection in summa-
rization. In Proceedings of the 11th Annual Meeting
of the Special Interest Group on Discourse and Di-
alogue, pages 147?156. Association for Computa-
tional Linguistics.
Minh-Thang Luong, Michael C Frank, and Mark John-
son. 2014. Parsing entire discourses as very long
strings: Capturing topic continuity in grounded lan-
guage learning.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243?281.
Daniel Marcu. 2000a. The rhetorical parsing of unre-
stricted texts: A surface-based approach. Computa-
tional Linguistics, 26(3):395?448.
Daniel Marcu. 2000b. The theory and practice of dis-
course parsing and summarization. MIT Press.
Wenzhe Pei, Tao Ge, and Chang Baobao. 2014. Max-
margin tensor neural network for chinese word seg-
mentation. In Proceedings of ACL.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind K Joshi, and Bon-
nie L Webber. 2008. The penn discourse treebank
2.0. In LREC. Citeseer.
David Reitter. 2003. Simple signals for complex
rhetorics: On rhetorical analysis with rich-feature
support vector models. In LDV Forum, volume 18,
pages 38?52.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop, pages 1?9.
Richard Socher, Eric H Huang, Jeffrey Pennington,
Andrew Y Ng, and Christopher D Manning. 2011a.
Dynamic pooling and unfolding recursive autoen-
coders for paraphrase detection. In NIPS, vol-
ume 24, pages 801?809.
Richard Socher, Cliff C Lin, Chris Manning, and An-
drew Y Ng. 2011b. Parsing natural scenes and nat-
ural language with recursive neural networks. In
Proceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 129?136.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 1631?1642.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical infor-
mation. In Proceedings of the 2003 Conference
of the North American Chapter of the Association
for Computational Linguistics on Human Language
Technology-Volume 1, pages 149?156. Association
for Computational Linguistics.
Caroline Sporleder and Mirella Lapata. 2005. Dis-
course chunking and its application to sentence com-
pression. In Proceedings of the conference on Hu-
man Language Technology and Empirical Methods
in Natural Language Processing, pages 257?264.
Association for Computational Linguistics.
Rajen Subba and Barbara Di Eugenio. 2009. An effec-
tive discourse parser that uses rich linguistic infor-
mation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 566?574. Association for
Computational Linguistics.
Suzan Verberne, Lou Boves, Nelleke Oostdijk, and
Peter-Arno Coppen. 2007. Evaluating discourse-
based answer extraction for why-question answer-
ing. In Proceedings of the 30th annual international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 735?736. ACM.
Longkai Zhang Houfeng Wang and Xu Sun Mairgup
Mansur. 2013. Exploring representations from un-
labeled data with co-training for chinese word seg-
mentation.
2069
Squibs
What Is a Paraphrase?
Rahul Bhagat?
USC Information Sciences Institute
Eduard Hovy??
USC Information Sciences Institute
Paraphrases are sentences or phrases that convey the same meaning using different wording.
Although the logical definition of paraphrases requires strict semantic equivalence, linguistics
accepts a broader, approximate, equivalence?thereby allowing far more examples of ?quasi-
paraphrase.? But approximate equivalence is hard to define. Thus, the phenomenon of para-
phrases, as understood in linguistics, is difficult to characterize. In this article, we list a set
of 25 operations that generate quasi-paraphrases. We then empirically validate the scope and
accuracy of this list by manually analyzing random samples of two publicly available paraphrase
corpora. We provide the distribution of naturally occurring quasi-paraphrases in English text.
1. Introduction
Sentences or phrases that convey the same meaning using different wording are called
paraphrases. For example, consider sentences (1) and (2):
(1) The school said that their buses seat 40 students each.
(2) The school said that their buses accommodate 40 students each.
Paraphrases are of interest for many current NLP tasks, including textual entailment,
machine reading, question answering, information extraction, and machine translation.
Whenever the text contains multiple ways of saying ?the same thing,? but the applica-
tion requires the same treatment of those various alternatives, an automated paraphrase
recognition mechanism would be useful.
One reason why paraphrase recognition systems have been difficult to build is
because paraphrases are hard to define. Although the strict interpretation of the
term ?paraphrase? is quite narrow because it requires exactly identical meaning,
in linguistics literature paraphrases are most often characterized by an approxi-
mate equivalence of meaning across sentences or phrases. De Beaugrande and Dressler
(1981, page 50) define paraphrases as ?approximate conceptual equivalence among
? 24515 SE 46th Terrace Issaquah, WA 98029. E-mail: me@rahulbhagat.net.
?? 24515 SE 46th Terrace Issaquah, WA 98029. E-mail: hovy@isi.edu.
Submission received: 5 July 2012; revised submission received: 21 January 2013; accepted for publication:
6 March 2013.
doi:10.1162/COLI a 00166
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 3
outwardly different material.? Hirst (2003, slide 9) defines paraphrases as ?talk(ing)
about the same situation in a different way.? He argues that paraphrases aren?t fully
synonymous: There are pragmatic differences in paraphrases, namely, difference of eval-
uation, connotation, viewpoint, and so forth. According to Mel?cuk (2012, page 7) ?An
approximate synonymy of sentences is considered as sufficient for them to be produced
from the same SemS.? He further adds that approximate paraphrases include implica-
tions (not in the logical sense, but in the everyday sense). Taking an extreme view, Clark
(1992, page 172) rejects the idea of absolute synonymy by saying ?Every two forms (in
language) contrast in meaning.? Overall, there is a large body of work in the linguistics
literature that argues that paraphrases are not restricted to strict synonymy.
In this article, we take a broad view of paraphrases. To avoid the conflict between
the notion of strict paraphrases as understood in logic and the broad notion in linguis-
tics, we use the term quasi-paraphrases to refer to the paraphrases that we deal with here.
In the context of this article, the term ?paraphrases? (even without the prefix ?quasi?)
means ?quasi-paraphrases.? We define quasi-paraphrases as ?sentences or phrases that
convey approximately the same meaning using different words.? We ignore the fine
grained distinctions of meaning between sentences and phrases, introduced due to the
speaker?s evaluation of the situation, connotation of the terms used, change of modality,
and so on. For example, consider sentences (3) and (4).
(3) The school said that their buses seat 40 students each.
(4) The school said that their buses cram in 40 students each.
Here, seat and cram in are not synonymous: They carry different evaluations by the
speaker about the same situation. We, however, consider sentences (3) and (4) to be
(quasi) paraphrases. Similarly, consider sentences (5) and (6).
(5) The school said that their buses seat 40 students each.
(6) The school is saying that their buses might accommodate 40 students each.
Here, said and is saying have different tenses. Also, might accommodate and seat are not
synonymous, due to the modal verb might. We consider sentences (5) and (6) to be
quasi-paraphrases, however.
Note that this article focuses on defining quasi-paraphrases. It does not provide
direct implementation/application results of using them. We believe, however, that
this work will allow computation-oriented researchers to focus their future work more
effectively on a subset of paraphrase types without concern for missing important
material, and it will provide linguistics-oriented researchers with a blueprint of the
overall distribution of the types of paraphrase.
2. Paraphrasing Phenomena Classified
Although approximate equivalence is hard to characterize, it is not a completely un-
structured phenomenon. By studying various existing paraphrase theories?Mel?cuk
(2012), Harris (1981), Honeck (1971)?and through an analysis of paraphrases obtained
from two different corpora, we have discovered that one can identify a set of 25 classes
of quasi-paraphrases, with each class having its own specific way of relaxing the re-
quirement of strict semantic equivalence. In this section, we define and describe these
classes.
464
Bhagat and Hovy What Is a Paraphrase?
The classes described here categorize quasi-paraphrases from the lexical perspec-
tive. The lexical perspective defines paraphrases in terms of the kinds of lexical changes
that can take place in a sentence/phrase resulting in the generation of its paraphrases.
1. Synonym substitution: Replacing a word/phrase by a synonymous word/phrase,
in the appropriate context, results in a paraphrase of the original sentence/phrase. This
category covers the special case of genitives, where the clitic ?s is replaced by other
genitive indicators like of, of the, and so forth. This category also covers near-synonymy,
that is, it allows for changes in evaluation, connotation, and so on, of words or phrases
between paraphrases. Example:
(a) Google bought YouTube. ? Google acquired YouTube.
(b) Chris is slim. ? Chris is slender. ? Chris is skinny.
2. Antonym substitution: Replacing a word/phrase by its antonym accompanied by
a negation or by negating some other word, in the appropriate context, results in a
paraphrase of the original sentence/phrase. This substitution may be accompanied by
the addition/deletion of appropriate function words. Example:
(a) Pat ate. ? Pat did not starve.
3. Converse substitution: Replacing a word/phrase with its converse and inverting
the relationship between the constituents of a sentence/phrase, in the appropriate
context, results in a paraphrase of the original sentence/phrase, presenting the sit-
uation from the converse perspective. This substitution may be accompanied by the
addition/deletion of appropriate function words and sentence restructuring. Example:
(a) Google bought YouTube. ? YouTube was sold to Google.
4. Change of voice: Changing a verb from its active to passive form and vice versa re-
sults in a paraphrase of the original sentence/phrase. This change may be accompanied
by the addition/deletion of appropriate function words and sentence restructuring.
This often generates the most strictly meaning-preserving paraphrase. Example:
(a) Pat loves Chris. ? Chris is loved by Pat.
5. Change of person: Changing the grammatical person of a referenced object results in
a paraphrase of the original sentence/phrase. This change may be accompanied by the
addition/deletion of appropriate function words. Example:
(a) Pat said, ?I like football.? ? Pat said that he liked football.
6. Pronoun/Co-referent substitution: Replacing a pronoun by the noun phrase it
co-refers with results in a paraphrase of the original sentence/phrase. This also often
generates the most strictly meaning-preserving paraphrase. Example:
(a) Pat likes Chris, because she is smart. ? Pat likes Chris, because Chris
is smart.
465
Computational Linguistics Volume 39, Number 3
7. Repetition/Ellipsis: Ellipsis or elliptical construction results in a paraphrase of the
original sentence/phrase. Similarly, this often generates the most strictly meaning-
preserving paraphrase. Example:
(a) Pat can run fast and Chris can run fast, too. ? Pat can run fast and Chris
can, too.
8. Function word variations: Changing the function words in a sentence/phrase with-
out affecting its semantics, in the appropriate context, results in a paraphrase of the
original sentence/phrase. This can involve replacing a light verb by another light verb,
replacing a light verb by copula, replacing certain prepositions with other prepositions,
replacing a determiner by another determiner, replacing a determiner by a preposition
and vice versa, and addition/removal of a preposition and/or a determiner. Example:
(a) Results of the competition have been declared. ? Results for the
competition have been declared.
(b) Pat showed a nice demo. ? Pat?s demo was nice.
9. Actor/Action substitution: Replacing the name of an action by a word/phrase denot-
ing the person doing the action (actor) and vice versa, in the appropriate context, results
in a paraphrase of the original sentence/phrase. This substitution may be accompanied
by the addition/deletion of appropriate function words. Example:
(a) I dislike rash drivers. ? I dislike rash driving.
10. Verb/?Semantic-role noun? substitution: Replacing a verb by a noun correspond-
ing to the agent of the action or the patient of the action or the instrument used for
the action or the medium used for the action, in the appropriate context, results in
a paraphrase of the original sentence/phrase. This substitution may be accompanied
by the addition/deletion of appropriate function words and sentence restructuring.
Example:
(a) Pat teaches Chris. ? Pat is Chris?s teacher.
(b) Pat teaches Chris. ? Chris is Pat?s student.
(c) Pat tiled his bathroom floor. ? Pat installed tiles on his bathroom floor.
11. Manipulator/Device substitution: Replacing the name of a device by a word/
phrase denoting the person using the device (manipulator) and vice versa, in the
appropriate context, results in a paraphrase of the original sentence/phrase. This
substitution may be accompanied by the addition/deletion of appropriate function
words. Example:
(a) The pilot took off despite the stormy weather. ? The plane took off despite
the stormy weather.
12. General/Specific substitution: Replacing a word/phrase by a more general or more
specific word/phrase, in the appropriate context, results in a paraphrase of the original
466
Bhagat and Hovy What Is a Paraphrase?
sentence/phrase. This substitution may be accompanied by the addition/deletion of ap-
propriate function words. Hypernym/hyponym substitution is a part of this category.
This often generates a quasi-paraphrase. Example:
(a) I dislike rash drivers. ? I dislike rash motorists.
(b) Pat is flying in this weekend. ? Pat is flying in this Saturday.
13. Metaphor substitution: Replacing a noun by its standard metaphorical use and
vice versa, in the appropriate context, results in a paraphrase of the original sentence/
phrase. This substitution may be accompanied by the addition/deletion of appropriate
function words. Example:
(a) I had to drive through fog today. ? I had to drive through a wall of fog
today.
(b) Immigrants have used this network to send cash. ? Immigrants have used
this network to send stashes of cash.
14. Part/Whole substitution: Replacing a part by its corresponding whole and vice
versa, in the appropriate context, results in a paraphrase of the original sentence/
phrase. This substitution may be accompanied by the addition/deletion of appropriate
function words. Example:
(a) American airplanes pounded the Taliban defenses. ? American airforce
pounded the Taliban defenses.
15. Verb/Noun conversion: Replacing a verb by its corresponding nominalized noun
form and vice versa, in the appropriate context, results in a paraphrase of the original
sentence/phrase. This substitution may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. Example:
(a) The police interrogated the suspects. ? The police subjected the suspects to
an interrogation.
(b) The virus spread over two weeks. ? Two weeks saw a spreading of the
virus.
16. Verb/Adjective conversion: Replacing a verb by the corresponding adjective form
and vice versa, in the appropriate context, results in a paraphrase of the original
sentence/phrase. This substitution may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. Example:
(a) Pat loves Chris. ? Chris is lovable to Pat.
17. Verb/Adverb conversion: Replacing a verb by its corresponding adverb form and
vice versa, in the appropriate context, results in a paraphrase of the original sentence/
phrase. This substitution may be accompanied by the addition/deletion of appropriate
function words and sentence restructuring. Example:
(a) Pat boasted about his work. ? Pat spoke boastfully about his work.
467
Computational Linguistics Volume 39, Number 3
18. Noun/Adjective conversion: Replacing a verb by its corresponding adjective form
and vice versa, in the appropriate context, results in a paraphrase of the original
sentence/phrase. This substitution may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. Example:
(a) I?ll fly by the end of June. ? I?ll fly late June.
19. Verb-preposition/Noun substitution: Replacing a verb and a preposition denoting
location by a noun denoting the location and vice versa, in the appropriate context,
results in a paraphrase of the original sentence/phrase. This substitution may be
accompanied by the addition/deletion of appropriate function words and sentence
restructuring. Example:
(a) The finalists will play in Giants stadium. ? Giants stadium will be the
playground for the finalists.
20. Change of tense: Changing the tense of a verb, in the appropriate context, results
in a paraphrase of the original sentence/phrase. This change may be accompanied
by the addition/deletion of appropriate function words. This often generates a quasi-
paraphrase, although it might be semantically less accurate than many other quasi-
paraphrases. Example:
(a) Pat loved Chris. ? Pat loves Chris.
21. Change of aspect: Changing the aspect of a verb, in the appropriate context, results
in a paraphrase of the original sentence/phrase. This change may be accompanied by
the addition/deletion of appropriate function words. Example:
(a) Pat is flying in today. ? Pat flies in today.
22. Change of modality: Addition/deletion of a modal or substitution of one modal
by another, in the appropriate context, results in a paraphrase of the original sen-
tence/phrase. This change may be accompanied by the addition/deletion of appro-
priate function words. This often generates a quasi-paraphrase, although it might be
semantically less accurate than many other quasi-paraphrases. Example:
(a) Google must buy YouTube. ? Google bought YouTube.
(b) The government wants to boost the economy. ? The government hopes to
boost the economy.
23. Semantic implication: Replacing a word/phrase denoting an action, event, and so
forth, by a word/phrase denoting its possible future effect, in the appropriate context,
results in a paraphrase of the original sentence/phrase. This may be accompanied by
the addition/deletion of appropriate function words and sentence restructuring. This
often generates a quasi-paraphrase. Example:
(a) Google is in talks to buy YouTube. ? Google bought YouTube.
(b) The Marines are fighting the terrorists. ? The Marines are eliminating
the terrorists.
468
Bhagat and Hovy What Is a Paraphrase?
24. Approximate numerical equivalences: Replacing a numerical expression (a word/
phrase denoting a number, often with a unit) by an approximately equivalent nu-
merical expression (even perhaps with change of unit), in the appropriate context,
results in a paraphrase of the original sentence/phrase. This often generates a quasi-
paraphrase. Example:
(a) At least 23 U.S. soldiers were killed in Iraq last month. ? About 25 U.S.
soldiers were killed in Iraq last month.
(b) Disneyland is 32 miles from here. ? Disneyland is around 30 minutes
from here.
25. External knowledge: Replacing a word/phrase by another word/phrase based on
extra-linguistic (world) knowledge, in the appropriate context, results in a paraphrase
of the original sentence/phrase. This may be accompanied by the addition/deletion of
appropriate function words and sentence restructuring. This often generates a quasi-
paraphrase, although in some cases preserves meaning exactly. Example:
(a) We must work hard to win this election. ? The Democrats must work hard
to win this election.
(b) The government declared victory in Iraq. ? Bush declared victory in Iraq.
3. Analysis of Paraphrases
In Section 2, we presented a list of lexical changes that define quasi-paraphrases. In this
section, we seek to validate the scope and accuracy of this list. Our analysis uses two
criteria:
1. Distribution: What is the distribution of each of these lexical changes in a paraphrase
corpus?
2. Human judgment: If one uses each of the lexical changes, on applicable sentences,
how often do each of these changes generate acceptable quasi-paraphrases?
3.1 Distribution
We used the following procedure to measure the distribution of the lexical changes:
1. We downloaded paraphrases from two publicly available data sets containing
sentence-level paraphrases: the Multiple-Translations Corpus (MTC) (Huang, Graff,
and Doddington 2002) and the Microsoft Research (MSR) paraphrase corpus (Dolan,
Quirk, and Brockett 2004). The paraphrase pairs come with their equivalent parts
manually aligned (Cohn, Callison-Burch, and Lapata 2008).
2. We selected 30 sentence-level paraphrase pairs from each of these corpora at random
and extracted the corresponding aligned and unaligned phrases.1 This resulted in 210
phrase pairs for the MTC corpus and 145 phrase pairs for the MSR corpus.
1 We assume that any unaligned phrase is paired with a null phrase and we discard it prior to the analysis.
469
Computational Linguistics Volume 39, Number 3
3. We labeled each of the phrase pairs with the appropriate lexical changes defined in
Section 2. If any phrase pair could not be labeled by a lexical change from Section 2, we
labeled it as unknown.
4. We finally calculated the distribution of each label (lexical change), over all the labels,
for each corpus. Table 1 shows the percentage distribution of the lexical changes in the
MTC (column 3) and MSR corpora (column 4).
3.2 Human Judgment
In this section, we explain the procedure we used to obtain the human judgments of the
changes that define paraphrases from the lexical perspective:
1. We randomly selected two words or phrases from publicly available resources (de-
pending on the lexical change) for each of the lexical operations from Section 2 (except
external knowledge). For example, to obtain words for synonym substitution, we used
WordNet (Fellbaum 1998) (and selected a word, say buy); to obtain implication rules
for semantic implication, we used the DIRT resource (Lin and Pantel 2001); and so on.
Table 1
Distribution and Precision of paraphrases. Distribution may not sum to 100% due to rounding.
# Category % Distribution MTC % Distribution MSR % Precision
1. Synonym substitution 37 19 95
2. Antonym substitution 0 0 65
3. Converse substitution 1 0 75
4. Change of voice 1 1 85
5. Change of person 0 1 80
6. Pronoun/Co-referent
substitution
1 1 70
7. Repetition/Ellipsis 4 4 100
8. Function word variations 37 30 85
9. Actor/Action substitution 0 0 75
10. Verb/?Semantic-role noun?
substitution
1 0 60
11. Manipulator/Device substitution 0 0 30
12. General/Specific substitution 4 3 80
13. Metaphor substitution 0 1 60
14. Part/Whole substitution 0 0 65
15. Verb/Noun conversion 2 3 100
16. Verb/Adjective conversion 1 0 55
17. Verb/Adverb conversion 0 0 65
18. Noun/Adjective conversion 0 0 80
19. Verb-preposition/
Noun substitution
0 0 65
20. Change of tense 4 1 70
21. Change of aspect 1 0 95
22. Change of modality 1 0 80
23. Semantic implication 1 4 70
24. Approximate numerical
equivalences
0 2 95
25. External knowledge 6 32 95
26. Unknown 0 0 NA
470
Bhagat and Hovy What Is a Paraphrase?
2. For each selected word or phrase, we obtained five random sentences from the Giga-
word corpus. These sentences were manually checked to make sure that they contained
the intended sense of the word or phrase. This gave us a total of 10 sentences for each
phenomenon. For example, for the word buy, one of the selected sentences might be:
(a) They want to buy a house.
3. For each sentence selected in step 2, we applied the corresponding lexical changes to
the word or phrase selected in step 1 to generate a potential paraphrase.2 For example,
we might apply synonym substitution to sentence (a) and replace the word buy with its
WordNet synonym purchase. This will result in the following sentence:
(b) They want to purchase a house.
4. For the phenomenon of external knowledge, we randomly sampled a total of 10 sen-
tence pairs from the MTC and MSR corpora, such that the pairs were paraphrases based
on external knowledge.
5. We gave the sentence pairs to two annotators and asked them to annotate them as
either paraphrases or non-paraphrases. For example, the annotator might be given the
sentence pair (a) and (b) and she/he might annotate this pair as paraphrases.
6. We used the annotations from each of the annotators to calculate the precision per-
centage for each lexical change. The final precision score was calculated as the average
of the precision scores obtained from the two annotations. Table 1 shows the percentage
precision (column 5) of lexical changes in this test corpus.
7. We finally calculated the kappa statistic (Siegal and Castellan Jr. 1988) to measure the
inter-annotator agreement. A kappa score of ? = 0.66 was obtained on the annotation
task.
4. Conclusion
A definition of what phenomena constitute paraphrases and what do not has been a
problem in the past. Whereas some people have used a very narrow interpretation
of paraphrases?paraphrases must be exactly logically equivalent?others have taken
broader perspectives that consider even semantic implications to be acceptable para-
phrases. To the best of our knowledge, outside of specific language interpretation frame-
works (like Meaning Text Theory [Mel?cuk 1996]), no one has tried to create a general,
exhaustive list of the transformations that define paraphrases. In this article we provide
such a list. We have also tried to empirically quantify the distribution and accuracy of
the list. It is notable that certain types of quasi-paraphrases dominate whereas others
are very rare. We also observed, however, that the dominating transformations vary
based on the type of paraphrase corpus used, thus indicating the variety of behavior
exhibited by the paraphrases. Based on the large variety of possible transformations that
can generate paraphrases, its seems likely that the kinds of paraphrases that are deemed
useful would depend on the application at hand. This might motivate the creation of
2 The words in the new sentence were allowed to be reordered (permuted) if needed and only function
words (and no content words) were allowed to be added to the new sentence.
471
Computational Linguistics Volume 39, Number 3
application-specific lists of the kinds of allowable paraphrases and the development of
automatic methods to distinguish the different kinds of paraphrases.
Acknowledgments
The authors wish to thank Jerry Hobbs and
anonymous reviewers for valuable
comments and feedback.
References
Clark, E. V. 1992. Conventionality and
contrasts: Pragmatic principles with
lexical consequences. In Andrienne Lehrer
and Eva Feder Kittay, editors, Frame,
Fields, and Contrasts: New Essays in
Semantic Lexical Organization. Lawrence
Erlbaum Associates, Hillsdale, NJ,
pages 171?188.
Cohn, T., C. Callison-Burch, and M. Lapata.
2008. Constructing corpora for the
development and evaluation of
paraphrase systems. Computational
Linguistics, 34(4):597?614.
De Beaugrande, R. and W. V. Dressler.
1981. Introduction to Text Linguistics.
Longman, New York, NY.
Dolan, B., C. Quirk, and C. Brockett.
2004. Unsupervised construction of
large paraphrase corpora: Exploiting
massively parallel news sources.
In Proceedings of the Conference on
Computational Linguistics (COLING),
pages 350?357, Geneva.
Fellbaum, C. 1998. An Electronic Lexical
Database. MIT Press, Cambridge, MA.
Harris, Z. 1981. Co-occurence and
transformation in linguistic structure.
In Henry Hiz, editor, Papers on Syntax.
D. Reidel Publishing Co., Dordrecht,
pages 143?210. First published in 1957.
Hirst, G. 2003. Paraphrasing paraphrased.
Invited talk at the ACL International
Workshop on Paraphrasing, Sapporo.
Honeck, Richard P. 1971. A study of
paraphrases. Journal of Verbal Learning
and Verbal Behavior, 10(4):367?381.
Huang, S., D. Graff, and G. Doddington.
2002. Multiple-translation Chinese
corpus. Linguistic Data Consortium,
Philadelphia, PA.
Lin, D. and P. Pantel. 2001. Dirt: Discovery of
inference rules from text. In ACM SIGKDD
International Conference on Knowledge
Discovery and Data Mining, pages 323?328,
San Francisco, CA.
Mel?cuk, I. 1996. Lexical functions: A tool for
description of lexical relations in a lexicon.
In Leo Wanner, editor, Lexical Functions
in Lexicography and Natural Language
Processing. John Benjamins Publishing Co.,
Philadelphia, PA, pages 37?102.
Mel?cuk, I., 2012. Semantics: From Meaning
to Text. John Benjamins Publishing Co.,
Philadelphia, PA.
Siegal, S. and N. J. Castellan, Jr. 1988.
Nonparametric Statistics for the Behavioral
Sciences. McGraw-Hill, Columbus, OH.
472
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 618?626,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Not All Seeds Are Equal: Measuring the Quality of Text Mining Seeds
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
Open-class semantic lexicon induction is of
great interest for current knowledge harvest-
ing algorithms. We propose a general frame-
work that uses patterns in bootstrapping fash-
ion to learn open-class semantic lexicons for
different kinds of relations. These patterns re-
quire seeds. To estimate the goodness (the po-
tential yield) of new seeds, we introduce a re-
gression model that considers the connectiv-
ity behavior of the seed during bootstrapping.
The generalized regression model is evaluated
on six different kinds of relations with over
10000 different seeds for English and Span-
ish patterns. Our approach reaches robust per-
formance of 90% correlation coefficient with
15% error rate for any of the patterns when
predicting the goodness of seeds.
1 Introduction: What is a Good Seed?
The automated construction of semantically typed
lexicons (terms classified into their appropriate se-
mantic class) from unstructured text is of great im-
portance for various kinds of information extraction
(Grishman and Sundheim, 1996), question answer-
ing (Moldovan et al, 1999), and ontology popu-
lation (Suchanek et al, 2007). Maintaining large
semantic lexicons is a time-consuming and tedious
task, because open classes (such as: all singers, all
types of insects) are hard to cover completely, and
even closed classes (such as: all countries, all large
software companies) change over time. Since it is
practically impossible for a human to collect such
knowledge adequately, many supervised, unsuper-
vised, and semi-supervised techniques have been de-
veloped.
All these techniques employ some sort of context
to specify the appearance in text of the desired in-
formation. This approach is based on the general
intuition, dating back at least to the distributional
similarity idea of (Harris, 1954), that certain con-
texts are specific enough to constrain terms or ex-
pressions within them to be specific classes or types.
Often, the context is a string of words with an empty
slot for the desired term(s); sometimes, it is a regu-
lar expression-like pattern that includes word classes
(syntactic or semantic); sometimes, it is a more ab-
stract set of features, including orthographic fea-
tures like capitalization, words, syntactic relations,
semantic types, and other characteristics, which is
the more complete version of the distributional sim-
ilarity approach.
In early information extraction work, these con-
texts were constructed manually, and resembled reg-
ular expressions (Appelt et al, 1995). More re-
cently, researchers have focused on learning them
automatically. Since unsupervised algorithms re-
quire large training data and may or may not produce
the types and granularities of the semantic class de-
sired by the user, and supervised algorithms may re-
quire a lot of manual oversight, semi-supervised al-
gorithms have become more popular. They require
only a couple of seeds (examples filling the desired
semantic context) to enable the learning mechanism
to learn patterns that extract from unlabeled texts
additional instances of the same class (Riloff and
Jones, 1999; Etzioni et al, 2005; Pasca, 2004).
Sometimes, the pattern(s) learned are satisfactory
618
enough to need no further elaboration. They are
applied to harvest as many additional terms of the
desired type as possible (for example, the instance-
learning pattern ?<type> such as ?? introduced in
(Hearst, 1992)). More often, the method is applied
recursively: once some pattern(s) have been learned,
they are used to find additional terms, which are then
used as new seeds in the patterns to search for addi-
tional new patterns, etc., until no further patterns are
found. At that point, the satisfactory patterns are se-
lected and large-scale harvesting proceeds as usual.
In an interesting variation of this method, (Kozareva
et al, 2008) describe the ?doubly-anchored pat-
tern? (DAP) that includes a seed term in conjunc-
tion with the open slot for the desired terms to be
learned, making the pattern itself recursive by al-
lowing learned terms to replace the initial seed terms
directly: ?<type> such as <seed> and ??.
Context-based information harvesting is well un-
derstood and has been the focus of extensive re-
search. The core unsolved problem is the selec-
tion of seeds. In current knowledge harvesting al-
gorithms, seeds are chosen either at random (Davi-
dov et al, 2007; Kozareva et al, 2008), by picking
the top N most frequent terms of the desired class
(Riloff and Jones, 1999; Igo and Riloff, 2009), or by
asking experts (Pantel et al, 2009). None of these
methods is quite satisfactory. (Etzioni et al, 2005)
report on the impact of seed set noise on the final
performance of semantic class learning, and Pan-
tel et al observe a tremendous variation in the en-
tity set expansion depending on the initial seed set
composition. These studies show that the selection
of ?good? seeds is very important. Recently, (Vyas
et al, 2009) proposed an automatic system for im-
proving the seeds generated by editors (Pantel et al,
2009). The results show 34% improvement in final
performance using the appropriate seed set. How-
ever, using editors to select seeds or to guide their
seed selection process is expensive and therefore not
always possible. Because of this, we address in this
paper two questions: ?What is a good seed?? and
?How can the goodness of seeds be automatically
measured without human intervention??.
The contributions of this paper are as follows:
? First, we use recursive patterns to automatically
learn seeds for open-class semantic lexicons.
? Second, we define what the ?goodness? of a
seed term is. Then we introduce a regression
model of seed quality measurement that, after
a certain amount of training, automatically es-
timates the goodness of new seeds with above
90% accuracy for bootstrapping with the given
relation.
? Next, importantly, we discover that training a
regression model on certain relations enables
one to predict the goodness of a seed even for
other relations that have never been seen be-
fore, with an accuracy rate of over 80%.
? We conduct experiments with six kinds of
relations and more than 10000 automatically
harvested seed examples in both English and
Spanish.
The rest of the paper is organized as follows.
In the next section, we review related work. Sec-
tion 3 describes the recursive pattern bootstrap-
ping (Kozareva et al, 2008). Section 4 presents our
seed quality measurement regression model. Sec-
tion 5 discusses experiments and results. Finally, we
conclude in Section 6.
2 Related Work
Seeds are used in automatic pattern extraction from
text corpora (Riloff and Jones, 1999) and from the
Web (Banko, 2009). Seeds are used to harvest in-
stances (Pasca, 2004; Etzioni et al, 2005; Kozareva
et al, 2008) or attributes of a given class (Pas?ca and
Van Durme, 2008), or to learn concept-specific re-
lations (Davidov et al, 2007), or to expand already
existing entity sets (Pantel et al, 2009). As men-
tioned above, (Etzioni et al, 2005) report that seed
set composition affects the correctness of the har-
vested instances, and (Pantel et al, 2009) observe an
increment of 42% precision and 39% recall between
the best and worst performing seed sets for the task
of entity set expansion.
Because of the large diversity of the usage of
seeds, there has been no general agreement regard-
ing exactly how many seeds are necessary for a
given task. According to (Pantel et al, 2009) 10 to
20 seeds are a sufficient starting set in a distribu-
tional similarity model to discover as many new cor-
rect instances as may ever be found. This observa-
tion differs from the claim of (Pas?ca and Van Durme,
2008) that 1 or 2 instances are sufficient to dis-
cover thousands of instance attributes. For some
619
pattern-based algorithms one to two seeds are suf-
ficient (Davidov et al, 2007; Kozareva et al, 2008),
some require ten seeds (Riloff and Jones, 1999; Igo
and Riloff, 2009), and others use a variation of 1, 5,
10 to 25 seeds (Talukdar et al, 2008).
As mentioned, seed selection is not yet well un-
derstood. Seeds may be chosen at random (Davi-
dov et al, 2007; Kozareva et al, 2008), by picking
the most frequent terms of the desired class (Riloff
and Jones, 1999; Igo and Riloff, 2009), or by ask-
ing humans (Pantel et al, 2009). The intuitions for
seed selection that experts develop over time seem
to prefer instances that are neither ambiguous nor
too frequent, but that at the same time are prolific
and quickly lead to the discovery of a diverse set of
instances. These criteria are vague and do not al-
ways lead to the discovery of good seeds. For some
approaches, infrequent and ambiguous seeds are ac-
ceptable while for others they lead to deterioration
in performance. For instance, the DAP (Kozareva et
al., 2008) performance is not affected by the ambi-
guity of the seed, because the class and the seed in
the pattern mutually disambiguate each other, while
for the distributional similarity model of (Pantel et
al., 2009), starting with an ambiguous seed leads
to ?leakage? and the harvesting of non-true class in-
stances. (Kozareva et al, 2008) show that for the
closed class country, both high-frequency seeds like
USA and low-frequency seeds like Burkina Faso
can equally well yield all remaining instances. An
open question to which no-one provides an answer
is whether and which high/low frequency seeds can
yield all instances of large, open classes like people
or singers.
3 Bootstrapping Recursive Patterns
There are many algorithms for harvesting informa-
tion from the Web. The main objective of our work
is not the creation of a new algorithm, but rather de-
termining the effect of seed selection on the gen-
eral class of recursive bootstrapping harvesting al-
gorithms for the acquisition of semantic lexicons for
open class relations. For our experiments, since it
is time-consuming and difficult for humans to pro-
vide large sets of seeds to start the bootstrapping
process, we employ the recursive DAP mechanism
introduced by (Kozareva et al, 2008) that produces
seeds on its own.
The algorithm starts with a seed of type class
which is fed into the doubly-anchored pattern
?<class> such as <seed> and *? and learns in the
* position new instances of type class. The newly
learned instances are then systematically placed into
the position of the seed in the DAP pattern, and the
harvesting process is repeated until no new instances
are found. The general framework is as follows:
1. Given:
a language L={English, Spanish}
a pattern Pi={e.g., [verb prep, noun, verb]}
a seed seed for Pi
2. Build a query in DAP-like fashion for Pi using
template Ti of the type ?class such as seed and
*?, ?* and seed verb prep?, ?* and seed noun?,
?* and seed verb?
3. submit Ti to Yahoo! or another search engine
4. extract instances occupying the * position
5. take instances from 4. and go to 2.
6. repeat steps 2?5 until no new instances are
found
At the end of bootstrapping, the harvested in-
stances can be considered to be seeds with which
the bootstrapping procedure could have been initi-
ated. We can now compare any of them to study
their relative ?goodness? as bootstrapping seeds.
4 Seed Quality Measurement
4.1 Problem Formulation
We define our task as:
Task Definition: Given a seed and a pattern in a
language (say English or Spanish), (1) use the boot-
strapping procedure to learn instances from the Web;
(2) build a predictive model to estimate the ?good-
ness? of seeds (whether generated by a human or
learned) .
Given a desired semantic class, a recursive harvest-
ing pattern expressing its context, and a seed term
for use in this pattern, we define the ?goodness? of
the seed as consisting of two measures:
? the yield: the total number of instances learned,
not counting duplicates, until the bootstrapping
procedure has run to exhaustion;
? the distance: the number of iterations required
by the process to reach exhaustion.
620
Our approach is to build a model of the behavior of
many seeds for the given pattern. Any new seed can
then be compared against this model, once its basic
characteristics have been determined, and its yield
and distance estimates produced. In order to deter-
mine the characteristics of the new seed, it first has
to be employed in the pattern for a small number of
iterations. The next subsection describes the regres-
sion model we employ in our approach.
4.2 Regression Model
Given a seed s, we seek to predict the yield g of s as
defined above. We do this via a parametrized func-
tion f :g? = f(s;w), where w ? Rd are the weights.
Our approach is to learn w from a collection of N
training examples {< si, gi >}Ni=1, where each si is
a seed and each gi ? R.
Support vector regression (Drucker et al, 1996)
is a well-known method for training a regression
model by solving the following optimization prob-
lem:
min
w?Rs
1
2
||w||2 + CN
N?
i=1
max(0, |gi ? f(si;w)| ? ?)
? ?? ?
?-insensitive loss function
where C is a regularization constant and ? con-
trols the training error. The training algorithm finds
weights w that define a function f minimizing the
empirical risk.
Let h be a function from seeds into some vector-
space representation ? Rd, then the function f takes
the form: f(s;w) = h(s)Tw = ?Ni=1 ?iK(s, si),
where f is re-parameterized in terms of a polyno-
mial kernel function K with dual weights ?i. K
measures the similarity between two seeds. Full de-
tails of the regression model and its implementation
are beyond the scope of this paper; for more de-
tails see (Scho?lkopf and Smola, 2001; Smola et al,
2003). In our experimental study, we use the freely
available implementation of SVM in Weka (Witten
and Frank, 2005).
To evaluate the quality of our prediction model,
we compare the actual yield of a seed with the pre-
dicted value obtained, and compute the correlation
coefficient and the relative absolute error.
5 Experiments and Results
5.1 Data Collection
We conducted an exhaustive evaluation study with
the open semantic classes people and city, initiated
with the seeds John and London. For each class, we
submitted the DAP patterns as web queries to Ya-
hoo!Boss and retrieved the top 1000 web snippets
for each query, keeping only unique instances. In
total, we collected 1.5GB of snippets for people and
1.9GB of snippets for cities. The algorithm ran un-
til complete exhaustion, requiring 19 iterations for
people and 12 for cities. The total number of unique
harvested instances was 3798 for people and 5090
for cities. We used all instances as seeds and instan-
tiated for each seed the bootstrapping process from
the very beginning. This resulted in 3798 and 5090
separate bootstrapping runs for people and cities re-
spectively. For each seed, we recorded the total
number of instances learned at the end of bootstrap-
ping, the number of iterations, and the number of
unique instances extracted on each iteration. After
the harvesting part terminated, we analyzed the con-
nectivity / bootstrapping behavior of the seeds, and
produced the regression model.
5.2 Seed Characteristics
For many knowledge harvesting algorithms, the se-
lection of a non-ambiguous seeds is of great impor-
tance. In the DAP bootstrapping framework, the am-
biguity of the seed is eliminated as the class and the
seed mutually disambiguate each other. Of great im-
portance to the bootstrapping algorithm is the selec-
tion of a seed that can yield a large number of in-
stances and can keep the bootstrapping process en-
ergized.
Figure 1: Seed Connectivity
Figure 1 shows the different kinds of seeds we
found on analyzing the results of the bootstrapping
process. Based on the yield learned on each iter-
ation, we identify four major kinds of seeds: her-
mit, one-step, mid, and high connectors. In the
figure, seed (a) is a hermit because it does not dis-
cover other instances. Seed (b) is a one-step connec-
tor as it discovers instances on the first iteration but
621
then becomes inactive. Seeds (d) and (e) are high
connectors because they find a rich population of in-
stances. Seed (c) is a mid connector because it has
lower yield than (d) and (e), but higher than (a) and
(b).
Table 1 shows the results of classifying the 3798
people and 5090 city seeds into the four kinds of
seed. The majority of the seeds for both patterns are
hermits, from 23 to 41% are high connectors, and
the rest are one-step and mid connectors. For each
kind of seed, we also show three examples.
people such as X and * examples
#hermit 2271 (60%) Leila, Anne Boleyn, Sophocles
#one-step 329 (9%) Helene, Frida Kahlo, Cornelius
#mid 315 (8%) Brent, Ferdinand, Olivia
#high 883 (23%) Diana, Donald Trump, Christopher
cities such as X and * examples
#hermit 2393 (47%) Belfast, Najafabad, El Mirador
#one-step 406 (8%) Durnstein, Wexford, Al-Qaim
#mid 207 (4%) Bialystok, Gori, New Albany
#high 2084 (41%) Vienna, Chicago, Marrakesh
Table 1: Connectivity-based Seed Classification.
This study shows that humans are very likely to
choose non-productive seeds for bootstrapping: it is
difficult for a human to know a priori that a name
like Diana will be more productive than Leila, He-
lene, or Olivia.
Another interesting characteristic of a seed is the
speed of learning. Some seeds, such as (e), ex-
tract large quantity of instances from the very be-
ginning, resulting in fewer bootstrapping iterations,
while others, such as (d), spike much later, resulting
in more. In our analysis, we found that some high
connector seeds of the people pattern can learn the
whole population in 12 iterations, while others re-
quire from 15 to 20 iterations. Figure 2 shows the
speed of learning of ten high connector seeds for
the people pattern. The y axis shows the number
of unique instances harvested on each iteration. In-
tuitively, a good seed is the one that produces a large
yield of instances in short distance. Thus the ?good-
ness? of seed (e) is better than that of seed (d).
As shown in Figure 2, for each seed, we observe
a single hump that corresponds to the point in which
a seed generates the maximum number of instances.
The peak occurs on different iterations because it is
dependent both on the yield learned with each iter-
ation and the total distance, for each seed. The oc-
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19
Yi
el
d
Iteration
s1
s2
s3
s4
s5
s6
s7
s8
s9
s10
Figure 2: Seed Learning Speed
currence of a single hump reveals regularity in the
connectivity behavior of seeds, and is discussed in
the Conclusion. We model this behavior as features
in our regression model and use it to measure the
quality of new seeds. The next subsection explains
the features of the regression model and the experi-
mental results obtained.
5.3 Predicting the Goodness of Seeds
Building a pattern specific model: For each pat-
tern, we build N different regression models, where
N corresponds to the total number of bootstrapping
iterations of the pattern. For regression model Ri,
we use the yield of a seed from iterations 1 to i as
features. This information is used to model the ac-
tivity of the seed in the bootstrapping process and
later on to predict the extraction power of new seeds.
For example, in Figure 1 on the first iteration seeds
(b), (c), and (d) have the same low connectivity com-
pared to seed (e). As bootstrapping progresses, seed
(d) reaches productive neighbors that discover more
instances, while seeds (b) and (c) become inactive.
This example shows that the yield in the initial stage
of bootstrapping is not sufficient to accurately pre-
dict the quality of the seeds. Since we do not know
exactly how many iterations are necessary to accu-
rately determine the ?goodness? of seeds, we model
the yield learned on each iteration by each seed and
subsequently include this information in the regres-
sion models.
The yield of a seed sk at iteration i is computed as
yield(sk)i =
?n
m=1(sm), where n is the total num-
ber of unique instances sm harvested on iteration i.
Y ield(sk)i is high when sk discovers a large number
of instances (new seeds), and small otherwise. For
hermit seeds, yield=0 at any iteration, because the
seeds are totally isolated and do not discover other
622
instances (seeds). For example, when building the
second regression model R2 using seeds (d) and (e)
from Figure 1, the feature values corresponding to
each seed in R2 are: yield(sd)1=1 and yield(sd)2=2
for seed (d), and yield(se)1=3 and yield(se)2=5 for
seed (e).
Results: Figure 3 shows the correlation coefficients
(cc) and the relative absolute errors of each regres-
sion model Ri for the people and city patterns. The
results are computed over ten-fold cross validation
of the 3798 people and 5090 city seeds. The x axis
shows the regression model Ri,. The y axis in the
two upper graphs shows the correlation coefficient
of the predicted and the actual total yield of the seeds
using Ri, and in the two lower graphs, the y axis
shows the error rate of each Ri.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Regression Model Ri
People
cut_off, t
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Regression Model Ri
Cities
cut_off, t
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 1  2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Regression Model Ri
People
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Regression Model Ri
Cities
Figure 3: Regression for People and City.
We consider as a baseline model the regression
R1 which uses only the yield of the seeds on first
iteration. The prediction of R1 has cc=0.6 with
50% error for people and cc=0.4 with 70% error
for cities. These results confirm our previous obser-
vation that the quality of the seeds cannot be accu-
rately measured in the very beginning of bootstrap-
ping. However, by the ninth iteration, the regres-
sion models for people and cities reach cc=1.0 with
5% error rate. To make such an accurate prediction,
the model uses around one half of all bootstrapping
iterations?generally, just past the hump in Figure 2,
once the yield starts dropping.
Often in real applications or when under limited
resources (e.g., a fixed amount of Web queries per
day), running half the bootstrapping iterations is not
feasible. This problem can be resolved by employ-
ing different stopping criteria, at the cost of lower
cc and greater error. For example, one cut-off point
can be the (averaged) iteration number of the hump
for the given pattern. For people, the average hump
occurs at the seventh iteration, and for the city at
the fifth iteration. At this point, both patterns have a
cc=0.9 with 15% error rate. An alternative stopping
point can be the fourth iteration, where cc=0.7?0.8
with 35% error.
Overall, our study shows that it is possible to
model the behavior of seeds and use it to accurately
predict the ?goodness? of previously unseen seeds.
The results obtained for both people and city pat-
terns are very promising. However, a disadvantage
of this regression is that it requires training over the
whole extent of the given pattern. Also, each regres-
sion model is specific to the particular pattern it is
trained over. Next, we propose a generalized regres-
sion model which surmounts the problem of training
pattern-specific regression models.
5.4 Generalized Model for Goodness of Seeds
We built a generalized regression model (RG) com-
bining evidence from the people and city patterns.
We generated the features of each model as previ-
ously described in Section 5.3. From each pattern,
we randomly picked 1000 examples which resulted
in 30% of the people and 20% of the city seeds. We
used these seed examples to train the RGi models.
In total, we built 15 RGi, which is the maximum
number of overlapping iterations between the two
patterns. We tested our RG model with the remain-
ing 2798 people and 4090 city seeds.
Figure 4 shows the results of the RGi models for
the people and city patterns. In the first two itera-
tions, the predictions of the RG model are poorer
compared to the pattern-specific regression. On the
fourth iteration, both models have cc=0.7 and 0.8 for
the people and city patterns respectively. The error
rates of the generalized model are 41% and 35% for
people and city, while for the pattern-specific model
the errors are 37% and 32%. The early iterations
show a difference of around 4% in the error rate of
the two models, but around the ninth iteration both
models have comparable results.
623
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Generalized Regression Model RGi
Cities
People
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 65
 70
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Generalized Regression Model RGi
Cities
People
Figure 4: Generalized Regression for People and City.
This study shows that it is possible to combine
evidence from two patterns harvesting different se-
mantic information to predict accurately the behav-
ior of unseen seed examples for either of the two
patterns.
5.5 Evaluating the Generalized Model on
Different Languages and Kinds of Patterns
So far, we have studied the performance of the gen-
eralized seed quality prediction method for specific
patterns in English. However, the connectivity be-
havior of the seeds might change for other languages
and kinds of patterns, making the generalized model
impractical to use in such cases. To verify this,
we evaluated the generalized model (RG) from Sec-
tion 5.4 with the people and city patterns in Spanish
(? gente como X y *? and ? ciudades como X y *?), as
well as with two new kinds of patterns (? * and X fly
to? and ? * and X work for?1). For each pattern, we
ran the bootstrapping process from Section 3 until
exhaustion and collected all seeds.
First, for each pattern we studied the connectivity
behavior of the seeds. Table 2 shows the obtained
results. The distribution is similar to the seed distri-
bution for the English people and cities patterns. Al-
though the total number of harvested instances (i.e.,
seeds) is different for each pattern, the proportion of
hermits to other seeds remains larger. From 20%
to 37% of the seeds are high connectors, and the
rest are one-step and mid connectors. This analysis
shows that the connectivity behavior of seeds across
different languages and patterns is similar, at least
for the examples studied. In addition to the seed
analysis, we show in the table the total number of
bootstrapping iterations for each pattern. The ?work
1The X indicates the position of the seed and (*) corresponds
to the instances learned during bootstrapping.
for? and ?fly to? patterns run for a longer distance
compared to the other patterns. While for the ma-
jority of the patterns the hump is observed on the
fifth or seventh iteration, for these two patterns the
average peak is observed on the fifteenth.
gente como X y ciudades como X y
#hermit 318 (56%) 1061 (51%)
#one-step 58 (10%) 150 (8%)
#mid 79 (14%) 79 (4%)
#high 117 (20%) 795 (38%)
tot#iter 20 16
and X fly to and X work for
#hermit 389 (45%) 1262 (48%)
#one-step 87 (9%) 238 (9%)
#mid 75 (8%) 214 (8%)
#high 322 (37%) 922 (35%)
tot#iter 26 33
Table 2: Seed Classification for Spanish and Verb-Prep
Patterns.
Second, we test the RGi models from Section 5.4,
which were trained on people and cities, to predict
the total yield of the seeds in the new patterns. Fig-
ure 5 shows the correlation coefficient and the rela-
tive absolute error results of each pattern for RGi.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
Co
rre
la
tio
n 
Co
ef
fic
ie
nt
Generalized Regression Model RGi
Work For
Fly To
Ciudades
Gente
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 55
 60
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15
R
el
at
iv
e 
Ab
so
lu
te
 E
rro
r (
%)
Generalized Regression Model RGi
Work For
Fly To
Ciudades
Gente
Figure 5: Generalized Regression for Different Lan-
guages and Patterns.
Interestingly, we found that our generalized
method has consistent performance across the dif-
ferent languages and patterns. On the twelfth iter-
ation, the model is able to predict the ?goodness?
of seeds with cc=1.0 and from 0.4% to 8.0% error
rate. Around the fifth and sixth iterations, all pat-
terns reach cc=0.8 with error of 5% to 15%. The
higher error bound is for patterns like ?work for? and
?fly to? which run for a longer distance. This experi-
mental study confirms the robustness of our general-
ized model which is trained on the behavior of seeds
from one kind of pattern and tested with seeds in dif-
ferent languages and on completely different kinds
of patterns.
624
6 Conclusions and Future Work
It would, a fortiori, seem impossible to estimate the
goodness of a seed term used in a recursive boot-
strapping pattern for harvesting information from
the web. After all, its eventual total yield and dis-
tance depend on the cumulation of the terms pro-
duced in each iteration of the bootstrapping, and
there are no external constraints or known web lan-
guage structure to be exploited.
We have shown that it is possible to create, using
regression, a model of the grown behavior of seeds
for a given pattern, and fitting it with an indication of
a new seed?s growth (considering its grown behavior
in a limited number of bootstrapping iterations) in
order to obtain a quite reliable estimate of the new
seed?s eventual yield and distance.
Going further, we are delighted to observe that
the regularity of the single-hump harvesting behav-
ior makes it possible to learn a regression model that
enables one to predict, with some accuracy, both the
yield and the distance of a new seed, even when the
pattern being considered is not yet seen. All that is
required is the indication of the seed?s growth be-
havior, obtained through a number of iterations us-
ing the pattern of interest.
Our ongoing analysis takes the following ap-
proach. Let Ti be the set of all new terms (terms
not yet found) harvested during iteration i. Then
T0 = {t0,1}, just the initial seed term. Let NY (ti,j)
be the novel yield of term ti,j , that is, the number
of as yet undiscovered terms produced by a single
application of the pattern using the term ti,j . Notice
that bootstrapping ceases when for some i = d (the
distance), ?j NY (td,j) = 0. Since the total number
of terms that can be learned,
?d
i=0
?
j NY (ti,j) =
N , is finite and fixed, there are exactly three al-
ternatives for the growth of the NY curve when
it is shown summed over each iteration: (i) either
?
j NY (ti,j) =
?
j NY (ti+1,j) and there is no
larger NY sum for any iteration; or (ii) ?j NY (ti,j)
grows to a maximal value for some iteration i =
m and then decreases again; or (iii) ?j NY (ti,j)
reaches more than one locally maximal value at dif-
ferent iterations. The first case, in which exactly
the same number of new terms is harvested every
iteration for several or all iterations, would require
that each new term once learned yields precisely and
only one subsequent new term, or that the number
of hermits is exactly balanced by the NY of one or
more of the other terms in that iteration. This situa-
tion is so unlikely as to be dismissed outright. Case
(ii), in which there is a single hump, appears to be
how text is written on the web, as shown in Fig-
ure 2. Case (iii), the multi-hump case, would re-
quire that the terms be linked in semi-disconnected
?islands?, with a relatively much smaller inter-island
connectivity than intra-island one. Given our stud-
ies, it appears that language on the web is not orga-
nized this way, at least not for the patterns we stud-
ied. However, it is not impossible: this two-hump
case would have to have occurred in (Kozareva et
al., 2008) when the ambiguous seed term Georgia
was used in the DAP ?states such as Georgia and *?,
where initially the US states were harvested but, at
some point, the learned term Georgia also initiated
harvesting of the ex-USSR states. Such ?leakage?
into a new semantic domain requires not only ambi-
guity of the seed but also parallel ambiguity of the
class term, which is highly unlikely as well.
Accepting case (ii), therefore, we postulate that
for any (or all regular) patterns there is some iter-
ation m in which ?j NY (tm,j) is maximal. The
question is how rapidly the summed NY curve ap-
proaches it and then abates again. This depends on
the out-degree connectivity of terms overall. In the
population of N terms for a given semantic pattern,
is the distribution of out-degrees Poisson (or Zip-
fian), or is it normal (Gaussian)? In the former case,
there will be a few high-degree connector terms and
a large number (the long tail) of one-step and hermit
terms; in the latter, there will be a small but equal
number of low-end and high-end connector terms,
with the bulk of terms falling in the mid-connector
range. One direction of our ongoing work is to deter-
mine this distribution, and to empirically derive its
parameters. It might be possible to discover some in-
teresting regularities about the (preferential) uses of
terms within semantic domains, as reflected in term
network connectivity.
Although not all seeds are equal, it appears to
be possible to treat them with a single regression
model, regardless of pattern, to predict their ?good-
ness?.
Acknowledgments: This research was supported by
NSF grant IIS-0705091.
625
References
Douglas E. Appelt, Jerry R. Hobbs, John Bear, David Is-
rael, Megumi Kameyama, Andy Kehler, David Martin,
Karen Myers, and Mabry Tyson. 1995. SRI Interna-
tional FASTUS system MUC-6 test results and analy-
sis. In Proceedings of the Sixth Message Understand-
ing Conference (MUC-6), pages 237?248.
Michele Banko. 2009. Open information extraction from
the web. In Ph.D. Dissertation from University of
Washington.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of the
45th Annual Meeting of the Association of Computa-
tional Linguistics, pages 232?239, June.
Harris Drucker, Chris J.C. Burges, Linda Kaufman, Alex
Smola, and Vladimir Vapnik. 1996. Support vector re-
gression machines. In Advances in NIPS, pages 155?
161.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Ralph Grishman and Beth Sundheim. 1996. Message
understanding conference-6: a brief history. In Pro-
ceedings of the 16th conference on Computational lin-
guistics, pages 466?471.
Zellig S. Harris. 1954. Distributional structure. Word,
10:140?162.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th confer-
ence on Computational linguistics, pages 539?545.
Sean Igo and Ellen Riloff. 2009. Corpus-based seman-
tic lexicon induction with web-based corroboration.
In Proceedings of the Workshop on Unsupervised and
Minimally Supervised Learning of Lexical Semantics.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of ACL-08:
HLT, pages 1048?1056.
Dan I. Moldovan, Sanda M. Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana Girju, and
Vasile Rus. 1999. Lasso: A tool for surfing the answer
net. In TREC.
Marius Pas?ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of ACL-08: HLT.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938?
947, August.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the thirteenth ACM
international conference on Information and knowl-
edge management, pages 137?145.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In AAAI ?99/IAAI ?99: Proceedings of the
sixteenth national conference on Artificial intelligence
and the eleventh Innovative applications of artificial
intelligence conference innovative applications of ar-
tificial intelligence.
Bernhard Scho?lkopf and Alexander J. Smola. 2001.
Learning with Kernels: Support Vector Machines,
Regularization, Optimization, and Beyond (Adaptive
Computation and Machine Learning). The MIT Press.
Alex J. Smola, Bernhard Schlkopf, and Bernhard Sch
Olkopf. 2003. A tutorial on support vector regression.
Technical report, Statistics and Computing.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,
Deepak Ravichandran, Rahul Bhagat, and Fernando
Pereira. 2008. Weakly-supervised acquisition of la-
beled class instances using graph random walks. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP 2008, pages
582?590.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Management,
CIKM, pages 225?234.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, second edition.
626
Proceedings of the NAACL HLT 2010: Demonstration Session, pages 5?8,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Summarizing Textual Information about Locations  
In a Geo-Spatial Information Display System

Congxing Cai Eduard Hovy
Information Sciences Institute Information Sciences Institute 
University of Southern California University of Southern California 
Marina del Rey, California, USA 90292 Marina del Rey, California, USA 90292 
ccai@isi.edu hovy@isi.edu 
Abstract 
This demo describes the summarization of 
textual material about locations in the context 
of a geo-spatial information display system. 
When the amount of associated textual data is 
large, it is organized and summarized before 
display. A hierarchical summarization frame-
work, conditioned on the small space availa-
ble for display, has been fully implemented. 
Snapshots of the system, with narrative de-
scriptions, demonstrate our results.  
1 Introduction 
Geospatial display systems are increasingly gain-
ing attention, given the large amounts of geospatial 
data and services available online. Although geos-
patial imagery and maps show geometric relations 
among entities, they cannot be used to present oth-
er kinds of knowledge about the temporal, topic, 
and other conceptual relations and entities. Given 
an entity on a map, a description of what happened 
there, in what order in time, when, and why, re-
quires additional types of information, typically 
contained in text, in order to support varied search 
and decision tasks.  
In this demo, we apply text summarization to a 
geo-spatial information display system with poten-
tially large amounts of textual data. By summariz-
ing the textual material linked to each location, we 
demonstrate the ways one can organize this ma-
terial for optimal display and search. 
Of the many different types of text-oriented re-
sources available, some are structured and others 
unstructured. This textual data can be linked to 
locations based on different reasons (containing 
place names, addresses, real objects with geo-
graphical features, etc.). Appropriately grouping 
and presenting the different aspects of the textual 
information in summarization is a challenging task. 
A second challenge stems from the huge amounts 
of web material related to some geographical ob-
jects. For example, one may find millions of pages 
for a famous place or event at a specific map loca-
tion. Given the common limitations of display 
space in most geospatial display systems, one must 
also design the interface to support dynamic 
browsing and search. 
All these challenges bring new problems to exist-
ing summarization techniques. In the following 
sections, we demonstrate a hierarchical summari-
zation framework that reduces displayed text and 
fully utilizes the small display space available for 
textual information.  
2 Related Work 
Associating each news page individually to its lo-
cation(s) may overwhelm the amount of informa-
tion displayable at any point and thereby limit the 
scalability of the system. Existing systems pre-
sented in (Teitler et al, 2008) and GeoTracker 
(Chen et al 2007) organize material (at the area 
level) by time instead of somehow aggregating 
over larger numbers of related content. Since fre-
quently the associated news contents overlap at 
least in part, a natural solution is to aggregate the 
content somehow to remove duplication. Moreo-
ver, the aggregation of news provides a global 
view of the textual information about the specific 
5
location. Our system is the first available geo-
spatial text aggregation system to our knowledge.  
Within geospatial display systems, the space avail-
able to display textual information is often quite 
limited. We therefore need to summarize the most 
important and relevant information about each lo-
cation, drawing from all the web pages linked to it. 
However, directly applying a multi-document 
summarization (Lin and Hovy, 2001) to the web 
pages will generate poor results, due to unrelated 
titles, duplicate articles, and noisy contents con-
tained in web pages. When several different events 
have occurred at a location, more than one distinct 
summary may be needed. It is therefore important 
to deploy topic recognition (Lin and Hovy, 2000) 
and/or topic clustering (Osinski and Weiss, 2005) 
to identify and group relevant pieces of each text 
into single-topic ?chunks?. We develop a novel 
hierarchical summarization system to improve the 
interactivity and browsability.   
3 Text Summarization 
3.1 Content Extraction and Summarization 
Multi-webpage summarization is different from 
traditional multi-doc summarization. First, most 
web pages are much more complex than pure text 
documents. Since the web contains a combination 
of types of information?static text, image, videos, 
dynamic layout, etc.?even a single page can be 
treated as multiple documents. Current linking 
functions are based on keywords, making the rele-
vant content of each relevant web page only a li-
mited block within the page. Second, our task is 
oriented to locations, and hence differs from gen-
eral content summarization. Hence, we need to 
identify and extract the essential part(s) of the 
webpage linked to the geospatial imagery for 
summarization and display. In our work, we utilize 
two important features, layout and semantics, to 
identify and extract the relevant content. 
By rendering each web page into a DOM tree, we 
segment the page into large blocks based on its 
layout, including header, footer, left bar, right bar, 
main block, etc. We implemented a rule-based ex-
tractor to extract the most relevant block from the 
web page based on the relevance to the location.  
3.2 Clustering 
Given a list of text blocks relevant to a local point 
of interest, one can employ traditional text summa-
rization techniques to produce a short summary for 
each one. This solution may not be helpful, how-
ever, since a long list of pages associated with each 
point of interest would be very hard for users to 
browse. Especially when the space allocated to text 
display by the geospatial system is also limited, a 
high compression ratio is typically required for the 
summarization system. 
The solution we adopt is to deploy cluster-based 
multi-document summarization. Clustering must 
observe two criteria: first, the location of interest, 
and second, the text topic. Different clustering me-
thods can be employed. To delimit topics, a simple 
heuristic is to introduce as additional criterion the 
event/article date: when the difference in document 
dates within a topical cluster is (far) larger than the 
actual duration of the topic event, we are probably 
dealing with multiple separate events at the same 
location. Better performance is obtained by using a 
topic detection module first, and then clustering 
documents based on the topics identified.  
Unfortunately, documents usually contain multiple 
locations and multiple topics. The problem of ?top-
ic drift? can cause confusion in a short summary. 
As in (Hearst, 1997), we segment each document 
into several ?mini-documents?, each one devoted to 
a single topic, and then to perform location- and 
topic-based clustering over the (now larger) set of 
mini-documents.  
3.3 Hierarchical Summary Generation  
Whatever the clustering approach, the result is a 
potentially rather large set of individual topics as-
sociated with each location. Since screen space for 
the summaries may be very limited next to the 
maps / imagery, they have to be formatted and pre-
sented for maximal interpretability. To address this 
problem, we adopt a hierarchical structure to dis-
play incrementally longer summaries for each loca-
tion of interest. At present we have found three 
levels of incrementally longer summaries to be 
most useful. 
Thumbnail: a very short ?topic? that characte-
rizes the (clusters of) documents or segments asso-
ciated with each location. We present essentially 
one or two single keywords -- the most informative 
6
words for each cluster. We implemented a new 
version of our topic signature technology, one that 
uses tf.idf instead of the entropy ratio, as scoring 
measure to rank each cluster?s words. 
Title: a headline-length phrase or short sen-
tence (or two). The original titles of the web pages 
are often noisy or even unrelated to the current top-
ic cluster. Sometimes, the title may be meaningless 
(it might for example contain the website?s name 
?Pr Newswire?), or two different web pages may 
share the same title. We implemented a topic-
related headline generator based on our previous 
work (Lin and Hovy, 2000) by incorporating a top-
ic-based selector. 
Snippet: a paragraph-length excerpt characteriz-
ing the cluster. To produce paragraph-length sum-
maries, we implemented an extraction-based text 
summarizer. We built a new version of previously 
investigated technology (Lin and Hovy, 2001), 
implementing several sentence scoring techniques 
and a score combination function. 
4 Demonstration 
4.1 Geospatial Interaction  
The hierarchical summarization service is built 
upon the geo-spatial information display system, 
GeoXRAY1, a commercial product developed by 
Geosemble Technologies2. Figure 1 shows the sys-
tem?s display to support search and browsing of 
text content based on location of interest.  
Figure 1. Geospatial Information Display System 
                                                          
1GeoXRAY: http://www.geosemble.com/products_geoxray.html
2Geosemble Technologies: http://www.geosemble.com/
The user can enter an address in the top search 
box, or search by business name. The system then 
centers the imagery at that address or business. 
Clicking on ?Get Features? invokes the web ser-
vices to get al features about the displayed image 
and displays the features in the ?AREA: Features 
Found? list, and also draws them as points on the 
maps.  
The user can explore the map using the navigation 
controller. On clicking the marker of an identified 
building, an information window pops up contain-
ing the associated structured web information 
(building name, business type, website, online im-
ages, and so on), as shown in Figure 2. 
Figure 2. Navigating the Integrated Map 
Clicking on ?Get News? retrieves all news related 
to the displayed features; features with associated 
news show a small newspaper icon (see next to 
?Sony Pictures Entertainment? in Figure 4). Click-
ing on the icon displays the news that was linked 
with the feature, sorted by date. 
The hierarchical summarization system, described 
in this paper extends the GeoXRAY system to 
show a summarized view of the news. The user can 
click on the ?Cluster News? link. The results are 
displayed in a tree, showing the title of the cluster 
(thumbnail and title), under which appears a small 
summary of the cluster, under which appear links 
to all the news articles belonging to that cluster. 
4.2 Summarization Example  
We provide an example of our text summariza-
tion system performance in Figure 3. In this exam-
ple, we have selected the location of Sony Film 
Studios in Culver City by clicking on the map. 
Figure 3(a) shows the titles and dates of some of 
7
the 126 news articles that contain the words ?Sony 
Pictures Entertainment?. As described above, these 
documents are clustered based on topics. Using our 
current parameter settings, 20 multi-result clusters 
are formed, leaving 34 results unclustered. (The 
size of clusters, or the number of clusters desired, 
can be varied by the user.) As mentioned above, 
each cluster is presented to the users by a minimal 
length thumbnail summary consisting of a few cha-
racteristic keywords; a partial list of these is shown 
in Figure 3(b). Figure 3(c) shows the result of se-
lecting the cluster labeled ?solar electrical system? 
(second from the bottom in Figure 3(b)), which 
contains two results. The summary contains the 5 
top-ranked sentences from the two documents, pre-
sented in document order. In addition, the sum-
mary includes two hyperlinks to the two full texts 
for further inspection. 
(a) Partial list of the news articles linked to Sony Pictures 
Entertainment 
(b) Clustering results relevant to Sony Pictures Entertainment 
(c) Summarization from the news articles in cluster Solar 
electricity system 
Figure 3. Document clustering and summarization for news 
relevant to Sony Picture Entertainment 
The summary illustrates some of the strengths but 
also the shortcomings of the current system. It is 
clearly about a solar energy system installed in 
2007 on top of the Jimmy Stewart Building by EI 
Solutions. This is enough detail for a user to de-
termine whether or not to read the texts any fur-
ther. However, two of the extracted sentences are 
not satisfactory: sentence 2 is broken off and sen-
tence 3 should not be part of the news text at all. 
Premature sentence breaks result from inadequate 
punctuation and line break processing, which is 
still a research problem exacerbated by the com-
plexity of web pages. 
By showing the summary results, we merely dem-
onstrate the improvement on browsability of the 
search system. We are relatively satisfied with the 
results. While the summaries are not always very 
good, they are uniformly understandable and com-
pletely adequate to prove that one can combine 
geospatial information access and text summariza-
tion in a usable and coherent manner.  
Acknowledgments 
Thanks to Geosemble Technologies for providing 
support of the geospatial information system.   
References  
Yih-Farn Robin Chen, Giuseppe Di Fabbrizio, David 
Gibbon, Serban Jora,  Bernard Renger and Bin Wei. 
Geotracker: Geospatial and temporal rss navigation. 
In WWW ?07: Proceedings of the 16th International 
Conference on World Wide Web, 2007. 
Marti A. Hearst. TexTiling: Segmenting text into multi-
paragraph subtopic passages. Computational Linguis-
tics, 23(1):33?64, 1997. 
Chin-Yew Lin and Eduard Hovy. The automated acqui-
sition of topic signatures for text summarization. In
Proceedings of the 18th Conference on Computation-
al Linguistics, 2000. 
Chin-Yew Lin and Eduard Hovy. From single to multi-
document summarization: A prototype system and its 
evaluation. In ACL ?02: Proceedings of the 40th An-
nual Meeting on Association for Computational Lin-
guistics, 2001. 
Stanislaw Osinski and Dawid Weiss. Carrot2: Design of 
a flexible and efficient web information retrieval 
framework. In AWIC, 2005. 
Benjamin E. Teitler, Michael D. Lieberman, Daniele 
Panozzo, Jagan Sankaranarayanan, Hanan Samet and 
Jon Sperling. Newsstand: a new view on news. In
GIS ?08: Proceedings of the 16th ACM SIGSPATIAL 
international conference on Advances in geographic 
information systems, 2008. 
8
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 646?655,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Structured Event Retrieval over Microblog Archives
Donald Metzler, Congxing Cai, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
Abstract
Microblog streams often contain a consider-
able amount of information about local, re-
gional, national, and global events. Most ex-
isting microblog search capabilities are fo-
cused on recent happenings and do not provide
the ability to search and explore past events.
This paper proposes the problem of structured
retrieval of historical event information over
microblog archives. Rather than retrieving in-
dividual microblog messages in response to an
event query, we propose retrieving a ranked
list of historical event summaries by distill-
ing high quality event representations using
a novel temporal query expansion technique.
The results of an exploratory study carried
out over a large archive of Twitter messages
demonstrates both the value of the microblog
event retrieval task and the effectiveness of our
proposed search methodologies.
1 Introduction
Real-time user generated content is one of the key
driving forces behind the growing popularity of so-
cial media-centric communication. The ability to in-
stantly share, often from your mobile phone, your
thoughts (via Twitter), your photos (via Facebook),
your location (via Foursquare), and a variety of other
information is changing the way that information is
created, communicated, and consumed.
There has been a substantial amount of research
effort devoted to user generated content-related
search tasks, including blog search, forum search,
and community-based question answering. How-
ever, there has been relatively little research on mi-
croblog search. Microblog services, such as Tumblr
and Twitter, provide users with the ability to broad-
cast short messages in real-time. This is in contrast
to traditional blogs that typically have considerably
more content that is updated less frequently. By
their very nature, microblog streams often contain
a considerable amount of information about local,
regional, national, and global news and events. A
recent study found that over 85% of trending topics
on Twitter are news-related (Kwak et al, 2010). An-
other recent study by Teevan et al that investigated
the differences between microblog and Web search
reported similar findings (Teevan et al, 2011). The
study also found that microblog search queries are
used to find information related to news and events,
while Web search queries are more navigational in
nature and used to find a variety of information on a
specific topic.
It is likely that microblogs have not received much
attention because, unlike blog search, there is no
well-defined microblog search task. Existing mi-
croblog search services, such as those offered by
Twitter and Google, only provide the ability to re-
trieve individual microblog posts in response to a
query. Unfortunately, this task has limited utility
since very few real information needs can be satis-
fied by a single short piece of text (e.g., the max-
imum length of a message on Twitter is 140 char-
acters). Hence, novel search tasks defined over mi-
croblog streams that go beyond ?message retrieval?
have the potential to add substantial value to users.
Given the somewhat limited utility of microblog
message search and the preponderance of news and
event-related material posted on microblogs, this pa-
646
July 16 2010 at 17 UTC, for 11 hours
Summary tweets:
i. Ok a 3.6 ?rocks? nothing. But boarding a plane
there now, Woodward ho! RT @todayshow: 3.6 mag-
nitude #earthquake rocks Washington DC area.
ii. RT @fredthompson: 3.6-magnitude earthquake hit
DC. President Obama said it was due to 8 years of
Bush failing to regulate plate tectonic ...
iii. 3.6-magnitude earthquake wakes Md. residents:
Temblor centered in Gaithersburg felt by as many as
3 million people... http://bit.ly/9iMLEk
Figure 1: Example structured event representation re-
trieved for the query ?earthquake?.
per proposes a novel search task that we call mi-
croblog event retrieval. Given a query that describes
an event, such as earthquake, terrorist bombing, or
bieber concert, the goal of the task is to retrieve a
ranked list of structured event representations, such
as the one shown in Figure 1, from a large archive of
historical microblog posts.
In this work, structured representations come in
the form of a list of timespans during which an in-
stance of the event occurred and was actively dis-
cussed within the microblog stream. Additionally,
for each timespan, a small set of relevant messages
are retrieved for the purpose of providing a high-
level summary of the event that occurred during the
timespan. This task leverages the large amount of
real-time, often first-hand information found in mi-
croblog archives to deliver a novel form of user gen-
erated content-based search results to users. Unlike
news search, which finds professionally written ar-
ticles on a news-related topic, and general-purpose
Web search, which is likely to find a large amount
of unrelated information, this task is designed to re-
trieve highly relevant news and event-related infor-
mation viewed through the lens of users who ex-
perienced or discussed the event while it happened
(or during its aftermath). Such search functional-
ity would not only be useful for everyday end-users,
but also social scientists, historians, journalists, and
emergency planners.
This paper has three primary contributions. First,
we introduce the microblog event retrieval task,
which retrieves a ranked list of structured event rep-
resentations in response to an event query. By going
beyond individual microblog message retrieval, the
task adds value to microblog archives and provides
users with the ability to find information that was
disseminated in real-time about past events, which
is not possible with news and Web search engines.
Second, we propose an unsupervised methodology
for distilling high quality event representations using
a novel temporal query expansion technique. The
technique synthesizes ideas from pseudo-relevance
feedback, term burstiness, and temporal aspects
of microblog streams. Third, we perform an ex-
ploratory evaluation of 50 event queries over a cor-
pus of 46 million Twitter messages. The results of
our evaluation demonstrate both the value of the mi-
croblog event retrieval task itself and the effective-
ness of our proposed search methodologies, which
show improvements of up to 42% compared to a
baseline approach.
2 Related Work
There are several directions of microblog research
that are related to our proposed work. First, there is
a growing body of literature that has focused on the
topical content of microblog posts. This research
has focused on microblog topic models (Hong and
Davison, 2010), event and topic detection and track-
ing (Sankaranarayanan et al, 2009; Cataldi et al,
2010; Petrovic? et al, 2010; Lin et al, 2010), predict-
ing flu outbreaks using keyword tracking (Culotta,
2010), and using microblog streams as a source
of features for improving recency ranking in Web
search (Dong et al, 2010). Most of these approaches
analyze content as it arrives in the system. While
tracking a small number of topics or keywords is fea-
sible using online algorithms, the general problem
of topic detection and tracking (Allan et al, 1998) is
considerably more challenging given the large num-
ber of topics being discussed at any one point. Our
work differs in that it does not attempt to track or
model topics as they arrive in the system. Instead,
given an event query, our system retrospectively an-
alyzes the corpus of microblog messages for the pur-
pose of retrieving structured event representations.
There is no shortage of previous work on using
pseudo-relevance feedback approaches for query ex-
pansion. Relevant research includes classical vector-
space approaches (Rocchio, 1971), language mod-
647
eling approaches (Lavrenko and Croft, 2001; Zhai
and Lafferty, 2001; Li and Croft, 2003), among oth-
ers (Metzler and Croft, 2007; Cao et al, 2008; Lv
and Zhai, 2010). The novel aspect of our proposed
temporal query expansion approach is the fact that
expansion is done over a temporal stream of very
short, noisy messages.
There has also been recent work on summarizing
sets of microblog posts (Sharifi et al, 2010). We
chose to make use of a simple approach in favor of
a more sophisticated one because summarization is
only a minor aspect of our proposed framework.
Finally, there are two previous studies that are
the most relevant to our work. First, Massoudi et
al. propose a retrieval model that uses query ex-
pansion and microblog quality indicators to retrieve
individual microblog messages (Massoudi et al,
2011). Their proposed query expansion approach
differs from ours in the sense that we utilize times-
pans from the (possibly distant) past when generat-
ing expanded queries and focus on event retrieval,
rather than individual message retrieval. The other
research that is closely related to ours is the work
done by Chieu and Lee (Chieu and Lee, 2004). The
authors propose an approach for automatically con-
structing timelines from news articles in response
to a query. The novelty of our proposed work de-
rives from our novel temporal query expansion ap-
proach, and the fact that our work focuses on mi-
croblog streams which are fundamentally different
in nature from news articles.
3 Microblog Event Retrieval
The primary goal of this paper is to introduce a new
microblog search paradigm that goes beyond retriev-
ing messages individually. We propose a novel task
called microblog event retrieval, which is defined as
follows. Given a query that specifies an event, re-
trieve a set of relevant structured event representa-
tions from a large archive of microblog messages.
This definition is purposefully general to allow for a
broad interpretation of the task.
There is nothing in our proposed retrieval frame-
work that precludes it from producing reasonable re-
sults for any type of query, not just those related to
events. However, we chose to primarily focus on
events in this paper because previous studies have
shown that a majority of trending topics within mi-
croblog streams are about news and events (Kwak et
al., 2010). The information found in microblogs is
difficult to find anywhere else, including news and
Web archives, thereby making it a valuable resource
for a wide variety of users.
3.1 Overview of Framework
Our microblog event retrieval framework takes a
query as input and returns a ranked list of struc-
tured event representations. To accomplish this, the
framework breaks the work into two steps ? times-
pan retrieval and summarization. The timespan re-
trieval step identifies the timespans when the event
happened, while the summarization step retrieves
a small set of microblog messages for each times-
pan that are meant to act as a summary. Figure 1
shows an example result that is returned in response
to the query ?earthquake?. The result consists of a
start time that indicates when the event began be-
ing discussed, a duration that specifies how long the
event was discussed, and a small number of mes-
sages posted during the time interval that are meant
to summarize what happened. This example corre-
sponds to an earthquake that struck the metropoli-
tan District of Colombia area in the United States.
The earthquake was heavily discussed for nearly 11
hours, because it hit a densely populated area that
does not typically experience earthquakes.
3.2 Temporal Query Expansion
We assume that queries issued to our retrieval frame-
work are simple keyword queries that consist of a
small number of terms. This sparse representation
of the user?s information need makes finding rel-
evant messages challenging, since microblog mes-
sages that are highly related to the query might not
contain any of the query keywords. It is common for
microblog messages about a given topic to express
the topic in a different, possibly shortened or slang,
manner. For example, rather than writing ?earth-
quake?, users may instead use the word ?quake? or
simply include a hashtag such as ?#eq? in their mes-
sage. It is impractical to manually identify the full
set of related keywords and folksonomy tags (i.e.,
hashtags) for each query. In information retrieval,
this is known as the vocabulary mismatch problem.
To address this problem, we propose a novel unsu-
648
pervised temporal query expansion technique. The
approach is unsupervised in the sense that it makes
use of a pseudo-relevance feedback-like mechanism
when extracting expansion terms. Traditional query
expansion approaches typically find terms that com-
monly co-occur with the query terms in documents
(or passages). However, such approaches are not
suitable for expanding queries in the microblog set-
ting since microblog messages are very short, yield-
ing unreliable co-occurrence information. Further-
more, microblog messages have an important tem-
poral dimension that should be considered when
they are being used to generate expansion terms.
Our proposed approach generates expansion
terms based on the temporal co-occurrence of terms.
Given keyword query q, we first automatically re-
trieve a set of N timespans for which the query key-
words were most heavily discussed. To do so, we
rank timespans according to the proportion of mes-
sages posted during the timespan that contain one
or more of the query keywords. This is a simple,
but highly reliable way of identifying timespans dur-
ing which a specific topic is being heavily discussed.
These timespans are then considered to be pseudo-
relevant. In our experiments, the microblog stream
is divided into hours, with each hour corresponding
to an atomic timespan. Although it is possible to
define timespans in many different ways, we found
that this was a suitable level of granularity for most
events that was neither overly broad nor overly spe-
cific.
For each pseudo-relevant timespan, a burstiness
score is computed for all of the terms that occur in
messages posted during the timespan. The bursti-
ness score is meant to quantify how trending a term
is during the timespan. Thus, if the query is be-
ing heavily discussed during the timespan and some
term is also trending during the timespan, then the
term may be related to the query. For each of the top
N time intervals, the burstiness score of each term
is computed as follows:
burstiness(w, TSi) =
P (w|TSi)
P (w)
(1)
which is the ratio of the term?s likelihood of occur-
ring within timespan TSi versus the likelihood of
the term occurring during any timespan. Hence, if
a term that generally infrequently occurs within the
message stream suddenly occurs many times within
a single time interval, then the term will be assigned
a high burstiness score. This weighting is similar in
nature to that proposed by Ponte for query expansion
within the language modeling framework for infor-
mation retrieval (Ponte, 1998). The following prob-
ability estimates are used for the expressions within
the burstiness score:
P (w|TSi) =
tfw,TSi + ?
tfw
N
|TSi|+ ?
, P (w) =
tfw +K
N +K|V |
where tfw,TSi is the number of occurrences of w in
timespan TSi, tfw is the number of occurrences of
w in the entire microblog archive, |TSi| is the num-
ber of terms in timespan TSi, N is the total number
of terms in the microblog archive, V is the vocabu-
lary size, and ? and K are smoothing parameters.
While it is common practice to smooth P (w|TSi)
using Dirichlet (or Bayesian) smoothing (Zhai and
Lafferty, 2004), it is less common to smooth the gen-
eral English language model P (w). However, we
found that this was necessary since term distribu-
tions in microblog services exhibit unique character-
istics. By smoothing P (w), we dampen the effect of
overweighting very rare terms. In our experiments,
we set the value of ? to 500 and K to 10 after some
preliminary exploration. We found that the overall
system effectiveness is generally insensitive to the
choice of smoothing parameters.
The final step of the query expansion process
involves aggregating the burstiness scores across
all pseudo-relevant timespans to generate an over-
all score for each term. To do so, we compute
the geometric mean of the burstiness scores across
the pseudo-relevant timespans. Preliminary experi-
ments showed that the arithmetic mean was suscep-
tible to overweighting terms that had a very large
burstiness score in a single timespan. By utiliz-
ing the geometric average instead, we ensure that
the highest weighted terms are those that have large
weights in a large number of the timespans, thereby
eliminating spurious terms. Seo and Croft (2010)
observed similar results with traditional pseudo-
relevance feedback techniques.
The k highest weighted terms are then used as
expansion terms. Using this approach, terms that
commonly trend during the same timespans that
649
the query terms commonly occur (i.e., the pseudo-
relevant timespans) are assigned high weights.
Hence, the approach is capable of capturing sim-
ple temporal dependencies between terms and query
keywords, which is not possible with traditional ap-
proaches.
3.3 Timespan Ranking
The end result of the query expansion process just
described is an expanded query q? that consists of a
set of k terms and their respective weights (denoted
as ?w). Our framework uses the expanded query q?
to retrieve relevant timespans. We hypothesize that
using the expanded version of the query for timespan
retrieval will yield significantly better results than
using the keyword version.
To retrieve timespans, we first identify the 1000
highest scoring timespans (with respect to q?). We
then merge contiguous timespans into a single,
longer timespan, where the score of the merged
timespan is the maximum score of its component
timespans. The final ranked list consists of the
merged timespans. Therefore, although our times-
pans are defined as hour intervals, it is possible for
our system to return longer (merged) timespans.
We now describe two scoring functions that can
be used to compute the relevance of a timespan with
respect to an expanded query representation.
3.3.1 Coverage Scoring Function
The coverage scoring function measures rele-
vance as the (weighted) number of expansion terms
that are covered within the timespan. This measure
assumes that the expanded query is a faithful repre-
sentation of the information need and that the more
times the highly weighted expansion terms occur,
the more relevant the timespan is. Using this defi-
nition, the coverage score of a time interval is com-
puted as:
s(q?, TS) =
?
w?q?
?w ? tfw,TS
where tfwi,TS is the term frequency of wi in times-
pan TS and ?w is the expansion weight of term w.
3.3.2 Burstiness Scoring Function
Since multiple events may occur at the same time,
microblog streams can easily be dominated by the
larger of two events. However, less popular events
may also exhibit burstiness at the same time. There-
fore, another measure of relevance is the burstiness
of the event signature during the timespan. If all
of the expansion terms exhibit burstiness during the
time interval, it strongly suggests the timespan may
be relevant to the query.
Therefore, to measure the relevance of the times-
pan, we first compute the burstiness scores for all of
the terms within the time interval. This yields a vec-
tor ?TS of burstiness scores. The cosine similarity
measure is used to compute the similarity between
the query burstiness scores and the timespan bursti-
ness scores. Hence, the burstiness scoring function
is computed as:
s(q?, TS) = cos(?q? , ?TS)
3.4 Timespan Summarization
The final step of the retrieval process is to produce
a short query-biased summary for each retrieved
time interval. The primary purpose for generating
this type of summary is to provide the user with
a quick overview of what happened during the re-
trieved timespans.
We utilize a simple, straightforward approach that
generates unexpectedly useful summaries. Given a
timespan, we use a relatively simple information re-
trieval model to retrieve a small set of microblog
messages posted during the timespan that are the
most relevant to the expanded representation of the
original query. These messages are then used as a
short summary of the timespan.
This is accomplished by scoring a microblog mes-
sageM with respect to an expanded query represen-
tation q? using a weighted variant of the query like-
lihood scoring function (Ponte and Croft, 1998):
s(q?,M) =
?
w?q?
?w ? logP (w|M)
where ?w is the burstiness score for expansion term
w and P (w|M) is a Dirichlet smoothed language
modeling estimate for term w in message M . This
scoring function is also equivalent to the cross en-
tropy and KL-divergence scoring functions (Lafferty
and Zhai, 2001).
650
Category Events
Business layoffs, bankruptcy, acquisition,
merger, hostile takeover
Celebrity wedding, divorce
Crime shooting, robbery, assassination,
court decision, school shooting
Death death, suicide, drowned
Energy blackout, brownout
Entertainment awards, championship game,
world record
Health recall, pandemic, disease, flu,
poisoning
Natural Disaster hurricane, tornado, earthquake,
flood, tsunami, wildfire, fire
Politics election, riots, protests
Terrorism hostage, explosion, terrorism,
bombing, terrorist attack, suicide
bombing, hijacked
Transportation plane crash, traffic jam, sinks,
pileup, road rage, train crash, de-
railed, capsizes
Table 1: The 50 event types used as queries during our
evaluation, divided into categories.
4 Experiments
This section describes our empirical evaluation of
the proposed microblog event retrieval task.
4.1 Microblog Corpus
Our microblog message archive consists of data
that we collected from Twitter using their Stream-
ing API. The API delivers a continuous 1% ran-
dom sample of public Twitter messages (also called
?tweets?). Our evaluation makes use of data col-
lected between July 16, 2010 and Jan 1st, 2011. Af-
ter eliminating all non-English tweets, our corpus
consists of 46,611,766 English tweets, which corre-
sponds to roughly 10,000 tweets per hour. Although
this only represents a 1% sample of all tweets, we
believe that the corpus is sizable enough to demon-
strate the utility of our proposed approach.
4.2 Event Queries
To evaluate our system, we prepared a list of 50
event types that fall into 11 different categories.
The event types and their corresponding categories
are listed in Table 1. The different event types
can have substantially different characteristics, such
as the frequency of occurrence, geographic or de-
mographic interest, popularity, etc. For example,
there are more weddings than earthquakes. Pub-
lic events, such as federal elections involve people
across the country. However, a car pileup typically
only attracts local attention. Moreover, microblog-
gers show different amounts of interest to each type
of event. For example, Twitter users are more likely
to tweet about politics than a business acquisition.
4.3 Methodology
To evaluate the quality of a particular configuration
of our framework, we run the microblog event re-
trieval task for the 50 different event type queries de-
scribed in the previous section. For each query, the
top 10 timespans retrieved are manually judged to
be relevant or non-relevant. If the summary returned
clearly indicated a real event instance occurred, then
the timespan was marked as relevant. The primary
metric of interest is precision at 10.
In addition to the temporal query expansion
approach (denoted TQE), we also ran exper-
iments using relevance-based language models,
which is a state-of-the-art query expansion ap-
proach (Lavrenko and Croft, 2001). We ran two
variants of relevance-based language models. In
the first, query expansion was done using the Twit-
ter corpus itself (denoted TwitterRM). This allows
us to compare the effectiveness of the TQE ap-
proach against a more traditional query expansion
approach. In the other variant, query expansion was
done using the English Gigaword corpus (denoted
NewsRM), which is a rich source of event informa-
tion created by traditional news media.
For all three query expansion approaches (TQE,
TwitterRM, and NewsRM), the two scoring func-
tions, burstiness and coverage, are used to rank
timespans. Hence, we evaluate six specific instances
of our framework. As a baseline, we use a sim-
ple (unexpanded) keyword retrieval approach that
scores timespans according to the relative frequency
of event keywords that occur during the timespan.
4.4 Timespan Retrieval Results
Before delving into the details of our quantitative
evaluation of effectiveness, we provide an illustra-
tive example of the type of results our system is ca-
pable of producing. Table 2 shows the top four re-
651
July 16 2010 at 17 UTC, for 11 hours
Ok a 3.6 ?rocks? nothing. But boarding a plane there
now, Woodward ho! RT @todayshow: 3.6 magnitude
#earthquake rocks Washington DC area.
September 28 2010 at 11 UTC, for 6 hours
RT @Quakeprediction: 2.6 earthquake
(possible foreshock) hits E of Los Ange-
les; http://earthquake.usgs.gov/
earthquakes/recenteqscanv/Fau ...
September 04 2010 at 01 UTC, for 3 hours
7.0 quake strikes New Zealand - A 7.0-magnitude
earthquake has struck near New Zealand?s second
largest city. Reside... http://ht.ly/18R2rw
October 27 2010 at 01 UTC, for 5 hours
RT @SURFER Magazine: Tsunami Strikes
Mentawais: Wave Spawned By A 7.5-Magnitude
Earthquake Off West Coast Of Indonesia
http://bit.ly/8Z9Lbv
Table 2: Top four timespans (with a single summary
tweet) retrieved for the query ?earthquake?.
sults retrieved using temporal query expansion with
the burstiness scoring function for the query ?earth-
quake?. Only a single summary tweet is displayed
for each timespan due to space restrictions. As we
can see from the tweets, all of the results are rele-
vant to the query, in that they all correspond to times
when an earthquake happened and was actively dis-
cussed on Twitter. Different from Web and news
search results, these types of ranked lists provide a
clear temporal picture of relevant events that were
actively discussed on Twitter.
The results of our microblog retrieval task are
shown in Table 3. The table reports the per-category
and overall precision at 10 for the baseline, and
the six configurations of our proposed framework.
Bolded values represent the best result per category.
As the results show, using temporal query expan-
sion with burstiness ranking yields a mean preci-
sion at 10 of 61%, making it the best overall sys-
tem configuration. The approach is 41.9% better
than the baseline, which is statistically significant
according to a one-sided paired t-test at the p < 0.01
level. Interestingly, the relevance model-based ex-
pansion techniques exhibit even worse performance,
on average, than our simple keyword baseline. For
example, the news-based expansion approach was
11.6% worse using the coverage scoring function
and 18.6% worse using the burstiness scoring func-
tion compared to the baseline. All of the traditional
query expansion results are statistically significantly
worse than the temporal query expansion-based ap-
proaches. Hence, the results suggest that capturing
temporal dependencies between terms yields bet-
ter expanded representations than simply capturing
term co-occurrences, as is done in traditional query
expansion approaches.
The results also indicate the burstiness scoring
function outperforms the coverage scoring function
for temporal query expansion. An analysis of the
results revealed that in many cases the timespans
returned using the coverage scoring function had a
small number of frequent terms that matched the ex-
panded query. This happened less often with the
burstiness scoring function, which is based on the
cosine similarity between the query and timespan?s
burstiness scores. The combination of burstiness
weighting and l2 normalization (when computing
the cosine similarity) appears to yield a more robust
scoring function.
4.5 Event Popularity Effects
It is also interesting to note that the retrieval perfor-
mance varies substantially across the different event
type categories. For example, the performance on
queries about ?natural disasters? and ?politics? is
consistently strong. Similar performance can also
be achieved for popular events related to celebri-
ties. However, energy-related event queries, such as
?blackout?, achieves very poor effectiveness. This
observation seems to suggest that the more popu-
lar an event is, the better the retrieval performance
that can be achieved. This is a reasonable hypothe-
sis since the more people tweet about the event, the
easier it is to identify the trend from the background.
To better understand this phenomenon, we com-
pute the correlation between timespan retrieval pre-
cision and event (query) popularity, where popular-
ity is measured according to:
Popularity(q) =
1
N
N?
i=1
burstiness(q, TSi),
where q is the event query, burstiness(q, TSi) is
the burstiness score of the event during timespan
652
Event Category Baseline
NewsRM TwitterRM TQE
burst cover burst cover burst cover
Business 0.50 0.46 0.30 0.70 0.18 0.74 0.64
Celebrity 0.75 0.30 0.40 0.50 0.60 0.80 0.45
Crime 0.44 0.28 0.54 0.22 0.32 0.46 0.28
Death 0.43 0.20 0.33 0.30 0.30 0.47 0.47
Energy 0.05 0.10 0.05 0.20 0.05 0.15 0.00
Entertainment 0.47 0.53 0.67 0.30 0.53 0.70 0.70
Health 0.48 0.28 0.36 0.44 0.16 0.60 0.60
Nat. Disaster 0.50 0.53 0.59 0.66 0.46 0.87 0.66
Politics 0.67 0.70 0.53 0.63 0.30 0.87 0.60
Terrorism 0.41 0.44 0.39 0.39 0.17 0.69 0.51
Transportation 0.21 0.08 0.08 0.08 0.10 0.31 0.19
All 0.43 0.35 0.38 0.40 0.26 0.61 0.47
Table 3: Per-category and overall (All) precision at 10 for the keyword only approach (Baseline), traditional newswire
expansion (NewsRM), traditional pseudo relevance feedback using the Twitter corpus (TwitterRM), and tempo-
ral query expansion (TQE). For the expansion-based approaches, results for the burstiness scoring (burst) and the
coverage-based scoring (cover) are given. Bold values indicate the best result per category.
Correlation
Baseline 0.63 (p < 0.01)
NewsRM 0.53 (p < 0.01)
TwitterRM 0.61 (p < 0.01)
TQE 0.50 (p < 0.01)
Table 4: Spearman rank correlation between event re-
trieval precisions and event popularity. All methods use
the burstiness scoring function.
TSi, as defined in Equation 1, and the sum goes over
the topN timespans retrieved for the event using our
proposed retrieval approach.
Using this measure, we find that Twitter users are
more interested in events related to entertainment
and politics, and less interested in events related to
energy or transportation. Also, we notice that Twit-
ter users actively discuss dramatic crisis-related top-
ics, including natural disasters (e.g., earthquakes,
hurricanes, tornado, etc.) and terrorist attacks.
Table 4 shows the correlations between effec-
tiveness and event popularity across different ap-
proaches. The correlations indicate a strong cor-
relation with event popularity for the keyword ap-
proach. This is expected, since the approach is based
on the number of times the keywords are mentioned
within the timespan. The correlations are signif-
icantly reduced by incorporating query expansion
terms. The configurations that use temporal query
expansion tend to have lower correlation than the
other approaches. Although the correlation is still
significant, the lower correlation suggests that tem-
poral query expansion approaches are more robust to
popularity effects than simple keywords approaches.
Additional work is necessary to better understand
the role of popularity in retrieval tasks like this.
5 Conclusions
In this paper, we proposed a novel microblog search
task called microblog event retrieval. Unlike previ-
ous microblog search tasks that retrieve individual
microblog messages, our task involves the retrieval
of structured event representations during which an
event occurs and is discussed within the microblog
community. In this way, users are presented with a
ranked list or timeline of event instances in response
to a query.
To tackle the microblog search task, we proposed
a novel timespan retrieval framework that first con-
structs an expanded representation of the incoming
query, performs timespan retrieval, and then pro-
duces a short summary of the timespan. Our experi-
mental evaluation, carried out over a corpus of over
46 million microblog messages collected from Twit-
ter, showed that microblog event retrieval is a feasi-
ble, challenging task, and that our proposed times-
pan retrieval framework is both robust and effective.
653
References
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic De-
tection and Tracking Pilot Study. In In Proceedings of
the DARPA Broadcast News Transcription and Under-
standing Workshop, pages 194?218.
Guihong Cao, Jian-Yun Nie, Jianfeng Gao, and Stephen
Robertson. 2008. Selecting good expansion terms for
pseudo-relevance feedback. In Proc. 31st Ann. Intl.
ACM SIGIR Conf. on Research and Development in In-
formation Retrieval, SIGIR ?08, pages 243?250, New
York, NY, USA. ACM.
Mario Cataldi, Luigi Di Caro, and Claudio Schifanella.
2010. Emerging topic detection on twitter based on
temporal and social terms evaluation. In Proceedings
of the Tenth International Workshop on Multimedia
Data Mining, MDMKDD ?10, pages 4:1?4:10, New
York, NY, USA. ACM.
Hai Leong Chieu and Yoong Keok Lee. 2004. Query
based event extraction along a timeline. In Proceed-
ings of the 27th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, SIGIR ?04, pages 425?432, New York, NY,
USA. ACM.
Aron Culotta. 2010. Towards detecting influenza epi-
demics by analyzing twitter messages. In 1st Work-
shop on Social Media Analytics (SOMA?10), July.
Anlei Dong, Ruiqiang Zhang, Pranam Kolari, Jing
Bai, Fernando Diaz, Yi Chang, Zhaohui Zheng, and
Hongyuan Zha. 2010. Time is of the essence: im-
proving recency ranking using twitter data. In Pro-
ceedings of the 19th international conference on World
wide web, WWW ?10, pages 331?340, New York, NY,
USA. ACM.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in twitter. In 1st Workshop on
Social Media Analytics (SOMA?10), July.
Haewoon Kwak, Changhyun Lee, Hosung Park, and Sue
Moon. 2010. What is twitter, a social network or
a news media? In Proceedings of the 19th inter-
national conference on World wide web, WWW ?10,
pages 591?600, New York, NY, USA. ACM.
J. Lafferty and C. Zhai. 2001. Document language mod-
els, query models, and risk minimization for informa-
tion retrieval. In Proc. 24th Ann. Intl. ACM SIGIR
Conf. on Research and Development in Information
Retrieval, pages 111?119.
V. Lavrenko and W. B. Croft. 2001. Relevance-based
language models. In Proc. 24th Ann. Intl. ACM SI-
GIR Conf. on Research and Development in Informa-
tion Retrieval, pages 120?127.
Xiaoyan Li and W. Bruce Croft. 2003. Time-based lan-
guage models. In Proc. 12th Intl. Conf. on Information
and Knowledge Management, CIKM ?03, pages 469?
475, New York, NY, USA. ACM.
Cindy Xide Lin, Bo Zhao, Qiaozhu Mei, and Jiawei Han.
2010. Pet: a statistical model for popular events track-
ing in social communities. In Proc. 16th Ann. Intl.
ACM SIGKDD Conf. on Knowledge Discovery and
Data Mining, KDD ?10, pages 929?938, New York,
NY, USA. ACM.
Yuanhua Lv and ChengXiang Zhai. 2010. Positional rel-
evance model for pseudo-relevance feedback. In Proc.
33rd Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, SIGIR ?10, pages
579?586, New York, NY, USA. ACM.
Kamran Massoudi, Manos Tsagkias, Maarten de Rijke,
and Wouter Weerkamp. 2011. Incorporating query ex-
pansion and quality indicators in searching microblog
posts. In Proc. 33rd European Conf. on Information
Retrieval, page To appear.
Donald Metzler and W. Bruce Croft. 2007. Latent con-
cept expansion using markov random fields. In Proc.
30th Ann. Intl. ACM SIGIR Conf. on Research and De-
velopment in Information Retrieval, SIGIR ?07, pages
311?318, New York, NY, USA. ACM.
Sas?a Petrovic?, Miles Osborne, and Victor Lavrenko.
2010. Streaming first story detection with applica-
tion to twitter. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
HLT ?10, pages 181?189, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
J. Ponte and W. Bruce Croft. 1998. A language modeling
approach to information retrieval. In Proc. 21st Ann.
Intl. ACM SIGIR Conf. on Research and Development
in Information Retrieval, pages 275?281.
Jay Ponte. 1998. A Language Modeling Approach to In-
formation Retrieval. Ph.D. thesis, University of Mas-
sachusetts, Amherst, MA.
J. J. Rocchio, 1971. Relevance Feedback in Information
Retrieval, pages 313?323. Prentice-Hall.
Jagan Sankaranarayanan, Hanan Samet, Benjamin E.
Teitler, Michael D. Lieberman, and Jon Sperling.
2009. Twitterstand: news in tweets. In Proceedings of
the 17th ACM SIGSPATIAL International Conference
on Advances in Geographic Information Systems, GIS
?09, pages 42?51, New York, NY, USA. ACM.
Jangwon Seo and W. Bruce Croft. 2010. Geometric rep-
resentations for multiple documents. In Proceeding of
the 33rd international ACM SIGIR conference on Re-
search and development in information retrieval, SI-
GIR ?10, pages 251?258, New York, NY, USA. ACM.
Beaux Sharifi, Mark-Anthony Hutton, and Jugal K.
Kalita. 2010. Experiments in microblog summariza-
tion. Social Computing / IEEE International Confer-
654
ence on Privacy, Security, Risk and Trust, 2010 IEEE
International Conference on, 0:49?56.
Jaime Teevan, Daniel Ramage, and Meredith Ringel Mor-
ris. 2011. #twittersearch: A comparison of microblog
search and web search. In WSDM 2011: Fourth Inter-
national Conference on Web Search and Data Mining,
Feb.
ChengXiang Zhai and John Lafferty. 2001. Model-based
feedback in the language modeling approach to infor-
mation retrieval. In Proc. 10th Intl. Conf. on Informa-
tion and Knowledge Management, pages 403?410.
C. Zhai and J. Lafferty. 2004. A study of smoothing
methods for language models applied to information
retrieval. ACM Trans. Inf. Syst., 22(2):179?214.
655
Proceedings of NAACL-HLT 2013, pages 1120?1130,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Learning Whom to Trust with MACE
Dirk Hovy1 Taylor Berg-Kirkpatrick2 Ashish Vaswani1 Eduard Hovy3
(1) Information Sciences Institute, University of Southern California, Marina del Rey
(2) Computer Science Division, University of California at Berkeley
(3) Language Technology Institute, Carnegie Mellon University, Pittsburgh
{dirkh,avaswani}@isi.edu, tberg@cs.berkeley.edu, hovy@cmu.edu
Abstract
Non-expert annotation services like Amazon?s
Mechanical Turk (AMT) are cheap and fast
ways to evaluate systems and provide categor-
ical annotations for training data. Unfortu-
nately, some annotators choose bad labels in
order to maximize their pay. Manual iden-
tification is tedious, so we experiment with
an item-response model. It learns in an un-
supervised fashion to a) identify which an-
notators are trustworthy and b) predict the
correct underlying labels. We match perfor-
mance of more complex state-of-the-art sys-
tems and perform well even under adversarial
conditions. We show considerable improve-
ments over standard baselines, both for pre-
dicted label accuracy and trustworthiness es-
timates. The latter can be further improved
by introducing a prior on model parameters
and using Variational Bayes inference. Ad-
ditionally, we can achieve even higher accu-
racy by focusing on the instances our model is
most confident in (trading in some recall), and
by incorporating annotated control instances.
Our system, MACE (Multi-Annotator Compe-
tence Estimation), is available for download1.
1 Introduction
Amazon?s MechanicalTurk (AMT) is frequently
used to evaluate experiments and annotate data in
NLP (Callison-Burch et al, 2010; Callison-Burch
and Dredze, 2010; Jha et al, 2010; Zaidan and
Callison-Burch, 2011). However, some turkers try to
maximize their pay by supplying quick answers that
have nothing to do with the correct label. We refer to
1Available under http://www.isi.edu/
publications/licensed-sw/mace/index.html
this type of annotator as a spammer. In order to mit-
igate the effect of spammers, researchers typically
collect multiple annotations of the same instance so
that they can, later, use de-noising methods to infer
the best label. The simplest approach is majority
voting, which weights all answers equally. Unfor-
tunately, it is easy for majority voting to go wrong.
A common and simple spammer strategy for cate-
gorical labeling tasks is to always choose the same
(often the first) label. When multiple spammers
follow this strategy, the majority can be incorrect.
While this specific scenario might seem simple to
correct for (remove annotators that always produce
the same label), the situation grows more tricky
when spammers do not annotate consistently, but in-
stead choose labels at random. A more sophisticated
approach than simple majority voting is required.
If we knew whom to trust, and when, we could
reconstruct the correct labels. Yet, the only way
to be sure we know whom to trust is if we knew
the correct labels ahead of time. To address this
circular problem, we build a generative model of the
annotation process that treats the correct labels as
latent variables. We then use unsupervised learning
to estimate parameters directly from redundant
annotations. This is a common approach in the
class of unsupervised models called item-response
models (Dawid and Skene, 1979; Whitehill et al,
2009; Carpenter, 2008; Raykar and Yu, 2012).
While such models have been implemented in
other fields (e.g., vision), we are not aware of their
availability for NLP tasks (see also Section 6).
Our model includes a binary latent variable that
explicitly encodes if and when each annotator is
spamming, as well as parameters that model the
annotator?s specific spamming ?strategy?. Impor-
1120
tantly, the model assumes that labels produced by
an annotator when spamming are independent of
the true label (though, a spammer can still produce
the correct label by chance).
In experiments, our model effectively differenti-
ates dutiful annotators from spammers (Section 4),
and is able to reconstruct the correct label with high
accuracy (Section 5), even under extremely adver-
sarial conditions (Section 5.2). It does not require
any annotated instances, but is capable of including
varying levels of supervision via token constraints
(Section 5.2). We consistently outperform major-
ity voting, and achieve performance equal to that of
more complex state-of-the-art models. Additionally,
we find that thresholding based on the posterior la-
bel entropy can be used to trade off coverage for ac-
curacy in label reconstruction, giving considerable
gains (Section 5.1). In tasks where correct answers
are more important than answering every instance,
e.g., when constructing a new annotated corpus, this
feature is extremely valuable. Our contributions are:
? We demonstrate the effectiveness of our model
on real world AMT datasets, matching the ac-
curacy of more complex state-of-the-art sys-
tems
? We show how posterior entropy can be used to
trade some coverage for considerable gains in
accuracy
? We study how various factors affect perfor-
mance, including number of annotators, anno-
tator strategy, and available supervision
? We provide MACE (Multi-Annotator Compe-
tence Estimation), a Java-based implementa-
tion of a simple and scalable unsupervised
model that identifies malicious annotators and
predicts labels with high accuracy
2 Model
We keep our model as simple as possible so that it
can be effectively trained from data where annotator
quality is unknown. If the model has too many
parameters, unsupervised learning can easily pick
up on and exploit coincidental correlations in the
data. Thus, we make a modeling assumption that
keeps our parameterization simple. We assume that
an annotator always produces the correct label when
N
T
i
M
A
ij
S
ij
T
A
2
C
2
A
3
C
3
A
1
C
1
Figure 1: Graphical model: Annotator j produces
label Aij on instance i. Label choice depends on
instance?s true label Ti, and whether j is spam-
ming on i, modeled by binary variable Sij . N =
|instances|, M = |annotators|.
for i = 1 . . . N :
Ti ? Uniform
for j = 1 . . .M :
Sij ? Bernoulli(1? ?j)
if Sij = 0 :
Aij = Ti
else :
Aij ? Multinomial(?j)
Figure 2: Generative process: see text for descrip-
tion.
he tries to. While this assumption does not reflect
the reality of AMT, it allows us to focus the model?s
power where it?s important: explaining away labels
that are not correlated with the correct label.
Our model generates the observed annotations as
follows: First, for each instance i, we sample the
true label Ti from a uniform prior. Then, for each
annotator j we draw a binary variable Sij from a
Bernoulli distribution with parameter 1 ? ?j . Sij
represents whether or not annotator j is spamming
on instance i. We assume that when an annotator
is not spamming on an instance, i.e. Sij = 0, he
just copies the true label to produce annotation Aij .
If Sij = 1, we say that the annotator is spamming
on the current instance, and Aij is sampled from
a multinomial with parameter vector ?j . Note that
in this case the annotation Aij does not depend on
the true label Ti. The annotations Aij are observed,
1121
while the true labels Ti and the spamming indicators
Sij are unobserved. The graphical model is shown
in Figure 1 and the generative process is described
in Figure 2.
The model parameters are ?j and ?j . ?j specifies
the probability of trustworthiness for annotator j
(i.e. the probability that he is not spamming on
any given instance). The learned value of ?j will
prove useful later when we try to identify reliable
annotators (see Section 4). The vector ?j determines
how annotator j behaves when he is spamming. An
annotator can produce the correct answer even while
spamming, but this can happen only by chance since
the annotator must use the same multinomial param-
eters ?j across all instances. This means that we only
learn annotator biases that are not correlated with
the correct label, e.g., the strategy of the spammer
who always chooses a certain label. This contrasts
with previous work where additional parameters are
used to model the biases that even dutiful annotators
exhibit. Note that an annotator can also choose not
to answer, which we can naturally accommodate be-
cause the model is generative. We enhance our gen-
erative model by adding Beta and Dirichlet priors on
?j and ?j respectively which allows us to incorporate
prior beliefs about our annotators (section 2.1).
2.1 Learning
We would like to set our model parameters to
maximize the probability of the observed data, i.e.,
the marginal data likelihood:
P (A; ?, ?) =
X
T,S
h NY
i=1
P (Ti) ?
MY
j=1
P (Sij ; ?j) ? P (Aij |Sij , Ti; ?j)
i
where A is the matrix of annotations, S is the
matrix of competence indicators, and T is the vector
of true labels.
We maximize the marginal data likelihood using
Expectation Maximization (EM) (Dempster et al,
1977), which has successfully been applied to
similar problems (Dawid and Skene, 1979). We ini-
tialize EM randomly and run for 50 iterations. We
perform 100 random restarts, and keep the model
with the best marginal data likelihood. We smooth
the M-step by adding a fixed value ? to the fractional
counts before normalizing (Eisner, 2002). We find
that smoothing improves accuracy, but, overall,
learning is robust to varying ?, and set ? = 0.1num labels .
We observe, however, that the average annota-
tor proficiency is usually high, i.e., most annota-
tors answer correctly. The distribution learned by
EM, however, is fairly linear. To improve the cor-
relation between model estimates and true annotator
proficiency, we would like to add priors about the
annotator behavior into the model. A straightfor-
ward approach is to employ Bayesian inference with
Beta priors on the proficiency parameters, ?j . We
thus also implement Variational-Bayes (VB) train-
ing with symmetric Beta priors on ?j and symmet-
ric Dirichlet priors on the strategy parameters, ?j .
Setting the shape parameters of the Beta distribution
to 0.5 favors the extremes of the distribution, i.e.,
either an annotator tried to get the right answer, or
simply did not care, but (almost) nobody tried ?a lit-
tle?. With VB training, we observe improved corre-
lations over all test sets with no loss in accuracy. The
hyper-parameters of the Dirichlet distribution on ?j
were clamped to 10.0 for all our experiments with
VB training. Our implementation is similar to John-
son (2007), which the reader can refer to for details.
3 Experiments
We evaluate our method on existing annotated
datasets from various AMT tasks. However, we
also want to ensure that our model can handle
adversarial conditions. Since we have no control
over the factors in existing datasets, we create
synthetic data for this purpose.
3.1 Natural Data
In order to evaluate our model, we use the
datasets from (Snow et al, 2008) that use discrete
label values (some tasks used continuous values,
which we currently do not model). Since they
compared AMT annotations to experts, gold anno-
tations exist for these sets. We can thus evaluate
the accuracy of the model as well as the proficiency
of each annotator. We show results for word sense
disambiguation (WSD: 177 items, 34 annotators),
recognizing textual entailment (RTE: 800 items,
164 annotators), and recognizing temporal relation
(Temporal: 462 items, 76 annotators).
3.2 Synthetic Data
In addition to the datasets above, we generate
synthetic data in order to control for different
1122
factors. This also allows us to create a gold standard
to which we can compare. We generate data sets
with 100 items, using two or four possible labels.
For each item, we generate answers from 20
different annotators. The ?annotators? are functions
that return one of the available labels according
to some strategy. Better annotators have a smaller
chance of guessing at random.
For various reasons, usually not all annotators see
or answer all items. We thus remove a randomly
selected subset of answers such that each item is
only answered by 10 of the annotators. See Figure
3 for an example annotation of three items.
annotators
ite
m
s ? 0 0 1 ? 0 ? ? 0 ?
1 ? ? 0 ? 1 0 ? ? 0
? ? 0 ? 0 1 ? 0 ? 0
Figure 3: Annotations: 10 annotators on three items,
labels {1, 0}, 5 annotations/item. Missing annota-
tions marked ???
3.3 Evaluations
First, we want to know which annotators to trust.
We evaluate whether our model?s learned trustwor-
thiness parameters ?j can be used to identify these
individuals (Section 4).
We then compare the label predicted by our model
and by majority voting to the correct label. The
results are reported as accuracy (Section 5). Since
our model computes posterior entropies for each
instance, we can use this as an approximation for the
model?s confidence in the prediction. If we focus on
predictions with high confidence (i.e., low entropy),
we hope to see better accuracy, even at the price of
leaving some items unanswered. We evaluate this
trade-off in Section 5.1. In addition, we investigate
the influence of the number of spammers and their
strategy on the accuracy of our model (Section 5.2).
4 Identifying Reliable Annotators
One of the distinguishing features of the model
is that it uses a parameter for each annotator to
estimate whether or not they are spamming. Can
we use this parameter to identify trustworthy indi-
viduals, to invite them for future tasks, and block
untrustworthy ones?
RTE Temporal WSD
raw agreement 0.78 0.73 0.81
Cohen?s ? 0.70 0.80 0.13
G-index 0.76 0.73 0.81
MACE-EM 0.87 0.88 0.44
MACE-VB (0.5,0.5) 0.91 0.90 0.90
Table 1: Correlation with annotator proficiency:
Pearson ? of different methods for various data sets.
MACE-VB?s trustworthiness parameter (trained
with Variational Bayes with ? = ? = 0.5) corre-
lates best with true annotator proficiency.
It is natural to apply some form of weighting.
One approach is to assume that reliable annotators
agree more with others than random annotators.
Inter-annotator agreement is thus a good candidate
to weigh the answers. There are various measures
for inter-annotator agreement.
Tratz and Hovy (2010) compute the average
agreement of each annotator and use it as a weight
to identify reliable ones. Raw agreement can be
directly computed from the data. It is related to
majority voting, since it will produce high scores for
all members of the majority class. Raw agreement
is thus a very simple measure.
In contrast, Cohen?s ? corrects the agreement
between two annotators for chance agreement. It
is widely used for inter-annotator agreement in
annotation tasks. We also compute the ? values
for each pair of annotators, and average them for
each annotator (similar to the approach in Tratz and
Hovy (2010)). However, whenever one label is more
prevalent (a common case in NLP tasks), ? overesti-
mates the effect of chance agreement (Feinstein and
Cicchetti, 1990) and penalizes disproportionately.
The G-index (Gwet, 2008) corrects for the number
of labels rather than chance agreement.
We compare these measures to our learned trust-
worthiness parameters ?j in terms of their ability to
select reliable annotators. A better measure should
lend higher score to annotators who answer correctly
more often than others. We thus compare the ratings
of each measure to the true proficiency of each
annotator. This is the percentage of annotated items
the annotator answered correctly. Methods that can
identify reliable annotators should highly correlate
1123
to the annotator?s proficiency. Since the methods
use different scales, we compute Pearson?s ? for the
correlation coefficient, which is scale-invariant. The
correlation results are shown in Table 1.
The model?s ?j correlates much more strongly
with annotator proficiency than either ? or raw
agreement. The variant trained with VB performs
consistently better than standard EM training, and
yields the best results. This show that our model
detects reliable annotators much better than any
of the other measures, which are only loosely
correlated to annotator proficiency.
The numbers for WSD also illustrate the low ?
score resulting when all annotators (correctly) agree
on a small number of labels. However, all inter-
annotator agreement measures suffer from an even
more fundamental problem: removing/ignoring
annotators with low agreement will always improve
the overall score, irrespective of the quality of their
annotations. Worse, there is no natural stopping
point: deleting the most egregious outlier always
improves agreement, until we have only one anno-
tator with perfect agreement left (Hovy, 2010). In
contrast, MACE does not discard any annotators,
but weighs their contributions differently. We are
thus not losing information. This works well even
under adversarial conditions (see Section 5.2).
5 Recovering the Correct Answer
RTE Temporal WSD
majority 0.90 0.93 0.99
Raykar/Yu 2012 0.93 0.94 ?
Carpenter 2008 0.93 ? ?
MACE-EM/VB 0.93 0.94 0.99
MACE-EM@90 0.95 0.97 0.99
MACE-EM@75 0.95 0.97 1.0
MACE-VB@90 0.96 0.97 1.0
MACE-VB@75 0.98 0.98 1.0
Table 2: Accuracy of different methods on data sets
from (Snow et al, 2008). MACE-VB uses Varia-
tional Bayes training. Results @n use the n% items
the model is most confident in (Section 5.1). Results
below double line trade coverage for accuracy and
are thus not comparable to upper half.
The previous sections showed that our model reli-
ably identifies trustworthy annotators. However, we
also want to find the most likely correct answer. Us-
ing majority voting often fails to find the correct la-
bel. This problem worsens when there are more than
two labels. We need to take relative majorities into
account or break ties when two or more labels re-
ceive the same number of votes. This is deeply un-
satisfying.
Figure 2 shows the accuracy of our model on
various data sets from Snow et al (2008). The
model outperforms majority voting on both RTE
and Temporal recognition sets. It performs as well
as majority voting for the WSD task. This last set
is somewhat of an exception, though, since almost
all annotators are correct all the time, so majority
voting is trivially correct. Still, we need to ensure
that the model does not perform worse under such
conditions. The results for RTE and Temporal data
also rival those reported in Raykar and Yu (2012)
and Carpenter (2008), yet were achieved with a
much simpler model.
Carpenter (2008) models instance difficulty as
a parameter. While it seems intuitively useful to
model which items are harder than other, it increases
the parameter space more than our trustworthiness
variable. We achieve comparable performance with-
out modeling difficulty, which greatly simplifies
inference. The model of Raykar and Yu (2012) is
more similar to our approach, in that it does not
model item difficulty. However, it adds an extra step
that learns priors from the estimated parameters. In
our model, this is part of the training process. For
more details on both models, see Section 6.
5.1 Trading Coverage for Accuracy
Sometimes, we want to produce an answer for ev-
ery item (e.g., when evaluating a data set), and some-
times, we value good answers more than answering
all items (e.g., when developing an annotated
corpus). Jha et al (2010) have demonstrated how to
achieve better coverage (i.e., answer more items) by
relaxing the majority voting constraints. Similarly,
we can improve accuracy if we only select high qual-
ity annotations, even if this incurs lower coverage.
We provide a parameter in MACE that allows
users to set a threshold for this trade-off: the
model only returns a label for an instance if it is
sufficiently confident in its answer. We approximate
the model?s confidence by the posterior entropy of
1124
0$&((0
0$&(9%
PDMRULW\
Figure 4: Tradeoff between coverage and accuracy for RTE (left) and temporal (right). Lower thresholds
lead to less coverage, but result in higher accuracy.
each instance. However, entropy depends strongly
on the specific makeup of the dataset (number of
annotators and labels, etc.), so it is hard for the user
to set a specific threshold.
Instead of requiring an exact entropy value, we
provide a simple thresholding between 0.0 and 1.0
(setting the threshold to 1.0 will include all items).
After training, MACE orders the posterior entropies
for all instances and selects the value that covers
the selected fraction of the instances. The threshold
thus roughly corresponds to coverage. It then only
returns answers for instances whose entropy is
below the threshold. This procedure is similar to
precision/recall curves.
Jha et al (2010) showed the effect of varying the
relative majority required, i.e., requiring that at least
n out of 10 annotators have to agree to count an
item. We use that method as baseline comparison,
evaluating the effect on coverage and accuracy
when we vary n from 5 to 10.
Figure 4 shows the tradeoff between coverage
and accuracy for two data sets. Lower thresholds
produce more accurate answers, but result in lower
coverage, as some items are left blank. If we pro-
duce answers for all items, we achieve accuracies
of 0.93 for RTE and 0.94 for Temporal, but by
excluding just the 10% of items in which the model
is least confident, we achieve accuracies as high as
0.95 for RTE and 0.97 for Temporal. We omit the
results for WSD here, since there is little headroom
and they are thus not very informative. Using Varia-
tional Bayes inference consistently achieves higher
results for the same coverage than the standard im-
plementation. Increasing the required majority also
improves accuracy, although not as much, and the
loss in coverage is larger and cannot be controlled.
In contrast, our method allows us to achieve better
accuracy at a smaller, controlled loss in coverage.
5.2 Influence of Strategy, Number of
Annotators, and Supervision
Adverse Strategy We showed that our model
recovers the correct answer with high accuracy.
However, to test whether this is just a function of
the annotator pool, we experiment with varying
the trustworthiness of the pool. If most annotators
answer correctly, majority voting is trivially correct,
as is our model. What happens, however, if more
and more annotators are unreliable? While some
agreement can arise from randomness, majority
voting is bound to become worse?can our model
overcome this problem? We set up a second set of
experiments to test this, using synthetic data. We
choose 20 annotators and vary the amount of good
annotators among them from 0 to 10 (after which
the trivial case sets in). We define a good annotator
as one who answers correctly 95% of the time.2
Adverse annotators select their answers randomly or
always choose a certain value (minimal annotators).
These are two frequent strategies of spammers.
For different numbers of labels and varying
percentage of spammers, we measure the accuracy
2The best annotators on the Snow data sets actually found
the correct answer 100% of the time.
1125
a) random annotators b) minimal annotators
Figure 5: Influence of adverse annotator strategy on label accuracy (y-axis). Number of possible labels
varied between 2 (top row) and 4 (bottom row). Adverse annotators either choose at random (a) or always
select the first label (b). MACE needs fewer good annotators to recover the correct answer.
0$&((0
0$&(9%
PDMRULW\
Figure 6: Varying number of annotators: effect on prediction accuracy. Each point averaged over 10 runs.
Note different scale for WSD.
of our model and majority voting on 100 items,
averaged over 10 runs for each condition. Figure
5 shows the effect of annotator proficiency on both
majority voting and our method for both kinds of
spammers. Annotator pool strategy affects majority
voting more than our model. Even with few good
annotators, our model learns to dismiss the spam-
mers as noise. There is a noticeable point on each
graph where MACE diverges from the majority
voting line. It thus reaches good accuracy much
1126
faster than majority voting, i.e., with fewer good an-
notators. This divergence point happens earlier with
more label values when adverse annotators label
randomly. In general, random annotators are easier
to deal with than the ones always choosing the first
label. Note that in cases where we have a majority
of adversarial annotators, VB performs worse than
EM, since this condition violates the implicit as-
sumptions we encoded with the priors in VB. Under
these conditions, setting different priors to reflect
the annotator pool should improve performance.
Obviously, both of these pools are extremes: it is
unlikely to have so few good or so many malicious
annotators. Most pools will be somewhere in
between. It does show, however, that our model
can pick up on reliable annotators even under very
unfavorable conditions. The result has a practical
upshot: AMT allows us to require a minimum rating
for annotators to work on a task. Higher ratings
improve annotation quality, but delay completion,
since there are fewer annotators with high ratings.
The results in this section suggest that we can find
the correct answer even in annotator pools with low
overall proficiency. We can thus waive the rating
requirement and allow more annotators to work on
the task. This considerably speeds up completion.
Number of Annotators Figure 6 shows the effect
different numbers of annotators have on accuracy.
As we increase the number of annotators, MACE
and majority voting achieve better accuracy results.
We note that majority voting results level or even
drop when going from an odd to an even number.
In these cases, the new annotator does not improve
accuracy if it goes with the previous majority (i.e.,
going from 3:2 to 4:2), but can force an error when
going against the previous majority (i.e., from 3:2 to
3:3), by creating a tie. MACE-EM and MACE-VB
dominate majority voting for RTE and Temporal.
For WSD, the picture is less clear, where majority
voting dominates when there are fewer annotators.
Note that the differences are minute, though (within
1 percentage point). For very small pool sizes (< 3),
MACE-VB outperforms both other methods.
Amount of Supervision So far, we have treated
the task as completely unsupervised. MACE does
not require any expert annotations in order to
achieve high accuracy. However, we often have
annotations for some of the items. These annotated
data points are usually used as control items (by
removing annotators that answer them incorrectly).
If such annotated data is available, we would like
to make use of it. We include an option that lets
users supply annotations for some of the items,
and use this information as token constraints in the
E-step of training. In those cases, the model does
not need to estimate the correct value, but only has
to adjust the trust parameter. This leads to improved
performance.3
We explore for RTE and Temporal how per-
formance changes when we vary the amount of
supervision in increments of 5%.4 We average over
10 runs for each value of n, each time supplying an-
notations for a random set of n items. The baseline
uses the annotated label whenever supplied, other-
wise the majority vote, with ties split at random.
Figure 7 shows that, unsurprisingly, all methods
improve with additional supervision, ultimately
reaching perfect accuracy. However, MACE uses
the information more effectively, resulting in
higher accuracy for a given amount of supervision.
This gain is more pronounced when only little
supervision is available.
6 Related Research
Snow et al (2008) and Sorokin and Forsyth
(2008) showed that Amazon?s MechanicalTurk use
in providing non-expert annotations for NLP tasks.
Various models have been proposed for predicting
correct annotations from noisy non-expert annota-
tions and for estimating annotator trustworthiness.
These models divide naturally into two categories:
those that use expert annotations for supervised
learning (Snow et al, 2008; Bian et al, 2009), and
completely unsupervised ones. Our method falls
into the latter category because it learns from the
redundant non-expert annotations themselves, and
makes no use of expertly annotated data.
Most previous work on unsupervised models
belongs to a class called ?Item-response models?,
used in psychometrics. The approaches differ with
respect to which aspect of the annotation process
3If we had annotations for all items, accuracy would be per-
fect and require no training.
4Given the high accuracy for the WSD data set even in the
fully unsupervised case, we omit the results here.
1127
Figure 7: Varying the amount of supervision: effect on prediction accuracy. Each point averaged over 10
runs. MACE uses supervision more efficiently.
they choose to focus on, and the type of annotation
task they model. For example, many methods ex-
plicitly model annotator bias in addition to annotator
competence (Dawid and Skene, 1979; Smyth et al,
1995). Our work models annotator bias, but only
when the annotator is suspected to be spamming.
Other methods focus modeling power on instance
difficulty to learn not only which annotators are
good, but which instances are hard (Carpenter,
2008; Whitehill et al, 2009). In machine vision,
several models have taken this further by parameter-
izing difficulty in terms of complex features defined
on each pairing of annotator and annotation instance
(Welinder et al, 2010; Yan et al, 2010). While
such features prove very useful in vision, they are
more difficult to define for the categorical problems
common to NLP. In addition, several methods are
specifically tailored to annotation tasks that involve
ranking (Steyvers et al, 2009; Lee et al, 2011),
which limits their applicability in NLP.
The method of Raykar and Yu (2012) is most
similar to ours. Their goal is to identify and filter
out annotators whose annotations are not correlated
with the gold label. They define a function of the
learned parameters that is useful for identifying
these spammers, and then use this function to build
a prior. In contrast, we use simple priors, but incor-
porate a model parameter that explicitly represents
the probability that an annotator is spamming. Our
simple model achieves the same accuracy on gold
label predictions as theirs.
7 Conclusion
We provide a Java-based implementation, MACE,
that recovers correct labels with high accuracy, and
reliably identifies trustworthy annotators. In
addition, it provides a threshold to control the
accuracy/coverage trade-off and can be trained with
standard EM or Variational Bayes EM. MACE
works fully unsupervised, but can incorporate token
constraints via annotated control items. We show
that even small amounts help improve accuracy.
Our model focuses most of its modeling power
on learning trustworthiness parameters, which
are highly correlated with true annotator relia-
bility (Pearson ? 0.9). We show on real-world
and synthetic data sets that our method is more
accurate than majority voting, even under ad-
versarial conditions, and as accurate as more
complex state-of-the-art systems. Focusing on high-
confidence instances improves accuracy consider-
ably. MACE is freely available for download under
http://www.isi.edu/publications/
licensed-sw/mace/index.html.
Acknowledgements
The authors would like to thank Chris Callison-
Burch, Victoria Fossum, Stephan Gouws, Marc
Schulder, Nathan Schneider, and Noah Smith for
invaluable discussions, as well as the reviewers for
their constructive feedback.
1128
References
Jiang Bian, Yandong Liu, Ding Zhou, Eugene Agichtein,
and Hongyuan Zha. 2009. Learning to recognize re-
liable users and content in social media with coupled
mutual reinforcement. In Proceedings of the 18th in-
ternational conference on World wide web, pages 51?
60. ACM.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with amazon?s mechanical
turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics.
Bob Carpenter. 2008. Multilevel Bayesian models of
categorical data annotation. Unpublished manuscript.
A. Philip Dawid and Allan M. Skene. 1979. Maximum
likelihood estimation of observer error-rates using the
EM algorithm. Applied Statistics, pages 20?28.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
Eduard Hovy. 2010. Annotation. A Tutorial. In 48th
Annual Meeting of the Association for Computational
Linguistics.
Mukund Jha, Jacob Andreas, Kapil Thadani, Sara Rosen-
thal, and Kathleen McKeown. 2010. Corpus creation
for new genres: A crowdsourced approach to pp at-
tachment. In Proceedings of the NAACL HLT 2010
Workshop on Creating Speech and Language Data
with Amazon?s Mechanical Turk, pages 13?20. Asso-
ciation for Computational Linguistics.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Michael D. Lee, Mark Steyvers, Mindy de Young, and
Brent J. Miller. 2011. A model-based approach to
measuring expertise in ranking tasks. In L. Carlson,
C. Ho?lscher, and T.F. Shipley, editors, Proceedings of
the 33rd Annual Conference of the Cognitive Science
Society, Austin, TX. Cognitive Science Society.
Vikas C. Raykar and Shipeng Yu. 2012. Eliminating
Spammers and Ranking Annotators for Crowdsourced
Labeling Tasks. Journal of Machine Learning Re-
search, 13:491?518.
Padhraic Smyth, Usama Fayyad, Mike Burl, Pietro Per-
ona, and Pierre Baldi. 1995. Inferring ground truth
from subjective labelling of Venus images. Advances
in neural information processing systems, pages 1085?
1092.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with Amazon Mechanical Turk. In
IEEE Computer Society Conference on Computer Vi-
sion and Pattern Recognition Workshops, CVPRW ?08,
pages 1?8. IEEE.
Mark Steyvers, Michael D. Lee, Brent Miller, and
Pernille Hemmer. 2009. The wisdom of crowds in the
recollection of order information. Advances in neural
information processing systems, 23.
Stephen Tratz and Eduard Hovy. 2010. A taxonomy,
dataset, and classifier for automatic noun compound
interpretation. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 678?687. Association for Computational
Linguistics.
Peter Welinder, Steve Branson, Serge Belongie, and
Pietro Perona. 2010. The multidimensional wisdom
of crowds. In Neural Information Processing Systems
Conference (NIPS), volume 6.
Jacob Whitehill, Paul Ruvolo, Tingfan Wu, Jacob
Bergsma, and Javier Movellan. 2009. Whose vote
should count more: Optimal integration of labels from
labelers of unknown expertise. Advances in Neural In-
formation Processing Systems, 22:2035?2043.
Yan Yan, Ro?mer Rosales, Glenn Fung, Mark Schmidt,
Gerardo Hermosillo, Luca Bogoni, Linda Moy, and
1129
Jennifer Dy. 2010. Modeling annotator expertise:
Learning when everybody knows a bit of something.
In International Conference on Artificial Intelligence
and Statistics.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229, Portland, Oregon, USA, June. Association for
Computational Linguistics.
1130
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678?687,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Taxonomy, Dataset, and Classifier for Automatic Noun Compound
Interpretation
Stephen Tratz and Eduard Hovy
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
{stratz,hovy}@isi.edu
Abstract
The automatic interpretation of noun-noun
compounds is an important subproblem
within many natural language processing
applications and is an area of increasing
interest. The problem is difficult, with dis-
agreement regarding the number and na-
ture of the relations, low inter-annotator
agreement, and limited annotated data. In
this paper, we present a novel taxonomy
of relations that integrates previous rela-
tions, the largest publicly-available anno-
tated dataset, and a supervised classifica-
tion method for automatic noun compound
interpretation.
1 Introduction
Noun compounds (e.g., ?maple leaf?) occur very
frequently in text, and their interpretation?
determining the relationships between adjacent
nouns as well as the hierarchical dependency
structure of the NP in which they occur?is an
important problem within a wide variety of nat-
ural language processing (NLP) applications, in-
cluding machine translation (Baldwin and Tanaka,
2004) and question answering (Ahn et al, 2005).
The interpretation of noun compounds is a difficult
problem for various reasons (Sp?rck Jones, 1983).
Among them is the fact that no set of relations pro-
posed to date has been accepted as complete and
appropriate for general-purpose text. Regardless,
automatic noun compound interpretation is the fo-
cus of an upcoming SEMEVAL task (Butnariu et
al., 2009).
Leaving aside the problem of determining the
dependency structure among strings of three or
more nouns?a problem we do not address in this
paper?automatic noun compound interpretation
requires a taxonomy of noun-noun relations, an
automatic method for accurately assigning the re-
lations to noun compounds, and, in the case of su-
pervised classification, a sufficiently large dataset
for training.
Earlier work has often suffered from using tax-
onomies with coarse-grained, highly ambiguous
predicates, such as prepositions, as various labels
(Lauer, 1995) and/or unimpressive inter-annotator
agreement among human judges (Kim and Bald-
win, 2005). In addition, the datasets annotated ac-
cording to these various schemes have often been
too small to provide wide coverage of the noun
compounds likely to occur in general text.
In this paper, we present a large, fine-grained
taxonomy of 43 noun compound relations, a
dataset annotated according to this taxonomy, and
a supervised, automatic classification method for
determining the relation between the head and
modifier words in a noun compound. We com-
pare and map our relations to those in other tax-
onomies and report the promising results of an
inter-annotator agreement study as well as an au-
tomatic classification experiment. We examine the
various features used for classification and iden-
tify one very useful, novel family of features. Our
dataset is, to the best of our knowledge, the largest
noun compound dataset yet produced. We will
make it available via http://www.isi.edu.
2 Related Work
2.1 Taxonomies
The relations between the component nouns in
noun compounds have been the subject of various
linguistic studies performed throughout the years,
including early work by Jespersen (1949). The
taxonomies they created are varied. Lees created
an early taxonomy based primarily upon grammar
(Lees, 1960). Levi?s influential work postulated
that complex nominals (Levi?s name for noun com-
pounds that also permits certain adjectival modi-
fiers) are all derived either via nominalization or
678
by deleting one of nine predicates (i.e., CAUSE,
HAVE, MAKE, USE, BE, IN, FOR, FROM, ABOUT)
from an underlying sentence construction (Levi,
1978). Of the taxonomies presented by purely
linguistic studies, our categories are most similar
to those proposed by Warren (1978), whose cat-
egories (e.g., MATERIAL+ARTEFACT, OBJ+PART)
are generally less ambiguous than Levi?s.
In contrast to studies that claim the existence of
a relatively small number of semantic relations,
Downing (1977) presents a strong case for the
existence of an unbounded number of relations.
While we agree with Downing?s belief that the
number of relations is unbounded, we contend that
the vast majority of noun compounds fits within a
relatively small set of categories.
The relations used in computational linguistics
vary much along the same lines as those proposed
earlier by linguists. Several lines of work (Finin,
1980; Butnariu and Veale, 2008; Nakov, 2008) as-
sume the existence of an unbounded number of re-
lations. Others use categories similar to Levi?s,
such as Lauer?s (1995) set of prepositional para-
phrases (i.e., OF, FOR, IN, ON, AT, FROM, WITH,
ABOUT) to analyze noun compounds. Some work
(e.g., Barker and Szpakowicz, 1998; Nastase and
Szpakowicz, 2003; Girju et al, 2005; Kim and
Baldwin, 2005) use sets of categories that are
somewhat more similar to those proposed by War-
ren (1978). While most of the noun compound re-
search to date is not domain specific, Rosario and
Hearst (2001) create and experiment with a taxon-
omy tailored to biomedical text.
2.2 Classification
The approaches used for automatic classification
are also varied. Vanderwende (1994) presents one
of the first systems for automatic classification,
which extracted information from online sources
and used a series of rules to rank a set of most
likely interpretations. Lauer (1995) uses corpus
statistics to select a prepositional paraphrase. Sev-
eral lines of work, including that of Barker and
Szpakowicz (1998), use memory-based methods.
Kim and Baldwin (2005) and Turney (2006) use
nearest neighbor approaches based upon WordNet
(Fellbaum, 1998) and Turney?s Latent Relational
Analysis, respectively. Rosario and Hearst (2001)
utilize neural networks to classify compounds ac-
cording to their domain-specific relation taxon-
omy. Moldovan et al (2004) use SVMs as well as
a novel algorithm (i.e., semantic scattering). Nas-
tase et al (2006) experiment with a variety of clas-
sification methods including memory-based meth-
ods, SVMs, and decision trees. ? S?aghdha and
Copestake (2009) use SVMs and experiment with
kernel methods on a dataset labeled using a rela-
tively small taxonomy. Girju (2009) uses cross-
linguistic information from parallel corpora to aid
classification.
3 Taxonomy
3.1 Creation
Given the heterogeneity of past work, we decided
to start fresh and build a new taxonomy of re-
lations using naturally occurring noun pairs, and
then compare the result to earlier relation sets.
We collected 17509 noun pairs and over a period
of 10 months assigned one or more relations to
each, gradually building and refining our taxon-
omy. More details regarding the dataset are pro-
vided in Section 4.
The relations we produced were then compared
to those present in other taxonomies (e.g., Levi,
1978; Warren, 1978; Barker and Szpakowicz,
1998; Girju et al, 2005), and they were found to
be fairly similar. We present a detailed comparison
in Section 3.4.
We tested the relation set with an initial
inter-annotator agreement study (our latest inter-
annotator agreement study results are presented in
Section 6). However, the mediocre results indi-
cated that the categories and/or their definitions
needed refinement. We then embarked on a se-
ries of changes, testing each generation by anno-
tation using Amazon?s Mechanical Turk service, a
relatively quick and inexpensive online platform
where requesters may publish tasks for anony-
mous online workers (Turkers) to perform. Me-
chanical Turk has been previously used in a va-
riety of NLP research, including recent work on
noun compounds by Nakov (2008) to collect short
phrases for linking the nouns within noun com-
pounds.
For the Mechanical Turk annotation tests, we
created five sets of 100 noun compounds from
noun compounds automatically extracted from a
random subset of New York Times articles written
between 1987 and 2007 (Sandhaus, 2008). Each
of these sets was used in a separate annotation
round. For each round, a set of 100 noun com-
pounds was uploaded along with category defini-
679
Category Name % Example Approximate Mappings
Causal Group
COMMUNICATOR OF COMMUNICATION 0.77 court order ?BGN:Agent, ?L:Acta+Producta, ?V:Subj
PERFORMER OF ACT/ACTIVITY 2.07 police abuse ?BGN:Agent, ?L:Acta+Producta, ?V:Subj
CREATOR/PROVIDER/CAUSE OF 2.55 ad revenue ?BGV:Cause(d-by), ?L:Cause2, ?N:Effect
Purpose/Activity Group
PERFORM/ENGAGE_IN 13.24 cooking pot ?BGV:Purpose, ?L:For, ?N:Purpose, ?W:Activity?Purpose
CREATE/PROVIDE/SELL 8.94 nicotine patch?BV:Purpose, ?BG:Result,?G:Make-Produce, ?GNV:Cause(s),
?L:Cause1?Make1?For, ?N:Product, ?W:Activity?Purpose
OBTAIN/ACCESS/SEEK 1.50 shrimp boat ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
MODIFY/PROCESS/CHANGE 1.50 eye surgery ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
MITIGATE/OPPOSE/DESTROY 2.34 flak jacket ?BGV:Purpose, ?L:For, ?N:Detraction, ?W:Activity?Purpose
ORGANIZE/SUPERVISE/AUTHORITY 4.82 ethics board ?BGNV:Purpose/Topic, ?L:For/Abouta, ?W:Activity
PROPEL 0.16 water gun ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
PROTECT/CONSERVE 0.25 screen saver ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
TRANSPORT/TRANSFER/TRADE 1.92 freight train ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
TRAVERSE/VISIT 0.11 tree traversal ?BGNV:Purpose, ?L:For, ?W:Activity?Purpose
Ownership, Experience, Employment, and Use
POSSESSOR + OWNED/POSSESSED 2.11 family estate ?BGNVW:Possess*, ?L:Have2
EXPERIENCER + COGINITION/MENTAL 0.45 voter concern ?BNVW:Possess*, ?G:Experiencer, ?L:Have2
EMPLOYER + EMPLOYEE/VOLUNTEER 2.72 team doctor ?BGNVW:Possess*, ?L:For/Have2, ?BGN:Beneficiary
CONSUMER + CONSUMED 0.09 cat food ?BGNVW:Purpose, ?L:For, ?BGN:Beneficiary
USER/RECIPIENT + USED/RECEIVED 1.02 voter guide ?BNVW:Purpose, ?G:Recipient, ?L:For, ?BGN:Beneficiary
OWNED/POSSESSED + POSSESSION 1.20 store owner ?G:Possession, ?L:Have1, ?W:Belonging-Possessor
EXPERIENCE + EXPERIENCER 0.27 fire victim ?G:Experiencer,?L:Have1
THING CONSUMED + CONSUMER 0.41 fruit fly ?W:Obj-SingleBeing
THING/MEANS USED + USER 1.96 faith healer ?BNV:Instrument, ?G:Means?Instrument, ?L:Use,
?W:MotivePower-Obj
Temporal Group
TIME [SPAN] + X 2.35 night work ?BNV:Time(At), ?G:Temporal, ?L:Inc, ?W:Time-Obj
X + TIME [SPAN] 0.50 birth date ?G:Temporal, ?W:Obj-Time
Location and Whole+Part/Member of
LOCATION/GEOGRAPHIC SCOPE OF X 4.99 hillside home ?BGV:Locat(ion/ive), ?L:Ina?Fromb, B:Source,
?N:Location(At/From), ?W:Place-Obj?PlaceOfOrigin
WHOLE + PART/MEMBER OF 1.75 robot arm ?B:Possess*, ?G:Part-Whole, ?L:Have2, ?N:Part,
?V:Whole-Part, ?W:Obj-Part?Group-Member
Composition and Containment Group
SUBSTANCE/MATERIAL/INGREDIENT + WHOLE 2.42 plastic bag ?BNVW:Material*,?GN:Source,?L:Froma, ?L:Have1,
?L:Make2b,?N:Content
PART/MEMBER + COLLECTION/CONFIG/SERIES 1.78 truck convoy ?L:Make2ac, ?N:Whole, ?V:Part-Whole, ?W:Parts-Whole
X + SPATIAL CONTAINER/LOCATION/BOUNDS 1.39 shoe box ?B:Content?Located, ?L:For, ?L:Have1, ?N:Location,
?W:Obj-Place
Topic Group
TOPIC OF COMMUNICATION/IMAGERY/INFO 8.37 travel story ?BGNV:Topic, ?L:Aboutab, ?W:SubjectMatter, ?G:Depiction
TOPIC OF PLAN/DEAL/ARRANGEMENT/RULES 4.11 loan terms ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF OBSERVATION/STUDY/EVALUATION 1.71 job survey ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF COGNITION/EMOTION 0.58 jazz fan ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF EXPERT 0.57 policy wonk ?BGNV:Topic, ?L:Abouta, ?W:SubjectMatter
TOPIC OF SITUATION 1.64 oil glut ?BGNV:Topic, ?L:Aboutc
TOPIC OF EVENT/PROCESS 1.09 lava flow ?G:Theme, ?V:Subj
Attribute Group
TOPIC/THING + ATTRIB 4.13 street name ?BNV:Possess*, ?G:Property, ?L:Have2, ?W:Obj-Quality
TOPIC/THING + ATTRIB VALUE CHARAC OF 0.31 earth tone
Attributive and Coreferential
COREFERENTIAL 4.51 fighter plane ?BV:Equative, ?G:Type?IS-A, ?L:BEbcd, ?N:Type?Equality,
?W:Copula
PARTIAL ATTRIBUTE TRANSFER 0.69 skeleton crew ?W:Resemblance, ?G:Type
MEASURE + WHOLE 4.37 hour meeting ?G:Measure, ?N:TimeThrough?Measure, ?W:Size-Whole
Other
HIGHLY LEXICALIZED / FIXED PAIR 0.65 pig iron
OTHER 1.67 contact lens
Table 1: The semantic relations, their frequency in the dataset, examples, and approximate relation
mappings to previous relation sets. ?-approximately equivalent; ?/?-super/sub set; ?-some overlap;
?-union; initials BGLNVW refer respectively to the works of (Barker and Szpakowicz, 1998; Girju et
al., 2005; Girju, 2007; Levi, 1978; Nastase and Szpakowicz, 2003; Vanderwende, 1994; Warren, 1978).
680
tions and examples. Turkers were asked to select
one or, if they deemed it appropriate, two cate-
gories for each noun pair. After all annotations for
the round were completed, they were examined,
and any taxonomic changes deemed appropriate
(e.g., the creation, deletion, and/or modification of
categories) were incorporated into the taxonomy
before the next set of 100 was uploaded. The cate-
gories were substantially modified during this pro-
cess. They are shown in Table 1 along with exam-
ples and an approximate mapping to several other
taxonomies.
3.2 Category Descriptions
Our categories are defined with sentences. For
example, the SUBSTANCE category has the
definition n1 is one of the primary physi-
cal substances/materials/ingredients that n2 is
made/composed out of/from. Our LOCATION cat-
egory?s definition reads n1 is the location / geo-
graphic scope where n2 is at, near, from, gener-
ally found, or occurs. Defining the categories with
sentences is advantageous because it is possible to
create straightforward, explicit defintions that hu-
mans can easily test examples against.
3.3 Taxonomy Groupings
In addition to influencing the category defini-
tions, some taxonomy groupings were altered with
the hope that this would improve inter-annotator
agreement for cases where Turker disagreement
was systematic. For example, LOCATION and
WHOLE + PART/MEMBER OF were commonly dis-
agreed upon by Turkers so they were placed within
their own taxonomic subgroup. The ambiguity
between these categories has previously been ob-
served by Girju (2009).
Turkers also tended to disagree between the
categories related to composition and contain-
ment. Due this apparent similarity they were also
grouped together in the taxonomy.
The ATTRIBUTE categories are positioned near
the TOPIC group because some Turkers chose a
TOPIC category when an ATTRIBUTE category was
deemed more appropriate. This may be because
attributes are relatively abstract concepts that are
often somewhat descriptive of whatever possesses
them. A prime example of this is street name.
3.4 Contrast with other Taxonomies
In order to ensure completeness, we mapped into
our taxonomy the relations proposed in most pre-
vious work including those of Barker and Sz-
pakowicz (1998) and Girju et al (2005). The
results, shown in Table 1, demonstrate that our
taxonomy is similar to several taxonomies used
in other work. However, there are three main
differences and several less important ones. The
first major difference is the absence of a signif-
icant THEME or OBJECT category. The second
main difference is that our taxonomy does not in-
clude a PURPOSE category and, instead, has sev-
eral smaller categories. Finally, instead of pos-
sessing a single TOPIC category, our taxonomy has
several, finer-grained TOPIC categories. These dif-
ferences are significant because THEME/OBJECT,
PURPOSE, and TOPIC are typically among the
most frequent categories.
THEME/OBJECT is typically the category to
which other researchers assign noun compounds
whose head noun is a nominalized verb and whose
modifier noun is the THEME/OBJECT of the verb.
This is typically done with the justification that the
relation/predicate (the root verb of the nominaliza-
tion) is overtly expressed.
While including a THEME/OBJECT category has
the advantage of simplicity, its disadvantages are
significant. This category leads to a significant
ambiguity in examples because many compounds
fitting the THEME/OBJECT category also match
some other category as well. Warren (1978) gives
the examples of soup pot and soup container
to illustrate this issue, and Girju (2009) notes a
substantial overlap between THEME and MAKE-
PRODUCE. Our results from Mechanical Turk
showed significant overlap between PURPOSE and
OBJECT categories (present in an earlier version of
the taxonomy). For this reason, we do not include
a separate THEME/OBJECT category. If it is im-
portant to know whether the modifier also holds a
THEME/OBJECT relationship, we suggest treating
this as a separate classification task.
The absence of a single PURPOSE category
is another distinguishing characteristic of our
taxonomy. Instead, the taxonomy includes a
number of finer-grained categories (e.g., PER-
FORM/ENGAGE_IN), which can be conflated to
create a PURPOSE category if necessary. During
our Mechanical Turk-based refinement process,
our now-defunct PURPOSE category was found
to be ambiguous with many other categories as
well as difficult to define. This problem has been
noted by others. For example, Warren (1978)
681
points out that tea in tea cup qualifies as both the
content and the purpose of the cup. Similarly,
while WHOLE+PART/MEMBER was selected by
most Turkers for bike tire, one individual chose
PURPOSE. Our investigation identified five main
purpose-like relations that most of our PURPOSE
examples can be divided into, including activity
performance (PERFORM/ENGAGE_IN), cre-
ation/provision (CREATE/PROVIDE/CAUSE OF),
obtainment/access (OBTAIN/ACCESS/SEEK),
supervision/management (ORGA-
NIZE/SUPERVISE/AUTHORITY), and opposition
(MITIGATE/OPPOSE/DESTROY).
The third major distinguishing different be-
tween our taxonomy and others is the absence of a
single TOPIC/ABOUT relation. Instead, our taxon-
omy has several finer-grained categories that can
be conflated into a TOPIC category. Unlike the
previous two distinguishing characteristics, which
were motivated primarily by Turker annotations,
this separation was largely motivated by author
dissatisfaction with a single TOPIC category.
Two differentiating characteristics of less im-
portance are the absence of BENEFICIARY or
SOURCE categories (Barker and Szpakowicz,
1998; Nastase and Szpakowicz, 2003; Girju et
al., 2005). Our EMPLOYER, CONSUMER, and
USER/RECIPIENT categories combined more or
less cover BENEFICIARY. Since SOURCE is am-
biguous in multiple ways including causation
(tsunami injury), provision (government grant),
ingredients (rice wine), and locations (north
wind), we chose to exclude it.
4 Dataset
Our noun compound dataset was created from
two principal sources: an in-house collection of
terms extracted from a large corpus using part-
of-speech tagging and mutual information and the
Wall Street Journal section of the Penn Treebank.
Compounds including one or more proper nouns
were ignored. In total, the dataset contains 17509
unique, out-of-context examples, making it by far
the largest hand-annotated compound noun dataset
in existence that we are aware of. Proper nouns
were not included.
The next largest available datasets have a vari-
ety of drawbacks for noun compound interpreta-
tion in general text. Kim and Baldwin?s (2005)
dataset is the second largest available dataset, but
inter-annotator agreement was only 52.3%, and
the annotations had an usually lopsided distribu-
tion; 42% of the data has TOPIC labels. Most
(73.23%) of Girju?s (2007) dataset consists of
noun-preposition-noun constructions. Rosario and
Heart?s (2001) dataset is specific to the biomed-
ical domain, while ? S?aghdha and Copestake?s
(2009) data is labeled with only 5 extremely
coarse-grained categories. The remaining datasets
are too small to provide wide coverage. See Table
2 below for size comparison with other publicly
available, semantically annotated datasets.
Size Work
17509 Tratz and Hovy, 2010
2169 Kim and Baldwin, 2005
2031 Girju, 2007
1660 Rosario and Hearst, 2001
1443 ? S?aghdha and Copestake, 2007
505 Barker and Szpakowicz, 1998
600 Nastase and Szpakowicz, 2003
395 Vanderwende, 1994
385 Lauer, 1995
Table 2: Size of various available noun compound
datasets labeled with relation annotations. Ital-
ics indicate that the dataset contains n-prep-n con-
structions and/or non-nouns.
5 Automated Classification
We use a Maximum Entropy (Berger et al, 1996)
classifier with a large number of boolean features,
some of which are novel (e.g., the inclusion of
words from WordNet definitions). Maximum En-
tropy classifiers have been effective on a variety of
NLP problems including preposition sense disam-
biguation (Ye and Baldwin, 2007), which is some-
what similar to noun compound interpretation. We
use the implementation provided in the MALLET
machine learning toolkit (McCallum, 2002).
5.1 Features Used
WordNet-based Features
? {Synonyms, Hypernyms} for all NN and VB
entries for each word
? Intersection of the words? hypernyms
? All terms from the ?gloss? for each word
? Intersection of the words? ?gloss? terms
? Lexicographer file names for each word?s NN
and VB entries (e.g., n1:substance)
682
? Logical AND of lexicographer file names
for the two words (e.g., n1:substance ?
n2:artifact)
? Lists of all link types (e.g., meronym links)
associated with each word
? Logical AND of the link types (e.g.,
n1:hasMeronym(s) ? n2:hasHolonym(s))
? Part-of-speech (POS) indicators for the exis-
tence of VB, ADJ, and ADV entries for each
of the nouns
? Logical AND of the POS indicators for the
two words
? ?Lexicalized? indicator for the existence of an
entry for the compound as a single term
? Indicators if either word is a part of the other
word according to Part-Of links
? Indicators if either word is a hypernym of the
other
? Indicators if either word is in the definition of
the other
Roget?s Thesaurus-based Features
? Roget?s divisions for all noun (and verb) en-
tries for each word
? Roget?s divisions shared by the two words
Surface-level Features
? Indicators for the suffix types (e.g., de-
adjectival, de-nominal [non]agentive, de-
verbal [non]agentive)
? Indicators for degree, number, order, or loca-
tive prefixes (e.g., ultra-, poly-, post-, and
inter-, respectively)
? Indicators for whether or not a preposition
occurs within either term (e.g., ?down? in
?breakdown?)
? The last {two, three} letters of each word
Web 1T N-gram Features
To provide information related to term usage to
the classifier, we extracted trigram and 4-gram fea-
tures from the Web 1T Corpus (Brants and Franz,
2006), a large collection of n-grams and their
counts created from approximately one trillion
words of Web text. Only n-grams containing low-
ercase words were used. 5-grams were not used
due to memory limitations. Only n-grams con-
taining both terms (including plural forms) were
extracted. Table 3 describes the extracted n-gram
features.
5.2 Cross Validation Experiments
We performed 10-fold cross validation on our
dataset, and, for the purpose of comparison,
we also performed 5-fold cross validation on ?
S?aghdha?s (2007) dataset using his folds. Our
classification accuracy results are 79.3% on our
data and 63.6% on the ? S?aghdha data. We
used the ?2 measure to limit our experiments
to the most useful 35000 features, which is the
point where we obtain the highest results on ?
S?aghdha?s data. The 63.6% figure is similar to the
best previously reported accuracy for this dataset
of 63.1%, which was obtained by ? S?aghdha and
Copestake (2009) using kernel methods.
For comparison with SVMs, we used Thorsten
Joachims? SVMmulticlass, which implements an
optimization solution to Cramer and Singer?s
(2001) multiclass SVM formulation. The best re-
sults were similar, with 79.4% on our dataset and
63.1% on ? S?aghdha?s. SVMmulticlass was, how-
ever, observed to be very sensitive to the tuning
of the C parameter, which determines the tradeoff
between training error and margin width. The best
results for the datasets were produced with C set
to 5000 and 375 respectively.
Trigram Feature Extraction Patterns
text <n1> <n2>
<*> <n1> <n2>
<n1> <n2> text
<n1> <n2> <*>
<n1> text <n2>
<n2> text <n1>
<n1> <*> <n2>
<n2> <*> <n1>
4-Gram Feature Extraction Patterns
<n1> <n2> text text
<n1> <n2> <*> text
text <n1> <n2> text
text text <n1> <n2>
text <*> <n1> <n2>
<n1> text text <n2>
<n1> text <*> <n2>
<n1> <*> text <n2>
<n1> <*> <*> <n2>
<n2> text text <n1>
<n2> text <*> <n1>
<n2> <*> text <n1>
<n2> <*> <*> <n1>
Table 3: Patterns for extracting trigram and 4-
Gram features from the Web 1T Corpus for a given
noun compound (n1 n2).
To assess the impact of the various features, we
ran the cross validation experiments for each fea-
ture type, alternating between including only one
683
feature type and including all feature types except
that one. The results for these runs using the Max-
imum Entropy classifier are presented in Table 4.
There are several points of interest in these re-
sults. The WordNet gloss terms had a surpris-
ingly strong influence. In fact, by themselves they
proved roughly as useful as the hypernym features,
and their removal had the single strongest negative
impact on accuracy for our dataset. As far as we
know, this is the first time that WordNet definition
words have been used as features for noun com-
pound interpretation. In the future, it may be valu-
able to add definition words from other machine-
readable dictionaries. The influence of the Web 1T
n-gram features was somewhat mixed. They had a
positive impact on the ? S?aghdha data, but their
affect upon our dataset was limited and mixed,
with the removal of the 4-gram features actually
improving performance slightly.
Our Data ? S?aghdha Data
1 M-1 1 M-1
WordNet-based
synonyms 0.674 0.793 0.469 0.626
hypernyms 0.753 0.787 0.539 0.626
hypernyms? 0.250 0.791 0.357 0.624
gloss terms 0.741 0.785 0.510 0.613
gloss terms? 0.226 0.793 0.275 0.632
lexfnames 0.583 0.792 0.505 0.629
lexfnames? 0.480 0.790 0.440 0.629
linktypes 0.328 0.793 0.365 0.631
linktypes? 0.277 0.792 0.346 0.626
pos 0.146 0.793 0.239 0.633
pos? 0.146 0.793 0.235 0.632
part-of terms 0.372 0.793 0.368 0.635
lexicalized 0.132 0.793 0.213 0.637
part of other 0.132 0.793 0.216 0.636
gloss of other 0.133 0.793 0.214 0.635
hypernym of other 0.132 0.793 0.227 0.627
Roget?s Thesaurus-based
div info 0.679 0.789 0.471 0.629
div info? 0.173 0.793 0.283 0.633
Surface level
affixes 0.200 0.793 0.274 0.637
affixes? 0.201 0.792 0.272 0.635
last letters 0.481 0.792 0.396 0.634
prepositions 0.136 0.793 0.222 0.635
Web 1T-based
trigrams 0.571 0.790 0.437 0.615
4-grams 0.558 0.797 0.442 0.604
Table 4: Impact of features; cross validation ac-
curacy for only one feature type and all but one
feature type experiments, denoted by 1 and M-1
respectively. ??features shared by both n1 and n2;
??n1 and n2 features conjoined by logical AND
(e.g., n1 is a ?substance? ? n2 is a ?artifact?)
6 Evaluation
6.1 Evaluation Data
To assess the quality of our taxonomy and classi-
fication method, we performed an inter-annotator
agreement study using 150 noun compounds ex-
tracted from a random subset of articles taken
from New York Times articles dating back to 1987
(Sandhaus, 2008). The terms were selected based
upon their frequency (i.e., a compound occurring
twice as often as another is twice as likely to be
selected) to label for testing purposes. Using a
heuristic similar to that used by Lauer (1995), we
only extracted binary noun compounds not part of
a larger sequence. Before reaching the 150 mark,
we discarded 94 of the drawn examples because
they were included in the training set. Thus, our
training set covers roughly 38.5% of the binary
noun compound instances in recent New York
Times articles.
6.2 Annotators
Due to the relatively high speed and low cost of
Amazon?s Mechanical Turk service, we chose to
use Mechanical Turkers as our annotators.
Using Mechanical Turk to obtain inter-
annotator agreement figures has several draw-
backs. The first and most significant drawback is
that it is impossible to force each Turker to label
every data point without putting all the terms onto
a single web page, which is highly impractical
for a large taxonomy. Some Turkers may label
every compound, but most do not. Second,
while we requested that Turkers only work on
our task if English was their first language, we
had no method of enforcing this. Third, Turker
annotation quality varies considerably.
6.3 Combining Annotators
To overcome the shortfalls of using Turkers for an
inter-annotator agreement study, we chose to re-
quest ten annotations per noun compound and then
combine the annotations into a single set of selec-
tions using a weighted voting scheme. To com-
bine the results, we calculated a ?quality? score for
each Turker based upon how often he/she agreed
with the others. This score was computed as the
average percentage of other Turkers who agreed
with his/her annotations. The score for each label
for a particular compound was then computed as
the sum of the Turker quality scores of the Turkers
684
who annotated the compound. Finally, the label
with the highest rating was selected.
6.4 Inter-annotator Agreement Results
The raw agreement scores along with Cohen?s ?
(Cohen, 1960), a measure of inter-annotator agree-
ment that discounts random chance, were calcu-
lated against the authors? labeling of the data for
each Turker, the weighted-voting annotation set,
and the automatic classification output. These
statistics are reported in Table 5 along with the
individual Turker ?quality? scores. The 54 Turk-
ers who made fewer than 3 annotations were ex-
cluded from the calculations under the assumption
that they were not dedicated to the task, leaving a
total of 49 Turkers. Due to space limitations, only
results for Turkers who annotated 15 or more in-
stances are included in Table 5.
We recomputed the ? statistics after conflating
the category groups in two different ways. The
first variation involved conflating all the TOPIC
categories into a single topic category, resulting in
a total of 37 categories (denoted by ?* in Table
5). For the second variation, in addition to con-
flating the TOPIC categories, we conflated the AT-
TRIBUTE categories into a single category and the
PURPOSE/ACTIVITY categories into a single cate-
gory, for a total of 27 categories (denoted by ?**
in Table 5).
6.5 Results Discussion
The .57-.67 ? figures achieved by the Voted an-
notations compare well with previously reported
inter-annotator agreement figures for noun com-
pounds using fine-grained taxonomies. Kim and
Baldwin (2005) report an agreement of 52.31%
(not ?) for their dataset using Barker and Sz-
pakowicz?s (1998) 20 semantic relations. Girju
et al (2005) report .58 ? using a set of 35 se-
mantic relations, only 21 of which were used, and
a .80 ? score using Lauer?s 8 prepositional para-
phrases. Girju (2007) reports .61 ? agreement
using a similar set of 22 semantic relations for
noun compound annotation in which the annota-
tors are shown translations of the compound in for-
eign languages. ? S?aghdha (2007) reports a .68
? for a relatively small set of relations (BE, HAVE,
IN, INST, ACTOR, ABOUT) after removing com-
pounds with non-specific associations or high lex-
icalization. The correlation between our automatic
?quality? scores for the Turkers who performed at
Id N Weight Agree ? ?* ?**
1 23 0.45 0.70 0.67 0.67 0.74
2 34 0.46 0.68 0.65 0.65 0.72
3 35 0.34 0.63 0.60 0.61 0.61
4 24 0.46 0.63 0.59 0.68 0.76
5 16 0.58 0.63 0.59 0.59 0.54
Voted 150 NA 0.59 0.57 0.61 0.67
6 52 0.45 0.58 0.54 0.60 0.60
7 38 0.35 0.55 0.52 0.54 0.56
8 149 0.36 0.52 0.49 0.53 0.58
Auto 150 NA 0.51 0.47 0.47 0.45
9 88 0.38 0.48 0.45 0.49 0.59
10 36 0.42 0.47 0.43 0.48 0.52
11 104 0.29 0.46 0.43 0.48 0.52
12 38 0.33 0.45 0.40 0.46 0.47
13 66 0.31 0.42 0.39 0.39 0.49
14 15 0.27 0.40 0.34 0.31 0.29
15 62 0.23 0.34 0.29 0.35 0.38
16 150 0.23 0.30 0.26 0.26 0.30
17 19 0.24 0.26 0.21 0.17 0.14
18 144 0.21 0.25 0.20 0.22 0.22
19 29 0.18 0.21 0.14 0.17 0.31
20 22 0.18 0.18 0.12 0.10 0.16
21 51 0.19 0.18 0.13 0.20 0.26
22 41 0.02 0.02 0.00 0.00 0.01
Table 5: Annotation results. Id ? annotator id; N
? number of annotations; Weight ? voting weight;
Agree ? raw agreement versus the author?s annota-
tions; ? ? Cohen?s ? agreement; ?* and ?** ? Co-
hen?s ? results after conflating certain categories.
Voted ? combined annotation set using weighted
voting; Auto ? automatic classification output.
least three annotations and their simple agreement
with our annotations was very strong at 0.88.
The .51 automatic classification figure is re-
spectable given the larger number of categories in
the taxonomy. It is also important to remember
that the training set covers a large portion of the
two-word noun compound instances in recent New
York Times articles, so substantially higher accu-
racy can be expected on many texts. Interestingly,
conflating categories only improved the ? statis-
tics for the Turkers, not the automatic classifier.
7 Conclusion
In this paper, we present a novel, fine-grained tax-
onomy of 43 noun-noun semantic relations, the
largest annotated noun compound dataset yet cre-
ated, and a supervised classification method for
automatic noun compound interpretation.
We describe our taxonomy and provide map-
pings to taxonomies used by others. Our inter-
annotator agreement study, which utilized non-
experts, shows good inter-annotator agreement
685
given the difficulty of the task, indicating that our
category definitions are relatively straightforward.
Our taxonomy provides wide coverage, with only
2.32% of our dataset marked as other/lexicalized
and 2.67% of our 150 inter-annotator agreement
data marked as such by the combined Turker
(Voted) annotation set.
We demonstrated the effectiveness of a straight-
forward, supervised classification approach to
noun compound interpretation that uses a large va-
riety of boolean features. We also examined the
importance of the different features, noting a novel
and very useful set of features?the words com-
prising the definitions of the individual words.
8 Future Work
In the future, we plan to focus on the interpretation
of noun compounds with 3 or more nouns, a prob-
lem that includes bracketing noun compounds into
their dependency structures in addition to noun-
noun semantic relation interpretation. Further-
more, we would like to build a system that can
handle longer noun phrases, including preposi-
tions and possessives.
We would like to experiment with including fea-
tures from various other lexical resources to deter-
mine their usefulness for this problem.
Eventually, we would like to expand our data
set and relations to cover proper nouns as well.
We are hopeful that our current dataset and re-
lation definitions, which will be made available
via http://www.isi.edu will be helpful to other re-
searchers doing work regarding text semantics.
Acknowledgements
Stephen Tratz is supported by a National Defense
Science and Engineering Graduate Fellowship.
References
Ahn, K., J. Bos, J. R. Curran, D. Kor, M. Nissim, and
B. Webber. 2005. Question Answering with QED
at TREC-2005. In Proc. of TREC-2005.
Baldwin, T. & T. Tanaka 2004. Translation by machine
of compound nominals: Getting it right. In Proc. of
the ACL 2004 Workshop on Multiword Expressions:
Integrating Processing.
Barker, K. and S. Szpakowicz. 1998. Semi-Automatic
Recognition of Noun Modifier Relationships. In
Proc. of the 17th International Conference on Com-
putational Linguistics.
Berger, A., S. A. Della Pietra, and V. J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics
22:39-71.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Corpus
Version 1.1. Linguistic Data Consortium.
Butnariu, C. and T. Veale. 2008. A concept-centered
approach to noun-compound interpretation. In Proc.
of 22nd International Conference on Computational
Linguistics (COLING 2008).
Butnariu, C., S.N. Kim, P. Nakov, D. ? S?aghdha, S.
Szpakowicz, and T. Veale. 2009. SemEval Task 9:
The Interpretation of Noun Compounds Using Para-
phrasing Verbs and Prepositions. In Proc. of the
NAACL HLT Workshop on Semantic Evaluations:
Recent Achievements and Future Directions.
Cohen, J. 1960. A coefficient of agreement for nomi-
nal scales. Educational and Psychological Measure-
ment. 20:1.
Crammer, K. and Y. Singer. On the Algorithmic Imple-
mentation of Multi-class SVMs In Journal of Ma-
chine Learning Research.
Downing, P. 1977. On the Creation and Use of English
Compound Nouns. Language. 53:4.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Finin, T. 1980. The Semantic Interpretation of Com-
pound Nominals. Ph.D dissertation University of
Illinois, Urbana, Illinois.
Girju, R., D. Moldovan, M. Tatu and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19.
Girju, R. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In Proc.
of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL 2007).
Girju, R. 2009. The Syntax and Semantics of
Prepositions in the Task of Automatic Interpreta-
tion of Nominal Phrases and Compounds: a Cross-
linguistic Study. In Computational Linguistics 35(2)
- Special Issue on Prepositions in Application.
Jespersen, O. 1949. A Modern English Grammar on
Historical Principles. Ejnar Munksgaard. Copen-
hagen.
Kim, S.N. and T. Baldwin. 2007. Interpreting Noun
Compounds using Bootstrapping and Sense Collo-
cation. In Proc. of the 10th Conf. of the Pacific As-
sociation for Computational Linguistics.
Kim, S.N. and T. Baldwin. 2005. Automatic
Interpretation of Compound Nouns using Word-
Net::Similarity. In Proc. of 2nd International Joint
Conf. on Natural Language Processing.
686
Lauer, M. 1995. Corpus statistics meet the compound
noun. In Proc. of the 33rd Meeting of the Associa-
tion for Computational Linguistics.
Lees, R.B. 1960. The Grammar of English Nominal-
izations. Indiana University. Bloomington, IN.
Levi, J.N. 1978. The Syntax and Semantics of Com-
plex Nominals. Academic Press. New York.
McCallum, A. K. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu. 2002.
Moldovan, D., A. Badulescu, M. Tatu, D. Antohe, and
R. Girju. 2004. Models for the semantic classifi-
cation of noun phrases. In Proc. of Computational
Lexical Semantics Workshop at HLT-NAACL 2004.
Nakov, P. and M. Hearst. 2005. Search Engine Statis-
tics Beyond the n-gram: Application to Noun Com-
pound Bracketing. In Proc. the Ninth Conference on
Computational Natural Language Learning.
Nakov, P. 2008. Noun Compound Interpretation
Using Paraphrasing Verbs: Feasibility Study. In
Proc. the 13th International Conference on Artifi-
cial Intelligence: Methodology, Systems, Applica-
tions (AIMSA?08).
Nastase V. and S. Szpakowicz. 2003. Exploring noun-
modifier semantic relations. In Proc. the 5th Inter-
national Workshop on Computational Semantics.
Nastase, V., J. S. Shirabad, M. Sokolova, and S. Sz-
pakowicz 2006. Learning noun-modifier semantic
relations with corpus-based and Wordnet-based fea-
tures. In Proc. of the 21st National Conference on
Artificial Intelligence (AAAI-06).
? S?aghdha, D. and A. Copestake. 2009. Using lexi-
cal and relational similarity to classify semantic re-
lations. In Proc. of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2009).
? S?aghdha, D. 2007. Annotating and Learning Com-
pound Noun Semantics. In Proc. of the ACL 2007
Student Research Workshop.
Rosario, B. and M. Hearst. 2001. Classifying the Se-
mantic Relations in Noun Compounds via Domain-
Specific Lexical Hierarchy. In Proc. of 2001 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP-01).
Sandhaus, E. 2008. The New York Times Annotated
Corpus. Linguistic Data Consortium, Philadelphia.
Sp?rck Jones, K. 1983. Compound Noun Interpreta-
tion Problems. Computer Speech Processing, eds.
F. Fallside and W A. Woods, Prentice-Hall, NJ.
Turney, P. D. 2006. Similarity of semantic relations.
Computation Linguistics, 32(3):379-416
Vanderwende, L. 1994. Algorithm for Automatic
Interpretation of Noun Sequences. In Proc. of
COLING-94.
Warren, B. 1978. Semantic Patterns of Noun-Noun
Compounds. Acta Universitatis Gothobugensis.
Ye, P. and T. Baldwin. 2007. MELB-YB: Prepo-
sition Sense Disambiguation Using Rich Semantic
Features. In Proc. of the 4th International Workshop
on Semantic Evaluations (SemEval-2007).
687
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423?1432,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Coreference Resolution across Corpora:
Languages, Coding Schemes, and Preprocessing Information
Marta Recasens
CLiC - University of Barcelona
Gran Via 585
Barcelona, Spain
mrecasens@ub.edu
Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey CA, USA
hovy@isi.edu
Abstract
This paper explores the effect that dif-
ferent corpus configurations have on the
performance of a coreference resolution
system, as measured by MUC, B3, and
CEAF. By varying separately three param-
eters (language, annotation scheme, and
preprocessing information) and applying
the same coreference resolution system,
the strong bonds between system and cor-
pus are demonstrated. The experiments
reveal problems in coreference resolution
evaluation relating to task definition, cod-
ing schemes, and features. They also ex-
pose systematic biases in the coreference
evaluation metrics. We show that system
comparison is only possible when corpus
parameters are in exact agreement.
1 Introduction
The task of coreference resolution, which aims to
automatically identify the expressions in a text that
refer to the same discourse entity, has been an in-
creasing research topic in NLP ever since MUC-6
made available the first coreferentially annotated
corpus in 1995. Most research has centered around
the rules by which mentions are allowed to corefer,
the features characterizing mention pairs, the algo-
rithms for building coreference chains, and coref-
erence evaluation methods. The surprisingly im-
portant role played by different aspects of the cor-
pus, however, is an issue to which little attention
has been paid. We demonstrate the extent to which
a system will be evaluated as performing differ-
ently depending on parameters such as the corpus
language, the way coreference relations are de-
fined in the corresponding coding scheme, and the
nature and source of preprocessing information.
This paper unpacks these issues by running the
same system?a prototype entity-based architec-
ture called CISTELL?on different corpus config-
urations, varying three parameters. First, we show
how much language-specific issues affect perfor-
mance when trained and tested on English and
Spanish. Second, we demonstrate the extent to
which the specific annotation scheme (used on the
same corpus) makes evaluated performance vary.
Third, we compare the performance using gold-
standard preprocessing information with that us-
ing automatic preprocessing tools.
Throughout, we apply the three principal coref-
erence evaluation measures in use today: MUC,
B3, and CEAF. We highlight the systematic prefer-
ences of each measure to reward different config-
urations. This raises the difficult question of why
one should use one or another evaluation mea-
sure, and how one should interpret their differ-
ences in reporting changes of performance score
due to ?secondary? factors like preprocessing in-
formation.
To this end, we employ three corpora: ACE
(Doddington et al, 2004), OntoNotes (Pradhan
et al, 2007), and AnCora (Recasens and Mart??,
2009). In order to isolate the three parameters
as far as possible, we benefit from a 100k-word
portion (from the TDT collection) that is common
to both ACE and OntoNotes. We apply the same
coreference resolution system in all cases. The re-
sults show that a system?s score is not informative
by itself, as different corpora or corpus parameters
lead to different scores. Our goal is not to achieve
the best performance to date, but rather to ex-
pose various issues raised by the choices of corpus
preparation and evaluation measure and to shed
light on the definition, methods, evaluation, and
complexities of the coreference resolution task.
The paper is organized as follows. Section 2
sets our work in context and provides the motiva-
tions for undertaking this study. Section 3 presents
the architecture of CISTELL, the system used in
the experimental evaluation. In Sections 4, 5,
1423
and 6, we describe the experiments on three differ-
ent datasets and discuss the results. We conclude
in Section 7.
2 Background
The bulk of research on automatic coreference res-
olution to date has been done for English and used
two different types of corpus: MUC (Hirschman
and Chinchor, 1997) and ACE (Doddington et al,
2004). A variety of learning-based systems have
been trained and tested on the former (Soon et al,
2001; Uryupina, 2006), on the latter (Culotta et
al., 2007; Bengtson and Roth, 2008; Denis and
Baldridge, 2009), or on both (Finkel and Manning,
2008; Haghighi and Klein, 2009). Testing on both
is needed given that the two annotation schemes
differ in some aspects. For example, only ACE
includes singletons (mentions that do not corefer)
and ACE is restricted to seven semantic types.1
Also, despite a critical discussion in the MUC task
definition (van Deemter and Kibble, 2000), the
ACE scheme continues to treat nominal predicates
and appositive phrases as coreferential.
A third coreferentially annotated corpus?the
largest for English?is OntoNotes (Pradhan et al,
2007; Hovy et al, 2006). Unlike ACE, it is not
application-oriented, so coreference relations be-
tween all types of NPs are annotated. The identity
relation is kept apart from the attributive relation,
and it also contains gold-standard morphological,
syntactic and semantic information.
Since the MUC and ACE corpora are annotated
with only coreference information,2 existing sys-
tems first preprocess the data using automatic tools
(POS taggers, parsers, etc.) to obtain the infor-
mation needed for coreference resolution. How-
ever, given that the output from automatic tools
is far from perfect, it is hard to determine the
level of performance of a coreference module act-
ing on gold-standard preprocessing information.
OntoNotes makes it possible to separate the coref-
erence resolution problem from other tasks.
Our study adds to the previously reported evi-
dence by Stoyanov et al (2009) that differences in
corpora and in the task definitions need to be taken
into account when comparing coreference resolu-
tion systems. We provide new insights as the cur-
rent analysis differs in four ways. First, Stoyanov
1The ACE-2004/05 semantic types are person, organiza-
tion, geo-political entity, location, facility, vehicle, weapon.
2ACE also specifies entity types and relations.
et al (2009) report on differences between MUC
and ACE, while we contrast ACE and OntoNotes.
Given that ACE and OntoNotes include some of
the same texts but annotated according to their re-
spective guidelines, we can better isolate the effect
of differences as well as add the additional dimen-
sion of gold preprocessing. Second, we evaluate
not only with the MUC and B3 scoring metrics,
but also with CEAF. Third, all our experiments
use true mentions3 to avoid effects due to spuri-
ous system mentions. Finally, including different
baselines and variations of the resolution model al-
lows us to reveal biases of the metrics.
Coreference resolution systems have been
tested on languages other than English only within
the ACE program (Luo and Zitouni, 2005), prob-
ably due to the fact that coreferentially annotated
corpora for other languages are scarce. Thus there
has been no discussion of the extent to which sys-
tems are portable across languages. This paper
studies the case of English and Spanish.4
Several coreference systems have been devel-
oped in the past (Culotta et al, 2007; Finkel
and Manning, 2008; Poon and Domingos, 2008;
Haghighi and Klein, 2009; Ng, 2009). It is not our
aim to compete with them. Rather, we conduct
three experiments under a specific setup for com-
parison purposes. To this end, we use a different,
neutral, system, and a dataset that is small and dif-
ferent from official ACE test sets despite the fact
that it prevents our results from being compared
directly with other systems.
3 Experimental Setup
3.1 System Description
The system architecture used in our experiments,
CISTELL, is based on the incrementality of dis-
course. As a discourse evolves, it constructs a
model that is updated with the new information
gradually provided. A key element in this model
are the entities the discourse is about, as they form
the discourse backbone, especially those that are
mentioned multiple times. Most entities, however,
are only mentioned once. Consider the growth of
the entity Mount Popocate?petl in (1).5
3The adjective true contrasts with system and refers to the
gold standard.
4Multilinguality is one of the focuses of SemEval-2010
Task 1 (Recasens et al, 2010).
5Following the ACE terminology, we use the term men-
tion for an instance of reference to an object, and entity for a
collection of mentions referring to the same object. Entities
1424
(1) We have an update tonight on [this, the volcano in
Mexico, they call El Popo]m3 . . . As the sun rises
over [Mt. Popo]m7 tonight, the only hint of the fire
storm inside, whiffs of smoke, but just a few hours
earlier, [the volcano]m11 exploding spewing rock
and red-hot lava. [The fourth largest mountain in
North America, nearly 18,000 feet high]m15, erupt-
ing this week with [its]m20 most violent outburst in
1,200 years.
Mentions can be pronouns (m20), they can be a
(shortened) string repetition using either the name
(m7) or the type (m11), or they can add new infor-
mation about the entity: m15 provides the super-
type and informs the reader about the height of the
volcano and its ranking position.
In CISTELL,6 discourse entities are conceived
as ?baskets?: they are empty at the beginning of
the discourse, but keep growing as new attributes
(e.g., name, type, location) are predicated about
them. Baskets are filled with this information,
which can appear within a mention or elsewhere
in the sentence. The ever-growing amount of in-
formation in a basket alows richer comparisons to
new mentions encountered in the text.
CISTELL follows the learning-based corefer-
ence architecture in which the task is split into
classification and clustering (Soon et al, 2001;
Bengtson and Roth, 2008) but combines them si-
multaneously. Clustering is identified with basket-
growing, the core process, and a pairwise clas-
sifier is called every time CISTELL considers
whether a basket must be clustered into a (grow-
ing) basket, which might contain one or more
mentions. We use a memory-based learning clas-
sifier trained with TiMBL (Daelemans and Bosch,
2005). Basket-growing is done in four different
ways, explained next.
3.2 Baselines and Models
In each experiment, we compute three baselines
(1, 2, 3), and run CISTELL under four different
models (4, 5, 6, 7).
1. ALL SINGLETONS. No coreference link is
ever created. We include this baseline given
the high number of singletons in the datasets,
since some evaluation measures are affected
by large numbers of singletons.
2. HEAD MATCH. All non-pronominal NPs that
have the same head are clustered into the
same entity.
containing one single mention are referred to as singletons.
6?Cistell? is the Catalan word for ?basket.?
3. HEAD MATCH + PRON. Like HEAD MATCH,
plus allowing personal and possessive pro-
nouns to link to the closest noun with which
they agree in gender and number.
4. STRONG MATCH. Each mention (e.g., m11) is
paired with previous mentions starting from
the beginning of the document (m1?m11, m2?
m11, etc.).7 When a pair (e.g., m3?m11) is
classified as coreferent, additional pairwise
checks are performed with all the mentions
contained in the (growing) entity basket (e.g.,
m7?m11). Only if all the pairs are classified
as coreferent is the mention under consider-
ation attached to the existing growing entity.
Otherwise, the search continues.8
5. SUPER STRONG MATCH. Similar to STRONG
MATCH but with a threshold. Coreference
pairwise classifications are only accepted
when TiMBL distance is smaller than 0.09.9
6. BEST MATCH. Similar to STRONG MATCH
but following Ng and Cardie (2002)?s best
link approach. Thus, the mention under anal-
ysis is linked to the most confident men-
tion among the previous ones, using TiMBL?s
confidence score.
7. WEAK MATCH. A simplified version of
STRONG MATCH: not all mentions in the
growing entity need to be classified as coref-
erent with the mention under analysis. A sin-
gle positive pairwise decision suffices for the
mention to be clustered into that entity.10
3.3 Features
We follow Soon et al (2001), Ng and Cardie
(2002) and Luo et al (2004) to generate most
of the 29 features we use for the pairwise
model. These include features that capture in-
formation from different linguistic levels: textual
strings (head match, substring match, distance,
frequency), morphology (mention type, coordi-
nation, possessive phrase, gender match, number
match), syntax (nominal predicate, apposition, rel-
ative clause, grammatical function), and semantic
match (named-entity type, is-a type, supertype).
7The opposite search direction was also tried but gave
worse results.
8Taking the first mention classified as coreferent follows
Soon et al (2001)?s first-link approach.
9In TiMBL, being a memory-based learner, the closer the
distance to an instance, the more confident the decision. We
chose 0.09 because it appeared to offer the best results.
10STRONG and WEAK MATCH are similar to Luo et al
(2004)?s entity-mention and mention-pair models.
1425
For Spanish, we use 34 features as a few varia-
tions are needed for language-specific issues such
as zero subjects (Recasens and Hovy, 2009).
3.4 Evaluation
Since they sometimes provide quite different re-
sults, we evaluate using three coreference mea-
sures, as there is no agreement on a standard.
? MUC (Vilain et al, 1995). It computes the
number of links common between the true
and system partitions. Recall (R) and preci-
sion (P) result from dividing it by the mini-
mum number of links required to specify the
true and the system partitions, respectively.
? B3 (Bagga and Baldwin, 1998). R and P are
computed for each mention and averaged at
the end. For each mention, the number of
common mentions between the true and the
system entity is divided by the number of
mentions in the true entity or in the system
entity to obtain R and P, respectively.
? CEAF (Luo, 2005). It finds the best one-to-
one alignment between true and system en-
tities. Using true mentions and the ?3 sim-
ilarity function, R and P are the same and
correspond to the number of common men-
tions between the aligned entities divided by
the total number of mentions.
4 Parameter 1: Language
The first experiment compared the performance
of a coreference resolution system on a Germanic
and a Romance language?English and Spanish?
to explore to what extent language-specific issues
such as zero subjects11 or grammatical gender
might influence a system.
Although OntoNotes and AnCora are two dif-
ferent corpora, they are very similar in those as-
pects that matter most for the study?s purpose:
they both include a substantial amount of texts
belonging to the same genre (news) and manu-
ally annotated from the morphological to the se-
mantic levels (POS tags, syntactic constituents,
NEs, WordNet synsets, and coreference relations).
More importantly, very similar coreference anno-
tation guidelines make AnCora the ideal Spanish
counterpart to OntoNotes.
11Most Romance languages are pro-drop allowing zero
subject pronouns, which can be inferred from the verb.
Datasets Two datasets of similar size were se-
lected from AnCora and OntoNotes in order to
rule out corpus size as an explanation of any differ-
ence in performance. Corpus statistics about the
distribution of mentions and entities are shown in
Tables 1 and 2. Given that this paper is focused on
coreference between NPs, the number of mentions
only includes NPs. Both AnCora and OntoNotes
annotate only multi-mention entities (i.e., those
containing two or more coreferent mentions), so
singleton entities are assumed to correspond to
NPs with no coreference annotation.
Apart from a larger number of mentions in
Spanish (Table 1), the two datasets look very sim-
ilar in the distribution of singletons and multi-
mention entities: about 85% and 15%, respec-
tively. Multi-mention entities have an average
of 3.9 mentions per entity in AnCora and 3.5 in
OntoNotes. The distribution of mention types (Ta-
ble 2), however, differs in two important respects:
AnCora has a smaller number of personal pro-
nouns as Spanish typically uses zero subjects, and
it has a smaller number of bare NPs as the definite
article accompanies more NPs than in English.
Results and Discussion Table 3 presents CIS-
TELL?s results for each dataset. They make evi-
dent problems with the evaluation metrics, namely
the fact that the generated rankings are contradic-
tory (Denis and Baldridge, 2009). They are con-
sistent across the two corpora though: MUC re-
wards WEAK MATCH the most, B3 rewards HEAD
MATCH the most, and CEAF is divided between
SUPER STRONG MATCH and BEST MATCH.
These preferences seem to reveal weaknesses
of the scoring methods that make them biased to-
wards a type of output. The model preferred by
MUC is one that clusters many mentions together,
thus getting a large number of correct coreference
links (notice the high R for WEAK MATCH), but
AnCora OntoNotes
Pronouns 14.09 17.62
Personal pronouns 2.00 12.10
Zero subject pronouns 6.51 ?
Possessive pronouns 3.57 2.96
Demonstrative pronouns 0.39 1.83
Definite NPs 37.69 20.67
Indefinite NPs 7.17 8.44
Demonstrative NPs 1.98 3.41
Bare NPs 33.02 42.92
Misc. 6.05 6.94
Table 2: Mention types (%) in Table 1 datasets.
1426
#docs #words #mentions #entities (e) #singleton e #multi-mention e
AnCora
Training 955 299,014 91,904 64,535 54,991 9,544
Test 30 9,851 2,991 2,189 1,877 312
OntoNotes
Training 850 301,311 74,692 55,819 48,199 7,620
Test 33 9,763 2,463 1,790 1,476 314
Table 1: Corpus statistics for the large portion of OntoNotes and AnCora.
MUC B3 CEAF
P R F P R F P / R / F
AnCora - Spanish
1. ALL SINGLETONS ? ? ? 100 73.32 84.61 73.32
2. HEAD MATCH 55.03 37.72 44.76 91.12 79.88 85.13 75.96
3. HEAD MATCH + PRON 48.22 44.24 46.14 86.21 80.66 83.34 76.30
4. STRONG MATCH 45.64 51.88 48.56 80.13 82.28 81.19 75.79
5. SUPER STRONG MATCH 45.68 36.47 40.56 86.10 79.09 82.45 77.20
6. BEST MATCH 43.10 35.59 38.98 85.24 79.67 82.36 75.23
7. WEAK MATCH 45.73 65.16 53.75 68.50 87.71 76.93 69.21
OntoNotes - English
1. ALL SINGLETONS ? ? ? 100 72.68 84.18 72.68
2. HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.05
3. HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.15
4. STRONG MATCH 47.94 55.42 51.41 81.13 84.30 82.68 78.03
5. SUPER STRONG MATCH 48.27 47.55 47.90 84.00 82.27 83.13 78.24
6. BEST MATCH 50.97 46.66 48.72 86.19 82.70 84.41 78.44
7. WEAK MATCH 47.46 66.72 55.47 70.36 88.05 78.22 71.21
Table 3: CISTELL results varying the corpus language.
also many spurious links that are not duly penal-
ized. The resulting output is not very desirable.12
In contrast, B3 is more P-oriented and scores con-
servative outputs like HEAD MATCH and BEST
MATCH first, even if R is low. CEAF achieves a
better compromise between P and R, as corrobo-
rated by the quality of the output.
The baselines and the system runs perform very
similarly in the two corpora, but slightly better
for English. It seems that language-specific issues
do not result in significant differences?at least
for English and Spanish?once the feature set has
been appropriately adapted, e.g., including fea-
tures about zero subjects or removing those about
possessive phrases. Comparing the feature ranks,
we find that the features that work best for each
language largely overlap and are language inde-
pendent, like head match, is-a match, and whether
the mentions are pronominal.
5 Parameter 2: Annotation Scheme
In the second experiment, we used the 100k-word
portion (from the TDT collection) shared by the
OntoNotes and ACE corpora (330 OntoNotes doc-
12Due to space constraints, the actual output cannot be
shown here. We are happy to send it to interested requesters.
uments occurred as 22 ACE-2003 documents, 185
ACE-2004 documents, and 123 ACE-2005 docu-
ments). CISTELL was trained on the same texts
in both corpora and applied to the remainder. The
three measures were then applied to each result.
Datasets Since the two annotation schemes dif-
fer significantly, we made the results comparable
by mapping the ACE entities (the simpler scheme)
onto the information contained in OntoNotes.13
The mapping allowed us to focus exclusively on
the differences expressed on both corpora: the
types of mentions that were annotated, the defi-
nition of identity of reference, etc.
Table 4 presents the statistics for the OntoNotes
dataset merged with the ACE entities. The map-
ping was not straightforward due to several prob-
lems: there was no match for some mentions
due to syntactic or spelling reasons (e.g., El Popo
in OntoNotes vs. Ell Popo in ACE). ACE men-
tions for which there was no parse tree node in
the OntoNotes gold-standard tree were omitted, as
creating a new node could have damaged the tree.
Given that only seven entity types are annotated
in ACE, the number of OntoNotes mentions is al-
13Both ACE entities and types were mapped onto the
OntoNotes dataset.
1427
#docs #words #mentions #entities (e) #singleton e #multi-mention e
OntoNotes
Training 297 87,068 22,127 15,983 13,587 2,396
Test 33 9,763 2,463 1,790 1,476 314
ACE
Training 297 87,068 12,951 5,873 3,599 2,274
Test 33 9,763 1,464 746 459 287
Table 4: Corpus statistics for the aligned portion of ACE and OntoNotes on gold-standard data.
MUC B3 CEAF
P R F P R F P / R / F
OntoNotes scheme
1. ALL SINGLETONS ? ? ? 100 72.68 84.18 72.68
2. HEAD MATCH 55.14 39.08 45.74 90.65 80.87 85.48 76.05
3. HEAD MATCH + PRON 47.10 53.05 49.90 82.28 83.13 82.70 75.15
4. STRONG MATCH 46.81 53.34 49.86 80.47 83.54 81.97 76.78
5. SUPER STRONG MATCH 46.51 40.56 43.33 84.95 80.16 82.48 76.70
6. BEST MATCH 52.47 47.40 49.80 86.10 82.80 84.42 77.87
7. WEAK MATCH 47.91 64.64 55.03 71.73 87.46 78.82 71.74
ACE scheme
1. ALL SINGLETONS ? ? ? 100 50.96 67.51 50.96
2. HEAD MATCH 82.35 39.00 52.93 95.27 64.05 76.60 66.46
3. HEAD MATCH + PRON 70.11 53.90 60.94 86.49 68.20 76.27 68.44
4. STRONG MATCH 64.21 64.21 64.21 76.92 73.54 75.19 70.01
5. SUPER STRONG MATCH 60.51 56.55 58.46 76.71 69.19 72.76 66.87
6. BEST MATCH 67.50 56.69 61.62 82.18 71.67 76.57 69.88
7. WEAK MATCH 63.52 80.50 71.01 59.76 86.36 70.64 64.21
Table 5: CISTELL results varying the annotation scheme on gold-standard data.
most twice as large as the number of ACE men-
tions. Unlike OntoNotes, ACE mentions include
premodifiers (e.g., state in state lines), national
adjectives (e.g., Iraqi) and relative pronouns (e.g.,
who, that). Also, given that ACE entities corre-
spond to types that are usually coreferred (e.g.,
people, organizations, etc.), singletons only rep-
resent 61% of all entities, while they are 85% in
OntoNotes. The average entity size is 4 in ACE
and 3.5 in OntoNotes.
A second major difference is the definition of
coreference relations, illustrated here:
(2) [This] was [an all-white, all-Christian community
that all the sudden was taken over ... by different
groups].
(3) [ [Mayor] John Hyman] has a simple answer.
(4) [Postville] now has 22 different nationalities ... For
those who prefer [the old Postville], Mayor John
Hyman has a simple answer.
In ACE, nominal predicates corefer with their
subject (2), and appositive phrases corefer with
the noun they are modifying (3). In contrast,
they do not fall under the identity relation in
OntoNotes, which follows the linguistic under-
standing of coreference according to which nom-
inal predicates and appositives express properties
of an entity rather than refer to a second (corefer-
ent) entity (van Deemter and Kibble, 2000). Fi-
nally, the two schemes frequently disagree on bor-
derline cases in which coreference turns out to be
especially complex (4). As a result, some features
will behave differently, e.g., the appositive feature
has the opposite effect in the two datasets.
Results and Discussion From the differences
pointed out above, the results shown in Table 5
might be surprising at first. Given that OntoNotes
is not restricted to any semantic type and is based
on a more sophisticated definition of coreference,
one would not expect a system to perform better
on it than on ACE. The explanation is given by the
ALL SINGLETONS baseline, which is 73?84% for
OntoNotes and only 51?68% for ACE. The fact
that OntoNotes contains a much larger number of
singletons?as Table 4 shows?results in an ini-
tial boost of performance (except with the MUC
score, which ignores singletons). In contrast, the
score improvement achieved by HEAD MATCH is
much more noticeable on ACE than on OntoNotes,
which indicates that many of its coreferent men-
tions share the same head.
The systematic biases of the measures that were
observed in Table 3 appear again in the case of
1428
MUC and B3. CEAF is divided between BEST
MATCH and STRONG MATCH. The higher value
of the MUC score for ACE is another indication
of its tendency to reward correct links much more
than to penalize spurious ones (ACE has a larger
proportion of multi-mention entities).
The feature rankings obtained for each dataset
generally coincide as to which features are ranked
best (namely NE match, is-a match, and head
match), but differ in their particular ordering.
It is also possible to compare the OntoNotes re-
sults in Tables 3 and 5, the only difference being
that the first training set was three times larger.
Contrary to expectation, the model trained on a
larger dataset performs just slightly better. The
fact that more training data does not necessarily
lead to an increase in performance conforms to
the observation that there appear to be few general
rules (e.g., head match) that systematically gov-
ern coreference relationships; rather, coreference
appeals to individual unique phenomena appear-
ing in each context, and thus after a point adding
more training data does not add much new gener-
alizable information. Pragmatic information (dis-
course structure, world knowledge, etc.) is proba-
bly the key, if ever there is a way to encode it.
6 Parameter 3: Preprocessing
The goal of the third experiment was to determine
how much the source and nature of preprocess-
ing information matters. Since it is often stated
that coreference resolution depends on many lev-
els of analysis, we again compared the two cor-
pora, which differ in the amount and correctness
of such information. However, in this experiment,
entity mapping was applied in the opposite direc-
tion: the OntoNotes entities were mapped onto the
automatically preprocessed ACE dataset. This ex-
poses the shortcomings of automated preprocess-
ing in ACE for identifying all the mentions identi-
fied and linked in OntoNotes.
Datasets The ACE data was morphologically
annotated with a tokenizer based on manual rules
adapted from the one used in CoNLL (Tjong
Kim Sang and De Meulder, 2003), with TnT 2.2,
a trigram POS tagger based on Markov models
(Brants, 2000), and with the built-in WordNet lem-
matizer (Fellbaum, 1998). Syntactic chunks were
obtained from YamCha 1.33, an SVM-based NP-
chunker (Kudoh and Matsumoto, 2000), and parse
trees from Malt Parser 0.4, an SVM-based parser
(Hall et al, 2007).
Although the number of words in Tables 4 and 6
should in principle be the same, the latter con-
tains fewer words as it lacks the null elements
(traces, ellipsed material, etc.) manually anno-
tated in OntoNotes. Missing parse tree nodes in
the automatically parsed data account for the con-
siderably lower number of OntoNotes mentions
(approx. 5,700 fewer mentions).14 However, the
proportions of singleton:multi-mention entities as
well as the average entity size do not vary.
Results and Discussion The ACE scores for the
automatically preprocessed models in Table 7 are
about 3% lower than those based on OntoNotes
gold-standard data in Table 5, providing evidence
for the advantage offered by gold-standard prepro-
cessing information. In contrast, the similar?if
not higher?scores of OntoNotes can be attributed
to the use of the annotated ACE entity types. The
fact that these are annotated not only for proper
nouns (as predicted by an automatic NER) but also
for pronouns and full NPs is a very helpful feature
for a coreference resolution system.
Again, the scoring metrics exhibit similar bi-
ases, but note that CEAF prefers HEAD MATCH
+ PRON in the case of ACE, which is indicative of
the noise brought by automatic preprocessing.
A further insight is offered from comparing the
feature rankings with gold-standard syntax to that
with automatic preprocessing. Since we are evalu-
ating now on the ACE data, the NE match feature
is also ranked first for OntoNotes. Head and is-a
match are still ranked among the best, yet syntac-
tic features are not. Instead, features like NP type
have moved further up. This reranking probably
indicates that if there is noise in the syntactic infor-
mation due to automatic tools, then morphological
and syntactic features switch their positions.
Given that the noise brought by automatic pre-
processing can be harmful, we tried leaving out the
grammatical function feature. Indeed, the results
increased about 2?3%, STRONG MATCH scoring
the highest. This points out that conclusions drawn
from automatically preprocessed data about the
kind of knowledge relevant for coreference reso-
lution might be mistaken. Using the most success-
ful basic features can lead to the best results when
only automatic preprocessing is available.
14In order to make the set of mentions as similar as possible
to the set in Section 5, OntoNotes singletons were mapped
from the ones detected in the gold-standard treebank.
1429
#docs #words #mentions #entities (e) #singleton e #multi-mention e
OntoNotes
Training 297 80,843 16,945 12,127 10,253 1,874
Test 33 9,073 1,931 1,403 1,156 247
ACE
Training 297 80,843 13,648 6,041 3,652 2,389
Test 33 9,073 1,537 775 475 300
Table 6: Corpus statistics for the aligned portion of ACE and OntoNotes on automatically parsed data.
MUC B3 CEAF
P R F P R F P / R / F
OntoNotes scheme
1. ALL SINGLETONS ? ? ? 100 72.66 84.16 72.66
2. HEAD MATCH 56.76 35.80 43.90 92.18 80.52 85.95 76.33
3. HEAD MATCH + PRON 47.44 54.36 50.66 82.08 83.61 82.84 74.83
4. STRONG MATCH 52.66 58.14 55.27 83.11 85.05 84.07 78.30
5. SUPER STRONG MATCH 51.67 46.78 49.11 85.74 82.07 83.86 77.67
6. BEST MATCH 54.38 51.70 53.01 86.00 83.60 84.78 78.15
7. WEAK MATCH 49.78 64.58 56.22 75.63 87.79 81.26 74.62
ACE scheme
1. ALL SINGLETONS ? ? ? 100 50.42 67.04 50.42
2. HEAD MATCH 81.25 39.24 52.92 94.73 63.82 76.26 65.97
3. HEAD MATCH + PRON 69.76 53.28 60.42 86.39 67.73 75.93 68.05
4. STRONG MATCH 58.85 58.92 58.89 73.36 70.35 71.82 66.30
5. SUPER STRONG MATCH 56.19 50.66 53.28 75.54 66.47 70.72 63.96
6. BEST MATCH 63.38 49.74 55.74 80.97 68.11 73.99 65.97
7. WEAK MATCH 60.22 78.48 68.15 55.17 84.86 66.87 59.08
Table 7: CISTELL results varying the annotation scheme on automatically preprocessed data.
7 Conclusion
Regarding evaluation, the results clearly expose
the systematic tendencies of the evaluation mea-
sures. The way each measure is computed makes
it biased towards a specific model: MUC is gen-
erally too lenient with spurious links, B3 scores
too high in the presence of a large number of sin-
gletons, and CEAF does not agree with either of
them. It is a cause for concern that they provide
contradictory indications about the core of coref-
erence, namely the resolution models?for exam-
ple, the model ranked highest by B3 in Table 7 is
ranked lowest by MUC. We always assume eval-
uation measures provide a ?true? reflection of our
approximation to a gold standard in order to guide
research in system development and tuning.
Further support to our claims comes from the
results of SemEval-2010 Task 1 (Recasens et al,
2010). The performance of the six participating
systems shows similar problems with the evalua-
tion metrics, and the singleton baseline was hard
to beat even by the highest-performing systems.
Since the measures imply different conclusions
about the nature of the corpora and the preprocess-
ing information applied, should we use them now
to constrain the ways our corpora are created in
the first place, and what preprocessing we include
or omit? Doing so would seem like circular rea-
soning: it invalidates the notion of the existence of
a true and independent gold standard. But if ap-
parently incidental aspects of the corpora can have
such effects?effects rated quite differently by the
various measures?then we have no fixed ground
to stand on.
The worrisome fact that there is currently no
clearly preferred and ?correct? evaluation measure
for coreference resolution means that we cannot
draw definite conclusions about coreference reso-
lution systems at this time, unless they are com-
pared on exactly the same corpus, preprocessed
under the same conditions, and all three measures
agree in their rankings.
Acknowledgments
We thank Dr. M. Anto`nia Mart?? for her generosity
in allowing the first author to visit ISI to work with
the second. Special thanks to Edgar Gonza`lez for
his kind help with conversion issues.
This work was partially supported by the Span-
ish Ministry of Education through an FPU schol-
arship (AP2006-00994) and the TEXT-MESS 2.0
Project (TIN2009-13391-C04-04).
1430
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC 1998 Workshop on Linguistic Coreference,
pages 563?566, Granada, Spain.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of EMNLP 2008, pages 294?303, Hon-
olulu, Hawaii.
Thorsten Brants. 2000. TnT ? A statistical part-of-
speech tagger. In Proceedings of ANLP 2000, Seat-
tle, WA.
Aron Culotta, Michael Wick, Robert Hall, and Andrew
McCallum. 2007. First-order probabilistic models
for coreference resolution. In Proceedings of HLT-
NAACL 2007, pages 81?88, Rochester, New York.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
42:87?96.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) Program - Tasks, Data, and Evaluation.
In Proceedings of LREC 2004, pages 837?840.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Jenny Rose Finkel and Christopher D. Manning.
2008. Enforcing transitivity in coreference resolu-
tion. In Proceedings of ACL-HLT 2008, pages 45?
48, Columbus, Ohio.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of EMNLP 2009, pages
1152?1161, Singapore. Association for Computa-
tional Linguistics.
Johan Hall, Jens Nilsson, Joakim Nivre, Gu?lsen
Eryigit, Bea?ta Megyesi, Mattias Nilsson, and
Markus Saers. 2007. Single malt or blended?
A study in multilingual parser optimization. In
Proceedings of the CoNLL shared task session of
EMNLP-CoNLL 2007, pages 933?939.
Lynette Hirschman and Nancy Chinchor. 1997. MUC-
7 Coreference Task Definition ? Version 3.0. In Pro-
ceedings of MUC-7.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: the 90% solution. In Proceedings of
HLT-NAACL 2006, pages 57?60.
Taku Kudoh and Yuji Matsumoto. 2000. Use of sup-
port vector learning for chunk identification. In Pro-
ceedings of CoNLL 2000 and LLL 2000, pages 142?
144, Lisbon, Portugal.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-lingual
coreference resolution with syntactic features. In
Proceedings of HLT-EMNLP 2005, pages 660?667,
Vancouver.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of ACL 2004, pages
21?26, Barcelona.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32, Vancouver.
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of ACL 2002, pages 104?111,
Philadelphia.
Vincent Ng. 2009. Graph-cut-based anaphoricity de-
termination for coreference resolution. In Proceed-
ings of NAACL-HLT 2009, pages 575?583, Boulder,
Colorado.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov logic.
In Proceedings of EMNLP 2008, pages 650?659,
Honolulu, Hawaii.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
ICSC 2007, pages 517?526, Washington, DC.
Marta Recasens and Eduard Hovy. 2009. A
Deeper Look into Features for Coreference Res-
olution. In S. Lalitha Devi, A. Branco, and
R. Mitkov, editors, Anaphora Processing and Ap-
plications (DAARC 2009), volume 5847 of LNAI,
pages 29?42. Springer-Verlag.
Marta Recasens and M. Anto`nia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI 10.1007/s10579-009-9108-x.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the Fifth In-
ternational Workshop on Semantic Evaluations (Se-
mEval 2010), Uppsala, Sweden.
Wee M. Soon, Hwee T. Ng, and Daniel C. Y. Lim.
2001. A machine learning approach to coreference
resolution of noun phrases. Computational Linguis-
tics, 27(4):521?544.
1431
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP 2009,
pages 656?664, Singapore.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003. Introduction to the CoNLL-2003 Shared
Task: Language-independent Named Entity Recog-
nition. In Walter Daelemans and Miles Osborne, ed-
itors, Proceedings of CoNLL 2003, pages 142?147.
Edmonton, Canada.
Olga Uryupina. 2006. Coreference resolution with
and without linguistic knowledge. In Proceedings
of LREC 2006.
Kees van Deemter and Rodger Kibble. 2000. On core-
ferring: Coreference in MUC and related annotation
schemes. Computational Linguistics, 26(4):629?
637.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52, San Francisco.
1432
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482?1491,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Arguments and Supertypes of Semantic Relations using
Recursive Patterns
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
A challenging problem in open informa-
tion extraction and text mining is the learn-
ing of the selectional restrictions of se-
mantic relations. We propose a mini-
mally supervised bootstrapping algorithm
that uses a single seed and a recursive
lexico-syntactic pattern to learn the ar-
guments and the supertypes of a diverse
set of semantic relations from the Web.
We evaluate the performance of our algo-
rithm on multiple semantic relations ex-
pressed using ?verb?, ?noun?, and ?verb
prep? lexico-syntactic patterns. Human-
based evaluation shows that the accuracy
of the harvested information is about 90%.
We also compare our results with existing
knowledge base to outline the similarities
and differences of the granularity and di-
versity of the harvested knowledge.
1 Introduction
Building and maintaining knowledge-rich re-
sources is of great importance to information ex-
traction, question answering, and textual entail-
ment. Given the endless amount of data we have at
our disposal, many efforts have focused on mining
knowledge from structured or unstructured text,
including ground facts (Etzioni et al, 2005), se-
mantic lexicons (Thelen and Riloff, 2002), ency-
clopedic knowledge (Suchanek et al, 2007), and
concept lists (Katz et al, 2003). Researchers have
also successfully harvested relations between en-
tities, such as is-a (Hearst, 1992; Pasca, 2004) and
part-of (Girju et al, 2003). The kinds of knowl-
edge learned are generally of two kinds: ground
instance facts (New York is-a city, Rome is the cap-
ital of Italy) and general relational types (city is-a
location, engines are part-of cars).
A variety of NLP tasks involving inference or
entailment (Zanzotto et al, 2006), including QA
(Katz and Lin, 2003) and MT (Mt et al, 1988),
require a slightly different form of knowledge, de-
rived from many more relations. This knowledge
is usually used to support inference and is ex-
pressed as selectional restrictions (Wilks, 1975)
(namely, the types of arguments that may fill a
given relation, such as person live-in city and air-
line fly-to location). Selectional restrictions con-
strain the possible fillers of a relation, and hence
the possible contexts in which the patterns ex-
pressing that relation can participate in, thereby
enabling sense disambiguation of both the fillers
and the expression itself.
To acquire this knowledge two common ap-
proaches are employed: clustering and patterns.
While clustering has the advantage of being fully
unsupervised, it may or may not produce the types
and granularity desired by a user. In contrast
pattern-based approaches are more precise, but
they typically require a handful to dozens of seeds
and lexico-syntactic patterns to initiate the learn-
ing process. In a closed domain these approaches
are both very promising, but when tackling an un-
bounded number of relations they are unrealistic.
The quality of clustering decreases as the domain
becomes more continuously varied and diverse,
and it has proven difficult to create collections of
effective patterns and high-yield seeds manually.
In addition, the output of most harvesting sys-
tems is a flat list of lexical semantic expressions
such as ?New York is-a city? and ?virus causes
flu?. However, using this knowledge in inference
requires it to be formulated appropriately and or-
ganized in a semantic repository. (Pennacchiotti
and Pantel, 2006) proposed an algorithm for au-
tomatically ontologizing semantic relations into
WordNet. However, despite its high precision en-
tries, WordNet?s limited coverage makes it impos-
sible for relations whose arguments are not present
in WordNet to be incorporated. One would like a
procedure that dynamically organizes and extends
1482
its semantic repository in order to be able to ac-
commodate all newly-harvested information, and
thereby become a global semantic repository.
Given these considerations, we address in this
paper the following question: How can the selec-
tional restrictions of semantic relations be learned
automatically from the Web with minimal effort us-
ing lexico-syntactic recursive patterns?
The contributions of the paper are as follows:
? A novel representation of semantic relations
using recursive lexico-syntactic patterns.
? An automatic procedure to learn the se-
lectional restrictions (arguments and super-
types) of semantic relations from Web data.
? An exhaustive human-based evaluation of the
harvested knowledge.
? A comparison of the results with some large
existing knowledge bases.
The rest of the paper is organized as follows. In
the next section, we review related work. Section
3 addresses the representation of semantic rela-
tions using recursive patterns. Section 4 describes
the bootstrapping mechanism that learns the selec-
tional restrictions of the relations. Section 5 de-
scribes data collection. Section 6 discusses the ob-
tained results. Finally, we conclude in Section 7.
2 Related Work
A substantial body of work has been done in at-
tempts to harvest bits of semantic information, in-
cluding: semantic lexicons (Riloff and Shepherd,
1997), concept lists (Lin and Pantel, 2002), is-
a relations (Hearst, 1992; Etzioni et al, 2005;
Pasca, 2004; Kozareva et al, 2008), part-of re-
lations (Girju et al, 2003), and others. Knowl-
edge has been harvested with varying success both
from structured text such as Wikipedia?s infoboxes
(Suchanek et al, 2007) or unstructured text such
as the Web (Pennacchiotti and Pantel, 2006; Yates
et al, 2007). A variety of techniques have been
employed, including clustering (Lin and Pantel,
2002), co-occurrence statistics (Roark and Char-
niak, 1998), syntactic dependencies (Pantel and
Ravichandran, 2004), and lexico-syntactic pat-
terns (Riloff and Jones, 1999; Fleischman and
Hovy, 2002; Thelen and Riloff, 2002).
When research focuses on a particular relation,
careful attention is paid to the pattern(s) that ex-
press it in various ways (as in most of the work
above, notably (Riloff and Jones, 1999)). But it
has proven a difficult task to manually find ef-
fectively different variations and alternative pat-
terns for each relation. In contrast, when re-
search focuses on any relation, as in TextRun-
ner (Yates et al, 2007), there is no standardized
manner for re-using the pattern learned. TextRun-
ner scans sentences to obtain relation-independent
lexico-syntactic patterns to extract triples of the
form (John, fly to, Prague). The middle string de-
notes some (unspecified) semantic relation while
the first and third denote the learned arguments of
this relation. But TextRunner does not seek spe-
cific semantic relations, and does not re-use the
patterns it harvests with different arguments in or-
der to extend their yields.
Clearly, it is important to be able to specify both
the actual semantic relation sought and use its tex-
tual expression(s) in a controlled manner for max-
imal benefit.
The objective of our research is to combine the
strengths of the two approaches, and, in addition,
to provide even richer information by automati-
cally mapping each harvested argument to its su-
pertype(s) (i.e., its semantic concepts). For in-
stance, given the relation destination and the pat-
tern X flies to Y, automatically determining that
John, Prague) and (John, conference) are two
valid filler instance pairs, that (RyanAir, Prague)
is another, as well as that person and airline are
supertypes of the first argument and city and event
of the second. This information provides the se-
lectional restrictions of the given semantic rela-
tion, indicating that living things like people can
fly to cities and events, while non-living things like
airlines fly mainly to cities. This is a significant
improvement over systems that output a flat list
of lexical semantic knowledge (Thelen and Riloff,
2002; Yates et al, 2007; Suchanek et al, 2007).
Knowing the sectional restrictions of a semantic
relation supports inference in many applications,
for example enabling more accurate information
extraction. (Igo and Riloff, 2009) report that pat-
terns like ?attack on ?NP?? can learn undesirable
words due to idiomatic expressions and parsing er-
rors. Over time this becomes problematic for the
bootstrapping process and leads to significant de-
terioration in performance. (Thelen and Riloff,
2002) address this problem by learning multiple
semantic categories simultaneously, relying on the
often unrealistic assumption that a word cannot
belong to more than one semantic category. How-
1483
ever, if we have at our disposal a repository of se-
mantic relations with their selectional restrictions,
the problem addressed in (Igo and Riloff, 2009)
can be alleviated.
In order to obtain selectional restriction classes,
(Pennacchiotti and Pantel, 2006) made an attempt
to ontologize the harvested arguments of is-a,
part-of, and cause relations. They mapped each
argument of the relation into WordNet and identi-
fied the senses for which the relation holds. Un-
fortunately, despite its very high precision en-
tries, WordNet is known to have limited cover-
age, which makes it impossible for algorithms to
map the content of a relation whose arguments
are not present in WordNet. To surmount this
limitation, we do not use WordNet, but employ
a different method of obtaining superclasses of a
filler term: the inverse doubly-anchored patterns
DAP?1 (Hovy et al, 2009), which, given two ar-
guments, harvests its supertypes from the source
corpus. (Hovy et al, 2009) show that DAP?1 is
reliable and it enriches WordNet with additional
hyponyms and hypernyms.
3 Recursive Patterns
A singly-anchored pattern contains one example
of the seed term (the anchor) and one open posi-
tion for the term to be learned. Most researchers
use singly-anchored patterns to harvest semantic
relations. Unfortunately, these patterns run out of
steam very quickly. To surmount this obstacle, a
handful of seeds is generally used, and helps to
guarantee diversity in the extraction of new lexico-
syntactic patterns (Riloff and Jones, 1999; Snow et
al., 2005; Etzioni et al, 2005).
Some algorithms require ten seeds (Riloff and
Jones, 1999; Igo and Riloff, 2009), while others
use a variation of 5, 10, to even 25 seeds (Taluk-
dar et al, 2008). Seeds may be chosen at ran-
dom (Davidov et al, 2007; Kozareva et al, 2008),
by picking the most frequent terms of the desired
class (Igo and Riloff, 2009), or by asking humans
(Pantel et al, 2009). As (Pantel et al, 2009) show,
picking seeds that yield high numbers of differ-
ent terms is difficult. Thus, when dealing with
unbounded sets of relations (Banko and Etzioni,
2008), providing many seeds becomes unrealistic.
Interestingly, recent work reports a class of pat-
terns that use only one seed to learn as much infor-
mation with only one seed. (Kozareva et al, 2008;
Hovy et al, 2009) introduce the so-called doubly-
anchored pattern (DAP) that has two anchor seed
positions ??type? such as ?seed? and *?, plus one
open position for the terms to be learned. Learned
terms can then be replaced into the seed position
automatically, creating a recursive procedure that
is reportedly much more accurate and has much
higher final yield. (Kozareva et al, 2008; Hovy et
al., 2009) have successfully applied DAP for the
learning of hyponyms and hypernyms of is-a rela-
tions and report improvements over (Etzioni et al,
2005) and (Pasca, 2004).
Surprisingly, this work was limited to the se-
mantic relation is-a. No other study has described
the use or effect of recursive patterns for differ-
ent semantic relations. Therefore, going beyond
(Kozareva et al, 2008; Hovy et al, 2009), we here
introduce recursive patterns other than DAP that
use only one seed to harvest the arguments and su-
pertypes of a wide variety of relations.
(Banko and Etzioni, 2008) show that seman-
tic relations can be expressed using a handful
of relation-independent lexico-syntactic patterns.
Practically, we can turn any of these patterns into
recursive form by giving as input only one of the
arguments and leaving the other one as an open
slot, allowing the learned arguments to replace the
initial seed argument directly. For example, for
the relation ?fly to?, the following recursive pat-
terns can be built: ?* and ?seed? fly to *?, ??seed?
and * fly to *?, ?* fly to ?seed? and *?, ?* fly to *
and ?seed??, ??seed? fly to *? or ?* fly to ?seed??,
where ?seed? is an example like John or Ryanair,
and (?) indicates the position on which the ar-
guments are learned. Conjunctions like and, or
are useful because they express list constructions
and extract arguments similar to the seed. Poten-
tially, one can explore all recursive pattern varia-
tions when learning a relation and compare their
yield, however this study is beyond the scope of
this paper.
We are particularly interested in the usage of re-
cursive patterns for the learning of semantic re-
lations not only because it is a novel method,
but also because recursive patterns of the DAP
fashion are known to: (1) learn concepts with
high precision compared to singly-anchored pat-
terns (Kozareva et al, 2008), (2) use only one
seed instance for the discovery of new previously
unknown terms, and (3) harvest knowledge with
minimal supervision.
1484
4 Bootstrapping Recursive Patterns
4.1 Problem Formulation
The main goal of our research is:
Task Definition: Given a seed and a semantic relation ex-
pressed using a recursive lexico-syntactic pattern, learn in
bootstrapping fashion the selectional restrictions (i.e., the
arguments and supertypes) of the semantic relation from
an unstructured corpus such as the Web.
Figure 1 shows an example of the task and the
types of information learned by our algorithm.
* and John fly to *
seed = Johnrelation = fly to    
BrianKate
politicianspeopleartists
DeltaAlaskaairlinescarriers
beesanimals
party event
ItalyFrance countries
New York city
flowerstrees plants
Figure 1: Bootstrapping Recursive Patterns.
Given a seed John and a semantic relation fly to
expressed using the recursive pattern ?* and John
fly to *?, our algorithm learns the left side argu-
ments {Brian, Kate, bees, Delta, Alaska} and the
right side arguments {flowers, trees, party, New
York, Italy, France}. For each argument, the algo-
rithm harvests supertypes such as {people, artists,
politicians, airlines, city, countries, plants, event}
among others. The colored links between the right
and left side concepts denote the selectional re-
strictions of the relation. For instance, people fly
to events and countries, but never to trees or flow-
ers.
4.2 System Architecture
We propose a minimally supervised bootstrap-
ping algorithm based on the framework adopted in
(Kozareva et al, 2008; Hovy et al, 2009). The al-
gorithm has two phases: argument harvesting and
supertype harvesting. The final output is a ranked
list of interlinked concepts which captures the se-
lectional restrictions of the relation.
4.2.1 Argument Harvesting
In the argument extraction phase, the first boot-
strapping iteration is initiated with a seed Y and a
recursive pattern ?X? and Y verb+prep|verb|noun
Z??, where X? and Z? are the placeholders for the
arguments to be learned. The pattern is submit-
ted to Yahoo! as a web query and all unique snip-
pets matching the query are retrieved. The newly
learned and previously unexplored arguments on
the X? position are used as seeds in the subse-
quent iteration. The arguments on the Z? posi-
tion are stored at each iteration, but never used
as seeds since the recursivity is created using the
terms on X and Y . The bootstrapping process is
implemented as an exhaustive breadth-first algo-
rithm which terminates when all arguments are ex-
plored.
We noticed that despite the specific lexico-
syntactic structure of the patterns, erroneous in-
formation can be acquired due to part-of-speech
tagging errors or flawed facts on the Web. The
challenge is to identify and separate the erroneous
from the true arguments. We incorporate the har-
vested arguments on X and Y positions in a di-
rected graph G = (V,E), where each vertex
v ? V is a candidate argument and each edge
(u, v) ? E indicates that the argument v is gener-
ated by the argument u. An edge has weight w cor-
responding to the number of times the pair (u, v)
is extracted from different snippets. A node u
is ranked by u=
?
?(u,v)?E
w(u,v)+
?
?(v,u)?E
w(v,u)
|V |?1
which represents the weighted sum of the outgo-
ing and incoming edges normalized by the total
number of nodes in the graph. Intuitively, our con-
fidence in a correct argument u increases when the
argument (1) discovers and (2) is discovered by
many different arguments.
Similarly, to rank the arguments standing on
the Z position, we build a bipartite graph G? =
(V ?, E?) that has two types of vertices. One set
of vertices represents the arguments found on the
Y position in the recursive pattern. We will call
these Vy. The second set of vertices represents the
arguments learned on the Z position. We will call
these Vz . We create an edge e?(u?, v?) ? E? be-
tween u? ? Vy and v? ? Vz when the argument on
the Z position represented by v? was harvested by
the argument on the Y position represented by u?.
The weight w? of the edge indicates the number
of times an argument on the Y position found Z.
Vertex v? is ranked as v?=
?
?(u?,v?)?E?
w(u?,v?)
|V ?|?1 . In
a very large corpus, like the Web, we assume that
a correct argument Z is the one that is frequently
discovered by various arguments Y .
1485
4.2.2 Supertype Harvesting
In the supertype extraction phase, we take all
<X,Y> argument pairs collected during the argu-
ment harvesting stage and instantiate them in the
inverse DAP?1 pattern ?* such as X and Y?. The
query is sent to Yahoo! as a web query and all 1000
snippets matching the pattern are retrieved. For
each <X,Y> pair, the terms on the (*) position are
extracted and considered as candidate supertypes.
To avoid the inclusion of erroneous supertypes,
again we build a bipartite graph G?? = (V ??, E??).
The set of vertices Vsup represents the supertypes,
while the set of vertices Vp corresponds to the
?X,Y? pair that produced the supertype. An edge
e??(u??, v??) ? E??, where u?? ? Vp and v?? ? Vsup
shows that the pair ?X,Y? denoted as u?? harvested
the supertype represented by v??.
For example, imagine that the argument X?=
Ryanair was harvested in the previous phase by
the recursive pattern ?X? and EasyJet fly to Z??.
Then the pair ?Ryanair,EasyJet? forms a new Web
query ?* such as Ryanair and EasyJet? which
learns the supertypes ?airlines? and ?carriers?.
The bipartite graph has two vertices v??1 and v
??
2 for
the supertypes ?airlines? and ?carriers?, one ver-
tex u??3 for the argument pair ?Ryanair, EasyJet?,
and two edges e??1(u
??
3, v
??
1) and e
??
2(u
??
3, v
??
1). A vertex
v?? ? Vsup is ranked by v??=
?
?(u??,v??)?E??
w(u??,v??)
|V ??|?1 .
Intuitively, a supertype which is discovered mul-
tiple times by various argument pairs is ranked
highly.
However, it might happen that a highly ranked
supertype actually does not satisfy the selectional
restrictions of the semantic relation. To avoid such
situations, we further instantiate each supertype
concept in the original pattern1. For example,
?aircompanies fly to *? and ?carriers fly to *?. If
the candidate supertype produces many web hits
for the query, then this suggests that the term is a
relevant supertype.
Unfortunately, to learn the supertypes of the Z
arguments, currently we have to form all possi-
ble combinations among the top 150 highly ranked
concepts, because these arguments have not been
learned through pairing. For each pair of Z argu-
ments, we repeat the same procedure as described
above.
1Except for the ?dress? and ?person? relations, where
the targeted arguments are adjectives, and the supertypes are
nouns.
5 Semantic Relations
So far, we have described the mechanism that
learns from one seed and a recursive pattern the
selectional restrictions of any semantic relation.
Now, we are interested in evaluating the per-
formance of our algorithm. A natural question
that arises is: ?How many patterns are there??.
(Banko and Etzioni, 2008) found that 95% of the
semantic relations can be expressed using eight
lexico-syntactic patterns. Space prevents us from
describing all of them, therefore we focus on the
three most frequent patterns which capture a large
diversity of semantic relations. The relative fre-
quency of these patterns is 37.80% for ?verbs?,
22.80% for ?noun prep?, and 16.00% for ?verb
prep?.
5.1 Data Collection
Table 1 shows the lexico-syntactic pattern and the
initial seed we used to express each semantic rela-
tion. To collect data, we ran our knowledge har-
vesting algorithm until complete exhaustion. For
each query submitted to Yahoo!, we retrieved the
top 1000 web snippets and kept only the unique
ones. In total, we collected 30GB raw data which
was part-of-speech tagged and used for the argu-
ment and supertype extraction. Table 1 shows the
obtained results.
recursive pattern seed X arg Z arg #iter
X and Y work for Z Charlie 2949 3396 20
X and Y fly to Z EasyJet 772 1176 19
X and Y go to Z Rita 18406 27721 13
X and Y work in Z John 4142 4918 13
X and Y work on Z Mary 4126 5186 7
X and Y work at Z Scott 1084 1186 14
X and Y live in Z Harry 8886 19698 15
X and Y live at Z Donald 1102 1175 15
X and Y live with Z Peter 1344 834 11
X and Y cause Z virus 12790 52744 19
X and Y celebrate Jim 6033 ? 12
X and Y drink Sam 1810 ? 13
X and Y dress nice 1838 ? 8
X and Y person scared 2984 ? 17
Table 1: Total Number of Harvested Arguments.
An interesting characteristic of the recursive
patterns is the speed of leaning which can be mea-
sured in terms of the number of unique argu-
ments acquired during each bootstrapping itera-
tion. Figure 2 shows the bootstrapping process for
the ?cause? and ?dress? relations. Although both
relations differ in terms of the total number of it-
erations and harvested items, the overall behavior
of the learning curves is similar. Learning starts
of very slowly and as bootstrapping progresses a
1486
rapid growth is observed until a saturation point is
reached.
 0
 10000
 20000
 30000
 40000
 50000
 60000
 1  2  3  4  5  6  7  8  9  10  11 12  13 14  15 16  17 18  19
#
I
t
e
m
s
 
L
e
a
r
n
e
d
Iterations
X and Y Cause Z
XZ
 0
 500
 1000
 1500
 2000
 1  2  3  4  5  6  7  8
#
I
t
e
m
s
 
L
e
a
r
n
e
d
Iterations
X and Y Dress
X
Figure 2: Items extracted in 10 iterations.
The speed of leaning is related to the connectiv-
ity behavior of the arguments of the relation. In-
tuitively, a densely connected graph takes shorter
time (i.e., fewer iterations) to be learned, as in the
?work on? relation, while a weakly connected net-
work takes longer time to harvest the same amount
of information, as in the ?work for? relation.
6 Results
In this section, we evaluate the results of our
knowledge harvesting algorithm. Initially, we de-
cided to conduct an automatic evaluation compar-
ing our results to knowledge bases that have been
extracted in a similar way (i.e., through pattern ap-
plication over unstructured text). However, it is
not always possible to perform a complete com-
parison, because either researchers have not fully
explored the same relations we have studied, or for
those relations that overlap, the gold standard data
was not available.
The online demo of TextRunner2 (Yates et al,
2007) actually allowed us to collect the arguments
for all our semantic relations. However, due to
Web based query limitations, TextRunner returns
only the first 1000 snippets. Since we do not have
the complete and ranked output of TextRunner,
comparing results in terms of recall and precision
is impossible.
Turning instead to results obtained from struc-
tured sources (which one expects to have high
correctness), we found that two of our relations
overlap with those of the freely available ontology
Yago (Suchanek et al, 2007), which was harvested
from the Infoboxes tables in Wikipedia. In addi-
tion, we also had two human annotators judge as
many results as we could afford, to obtain Preci-
sion. We conducted two evaluations, one for the
arguments and one for the supertypes.
2http://www.cs.washington.edu/research/textrunner/
6.1 Human-Based Argument Evaluation
In this section, we discuss the results of the har-
vested arguments. For each relation, we selected
the top 200 highly ranked arguments. We hired
two annotators to judge their correctness. We cre-
ated detailed annotation guidelines that define the
labels for the arguments of the relations, as shown
in Table 2. (Previously, for the same task, re-
searchers have not conducted such an exhaustive
and detailed human-based evaluation.) The anno-
tation was conducted using the CAT system3.
TYPE LABEL EXAMPLES
Correct Person John, Mary
Role mother, president
Group team, Japanese
Physical yellow, shabby
NonPhysical ugly, thought
NonLiving airplane
Organization IBM, parliament
Location village, New York, in the house
Time at 5 o?clock
Event party, prom, earthquake
State sick, anrgy
Manner live in happiness
Medium work on Linux, Word
Fixed phrase go to war
Incorrect Error wrong part-of-speech tag
Other none of the above
Table 2: Annotation Labels.
We allow multiple labels to be assigned to the
same concept, because sometimes the concept can
appear in different contexts that carry various con-
ceptual representations. Although the labels can
be easily collapsed to judge correct and incorrect
terms, the fine-grained annotation shown here pro-
vides a better overview of the information learned
by our algorithm.
We measured the inter-annotator agreement for
all labels and relations considering that a single
entry can be tagged with multiple labels. The
Kappa score is around 0.80. This judgement is
good enough to warrant using these human judge-
ments to estimate the accuracy of the algorithm.
We compute Accuracy as the number of examples
tagged as Correct divided by the total number of
examples.
Table 4 shows the obtained results. The over-
all accuracy of the argument harvesting phase is
91%. The majority of the occurred errors are due
to part-of-speech tagging. Table 3 shows a sam-
ple of 10 randomly selected examples from the top
200 ranked and manually annotated arguments.
3http://cat.ucsur.pitt.edu/default.aspx
1487
Relation Arguments
(X) Dress: stylish, comfortable, expensive, shabby, gorgeous
silver, clean, casual, Indian, black
(X) Person: honest, caring, happy, intelligent, gifted
friendly, responsible, mature, wise, outgoing
(X) Cause: pressure, stress, fire, bacteria, cholesterol
flood, ice, cocaine, injuries, wars
GoTo (Z): school, bed, New York, the movies, the park, a bar
the hospital, the church, the mall, the beach
LiveIn (Z): peace, close proximity, harmony, Chicago, town
New York, London, California, a house, Australia
WorkFor (Z): a company, the local prison, a gangster, the show
a boss, children, UNICEF, a living, Hispanics
Table 3: Examples of Harvested Arguments.
6.2 Comparison against Existing Resources
In this section, we compare the performance of our
approach with the semantic knowledge base Yago4
that contains 2 million entities5, 95% of which
were manually confirmed to be correct. In this
study, we compare only the unique arguments of
the ?live in? and ?work at? relations. We provide
Precision scores using the following measures:
PrY ago =
#terms found in Y ago
#terms harvested by system
PrHuman =
#terms judged correct by human
#terms harvested by system
NotInY ago = #terms judged correct by human but not in Y ago
Table 5 shows the obtained results.
We carefully analyzed those arguments that
were found by one of the systems but were miss-
ing in the other. The recursive patterns learn infor-
mation about non-famous entities like Peter and
famous entities like Michael Jordan. In contrast,
Yago contains entries mostly about famous enti-
ties, because this is the predominant knowledge in
Wikipedia. For the ?live in? relation, both repos-
itories contain the same city and country names.
However, the recursive pattern learned arguments
like pain, effort which express a manner of living,
and locations like slums, box. This information is
missing from Yago. Similarly for the ?work at?
relation, both systems learned that people work
at universities. In addition, the recursive pattern
learned a diversity of company names absent from
Yago.
While it is expected that our algorithm finds
many terms not contained in Yago?specifically,
the information not deemed worthy of inclusion
in Wikipedia?we are interested in the relatively
large number of terms contained in Yago but not
found by our algorithm. To our knowledge, no
4http://www.mpi-inf.mpg.de/yago-naga/yago/
5Names of cities, people, organizations among others.
X WorkFor A1 A2 WorkFor Z A1 A2
Person 148 152 Organization 111 110
Role 5 7 Person 60 60
Group 12 14 Event 4 2
Organization 8 7 Time 4 5
NonPhysical 22 23 NonPhysical 18 19
Other 5 5 Other 3 4
Acc. .98 .98 Acc. .99 .98
X Cause A1 A2 Cause Z A1 A2
PhysicalObj 82 75 PhysicalObj 15 20
NonPhysicalObj 69 66 NonPhysicalObj 89 91
Event 21 24 Event 72 72
State 29 31 State 50 50
Other 3 4 Other 5 4
Acc. .99 .98 Acc. .98 .98
X GoTo A1 A2 GoTo Z A1 A2
Person 190 188 Location 163 155
Role 4 4 Event 21 30
Group 3 3 Person 11 13
NonPhysical 1 3 NonPhysical 2 1
Other 2 2 Other 3 1
Acc. .99 .99 Acc. .99 .99
X FlyTo A1 A2 FlyTo Z A1 A2
Person 140 139 Location 199 198
Organization 54 57 Event 1 2
NonPhysical 2 2 Person 0 0
Other 4 2 Other 0 0
Acc. .98 .99 Acc. 1 1
X WorkOn A1 A2 WorkOn Z A1 A2
Person 173 172 Location 110 108
Role 2 3 Organization 27 25
Group 4 5 Manner 38 40
Organization 6 6 Time 4 4
NonPhysical 15 14 NonPhysical 18 21
Error 1 1 Medium 8 8
Other 1 1 Other 13 15
Acc. .99 .99 Acc. .94 .93
X WorkIn A1 A2 WorkIn Z A1 A2
Person 117 118 Location 104 111
Group 10 9 Organization 10 25
Organization 3 3 Manner 39 40
Fixed 3 1 Time 4 4
NonPhysical 55 59 NonPhysical 22 21
Error 12 10 Medium 8 8
Other 0 0 Error 13 15
Acc. .94 .95 Acc. .94 .93
X WorkAt A1 A2 WorkAt Z A1 A2
Person 193 192 Organization 189 190
Role 1 1 Manner 5 4
Group 1 1 Time 3 3
Organization 0 0 Error 3 2
Other 5 6 Other 0 1
Acc. .98 .97 Acc. .99 .99
X LiveIn A1 A2 LiveIn Z A1 A2
Person 185 185 Location 182 186
Role 3 4 Manner 6 8
Group 9 8 Time 1 2
NonPhysical 1 2 Fixed 5 2
Other 2 1 Other 6 2
Acc. .99 .99 Acc. .97 .99
X LiveAt A1 A2 LiveAt Z A1 A2
Person 196 195 Location 158 157
Role 1 1 Person 5 7
NonPhysical 0 1 Manner 1 2
Other 3 3 Error 36 34
Acc. .99 .99 Acc. .82 .83
X LiveWith A1 A2 LiveWith Z A1 A2
Person 188 187 Person 165 163
Role 6 6 Animal 2 4
Group 2 2 Manner 15 15
NonPhysical 2 3 NonPhysical 15 15
Other 2 2 Other 3 3
Acc. .99 .99 Acc. .99 .99
X Dress A1 A2 X Person A1 A2
Physical 72 59 Physical 8 2
NonPhysical 120 136 NonPhysical 188 194
Other 8 5 Other 4 4
Acc .96 .98 Acc. .98 .98
X Drink A1 A2 X Celebrate A1 A2
Living 165 174 Living 157 164
NonLiving 8 2 NonLiving 42 35
Error 27 24 Error 1 1
Acc .87 .88 Acc. .99 .99
Table 4: Harvested Arguments.
1488
PrY ago PrHuman NotInYago
X LiveIn .19 (2863/14705) .58 (5165)/8886 2302
LiveIn Z .10 (495/4754) .72 (14248)/19698 13753
X WorkAt .12(167/1399) .88 (959)/1084 792
WorkAt Z .3(15/525) .95 (1128)/1186 1113
Table 5: Comparison against Yago.
other automated harvesting algorithm has ever
been compared to Yago, and our results here form
a baseline that we aim to improve upon. And in
the future, one can build an extensive knowledge
harvesting system combining the wisdom of the
crowd and Wikipedia.
6.3 Human-Based Supertype Evaluation
In this section, we discuss the results of harvest-
ing the supertypes of the learned arguments. Fig-
ure 3 shows the top 100 ranked supertypes for the
?cause? and ?work on? relations. The x-axis in-
dicates a supertype, the y-axis denotes the number
of different argument pairs that lead to the discov-
ery of the supertype.
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 10  20  30  40  50  60  70  80  90  100
#
P
a
i
r
s
 
D
i
s
c
o
v
e
r
i
n
g
 
t
h
e
 
S
u
p
e
r
t
y
p
e
Supertype
WorkOnCause
Figure 3: Ranked Supertypes.
The decline of the curve indicates that certain
supertypes are preferred and shared among differ-
ent argument pairs. It is interesting to note that the
text on the Web prefers a small set of supertypes,
and to see what they are. These most-popular har-
vested types tend to be the more descriptive terms.
The results indicate that one does not need an elab-
orate supertype hierarchy to handle the selectional
restrictions of semantic relations.
Since our problem definition differs from avail-
able related work, and WordNet does not contain
all harvested arguments as shown in (Hovy et al,
2009), it is not possible to make a direct compar-
ison. Instead, we conduct a manual evaluation of
the most highly ranked supertypes which normally
are the top 20. The overall accuracy of the super-
types for all relations is 92%. Table 6 shows the
Relation Arguments
(Supx) Celebrate: men, people, nations, angels, workers, children
countries, teams, parents, teachers
(Supx) Dress: colors, effects, color tones, activities, patterns
styles, materials, size, languages, aspects
(Supx) FlyTo: airlines, carriers, companies, giants, people
competitors, political figures, stars, celebs
Cause (Supz): diseases, abnormalities, disasters, processes, isses
disorders, discomforts, emotions, defects, symptoms
WorkFor (Supz) organizations, industries, people, markets, men
automakers, countries, departments, artists, media
GoTo (Supz) : countries, locations, cities, people, events
men, activities, games, organizations,
FlyTo (Supz) places, countries, regions, airports, destinations
locations, cities, area, events
Table 6: Examples of Harvested Supertypes.
top 10 highly ranked supertypes for six of our re-
lations.
7 Conclusion
We propose a minimally supervised algorithm that
uses only one seed example and a recursive lexico-
syntactic pattern to learn in bootstrapping fash-
ion the selectional restrictions of a large class of
semantic relations. The principal contribution of
the paper is to demonstrate that this kind of pat-
tern can be applied to almost any kind of se-
mantic relation, as long as it is expressible in
a concise surface pattern, and that the recursive
mechanism that allows each newly acquired term
to restart harvesting automatically is a signifi-
cant advance over patterns that require a handful
of seeds to initiate the learning process. It also
shows how one can combine free-form but undi-
rected pattern-learning approaches like TextRun-
ner with more-controlled but effort-intensive ap-
proaches like commonly used.
In our evaluation, we show that our algorithm is
capable of extracting high quality non-trivial in-
formation from unstructured text given very re-
stricted input (one seed). To measure the perfor-
mance of our approach, we use various semantic
relations expressed with three lexico-syntactic pat-
terns. For two of the relations, we compare results
with the freely available ontology Yago, and con-
duct a manual evaluation of the harvested terms.
We will release the annotated and the harvested
data to the public to be used for comparison by
other knowledge harvesting algorithms.
The success of the proposed framework opens
many challenging directions. We plan to use the
algorithm described in this paper to learn the se-
lectional restrictions of numerous other relations,
in order to build a rich knowledge repository
1489
that can support a variety of applications, includ-
ing textual entailment, information extraction, and
question answering.
Acknowledgments
This research was supported by DARPA contract
number FA8750-09-C-3705.
References
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, June.
Dmitry Davidov, Ari Rappoport, and Moshel Koppel.
2007. Fully unsupervised discovery of concept-
specific relationships by web mining. In Proc. of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 232?239, June.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Un-
supervised named-entity extraction from the web:
an experimental study. Artificial Intelligence,
165(1):91?134, June.
Michael Fleischman and Eduard Hovy. 2002. Fine
grained classification of named entities. In Proceed-
ings of the 19th international conference on Compu-
tational linguistics, pages 1?7.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the auto-
matic discovery of part-whole relations. In Proc. of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 1?8.
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proc. of the
14th conference on Computational linguistics, pages
539?545.
Eduard Hovy, Zornitsa Kozareva, and Ellen Riloff.
2009. Toward completeness in concept extraction
and classification. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, pages 948?957.
Sean Igo and Ellen Riloff. 2009. Corpus-based se-
mantic lexicon induction with web-based corrobora-
tion. In Proceedings of the Workshop on Unsuper-
vised and Minimally Supervised Learning of Lexical
Semantics.
Boris Katz and Jimmy Lin. 2003. Selectively using re-
lations to improve precision in question answering.
In In Proceedings of the EACL-2003 Workshop on
Natural Language Processing for Question Answer-
ing, pages 43?50.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003.
Integrating web-based and corpus-based techniques
for question answering. In Proceedings of the
twelfth text retrieval conference (TREC), pages 426?
435.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy.
2008. Semantic class learning from the web with
hyponym pattern linkage graphs. In Proceedings of
ACL-08: HLT, pages 1048?1056.
Dekang Lin and Patrick Pantel. 2002. Concept dis-
covery from text. In Proc. of the 19th international
conference on Computational linguistics, pages 1?7.
Characteristics Of Mt, John Lehrberger, Laurent
Bourbeau, Philadelphia John Benjamins, and Rita
Mccardell. 1988. Machine Translation: Linguistic
Characteristics of Mt Systems and General Method-
ology of Evaluation. John Benjamins Publishing
Co(1988-03).
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Proc. of Hu-
man Language Technology Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 321?328.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-
scale distributional similarity and entity set expan-
sion. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 938?947, August.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proc. of the thirteenth
ACM international conference on Information and
knowledge management, pages 137?145.
Marco Pennacchiotti and Patrick Pantel. 2006. On-
tologizing semantic relations. In ACL-44: Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 793?800.
Ellen Riloff and Rosie Jones. 1999. Learning dic-
tionaries for information extraction by multi-level
bootstrapping. In AAAI ?99/IAAI ?99: Proceedings
of the Sixteenth National Conference on Artificial in-
telligence.
Ellen Riloff and Jessica Shepherd. 1997. A Corpus-
Based Approach for Building Semantic Lexicons.
In Proc. of the Second Conference on Empirical
Methods in Natural Language Processing, pages
117?124.
Brian Roark and Eugene Charniak. 1998. Noun-
phrase co-occurrence statistics for semiautomatic
semantic lexicon construction. In Proceedings of the
17th international conference on Computational lin-
guistics, pages 1110?1116.
1490
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. In Advances in Neural Information Pro-
cessing Systems 17, pages 1297?1304. MIT Press.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowl-
edge. In WWW ?07: Proceedings of the 16th inter-
national conference on World Wide Web, pages 697?
706.
Partha Pratim Talukdar, Joseph Reisinger, Marius
Pasca, Deepak Ravichandran, Rahul Bhagat, and
Fernando Pereira. 2008. Weakly-supervised acqui-
sition of labeled class instances using graph random
walks. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP 2008, pages 582?590.
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons Using
Extraction Pattern Contexts. In Proc. of the 2002
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 214?221.
Yorick Wilks. 1975. A preferential pattern-seeking,
semantics for natural language inference. Artificial
Intelligence, 6(1):53?74.
Alexander Yates, Michael Cafarella, Michele Banko,
Oren Etzioni, Matthew Broadhead, and Stephen
Soderland. 2007. Textrunner: open information ex-
traction on the web. In NAACL ?07: Proceedings of
Human Language Technologies: The Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Demonstra-
tions on XX, pages 25?26.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In ACL-44: Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 849?
856.
1491
Tutorial Abstracts of ACL 2010, page 4,
Uppsala, Sweden, 11 July 2010. c?2010 Association for Computational Linguistics
Annotation Eduard Hovy Information Sciences Institute University of Southern California email: hovy@isi.edu  1. Introduction As researchers seek to apply their machine learning algorithms to new problems, corpus annotation is increasingly gaining importance in the NLP community. But since the community currently has no general paradigm, no textbook that covers all the issues (though Wilcock?s book published in Dec 2009 covers some basic ones very well), and no accepted standards, setting up and performing small-, medium-, and large-scale annotation projects remains something of an art.   To attend, no special expertise in computation or linguistics is required.   2. Content Overview  This tutorial is intended to provide the attendee with an in-depth look at the procedures, issues, and problems in corpus annotation, and highlights the pitfalls that the annotation manager should avoid. The tutorial first discusses why annotation is becoming increasingly relevant for NLP and how it fits into the generic NLP methodology of train-evaluate-apply. It then reviews currently available resources, services, and frameworks that support someone wishing to start an annotation project easily. This includes the QDAP annotation center, Amazon?s Mechanical Turk, annotation facilities in GATE, and other resources such as UIMA.  It then discusses the seven major open issues at the heart of annotation for which there are as yet no standard and fully satisfactory answers or methods.  Each issue is described in detail and current practice is shown.  The seven issues are: 1. How does one decide what specific phenomena to annotate?  How does one adequately capture the theory behind the phenomenon/a and express it in simple annotation instructions? 2. How does one obtain a balanced corpus to annotate, and when is a corpus balanced (and representative)? 3. When hiring annotators, what characteristics are important?  How does one ensure that they are adequately (but not over- or under-) trained?  4. How does one 
establish a simple, fast, and trustworthy annotation procedure? How and when does one apply measures to ensure that the procedure remains on track?  How and where can active learning help?  5. What interface(s) are best for each type of problem, and what should one know to avoid?  How can one ensure that the interfaces do not influence the annotation results?  6. How does one evaluate the results?  What are the appropriate agreement measures?  At which cutoff points should one redesign or re-do the annotations?  7. How should one formulate and store the results?  When, and to whom, should one release the corpus? How should one report the annotation effort and results for best impact?   The notes include several pages of references and suggested readings.   3. Tutorial Overview  1. Toward a Science of Annotation  a. What is Annotation, and Why do We Need It?  2. Setting up an Annotation Project  a. The Basic Steps  b. Useful Resources and Services  3. Examples of Annotation Projects  4. The Seven Questions of Annotation  a. Instantiating the Theory b. Selecting the Corpus c. Designing the Annotation Interface  d. Selecting and Training Annotators  e. Specifying the Annotation Procedure  f. Evaluation and Validation  g. Distribution and Maintenance  5. Closing: The Future of Annotation in NLP  
4
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1466?1475,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Unsupervised Discovery of Domain-Specific Knowledge from Text
Dirk Hovy, Chunliang Zhang, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, czheng, hovy}@isi.edu
Anselmo Pen?as
UNED NLP and IR Group
Juan del Rosal 16
28040 Madrid, Spain
anselmo@lsi.uned.es
Abstract
Learning by Reading (LbR) aims at enabling
machines to acquire knowledge from and rea-
son about textual input. This requires knowl-
edge about the domain structure (such as en-
tities, classes, and actions) in order to do in-
ference. We present a method to infer this im-
plicit knowledge from unlabeled text. Unlike
previous approaches, we use automatically ex-
tracted classes with a probability distribution
over entities to allow for context-sensitive la-
beling. From a corpus of 1.4m sentences, we
learn about 250k simple propositions about
American football in the form of predicate-
argument structures like ?quarterbacks throw
passes to receivers?. Using several statisti-
cal measures, we show that our model is able
to generalize and explain the data statistically
significantly better than various baseline ap-
proaches. Human subjects judged up to 96.6%
of the resulting propositions to be sensible.
The classes and probabilistic model can be
used in textual enrichment to improve the per-
formance of LbR end-to-end systems.
1 Introduction
The goal of Learning by Reading (LbR) is to enable
a computer to learn about a new domain and then
to reason about it in order to perform such tasks as
question answering, threat assessment, and explana-
tion (Strassel et al, 2010). This requires joint efforts
from Information Extraction, Knowledge Represen-
tation, and logical inference. All these steps depend
on the system having access to basic, often unstated,
foundational knowledge about the domain.
Most documents, however, do not explicitly men-
tion this information in the text, but assume basic
background knowledge about the domain, such as
positions (?quarterback?), titles (?winner?), or ac-
tions (?throw?) for sports game reports. Without
this knowledge, the text will not make sense to the
reader, despite being well-formed English. Luckily,
the information is often implicitly contained in the
document or can be inferred from similar texts.
Our system automatically acquires domain-
specific knowledge (classes and actions) from large
amounts of unlabeled data, and trains a probabilis-
tic model to determine and apply the most infor-
mative classes (quarterback, etc.) at appropriate
levels of generality for unseen data. E.g., from
sentences such as ?Steve Young threw a pass to
Michael Holt?, ?Quarterback Steve Young finished
strong?, and ?Michael Holt, the receiver, left early?
we can learn the classes quarterback and receiver,
and the proposition ?quarterbacks throw passes to
receivers?.
We will thus assume that the implicit knowl-
edge comes in two forms: actions in the form of
predicate-argument structures, and classes as part of
the source data. Our task is to identify and extract
both. Since LbR systems must quickly adapt and
scale well to new domains, we need to be able to
work with large amounts of data and minimal super-
vision. Our approach produces simple propositions
about the domain (see Figure 1 for examples of ac-
tual propositions learned by our system).
American football was the first official evaluation
domain in the DARPA-sponsored Machine Reading
program, and provides the background for a number
1466
of LbR systems (Mulkar-Mehta et al, 2010). Sports
is particularly amenable, since it usually follows a
finite, explicit set of rules. Due to its popularity,
results are easy to evaluate with lay subjects, and
game reports, databases, etc. provide a large amount
of data. The same need for basic knowledge appears
in all domains, though. In music, musicians play in-
struments, in electronics, components constitute cir-
cuits, circuits use electricity, etc.
Teams beat teams
Teams play teams
Quarterbacks throw passes
Teams win games
Teams defeat teams
Receivers catch passes
Quarterbacks complete passes
Quarterbacks throw passes to receivers
Teams play games
Teams lose games
Figure 1: The ten most frequent propositions discovered
by our system for the American football domain
Our approach differs from verb-argument identi-
fication or Named Entity (NE) tagging in several re-
spects. While previous work on verb-argument se-
lection (Pardo et al, 2006; Fan et al, 2010) uses
fixed sets of classes, we cannot know a priori how
many and which classes we will encounter. We
therefore provide a way to derive the appropriate
classes automatically and include a probability dis-
tribution for each of them. Our approach is thus
less restricted and can learn context-dependent, fine-
grained, domain-specific propositions. While a NE-
tagged corpus could produce a general proposition
like ?PERSON throws to PERSON?, our method
enables us to distinguish the arguments and learn
?quarterback throws to receiver? for American foot-
ball and ?outfielder throws to third base? for base-
ball. While in NE tagging each word has only one
correct tag in a given context, we have hierarchical
classes: an entity can be correctly labeled as a player
or a quarterback (and possibly many more classes),
depending on the context. By taking context into
account, we are also able to label each sentence in-
dividually and account for unseen entities without
using external resources.
Our contributions are:
? we use unsupervised learning to train a model
that makes use of automatically extracted
classes to uncover implicit knowledge in the
form of predicate-argument propositions
? we evaluate the explanatory power, generaliza-
tion capability, and sensibility of the proposi-
tions using both statistical measures and human
judges, and compare them to several baselines
? we provide a model and a set of propositions
that can be used to improve the performance
of end-to-end LbR systems via textual enrich-
ment.
2 Methods
INPUT:
Steve Young threw a pass to Michael Holt
1. PARSE INPUT:
2. JOIN NAMES, EXTRACT PREDICATES:
NVN: Steve_Young throw pass 
NVNPN: Steve_Young throw pass to Michael_Holt
3. DECODE TO INFER PROPOSITIONS:
QUARTERBACK throw pass
QUARTERBACK throw pass to RECEIVER
Steve/NNP
Young/NNP
throw/VBD
pass/NN
a/DT
to/TO
Michael/NNP
Holt/NNP
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 2: Illustrated example of different processing steps
Our running example will be ?Steve Young threw
a pass to Michael Holt?. This is an instance of the
underlying proposition ?quarterbacks throw passes
to receivers?, which is not explicitly stated in the
data. A proposition is thus a more general state-
ment about the domain than the sentences it de-
rives. It contains domain-specific classes (quarter-
back, receiver), as well as lexical items (?throw?,
?pass?). In order to reproduce the proposition,
given the input sentences, our system has to not
only identify the classes, but also learn when to
1467
abstract away from the lexical form to the ap-
propriate class and when to keep it (cf. Figure
2, step 3). To facilitate extraction, we focus on
propositions with the following predicate-argument
structures: NOUN-VERB-NOUN (e.g., ?quarter-
backs throw passes?), or NOUN-VERB-NOUN-
PREPOSITION-NOUN (e.g., ?quarterbacks throw
passes to receivers?. There is nothing, though, that
prevents the use of other types of structures as well.
We do not restrict the verbs we consider (Pardo et
al., 2006; Ritter et al, 2010)), which extracts a high
number of hapax structures.
Given a sentence, we want to find the most likely
class for each word and thereby derive the most
likely proposition. Similar to Pardo et al (2006), we
assume the observed data was produced by a process
that generates the proposition and then transforms
the classes into a sentence, possibly adding addi-
tional words. We model this as a Hidden Markov
Model (HMM) with bigram transitions (see Section
2.3) and use the EM algorithm (Dempster et al,
1977) to train it on the observed data, with smooth-
ing to prevent overfitting.
2.1 Data
We use a corpus of about 33k texts on Ameri-
can football, extracted from the New York Times
(Sandhaus, 2008). To identify the articles, we rely
on the provided ?football? keyword classifier. The
resulting corpus comprises 1, 359, 709 sentences
from game reports, background stories, and opin-
ion pieces. In a first step, we parse all documents
with the Stanford dependency parser (De Marneffe
et al, 2006) (see Figure 2, step 1). The output
is lemmatized (collapsing ?throws?, ?threw?, etc.,
into ?throw?), and marked for various dependen-
cies (nsubj, amod, etc.). This enables us to ex-
tract the predicate argument structure, like subject-
verb-object, or additional prepositional phrases (see
Figure 2, step 2). These structures help to sim-
plify the model by discarding additional words like
modifiers, determiners, etc., which are not essen-
tial to the proposition. The same approach is used
by (Brody, 2007). We also concatenate multi-
word names (identified by sequences of NNPs) with
an underscore to form a single token (?Steve/NNP
Young/NNP?? ?Steve Young?).
2.2 Deriving Classes
To derive the classes used for entities, we do not re-
strict ourselves to a fixed set, but derive a domain-
specific set directly from the data. This step is per-
formed simultaneously with the corpus generation
described above. We utilize three syntactic construc-
tions to identify classes, namely nominal modifiers,
copula verbs, and appositions, see below. This is
similar in nature to Hearst?s lexico-syntactic patterns
(Hearst, 1992) and other approaches that derive IS-
A relations from text. While we find it straightfor-
ward to collect classes for entities in this way, we
did not find similar patterns for verbs. Given a suit-
able mechanism, however, these could be incorpo-
rated into our framework as well.
Nominal modifier are common nouns (labeled
NN) that precede proper nouns (labeled NNP), as in
?quarterback/NN Steve/NNP Young/NNP?, where
?quarterback? is the nominal modifier of ?Steve
Young?. Similar information can be gained from ap-
positions (e.g., ?Steve Young, the quarterback of his
team, said...?), and copula verbs (?Steve Young is
the quarterback of the 49ers?). We extract those co-
occurrences and store the proper nouns as entities
and the common nouns as their possible classes. For
each pair of class and entity, we collect counts over
the corpus to derive probability distributions.
Entities for which we do not find any of the above
patterns in our corpus are marked ?UNK?. These
entities are instantiated with the 20 most frequent
classes. All other (non-entity) words (including
verbs) have only their identity as class (i.e., ?pass?
remains ?pass?).
The average number of classes per entity is 6.87.
The total number of distinct classes for entities is
63, 942. This is a huge number to model in our state
space.1 Instead of manually choosing a subset of the
classes we extracted, we defer the task of finding the
best set to the model.
We note, however, that the distribution of classes
for each entity is highly skewed. Due to the unsuper-
vised nature of the extraction process, many of the
extracted classes are hapaxes and/or random noise.
Most entities have only a small number of applicable
classes (a football player usually has one main posi-
1NE taggers usually use a set of only a few dozen classes at
most.
1468
tion, and a few additional roles, such as star, team-
mate, etc.). We handle this by limiting the number of
classes considered to 3 per entity. This constraint re-
duces the total number of distinct classes to 26, 165,
and the average number of classes per entity to 2.53.
The reduction makes for a more tractable model size
without losing too much information. The class al-
phabet is still several magnitudes larger than that for
NE or POS tagging. Alternatively, one could use ex-
ternal resources such as Wikipedia, Yago (Suchanek
et al, 2007), or WordNet++ (Ponzetto and Navigli,
2010) to select the most appropriate classes for each
entity. This is likely to have a positive effect on the
quality of the applicable classes and merits further
research. Here, we focus on the possibilities of a
self-contained system without recurrence to outside
resources.
The number of classes we consider for each entity
also influences the number of possible propositions:
if we consider exactly one class per entity, there will
be little overlap between sentences, and thus no gen-
eralization possible?the model will produce many
distinct propositions. If, on the other hand, we used
only one class for all entities, there will be similar-
ities between many sentences?the model will pro-
duce very few distinct propositions.
2.3 Probabilistic Model
INPUT:
Steve Young threw a pass to Michael Holt
PARSE:
INSTANCES:
Steve_Young throw pass 
Steve_Young throw pass to Michael_Holt
PROPOSITIONS:
Quarterback throw pass
Quarterback throw pass to receiver
Steve
Young
throw
pass
a
to
Michael
Holt
nsubj
dobj
prep
nn
nn
pobjdet
Steve_Young    threw      a         pass       to    Michael_Holt
s1 s2 x1 s3 s4 s5
p1 p2 p3 p4 p5
quarterback      throw                pass          to         receiver
Figure 3: Graphical model for the running example
We use a generative noisy-channel model to cap-
ture the joint probability of input sentences and their
underlying proposition. Our generative story of how
a sentence s (with words s1, ..., sn) was generated
assumes that a proposition p is generated as a se-
quence of classes p1, ..., pn, with transition proba-
bilities P (pi|pi?1). Each class pi generates a word
si with probability P (si|pi). We allow additional
words x in the sentence which do not depend on any
class in the proposition and are thus generated inde-
pendently with P (x) (cf. model in Figure 3).
Since we observe the co-occurrence counts of
classes and entities in the data, we can fix the emis-
sion parameter P (s|p) in our HMM. Further, we do
not want to generate sentences from propositions, so
we can omit the step that adds the additional words
x in our model. The removal of these words is re-
flected by the preprocessing step that extracts the
structure (cf. Section 2.1).
Our model is thus defined as
P (s,p) =P (p1) ?
n?
i=1
(
P (pi|pi?1) ? P (si|pi)
)
(1)
where si, pi denote the ith word of sentence s and
proposition p, respectively.
3 Evaluation
We want to evaluate how well our model predicts
the data, and how sensible the resulting propositions
are. We define a good model as one that generalizes
well and produces semantically useful propositions.
We encounter two problems. First, since we de-
rive the classes in a data-driven way, we have no
gold standard data available for comparison. Sec-
ond, there is no accepted evaluation measure for this
kind of task. Ultimately, we would like to evaluate
our model externally, such as measuring its impact
on performance of a LbR system. In the absence
thereof, we resort to several complementary mea-
sures, as well as performing an annotation task. We
derive evaluation criteria as follows. A model gener-
alizes well if it can cover (?explain?) all the sentences
in the corpus with a few propositions. This requires
a measure of generality. However, while a proposi-
tion such as ?PERSON does THING?, has excellent
generality, it possesses no discriminating power. We
also need the propositions to partition the sentences
into clusters of semantic similarity, to support effec-
tive inference. This requires a measure of distribu-
tion. Maximal distribution, achieved by assigning
every sentence to a different proposition, however,
is not useful either. We need to find an appropri-
ate level of generality within which the sentences
are clustered into propositions for the best overall
groupings to support inference.
To assess the learned model, we apply the mea-
sures of generalization, entropy, and perplexity (see
1469
Sections 3.2, 3.3, and 3.4). These measures can be
used to compare different systems. We do not at-
tempt to weight or combine the different measures,
but present each in its own right.
Further, to assess label accuracy, we use Ama-
zon?s Mechanical Turk annotators to judge the sen-
sibility of the propositions produced by each sys-
tem (Section 3.5). We reason that if our system
learned to infer the correct classes, then the resulting
propositions should constitute true, general state-
ments about that domain, and thus be judged as sen-
sible.2 This approach allows the effective annotation
of sufficient amounts of data for an evaluation (first
described for NLP in (Snow et al, 2008)).
3.1 Evaluation Data
With the trained model, we use Viterbi decoding to
extract the best class sequence for each example in
the data. This translates the original corpus sen-
tences into propositions. See steps 2 and 3 in Figure
2.
We create two baseline systems from the same
corpus, one which uses the most frequent class
(MFC) for each entity, and another one which uses
a class picked at random from the applicable classes
of each entity.
Ultimately, we are interested in labeling unseen
data from the same domain with the correct class,
so we evaluate separately on the full corpus and
the subset of sentences that contain unknown enti-
ties (i.e., entities for which no class information was
available in the corpus, cf. Section 2.2).
For the latter case, we select all examples con-
taining at least one unknown entity (labeled UNK),
resulting in a subset of 41, 897 sentences, and repeat
the evaluation steps described above. Here, we have
to consider a much larger set of possible classes per
entity (the 20 overall most frequent classes). The
MFC baseline for these cases is the most frequent
of the 20 classes for UNK tokens, while the random
baseline chooses randomly from that set.
3.2 Generalization
Generalization measures how widely applicable the
produced propositions are. A completely lexical ap-
2Unfortunately, if judged insensible, we can not infer
whether our model used the wrong class despite better options,
or whether we simply have not learned the correct label.
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.04 0.01
0.12 0.09
0.25
0.66
Generalization
random
MFC
model
Figure 4: Generalization of models on the data sets
proach, at one extreme, would turn each sentence
into a separate proposition, thus achieving a gener-
alization of 0%. At the other extreme, a model that
produces only one proposition would generalize ex-
tremely well (but would fail to explain the data in
any meaningful way). Both are of course not desir-
able.
We define generalization as
g = 1?
|propositions|
|sentences|
(2)
The results in Figure 4 show that our model is
capable of abstracting away from the lexical form,
achieving a generalization rate of 25% for the full
data set. The baseline approaches do significantly
worse, since they are unable to detect similarities
between lexically different examples, and thus cre-
ate more propositions. Using a two-tailed t-test, the
difference between our model and each baseline is
statistically significant at p < .001.
Generalization on the unknown entity data set is
even higher (65.84%). The difference between the
model and the baselines is again statistically signif-
icant at p < .001. MFC always chooses the same
class for UNK, regardless of context, and performs
much worse. The random baseline chooses between
20 classes per entity instead of 3, and is thus even
less general.
3.3 Normalized Entropy
Entropy is used in information theory to measure
how predictable data is. 0 means the data is com-
pletely predictable. The higher the entropy of our
propositions, the less well they explain the data. We
are looking for models with low entropy. The ex-
treme case of only one proposition has 0 entropy:
1470
entropy
Page 1
full data set
unknown entities
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
1.00
1.00 1.000.99 0.99
0.89
0.50
Normalized Entropy
random
MFC
model
Figure 5: Entropy of models on the data sets
we know exactly which sentences are produced by
the proposition.
Entropy is directly influenced by the number of
propositions used by a system.3 In order to compare
different models, we thus define normalized entropy
as
HN =
?
n?
i=0
Pi ? logPi
log n
(3)
where Pi is the coverage of the proposition, or the
percentage of sentences explained by it, and n is the
number of distinct propositions.
The entropy of our model on the full data set is
relatively high with 0.89, see Figure 5. The best
entropy we can hope to achieve given the number
of propositions and sentences is actually 0.80 (by
concentrating the maximum probability mass in one
proposition). The model thus does not perform as
badly as the number might suggest. The entropy of
our model on unseen data is better, with 0.50 (best
possible: 0.41). This might be due to the fact that
we considered more classes for UNK than for regu-
lar entities.
3.4 Perplexity
Since we assume that propositions are valid sen-
tences in our domain, good propositions should have
a higher probability than bad propositions in a lan-
guage model. We can compute this using perplex-
3Note that how many classes we consider per entity influ-
ences how many propositions are produced (cf. Section 2.2),
and thus indirectly puts a bound on entropy.
entropy
Page 1
full data set unknown entities
50.00
51.00
52.00
53.00
54.00
55.00
56.00
57.00
58.00
59.00
60.00 59.52
57.0357.03 57.1556.84
54.92
Perplexity
random
MFC
model
Figure 6: Perplexity of models on the data sets
ity:4
perplexity(data) = 2
? logP (data)
n (4)
where P (data) is the product of the proposition
probabilities, and n is the number of propositions.
We use the uni-, bi-, and trigram counts of the
GoogleGrams corpus (Brants and Franz, 2006) with
simple interpolation to compute the probability of
each proposition.
The results in Figure 6 indicate that the proposi-
tions found by the model are preferable to the ones
found by the baselines. As would be expected, the
sensibility judgements for MFC and model5 (Tables
1 and 2, Section 3.5) are perfectly anti-correlated
(correlation coefficient ?1) with the perplexity for
these systems in each data set. However, due to the
small sample size, this should be interpreted cau-
tiously.
3.5 Sensibility and Label Accuracy
In unsupervised training, the model with the best
data likelihood does not necessarily produce the best
label accuracy. We evaluate label accuracy by pre-
senting subjects with the propositions we obtained
from the Viterbi decoding of the corpus, and ask
them to rate their sensibility. We compare the dif-
ferent systems by computing sensibility as the per-
centage of propositions judged sensible for each sys-
tem. Since the underlying probability distributions
are quite different, we weight the sensibility judge-
ment for each proposition by the likelihood of that
proposition. We report results for both aggregate
4Perplexity also quantifies the uncertainty of the resulting
propositions, where 0 perplexity means no uncertainty.
5We did not collect sensibility judgements for the random
baseline.
1471
accuracy
Page 1
System
90.16 92.13 69.35 70.57 88.84 90.37
94.28 96.55 70.93 70.45 93.06 95.16
100 most frequent random combined
Data set agg maj agg maj agg maj
full
baseline
model
Table 1: Percentage of propositions derived from labeling the full data set that were judged sensible
accuracy
Page 1
System
51.92 51.51 32.39 28.21 50.39 49.66
66.00 69.57 48.14 41.74 64.83 67.76
100 most frequent random combined
Data set agg maj agg maj agg maj
unknown
baseline
model
Table 2: Percentage of propositions derived from labeling unknown entities that were judged sensible
sensibility (using the total number of individual an-
swers), and majority sensibility, where each propo-
sition is scored according to the majority of annota-
tors? decisions.
The model and baseline propositions for the full
data set are both judged highly sensible, achieving
accuracies of 96.6% and 92.1% (cf. Table 1). While
our model did slightly better, the differences are not
statistically significant when using a two-tailed test.
The propositions produced by the model from un-
known entities are less sensible (67.8%), albeit still
significantly above chance level, and the baseline
propositions for the same data set (p < 0.01). Only
49.7% propositions of the baseline were judged sen-
sible (cf. Table 2).
3.5.1 Annotation Task
Our model finds 250, 169 distinct propositions,
the MFC baseline 293, 028. We thus have to restrict
ourselves to a subset in order to judge their sensi-
bility. For each system, we sample the 100 most
frequent propositions and 100 random propositions
found for both the full data set and the unknown enti-
ties6 and have 10 annotators rate each proposition as
sensible or insensible. To identify and omit bad an-
notators (?spammers?), we use the method described
in Section 3.5.2, and measure inter-annotator agree-
ment as described in Section 3.5.3. The details of
this evaluation are given below, the results can be
found in Tables 1 and 2.
The 200 propositions from each of the four sys-
6We omit the random baseline here due to size issues, and
because it is not likely to produce any informative comparison.
tems (model and baseline on both full and unknown
data set), contain 696 distinct propositions. We
break these up into 70 batches (Amazon Turk an-
notation HIT pages) of ten propositions each. For
each proposition, we request 10 annotators. Overall,
148 different annotators participated in our annota-
tion. The annotators are asked to state whether each
proposition represents a sensible statement about
American Football or not. A proposition like ?Quar-
terbacks can throw passes to receivers? should make
sense, while ?Coaches can intercept teams? does
not. To ensure that annotators judge sensibility and
not grammaticality, we format each proposition the
same way, namely pluralizing the nouns and adding
?can? before the verb. In addition, annotators can
state whether a proposition sounds odd, seems un-
grammatical, is a valid sentence, but against the
rules (e.g., ?Coaches can hit players?) or whether
they do not understand it.
3.5.2 Spammers
Some (albeit few) annotators on Mechanical Turk
try to complete tasks as quickly as possible with-
out paying attention to the actual requirements, in-
troducing noise into the data. We have to identify
these spammers before the evaluation. One way is
to include tests. Annotators that fail these tests will
be excluded. We use a repetition (first and last ques-
tion are the same), and a truism (annotators answer-
ing ?no? either do not know about football or just
answered randomly). Alternatively, we can assume
that good annotators, who are the majority, will ex-
hibit similar behavior to one another, while spam-
1472
mers exhibit a deviant answer pattern. To identify
those outliers, we compare each annotator?s agree-
ment to the others and exclude those whose agree-
ment falls more than one standard deviation below
the average overall agreement.
We find that both methods produce similar results.
The first method requires more careful planning, and
the resulting set of annotators still has to be checked
for outliers. The second method has the advantage
that it requires no additional questions. It includes
the risk, though, that one selects a set of bad annota-
tors solely because they agree with one another.
3.5.3 Agreement
agreement
Page 1
0.88 0.76 0.82
? 0.45 0.50 0.48
0.66 0.53 0.58
measure 100 most frequent random combined
agreement
G-index
Table 3: Agreement measures for different samples
We use inter-annotator agreement to quantify the
reliability of the judgments. Apart from the simple
agreement measure, which records how often an-
notators choose the same value for an item, there
are several statistics that qualify this measure by ad-
justing for other factors. One frequently used mea-
sure, Cohen?s ?, has the disadvantage that if there
is prevalence of one answer, ? will be low (or even
negative), despite high agreement (Feinstein and Ci-
cchetti, 1990). This phenomenon, known as the ?
paradox, is a result of the formula?s adjustment for
chance agreement. As shown by Gwet (2008), the
true level of actual chance agreement is realistically
not as high as computed, resulting in the counterin-
tuitive results. We include it for comparative rea-
sons. Another statistic, the G-index (Holley and
Guilford, 1964), avoids the paradox. It assumes that
expected agreement is a function of the number of
choices rather than chance. It uses the same general
formula as ?,
(Pa ? Pe)
(1? Pe)
(5)
where Pa is the actual raw agreement measured, and
Pe is the expected agreement. The difference with
? is that Pe for the G-index is defined as Pe = 1/q,
where q is the number of available categories, in-
stead of expected chance agreement. Under most
conditions, G and ? are equivalent, but in the case
of high raw agreement and few categories, G gives a
more accurate estimation of the agreement. We thus
report raw agreement, ?, and G-index.
Despite early spammer detection, there are still
outliers in the final data, which have to be accounted
for when calculating agreement. We take the same
approach as in the statistical spammer detection and
delete outliers that are more than one standard devi-
ation below the rest of the annotators? average.
The raw agreement for both samples combined is
0.82, G = 0.58, and ? = 0.48. The numbers show
that there is reasonably high agreement on the label
accuracy.
4 Related Research
The approach we describe is similar in nature to un-
supervised verb argument selection/selectional pref-
erences and semantic role labeling, yet goes be-
yond it in several ways. For semantic role label-
ing (Gildea and Jurafsky, 2002; Fleischman et al,
2003), classes have been derived from FrameNet
(Baker et al, 1998). For verb argument detec-
tion, classes are either semi-manually derived from
a repository like WordNet, or from NE taggers
(Pardo et al, 2006; Fan et al, 2010). This allows
for domain-independent systems, but limits the ap-
proach to a fixed set of oftentimes rather inappropri-
ate classes. In contrast, we derive the level of gran-
ularity directly from the data.
Pre-tagging the data with NE classes before train-
ing comes at a cost. It lumps entities together which
can have very different classes (i.e., all people be-
come labeled as PERSON), effectively allowing only
one class per entity. Etzioni et al (2005) resolve the
problem with a web-based approach that learns hi-
erarchies of the NE classes in an unsupervised man-
ner. We do not enforce a taxonomy, but include sta-
tistical knowledge about the distribution of possible
classes over each entity by incorporating a prior dis-
tribution P (class, entity). This enables us to gen-
eralize from the lexical form without restricting our-
selves to one class per entity, which helps to bet-
ter fit the data. In addition, we can distinguish sev-
eral classes for each entity, depending on the context
1473
(e.g., winner vs. quarterback). Ritter et al (2010)
also use an unsupervised model to derive selectional
predicates from unlabeled text. They do not assign
classes altogether, but group similar predicates and
arguments into unlabeled clusters using LDA. Brody
(2007) uses a very similar methodology to establish
relations between clauses and sentences, by cluster-
ing simplified propositions.
Pen?as and Hovy (2010) employ syntactic patterns
to derive classes from unlabeled data in the context
of LbR. They consider a wider range of syntactic
structures, but do not include a probabilistic model
to label new data.
5 Conclusion
We use an unsupervised model to infer domain-
specific classes from a corpus of 1.4m unlabeled
sentences, and applied them to learn 250k propo-
sitions about American football. Unlike previous
approaches, we use automatically extracted classes
with a probability distribution over entities to al-
low for context-sensitive selection of appropriate
classes.
We evaluate both the model qualities and sensibil-
ity of the resulting propositions. Several measures
show that the model has good explanatory power and
generalizes well, significantly outperforming two
baseline approaches, especially where the possible
classes of an entity can only be inferred from the
context.
Human subjects on Amazon?s Mechanical Turk
judged up to 96.6% of the propositions for the full
data set, and 67.8% for data containing unseen enti-
ties as sensible. Inter-annotator agreement was rea-
sonably high (agreement = 0.82, G = 0.58, ? =
0.48).
The probabilistic model and the extracted propo-
sitions can be used to enrich texts and support post-
parsing inference for question answering. We are
currently applying our method to other domains.
Acknowledgements
We would like to thank David Chiang, Victoria Fos-
sum, Daniel Marcu, and Stephen Tratz, as well as the
anonymous ACL reviewers for comments and sug-
gestions to improve the paper. Research supported
in part by Air Force Contract FA8750-09-C-0172
under the DARPA Machine Reading Program.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet Project. In Proceed-
ings of the 17th international conference on Computa-
tional linguistics-Volume 1, pages 86?90. Association
for Computational Linguistics Morristown, NJ, USA.
Thorsten Brants and Alex Franz, editors. 2006. The
Google Web 1T 5-gram Corpus Version 1.1. Number
LDC2006T13. Linguistic Data Consortium, Philadel-
phia.
Samuel Brody. 2007. Clustering Clauses for High-
Level Relation Detection: An Information-theoretic
Approach. In Annual Meeting-Association for Com-
putational Linguistics, volume 45, page 448.
Marie-Catherine De Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses. In
LREC 2006. Citeseer.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Oren Etzioni, Michael Cafarella, Doug. Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: An exper-
imental study. Artificial Intelligence, 165(1):91?134.
James Fan, David Ferrucci, David Gondek, and Aditya
Kalyanpur. 2010. Prismatic: Inducing knowledge
from a large scale lexicalized relation resource. In
Proceedings of the NAACL HLT 2010 First Interna-
tional Workshop on Formalisms and Methodology for
Learning by Reading, pages 122?127, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Alvan R. Feinstein and Domenic V. Cicchetti. 1990.
High agreement but low kappa: I. the problems of
two paradoxes. Journal of Clinical Epidemiology,
43(6):543?549.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for FrameNet classi-
fication. In Proceedings of EMNLP, volume 3.
Danies Gildea and Dan Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245?288.
Kilem Li Gwet. 2008. Computing inter-rater reliabil-
ity and its variance in the presence of high agreement.
British Journal of Mathematical and Statistical Psy-
chology, 61(1):29?48.
1474
Marti A. Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of the
14th conference on Computational linguistics-Volume
2, pages 539?545. Association for Computational Lin-
guistics.
Jasper Wilson Holley and Joy Paul Guilford. 1964. A
Note on the G-Index of Agreement. Educational and
Psychological Measurement, 24(4):749.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Thiago Pardo, Daniel Marcu, and Maria Nunes. 2006.
Unsupervised Learning of Verb Argument Structures.
Computational Linguistics and Intelligent Text Pro-
cessing, pages 59?70.
Anselmo Pen?as and Eduard Hovy. 2010. Semantic en-
richment of text with background knowledge. In Pro-
ceedings of the NAACL HLT 2010 First International
Workshop on Formalisms and Methodology for Learn-
ing by Reading, pages 15?23, Los Angeles, California,
June. Association for Computational Linguistics.
Simone Paolo Ponzetto and Roberto Navigli. 2010.
Knowledge-rich Word Sense Disambiguation rivaling
supervised systems. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, pages 1522?1531. Association for Computational
Linguistics.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 424?434,
Uppsala, Sweden, July. Association for Computational
Linguistics.
Evan Sandhaus, editor. 2008. The New York Times Anno-
tated Corpus. Number LDC2008T19. Linguistic Data
Consortium, Philadelphia.
Rion Snow, Brendan O?Connor, Dan Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast?but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 254?263. Association for Computational Lin-
guistics.
Stephanie Strassel, Dan Adams, Henry Goldberg,
Jonathan Herr, Ron Keesing, Daniel Oblinger, Heather
Simpson, Robert Schrag, and Jonathan Wright. 2010.
The DARPA Machine Reading Program-Encouraging
Linguistic and Reasoning Research with a Series of
Reading Tasks. In Proceedings of LREC 2010.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In Proceedings of the 16th international conference on
World Wide Web, pages 697?706. ACM.
1475
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1616?1625,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Insights from Network Structure for Text Mining
Zornitsa Kozareva and Eduard Hovy
USC Information Sciences Institute
4676 Admiralty Way
Marina del Rey, CA 90292-6695
{kozareva,hovy}@isi.edu
Abstract
Text mining and data harvesting algorithms
have become popular in the computational lin-
guistics community. They employ patterns
that specify the kind of information to be har-
vested, and usually bootstrap either the pat-
tern learning or the term harvesting process (or
both) in a recursive cycle, using data learned
in one step to generate more seeds for the next.
They therefore treat the source text corpus as
a network, in which words are the nodes and
relations linking them are the edges. The re-
sults of computational network analysis, espe-
cially from the world wide web, are thus ap-
plicable. Surprisingly, these results have not
yet been broadly introduced into the computa-
tional linguistics community. In this paper we
show how various results apply to text mining,
how they explain some previously observed
phenomena, and how they can be helpful for
computational linguistics applications.
1 Introduction
Text mining / harvesting algorithms have been ap-
plied in recent years for various uses, including
learning of semantic constraints for verb participants
(Lin and Pantel, 2002) related pairs in various rela-
tions, such as part-whole (Girju et al, 2003), cause
(Pantel and Pennacchiotti, 2006), and other typical
information extraction relations, large collections
of entities (Soderland et al, 1999; Etzioni et al,
2005), features of objects (Pasca, 2004) and ontolo-
gies (Carlson et al, 2010). They generally start with
one or more seed terms and employ patterns that
specify the desired information as it relates to the
seed(s). Several approaches have been developed
specifically for learning patterns, including guided
pattern collection with manual filtering (Riloff and
Shepherd, 1997) automated surface-level pattern in-
duction (Agichtein and Gravano, 2000; Ravichan-
dran and Hovy, 2002) probabilistic methods for tax-
onomy relation learning (Snow et al, 2005) and ker-
nel methods for relation learning (Zelenko et al,
2003). Generally, the harvesting procedure is recur-
sive, in which data (terms or patterns) gathered in
one step of a cycle are used as seeds in the following
step, to gather more terms or patterns.
This method treats the source text as a graph or
network, consisting of terms (words) as nodes and
inter-term relations as edges. Each relation type in-
duces a different network1. Text mining is a process
of network traversal, and faces the standard prob-
lems of handling cycles, ranking search alternatives,
estimating yield maxima, etc.
The computational properties of large networks
and large network traversal have been studied inten-
sively (Sabidussi, 1966; Freeman, 1979; Watts and
Strogatz, 1998) and especially, over the past years,
in the context of the world wide web (Page et al,
1999; Broder et al, 2000; Kleinberg and Lawrence,
2001; Li et al, 2005; Clauset et al, 2009). Surpris-
ingly, except in (Talukdar and Pereira, 2010), this
work has not yet been related to text mining research
in the computational linguistics community.
The work is, however, relevant in at least two
ways. It sometimes explains why text mining algo-
1These networks are generally far larger and more densely
interconnected than the world wide web?s network of pages and
hyperlinks.
1616
rithms have the limitations and thresholds that are
empirically found (or suspected), and it may suggest
ways to improve text mining algorithms for some
applications.
In Section 2, we review some related work. In
Section 3 we describe the general harvesting proce-
dure, and follow with an examination of the various
statistical properties of implicit semantic networks
in Section 4, using our implemented harvester to
provide illustrative statistics. In Section 5 we dis-
cuss implications for computational linguistics re-
search.
2 Related Work
The Natural Language Processing knowledge har-
vesting community has developed a good under-
standing of how to harvests various kinds of se-
mantic information and use this information to im-
prove the performance of tasks such as information
extraction (Riloff, 1993), textual entailment (Zan-
zotto et al, 2006), question answering (Katz et
al., 2003), and ontology creation (Suchanek et al,
2007), among others. Researchers have focused
on the automated extraction of semantic lexicons
(Hearst, 1992; Riloff and Shepherd, 1997; Girju et
al., 2003; Pasca, 2004; Etzioni et al, 2005; Kozareva
et al, 2008). While clustering approaches tend to
extract general facts, pattern based approaches have
shown to produce more constrained but accurate lists
of semantic terms. To extract this information, (Lin
and Pantel, 2002) showed the effect of using differ-
ent sizes and genres of corpora such as news and
Web documents. The latter has been shown to pro-
vide broader and more complete information.
Researchers outside computational linguistics
have studied complex networks such as the World
Wide Web, the Social Web, the network of scien-
tific papers, among others. They have investigated
the properties of these text-based networks with the
objective of understanding their structure and ap-
plying this knowledge to determine node impor-
tance/centrality, connectivity, growth and decay of
interest, etc. In particular, the ability to analyze net-
works, identify influential nodes, and discover hid-
den structures has led to important scientific and
technological breakthroughs such as the discovery
of communities of like-minded individuals (New-
man and Girvan, 2004), the identification of influ-
ential people (Kempe et al, 2003), the ranking of
scientists by their citation indexes (Radicchi et al,
2009), and the discovery of important scientific pa-
pers (Walker et al, 2006; Chen et al, 2007; Sayyadi
and Getoor, 2009). Broder et al (2000) demon-
strated that the Web link structure has a ?bow-tie?
shape, while (2001) classified Web pages into au-
thorities (pages with relevant information) and hubs
(pages with useful references). These findings re-
sulted in the development of the PageRank (Page et
al., 1999) algorithm which analyzes the structure of
the hyperlinks of Web documents to find pages with
authoritative information. PageRank has revolution-
ized the whole Internet search society.
However, no-one has studied the properties of the
text-based semantic networks induced by semantic
relations between terms with the objective of un-
derstanding their structure and applying this knowl-
edge to improve concept discovery. Most relevant
to this theme is the work of Steyvers and Tenen-
baum (Steyvers and Tenenbaum, 2004), who stud-
ied three manually built lexical networks (associa-
tion norms, WordNet, and Roget?s Thesaurus (Ro-
get, 1911)) and proposed a model of the growth of
the semantic structure over time. These networks are
limited to the semantic relations among nouns.
In this paper we take a step further to explore the
statistical properties of semantic networks relating
proper names, nouns, verbs, and adjectives. Under-
standing the semantics of nouns, verbs, and adjec-
tives has been of great interest to linguists and cog-
nitive scientists such as (Gentner, 1981; Levin and
Somers, 1993; Gasser and Smith, 1998). We imple-
ment a general harvesting procedure and show its re-
sults for these word types. A fundamental difference
with the work of (Steyvers and Tenenbaum, 2004)
is that we study very large semantic networks built
?naturally? by (millions of) users rather than ?artifi-
cially? by a small set of experts. The large networks
capture the semantic intuitions and knowledge of the
collective mass. It is conceivable that an analysis
of this knowledge can begin to form the basis of a
large-scale theory of semantic meaning and its inter-
connections, support observation of the process of
lexical development and usage in humans, and even
suggest explanations of how knowledge is organized
in our brains, especially when performed for differ-
1617
ent languages on the WWW.
3 Inducing Semantic Networks in the Web
Text mining algorithms such as those mentioned
above raise certain questions, such as: Why are some
seed terms more powerful (provide a greater yield)
than others?, How can one find high-yield terms?,
How many steps does one need, typically, to learn
all terms for a given relation?, Can one estimate the
total eventual yield of a given relation?, and so on.
On the face of it, one would need to know the struc-
ture of the network a priori to be able to provide an-
swers. But research has shown that some surpris-
ing regularities hold. For example, in the text min-
ing community, (Kozareva and Hovy, 2010b) have
shown that one can obtain a quite accurate estimate
of the eventual yield of a pattern and seed after only
five steps of harvesting. Why is this? They do not
provide an answer, but research from the network
community does.
To illustrate the properties of networks of the kind
induced by semantic relations, and to show the ap-
plicability of network research to text harvesting, we
implemented a harvesting algorithm and applied it
to a representative set of relations and seeds in two
languages.
Since the goal of this paper is not the development
of a new text harvesting algorithm, we implemented
a version of an existing one: the so-called DAP
(doubly-anchored pattern) algorithm (Kozareva et
al., 2008), because it (1) is easy to implement, (2)
requires minimum input (one pattern and one seed
example), (3) achieves very high precision com-
pared to existing methods (Pasca, 2004; Etzioni et
al., 2005; Pasca, 2007), (4) enriches existing se-
mantic lexical repositories such as WordNet and
Yago (Suchanek et al, 2007), (5) can be formulated
to learn semantic lexicons and relations for noun,
verb and verb+preposition syntactic constructions;
(6) functions equally well in different languages.
Next we describe the knowledge harvesting proce-
dure and the construction of the text-mined semantic
networks.
3.1 Harvesting to Induce Semantic Networks
For a given semantic class of interest say singers, the
algorithm starts with a seed example of the class, say
Madonna. The seed term is inserted in the lexico-
syntactic pattern ?class such as seed and *?, which
learns on the position of the ? new terms of type
class. The newly learned terms are then individually
placed into the position of the seed in the pattern,
and the bootstrapping process is repeated until no
new terms are found. The output of the algorithm
is a set of terms for the semantic class. The algo-
rithm is implemented as a breadth-first search and
its mechanism is described as follows:
1. Given:
a language L={English, Spanish}
a pattern Pi={such as, including, verb prep,
noun}
a seed term seed for Pi
2. Build a query for Pi using template Ti ?class such
as seed and *?, ?class including seed and *?, ?*
and seed verb prep?, ?* and seed noun?, ?seed
and * noun?
3. Submit Ti to Yahoo! or other search engine
4. Extract terms occupying the * position
5. Feed terms from 4. into 2.
6. Repeat steps 2?5. until no new terms are found
The output of the knowledge harvesting algorithm
is a network of semantic terms interconnected by
the semantic relation captured in the pattern. We
can represent the traversed (implicit) network as a
directed graph G(V,E) with nodes V (|V | = n)
and edges E(|E| = m). A node u in the net-
work corresponds to a term discovered during boot-
strapping. An edge (u, v) ? E represents an ex-
isting link between two terms. The direction of the
edge indicates that the term v was generated by the
term u. For example, given the sentence (where
the pattern is in italics and the extracted term is un-
derlined) ?He loves singers such as Madonna and
Michael Jackson?, two nodes Madonna and Michael
Jackson with an edge e=(Madonna, Michael Jack-
son) would be created in the graph G. Figure 1
shows a small example of the singer network. The
starting seed term Madonna is shown in red color
and the harvested terms are in blue.
3.2 Data
We harvested data from the Web for a representa-
tive selection of semantic classes and relations, of
1618
!"#$%%"&
'()$%&*$+%&!,-+".(&*"-/0$%&
1.(,%.&2,$%&
'33"& 45,%-.& 6.7$%-.& 8"57&9,+"%%"&
:5.##,.&!.5-;57&
<(,-,"&8.70&
=+"/,5"&
2>>& 9,-/.7&!"5?%&=).@,.&A$%#.5&
B,%"&B;5%.5&
2$((7&4")$%&
Figure 1: Harvesting Procedure.
the type used in (Etzioni et al, 2005; Pasca, 2007;
Kozareva and Hovy, 2010a):
? semantic classes that can be learned using dif-
ferent seeds (e.g., ?singers such as Madonna
and *? and ?singers such as Placido Domingo
and *?);
? semantic classes that are expressed through dif-
ferent lexico-syntactic patterns (e.g., ?weapons
such as bombs and *? and ?weapons including
bombs and *?);
? verbs and adjectives characterizing the seman-
tic class (e.g., ?expensive and * car?, ?dogs
run and *?);
? semantic relations with more complex lexico-
syntactic structure (e.g., ?* and Easyjet fly to?,
?* and Sam live in?);
? semantic classes that are obtained in differ-
ent languages, such as English and Spanish
(e.g., ?singers such as Madonna and *? and
?cantantes como Madonna y *?);
While most of these variations have been explored
in individual papers, we have found no paper that
covers them all, and none whatsoever that uses verbs
and adjectives as seeds.
Using the above procedure to generate the data,
each pattern was submitted as a query to Ya-
hoo!Boss. For each query the top 1000 text snippets
were retrieved. The algorithm ran until exhaustion.
In total, we collected 10GB of data which was part-
of-speech tagged with Treetagger (Schmid, 1994)
and used for the semantic term extraction. Table 1
summarizes the number of nodes and edges learned
for each semantic network using pattern Pi and the
initial seed shown in italics.
Lexico-Syntactic Pattern Nodes Edges
P1=?singers such as Madonna and *? 1115 1942
P2=?singers such as Placido Domingo and *? 815 1114
P3=?emotions including anger and *? 113 250
P4=?emotions such as anger and *? 748 2547
P5=?diseases such as malaria and *? 3168 6752
P6=?drugs such as ibuprofen and *? 2513 9428
P7=?expensive and * cars? 4734 22089
P8=?* and tasty fruits? 1980 7874
P9=?whales swim and *? 869 2163
P10=?dogs chase and *? 4252 20212
P11=?Britney Spears dances and *? 354 540
P12=?John reads and *? 3894 18545
P13=?* and Easyjet fly to? 3290 6480
P14=?* and Charlie work for? 2125 3494
P15=?* and Sam live in? 6745 24348
P16=?cantantes como Madonna y *? 240 318
P17=?gente como Jorge y *? 572 701
Table 1: Size of the Semantic Networks.
4 Statistical Properties of Text-Mined
Semantic Networks
In this section we apply a range of relevant mea-
sures from the network analysis community to the
networks described above.
4.1 Centrality
The first statistical property we explore is centrality.
It measures the degree to which the network struc-
ture determines the importance of a node in the net-
work (Sabidussi, 1966; Freeman, 1979).
We explore the effect of two centrality measures:
indegree and outdegree. The indegree of a node
u denoted as indegree(u)=
?
(v, u) considers the
sum of all incoming edges to u and captures the abil-
ity of a semantic term to be discovered by other se-
mantic terms. The outdegree of a node u denoted
as outdegree(u)=
?
(u, v) considers the number of
outgoing edges of the node u and measures the abil-
ity of a semantic term to discover new terms. In-
tuitively, the more central the node u is, the more
confident we are that it is a correct term.
Since harvesting algorithms are notorious for ex-
tracting erroneous information, we use the two cen-
trality measures to rerank the harvested elements.
Table 2 shows the accuracy2 of the singer seman-
tic terms at different ranks using the in and out
degree measures. Consistently, outdegree outper-
forms indegree and reaches higher accuracy. This
2Accuracy is calculated as the number of correct terms at
rank R divided by the total number of terms at rank R.
1619
shows that for the text-mined semantic networks, the
ability of a term to discover new terms is more im-
portant than the ability to be discovered.
@rank in-degree out-degree
10 .92 1.0
25 .91 1.0
50 .90 .97
75 .90 .96
100 .89 .96
150 .88 .95
Table 2: Accuracy of the Singer Terms.
This poses the question ?What are the terms with
high and low outdegree??. Table 3 shows the top
and bottom 10 terms of the semantic class.
Semantic Class top 10 outDegree bottom 10 outDegree
Singers Frank Sinatra Alanis Morisette
Ella Fitzgerald Christine Agulera
Billie Holiday Buffy Sainte-Marie
Britney Spears Cece Winans
Aretha Franklin Wolfman Jack
Michael Jackson Billie Celebration
Celine Dion Alejandro Sanz
Beyonce France Gall
Bessie Smith Peter
Joni Mitchell Sarah
Table 3: Singer Term Ranking with Centrality Measures.
The nodes with high outdegree correspond to fa-
mous or contemporary singers. The lower-ranked
nodes are mostly spelling errors such as Alanis
Morisette and Christine Agulera, less known singers
such as Buffy Sainte-Marie and Cece Winans, non-
American singers such as Alejandro Sanz and
France Gall, extractions due to part-of-speech tag-
ging errors such as Billie Celebration, and general
terms such as Peter and Sarah. Potentially, know-
ing which terms have a high outdegree allows one to
rerank candidate seeds for more effective harvesting.
4.2 Power-law Degree Distribution
We next study the degree distributions of the net-
works. Similarly to the Web (Broder et al, 2000)
and social networks like Orkut and Flickr, the text-
mined semantic networks also exhibit a power-law
distribution. This means that while a few terms have
a significantly high degree, the majority of the se-
mantic terms have small degree. Figure 2 shows the
indegree and outdegree distributions for different
semantic classes, lexico-syntactic patterns, and lan-
guages (English and Spanish). For each semantic
network, we plot the best-fitting power-law function
(Clauset et al, 2009) which fits well all degree dis-
tributions. Table 4 shows the power-law exponent
values for all text-mined semantic networks.
Patt. ?in ?out Patt. ?in ?out
P1 2.37 1.27 P10 1.65 1.12
P2 2.25 1.21 P11 2.42 1.41
P3 2.20 1.76 P12 1.60 1.13
P4 2.28 1.18 P13 2.26 1.20
P5 2.49 1.18 P14 2.43 1.25
P6 2.42 1.30 P15 2.51 1.43
P7 1.95 1.20 P16 2.74 1.31
P8 1.94 1.07 P17 2.90 1.20
P9 1.96 1.30
Table 4: Power-Law Exponents of Semantic Networks.
It is interesting to note that the indegree power-
law exponents for all semantic networks fall within
the same range (?in ? 2.4), and similarly for the
outdegree exponents (?out ? 1.3). However, the
values of the indegree and outdegree exponents
differ from each other. This observation is consistent
with Web degree distributions (Broder et al, 2000).
The difference in the distributions can be explained
by the link asymmetry of semantic terms: A discov-
ering B does not necessarily mean that B will dis-
cover A. In the text-mined semantic networks, this
asymmetry is caused by patterns of language use,
such as the fact that people use first adjectives of the
size and then of the color (e.g., big red car), or prefer
to place male before female proper names. Harvest-
ing patterns should take into account this tendency.
4.3 Sparsity
Another relevant property of the semantic networks
concerns sparsity. Following Preiss (Preiss, 1999), a
graph is sparse if |E| = O(|V |k) and 1 < k < 2,
where |E| is the number of edges and |V | is the num-
ber of nodes, otherwise the graph is dense. For the
studied text-semantic networks, k is ? 1.08. Spar-
sity can be also captured through the density of the
semantic network which is computed as |E|V (V?1) . All
networks have low density which suggests that the
networks exhibit a sparse connectivity pattern. On
average a node (semantic term) is connected to a
very small percentage of other nodes. Similar be-
havior was reported for the WordNet and Roget?s se-
mantic networks (Steyvers and Tenenbaum, 2004).
1620
 0 50 100 150 200 250 300 350
 400 450 500
 0  10  20  30  40  50  60  70  80  90Number of Nodes Indegree 'emotions'power-law exponent=2.28  0 20 40 60 80 100 120  0  20  40  60  80  100  120Number of Nodes Outdegree 'emotions'power-law exponent=1.18
 0 500 1000 1500 2000
 2500
 0  10  20  30  40  50  60Number of Nodes Indegree 'travel_to'power-law exponent=2.26  0 100 200 300 400 500 600 700  0  5  10  15  20  25  30  35Number of Nodes Outdegree 'fly_to'power-law exponent=1.20
 0 50 100 150 200 250 300 350 400
 450 500
 1  2  3  4  5  6  7  8Number of Nodes Indegree 'gente'power-law exponent=2.90  0 20 40 60 80 100 120  0  2  4  6  8  10  12  14Number of Nodes Outdegree 'gente'power-law exponent=1.20
Figure 2: Degree Distributions of Semantic Networks.
4.4 Connectedness
For every network, we computed the strongly con-
nected component (SCC) such that for all nodes (se-
mantic terms) in the SCC, there is a path from any
node to another node in the SCC considering the di-
rection of the edges between the nodes. For each
network, we found that there is only one SCC. The
size of the component is shown in Table 5. Un-
like WordNet and Roget?s semantic networks where
the SCC consists 96% of all semantic terms, in the
text-mined semantic networks only 12 to 55% of the
terms are in the SCC. This shows that not all nodes
can reach (discover) every other node in the net-
work. This also explains the findings of (Kozareva
et al, 2008; Vyas et al, 2009) why starting with a
good seed is important.
4.5 Path Lengths and Diameter
Next, we describe the properties of the shortest paths
between the semantic terms in the SCC. The dis-
tance between two nodes in the SCC is measured as
the length of the shortest path connecting the terms.
The direction of the edges between the terms is taken
into consideration. The average distance is the aver-
age value of the shortest path lengths over all pairs
of nodes in the SCC. The diameter of the SCC is
calculated as the maximum distance over all pairs of
nodes (u, v), such that a node v is reachable from
node u. Table 5 shows the average distance and the
diameter of the semantic networks.
Patt. #nodes in SCC SCC Average Distance SCC Diameter
P1 364 (.33) 5.27 16
P2 285 (.35) 4.65 13
P3 48 (.43) 2.85 6
P4 274 (.37) 2.94 7
P5 1249 (.38) 5.99 17
P6 1471 (.29) 4.82 15
P7 2255 (.46 ) 3.51 11
P8 1012 (.50) 3.87 11
P9 289 (.33) 4.93 13
P10 2342 (.55) 4.50 12
P11 87 (.24) 5.00 11
P12 1967 (.51) 3.20 13
P13 1249 (.38) 4.75 13
P14 608 (.29) 7.07 23
P15 1752 (.26) 5.32 15
P16 56 (.23) 4.79 12
P17 69 (.12 ) 5.01 13
Table 5: SCC, SCC Average Distance and SCC Diameter
of the Semantic Networks.
The diameter shows the maximum number of
steps necessary to reach from any node to any other,
while the average distance shows the number of
steps necessary on average. Overall, all networks
have very short average path lengths and small di-
ameters that are consistent with Watt?s finding for
small-world networks. Therefore, the yield of har-
vesting seeds can be predicted within five steps ex-
plaining (Kozareva and Hovy, 2010b; Vyas et al,
2009).
We also compute for any randomly selected node
in the semantic network on average how many hops
(steps) are necessary to reach from one node to an-
other. Figure 3 shows the obtained results for some
of the studied semantic networks.
4.6 Clustering
The clustering coefficient (C) is another measure
to study the connectivity structure of the networks
(Watts and Strogatz, 1998). This measure captures
the probability that the two neighbors of a randomly
selected node will be neighbors. The clustering co-
efficient of a node u is calculated as Cu=
|eij |
ku(ku?1)
1621
 0 10 20 30 40 50
 60 70
 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15Number of Nodes Distance (Hops)Britney Spears (verb harvesting)  0 50 100 150 200 250 300 350 400  1  2  3  4  5  6  7  8  9  10  11Number of Nodes Distance (Hops)fruits (adjective harvesting)
 0 50 100 150 200
 250
 2  4  6  8  10  12  14  16  18  20  22  24Number of Nodes Distance (Hops)work for  0 5 10 15 20 25 30  2  4  6  8  10  12  14  16  18  20Number of Nodes Distance (Hops) gente
Figure 3: Hop Plot of the Semantic Networks.
: vi, vj ? Nu, eij ? E, where ku is the total degree
of the node u and Nu is the neighborhood of u. The
clustering coefficient C for the whole semantic net-
work is the average clustering coefficient of all its
nodes, C= 1n
?
Ci. The value of the clustering coef-
ficient ranges between [0, 1], where 0 indicates that
the nodes do not have neighbors which are them-
selves connected, while 1 indicates that all nodes are
connected. Table 6 shows the clustering coefficient
for all text-mined semantic networks together with
the number of closed and open triads3. The analysis
suggests the presence of a strong local cluster, how-
ever there are few possibilities to form overlapping
neighborhoods of nodes. The clustering coefficient
of WordNet (Steyvers and Tenenbaum, 2004) is sim-
ilar to those of the text-mined networks.
4.7 Joint Degree Distribution
In social networks, understanding the preferential at-
tachment of nodes is important to identify the speed
with which epidemics or gossips spread. Similarly,
we are interested in understanding how the nodes of
the semantic networks connect to each other. For
this purpose, we examine the Joint Degree Distribu-
tion (JDD) (Li et al, 2005; Newman, 2003). JDD
is approximated by the degree correlation function
knn which maps the outdegree and the average
3A triad is three nodes that are connected by either two (open
triad) or three (closed triad) directed ties.
Patt. C ClosedTriads OpenTriads
P1 .01 14096 (.97) 388 (.03)
P2 .01 6487 (.97) 213 (.03)
P3 .30 1898 (.94) 129 (.06)
P4 .33 60734 (.94) 3944 (.06)
P5 .10 79986 (.97) 2321 (.03)
P6 .11 78716 (.97) 2336 (.03)
P7 .17 910568 (.95) 43412 (.05)
P8 .19 21138 (.95) 10728 (.05)
P9 .20 27830 (.95) 1354 (.05)
P10 .15 712227 (.96) 62101(.04)
P11 .09 3407 (.98) 63 (.02)
P12 .15 734724 (.96) 32517 (.04)
P13 .06 66162 (.99) 858 (.01)
P14 .05 28216 (.99) 408 (.01)
P15 .09 1336679 (.97) 47110 (.03)
P16 .09 1525 (.98) 37 ( .02)
P17 .05 2222 (.99) 21 (.01)
Table 6: Clustering Coefficient of the Semantic Networks.
indegree of all nodes connected to a node with
that outdegree. High values of knn indicate that
high-degree nodes tend to connect to other high-
degree nodes (forming a ?core? in the network),
while lower values of knn suggest that the high-
degree nodes tend to connect to low-degree ones.
Figure 4 shows the knn for the singer, whale, live
in, cars, cantantes, and gente networks. The figure
plots the outdegree and the average indegree of the
semantic terms in the networks on a log-log scale.
We can see that for all networks the high-degree
nodes tend to connect to other high-degree ones.
This explains why text mining algorithms should fo-
cus their effort on high-degree nodes.
4.8 Assortivity
The property of the nodes to connect to other nodes
with similar degrees can be captured through the as-
sortivity coefficient r (Newman, 2003). The range of
r is [?1, 1]. A positive assortivity coefficient means
that the nodes tend to connect to nodes of similar
degree, while negative coefficient means that nodes
are likely to connect to nodes with degree very dif-
ferent from their own. We find that the assortivi-
tiy coefficient of our semantic networks is positive,
ranging from 0.07 to 0.20. In this respect, the se-
mantic networks differ from the Web, which has a
negative assortivity (Newman, 2003). This implies
a difference in text mining and web search traver-
sal strategies: since starting from a highly-connected
seed term will tend to lead to other highly-connected
terms, text mining algorithms should prefer depth-
first traversal, while web search algorithms starting
1622
 1 10
 100
 1  10  100knn Outdegreesinger (seed is Madonna)  1 10 100  1  10  100knn Outdegreewhale (verb harvesting)
 1 10
 100
 1  10  100knn Outdegree live in  1 10 100  1  10  100knn Outdegreecars (adjective harvesting)
 1
 10
 1  10knn Outdegreecantantes  1 10  1  10knn Outdegree gente
Figure 4: Joint Degree Distribution of the Semantic Net-
works.
from a highly-connected seed page should prefer a
breadth-first strategy.
5 Discussion
The above studies show that many of the proper-
ties discovered of the network formed by the web
hold also for the networks induced by semantic rela-
tions in text mining applications, for various seman-
tic classes, semantic relations, and languages. We
can therefore apply some of the research from net-
work analysis to text mining.
The small-world phenomenon, for example, holds
that any node is connected to any other node in at
most six steps. Since as shown in Section 4.5 the se-
mantic networks also exhibit this phenomenon, we
can explain the observation of (Kozareva and Hovy,
2010b) that one can quite accurately predict the rel-
ative ?goodness? of a seed term (its eventual total
yield and the number of steps required to obtain that)
within five harvesting steps. We have shown that due
to the strongly connected components in text min-
ing networks, not all elements within the harvested
graph can discover each other. This implies that har-
vesting algorithms have to be started with several
seeds to obtain adequate Recall (Vyas et al, 2009).
We have shown that centrality measures can be used
successfully to rank harvested terms to guide the net-
work traversal, and to validate the correctness of the
harvested terms.
In the future, the knowledge and observations
made in this study can be used to model the lexi-
cal usage of people over time and to develop new
semantic search technology.
6 Conclusion
In this paper we describe the implicit ?hidden? se-
mantic network graph structure induced over the text
of the web and other sources by the semantic rela-
tions people use in sentences. We describe how term
harvesting patterns whose seed terms are harvested
and then applied recursively can be used to discover
these semantic term networks. Although these net-
works differ considerably from the web in relation
density, type, and network size, we show, some-
what surprisingly, that the same power-law, small-
world effect, transitivity, and most other character-
istics that apply to the web?s hyperlinked network
structure hold also for the implicit semantic term
graphs?certainly for the semantic relations and lan-
guages we have studied, and most probably for al-
most all semantic relations and human languages.
This rather interesting observation leads us to sur-
mise that the hyperlinks people create in the web are
of essentially the same type as the semantic relations
people use in normal sentences, and that they form
an extension of normal language that was not needed
before because people did not have the ability within
the span of a single sentence to ?embed? structures
larger than a clause?certainly not a whole other
page?s worth of information. The principal excep-
tion is the academic citation reference (lexicalized
as ?see?), which is not used in modern webpages.
Rather, the ?lexicalization? now used is a formatting
convention: the hyperlink is colored and often un-
derlined, facilities offered by computer screens but
not available to speech or easy in traditional typeset-
ting.
1623
Acknowledgments
We acknowledge the support of DARPA contract
number FA8750-09-C-3705 and NSF grant IIS-
0429360. We would like to thank Sujith Ravi for
his useful comments and suggestions.
References
Eugene Agichtein and Luis Gravano. 2000. Snowball:
Extracting relations from large plain-text collections.
pages 85?94.
Andrei Broder, Ravi Kumar, Farzin Maghoul, Prabhakar
Raghavan, Sridhar Rajagopalan, Raymie Stata, An-
drew Tomkins, and Janet Wiener. 2000. Graph struc-
ture in the web. Comput. Netw., 33(1-6):309?320.
Andrew Carlson, Justin Betteridge, Richard C. Wang, Es-
tevam R. Hruschka Jr., and Tom M. Mitchell. 2010.
Coupled semi-supervised learning for information ex-
traction. pages 101?110.
Peng Chen, Huafeng Xie, Sergei Maslov, and Sid Redner.
2007. Finding scientific gems with google?s pagerank
algorithm. Journal of Informetrics, 1(1):8?15, Jan-
uary.
Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. New-
man. 2009. Power-law distributions in empirical data.
SIAM Rev., 51(4):661?703.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsuper-
vised named-entity extraction from the web: an exper-
imental study. Artificial Intelligence, 165(1):91?134,
June.
Linton Freeman. 1979. Centrality in social networks
conceptual clarification. Social Networks, 1(3):215?
239.
Michael Gasser and Linda B. Smith. 1998. Learning
nouns and adjectives: A connectionist account. In
Language and Cognitive Processes, pages 269?306.
Demdre Gentner. 1981. Some interesting differences be-
tween nouns and verbs. Cognition and Brain Theory,
pages 161?178.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2003. Learning semantic constraints for the automatic
discovery of part-whole relations. In Proceedings of
the 2003 Conference of the North American Chapter of
the Association for Computational Linguistics on Hu-
man Language Technology, pages 1?8.
Marti Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proceedings of the 14th
conference on Computational linguistics, pages 539?
545.
Boris Katz, Jimmy Lin, Daniel Loreto, Wesley Hilde-
brandt, Matthew Bilotti, Sue Felshin, Aaron Fernan-
des, Gregory Marton, and Federico Mora. 2003. In-
tegrating web-based and corpus-based techniques for
question answering. In Proceedings of the twelfth text
retrieval conference (TREC), pages 426?435.
David Kempe, Jon Kleinberg, and E?va Tardos. 2003.
Maximizing the spread of influence through a social
network. In KDD ?03: Proceedings of the ninth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 137?146.
Jon Kleinberg and Steve Lawrence. 2001. The structure
of the web. Science, 29:1849?1850.
Zornitsa Kozareva and Eduard Hovy. 2010a. Learning
arguments and supertypes of semantic relations using
recursive patterns. In Proceedings of the 48th Annual
Meeting of the Association for Computational Linguis-
tics, ACL 2010, pages 1482?1491, July.
Zornitsa Kozareva and Eduard Hovy. 2010b. Not all
seeds are equal: Measuring the quality of text mining
seeds. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
618?626.
Zornitsa Kozareva, Ellen Riloff, and Eduard Hovy. 2008.
Semantic class learning from the web with hyponym
pattern linkage graphs. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics ACL-08: HLT, pages 1048?1056.
Beth Levin and Harold Somers. 1993. English verb
classes and alternations: A preliminary investigation.
Lun Li, David Alderson, Reiko Tanaka, John C. Doyle,
and Walter Willinger. 2005. Towards a Theory of
Scale-Free Graphs: Definition, Properties, and Impli-
cations (Extended Version). Internet Mathematica,
2(4):431?523.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of the 19th international confer-
ence on Computational linguistics, pages 1?7.
Mark E. Newman and Michelle Girvan. 2004. Find-
ing and evaluating community structure in networks.
Physical Review, 69(2).
Mark Newman. 2003. Mixing patterns in networks.
Physical Review E, 67.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The pagerank citation ranking:
Bringing order to the web.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. pages 113?120.
Marius Pasca. 2004. Acquisition of categorized named
entities for web search. In Proceedings of the thir-
teenth ACM international conference on Information
and knowledge management, pages 137?145.
1624
Marius Pasca. 2007. Weakly-supervised discovery of
named entities using web search queries. In Proceed-
ings of the Sixteenth ACM Conference on Information
and Knowledge Management, CIKM 2007, pages 683?
690.
Bruno R. Preiss. 1999. Data structures and algorithms
with object-oriented design patterns in C++.
Filippo Radicchi, Santo Fortunato, Benjamin Markines,
and Alessandro Vespignani. 2009. Diffusion of scien-
tific credits and the ranking of scientists. In Phys. Rev.
E 80, 056103.
Deepack Ravichandran and Eduard H. Hovy. 2002.
Learning surface text patterns for a question answer-
ing system. pages 41?47.
Ellen Riloff and Jessica Shepherd. 1997. A corpus-based
approach for building semantic lexicons. In Proceed-
ings of the Empirical Methods for Natural Language
Processing, pages 117?124.
Ellen Riloff. 1993. Automatically constructing a dictio-
nary for information extraction tasks. pages 811?816.
Peter Mark Roget. 1911. Roget?s thesaurus of English
Words and Phrases. New York Thomas Y. Crowell
company.
Gert Sabidussi. 1966. The centrality index of a graph.
Psychometrika, 31(4):581?603.
Hassan Sayyadi and Lise Getoor. 2009. Future rank:
Ranking scientific articles by predicting their future
pagerank. In 2009 SIAM International Conference on
Data Mining (SDM09).
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44?49.
Rion Snow, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym
discovery. pages 1297?1304.
Stephen Soderland, Claire Cardie, and Raymond
Mooney. 1999. Learning information extraction rules
for semi-structured and free text. Machine Learning,
34(1-3), pages 233?272.
Mark Steyvers and Joshua B. Tenenbaum. 2004. The
large-scale structure of semantic networks: Statistical
analyses and a model of semantic growth. Cognitive
Science, 29:41?78.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard
Weikum. 2007. Yago: a core of semantic knowledge.
In WWW ?07: Proceedings of the 16th international
conference on World Wide Web, pages 697?706.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Graph-based weakly-supervised methods for informa-
tion extraction and integration. pages 1473?1481.
Vishnu Vyas, Patrick Pantel, and Eric Crestan. 2009.
Helping editors choose better seed sets for entity set
expansion. In Proceedings of the 18th ACM Con-
ference on Information and Knowledge Management,
CIKM, pages 225?234.
Dylan Walker, Huafeng Xie, Koon-Kiu Yan, and Sergei
Maslov. 2006. Ranking scientific publications using a
simple model of network traffic. December.
Duncan Watts and Steven Strogatz. 1998. Collec-
tive dynamics of ?small-world? networks. Nature,
393(6684):440?442.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using selec-
tional preferences. In ACL-44: Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics, pages 849?856.
Dmitry Zelenko, Chinatsu Aone, Anthony Richardella,
Jaz K, Thomas Hofmann, Tomaso Poggio, and John
Shawe-taylor. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research 3.
1625
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 323?328,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Models and Training for Unsupervised Preposition Sense Disambiguation
Dirk Hovy and Ashish Vaswani and Stephen Tratz and
David Chiang and Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh,avaswani,stratz,chiang,hovy}@isi.edu
Abstract
We present a preliminary study on unsu-
pervised preposition sense disambiguation
(PSD), comparing different models and train-
ing techniques (EM, MAP-EM with L0 norm,
Bayesian inference using Gibbs sampling). To
our knowledge, this is the first attempt at un-
supervised preposition sense disambiguation.
Our best accuracy reaches 56%, a significant
improvement (at p <.001) of 16% over the
most-frequent-sense baseline.
1 Introduction
Reliable disambiguation of words plays an impor-
tant role in many NLP applications. Prepositions
are ubiquitous?they account for more than 10% of
the 1.16m words in the Brown corpus?and highly
ambiguous. The Preposition Project (Litkowski and
Hargraves, 2005) lists an average of 9.76 senses
for each of the 34 most frequent English preposi-
tions, while nouns usually have around two (Word-
Net nouns average about 1.2 senses, 2.7 if monose-
mous nouns are excluded (Fellbaum, 1998)). Dis-
ambiguating prepositions is thus a challenging and
interesting task in itself (as exemplified by the Sem-
Eval 2007 task, (Litkowski and Hargraves, 2007)),
and holds promise for NLP applications such as
Information Extraction or Machine Translation.1
Given a sentence such as the following:
In the morning, he shopped in Rome
we ultimately want to be able to annotate it as
1See (Chan et al, 2007) for how using WSD can help MT.
in/TEMPORAL the morning/TIME he/PERSON
shopped/SOCIAL in/LOCATIVE
Rome/LOCATION
Here, the preposition in has two distinct meanings,
namely a temporal and a locative one. These mean-
ings are context-dependent. Ultimately, we want
to disambiguate prepositions not by and for them-
selves, but in the context of sequential semantic la-
beling. This should also improve disambiguation of
the words linked by the prepositions (here, morn-
ing, shopped, and Rome). We propose using un-
supervised methods in order to leverage unlabeled
data, since, to our knowledge, there are no annotated
data sets that include both preposition and argument
senses. In this paper, we present our unsupervised
framework and show results for preposition disam-
biguation. We hope to present results for the joint
disambiguation of preposition and arguments in a
future paper.
The results from this work can be incorporated
into a number of NLP problems, such as seman-
tic tagging, which tries to assign not only syntac-
tic, but also semantic categories to unlabeled text.
Knowledge about semantic constraints of preposi-
tional constructions would not only provide better
label accuracy, but also aid in resolving preposi-
tional attachment problems. Learning by Reading
approaches (Mulkar-Mehta et al, 2010) also cru-
cially depend on unsupervised techniques as the
ones described here for textual enrichment.
Our contributions are:
? we present the first unsupervised preposition
sense disambiguation (PSD) system
323
? we compare the effectiveness of various models
and unsupervised training methods
? we present ways to extend this work to prepo-
sitional arguments
2 Preliminaries
A preposition p acts as a link between two words, h
and o. The head word h (a noun, adjective, or verb)
governs the preposition. In our example above, the
head word is shopped. The object of the preposi-
tional phrase (usually a noun) is denoted o, in our
example morning and Rome. We will refer to h and
o collectively as the prepositional arguments. The
triple h, p, o forms a syntactically and semantically
constrained structure. This structure is reflected in
dependency parses as a common construction. In
our example sentence above, the respective struc-
tures would be shopped in morning and shopped in
Rome. The senses of each element are denoted by a
barred letter, i.e., p? denotes the preposition sense, h?
denotes the sense of the head word, and o? the sense
of the object.
3 Data
We use the data set for the SemEval 2007 PSD
task, which consists of a training (16k) and a test
set (8k) of sentences with sense-annotated preposi-
tions following the sense inventory of The Preposi-
tion Project, TPP (Litkowski and Hargraves, 2005).
It defines senses for each of the 34 most frequent
prepositions. There are on average 9.76 senses per
preposition. This corpus was chosen as a starting
point for our study since it allows a comparison with
the original SemEval task. We plan to use larger
amounts of additional training data.
We used an in-house dependency parser to extract
the prepositional constructions from the data (e.g.,
?shop/VB in/IN Rome/NNP?). Pronouns and num-
bers are collapsed into ?PRO? and ?NUM?, respec-
tively.
In order to constrain the argument senses, we con-
struct a dictionary that lists for each word all the
possible lexicographer senses according to Word-
Net. The set of lexicographer senses (45) is a higher
level abstraction which is sufficiently coarse to allow
for a good generalization. Unknown words are as-
sumed to have all possible senses applicable to their
respective word class (i.e. all noun senses for words
labeled as nouns, etc).
4 Graphical Model
ph o
p?h? o?
h o
p?h? o?
h o
p?h? o?
a)
b)
c)
Figure 1: Graphical Models. a) 1st order HMM. b)
variant used in experiments (one model/preposition,
thus no conditioning on p). c) incorporates further
constraints on variables
As shown by Hovy et al (2010), preposition
senses can be accurately disambiguated using only
the head word and object of the PP. We exploit this
property of prepositional constructions to represent
the constraints between h, p, and o in a graphical
model. We define a good model as one that reason-
ably constrains the choices, but is still tractable in
terms of the number of parameters being estimated.
As a starting point, we choose the standard first-
order Hidden Markov Model as depicted in Figure
1a. Since we train a separate model for each preposi-
tion, we can omit all arcs to p. This results in model
1b. The joint distribution over the network can thus
be written as
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (1)
P (p?|h?) ? P (o?|p?) ? P (o|o?)
We want to incorporate as much information as
possible into the model to constrain the choices. In
Figure 1c, we condition p? on both h? and o?, to reflect
the fact that prepositions act as links and determine
324
their sense mainly through context. In order to con-
strain the object sense o?, we condition on h?, similar
to a second-order HMM. The actual object o is con-
ditioned on both p? and o?. The joint distribution is
equal to
Pp(h, o, h?, p?, o?) = P (h?) ? P (h|h?) ? (2)
P (o?|h?) ? P (p?|h?, o?) ? P (o|o?, p?)
Though we would like to also condition the prepo-
sition sense p? on the head word h (i.e., an arc be-
tween them in 1c) in order to capture idioms and
fixed phrases, this would increase the number of pa-
rameters prohibitively.
5 Training
The training method largely determines how well the
resulting model explains the data. Ideally, the sense
distribution found by the model matches the real
one. Since most linguistic distributions are Zipfian,
we want a training method that encourages sparsity
in the model.
We briefly introduce different unsupervised train-
ing methods and discuss their respective advantages
and disadvantages. Unless specified otherwise, we
initialized all models uniformly, and trained until the
perplexity rate stopped increasing or a predefined
number of iterations was reached. Note that MAP-
EM and Bayesian Inference require tuning of some
hyper-parameters on held-out data, and are thus not
fully unsupervised.
5.1 EM
We use the EM algorithm (Dempster et al, 1977) as
a baseline. It is relatively easy to implement with ex-
isting toolkits like Carmel (Graehl, 1997). However,
EM has a tendency to assume equal importance for
each parameter. It thus prefers ?general? solutions,
assigning part of the probability mass to unlikely
states (Johnson, 2007). We ran EM on each model
for 100 iterations, or until the perplexity stopped de-
creasing below a threshold of 10?6.
5.2 EM with Smoothing and Restarts
In addition to the baseline, we ran 100 restarts with
random initialization and smoothed the fractional
counts by adding 0.1 before normalizing (Eisner,
2002). Smoothing helps to prevent overfitting. Re-
peated random restarts help escape unfavorable ini-
tializations that lead to local maxima. Carmel pro-
vides options for both smoothing and restarts.
5.3 MAP-EM with L0 Norm
Since we want to encourage sparsity in our mod-
els, we use the MDL-inspired technique intro-
duced by Vaswani et al (2010). Here, the goal
is to increase the data likelihood while keeping
the number of parameters small. The authors use
a smoothed L0 prior, which encourages probabil-
ities to go down to 0. The prior involves hyper-
parameters ?, which rewards sparsity, and ?, which
controls how close the approximation is to the true
L0 norm.2 We perform a grid search to tune the
hyper-parameters of the smoothed L0 prior for ac-
curacy on the preposition against, since it has a
medium number of senses and instances. For HMM,
we set ?trans =100.0, ?trans =0.005, ?emit =1.0,
?emit =0.75. The subscripts trans and emit de-
note the transition and emission parameters. For
our model, we set ?trans =70.0, ?trans =0.05,
?emit =110.0, ?emit =0.0025. The latter resulted
in the best accuracy we achieved.
5.4 Bayesian Inference
Instead of EM, we can use Bayesian inference with
Gibbs sampling and Dirichlet priors (also known as
the Chinese Restaurant Process, CRP). We follow
the approach of Chiang et al (2010), running Gibbs
sampling for 10,000 iterations, with a burn-in pe-
riod of 5,000, and carry out automatic run selec-
tion over 10 random restarts.3 Again, we tuned the
hyper-parameters of our Dirichlet priors for accu-
racy via a grid search over the model for the prepo-
sition against. For both models, we set the concen-
tration parameter ?trans to 0.001, and ?emit to 0.1.
This encourages sparsity in the model and allows for
a more nuanced explanation of the data by shifting
probability mass to the few prominent classes.
2For more details, the reader is referred to Vaswani et al
(2010).
3Due to time and space constraints, we did not run the 1000
restarts used in Chiang et al (2010).
325
result table
Page 1
HMM
0.40 (0.40)
0.42 (0.42) 0.55 (0.55) 0.45 (0.45) 0.53 (0.53)
0.41 (0.41) 0.49 (0.49) 0.55 (0.56) 0.48 (0.49)
baseline Vanilla EM
EM, smoothed, 
100 random 
restarts
MAP-EM + 
smoothed L0 
norm
CRP, 10 random 
restarts
our model
Table 1: Accuracy over all prepositions w. different models and training. Best accuracy: MAP-
EM+smoothed L0 norm on our model. Italics denote significant improvement over baseline at p <.001.
Numbers in brackets include against (used to tune MAP-EM and Bayesian Inference hyper-parameters)
6 Results
Given a sequence h, p, o, we want to find the se-
quence of senses h?, p?, o? that maximizes the joint
probability. Since unsupervised methods use the
provided labels indiscriminately, we have to map the
resulting predictions to the gold labels. The pre-
dicted label sequence h?, p?, o? generated by the model
via Viterbi decoding can then be compared to the
true key. We use many-to-1 mapping as described
by Johnson (2007) and used in other unsupervised
tasks (Berg-Kirkpatrick et al, 2010), where each
predicted sense is mapped to the gold label it most
frequently occurs with in the test data. Success is
measured by the percentage of accurate predictions.
Here, we only evaluate p?.
The results presented in Table 1 were obtained
on the SemEval test set. We report results both
with and without against, since we tuned the hyper-
parameters of two training methods on this preposi-
tion. To test for significance, we use a two-tailed
t-test, comparing the number of correctly labeled
prepositions. As a baseline, we simply label all word
types with the same sense, i.e., each preposition to-
ken is labeled with its respective name. When using
many-to-1 accuracy, this technique is equivalent to a
most-frequent-sense baseline.
Vanilla EM does not improve significantly over
the baseline with either model, all other methods
do. Adding smoothing and random restarts increases
the gain considerably, illustrating how important
these techniques are for unsupervised training. We
note that EM performs better with the less complex
HMM.
CRP is somewhat surprisingly roughly equivalent
to EM with smoothing and random restarts. Accu-
racy might improve with more restarts.
MAP-EM with L0 normalization produces the
best result (56%), significantly outperforming the
baseline at p < .001. With more parameters (9.7k
vs. 3.7k), which allow for a better modeling of
the data, L0 normalization helps by zeroing out in-
frequent ones. However, the difference between
our complex model and the best HMM (EM with
smoothing and random restarts, 55%) is not signifi-
cant.
The best (supervised) system in the SemEval task
(Ye and Baldwin, 2007) reached 69% accuracy. The
best current supervised system we are aware of
(Hovy et al, 2010) reaches 84.8%.
7 Related Work
The semantics of prepositions were topic of a special
issue of Computational Linguistics (Baldwin et al,
2009). Preposition sense disambiguation was one of
the SemEval 2007 tasks (Litkowski and Hargraves,
2007), and was subsequently explored in a number
of papers using supervised approaches: O?Hara and
Wiebe (2009) present a supervised preposition sense
disambiguation approach which explores different
settings; Tratz and Hovy (2009), Hovy et al (2010)
make explicit use of the arguments for preposition
sense disambiguation, using various features. We
differ from these approaches by using unsupervised
methods and including argument labeling.
The constraints of prepositional constructions
have been explored by Rudzicz and Mokhov (2003)
and O?Hara and Wiebe (2003) to annotate the se-
mantic role of complete PPs with FrameNet and
Penn Treebank categories. Ye and Baldwin (2006)
explore the constraints of prepositional phrases for
326
semantic role labeling. We plan to use the con-
straints for argument disambiguation.
8 Conclusion and Future Work
We evaluate the influence of two different models (to
represent constraints) and three unsupervised train-
ing methods (to achieve sparse sense distributions)
on PSD. Using MAP-EM with L0 norm on our
model, we achieve an accuracy of 56%. This is a
significant improvement (at p <.001) over the base-
line and vanilla EM. We hope to shorten the gap to
supervised systems with more unlabeled data. We
also plan on training our models with EM with fea-
tures (Berg-Kirkpatrick et al, 2010).
The advantage of our approach is that the models
can be used to infer the senses of the prepositional
arguments as well as the preposition. We are cur-
rently annotating the data to produce a test set with
Amazon?s Mechanical Turk, in order to measure la-
bel accuracy for the preposition arguments.
Acknowledgements
We would like to thank Steve DeNeefe, Jonathan
Graehl, Victoria Fossum, and Kevin Knight, as well
as the anonymous reviewers for helpful comments
on how to improve the paper. We would also like
to thank Morgan from Curious Palate for letting us
write there. Research supported in part by Air Force
Contract FA8750-09-C-0172 under the DARPA Ma-
chine Reading Program and by DARPA under con-
tract DOI-NBC N10AP20031.
References
Tim Baldwin, Valia Kordoni, and Aline Villavicencio.
2009. Prepositions in applications: A survey and in-
troduction to the special issue. Computational Lin-
guistics, 35(2):119?149.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless Unsu-
pervised Learning with Features. In North American
Chapter of the Association for Computational Linguis-
tics.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Annual Meeting ? Association
For Computational Linguistics, volume 45, pages 33?
40.
David Chiang, Jonathan Graehl, Kevin Knight, Adam
Pauls, and Sujith Ravi. 2010. Bayesian inference
for Finite-State transducers. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 447?455. Association for
Computational Linguistics.
Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-
bin. 1977. Maximum likelihood from incomplete data
via the EM algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), 39(1):1?38.
Jason Eisner. 2002. An interactive spreadsheet for teach-
ing the forward-backward algorithm. In Proceed-
ings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language process-
ing and computational linguistics-Volume 1, pages 10?
18. Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Jonathan Graehl. 1997. Carmel Finite-state Toolkit.
ISI/USC.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2005. The prepo-
sition project. ACL-SIGSEM Workshop on ?The Lin-
guistic Dimensions of Prepositions and Their Use in
Computational Linguistic Formalisms and Applica-
tions?, pages 171?179.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Rutu Mulkar-Mehta, James Allen, Jerry Hobbs, Eduard
Hovy, Bernardo Magnini, and Christopher Manning,
editors. 2010. Proceedings of the NAACL HLT
2010 First International Workshop on Formalisms and
Methodology for Learning by Reading. Association
for Computational Linguistics, Los Angeles, Califor-
nia, June.
Tom O?Hara and Janyce Wiebe. 2003. Preposi-
tion semantic classification via Penn Treebank and
FrameNet. In Proceedings of CoNLL, pages 79?86.
Tom O?Hara and Janyce Wiebe. 2009. Exploiting se-
mantic role resources for preposition disambiguation.
Computational Linguistics, 35(2):151?184.
327
Frank Rudzicz and Serguei A. Mokhov. 2003. Towards
a heuristic categorization of prepositional phrases in
english with wordnet. Technical report, Cornell
University, arxiv1.library.cornell.edu/abs/1002.1095-
?context=cs.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an MDL-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Patrick Ye and Tim Baldwin. 2006. Semantic role la-
beling of prepositional phrases. ACM Transactions
on Asian Language Information Processing (TALIP),
5(3):228?244.
Patrick Ye and Timothy Baldwin. 2007. MELB-YB:
Preposition Sense Disambiguation Using Rich Seman-
tic Features. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
328
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 546?551,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Empirical Evaluation of Data-Driven Paraphrase Generation Techniques
Donald Metzler
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
metzler@isi.edu
Eduard Hovy
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
hovy@isi.edu
Chunliang Zhang
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA, USA
czheng@isi.edu
Abstract
Paraphrase generation is an important task
that has received a great deal of interest re-
cently. Proposed data-driven solutions to the
problem have ranged from simple approaches
that make minimal use of NLP tools to more
complex approaches that rely on numerous
language-dependent resources. Despite all of
the attention, there have been very few direct
empirical evaluations comparing the merits of
the different approaches. This paper empiri-
cally examines the tradeoffs between simple
and sophisticated paraphrase harvesting ap-
proaches to help shed light on their strengths
and weaknesses. Our evaluation reveals that
very simple approaches fare surprisingly well
and have a number of distinct advantages, in-
cluding strong precision, good coverage, and
low redundancy.
1 Introduction
A popular idiom states that ?variety is the spice of
life?. As with life, variety also adds spice and appeal
to language. Paraphrases make it possible to express
the same meaning in an almost unbounded number
of ways. While variety prevents language from be-
ing overly rigid and boring, it also makes it difficult
to algorithmically determine if two phrases or sen-
tences express the same meaning. In an attempt to
address this problem, a great deal of recent research
has focused on identifying, generating, and harvest-
ing phrase- and sentence-level paraphrases (Barzi-
lay and McKeown, 2001; Bhagat and Ravichan-
dran, 2008; Barzilay and Lee, 2003; Bannard and
Callison-Burch, 2005; Callison-Burch, 2008; Lin
and Pantel, 2001; Pang et al, 2003; Pasca and Di-
enes, 2005)
Many data-driven approaches to the paraphrase
problem have been proposed. The approaches vastly
differ in their complexity and the amount of NLP re-
sources that they rely on. At one end of the spec-
trum are approaches that generate paraphrases from
a large monolingual corpus and minimally rely on
NLP tools. Such approaches typically make use
of statistical co-occurrences, which act as a rather
crude proxy for semantics. At the other end of
the spectrum are more complex approaches that re-
quire access to bilingual parallel corpora and may
also rely on part-of-speech (POS) taggers, chunkers,
parsers, and statistical machine translation tools.
Constructing large comparable and bilingual cor-
pora is expensive and, in some cases, impossible.
Despite all of the previous research, there have
not been any evaluations comparing the quality of
simple and sophisticated data-driven approaches for
generating paraphrases. Evaluation is not only im-
portant from a practical perspective, but also from
a methodological standpoint, as well, since it is of-
ten more fruitful to devote attention to building upon
the current state-of-the-art as opposed to improv-
ing upon less effective approaches. Although the
more sophisticated approaches have garnered con-
siderably more attention from researchers, from a
practical perspective, simplicity, quality, and flexi-
bility are the most important properties. But are sim-
ple methods adequate enough for the task?
The primary goal of this paper is to take a small
step towards addressing the lack of comparative
evaluations. To achieve this goal, we empirically
546
evaluate three previously proposed paraphrase gen-
eration techniques, which range from very simple
approaches that make use of little-to-no NLP or
language-dependent resources to more sophisticated
ones that heavily rely on such resources. Our eval-
uation helps develop a better understanding of the
strengths and weaknesses of each type of approach.
The evaluation also brings to light additional proper-
ties, including the number of redundant paraphrases
generated, that future approaches and evaluations
may want to consider more carefully.
2 Related Work
Instead of exhaustively covering the entire spectrum
of previously proposed paraphrasing techniques, our
evaluation focuses on two families of data-driven ap-
proaches that are widely studied and used. More
comprehensive surveys of data-driven paraphrasing
techniques can be found in Androutsopoulos and
Malakasiotis (2010) and Madnani and Dorr (2010).
The first family of approaches that we consider
harvests paraphrases from monolingual corpora us-
ing distributional similarity. The DIRT algorithm,
proposed by Lin and Pantel (2001), uses parse tree
paths as contexts for computing distributional sim-
ilarity. In this way, two phrases were considered
similar if they occurred in similar contexts within
many sentences. Although parse tree paths serve as
rich representations, they are costly to construct and
yield sparse representations. The approach proposed
by Pasca and Dienes (2005) avoided the costs asso-
ciated with parsing by using n-gram contexts. Given
the simplicity of the approach, the authors were able
to harvest paraphrases from a very large collection
of news articles. Bhagat and Ravichandran (2008)
proposed a similar approach that used noun phrase
chunks as contexts and locality sensitive hashing
to reduce the dimensionality of the context vectors.
Despite their simplicity, such techniques are suscep-
tible to a number of issues stemming from the distri-
butional assumption. For example, such approaches
have a propensity to assign large scores to antonyms
and other semantically irrelevant phrases.
The second line of research uses comparable or
bilingual corpora as the ?pivot? that binds para-
phrases together (Barzilay and McKeown, 2001;
Barzilay and Lee, 2003; Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Pang et al,
2003). Amongst the most effective recent work,
Bannard and Callison-Burch (2005) show how dif-
ferent English translations of the same entry in a
statistically-derived translation table can be viewed
as paraphrases. The recent work by Zhao et al
(Zhao et al, 2009) uses a generalization of DIRT-
style patterns to generate paraphrases from a bilin-
gual parallel corpus. The primary drawback of these
type of approaches is that they require a consider-
able amount of resource engineering that may not be
available for all languages, domains, or applications.
3 Experimental Evaluation
The goal of our experimental evaluation is to ana-
lyze the effectiveness of a variety of paraphrase gen-
eration techniques, ranging from simple to sophis-
ticated. Our evaluation focuses on generating para-
phrases for verb phrases, which tend to exhibit more
variation than other types of phrases. Furthermore,
our interest in paraphrase generation was initially
inspired by challenges encountered during research
related to machine reading (Barker et al, 2007). In-
formation extraction systems, which are key compo-
nent of machine reading systems, can use paraphrase
technology to automatically expand seed sets of re-
lation triggers, which are commonly verb phrases.
3.1 Systems
Our evaluation compares the effectiveness of the
following paraphrase harvesting approaches:
PD: The basic distributional similarity-inspired
approach proposed by Pasca and Dienes (2005)
that uses variable-length n-gram contexts and
overlap-based scoring. The context of a phrase
is defined as the concatenation of the n-grams
immediately to the left and right of the phrase. We
set the minimum length of an n-gram context to be
2 and the maximum length to be 3. The maximum
length of a phrase is set to 5.
BR: The distributional similarity approach proposed
by Bhagat and Ravichandran (2008) that uses noun
phrase chunks as contexts and locality sensitive
hashing to reduce the dimensionality of the contex-
tual vectors.
547
BCB-S: An extension of the Bannard Callison-
Burch (Bannard and Callison-Burch, 2005)
approach that constrains the paraphrases to have the
same syntactic type as the original phrase (Callison-
Burch, 2008). We constrained all paraphrases to be
verb phrases.
We chose these three particular systems because
they span the spectrum of paraphrase approaches, in
that the PD approach is simple and does not rely on
any NLP resources while the BCB-S approach is so-
phisticated and makes heavy use of NLP resources.
For the two distributional similarity approaches
(PD and BR), paraphrases were harvested from the
English Gigaword Fourth Edition corpus and scored
using the cosine similarity between PMI weighted
contextual vectors. For the BCB-S approach, we
made use of a publicly available implementation1.
3.2 Evaluation Methodology
We randomly sampled 50 verb phrases from 1000
news articles about terrorism and another 50 verb
phrases from 500 news articles about American
football. Individual occurrences of verb phrases
were sampled, which means that more common verb
phrases were more likely to be selected and that a
given phrase could be selected multiple times. This
sampling strategy was used to evaluate the systems
across a realistic sample of phrases. To obtain a
richer class of phrases beyond basic verb groups, we
defined verb phrases to be contiguous sequences of
tokens that matched the following POS tag pattern:
(TO | IN | RB | MD | VB)+.
Following the methodology used in previous
paraphrase evaluations (Bannard and Callison-
Burch, 2005; Callison-Burch, 2008; Kok and Brock-
ett, 2010), we presented annotators with two sen-
tences. The first sentence was randomly selected
from amongst all of the sentences in the evaluation
corpus that contain the original phrase. The second
sentence was the same as the first, except the orig-
inal phrase is replaced with the system generated
paraphrase. Annotators were given the following
options, which were adopted from those described
by Kok and Brockett (2010), for each sentence pair:
0) Different meaning; 1) Same meaning; revised is
1Available at http://www.cs.jhu.edu/?ccb/.
grammatically incorrect; and 2) Same meaning; re-
vised is grammatically correct. Table 1 shows three
example sentence pairs and their corresponding an-
notations according to the guidelines just described.
Amazon?s Mechanical Turk service was used to
collect crowdsourced annotations. For each para-
phrase system, we retrieve (up to) 10 paraphrases
for each phrase in the evaluation set. This yields
a total of 6,465 unique (phrase, paraphrase) pairs
after pooling results from all systems. Each Me-
chanical Turk HIT consisted of 12 sentence pairs.
To ensure high quality annotations and help iden-
tify spammers, 2 of the 12 sentence pairs per HIT
were actually ?hidden tests? for which the correct
answer was known by us. We automatically rejected
any HITs where the worker failed either of these hid-
den tests. We also rejected all work from annotators
who failed at least 25% of their hidden tests. We
collected a total of 51,680 annotations. We rejected
65% of the annotations based on the hidden test fil-
tering just described, leaving 18,150 annotations for
our evaluation. Each sentence pair received a mini-
mum of 1, a median of 3, and maximum of 6 anno-
tations. The raw agreement of the annotators (after
filtering) was 77% and the Fleiss? Kappa was 0.43,
which signifies moderate agreement (Fleiss, 1971;
Landis and Koch, 1977).
The systems were evaluated in terms of coverage
and expected precision at k. Coverage is defined
as the percentage of phrases for which the system
returned at least one paraphrase. Expected precision
at k is the expected number of correct paraphrases
amongst the top k returned, and is computed as:
E[p@k] =
1
k
k?
i=1
pi
where pi is the proportion of positive annotations
for item i. When computing the mean expected
precision over a set of input phrases, only those
phrases that generate one or more paraphrases is
considered in the mean. Hence, if precision were
to be averaged over all 100 phrases, then systems
with poor coverage would perform significantly
worse. Thus, one should take a holistic view of the
results, rather than focus on coverage or precision
in isolation, but consider them, and their respective
tradeoffs, together.
548
Sentence Pair Annotation
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was officially announced in Aden.
0
A five-man presidential council for the independent state newly proclaimed in south Yemen
was named overnight Saturday, it was cancelled in Aden.
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem to
protest against the killing of Sharif.
1
Dozens of Palestinian youths held rally in the Abu Dis Arab village in East Jerusalem in
protest of against the killing of Sharif.
It says that foreign companies have no greater right to compensation ? establishing debts at a
1/1 ratio of the dollar to the peso ? than Argentine citizens do.
2
It says that foreign companies have no greater right to compensation ? setting debts at a 1/1
ratio of the dollar to the peso ? than Argentine citizens do.
Table 1: Example annotated sentence pairs. In each pair, the first sentence is the original and the second has a system-
generated paraphrase filled in (denoted by the bold text).
Method C
Lenient Strict
P1 P5 P10 P1 P5 P10
PD 86 .48 .42 .36 .25 .22 .19
BR 84 .83 .65 .52 .16 .17 .15
BCB-S 62 .63 .45 .34 .22 .17 .13
Table 2: Coverage (C) and expected precision at k (Pk)
under lenient and strict evaluation criteria.
Two binarized evaluation criteria are reported.
The lenient criterion allows for grammatical er-
rors in the paraphrased sentence, while the strict
criterion does not.
3.3 Basic Results
Table 2 summarizes the results of our evaluation.
For this evaluation, all 100 verb phrases were run
through each system. The paraphrases returned by
the systems were then ranked (ordered) in descend-
ing order of their score, thus placing the highest
scoring item at rank 1. Bolded values represent the
best result for a given metric.
As expected, the results show that the systems
perform significantly worse under the strict evalu-
ation criteria, which requires the paraphrased sen-
tences to be grammatically correct. None of the ap-
proaches tested used any information from the eval-
uation sentences (other than the fact a verb phrase
was to be filled in). Recent work showed that us-
ing language models and/or syntactic clues from the
evaluation sentence can improve the grammatical-
ity of the paraphrased sentences (Callison-Burch,
Method
Lenient Strict
P1 P5 P10 P1 P5 P10
PD .26 .22 .20 .19 .16 .15
BR .05 .10 .11 .04 .05 .05
BCB-S .24 .25 .20 .17 .14 .10
Table 3: Expected precision at k (Pk) when considering
redundancy under lenient and strict evaluation criteria.
2008). Such approaches could likely be used to im-
prove the quality of all of the approaches under the
strict evaluation criteria.
In terms of coverage, the distributional similarity
approaches performed the best. In another set of ex-
periments, we used the PD method to harvest para-
phrases from a large Web corpus, and found that the
coverage was 98%. Achieving similar coverage with
resource-dependent approaches would likely require
more human and machine effort.
3.4 Redundancy
After manually inspecting the results returned by the
various paraphrase systems, we noticed that some
approaches returned highly redundant paraphrases
that were of limited practical use. For example,
for the phrase ?were losing?, the BR system re-
turned ?are losing?, ?have been losing?, ?have lost?,
?lose?, ?might lose?, ?had lost?, ?stand to lose?,
?who have lost? and ?would lose? within the top 10
paraphrases. All of these are simple variants that
contain different forms of the verb ?lose?. Under
the lenient evaluation criterion almost all of these
paraphrases would be marked as correct, since the
549
same verb is being returned with some grammati-
cal modifications. While highly redundant output
of this form may be useful for some tasks, for oth-
ers (such as information extraction) is it more useful
to identify paraphrases that contain a diverse, non-
redundant set of verbs.
Therefore, we carried out another evaluation
aimed at penalizing highly redundant outputs. For
each approach, we manually identified all of the
paraphrases that contained the same verb as the
main verb in the original phrase. During evalua-
tion, these ?redundant? paraphrases were regarded
as non-related.
The results from this experiment are provided in
Table 3. The results are dramatically different com-
pared to those in Table 2, suggesting that evaluations
that do not consider this type of redundancy may
over-estimate actual system quality. The percent-
age of results marked as redundant for the BCB-S,
BR, and PD approaches were 22.6%, 52.5%, and
22.9%, respectively. Thus, the BR system, which
appeared to have excellent (lenient) precision in our
initial evaluation, returns a very large number of re-
dundant paraphrases. This remarkably reduces the
lenient P1 from 0.83 in our initial evaluation to just
0.05 in our redundancy-based evaluation. The BCB-
S and PD approaches return a comparable number of
redundant results. As with our previous evaluation,
the BCB-S approach tends to perform better under
the lenient evaluation, while PD is better under the
strict evaluation. Estimated 95% confidence inter-
vals show all differences between BCB-S and PD
are statistically significant, except for lenient P10.
Of course, existing paraphrasing approaches do
not explicitly account for redundancy, and hence this
evaluation is not completely fair. However, these
findings suggest that redundancy may be an impor-
tant issue to consider when developing and evalu-
ating data-driven paraphrase approaches. There are
likely other characteristics, beyond redundancy, that
may also be important for developing robust, effec-
tive paraphrasing techniques. Exploring the space
of such characteristics in a task-dependent manner
is an important direction of future work.
3.5 Discussion
In all of our evaluations, we found that the simple
approaches are surprisingly effective in terms of pre-
cision, coverage, and redundancy, making them a
reasonable choice for an ?out of the box? approach
for this particular task. However, additional task-
dependent comparative evaluations are necessary to
develop even deeper insights into the pros and cons
of the different types of approaches.
From a high level perspective, it is also important
to note that the precision of these widely used, com-
monly studied paraphrase generation approaches is
still extremely poor. After accounting for redun-
dancy, the best approaches achieve a precision at 1
of less than 20% using the strict criteria and less than
26% when using the lenient criteria. This suggests
that there is still substantial work left to be done be-
fore the output of these systems can reliably be used
to support other tasks.
4 Conclusions and Future Work
This paper examined the tradeoffs between simple
paraphrasing approaches that do not make use of any
NLP resources and more sophisticated approaches
that use a variety of such resources. Our evaluation
demonstrated that simple harvesting approaches fare
well against more sophisticated approaches, achiev-
ing state-of-the-art precision, good coverage, and
relatively low redundancy.
In the future, we would like to see more em-
pirical evaluations and detailed studies comparing
the practical merits of various paraphrase genera-
tion techniques. As Madnani and Dorr (Madnani
and Dorr, 2010) suggested, it would be beneficial
to the research community to develop a standard,
shared evaluation that would act to catalyze further
advances and encourage more meaningful compara-
tive evaluations of such approaches moving forward.
Acknowledgments
The authors gratefully acknowledge the support of
the DARPA Machine Reading Program under AFRL
prime contract no. FA8750-09-C-3705. Any opin-
ions, findings, and conclusion or recommendations
expressed in this material are those of the au-
thors and do not necessarily reflect the view of the
DARPA, AFRL, or the US government. We would
also like to thank the anonymous reviewers for their
valuable feedback and the Mechanical Turk workers
for their efforts.
550
References
I. Androutsopoulos and P. Malakasiotis. 2010. A survey
of paraphrasing and textual entailment methods. Jour-
nal of Artificial Intelligence Research, 38:135?187.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of the 43rd Annual Meeting on Association for
Computational Linguistics, ACL ?05, pages 597?604,
Morristown, NJ, USA. Association for Computational
Linguistics.
Ken Barker, Bhalchandra Agashe, Shaw-Yi Chaw, James
Fan, Noah Friedland, Michael Glass, Jerry Hobbs,
Eduard Hovy, David Israel, Doo Soon Kim, Rutu
Mulkar-Mehta, Sourabh Patwardhan, Bruce Porter,
Dan Tecuci, and Peter Yeh. 2007. Learning by read-
ing: a prototype system, performance baseline and
lessons learned. In Proceedings of the 22nd national
conference on Artificial intelligence - Volume 1, pages
280?286. AAAI Press.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: an unsupervised approach using multiple-
sequence alignment. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 16?
23, Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Regina Barzilay and Kathleen R. McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Pro-
ceedings of the 39th Annual Meeting on Association
for Computational Linguistics, ACL ?01, pages 50?57,
Morristown, NJ, USA. Association for Computational
Linguistics.
Rahul Bhagat and Deepak Ravichandran. 2008. Large
scale acquisition of paraphrases for learning surface
patterns. In Proceedings of ACL-08: HLT, pages 674?
682, Columbus, Ohio, June. Association for Computa-
tional Linguistics.
Chris Callison-Burch. 2008. Syntactic constraints on
paraphrases extracted from parallel corpora. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, EMNLP ?08, pages
196?205, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Joseph L. Fleiss. 1971. Measuring Nominal Scale
Agreement Among Many Raters. Psychological Bul-
letin, 76(5):378?382.
Stanley Kok and Chris Brockett. 2010. Hitting the right
paraphrases in good time. In Human Language Tech-
nologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, HLT ?10, pages 145?153, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
J. R. Landis and G. G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174, March.
Dekang Lin and Patrick Pantel. 2001. Discovery of in-
ference rules for question-answering. Nat. Lang. Eng.,
7:343?360, December.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Comput. Linguist., 36:341?387.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: ex-
tracting paraphrases and generating new sentences.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology -
Volume 1, NAACL ?03, pages 102?109, Morristown,
NJ, USA. Association for Computational Linguistics.
Marius Pasca and Pter Dienes. 2005. Aligning needles
in a haystack: Paraphrase acquisition across the web.
In Robert Dale, Kam-Fai Wong, Jian Su, and Oi Yee
Kwong, editors, Natural Language Processing IJC-
NLP 2005, volume 3651 of Lecture Notes in Computer
Science, pages 119?130. Springer Berlin / Heidelberg.
Shiqi Zhao, Haifeng Wang, Ting Liu, and Sheng Li.
2009. Extracting paraphrase patterns from bilin-
gual parallel corpora. Natural Language Engineering,
15(Special Issue 04):503?526.
551
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 372?381,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatic Interpretation of the English Possessive
Stephen Tratz ?
Army Research Laboratory
Adelphi Laboratory Center
2800 Powder Mill Road
Adelphi, MD 20783
stephen.c.tratz.civ@mail.mil
Eduard Hovy ?
Carnegie Mellon University
Language Technologies Institute
5000 Forbes Avenue
Pittsburgh, PA 15213
hovy@cmu.edu
Abstract
The English ?s possessive construction oc-
curs frequently in text and can encode
several different semantic relations; how-
ever, it has received limited attention from
the computational linguistics community.
This paper describes the creation of a se-
mantic relation inventory covering the use
of ?s, an inter-annotator agreement study
to calculate how well humans can agree
on the relations, a large collection of pos-
sessives annotated according to the rela-
tions, and an accurate automatic annota-
tion system for labeling new examples.
Our 21,938 example dataset is by far the
largest annotated possessives dataset we
are aware of, and both our automatic clas-
sification system, which achieves 87.4%
accuracy in our classification experiment,
and our annotation data are publicly avail-
able.
1 Introduction
The English ?s possessive construction occurs fre-
quently in text?approximately 1.8 times for every
100 hundred words in the Penn Treebank1(Marcus
et al, 1993)?and can encode a number of
different semantic relations including ownership
(John?s car), part-of-whole (John?s arm), extent (6
hours? drive), and location (America?s rivers). Ac-
curate automatic possessive interpretation could
aid many natural language processing (NLP) ap-
plications, especially those that build semantic
representations for text understanding, text gener-
ation, question answering, or information extrac-
tion. These interpretations could be valuable for
machine translation to or from languages that al-
low different semantic relations to be encoded by
?The authors were affiliated with the USC Information
Sciences Institute at the time this work was performed.
the possessive/genitive.
This paper presents an inventory of 17 semantic
relations expressed by the English ?s-construction,
a large dataset annotated according to the this in-
ventory, and an accurate automatic classification
system. The final inter-annotator agreement study
achieved a strong level of agreement, 0.78 Fleiss?
Kappa (Fleiss, 1971) and the dataset is easily the
largest manually annotated dataset of possessive
constructions created to date. We show that our
automatic classication system is highly accurate,
achieving 87.4% accuracy on a held-out test set.
2 Background
Although the linguistics field has devoted signif-
icant effort to the English possessive (?6.1), the
computational linguistics community has given it
limited attention. By far the most notable excep-
tion to this is the line of work by Moldovan and
Badulescu (Moldovan and Badulescu, 2005; Bad-
ulescu and Moldovan, 2009), who define a tax-
onomy of relations, annotate data, calculate inter-
annotator agreement, and perform automatic clas-
sification experiments. Badulescu and Moldovan
(2009) investigate both ?s-constructions and of
constructions in the same context using a list of 36
semantic relations (including OTHER). They take
their examples from a collection of 20,000 ran-
domly selected sentences from Los Angeles Times
news articles used in TREC-9. For the 960 ex-
tracted ?s-possessive examples, only 20 of their se-
mantic relations are observed, including OTHER,
with 8 of the observed relations occurring fewer
than 10 times. They report a 0.82 Kappa agree-
ment (Siegel and Castellan, 1988) for the two
computational semantics graduates who annotate
the data, stating that this strong result ?can be ex-
plained by the instructions the annotators received
1Possessive pronouns such as his and their are treated as
?s constructions in this work.
372
prior to annotation and by their expertise in Lexi-
cal Semantics.?
Moldovan and Badulescu experiment with sev-
eral different classification techniques. They find
that their semantic scattering technique signifi-
cantly outperforms their comparison systems with
its F-measure score of 78.75. Their SVM system
performs the worst with only 23.25% accuracy?
suprisingly low, especially considering that 220 of
the 960 ?s examples have the same label.
Unfortunately, Badulescu and Moldovan (2009)
have not publicly released their data2. Also, it is
sometimes difficult to understand the meaning of
the semantic relations, partly because most rela-
tions are only described by a single example and,
to a lesser extent, because the bulk of the given
examples are of -constructions. For example, why
President of Bolivia warrants a SOURCE/FROM re-
lation but University of Texas is assigned to LOCA-
TION/SPACE is unclear. Their relations and pro-
vided examples are presented below in Table 1.
Relation Examples
POSSESSION Mary?s book
KINSHIP Mary?s brother
PROPERTY John?s coldness
AGENT investigation of the crew
TEMPORAL last year?s exhibition
DEPICTION-DEPICTED a picture of my niece
PART-WHOLE the girl?s mouth
CAUSE death of cancer
MAKE/PRODUCE maker of computer
LOCATION/SPACE Univerity of Texas
SOURCE/FROM President of Bolivia
TOPIC museum of art
ACCOMPANIMENT solution of the problem
EXPERIENCER victim of lung disease
RECIPIENT Josephine?s reward
ASSOCIATED WITH contractors of shipyard
MEASURE hundred (sp?) of dollars
THEME acquisition of the holding
RESULT result of the review
OTHER state of emergency
Table 1: The 20 (out of an original 36) seman-
tic relations observed by Badulescu and Moldovan
(2009) along with their examples.
3 Dataset Creation
We created the dataset used in this work from
three different sources, each representing a distinct
genre?newswire, non-fiction, and fiction. Of the
2Email requests asking for relation definitions and the
data were not answered, and, thus, we are unable to provide
an informative comparison with their work.
21,938 total examples, 15,330 come from sections
2?21 of the Penn Treebank (Marcus et al, 1993).
Another 5,266 examples are from The History of
the Decline and Fall of the Roman Empire (Gib-
bon, 1776), a non-fiction work, and 1,342 are from
The Jungle Book (Kipling, 1894), a collection of
fictional short stories. For the Penn Treebank, we
extracted the examples using the provided gold
standard parse trees, whereas, for the latter cases,
we used the output of an open source parser (Tratz
and Hovy, 2011).
4 Semantic Relation Inventory
The initial semantic relation inventory for pos-
sessives was created by first examining some of
the relevant literature on possessives, including
work by Badulescu and Moldovan (2009), Barker
(1995), Quirk et al (1985), Rosenbach (2002), and
Taylor (1996), and then manually annotating the
large dataset of examples. Similar examples were
grouped together to form initial categories, and
groups that were considered more difficult were
later reexamined in greater detail. Once all the
examples were assigned to initial categories, the
process of refining the definitions and annotations
began.
In total, 17 relations were created, not including
OTHER. They are shown in Table 3 along with ap-
proximate (best guess) mappings to relations de-
fined by others, specifically those of Quirk et al
(1985), whose relations are presented in Table 2,
as well as Badulescu and Moldovan?s (2009) rela-
tions.
Relation Examples
POSSESSIVE my wife?s father
SUBJECTIVE boy?s application
OBJECTIVE the family?s support
ORIGIN the general?s letter
DESCRIPTIVE a women?s college
MEASURE ten days? absense
ATTRIBUTE the victim?s courage
PARTITIVE the baby?s eye
APPOSITION (marginal) Dublin?s fair city
Table 2: The semantic relations proposed by Quirk
et al (1985) for ?s along with some of their exam-
ples.
4.1 Refinement and Inter-annotator
Agreement
The semantic relation inventory was refined us-
ing an iterative process, with each iteration involv-
373
Relation Example HDFRE JB PTB Mappings
SUBJECTIVE Dora?s travels 1083 89 3169 Q:SUBJECTIVE, B:AGENT
PRODUCER?S PRODUCT Ford?s Taurus 47 44 1183 Q:ORIGIN, B:MAKE/PRODUCE
B:RESULT
OBJECTIVE Mowgli?s capture 380 7 624 Q:OBJECTIVE, B:THEME
CONTROLLER/OWNER/USER my apartment 882 157 3940 QB:POSSESSIVE
MENTAL EXPERIENCER Sam?s fury 277 22 232 Q:POSSESSIVE, B:EXPERIENCER
RECIPIENT their bonuses 12 6 382 Q:POSSESSIVE, B:RECIPIENT
MEMBER?S COLLECTION John?s family 144 31 230 QB:POSSESSIVE
PARTITIVE John?s arm 253 582 451 Q:PARTITIVE, B:PART-WHOLE
LOCATION Libya?s people 24 0 955 Q:POSSESSIVE, B:SOURCE/FROM
B:LOCATION/SPACE
TEMPORAL today?s rates 0 1 623 Q:POSSESSIVE, B:TEMPORAL
EXTENT 6 hours? drive 8 10 5 QB:MEASURE
KINSHIP Mary?s kid 324 156 264 Q:POSSESSIVE, B:KINSHIP
ATTRIBUTE picture?s vividness 1013 34 1017 Q:ATTRIBUTE, B:PROPERTY
TIME IN STATE his years in Ohio 145 32 237 QB:POSSESSIVE
POSSESSIVE COMPOUND the [men?s room] 0 0 67 Q:DESCRIPTIVE
ADJECTIVE DETERMINED his fellow Brit 12 0 33
OTHER RELATIONAL NOUN his friend 629 112 1772 QB:POSSESSIVE
OTHER your Lordship 33 59 146 B:OTHER
Table 3: Possessive semantic relations along with examples, counts, and approximate mappings to other
inventories. Q and B represent Quirk et al (1985) and Badulescu and Moldovan (2009), respectively.
HDFRE, JB, PTB: The History of the Decline and Fall of the Roman Empire, The Jungle Book, and the
Penn Treebank, respectively.
ing the annotation of a random set of 50 exam-
ples. Each set of examples was extracted such
that no two examples had an identical possessee
word. For a given example, annotators were in-
structed to select the most appropriate option but
could also record a second-best choice to provide
additional feedback. Figure 1 presents a screen-
shot of the HTML-based annotation interface. Af-
ter the annotation was complete for a given round,
agreement and entropy figures were calculated and
changes were made to the relation definitions and
dataset. The number of refinement rounds was ar-
bitrarily limited to five. To measure agreement,
in addition to calculating simple percentage agree-
ment, we computed Fleiss? Kappa (Fleiss, 1971),
a measure of agreement that incorporates a cor-
rection for agreement due to chance, similar to
Cohen?s Kappa (Cohen, 1960), but which can be
used to measure agreement involving more than
two annotations per item. The agreement and en-
tropy figures for these five intermediate annotation
rounds are given in Table 4. In all the possessive
annotation tables, Annotator A refers to the pri-
mary author and the labels B and C refer to two
additional annotators.
To calculate a final measure of inter-annotator
agreement, we randomly drew 150 examples from
the dataset not used in the previous refinement it-
erations, with 50 examples coming from each of
Figure 1: Screenshot of the HTML template page
used for annotation.
the three original data sources. All three annota-
tors initially agreed on 82 of the 150 examples,
leaving 68 examples with at least some disagree-
ment, including 17 for which all three annotators
disagreed.
Annotators then engaged in a new task in which
they re-annotated these 68 examples, in each case
being able to select only from the definitions pre-
viously chosen for each example by at least one
annotator. No indication of who or how many
people had previously selected the definitions was
374
Figure 2: Semantic relation distribution for the dataset presented in this work. HDFRE: History of the
Decline and Fall of the Roman Empire; JB: Jungle Book; PTB: Sections 2?21 of the Wall Street Journal
portion of the Penn Treebank.
given3. Annotators were instructed not to choose
a definition simply because they thought they had
chosen it before or because they thought some-
one else had chosen it. After the revision pro-
cess, all three annotators agreed in 109 cases and
all three disagreed in only 6 cases. During the re-
vision process, Annotator A made 8 changes, B
made 20 changes, and C made 33 changes. Anno-
tator A likely made the fewest changes because he,
as the primary author, spent a significant amount
of time thinking about, writing, and re-writing the
definitions used for the various iterations. Anno-
tator C?s annotation work tended to be less consis-
tent in general than Annotator B?s throughout this
work as well as in a different task not discussed
within this paper, which probably why Annotator
C made more changes than Annotator B. Prior to
this revision process, the three-way Fleiss? Kappa
score was 0.60 but, afterwards, it was at 0.78. The
inter-annotator agreement and entropy figures for
before and after this revision process, including
pairwise scores between individual annotators, are
presented in Tables 5 and 6.
4.2 Distribution of Relations
The distribution of semantic relations varies some-
what by the data source. The Jungle Book?s dis-
tribution is significantly different from the oth-
3Of course, if three definitions were present, it could be
inferred that all three annotators had initially disagreed.
ers, with a much larger percentage of PARTITIVE
and KINSHIP relations. The Penn Treebank and
The History of the Decline and Fall of the Ro-
man Empire were substantially more similar, al-
though there are notable differences. For instance,
the LOCATION and TEMPORAL relations almost
never occur in The History of the Decline and Fall
of the Roman Empire. Whether these differences
are due to variations in genre, time period, and/or
other factors would be an interesting topic for fu-
ture study. The distribution of relations for each
data source is presented in Figure 2.
Though it is harder to compare across datasets
using different annotation schemes, there are
at least a couple notable differences between
the distribution of relations for Badulescu and
Moldovan?s (2009) dataset and the distribution of
relations used in this work. One such difference is
the much higher percentage of examples labeled as
TEMPORAL?11.35% vs only 2.84% in our data.
Another difference is a higher incidence of the
KINSHIP relation (6.31% vs 3.39%), although it
is far less frequent than it is in The Jungle Book
(11.62%).
4.3 Encountered Ambiguities
One of the problems with creating a list of rela-
tions expressed by ?s-constructions is that some
examples can potentially fit into multiple cate-
gories. For example, Joe?s resentment encodes
375
both SUBJECTIVE relation and MENTAL EXPE-
RIENCER relations and UK?s cities encodes both
PARTITIVE and LOCATION relations. A represen-
tative list of these types of issues along with ex-
amples designed to illustrate them is presented in
Table 7.
5 Experiments
For the automatic classification experiments, we
set aside 10% of the data for test purposes, and
used the the remaining 90% for training. We used
5-fold cross-validation performed using the train-
ing data to tweak the included feature templates
and optimize training parameters.
5.1 Learning Approach
The LIBLINEAR (Fan et al, 2008) package was
used to train linear Support Vector Machine
(SVMs) for all the experiments in the one-against-
the-rest style. All training parameters took their
default values with the exception of the C pa-
rameter, which controls the tradeoff between mar-
gin width and training error and which was set to
0.02, the point of highest performance in the cross-
validation tuning.
5.2 Feature Generation
For feature generation, we conflated the pos-
sessive pronouns ?his?, ?her?, ?my?, and ?your?
to ?person.? Similarly, every term match-
ing the case-insensitive regular expression
(corp|co|plc|inc|ag|ltd|llc)\\.?) was replaced with
the word ?corporation.?
All the features used are functions of the follow-
ing five words.
? The possessor word
? The possessee word
? The syntactic governor of the possessee word
? The set of words between the possessor and
possessee word (e.g., first in John?s first kiss)
? The word to the right of the possessee
The following feature templates are used to gener-
ate features from the above words. Many of these
templates utilize information from WordNet (Fell-
baum, 1998).
? WordNet link types (link type list) (e.g., at-
tribute, hypernym, entailment)
? Lexicographer filenames (lexnames)?top
level categories used in WordNet (e.g.,
noun.body, verb.cognition)
? Set of words from the WordNet definitions
(gloss terms)
? The list of words connected via WordNet
part-of links (part words)
? The word?s text (the word itself)
? A collection of affix features (e.g., -ion, -er,
-ity, -ness, -ism)
? The last {2,3} letters of the word
? List of all possible parts-of-speech in Word-
Net for the word
? The part-of-speech assigned by the part-of-
speech tagger
? WordNet hypernyms
? WordNet synonyms
? Dependent words (all words linked as chil-
dren in the parse tree)
? Dependency relation to the word?s syntactic
governor
5.3 Results
The system predicted correct labels for 1,962 of
the 2,247 test examples, or 87.4%. The accuracy
figures for the test instances from the Penn Tree-
bank, The Jungle Book, and The History of the De-
cline and Fall of the Roman Empire were 88.8%,
84.7%, and 80.6%, respectively. The fact that the
score for The Jungle Book was the lowest is some-
what surprising considering it contains a high per-
centage of body part and kinship terms, which tend
to be straightforward, but this may be because the
other sources comprise approximately 94% of the
training examples.
Given that human agreement typically repre-
sents an upper bound on machine performance
in classification tasks, the 87.4% accuracy figure
may be somewhat surprising. One explanation is
that the examples pulled out for the inter-annotator
agreement study each had a unique possessee
word. For example, ?expectations?, as in ?ana-
lyst?s expectations?, occurs 26 times as the pos-
sessee in the dataset, but, for the inter-annotator
agreement study, at most one of these examples
could be included. More importantly, when the
initial relations were being defined, the data were
first sorted based upon the possessee and then the
possessor in order to create blocks of similar ex-
amples. Doing this allowed multiple examples to
be assigned to a category more quickly because
one can decide upon a category for the whole lot
at once and then just extract the few, if any, that be-
long to other categories. This is likely to be both
faster and more consistent than examining each
376
Agreement (%) Fleiss? ? Entropy
Iteration A vs B A vs C B vs C A vs B A vs C B vs C All A B C
1 0.60 0.68 0.54 0.53 0.62 0.46 0.54 3.02 2.98 3.24
2 0.64 0.44 0.50 0.59 0.37 0.45 0.47 3.13 3.40 3.63
3 0.66 0.66 0.72 0.57 0.58 0.66 0.60 2.44 2.47 2.70
4 0.64 0.30 0.38 0.57 0.16 0.28 0.34 2.80 3.29 2.87
5 0.72 0.66 0.60 0.67 0.61 0.54 0.61 3.21 3.12 3.36
Table 4: Intermediate results for the possessives refinement work.
Agreement (%) Fleiss? ? Entropy
Portion A vs B A vs C B vs C A vs B A vs C B vs C All A B C
PTB 0.62 0.62 0.54 0.56 0.56 0.46 0.53 3.22 3.17 3.13
HDFRE 0.82 0.78 0.72 0.77 0.71 0.64 0.71 2.73 2.75 2.73
JB 0.74 0.56 0.54 0.70 0.50 0.48 0.56 3.17 3.11 3.17
All 0.73 0.65 0.60 0.69 0.61 0.55 0.62 3.43 3.35 3.51
Table 5: Final possessives annotation agreement figures before revisions.
Agreement (%) Fleiss? ? Entropy
Source A vs B A vs C B vs C A vs B A vs C B vs C All A B C
PTB 0.78 0.74 0.74 0.75 0.70 0.70 0.72 3.30 3.11 3.35
HDFRE 0.78 0.76 0.76 0.74 0.72 0.72 0.73 3.03 2.98 3.17
JB 0.92 0.90 0.86 0.90 0.87 0.82 0.86 2.73 2.71 2.65
All 0.83 0.80 0.79 0.80 0.77 0.76 0.78 3.37 3.30 3.48
Table 6: Final possessives annotation agreement figures after revisions.
First Relation Second Relation Example
PARTITIVE CONTROLLER/... BoA?s Mr. Davis
PARTITIVE LOCATION UK?s cities
PARTITIVE OBJECTIVE BoA?s adviser
PARTITIVE OTHER RELATIONAL NOUN BoA?s chairman
PARTITIVE PRODUCER?S PRODUCT the lamb?s wool
CONTROLLER/... PRODUCER?S PRODUCT the bird?s nest
CONTROLLER/... OBJECTIVE his assistant
CONTROLLER/... LOCATION Libya?s oil company
CONTROLLER/... ATTRIBUTE Joe?s strength
CONTROLLER/... MEMBER?S COLLECTION the colonel?s unit
CONTROLLER/... RECIPIENT Joe?s trophy
RECIPIENT OBJECTIVE Joe?s reward
SUBJECTIVE PRODUCER?S PRODUCT Joe?s announcement
SUBJECTIVE OBJECTIVE its change
SUBJECTIVE CONTROLLER/... Joe?s employee
SUBJECTIVE LOCATION Libya?s devolution
SUBJECTIVE MENTAL EXPERIENCER Joe?s resentment
OBJECTIVE MENTAL EXPERIENCER Joe?s concern
OBJECTIVE LOCATION the town?s inhabitants
KINSHIP OTHER RELATIONAL NOUN his fiancee
Table 7: Ambiguous/multiclass possessive examples.
example in isolation. This advantage did not ex-
ist in the inter-annotator agreement study.
5.4 Feature Ablation Experiments
To evaluate the importance of the different types
of features, the same experiment was re-run multi-
ple times, each time including or excluding exactly
one feature template. Before each variation, the C
parameter was retuned using 5-fold cross valida-
tion on the training data. The results for these runs
are shown in Table 8.
Based upon the leave-one-out and only-one fea-
ture evaluation experiment results, it appears that
the possessee word is more important to classifica-
tion than the possessor word. The possessor word
is still valuable though, with it likely being more
377
valuable for certain categories (e.g., TEMPORAL
and LOCATION) than others (e.g., KINSHIP). Hy-
pernym and gloss term features proved to be about
equally valuable. Curiously, although hypernyms
are commonly used as features in NLP classifica-
tion tasks, gloss terms, which are rarely used for
these tasks, are approximately as useful, at least in
this particular context. This would be an interest-
ing result to examine in greater detail.
6 Related Work
6.1 Linguistics
Semantic relation inventories for the English ?s-
construction have been around for some time; Tay-
lor (1996) mentions a set of 6 relations enumerated
by Poutsma (1914?1916). Curiously, there is not
a single dominant semantic relation inventory for
possessives. A representative example of semantic
relation inventories for ?s-constructions is the one
given by Quirk et al (1985) (presented earlier in
Section 2).
Interestingly, the set of relations expressed by
possessives varies by language. For example,
Classical Greek permits a standard of comparison
relation (e.g., ?better than Plato?) (Nikiforidou,
1991), and, in Japanese, some relations are ex-
pressed in the opposite direction (e.g., ?blue eye?s
doll?) while others are not (e.g., ?Tanaka?s face?)
(Nishiguchi, 2009).
To explain how and why such seemingly dif-
ferent relations as whole+part and cause+effect
are expressed by the same linguistic phenomenon,
Nikiforidou (1991) pursues an approach of
metaphorical structuring in line with the work
of Lakoff and Johnson (1980) and Lakoff (1987).
She thus proposes a variety of such metaphors as
THINGS THAT HAPPEN (TO US) ARE (OUR) POS-
SESSIONS and CAUSES ARE ORIGINS to explain
how the different relations expressed by posses-
sives extend from one another.
Certainly, not all, or even most, of the linguis-
tics literature on English possessives focuses on
creating lists of semantic relations. Much of the
work covering the semantics of the ?s construc-
tion in English, such as Barker?s (1995) work,
dwells on the split between cases of relational
nouns, such as sister, that, by their very definition,
hold a specific relation to other real or conceptual
things, and non-relational, or sortal nouns (L?b-
ner, 1985), such as car.
Vikner and Jensen?s (2002) approach for han-
dling these disparate cases is based upon Puste-
jovsky?s (1995) generative lexicon framework.
They coerce sortal nouns (e.g., car) into being
relational, purporting to create a uniform analy-
sis. They split lexical possession into four types:
inherent, part-whole, agentive, and control, with
agentive and control encompassing many, if not
most, of the cases involving sortal nouns.
A variety of other issues related to possessives
considered by the linguistics literature include ad-
jectival modifiers that significantly alter interpre-
tation (e.g., favorite and former), double geni-
tives (e.g., book of John?s), bare possessives (i.e.,
cases where the possessee is omitted, as in ?Eat
at Joe?s?), possessive compounds (e.g., driver?s
license), the syntactic structure of possessives,
definitiveness, changes over the course of his-
tory, and differences between languages in terms
of which relations may be expressed by the geni-
tive. Representative work includes that by Barker
(1995), Taylor (1996), Heine (1997), Partee and
Borschev (1998), Rosenbach (2002), and Vikner
and Jensen (2002).
6.2 Computational Linguistics
Though the relation between nominals in the
English possessive construction has received lit-
tle attention from the NLP community, there is
a large body of work that focuses on similar
problems involving noun-noun relation interpreta-
tion/paraphrasing, including interpreting the rela-
tions between the components of noun compounds
(Butnariu et al, 2010), disambiguating preposition
senses (Litkowski and Hargraves, 2007), or anno-
tating the relation between nominals in more arbi-
trary constructions within the same sentence (Hen-
drickx et al, 2009).
Whereas some of these lines of work use fixed
inventories of semantic relations (Lauer, 1995;
Nastase and Szpakowicz, 2003; Kim and Bald-
win, 2005; Girju, 2009; ? S?aghdha and Copes-
take, 2009; Tratz and Hovy, 2010), other work al-
lows for a nearly infinite number of interpretations
(Butnariu and Veale, 2008; Nakov, 2008). Recent
SemEval tasks (Butnariu et al, 2009; Hendrickx et
al., 2013) pursue this more open-ended strategy. In
these tasks, participating systems recover the im-
plicit predicate between the nouns in noun com-
pounds by creating potentially unique paraphrases
for each example. For instance, a system might
generate the paraphrase made of for the noun com-
378
Feature Type Word(s) Results
L R C G B N LOO OO
Gloss Terms  0.867 (0.04) 0.762 (0.08)
Hypernyms  0.870 (0.04) 0.760 (0.16)
Synonyms  0.873 (0.04) 0.757 (0.32)
Word Itself  0.871 (0.04) 0.745 (0.08)
Lexnames  0.871 (0.04) 0.514 (0.32)
Last Letters  0.870 (0.04) 0.495 (0.64)
Lexnames  0.872 (0.04) 0.424 (0.08)
Link types  0.874 (0.02) 0.398 (0.64)
Link types  0.870 (0.04) 0.338 (0.32)
Word Itself  0.870 (0.04) 0.316 (0.16)
Last Letters  0.872 (0.02) 0.303 (0.16)
Gloss Terms  0.872 (0.02) 0.271 (0.04)
Hypernyms  0.875 (0.02) 0.269 (0.08)
Word Itself  0.874 (0.02) 0.261 (0.08)
Synonyms  0.874 (0.02) 0.260 (0.04)
Lexnames  0.874 (0.02) 0.247 (0.04)
Part-of-speech List  0.873 (0.02) 0.245 (0.16)
Part-of-speech List  0.874 (0.02) 0.243 (0.16)
Dependency  0.872 (0.02) 0.241 (0.16)
Part-of-speech List  0.874 (0.02) 0.236 (0.32)
Link Types  0.874 (0.02) 0.236 (0.64)
Word Itself  0.870 (0.02) 0.234 (0.32)
Assigned Part-of-Speech  0.874 (0.02) 0.228 (0.08)
Affixes  0.873 (0.02) 0.227 (0.16)
Assigned Part-of-Speech  0.873 (0.02) 0.194 (0.16)
Hypernyms  0.873 (0.02) 0.186 (0.04)
Lexnames  0.870 (0.04) 0.170 (0.64)
Text of Dependents  0.874 (0.02) 0.156 (0.08)
Parts List  0.873 (0.02) 0.141 (0.16)
Affixes  0.870 (0.04) 0.114 (0.32)
Affixes  0.873 (0.02) 0.105 (0.04)
Parts List  0.874 (0.02) 0.103 (0.16)
Table 8: Results for leave-one-out and only-one feature template ablation experiment results for all
feature templates sorted by the only-one case. L, R, C, G, B, and N stand for left word (possessor), right
word (possessee), pairwise combination of outputs for possessor and possessee, syntactic governor of
possessee, all tokens between possessor and possessee, and the word next to the possessee (on the right),
respectively. The C parameter value used to train the SVMs is shown in parentheses.
pound pepperoni pizza. Computer-generated re-
sults are scored against a list of human-generated
options in order to rank the participating systems.
This approach could be applied to possessives in-
terpretation as well.
Concurrent with the lack of NLP research on
the subject is the absence of available annotated
datasets for training, evaluation, and analysis. The
NomBank project (Meyers et al, 2004) provides
coarse annotations for some of the possessive con-
structions in the Penn Treebank, but only those
that meet their criteria.
7 Conclusion
In this paper, we present a semantic relation in-
ventory for ?s possessives consisting of 17 rela-
tions expressed by the English ?s construction, the
largest available manually-annotated collection of
possessives, and an effective method for automat-
ically assigning the relations to unseen examples.
We explain our methodology for building this in-
ventory and dataset and report a strong level of
inter-annotator agreement, reaching 0.78 Kappa
overall. The resulting dataset is quite large, at
21,938 instances, and crosses multiple domains,
including news, fiction, and historical non-fiction.
It is the only large fully-annotated publicly-
available collection of possessive examples that
we are aware of. The straightforward SVM-
based automatic classification system achieves
87.4% accuracy?the highest automatic posses-
sive interpretation accuracy figured reported to
date. These high results suggest that SVMs are
a good choice for automatic possessive interpre-
379
tation systems, in contrast to Moldovan and Bad-
ulescu (2005) findings. The data and software
presented in this paper are available for down-
load at http://www.isi.edu/publications/licensed-
sw/fanseparser/index.html.
8 Future Work
Going forward, we would like to examine the var-
ious ambiguities of possessives described in Sec-
tion 4.3. Instead of trying to find the one-best
interpretation for a given possessive example, we
would like to produce a list of all appropriate in-
tepretations.
Another avenue for future research is to study
variation in possessive use across genres, includ-
ing scientific and technical genres. Similarly, one
could automatically process large volumes of text
from various time periods to investigate changes
in the use of the possessive over time.
Acknowledgments
We would like to thank Charles Zheng and Sarah
Benzel for all their annotation work and valuable
feedback.
References
Adriana Badulescu and Dan Moldovan. 2009. A Se-
mantic Scattering Model for the Automatic Interpre-
tation of English Genitives. Natural Language En-
gineering, 15:215?239.
Chris Barker. 1995. Possessive Descriptions. CSLI
Publications, Stanford, CA, USA.
Cristina Butnariu and Tony Veale. 2008. A Concept-
Centered Approach to Noun-Compound Interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics, pages 81?88.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid ? S?aghdha, Stan Szpakowicz, and Tony
Veale. 2009. SemEval-2010 Task 9: The Inter-
pretation of Noun Compounds Using Paraphrasing
Verbs and Prepositions. In DEW ?09: Proceedings
of the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions, pages 100?
105.
Cristina Butnariu, Su Nam Kim, Preslav Nakov, Di-
armuid. ? S?aghdha, Stan Szpakowicz, and Tony
Veale. 2010. SemEval-2010 Task 9: The Interpreta-
tion of Noun Compounds Using Paraphrasing Verbs
and Prepositions. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
39?44.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37?46.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press, Cambridge, MA.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378?382.
Edward Gibbon. 1776. The History of the Decline
and Fall of the Roman Empire, volume I of The His-
tory of the Decline and Fall of the Roman Empire.
Printed for W. Strahan and T. Cadell.
Roxanna Girju. 2009. The Syntax and Seman-
tics of Prepositions in the Task of Automatic In-
terpretation of Nominal Phrases and Compounds:
A Cross-linguistic Study. Computational Linguis-
tics - Special Issue on Prepositions in Application,
35(2):185?228.
Bernd Heine. 1997. Possession: Cognitive Sources,
Forces, and Grammaticalization. Cambridge Uni-
versity Press, United Kingdom.
Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,
Preslav Nakov, Diarmuid ? S?aghdha, Sebastian
Pad?, Marco Pennacchiotti, Lorenza Romano, and
Stan Szpakowicz. 2009. Semeval-2010 task
8: Multi-Way Classification of Semantic Relations
between Pairs of Nominals. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 94?99.
Iris Hendrickx, Zornitsa Kozareva, Preslav Nakov,
Diarmuid ? S?aghdha, Stan Szpakowicz, and
Tony Veale. 2013. Task Description: SemEval-
2013 Task 4: Free Paraphrases of Noun Com-
pounds. http://www.cs.york.ac.uk/
semeval-2013/task4/. [Online; accessed
1-May-2013].
Su Nam Kim and Timothy Baldwin. 2005. Automatic
Interpretation of Noun Compounds using Word-
Net::Similarity. Natural Language Processing?
IJCNLP 2005, pages 945?956.
Rudyard Kipling. 1894. The Jungle Book. Macmillan,
London, UK.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live by. The University of Chicago Press,
Chicago, USA.
George Lakoff. 1987. Women, Fire, and Dangerous
Things: What Categories Reveal about the Mind.
The University of Chicago Press, Chicago, USA.
Mark Lauer. 1995. Corpus Statistics Meet the Noun
Compound: Some Empirical Results. In Proceed-
ings of the 33rd Annual Meeting on Association for
Computational Linguistics, pages 47?54.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of
Prepositions. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, pages
24?29.
380
Sebastian L?bner. 1985. Definites. Journal of Seman-
tics, 4(4):279.
Mitchell P. Marcus, Mary A. Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):330.
Adam Meyers, Ruth Reeves, Catherine Macleod,
Rachel Szekely, Veronika Zielinska, Brian Young,
and Ralph Grishman. 2004. The NomBank
Project: An Interim Report. In Proceedings of the
NAACL/HLT Workshop on Frontiers in Corpus An-
notation.
Dan Moldovan and Adriana Badulescu. 2005. A Se-
mantic Scattering Model for the Automatic Interpre-
tation of Genitives. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 891?898.
Preslav Nakov. 2008. Noun Compound Interpreta-
tion Using Paraphrasing Verbs: Feasibility Study. In
Proceedings of the 13th International Conference on
Artificial Intelligence: Methodology, Systems, and
Applications, pages 103?117.
Vivi Nastase and Stan Szpakowicz. 2003. Explor-
ing Noun-Modifier Semantic Relations. In Fifth In-
ternational Workshop on Computational Semantics
(IWCS-5), pages 285?301.
Kiki Nikiforidou. 1991. The Meanings of the Gen-
itive: A Case Study in the Semantic Structure and
Semantic Change. Cognitive Linguistics, 2(2):149?
206.
Sumiyo Nishiguchi. 2009. Qualia-Based Lexical
Knowledge for the Disambiguation of the Japanese
Postposition No. In Proceedings of the Eighth Inter-
national Conference on Computational Semantics.
Diarmuid ? S?aghdha and Ann Copestake. 2009. Us-
ing Lexical and Relational Similarity to Classify Se-
mantic Relations. In Proceedings of the 12th Con-
ference of the European Chapter of the Association
for Computational Linguistics, pages 621?629.
Barbara H. Partee and Vladimir Borschev. 1998. In-
tegrating Lexical and Formal Semantics: Genitives,
Relational Nouns, and Type-Shifting. In Proceed-
ings of the Second Tbilisi Symposium on Language,
Logic, and Computation, pages 229?241.
James Pustejovsky. 1995. The Generative Lexicon.
MIT Press, Cambridge, MA, USA.
Randolph Quirk, Sidney Greenbaum, Geoffrey Leech,
and Jan Svartvik. 1985. A Comprehensive Gram-
mar of the English Language. Longman Inc., New
York.
Anette Rosenbach. 2002. Genitive Variation in En-
glish: Conceptual Factors in Synchronic and Di-
achronic Studies. Topics in English linguistics.
Mouton de Gruyter.
Sidney Siegel and N. John Castellan. 1988. Non-
parametric statistics for the behavioral sciences.
McGraw-Hill.
John R. Taylor. 1996. Possessives in English. Oxford
University Press, New York.
Stephen Tratz and Eduard Hovy. 2010. A Taxonomy,
Dataset, and Classifier for Automatic Noun Com-
pound Interpretation. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 678?687.
Stephen Tratz and Eduard Hovy. 2011. A Fast, Accu-
rate, Non-Projective, Semantically-Enriched Parser.
In Proceedings of the 2011 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1257?1268.
Carl Vikner and Per Anker Jensen. 2002. A Seman-
tic Analysis of the English Genitive. Interation of
Lexical and Formal Semantics. Studia Linguistica,
56(2):191?226.
381
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467?473,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model for Event Co-reference
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel ap-
proach to modelling distributional seman-
tics that represents meaning as distribu-
tions over relations in syntactic neighbor-
hoods. We argue that our model approxi-
mates meaning in compositional configu-
rations more effectively than standard dis-
tributional vectors or bag-of-words mod-
els. We test our hypothesis on the problem
of judging event coreferentiality, which in-
volves compositional interactions in the
predicate-argument structure of sentences,
and demonstrate that our model outper-
forms both state-of-the-art window-based
word embeddings as well as simple ap-
proaches to compositional semantics pre-
viously employed in the literature.
1 Introduction
Distributional Semantic Models (DSM) are popu-
lar in computational semantics. DSMs are based
on the hypothesis that the meaning of a word or
phrase can be effectively captured by the distribu-
tion of words in its neighborhood. They have been
successfully used in a variety of NLP tasks includ-
ing information retrieval (Manning et al, 2008),
question answering (Tellex et al, 2003), word-
sense discrimination (Sch?tze, 1998) and disam-
biguation (McCarthy et al, 2004), semantic sim-
ilarity computation (Wong and Raghavan, 1984;
McCarthy and Carroll, 2003) and selectional pref-
erence modeling (Erk, 2007).
A shortcoming of DSMs is that they ignore the
syntax within the context, thereby reducing the
distribution to a bag of words. Composing the
?*Equally contributing authors
distributions for ?Lincoln?, ?Booth?, and ?killed?
gives the same result regardless of whether the in-
put is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. But as suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately yields greater expressive power. Thus, to
remedy the bag-of-words failing, we extend the
generic DSM model to several relation-specific
distributions over syntactic neighborhoods. In
other words, one can think of the Structured DSM
(SDSM) representation of a word/phrase as sev-
eral vectors defined over the same vocabulary,
each vector representing the word?s selectional
preferences for its various syntactic arguments.
We argue that this representation not only cap-
tures individual word semantics more effectively
than the standard DSM, but is also better able to
express the semantics of compositional units. We
prove this on the task of judging event coreference.
Experimental results indicate that our model
achieves greater predictive accuracy on the task
than models that employ weaker forms of compo-
sition, as well as a baseline that relies on state-
of-the-art window based word embeddings. This
suggests that our formalism holds the potential of
greater expressive power in problems that involve
underlying semantic compositionality.
2 Related Work
Next, we relate and contrast our work to prior re-
search in the fields of Distributional Vector Space
Models, Semantic Compositionality and Event
Co-reference Resolution.
2.1 DSMs and Compositionality
The underlying idea that ?a word is characterized
by the company it keeps? was expressed by Firth
467
(1957). Several works have defined approaches to
modelling context-word distributions anchored on
a target word, topic, or sentence position. Collec-
tively these approaches are called Distributional
Semantic Models (DSMs).
While DSMs have been very successful on a va-
riety of tasks, they are not an effective model of
semantics as they lack properties such as compo-
sitionality or the ability to handle operators such
as negation. In order to model a stronger form of
semantics, there has been a recent surge in stud-
ies that phrase the problem of DSM composition-
ality as one of vector composition. These tech-
niques derive the meaning of the combination of
two words a and b by a single vector c = f(a, b).
Mitchell and Lapata (2008) propose a framework
to define the composition c = f(a, b, r,K) where
r is the relation between a and b, and K is the
additional knowledge used to define composition.
While this framework is quite general, the actual
models considered in the literature tend to disre-
gard K and r and mostly perform component-wise
addition and multiplication, with slight variations,
of the two vectors. To the best of our knowledge
the formulation of composition we propose is the
first to account for both K and r within this com-
positional framework.
Dinu and Lapata (2010) and S?aghdha and Ko-
rhonen (2011) introduced a probabilistic model
to represent word meanings by a latent variable
model. Subsequently, other high-dimensional ex-
tensions by Rudolph and Giesbrecht (2010), Ba-
roni and Zamparelli (2010) and Grefenstette et
al. (2011), regression models by Guevara (2010),
and recursive neural network based solutions by
Socher et al (2012) and Collobert et al (2011)
have been proposed. However, these models do
not efficiently account for structure.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempt to include syntactic context in distribu-
tional models. A quasi-compositional approach
was attempted in Thater et al (2010) by a com-
bination of first and second order context vectors.
But they do not explicitly construct phrase-level
meaning from words which limits their applicabil-
ity to real world problems. Furthermore, we also
include structure into our method of composition.
Prior work in structure aware methods to the best
of our knowledge are (Weisman et al, 2012) and
(Baroni and Lenci, 2010). However, these meth-
ods do not explicitly model composition.
2.2 Event Co-reference Resolution
While automated resolution of entity coreference
has been an actively researched area (Haghighi
and Klein, 2009; Stoyanov et al, 2009; Raghu-
nathan et al, 2010), there has been relatively lit-
tle work on event coreference resolution. Lee
et al (2012) perform joint cross-document entity
and event coreference resolution using the two-
way feedback between events and their arguments.
We, on the other hand, attempt a slightly different
problem of making co-referentiality judgements
on event-coreference candidate pairs.
3 Structured Distributional Semantics
In this paper, we propose an approach to incorpo-
rate structure into distributional semantics (more
details in Goyal et al (2013)). The word distribu-
tions drawn from the context defined by a set of
relations anchored on the target word (or phrase)
form a set of vectors, namely a matrix for the tar-
get word. One axis of the matrix runs over all
the relations and the other axis is over the distri-
butional word vocabulary. The cells store word
counts (or PMI scores, or other measures of word
association). Note that collapsing the rows of the
matrix provides the standard dependency based
distributional representation.
3.1 Building Representation: The PropStore
To build a lexicon of SDSM matrices for a given
vocabulary we first construct a proposition knowl-
edge base (the PropStore) created by parsing the
Simple English Wikipedia. Dependency arcs are
stored as 3-tuples of the form ?w1, r, w2?, denot-
ing an occurrence of words w1, word w2 related
by r. We also store sentence indices for triples
as this allows us to achieve an intuitive technique
to achieve compositionality. In addition to the
words? surface-forms, the PropStore also stores
their POS tags, lemmas, and Wordnet supersenses.
This helps to generalize our representation when
surface-form distributions are sparse.
The PropStore can be used to query for the ex-
pectations of words, supersenses, relations, etc.,
around a given word. In the example in Figure 1,
the query (SST(W1) = verb.consumption, ?, dobj)
i.e. ?what is consumed? might return expectations
[pasta:1, spaghetti:1, mice:1 . . . ]. Relations and
POS tags are obtained using a dependency parser
Tratz and Hovy (2011), supersense tags using sst-
light Ciaramita and Altun (2006), and lemmas us-
468
Figure 1: Sample sentences & triples
ing Wordnet Fellbaum (1998).
3.2 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix symbol-
ism in a bottom-up fashion using the PropStore.
The combination hinges on the intuition that when
lexical units combine to form a larger syntactically
connected phrase, the representation of the phrase
is given by its own distributional neighborhood
within the embedded parse tree. The distributional
neighborhood of the net phrase can be computed
using the PropStore given syntactic relations an-
chored on its parts. For the example in Figure
1, we can compose SST(w1) = Noun.person and
Lemma(W1) = eat appearing together with a nsubj
relation to obtain expectations around ?people eat?
yielding [pasta:1, spaghetti:1 . . . ] for the object
relation, [room:2, restaurant:1 . . .] for the location
relation, etc. Larger phrasal queries can be built to
answer queries like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
us to account for both relation r and knowledge K
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition
of two words is given in Algorithm 1, which
returns the distributional expectation around the
composed unit. Note that the entire algorithm can
conveniently be written in the form of database
queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1) (1)
M2 ? queryMatrix(w2) (2)
SentIDs?M1(r) ?M2(r) (3)
return ((M1? SentIDs) ? (M2? SentIDs)) (4)
For the example ?noun.person nsubj eat?, steps
(1) and (2) involve querying the PropStore for the
individual tokens, noun.person and eat. Let the re-
sulting matrices be M1 and M2, respectively. In
step (3), SentIDs (sentences where the two words
appear with the specified relation) are obtained by
taking the intersection between the nsubj compo-
nent vectors of the two matrices M1 and M2. In
step (4), the entries of the original matrices M1
and M2 are intersected with this list of common
SentIDs. Finally, the resulting matrix for the com-
position of the two words is simply the union of
all the relationwise intersected sentence IDs. Intu-
itively, through this procedure, we have computed
the expectation around the words w1 and w2 when
they are connected by the relation ?r?.
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts (described in Algorithm 2).
Let the E = {e1 . . . en} be the set of edges in T ,
ei = (wi1, ri, wi2)?i = 1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
The phrase representations becomes sparser as
phrase length increases. For this study, we restrict
phrasal query length to a maximum of three words.
3.3 Event Coreferentiality
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
469
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ. It may be
noted that this task is different from the task of full
event coreference and hence is not directly compa-
rable to previous experimental results in the liter-
ature. Two mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
In our corpus, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for
the role. It may be noted that the SDSM repre-
sentation relies on syntactic dependancy relations.
Hence, to bridge the gap between these relations
and the composition of semantic role participants
of event mentions we empirically determine those
syntactic relations which most strongly co-occur
with the semantic relations connecting events,
agents and patients. The triple (e, a, p) is thus the
composition of the triples (a, relationsetagent, e)
and (p, relationsetpatient, e), and hence a com-
plex object. To determine equality of this complex
composed representation we generate three levels
of progressively simplified event constituents for
comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p)
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance
metric (Euclidean, Cityblock, Cosine), and
score normalization techniques (Row-wise, Full,
Column-collapsed). This results in 159 similarity-
based features for every pair of events, which are
used to train a classifier to decide conference.
4 Experiments
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
4.1 Datasets
IC Event Coreference Corpus: The dataset
(Hovy et al, 2013), drawn from 100 news articles
about violent events, contains manually created
annotations for 2214 pairs of co-referent and non-
coreferent events each. Where available, events?
semantic role-fillers for agent and patient are an-
notated as well. When missing, empirical substi-
tutes were obtained by querying the PropStore for
the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
4.2 Baselines
To establish the efficacy of our model, we compare
SDSM against a purely window-based baseline
(DSM) trained on the same corpus. In our exper-
iments we set a window size of seven words. We
also compare SDSM against the window-based
embeddings trained using a recursive neural net-
work (SENNA) (Collobert et al, 2011) on both
datsets. SENNA embeddings are state-of-the-art
for many NLP tasks. The second baseline uses
470
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 1: Cross-validation Performance on IC and ECB dataset
SENNA to generate level 3 similarity features for
events? individual words (agent, patient and ac-
tion). As our final set of baselines, we extend two
simple techniques proposed by (Mitchell and Lap-
ata, 2008) that use element-wise addition and mul-
tiplication operators to perform composition. We
extend it to our matrix representation and build
two baselines AVC (element-wise addition) and
MVC (element-wise multiplication).
4.3 Discussion
Among common classifiers, decision-trees (J48)
yielded best results in our experiments. Table 1
summarizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of compositionality than
simple additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
event coreferentiality. The forward selection pro-
cedure reveals that the most informative attributes
are the level 2 compositional features involving
the agent and the action, as well as their individ-
ual level 3 features. This corresponds to the in-
tuition that the agent and the action are the prin-
cipal determiners for identifying events. Features
involving the patient and level 1 features are least
useful. This is probably because features involv-
ing full composition are sparse, and not as likely
to provide statistically significant evidence. This
may change as our PropStore grows in size.
5 Conclusion and Future Work
We outlined an approach that introduces structure
into distributed semantic representations gives us
an ability to compare the identity of two repre-
sentations derived from supposedly semantically
identical phrases with different surface realiza-
tions. We employed the task of event coreference
to validate our representation and achieved sig-
nificantly higher predictive accuracy than several
baselines.
In the future, we would like to extend our model
to other semantic tasks such as paraphrase detec-
tion, lexical substitution and recognizing textual
entailment. We would also like to replace our syn-
tactic relations to semantic relations and explore
various ways of dimensionality reduction to solve
this problem.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
References
Marco Baroni and Alessandro Lenci. 2010. Distri-
butional memory: A general framework for corpus-
based semantics. Comput. Linguist., 36(4):673?721,
December.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
471
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Bradford Books.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Kartik. Goyal, Sujay Kumar Jauhar, Mrinmaya Sachan,
Shashank Srivastava, Huiying Li, and Eduard Hovy.
2013. A structured distributional semantic model
: Integrating structure with semantics. In Proceed-
ings of the 1st Continuous Vector Space Models and
their Compositionality Workshop at the conference
of ACL 2013.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
E.H. Hovy, T. Mitamura, M.F. Verdejo, J. Araki, and
A. Philpot. 2013. Events are not simple: Iden-
tity, non-identity, and quasi-identity. In Proceedings
of the 1st Events Workshop at the conference of the
HLT-NAACL 2013.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, pages 236?244.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
472
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido
Dagan. 2012. Learning verb inference rules from
linguistically-motivated evidence. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 194?204, Stroudsburg, PA, USA. Association
for Computational Linguistics.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
473
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 165?174,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Weakly Supervised User Profile Extraction from Twitter
Jiwei Li
1
, Alan Ritter
2
, Eduard Hovy
1
1
Language Technology Institute,
2
Machine Learning Department
Carnegie Mellon University, Pittsburgh, PA 15213, USA
bdlijiwei@gmail.com, rittera@cs.cmu.edu, ehovy@andrew.cmu.edu
Abstract
While user attribute extraction on social
media has received considerable attention,
existing approaches, mostly supervised,
encounter great difficulty in obtaining gold
standard data and are therefore limited
to predicting unary predicates (e.g., gen-
der). In this paper, we present a weakly-
supervised approach to user profile extrac-
tion from Twitter. Users? profiles from so-
cial media websites such as Facebook or
Google Plus are used as a distant source
of supervision for extraction of their at-
tributes from user-generated text. In addi-
tion to traditional linguistic features used
in distant supervision for information ex-
traction, our approach also takes into ac-
count network information, a unique op-
portunity offered by social media. We test
our algorithm on three attribute domains:
spouse, education and job; experimental
results demonstrate our approach is able
to make accurate predictions for users? at-
tributes based on their tweets.
1
1 Introduction
The overwhelming popularity of online social me-
dia creates an opportunity to display given as-
pects of oneself. Users? profile information in
social networking websites such as Facebook
2
or
Google Plus
3
provides a rich repository personal
information in a structured data format, making it
amenable to automatic processing. This includes,
for example, users? jobs and education, and pro-
vides a useful source of information for applica-
tions such as search
4
, friend recommendation, on-
1
Both code and data are available at http://aclweb.
org/aclwiki/index.php?title=Profile_data
2
https://www.facebook.com/
3
https://plus.google.com/
4
https://www.facebook.com/about/
graphsearch
@[shanenicholson] has taken all the kids today so
I can go shopping-CHILD FREE! #iloveyoushano
#iloveyoucreditcard
Tamworth promo day with my handsome classy husband
@[shanenicholson]
Spouse: shanenicholson
I got accepted to be part of the UofM engineering safety
pilot program in [FSU]
Here in class. (@ [Florida State University] - Williams
Building)
Don?t worry , guys ! Our beloved [FSU] will always con-
tinue to rise ? to the top !
Education: Florida State University (FSU)
first day of work at [HuffPo], a sports bar woo come visit
me yo..
start to think we should just add a couple desks to the
[HuffPo] newsroom for Business Insider writers
just back from [HuffPo], what a hell !
Job: HuffPo
Table 1: Examples of Twitter message clues for
user profile inference.
line advertising, computational social science and
more.
Although profiles exist in an easy-to-use, struc-
tured data format, they are often sparsely popu-
lated; users rarely fully complete their online pro-
files. Additionally, some social networking ser-
vices such as Twitter don?t support this type of
structured profile data. It is therefore difficult to
obtain a reasonably comprehensive profile of a
user, or a reasonably complete facet of information
(say, education level) for a class of users. While
many users do not explicitly list all their personal
information in their online profile, their user gen-
erated content often contains strong evidence to
suggest many types of user attributes, for example
education, spouse, and employment (See Table 1).
Can one use such information to infer more de-
tails? In particular, can one exploit indirect clues
from an unstructured data source like Twitter to
obtain rich, structured user profiles?
In this paper we demonstrate that it is feasi-
ble to automatically extract Facebook-style pro-
165
files directly from users? tweets, thus making
user profile data available in a structured format
for upstream applications. We view user profile
inference as a structured prediction task where
both text and network information are incorpo-
rated. Concretely, we cast user profile predic-
tion as binary relation extraction (Brin, 1999),
e.g., SPOUSE(User
i
, User
j
), EDUCATION(User
i
,
Entity
j
) and EMPLOYER(User
i
, Entity
j
). Inspired
by the concept of distant supervision, we collect
training tweets by matching attribute ground truth
from an outside ?knowledge base? such as Face-
book or Google Plus.
One contribution of the work presented here is
the creation of the first large-scale dataset on three
general Twitter user profile domains (i.e., EDUCA-
TION, JOB, SPOUSE). Experiments demonstrate
that by simultaneously harnessing both text fea-
tures and network information, our approach is
able to make accurate user profile predictions. We
are optimistic that our approach can easily be ap-
plied to further user attributes such as HOBBIES
and INTERESTS (MOVIES, BOOKS, SPORTS or
STARS), RELIGION, HOMETOWN, LIVING LOCA-
TION, FAMILY MEMBERS and so on, where train-
ing data can be obtained by matching ground truth
retrieved from multiple types of online social me-
dia such as Facebook, Google Plus, or LinkedIn.
Our contributions are as follows:
? We cast user profile prediction as an informa-
tion extraction task.
? We present a large-scale dataset for this task
gathered from various structured and unstruc-
tured social media sources.
? We demonstrate the benefit of jointly rea-
soning about users? social network structure
when extracting their profiles from text.
? We experimentally demonstrate the effective-
ness of our approach on 3 relations: SPOUSE,
JOB and EDUCATION.
The remainder of this paper is organized as fol-
lows: We summarize related work in Section 2.
The creation of our dataset is described in Section
3. The details of our model are presented in Sec-
tion 4. We present experimental results in Section
5 and conclude in Section 6.
2 Related Work
While user profile inference from social media has
received considerable attention (Al Zamal et al,
2012; Rao and Yarowsky, 2010; Rao et al, 2010;
Rao et al, 2011), most previous work has treated
this as a classification task where the goal is to pre-
dict unary predicates describing attributes of the
user. Examples include gender (Ciot et al, 2013;
Liu and Ruths, 2013; Liu et al, 2012), age (Rao et
al., 2010), or political polarity (Pennacchiotti and
Popescu, 2011; Conover et al, 2011).
A significant challenge that has limited previous
efforts in this area is the lack of available training
data. For example, researchers obtain training data
by employing workers from Amazon Mechanical
Turk to manually identify users? gender from pro-
file pictures (Ciot et al, 2013). This approach is
appropriate for attributes such as gender with a
small numbers of possible values (e.g., male or fe-
male), for which the values can be directly iden-
tified. However for attributes such as spouse or
education there are many possible values, making
it impossible to manually search for gold standard
answers within a large number of tweets which
may or may not contain sufficient evidence.
Also related is the Twitter user timeline extrac-
tion algorithm of Li and Cardie (2013). This work
is not focused on user attribute extraction, how-
ever.
Distant Supervision Distant supervision, also
known as weak supervision, is a method for learn-
ing to extract relations from text using ground
truth from an existing database as a source of
supervision. Rather than relying on mention-
level annotations, which are expensive and time
consuming to generate, distant supervision lever-
ages readily available structured data sources as
a weak source of supervision for relation ex-
traction from related text corpora (Craven et
al., 1999). For example, suppose r(e
1
, e
2
) =
IsIn(Paris, France) is a ground tuple in the
database and s =?Paris is the capital of France?
contains synonyms for both ?Paris? and ?France?,
then we assume that s may express the fact
r(e
1
, e
2
) in some way and can be used as pos-
itive training examples. In addition to the wide
use in text entity relation extraction (Mintz et al,
2009; Ritter et al, 2013; Hoffmann et al, 2011;
Surdeanu et al, 2012; Takamatsu et al, 2012),
distant supervision has been applied to multiple
166
Figure 1: Illustration of Goolge Plus ?knowledge
base?.
fields such as protein relation extraction (Craven
et al, 1999; Ravikumar et al, 2012), event extrac-
tion from Twitter (Benson et al, 2011), sentiment
analysis (Go et al, 2009) and Wikipedia infobox
generation (Wu and Weld, 2007).
Homophily Online social media offers a rich
source of network information. McPherson et
al. (2001) discovered that people sharing more
attributes such as background or hobby have
a higher chance of becoming friends in social
media. This property, known as HOMOPHILY
(summarized by the proverb ?birds of a feather
flock together?) (Al Zamal et al, 2012) has been
widely applied to community detection (Yang and
Leskovec, 2013) and friend recommendation (Guy
et al, 2010) on social media. In the user attribute
extraction literature, researchers have considered
neighborhood context to boost inference accuracy
(Pennacchiotti and Popescu, 2011; Al Zamal et al,
2012), where information about the degree of their
connectivity to their pre-labeled users is included
in the feature vectors. A related algorithm by Mis-
love et al (2010) crawled Facebook profiles of
4,000 Rice University students and alumni and in-
ferred attributes such as major and year of ma-
triculation purely based on network information.
Mislove?s work does not consider the users? text
stream, however. As we demonstrate below, rely-
ing solely on network information is not enough to
enable inference about attributes.
3 Dataset Creation
We now describe the generation of our distantly
supervised training dataset in detail. We make
use of Google Plus and Freebase to obtain ground
facts and extract positive/negative bags of post-
ings from users? twitter streams according to the
ground facts.
Figure 2: Example of fetching tweets containing
entity USC mention from Miranda Cosgrove (an
American actress and singer-songwriter)?s twitter
stream.
Education/Job We first used the Google Plus
API
5
(shown in Figure 1) to obtain a seed set
of users whose profiles contain both their educa-
tion/job status and a link to their twitter account.
6
Then, we fetched tweets containing the mention of
the education/job entity from each correspondent
user?s twitter stream using Twitter?s search API
7
(shown in Figure 2) and used them to construct
positive bags of tweets expressing the associated
attribute, namely EDUCATION(User
i
, Entity
j
), or
EMPLOYER(User
i
, Entity
j
). The Freebase API
8
is employed for alias recognition, to match terms
such as ?Harvard University?, ?Harvard?, ?Har-
vard U? to a single The remainder of each corre-
sponding user?s entire Twitter feed is used as neg-
ative training data.
9
We expanded our dataset from the seed users
according to network information provided by
Google Plus and Twitter. Concretely, we crawled
circle information of users in the seed set from
both their Twitter and Google Plus accounts and
performed a matching to pick out shared users
between one?s Twitter follower list and Google
Plus Circle. This process assures friend identity
and avoids the problem of name ambiguity when
matching accounts across websites. Among candi-
date users, those who explicitly display Job or Ed-
ucation information on Google Plus are preserved.
We then gathered positive and negative data as de-
scribed above.
Dataset statistics are presented in Table 2. Our
5
https://developers.google.com/+/api/
6
An unambiguous twitter account link is needed here be-
cause of the common phenomenon of name duplication.
7
https://twitter.com/search
8
http://wiki.freebase.com/wiki/
Freebase_API
9
Due to Twitter user timeline limit, we crawled at most
3200 tweets for each user.
167
education dataset contains 7,208 users, 6,295 of
which are connected to others in the network. The
positive training set for the EDUCATION is com-
prised of 134,060 tweets.
Spouse Facebook is the only type of social me-
dia where spouse information is commonly dis-
played. However, only a tiny amount of individ-
ual information is publicly accessible from Face-
book Graph API
10
. To obtain ground truth for the
spouse relation at large scale, we turned to Free-
base
11
, a large, open-domain database, and gath-
ered instances of the /PEOPLE/PERSON/SPOUSE
relation. Positive/negative training tweets are ob-
tained in the same way as was previously de-
scribed for EDUCATION and JOB. It is worth
noting that our Spouse dataset is not perfect, as
individuals retrieved from Freebase are mostly
celebrities, and thus it?s not clear whether this
group of people are representative of the general
population.
SPOUSE is an exception to the ?ho-
mophily? effect. But it exhibits another
unique property, known as, REFLEXIVITY: fact
IsSpouseOf(e
1
, e
2
) and IsSpouseOf(e
2
, e
1
)
will hold or not hold at the same time. Given train-
ing data expressing the tuple IsSpouseOf(e
1
, e
2
)
from user e
1
?s twitter stream, we also gather user
e
2
?s tweet collection, and fetch tweets with the
mention of e
1
. We augment negative training
data from e
2
as in the case of Education and Job.
Our Spouse dataset contains 1,636 users, where
there are 554 couples (1108 users). Note that
the number of positive entities (3,121) is greater
than the number of users as (1) one user can have
multiple spouses at different periods of time (2)
multiple entities may point to the same individual,
e.g., BarackObama, Barack Obama and Barack.
4 Model
We now describe our approach to predicting user
profile attributes.
4.1 Notation
Message X: Each user i ? [1, I] is associ-
ated with his Twitter ID and his tweet corpus
X
i
. X
i
is comprised of a collection of tweets
X
i
= {x
i,j
}
j=N
i
j=1
, where N
i
denotes the number
of tweets user i published.
10
https://developers.facebook.com/docs/
graph-api/
11
http://www.freebase.com/
Education Job Spouse
#Users 7,208 1,806 1,636
#Users Con-
nected
6,295 1,407 1,108
#Edges 11,167 3,565 554
#Pos Entities 451 380 3121
#Pos Tweets 124,801 65,031 135,466
#Aver Pos
Tweets per User
17.3 36.6 82.8
#Neg Entity 6,987,186 4,405,530 8,840,722
#Neg Tweets 16,150,600 10,687,403 12,872,695
Table 2: Statistics for our Dataset
Tweet Collection L
e
i
: L
e
i
denotes the collection
of postings containing the mention of entity e from
user i. L
e
i
? X
i
.
Entity attribute indicator z
k
i,e
and z
k
i,x
: For
each entity e ? X
i
, there is a boolean variable z
k
i,e
,
indicating whether entity e expresses attribute k of
user i. Each posting x ? L
e
i
is associated with at-
tribute indicator z
k
i,x
indicating whether posting x
expresses attribute k of user i. z
k
i,e
and z
k
i,x
are
observed during training and latent during testing.
Neighbor set F
k
i
: F
k
i
denotes the neighbor set
of user i. For Education (k = 0) and Job (k = 1),
F
k
i
denotes the group of users within the network
that are in friend relation with user i. For Spouse
attribute, F
k
i
denote current user?s spouse.
4.2 Model
The distant supervision assumes that if entity e
corresponds to an attribute for user i, at least one
posting from user i?s Twitter stream containing a
mention of emight express that attribute. For user-
level attribute prediction, we adopt the following
two strategies:
(1) GLOBAL directly makes aggregate (entity)
level prediction for z
k
i,e
, where features for all
tweets from L
e
i
are aggregated to one vector for
training and testing, following Mintz et al (2009).
(2) LOCAL makes local tweet-level predictions
for each tweet z
e
i,x
, x ? L
k
i
in the first place, mak-
ing the stronger assumption that all mentions of an
entity in the users? profile are expressing the asso-
ciated attribute. An aggregate-level decision z
k
i,e
is
then made from the deterministic OR operators.
z
e
i,x
=
{
1 ?x ? L
e
i
, s.t.z
k
i,x
= 1
0 Otherwise
(1)
The rest of this paper describes GLOBAL in de-
tail. The model and parameters with LOCAL are
identical to those in GLOBAL except that LOCAL
168
encode a tweet-level feature vector rather than an
aggregate one. They are therefore excluded for
brevity. For each attribute k, we use a model that
factorizes the joint distribution as product of two
distributions that separately characterize text fea-
tures and network information as follows:
?(z
k
i,e
, X
i
, F
k
i
: ?) ?
?
text
(z
k
i,e
, X
i
)?
Neigh
(z
k
i,e
, F
k
i
)
(2)
Text Factor We use ?
text
(z
k
e
, X
i
) to capture the
text related features which offer attribute clues:
?
text
(z
k
e
, , X
i
) = exp[(?
k
text
)
T
? ?
text
(z
k
i,e
, X
i
)]
(3)
The feature vector ?
text
(z
k
i,e
, X
i
) encodes the fol-
lowing standard general features:
? Entity-level: whether begins with capital let-
ter, length of entity.
? Token-level: for each token t ? e, word iden-
tity, word shape, part of speech tags, name
entity tags.
? Conjunctive features for a window of k
(k=1,2) words and part of speech tags.
? Tweet-level: All tokens in the correspondent
tweet.
In addition to general features, we employ
attribute-specific features, such as whether the en-
tity matches a bag of words observed in the list
of universities, colleges and high schools for Edu-
cation attribute, whether it matches terms in a list
of companies for Job attribute
12
. Lists of universi-
ties and companies are taken from knowledge base
NELL
13
.
Neighbor Factor For Job and Education, we
bias friends to have a larger possibility to share
the same attribute. ?
Neigh
(z
k
i,e
, F
k
i
) captures such
influence from friends within the network:
?
Neigh
(z
k
i,e
, F
k
i
) =
?
j?F
k
i
?
Neigh
(z
k
e
, X
j
)
?
Neigh
(z
k
i,e
, X
j
)
= exp[(?
k
Neigh
)
T
? ?
Neigh
(z
k
i,e
, X
j
)]
(4)
Features we explore include the whether entity e
is also the correspondent attribute with neighbor
user j, i.e., I(z
e
j,k
= 0) and I(z
e
j,k
= 1).
12
Freebase is employed for alias recognition.
13
http://rtw.ml.cmu.edu/rtw/kbbrowser/
Input: Tweet Collection {X
i
}, Neighbor set
{F
k
i
}
Initialization:
? for each user i:
for each candidate entity e ? X
i
z
k
i,e
= argmax
z
?
?(z
?
, X
i
) from text
features
End Initialization
while not convergence:
? for each user i:
update attribute values for j ? F
k
i
for each candidate entity e ? X
i
z
k
i,e
= argmax
z
?
?(z
?
, X
i
, F
k
i
)
end while:
Figure 3: Inference for NEIGH-LATENT setting.
For Spouse, we set F
spouse
i
= {e} and the
neighbor factor can be rewritten as:
?
Neigh
(z
k
i,e
, X
j
) = ?
Neigh
(C
i
, X
e
) (5)
It characterizes whether current user C
i
to be the
spouse of user e (if e corresponds to a Twitter
user). We expect clues about whether C
i
being en-
tity e?s spouse from e?s Twitter corpus will in turn
facilitate the spouse inference procedure of user i.
?
Neigh
(C
i
, X
e
) encodes I(C
i
? S
e
), I(C
i
6? S
e
).
Features we explore also include whether C
i
?s
twitter ID appears in e?s corpus.
4.3 Training
We separately trained three classifiers regarding
the three attributes. All variables are observed
during training; we therefore take a feature-based
approach to learning structure prediction models
inspired by structure compilation (Liang et al,
2008). In our setting, a subset of the features
(those based on network information) are com-
puted based on predictions that will need to be
made at test time, but are observed during train-
ing. This simplified approach to learning avoids
expensive inference; at test time, however, we still
need to jointly predict the best attribute values for
friends as is described in section 4.4.
4.4 Inference
Job and Education Our inference algorithm
for Job/Education is performed on two settings,
depending on whether neighbor information is
169
observed (NEIGH-OBSERVED) or latent (NEIGH-
LATENT). Real world applications, where network
information can be partly retrieved from all types
of social networks, can always falls in between.
Inference in the NEIGH-OBSERVED setting is
trivial; for each entity e ? G
i
, we simply predict
it?s candidate attribute values using Equ.6.
z
k
i,e
= argmax
z
?
?(z
?
, X
i
, F
k
i
) (6)
For NEIGH-LATENT setting, attributes for each
node along the network are treated latent and user
attribute prediction depends on attributes of his
neighbors. The objective function for joint infer-
ence would be difficult to optimize exactly, and
algorithms for doing so would be unlikely to scale
to network of the size we consider. Instead, we use
a sieve-based greedy search approach to inference
(shown in Figure 3) inspired by recent work on
coreference resolution (Raghunathan et al, 2010).
Attributes are initialized using only text features,
maximizing ?
text
(e,X
i
), and ignoring network
information. Then for each user we iteratively re-
estimate their profile given both their text features
and network features (computed based on the cur-
rent predictions made for their friends) which pro-
vide additional evidence.
In this way, highly confident predictions will be
made strictly from text in the first round, then the
network can either support or contradict low con-
fidence predictions as more decisions are made.
This process continues until no changes are made
at which point the algorithm terminates. We em-
pirically found it to work well in practice. We ex-
pect that NEIGH-OBSERVED performs better than
NEIGH-LATENT since the former benefits from
gold network information.
Spouse For Spouse inference, if candidate entity
e has no correspondent twitter account, we directly
determine z
k
i,e
= argmax
z
?
?(z
?
, X
i
) from text
features. Otherwise, the inference of z
k
i,e
depends
on the z
k
e,C
i
. Similarly, we initialize z
k
i,e
and z
k
e,C
i
by maximizing text factor, as we did for Educa-
tion and Job. Then we iteratively update z
k
given
by the rest variables until convergence.
5 Experiments
In this Section, we present our experimental re-
sults in detail.
Education Job
AFFINITY 74.3 14.5
Table 3: Affinity values for Education and Job.
5.1 Preprocessing and Experiment Setup
Each tweet posting is tokenized using Twitter NLP
tool introduced by Noah?s Ark
14
with # and @
separated following tokens. We assume that at-
tribute values should be either name entities or
terms following @ and #. Name entities are ex-
tracted using Ritter et al?s NER system (2011).
Consecutive tokens with the same named entity
tag are chunked (Mintz et al, 2009). Part-of-
speech tags are assigned based on Owoputi et als
tweet POS system (Owoputi et al, 2013).
Data is divided in halves. The first is used as
training data and the other as testing data.
5.2 Friends with Same Attribute
Our network intuition is that users are much more
likely to be friends with other users who share at-
tributes, when compared to users who have no at-
tributes in common. In order to statistically show
this, we report the value of AFFINITY defined by
Mislove et al(2010), which is used to quantita-
tively evaluate the degree of HOMOPHILY in the
network. AFFINITY is the ratio of the fraction of
links between attribute (k)-sharing users (S
k
), rel-
ative to what is expected if attributes are randomly
assigned in the network (E
k
).
S
k
=
?
i
?
j?F
k
i
I(P
k
i
= P
k
j
)
?
i
?
j?F
k
i
I
E
k
=
?
m
T
k
m
(T
k
m
? 1)
U
k
(U
k
? 1)
(7)
where T
k
m
denotes the number of users with m
value for attribute k and U
k
=
?
m
T
k
m
. Table 3
shows the affinity value of the Education and Job.
As we can see, the property of HOMOPHILY in-
deed exists among users in the social network with
respect to Education and Job attribute, as signifi-
cant affinity is observed. In particular, the affinity
value for Education is 74.3, implying that users
connected by a link in the network are 74.3 times
more likely affiliated in the same school than as
expected if education attributes are randomly as-
signed. It is interesting to note that Education ex-
hibits a much stronger HOMOPHILY property than
14
https://code.google.com/p/
ark-tweet-nlp/downloads/list
170
Job. Such affinity demonstrates that our approach
that tries to take advantage of network information
for attribute prediction of holds promise.
5.3 Evaluation and Discussion
We evaluate settings described in Section 4.2 i.e.,
GLOBAL setting, where user-level attribute is pre-
dicted directly from jointly feature space and LO-
CAL setting where user-level prediction is made
based on tweet-level prediction along with differ-
ent inference approaches described in Section 4.4,
i.e. NEIGH-OBSERVED and NEIGH-LATENT, re-
garding whether neighbor information is observed
or latent.
Baselines We implement the following base-
lines for comparison and use identical processing
techniques for each approach for fairness.
? Only-Text: A simplified version of our algo-
rithm where network/neighbor influence is ig-
nored. Classifier is trained and tested only based
on text features.
? NELL: For Job and Education, candidate is se-
lected as attribute value once it matches bag of
words in the list of universities or companies
borrowed from NELL. For Education, the list is
extended by alias identification based on Free-
base. For Job, we also fetch the name abbrevia-
tions
15
. NELL is only implemented for Educa-
tion and Job attribute.
For each setting from each approach, we report
the (P)recision, (R)ecall and (F)1-score. For LO-
CAL setting, we report the performance for both
entity-level prediction (Entity) and posting-level
prediction (Tweet). Results for Education, Job and
Spouse from different approaches appear in Table
4, 5 and 6 respectively.
Local or Global For horizontal comparison, we
observe that GLOBAL obtains a higher Precision
score but a lower Recall than LOCAL(ENTITY).
This can be explained by the fact that LOCAL(U)
sets z
k
i,e
= 1 once one posting x ? L
e
i
is identified
as attribute related, while GLOBAL tend to be more
meticulous by considering the conjunctive feature
space from all postings.
Homophile effect In agreement with our ex-
pectation, NEIGH-OBSERVED performs better than
NEIGH-LATENT since erroneous predictions in
15
http://www.abbreviations.com/
NEIGH-LATENT setting will have negative in-
fluence on further prediction during the greedy
search process. Both NEIGH-OBSERVED and
NEIGH-LATENT where network information is
harnessed, perform better than Only-Text, which
the prediction is made independently on user?s text
features. The improvement of NEIGH-OBSERVED
over Only-Text is 22.7% and 6.4% regarding F-
1 score for Education and Job respectively, which
further illustrate the usefulness of making use of
Homophile effect for attribute inference on online
social media. It is also interesting to note the im-
provement much more significant in Education in-
ference than Job inference. This is in accord with
what we find in Section 5.2, where education net-
work exhibits stronger HOMOPHILE property than
Job network, enabling a significant benefit for ed-
ucation inference, but limited for job inference.
Spouse prediction also benefits from neighbor-
ing effect and the improvement is about 12% for
LOCAL(ENTITY) setting. Unlike Education and
Job prediction, for which in NEIGH-OBSERVED
setting all neighboring variables are observed, net-
work variables are hidden during spouse predic-
tion. By considering network information, the
model benefits from evident clues offered by tweet
corpus of user e?s spouse when making prediction
for e, but also suffers when erroneous decision are
made and then used for downstream predictions.
NELL Baseline Notably, NELL achieves high-
est Recall score for Education inference. It is
also worth noting that most of education men-
tions that NELL fails to retrieve are those in-
volve irregular spellings, such as HarvardUniv and
Cornell U, which means Recall score for NELL
baseline would be even higher if these irregular
spellings are recognized in a more sophisticated
system. The reason for such high recall is that as
our ground truths are obtained from Google plus,
the users from which are mostly affiliated with de-
cent schools found in NELL dictionary. However,
the high recall from NELL is sacrificed at preci-
sion, as users can mention school entities in many
of situations, such as paying a visit or reporting
some relevant news. NELL will erroneously clas-
sify these cases as attribute mentions.
NELL does not work out for Job, with a fairly
poor 0.0156 F1 score for LOCAL(ENTITY) and
0.163 for LOCAL(TWEET). Poor precision is ex-
pected for as users can mention firm entity in a
great many of situations. The recall score for
171
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach
NEIGH-OBSERVED 0.804 0.515 0.628 0.524 0.780 0.627 0.889 0.729 0.801
NEIGH-LATENT 0.755 0.440 0.556 0.420 0.741 0.536 0.854 0.724 0.783
Only-Text ?- 0.735 0.393 0.512 0.345 0.725 0.467 0.809 0.724 0.764
NELL ?- ?- ?- ?- 0.170 0.798 0.280 0.616 0.848 0.713
Table 4: Results for Education Prediction
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach
NEIGH-OBSERVED 0.643 0.330 0.430 0.374 0.620 0.467 0.891 0.698 0.783
NEIGH-LATENT 0.617 0.320 0.421 0.226 0.544 0.319 0.804 0.572 0.668
Only-Text ?- 0.602 0.304 0.404 0.155 0.501 0.237 0.764 0.471 0.583
NELL ?- ?- ?- ?- 0.0079 0.509 0.0156 0.094 0.604 0.163
Table 5: Results for Job Prediction
GLOBAL LOCAL(ENTITY) LOCAL(TWEET)
P R F P R F P R F
Our approach ?- 0.870 0.560 0.681 0.593 0.857 0.701 0.904 0.782 0.839
Only-Text ?- 0.852 0.448 0.587 0.521 0.781 0.625 0.890 0.729 0.801
Table 6: Results for Spouse Prediction
NELL in job inference is also quite low as job
related entities exhibit a greater diversity of men-
tions, many of which are not covered by the NELL
dictionary.
Vertical Comparison: Education, Job and
Spouse Job prediction turned out to be much
more difficult than Education, as shown in Ta-
bles 4 and 5. Explanations are as follows: (1)
Job contains a much greater diversity of mentions
than Education. Education inference can benefit a
lot from the dictionary relevant feature which Job
may not. (2) Education mentions are usually asso-
ciated with clear evidence such as homework, ex-
ams, studies, cafeteria or books, while situations
are much more complicated for job as vocabular-
ies are usually specific for different types of jobs.
(3) The boundary between a user working in and
a fun for a specific operation is usually ambigu-
ous. For example, a Google engineer may con-
stantly update information about outcome prod-
ucts of Google, so does a big fun. If the aforemen-
tioned engineer barely tweets about working con-
ditions or colleagues (which might still be ambigu-
ous), his tweet collection, which contains many of
mentions about outcomes of Google product, will
be significantly similar to tweets published by a
Google fun. Such nuisance can be partly solved
by the consideration of network information, but
not totally.
The relatively high F1 score for spouse predic-
tion is largely caused by the great many of non-
individual related entities in the dataset, the iden-
tification of which would be relatively simpler. A
deeper look at the result shows that the classifier
frequently makes wrong decisions for entities such
as userID and name entities. Significant as some
spouse relevant features are, such as love, hus-
band, child, in most circumstances, spouse men-
tions are extremely hard to recognize. For exam-
ple, in tweets ?Check this out, @alancross, it?s
awesome bit.ly/1bnjYHh.? or ?Happy Birth-
day @alancross !?. alancross can reasonably be
any option among current user?s friend, colleague,
parents, child or spouse. Repeated mentions add
no confidence. Although we can identify alan-
cross as spouse attribute once it jointly appear
with other strong spouse indicators, they are still
many cases where they never co-appear. How to
integrate more useful side information for spouse
recognition constitutes our future work.
6 Conclusion and Future Work
In this paper, we propose a framework for user at-
tribute inference on Twitter. We construct the pub-
licly available dataset based on distant supervision
and experiment our model on three useful user
profile attributes, i.e., Education, Job and Spouse.
Our model takes advantage of network informa-
tion on social network. We will keep updating the
dataset as more data is collected.
One direction of our future work involves ex-
ploring more general categories of user profile at-
172
tributes, such as interested books, movies, home-
town, religion and so on. Facebook would an
ideal ground truth knowledge base. Another direc-
tion involves incorporating richer feature space for
better inference performance, such as multi-media
sources (i.e. pictures and video).
7 Acknowledgments
A special thanks is owned to Dr. Julian McAuley
and Prof. Jure Leskovec from Stanford University
for the Google+ circle/network crawler, without
which the network analysis would not have been
conducted. This work was supported in part by
DARPA under award FA8750-13-2-0005.
References
Faiyaz Zamal, Wendy Liu, and Derek Ruths. 2012.
Homophily and latent attribute inference: Inferring
latent attributes of twitter users from neighbors. In
ICWSM.
Edward Benson, Aria Haghighi, and Regina Barzilay.
2011. Event discovery in social media feeds. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies-Volume 1, pages 389?398. As-
sociation for Computational Linguistics.
Sergey Brin. 1999. Extracting patterns and relations
from the world wide web. In The World Wide Web
and Databases.
Morgane Ciot, Morgan Sonderegger, and Derek Ruths.
2013. Gender inference of twitter users in non-
english contexts. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, Seattle, Wash, pages 18?21.
Michael Conover, Jacob Ratkiewicz, Matthew Fran-
cisco, Bruno Gonc?alves, Filippo Menczer, and
Alessandro Flammini. 2011. Political polarization
on twitter. In ICWSM.
Mark Craven and Johan Kumlien 1999. Construct-
ing biological knowledge bases by extracting infor-
mation from text sources. In ISMB, volume 1999,
pages 77?86.
Alec Go, Richa Bhayani, and Lei Huang. 2009. Twit-
ter sentiment classification using distant supervision.
CS224N Project Report, Stanford, pages 1?12.
Ido Guy, Naama Zwerdling, Inbal Ronen, David
Carmel, and Erel Uziel. 2010. Social media recom-
mendation based on people and tags. In Proceedings
of the 33rd international ACM SIGIR conference on
Research and development in information retrieval,
pages 194?201. ACM.
Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S
Zettlemoyer, and Daniel S Weld. 2011. Knowledge-
based weak supervision for information extraction
of overlapping relations. In ACL, pages 541?550.
Jiwei Li and Claire Cardie. 2013. Timeline generation:
Tracking individuals on twitter. Proceedings of the
23rd international conference on World wide web.
Percy Liang, Hal Daum?e III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning.
Wendy Liu and Derek Ruths. 2013. Whats in a name?
using first names as features for gender inference in
twitter. In 2013 AAAI Spring Symposium Series.
Wendy Liu, Faiyaz Zamal, and Derek Ruths. 2012.
Using social media to infer gender composition of
commuter populations. In Proceedings of the When
the City Meets the Citizen Workshop, the Interna-
tional Conference on Weblogs and Social Media.
Miller McPherson, Lynn Smith-Lovin, and James M
Cook. 2001. Birds of a feather: Homophily in social
networks. Annual review of sociology, pages 415?
444.
Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-
rafsky. 2009. Distant supervision for relation ex-
traction without labeled data. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Vol-
ume 2-Volume 2, pages 1003?1011. Association for
Computational Linguistics.
Alan Mislove, Bimal Viswanath, Krishna Gummadi,
and Peter Druschel. 2010. You are who you know:
inferring user profiles in online social networks. In
Proceedings of the third ACM international confer-
ence on Web search and data mining, pages 251?
260. ACM.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL-HLT, pages 380?390.
Marco Pennacchiotti and Ana Popescu. 2011. A ma-
chine learning approach to twitter user classification.
In ICWSM.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing.
Delip Rao and David Yarowsky. 2010. Detecting latent
user properties in social media. In Proc. of the NIPS
MLSN Workshop.
173
Delip Rao, David Yarowsky, Abhishek Shreevats, and
Manaswi Gupta. 2010. Classifying latent user at-
tributes in twitter. In Proceedings of the 2nd in-
ternational workshop on Search and mining user-
generated contents, pages 37?44. ACM.
Delip Rao, Michael Paul, Clayton Fink, David
Yarowsky, Timothy Oates, and Glen Coppersmith.
2011. Hierarchical bayesian models for latent at-
tribute detection in social media. In ICWSM.
Haibin Liu, Michael Wall, Karin Verspoor, et al 2012.
Literature mining of protein-residue associations
with graph rules learned through distant supervision.
Journal of biomedical semantics, 3(Suppl 3):S2.
Alan Ritter, Sam Clark, Mausam, Oren Etzioni, et al
2011. Named entity recognition in tweets: an ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1524?1534. Association for Compu-
tational Linguistics.
Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-
zioni. 2013. Modeling missing data in distant su-
pervision for information extraction.
Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,
and Christopher Manning. 2012. Multi-instance
multi-label learning for relation extraction. In Pro-
ceedings of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning, pages 455?
465. Association for Computational Linguistics.
Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012. Reducing wrong labels in distant supervi-
sion for relation extraction. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
721?729. Association for Computational Linguis-
tics.
Fei Wu and Daniel S Weld. 2007. Autonomously se-
mantifying wikipedia. In Proceedings of the six-
teenth ACM conference on Conference on infor-
mation and knowledge management, pages 41?50.
ACM.
Jaewon Yang and Jure Leskovec. 2013. Overlapping
community detection at scale: A nonnegative matrix
factorization approach. In Proceedings of the sixth
ACM international conference on Web search and
data mining, pages 587?596. ACM.
174
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 634?643,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Vector space semantics with frequency-driven motifs
Shashank Srivastava
Carnegie Mellon University
Pittsburgh, PA 15217
ssrivastava@cmu.edu
Eduard Hovy
Carnegie Mellon University
Pittsburgh, PA 15217
hovy@cmu.edu
Abstract
Traditional models of distributional se-
mantics suffer from computational issues
such as data sparsity for individual lex-
emes and complexities of modeling se-
mantic composition when dealing with
structures larger than single lexical items.
In this work, we present a frequency-
driven paradigm for robust distributional
semantics in terms of semantically cohe-
sive lineal constituents, or motifs. The
framework subsumes issues such as dif-
ferential compositional as well as non-
compositional behavior of phrasal con-
situents, and circumvents some problems
of data sparsity by design. We design
a segmentation model to optimally par-
tition a sentence into lineal constituents,
which can be used to define distributional
contexts that are less noisy, semantically
more interpretable, and linguistically dis-
ambiguated. Hellinger PCA embeddings
learnt using the framework show competi-
tive results on empirical tasks.
1 Introduction
Meaning in language is a confluence of experien-
tially acquired semantics of words or multi-word
phrases, and their semantic composition to create
new meanings. For instance, successfully inter-
preting a sentence such as
The old senator kicked the bucket.
requires the knowledge that the semantic conno-
tations of ?kicking the bucket? as a unit are the
same as those for ?dying?. Short of explicit su-
pervision, such semantic mappings must be in-
ferred by a new language speaker through induc-
tive mechanisms operating on observed linguis-
tic usage. This perspective of acquired meaning
aligns with the ?meaning is usage? adage, conso-
nant with Wittgenstein?s view of semantics. At
the same time, the ability to adaptively commu-
nicate elaborate meanings can only be conciled
through Frege?s principle of compositionality, i.e.,
meanings of larger linguistic constructs can be
derived from the meanings of individual compo-
nents, modulated by their syntactic interrelations.
Indeed, most linguistic usage appears composi-
tional. This is supported by the fact even with
very limited vocabulary, children and non-native
speakers can often communicate surprisingly ef-
fectively.
It can be argued that to be sustainable, induc-
tive aspects of meaning must be recurrent enough
to be learnable by new users. That is, a non-
compositional phrase such as ?kick the bucket? is
likely to persist in common parlance only if it is
frequently used with its associated semantic map-
ping. If a usage-driven meaning of a motif is not
recurrent enough, learning this mapping is inef-
ficient in two ways. First, the sparseness of ob-
servations would severely limit accurate inductive
acquisition by new observers. Second, the value
of learning a very infrequent semantic mapping
is likely marginal. This motivates the need for
a frequency-driven view of lexical semantics. In
particular, such a perspective can be especially
advantageous for distributional semantics for rea-
sons we outline below.
Distributional semantic models (DSMs) that
represent words as distributions over neighbouring
contexts have been particularly effective in captur-
ing fine-grained lexical semantics (Turney et al,
2010). Such models have engendered improve-
ments in diverse applications such as selectional
preference modeling (Erk, 2007), word-sense dis-
crimination (McCarthy and Carroll, 2003), auto-
matic dictionary building (Curran, 2003), and in-
formation retrieval (Manning et al, 2008). How-
ever, while conventional DSMs consider colloca-
634
With the bad press in wake of the financial crisis, businesses are leaving our shores .
crisis: <bad, businesses, financial, leaving, press, shores, wake>
financial crisis: <bad press, businesses, in wake of, leaving our shores>
Table 1: Meaning representation by conventional DSMs vs notional ideal
tion strengths (through counts and PMI scores) of
word neighbourhoods, they disregard much of the
regularity in human language. Most significantly,
word tokens that act as latent dimensions are of-
ten derived from arbitrary tokenization. The ex-
ample given in Table 1 succinctly describes this.
The first row in the table shows a representation
of the meaning of the token ?crisis? that a conven-
tional DSM might extract from the given sentence
after stopword removal. While helpful, the repre-
sentation seems unsatisfying since words such as
?press?, ?wake? and ?shores? seem to have little to
do with a crisis. From a semantic perspective, a
representation similar to the second is more valu-
able: not only does it represent a semantic map-
ping for a more specific meaning, but the latent di-
mensions of the representation have are less noisy
(e.g., while ?wake? is semantically ambiguous, its
surrounding context in ?in wake of? disambiguates
it) and more intuitive in regards of semantic in-
terepretability. This is the overarching theme of
this work: we present a frequency driven paradigm
for extending distributional semantics to phrasal
and sentential levels in terms of such semantically
cohesive, recurrent lexical units or motifs.
We propose to identify such semantically
cohesive motifs in terms of features inspired
from frequency-characteristics, linguistic idiosyn-
crasies, and shallow syntactic analysis; and ex-
plore both supervised and semi-supervised mod-
els to optimally segment a sentence into such mo-
tifs. Through exploiting regularities in language
usage, the framework can efficiently account for
both compositional and non-compositional word
usage, while avoiding the issue of data-sparsity by
design. Our principal contributions in this paper
are:
? We present a framework for extending dis-
tributional semantics to learn semantic repre-
sentations of both words and phrases in terms
of recurrent motifs, rather than arbitrary word
tokens
? We present a simple model to segment a sen-
tence into such motifs using a feature-set
drawing from frequency statistics, informa-
tion theory, linguistic theories and shallow
syntactic analysis
? Word and phrasal representations learnt
through the approach outperform conven-
tional DSM representations on empirical
tasks
This paper is organized as follows: In Sec-
tion 2, we briefly review related work in the do-
main of compositional distributional semantics,
and motivate our formulation. Section 3 describes
our methodology, which consists of a frequency-
driven segmentation model to partition text into
semantically meaningful recurring lineal-subunits,
a representation learning framework for learning
new semantic embeddings based on this segmen-
tation, and an approach to use such embeddings in
downstream applications. We present experiments
and empirical evaluations for our method in Sec-
tion 4. Finally, we conclude in Section 5 with a
summary of our principal findings, and a discus-
sion of possible directions for future work.
2 Related Work
While DSMs have been valuable in representing
semantics of single words, approaches to extend
them to represent the semantics of phrases and
sentences has met with only marginal success.
While there is considerable variety in approaches
and formulations, existing approaches for phrasal
level and sentential semantics can broadly be par-
titioned into two categories.
2.1 Compositional approaches
These have aimed at using semantic representa-
tions for individual words to learn semantic rep-
resentations for larger linguistic structures. These
methods implicitly make an assumption of com-
positionality, and often include explicit computa-
tional models of compositionality. Notable among
such models are the additive and multiplicative
models of composition by Mitchell and Lapata
(2008), Grefenstette et al (2010), Baroni and
635
Zamparelli?s (2010) model that differentially mod-
els content and function words for semantic com-
position, and Goyal et al?s SDSM model (2013)
that incorporates syntactic roles to model seman-
tic composition. Notable among the most effec-
tive distributional representations are the recent
deep-learning approaches by Socher et al (2012),
that model vector composition through non-linear
transformations. While word embeddings and lan-
guage models from such methods have been use-
ful for tasks such as relation classification, polarity
detection, event coreference and parsing; much of
existing literature on composition is based on ab-
stract linguistic theory and conjecture, and there
is little evidence to support that learnt represen-
tations for larger linguistic units correspond to
their semantic meanings. While works such as
the SDSM model suffer from the problem of spar-
sity in composing structures beyond bigrams and
trigrams, methods such as Mitchell and Lapata
(2008)and (Socher et al, 2012) and Grefenstette
and Sadrzadeh (2011) are restricted by signifi-
cant model biases in representing semantic com-
position by generic algebraic operations. Finally,
the assumption that semantic meanings for sen-
tences could have representations similar to those
for smaller individual tokens is in some sense un-
intuitive, and not supported by linguistic or seman-
tic theories.
2.2 Tree kernels
Tree Kernel methods have gained popularity in
the last decade for capturing syntactic information
in the structure of parse trees (Collins and Duffy,
2002; Moschitti, 2006). Instead of procuring ex-
plicit representations, the kernel paradigm directly
focuses on the larger goal of quantifying semantic
similarity of larger linguistic units. Structural ker-
nels for NLP are based on matching substructures
within two parse trees , consisting of word-nodes
with similar labels. These methods have been use-
ful for eclectic tasks such as parsing, NER, se-
mantic role labeling, and sentiment analysis. Re-
cent approaches such as by Croce et al (2011)
and Srivastava et al (2013) have attempted to pro-
vide formulations to incorporate semantics into
tree kernels through the use of distributional word
vectors at the individual word-nodes. While this
framework is attractive in the lack of assumptions
on representation that it makes, the use of distri-
butional embeddings for individual tokens means
that it suffers from the same shortcomings as de-
scribed for the example in Table 1, and hence these
methods model semantic relations between word-
nodes very weakly. Figure 1 shows an example of
the shortcomings of this general approach.
Figure 1: Tokenwise syntactic and semantic simi-
larities don?t imply sentential semantic similarity
While the two sentences in consideration have
near-identical syntax and could be argued to have
semantically aligned words in similar positions,
the semantics of the complete sentences are widely
divergent. Specifically, the ?bag of words? as-
sumption in tree kernels doesn?t suffice for these
lexemes, and a stronger semantic model is needed
to capture phrasal semantics as well as diverging
inter-word relations such as in ?coffee table? and
?water table?. Our hypothesis is that a model that
can even weakly identify recurrent motifs such as
?water table? or ?breaking a fall? would be help-
ful in building more effective semantic represen-
tations. A significant advantage of a frequency
driven view is that it makes the concern of com-
positionality of recurrent phrases immaterial. If a
motif occurs frequently enough in common par-
lance, its semantics could be captured with distri-
butional models irrespective of whether its associ-
ated semantics are compositional or acquired.
2.3 Identifying multi-word expressions
Several approaches have focused on supervised
identification of multi-word expressions (MWEs)
through statistical (Pecina, 2008; Villavicencio et
al., 2007) and linguistically motivated (Piao et al,
2005) techniques. More recently, hybrid methods
based on both statistical as well as linguistic fea-
tures have been popular (Tsvetkov and Wintner,
2011). Ramisch et al (2008) demonstrate that
adding part-of-speech tags to frequency counts
substantially improves performance. Other meth-
ods have attempted to exploit morphological, syn-
tactic and semantic characteristics of MWEs. In
636
particular, approaches such as Bannard (2007) use
syntactic rigidity to characterize MWEs. While
existing work has focused on the classification
task of categorizing a phrasal constituent as a
MWE or a non-MWE, the general ideas of most
of these works are in line with our current frame-
work, and the feature-set for our motif segmen-
tation model is designed to subsume most of
these ideas. It is worthwhile to point out that
the task of motif segmentation is slightly differ-
ent from MWE identification. Specifically, the
onus on recurrent occurrences means that non-
decomposibility is not an essential consideration
for a word to be considered a motif. In line with
the proposed paradigm, typical MWEs such as
?shoot the breeze?, ?sour note? and ?hot dog? would
be considered valid lineal motifs.
1
In addition,
even decomposable recurrent lineal phrases such
as ?love story?, ?federal government?, and ?mil-
lions of people? are marked as meaningful recur-
rent motifs. Finally, and least interestingly, we
include common named entities such as ?United
States? and ?Java Virtual Machine? within the am-
bit of motifs.
3 Method
In this section, we define our frequency-driven
framework for distributional semantics in detail.
As just described above, our definition for motifs
is less specific than MWEs. With such a working
definition, contiguous motifs are likely to make
distributional representations less noisy and also
assist in disambiguating context. Also, the lack of
specificity ensures that such motifs are common
enough to meaningfully influence distributional
representation beyond single tokens. A method
towards frequency-driven distributional semantics
could involve the following principal components:
3.1 Linear segmentation model
The segmentation model forms the core of the
framework. Ideally, it fragments a given sen-
tence into non-overlapping, semantically mean-
ingful, empirically frequent contiguous sub-units
or motifs. The model accounts for possible seg-
mentations of a sentence into potential motifs, and
prefers recurrent and cohesive motifs through fea-
tures that capture frequency-based and statistical
1
We note that since we take motifs as lineal units,
the current method doesn?t subsume several common non-
contiguous MWEs such as ?let off? in ?let him off?.
features, as well as linguistic idiosyncracies. This
is accomplished using a very simple linear chain
model and a rich feature set consisting of a combi-
nation of frequency-driven, information theoretic
and linguistically motivated features.
Let an observed sentence be denoted by x, with
the individual tokens x
i
denoting the i?th token in
the sentence. The segmentation model is a chain
LVM (latent variable model) that aims to maxi-
mize a linear objective defined by:
J =
?
i
w
i
f
i
(y
k
, y
k?1
,x)
where f
i
are arbitrary Markov features that can
depend on segments (potential motifs) of the ob-
served sentence x, and contiguous latent states.
The features are chosen so as to best represent
frequency-based, statistical as well as linguistic
considerations for treating a segment as an ag-
glutinative unit, or a motif. In specific, these
features could encode characteristics such as fre-
quency statistics, collocation strengths and syn-
tactic distinctness, or inflectional rigidity of the
considered segments; described in detail in Sec-
tion 3.2. The model is an instantiation of a sim-
ple featurized HMM, and the weighted sum of fea-
tures corresponding to a segment is cognate with
an affinity score for the ?stickiness? of the segment,
i.e., the affinity for the segment to be treated as
holistic unit or a single motif.
We also associate a penalizing cost for each non
unary-motif to avoid aggressive agglutination of
tokens. In particular, for an ngram occurrence to
be considered a motif, the marginal contribution
due to the affinity of the prospective motif should
at minimum exceed this penalty. The weights for
the affinity functions as well as these penalties are
learnt from data using full as well as partial anno-
tations. The latent state-variables y
k
denotes the
membership of the token x
k
to a unary or a larger
motif; and the state-sequence collectively gives
the segmentation of the sentence. An individual
state-variable y
k
encodes a pairing of the size of
the encompassing ngram motif, and the position
of the word x
k
within it. For instance, y
k
= T
3
denotes that the token x
k
is the final position in a
trigram motif.
3.1.1 Inference of optimal segmentation
If the optimal weights w
i
are known, inference
for the best motif segmentation can be performed
637
in linear time (in the number of tokens) follow-
ing the generalized Viterbi algorithm. A slightly
modified version of Viterbi could also be used to
find segmentations that are constrained to agree
with some given motif boundaries, but can seg-
ment other parts of the sentence optimally under
these constraints. This is necessary for the sce-
nario of semi-supervised learning of weights with
partially annotated sentences, as described later.
3.2 Learning motif affinities and penalties
We briefly discuss data-driven learning of weights
for features that define the motif affinity scores
and penalties. We describe learning of the model
parameters with fully annotated training data, as
well as an approach for learning motif segmenta-
tion that requires only partial supervision.
Supervised learning: In the supervised case, op-
timal state sequences y
(k)
are fully observed for
the training set. For this purpose, we created a
dataset of 1000 sentences from the Simple En-
glish Wikipedia and the Gigaword Corpus, and
manually annotated it with motif boundaries us-
ing BRAT (Stenetorp et al, 2012). In this case,
learning can follow the online structured percep-
tron learning procedure by Collins (2002), where
weights updates for the k?th training example
(x
(k)
,y
(k)
) are given as:
w
i
? w
i
+ ?(f
i
(x
(k)
,y
(k)
)? f
i
(x
(k)
,y
?
))
Here y
?
= Decode(x
(k)
,w) is the optimal
Viterbi decoding using the current estimates of
the weights. Updates are run for a large number
of iterations until the change in objective drops
below a threshold, and the learning rate ? is
adaptively modified as described in Collins et al
Implicitly, the weight learning algorithm can be
seen as a gradient descent procedure minimizing
the difference between the scores of highest
scoring (Viterbi) state sequences, and the label
state sequences.
Semi-supervised learning: In the semi-
supervised case, the labels y
(k)
i
are known
only for some of the tokens in x
(k)
. This is a
commonplace scenario, where a part of a sentence
has clear motif-boundaries, whereas the rest of the
sentence is not annotated. For accumulating such
data, we looked for occurrences of 2500 expres-
sions from the WikiMWE dataset in sentences
from the combined Simple English Wikipedia
and Gigaword corpora. The query expressions in
the retrieved sentences were marked with motif
boundaries, while the remaining tokens in the
sentences were left unannotated.
While the Viterbi algorithm can be used for tag-
ging optimal state-sequences given the weights,
the structured perceptron can learn optimal model
weights given gold-standard sequence labels.
Hence, in this case, we use a variation of the hard
EM algorithm for learning. The algorithm pro-
ceeds as follows: in the E-step, we use the current
values of weights to compute hard-expectations,
i.e., the best scoring Viterbi sequences among
those consistent with the observed state labels. In
the M-step, we take the decoded state-sequences
in the E-step as observed, and run perceptron
learning to update feature weightsw
i
. Pseudocode
of the learning algorithm for the partially labeled
case is given in Algorithm 1.
Algorithm 1
1: Input: Partially labeled data D = {(x, y)
i
}
2: Output: Weights w
3: Initialization: Set w
i
randomly, ?i
4: for i : 1 to maxIter do
5: Decode D with current w to find optimal
Viterbi paths that agree with (partial) ground
truths.
6: Run Structured Perceptron algorithm with de-
coded tag-sequences to update weights w
7: end for
8: return w
The semi-supervised approach enables incor-
poration of significantly more training data. In
particular, this method could be used in conjunc-
tion with a supervised approach. This would in-
volve initializing the weights prior to the semi-
supervised procedure with the weights from the
supervised learning model, so as to seed the semi-
supervised approach with reasonable model, and
use the partially annotated data to fine-tune the su-
pervised model. The sequential approach, akin to
annealing weights, can efficiently utilize both full
and partial annotations.
3.2.1 Feature engineering
In this section, we describe the principal features
used in the segmentation model
Transitional features and penalties:
? Transitional features f
trans
(y
i?1
, y
i
) =
638
Iy
i?1
,y
i
2
describing the transitional affinities
of state pairs. Since our state definitions pre-
clude certain transitions (such as from state
T
2
to T
1
), these weights are initialized to??
to expedite training.
? N-gram penalties: f
ngram
We define a
penalty for tagging each non-unary motif as
described before. For a motif to be tagged,
the improvement in objective score should at
least exceed the corresponding penalty. e.g.,
f
qgram
(y
i
) = I
y
i
=Q
4
denotes the penalty for
tagging a tetragram.
3
Frequency-based, information theoretic, and POS
features:
? Absolute and log-normalized motif frequen-
cies f
ngram
(x
i?n+1
, ...x
i?1
, x
i
, y
i
). This
feature is associated with a particular token-
sequence and ngram-tag, and takes the
value of the motif-frequency if the motif
token-sequence matches the feature token-
sequence, and is marked as with a match-
ing tag. e.g., f
bgram
(x
i?1
= love, x
i
=
story, y
i
= B
2
).
? Absolute and log-normalized motif frequen-
cies for a particular POS-sequence. This
feature is associated with a particular POS-
tag sequence and ngram-tag, and takes the
value of the motif-frequency if the motif
token-sequence gets a matching tag, and is
marked as with a matching ngram tag. e.g.,
f
bgram
(p
i?1
= V B, p
i
= NN, y
i
= B
2
).
? Medians and maxima of pairwise collocation
statistics for tokens for a particular size of
ngram motifs: we use the following statis-
tics: pointwise mutual information, Chi-
square statistic, and conditional probability.
We also used POS sensitive versions of these,
which performed much better than plain ver-
sions in our evaluations.
? Histogram counts of inflectional forms of to-
ken sequence for the corresponding ngram
motif and POS sequence: this features takes
the value of the count of inflectional forms
of an ngram that account for 90% of occur-
rences of all inflectional forms.
2
Here, I denotes the indicator function
3
It is straightforward to preclude partial n-gram annota-
tions near sentence boundaries with prohibitive penalties.
? Entropies of histogram distributions of inflec-
tional variants (described above).
? Features encoding syntactic rigidity: ratios
and log-ratios of frequencies of an ngram mo-
tif and variations by replacing a token using
near synonyms from its synset.
Additionally, a few feature for the segmenta-
tions model contained minor orthographic features
based on word shape (length and capitalization
patterns). Also, all numbers, URLs, and cur-
rency symbols were normalized to the special NU-
MERIC, URL, and CURRENCY tokens respec-
tively. Finally, a gazetteer feature checked for oc-
currences of motifs in a gazetteer of named enti-
ties.
3.3 Representation learning
With the segmentation model described in the pre-
vious section, we process text from the English Gi-
gaword corpus and the Simple English Wikipedia
to partition sentences into motifs. Since the seg-
mentation model accounts for the contexts of the
entire sentence in determining motifs, different in-
stances of the same token could evoke different
meaning representations. Consider the following
sentences tagged by the segmentation model, that
would correspond to different representations of
the token ?remains?: once as a standalone motif,
and once as part of an encompassing bigram motif
(?remains classified?).
Hog prices have declined sharply , while the
cost of corn remains relatively high.
Even with the release of such documents, ques-
tions are not answered, since only the agency
knows what remains classified
Given constituent motifs of each sentence in the
data, we can now define neighbourhood distribu-
tions for unary or phrasal motifs in terms of other
motifs (as envisioned in Table 1). In our experi-
ments, we use a window-length of 5 adjoining mo-
tifs on either side to define the neighbourhood of
a constituent. Naturally, in the presence of multi-
word motifs, the neighbourhood boundary could
be more extended than in a conventional DSM.
With such neighbourhood contexts, the distri-
butional paradigm posits that semantic similarity
between a pair of motifs can be given by a sense
of ?distance? between the two distributions. Most
popularly, traditional measures of vector distance
639
such as the cosine similarity, Euclidean distance
and City-block distance have been used in sev-
eral distributional approaches. Additionally, sev-
eral distance measures between discrete distribu-
tions exist in statistical literature, most famously
the Kullback Leibler divergence, Bhattacharyya
distance and the Hellinger distance. Recent work
(Lebret and Lebret, 2013) has shown that the
Hellinger distance is an especially effective mea-
sure in learning distributional embeddings, with
Hellinger PCA being much more computationally
inexpensive than neural language modeling ap-
proaches, while performing much better than stan-
dard PCA, and competitive with the state-of-the-
art in downstream evaluations. Hence, we use the
Hellinger measure between neighbourhood motif
distributions in learning representations.
The Hellinger distance between two categorical
distributions P = (p
1
...p
k
) and Q = (q
1
...q
k
) is
defined as:
H(P,Q) =
1
?
2
?
?
?
?
k
?
i=1
(
?
p
i
?
?
q
i
)
2
=
1
?
2
?
?
?
?
P ?
?
Q
?
?
?
2
The Hellinger measure has intuitively desir-
able properties: specifically, it can be seen
as the Euclidean distance between the square-
roots transformed distributions, where both vec-
tors
?
P and
?
Q are length-normalized under the
same(Euclidean) norm. Finally, we perform SVD
on the motif similarity matrix (with size of the or-
der of the total vocabulary in the corpus), and re-
tain the first k principal eigenvectors to obtain low-
dimensional vector representations that are more
convenient to work with. In our preliminary ex-
periments, we found that k = 300 gave quanti-
tatively good results, with marginal change with
added dimensionality. We use this setting for all
our experiments.
4 Experiments
In this section, we describe some experimental
evaluations and findings for our approach. We first
quantitatively and qualitatively analyze the perfor-
mance of the segmentation model, and then evalu-
ate the distributional motif representations learnt
by the model through two downstream applica-
tions.
4.1 Motif segmentation
In an evaluation of the motif segmentations model
within the perspective of our framework, we be-
lieve that exact correspondence to human judg-
ment is unrealistic, since guiding principles for
defining motifs, such as semantic cohesion, are
hard to define and only serve as working princi-
ples. However, for purposes of relative compar-
ison, we quantitatively evaluate the performance
of the motif segmentation models on the fully an-
notated dataset. For this experiment, the gold-
annotated corpus was split into a training and test
sets in a 9:1 proportion. A small fraction of the
training split was set apart for development and
validation. For this evaluation, we considered a
motif boundary as correct only for an exact match,
i.e., when both its boundaries (left and right) were
correctly predicted. Also, since a majority of mo-
tifs are unary tokens, including them into consider-
ation artificially boosts the accuracy, whereas we
are more interested in the prediction of larger n-
gram tokens. Hence we report results on the per-
formance on only non-unary motifs.
P R F
Rule-based baseline 0.85 0.10 0.18
Supervised 0.62 0.28 0.39
Semi-supervised 0.30 0.17 0.22
Supervised + annealing 0.69 0.38 0.49
Table 2: Results for motif segmentations
Table 2 shows the performance of the segmen-
tation model with the three proposed learning ap-
proaches described earlier. For a baseline, we
consider a rule-based model that simply learns all
ngram segmentations seen in the training data, and
marks any occurrence of a matching token se-
quence as a motif; without taking neighbouring
context into account. We observe that this model
has a very high precision (since many token se-
quences marked as motifs would recur in simi-
lar contexts, and would thus have the same mo-
tif boundaries). However, the rule-based method
has a very row recall due to lack of generaliza-
tion capabilities. We see that while all three learn-
ing algorithms perform better than the baseline,
the performance of the purely unsupervised sys-
tem is inferior to supervised approaches. This is
not unexpected: the supervision provided to the
model is very weak due to a lack of negative ex-
amples (which leads to spurious motif taggings,
640
While men often (openly or privately) sympathized with Prince Charles when the princess went public
about her rotten marriage , women cheered her on.
The healthcare initiative has become a White elephant for the federal government.
Chirac and Juppe have made a bad situation worse by seeking to meet Maastricht criteria not by
cutting spending, but by raising taxes still further.
Now , say Vatican observers , Pope John Paul II wants to show the world that many church members
did resist the Third Reich and paid the price.
Table 3: Examples of output from sentence segmentation model
leading to a low precision), as well as no examples
of transitions between adjacent motifs (to learn
transitional weights and penalties). The super-
vised model expectedly outperforms both the rule-
based and the semi-supervised systems. However,
the supervised learning model with subsequent an-
nealing outperforms the supervised model in terms
of both precision and recall; showing the utility of
the semi-supervised method when seeded with a
good initial model, and the additive value of par-
tially labeled data.
Qualitative analysis of motif-segmented sen-
tences shows that our designed feature-set is effec-
tive and helpful in identifying semantically cohe-
sive ngrams. Table 3 provides four examples. The
first example correctly identifies ?went public?,
while missing out on the potential motif ?cheered
her on?. In general, these examples illustrate that
the model can identify idiomatic and idiosyncratic
themes as well as commonly recurrent ngrams (in
the second example, the model picks out ?has be-
come? which is highly recurrent, but doesn?t have
the semantic cohesiveness of some of the other
motifs). In particular, consider the second exam-
ple, where the model picks ?white elephant? as a
motif. In such cases, the disambiguating influence
of context incorporated by the motif is apparent.
Elephant White elephant
tusks expensive
trunk spend
african biggest
white the project
indian very high
baby multibillion dollar
The above table shows some of the top results
for the unary token ?elephant? by frequency, and
frequent unary and non-unary motifs for the mo-
tif ?white elephant? retrieved by the segmentation
model.
4.2 Distributional representations
For evaluating distributional representations for
motifs (in terms of other motifs) learnt by the
framework, we test these representations in two
downstream tasks: sentence polarity classification
and metaphor detection. For sentence polarity, we
consider the Cornell Sentence Polarity corpus by
Pang and Lee (2005), where the task is to classify
the polarity of a sentence as positive or negative.
The data consists of 10662 sentences from movie
reviews that have been annotated as either posi-
tive or negative. For composing the motifs repre-
sentations to get judgments on semantic similarity
of sentences, we use our recent Vector Tree Ker-
nel approach The VTK approach defines a convo-
lutional kernel over graphs defined by the depen-
dency parses of sentences, using a vector repre-
sentation at each graph node that representing a
single lexical token. For our purposes, we mod-
ify the approach to merge the nodes of all tokens
that constitute a motif occurrence, and use the mo-
tif representation as the vector associated with the
node. Table 4 shows results for the sentence polar-
ity task.
P R F1
DSM 0.56 0.50 0.53
AVM 0.55 0.53 0.54
MVM 0.55 0.49 0.52
VTK 0.65 0.58 0.62
VTK + MotifDSM 0.66 0.60 0.63
Table 4: Results for Sentence Polarity detection
For this task, the motif based distributional em-
beddings vastly outperform a conventional distri-
butional model (DSM) based on token distribu-
tions, as well as additive (AVM) and multiplica-
tive (MVM) models of vector compositionality, as
641
proposed by Lapata et al The model is compet-
itive with the state-of-the-art VTK (Srivastava et
al., 2013) that uses the SENNA neural embeddings
by Collobert et al (2011).
P R F1
CRF 0.74 0.50 0.59
SVM+DSM 0.63 0.80 0.71
VTK+ SENNA 0.67 0.87 0.76
VTK+ MotifDSM 0.70 0.87 0.78
Table 5: Results for Metaphor identification
On the metaphor detection task, we use the
Metaphor dataset (Hovy et al, 2013). The data
consists of sentences with defined phrases, and
the task consists of identifying the linguistic use
in these phrases as metaphorical or literal. For
this task, the motif based model is expected to
perform well as common metaphorical usage is
generally through idiosyncratic MWEs, which the
motif based models is specially geared to capture
through the features of the segmentation model.
For this task, we again use the VTK formalism
for combining vector representations of the indi-
vidual motifs. Table 5 shows that the motif-based
DSM does better than discriminative models such
as CRFs and SVMs, and also slightly improves on
the VTK kernel with distributional embeddings.
5 Conclusion
We have presented a new frequency-driven frame-
work for distributional semantics of not only lex-
ical items but also longer cohesive motifs. The
theme of this work is a general paradigm of seek-
ing motifs that are recurrent in common parlance,
are semantically coherent, and are possibly non-
compositional. Such a framework for distribu-
tional models avoids the issue of data sparsity
in learning of representations for larger linguis-
tic structures. The approach depends on drawing
features from frequency statistics, statistical cor-
relations, and linguistic theories; and this work
provides a computational framework to jointly
model recurrence and semantic cohesiveness of
motifs through compositional penalties and affin-
ity scores in a data driven way.
While being deliberately vague in our work-
ing definition of motifs, we have presented simple
efficient formulations to extract such motifs that
uses both annotated as well as partially unanno-
tated data. The qualitative and quantitative analyis
of results from our preliminary motif segmenta-
tion model indicate that such motifs can help to
disambiguate contexts of single tokens, and pro-
vide cleaner, more interpretable representations.
Finally, we obtain motif representations in form
of low-dimensional vector-space embeddings, and
our experimental findings indicate value of the
learnt representations in downstream applications.
We believe that the approach has considerable the-
oretical as well as practical merits, and provides a
simple and clean formulation for modeling phrasal
and sentential semantics.
In particular, we believe that ours is the first
method that can invoke different meaning repre-
sentations for a token depending on textual context
of the sentence. The flexibility of having separate
representations to model different semantic senses
has considerable valuable, as compared with ex-
tant approaches that assign a single representation
to each token, and are hence constrained to con-
flate several semantic senses into a common repre-
sentation. The approach also elegantly deals with
the problematic issue of differential compositional
and non-compositional usage of words. Future
work can focus on a more thorough quantitative
evaluation of the paradigm, as well as extension to
model non-contiguous motifs.
References
Colin Bannard. 2007. A measure of syntactic flexibil-
ity for automatically identifying multiword expres-
sions in corpora. In Proceedings of the Workshop
on a Broader Perspective on Multiword Expressions,
pages 1?8. Association for Computational Linguis-
tics.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
1183?1193. Association for Computational Linguis-
tics.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: Kernels over
discrete structures, and the voted perceptron. In Pro-
ceedings of the 40th annual meeting on association
for computational linguistics, pages 263?270. Asso-
ciation for Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and exper-
iments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing-Volume 10, pages 1?8.
Association for Computational Linguistics.
642
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493?2537.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured lexical similarity via con-
volution kernels on dependency trees. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1034?1046. Asso-
ciation for Computational Linguistics.
James Richard Curran. 2003. From Distributional to
Semantic Similarity. Ph.D. thesis, Institute for Com-
municating and Collaborative Systems School of In-
formatics University of Edinburgh.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences. In ACL.
Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-
maya Sachan, Shashank Srivastava, and Eduard
Hovy. 2013. A structured distributional semantic
model: Integrating structure with semantics. ACL
2013, page 20.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical composi-
tional distributional model of meaning. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, pages 1394?1404. Asso-
ciation for Computational Linguistics.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2010.
Concrete sentence spaces for compositional dis-
tributional models of meaning. arXiv preprint
arXiv:1101.0309.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. Meta4NLP
2013, page 52.
R?emi Lebret and Ronan Lebret. 2013. Word
emdeddings through hellinger pca. arXiv preprint
arXiv:1312.5542.
Christopher D Manning, Prabhakar Raghavan, and
Hinrich Sch?utze. 2008. Introduction to information
retrieval, volume 1. Cambridge University Press
Cambridge.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Compu-
tational Linguistics, 29(4):639?654.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In ACL, pages
236?244.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Machine Learning: ECML 2006, pages 318?329.
Springer.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In ACL.
Pavel Pecina. 2008. A machine learning approach to
multiword expression extraction. In Proceedings of
the LREC MWE 2008 Workshop, pages 54?57. Cite-
seer.
Scott Songlin Piao, Paul Rayson, Dawn Archer, and
Tony McEnery. 2005. Comparing and combining
a semantic tagger and a statistical tool for mwe ex-
traction. Computer Speech & Language, 19(4):378?
397.
Carlos Ramisch, Paulo Schreiner, Marco Idiart, and
Aline Villavicencio. 2008. An evaluation of meth-
ods for the extraction of multiword expressions.
In Proceedings of the LREC Workshop-Towards
a Shared Task for Multiword Expressions (MWE
2008), pages 50?53.
Richard Socher, Brody Huval, Christopher D Manning,
and Andrew Y Ng. 2012. Semantic compositional-
ity through recursive matrix-vector spaces. In Pro-
ceedings of the 2012 Joint Conference on Empiri-
cal Methods in Natural Language Processing and
Computational Natural Language Learning, pages
1201?1211. Association for Computational Linguis-
tics.
Shashank Srivastava, Dirk Hovy, and Eduard H. Hovy.
2013. A walk-based semantically enriched tree
kernel over distributed word representations. In
EMNLP, pages 1411?1416.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. Brat: a web-based tool for nlp-assisted
text annotation. In Proceedings of the Demonstra-
tions at the 13th Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 102?107. Association for Computational Lin-
guistics.
Yulia Tsvetkov and Shuly Wintner. 2011. Identifica-
tion of multi-word expressions by combining mul-
tiple linguistic information sources. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing, pages 836?845. Association
for Computational Linguistics.
Peter D Turney, Patrick Pantel, et al 2010. From
frequency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141?188.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In EMNLP-
CoNLL, pages 1034?1043.
643
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1566?1576,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Towards a General Rule for Identifying Deceptive Opinion Spam
Jiwei Li
1
, Myle Ott
2
, Claire Cardie
2
, Eduard Hovy
1
1
Language Technology Institute, Carnegie Mellon University, Pittsburgh, P.A. 15213, USA
2
Department of Computer Science, Cornell University, Ithaca, N.Y., 14853, USA
bdlijiwei@gmail.com, myleott@cs.cornell.edu
cardie@cs.cornell.edu, ehovy@andrew.cmu.edu
Abstract
Consumers? purchase decisions are in-
creasingly influenced by user-generated
online reviews. Accordingly, there has
been growing concern about the poten-
tial for posting deceptive opinion spam?
fictitious reviews that have been deliber-
ately written to sound authentic, to de-
ceive the reader. In this paper, we ex-
plore generalized approaches for identify-
ing online deceptive opinion spam based
on a new gold standard dataset, which is
comprised of data from three different do-
mains (i.e. Hotel, Restaurant, Doctor),
each of which contains three types of re-
views, i.e. customer generated truthful re-
views, Turker generated deceptive reviews
and employee (domain-expert) generated
deceptive reviews. Our approach tries to
capture the general difference of language
usage between deceptive and truthful re-
views, which we hope will help customers
when making purchase decisions and re-
view portal operators, such as TripAdvisor
or Yelp, investigate possible fraudulent ac-
tivity on their sites.
1
1 Introduction
Consumers increasingly rely on user-generated
online reviews when making purchase deci-
sion (Cone, 2011; Ipsos, 2012). Unfortunately,
the ease of posting content to the Web, poten-
tially anonymously, creates opportunities and in-
centives for unscrupulous businesses to post de-
ceptive opinion spam?fictitious reviews that are
deliberately written to sound authentic, in order to
deceive the reader.
2
Accordingly, there appears
1
Dataset available by request from the first author.
2
Manipulating online reviews may also have legal conse-
quences. For example, the Federal Trade Commission (FTC)
to be widespread and growing concern among
both businesses and the public about this poten-
tial abuse (Meyer, 2009; Miller, 2009; Streitfeld,
2012; Topping, 2010; Ott, 2013).
Existing approaches for spam detection are usu-
ally focused on developing supervised learning-
based algorithms to help users identify decep-
tive opinion spam, which are highly dependent
upon high-quality gold-standard labeled data (Jin-
dal and Liu, 2008; Jindal et al, 2010; Lim et al,
2010; Wang et al, 2011; Wu et al, 2010). Stud-
ies in the literature rely on a couple of approaches
for obtaining labeled data, which usually fall into
two categories. The first relies on the judge-
ments of human annotators (Jindal et al, 2010;
Mukherjee et al, 2012). However, recent stud-
ies show that deceptive opinion spam is not eas-
ily identified by human readers (Ott et al, 2011).
An alternative approach, as introduced by Ott et
al. (2011), crowdsourced deceptive reviews using
Amazon Mechanical Turk.
3
A couple of follow-up
works have been introduced based on Ott et al?s
dataset, including estimating prevalence of decep-
tion in online reviews (Ott et al, 2012), identifica-
tion of negative deceptive opinion spam (Ott et al,
2013), and identifying manipulated offerings (Li
et al, 2013b).
Despite the advantages of soliciting deceptive
gold-standard material from Turkers (it is easy,
large-scale, and affordable), it is unclear whether
Turkers are representative of the general popula-
tion that generate fake reviews, or in other words,
Ott et al?s data set may correspond to only one
type of online deceptive opinion spam ? fake re-
views generated by people who have never been
to offerings or experienced the entities. Specifi-
cally, according to their findings (Ott et al, 2011;
has updated their guidelines on the use of endorsements and
testimonials in advertising to suggest that posting deceptive
reviews may be unlawful in the United States (FTC, 2009).
3
http://www.mturk.com
1566
Li et al, 2013a), truthful hotel reviews encode
more spatial details, characterized by terms such
as ?bathroom? and ?location?, while deceptive re-
views talk about general concepts such as why or
with whom they went to the hotel. However, a
hotel can instead solicit fake reviews from their
employees or customers who possess substantial
domain knowledge to write fake reviews and en-
code more spatial details in their lies. Indeed,
cases have been reported where hotel owners bribe
guests in return for good reviews on TripAdvi-
sor
4
, or companies ordered employees to pretend
they were satisfied customers and write glowing
reviews of its face-lift procedure on Web sites.
5
The domain knowledge possessed by domain ex-
perts enables them to craft reviews that are much
more difficult for classifiers to detect, compared to
the crowdsourced fake reviews.
Additionally, existing supervised algorithms in
the literature are usually narrowed to one spe-
cific domain and heavily rely on domain-specific
vocabulary. For example, classifiers assign high
weights to domain-specific terms such as ?hotel?,
?rooms?, or even the name of the hotels such as
?Hilton? when trained on reviews on hotels. It
is unclear whether these classifiers will perform
well at detecting deception in other domains, e.g.,
Restaurant or Doctor reviews. Even in a single do-
main, e.g., Hotel, classifiers trained from reviews
of one city (e.g., Chicago) may not be effective if
directly applied to reviews from other cities (e.g.,
New York City) (Li et al, 2013b). In the exam-
ples in Table 1, we trained a linear SVM clas-
sifier on Ott?s Chicago-hotel dataset on unigram
features and tested it on a couple of different do-
mains (the details of data acquisition are illustrated
in Section 3). Good performance is obtained on
Chicago-hotel reviews (Ott et al, 2011), but not as
good on New York City ones. The performance is
reasonable in Restaurant reviews due to the many
shared properties among restaurants and hotels,
but suffers in Doctor settings.
In this paper, we try to obtain a deeper under-
standing of the general nature of deceptive opin-
ion spam. One contribution of the work presented
here is the creation of the cross-domain (i.e., Ho-
tel, Restaurant and Doctor) gold-standard dataset.
4
http://www.dailymail.co.uk/travel/article-
2013391/Tripadvisor-Hotel-owners-bribe-guests-return-
good-reviews.html
5
http://www.nytimes.com/2009/07/15/
technology/internet/15lift.html?_r=0
Accuracy Precision Recall F1
NYC-Hotel 0.799 0.794 0.758 0.766
Chicago-Restaurant 0.785 0.813 0.742 0.778
Doctor 0.550 0.537 0.725 0.617
Table 1: SVM performance on datasets for a clas-
sifier trained on Chicago hotel review based on
Unigram feature.
In contrast to existing work (Ott et al, 2011; Li et
al., 2013b), our new gold standard includes three
types of reviews: domain expert deceptive opinion
spam (Employee), crowdsourced deceptive opin-
ion spam (Turker), and truthful Customer reviews
(Customer). In addition, some of domains contain
both positive (P) and negative (N) reviews.
6
To explore the general rule of deceptive opinion
spam, we extended SAGE Model (Eisenstein et
al., 2011), a bayesian generative approach that can
capture the multiple generative facets (i.e., decep-
tive vs truthful, positive vs negative, experienced
vs non-experienced, hotel vs restaurant vs doctor)
in the text collection. We find that more general
features, such as LIWC and POS, are more robust
when modeled using SAGE, compared with just
bag-of-words.
We additionally make theoretical contributions
that may shed light on a longstanding debate in the
literature about deception. For example, in con-
trast to existing findings that highlight the lack of
spatial detail in deceptive reviews (Ott et al, 2011;
Li et al, 2013b), we find that a lack of spatial de-
tail may not be a universal cue to deception, since
it does not apply to fake reviews written by domain
experts. Instead, our finding suggest that other lin-
guistic features may offer more robust cues to de-
ceptive opinion spam, such as overly highlighted
sentiment in the review or the overuse of first-
person singular pronouns.
The rest of this paper is organized as follows.
In Section 2, we briefly go over related work. We
describe the creation of our data set in Section 3
and present our model in Section 4. Experimental
results are shown in Section 5. We present anal-
ysis of general cues to deception in Section 6 and
conclude this paper in Section 7.
6
For example, a hotel manager could hire people to write
positive reviews to increase the reputation of his own hotel
or post negative ones to degrade his competitors. Identify-
ing positive/negative opinion spam is explored in (Ott et al,
2011; Ott et al, 2013)
1567
2 Related Work
Spam has been historically studied in the contexts
of Web text (Gy?ongyi et al, 2004; Ntoulas et al,
2006) or email (Drucker et al, 1999). Recently
there has been increasing concern about deceptive
opinion spam (Jindal and Liu, 2008; Ott et al,
2011; Wu et al, 2010; Mukherjee et al, 2013b;
Wang et al, 2012).
Jindal and Liu (2008) first studied the deceptive
opinion problem and trained models using features
based on the review text, reviewer, and product
to identify duplicate opinions, i.e., opinions that
appear more than once in the corpus with simi-
lar contexts. Wu et al (2010) propose an alter-
native strategy to detect deceptive opinion spam
in the absence of a gold standard. Yoo and Gret-
zel (2009) gathered 40 truthful and 42 deceptive
hotel reviews and manually compare the linguis-
tic differences between them. Ott et al created
a gold-standard collection by employing Turkers
to write fake reviews, and follow-up research was
based on their data (Ott et al, 2012; Ott et al,
2013; Li et al, 2013b; Feng and Hirst, 2013). For
example, Song et al (2012) looked into syntactic
features from Context Free Grammar parse trees
to improve the classifier performance. A step fur-
ther, Feng and Hirst (2013) make use of degree
of compatibility between the personal experiment
and a collection of reference reviews about the
same product rather than simple textual features.
In addition to exploring text or linguistic fea-
tures in deception, some existing work looks
into customers? behavior to identify deception
(Mukherjee et al, 2013a). For example, Mukher-
jee et al (2011; 2012) delved into group behavior
to identify group of reviewers who work collabo-
ratively to write fake reviews. Qian and Liu (2013)
identified multiple user IDs that are generated by
the same author, as these authors are more likely
to generate deceptive reviews.
In the psychological literature, researchers have
looked into possible linguistic cues to deception
(Newman et al, 2003), such as decreased spatial
detail, which is consistent with theories of reality
monitoring (Johnson and Raye, 1981), increased
negative emotion terms (Newman et al, 2003), or
the writing style difference between informative
(truthful) and imaginative (deceptive) writings in
(Rayson et al, 2001). The former typically con-
sists of more nouns, adjectives, prepositions, de-
terminers, and coordinating conjunctions, while
the latter consists of more verbs, adverbs, pro-
nouns, and pre-determiners.
SAGE (Sparse Additive Generative Model):
SAGE is an generative bayesian approach in-
troduced by Eisenstein et al (2011), which
can be viewed as an combination of topic mod-
els (Blei et al, 2003) and generalized additive
models (Hastie and Tibshirani, 1990). Unlike
other derivatives of topic models, SAGE drops
the Dirichlet-multinomial assumption and adopts
a Laplacian prior, triggering sparsity in topic-word
distribution. The reason why SAGE is tailored for
our task is that SAGE constructs multi-faceted la-
tent variable models by simply adding together the
component vectors rather than incorporating mul-
tiple switching latent variables in multiple facets.
3 Dataset Construction
In this section, we report our efforts to gather gold-
standard opinion spam datasets. Our datasets con-
tain the following domains, namely Hotel, Restau-
rant, and Doctor.
3.1 Turker set, using Mechanical Turk
Crowdsourcing services such as AMT greatly fa-
cilitate large-scale data annotation and collection
efforts. Anyone with basic programming skills can
create Human Intelligence Tasks (HITs) and ac-
cess a marketplace of anonymous online workers
(Turkers) willing to complete the tasks. We bor-
rowed some rules used by Ott et al to create their
dataset, such as restricting task to Turkers located
in the United States, and who maintain an approval
rating of at least 90%.
Hotel-Turker : We directly borrowed datasets
from Ott
7
and Li.
8
Restaurant-Turker : We gathered 20 positive
(P) deceptive reviews for each of 10 of the most
popular restaurants in Chicago, for a total of 200
positive deceptive restaurant reviews.
Doctor-Turker : We gathered a total number of
200 positive reviews from Turkers.
3.2 Employee set, by domain experts
We seek deceptive opinion spam written by people
with expert-level domain knowledge. It is not ap-
propriate to use crowdsourcing to obtain this data,
7
http://myleott.com/op_spam/
8
http://www.cs.cmu.edu/
?
jiweil/html/
four_city.html
1568
Turker Expert Customer
Hotel (P/N) 400/400 140/140 400/400
Restaurant (P/N) 200/0 120/0 200/200
Doctor (P/N) 200/0 32/0 200/0
Table 2: Statistics for our dataset.
so instead we solicit reviews written by employees
in each domain.
Hotel-Employee: We asked two hotel employ-
ees from each of seven hotels (14 employees to-
tal) each to write 10 deceptive positive-sentiment
reviews of their own hotel, and 10 deceptive
negative-sentiment reviews of their biggest local
competitor?s hotel. In total, we obtained 280 de-
ceptive reviews of 14 hotels, including a balanced
mix of positive- and negative-sentiment reviews.
Restaurant-Employee: We asked employees
from selected restaurants (a waiter/waitress or
cook) to each write positive-sentiment reviews of
their restaurant.
Doctor-Employee: We asked real doctors to
write positive fake reviews about themselves. In
total we obtained 32 reviews from 15 doctors.
3.3 Customer set from Actual Customers
Hotel-Customer: We borrowed from Ott et al?s
dataset.
Restaurant/Doctor-Customer: We solicited
data by matching a set of truthful reviews as Ott
et al did in collecting truthful hotel reviews.
3.4 Summary for Data Creation
Statistics for our data set is presented in Table 2.
Due to the difficulty in obtaining gold-standard
data in the literature, there is no doubt that our data
set is not perfect. Some parts are missing, some
are unbalanced, participants in the survey may not
be representative of the general population. How-
ever, as far as we know, this is the most compre-
hensive dataset for deceptive opinion spam so far,
and may to some extent shed insights on the nature
of online deception.
4 Feature-based Additive Model
In this section, we briefly describe our model.
Since mathematics are not the main theme of this
paper, we omit the exact details for inference,
which can be found in (Eisenstein et al, 2011).
Before describing the model in detail, we note
the following advantages of the SAGE model, and
our reasons for using it in this paper:
1. the ?additive? nature of SAGE allows a better
understanding of which features contribute
most to each type of deceptive review and
how much each such feature contributes to
the final decision jointly. If we instead use
SVM, for example, we would have to train
classifiers one by one (due to the distinct fea-
tures from different sources) to draw con-
clusions regarding the differences between
Turker vs Expert vs truthful reviews, positive
expert vs negative expert reviews, or reviews
from different domains. This would not only
become intractable, but would make the con-
clusions less clear.
2. For cross-domain classification task, standard
machine learning approaches may suffer due
to domain-specific properties (See Section
5.2).
4.1 Model
In SAGE, each termw is drawn from a distribution
proportional to exp(m
(w)
+ ?
(T )(w)
y
d
+ ?
(A)(w)
z
n
+
?
(I)(w)
y
d
,z
n
), where m
(w)
is the observed background
term frequency, ?
y
d
, ?
z
n
and ?
y
d
,z
n
denote the log
frequency deviation representing topic z
n
, facet
y
d
, and the second-order interaction part respec-
tively. Superscripts T ,A and I respectively denote
the index of the topic, facet, and second-order in-
teraction. In our task, we adapt the SAGE model
as follows:
Y = {y
Sentiment
? {positive, negative},
y
Domain
? {hotel, restaurant, doctor},
y
Source
? {employee, turker, customer}}
We model three ??s, one for each type of y. Let
i, j, k denote the index of the different types of y,
so that each term w is drawn as follows:
P (w|i, j, k) ? exp(m
(w)
+ ?
(i)(w)
y
Sentiment
+?
(j)(w)
y
Domain
+ ?
(k)(w)
y
Scource
+ higher order)
where the higher order parts denote the interac-
tions between different facets.
In our approach each document-level feature f
is drawn from the following distribution:
P (f |i, j, k) ? exp(m
(f)
+ ?
(i)(f)
y
Sentiment
+ ?
(j)(f)
y
Domain
+ ?
(k)(f)
y
Scource
+ higher order)
(1)
1569
where m
(f)
can be interpreted as the background
value of feature f . For each review d, the proba-
bility that it is drawn from facets with index i, j, k
is as follows:
P (d|i, j, k) =
?
f?d
P (f |i, j, k)
?
w?d
P (w|i, j, k) (2)
In the training process, parameters ?
(w)
y
and ?
(f)
y
are to be learned by maximizing the posterior
distribution following the original SAGE training
procedure. For prediction, we estimate y
Source
for
each document given all or part of ?
(w)
y
and ?
(f)
y
as follows:
y
Source
=
argmax
y
?
Source
P (d|y
?
Source
, y
Sentiment
, y
Domain
),
where we assume y
Sentiment
and y
Domain
are
given for each document d. Note that we as-
sume conditional independence between features
and words given y, similar to other topic mod-
els (Blei et al, 2003). Notably, our revised SAGE
model degenerates into a model similar to Gen-
eralized Additive Model (Hastie and Tibshirani,
1990) when word features are not considered.
5 Experiments
In this section, we report our experimental results.
We first restrict experiments to the within-domain
task and see what features most characterize the
deceptive reviews, and how. We later extend it to
cross domains to explore a more general classifier
of deceptive opinion spam.
5.1 Intra-Domain Classification
We explore the effect of both domain experts
and crowdsourcing workers on intra-domain de-
ception. Specifically, we reframe it as a intra-
domain multi-class classification task, where
given the labeled training data from one domain,
we learn a classifier to classify reviews accord-
ing to their source, i.e., Employee, Turker and
Customer. Since the machine learning classi-
fier is trained and tested within the same domain,
?
(j)(w)
y
Domain
and ?
(i)(f)
y
Domain
are not considered here.
We use a One-Versus-Rest (OvR) scheme, in
which we train m classifiers using SAGE, such
that each classifier f
i
, for i ? [1,m], is trained to
distinguish between class i on the one hand, and
all classes except i on the other. To make an m-
way decision, we then choose the class c with the
most confident prediction. OvR approaches have
been shown to produce state-of-art performance
compared to other multi-class approaches such as
Multinomial Naive Bayes or One-Versus-One clas-
sification scheme. We train the OvR classifier on
three sets of features, LIWC, Unigram, and POS.
9
Multi-class classification results are given at Ta-
ble 3. We report both OvR performance and the
performance of three One-versus-One binary clas-
sifiers, trained to distinguish between each pair
of classes. In particular, the three-class classifier
is around 65% accurate at distinguishing between
Employee, Customer, and Turker for each of the
domains using Unigram, significantly higher than
random guess. We also observe that each of the
three One-versus-One binary classifications per-
forms significantly better than chance, suggesting
that Employee, Customer, and Turker are in fact
three different classes. In particular, the two-class
classifier is around 0.76 accurate in distinguish-
ing between Turker and Employee reviews, de-
spite both kinds of reviews being deceptive opin-
ion spam.
Best performance is achieved on Unigram fea-
tures, constantly outperforming LIWC and POS
features in both three-class and two-class settings
in the hotel domain. Similar results are observed
for restaurant and doctor domains and details are
excluded for brevity. This suggests that a universal
set of keyword-based deception cues (e.g., LIWC)
is not the best approach for Intra-Domain Classifi-
cation. Similar results were also reported in previ-
ous work (Ott et al, 2012; Ott, 2013).
5.2 Cross-domain Classification
In this subsection, we frame our problem as a
domain adaptation task (Pan and Yang, 2010).
Again, we explore 3 feature sets: LIWC, Uni-
gram and POS. We train a classifier on hotel re-
views, and evaluate the performance on other do-
mains. For simplicity, we focus on truthful (Cus-
tomer) versus deceptive (Turker) binary classifi-
cation rather than a multi-class classification.
We report results from SAGE and SVM
10
in Ta-
ble 4. We first observe that classifiers trained on
hotel reviews apply well in the restaurant domain,
which is reasonable due to the many shared prop-
9
Part-of-speech tags were assigned based on Stan-
ford Parser http://nlp.stanford.edu/software/
lex-parser.shtml
10
We use SVMlight (Joachims, 1999) to train our linear
SVM classifiers
1570
Domain Setting Features
Customer Employee Turker
A P R P R P R
Hotel
Three-Class
Unigram 0.664 0.678 0.669 0.589 0.610 0.641 0.582
LIWC 0.602 0.617 0.613 0.541 0.598 0.590 0.511
POS 0.517 0.532 0.669 0.481 0.479 0.482 0.416
Customer vs Turker
Unigram 0.818 0.812 0.840 - - 0.820 0.809
LIWC 0.764 0.774 0.771 - - 0.723 0.749
POS 0.729 0.748 0.692 - - 0.707 0.759
Customer vs Employee
Unigram 0.799 0.832 0.784 0.804 0.820 - -
LIWC 0.732 0.746 0.751 0.714 0.722 - -
POS 0.728 0.713 0.742 0.707 0.754 - -
Employee vs Turker
Unigram 0.762 - - 0.786 0.806 0.826 0.794
LIWC 0.720 - - 0.728 0.726 0.698 0.739
POS 0.701 - - 0.688 0.710 0.701 0.697
Restaurant
Three-Class
Unigram
0.647 0.692 0.725 0.625 0.648 0.686 0.702
Customer vs Turker 0.817 0.842 0.816 - - 0.804 0.812
Customer vs Employee 0.785 0.790 0.814 0.769 0.826 - -
Employee vs Turker 0.774 - - 0.784 0.804 0.802 0.763
Doctor Customer vs Turker 0.745 0.772 0.701 - - 0.752 0.718
Table 3: Within-domain multi-class classifier performance.
Model Features Domain A P R F1 Domain A P R F1
SVM
Unigram Restaurant 0.785 0.813 0.742 0.778 Doctor 0.550 0.537 0.725 0.617
LIWC Restaurant 0.745 0.692 0.840 0.759 Doctor 0.521 0.512 0.965 0.669
POS Restaurant 0.735 0.697 0.815 0.751 Doctor 0.540 0.521 0.975 0.679
SAGE
Unigram Restaurant 0.770 0.793 0.750 0.784 Doctor 0.520 0.547 0.705 0.616
LIWC Restaurant 0.742 0.728 0.749 0.738 Doctor 0.647 0.650 0.608 0.628
POS Restaurant 0.746 0.732 0.687 0.701 Doctor 0.634 0.623 0.682 0.651
Table 4: Classifier performance in cross-domain adaptation.
erties among restaurants and hotels. Among three
types of features, Unigram still performs best.
POS and LIWC features are also robust across do-
mains.
In the doctor domain, we observe that models
trained on Unigram features from the hotels do-
main do not generalize well to doctor reviews, and
the performance is a little bit better than random
guess with only 0.55 accuracy. For SVM, models
trained on POS and LIWC features achieve even
lower accuracy than Unigram. POS and LIWC
features obtain around 0.5 precision and 1.0 re-
call, indicating that all doctor reviews are classi-
fied as deceptive by the classifier. One plausible
explanation could be doctor reviews generally en-
code some type of positive-weighted (deceptive)
features more than hotel reviews and these types
of features dominate the decision making proce-
dures, leading all reviews to be classified as de-
ceptive.
Tables 5 and 6 give the top weighted LIWC and
POS features. We observe that many features are
indeed shared among doctor and hotel domains.
Notably, POS features are more robust than LIWC
as more shared features are observed. As domain
specific properties will be considered in the in-
teraction part (?
LIWC
domain
and ?
POS
domain
) of the addi-
LIWC (hotel) LIWC (doctor)
deceptive truthful deceptive truthful
i AllPct Sixletters present
family number past AllPct
pronoun hear work social
Sixletters we health shehe
see space i number
posemo dash friend time
certain human posemo we
leisure exclusive feel you
future past perceptual negemo
perceptual home leisure Period
feel otherpunct insight relativ
comma negemo comma ingest
cause dash future money
Table 5: Top weighted LIWC features for Turker
vs Customer in Doctor and Hotel reviews. Blue
denotes shared positive (deceptive) features and
red denotes negative (truthful) features.
tive model, SAGE achieve much better results than
SVM, and is around 0.65 accurate in the cross-
domain task.
6 General Linguistic Cues of Deceptive
Opinion Spam
In this section, we examine a number of general
POS and LIWC features that may shed light on
a general rule for identifying deceptive opinion
1571
Figure 1: Visualization of the ? for POS features: Horizontal axes correspond to the values ? and are
NORMALIZED from the log-frequency function.
POS (hotel) POS (doctor)
deceptive truthful deceptive truthful
PRP$ CD VBD CD
PRP RRB NNP VBZ
VB LRB VB VBP
TO CC TO FW
NNP NNS VBG RRB
VBG RP PRP$ LRB
MD VBN JJS RB
VBP IN JJ LS
RB EX WRB PDT
JJS VBZ PRP VBN
Table 6: Top weighted POS features for Turker vs
Customer in Doctor and Hotel reviews. Blue de-
notes shared positive (deceptive) features and red
denotes negative (truthful) features.
spam. Our modified SAGE model provides us
with a tailored tool for this analysis. Specifically,
each feature f is associated with a background
valuem
f
. For each facetA, ?
f
A
, presents the facet-
specific preference value for feature f . Note that
sentiments are separated into positive and negative
dimensions, which is necessary because hotel em-
ployee authors wrote positive-sentiment reviews
when reviewing their own hotels, and negative-
sentiment reviews when reviewing their competi-
tors? hotels.
6.1 POS features
Early findings in the literature (Rayson et al,
2001; Buller and Burgoon, 1996; Biber et al,
1999) found that informative (truthful) writings
typically consist of more nouns, adjectives, prepo-
sitions, determiners, and coordinating conjunc-
tions, while imaginative (deceptive) writing con-
sist of more verbs, adverbs, pronouns, and pre-
determiners (with a few exceptions). Our find-
ings with POS features are largely in agreement
with these findings when distinguishing between
Turker and Customer reviews, but are violated in
the Employee set.
We present the eight types of POS features in
Figure 1, namely, N (Noun), JJ (Adjective), IN
(Preposition or subordinating conjunction) and DT
(Determiner), V (Verb), RB (Adverb), PRP (Pro-
nouns, both personal and possessive) and PDT
(Pre-Determiner).
From Figures 1(a)(b)(e)(f), we observe that with
the exception of PDT, the word frequency of
which is too small to draw a conclusion, Turker
and Customer reviews exhibit linguistic patterns in
agreement with previous findings in the literature,
where truthful reviews (Customer) tend to include
more N, JJ, IN and DT, while deceptive writings
tend to encode more V, RB and PRP.
However, in the case of the Employee-Positive
dataset, which is equally deceptive, most of these
rules are violated. Notably, reviews from the
Employee-Positive set did not encode fewer N, JJ
and DT terms, as expected (see Figures 1(a)(c)).
Instead, they encode even more N, JJ and DT
vocabularies than truthful reviews from the Cus-
tomer reviews. Also, fewer V and RB are found
in Employee-Positive reviews compared with Cus-
tomer reviews (see Figures 1(e)(g)).
One explanation for these observations is that
informative (truthful) writing tends to be more in-
troductory and descriptive, encoding more con-
crete details, when compared with imaginary writ-
ings. As domain experts possess considerable
knowledge of their own offerings, they highlight
1572
Figure 2: Visualization of the ? for LIWC features: Horizontal axes correspond to the values ? and are
normalized from the log-frequency function.
the details and their lies may be even more in-
formative and descriptive than those generated by
real customers! This explains why Employee-
Positive contains more N, IN and DT. Meanwhile,
as domain experts are engaged more in talking
about the details, they inevitably overlook other
information, possibly leading to fewer V and RB.
For Employee-Positive reviews, shown in Fig-
ures 1(d)(h), it turns out that domain experts do
not compensate for their lack of prior experience
when writing negative reviews for competitors? of-
ferings, as we will see again with LIWC features
in the next subsection.
6.2 LIWC features
We explore 3 LIWC categories (from left to right
in subfigures of Figure 2): sentiment (neg emo and
pos emo), spatial detail (space), and first-person
singular pronouns (first-person).
Space: Note that spatial details are more spe-
cific in the Hotel and Restaurant domains,
which is reflected in the high positive value of
?
Hotel,space
domain
(see Figure 2(g)) and negative value
of ?
Doctor,space
domain
(see Figure 2(h)). It illustrates how
domain-specific details can be predictive of decep-
tive text. Similarly predictive LIWC features are
home for the Hotel domain, ingest for the Restau-
rant domain, and health and body for the Doctor
domain.
In Figure 2(i)(j)(k)(l), we can easily see that
both actual customers and domain experts encode
more spatial details in their reviews (positive value
of ?), which is in agreement with our expectation.
This further demonstrates that a lack of spatial de-
tails would not be a general cue for deception.
Moreover, it appears that general domain expertise
does not compensate for the lack of prior experi-
ence when writing deceptive negative reviews for
competitors? hotels, as demonstrated by the lack
of spatial details in the negative-sentiment reviews
by employees shown in Figure 2(k).
Sentiment: According to our findings, the pres-
ence of sentiment is a general cue to deceptive
opinion spam, as observed when comparing Fig-
ure 2(b) to Figure 2(c) and (d). Participants, both
Employees and Turkers, tend to exaggerate senti-
ment, and include more sentiment-related vocabu-
laries in their lies. In other words, positive decep-
tive reviews were generally more positive and neg-
ative deceptive reviews were more negative in sen-
timent when compared with the truthful reviews
generated by actual customers. A similar pattern
can also be observed when comparing Figure 2(i)
to Figure 2(j).
1573
First-Person Singular Pronouns: The litera-
ture also associates deception with decreased us-
age of first-person singular pronouns, an effect at-
tributed to psychological distancing, whereby de-
ceivers talk less about themselves due either to a
lack of personal experience, or to detach them-
selves from the lie (Newman et al, 2003; Zhou
et al, 2004; Buller et al, 1996; Knapp and Co-
maden, 1979). However, according to our find-
ings, we find the opposite to hold. Increased first
person singular is an apparent indicator of decep-
tion, when comparing Figure 2(b) to 2(c) and 2(e).
We suspect that this relates to an effect observed
in previous studies of deception, where liars inad-
vertently undermine their lies by overemphasizing
aspects of their deception that they believe reflect
credibility (Bond and DePaulo, 2006; DePaulo et
al., 2003). One interpretation for this phenomenon
would be that deceivers try to overemphasize their
physical presence because they believe that this in-
creases their credibility.
7 Conclusion and Discussion
In this work, we have developed a multi-domain
large-scale dataset containing gold-standard de-
ceptive opinion spam. It includes reviews of Ho-
tels, Restaurants and Doctors, generated through
crowdsourcing and domain experts. We study this
data using SAGE, which enables us to make ob-
servations about the respects in which truthful and
deceptive text differs. Our model includes sev-
eral domain-independent features that shed light
on these differences, which further allows us to
formulate some general rules for recognizing de-
ceptive opinion spam.
We also acknowledge several important caveats
to this work. By soliciting fake reviews from par-
ticipants, including crowd workers and domain
experts, we have found that is possible to de-
tect fake reviews with above-chance accuracy, and
have used our models to explore several psycho-
logical theories of deception. However, it is still
very difficult to estimate the practical impact of
such methods, as it is very challenging to obtain
gold-standard data in the real world. Moreover,
by soliciting deceptive opinion spam in an arti-
ficial environment, we are endorsing the decep-
tion, which may influence the cues that we ob-
serve (Feeley and others, 1998; Frank and Ekman,
1997; Newman et al, 2003; Ott, 2013). Finally, it
may be possible to train people to tell more con-
vincing lies. Many of the characteristics regard-
ing fake review generation might be overcome by
well-trained fake review writers, which would re-
sults in opinion spam that is harder for detect. Fu-
ture work may wish to consider some of these ad-
ditional challenges.
8 Acknowledgement
We thank Wenjie Li and Xun Wang for useful dis-
cussions and suggestions. This work was sup-
ported in part by National Science Foundation
Grant BCS-0904822, a DARPA Deft grant, as well
as a gift from Google. We also thank the ACL re-
viewers for their helpful comments and advice.
References
Douglas Biber, Stig Johansson, Geoffrey Leech, Su-
san Conrad, Edward Finegan, and Randolph Quirk.
1999. Longman grammar of spoken and written En-
glish, volume 2. MIT Press.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. the Journal of machine
Learning research, 3:993?1022.
Charles Bond and Bella DePaulo. 2006. Accuracy of
deception judgments. Personality and Social Psy-
chology Review, 10(3):214?234.
David B Buller and Judee K Burgoon. 1996. Inter-
personal deception theory. Communication theory,
6(3):203?242.
David B Buller, Judee K Burgoon, Aileen Buslig, and
James Roiger. 1996. Testing interpersonal decep-
tion theory: The language of interpersonal decep-
tion. Communication theory, 6(3):268?289.
Paul-Alexandru Chirita, J?org Diederich, and Wolfgang
Nejdl. 2005. Mailrank: using ranking for spam
detection. In Proceedings of the 14th ACM inter-
national conference on Information and knowledge
management, pages 373?380. ACM.
Cone. 2011. 2011 Online Influence Trend Tracker.
http://www.coneinc.com/negative-reviews-online-
reverse-purchase-decisions, August.
Bella DePaulo, James Lindsay, Brian Malone, Laura
Muhlenbruck, Kelly Charlton, and Harris Cooper.
2003. Cues to deception. Psychological bulletin,
129(1):74.
Harris Drucker, Donghui Wu, and Vladimir Vapnik.
1999. Support vector machines for spam catego-
rization. Neural Networks, IEEE Transactions on,
10(5):1048?1054.
1574
Jacob Eisenstein, Amr Ahmed, and Eric P Xing. 2011.
Sparse additive generative models of text. In Pro-
ceedings of the 28th International Conference on
Machine Learning (ICML-11), pages 1041?1048.
Thomas Feeley. 1998. The behavioral correlates of
sanctioned and unsanctioned deceptive communica-
tion. Journal of Nonverbal Behavior, 22(3):189?
204.
Vanessa Feng and Graeme Hirst. 2013. Detecting de-
ceptive opinions with profile compatibility. In Pro-
ceedings of the 6th International Joint Conference
on Natural Language Processing, Nagoya, Japan,
pages 14?18.
Song Feng, Ritwik Banerjee, and Yejin Choi. 2012.
Syntactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics: Short
Papers-Volume 2, pages 171?175. Association for
Computational Linguistics.
Mark Frank and Paul Ekman. 1997. The ability to de-
tect deceit generalizes across different types of high-
stake lies. Journal of personality and social psychol-
ogy, 72(6):1429.
Zolt?an Gy?ongyi, Hector Garcia-Molina, and Jan Ped-
ersen. 2004. Combating web spam with trustrank.
In Proceedings of the Thirtieth international con-
ference on Very large data bases-Volume 30, pages
576?587. VLDB Endowment.
Trevor J Hastie and Robert J Tibshirani. 1990. Gener-
alized additive models, volume 43. CRC Press.
Ipsos. 2012. Socialogue: Five Stars? Thumbs Up? A+
or Just Average? http://www.ipsos-na.com/news-
polls/pressrelease.aspx?id=5929.
Nitin Jindal and Bing Liu. 2008. Opinion spam and
analysis. In Proceedings of the international con-
ference on Web search and web data mining, pages
219?230. ACM.
Nitin Jindal, Bing Liu, and Ee-Peng Lim. 2010. Find-
ing unusual review patterns using unexpected rules.
In Proceedings of the 19th ACM international con-
ference on Information and knowledge management,
pages 1549?1552. ACM.
Thorsten Joachims. 1999. Making large scale svm
learning practical.
Marcia K Johnson and Carol L Raye. 1981. Reality
monitoring. Psychological review, 88(1):67.
Mark Knapp and Mark Comaden. 1979. Telling it like
it isn?t: A review of theory and research on decep-
tive communications. Human Communication Re-
search, 5(3):270?285.
Jiwei Li, Claire Cardie, and Sujian Li. 2013a. Top-
icspam: a topic-model-based approach for spam de-
tection. In Proceedings of the 51th Annual Meeting
of the Association for Computational Linguis-tics.
Jiwei Li, Myle Ott, and Claire Cardie. 2013b. Iden-
tifying manipulated offerings on review portals. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, Seattle,
Wash, pages 18?21.
Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,
and Hady Wirawan Lauw. 2010. Detecting prod-
uct review spammers using rating behaviors. In Pro-
ceedings of the 19th ACM international conference
on Information and knowledge management, pages
939?948. ACM.
Juan Martinez-Romo and Lourdes Araujo. 2009. Web
spam identification through language model analy-
sis. In Proceedings of the 5th International Work-
shop on Adversarial Information Retrieval on the
Web, pages 21?28. ACM.
David Meyer. 2009. Fake reviews prompt belkin apol-
ogy.
Claire Miller. 2009. Company settles case of reviews
it faked. New York Times.
Arjun Mukherjee, Bing Liu, Junhui Wang, Natalie
Glance, and Nitin Jindal. 2011. Detecting group
review spam. In Proceedings of the 20th interna-
tional conference companion on World wide web,
pages 93?94. ACM.
Arjun Mukherjee, Bing Liu, and Natalie Glance. 2012.
Spotting fake reviewer groups in consumer reviews.
In Proceedings of the 21st international conference
on World Wide Web, pages 191?200. ACM.
Arjun Mukherjee, Abhinav Kumar, Bing Liu, Junhui
Wang, Meichun Hsu, Malu Castellanos, and Riddhi-
man Ghosh. 2013a. Spotting opinion spammers us-
ing behavioral footprints. In Proceedings of the 19th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 632?640.
ACM.
Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and
Natalie Glance. 2013b. What yelp fake review fil-
ter might be doing. In Seventh International AAAI
Conference on Weblogs and Social Media.
Matthew L Newman, James W Pennebaker, Diane S
Berry, and Jane M Richards. 2003. Lying words:
Predicting deception from linguistic styles. Person-
ality and social psychology bulletin, 29(5):665?675.
Alexandros Ntoulas, Marc Najork, Mark Manasse, and
Dennis Fetterly. 2006. Detecting spam web pages
through content analysis. In Proceedings of the 15th
international conference on World Wide Web, pages
83?92. ACM.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.
Hancock. 2011. Finding deceptive opinion spam
by any stretch of the imagination. In Proceedings
of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 309?319.
1575
Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Esti-
mating the prevalence of deception in online review
communities. In Proceedings of the 21st interna-
tional conference on World Wide Web, pages 201?
210. ACM.
Myle Ott, Claire Cardie, and Jeffrey T. Hancock. 2013.
Negative deceptive opinion spam. In Proceedings of
the 2013 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Short Papers, At-
lanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Myle Ott. 2013. Computational lingustic models of
deceptive opinion spam. PHD, thesis.
Sinno Pan and Qiang Yang. 2010. A survey on transfer
learning. Knowledge and Data Engineering, IEEE
Transactions on, 22(10):1345?1359.
Tieyun Qian and Bing Liu. 2013. Identifying multiple
userids of the same author. In Proceedings of the
2013 Conference on Empirical Methods in Natural
Language Processing, Seattle, Wash, pages 18?21.
Paul Rayson, Andrew Wilson, and Geoffrey Leech.
2001. Grammatical word class variation within
the british national corpus sampler. Language and
Computers, 36(1):295?306.
David Streitfeld. 2012. For 2 a star, an online retailer
gets 5-star product reviews. New York Times., 26.
Alexandra Topping. 2010. Historian orlando figes
agrees to pay damages for fake reviews. The
Guardian., 16.
Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.
2011. Review graph based online store review
spammer detection. In Data Mining (ICDM),
2011 IEEE 11th International Conference on, pages
1242?1247. IEEE.
Guan Wang, Sihong Xie, Bing Liu, and Philip Yu.
2012. Identify online store review spammers via so-
cial review graph. ACM Transactions on Intelligent
Systems and Technology (TIST), 3(4):61.
Guangyu Wu, Derek Greene, Barry Smyth, and P?adraig
Cunningham. 2010. Distortion as a validation cri-
terion in the identification of suspicious reviews. In
Proceedings of the First Workshop on Social Media
Analytics, pages 10?13. ACM.
Kyung-Hyan Yoo and Ulrike Gretzel. 2009. Com-
parison of deceptive and truthful travel reviews.
In Information and communication technologies in
tourism 2009, pages 37?47. Springer.
Lina Zhou, Judee K Burgoon, Douglas P Twitchell,
Tiantian Qin, and Jay F Nunamaker Jr. 2004. A
comparison of classification methods for predict-
ing deception in computer-mediated communica-
tion. Journal of Management Information Systems,
20(4):139?166.
1576
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 24?29,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
An Extension of BLANC to System Mentions
Xiaoqiang Luo
Google Inc.
111 8th Ave, New York, NY 10011
xql@google.com
Sameer Pradhan
Harvard Medical School
300 Longwood Ave., Boston, MA 02115
sameer.pradhan@childrens.harvard.edu
Marta Recasens
Google Inc.
1600 Amphitheatre Pkwy,
Mountain View, CA 94043
recasens@google.com
Eduard Hovy
Carnegie Mellon University
5000 Forbes Ave.
Pittsburgh, PA 15213
hovy@cmu.edu
Abstract
BLANC is a link-based coreference eval-
uation metric for measuring the qual-
ity of coreference systems on gold men-
tions. This paper extends the original
BLANC (?BLANC-gold? henceforth) to
system mentions, removing the gold men-
tion assumption. The proposed BLANC
falls back seamlessly to the original one if
system mentions are identical to gold men-
tions, and it is shown to strongly correlate
with existing metrics on the 2011 and 2012
CoNLL data.
1 Introduction
Coreference resolution aims at identifying natu-
ral language expressions (or mentions) that refer
to the same entity. It entails partitioning (often
imperfect) mentions into equivalence classes. A
critically important problem is how to measure the
quality of a coreference resolution system. Many
evaluation metrics have been proposed in the past
two decades, including the MUC measure (Vilain
et al, 1995), B-cubed (Bagga and Baldwin, 1998),
CEAF (Luo, 2005) and, more recently, BLANC-
gold (Recasens and Hovy, 2011). B-cubed and
CEAF treat entities as sets of mentions and mea-
sure the agreement between key (or gold standard)
entities and response (or system-generated) enti-
ties, while MUC and BLANC-gold are link-based.
In particular, MUC measures the degree of
agreement between key coreference links (i.e.,
links among mentions within entities) and re-
sponse coreference links, while non-coreference
links (i.e., links formed by mentions from different
entities) are not explicitly taken into account. This
leads to a phenomenon where coreference systems
outputting large entities are scored more favorably
than those outputting small entities (Luo, 2005).
BLANC (Recasens and Hovy, 2011), on the other
hand, considers both coreference links and non-
coreference links. It calculates recall, precision
and F-measure separately on coreference and non-
coreference links in the usual way, and defines
the overall recall, precision and F-measure as the
mean of the respective measures for coreference
and non-coreference links.
The BLANC-gold metric was developed with
the assumption that response mentions and key
mentions are identical. In reality, however, men-
tions need to be detected from natural language
text and the result is, more often than not, im-
perfect: some key mentions may be missing in
the response, and some response mentions may be
spurious?so-called ?twinless? mentions by Stoy-
anov et al (2009). Therefore, the identical-
mention-set assumption limits BLANC-gold?s ap-
plicability when gold mentions are not available,
or when one wants to have a single score mea-
suring both the quality of mention detection and
coreference resolution. The goal of this paper is
to extend the BLANC-gold metric to imperfect re-
sponse mentions.
We first briefly review the original definition of
BLANC, and rewrite its definition using set nota-
tion. We then argue that the gold-mention assump-
tion in Recasens and Hovy (2011) can be lifted
without changing the original definition. In fact,
the proposed BLANC metric subsumes the origi-
nal one in that its value is identical to the original
one when response mentions are identical to key
mentions.
The rest of the paper is organized as follows.
We introduce the notions used in this paper in
Section 2. We then present the original BLANC-
gold in Section 3 using the set notation defined in
Section 2. This paves the way to generalize it to
24
imperfect system mentions, which is presented in
Section 4. The proposed BLANC is applied to the
CoNLL 2011 and 2012 shared task participants,
and the scores and its correlations with existing
metrics are shown in Section 5.
2 Notations
To facilitate the presentation, we define the nota-
tions used in the paper.
We use key to refer to gold standard mentions or
entities, and response to refer to system mentions
or entities. The collection of key entities is denoted
by K = {k
i
}
|K|
i=1
, where k
i
is the i
th
key entity;
accordingly, R = {r
j
}
|R|
j=1
is the set of response
entities, and r
j
is the j
th
response entity. We as-
sume that mentions in {k
i
} and {r
j
} are unique;
in other words, there is no duplicate mention.
Let C
k
(i) and C
r
(j) be the set of coreference
links formed by mentions in k
i
and r
j
:
C
k
(i) = {(m
1
,m
2
) : m
1
? k
i
,m
2
? k
i
,m
1
6= m
2
}
C
r
(j) = {(m
1
,m
2
) : m
1
? r
j
,m
2
? r
j
,m
1
6= m
2
}
As can be seen, a link is an undirected edge be-
tween two mentions, and it can be equivalently
represented by a pair of mentions. Note that when
an entity consists of a single mention, its corefer-
ence link set is empty.
Let N
k
(i, j) (i 6= j) be key non-coreference
links formed between mentions in k
i
and those
in k
j
, and let N
r
(i, j) (i 6= j) be response non-
coreference links formed between mentions in r
i
and those in r
j
, respectively:
N
k
(i, j) = {(m
1
,m
2
) : m
1
? k
i
,m
2
? k
j
}
N
r
(i, j) = {(m
1
,m
2
) : m
1
? r
i
,m
2
? r
j
}
Note that the non-coreference link set is empty
when all mentions are in the same entity.
We use the same letter and subscription with-
out the index in parentheses to denote the union of
sets, e.g.,
C
k
= ?
i
C
k
(i), N
k
= ?
i 6=j
N
k
(i, j)
C
r
= ?
j
C
r
(j), N
r
= ?
i6=j
N
r
(i, j)
We use T
k
= C
k
? N
k
and T
r
= C
r
? N
r
to
denote the total set of key links and total set of
response links, respectively. Clearly, C
k
and N
k
form a partition of T
k
since C
k
? N
k
= ?, T
k
=
C
k
?N
k
. Likewise, C
r
and N
r
form a partition of
T
r
.
We say that a key link l
1
? T
k
equals a response
link l
2
? T
r
if and only if the pair of mentions
from which the links are formed are identical. We
write l
1
= l
2
if two links are equal. It is easy to
see that the gold mention assumption?same set
of response mentions as the set of key mentions?
can be equivalently stated as T
k
= T
r
(this does
not necessarily mean that C
k
= C
r
or N
k
= N
r
).
We also use | ? | to denote the size of a set.
3 Original BLANC
BLANC-gold is adapted from Rand Index (Rand,
1971), a metric for clustering objects. Rand Index
is defined as the ratio between the number of cor-
rect within-cluster links plus the number of correct
cross-cluster links, and the total number of links.
When T
k
= T
r
, Rand Index can be applied di-
rectly since coreference resolution reduces to a
clustering problem where mentions are partitioned
into clusters (entities):
Rand Index =
|C
k
? C
r
|+ |N
k
?N
r
|
1
2
(
|T
k
|(|T
k
| ? 1)
)
(1)
In practice, though, the simple-minded adoption
of Rand Index is not satisfactory since the number
of non-coreference links often overwhelms that of
coreference links (Recasens and Hovy, 2011), or,
|N
k
|  |C
k
| and |N
r
|  |C
r
|. Rand Index, if
used without modification, would not be sensitive
to changes of coreference links.
BLANC-gold solves this problem by averaging
the F-measure computed over coreference links
and the F-measure over non-coreference links.
Using the notations in Section 2, the recall, pre-
cision, and F-measure on coreference links are:
R
(g)
c
=
|C
k
? C
r
|
|C
k
? C
r
|+ |C
k
?N
r
|
(2)
P
(g)
c
=
|C
k
? C
r
|
|C
r
? C
k
|+ |C
r
?N
k
|
(3)
F
(g)
c
=
2R
(g)
c
P
(g)
c
R
(g)
c
+ P
(g)
c
; (4)
Similarly, the recall, precision, and F-measure on
non-coreference links are computed as:
R
(g)
n
=
|N
k
?N
r
|
|N
k
? C
r
|+ |N
k
?N
r
|
(5)
P
(g)
n
=
|N
k
?N
r
|
|N
r
? C
k
|+ |N
r
?N
k
|
(6)
F
(g)
n
=
2R
(g)
n
P
(g)
n
R
(g)
n
+ P
(g)
n
. (7)
25
Finally, the BLANC-gold metric is the arithmetic
average of F
(g)
c
and F
(g)
n
:
BLANC
(g)
=
F
(g)
c
+ F
(g)
n
2
. (8)
Superscript
g
in these equations highlights the fact
that they are meant for coreference systems with
gold mentions.
Eqn. (8) indicates that BLANC-gold assigns
equal weight to F
(g)
c
, the F-measure from coref-
erence links, and F
(g)
n
, the F-measure from non-
coreference links. This avoids the problem that
|N
k
|  |C
k
| and |N
r
|  |C
r
|, should the original
Rand Index be used.
In Eqn. (2) - (3) and Eqn. (5) - (6), denominators
are written as a sum of disjoint subsets so they can
be related to the contingency table in (Recasens
and Hovy, 2011). Under the assumption that T
k
=
T
r
, it is clear that C
k
= (C
k
? C
r
) ? (C
k
?N
r
),
C
r
= (C
k
? C
r
) ? (N
k
? C
r
), and so on.
4 BLANC for Imperfect Response
Mentions
Under the assumption that the key and response
mention sets are identical (which implies that
T
k
= T
r
), Equations (2) to (7) make sense. For
example, R
c
is the ratio of the number of correct
coreference links over the number of key corefer-
ence links; P
c
is the ratio of the number of cor-
rect coreference links over the number of response
coreference links, and so on.
However, when response mentions are not iden-
tical to key mentions, a key coreference link may
not appear in either C
r
or N
r
, so Equations (2) to
(7) cannot be applied directly to systems with im-
perfect mentions. For instance, if the key entities
are {a,b,c} {d,e}; and the response entities
are {b,c} {e,f,g}, then the key coreference
link (a,b) is not seen on the response side; sim-
ilarly, it is possible that a response link does not
appear on the key side either: (c,f) and (f,g)
are not in the key in the above example.
To account for missing or spurious links, we ob-
serve that
? C
k
\ T
r
are key coreference links missing in
the response;
? N
k
\ T
r
are key non-coreference links miss-
ing in the response;
? C
r
\ T
k
are response coreference links miss-
ing in the key;
? N
r
\ T
k
are response non-coreference links
missing in the key,
and we propose to extend the coreference F-
measure and non-coreference F-measure as fol-
lows. Coreference recall, precision and F-measure
are changed to:
R
c
=
|C
k
? C
r
|
|C
k
? C
r
|+ |C
k
?N
r
|+ |C
k
\ T
r
|
(9)
P
c
=
|C
k
? C
r
|
|C
r
? C
k
|+ |C
r
?N
k
|+ |C
r
\ T
k
|
(10)
F
c
=
2R
c
P
c
R
c
+ P
c
(11)
Non-coreference recall, precision and F-measure
are changed to:
R
n
=
|N
k
?N
r
|
|N
k
? C
r
|+ |N
k
?N
r
|+ |N
k
\ T
r
|
(12)
P
n
=
|N
k
?N
r
|
|N
r
? C
k
|+ |N
r
?N
k
|+ |N
r
\ T
k
|
(13)
F
n
=
2R
n
P
n
R
n
+ P
n
. (14)
The proposed BLANC continues to be the arith-
metic average of F
c
and F
n
:
BLANC =
F
c
+ F
n
2
. (15)
We observe that the definition of the proposed
BLANC, Equ. (9)-(14) subsume the BLANC-
gold (2) to (7) due to the following proposition:
If T
k
= T
r
, then BLANC = BLANC
(g)
.
Proof. We only need to show that R
c
= R
(g)
c
,
P
c
= P
(g)
c
, R
n
= R
(g)
n
, and P
n
= P
(g)
n
. We prove
the first one (the other proofs are similar and elided
due to space limitations). Since T
k
= T
r
and
C
k
? T
k
, we have C
k
? T
r
; thus C
k
\T
r
= ?, and
|C
k
? T
r
| = 0. This establishes that R
c
= R
(g)
c
.
Indeed, since C
k
is a union of three disjoint sub-
sets: C
k
= (C
k
? C
r
) ? (C
k
? N
r
) ? (C
k
\ T
r
),
R
(g)
c
and R
c
can be unified as
|C
k
?C
r
|
|C
K
|
. Unification
for other component recalls and precisions can be
done similarly. So the final definition of BLANC
can be succinctly stated as:
R
c
=
|C
k
? C
r
|
|C
k
|
, P
c
=
|C
k
? C
r
|
|C
r
|
(16)
R
n
=
|N
k
?N
r
|
|N
k
|
, P
n
=
|N
k
?N
r
|
|N
r
|
(17)
F
c
=
2|C
k
? C
r
|
|C
k
|+ |C
r
|
, F
n
=
2|N
k
?N
r
|
|N
k
|+ |N
r
|
(18)
BLANC =
F
c
+ F
n
2
(19)
26
4.1 Boundary Cases
Care has to be taken when counts of the BLANC
definition are 0. This can happen when all key
(or response) mentions are in one cluster or are
all singletons: the former case will lead to N
k
= ?
(or N
r
= ?); the latter will lead to C
k
= ? (or
C
r
= ?). Observe that as long as |C
k
|+ |C
r
| > 0,
F
c
in (18) is well-defined; as long as |N
k
|+|N
r
| >
0, F
n
in (18) is well-defined. So we only need to
augment the BLANC definition for the following
cases:
(1) If C
k
= C
r
= ? and N
k
= N
r
= ?, then
BLANC = I(M
k
= M
r
), where I(?) is an in-
dicator function whose value is 1 if its argument
is true, and 0 otherwise. M
k
and M
r
are the key
and response mention set. This can happen when a
document has no more than one mention and there
is no link.
(2) If C
k
= C
r
= ? and |N
k
| + |N
r
| > 0, then
BLANC = F
n
. This is the case where the key
and response side has only entities consisting of
singleton mentions. Since there is no coreference
link, BLANC reduces to the non-coreference F-
measure F
n
.
(3) If N
k
= N
r
= ? and |C
k
| + |C
r
| > 0, then
BLANC = F
c
. This is the case where all mentions
in the key and response are in one entity. Since
there is no non-coreference link, BLANC reduces
to the coreference F-measure F
c
.
4.2 Toy Examples
We walk through a few examples and show how
BLANC is calculated in detail. In all the examples
below, each lower-case letter represents a mention;
mentions in an entity are closed in {}; two letters
in () represent a link.
Example 1. Key entities are {abc} and {d}; re-
sponse entities are {bc} and {de}. Obviously,
C
k
= {(ab), (bc), (ac)};
N
k
= {(ad), (bd), (cd)};
C
r
= {(bc), (de)};
N
r
= {(bd), (be), (cd), (ce)}.
Therefore, C
k
? C
r
= {(bc)}, N
k
? N
r
=
{(bd), (cd)}, and R
c
=
1
3
, P
c
=
1
2
, F
c
=
2
5
; R
n
=
2
3
, P
n
=
2
4
, F
n
=
4
7
. Finally, BLANC =
17
35
.
Example 2. Key entity is {a}; response entity
is {b}. This is boundary case (1): BLANC = 0.
Example 3. Key entities are {a}{b}{c}; re-
sponse entities are {a}{b}{d}. This is boundary
case (2): there are no coreference links. Since
N
k
= {(ab), (bc), (ca)},
Participant R P BLANC
lee 50.23 49.28 48.84
sapena 40.68 49.05 44.47
nugues 47.83 44.22 45.95
chang 44.71 47.48 45.49
stoyanov 49.37 29.80 34.58
santos 46.74 37.33 41.33
song 36.88 39.69 30.92
sobha 35.42 39.56 36.31
yang 47.95 29.12 36.09
charton 42.32 31.54 35.65
hao 45.41 32.75 36.98
zhou 29.93 45.58 34.95
kobdani 32.29 33.01 32.57
xinxin 36.83 34.39 35.02
kummerfeld 34.84 29.53 30.98
zhang 30.10 43.96 35.71
zhekova 26.40 15.32 15.37
irwin 3.62 28.28 6.28
Table 1: The proposed BLANC scores of the
CoNLL-2011 shared task participants.
N
r
= {(ab), (bd), (ad)},
we have
N
k
?N
r
= {(ab)}, and R
n
=
1
3
, P
n
=
1
3
.
So BLANC = F
n
=
1
3
.
Example 4. Key entity is {abc}; response entity
is {bc}. This is boundary case (3): there are no
non-coreference links. Since
C
k
= {(ab), (bc), (ca)}, and C
r
= {(bc)},
we have
C
k
? C
r
= {(bc)}, and R
c
=
1
3
, P
c
= 1,
So BLANC = F
c
=
2
4
=
1
2
.
5 Results
5.1 CoNLL-2011/12
We have updated the publicly available CoNLL
coreference scorer
1
with the proposed BLANC,
and used it to compute the proposed BLANC
scores for all the CoNLL 2011 (Pradhan et al,
2011) and 2012 (Pradhan et al, 2012) participants
in the official track, where participants had to au-
tomatically predict the mentions. Tables 1 and 2
report the updated results.
2
5.2 Correlation with Other Measures
Figure 1 shows how the proposed BLANC mea-
sure works when compared with existing met-
rics such as MUC, B-cubed and CEAF, us-
ing the BLANC and F1 scores. The proposed
BLANC is highly positively correlated with the
1
http://code.google.com/p/reference-coreference-scorers
2
The order is kept the same as in Pradhan et al (2011) and
Pradhan et al (2012) for easy comparison.
27
Participant R P BLANC
Language: Arabic
fernandes 33.43 44.66 37.99
bjorkelund 32.65 45.47 37.93
uryupina 31.62 35.26 33.02
stamborg 32.59 36.92 34.50
chen 31.81 31.52 30.82
zhekova 11.04 62.58 18.51
li 4.60 56.63 8.42
Language: English
fernandes 54.91 63.66 58.75
martschat 52.00 58.84 55.04
bjorkelund 52.01 59.55 55.42
chang 52.85 55.03 53.86
chen 50.52 56.82 52.87
chunyang 51.19 55.47 52.65
stamborg 54.39 54.88 54.42
yuan 50.58 54.29 52.11
xu 45.99 54.59 46.47
shou 49.55 52.46 50.44
uryupina 44.15 48.89 46.04
songyang 40.60 50.85 45.10
zhekova 41.46 33.13 34.80
xinxin 44.39 32.79 36.54
li 25.17 52.96 31.85
Language: Chinese
chen 48.45 62.44 54.10
yuan 53.15 40.75 43.20
bjorkelund 47.58 45.93 44.22
xu 44.11 36.45 38.45
fernandes 42.36 61.72 49.63
stamborg 39.60 55.12 45.89
uryupina 33.44 56.01 41.88
martschat 27.24 62.33 37.89
chunyang 37.43 36.18 36.77
xinxin 36.46 39.79 37.85
li 21.61 62.94 30.37
chang 18.74 40.76 25.68
zhekova 21.50 37.18 22.89
Table 2: The proposed BLANC scores of the
CoNLL-2012 shared task participants.
R P F1
MUC 0.975 0.844 0.935
B-cubed 0.981 0.942 0.966
CEAF-m 0.941 0.923 0.966
CEAF-e 0.797 0.781 0.919
Table 3: Pearson?s r correlation coefficients be-
tween the proposed BLANC and the other coref-
erence measures based on the CoNLL 2011/2012
results. All p-values are significant at < 0.001.
l
lll
l
l
l
llll lll
l
l
l
l
ll
l
l
lll
l
l l
llllll
lll
ll l
l
lll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
MUC
BLA
NC
l
lll
l
l
l
lllllll
l
l
l
l
ll
l
lllll
l
ll
llllll
ll
lll
ll
l
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
B?cubed
BLA
NC
l
lll
l
l
l
lllllll
l
l
l
l
ll
l
lllll
l
ll
lllllll
ll
lll
ll
ll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
CEAF?m
BLA
NC
l
ll
l
l
l
llll lll l
l
l
l
l
ll
l
llllll
l
ll
llllllll
ll
l
l
ll
l
l
0 10 20 30 40 50 60 70
10
20
30
40
50
60
CEAF?e
BLA
NC
Figure 1: Correlation plot between the proposed
BLANC and the other measures based on the
CoNLL 2011/2012 results. All values are F1
scores.
other measures along R, P and F1 (Table 3),
showing that BLANC is able to capture most
entity-based similarities measured by B-cubed and
CEAF. However, the CoNLL data sets come from
OntoNotes (Hovy et al, 2006), where singleton
entities are not annotated, and BLANC has a wider
dynamic range on data sets with singletons (Re-
casens and Hovy, 2011). So the correlations will
likely be lower on data sets with singleton entities.
6 Conclusion
The original BLANC-gold (Recasens and Hovy,
2011) requires that system mentions be identical
to gold mentions, which limits the metric?s utility
since detected system mentions often have missing
key mentions or spurious mentions. The proposed
BLANC is free from this assumption, and we
have shown that it subsumes the original BLANC-
gold. Since BLANC works on imperfect system
mentions, we have used it to score the CoNLL
2011 and 2012 coreference systems. The BLANC
scores show strong correlation with existing met-
rics, especially B-cubed and CEAF-m.
Acknowledgments
We would like to thank the three anonymous re-
viewers for their invaluable suggestions for im-
proving the paper. This work was partially sup-
ported by grants R01LM10090 from the National
Library of Medicine.
28
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the Linguistic Coreference Workshop at The First In-
ternational Conference on Language Resources and
Evaluation (LREC?98), pages 563?566.
Eduard Hovy, Mitchell Marcus, Martha Palmer,
Lance Ramshaw, and Ralph Weischedel. 2006.
OntoNotes: The 90% solution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
57?60, New York City, USA, June. Association for
Computational Linguistics.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proc. of Human Language
Technology (HLT)/Empirical Methods in Natural
Language Processing (EMNLP).
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Fifteenth Conference on Computational
Natural Language Learning: Shared Task, pages 1?
27, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1?40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
W. M. Rand. 1971. Objective criteria for the evalua-
tion of clustering methods. Journal of the American
Statistical Association, 66(336):846?850.
M. Recasens and E. Hovy. 2011. BLANC: Implement-
ing the Rand index for coreference evaluation. Nat-
ural Language Engineering, 17:485?510, 10.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, , and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In In Proc. of MUC6, pages 45?52.
29
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30?35,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Scoring Coreference Partitions of Predicted Mentions:
A Reference Implementation
Sameer Pradhan
1
, Xiaoqiang Luo
2
, Marta Recasens
3
,
Eduard Hovy
4
, Vincent Ng
5
and Michael Strube
6
1
Harvard Medical School, Boston, MA,
2
Google Inc., New York, NY
3
Google Inc., Mountain View, CA,
4
Carnegie Mellon University, Pittsburgh, PA
5
HLTRI, University of Texas at Dallas, Richardson, TX,
6
HITS, Heidelberg, Germany
sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com,
hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org
Abstract
The definitions of two coreference scoring
metrics?B
3
and CEAF?are underspeci-
fied with respect to predicted, as opposed
to key (or gold) mentions. Several varia-
tions have been proposed that manipulate
either, or both, the key and predicted men-
tions in order to get a one-to-one mapping.
On the other hand, the metric BLANC was,
until recently, limited to scoring partitions
of key mentions. In this paper, we (i) ar-
gue that mention manipulation for scoring
predicted mentions is unnecessary, and po-
tentially harmful as it could produce unin-
tuitive results; (ii) illustrate the application
of all these measures to scoring predicted
mentions; (iii) make available an open-
source, thoroughly-tested reference imple-
mentation of the main coreference eval-
uation measures; and (iv) rescore the re-
sults of the CoNLL-2011/2012 shared task
systems with this implementation. This
will help the community accurately mea-
sure and compare new end-to-end corefer-
ence resolution algorithms.
1 Introduction
Coreference resolution is a key task in natural
language processing (Jurafsky and Martin, 2008)
aiming to detect the referential expressions (men-
tions) in a text that point to the same entity.
Roughly over the past two decades, research in
coreference (for the English language) had been
plagued by individually crafted evaluations based
on two central corpora?MUC (Hirschman and
Chinchor, 1997; Chinchor and Sundheim, 2003;
Chinchor, 2001) and ACE (Doddington et al,
2004). Experimental parameters ranged from us-
ing perfect (gold, or key) mentions as input for
purely testing the quality of the entity linking al-
gorithm, to an end-to-end evaluation where pre-
dicted mentions are used. Given the range of
evaluation parameters and disparity between the
annotation standards for the two corpora, it was
very hard to grasp the state of the art for the
task of coreference. This has been expounded in
Stoyanov et al (2009). The activity in this sub-
field of NLP can be gauged by: (i) the contin-
ual addition of corpora manually annotated for
coreference?The OntoNotes corpus (Pradhan et
al., 2007; Weischedel et al, 2011) in the general
domain, as well as the i2b2 (Uzuner et al, 2012)
and THYME (Styler et al, 2014) corpora in the
clinical domain would be a few examples of such
emerging corpora; and (ii) ongoing proposals for
refining the existing metrics to make them more
informative (Holen, 2013; Chen and Ng, 2013).
The CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus (Prad-
han et al, 2011; Pradhan et al, 2012) were an
attempt to standardize the evaluation settings by
providing a benchmark annotated corpus, scorer,
and state-of-the-art system results that would al-
low future systems to compare against them. Fol-
lowing the timely emphasis on end-to-end evalu-
ation, the official track used predicted mentions
and measured performance using five coreference
measures: MUC (Vilain et al, 1995), B
3
(Bagga
and Baldwin, 1998), CEAF
e
(Luo, 2005), CEAF
m
(Luo, 2005), and BLANC (Recasens and Hovy,
2011). The arithmetic mean of the first three was
the task?s final score.
An unfortunate setback to these evaluations had
its root in three issues: (i) the multiple variations
of two of the scoring metrics?B
3
and CEAF?
used by the community to handle predicted men-
tions; (ii) a buggy implementation of the Cai and
Strube (2010) proposal that tried to reconcile these
variations; and (iii) the erroneous computation of
30
the BLANC metric for partitions of predicted men-
tions. Different interpretations as to how to com-
pute B
3
and CEAF scores for coreference systems
when predicted mentions do not perfectly align
with key mentions?which is usually the case?
led to variations of these metrics that manipulate
the gold standard and system output in order to
get a one-to-one mention mapping (Stoyanov et
al., 2009; Cai and Strube, 2010). Some of these
variations arguably produce rather unintuitive re-
sults, while others are not faithful to the original
measures.
In this paper, we address the issues in scor-
ing coreference partitions of predicted mentions.
Specifically, we justify our decision to go back
to the original scoring algorithms by arguing that
manipulation of key or predicted mentions is un-
necessary and could in fact produce unintuitive re-
sults. We demonstrate the use of our recent ex-
tension of BLANC that can seamlessly handle pre-
dicted mentions (Luo et al, 2014). We make avail-
able an open-source, thoroughly-tested reference
implementation of the main coreference evalua-
tion measures that do not involve mention manip-
ulation and is faithful to the original intentions of
the proposers of these metrics. We republish the
CoNLL-2011/2012 results based on this scorer, so
that future systems can use it for evaluation and
have the CoNLL results available for comparison.
The rest of the paper is organized as follows.
Section 2 provides an overview of the variations
of the existing measures. We present our newly
updated coreference scoring package in Section 3
together with the rescored CoNLL-2011/2012 out-
puts. Section 4 walks through a scoring example
for all the measures, and we conclude in Section 5.
2 Variations of Scoring Measures
Two commonly used coreference scoring metrics
?B
3
and CEAF?are underspecified in their ap-
plication for scoring predicted, as opposed to key
mentions. The examples in the papers describing
these metrics assume perfect mentions where pre-
dicted mentions are the same set of mentions as
key mentions. The lack of accompanying refer-
ence implementation for these metrics by its pro-
posers made it harder to fill the gaps in the speci-
fication. Subsequently, different interpretations of
how one can evaluate coreference systems when
predicted mentions do not perfectly align with key
mentions led to variations of these metrics that ma-
nipulate the gold and/or predicted mentions (Stoy-
anov et al, 2009; Cai and Strube, 2010). All these
variations attempted to generate a one-to-one map-
ping between the key and predicted mentions, as-
suming that the original measures cannot be ap-
plied to predicted mentions. Below we first pro-
vide an overview of these variations and then dis-
cuss the unnecessity of this assumption.
Coining the term twinless mentions for those
mentions that are either spurious or missing from
the predicted mention set, Stoyanov et al (2009)
proposed two variations to B
3
? B
3
all
and B
3
0
?to
handle them. In the first variation, all predicted
twinless mentions are retained, whereas the lat-
ter discards them and penalizes recall for twin-
less predicted mentions. Rahman and Ng (2009)
proposed another variation by removing ?all and
only those twinless system mentions that are sin-
gletons before applying B
3
and CEAF.? Follow-
ing upon this line of research, Cai and Strube
(2010) proposed a unified solution for both B
3
and
CEAF
m
, leaving the question of handling CEAF
e
as future work because ?it produces unintuitive
results.? The essence of their solution involves
manipulating twinless key and predicted mentions
by adding them either from the predicted parti-
tion to the key partition or vice versa, depend-
ing on whether one is computing precision or re-
call. The Cai and Strube (2010) variation was used
by the CoNLL-2011/2012 shared tasks on corefer-
ence resolution using the OntoNotes corpus, and
by the i2b2 2011 shared task on coreference res-
olution using an assortment of clinical notes cor-
pora (Uzuner et al, 2012).
1
It was later identified
by Recasens et al (2013) that there was a bug in
the implementation of this variation in the scorer
used for the CoNLL-2011/2012 tasks. We have
not tested the correctness of this variation in the
scoring package used for the i2b2 shared task.
However, it turns out that the CEAF metric (Luo,
2005) was always intended to work seamlessly on
predicted mentions, and so has been the case with
the B
3
metric.
2
In a latter paper, Rahman and Ng
(2011) correctly state that ?CEAF can compare par-
titions with twinless mentions without any modifi-
cation.? We will look at this further in Section 4.3.
We argue that manipulations of key and re-
sponse mentions/entities, as is done in the exist-
ing B
3
variations, not only confound the evalu-
ation process, but are also subject to abuse and
can seriously jeopardize the fidelity of the evalu-
1
Personal communication with Andreea Bodnari, and
contents of the i2b2 scorer code.
2
Personal communication with Breck Baldwin.
31
ation. Given space constraints we use an exam-
ple worked out in Cai and Strube (2010). Let
the key contain an entity with mentions {a, b, c}
and the prediction contain an entity with mentions
{a, b, d}. As detailed in Cai and Strube (2010,
p. 29-30, Tables 1?3), B
3
0
assigns a perfect pre-
cision of 1.00 which is unintuitive as the system
has wrongly predicted a mention d as belonging to
the entity. For the same prediction, B
3
all
assigns a
precision of 0.556. But, if the prediction contains
two entities {a, b, d} and {c} (i.e., the mention c
is added as a spurious singleton), then B
3
all
preci-
sion increases to 0.667 which is counter-intuitive
as it does not penalize the fact that c is erroneously
placed in its own entity. The version illustrated in
Section 4.2, which is devoid of any mention ma-
nipulations, gives a precision of 0.444 in the first
scenario and the precision drops to 0.333 in the
second scenario with the addition of a spurious
singleton entity {c}. This is a more intuitive be-
havior.
Contrary to both B
3
and CEAF, the BLANC mea-
sure (Recasens and Hovy, 2011) was never de-
signed to handle predicted mentions. However, the
implementation used for the SemEval-2010 shared
task as well as the one for the CoNLL-2011/2012
shared tasks accepted predicted mentions as input,
producing undefined results. In Luo et al (2014)
we have extended the BLANC metric to deal with
predicted mentions
3 Reference Implementation
Given the potential unintuitive outcomes of men-
tion manipulation and the misunderstanding that
the original measures could not handle twinless
predicted mentions (Section 2), we redesigned the
CoNLL scorer. The new implementation:
? is faithful to the original measures;
? removes any prior mention manipulation,
which might depend on specific annotation
guidelines among other problems;
? has been thoroughly tested to ensure that it
gives the expected results according to the
original papers, and all test cases are included
as part of the release;
? is free of the reported bugs that the CoNLL
scorer (v4) suffered (Recasens et al, 2013);
? includes the extension of BLANC to handle
predicted mentions (Luo et al, 2014).
This is the open source scoring package
3
that
we present as a reference implementation for the
3
http://code.google.com/p/reference-coreference-scorers/
SYSTEM MD MUC B
3
CEAF BLANC CONLL
m e AVERAGE
F
1
F
1
1
F
2
1
F
1
F
3
1
CoNLL-2011; English
lee 70.7 59.6 48.9 53.0 46.1 48.8 51.5
sapena 68.4 59.5 46.5 51.3 44.0 44.5 50.0
nugues 69.0 58.6 45.0 48.4 40.0 46.0 47.9
chang 64.9 57.2 46.0 50.7 40.0 45.5 47.7
stoyanov 67.8 58.4 40.1 43.3 36.9 34.6 45.1
santos 65.5 56.7 42.9 45.1 35.6 41.3 45.0
song 67.3 60.0 41.4 41.0 33.1 30.9 44.8
sobha 64.8 50.5 39.5 44.2 39.4 36.3 43.1
yang 63.9 52.3 39.4 43.2 35.5 36.1 42.4
charton 64.3 52.5 38.0 42.6 34.5 35.7 41.6
hao 64.3 54.5 37.7 41.9 31.6 37.0 41.3
zhou 62.3 49.0 37.0 40.6 35.0 35.0 40.3
kobdani 61.0 53.5 34.8 38.1 34.1 32.6 38.7
xinxin 61.9 46.6 34.9 37.7 31.7 35.0 37.7
kummerfeld 62.7 42.7 34.2 38.8 35.5 31.0 37.5
zhang 61.1 47.9 34.4 37.8 29.2 35.7 37.2
zhekova 48.3 24.1 23.7 23.4 20.5 15.4 22.8
irwin 26.7 20.0 11.7 18.5 14.7 6.3 15.5
CoNLL-2012; English
fernandes 77.7 70.5 57.6 61.4 53.9 58.8 60.7
martschat 75.2 67.0 54.6 58.8 51.5 55.0 57.7
bjorkelund 75.4 67.6 54.5 58.2 50.2 55.4 57.4
chang 74.3 66.4 53.0 57.1 48.9 53.9 56.1
chen 73.8 63.7 51.8 55.8 48.1 52.9 54.5
chunyang 73.7 63.8 51.2 55.1 47.6 52.7 54.2
stamborg 73.9 65.1 51.7 55.1 46.6 54.4 54.2
yuan 72.5 62.6 50.1 54.5 46.0 52.1 52.9
xu 72.0 66.2 50.3 51.3 41.3 46.5 52.6
shou 73.7 62.9 49.4 53.2 46.7 50.4 53.0
uryupina 70.9 60.9 46.2 49.3 42.9 46.0 50.0
songyang 68.8 59.8 45.9 49.6 42.4 45.1 49.4
zhekova 67.1 53.5 35.7 39.7 32.2 34.8 40.5
xinxin 62.8 48.3 35.7 38.0 31.9 36.5 38.6
li 59.9 50.8 32.3 36.3 25.2 31.9 36.1
CoNLL-2012; Chinese
chen 71.6 62.2 55.7 60.0 55.0 54.1 57.6
yuan 68.2 60.3 52.4 55.8 50.2 43.2 54.3
bjorkelund 66.4 58.6 51.1 54.2 47.6 44.2 52.5
xu 65.2 58.1 49.5 51.9 46.6 38.5 51.4
fernandes 66.1 60.3 49.6 54.4 44.5 49.6 51.5
stamborg 64.0 57.8 47.4 51.6 41.9 45.9 49.0
uryupina 59.0 53.0 41.7 46.9 37.6 41.9 44.1
martschat 58.6 52.4 40.8 46.0 38.2 37.9 43.8
chunyang 61.6 49.8 39.6 44.2 37.3 36.8 42.2
xinxin 55.9 48.1 38.8 42.9 34.5 37.9 40.5
li 51.5 44.7 31.5 36.7 25.3 30.4 33.8
chang 47.6 37.9 28.8 36.1 29.6 25.7 32.1
zhekova 47.3 40.6 28.1 31.4 21.2 22.9 30.0
CoNLL-2012; Arabic
fernandes 64.8 46.5 42.5 49.2 46.5 38.0 45.2
bjorkelund 60.6 47.8 41.6 46.7 41.2 37.9 43.5
uryupina 55.4 41.5 36.1 41.4 35.0 33.0 37.5
stamborg 59.5 41.2 35.9 40.0 32.9 34.5 36.7
chen 59.8 39.0 32.1 34.7 26.0 30.8 32.4
zhekova 41.0 29.9 22.7 31.1 25.9 18.5 26.2
li 29.7 18.1 13.1 21.0 17.3 8.4 16.2
Table 1: Performance on the official, closed track
in percentages using all predicted information for
the CoNLL-2011 and 2012 shared tasks.
community to use. It is written in perl and stems
from the scorer that was initially used for the
SemEval-2010 shared task (Recasens et al, 2010)
and later modified for the CoNLL-2011/2012
shared tasks.
4
Partitioning detected mentions into entities (or
equivalence classes) typically comprises two dis-
tinct tasks: (i) mention detection; and (ii) coref-
erence resolution. A typical two-step coreference
algorithm uses mentions generated by the best
4
We would like to thank Emili Sapena for writing the first
version of the scoring package.
32
a     b
c
de fg
h
a     bc
de
hi i
f    g f    g
h i
cd
a     b
Solid: KeyDashed: Response Solid: KeyDashed: partition wrt Response Solid: Partition wrt KeyDashed: Response
Figure 1: Example key and response entities along
with the partitions for computing the MUC score.
possible mention detection algorithm as input to
the coreference algorithm. Therefore, ideally one
would want to score the two steps independently
of each other. A peculiarity of the OntoNotes
corpus is that singleton referential mentions are
not annotated, thereby preventing the computation
of a mention detection score independently of the
coreference resolution score. In corpora where all
referential mentions (including singletons) are an-
notated, the mention detection score generated by
this implementation is independent of the corefer-
ence resolution score.
We used this reference implementation to
rescore the CoNLL-2011/2012 system outputs for
the official task to enable future comparisons with
these benchmarks. The new CoNLL-2011/2012
results are in Table 1. We found that the over-
all system ranking remained largely unchanged for
both shared tasks, except for some of the lower
ranking systems that changed one or two places.
However, there was a considerable drop in the
magnitude of all B
3
scores owing to the combi-
nation of two things: (i) mention manipulation, as
proposed by Cai and Strube (2010), adds single-
tons to account for twinless mentions; and (ii) the
B
3
metric allows an entity to be used more than
once as pointed out by Luo (2005). This resulted
in a drop in the CoNLL averages (B
3
is one of the
three measures that make the average).
4 An Illustrative Example
This section walks through the process of com-
puting each of the commonly used metrics for
an example where the set of predicted mentions
has some missing key mentions and some spu-
rious mentions. While the mathematical formu-
lae for these metrics can be found in the original
papers (Vilain et al, 1995; Bagga and Baldwin,
1998; Luo, 2005), many misunderstandings dis-
cussed in Section 2 are due to the fact that these
papers lack an example showing how a metric is
computed on predicted mentions. A concrete ex-
ample goes a long way to prevent similar misun-
derstandings in the future. The example is adapted
from Vilain et al (1995) with some slight modifi-
cations so that the total number of mentions in the
key is different from the number of mentions in
the prediction. The key (K) contains two entities
with mentions {a, b, c} and {d, e, f, g} and the re-
sponse (R) contains three entities with mentions
{a, b}; {c, d} and {f, g, h, i}:
K =
K
1
? ?? ?
{a, b, c}
K
2
? ?? ?
{d, e, f, g} (1)
R =
R
1
? ?? ?
{a, b}
R
2
? ?? ?
{c, d}
R
3
? ?? ?
{f, g, h, i}. (2)
Mention e is missing from the response, and men-
tions h and i are spurious in the response. The fol-
lowing sections use R to denote recall and P for
precision.
4.1 MUC
The main step in the MUC scoring is creating the
partitions with respect to the key and response re-
spectively, as shown in Figure 1. Once we have
the partitions, then we compute the MUC score by:
R =
?
N
k
i=1
(|K
i
| ? |p(K
i
)|)
?
N
k
i=1
(|K
i
| ? 1)
=
(3? 2) + (4? 3)
(3? 1) + (4? 1)
= 0.40
P =
?
N
r
i=1
(|R
i
| ? |p
?
(R
i
)|)
?
N
r
i=1
(|R
i
| ? 1)
=
(2? 1) + (2? 2) + (4? 3)
(2? 1) + (2? 1) + (4? 1)
= 0.40,
where K
i
is the i
th
key entity and p(K
i
) is the
set of partitions created by intersecting K
i
with
response entities (cf. the middle sub-figure in Fig-
ure 1); R
i
is the i
th
response entity and p
?
(R
i
) is
the set of partitions created by intersectingR
i
with
key entities (cf. the right-most sub-figure in Fig-
ure 1); and N
k
and N
r
are the number of key and
response entities, respectively.
The MUC F
1
score in this case is 0.40.
4.2 B
3
For computing B
3
recall, each key mention is as-
signed a credit equal to the ratio of the number of
correct mentions in the predicted entity contain-
ing the key mention to the size of the key entity to
which the mention belongs, and the recall is just
33
the sum of credits over all key mentions normal-
ized over the number of key mentions. B
3
preci-
sion is computed similarly, except switching the
role of key and response. Applied to the example:
R =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|K
i
|
?
N
k
i=1
|K
i
|
=
1
7
? (
2
2
3
+
1
2
3
+
1
2
4
+
2
2
4
) =
1
7
?
35
12
? 0.42
P =
?
N
k
i=1
?
N
r
j=1
|K
i
?R
j
|
2
|R
j
|
?
N
r
i=1
|R
j
|
=
1
8
? (
2
2
2
+
1
2
2
+
1
2
2
+
2
2
4
) =
1
8
?
4
1
= 0.50
Note that terms with 0 value are omitted. The B
3
F
1
score is 0.46.
4.3 CEAF
The first step in the CEAF computation is getting
the best scoring alignment between the key and
response entities. In this case the alignment is
straightforward. Entity R
1
aligns with K
1
and R
3
aligns with K
2
. R
2
remains unaligned.
CEAF
m
CEAF
m
recall is the number of aligned mentions
divided by the number of key mentions, and preci-
sion is the number of aligned mentions divided by
the number of response mentions:
R =
|K
1
? R
1
|+ |K
2
? R
3
|
|K
1
|+ |K
2
|
=
(2 + 2)
(3 + 4)
? 0.57
P =
|K
1
? R
1
|+ |K
2
? R
3
|
|R
1
|+ |R
2
|+ |R
3
|
=
(2 + 2)
(2 + 2 + 4)
= 0.50
The CEAF
m
F
1
score is 0.53.
CEAF
e
We use the same notation as in Luo (2005):
?
4
(K
i
, R
j
) to denote the similarity between a key
entity K
i
and a response entity R
j
. ?
4
(K
i
, R
j
) is
defined as:
?
4
(K
i
, R
j
) =
2? |K
i
? R
j
|
|K
i
|+ |R
j
|
.
CEAF
e
recall and precision, when applied to this
example, are:
R =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
k
=
(2?2)
(3+2)
+
(2?2)
(4+4)
2
= 0.65
P =
?
4
(K
1
, R
1
) + ?
4
(K
2
, R
3
)
N
r
=
(2?2)
(3+2)
+
(2?2)
(4+4)
3
? 0.43
The CEAF
e
F
1
score is 0.52.
4.4 BLANC
The BLANC metric illustrated here is the one in
our implementation which extends the original
BLANC (Recasens and Hovy, 2011) to predicted
mentions (Luo et al, 2014).
Let C
k
and C
r
be the set of coreference links
in the key and response respectively, and N
k
and
N
r
be the set of non-coreference links in the key
and response respectively. A link between a men-
tion pair m and n is denoted by mn; then for the
example in Figure 1, we have
C
k
= {ab, ac, bc, de, df, dg, ef, eg, fg}
N
k
= {ad, ae, af, ag, bd, be, bf, bg, cd, ce, cf, cg}
C
r
= {ab, cd, fg, fh, fi, gh, gi, hi}
N
r
= {ac, ad, af, ag, ah, ai, bc, bd, bf, bg, bh, bi,
cf, cg, ch, ci, df, dg, dh, di}.
Recall and precision for coreference links are:
R
c
=
|C
k
? C
r
|
|C
k
|
=
2
9
? 0.22
P
c
=
|C
k
? C
r
|
|C
r
|
=
2
8
= 0.25
and the coreference F-measure, F
c
? 0.23. Sim-
ilarly, recall and precision for non-coreference
links are:
R
n
=
|N
k
?N
r
|
|N
k
|
=
8
12
? 0.67
P
n
=
|N
k
?N
r
|
|N
r
|
=
8
20
= 0.40,
and the non-coreference F-measure, F
n
= 0.50.
So the BLANC score is
F
c
+F
n
2
? 0.36.
5 Conclusion
We have cleared several misunderstandings about
coreference evaluation metrics, especially when a
response contains imperfect predicted mentions,
and have argued against mention manipulations
during coreference evaluation. These misunder-
standings are caused partially by the lack of il-
lustrative examples to show how a metric is com-
puted on predicted mentions not aligned perfectly
with key mentions. Therefore, we provide detailed
steps for computing all four metrics on a represen-
tative example. Furthermore, we have a reference
implementation of these metrics that has been rig-
orously tested and has been made available to the
public as open source software. We reported new
scores on the CoNLL 2011 and 2012 data sets,
which can serve as the benchmarks for future re-
search work.
Acknowledgments
This work was partially supported by grants
R01LM10090 from the National Library of
Medicine and IIS-1219142 from the National Sci-
ence Foundation.
34
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
LREC, pages 563?566.
Jie Cai and Michael Strube. 2010. Evaluation metrics
for end-to-end coreference resolution systems. In
Proceedings of SIGDIAL, pages 28?36.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Pro-
ceedings of the Sixth IJCNLP, pages 1366?1374,
Nagoya, Japan, October.
Nancy Chinchor and Beth Sundheim. 2003. Mes-
sage understanding conference (MUC) 6. In
LDC2003T13.
Nancy Chinchor. 2001. Message understanding con-
ference (MUC) 7. In LDC2001T02.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The automatic content extrac-
tion (ACE) program-tasks, data, and evaluation. In
Proceedings of LREC.
Lynette Hirschman and Nancy Chinchor. 1997. Coref-
erence task definition (v3.0, 13 jul 97). In Proceed-
ings of the 7th Message Understanding Conference.
Gordana Ilic Holen. 2013. Critical reflections on
evaluation practices in coreference resolution. In
Proceedings of the NAACL-HLT Student Research
Workshop, pages 1?7, Atlanta, Georgia, June.
Daniel Jurafsky and James H. Martin. 2008. Speech
and Language Processing: An Introduction to Nat-
ural Language Processing, Computational Linguis-
tics, and Speech Recognition. Prentice Hall. Second
Edition.
Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, and
Eduard Hovy. 2014. An extension of BLANC to
system mentions. In Proceedings of ACL, Balti-
more, Maryland, June.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of HLT-EMNLP,
pages 25?32.
Sameer Pradhan, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. OntoNotes: A Unified Rela-
tional Semantic Representation. International Jour-
nal of Semantic Computing, 1(4):405?419.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 shared task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of CoNLL: Shared Task, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
of CoNLL: Shared Task, pages 1?40.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP, pages 968?977.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
ACL, pages 814?824.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand Index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of SemEval,
pages 1?8.
Marta Recasens, Marie-Catherine de Marneffe, and
Chris Potts. 2013. The life and death of discourse
entities: Identifying singleton mentions. In Pro-
ceedings of NAACL-HLT, pages 627?633.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of ACL-IJCNLP, pages
656?664.
William F. Styler, Steven Bethard an Sean Finan,
Martha Palmer, Sameer Pradhan, Piet C de Groen,
Brad Erickson, Timothy Miller, Chen Lin, Guergana
Savova, and James Pustejovsky. 2014. Temporal
annotation in the clinical domain. Transactions of
Computational Linguistics, 2(April):143?154.
Ozlem Uzuner, Andreea Bodnari, Shuying Shen, Tyler
Forbush, John Pestian, and Brett R South. 2012.
Evaluating the state of the art in coreference res-
olution for electronic medical records. Journal of
American Medical Informatics Association, 19(5),
September.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model theo-
retic coreference scoring scheme. In Proceedings of
the 6th Message Understanding Conference, pages
45?52.
Ralph Weischedel, Eduard Hovy, Mitchell Marcus,
Martha Palmer, Robert Belvin, Sameer Pradhan,
Lance Ramshaw, and Nianwen Xue. 2011.
OntoNotes: A large training corpus for enhanced
processing. In Joseph Olive, Caitlin Christian-
son, and John McCary, editors, Handbook of Natu-
ral Language Processing and Machine Translation:
DARPA Global Autonomous Language Exploitation.
Springer.
35
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 222?225,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
ISI: Automatic Classification of Relations Between Nominals Using a
Maximum Entropy Classifier
Stephen Tratz and Eduard Hovy
Information Sciences Institute
University of Southern California
Marina del Rey, CA 90292
{stratz,hovy}@isi.edu
Abstract
The automatic interpretation of semantic
relations between nominals is an impor-
tant subproblem within natural language
understanding applications and is an area
of increasing interest. In this paper, we
present the system we used to participate
in the SEMEVAL 2010 Task 8 Multi-Way
Classification of Semantic Relations be-
tween Pairs of Nominals. Our system,
based upon a Maximum Entropy classifier
trained using a large number of boolean
features, received the third highest score.
1 Introduction
Semantic interpretation of the relations between
nominals in text is an area of growing interest
within natural language processing (NLP). It has
potential uses for a variety of tasks including ma-
chine translation (Baldwin and Tanaka, 2004) and
question answering (Ahn et al, 2005). The related
and more narrowly-focused problem of automatic
interpretation of noun compounds is the focus of
another SEMEVAL task (Butnariu et al, 2009).
In this paper, we discuss the overall setup of
SEMEVAL 2010 Task 8 (Hendrickx et al, 2010),
present the system we used to participate, and
discuss our system?s performance. Our system,
which consists of a Maximum Entropy classifier
trained using a large variety of boolean features,
received the third highest official score of all the
entries.
2 Related Work
The groundwork for SEMEVAL 2010 Task 8 was
laid by an earlier SEMEVAL task (Girju et al,
2007). For SEMEVAL 2007 Task 4, participants
provided yes or no answers as to whether a partic-
ular relation held for each test example. For SE-
MEVAL 2010, instead of providing a binary out-
put for a single class, participants were required to
perform multi-way classification, that is, select the
most appropriate relation from a set of 10 relations
including the OTHER relation.
The selection of a semantic relation for a pair
of nominals within a sentence is somewhat sim-
ilar to the task of noun compound interpretation,
which is a more restricted problem focused only
upon the nouns within noun compounds. Some
of the recent work on this problem includes that
of Butnariu et al (2009), Girju (2007), Girju
et al (2005), Kim and Baldwin (2005), Nakov
(2008), Nastase et al (2006), Turney (2006), and
? S?aghdha and Copestake (2009).
3 Task Overview
The task is, given a pair of nominals within their
sentence context, select the most appropriate se-
mantic relation from the set of available relations
and indicate the direction of the relation. Though
the final score was based upon the output of the
system trained using the whole training dataset,
participants were also required to submit three ad-
ditional label sets using the first 12.5%, 25%, and
50% of the training data.
3.1 Relation Scheme
The relations were taken from earlier work on
noun compounds by Nastase and Szpakowicz
(2003).
A total of 10 relations were used includ-
ing CAUSE-EFFECT, COMPONENT-WHOLE,
CONTENT-CONTAINER, ENTITY-ORIGIN,
ENTITY-DESTINATION, INSTRUMENT-AGENCY,
MEMBER-COLLECTION, MESSAGE-TOPIC,
OTHER, and PRODUCT-PRODUCER. Since each
relation except the OTHER relation must have its
direction specified, there are a total of 19 possible
labels.
222
3.2 Data
The training and testing datasets consist of 8000
and 2717 examples respectively. Each example
consists of a single sentence with two of its nomi-
nals marked as being the nominals of interest. The
training data also provides the correct relation for
each example.
4 Method
4.1 Classifier
We use a Maximum Entropy (Berger et al, 1996)
classifier trained using a large number of boolean
features. Maximum Entropy classifiers have
proven effective for a variety of NLP problems in-
cluding word sense disambiguation (Tratz et al,
2007; Ye and Baldwin, 2007). We use the imple-
mentation provided in the MALLET machine learn-
ing toolkit (McCallum, 2002). We used the default
Gaussian prior parameter value of 1.0.
4.2 Features Used
We generate features from individual words, in-
cluding both the nominals and their context, and
from combinations of the nominals.
To generate the features for individual words,
we first use a set of word selection rules to se-
lect the words of interest and then run these words
of interest through a variety of feature-generating
functions. Some words may be selected by multi-
ple word selection rules. For example, the word to
the right of the first nominal will be identified by
the word 1 to the right of the 1st nominal rule, the
words that are 3 or less to the right of the 1st nom-
inal rule, and the all words between the nominals
rule. In these cases, the actual feature is the com-
bination of an identifier for the word selection rule
and the output from the feature-generating func-
tion. The 19 word-selection rules are listed below:
Word-Selection Rules
? The {1st, 2nd} nominal (2 rules)
? Word {1, 2, 3} to the {left, right} of the {1st,
2nd} nominal (12 rules)
? Words that are 3 or less to the {left, right} of
the {1st, 2nd} nominal (4 rules)
? All words between the two nominals (1 rule)
The features generated from the individual
words come from a variety of sources includ-
ing word orthography, simple gazetteers, pattern
matching, WordNet (Fellbaum, 1998), and Ro-
get?s Thesaurus.
Orthographic Features
? Capitalization indicator
? The {first, last} {two, three} letters of each
word
? Indicator if the first letter of the word is a/A.
? Indicator for the overall form of the word
(e.g. jump -> a, Mr. -> Aa., SemEval2 ->
AaAa0)
? Indicators for the suffix types (e.g., de-
adjectival, de-nominal [non]agentive, de-
verbal [non]agentive)
? Indicators for a wide variety of affixes includ-
ing those related to degree, number, order,
etc. (e.g., ultra-, poly-, post-)
? Indicators for whether or not a preposition
occurs within either term (e.g., ?down? in
?breakdown?)
Gazetteer and Pattern Features
? Indicators if the word is one of a number of
closed classes (e.g. articles, prepositions)
? Indicator if the word is listed in the U.S. Cen-
sus 2000?s most common surnames list
? Indicator if the word is listed in the U.S. Cen-
sus 2000?s most common first names list
? Indicator if the word is a name or location
based upon some simple regular expressions
WordNet-based Features
? Lemmatized version of the word
? Synonyms for all NN and VB entries for the
word
? Hypernyms for all NN and VB entries for the
word
? All terms in the definitions (?gloss?) for the
word
? Lexicographer file names for the word
? Lists of all link types (e.g., meronym links)
associated with the word
? Part-of-speech indicators for the existence of
NN/VB/JJ/RB entries for the word
? All sentence frames for the word
? All part, member, substance-of holonyms for
the word
Roget?s Thesaurus-based Features
? Roget?s divisions for all noun (and verb) en-
tries for the word
223
Some additional features were extracted using
combinations of the nominals. These include fea-
tures generated using The Web 1T corpus (Brants
and Franz, 2006), and the output of a noun com-
pound interpretation system.
Web 1T N-gram Features
To provide information related to term usage
to the classifier, we extracted trigram and 4-gram
features from the Web 1T Corpus (Brants and
Franz, 2006). Only n-grams containing lowercase
words were used. The nominals were converted
to lowercase if needed. Only n-grams contain-
ing both terms (including plural forms) were ex-
tracted. We included the n-gram, with the nomi-
nals replaced with N1 and N2 respectively, as in-
dividual boolean features. We also included ver-
sions of the n-gram features with the words re-
placed with wild cards. For example, if the nomi-
nals were ?food? and ?basket? and the extracted n-
gram was ?put_N1_in_the_N2?, we also included
?*_N1_in_the_N2?, ?*_N1_*_the_N2?, etc. as
features.
Noun Compound System Features
We also ran the nominals through an in-house
noun compound interpretation system and took its
output as features. We will not be discussing the
noun compound interpretation system in detail in
this paper. It uses a similar approach to that de-
scribed in this paper including a Maximum En-
tropy classifier trained with similar features that
outputs a ranked list of a fixed set of semantic re-
lations. The relations ranked within the top 5 and
bottom 5 were included as features. For example,
if ?Topic of Communication? was the third high-
est relation, both ?top:3:Topic of Communication?
and ?top:*:Topic of Communication? would be in-
cluded as features.
4.3 Feature Filtering
The aforementioned feature generation process
creates a very large number of features. To deter-
mine the final feature set, we first ranked the fea-
tures according to the Chi-Squared metric. Then,
by holding out one tenth of the training data
and trying different thresholds, we concluded that
100,000 features was roughly optimal. For the
cases where we used 12.5%, 25%, and 50%, we
tested on the remaining training data and came up
different cutoffs: 25,000, 40,000, and 60,000, re-
spectively.
5 Results
Each participating site was allowed to submit mul-
tiple runs based upon different systems or config-
urations thereof. The results for the best perform-
ing submissions from each team are presented in
Table 1. The official metric for the task was F1
macroaveraged across the different relations. We
are pleased to see that our system received the
third highest score.
Our results by the different relation types are
shown in Table 2. We note that the performance
on the OTHER relation is relatively low.
Top Results
System Macroaveraged F1
12.5% 25% 50% 100%
UTD 73.08 77.02 79.93 82.19
FBK_IRST 63.61 70.20 73.40 77.62
ISI 66.68 71.01 75.51 77.57
ECNU 49.32 50.70 72.63 75.43
TUD 58.35 62.45 66.86 69.23
ISTI 50.49 55.80 61.14 68.42
FBK_NK 55.71 64.06 67.80 68.02
SEKA 51.81 56.34 61.10 66.33
JU 41.62 44.98 47.81 52.16
UNITN 16.57 18.56 22.45 26.67
Table 1: Final results (macroaveraged F1) for the
highest ranking (based upon result for training
with the complete training set) submissions for
each site. 12.5%, 25%, 50%, and 100% indicate
the amount of training data used.
Results by Relation
Relation P R F1
Cause-Effect 87.77 87.50 87.63
Component-Whole 73.21 75.32 74.25
Content-Container 82.74 84.90 83.80
Entity-Destination 81.51 81.51 81.51
Entity-Origin 81.86 75.19 78.38
Instrument-Agency 64.34 58.97 61.54
Member-Collection 84.62 84.98 84.80
Message-Topic 75.91 79.69 77.76
Product-Producer 70.83 66.23 68.46
Other 43.28 45.37 44.30
Table 2: Precision, recall, and F1 results for our
system by semantic relation.
224
6 Conclusion
We explain the system we used to participate in
the SEMEVAL 2010 Task 8: Multi-Way Classi-
fication of Semantic Relations Between Pairs of
Nominals and present its results. The overall ap-
proach is straight forward, consisting of a single
Maximum Entropy classifier using a large number
of boolean features, and proves effective, with our
system receiving the third highest score of all the
submissions.
7 Future Work
In the future, we are interested in utilizing pars-
ing and part-of-speech tagging to enrich the fea-
ture set. We also want to investigate the relatively
low performance for the OTHER category and see
if we could develop a method to improve this.
Acknowledgements
Stephen Tratz is supported by a National De-
fense Science and Engineering Graduate Fellow-
ship. We would like to thank the organizers of
this task for their hard work in putting this task
together.
References
Ahn, K., J. Bos, J. R. Curran, D. Kor, M. Nissim, and
B. Webber. 2005. Question Answering with QED
at TREC-2005. In Proc. of TREC-2005.
Baldwin, T. & T. Tanaka 2004. Translation by machine
of compound nominals: Getting it right. In Proc. of
the ACL 2004 Workshop on Multiword Expressions:
Integrating Processing.
Berger, A., S. A. Della Pietra, and V. J. Della Pietra.
1996. A Maximum Entropy Approach to Natural
Language Processing. Computational Linguistics,
22.
Brants, T. and A. Franz. 2006. Web 1T 5-gram Corpus
Version 1.1. Linguistic Data Consortium.
Butnariu, C. and T. Veale. 2008. A concept-centered
approach to noun-compound interpretation. In Proc.
of 22nd International Conference on Computational
Linguistics (COLING 2008).
Butnariu, C., S.N. Kim, P. Nakov, D. ? S?aghdha, S.
Szpakowicz, and T. Veale. 2009. SemEval Task 9:
The Interpretation of Noun Compounds Using Para-
phrasing Verbs and Prepositions. In Proc. of the
NAACL HLT Workshop on Semantic Evaluations:
Recent Achievements and Future Directions.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, MA.
Girju, R., D. Moldovan, M. Tatu and D. Antohe. 2005.
On the semantics of noun compounds. Computer
Speech and Language, 19.
Girju, R., P. Nakov, V. Nastase, S. Szpakowicz, P. Tur-
ney, and D. Yuret. 2007. SemEval-2007 Task 04:
Classification of Semantic Relations between Nom-
inals In Proc. of the 4th Semantic Evaluation Work-
shop (SemEval-2007).
Hendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov, D.
? S?aghdha, Sebastian Pad?, M. Pennacchiotti, L.
Romano, and S. Szpakowicz. 2010. Improving the
interpretation of noun phrases with cross-linguistic
information. In Proc. of the 5th SIGLEX Workshop
on Semantic Evaluation.
Girju, R. 2007. Improving the interpretation of noun
phrases with cross-linguistic information. In Proc.
of the 45th Annual Meeting of the Association of
Computational Linguistics (ACL 2007).
Kim, S.N. and T. Baldwin. 2005. Automatic
Interpretation of Compound Nouns using Word-
Net::Similarity. In Proc. of 2nd International Joint
Conf. on Natural Language Processing.
McCallum, A. K. MALLET: A Machine Learning for
Language Toolkit. http://mallet.cs.umass.edu. 2002.
Nakov, P. 2008. Noun Compound Interpretation
Using Paraphrasing Verbs: Feasibility Study. In
Proc. the 13th International Conference on Artifi-
cial Intelligence: Methodology, Systems, Applica-
tions (AIMSA?08).
Nastase V. and S. Szpakowicz. 2003. Exploring noun-
modifier semantic relations. In Proc. the 5th Inter-
national Workshop on Computational Semantics.
Nastase, V., J. S. Shirabad, M. Sokolova, and S. Sz-
pakowicz 2006. Learning noun-modifier semantic
relations with corpus-based and Wordnet-based fea-
tures. In Proc. of the 21st National Conference on
Artificial Intelligence (AAAI-06).
? S?aghdha, D. and A. Copestake. 2009. Using lexi-
cal and relational similarity to classify semantic re-
lations. In Proc. of the 12th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics (EACL 2009).
Tratz, S., A. Sanfilippo, M. Gregory, A. Chappell, C.
Posse, and P. Whitney. 2007. PNNL: A Supervised
Maximum Entropy Approach to Word Sense Disam-
biguation In Proc. of the 4th International Workshop
on Semantic Evaluations (SemEval-2007).
Turney, P. D. 2006. Similarity of semantic relations.
Computation Linguistics, 32(3):379-416
Ye, P. and T. Baldwin. 2007. MELB-YB: Prepo-
sition Sense Disambiguation Using Rich Semantic
Features. In Proc. of the 4th International Workshop
on Semantic Evaluations (SemEval-2007).
225
Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 15?23,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Semantic Enrichment of Text with Background Knowledge 

Anselmo Pe?as Eduard Hovy
UNED NLP & IR Group USC Information Sciences Institute 
Juan del Rosal, 16 4676 Admiralty Way 
28040 Madrid, Spain Marina del Rey, CA 90292-6695
anselmo@lsi.uned.es hovy@isi.edu
Abstract
Texts are replete with gaps, information omit-
ted since authors assume a certain amount of 
background knowledge.  We describe the kind 
of information (the formalism and methods to 
derive the content) useful for automated fill-
ing of such gaps.  We describe a stepwise pro-
cedure with a detailed example.   
1 Introduction 
Automated understanding of connected text re-
mains an unsolved challenge in NLP.  In contrast 
to systems that harvest information from large col-
lections of text, or that extract only certain pre-
specified kinds of information from single texts, 
the task of extracting and integrating all informa-
tion from a single text, and building a coherent and 
relatively complete representation of its full con-
tent, is still beyond current capabilities.   
A significant obstacle is the fact that text always 
omits information that is important, but that people 
recover effortlessly. Authors leave out information 
that they assume is known to their readers, since its 
inclusion (under the Gricean maxim of minimality) 
would carry an additional, often pragmatic, import. 
The problem is that systems cannot perform the 
recovery since they lack the requisite background 
knowledge and inferential machinery to use it.   
In this research we address the problem of 
automatically recovering such omitted information 
to ?plug the gaps? in text.  To do so, we describe 
the background knowledge required as well as a 
procedure for recognizing where gaps exist and 
determining which kinds of background knowl-
edge are needed.   
We are looking for the synchronization between 
the text representation achievable by current NLP 
and a knowledge representation (KR) scheme that 
can permit further inference for text interpretation.   
1.1 Vision
Clearly, producing a rich text interpretation re-
quires both NLP and KR capabilities.  The strategy 
we explore is the enablement of bidirectional 
communication between the two sides from the 
very beginning of the text processing. We assume 
that the KR system doesn?t require a full represen-
tation of the text meaning, but can work with a par-
tial interpretation, namely of the material explicitly 
present in the text, and can then flesh out this in-
terpretation as required for its specific task. Al-
though the NLP system initially provides simpler 
representations (even possibly ambiguous or 
wrong ones), the final result contains the semantics 
of the text according to the working domain.  
In this model, the following questions arise: 
How much can we simplify our initial text repre-
sentation and still permit the attachment of back-
ground knowledge for further inference and 
interpretation?  How should background knowl-
edge be represented for use by the KR system?  
How can the incompleteness and brittleness typical 
of background knowledge (its representational in-
flexibility, or limitation to a single viewpoint or 
expressive phrasing) (Barker 2007) be overcome?  
In what sequence can a KR system enrich an initial 
and/or impoverished reading, and how can the en-
richment benefit subsequent text processing?   
1.2 Approach
Although we are working toward it, we do not yet 
have such a system.  The aim of our current work 
is to rapidly assemble some necessary pieces and 
explore how to (i) attach background knowledge to 
flesh out a simple text representation and (ii) there 
by make explicit the meanings attached to some of 
its syntactic relations.  We begin with an initial 
simple text representation, a background knowl-
edge base corresponding to the text, and a simple 
15
formalized procedure to attach elements from the 
background knowledge to the entities and implicit 
relations present in the initial text representation.   
Surprisingly, we find that some quite simple 
processing can be effective if we are able to con-
textualize the text under interpretation. 
For our exploratory experiments, we are work-
ing with a collection of 30,000 documents in the 
domain of US football. We parsed the collection 
using a standard dependency parser (Marneffe and 
Manning, 2008; Klein and Maning, 2003) and, af-
ter collapsing some syntactic dependencies, ob-
tained the simple textual representations shown in 
Section 2. From them, we built a Background 
Knowledge Base by automatically harvesting 
propositions expressed in the collection (Section 
3). Their frequency in the collection lead the en-
richment process: given a new text in the same 
domain, we build exactly the same kind of repre-
sentation, and attach the background knowledge 
propositions as related to the text (Section 4).  
Since this is an exploratory sketch, we cannot 
provide a quantitative evaluation yet, but the quali-
tative study over some examples suggest that this 
simple framework is promising enough to start a 
long term research (Section 5). Finally, we con-
clude with the next steps we want to follow and the 
kind of evaluation we plan to do.  
2 Text Representation 
The starting text representation must capture the 
first shot of what?s going on in the text, taking 
some excerpts into account and (unfortunately) 
losing others. After the first shot, in accord with 
the purpose of the reading, we will ?contextualize? 
each sentence, expanding its initial representation 
with the relevant related background knowledge in 
our base. 
During this process of making explicit the im-
plicit semantic relations (which we call contextu-
alization or interpretation) it will become apparent 
whether we need to recover some of the discarded 
elements, whether we need to expand some others, 
etc. So the process of interpretation is identified 
with the growing of the context (according to the 
KB) until the interpretation is possible. This is re-
lated to some well-known theories such as the 
Theory of Relevance (Sperber and Wilson, 1995). 
The particular method we envisage is related to 
Interpretation as Abduction (Hobbs et al 1993). 
How can the initial information be represented 
so as to enable the context to grow into an interpre-
tation? We hypothesize that: 
1. Behind certain syntactic dependencies there 
are semantic relations. 
2. In the case of dependencies between nouns, 
this semantic relation can be made more ex-
plicit using verbs and/or prepositions. The 
knowledge base must help us find them. 
We look for a semantic representation close 
enough to the syntactic representation we can ob-
tain from the dependency graph. The main syntac-
tic dependencies we want to represent in order to 
enable enrichment are: 
1. Dependencies between nouns such as noun-
noun compounds (nn) or possessive (poss). 
2. Dependencies between nouns and verbs, 
such as subject and object relations. 
3. Prepositions having two nouns as argu-
ments. Then the preposition becomes the la-
bel for the relation between the two nouns, 
being the object of the preposition the target 
of the relation. 
For these selected elements, we produce two very 
simple transformations of the syntactic dependency 
graph:
1. Invert the direction of the syntactic depend-
ency for the modifiers. Since we work with 
the hypothesis that behind a syntactic de-
pendency there is a semantic relation, we re-
cord the direction of the semantic relation. 
2. Collapse the syntactic dependencies be-
tween verb, subject, and object into a single 
semantic relation. Since we are assuming 
that the verb is the more explicit expression 
of a semantic relation, we fix this in the ini-
tial representation. The subject will be the 
source of the relation and the object will be 
the target of the relation. When the verb has 
more arguments we consider its expansion 
as a new node as referred in Section 4.4.  
Figure 1 shows the initial minimal representa-
tion for the sentence we will use for our discus-
sion:
San_Francisco's Eric_Davis intercepted 
a Steve_Walsh pass on the next series to 
set_up a seven-yard Young touchdown pass 
to Brent_Jones.
Notice that some pieces of the text are lost in the 
initial representation of the text as for example ?on 
the next series? or ?seven-yard?.
16
3    Background Knowledge Base  
The Background Knowledge Base (BKB) is built 
from a collection in the domain of the texts we 
want to semanticize. The collection consists of 
30,826 New York Times news about American 
football, similar to the kind of texts we want to 
interpret. The elements in the BKB (3,022,305 in 
total) are obtained as a result of applying general 
patterns over dependency trees. We take advantage 
of the typed dependencies (Marneffe and Manning, 
2008) produced by the Stanford parser (Klein and 
Maning, 2003). 
3.1 Types of elements in the BKB 
We distinguish three elements in our Background 
Knowledge Base: Entities, Propositions, and Lexi-
cal relations. All of them have associated their fre-
quency in the reference collection. 
Entities
We distinguish between entity classes and entity 
instances:
1. Entity classes: Entity classes are denoted by 
the nouns that participate in a copulative rela-
tion or as noun modifier. In addition, we intro-
duce two special classes: Person and Group. 
These two classes are related to the use of pro-
nouns in text. Pronouns ?I?, ?he? and ?she? are 
linked to class Person. Pronouns ?we? and 
?they? are linked to class Group. For example, 
the occurrence of the pronoun ?he? in ?He 
threw a pass? would produce an additional 
count of the proposition ?person:throw:pass?. 
2. Entity Instances: Entity instances are indicated 
by proper nouns. Proper nouns are identified 
by the part of speech tagging. Some of these 
instances will participate in the ?has-instance? 
relation (see below).   When they participate in 
a proposition they produce proposition in-
stances.
Figure 1. Representation of the sentence: San_Francisco's Eric_Davis intercepted a Steve_Walsh
pass on the next series to set_up a seven-yard Young touchdown pass to Brent_Jones. 
Propositions
Following Clark and Harrison (2009) we call 
propositions the tuples of words that have some 
determined pattern of syntactic relations among 
them. We focus on NVN, NVNPN and NPN 
proposition types. For example, a NVNPN propo-
sition is a full instantiation of: 
Subject:Verb:Object:Prep:Complement
The first three elements are the subject, the verb 
and the direct object. Fourth is the preposition that 
attaches the PP complement to the verb. For sim-
plicity, indirect objects are considered as a Com-
plement with the preposition ?to?. 
The following are the most frequent NVN 
propositions in the BKB ordered by frequency. 
NVN 2322 'NNP':'beat':'NNP' 
NVN 2231 'NNP':'catch':'pass' 
NVN 2093 'NNP':'throw':'pass' 
NVN 1799 'NNP':'score':'touchdown' 
NVN 1792 'NNP':'lead':'NNP' 
NVN 1571 'NNP':'play':'NNP' 
NVN 1534 'NNP':'win':'game' 
NVN 1355 'NNP':'coach':'NNP' 
NVN 1330 'NNP':'replace':'NNP' 
NVN 1322 'NNP':'kick':'goal' 
NVN 1195 'NNP':'win':'NNP' 
NVN 1155 'NNP':'defeat':'NNP' 
NVN 1103 'NNP':'gain':'yard' 
The ?NNP? tag replaces specific proper nouns 
found in the proposition.  
When a sentence has more than one comple-
ment, a new occurrence is counted for each com-
plement. For example, given the sentence 
?Steve_Walsh threw a pass to Brent_Jones 
in the first quarter?, we would add a count to 
each of the following propositions: 
17
Steve_Walsh:throw:pass
Steve_Walsh:throw:pass:to:Brent_Jones
Steve_Walsh:throw:pass:in:quarter
Notice that right now we include only the heads 
of the noun phrases in the propositions. 
We call proposition classes the propositions that 
only involve instance classes (e.g., ?per-
son:throw:pass?), and proposition instances
those that involve at least one entity instance (e.g., 
?Steve_Walsh:throw:pass?).
Proposition instances are useful for the tracking 
of a entity instance. For example, 
?'Steve_Walsh':'supplant':'John_Fourcade':
'as':'quarterback'?. When a proposition in-
stance is found, it is stored also as a proposition 
class replacing the proper nouns by a special word 
(NNP) to indicate the presence of a entity instance. 
The enrichment of the text is based on the use of 
most frequent proposition classes.  
Lexical relations 
At the moment, we make use of the copulative 
verbs (detected by the Stanford?s parser) in order 
to extract ?is?, and ?has-instance? relations: 
1. Is: between two entity classes. They denote a 
kind of identity between both entity classes, 
but not in any specific hierarchical relation 
such as hyponymy. Neither is a relation of 
synonymy. As a result, is somehow a kind of 
underspecified relation that groups those more 
specific. For example, if we ask the BKB what 
a ?receiver? is, the most frequent relations are: 
290 'person':is:'receiver' 
29 'player':is:'receiver' 
16 'pick':is:'receiver' 
15 'one':is:'receiver' 
14 'receiver':is:'target' 
8 'end':is:'receiver' 
7 'back':is:'receiver' 
6 'position':is:'receiver' 
The number indicates the number of times the 
relation appears explicitly in the collection. 
2. Has-instance: between an entity class and an 
entity instance. For example, if we ask for in-
stances of team, the top 10 instances with more 
support in the collection are: 
192 'team':has-instance:'Jets' 
189 'team':has-instance:'Giants' 
43 'team':has-instance:'Eagles' 
40 'team':has-instance:'Bills' 
36 'team':has-instance:'Colts' 
35 'team':has-instance:'Miami' 
35 'team':has-instance:'Vikings' 
34 'team':has-instance:'Cowboys' 
32 'team':has-instance:'Patriots' 
31 'team':has-instance:'Dallas' 
But we can ask also for the possible classes of 
an instance. For example, all the entity classes for 
?Eric_Davis? are: 
12 'cornerback':has-instance:'Eric_Davis' 
1 'hand':has-instance:'Eric_Davis' 
1 'back':has-instance:'Eric_Davis'  
There are other lexical relations as ?part-of? and 
?is-value-of? in which we are still working. For 
example, the most frequent ?is-value-of? relations 
are:
5178 '[0-9]-[0-9]':is-value-of:'lead' 
3996 '[0-9]-[0-9]':is-value-of:'record' 
2824 '[0-9]-[0-9]':is-value-of:'loss' 
1225 '[0-9]-[0-9]':is-value-of:'season' 
4 Enrichment procedure 
The goal of the enrichment procedure is to deter-
mine what kind of events and entities are involved 
in the text, and what semantic relations are hidden 
by some syntactic dependencies such as noun-noun 
compound or some prepositions. 
4.1 Fusion of nodes 
Sometimes, the syntactic dependency ties two or 
more words that form a single concept. This is the 
case with multiword terms such as ?tight end?, 
?field goal?, ?running back?, etc. In these cases, 
the meaning of the compound is beyond the syn-
tactic dependency. Thus, we shouldn?t look for its 
explicit meaning. Instead, we activate the fusion of 
the nodes into a single one. 
However, there are some open issues related to 
the cases were fusion is not preferred. Otherwise, 
the process could be done with standard measures 
like mutual information, before the parsing step 
(and possibly improving its results). 
The question is whether the fusion of the words 
into a single expression allows or not the consid-
eration of possible paraphrases. For example, in 
the case of ?field:nn:goal?, we don?t find other 
ways to express the concept in the BKB. However, 
in the case of ?touchdown:nn:pass? we can find, 
for example, ?pass:for:touchdown? a significant 
amount of times, and we want to identify them as 
equivalent expressions. For this reason, we find not 
convenient to fuse these cases. 
18
4.2 Building context for instances 
Suppose we wish to determine what kind of entity 
?Steve Walsh? is in the context of the syntactic 
dependency ?Steve_Walsh:nn:pass?. First, we 
look into the BKB for the possible entity classes of 
Steve_Walsh previously found in the collection. In 
this particular case, the most frequent class is 
?quarterback?:
40 'quarterback':has-instance:'Steve_Walsh' 
2 'junior':has-instance:'Steve_Walsh' 
But, what happens if we see ?Steve_Walsh? for 
the first time? Then we need to find evidence from 
other entities in the same syntactic context. We 
found that ?Marino?, ?Kelly?, ?Elway?,
?Dan_Marino?, etc. appear in the same kind of 
proposition (?N:nn:pass?) where we found 
?Steve_Walsh?, each of them supported by 24, 17, 
15 and 10 occurrences respectively. However, 
some of the names can be ambiguous. For exam-
ple, searching for ?Kelly? in our BKB yields: 
153 'quarterback':has-instance:'Jim_Kelly' 
19 'linebacker':has-instance:'Joe_Kelly' 
17 'quarterback':has-instance:'Kelly' 
14 'quarterback':has-instance:'Kelly_Stouffer' 
10 'quarterback':has-instance:'Kelly_Ryan' 
8 'quarterback':has-instance:'Kelly_Holcomb' 
7 'cornerback':has-instance:'Brian_Kelly'  
Whereas others are not so ambiguous: 
113 'quarterback':has-instance:'Dan_Marino' 
6 'passer':has-instance:'Dan_Marino' 
5 'player':has-instance:'Dan_Marino'  
Taking this into account, we are able to infer that 
the most plausible class for an entity involved in a 
?NNP:nn:pass? proposition is a quarterback. 
4.3 Building context for dependencies 
Now we want to determine the meaning behind 
such syntactic dependencies as 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?, 
?Young:nn:pass? or ?pass:to:Brent_Jones?. 
We have two ways for adding more meaning to 
these syntactic dependencies: find the most appro-
priate prepositions to describe them, and find the 
most appropriate verbs. Whether one, the other or 
both is more useful has to be determined during the 
reasoning system development. 
Finding the prepositions 
There are several types of propositions in the 
BKB that involve prepositions. The most relevant 
are NPN and NVNPN. In the case of ?touch-
down:nn:pass?, preposition ?for? is clearly the best 
interpretation for the ?nn? dependency: 
NPN 712 'pass':'for':'touchdown' 
NPN 24 'pass':'include':'touchdown' 
NPN 3 'pass':'with':'touchdown' 
NPN 2 'pass':'of':'touchdown' 
NPN 1 'pass':'in':'touchdown' 
NPN 1 'pass':'follow':'touchdown' 
NPN 1 'pass':'to':'touchdown' 
In the case of ?Steve_Walsh:nn:pass? and 
?Young:nn:pass?, assuming they are quarterbacks, 
we can ask for all the prepositions between ?pass? 
and ?quarterback?: 
NPN 23 'pass':'from':'quarterback' 
NPN 14 'pass':'by':'quarterback' 
NPN 2 'pass':'of':'quarterback' 
NPN 1 'pass':'than':'quarterback' 
NPN 1 'pass':'to':'quarterback' 
Notice how lower frequencies involve more 
noisy options. 
If we don?t have any evidence on the instance 
class, and we know only that they are instances, 
the pertinent query to the BKB obtains: 
NPN 1305 'pass':'to':'NNP' 
NPN 1085 'pass':'from':'NNP' 
NPN 147 'pass':'by':'NNP' 
NPN 144 'pass':'for':'NNP' 
In the case of ?Young:nn:pass? (in ?Young 
pass to Brent Jones?), there exists already the 
preposition ?to? (?pass:to:Brent_Jones?), so the 
most promising choice become the second, 
?pass:from:Young?, which has one order of magni-
tude more occurrences than the following. 
In the case of ?Steve_Walsh:nn:pass? (in ?Eric 
Davis intercepted a Steve Walsh pass?) we can use 
additional information: we know that 
?Eric_Davis:intercept:pass?. So, we can try to 
find the appropriate preposition using NVNPN 
propositions in the following way: 
Eric_Davis:intercept:pass:P:Steve_Walsh?
Asking the BKB about the propositions that in-
volve two instances with ?intercept? and ?pass? we 
get:
NVNPN 48 'NNP':'intercept':'pass':'by':'NNP' 
NVNPN 26 'NNP':'intercept':'pass':'at':'NNP' 
NVNPN 12 'NNP':'intercept':'pass':'from':'NNP' 
We could also query the BKB with the classes 
we already found for ?Eric_Davis? (cornerback, 
player, person): 
NVNPN 11 'person':'intercept':'pass':'by':'NNP' 
NVNPN 4 'person':'intercept':'pass':'at':'NNP' 
NVNPN 2 'person':'intercept':'pass':'in':'NNP' 
19
NVNPN 2 'person':'intercept':'pass':'against':'NNP' 
NVNPN 1 'cornerback':'intercept':'pass':'by':'NNP' 
All these queries accumulate evidence over a cor-
rect preposition ?by? (?pass:by:Steve_Walsh?). 
However, an explicit entity classification would 
make the procedure more robust. 
Finding the verbs 
Now the exercise is to find a verb able to give 
meaning to the syntactic dependencies such as 
?Steve_Walsh:nn:pass?, ?touchdown:nn:pass?, 
?Young:nn:pass? or ?pass:to:Brent_Jones?.
We can ask the BKB what instances (NNP) do 
with passes. The most frequent propositions are: 
NVN 2241 'NNP':'catch':'pass' 
NVN 2106 'NNP':'throw':'pass' 
NVN 844 'NNP':'complete':'pass' 
NVN 434 'NNP':'intercept':'pass' 
NVNPN 758 'NNP':'throw':'pass':'to':'NNP' 
NVNPN 562 'NNP':'catch':'pass':'for':'yard' 
NVNPN 338 'NNP':'complete':'pass':'to':'NNP' 
NVNPN 255 'NNP':'catch':'pass':'from':'NNP' 
Considering the evidence of ?Brent_Jones? be-
ing instance of ?end? (tight end), if we ask the 
BKB about the most frequent relations between 
?end? and ?pass? we find: 
NVN 28 'end':'catch':'pass' 
NVN 6 'end':'drop':'pass' 
So, in this case, the BKB suggests that the syn-
tactic dependency ?pass:to:Brent_Jones? means 
?Brent_Jones is an end catching a pass?. Or in 
other words, that ?Brent_Jones? has a role of 
?catch-ER? with respect to ?pass?. 
If we want to accumulate more evidence on this 
we can consider NVNPN propositions including 
touchdown. We only find evidence for the most 
general classes (NNP and person): 
NVNPN 189 'NNP':'catch':'pass':'for':'touchdown' 
NVNPN 26 'NNP':'complete':'pass':'for':'touchdown' 
NVNPN 84 'person':'catch':'pass':'for':'touchdown' 
NVNPN 18 'person':'complete':'pass':'for':'touchdown' 
This means, that when we have ?touchdown?, 
we don?t have counting for the second option 
?Brent_Jones:drop:pass?, while ?catch? becomes 
stronger.
In the case of ?Steve_Walsh:nn:pass? we hy-
pothesize that ?Steve_Walsh? is a quarterback. 
Asking the BKB about the most plausible relation 
between a quarterback and a pass we find: Figure 2. Graphical representation of the enriched 
text.
20
NVN 98 'quarterback':'throw':'pass' 
NVN 27 'quarterback':'complete':'pass' 
Again, if we take into account that it is a 
?touchdown:nn:pass?, then only the second op-
tion ?Steve_Walsh:complete:pass? is consistent 
with the NVNPN propositions. 
So, in this case, the BKB suggests that the syn-
tactic dependency ?Steve_Walsh:nn:pass? means 
?Steve_Walsh is a quarterback completing a pass?. 
Finally, with respect to ?touchdown:nn:pass?, 
we can ask about the verbs that relate them: 
NVN 14 'pass':'set_up':'touchdown' 
NVN 6 'pass':'score':'touchdown' 
NVN 5 'pass':'produce':'touchdown' 
Figure 2 shows the graphical representation of 
the sentence after some enrichment. 
4.4 Expansion of relations 
Sometimes, the sentence shows a verb with several 
arguments. In our example, we have 
?Eric_David:intercept:pass:on:series?. In 
these cases, the relation can be expanded and be-
come a node. 
In our example, the new node is the eventuality 
of ?intercept? (let?s say ?intercept-ION?), 
?Eric_Davis? is the ?intercept-ER? and ?pass? is 
the ?intercept-ED?. Then, we can attach the miss-
ing information to the new node (see Figure 3).  
Figure 3. Expansion of the "intercept" relation.  
In addition, we can proceed with the expansion 
of the context considering this new node. For ex-
ample, we are working with the hypothesis that 
?Steve_Walsh? is an instance of quarterback and 
thus, its most plausible relations with pass are 
?throw? and ?complete?. However, now we can 
ask about the most frequent relation between 
?quarterback? and ?interception?. The most fre-
quent is ?quarterback:throw:interception?
supported 35 times in the collection. From this, 
two actions can be done: reinforce the hypothesis 
of ?throw:pass? instead of ?complete:pass?, and 
add the hypothesis that 
?Steve_Walsh:throw:interception?.
Finally, notice that since ?set_up? doesn?t need 
to accommodate more arguments, we can maintain 
the collapsed edge. 
4.5 Constraining the interpretations 
Some of the inferences being performed are local 
in the sense that they involve only an entity and a 
relation. However, these local inferences must be 
coherent both with the sentence and the complete 
document. 
To ensure this coherence we can use additional 
information as a way to constrain different hy-
potheses. In section 4.3 we showed the use of 
NVNPN propositions to constrain NVN ones. 
 Another example is the case of 
?Eric_Davis:intercept:pass?. We can ask the 
BKB for the entity classes that participate in such 
kind of proposition: 
NVN 75 'person':'intercept':'pass' 
NVN 14 'cornerback':'intercept':'pass' 
NVN 11 'defense':'intercept':'pass' 
NVN 8 'safety':'intercept':'pass' 
NVN 7 'group':'intercept':'pass' 
NVN 5 'linebacker':'intercept':'pass' 
So the local inference for the kind of entity 
?Eric_Davis? is (cornerback) must be coherent 
with the fact that it intercepted a pass. In this case 
?cornerback? and ?person? are properly reinforced. 
In some sense, we are using these additional con-
strains as shallow selectional preferences. 
5 Evaluation
The evaluation of the enrichment process is a chal-
lenge by itself. Eventually, we will use extrinsic 
measures such as system performance on a QA 
task, applied first after reading a text, and then a 
second time after the enrichment process. This will 
measure the ability of the system to absorb and use 
knowledge across texts to enrich the interpretation 
of the target text.  In the near term, however, it re-
mains unclear which intrinsic evaluation measures 
to apply.  It is not informative simply to count the 
number of additional relations one can attach to 
representation elements, or to count the increase in 
degree of interlinking of the nodes in the represen-
tation of a paragraph.   
21
6 Related Work 
To build the knowledge base we take an approach 
closely related to DART (Clark and Harrison, 
2009) which in turn is related to KNEXT (Van 
Durme and Schubert, 2008). It is also more dis-
tantly related to TextRunner (Banko et al 2007). 
Like DART, we make use of a dependency 
parser instead of partial parsing. So we capture 
phrase heads instead complete phrases. The main 
differences between the generation of our BKB 
and the generation of DART are: 
1. We use the dependencies involving copula-
tive verbs as a source of evidence for ?is? 
and ?has-instance? relations. 
2. Instead of replacing proper nouns by ?per-
son?, ?place?, or ?organization?, we con-
sider all of them just as instances in our 
BKB. Furthermore, when a proposition con-
tains a proper noun, we count it twice: one 
as the original proposition instance, and a 
second replacing the proper nouns with a 
generic tag indicating that there was a name. 
3. We make use of the modifiers that involve 
an instance (proper noun) to add counting to 
the ?has-instance? relation. 
4. Instead of replacing pronouns by ?person? 
or ?thing?, we replace them by ?person?, 
?group? or ?thing?, taking advantage of the 
preposition number. This is particular useful 
for the domain of football where players and 
teams are central. 
5. We add a new set of propositions that relate 
two clauses in the same sentence (e.g., 
Floyd:break:takle:add:touchdown). We 
tagged these propositions NVV, NVNV, 
NVVN and NVNVN. 
6. Instead of an unrestricted domain collection, 
we consider documents closely related to the 
domain in which we want to interpret texts. 
The consideration of a specific domain collec-
tion seems a very powerful option. Ambiguity is 
reduced inside a domain so the counting for propo-
sitions is more robust. Also frequency distribution 
of propositions is different from one domain into 
another. For example, the list of the most frequent 
NVN propositions in our BKB (see Section 3.1) is, 
by itself, an indication of the most salient and im-
portant events in the American football domain. 
7 Conclusion and Future Work
The task of inferring omitted but necessary infor-
mation is a significant part of automated text inter-
pretation. In this paper we show that even simple 
kinds of information, gleaned relatively straight-
forwardly from a parsed corpus, can be quite use-
ful.  Though they are still lexical and not even 
starting to be semantic, propositions consisting of 
verbs as relations between nouns seem to provide a 
surprising amount of utility.  It remains a research 
problem to determine what kinds and levels of 
knowledge are most useful in the long run.   
In the paper, we discuss only the propositions 
that are grounded in instantial statements about 
players and events.  But for true learning by read-
ing, a system has to be able to recognize when the 
input expresses general rules, and to formulate 
such input as axioms or inferences.  In addition, 
augmenting that is the significant challenge of 
generalizing certain kinds of instantial propositions 
to produce inferences.  At which point, for exam-
ple, should the system decide that ?all football 
players have teams?, and how should it do so? 
How to do so remains a topic for future work.   
A further topic of investigation is the time at 
which expansion should occur.  Doing so at ques-
tion time, in the manner of traditional task-oriented 
back-chaining inference, is the obvious choice, but 
some limited amount of forward chaining at read-
ing time seems appropriate too, especially if it can 
significantly assist with text processing tasks, in 
the manner of expectation-driven understanding.    
Finally, as discussed above, the evaluation of 
our reading augmentation procedures remains to be 
developed.
Acknowledgments 
We are grateful to Hans Chalupsky and David 
Farwell for their comments and input along this 
work. This work has been partially supported by 
the Spanish Government through the "Programa 
Nacional de Movilidad de Recursos Humanos del 
Plan Nacional de I+D+i 2008-2011 (Grant 
PR2009-0020). We acknowledge the support of 
DARPA contract number: FA8750-09-C-0172. 
22
References
1. Banko, M., Cafarella, M., Soderland, S., 
Broadhead, M., Etzioni, O. 2007. Open Infor-
mation Extraction from the Web. IJCAI 2007. 
2. Barker, K. 2007. Building Models by Reading 
Texts. Invited talk at the AAAI 2007 Spring 
Symposium on Machine Reading, Stanford 
University. 
3. Clark, P. and Harrison, P. 2009. Large-scale 
extraction and use of knowledge from text. 
The Fifth International Conference on Knowl-
edge Capture (K-CAP 2009). 
http://www.cs.utexas.edu/users/pclark/dart/ 
4. Hobbs, J.R., Stickel, M., Appelt, D. and Mar-
tin, P., 1993. Interpretation as Abduction. Arti-
ficial Intelligence, Vol. 63, Nos. 1-2, pp. 69-
142.
http://www.isi.edu/~hobbs/interp-abduct-ai.pdf 
5. Klein, D. and Manning, C.D. 2003. Accurate 
Unlexicalized Parsing. Proceedings of the 41st 
Meeting of the Association for Computational 
Linguistics, pp. 423-430 
6. Marneffe, M. and Manning, C.D. 2008. The 
Stanford typed dependencies representation. In 
COLING 2008 Workshop on Cross-framework 
and Cross-domain Parser Evaluation. 
7. Sperber, D. and Wilson, D. 1995. Relevance: 
Communication and cognition (2nd ed.) Ox-
ford, Blackwell. 
8. Van Durme, B., Schubert, L. 2008. Open 
Knowledge Extraction through Compositional 
Language Processing. Symposium on Seman-
tics in Systems for Text Processing, STEP 
2008. 
23
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, page 79,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Injecting Linguistics into NLP through Annotation
Eduard Hovy
USC/ISI
4676 Admiralty Way
Marina del Rey, CA 90292
USA
hovy@isi.edu
Over the past 20 years, the size of the L in Com-
putational Linguistics has been shrinking relative
to the size of the C. The result is that we are in-
creasingly becoming a community of uninformed
but sophisticated engineers, applying to problems
very complex machine learning techniques that
use very simple (simplistic?) analyses/theories.
(Try finding a theoretical account of subjectiv-
ity, opinion, entailment, or inference in publica-
tions surrounding the associated competitions of
the past few years.)
When we grow tired of embarrassing ourselves,
what should we do? Fortunately, injecting some
linguistic (and other) sophistication into our work
is not that complicated. The key is annotation: by
using a theoretically informed set of choices rather
than a bottom-up naive one, we can have annota-
tors tag corpora with labels that reflect some un-
derlying theories. While the large-C contingent
of our community will not care, researchers in-
terested in investigating language rather than pro-
cessing will be able to find new ways to connect
with Corpus Linguists, Psycholinguists, and even
Ontologists.
It turns out that many of our surrounding aca-
demic communities ? Linguists, Political Scien-
tists, Biocurators, etc. ? have been performing an-
notation for years in order to build and prove their
theories. They have however been largely unaware
of the power of NLP technology and the benefits
we can bring to them. There is a natural marriage
? several, actually ? waiting to happen.
What is the benefit to us? What?s wrong with
simply continuing to use half-baked annotation
schemes to train our machine learning systems on?
Several things:
? half-baked schemes generally fail in the long
run-that?s why more-sophisticated ones are
developed
? there are dozens to hundreds of graduate
students and young researchers in surround-
ing communities eager to help build cor-
pora by running annotation efforts and using
the problems uncovered while annotating to
drive further theory formation
? because they?re generally more ?correct?,
more-sophisticated annotations allow stack-
ing of multiple phenomena upon the same
material with fewer internal inconsistencies
and problems.
Such stacking eventually enables multi-
phenomenon analysis and mutual disambiguation
in ways that an incommensurately annotated
corpus does not.
79
Proceedings of the Workshop on Negation and Speculation in Natural Language Processing, page 50,
Uppsala, July 2010.
Negation and Modality in Distributional Semantics
Ed Hovy
Information Sciences Institute
University of Southern California
hovy@isi.edu
Abstract
In Natural Language Processing, negation and modality have mostly been handled
using the older, pre-statistical methodologies of formal representations subject to
rule-based processing. This fits the traditional treatment of negation and modality
in logic-based knowledge representation and linguistics. However, in modern-day
statistics-based NLP, how exactly negation and modality should be taken into ac-
count, and what role these phenomena play overall, is much less clear. The closest
statistics-based NLP gets to semantics at this time is lexical-based word distribu-
tions (such as used in word sense disambiguation) and topic models (such as pro-
duced by Latent Dirichlet Allocation). What exactly in such representations should
a negation or a modality actually apply to? What would, or should, the resulting
effects be? The traditional approaches are of little or no help.
In this talk I argue that neither model is adequate, and that one needs a different
model of semantics to be able to accommodate negation and modality. The tradi-
tional formalisms are impoverished in their absence of an explicit representation
of the denotations of each symbol, and the statistics-based word distributions do
not support the compositionality required of semantics since it is unclear how to
link together two separate word distributions in a semantically meaningful way. A
kind of hybrid, which one could call Distributional Semantics, should be formu-
lated to include the necessary aspects of both: the ability to carry explicit word
associations that are still partitioned so as to allow negation and modality to affect
the representations in intuitively plausible ways is what is required.
I present a specific model of Distributional Semantics that, although still rudi-
mentary, exhibits some of the desired features. I explore the possibilities for ac-
commodating the phenomena of negation and modality. The talk poses many more
questions than it answers and is an invitation to consider Distributional Semantics
as a model for richer and more semantics-oriented statistics-based NLP.
50
Proceedings of the 2nd Workshop on Cognitive Aspects of the Lexicon (CogALex 2010), page 1,
Beijing, August 2010
INVITED KEYNOTE PRESENTATION 
  Distributional Semantics and the Lexicon 
 Eduard Hovy Information Sciences Institute  University of Southern California  
hovy@isi.edu 
 
  The lexicons used in computational linguistics systems contain morphological, syntactic, and occasionally also some semantic information (such as definitions, pointers to an ontology, verb frame filler preferences, etc.). But the human cognitive lexicon contains a great deal more, crucially, expectations about how a word tends to combine with others: not just general information-extraction-like patterns, but specific instantial expectations. Such in-formation is very useful when it comes to lis-tening in bad aural conditions and reading texts in which background information is taken for granted; without such specific ex-pectation, one would be hard-pressed (and computers are completely unable) to form co-herent and richly connected multi-sentence interpretations. Over the past few years, NLP work has in-creasingly treated topic signature word distri-butions (also called ?context vectors?, ?topic models?, etc.) as a de facto replacement for semantics. Whether the task is wordsense dis-ambiguation, certain forms of textual entail-ment, information extraction, paraphrase learning, and so on, it turns out to be very use-ful to consider a word(sense) as being defined by the distribution of word(senses) that regu-  
larly accompany it (in the classic words of Firth, ?you shall know a word by the company it keeps?). And this is true not only for indi-vidual wordsenses, but also for larger units such as topics: the product of LDA and similar topic characterization engines is similar. In this talk I argue for a new kind of seman-tics, which is being called Distributional Se-mantics. It combines traditional symbolic logic-based semantics with (computation-based) statistical word distribution information. The core resource is a single lexico-semantic lexicon that can be used for a variety of tasks, provided that it is reformulated accordingly. I show how to define such a semantics, how to build the appropriate lexicon, how to format it, and how to use it for various tasks. The talk pulls together a wide range of related topics, including Pantel-style resources like DIRT, inferences / expectations such as those used in Schank-style expectation-based parsing and expectation-driven NLU, PropBank-style word valence lexical items, and the treatment of negation and modalities. I conclude by arguing that the human cognitive lexicon has to have the same kinds of properties as the Distributional Semantics lexicon, given the ways people do things with words.   
1
A New Semantics:  
Merging Propositional and Distributional Information  
Eduard Hovy 
Information Sciences Institute  
University of Southern California  
hovy@isi.edu 
 
Despite hundreds of years of study on semantics, theories and representations of semantic 
content?the actual meaning of the symbols used in semantic propositions?remain 
impoverished.  The traditional extensional and intensional models of semantics are difficult 
to actually flesh out in practice, and no large-scale models of this kind exist.  Recently, 
researchers in Natural Language Processing (NLP) have increasingly treated topic signature 
word distributions (also called ?context vectors?, ?topic models?, ?language models?, etc.) as a 
de facto placeholder for semantics at various levels of granularity.  This talk argues for a new 
kind of semantics that combines traditional symbolic logic-based proposition-style semantics 
(of the kind used in older NLP) with (computation-based) statistical word distribution 
information (what is being called Distributional Semantics in modern NLP).  The core 
resource is a single lexico-semantic ?lexicon? that can be used for a variety of tasks.  I show 
how to define such a lexicon, how to build and format it, and how to use it for various tasks. 
Combining the two views of semantics opens many fascinating questions that beg study, 
including the operation of logical operators such as negation and modalities over word(sense) 
distributions, the nature of ontological facets required to define concepts, and the action of 
compositionality over statistical concepts.   
 
14
Granularity in Natural Language Discourse
Rutu Mulkar-Mehta, Jerry Hobbs and Eduard Hovy
University of Southern California, Information Sciences Institute
me@rutumulkar.com, hobbs@isi.edu, hovy@isi.edu
Abstract
This paper discusses the phenomenon of granularity in natural language1. By ?granularity? we
mean the level of detail of description of an event or object. Humans can seamlessly shift their gran-
ularity perspective while reading or understanding a text. To emulate this mechanism, we describe
a set of features that identify the levels of granularity in text, and empirically verify this feature set
using a human annotation study for granularity identification. This theory is the foundation for any
system that can learn the (global) behavior of event descriptions from (local) behavior descriptions.
This is the first research initiative, to our knowledge, for identifying granularity shifts in natural
language descriptions.
1 Introduction
Granularity is the concept of breaking down an event into smaller parts or granules such that each indi-
vidual granule plays a part in the higher level event. For example, the activity of driving to the grocery
store involves some fine-grained events like opening the car door, starting the engine, planning the route,
and driving to the destination. Each of these may in turn be decomposed further into finer levels of
granularity. For instance, planning the route might involve entering an address into GPS and following
directions. The phenomenon of granularity is observed in various domains, including scientific literature,
game reports, and political descriptions. In scientific literature, the process of photosynthesis on closer
examination is made up of smaller individual fine-grained processes such as the light dependent reaction
and the light independent reaction.
Granularity is not a new concept. It has been studied actively in various disciplines. In philosophy, Bit-
tner and Smith (2001) have worked on formalizing granularity and part-hood relations. In information
retrieval, Lau et al (2009) have used granularity concepts to extract relevant detail of information result-
ing from a given search query. In theoretical computer science and ontology development, Keet (2008)
has worked on formalizing the concept of entity granularity and hierarchy and applied it biological sci-
ences. In natural language processing, Mani (1998) has worked on applying concepts of granularity to
polysemy and Hobbs (1985) has worked on using granularity for decomposing complex theories into
simple theories.
Although all of the above work emphasizes the importance of granularity relations for language un-
derstanding and formalization, none of it has attempted to observe whether granularity structures exist in
natural language texts, explored whether granularity structures can be identified and extracted automati-
cally, or tried to analyze how harvesting granularity relations can possibly help with other NLP problems.
This paper focuses on two items: First, we present a model of granularity as it exists in natural language
(Section 2); and second, we present an annotation study which we conducted to verify the proposed
model of granularity in natural language (Section 3).
1This research was supported by the Defense Advanced Research Projects Agency (DARPA) Machine Reading Program
under Air Force Research Laboratory (AFRL) prime contract no. FA8750-09-C-0172. Any opinions, findings, and conclusion
or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA,
AFRL, ONR, or the US government.
360
(a) (b)
Figure 1: 1(a): Granularity in Natural Language Descriptions; 1(b): Instantiating Natural Language to
the Granularity model
2 Modeling Granularity in Natural Language Texts
Humans can easily shift through various levels of granularity in understanding text. However, for auto-
mated granularity identification and extraction, it is necessary to explicitly recognize the identifiers that
indicate a shift in granularity. Figure 1(a) illustrates our theory of granularity. A granularity structure
exists only if at least two levels of information are present in text, such that the events at the coarse gran-
ularity can be decomposed into the events at the fine granularity, and the events at the fine granularity
combine together to form at least one segment of the event at the coarse granularity. In Figure 1(a),
Gc represents the phrase or sentence with coarse granularity information and Gf represents a phrase
or sentence with fine granularity information. Three types of relations can exist between the objects at
coarse and fine granularity: part-whole relationships between entities, part-whole relationships between
events, and causal relationships between the fine and coarse granularities. These relations signal a shift
in granularity. Instantiating text phrases into this model will expose granularities of text. For example,
consider the following sentence:
The San Francisco 49ers moved ahead 7?3 11 minutes into the game when William Floyd scored a two-yard
touchdown run.
The event of the player scoring a touchdown (the second clause of the sentence) is a decomposition
of the event of the team moving forward in the game (the first clause), and thus a finer granularity rep-
resentation of the San Francisco 49ers moving ahead in the game. When instantiated in our model of
granularity (Figure 1(a)), the graphical representation is shown in Figure 1(b).
Having described the overall model of granularity, we now elaborate on the components of the gran-
ularity model, namely part-whole relations and causal relations.
2.1 Part-Whole Relations
Two types of part-whole relations are present: meronymic and mereologic. Mereology (for more details
read Keet (2008)) is a partial ordering relation that is reflexive, transitive, and antisymmetric. According
to the concept of mereology, if x, y and z are three entities, then: x is a part of x; if x is part of y and y is
part of z then x is part of z; and if x is part of y then y cannot be part of x. However, various types of part-
whole relations that occur in natural language, such as member of, do not satisfy the transitivity relation,
in which case they will be mereologic but not meronymic: they might be ontologically accurate but
not linguistically correct. For instance, if John?s arm is part of John, and John is a member of a football
team, the transitivity relation that John?s arm is part of a football team, is not a valid meronymic relation.
Another instance which is mereologic but not meronymic is the following: A cup is made of steel, and
steel is made of molecules. Therefore a cup is made of molecules. The concept of mereology does not
361
reflect the way part of is used in natural language, and so mereology cannot be used for linguistic based
research.
One of the early works on part-whole relations in natural language (meronymy) Winston et al (1987)
was later refined in their empirical experiments Chaffin et al (1988). Winston et al discuss meronymic
relations and a taxonomy for representing them. They introduce six types of part-whole relationships:
(i) Component-Integral (e.g., pedal is a component of the integral bike), (ii) Member-Collection (e.g.,
a ship is a member of the collection, a fleet), (ii) Portion-Mass (e.g., a slice is a portion of the mass, a
pie), (iv) Stuff-Object (e.g., steel is one of the ingredients/stuff of the object car), (v) Feature-Activity
(e.g., paying is one of the features of the whole activity of shopping), (vi) Place-Area (e.g., Everglades
is a place within the area of Florida). The definition and classification in Winston et al (1987) for
part-whole relations is very relevant for language based analysis of part-whole relations. For granularity
identification in our work, the Feature-Activity type relation is used as the part-whole relation for events,
and the rest are part-whole relations for entities.
2.2 Causal Relations
Girju and Moldovan (2002) provide a broad compilation of causality research ranging from philosophy,
planning in AI, commonsense reasoning, and computational linguistics. Causation in computational
linguistics is the only form of causality that is relevant for granularity identification and extraction. The
following are the categories of causal constructs relevant for granularity identification and extraction:
? Causal Connectives: These are usually prepositional (such as because of, thanks to, due to), adver-
bial (such as for this reason, the result that), or clause links (such as because, since, for).
? Causation Verbs: These usually have a causal relation integrated with the verb. For example, kill,
melt (represent a causal link with the resulting situation), poison, hang, clean (represent a causal
link with the a part of the causing event)
? Conditionals: Girju and Moldovan (2002) describe conditionals as complex linguistic structures
typically of the form If S1 then S2. These structures represent causation, temporal relations, among
other relations, and are very complex structures in language.
3 Evaluation of the Granularity Model in Natural Language
We conducted an evaluation study to judge the ?goodness? of the granularity model proposed. In
this study the annotators were asked to annotate granularity relations between two given paragraphs.
Paragraph-based analysis was preferred to event-word-based analysis because people reason much more
easily with paragraph descriptions than with individual event mentions 2. The annotation set consisted of
paragraph pairs from three domains: travel articles (confluence.org), Timebank annotated data Pan et al
(2006), and Wikipedia articles on games. We selected a total of 37 articles: 10 articles about travel, 10
about games, and 17 from Timebank. Both paragraphs of a given question were selected from the same
article and referred to the same overall concept.
3.1 Annotation Task
The articles were uploaded to Mechanical Turk and were annotated by non-expert annotators (regular
Turkers). The entire set of 37 articles was annotated by 5 people. The annotators were given a pair
of paragraphs and were asked four questions about the relations between them: (i) Is one paragraph a
subevent of the other paragraph?, (ii) Did one paragraph cause the other paragraph?, (iii) Is one paragraph
less detailed and the other paragraph more detailed?, (iv) Did one paragraph happen after the other para-
graph? They were then presented with the comments of other annotators, and asked whether they agreed
2This was deduced as a result of an earlier annotation study for granularity identification using individual words as events.
362
(a) (b)
Figure 2: 2(a) shows the Inter-Annotator agreement for 37 articles and 2(b) shows the Pairwise Kappa
Agreement for 37 articles and 5 annotators
with any of the other annotations or explanations. The annotators were asked to provide a justification of
their choices.
3.2 Results
The Kappa statistic (Cohen (1960)) is the standard for measuring inter-annotator agreement: k =
(p(a)?p(e))
(1?p(e)) , where p(a) is the observed agreement and p(e) is the chance agreement between annota-
tors. More refined than simple Percentage Agreement, Kappa corrects for chance agreements.
In our study, two annotators were considered to be in agreement if they agreed with questions (i)
Subevents, (iii) More or less detail and (iv) Sequence. Unfortunately question (ii) Causality, as pro-
vided to the annotators, could not be taken into account for agreement measurement as individuals had
different conceptualizations of causality, and a crisp definition of causality was not provided to them.
For instance, consider the following two paragraphs:
1: I wanted to visit the confluence point located in the extreme southwest of Hunan Province.
2: To get to the confluence, I caught the Hong Kong-to-Shanghai intercity train on Friday afternoon.
Analysis: Some annotators annotated para2 causes para1, providing the explanation that the goal para1 could
be achieved due to the events of para2. Others annotated para1 causes para2, providing the justification that the
events of para2 only exist to fulfill the original goal para1. We are interested in the first type of causality, i.e.,
causality which explains how a given event happens. All the annotators agreed that a sub-event explains how an
event happens, or a sub-event causes an event. We counted this in lieu of our causality question (ii).
Figure 2(a) shows the overall agreement of the five annotators on the 37 articles and Figure 2(b) shows
the pairwise Kappa agreement for the five annotators. All the annotators agreed in 33/37 cases (23 article
pairs were annotated as having a granularity shift, 10 articles were annotated as having no granularity
shift). The average pairwise Kappa was 0.85. If the newspaper articles were removed, the overall agree-
ment was 100% for all the annotators. High agreement implied good quality of the annotation guidelines,
and provided evidence that people shift through various levels of granularity while reading and under-
standing text.
3.3 Analysis of the Causes of Disagreement
Where disagreements occurred, different interpretations of the same text were observed to be a major
cause. All these disagreements were limited to the newspaper articles. For instance, consider the follow-
ing:
363
1: Some 1,500 ethnic Albanians marched Sunday in downtown Istanbul, burning Serbian flags.
2: The police barred the crowd from reaching the Yugoslavian consulate in downtown Istanbul, but allowed them
to demonstrate on nearby streets.
Positive Granularity Shift: Some annotators commented that ?demonstrations? happen as a part of a ?march?.
So, para2 is a sub-event of para1.
Negative Granularity Shift: Other annotators felt that para2 happened after para1, and so there was no granular-
ity shift.
Overall, we can observe that although disagreement arises due to individual and unique interpretations
of text, people agree based on the discriminating features provided to them (part-whole relations and
causality) when identifying granularity shifts. This shows that part-whole relations and causality provide
a good set of features for identifying granularity shifts.
4 Conclusion and Future Work
In this paper we present the phenomenon of granularity as it occurs in natural language texts. We validate
our model of granularity with the help of an annotation study. We are currently developing a system for
automatic granularity extraction. We will compare its performance with state of the art techniques for
answering causality-style questions to empirically evaluate the significance of granularity structures for
automated Question Answering.
References
Bittner, T. and B. Smith (2001). Granular partitions and vagueness. Proceedings of the international
conference on Formal Ontology in Information Systems - FOIS ?01, 309?320.
Chaffin, R., D. J. Herrmann, and M. E. Winston (1988). An empirical taxonomy of part-whole relations:
Effects of part-whole relation type on relation identification. Language and Cognitive Processes 3(1).
Cohen, J. (1960). A coefficientof agreement for nominal scales. Educational and Psychological Mea-
surement 20, 37?46.
Girju, R. and D. Moldovan (2002). Mining Answers for Causation. Proceedings of American Association
of Artificial Intelligence, 15?25.
Hobbs, J. R. (1985). Granularity. In Proceedings of the Ninth International Joint Conference on Artificial
Intelligence, 432?435.
Keet, C. M. (2008). A Formal Theory of Granularity. Ph. D. thesis, Faculty of Computer Science, Free
University of Bozen-Balzano, Italy.
Lau, R. Y. K., C. C. L. Lai, and Y. Li (2009). Mining Fuzzy Ontology for a Web-Based Granular
Information Retrieval System. Lecture Notes in Computer Science, 239?246.
Mani, I. (1998). A Theory of Granularity and its Application to Problems of Polysemy and Underspec-
ification of Meaning. In Principles of Knowledge Representation and Reasoning: Proceedings of the
Sixth International Conference (KR?98), 245?255.
Pan, F., R. Mulkar, and J. R. Hobbs (2006). An Annotated Corpus of Typical Durations of Events. In
Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC),
77?83.
Winston, M. E., R. Chaffin, and D. Herrmann (1987, October). A Taxonomy of Part-Whole Relations.
Cognitive Science 11(4), 417?444.
364
Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 46?55,
Portland, Oregon, USA, June 23-24, 2011. c?2011 Association for Computational Linguistics
The Role of Information Extraction in the Design of a Document Triage
Application for Biocuration
Sandeep Pokkunuri
School of Computing
University of Utah
Salt Lake City, UT
sandeepp@cs.utah.edu
Cartic Ramakrishnan
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
cartic@isi.edu
Ellen Riloff
School of Computing
University of Utah
Salt Lake City, UT
riloff@cs.utah.edu
Eduard Hovy
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
hovy@isi.edu
Gully APC Burns
Information Sciences Institute
Univ. of Southern California
Marina del Rey, CA
burns@isi.edu
Abstract
Traditionally, automated triage of papers is
performed using lexical (unigram, bigram,
and sometimes trigram) features. This pa-
per explores the use of information extrac-
tion (IE) techniques to create richer linguistic
features than traditional bag-of-words models.
Our classifier includes lexico-syntactic pat-
terns and more-complex features that repre-
sent a pattern coupled with its extracted noun,
represented both as a lexical term and as a
semantic category. Our experimental results
show that the IE-based features can improve
performance over unigram and bigram fea-
tures alone. We present intrinsic evaluation
results of full-text document classification ex-
periments to determine automatically whether
a paper should be considered of interest to
biologists at the Mouse Genome Informatics
(MGI) system at the Jackson Laboratories. We
also further discuss issues relating to design
and deployment of our classifiers as an ap-
plication to support scientific knowledge cu-
ration at MGI.
1 Introduction
A long-standing promise of Biomedical Natural
Language Processing is to accelerate the process of
literature-based ?biocuration?, where published in-
formation must be carefully and appropriately trans-
lated into the knowledge architecture of a biomed-
ical database. Typically, biocuration is a manual
activity, performed by specialists with expertise in
both biomedicine and the computational represen-
tation of the target database. It is widely acknowl-
edged as a vital lynch-pin of biomedical informatics
(Bourne and McEntyre, 2006).
A key step in biocuration is the initial triage of
documents in order to direct to specialists only the
documents appropriate for them. This classifica-
tion (Cohen and Hersh, 2006)(Hersh W, 2005) can
be followed by a step in which desired information
is extracted and appropriately standardized and for-
malized for entry into the database. Both these steps
can be enhanced by suitably powerful Natural Lan-
guage Processing (NLP) technology. In this paper,
we address text mining as a step within the broader
context of developing both infrastructure and tools
for biocuration support within the Mouse Genome
Informatics (MGI) system at the Jackson Labora-
tories. We previously identified ?document triage?
as a crucial bottleneck (Ramakrishnan et al, 2010)
within MGI?s biocuration workflow.
Our research explores the use of information ex-
traction (IE) techniques to create richer linguis-
tic features than traditional bag-of-words models.
These features are employed by a classifier to per-
form the triage step. The features include lexico-
syntactic patterns as well as more-complex features,
such as a pattern coupled with its extracted noun,
where the noun is represented both as a lexical term
and by its semantic category. Our experimental re-
sults show that the IE-based enhanced features can
improve performance over unigram and bigram fea-
tures alone.
46
Evaluating the performance of BioNLP tools is
not trivial. So-called intrinsic metrics measure the
performance of a tool against some gold standard of
performance, while extrinsic ones (Alex et al, 2008)
measure how much the overall biocuration process
is benefited. Such metrics necessarily involve the
deployment of the software in-house for testing
by biocurators, and require a large-scale software-
engineering infrastructure effort. In this paper, we
present intrinsic evaluation results of full-text doc-
ument classification experiments to determine auto-
matically whether a paper should be considered of
interest to MGI curators. We plan in-house deploy-
ment and extrinsic evaluation in near-term work.
Our work should be considered as the first step of
a broader process within which (a) the features used
in this particular classification approach will be re-
engineered so that they may be dynamically recre-
ated in any new domain by a reusable component,
(b) this component is deployed into reusable infras-
tructure that also includes document-, annotation-
and feature-storage capabilities that support scaling
and reuse, and (c) the overall functionality can then
be delivered as a software application to biocurators
themselves for extrinsic evaluation in any domain
they choose. Within the ?SciKnowMine? project, we
are constructing such a framework (Ramakrishnan et
al., 2010), and this work reported here forms a pro-
totype component that we plan to incorporate into
a live application. We describe the underlying NLP
research here, and provide context for the work by
describing the overall design and implementation of
the SciKnowMine infrastructure.
1.1 Motivation
MGI?s biocurators use very specific guidelines for
triage that continuously evolve. These guidelines
are tailored to specific subcategories within MGI?s
triage task (phenotype, Gene Ontology1 (GO) term,
gene expression, tumor biology and chromosomal
location mapping). They help biocurators decide
whether a paper is relevant to one or more subcat-
egories. As an example, consider the guideline for
the phenotype category shown in Table 1.
This example makes clear that it is not sufficient
to match on relevant words like ?transgene? alone.
1http://www.geneontology.org/
?Select paper
If: it is about transgenes where a gene from any
species is inserted in mice and this results in
a phenotype.
Except: if the paper uses transgenes to
examine promoter function?.
Table 1: Sample triage guideline used by MGI biocura-
tors
To identify a paper as being ?within-scope? or ?out-
of-scope? requires that a biocurator understand the
context of the experiment described in the paper.
To check this we examined two sample papers; one
that matches the precondition of the above rule and
another that matches its exception. The first paper
(Sjo?gren et al, 2009) is about a transgene inser-
tion causing a pheotype and is a positive example
of the category phenotype, while the second paper
(Bouatia-Naji et al, 2010) is about the use of trans-
genes to study promoter function and is a negative
example for the same category.
Inspection of the negative-example paper illus-
trates the following issues concerning the language
used: (1) This paper is about transgene-use in study-
ing promoter function. Understanding this requires
the following background knowledge: (a) the two
genes mentioned in the title are transgenes; (b) the
phrase ?elevation of fasting glucose levels? in the ti-
tle represents an up-regulation phenotype event. (2)
Note that the word ?transgene? never occurs in the
entire negative-example paper. This suggests that
recognizing that a paper involves the use of trans-
genes requires annotation of domain-specific enti-
ties and a richer representation than that offered by
a simple bag-of-words model.
Similar inspection of the positive-example paper
reveals that (3) the paper contains experimental ev-
idence showing the phenotype resulting from the
transgene insertion. (4) The ?Materials and Meth-
ods? section of the positive-example paper clearly
identifies the construction of the transgene and the
?Results? section describes the development of the
transgenic mouse model used in the study. (3)
and (4) above suggest that domain knowledge about
complex biological phenomena (events) such as
phenotype and experimental protocol may be help-
ful for the triage task.
47
Together, points (1)?(4) suggest that different
sections of a paper contain additional important
context-specific clues. The example highlights the
complex nature of the triage task facing the MGI
biocurators. At present, this level of nuanced ?un-
derstanding? of content semantics is extremely hard
for machines to replicate. Nonetheless, merely treat-
ing the papers as a bag-of-words is unlikely to make
nuanced distinctions between positive and negative
examples with the level of precision and recall re-
quired in MGI?s triage task.
In this paper we therefore describe: (1) the design
and performance of a classifier that is enriched with
three types of features, all derived from informa-
tion extraction: (a) lexico-syntactic patterns, (b) pat-
terns coupled with lexical extractions, and (c) pat-
terns coupled with semantic extractions. We com-
pare the enriched classifier against classifiers that
use only unigram and bigram features; (2) the de-
sign of a biocuration application for MGI along with
the first prototype system where we emphasize the
infrastructure necessary to support the engineering
of domain-specific features of the kind described
in the examples above. Our application is based
on Unstructured Information Management Architec-
ture (UIMA) (Ferrucci and Lally, 2004), which is
a pipeline-based framework for the development of
software systems that analyze large volumes of un-
structured information.
2 Information Extraction for Triage
Classification
In this section, we present the information extraction
techniques that we used as the basis for our IE-based
features, and we describe the three types of IE fea-
tures that we incorporated into the triage classifier.
2.1 Information Extraction Techniques
Information extraction (IE) includes a variety of
techniques for extracting factual information from
text. We focus on pattern-based IE methods
that were originally designed for event extrac-
tion. Event extraction systems identify the role
fillers associated with events. For example, con-
sider the task of extracting information from dis-
ease outbreak reports, such as ProMed-mail arti-
cles (http://www.promedmail.org/). In contrast to a
named entity recognizer, which should identify all
mentions of diseases and people, an event extraction
system should only extract the diseases involved in
an outbreak incident and the people who were the
victims. Other mentions of diseases (e.g., in histori-
cal discussions) or people (e.g., doctors or scientists)
should be discarded.
We utilized the Sundance/AutoSlog software
package (Riloff and Phillips, 2004), which is freely
available for research. Sundance is an information
extraction engine that applies lexico-syntactic pat-
terns to extract noun phrases from specific linguistic
contexts. Sundance performs its own syntactic anal-
ysis, which includes morphological analysis, shal-
low parsing, clause segmentation, and syntactic role
assignment (i.e., identifying subjects and direct ob-
jects of verb phrases). Sundance labels verb phrases
with respect to active/passive voice, which is im-
portant for event role labelling. For example, ?Tom
Smith was diagnosed with bird flu? means that Tom
Smith is a victim, but ?Tom Smith diagnosed the el-
derly man with bird flu? means that the elderly man
is the victim.
Sundance?s information extraction engine can ap-
ply lexico-syntactic patterns to extract noun phrases
that participate in syntactic relations. Each pat-
tern represents a linguistic expression, and extracts
a noun phrase (NP) argument from one of three syn-
tactic positions: Subject, Direct Object, or Prepo-
sitional Phrase. Patterns may be defined manu-
ally, or they can be generated by the AutoSlog pat-
tern generator (Riloff, 1993), which automatically
generates patterns from a domain-specific text cor-
pus. AutoSlog uses 17 syntactic ?templates? that are
matched against the text. Lexico-syntactic patterns
are generated by instantiating the matching words in
the text with the syntactic template. For example,
five of AutoSlog?s syntactic templates are shown in
Table 2:
(a) <SUBJ> PassVP
(b) PassVP Prep <NP>
(c) <SUBJ> ActVP
(d) ActVP Prep <NP>
(e) Subject PassVP Prep <NP>
Table 2: Five example syntactic templates (PassVP
means passive voice verb phrase, ActVP means active
voice verb phrase)
48
Pattern (a) matches any verb phrase (VP) in a pas-
sive voice construction and extracts the Subject of
the VP. Pattern (b) matches passive voice VPs that
are followed by a prepositional phrase. The NP
in the prepositional phrase is extracted. Pattern (c)
matches any active voice VP and extracts its Subject,
while Pattern (d) matches active voice VPs followed
by a prepositional phrase. Pattern (e) is a more com-
plex pattern that requires a specific Subject2, passive
voice VP, and a prepositional phrase. We applied the
AutoSlog pattern generator to our corpus (described
in Section 3.1) to exhaustively generate every pat-
tern that occurs in the corpus.
As an example, consider the following sentence,
taken from an article in PLoS Genetics:
USP14 is endogenously expressed in
HEK293 cells and in kidney tissue derived
from wt mice.
<SUBJ> PassVP(expressed)
<SUBJ> ActVP(derived)
PassVP(expressed) Prep(in) <NP>
ActVP(derived) Prep(from) <NP>
Subject(USP14) PassVP(expressed) Prep(in) <NP>
Table 3: Lexico-syntactic patterns for the PLoS Genetics
sentence shown above.
AutoSlog generates five patterns from this sen-
tence, which are shown in Table 3:
The first pattern matches passive voice instances
of the verb ?expressed?, and the second pattern
matches active voice instances of the verb ?de-
rived?.3 These patterns rely on syntactic analysis,
so they will match any syntactically appropriate con-
struction. For example, the first pattern would match
?was expressed?, ?were expressed?, ?have been ex-
pressed? and ?was very clearly expressed?. The third
and fourth patterns represent the same two VPs but
also require the presence of a specific prepositional
phrase. The prepositional phrase does not need to
be adjacent to the VP, so long as it is attached to
the VP syntactically. The last pattern is very spe-
cific and will only match passive voice instances of
2Only the head nouns must match.
3Actually, the second clause is in reduced passive voice (i.e.,
tissue that was derived from mice), but the parser misidentifies
it as an active voice construction.
?expressed? that also have a Subject with a particular
head noun (?USP14?) and an attached prepositional
phrase with the preposition ?in?.
The example sentence contains four noun phrases,
which are underlined. When the patterns generated
by AutoSlog are applied to the sentence, they pro-
duce the following NP extractions (shown in bold-
face in Table 4):
<USP14> PassVP(expressed)
<kidney tissue> ActVP(derived)
PassVP(expressed) Prep(in) <HEK293 cells>
ActVP(derived) Prep(from) <wt mice>
Subject(USP14) PassVP(expressed) Prep(in) <HEK293
cells>
Table 4: Noun phrase extractions produced by Sundance
for the sample sentence.
In the next section, we explain how we use the in-
formation extraction system to produce rich linguis-
tic features for our triage classifier.
2.2 IE Pattern Features
For the triage classification task, we experimented
with four types of IE-based features: Patterns, Lexi-
cal Extractions, and Semantic Extractions.
The Pattern features are the lexico-syntactic IE
patterns. Intuitively, each pattern represents a phrase
or expression that could potentially capture contexts
associated with mouse genomics better than isolated
words (unigrams). We ran the AutoSlog pattern gen-
erator over the training set to exhaustively generate
every pattern that appeared in the corpus. We then
defined one feature for each pattern and gave it a
binary feature value (i.e., 1 if the pattern occurred
anywhere in the document, 0 otherwise).
We also created features that capture not just the
pattern expression, but also its argument. The Lex-
ical Extraction features represent a pattern paired
with the head noun of its extracted noun phrase.
Table 5 shows the Lexical Extraction features that
would be generated for the sample sentence shown
earlier. Our hypothesis was that these features could
help to distinguish between contexts where an activ-
ity is relevant (or irrelevant) to MGI because of the
combination of an activity and its argument.
The Lexical Extraction features are very specific,
requiring the presence of multiple terms. So we
49
PassVP(expressed), USP14
ActVP(derived), tissue
PassVP(expressed) Prep(in), cells
ActVP(derived) Prep(from), mice
Subject(USP14) PassVP(expressed) Prep(in), cells
Table 5: Lexical Extraction features
also experimented with generalizing the extracted
nouns by replacing them with a semantic category.
To generate a semantic dictionary for the mouse ge-
nomics domain, we used the Basilisk bootstrapping
algorithm (Thelen and Riloff, 2002). Basilisk has
been used previously to create semantic lexicons for
terrorist events (Thelen and Riloff, 2002) and senti-
ment analysis (Riloff et al, 2003), and recent work
has shown good results for bioNLP domains using
similar bootstrapping algorithms (McIntosh, 2010;
McIntosh and Curran, 2009).
As input, Basilisk requires a domain-specific text
corpus (unannotated) and a handful of seed nouns
for each semantic category to be learned. A boot-
strapping algorithm then iteratively hypothesizes ad-
ditional words that belong to each semantic cat-
egory based on their association with the seed
words in pattern contexts. The output is a lexicon
of nouns paired with their corresponding semantic
class. (e.g., liver : BODY PART).
We used Basilisk to create a lexicon for eight se-
mantic categories associated with mouse genomics:
BIOLOGICAL PROCESS, BODY PART, CELL TYPE,
CELLULAR LOCATION, BIOLOGICAL SUBSTANCE,
EXPERIMENTAL REAGENT, RESEARCH SUBJECT,
TUMOR. To choose the seed nouns, we parsed
the training corpus, ranked all of the nouns by fre-
quency4, and selected the 10 most frequent, unam-
biguous nouns belonging to each semantic category.
The seed words that we used for each semantic cat-
egory are shown in Table 6.
Finally, we defined Semantic Extraction features
as a pair consisting of a pattern coupled with the
semantic category of the noun that it extracted. If
the noun was not present in the semantic lexicons,
then no feature was created. The Basilisk-generated
lexicons are not perfect, so some entries will be in-
correct. But our hope was that replacing the lexical
terms with semantic categories might help the clas-
4We only used nouns that occurred as the head of a NP.
BIOLOGICAL PROCESS: expression, ac-
tivity, activation, development, function,
production, differentiation, regulation, re-
duction, proliferation
BODY PART: brain, muscle, thymus, cor-
tex, retina, skin, spleen, heart, lung, pan-
creas
CELL TYPE: neurons, macrophages, thy-
mocytes, splenocytes, fibroblasts, lym-
phocytes, oocytes, monocytes, hepato-
cytes, spermatocytes
CELLULAR LOCATION: receptor, nu-
clei, axons, chromosome, membrane, nu-
cleus, chromatin, peroxisome, mitochon-
dria, cilia
BIOLOGICAL SUBSTANCE: antibody,
lysates, kinase, cytokines, peptide, anti-
gen, insulin, ligands, peptides, enzyme
EXPERIMENTAL REAGENT: buffer,
primers, glucose, acid, nacl, water, saline,
ethanol, reagents, paraffin
RESEARCH SUBJECT: mice, embryos,
animals, mouse, mutants, patients, litter-
mates, females, males, individuals
TUMOR: tumors, tumor, lymphomas,
tumours, carcinomas, malignancies,
melanoma, adenocarcinomas, gliomas,
sarcoma
Table 6: Seed words given to Basilisk
sifier learn more general associations. For exam-
ple, ?PassVP(expressed) Prep(in), CELLULAR LO-
CATION? will apply much more broadly than the
corresponding lexical extraction with just one spe-
cific cellular location (e.g., ?mitochondria?).
Information extraction patterns and their argu-
ments have been used for text classification in pre-
vious work (Riloff and Lehnert, 1994; Riloff and
Lorenzen, 1999), but the patterns and arguments
were represented separately and the semantic fea-
tures came from a hand-crafted dictionary. In con-
trast, our work couples each pattern with its ex-
tracted argument as a single feature, uses an auto-
matically generated semantic lexicon, and is the first
application of these techniques to the biocuration
triage task.
50
3 Results
3.1 Data Set
For our experiments in this paper we use articles
within the PubMed Central (PMC) Open Access
Subset5. From this subset we select all articles that
are published in journals of interest to biocurators
at MGI. This results in a total of 14,827 documents
out of which 981 have been selected manually by
MGI biocurators as relevant (referred to as IN docu-
ments). This leaves 13,846 that are presumably out
of scope (referred to as OUT documents), although
it was not guaranteed that all of them had been man-
ually reviewed so some relevant documents could be
included as well. (We plan eventually to present to
the biocurators those papers not included by them
but nonetheless selected by our tools as IN with
high confidence, for possible reclassification. Such
changes will improve the system?s evaluated score.)
As preprocessing for the NLP tools, we split
the input text into sentences using the Lin-
gua::EN::Sentence perl package. We trimmed non-
alpha-numerics attached before and after words.
We also removed stop words using the Lin-
gua::EN::StopWords package.
3.2 Classifier
We used SVM Light6(Joachims, 1999) for all of our
experiments. We used a linear kernel and a tol-
erance value of 0.1 for QP solver termination. In
preliminary experiments, we observed that the cost
factor (C value) made a big difference in perfor-
mance. In SVMs, the cost factor represents the
importance of penalizing errors on the training in-
stances in comparison to the complexity (general-
ization) of the model. We observed that higher val-
ues of C produced increased recall, though at the ex-
pense of some precision. We used a tuning set to
experiment with different values of C, trying a wide
range of powers of 2. We found that C=1024 gen-
erally produced the best balance of recall and preci-
sion, so we used that value throughout our experi-
ments.
5http://www.ncbi.nlm.nih.gov/pmc/about/
openftlist.html
6http://svmlight.joachims.org/
3.3 Experiments
We randomly partitioned our text corpus into 5 sub-
sets of 2,965 documents each.7 We used the first 4
subsets as the training set, and reserved the fifth sub-
set as a blind test set.
In preliminary experiments, we found that the
classifiers consistently benefitted from feature se-
lection when we discarded low-frequency features.
This helps to keep the classifier from overfitting to
the training data. For each type of feature, we set
a frequency threshold ? and discarded any features
that occurred fewer than ? times in the training set.
We chose these ? values empirically by performing
4-fold cross-validation on the training set. We eval-
uated ? values ranging from 1 to 50, and chose the
value that produced the highest F score. The ? val-
ues that were selected are: 7 for unigrams, 50 for
bigrams, 35 for patterns, 50 for lexical extractions,
and 5 for semantic extractions.
Finally, we trained an SVM classifier on the en-
tire training set and evaluated the classifier on the
test set. We computed Precision (P), Recall (R), and
the F score, which is the harmonic mean of preci-
sion and recall. Precision and recall were equally
weighted, so this is sometimes called an F1 score.
Table 7 shows the results obtained by using each
of the features in isolation. The lexical extraction
features are shown as ?lexExts? and the semantic ex-
traction features are shown as ?semExts?. We also
experimented with using a hybrid extraction fea-
ture, ?hybridExts?, which replaced a lexical extrac-
tion noun with its semantic category when one was
available but left the noun as the extraction term
when no semantic category was known.
Table 7 shows that the bigram features produced
the best Recall (65.87%) and F-Score (74.05%),
while the hybrid extraction features produced the
best Precision (85.52%) but could not match the bi-
grams in terms of recall. This is not surprising be-
cause the extraction features on their own are quite
specific, often requiring 3-4 words to match.
Next, we experimented with adding the IE-based
features to the bigram features to allow the classifier
to choose among both feature sets and get the best
of both worlds. Combining bigrams with IE-based
7Our 5-way random split left 2 documents aside, which we
ignored for our experiments.
51
Feature P R F
unigrams 79.75 60.58 68.85
bigrams 84.57 65.87 74.05
patterns 78.98 59.62 67.95
lexExts 76.54 59.62 67.03
semExts 72.39 46.63 56.73
hybridExts 85.52 59.62 70.25
bigrams + patterns 84.87 62.02 71.67
bigrams + lexExts 85.28 66.83 74.93
bigrams + semExts 85.43 62.02 71.87
bigrams + hybridExts 87.10 64.90 74.38
Table 7: Triage classifier performance using different sets
of features.
features did in fact yield the best results. Using bi-
grams and lexical extraction features achieved both
the highest recall (66.83%) and the highest F score
(74.93%). In terms of overall F score, we see a rela-
tively modest gain of about 1% by adding the lexical
extraction features to the bigram features, which is
primarily due to the 1% gain in recall.
However, precision is of paramount importance
for many applications because users don?t want to
wade through incorrect predictions. So it is worth
noting that adding the hybrid extraction features to
the bigram features produced a 2.5% increase in pre-
cision (84.57% ? 87.10%) with just a 1% drop in
recall. This recall/precision trade-off is likely to be
worthwhile for many real-world application settings,
including biocuration.
4 Biocuration Application for MGI
Developing an application that supports MGI biocu-
rators necessitates an application design that mini-
mally alters existing curation workflows while main-
taining high classification F-scores (intrinsic mea-
sures) and speeding up the curation process (extrin-
sic measures). We seek improvements with respect
to intrinsic measures by engineering context-specific
features and seek extrinsic evaluations by instru-
menting the deployed triage application to record us-
age statistics that serve as input to extrinsic evalua-
tion measures.
4.1 Software Architecture
As stated earlier, one of our major goals is to build,
deploy, and extrinsically evaluate an NLP-assisted
curation application (Alex et al, 2008) for triage at
MGI. By definition, an extrinsic evaluation of our
triage application requires its deployment and sub-
sequent tuning to obtain optimal performance with
respect to extrinsic evaluation criteria. We antici-
pate that features, learning parameters, and training
data distributions may all need to be adjusted during
a tuning process. Cognizant of these future needs,
we have designed the SciKnowMine system so as
to integrate the various components and algorithms
using the UIMA infrastructure. Figure 1 shows a
schematic of SciKnowMine?s overall architecture.
4.1.1 Building configurable & reusable UIMA
pipelines
The experiments we have presented in this paper
have been conducted using third party implementa-
tions of a variety of algorithms implemented on a
wide variety of platforms. We use SVMLight to
train a triage classifier on features that were pro-
duced by AutoSlog and Sundance on sentences iden-
tified by the perl package Lingua::EN::Sentence.
Each of these types of components has either been
reimplemented or wrapped as a component reusable
in UIMA pipelines within the SciKnowMine in-
frastructure. We hope that building such a li-
brary of reusable components will help galvanize the
BioNLP community towards standardization of an
interoperable and open-access set of NLP compo-
nents. Such a standardization effort is likely to lower
the barrier-of-entry for NLP researchers interested in
applying their algorithms to knowledge engineering
problems in Biology (such as biocuration).
4.1.2 Storage infrastructure for annotations &
features
As we develop richer section-specific and
context-specific features we anticipate the need for
provenance pertaining to classification decisions for
a given paper. We have therefore built an Annotation
Store and a Feature Store collectively referred to as
the Classification Metadata Store8 in Figure 1. Fig-
ure 1 also shows parallel pre-processing populating
the annotation store. We are working on develop-
ing parallel UIMA pipelines that extract expensive
(resource & time intensive) features (such as depen-
8Our classification metadata store has been implemented us-
ing Solr http://lucene.apache.org/solr/
52
dency parses).The annotation store holds features
produced by pre-processing pipelines. The annota-
tion store has been designed to support query-based
composition of feature sets specific to a classifica-
tion run. These feature sets can be asserted to the
feature store and reused later by any pipeline. This
design provides us with the flexibility necessary to
experiment with a wide variety of features and tune
our classifiers in response to feedback from biocura-
tors.
5 Discussions & Conclusions
In this paper we have argued the need for richer se-
mantic features for the MGI biocuration task. Our
results show that simple lexical and semantic fea-
tures used to augment bigram features can yield
higher classification performance with respect to in-
trinsic metrics (such as F-Score). It is noteworthy
that using a hybrid of lexical and semantic features
results in the highest precision of 87%.
In our motivating example, we have proposed
the need for sectional-zoning of articles and have
demonstrated that certain zones like the ?Materi-
als and Methods? section can contain contextual
features that might increase classification perfor-
mance. It is clear from the samples of MGI man-
ual classification guidelines that biocurators do, in
fact, use zone-specific features in triage. It there-
fore seems likely that section specific feature ex-
traction might result in better classification perfor-
mance in the triage task. Our preliminary analysis of
the MGI biocuration guidelines suggests that exper-
imental procedures described in the ?Materials and
Methods? seem to be a good source of triage clues.
We therefore propose to investigate zone and context
specific features and the explicit use of domain mod-
els of experimental procedure as features for docu-
ment triage.
We have also identified infrastructure needs aris-
ing within the construction of a biocuration applica-
tion. In response we have constructed preliminary
versions of metadata stores and UIMA pipelines to
support MGI?s biocuration. Our next step is to de-
ploy a prototype assisted-curation application that
uses a classifier trained on the best performing fea-
tures discussed in this paper. This application will
be instrumented to record usage statistics for use in
extrinsic evaluations (Alex et al, 2008). We hope
that construction on such an application will also
engender the creation of an open environment for
NLP scientists to apply their algorithms to biomedi-
cal corpora in addressing biomedical knowledge en-
gineering challenges.
6 Acknowledgements
This research is funded by the U.S. National Sci-
ence Foundation under grant #0849977 for the
SciKnowMine project (http://sciknowmine.
isi.edu/). We wish to acknowledge Kevin Co-
hen for helping us collect the seed terms for Basilisk
and Karin Verspoor for discussions regarding feature
engineering.
References
[Alex et al2008] Beatrice Alex, Claire Grover, Barry
Haddow, Mijail Kabadjov, Ewan Klein, Michael
Matthews, Stuart Roebuck, Richard Tobin, and Xin-
glong Wang. 2008. Assisted curation: does text min-
ing really help? Pacific Symposium On Biocomputing,
567:556?567.
[Bouatia-Naji et al2010] Nabila Bouatia-Naji, Ame?lie
Bonnefond, Devin A Baerenwald, Marion Marchand,
Marco Bugliani, Piero Marchetti, Franc?ois Pattou,
Richard L Printz, Brian P Flemming, Obi C Umu-
nakwe, Nicholas L Conley, Martine Vaxillaire, Olivier
Lantieri, Beverley Balkau, Michel Marre, Claire Le?vy-
Marchal, Paul Elliott, Marjo-Riitta Jarvelin, David
Meyre, Christian Dina, James K Oeser, Philippe
Froguel, and Richard M O?Brien. 2010. Genetic and
functional assessment of the role of the rs13431652-
A and rs573225-A alleles in the G6PC2 promoter that
are strongly associated with elevated fasting glucose
levels. Diabetes, 59(10):2662?2671.
[Bourne and McEntyre2006] Philip E Bourne and Jo-
hanna McEntyre. 2006. Biocurators: Contributors to
the World of Science. PLoS Computational Biology,
2(10):1.
[Cohen and Hersh2006] Aaron M Cohen and William R
Hersh. 2006. The TREC 2004 genomics track cate-
gorization task: classifying full text biomedical docu-
ments. Journal of Biomedical Discovery and Collab-
oration, 1:4.
[Ferrucci and Lally2004] D Ferrucci and A Lally. 2004.
Building an example application with the Unstructured
Information Management Architecture. IBM Systems
Journal, 43(3):455?475.
53
Citation
Store
Feature
Store 
(token trigrams, 
stemmed bigrams)
Document
Store
MGI
training
Corpus
Digital Library
Parallel
pre-processing pipelines
Classification Metadata Store
Annotation Store 
(token, Sundance 
patterns, parse trees,)
Classifier training
pipeline
trained triage 
classification 
model
Ranked Triage ResultsDigital Library
MGI Biocuration Application
Newly 
published
papers
Figure 1: Design schematic of the MGI biocuration application. The components of the application are: (A) Digital
Library composed of a citation store and document store. (B) Pre-processing UIMA pipelines which are a mecha-
nism to pre-extract standard features such as parse trees, tokenizations etc. (C) Classification Metadata Store which
is composed of an Annotation Store for the pre-extracted standard features from (B), and a Feature Store to hold de-
rived features constructed from the standard ones in the Annotation Store. (D) Classifier training pipeline. (E) MGI
Biocuration Application.
[Hersh W2005] Yang J Bhupatiraju RT Roberts P M.
Hearst M Hersh W, Cohen AM. 2005. TREC 2005
genomics track overview. In The Fourteenth Text Re-
trieval Conference.
[Joachims1999] Thorsten Joachims. 1999. Making
Large-Scale SVM Learning Practical. Advances in
Kernel Methods Support Vector Learning, pages 169?
184.
[McIntosh and Curran2009] T. McIntosh and J. Curran.
2009. Reducing Semantic Drift with Bagging and
Distributional Similarity. In Proceedings of the 47th
Annual Meeting of the Association for Computational
Linguistics.
[McIntosh2010] Tara McIntosh. 2010. Unsupervised dis-
covery of negative categories in lexicon bootstrapping.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, number Oc-
tober, pages 356?365. Association for Computational
Linguistics.
[Ramakrishnan et al2010] Cartic Ramakrishnan, William
A Baumgartner Jr, Judith A Blake, Gully A P C
Burns, K Bretonnel Cohen, Harold Drabkin, Janan
Eppig, Eduard Hovy, Chun-Nan Hsu, Lawrence E
Hunter, Tommy Ingulfsen, Hiroaki Rocky Onda,
Sandeep Pokkunuri, Ellen Riloff, and Karin Verspoor.
2010. Building the Scientific Knowledge Mine ( Sci-
KnowMine 1 ): a community- driven framework for
text mining tools in direct service to biocuration. In
proceeding of Workshop ?New Challenges for NLP
Frameworks? collocated with The seventh interna-
tional conference on Language Resources and Eval-
uation (LREC) 2010.
[Riloff and Lehnert1994] E. Riloff and W. Lehnert. 1994.
Information Extraction as a Basis for High-Precision
Text Classification. ACM Transactions on Information
54
Systems, 12(3):296?333, July.
[Riloff and Lorenzen1999] E. Riloff and J. Lorenzen.
1999. Extraction-based text categorization: Generat-
ing domain-specific role relationships automatically.
In Tomek Strzalkowski, editor, Natural Language In-
formation Retrieval. Kluwer Academic Publishers.
[Riloff and Phillips2004] E. Riloff and W. Phillips. 2004.
An Introduction to the Sundance and AutoSlog Sys-
tems. Technical Report UUCS-04-015, School of
Computing, University of Utah.
[Riloff et al2003] E. Riloff, J. Wiebe, and T. Wilson.
2003. Learning Subjective Nouns using Extraction
Pattern Bootstrapping. In Proceedings of the Seventh
Conference on Natural Language Learning (CoNLL-
2003), pages 25?32.
[Riloff1993] E. Riloff. 1993. Automatically Construct-
ing a Dictionary for Information Extraction Tasks. In
Proceedings of the 11th National Conference on Arti-
ficial Intelligence.
[Sjo?gren et al2009] Klara Sjo?gren, Marie Lagerquist,
Sofia Moverare-Skrtic, Niklas Andersson, Sara H
Windahl, Charlotte Swanson, Subburaman Mohan,
Matti Poutanen, and Claes Ohlsson. 2009. Elevated
aromatase expression in osteoblasts leads to increased
bone mass without systemic adverse effects. Journal
of bone and mineral research the official journal of
the American Society for Bone and Mineral Research,
24(7):1263?1270.
[Thelen and Riloff2002] M. Thelen and E. Riloff. 2002.
A Bootstrapping Method for Learning Semantic Lexi-
cons Using Extraction Pa ttern Contexts. In Proceed-
ings of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 214?221.
55
Proceedings of the Workshop on Language in Social Media (LSM 2011), pages 20?29,
Portland, Oregon, 23 June 2011. c?2011 Association for Computational Linguistics
Contextual Bearing on Linguistic Variation in Social Media
Stephan Gouws?, Donald Metzler, Congxing Cai and Eduard Hovy
{gouws, metzler, ccai, hovy}@isi.edu
USC Information Sciences Institute
Marina del Rey, CA
90292, USA
Abstract
Microtexts, like SMS messages, Twitter posts,
and Facebook status updates, are a popular
medium for real-time communication. In this
paper, we investigate the writing conventions
that different groups of users use to express
themselves in microtexts. Our empirical study
investigates properties of lexical transforma-
tions as observed within Twitter microtexts.
The study reveals that different populations of
users exhibit different amounts of shortened
English terms and different shortening styles.
The results reveal valuable insights into how
human language technologies can be effec-
tively applied to microtexts.
1 Introduction
Microtexts, like SMS messages, Twitter posts, and
Facebook status updates, are becoming a popular
medium for real-time communication in the modern
digital age. The ubiquitous nature of mobile phones,
tablets, and other Internet-enabled consumer devices
provide users with the ability to express what is
on their mind nearly anywhere and at just about
any time. Since such texts have the potential to
provide unique perspectives on human experiences,
they have recently become the focus of many studies
within the natural language processing and informa-
tion retrieval research communities.
The informal nature of microtexts allows users
to invent ad hoc writing conventions that suit their
?This work was done while the first author was a visiting stu-
dent at ISI from the MIH Media Lab at Stellenbosch University,
South Africa. Correspondence may alternatively be directed to
stephan@ml.sun.ac.za.
particular needs. These needs strongly depend on
various user contexts, such as their age, geographic
location, how they want to be outwardly perceived,
and so on. Hence, social factors influence the way
that users express themselves in microtexts and other
forms of media.
In addition to social influences, there are also us-
ability and interface issues that may affect the way a
user communicates using microtexts. For example,
the Twitter microblog service imposes an explicit
message length limit of 140 characters. Users of
such services also often send messages using mobile
devices. There may be high input costs associated
with using mobile phone keypads, thus directly im-
pacting the nature of how users express themselves.
In this paper, we look specifically at understand-
ing the writing conventions that different groups
of users use to express themselves. This is ac-
complished by carrying out a novel empirical in-
vestigation of the lexical transformation character-
istics observed within Twitter microtexts. Our em-
pirical evaluation includes: (i) an analysis of how
frequently different user populations apply lexical
transformations, and (ii) a study of the types of
transformations commonly employed by different
populations of users. We investigate several ways of
defining user populations (e.g., based on the Twitter
client, time zone, etc.). Our results suggest that not
all microtexts are created equal, and that certain pop-
ulations of users are much more likely to use certain
types of lexical transformations than others.
This paper has two primary contributions. First,
we present a novel methodology for contextualized
analysis of lexical transformations found within mi-
20
crotexts. The methodology leverages recent ad-
vances in automated techniques for cleaning noisy
text. This approach enables us to study the fre-
quency and types of transformations that are com-
mon within different user populations and user con-
texts. Second, we present results from an empirical
evaluation over microtexts collected from the Twit-
ter microblog service. Our empirical analysis re-
veals that within Twitter microtexts, different user
populations and user contexts give rise to different
forms of expression, by way of different styles of
lexical transformations.
The remainder of this paper is laid out as follows.
Section 2 describes related work, while Section 3
motivates our investigation. Our multi-pronged
methodology for analyzing lexical transformations
is described in Section 4. Section 5 describes our
experimental results. Finally, Section 6 concludes
the paper and describes possible directions for fu-
ture work.
2 Related Work
Although our work is primarily focused on analyz-
ing the lexical variation in language found in on-
line social media, our analysis methodology makes
strong use of techniques for normalizing ?noisy text?
such as SMS-messages and Twitter messages into
standard English.
Normalizing text can traditionally be approached
using three well-known NLP metaphors, namely
that of spell-checking, machine translation (MT) and
automatic speech recognition (ASR) (Kobus et al,
2008).
In the spell-checking approach, corrections from
?noisy? words to ?clean? words proceed on a word-
by-word basis. Choudhury (2007) implements
the noisy channel model (Shannon and Weaver,
1948) using a hidden Markov model to handle both
graphemic and phonemic variations, and Cook and
Stevenson (2009) improve on this model by adapt-
ing the channel noise according to several predefined
word formations such as stylistic variation, word
clipping, etc. However, spelling correction is tra-
ditionally conducted in media with relatively high
percentages of well-formed text where one can per-
form word boundary detection and thus tokenization
to a high degree of accuracy. The main drawback is
the strong confidence this approach places on word
boundaries (Beaufort et al, 2010), since detecting
word boundaries in noisy text is not a trivial prob-
lem.
In the machine translation approach (Bangalore
et al, 2002; Aw et al, 2006), normalizing noisy
text is considered as a translation task from a source
language (the noisy text) to a target language (the
cleansed text). Since noisy- and clean text typically
vary wildly, it satisfies the notion of translating be-
tween two languages. However, since these trans-
formations can be highly creative, they usually need
a wide context (more than one word) to be resolved
adequately. Kobus (2008) also points out that de-
spite the fairly good results achieved with this sys-
tem, such a purely phrase-based translation model
cannot adequately handle the wide level of lexical
creativity found in these media.
Finally, the ASR approach is based on the ob-
servation that many noisy word forms in SMSes
or other noisy text are based on phonetic plays of
the clean word. This approach starts by convert-
ing the input message into a phone lattice, which
is converted to a word lattice using a phoneme-
grapheme dictionary. Finally the word lattice is de-
coded by applying a language model to the word lat-
tice and using a best-path algorithm to recover the
most likely original word sequence. This approach
has the advantage of being able to handle badly seg-
mented word boundaries efficiently, however it pre-
vents the next normalization steps from knowing
what graphemes were in the initial sequence (Kobus
et al, 2008).
What fundamentally separates the noisy text
cleansing task from the spell-checking problem is
that most often lexical ill-formedness in these me-
dia is intentional. Han (2011) proposes that this
might be in an attempt to save characters in length-
constrained media (such as Twitter or SMS), for
social identity (conversing in the dialect of a spe-
cific group), or due to convention of the medium.
Emotional context is typically expressed with re-
peat characters such as ?I am sooooooo tired? or
excessive punctuation. At times, however, out-
of-vocabulary tokens (spelling errors) might result
purely as the result of cognitive oversight.
Cook and Stevenson (2009) are one of the first to
explicitly analyze the types of transformations found
21
in short message domains. They identify: 1) stylis-
tic variation (better?betta), 2) subsequence abbre-
viation (doing?dng), 3) clipping of the letter ?g?
(talking?talkin), 4) clipping of ?h? (hello?ello),
and 5) general syllable clipping (anyway?neway),
to be the most frequent transformations. Cook and
Stevenson then incorporate these transformations
into their model. The idea is that such an unsuper-
vised approach based on the linguistic properties of
creative word forms has the potential to be adapted
for normalization in other similar genres without the
cost of developing a large training corpus. Most im-
portantly, they find that many creative texting forms
are the result of a small number of specific word for-
mation processes.
Han (2011) performs a simple analysis on the out-
of-vocabulary words found in Twitter, and find that
the majority of ill-formed words in Twitter can be
attributed to instances where letters are missing or
where there are extraneous letters, but the lexical
correspondence to the target word is trivially acces-
sible. They find that most ill-formed words are based
on morphophonemic variations.
3 Motivation
All of the previous work described in Section 2 ei-
ther
i) only focus on recovering the most likely ?stan-
dard English? form of a message, disregarding
the stylistic structure of the original noisy text,
or
ii) considers the structure of the noisy text found
in a medium as a whole, only as a first step
(the means) to identify common types of noisy
transformations which can subsequently be ac-
counted for (or ?corrected?) to produce normal-
ized messages (the desired end result).
However, based on the fact that language is highly
contextual, we ask the question: What influence
does the context in which a message is produced
have on the resulting observed surface structure and
style of the message?
In general, since some topics are for instance
more formal or informal than others, vocabulary and
linguistic style often changes based on the topic that
is being discussed. Moreover, in social media one
can identify several other types of context. Specif-
ically in Twitter, one might consider a user?s geo-
graphical location, the client from which a user is
broadcasting her message, how long she has been
using the Twitter service, and so forth.
The intuition is that the unconstrained nature of
these media afford users the ability to invent writing
conventions to suit their needs. Since users? needs
depend on their circumstances, and hence their con-
text, we hypothesize that the observed writing sys-
tems might be influenced by some elements of their
context. For instance, phonemic writing systems
might be related to a user?s dialect which is re-
lated to a user?s geographical location. Furthermore,
highly compressed writing conventions (throwing
away vowels, using prefixes of words, etc.) might
result from the relatively high input cost associ-
ated with using unwieldy keypads on some mobile
clients, etc.
The present work is focused on looking at these
stylistic elements of messages found in social media,
by analyzing the types of stylistic variation at the
lexical level, across these contextual dimensions.
4 Method
In the following discussion we make a distinc-
tion between within-tweet context and the general
message-context in which a message is created.
Within-tweet context is the linguistic context (the
other terms) that envelopes a term in a Twitter mes-
sage. The general context of a Twitter message is the
observable elements of the environment in which it
was conceived. For the current study, we record
1. the user?s location, and
2. the client from which the message was sent,
We follow a two-pronged analytic approach:
Firstly, we conduct a na??ve, context-free analysis
(at the linguistic level) of all words not commonly
found in standard, everyday English. This analy-
sis purely looks at the terminology that are found
on Twitter, and does not attempt to normalize these
messages in any way. Therefore, different surface
forms of the same word, such as ?today?, ?2day?,
?2d4y?, are all considered distinct terms. We then
analyse the terminology over different contextual di-
mensions such as client and location.
22
Secondly, we perform a more in-depth and con-
textual analysis (at the word level) by first normaliz-
ing the potentially noisy message to recover the most
likely surface form of the message and recording the
types of changes that were made, and then analyz-
ing these types of changes across different general
contextual dimensions (client and location).
As noted in Section 2, text message normalization
is not a trivial process. As shown by Han (2011),
most transformations from in-vocabulary words to
out-of-vocabulary words can be attributed to a single
letter that is changed, removed, or added. Further-
more, they note that most ill-formed words are re-
lated to some morphophonemic variation. We there-
fore implemented a text cleanser based on the de-
sign of Contractor (2010) using pre-processing tech-
niques discussed in (Kaufmann and Kalita, 2010).
It works as follows: For each input message, we
replace @-usernames with ?*USR*? and urls with
?*URL*?. Hash tags can either be part of the sen-
tence (?just got a #droid today?) or be peripheral to
the sentence (?what a loooong day! #wasted?). Fol-
lowing Kaufmann (2010) we remove hashtags at the
end of messages when they are preceded by typical
end-of-sentence punctuation marks. Hash tags in the
middle of messages are retained, and the hash sign
removed.
Next we tokenize this preprocessed message us-
ing the NLTK tokenizer (Loper and Bird, 2002). As
noted earlier, standard NLP tools do not perform
well on noisy text out-of-the-box. Based on inspec-
tion of incorrectly tokenized output, we therefore in-
clude a post-tokenization phase where we split all
tokens that include a punctuation symbol into the in-
dividual one or two alphanumeric tokens (on either
side of the punctuation symbol), and the punctuation
symbol1. This heuristic catches most cases of run-on
sentences.
Given a set of input tokens, we process these one
by one, by comparing each token to the words in
the lexicon L and constructing a confusion network
CN. Each in-vocabulary term, punctuation token or
other valid-but-not-in-vocabulary term is added to
CN with probability 1.0 as shown in Algorithm 1.
1This is easily accomplished using a regular expression
group-substitution of the form (\w*)([P])(\w*)?[\1,
\2, \3], where \w represents the set of alphanumeric char-
acters, and P is the set of all punctuation marks [.,;?". . .]
Character Transliteration candidates
1 ?1?, ?l?, ?one?
2 ?2?, ?to?, ?too?, ?two?
3 ?3?, ?e?, ?three?
4 ?4?, ?a?, ?for?, ?four?
5 ?5?, ?s?, ?five?
6 ?6?, ?b?, ?six?
7 ?7?, ?t?, ?seven?
8 ?8?, ?ate?, ?eight?
9 ?9?, ?g?, ?nine?
0 ?0?, ?o?, ?zero?
?@? ?@?, ?at?
?&? ?&?, ?and?
Table 1: Transliteration lookup table.
valid tok(wi) checks for ?*USR*?, ?*URL*?, or
any token longer than 1 character with no alphabet-
ical characters. This heuristic retains tokens such as
?9-11?, ?12:44?, etc.
At this stage, all out-of-vocabulary (OOV) terms
represent the terms that we are uncertain about, and
hence candidate terms for cleansing. First, for each
OOV term, we enumerate each possibly ambiguous
character into all its possible interpretations with the
transliteration table shown in Table 1. This expands,
for example, ?t0day?? [?t0day?, ?today?], and also
?2day?? [?2day?, ?twoday?, ?today?], etc.
Each transliterated candidate word in each con-
fusion set produced this way is then scored with
the original word and ranked using the heuristic
function (sim()) described in (Contractor et al,
2010)2. We also evaluated a purely phonetic edit-
distance similarity function, based on the Double
Metaphone algorithm (Philips, 2000), but found the
string-similarity-based function to give more reli-
able results.
Each confusion set produced this way (see Al-
gorithm 2) is joined to its previous set to form a
growing confusion lattice. Finally this lattice is de-
coded by converting it into the probabilistic finite-
state grammar format, and by using the SRI-LM
toolkit?s (Stolcke, 2002) lattice-tool com-
mand to find the best path through the lattice by
2The longest common subsequence between the two words,
normalized by the edit distances between their consonant skele-
tons.
23
Transformation Type Rel %
single char (?see?? ?c?) 29.1%
suffix (?why?? ?y?) 18.8%
drop vowels (?be?? ?b?) 16.4%
prefix (?tomorrow?? ?tom?) 9.0%
you to u (?you?? ?u?) 8.3%
drop last char (?running?? ?runnin?) 7.0%
repeat letter (?so?? ?soooo?) 5.5%
contraction (?you will?? ?you?ll?) 5.0%
th to d (?this?? ?dis?) 1.0%
Table 2: Most frequently observed types of transforma-
tions with an example in parentheses. Rel % shows the
relative percentage of the top-10 transformations which
were identified (excluding unidentified transformations)
to belong to a specific class.
making use of a language model to promote fluid-
ity in the text, and trained as follows:
We generated a corpus containing roughly 10M
tokens of clean English tweets. We used a simple
heuristic for selecting clean tweets: For each tweet
we computed if #(OOV )#(IV )+1 < ?, where ? = 0.5
was found to give good results. On this corpus
we trained a trigram language model, using Good-
Turing smoothing. Next, a subset of the LA Times
containing 30M words was used to train a ?general
English? language model in the same way. These
two models were combined3 in the ratio 0.7 to 0.3.
The result of the decoding process is the hypoth-
esized clean tokens of the original sentence. When-
ever the cleanser makes a substitution, it is recorded
for further analysis. Upon closer inspection, it was
found that most transformation types can be recog-
nized by using a fairly simple post-processing step.
Table 2 lists the most frequent types of transforma-
tions. While these transformations do not have per-
fect coverage, they account for over 90% of the (cor-
rect) transformations produced by the cleanser. The
rules fail to cover relatively infrequent edge cases,
such as ?l8r ? later?, ?cuz ? because?, ?dha ?
the?, and ?yep? yes? 4.
3Using the -mix-lm and -lambda and -mix-lambda2
options to the SRI-LM toolkit?s ngram module.
4To our surprise these ?typical texting forms? disappeared
into the long tail in our data set.
Original Cleansed
Swet baby jeebus, some-
one PLEASE WINE ME!
sweet baby jesus , some-
one please wine me !
2 years with Katie today! two years with katie to-
day!
k,hope nobody was
hurt.gud mornin jare
okay , hope nobody was
hurt . good morning jamie
When u a bum but think u
da best person on da court
you doodooforthebooboo
when you a bum but think
you the best person on the
court you dorothy
NYC premiere 2morrow. nice premiere tomorrow .
Table 3: Examples of original and automatically cleansed
versions of Twitter messages.
Algorithm 1 Main cleanser algorithm pseudo code.
The decode() command converts the confusion
network (CN) into PFSG format and decodes it us-
ing the lattice-tool of the SRI-LM toolkit.
Require: Lexicon L, Punctuation set P
function CLEANSE MAIN(Min)
for wi ?Min do
if wi ? L ? P or valid tok(wi) then
Add (1.0, wi) to CNout . Probability 1.0
else
Add conf set(wi) to CNout
end if
end for
return decode(CNout)
end function
Table 3 illustrates some example corrections
made by the cleanser. As the results show, the
cleanser is able to correct many of the more com-
mon types of transformations, but can fail when it
encounters infrequent or out-of-vocabulary terms.
5 Evaluation
This section describes our empirical evaluation and
analysis of how users in different contexts express
themselves differently using microtexts. We focus
specifically on the types of lexical transformations
that are commonly applied globally, within popula-
tions of users, and in a contextualized manner.
24
Algorithm 2 Algorithm pseudo code for generating
confusion set CS. L[wi] is the lexicon partitioning
function for word wi.
Require: Lexicon L, confusion set CS, implemented as
top-K heap containing (si, wi), indexed on si
function CONF SET(wi)
W? translits(wi)
for wj ?W do
for wk ? L[wj ] do
sk ? sim(wj , wk)
if sk > min(CS) then
Add (sk, wk) to CS
end if
end for
end for
return CS
end function
5.1 Out-of-Vocabulary Analysis
We begin by analyzing the types of terms that are
common in microtexts but not typically used in
proper, everyday English texts (such as newspapers).
We refer to such terms as being out-of-vocabulary,
since they are not part of the common written En-
glish lexicon. The goal of this analysis is to un-
derstand how different contexts affect the number
of out-of-vocabulary terms found in microtexts. We
hypothesize that certain contextual factors may in-
fluence a user?s ability (or interest) to formulate
clean microtexts that only contain common English
terms.
We ran our analysis over a collection of one mil-
lion Twitter messages collected using the Twitter
streaming API during 2010. Tweets gathered from
the Twitter API are tagged with a language identifier
that indicates the language a user has chosen for his
or her account. However, we found that many tweets
purported to be English were in fact not. Hence,
we ran all of the tweets gathered through a simple
English language classifier that was trained using a
small set of manually labeled tweets, uses character
trigrams and average word length as features, and
achieves an accuracy of around 93%. The every-
day written English lexicon, which we treat as the
?gold standard? lexicon, was distilled from the same
collection of LA Times news articles described in
Section 4. This yielded a comprehensive lexicon of
approximately half a million terms.
Timezone % In-Vocabulary
Australia 86%
UK 85%
US (Atlantic) 84%
Hong Kong 83%
US (Pacific) 81%
Hawaii 81%
Overall 81%
Table 4: Percentage of in-vocabulary found in large En-
glish lexicon for different geographic locations.
For each tweet, the tokenized terms were looked
up in the LA Times lexicon to determine if the
term was out-of-vocabulary or not. Not surprisingly,
the most frequent out-of-vocabulary terms identi-
fied are Twitter usernames, URLs, hasthags, and RT
(the terminology for a re-broadcast, or re-tweeted,
message). These tokens alone account for approx-
imately half of all out-of-vocabulary tokens. The
most frequent out-of-vocabulary terms include ?lol?,
?haha?, ?gonna?, ?lmao?, ?wanna?, ?omg?, ?gotta?.
Numerous expletives also appear amongst the most
common out-of-vocabulary terms, since such terms
never appear in the LA Times. Out of vocabulary
terms make up 19% of all terms in our data set.
In the remainder of this section, we examine
the out-of-vocabulary properties of different popu-
lations of users based on their geographic location
and their client (e.g., Web-based or mobile phone-
based).
5.1.1 Geographic Locations
To analyze the out-of-vocabulary properties of
users in different geographic locations, we extracted
the time zone information from each Tweet in our
data set. Although Twitter allows users to specify
their location, many users leave this field blank, use
informal terminology (?lower east side?), or fabri-
cate non-existent locations (e.g., ?wherever i want
to be?). Therefore, we use the user?s time zone as
a proxy for their actual location, in hopes that users
have less incentive to provide incorrect information.
For the Twitter messages associated with a given
time zone, we computed the percentage of tokens
found within our LA Times-based lexicon. The re-
sults from this analysis are provided in Table 4. It is
25
Client % In-Vocabulary
Facebook 88%
Twitter for iPhone 84%
Twitter for Blackberry 83%
Web 82%
UberTwitter 78%
Snaptu 73%
Overall 81%
Table 5: Percentage of in-vocabulary found in large En-
glish lexicon for different Twitter clients.
important to note that these results were computed
over hundreds of thousands of tokens, and hence
the variance of our estimates is very small. This
means that the differences observed here are statis-
tically meaningful, even though the absolute differ-
ences tend to be somewhat small.
These results indicate that microtexts composed
by users in different geographic locations exhibit
different amounts of out-of-vocabulary terms. Users
in Australia, the United Kingdom, Hong Kong, and
the East Coast of the United States (e.g., New York
City) include fewer out-of-vocabulary terms in their
Tweets than average. However, users from the West
Coast of the United States (e.g., Los Angeles, CA)
and Hawaii are on-par with the overall average, but
include 5% more out-of-vocabulary terms than the
Australian users.
As expected, the locations with fewer-than-
average in-vocabulary tokens are associated with
non-English speaking countries, despite the output
from the classifier.
5.1.2 Twitter Clients
In a similar experiment, we also investigated the
frequency of out-of-vocabulary terms conditioned
on the Twitter client (or ?source?) used to compose
the message. Example Twitter clients include the
Web-based client at www.twitter.com, official
Twitter clients for specific mobile platforms (e.g.,
iPhone, Android, etc.), and third-party clients. Each
client has its own characteristics, target user base,
and features.
In Table 5, we show the percentage of in-
vocabulary terms for a sample of the most widely
used Twitter clients. Unlike the geographic location-
based analysis, which showed only minor differ-
ences amongst the user populations, we see much
more dramatic differences here. Some clients, such
as Facebook, which provides a way of cross-posting
status updates between the two services, has the
largest percentage of in-vocabulary terms of the ma-
jor clients in our data.
One interesting, but unexpected, finding is that the
mobile phone (i.e., iPhone and Blackberry) clients
have fewer out-of-vocabulary terms, on average,
than the Web-based client. This suggests that ei-
ther the users of the clients are less likely to misspell
words or use slang terminology or that the clients
may have better or more intuitive spell checking ca-
pabilities. A more thorough analysis is necessary to
better understand the root cause of this phenomenon.
At the other end of the spectrum are the UberTwit-
ter and Snaptu clients, which exhibit a substantially
larger number of out-of-vocabulary terms. These
clients are also typically used on mobile devices. As
with our previous analysis, it is difficult to pinpoint
the exact cause of such behavior, but we hypothe-
size that it is a function of user demographics and
difficulties associated with inputting text on mobile
devices.
5.2 Contextual Analysis
In this section, we test the hypothesis that different
user populations make use of different types of lex-
ical transformations. To achieve this goal, we make
use of our noisy text cleanser. For each Twitter mes-
sage run through the cleanser, we record the origi-
nal and cleaned version of each term. For all of the
terms that the cleanser corrects, we automatically
identify which (if any) of the transformation rules
listed in Table 2 explain the transformation between
the original and clean version of the term. We use
this output to analyze the distribution of transforma-
tions observed across different user populations.
We begin by analyzing the types of transforma-
tions observed across Twitter clients. Figure 1 plots
the (normalized) distribution of lexical transforma-
tions observed for the Web, Twitter for Blackberry,
Twitter for iPhone, and UberTwitter clients, grouped
by the transformations. We also group the trans-
formations by the individual clients in Figure 2 for
more direct comparison.
The results show that Web users tend to use more
26
Figure 1: Proportion of transformations observed across
Twitter clients, grouped by transformation type.
contractions than Blackberry and UberTwitter users.
We relate this result to the differences in typing on
a virtual compared to a multi-touch keypad. It was
surprising to see that iPhone users tended to use con-
siderably more contractions than the other mobile
device clients, which we relate to its word-prediction
functionality. Another interesting result is the fact
that Web users often drop vowels to shorten terms
more than their mobile client counterparts. Instead,
mobile users often use suffix-style transformations
more, which is often more aggressive than the drop-
ping vowels transformation, and possibly a result of
the pervasiveness of mobile phones: Large popu-
lations of people?s first interaction with technology
these days are through a mobile phone, a device
where strict length limits are imposed on texting,
and which hence enforce habits of aggressive lex-
ical compression, which might transfer directly to
their use of PCs. Finally, we observe that mobile de-
vice users replace ?you? with ?u? substantially more
than users of the Web client.
We also performed the same analysis across time
zones/locations. The results are presented in Fig-
ure 3 by transformation-type, and again grouped by
location for direct comparison in Figure 4. We ob-
serve, perhaps not surprisingly, that the East Coast
US, West Coast US, and Hawaii are the most similar
with respect to the types of transformations that they
Figure 2: Proportion of transformations observed across
Twitter clients, grouped by client.
commonly use. However, the most interesting find-
ing here is that British users tend to utilize a notice-
ably different set of transformations than American
users in the Pacific time zones. For example, British
users are much more likely to use contractions and
suffixes, but far less likely to drop the last letter of
a word, drop all of the vowels in a word, use prefix-
style transformations, or to repeat a given letter mul-
tiple times. In a certain sense, this suggests that
British users tend to write more proper, less informal
English and make use of strikingly different styles
for shortening words compared to American users.
This might be related to the differences in dialects
between the two regions manifesting itself during a
process of phonetic transliteration when composing
the messages: Inhabitants of the south-west regions
in the US are known for pronouncing for instance
running as runnin?, which manifests as dropping the
last letter, and so forth.
Therefore, when taken with our out-of-vocabulary
analysis, our experimental evaluation shows clear
evidence that different populations of users express
themselves differently online and use different types
of lexical transformations depending on their con-
text. It is our hope that the outcome of this study
will spark further investigation into these types of
issues and ultimately lead to effective contextually-
aware natural language processing and information
retrieval approaches that can adapt to a wide range
of user contexts.
27
Figure 3: Proportion of transformations observed across
geographic locations, grouped by transformation type.
6 Conclusions and Future Work
This paper investigated the writing conventions that
different groups of users use to express themselves
in microtexts. We analyzed characteristics of terms
that are commonly found in English Twitter mes-
sages but are never seen within a large collection
of LA Times news articles. The results showed
that a very small number of terms account for a
large proportion of the out-of-vocabulary terms. The
same analysis revealed that different populations of
users exhibit different propensities to use out-of-
vocabulary terms. For example, it was found that
British users tend to use fewer out-of-vocabulary
terms compared to users within the United States.
We also carried out a contextualized analysis that
leveraged a state-of-the-art noisy text cleanser. By
analyzing the most common types of lexical trans-
formations, it was observed that the types of trans-
formations used varied across Twitter clients (e.g.,
Web-based clients vs. mobile phone-based clients)
and geographic location. This evidence supported
our hypothesis that the measurable contextual indi-
cators surrounding messages in social media play an
important role in determining how messages in these
media vary at the surface (lexical) level from what
might be considered standard English.
The outcome of our empirical evaluation and
subsequent analysis suggests that human language
Figure 4: Proportion of transformations observed across
geographic locations, grouped by location.
technologies (especially natural language process-
ing techniques that rely on well-formed inputs) are
likely to be highly susceptible to failure as the result
of lexical transformations across nearly all popula-
tions and contexts. However, certain simple rules
can be used to clean up a large number of out-of-
vocabulary tokens. Unfortunately, such rules would
not be able to properly correct the long tail of
the out-of-vocabulary distribution. In such cases,
more sophisticated approaches, such as the noisy
text cleanser used in this work, are necessary to
combat the noise. Interestingly, most of the lexical
transformations observed affect non-content words,
which means that most information retrieval tech-
niques will be unaffected by such transformations.
As part of future work, we are generally interested
in developing population and/or context-aware lan-
guage processing and understanding techniques on
top of microtexts. We are also interested in ana-
lyzing different user contexts, such as those based
on age and gender and to empirically quantify the
effect of noise on actual natural language process-
ing and information retrieval tasks, such as part of
speech tagging, parsing, summarization, etc.
Acknowledgments
We would like to thank the anonymous reviewers for
their insightful comments. Stephan Gouws would
like to thank MIH Holdings Ltd. for financial sup-
port during the course of this work.
28
References
A.T. Aw, M. Zhang, J. Xiao, and J. Su. 2006. A Phrase-
based Statistical Model for SMS Text Normalization.
In Proceedings of the COLING/ACL Main Conference
Poster Sessions, pages 33?40. Association for Compu-
tational Linguistics.
S. Bangalore, V. Murdock, and G. Riccardi. 2002. Boot-
strapping Bilingual Data Using Consensus Transla-
tion for a Multilingual Instant Messaging System. In
Proceedings of the 19th International Conference on
Computational Linguistics-Volume 1, pages 1?7. As-
sociation for Computational Linguistics.
R. Beaufort, S. Roekhaut, L.A. Cougnon, and C. Fa-
iron. 2010. A Hybrid Rule/Model-based Finite-State
Framework for Normalizing SMS Messages. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 770?779. Asso-
ciation for Computational Linguistics.
M. Choudhury, R. Saraf, V. Jain, A. Mukherjee, S. Sarkar,
and A. Basu. 2007. Investigation and Modeling of the
Structure of Texting Language. International Journal
on Document Analysis and Recognition, 10(3):157?
174.
D. Contractor, T.A. Faruquie, and L.V. Subramaniam.
2010. Unsupervised Cleansing of Noisy Text. In
Proceedings of the 23rd International Conference on
Computational Linguistics: Posters, pages 189?196.
Association for Computational Linguistics.
P. Cook and S. Stevenson. 2009. An Unsupervised
Model for Text Message Normalization. In Proceed-
ings of the Workshop on Computational Approaches
to Linguistic Creativity, pages 71?78. Association for
Computational Linguistics.
Bo Han and Timothy Baldwin. 2011. Lexical Normal-
isation of Short Text Messages: Makn Sens a #twit-
ter. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies. Association for Compu-
tational Linguistics.
M. Kaufmann and J. Kalita. 2010. Syntactic Normaliza-
tion of Twitter Messages.
C. Kobus, F. Yvon, and G. Damnati. 2008. Normaliz-
ing SMS: Are Two Metaphors Better Than One? In
Proceedings of the 22nd International Conference on
Computational Linguistics-Volume 1, pages 441?448.
Association for Computational Linguistics.
E. Loper and S. Bird. 2002. NLTK: The Natural Lan-
guage Toolkit. In Proceedings of the ACL-02 Work-
shop on Effective tools and Methodologies for Teach-
ing Natural Language Processing and Computational
Linguistics-Volume 1, pages 63?70. Association for
Computational Linguistics.
L. Philips. 2000. The Double Metaphone Search Algo-
rithm. CC Plus Plus Users Journal, 18(6):38?43.
C.E. Shannon and W. Weaver. 1948. The Mathemati-
cal Theory of Communication. Bell System Technical
Journal, 27:623?656.
A. Stolcke. 2002. SRILM - An Extensible Language
Modeling Toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901?904.
29
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 31?38,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Exploiting Partial Annotations with EM Training
Dirk Hovy, Eduard Hovy
Information Sciences Institute
University of Southern California
4676 Admiralty Way, Marina del Rey, CA 90292
{dirkh, hovy}@isi.edu
Abstract
For many NLP tasks, EM-trained HMMs are
the common models. However, in order to es-
cape local maxima and find the best model, we
need to start with a good initial model. Re-
searchers suggested repeated random restarts
or constraints that guide the model evolu-
tion. Neither approach is ideal. Restarts are
time-intensive, and most constraint-based ap-
proaches require serious re-engineering or ex-
ternal solvers. In this paper we measure the ef-
fectiveness of very limited initial constraints:
specifically, annotations of a small number of
words in the training data. We vary the amount
and distribution of initial partial annotations,
and compare the results to unsupervised and
supervised approaches. We find that partial
annotations improve accuracy and can reduce
the need for random restarts, which speeds up
training time considerably.
1 Introduction
While supervised learning methods achieve good
performance in many NLP tasks, they are inca-
pable of dealing with missing annotations. For most
new problems, however, missing data is the norm,
which makes it impossible to train supervised mod-
els. Unsupervised learning techniques can make
use of unannotated data and are thus well-suited for
these problems.
For sequential labeling tasks (POS-tagging, NE-
recognition), EM-trained HMMs are the most com-
mon unsupervised model. However, running vanilla
forward-backward-EM leads to mediocre results,
due to various properties of the training method
(Johnson, 2007). Running repeated restarts with
random initialization can help escape local maxima,
but in order to find the global optimum, we need to
run a great number (100 or more) of them (Ravi and
Knight, 2009; Hovy et al, 2011). However, there
is another solution. Various papers have shown that
the inclusion of some knowledge greatly enhances
performance of unsupervised systems. They intro-
duce constraints on the initial model and the param-
eters. This directs the learning algorithm towards a
better parameter configuration. Types of constraints
include ILP-based methods (Chang et al, 2007;
Chang et al, 2008; Ravi and Knight, 2009), and pos-
terior regularization (Grac?a et al, 2007; Ganchev et
al., 2010). While those approaches are powerful and
yield good results, they require us to reformulate the
constraints in a certain language, and either use an
external solver, or re-design parts of the maximiza-
tion step. This is time-consuming and requires a cer-
tain expertise.
One of the most natural ways of providing con-
straints is to annotate a small amount of data. This
can either be done manually, or via simple heuris-
tics, for example, if some words? parts of speech
are unambiguous. This can significantly speed up
learning and improve accuracy of the learned mod-
els. These partial annotations are a common tech-
nique for semi-supervised learning. It requires no
changes to the general framework, or the use of ex-
ternal solvers.
While this well-known, it is unclear exactly how
much annotation, and annotation of what, is most ef-
fective to improve accuracy. To our knowledge, no
paper has investigated this aspect empirically. We
31
Inputs: I went to the show
walk on water
Partial Annotations: I went to the:DET show:NN
walk on:sense5 water
Figure 1: In partial annotation, words are replaced by
their label
explore the use of more unlabeled data vs. partial
annotation of a small percentage. For the second
case, we investigate how much annotation we need
to achieve a particular accuracy, and what the best
distribution of labels is. We test our approach on
a POS-tagging and word sense disambiguation task
for prepositions.
We find that using partial annotations improves
accuracy and reduces the effect of random restarts.
This indicates that the same accuracy can be reached
with fewer restarts, which speeds up training time
considerably.
Our contributions are:
? we show how to include partial annotations in
EM training via parameter tying
? we show how the amounts and distribution of
partial annotations influence accuracy
? we evaluate our method on an existing data set,
comparing to both supervised and unsupervised
methods on two tasks
2 Preliminaries
2.1 Partial Annotations
When training probabilistic models, more con-
straints generally lead to improved accuracy. The
more knowledge we can bring to bear, the more we
constrain the number of potential label sequences
the training algorithm has to consider. They also
help us to find a good initial model: it has to explain
those fixed cases.
The purest form of unsupervised learning as-
sumes the complete lack of annotation. However,
in many cases, we can use prior knowledge to label
words in context based on heuristics. It is usually
not the case that all labels apply to all observations.
If we know the alphabet of labels we use, we of-
ten also know which labels are applicable to which
observations. This is encoded in a dictionary. For
POS-tagging, it narrows the possible tags for each
word?irrespective of context?down to a manageable
set. Merialdo (1994) showed how the amount of
available dictionary information is correlated with
performance. However, dictionaries list all applica-
ble labels per word, regardless of context. We can
often restrict the applicable label for an observation
in a specific context even more. We extend this to
include constraints applied to some, but not all in-
stances. This allows us to restrict the choice for an
observation to one label. We substitute the word in
case by a special token with just one label. Based on
simple heuristics, we can annotate individual words
in the training data with their label. For example, we
can assume that ?the? is always a determiner. This
is a unigram constraint. We can expand those con-
straints to include a wider context. In a sentence like
?I went to the show?, we know that NN is the only
applicable tag for ?show?, even if a dictionary lists
the possible tags NN and VB. In fact, we can make
that assumption for all words with a possible POS
tag of NN that follow ?the?. This is an n-gram con-
straint.
Partial annotations provide local constraints.
They arise from a number of different cases:
? simple heuristics that allow the disambiguation
of some words in context (such as words after
?the? being nouns)
? when we can leverage annotated data from a
different task
? manual labeling of a few instances
While the technique is mainly useful for problems
where only few labeled examples are available, we
make use of a corpus of annotated data. This allows
us to control the effect of the amount and type of
annotated data on accuracy.
We evaluate the impact of partial annotations on
two tasks: preposition sense disambiguation and
POS tagging.
2.2 Preposition Sense Disambiguation
Prepositions are ubiquitous and highly ambiguous.
Disambiguating prepositions is thus a challenging
and interesting task in itself (see SemEval 2007 task,
32
(Litkowski and Hargraves, 2007)). There are three
elements in the syntactic structure of prepositional
phrases, namely the head word h (usually a noun,
verb, or adjective), the preposition p, and the object
of the preposition, o. The triple (h, p, o) forms a
syntactically and semantically constrained structure.
This structure is reflected in dependency parses as a
common construction.
Tratz and Hovy (2009) show how to use the de-
pendency structure to solve it. Their method out-
performed the previous state-of-the-art (which used
a window-based approach) by a significant margin.
Hovy et al (2011) showed how the sequential na-
ture of the problem can be exploited in unsupervised
learning. They present various sequential models
and training options. They compare a standard bi-
gram HMM and a very complex model that is de-
signed to capture mutual constraints. In contrast to
them, we use a trigram HMM, but move the preposi-
tion at the end of the observed sequence, to condition
it on the previous words. As suggested there, we use
EM with smoothing and random restarts.
2.3 Unsupervised POS-tagging
Merialdo (1994) introduced the task of unsupervised
POS tagging using a dictionary. For each word,
we know the possible labels in general. The model
has to learn the labels in context. Subsequent work
(Johnson, 2007; Ravi and Knight, 2009; Vaswani et
al., 2010) has expanded on this in various ways, with
accuracy between 86% and 96%. In this paper, we
do not attempt to beat the state of the art, but rather
test whether our constraints can be applied to a dif-
ferent task and data set.
3 Methodology
3.1 Data
For PSD, we use the SemEval task data. It con-
sists of a training (16k) and a test set (8k) of sen-
tences with sense-annotated prepositions following
the sense inventory of The Preposition Project, TPP
(Litkowski, 2005). It defines senses for each of the
34 most frequent English prepositions. There are on
average 9.76 senses per preposition (between 2 and
25). We combine training and test and use the an-
notations from the training data to partially label our
corpus. The test data remains unlabeled. We use the
WordNet lexicographer senses as labels for the argu-
ments. It has 45 labels for nouns, verbs, and adjec-
tives and is thus roughly comparable to the prepo-
sitions sense granularity. It also allows us to con-
struct a dictionary for the arguments from WordNet.
Unknown words are assumed to have all possible
senses applicable to their respective word class (i.e.
all noun senses for words labeled as nouns, etc). We
assume that pronouns other than ?it? refer to people.
For the POS-tagged data, we use the Brown cor-
pus. It contains 57k sentences and about 1, 16m
words. We assume a simplified tag set with 38 tags
and a dictionary that lists all possible tags for each
word. For the partial annotations, we label every oc-
currence of ?the?, ?a?, and ?an? as DET, and the next
word with possible tag NN as NN. Additional con-
straints label all prepositions as ?P? and all forms of
?be? as ?V?. We train on the top two thirds and test
on the last third.
For both data sets, we converted all words to
lower case and replaced numbers by ?@?.
3.2 Models
w
1
w
2
l
1
l
2
walk water on
w
3
l
3
Figure 2: PSD: Trigram HMM with preposition as last
element
For POS-tagging, we use a standard bigram
HMM without back-off.
For PSD, we use a trigram HMM, but move the
preposition at the end of the observed sequence, to
condition it on the previous words (see Figure 2).
Since not all prepositions have the same set of la-
bels, we train individual models for each preposi-
tion. We can thus learn different parameter settings
for the different prepositions.
We use EM with smoothing and random restarts
to train our models. For smoothing,  is added to
each fractional count before normalization at each
iteration to prevent overfitting (Eisner, 2002a). We
33
set  to 0.01. We stop training after 40 iterations,
or if the perplexity change between iterations was
less than 0.0001. We experimented with different
numbers of random restarts (none, 10, 50, and 100).
3.3 Dealing with Partial Annotations
The most direct way to constrain a specific word to
only one label is to substitute it for a special to-
ken that has only that label. If we have a partially
annotated example ?walk on-sense5 water? as in-
put (see Figure 1), we add an emission probability
P (word = label |tag = label) to our model.
However, this is problematic in two ways. Firstly,
we have effectively removed a great number of
instances where ?on? should be labeled ?sense5 ?
from our training data, and replaced them with an-
other token: there are now fewer instances from
which we collect C(on|sense5 ). The fractional
counts for our transition parameters are not af-
fected by this, but the counts for emission param-
eter are skewed. We thus essentially siphon prob-
ability mass from P (on|sense5 ) and move it to
P (on : sense5 |sense5 ). Since the test data never
contains labels such as sense5 , our partial annota-
tions have moved a large amount of probability mass
to a useless parameter: we are never going to use
P (on : sense5 |sense5 ) during inference!
Secondly, since EM tends to find uniform distri-
butions (Johnson, 2007), other, rarer labels will also
have to receive some probability. The counts for la-
bels with partial annotations are fixed, so in order to
use the rare labels (for which we have no partial an-
notations), their emission counts need to come from
unlabeled instances. Say sense1 is a label for which
we have no partial annotations. Every time EM col-
lects emission counts from a word ?on? (and not a
labeled version ?on:sensen?), it assigns some of it
to P (on|sense1 ). Effectively, we thus assign too
much probability mass to the emission of the word
from rare labels.
The result of these two effects is the inverse of
what we want: our model will use the label with
the least partial annotations (i.e., a rare label) dis-
proportionately often during inference, while the la-
bels for which we had partial annotations are rarely
used. The resulting annotation has a low accuracy.
We show an example of this in Section 5.
The solution to this problem is simple: param-
eter tying. We essentially have to link each par-
tial annotation to the original word that we replaced.
The observed word ?on? and the partial annotation
?on : sense5 ? should behave the same way during
training. This way, our emission probabilities for
the word ?on? given a label (say, ?sense5 ?) take
the information from the partial annotations into ac-
count. This technique is also described in Eisner
(2002b) for a phonological problem with similar
properties. Technically, the fractional counts we col-
lect for C(on : sense5 |sense5 ) should also count
for C(on|sense5 ). By tying the two parameters to-
gether, we achieve exactly that. This way, we can
prevent probability mass from being siphoned away
from the emission probability of the word, and an
undue amount of probability mass from being as-
signed to rare labels.
4 Experiments
4.1 How Much Annotation Is Needed?
In order to test the effect of partial annotations on
accuracy, we built different training sets. We varied
the amount of partial annotations from 0 to 65% in
increments of 5%. The original corpus we use con-
tains 67% partial annotations, so we were unable to
go beyond this number. We created the different cor-
pora by randomly removing the existing annotations
from our corpus. Since this is done stochastically,
we ran 5 trials for each batch and averaged the re-
sults.
We also test the effect more unsupervised data has
on the task. Theoretically, unsupervised methods
should be able to exploit additional training data. We
use 27k examples extracted from the prepositional
attachment corpus from Ratnaparkhi et al (1994).
4.2 What Kind of Annotation Is Needed?
We can assume that not only the quantity, but also
the distribution of the partial annotations makes a
difference. Given that we can only annotate a cer-
tain percentage of the data, how should we best dis-
tribute those annotations among instances to max-
imize accuracy? In order to test this, we hold the
amount of annotated data fixed, but vary the labels
we use. We choose one sense and annotate only the
instances that have that sense, while leaving the rest
unlabeled.
34
Ideally, one would like to examine all subsets of
annotations, from just a single annotation to all but
one instances of the entire training data. This would
cover the spectrum from unsupervised to supervised.
It is unlikely that there is a uniform best number that
holds for all problems within this immense search
space. Rather, we explore two very natural cases,
and compare them to the unsupervised case, for var-
ious numbers of random restarts:
1. all partial annotations are of the same sense
2. one labeled example of each sense
5 Results
System Acc. (%)
semi-supervised w/o param tying 4.73
MFS baseline 40.00
unsupervised (Hovy et al, 2011) 55.00
semi-supervised, no RR 63.18
semi-supervised, 10 RR 63.12
semi-supervised, 50 RR 63.16
semi-supervised, 100 RR 63.22
semi-supervised, addtl. data, no RR 62.67
semi-supervised, addtl. data, 10 RR 62.47
semi-supervised, addtl. data, 50 RR 62.58
semi-supervised, addtl. data, 100 RR 62.58
supervised (Hovy et al, 2010) 84.50
Table 1: Accuracy of various PSD systems. Baseline is
most frequent sense.
Table 1 shows the results for the PSD systems we
tested. Since not all test sets are the same size, we re-
port the weighted average over all prepositions. For
significance tests, we use two-tailed t-tests over the
difference in accuracy at p < 0.001.
The difference between our models and the base-
line as well as the best unsupervised models in
Hovy et al (2011) are significant. The low accu-
racy achieved without parameter tying underscores
the importance of this technique. We find that the
differences between none and 100 random restarts
are not significant if partial annotations are used.
Presumably, the partial annotations provide a strong
enough constraint to overcome the effect of the ran-
dom initializations. I.e., the fractional counts from
the partial annotations overwhelm any initial param-
eter settings and move the model to a more advanta-
geous position in the state space. The good accuracy
for the case with no restarts corroborates this.
50
55
60
65
70
75
80
85
90
95
100
0 5 10 15 20 25 30 35 40 45 50 55 60 65
a
c
c
u
r
a
c
y
 
(
%
)
amount of annotated prepositions (%)
Figure 3: Accuracy for PSD systems improves linearly
with amount of partial annotations. Accuracies above
dotted line improve significantly (at p < 0.001) over un-
supervised approach (Hovy et al, 2011)
Figure 3 shows the effect of more partial anno-
tations on PSD accuracy. Using no annotations at
all, just the dictionary, we achieve roughly the same
results as reported in Hovy et al (2011). Each incre-
ment of partial annotations increases accuracy. At
around 27% annotated training examples, the differ-
ence starts to be significant. This shows that unsu-
pervised training methods can benefit from partial
annotations. When adding more unsupervised data,
we do not see an increase in accuracy. In this case,
the algorithm failed to make use of the additional
training data. This might be because the two data
sets were not heterogenous enough, or because the
number of emission parameters grew faster than the
amount of available training examples. A possible,
yet somewhat unsatisfying explanation is that when
we increase the overall training data, we reduce the
percentage of labeled data (here to 47%; the result
was comparable to the one observed in our ablation
studies). It seems surprising, though, that the model
does not benefit from the additional data1. More ag-
gressive smoothing might help alleviate that prob-
lem.
The results on the distribution of partial annota-
tion are shown in Figure 4. Using only the most
1Note that similar effects were observed by (Smith and Eis-
ner, 2005; Goldwater and Griffiths, 2007).
35
010
20
30
40
50
60
70
80
90
100
all 1st 2nd 3rd 4th 5th one each
53.55
48.77
44.71
43.00
45.65
49.69
63.12
a
c
c
u
r
a
c
y
 
(
%
)
senses used
Figure 4: Labeling one example of each sense yields bet-
ter results than all examples of any one sense. Senses
ordered by frequency
frequent sense, accuracy drops to 49.69%. While
this is better than the baseline which simply assigns
this sense to every instance, it is a steep drop. We
get better results using just one annotated example
of each sense (53.55%).
System Acc. (%)
(Merialdo, 1994) 86.60
random baseline 62.46
unsupervised, no RR 82.77
semi-supervised, DET+NN 88.51
semi-supervised, DET+NN+P 88.97
semi-supervised, DET+NN+P+V 87.07
Table 2: Accuracy of various POS systems. Random
baseline averaged over 10 runs.
The results for POS tagging confirm our previ-
ous findings. The random baseline chooses for each
word one of the possible tags. We averaged the re-
sults over 10 runs. The difference in accuracy be-
tween both the baseline and the unsupervised ap-
proach as well as the unsupervised approach and any
of the partial annotations are significant. However,
the drop in accuracy when adding the last heuris-
tic points to a risk: partial annotation with heuris-
tics can introduce errors and offset the benefits of
the constraints. Careful selection of the right heuris-
tics and the tradeoff between false positives they in-
troduce and true positives they capture can alleviate
this problem.
6 Related Research
Unsupervised methods have great appeal for
resource-poor languages and new tasks. They have
been applied to a wide variety of sequential label-
ing tasks, such as POS tagging, NE recognition, etc.
The most common training technique is forward-
backward EM. While EM is guaranteed to improve
the data likelihood, it can get stuck in local max-
ima. Merialdo (1994) showed how the the initialized
model influences the outcome after a fixed number
of iterations. The importance is underscored suc-
cinctly by Goldberg et al (2008). They experiment
with various constraints.
The idea of using partial annotations has been
explored in various settings. Druck et al (2008)
present an approach to label features instead of
instances for discriminative probabilistic models,
yielding substantial improvements. They also study
the effectiveness of labeling features vs. labeling in-
stances. Rehbein et al (2009) study the utility of
partial annotations as precursor to further, human
annotation. Their experiments do not extend to un-
supervised training. Tsuboi et al (2008) used data
that was not full annotated. However, their setting
is in principle supervised, only few words are miss-
ing. Instead of no labels, those words have a limited
number of possible alternatives. This works well for
tasks with a small label alphabet or data where anno-
tators left multiple options for some words. In con-
trast, we start out with unannotated data and assume
that some words can be labeled. Gao et al (2010)
present a successful word alignment approach that
uses partial annotations. These are derived from
human annotation or heuristics. Their method im-
proves BLEU, but requires some modification of the
EM framework.
7 Conclusion and Future Work
It is obvious, and common knowledge, that provid-
ing some annotation to an unsupervised algorithm
will improve accuracy and learning speed. Surpris-
ingly, however, our literature search did not turn up
any papers stating exactly how and to what degree
the improvements appear. We therefore selected a
36
very general training method, EM, and a simple ap-
proach to include partial annotations in it using pa-
rameter tying. This allows us to find more stable
starting points for sequential labeling tasks than ran-
dom or uniform initialization. We find that we would
need a substantial amount of additional unlabeled
data in order to boost accuracy. In contrast, we can
get significant improvements by partially annotating
some instances (around 27%). Given that we can
only annotate a certain percentage of the data, it is
best to distribute those annotations among all appli-
cable senses, rather than focus on one. This obviates
the need for random restarts and speeds up training.
This work suggests several interesting new av-
enues to explore. Can one integrate this procedure
into a large-scale human annotation effort to ob-
tain a kind of active learning, suggesting which in-
stances to annotate next, until appropriate stopping
criteria are satisfied (Zhu et al, 2008)? Can one
determine upper bounds for the number of random
restarts given the amount of annotations?
Acknowledgements
We would like to thank Victoria Fossum, Kevin
Knight, Zornitsa Kozareva, and Ashish Vaswani for
invaluable discussions and advice. We would also
like to thank the reviewers who provided us with
helpful feedback and suggestions. Research sup-
ported in part by Air Force Contract FA8750-09-C-
0172 under the DARPA Machine Reading Program.
References
Ming-Wei Chang, Lev Ratinov, and Dan Roth.
2007. Guiding semi-supervision with constraint-
driven learning. In Proceedings of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 280?287, Prague, Czech Republic. Associ-
ation for Computational Linguistics.
Ming-Wei Chang, Lev Ratinov, Nicholas Rizzolo, and
Dan Roth. 2008. Learning and inference with con-
straints. In Proceedings of the 23rd national confer-
ence on Artificial intelligence, pages 1513?1518.
Gregory Druck, Gideon Mann, and Andrew McCallum.
2008. Learning from labeled features using gener-
alized expectation criteria. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 595?602. ACM.
Jason Eisner. 2002a. An interactive spreadsheet for
teaching the forward-backward algorithm. In Pro-
ceedings of the ACL-02 Workshop on Effective tools
and methodologies for teaching natural language pro-
cessing and computational linguistics-Volume 1, pages
10?18. Association for Computational Linguistics.
Jason Eisner. 2002b. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 1?8. Association for Com-
putational Linguistics.
Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, and
Ben Taskar. 2010. Posterior regularization for struc-
tured latent variable models. The Journal of Machine
Learning Research, 11:2001?2049.
Qin Gao, Nguyen Bach, and Stephan Vogel. 2010. A
semi-supervised word alignment algorithm with par-
tial manual alignments. In Proceedings of the Joint
Fifth Workshop on Statistical Machine Translation and
MetricsMATR, pages 1?10. Association for Computa-
tional Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
Em can find pretty good hmm pos-taggers (when given
a good start). In Proceedings of ACL.
Sharon Goldwater and Thomas Griffiths. 2007. A fully
bayesian approach to unsupervised part-of-speech tag-
ging. In ANNUAL MEETING-ASSOCIATION FOR
COMPUTATIONAL LINGUISTICS, volume 45, page
744.
Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar. 2007.
Expectation maximization and posterior constraints.
Advances in Neural Information Processing Systems,
20:569?576.
Dirk Hovy, Stephen Tratz, and Eduard Hovy. 2010.
What?s in a Preposition? Dimensions of Sense Dis-
ambiguation for an Interesting Word Class. In Coling
2010: Posters, pages 454?462, Beijing, China, Au-
gust. Coling 2010 Organizing Committee.
Dirk Hovy, Ashish Vaswani, Stephen Tratz, David Chi-
ang, and Eduard Hovy. 2011. Models and training
for unsupervised preposition sense disambiguation. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 323?328, Portland, Ore-
gon, USA, June. Association for Computational Lin-
guistics.
Mark Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 296?305.
Ken Litkowski and Orin Hargraves. 2007. SemEval-
2007 Task 06: Word-Sense Disambiguation of Prepo-
sitions. In Proceedings of the 4th International
37
Workshop on Semantic Evaluations (SemEval-2007),
Prague, Czech Republic.
Ken Litkowski. 2005. The preposition project.
http://www.clres.com/prepositions.html.
Bernard Merialdo. 1994. Tagging English text with
a probabilistic model. Computational linguistics,
20(2):155?171.
Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.
1994. A maximum entropy model for prepositional
phrase attachment. In Proceedings of the workshop on
Human Language Technology, pages 250?255. Asso-
ciation for Computational Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized models
for unsupervised part-of-speech tagging. In Proceed-
ings of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Confer-
ence on Natural Language Processing of the AFNLP:
Volume 1-Volume 1, pages 504?512. Association for
Computational Linguistics.
Ines Rehbein, Josef Ruppenhofer, and Caroline
Sporleder. 2009. Assessing the benefits of par-
tial automatic pre-labeling for frame-semantic
annotation. In Proceedings of the Third Linguistic
Annotation Workshop, pages 19?26. Association for
Computational Linguistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proceedings of the 43rd Annual Meeting on Associ-
ation for Computational Linguistics, pages 354?362.
Association for Computational Linguistics.
Stephen Tratz and Dirk Hovy. 2009. Disambiguation of
Preposition Sense Using Linguistically Motivated Fea-
tures. In Proceedings of Human Language Technolo-
gies: The 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics, Companion Volume: Student Research Work-
shop and Doctoral Consortium, pages 96?100, Boul-
der, Colorado, June. Association for Computational
Linguistics.
Yuta Tsuboi, Hisashi Kashima, Hiroki Oda, Shinsuke
Mori, and Yuji Matsumoto. 2008. Training condi-
tional random fields using incomplete annotations. In
Proceedings of the 22nd International Conference on
Computational Linguistics, volume 1, pages 897?904.
Association for Computational Linguistics.
Ashish Vaswani, Adam Pauls, and David Chiang. 2010.
Efficient optimization of an mdl-inspired objective
function for unsupervised part-of-speech tagging. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, pages 209?214. Association for Computational
Linguistics.
Jingbo Zhu, Huizhen Wang, and Eduard Hovy. 2008.
Multi-criteria-based strategy to stop active learning
for data annotation. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics-
Volume 1, pages 1129?1136. Association for Compu-
tational Linguistics.
38
Proceedings of the First Workshop on Metaphor in NLP, pages 52?57,
Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational Linguistics
Identifying Metaphorical Word Use with Tree Kernels
Dirk Hovy1 Shashank Srivastava2 Sujay Kumar Jauhar2 Mrinmaya Sachan2
Kartik Goyal2 Huiying Li2 Whitney Sanders2 Eduard Hovy2
(1) ISI, University of Southern California, Marina del Rey
(2) LTI, Carnegie Mellon University, Pittsburgh
dirkh@isi.edu, {shashans,sjauhar,mrinmays,kartikgo,huiyingl,wsanders,hovy}@cs.cmu.edu
Abstract
A metaphor is a figure of speech that refers
to one concept in terms of another, as in ?He
is such a sweet person?. Metaphors are ubiq-
uitous and they present NLP with a range
of challenges for WSD, IE, etc. Identifying
metaphors is thus an important step in lan-
guage understanding. However, since almost
any word can serve as a metaphor, they are
impossible to list. To identify metaphorical
use, we assume that it results in unusual se-
mantic patterns between the metaphor and its
dependencies. To identify these cases, we use
SVMs with tree-kernels on a balanced corpus
of 3872 instances, created by bootstrapping
from available metaphor lists.1 We outper-
form two baselines, a sequential and a vector-
based approach, and achieve an F1-score of
0.75.
1 Introduction
A metaphor is a figure of speech used to transfer
qualities of one concept to another, as in ?He is
such a sweet person?. Here, the qualities of ?sweet?
(the source) are transferred to a person (the target).
Traditionally, linguistics has modeled metaphors as
a mapping from one domain to another (Lakoff and
Johnson, 1980).
Metaphors are ubiquitous in normal language and
present NLP with a range of challenges. First, due to
their very nature, they cannot be interpreted at face
value, with consequences for WSD, IE, etc. Second,
metaphors are very productive constructions, and
almost any word can be used metaphorically (e.g.,
1Available at http://www.edvisees.cs.cmu.edu/
metaphordata.tar.gz
?This is the Donald Trump of sandwiches.?). This
property makes them impossible to pre-define or
list. Third, repeated use of a metaphor eventu-
ally solidifies it into a fixed expression with the
metaphorical meaning now accepted as just another
sense, no longer recognized as metaphorical at all.
This gradient makes it hard to determine a boundary
between literal and metaphorical use of some ex-
pressions. Identifying metaphors is thus a difficult
but important step in language understanding.2
Since many words can be productively used as
new metaphors, approaches that try to identify
them based on lexical features alone are bound to
be unsuccessful. Some approaches have therefore
suggested considering distributional properties
and ?abstractness? of the phrase (Turney et al,
2011). This nicely captures the contextual nature
of metaphors, but their ubiquity makes it impossible
to find truly ?clean? data to learn the separate
distributions of metaphorical and literal use for
each word. Other approaches have used pre-defined
mappings from a source to a target domain, as in
?X is like Y?, e.g., ?emotions are like temperature?
(Mason, 2004). These approaches tend to do well
on the defined mappings, but they do not generalize
to new, creative metaphors. It is doubtful that it
is feasible to list all possible mappings, so these
approaches remain brittle.
In contrast, we do not assume any predefined
mappings. We hypothesize instead that if we inter-
preted every word literally, metaphors will manifest
themselves as unusual semantic compositions.
Since these compositions most frequently occur
2Shutova (2010) distinguishes between metaphor identifica-
tion (which she calls recognition) and interpretation. We are
solely concerned with the former.
52
in certain syntactic relations, they are usually con-
sidered semantic preference violations; e.g., in the
metaphorical ?You will have to eat your words?, the
food-related verb heads a noun of communication.
In contrast, with the literal sense of ?eat? in ?You
will have to eat your peas?, it heads a food noun.
This intuition is the basis of the approaches in
(Iverson and Helmreich, 1991; Krishnakumaran
and Zhu, 2007; Baumer et al, 2010; Turney et
al., 2011).3 We generalize this intuition beyond
preference selections of verbs and relational nouns.
Given enough labeled examples of a word, we
expect to find distinctive differences in the compo-
sitional behavior of its literal and metaphorical uses
in certain preferred syntactic relationships. If we
can learn to detect such differences/anomalies, we
can reliably identify metaphors. Since we expect
these patterns in levels other than the lexical level,
the approach expands well to creative metaphors.
The observation that the anomaly tends to occur
between syntactically related words makes depen-
dency tree kernels a natural fit for the problem. Tree
kernels have been successfully applied to a wide
range of NLP tasks that involve (syntactic) relations
(Culotta and Sorensen, 2004; Moschitti, 2006; Qian
et al, 2008; Giuliano et al, 2009; Mirroshandel et
al., 2011).
Our contributions in this paper are:
? we annotate and release a corpus of 3872 in-
stances for supervised metaphor classification
? we are the first to use tree kernels for metaphor
identification
? our approach achieves an F1-score of 0.75, the
best score of of all systems tested.
2 Data
2.1 Annotation
We downloaded a list of 329 metaphor examples
from the web4. For each expression, we extracted
sentences from the Brown corpus that contained
the seed (see Figure 1 for an example). To decide
3A similar assumption can be used to detect the literal/non-
literal uses of idioms (Fazly et al, 2009).
4http://www.metaphorlist.com and http://
www.macmillandictionaryblog.com
whether a particular instance is used metaphorically,
we set up an annotation task on Amazon Mechanical
Turk (AMT).
Annotators were asked to decide whether a
highlighted expression in a sentence was used
metaphorically or not (see Figure 2 for a screen-
shot). They were prompted to think about whether
the expression was used in its original meaning.5
In some cases, it is not clear whether an expression
is used metaphorically or not (usually in short
sentences such as ?That?s sweet?), so annotators
could state that it was not possible to decide. We
paid $0.09 for each set of 10 instances.
Each instance was annotated by 7 annotators.
Instances where the annotators agreed that it was
impossible to tell whether it is a metaphor or not
were discarded. Inter-annotator agreement was
0.57, indicating a difficult task. In order to get the
label for each instance, we weighted the annotator?s
answers using MACE (Hovy et al, 2013), an
implementation of an unsupervised item-response
model. This weighted voting produces more reliable
estimates than simple majority voting, since it is
capable of sorting out unreliable annotators. The
final corpus consisted of 3872 instances, 1749 of
them labeled as metaphors.
Figure 2: Screenshot of the annotation interface on Ama-
zon?s Mechanical Turk
We divided the data into training, dev, and test
sets, using a 80-10-10 split. All results reported
here were obtained on the test set. Tuning and
development was only carried out on the dev set.
2.2 Vector Representation of Words
The same word may occur in a literal and a
metaphorical usage. Lexical information alone is
5While this is somewhat imprecise and not always easy to
decide, it proved to be a viable strategy for untrained annotators.
53
A bright idea.
? Peter is the bright , sympathetic guy when you ?re doing a deal , ? says one agent . yes
Below he could see the bright torches lighting the riverbank . no
Her bright eyes were twinkling . yes
Washed , they came out surprisingly clear and bright . no
Figure 1: Examples of a metaphor seed, the matching Brown sentences, and their annotations
thus probably not very helpful. However, we would
like to capture semantic aspects of the word and
represent it in an expressive way. We use the exist-
ing vector representation SENNA (Collobert et al,
2011) which is derived from contextual similarity.
In it, semantically similar words are represented
by similar vectors, without us having to define
similarity or looking at the word itself. In initial
tests, these vectors performed better than binary
vectors straightforwardly derived from features of
the word in context.
2.3 Constructing Trees
a) b) c)
like
I people
the sweet in
Boston
NNS
DT JJ IN
n.group
O adj.all O
NNP n.location
VB
PRP
v.emotion
O
Figure 3: Graphic demonstration of our approach. a) de-
pendency tree over words, with node of interest labeled.
b) as POS representation. c) as supersense representation
The intuition behind our approach is that
metaphorical use differs from literal use in certain
syntactic relations. For example, the only difference
between the two sentences ?I like the sweet people
in Boston? and ?I like the sweet pies in Boston? is
the head of ?sweet?. Our assumption is that?given
enough examples?certain patterns emerge (e.g.,
that ?sweet? in combination with food nouns is
literal, but is metaphorical if governed by a noun
denoting people).
We assume that these patterns occur on different
levels, and mainly between syntactically related
words. We thus need a data representation to
capture these patterns. We borrow its structure from
dependency trees, and the different levels from
various annotations. We parse the input sentence
with the FANSE parser (Tratz and Hovy, 2011)6. It
provides the dependency structure, POS tags, and
other information.
To construct the different tree representations,
we replace each node in the tree with its word,
lemma, POS tag, dependency label, or supersense
(the WordNet lexicographer name of the word?s
first sense (Fellbaum, 1998)), and mark the word
in question with a special node. See Figure 3 for
a graphical representation. These trees are used in
addition to the vectors.
This approach is similar to the ones described in
(Moschitti et al, 2006; Qian et al, 2008; Hovy et
al., 2012).
2.4 Classification Models
A tree kernel is simply a similarity matrix over tree
instances. It computes the similarity between two
trees T1, T2 based on the number of shared subtrees.
We want to make use of the information en-
coded in the different tree representations during
classification, i.e., a forest of tree kernels. We thus
combine the contributions of the individual tree
representation kernels via addition. We use kernels
over the lemma, POS tag, and supersense tree
representations, the combination which performed
best on the dev set in terms of accuracy.
We use the SVMlight TK implementation by
Moschitti (2006).7 We left most parameters set
to default values, but tuned the weight of the
contribution of the trees and the cost factor on the
dev set. We set the multiplicative constant for the
trees to 2.0, and the cost factor for errors on positive
examples to 1.7.
6http://www.isi.edu/publications/
licensed-sw/fanseparser/index.html
7http://disi.unitn.it/moschitti/
Tree-Kernel.htm
54
If we assume any word can be used metaphori-
cally, we ultimately want to label every word in a
sentence, so we also evaluate a sequential model, in
this case a CRF. We use CRFsuite (Okazaki, 2007)8
to implement the CRF, and run it with averaged
perceptron. While the CRF produces labels for
every word, we only evaluate on the words that
were annotated in our corpus (to make it maximally
comparable), and use the same representations
(lemma, POS and SST) of the word and its parent
as features as we did for the SVM. Training method
and feature selection were again tuned on the dev
set to maximize accuracy.
3 Experiments
system acc P R F1
BLall 0.49 0.49 1.0 0.66
BLmost freq. class 0.70 0.66 0.65 0.65
CRF 0.69? 0.74? 0.50 0.59
SVMvector?only 0.70? 0.63? 0.80 0.71
SVM+tree 0.75? 0.70? 0.80 0.75?
Table 1: Accuracy, precision, recall, and F1 for various
systems on the held-out test set. Values significantly bet-
ter than baseline at p < .02 are marked ? (two-tailed t-
test).
We compare the performance of two baselines,
the CRF model, vanilla SVM, and SVM with tree
kernels and report accuracy, precision, recall, and
F1 (Table 1).
The first baseline (BLall) labels every instance
as metaphor. Its accuracy and precision reflect the
metaphor ratio in the data, and it naturally achieves
perfect recall. This is a rather indiscriminate
approach and not very viable in practice, so we
also apply a more realistic baseline, labeling each
word with the class it received most often in the
training data (BLmost freq. class ). This is essentially
like assuming that every word has a default class.
Accuracy and precision for this baseline are much
better, although recall naturally suffers.
The CRF improves in terms of accuracy and
precision, but lacks the high recall the baseline
has, resulting in a lower F1-score. It does yield
8http://www.chokkan.org/software/
crfsuite/
the highest precision of all models, though. So
while not capturing every metaphor in the data, it is
usually correct if it does label a word as metaphor.
SVMlight allows us to evaluate the performance
of a classification using only the vector representa-
tion (SVMvector?only). This model achieves better
accuracy and recall than the CRF, but is less precise.
Accuracy is the same as for the most-frequent-
class baseline, indicating that the vector-based
SVM learns to associate a class with each lexical
item. Once we add the tree kernels to the vector
(SVM+tree), we see considerable gains in accuracy
and precision. This confirms our hypothesis that
metaphors are not only a lexical phenomenon, but
also a product of the context a word is used in. The
contextual interplay with their dependencies creates
patterns that can be exploited with tree kernels.
We note that the SVM with tree kernels is the only
system whose F1 significantly improves over the
baseline (at p < .02).
Testing with one tree representation at a time,
we found the various representations differ in terms
of informativeness. Lemma, POS, and supersense
performed better than lexemes or dependency labels
(when evaluated on the dev set) and were thus used
in the reported system. Combining more than one
representation in the same tree to form compound
leaves (e.g. lemma+POS, such as ?man-NN?)
performed worse in all combinations tested. We
omit further details here, since the combinatorics of
these tests are large and yield only little insight.
Overall, our results are similar to comparable
methods on balanced corpora, and we encourage
the evaluation of other methods on our data set.
4 Related Work
There is plenty of research into metaphors. While
many are mainly interested in their general proper-
ties (Shutova, 2010; Nayak, 2011), we focus on the
ones that evaluate their results empirically.
Gedigian et al (2006) use a similar approach
to identify metaphors, but focus on frames. Their
corpus is with about 900 instances relatively small.
They improve over the majority baseline, but only
report accuracy. Both their result and the baseline
are in the 90s, which might be due to the high
number of metaphors (about 90%). We use a larger,
55
more balanced data set. Since accuracy can be
uninformative in cases of unbalanced data sets, we
also report precision, recall, and F1.
Krishnakumaran and Zhu (2007) also use se-
mantic relations between syntactic dependencies
as basis for their classification. They do not aim to
distinguish literal and metaphorical use, but try to
differentiate various types of metaphors. They use a
corpus of about 1700 sentences containing different
metaphors, and report a precision of 0.70, recall of
0.61 (F1 = 0.65), and accuracy of 0.58.
Birke and Sarkar (2006) and Birke and Sarkar
(2007) present unsupervised and active learning
approaches to classifying metaphorical and literal
expressions, reporting F1 scores of 0.54 and 0.65,
outperforming baseline approaches. Unfortunately,
as they note themselves, their data set is ?not large
enough to [...] support learning using a supervised
learning method? (Birke and Sarkar, 2007, 22),
which prevents a direct comparison.
Similarly to our corpus construction, (Shutova et
al., 2010) use bootstrapping from a small seed set.
They use an unsupervised clustering approach to
identify metaphors and report a precision of 0.79,
beating the baseline system by a wide margin. Due
to the focus on corpus construction, they cannot
provide recall or F1. Their approach considers only
pairs of a single verbs and nouns, while we allow
for any syntactic combination.
Tree kernels have been applied to a wide va-
riety of NLP tasks (Culotta and Sorensen, 2004;
Moschitti et al, 2006; Qian et al, 2008; Hovy et
al., 2012). They are specifically adept in capturing
long-range syntactic relationships. In our case, we
use them to detect anomalies in syntactic relations.
5 Conclusion
Under the hypothesis that the metaphorical use of a
word creates unusual patterns with its dependencies,
we presented the first tree-kernel based approach
to metaphor identification. Syntactic dependencies
allow us to capture those patterns at different
levels of representations and identify metaphorical
use more reliably than non-kernel methods. We
outperform two baselines, a sequential model, and
purely vector-based SVM approaches, and reach an
F1 of 0.75. Our corpus is available for download
at http://www.edvisees.cs.cmu.edu/
metaphordata.tar.gz and we encourage the
research community to evaluate other methods on it.
Acknowledgements
The authors would like to thank the reviewers for
helping us clarify several points and giving con-
structive input that helped to improve the quality of
this paper. This work was (in part) supported by
the Intelligence Advanced Research Projects Activ-
ity (IARPA) via Department of Defense US Army
Research Laboratory contract number W911NF-12-
C-0025. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental
purposes notwithstanding any copyright annotation
thereon. Disclaimer: The views and conclusions
contained herein are those of the authors and should
not be interpreted as necessarily representing the of-
ficial policies or endorsements, either expressed or
implied, of IARPA, DoD/ARL, or the U.S. Govern-
ment.
References
Eric P.S. Baumer, James P. White, and Bill Tomlinson.
2010. Comparing semantic role labeling with typed
dependency parsing in computational metaphor identi-
fication. In Proceedings of the NAACL HLT 2010 Sec-
ond Workshop on Computational Approaches to Lin-
guistic Creativity, pages 14?22. Association for Com-
putational Linguistics.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of EACL, volume 6,
pages 329?336.
Julia Birke and Anoop Sarkar. 2007. Active learning for
the identification of nonliteral language. In Proceed-
ings of the Workshop on Computational Approaches
to Figurative Language, pages 21?28. Association for
Computational Linguistics.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Aron Culotta and Jeffrey Sorensen. 2004. Dependency
tree kernels for relation extraction. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, page 423. Association for Com-
putational Linguistics.
56
Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.
2009. Unsupervised type and token identification
of idiomatic expressions. Computational Linguistics,
35(1):61?103.
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press USA.
Matt Gedigian, John Bryant, Srini Narayanan, and Bran-
imir Ciric. 2006. Catching metaphors. In Proceedings
of the 3rd Workshop on Scalable Natural Language
Understanding, pages 41?48.
Claudio Giuliano, Alfio Massimiliano Gliozzo, and Carlo
Strapparava. 2009. Kernel methods for minimally su-
pervised wsd. Computational Linguistics, 35(4).
Dirk Hovy, James Fan, Alfio Gliozzo, Siddharth Patward-
han, and Christopher Welty. 2012. When Did that
Happen? ? Linking Events and Relations to Times-
tamps. In Proceedings of EACL.
Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,
and Eduard Hovy. 2013. Learning Whom to trust with
MACE. In Proceedings of NAACL HLT.
Eric Iverson and Stephen Helmreich. 1991. Non-literal
word sense identification through semantic network
path schemata. In Proceedings of the 29th annual
meeting on Association for Computational Linguistics,
pages 343?344. Association for Computational Lin-
guistics.
Saishuresh Krishnakumaran and Xiaojian Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors we
live by, volume 111. University of Chicago Press.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Seyed A. Mirroshandel, Mahdy Khayyamian, and Gho-
lamreza Ghassem-Sani. 2011. Syntactic tree ker-
nels for event-time temporal relation learning. Human
Language Technology. Challenges for Computer Sci-
ence and Linguistics, pages 213?223.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2006. Tree kernel engineering for proposition
re-ranking. MLG 2006, page 165.
Alessandro Moschitti. 2006. Making Tree Kernels Prac-
tical for Natural Language Learning. In In Proceed-
ings of the 11th Conference of the European Chapter
of the Association for Computational Linguistics.
Sushobhan Nayak. 2011. Towards a grounded model
for ontological metaphors. In Student Research Work-
shop, pages 115?120.
Naoaki Okazaki. 2007. CRFsuite: a fast implementation
of Conditional Random Fields (CRFs).
Longhua Qian, Guodong Zhou, Fang Kong, Qiaoming
Zhu, and Peide Qian. 2008. Exploiting constituent
dependencies for tree kernel-based semantic relation
extraction. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 697?704. Association for Computational Lin-
guistics.
Ekaterina Shutova, Lin Sun, and Anna Korhonen. 2010.
Metaphor identification using verb and noun cluster-
ing. In Proceedings of the 23rd International Confer-
ence on Computational Linguistics, pages 1002?1010.
Association for Computational Linguistics.
Ekaterina Shutova. 2010. Models of metaphor in nlp. In
Proceedings of the 48th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 688?697.
Association for Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, accurate,
non-projective, semantically-enriched parser. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing, pages 1257?1268. As-
sociation for Computational Linguistics.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the 2011 Conference on the Empirical
Methods in Natural Language Processing, pages 680?
690.
57
Proceedings of the The 1st Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 21?28,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
Events are Not Simple: Identity, Non-Identity, and Quasi-Identity   Eduard Hovy Language Technology Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA hovy@cmu.edu  
Teruko Mitamura Language Technology Institute Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213, USA teruko@cs.cmu.edu  
Felisa Verdejo E.T.S.I. Inform?tica, UNED C/ Juan del Rosal, 16 (Ciudad Universitaria) 28040 Madrid, Spain felisa@lsi.uned.es                  Jun Araki Andrew Philpot                   Language Technology Institute Information Sciences Institute                   Carnegie Mellon University University of Southern California                 5000 Forbes Avenue                  Pittsburgh, PA 15213, USA 4676 Admiralty Way Marina del Rey, CA 90292, USA        junaraki@cs.cmu.edu philpot@isi.edu  Abstract1 Despite considerable theoretical and computa-tional work on coreference, deciding when two entities or events are identical is very difficult.  In a project to build corpora containing corefer-ence links between events, we have identified three levels of event identity (full, partial, and none). Event coreference annotation on two cor-pora was performed to validate the findings.    1 The Problem of Identity Last year we had HLT in Montreal, and this year we did it in Atlanta.   Does the ?did it? refer to the same conference or a different one?  The two conferences are not iden-tical, of course, but they are also not totally unre-lated?else the ?did it? would not be interpretable.   When creating text, we treat instances of entities and events as if they are fixed, well-described, and well-understood.  When we say ?that boat over there? or ?Mary?s wedding next month?, we as-sume the reader creates a mental representation of the referent, and we proceed to refer to it without further thought.   However, as has been often noted in theoretical studies of semantics, this assumption is very prob-lematic (Mill, 1872; Frege 1892; Guarino, 1999).  Entities and (even more so) events are complex composite phenomena in the world, and they un-dergo change.                                                              1 This work was supported by grants from DARPA and NSF, as well as by funding that supported Prof. M. Felisa Vedejo from UNED Madrid. 
Since nobody has complete knowledge, the au-thor?s mental image of the entity or event in ques-tion might differ from the reader?s, and from the truth.  Specifically, the properties the author as-sumes for the event or entity might not be the ones the reader assumes. This difference has deep con-sequences for the treatment of the semantic mean-ing of a text.  In particular, it fundamentally affects how one must perform coreference among entities or events.   As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al, 1997).  Determining when two event mentions in text corefer is, however, an un-solved problem2.  Past work in NLP has avoided some of the more complex problems by consider-ing only certain types of coreference, or by simply ignoring the major problems.  The results have been partial, or inconsistent, annotations.   In this paper we describe our approach to the problem of coreference among events.  In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary to create a model of event identi-ty that is more elaborate than is usually assumed in the NLP literature, and to formulate quite specific definitions for its central concepts.                                                               2 In this work, we mean both events and states when we say ?event?.  A state refers to a fixed, or regularly changing, con-figuration of entities in the world, such as ?it is hot? or ?he is running?.  An event occurs when there is a change of state in the world, such as ?he stops running? or ?the plane took off?. 
21
 Event coreference is the problem of determin-ing when two mentions in a text refer to the ?same? event. Whether or not the event actually occurred in reality is a separate issue; a text can describe people flying around on dragons or broomsticks.  While the events might be actual occurrences, hy-pothesized or desired ones, etc., they exist in the text as Discourse Elements (DEs), and this is what we consider in this work. Each DE is referred to (explicitly or implicitly) in the text by a mention, for example ?destroy?, ?the attack?, ?that event?, or ?it?. But it is often unclear whether two mentions refer to the same DE or to closely related ones, or to something alto-gether different. The following example illustrates two principal problems of event coreference:  While Turkish troops have been fighting_E.1 a Kurdish faction in northern Iraq, two other Kurdish groups have been battling_E.2 each other. A radio station operated_E.3 by the Kurdistan Democratic Party said_E.4 the party's forces attacked_E.5 positions of the Patriotic Union of Kurdistan on Monday in the Kurdish re-gion's capital Irbil. The Voice of Iraqi Kurdistan radio, moni-tored_E.6 by the British Broadcasting Corp., said_E.7 more than 80 Patriotic Union fight-ers were killed_E.8 and at least 150 wound-ed_E.9. The fighting_E.10 was also reported_E.11 by a senior Patriotic Union official, Kusret Rasul Ali, who said_E.12 PUK forces re-pelled_E.13 a large KDP attack_E.14. ? Ali claimed_E.16 that 300 KDP fighters were killed_E.17 or wounded_E.18 and only 11 Patriotic Union members died_E.19. Problem 1: Partial event overlap.  Event E.2, ?battling each other?, refers to an ongoing series of skirmishes between two Kurdish groups, the KDP and the PUK.  Since one of these battles, where the KDP attacked positions of the PUK, is E.5, it is natural to say that E.2 and E.5 corefer.  However, E.2 clearly denotes other battles as well, and there-fore E.5 and E.2 cannot fully corefer.  In another example, event E.8 refers to the killing of a num-ber of soldiers as part of this fight E.5, and event E.9 to the wounding of others.  Both events E.8 
and E.9 constitute an intrinsic part of the attack E.5, and hence corefer to it, but are each only part of E.5, and hence neither can fully corefer to it.   Problem 2: Inconsistent reporting.  This news fragment contains two reports of the fight: E.5 and E.10.  Since E.10 describes E.5 from the perspec-tive of a senior PUK official, it should corefer to E.5.  But where the KDP?s report claims more than 80 PUK fighters killed (event E.8, part of E.5), the PUK official said that only 11 PUK members died (event E.19, part of E.10).  Without taking into account the fact that the two killing events are re-ports made by different speakers, it would not be possible to recognize them as coreferent.   Examples of partial event overlap and incon-sistent reporting are common in text, and occur as various types.  In our work, we formally recognize partial event overlap, calling it partial event identi-ty, which permits different degrees and types of event coreference.  This approach simplifies the coreference problem and highlights various inter-event relationships that facilitates grouping events into ?families? that support further analysis and combination with other NLP system components.   In this paper, we introduce the idea that there are three degrees of event identity: fully identical, qua-si-identical, and fully independent (not identical).  Full identity reflects in full coreference and quasi-identity in partial coreference.  Fully independent events are singletons.  Our claims in this paper are:  ? Events, being complex phenomena, can corefer fully (identity) or partially (quasi-identity).  ? Event coreference annotation is considera-bly clarified when partial coreference is allowed.  ? A relatively small fixed set of types of quasi-identity suffices to describe most of them.  ? Different domains and genres highlight different subsets of these quasi-identity types.   ? Different auxiliary knowledge sources and texts are relevant for different types. 2 Types of Full and Partial Identity Def: Two mentions fully corefer if their activi-ty/event/state DE is identical in all respects, as far as one can tell from their occurrence in the text.  (In particular, their agents, location, and time are identical or compatible.)  One can distinguish sev-eral types of identity, as spelled out below.  
22
Def: Two mentions partially corefer if activi-ty/event/state DE is quasi-identical: most aspects are the same, but some additional information is provided for one or the other that is not shared. There are two principal types of quasi-identity, as defined below.  Otherwise, two mentions do not corefer.  2.1 Full Identity  Mention1 is identical to mention2 iff there is no semantic (meaning) difference between them. Just one DE, and exactly the same aspects of the DE, are understood from both mentions in their con-texts. It is possible to replace the one mention with the other without any semantic change (though some small syntactic changes might be required to ensure grammaticality). Note that mention2 may contain less detail than mention1 and remain iden-tical, if it carries over information from mention1 that is understood / inherited from the context.  However, when mention2 provides more or new information not contained in mention1 or naturally inferred for it, then the two are no longer identical. Usually, exact identity is rare within a single text, but may occur more often across texts.  We identi-fy the following types:  1. Lexical identity: The two mentions use exactly the same senses of the same word(s), in-cluding derivational words (e.g., ?destroy?, ?de-struction?). 2. Synonym: One mention?s word is a syno-nym of the other?s word.  3. Wide-reading: One mention is a synonym of the wide reading of the other (defined below, under Quasi-identity:Scriptal).  For example, in ?the attack(E1) took place yesterday.  The bomb-ing(E2) killed four people?, E1 and E2 are fully coreferent only when ?bombing? is read in its wide sense that denotes the whole attack, not the narrow sense that denotes just the actual exploding of the bomb.   4. Paraphrase: One mention is a paraphrase of the other.  Here some syntactic differences may occur.  Some examples are active/passive trans-formation (?she gave him the book? / ?he was giv-en the book by her?), shifts of perspective that do not add or lose information (?he went to Boston? / ?he came to Boston?), etc.  No extra semantic in-formation is provided in one mention or the other.    
5. Pronoun: One mention refers deictically to the DE, as in (?the party? / ?that event?), (?the election [went well]? / ?it [went well]?).   2.2  Quasi-identity  Mention1 is quasi- (partially) identical to mention2 iff they refer to the ?same? DE but one mention includes information that is not contained in the other, not counting information understood/inhe-rited from the context.  They are semantically not fully identical, though the core part of the two mentions is.  One mention can replace the other, but some information will be changed, added, or lost.  (This is the typical case between possible coreferent mentions within a document.)   We distinguish between two core types of partial identity: Membership and Subevent.  The essential difference between the two is which aspects of the two events in question differ.  Member-of obtains when we have two instances of the same event that differ in some particulars, such as time and loca-tion and [some] participants (agents, patients, etc).  In contrast, Subevent obtains when we have differ-ent events that occur at more or less the same place and time with the same cast of participants.   Membership: Mention1 is a set of similar DEs (multiple instances of the same kind of event), like several birthday parties, and mention2 is one or more of them.  More precisely, we say that an event B is a member of A if: (i) A is a set of mul-tiple instances of the same type of event (and hence its mention usually pluralized); (ii) B?s DE(s) is one or more (but not all) of them; (iii) ei-ther or both the time and the place of B?s DE(s) and (some of) A?s DEs are different.  For example, in ?I attended three parties(E1) last month.  The first one(E2) was the best?, E2 is a member of E1.  The relation that links the single instance to the set is member-of.  Subevent: The DE of mention1 is a script (a ste-reotypical sequence of events, performed by an agent in pursuit of a given goal, such as eating at a restaurant, executing a bombing, running for elec-tion), and mention2 is one of the actions/events executed as part of that script (say, paying the waiter, or detonating the bomb, or making a cam-paign speech).  More precisely, we say that an event B is a subevent of an event A if: (i) A is a complex sequence of activities, mostly performed by the same (or compatible) agent; (ii) B is one of 
23
these activities; and (iii) B occurs at the same time and place as A.  Here A acts as a kind of collector event.  Often, the whole script is named by the key event of the script (for example, in ?he planned the explosion?, the ?explosion? signifies the whole script, including planning, planting the bomb, the detonation, etc.; but the actual detonation event itself can also be called ?the explosion?).  We call the interpretation of the mention that refers to the whole script its wide reading, and the interpreta-tion that refers to just the key subevent the narrow reading.  It is important not to confuse the two; a wide reading and a narrow reading of a word can-not corefer3. The relation that links the narrow reading DE to the wide one is sub-to.   Several aspects of the events in question provide key information to differentiate between members and subevents:   1. Time: When the time of occurrence of mention1 is temporally ?close enough? to the time of occurrence of mention2, then it is likely that one is a Subevent of the other.  More precisely, we say that an event B is a subevent of event A if: (i) A and B are both events; (ii) the mentions of A and B both refer to the same overall DE; and (iii) the time of occurrence of B is contained in the time of oc-currence of A. But if (i) and (ii) hold but not (iii), and A is a set of events (plural), then B is a mem-ber of A.  (In (Humphreys et al, 1997), any varia-tion in time automatically results in a decision of non-coreference.)   2. Space/location: The location of mention1 is spatially ?close enough? to the location of men-tion2.  More precisely, we say that an event B is a subevent of event A if: (i) A and B are both events; (ii) the mentions of A and B both refer to the same overall DE; and (iii) the location of oc-currence of B is contained in, or overlaps with, or abuts the location of occurrence of A.  But if (i) and (ii) hold but not (iii), and A is a set of events (plural), then B is a member of A. 
                                                            3 For example, in ?James perpetrated the shooting. He was arrested for the attack?, ?shooting? is used in its wide sense and here is coreferent with ?attack?, since it applies to a whole sequence of events.  In contrast, ?James perpetrated the shoot-ing.  He is the one who actually pulled the trigger?, ?shooting? is used in its narrow sense to mean just the single act.  Typi-cally, a word with two readings can corefer (i.e., be lexically or synonymically identical to) another in the same reading only. 
3.  Event participants: Mention1 and men-tion2 refer to the same DE but differ in the overall cast of participants involved.  In these cases, the member relation obtains, and can be differentiated into subtypes, since participants of events can dif-fer in several ways.  For example, if: (i) the men-tions of events A and B refer to the same overall DE; and (ii) the participants (agents, patients, etc.) of mention2 are a subset of the participants of mention1, as in ?the crowd demonstrated on the square. Susan and Mary were in it?, then event B is a participant-member of event A.  In another ex-ample, event B is a participant-instance-member of event A if: (i) the mentions of events A and B refer to the same overall DE; and (ii) one or more of the participants (agents, patients, etc.) of men-tion2 is/are an instance of the participants of men-tion1, as in ?a firebrand addressed the crowd on the square. Joe spoke for an hour?, where Joe is the firebrand.  There are other ways in which two mentions may refer to the same DE but differ from one an-other.  Usually these differences are not semantic but reflect an orientation or perspective difference.  For example, one mention may include the speak-er?s evaluation/opinion, while the other is neutral, as in ?He sang the silly song.  He embarrassed himself?, or the spatial orientation of the speaker, as in ?she went to New York? / ?she came to New York?.  We treat these cases as fully coreferent.   Sometimes it is very difficult to know whether two mentions are bidirectionally implied, meaning that the two must corefer, or whether they are only quasi-identical (i.e., one entails the other but not vice versa).  For example, in ?he had a heart at-tack? / ?he died?, the two mentions are not identi-cal because one can have a heart attack and not die from it. In contrast, ?he had a fatal heart attack? / ?he died from a heart attack? are identical.  In ?she was elected President? / ?she took office as Presi-dent?, it is more difficult to decide. Does being elected automatically entail taking office?  In some political systems it may, and in others it may not.  When in doubt, we treat the case as only quasi-identical.  Thus, comparing to examples from Full-Identity: Paraphrase, the following are only quasi-identical because of additional information: ?she sold the book? / ?she sold Peter the book?; ?she sold Peter the book? / ?Peter got [not bought] the book from her?. 
24
Quasi-identity has been considered in corefer-ence before in (Hasler et al, 2006) but not as ex-tensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities.  When applied to events, the issue becomes more complex.  3 Two Problems  3.1 Domain and Reporting Events  As described above, inconsistent reporting occurs when a DE stated in reported text contains signifi-cant differences from the author?s description of the same DE.   To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation be-cause they provide a context in which a DE is stat-ed.  We identify two principal types of Reporting verbs: locutionary verbs ?say?, ?report?, ?an-nounce?, etc.) and Speech Acts (?condemn?, ?promise?, ?support?, ?blame?, etc.).  Where the former verbs signal merely a telling, the latter verbs both say and thereby do something.  For ex-ample in the following paragraph, ?admitted? and ?say? are communication events:  Memon admitted_R.7,in-sayR.3 his in-volvement in activities_E.8,in-sayR.3 in-volving an explosives-laden van near the president's motorcade, police said_R.3?.  Sometimes the same event can participate in-side two reporting events, as in   ?The LA Times lauded_R.1 the decision_E.2,in-sayR.1,in-sayR.3, which the NY Times lampooned_R.3. Though an added annotation burden, the link from a DE to a reporting event allows the analyst or learning system to discount apparent contradictory aspects of the DE and make more accurate identity decisions.     3.2 Unclear Semantics of Events  Sometimes it is difficult to determine the exact relationships between events since their semantics is unclear.  In the following, is E.45 coreferent to E.44, or only partially?  If so, how?  Amnesty International has accused both sides of violating_E.44 international humanitarian law by targeting_E.45 civilian areas, and ... 
We decided that E.44 is not fully coreferent with E.45, since violating is not the same as targeting.  Also, E.45 is not a subevent of E.44 since ?violat-ing? is not a script with a well-defined series of steps, does not trigger ?targeting?, and does not occur before ?targeting?.  Rather, targeting is a certain form or example of violation/violating. (It might be easier if the sentence were: ?... of violat-ing international humanitarian law by targeting civilian areas and the human rights group, by kill-ing civilians, and by....?.  As such E.45 could be interpreted as a member of E.44, interpreting the latter as a series of violations.)   4 Annotation  To validate these ideas we have been annotating newspaper texts within the context of a large pro-ject on automated deep reading of text. This pro-ject combines Information Extraction, parsing, and various forms of inference to analyze a small num-ber of texts and to then answer questions about them.  The inability of current text analysis engines to handle event coreference has been a stumbling block in the project.  By creating a corpus of texts annotated for coreference we are working to enable machine learning systems to learn which features are relevant for coreference and then ultimately to perform such coreference as well.  We are annotating two corpora: 1. The Intelligence Community (IC) Corpus contains texts in the Violent Events domain (bombings, killings, wars, etc.).  Given the relative scarcity of the partial coreference subtypes, we annotated only instances of full coreference, Subevent, and Member relations.  To handle Subevents one needs an unambiguous definition of the scripts in the domain.  Fortunately this domain offers a manageable set of events (our event ontol-ogy comprises approximately 50 terms) with a subevent structure that is not overly complex but still realistic.  We did not find the need to exceed three layers of scriptal granularity, as in  campaign > {bombing, attack} > {blast, kill, wound}.  2. The Biography (Bio) Corpus contains texts describing the lives of famous people. Typically, these texts are written when the person dies or has some notable achievement.  Given the complexi-ties of description of artistic and other creative achievements, we restrict our corpus to achieve-
25
ments in politics, science, sports, and other more factual endeavors.  More important than scriptal granularity in this domain is temporal sequencing.  We obtained and modified a version of the An-CoraPipe entity coreference annotation interface (Bertran et al, 2010) that was kindly given us by the AnCora team at the University of Barcelona.  We implemented criteria and an automated method for automatically identifying domain and reporting events.  We also created a tool to check and dis-play the results of annotation, and technology to deliver various agreement scores.  Using different sets of annotators (from 3 to 6 people per text), we have completed a corpus of 100 texts in the IC domain and are in process of annotating the Bio corpus. Our various types of full and partial coreference and the associated an-notation guidelines were developed and refined over the first third of these documents.   Table 1 shows statistics and inter-annotator agreement for the remaining 65 articles.  The aver-age number of domain and reporting events per article is 41.2.  We use Fleiss?s kappa since we have more than two annotators per article.  The (rather low) score for member coreference is not really reliable given the small number of instances.     Avg no per article Agreement (Fleiss?s kappa) Full coreference relations Member coreference relations Subevent coreference relations 19.5 0.620 2.7 0.213 7.2 0.467 Table 1: Annotation statistics and agreement. 5 Validation and Use To validate the conceptualization and definitions of full and partial identity relations, we report in (Araki et al, 2013) a study that determines correla-tions between the Member and Subevent relation instances and a variety of syntactic and lexico-semantic features.  The utility of these features to support automated event coreference is reported in the same paper.   We are now developing a flexible recursive pro-cedure that integrates coreference of events and of their pertinent participants (including locations and times).  This procedure employs inference in addi-tion to feature-based classification to compensate for the shortcomings of each method alone.   
6 Relevant Past Work The problem of identity has been addressed by scholars since antiquity.  In the intensional ap-proach (for example, De Saussure, 1896) a concept is defined as a set of attributes (differentiae), that serve to distinguish it from other concepts; two concepts are identical iff all their attributes and values are.  In the extensional approach (Frege, 1982) a concept can be defined as the set of all in-stances of that concept; two concepts are identical when their two extensional sets are.    Given the impossibility of either approach to support practical work, AI scholars have devoted some attention to so-called Identity Criteria.  Gua-rino (1999) outlines several ?dimensions? along which entities can remain identical or change un-der transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership.   There has not been much theoretical work on semantic identity in the NLP community.  But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Cu-lotta et al, 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009).  Focusing on event coreference are (Humphries et al, 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010).   Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al, 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Ku?ov? and Haji?ov?. 2004), the ACE corpus (Walker et al, 2006), and OntoNotes (Pra-dhan et al, 2007).  Most similar to our work is that of (Hasler et al, 2006). In that study, coreferential events and their arguments (also coreference between the argu-ments) were annotated for the terrorism/security domain, considering five event categories (attack, defend, injure, die, contact), and five event clusters (Bukavu bombing, Peru hostages, Tajikistan hos-tages, Israel suicide bombing and China-Taiwan 
26
hijacking). They also annotated information about the kind of coreferential link, such as identity / synonymy / generalization / specialization / other.   Our work takes further the ideas of (Hasler et al, 2006) and (Recasens et al, 2011) in elaborating the types of full and partial identity, as they are manifest in event coreference.   7 Conclusion The problem of entity and event identity, and hence coreference, is challenging.  We provide a definition of identity and two principal types of quasi-identity, with differentiation based on differ-ences in location, time, and participants.  We hope that these ideas help to clarify the problem and im-prove inter-annotator agreement. Acknowledgments Our grateful thanks goes to Prof. Antonia Mart? and her team for their extensive work on the modi-fications of the AnCoraPipe annotation interface. References  Araki, J., T, Mitamura, and E.H. Hovy. 2013. Identity and Quasi-Identity Relations for Event Coreference. Unpublished manuscript. Bejan, C.A. and S. Harabagiu. 2008. A Linguistic Re-source for Discovering Event Structures and Resolv-ing Event Coreference. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC 08). Bejan, C.A. and S. Harabagiu. 2010. Unsupervised Event Coreference Resolution with Rich Linguistic Features. Proceedings of the 48th conference of the Association for Computational Linguistics (ACL 10). Bertran, M., O. Borrega, M.A. Mart?, and M. Taul?, 2010. AnCoraPipe: A New Tool for Corpora Annota-tion. Working paper 1: TEXT-MESS 2.0 (Text-Knowledge 2.0). Available at http://clic.ub.edu/files/AnCoraPipe_0.pdf  Chen, Z. and H. Ji. 2009. Graph-based Event Corefer-ence Resolution. Proceedings of the ACL-IJCNLP 09 workshop on TextGraphs-4: Graph-based Methods for Natural Language Processing. Culotta, A., M. Wick, and A. McCallum. 2007. First-order probabilistic models for coreference resolution. Proceedings of the HLT/NAACL conference.  
De Saussure, F. 1896. Course in General Linguistics. Open Court Classics. Finkel, J.R. and C.D. Manning. 2008. Enforcing transi-tivity in coreference resolution. Proceedings of the ACL-HLT conference, pp. 45?48.  Florian, R., J F Pitrelli, S Roukos, I Zitouni. 2010. Im-proving Mention Detection Robustness to Noisy In-put.  Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). Frege, G. 1892. On Sense and Reference. Reprinted in P. Geach and M. Black (eds.) Translations from the Philosophical Writings of Gottlob Frege. Oxford: Blackwell, 1960. Guarino, N. 1999. The Role of Identity Conditions in Ontology Design. In C. Freksa and D.M. Mark (eds.), Spatial Information Theory: Cognitive and Computa-tional Foundations of Geographic Information Sci-ence. Proceedings of International Conference COSIT '99.  Springer Verlag. Hasler, L., C. Orasan, and K. Naumann. 2006. NPs for Events: Experiments in Coreference Annotation. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-06), pp. 1167?1172.  Hasler, L. and C. Orasan. 2009. Do Coreferential Ar-guments make Event Mentions Coreferential? Pro-ceedings of the 7th Discourse Anaphora and Anaphor Resolution Colloquium (DAARC 09), pp 151?163. Humphreys, K., R. Gaizauskas and S. Azzam. 1997.  Event Coreference for Information Extraction. Pro-ceedings of the ACL conference Workshop on Opera-tional Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts (ANARESOLU-TION 97).  Ku?ov?, L. and E. Haji?ov?. 2004. Coreferential rela-tions in the Prague Dependency Treebank. Proceed-ings of the DAARC workshop, pp. 97?102. Mani, I. and J. Pustejovsky. 2004. Temporal Discourse Models for Narrative Structure. Proceedings of the ACL 2004 Workshop on Discourse Annotation.  McCarthy, J.F. and W. Lehnert. 1995. Using Decision trees for Coreference Resolution. Proceedings of the IJCAI conference.  Mill, J.S. 1872. A System of Logic, definitive 8th edi-tion. 1949 reprint, London: Longmans, Green and Company. Ng, V. 2007. Shallow Semantics for Coreference Reso-lution. Proceedings of the IJCAI conference. 
27
Ng, V. 2009. Graph-cut-based Anaphoricity Determina- tion for Coreference Resolution. Proceedings of the NAACL-HLT conference, pp. 575?583. Poesio, M. and R. Artstein. 2005. The reliability of ana-phoric annotation, reconsidered: Taking ambiguity into account. Proceedings of the ACL Workshop on Frontiers in Corpus Annotation II. Poesio, M. and R. Artstein. 2008. Anaphoric annotation in the ARRAU corpus. Proceedings of the LREC conference. Pradhan, S., E.H. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel 2007. OntoNotes: A Unified Relational Semantic Representation. Interna-tional Journal of Semantic Computing 1(4), pp. 405?420.  Pustejovsky, J., J. Casta?o, R. Ingria, R. Saur?, R. Gai-zauskas, A. Setzer and G. Katz. 2003. TimeML: Ro-bust Specification of Event and Temporal Expressions in Text. Proceedings of IWCS-5, Fifth International Workshop on Computational Seman-tics. Recasens, M. and E.H. Hovy. 2010a. Coreference Reso-lution across Corpora: Languages, Coding Schemes, and Preprocessing Information. Proceedings of the Association of Computational Linguistics conference (ACL 10).  Recasens, M. and E.H. Hovy. 2010b. BLANC: Imple-menting the Rand Index for Coreference Evaluation.  Journal of Natural Language Engineering 16(5).  Recasens, M., E.H. Hovy, and M.A. Mart?. 2011. Identi-ty, Non-identity, and Near-identity: Addressing the Complexity of Coreference.  Lingua.   Taul?, M., M.A. Mart?. and M. Recasens. 2008. An-Cora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the LREC 08 conference, pp. 96?101.  Walker, C., S. Strassel, J. Medero 2006. The ACE 2005 multilingual training corpus.  Linguistic Data Con-sortium, University of Pennsylvania, Philadelphia. 
28
Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20?29,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
A Structured Distributional Semantic Model : Integrating Structure with
Semantics
Kartik Goyal? Sujay Kumar Jauhar? Huiying Li?
Mrinmaya Sachan? Shashank Srivastava? Eduard Hovy
Language Technologies Institute
School of Computer Science
Carnegie Mellon University
{kartikgo,sjauhar,huiyingl,mrinmays,shashans,hovy}@cs.cmu.edu
Abstract
In this paper we present a novel approach
(SDSM) that incorporates structure in dis-
tributional semantics. SDSM represents
meaning as relation specific distributions
over syntactic neighborhoods. We em-
pirically show that the model can effec-
tively represent the semantics of single
words and provides significant advantages
when dealing with phrasal units that in-
volve word composition. In particular, we
demonstrate that our model outperforms
both state-of-the-art window-based word
embeddings as well as simple approaches
for composing distributional semantic rep-
resentations on an artificial task of verb
sense disambiguation and a real-world ap-
plication of judging event coreference.
1 Introduction
With the advent of statistical methods for NLP,
Distributional Semantic Models (DSMs) have
emerged as powerful method for representing
word semantics. In particular, the distributional
vector formalism, which represents meaning by a
distribution over neighboring words, has gained
the most popularity.
DSMs are widely used in information re-
trieval (Manning et al, 2008), question answer-
ing (Tellex et al, 2003), semantic similarity com-
putation (Wong and Raghavan, 1984; McCarthy
and Carroll, 2003), automated dictionary building
(Curran, 2003), automated essay grading (Lan-
dauer and Dutnais, 1997), word-sense discrimina-
tion and disambiguation (McCarthy et al, 2004;
?*Equally contributing authors
Sch?tze, 1998), selectional preference model-
ing (Erk, 2007) and identification of translation
equivalents (Hjelm, 2007).
Systems that use DSMs implicitly make a bag
of words assumption: that the meaning of a phrase
can be reasonably estimated from the meaning of
its constituents. However, semantics in natural
language is a compositional phenomenon, encom-
passing interactions between syntactic structures,
and the meaning of lexical constituents. It fol-
lows that the DSM formalism lends itself poorly
to composition since it implicitly disregards syn-
tactic structure. For instance, the distributions for
?Lincoln?, ?Booth?, and ?killed? when merged
produce the same result regardless of whether the
input is ?Booth killed Lincoln? or ?Lincoln killed
Booth?. As suggested by Pantel and Lin (2000)
and others, modeling the distribution over prefer-
ential attachments for each syntactic relation sep-
arately can yield greater expressive power.
Attempts have been made to model linguistic
composition of individual word vectors (Mitchell
and Lapata, 2009), as well as remedy the inher-
ent failings of the standard distributional approach
(Erk and Pad?, 2008). The results show vary-
ing degrees of efficacy, but have largely failed to
model deeper lexical semantics or compositional
expectations of words and word combinations.
In this paper we propose an extension to the
traditional DSM model that explicitly preserves
structural information and permits the approxima-
tion of distributional expectation over dependency
relations. We extend the generic DSM model by
representing a word as distributions over relation-
specific syntactic neighborhoods. One can think
of the Structured DSM (SDSM) representation
of a word/phrase as several vectors defined over
the same vocabulary, each vector representing the
20
word?s selectional preferences for a different syn-
tactic argument. We argue that this represen-
tation captures individual word semantics effec-
tively, and is better able to express the semantics
of composed units.
The overarching theme of our framework of
evaluation is to explore the semantic space of the
SDSM. We do this by measuring its ability to dis-
criminate between varying surface forms of the
same underlying concept. We perform the follow-
ing set of experiments to evaluate its expressive
power, and conclude the following:
1. Experiments with single words on similar-
ity scoring and substitute selection: SDSM
performs at par with window-based distribu-
tional vectors.
2. Experiments with phrasal units on two-word
composition: state-of-the-art results are pro-
duced on the dataset from Mitchell and Lap-
ata (2008) in terms of correlation with human
judgment.
3. Experiments with larger structures on the
task of judging event coreferentiality: SDSM
shows superior performance over state-of-
the-art window-based word embeddings, and
simple models for composing distributional
semantic representations.
2 Related Work
Distributional Semantic Models are based on the
intuition that ?a word is characterized by the com-
pany it keeps? (Firth, 1957). While DSMs have
been very successful on a variety of NLP tasks,
they are generally considered inappropriate for
deeper semantics because they lack the ability to
model composition, modifiers or negation.
Recently, there has been a surge in studies to
model a stronger form of semantics by phrasing
the problem of DSM compositionality as one of
vector composition. These techniques derive the
meaning of the combination of two words a and
b by a single vector c = f(a, b). Mitchell and
Lapata (2008) propose a framework to define the
composition c = f(a, b, r,K) where r is the re-
lation between a and b, and K is the additional
knowledge used to define composition.
While the framework is quite general, most
models in the literature tend to disregard K and
r and are generally restricted to component-wise
addition and multiplication on the vectors to be
composed, with slight variations. Dinu and Lap-
ata (2010) and S?aghdha and Korhonen (2011) in-
troduced a probabilistic model to represent word
meanings by a latent variable model. Subse-
quently, other high-dimensional extensions by
Rudolph and Giesbrecht (2010), Baroni and Zam-
parelli (2010) and Grefenstette et al (2011), re-
gression models by Guevara (2010), and recursive
neural network based solutions by Socher et al
(2012) and Collobert et al (2011) have been pro-
posed.
Pantel and Lin (2000) and Erk and Pad? (2008)
attempted to include syntactic context in distri-
butional models. However, their approaches do
not explicitly construct phrase-level meaning from
words which limits their applicability to real world
problems. A quasi-compositional approach was
also attempted in Thater et al (2010) by a system-
atic combination of first and second order context
vectors. To the best of our knowledge the formu-
lation of composition we propose is the first to ac-
count for K and r within the general framework
of composition c = f(a, b, r,K).
3 Structured Distributional Semantics
In this section, we describe our Structured Distri-
butional Semantic framework in detail. We first
build a large knowledge base from sample english
texts and use it to represent basic lexical units.
Next, we describe a technique to obtain the repre-
sentation for larger units by composing their con-
stituents.
3.1 The PropStore
To build a lexicon of SDSM representations for
a given vocabulary we construct a proposition
knowledge base (the PropStore) by processing the
text of Simple English Wikipedia through a de-
pendency parser. Dependency arcs are stored as
3-tuples of the form ?w1, r, w2?, denoting occur-
rences of words w1 and word w2 related by the
syntactic dependency r. We also store sentence
identifiers for each triple for reasons described
later. In addition to the words? surface-forms, the
PropStore also stores their POS tags, lemmas, and
Wordnet supersenses.
The PropStore can be used to query for pre-
ferred expectations of words, supersenses, re-
lations, etc., around a given word. In the
example in Figure 1, the query (SST(W1)
21
Figure 1: Sample sentences & triples
= verb.consumption, ?, dobj) i.e., ?what is
consumed?, might return expectations [pasta:1,
spaghetti:1, mice:1 . . . ]. In our implementation,
the relations and POS tags are obtained using the
Fanseparser (Tratz and Hovy, 2011), supersense
tags using sst-light (Ciaramita and Altun, 2006),
and lemmas are obtained from Wordnet (Miller,
1995).
3.2 Building the Representation
Next, we describe a method to represent lexical
entries as structured distributional matrices using
the PropStore.
The canonical form of a concept C (word,
phrase etc.) in the SDSM framework is a matrix
MC , whose entry MCij is a list of sentence identi-
fiers obtained by querying the PropStore for con-
texts in which C appears in the syntactic neigh-
borhood of the word j linked by the dependency
relation i. As with other distributional models in
the literature, the content of a cell is the frequency
of co-occurrence of its concept and word under the
given relational constraint.
This canonical matrix form can be interpreted
in several different ways. Each interpretation is
based on a different normalization scheme.
1. Row Norm: Each row of the matrix is inter-
preted as a distribution over words that attach
to the target concept with the given depen-
dency relation.
MCij =
Mij
?jMij
?i
2. Full Norm: The entire matrix is interpreted
as a distribution over the word-relation pairs
which can attach to the target concept.
MCij =
Mij
?i,jMij
?i, j
Figure 2: Mimicking composition of two words
3. Collapsed Vector Norm: The columns of
the matrix are collapsed to form a standard
normalized distributional vector trained on
dependency relations rather than sliding win-
dows.
MCj =
?iMij
?i,jMij
?j
3.3 Mimicking Compositionality
For representing intermediate multi-word phrases,
we extend the above word-relation matrix sym-
bolism in a bottom-up fashion. The combina-
tion hinges on the intuition that when lexical units
combine to form a larger syntactically connected
phrase, the representation of the phrase is given
by its own distributional neighborhood within the
embedded parse tree. The distributional neighbor-
hood of the net phrase can be computed using the
PropStore given syntactic relations anchored on its
parts. For the example in Figure 1, we can com-
pose SST(w1) = Noun.person and Lemma(W1)
= eat with relation ?nsubj? to obtain expectations
around ?people eat? yielding [pasta:1, spaghetti:1
. . . ] for the object relation ([dining room:2, restau-
rant:1 . . .] for the location relation, etc.) (See Fig-
ure 2). Larger phrasal queries can be built to an-
swer questions like ?What do people in China eat
with??, ?What do cows do??, etc. All of this helps
22
us to account for both relation r and knowledgeK
obtained from the PropStore within the composi-
tional framework c = f(a, b, r,K).
The general outline to obtain a composition of
two words is given in Algorithm 1. Here, we
first determine the sentence indices where the two
words w1 and w2 occur with relation r. Then,
we return the expectations around the two words
within these sentences. Note that the entire algo-
rithm can conveniently be written in the form of
database queries to our PropStore.
Algorithm 1 ComposePair(w1, r, w2)
M1 ? queryMatrix(w1)
M2 ? queryMatrix(w2)
SentIDs?M1(r) ?M2(r)
return ((M1? SentIDs) ? (M2? SentIDs))
Similar to the two-word composition process,
given a parse subtree T of a phrase, we obtain
its matrix representation of empirical counts over
word-relation contexts. This procedure is de-
scribed in Algorithm 2. Let the E = {e1 . . . en}
be the set of edges in T , ei = (wi1, ri, wi2)?i =
1 . . . n.
Algorithm 2 ComposePhrase(T )
SentIDs? All Sentences in corpus
for i = 1? n do
Mi1 ? queryMatrix(wi1)
Mi2 ? queryMatrix(wi2)
SentIDs? SentIDs ?(M1(ri) ?M2(ri))
end for
return ((M11? SentIDs) ? (M12? SentIDs)
? ? ? ? (Mn1? SentIDs) ? (Mn2? SentIDs))
3.4 Tackling Sparsity
The SDSM model reflects syntactic properties of
language through preferential filler constraints.
But by distributing counts over a set of relations
the resultant SDSM representation is compara-
tively much sparser than the DSM representation
for the same word. In this section we present some
ways to address this problem.
3.4.1 Sparse Back-off
The first technique to tackle sparsity is to back
off to progressively more general levels of lin-
guistic granularity when sparse matrix represen-
tations for words or compositional units are en-
countered or when the word or unit is not in the
lexicon. For example, the composition ?Balthazar
eats? cannot be directly computed if the named en-
tity ?Balthazar? does not occur in the PropStore?s
knowledge base. In this case, a query for a su-
persense substitute ? ?Noun.person eat? ? can be
issued instead. When supersenses themselves fail
to provide numerically significant distributions for
words or word combinations, a second back-off
step involves querying for POS tags. With coarser
levels of linguistic representation, the expressive
power of the distributions becomes diluted. But
this is often necessary to handle rare words. Note
that this is an issue with DSMs too.
3.4.2 Densification
In addition to the back-off method, we also pro-
pose a secondary method for ?densifying? distri-
butions. A concept?s distribution is modified by
using words encountered in its syntactic neighbor-
hood to infer counts for other semantically similar
words. In other terms, given the matrix represen-
tation of a concept, densification seeks to popu-
late its null columns (which each represent a word-
dimension in the structured distributional context)
with values weighted by their scaled similarities to
words (or effectively word-dimensions) that actu-
ally occur in the syntactic neighborhood.
For example, suppose the word ?play? had an
?nsubj? preferential vector that contained the fol-
lowing counts: [cat:4 ; Jane:2]. One might then
populate the column for ?dog? in this vector with
a count proportional to its similarity to the word
cat (say 0.8), thus resulting in the vector [cat:4 ;
Jane:2 ; dog:3.2]. These counts could just as well
be probability values or PMI associations (suitably
normalized). In this manner, the k most similar
word-dimensions can be densified for each word
that actually occurs in a syntactic context. As with
sparse back-off, there is an inherent trade-off be-
tween the degree of densification k and the expres-
sive power of the resulting representation.
3.4.3 Dimensionality Reduction
The final method tackles the problem of sparsity
by reducing the representation to a dense low-
dimensional word embedding using singular value
decomposition (SVD). In a typical term-document
matrix, SVD finds a low-dimensional approxima-
tion of the original matrix where columns become
latent concepts while similarity structure between
rows are preserved. The PropStore, as described in
Section 3.1, is an order-3 tensor with w1, w2 and
23
rel as its three axes. We explore the following two
possibilities to perform dimensionality reduction
using SVD.
Word-word matrix SVD. In this experiment,
we preserve the axes w1 and w2 and ignore the re-
lational information. Following the SVD regime (
W = U?V T ) where ? is a square diagonal ma-
trix of k largest singular values, and U and V are
m? k and n? k matrices respectively. We adopt
matrixU as the compacted concept representation.
Tensor SVD. To remedy the relation-agnostic
nature of the word-word SVD matrix represen-
tation, we use tensor SVD (Vasilescu and Ter-
zopoulos, 2002) to preserve the structural infor-
mation. The mode-n vectors of an order-N tensor
A?RI1?I2?...?IN are the In-dimensional vectors
obtained from A by varying index in while keep-
ing other indices fixed. The matrix formed by all
the mode-n vectors is a mode-n flattening of the
tensor. To obtain the compact representations of
concepts we thus first apply mode w1 flattening
and then perform SVD on the resulting tensor.
4 Single Word Evaluation
In this section we describe experiments and re-
sults for judging the expressive power of the struc-
tured distributional representation for individual
words. We use a similarity scoring task and a lexi-
cal substitute selection task for the purpose of this
evaluation. We compare the SDSM representa-
tion to standard window-based distributional vec-
tors trained on the same corpus (Simple English
Wikipedia). We also experiment with different
normalization techniques outlined in Section 3.2,
which effectively lead to structured distributional
representations with distinct interpretations.
We experimented with various similarity met-
rics and found that the normalized cityblock dis-
tance metric provides the most stable results.
CityBlock(X,Y ) =
ArcTan(d(X,Y ))
d(X,Y )
d(X,Y ) =
1
|R|
?
r?R
d(Xr, Yr)
Results in the rest of this section are thus reported
using the normalized cityblock metric. We also
report experimental results for the two methods
of alleviating sparsity discussed in Section 3.4,
namely, densification and SVD.
4.1 Similarity Scoring
On this task, the different semantic representations
were used to compute similarity scores between
two (out of context) words. We used a dataset
from Finkelstein et al (2002) for our experiments.
It consists of 353 pairs of words along with an av-
eraged similarity score on a scale of 1.0 to 10.0
obtained from 13?16 human judges.
4.2 Lexical Substitute Selection
In the second task, the same set of semantic repre-
sentations was used to produce a similarity rank-
ing on the Turney (2002) ESL dataset. This dataset
comprises 50 words that appear in a context (we
discarded the context in this experiment), along
with 4 candidate lexical substitutions. We eval-
uate the semantic representations on the basis of
their ability to discriminate the top-ranked candi-
date.1
4.3 Results and Discussion
Table 1 summarizes the results for the window-
based baseline and each of the structured distri-
butional representations on both tasks. It shows
that our representations for single words are com-
petitive with window based distributional vectors.
Densification in certain conditions improves our
results, but no consistent pattern is discernible.
This can be attributed to the trade-off between the
gain from generalization and the noise introduced
by semantic drift.
Hence we resort to dimensionality reduction as
an additional method of reducing sparsity. Table
2 gives correlation scores on the Finkelstein et al
(2002) dataset when SVD is performed on the rep-
resentations, as described in Section 3.4.3. We
give results when 100 and 500 principal compo-
nents are preserved for both SVD techniques.
These experiments suggest that though afflicted
by sparsity, the proposed structured distributional
paradigm is competitive with window-based dis-
tributional vectors. In the following sections we
show that that the framework provides consid-
erably greater power for modeling composition
when dealing with units consisting of more than
one word.
1While we are aware of the standard lexical substitution
corpus from McCarthy and Navigli (2007) we chose the one
mentioned above for its basic vocabulary, lower dependence
on context, and simpler evaluation framework.
24
Model Finklestein (Corr.) ESL (% Acc.)
DSM 0.283 0.247
Collapsed 0.260 0.178
FullNorm 0.282 0.192
RowNorm 0.236 0.264
Densified RowNorm 0.259 0.267
Table 1: Single Word Evaluation
Model Correlation
matSVD100 0.207
matSVD500 0.221
tenSVD100 0.267
tenSVD500 0.315
Table 2: Finklestein: Correlation using SVD
5 Verb Sense Disambiguation using
Composition
In this section, we examine how well our model
performs composition on a pair of words. We
derive the compositional semantic representations
for word pairs from the M&L dataset (Mitchell
and Lapata, 2008) and compare our performance
with M&L?s additive and multiplicative models of
composition.
5.1 Dataset
The M&L dataset consists of polysemous intransi-
tive verb and subject pairs that co-occur at least 50
times in the BNC corpus. Additionally two land-
mark words are given for every polysemous verb,
each corresponding to one of its senses. The sub-
ject nouns provide contextual disambiguation for
the senses of the verb. For each [subject, verb,
landmark] tuple, a human assigned score on a 7-
point scale is provided, indicating the compatibil-
ity of the landmark with the reference verb-subj
pair. For example, for the pair ?gun bomb?, land-
mark ?thunder? is more similar to the verb than
landmark ?prosper?. The corpus contains 120 tu-
ples and altogether 3600 human judgments. Re-
liability of the human ratings is examined by cal-
culating inter-annotator Spearman?s ? correlation
coefficient.
5.2 Experiment procedure
For each tuple in the dataset, we derive the com-
posed word-pair matrix for the reference verb-subj
pair based on the algorithm described in Section
3.3 and query the single-word matrix for the land-
mark word. A few modifications are made to ad-
just the algorithm for the current task:
1. In our formulation, the dependency relation
needs to be specified in order to compose
a pair of words. Hence, we determine the
five most frequent relations between w1 and
w2 by querying the PropStore. We then use
the algorithm in Section 3.3 to compose the
verb-subj word pair using these relations, re-
sulting in five composed representations.
2. The word pairs in M&L corpus are ex-
tracted from a parsed version of the BNC cor-
pus, while our PropStore is built on Simple
Wikipedia texts, whose vocabulary is signif-
icantly different from that of the BNC cor-
pus. This causes null returns in our PropStore
queries, in which case we back-off to retriev-
ing results for super-sense tags of both the
words. Finally, the composed matrix and the
landmark matrix are compared against each
other by different matrix distance measures,
which results in a similarity score. For a [sub-
ject, verb, landmark] tuple, we average the
similarity scores yielded by the relations ob-
tained in 1.
The Spearman Correlation ? between our sim-
ilarity ratings and the ones assigned by human
judges is computed over all the tuples. Follow-
ing M&L?s experiments, the inter-annotator agree-
ment correlation coefficient serves an upper bound
on the task.
5.3 Results and Discussion
As in Section 4, we choose the cityblock mea-
sure as the similarity metric of choice. Table 3
shows the evaluation results for two word compo-
sition. Except for row normalization, both forms
of normalization in the structured distributional
paradigm show significant improvement over the
results reported by M&L. The results are statisti-
cally significant at p-value = 0.004 and 0.001 for
Full Norm and Collapsed Vector Norm, respec-
tively.
Model ?
M&L combined 0.19
Row Norm 0.134
Full Norm 0.289
Collapsed Vector Norm 0.259
UpperBound 0.40
Table 3: Two Word Composition Evaluation
These results validate our hypothesis that the in-
tegration of structure into distributional semantics
25
as well as our framing of word composition to-
gether outperform window-based representations
under simplistic models of composition such as
addition and multiplication. This finding is further
re-enforced in the following experiments on event
coreferentiality judgment.
6 Event Coreference Judgment
Given the SDSM formulation and assuming no
sparsity constraints, it is possible to calculate
SDSM matrices for composed concepts. However,
are these correct? Intuitively, if they truly capture
semantics, the two SDSM matrix representations
for ?Booth assassinated Lincoln? and ?Booth shot
Lincoln with a gun" should be (almost) the same.
To test this hypothesis we turn to the task of pre-
dicting whether two event mentions are coreferent
or not, even if their surface forms differ.
While automated resolution of entity coref-
erence has been an actively researched area
(Haghighi and Klein, 2009; Stoyanov et al, 2009;
Raghunathan et al, 2010), there has been rela-
tively little work on event coreference resolution.
Lee et al (2012) perform joint cross-document
entity and event coreference resolution using the
two-way feedback between events and their argu-
ments.
In this paper, however, we only consider coref-
erentiality between pairs of events. Formally,
two event mentions generally refer to the same
event when their respective actions, agents, pa-
tients, locations, and times are (almost) the same.
Given the non-compositional nature of determin-
ing equality of locations and times, we represent
each event mention by a triple E = (e, a, p) for
the event, agent, and patient.
While linguistic theory of argument realiza-
tion is a debated research area (Levin and Rap-
paport Hovav, 2005; Goldberg, 2005), it is com-
monly believed that event structure (Moens and
Steedman, 1988) centralizes on the predicate,
which governs and selects its role arguments
(Jackendoff, 1987). In the corpora we use for
our experiments, most event mentions are verbs.
However, when nominalized events are encoun-
tered, we replace them by their verbal forms. We
use SRL Collobert et al (2011) to determine the
agent and patient arguments of an event mention.
When SRL fails to determine either role, its empir-
ical substitutes are obtained by querying the Prop-
Store for the most likely word expectations for the
role. The triple (e, a, p) is thus the composition
of the triples (a, relagent, e) and (p, relpatient, e),
and hence a complex object. To determine equal-
ity of this complex composed representation we
generate three levels of progressively simplified
event constituents for comparison:
Level 1: Full Composition:
Mfull = ComposePhrase(e, a, p).
Level 2: Partial Composition:
Mpart:EA = ComposePair(e, r, a)
Mpart:EP = ComposePair(e, r, p).
Level 3: No Composition:
ME = queryMatrix(e)
MA = queryMatrix(a)
MP = queryMatrix(p).
To judge coreference between
events E1 and E2, we compute pair-
wise similarities Sim(M1full,M2full),
Sim(M1part:EA,M2part:EA), etc., for each
level of the composed triple representation. Fur-
thermore, we vary the computation of similarity
by considering different levels of granularity
(lemma, SST), various choices of distance metric
(Euclidean, Cityblock, Cosine), and score nor-
malization techniques (Row-wise, Full, Column
collapsed). This results in 159 similarity-based
features for every pair of events, which are used
to train a classifier to make a binary decision for
coreferentiality.
6.1 Datasets
We evaluate our method on two datasets and com-
pare it against four baselines, two of which use
window based distributional vectors and two that
employ weaker forms of composition.
IC Event Coreference Corpus: The dataset
(citation suppressed), drawn from 100 news arti-
cles about violent events, contains manually cre-
ated annotations for 2214 pairs of co-referent
and non-coreferent events each. Where available,
events? semantic role-fillers for agent and patient
are annotated as well. When missing, empirical
substitutes were obtained by querying the Prop-
Store for the preferred word attachments.
EventCorefBank (ECB) corpus: This corpus
(Bejan and Harabagiu, 2010) of 482 documents
from Google News is clustered into 45 topics,
with event coreference chains annotated over each
topic. The event mentions are enriched with se-
mantic roles to obtain the canonical event struc-
ture described above. Positive instances are ob-
26
IC Corpus ECB Corpus
Prec Rec F-1 Acc Prec Rec F-1 Acc
SDSM 0.916 0.929 0.922 0.906 0.901 0.401 0.564 0.843
Senna 0.850 0.881 0.865 0.835 0.616 0.408 0.505 0.791
DSM 0.743 0.843 0.790 0.740 0.854 0.378 0.524 0.830
MVC 0.756 0.961 0.846 0.787 0.914 0.353 0.510 0.831
AVC 0.753 0.941 0.837 0.777 0.901 0.373 0.528 0.834
Table 4: Cross-validation Performance on IC and ECB dataset
tained by taking pairwise event mentions within
each chain, and negative instances are generated
from pairwise event mentions across chains, but
within the same topic. This results in 11039 posi-
tive instances and 33459 negative instances.
6.2 Baselines:
To establish the efficacy of our model, we com-
pare SDSM against a purely window-based base-
line (DSM) trained on the same corpus. In our ex-
periments we set a window size of three words to
either side of the target. We also compare SDSM
against the window-based embeddings trained us-
ing a recursive neural network (SENNA) (Col-
lobert et al, 2011) on both datsets. SENNA em-
beddings are state-of-the-art for many NLP tasks.
The second baseline uses SENNA to generate
level 3 similarity features for events? individual
words (agent, patient and action). As our final
set of baselines, we extend two simple techniques
proposed by Mitchell and Lapata (2008) that use
element-wise addition and multiplication opera-
tors to perform composition. The two baselines
thus obtained are AVC (element-wise addition)
and MVC (element-wise multiplication).
6.3 Results and Discussion:
We experimented with a number of common clas-
sifiers, and selected decision-trees (J48) as they
give the best classification accuracy. Table 4 sum-
marizes our results on both datasets.
The results reveal that the SDSM model con-
sistently outperforms DSM, SENNA embeddings,
and the MVC and AVC models, both in terms
of F-1 score and accuracy. The IC corpus com-
prises of domain specific texts, resulting in high
lexical overlap between event mentions. Hence,
the scores on the IC corpus are consistently higher
than those on the ECB corpus.
The improvements over DSM and SENNA em-
beddings, support our hypothesis that syntax lends
greater expressive power to distributional seman-
tics in compositional configurations. Furthermore,
the increase in predictive accuracy over MVC and
AVC shows that our formulation of composition
of two words based on the relation binding them
yields a stronger form of composition than simple
additive and multiplicative models.
Next, we perform an ablation study to deter-
mine the most predictive features for the task of
determining event coreferentiality. The forward
selection procedure reveals that the most informa-
tive attributes are the level 2 compositional fea-
tures involving the agent and the action, as well as
their individual level 3 features. This corresponds
to the intuition that the agent and the action are the
principal determiners for identifying events. Fea-
tures involving the patient and level 1 features are
least useful. The latter involves full composition,
resulting in sparse representations and hence have
low predictive power.
7 Conclusion and Future Work
In this paper we outlined an approach that intro-
duces structure into distributional semantics. We
presented a method to compose distributional rep-
resentations of individual units into larger com-
posed structures. We tested the efficacy of our
model on several evaluation tasks. Our model?s
performance is competitive for tasks dealing with
semantic similarity of individual words, even
though it suffers from the problem of sparsity.
Additionally, it outperforms window-based ap-
proaches on tasks involving semantic composi-
tion. In future work we hope to extend this for-
malism to other semantic tasks like paraphrase de-
tection and recognizing textual entailment.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their valuable comments and sugges-
tions to improve the quality of the paper. This
work was supported in part by the following
grants: NSF grant IIS-1143703, NSF award IIS-
1147810, DARPA grant FA87501220342.
27
References
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?10, pages 1183?1193, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Cosmin Adrian Bejan and Sanda Harabagiu. 2010.
Unsupervised event coreference resolution with rich
linguistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, ACL ?10, pages 1412?1422, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Massimiliano Ciaramita and Yasemin Altun. 2006.
Broad-coverage sense disambiguation and informa-
tion extraction with a supersense sequence tagger.
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?06, pages 594?602, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Ronan Collobert, Jason Weston, L?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. J. Mach. Learn. Res., 999888:2493?2537,
November.
James Richard Curran. 2003. From distributional to
semantic similarity. Technical report.
Georgiana Dinu and Mirella Lapata. 2010. Measuring
distributional similarity in context. In Proceedings
of the 2010 Conference on Empirical Methods in
Natural Language Processing, EMNLP ?10, pages
1162?1172, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Katrin Erk and Sebastian Pad?. 2008. A structured
vector space model for word meaning in context. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?08,
pages 897?906, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Katrin Erk. 2007. A simple, similarity-based model
for selectional preferences.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan
Ruppin. 2002. Placing search in context: The con-
cept revisited. In ACM Transactions on Information
Systems, volume 20, pages 116?131, January.
John R. Firth. 1957. A Synopsis of Linguistic Theory,
1930-1955. Studies in Linguistic Analysis, pages 1?
32.
Adele E. Goldberg. 2005. Argument Realization: Cog-
nitive Grouping and Theoretical Extensions.
Edward Grefenstette, Mehrnoosh Sadrzadeh, Stephen
Clark, Bob Coecke, and Stephen Pulman. 2011.
Concrete sentence spaces for compositional distri-
butional models of meaning. In Proceedings of the
Ninth International Conference on Computational
Semantics, IWCS ?11, pages 125?134, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of the 2010 Workshop on
GEometrical Models of Natural Language Seman-
tics, GEMS ?10, pages 33?37, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 3 - Volume 3, EMNLP ?09, pages 1152?
1161, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Hans Hjelm. 2007. Identifying cross language term
equivalents using statistical machine translation and
distributional association measures. In Proceedings
of NODALIDA, pages 97?104. Citeseer.
Ray Jackendoff. 1987. The status of thematic roles in
linguistic theory. Linguistic Inquiry, 18(3):369?411.
Thomas K Landauer and Susan T. Dutnais. 1997.
A solution to plato?s problem: The latent semantic
analysis theory of acquisition, induction, and rep-
resentation of knowledge. Psychological review,
pages 211?240.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint entity
and event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 489?500, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Beth Levin and Malka Rappaport Hovav. 2005. Argu-
ment Realization. Cambridge University Press.
Christopher D. Manning, Prabhakar Raghavan, and
Hinrich Sch?tze. 2008. Introduction to Information
Retrieval. Cambridge University Press, New York,
NY, USA.
Diana McCarthy and John Carroll. 2003. Disam-
biguating nouns, verbs, and adjectives using auto-
matically acquired selectional preferences. Comput.
Linguist., 29(4):639?654, December.
Diana McCarthy and Roberto Navigli. 2007. Semeval-
2007 task 10: English lexical substitution task.
In Proceedings of the 4th International Workshop
on Semantic Evaluations (SemEval-2007), Prague,
Czech Republic, pages 48?53.
28
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses in
untagged text. In Proceedings of the 42nd Annual
Meeting on Association for Computational Linguis-
tics, ACL ?04, Stroudsburg, PA, USA. Association
for Computational Linguistics.
George A. Miller. 1995. Wordnet: a lexical database
for english. Commun. ACM, 38(11):39?41, Novem-
ber.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings
of ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language
models based on semantic composition. In Proceed-
ings of the 2009 Conference on Empirical Methods
in Natural Language Processing: Volume 1 - Volume
1, EMNLP ?09, pages 430?439, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Marc Moens and Mark Steedman. 1988. Temporal on-
tology and temporal reference. Computational lin-
guistics, 14(2):15?28.
Patrick Pantel and Dekang Lin. 2000. Word-for-word
glossing with contextually similar words. In Pro-
ceedings of the 1st North American chapter of the
Association for Computational Linguistics confer-
ence, NAACL 2000, pages 78?85, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?10,
pages 492?501, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Rudolph and Eugenie Giesbrecht. 2010.
Compositional matrix-space models of language. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 907?916, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Hinrich Sch?tze. 1998. Automatic word sense dis-
crimination. Comput. Linguist., 24(1):97?123.
Diarmuid ? S?aghdha and Anna Korhonen. 2011.
Probabilistic models of similarity in syntactic con-
text. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1047?1057, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Richard Socher, Brody Huval, Christopher D. Man-
ning, and Andrew Y. Ng. 2012. Semantic com-
positionality through recursive matrix-vector spaces.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
EMNLP-CoNLL ?12, pages 1201?1211, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP: Volume 2 - Volume 2,
ACL ?09, pages 656?664, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Stefanie Tellex, Boris Katz, Jimmy J. Lin, Aaron Fer-
nandes, and Gregory Marton. 2003. Quantitative
evaluation of passage retrieval algorithms for ques-
tion answering. In SIGIR, pages 41?47.
Stefan Thater, Hagen F?rstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations us-
ing syntactically enriched vector models. In Pro-
ceedings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, ACL ?10, pages
948?957, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Stephen Tratz and Eduard Hovy. 2011. A fast, ac-
curate, non-projective, semantically-enriched parser.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
?11, pages 1257?1268, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Peter D. Turney. 2002. Mining the web for synonyms:
Pmi-ir versus lsa on toefl. CoRR.
M. Alex O. Vasilescu and Demetri Terzopoulos. 2002.
Multilinear analysis of image ensembles: Tensor-
faces. In In Proceedings of the European Confer-
ence on Computer Vision, pages 447?460.
S. K. M. Wong and Vijay V. Raghavan. 1984. Vector
space model of information retrieval: a reevaluation.
In Proceedings of the 7th annual international ACM
SIGIR conference on Research and development in
information retrieval, SIGIR ?84, pages 167?185,
Swinton, UK. British Computer Society.
29
Proceedings of the Second Workshop on Metaphor in NLP, pages 18?26,
Baltimore, MD, USA, 26 June 2014.
c?2014 Association for Computational Linguistics
Metaphor Detection through Term Relevance
Marc Schulder
Spoken Language Systems
Saarland University
Saarbr?ucken, Germany
marc.schulder@lsv.uni-saarland.de
Eduard Hovy
Language Technologies Institute
Carnegie Mellon University
Pittsburth, PA, USA
hovy@cs.cmu.edu
Abstract
Most computational approaches to
metaphor detection try to leverage either
conceptual metaphor mappings or selec-
tional preferences. Both require extensive
knowledge of the mappings/preferences
in question, as well as sufficient data for
all involved conceptual domains. Creating
these resources is expensive and often
limits the scope of these systems.
We propose a statistical approach to
metaphor detection that utilizes the rarity
of novel metaphors, marking words that
do not match a text?s typical vocabulary
as metaphor candidates. No knowledge
of semantic concepts or the metaphor?s
source domain is required.
We analyze the performance of this
approach as a stand-alone classifier and
as a feature in a machine learning model,
reporting improvements in F
1
measure
over a random baseline of 58% and 68%,
respectively. We also observe that, as
a feature, it appears to be particularly
useful when data is sparse, while its effect
diminishes as the amount of training data
increases.
1 Introduction
Metaphors are used to replace complicated or un-
familiar ideas with familiar, yet unrelated concepts
that share an important attribute with the intended
idea. In NLP, detecting metaphors and other non-
literal figures of speech is necessary to interpret
their meaning correctly. As metaphors are a pro-
ductive part of language, listing known examples
is not sufficient. Most computational approaches
to metaphor detection are based either on the the-
ory of conceptual mappings (Lakoff and John-
son, 1980) or that of preference violation (Wilks,
1978).
Lakoff and Johnson (1980) showed that
metaphors have underlying mappings between
two conceptual domains: The figurative source
domain that the metaphor is taken from and the
literal target domain of the surrounding context in
which it has to be interpreted. Various metaphors
can be based on the same conceptual metaphor
mapping, e.g. both ?The economy is a house of
cards? and ?the stakes of our debates appear
small? match POLITICS IS A GAME.
Another attribute of metaphors is that they
violate semantic selectional preferences (Wilks,
1978). The theory of selectional preference ob-
serves that verbs constrain their syntactic argu-
ments by the semantic concepts they accept in
these positions. Metaphors violate these con-
straints, combining incompatible concepts.
To make use of these theories, extensive knowl-
edge of pairings (either mappings or preferences)
and the involved conceptual domains is required.
Especially in the case of conceptual mappings, this
makes it very difficult for automated systems to
achieve appropriate coverage of metaphors. Even
when limited to a single target domain, detecting
all metaphors would require knowledge of many
metaphoric source domains to cover all relevant
mappings (which themselves have to be known,
too). As a result of this, many systems attempt
to achieve high precision for specific mappings,
rather than provide general coverage.
Many approaches (Gedigian et al., 2006; Krish-
nakumaran and Zhu, 2007; Mohler et al., 2013;
Tsvetkov et al., 2013, and more) make use of man-
ually crafted knowledge bases such as WordNet or
FrameNet to establish concept domains. Other re-
cent works establish domains via topic modeling
(Shutova et al., 2010; Heintz et al., 2013), ad-hoc
clustering (Strzalkowski et al., 2013) or by using
semantic similarity vectors (Hovy et al., 2013).
We introduce term relevance as a measure for
how ?out of place? a word is in a given con-
18
text. Our hypothesis is that words will often be
out of place because they are not meant literally,
but rather metaphorically. Term relevance is based
on term frequency measures for target domains
and mixed-domain data. The advantage of this
approach is that it only requires knowledge of a
text?s literal target domain, but none about any
source domains or conceptual mappings. As it
does not require sentence structure information,
it is also resistant to noisy data, allowing the use
of large, uncurated corpora. While some works
that utilize domain-mappings circumvent the need
for pre-existing source data by generating it them-
selves (Strzalkowski et al., 2013; Mohler et al.,
2013), our approach is truly source-independent.
We present a threshold classifier that uses term
relevance as its only metric for metaphor detec-
tion. In addition we evaluate the impact of term
relevance at different training sizes.
Our contributions are:
? We present a measure for non-literalness that
only requires data for the literal domain(s) of
a text.
? Our approach detects metaphors indepen-
dently of their source domain.
? We report improvements for F
1
of 58%
(stand-alone) and 68% (multi-feature) over a
random baseline.
2 Term Relevance
We hypothesize that novel metaphoric language
is marked by its unusualness in a given context.
There will be a clash of domains, so the vocab-
ulary will be noticeably different
1
. Therefore, an
unusual choice of words may indicate metaphoric-
ity (or non-literalness, at the least).
We measure this fact through a domain-specific
term relevance metric. The metric consists of
two features: Domain relevance, which measures
whether a term is typical for the literal target do-
main of the text, and common relevance, which
indicates terms that are so commonly used across
domains that they have no discriminative power.
If a term is not typical for a text?s domain (i.e.
1
Strongly conventionalized metaphors will not meet this
expectation, as they have become part of the target domain?s
vocabulary. Such metaphors can be easily detected by con-
ventional means, such as knowledge bases. Our concern is
therefore focused on novel metaphors.
has a low relevance), but is not very common ei-
ther, it is considered a metaphor candidate. This
can of course be extended to multiple literal do-
mains (e.g. a political speech on fishing regula-
tions will have both governance and maritime vo-
cabulary), in which case a word is only considered
as a metaphor if it is untypical for all domains in-
volved.
2.1 Metric
We base domain relevance on TF-IDF (term fre-
quency inverse document frequency), which is
commonly used to measure the impact of a term
on a particular document. Terms with a great im-
pact receive high scores, while low scores are as-
signed to words that are either not frequent in the
document or otherwise too frequent among other
documents.
We adapt this method for domain relevance (dr)
by treating all texts of a domain as a single ?doc-
ument?. This new term frequency inverse domain
frequency measures the impact of a term on the
domain.
tf
dom
(t, d) =
# of term t in domain d
# of terms in domain d
(1)
idf
dom
(t) = log
# of domains
# of domains containing t
(2)
dr(t, d) = tf
dom
(t, d)? idf
dom
(t) (3)
To detect metaphors, we look for terms with low
scores in this feature. However, due to the nature
of TF-IDF, a low score might also indicate a word
that is common among all domains. To filter out
such candidates, we use normalized document fre-
quency as a common relevance indicator.
cr(t) =
# of documents containing t
# of documents
(4)
In theory, we could also use domain frequency
to determine common relevance, as we already
compute it for domain relevance. However, as this
reduces the feature?s granularity and otherwise be-
haves the same (as long as domains are of equal
size), we keep regular document frequency.
2.2 Generating Domains
We need an adequate number of documents for
each domain of interest to compute domain rele-
vance for it. We require specific data for the literal
domain(s) of a text, but none for the metaphor?s
19
source domains. This reduces the required num-
ber of domain data sets significantly without rul-
ing out any particular metaphor mappings.
We extract domain-specific document collec-
tions from a larger general corpus, using the key-
word query search of Apache Lucene
2
, a software
for indexed databases. The keywords of the query
search are a set of seed terms that are considered
typical literal terms for a domain. They can be
manually chosen or extracted from sample data.
For each domain we extract the 10,000 highest
ranking documents and use them as the domain?s
dataset.
Afterwards, all remaining documents are ran-
domly assigned to equally sized pseudo-domain
datasets. These pseudo-domains allow us to com-
pute the inverse of the domain frequency for the
TF-IDF without the effort of assigning all docu-
ments to proper domains. The document frequency
score that will be used as common relevance is di-
rectly computed on the documents of the complete
corpus.
3 Data
We make use of two different corpora. The first is
the domain-independent corpus required for com-
puting term relevance. The second is an evalua-
tion corpus for the governance domain on which
we train and test our systems.
Both corpora are preprocessed using NLTK
(Loper and Bird, 2002)
3
. After tokenization, stop-
words and punctuation are removed, contractions
expanded (e.g. we?ve to we have) and numbers
generalized (e.g. 1990?s to @?s). The remaining
words are reduced to their stem to avoid data spar-
sity due to morphological variation.
In case of the domain corpus, we also removed
generic web document contents, such as HTML
mark-up, JavaScript/CSS code blocks and similar
boilerplate code
4
.
3.1 Domain Corpus
As a basis for term relevance, we require a large
corpus that is domain-independent and ideally also
style-independent (i.e. not a newspaper corpus or
2
http://lucene.apache.org/core/
3
http://nltk.org
4
Mark-up and boilerplate removal scripts adapted
from http://love-python.blogspot.com/
2011/04/html-to-text-in-python.html and
http://effbot.org/zone/re-sub.htm
Wikipedia). The world wide web meets these re-
quirements. However, we cannot use public online
search engines, such as Google or Bing, because
they do not allow a complete overview of their in-
dexed documents. As we require this provide to
generate pseudo-domains and compute the inverse
document/domain frequencies, we use a precom-
piled web corpus instead.
ClueWeb09
5
contains one billion web pages,
half of which are English. For reasons of process-
ing time and data storage, we limited our experi-
ments to a single segment (en0000), containing 3
million documents. The time and storage consid-
erations apply to the generation of term relevance
values during preprocessing, due to the require-
ments of database indexing. They do not affect
the actual metaphor detection process, therefore,
we do not expect scalability to be an issue. As
ClueWeb09 is an unfiltered web corpus, spam fil-
tering was required. We removed 1.2 million spam
documents using the Waterloo Spam Ranking for
ClueWeb09
6
by Cormack et al. (2011).
3.2 Evaluation Corpus
Evaluation of the two classifiers is done with a cor-
pus of documents related to the concept of gov-
ernance. Texts were annotated for metaphoric
phrases and phrases that are decidedly in-domain,
as well as other factors (e.g. affect) that we will not
concern ourselves with. The focus of annotation
was to exhaustively mark metaphors, irrespective
of their novelty, but avoid idioms and metonymy.
The corpus is created as part of the MICS:
Metaphor Interpretation in terms of Culturally-
relevant Schemas project by the U.S. Intelligence
Advanced Research Projects Activity (IARPA).
We use a snapshot containing 2,510 English sen-
tences, taken from 312 documents. Of the 2,078
sentences that contain metaphors, 72% contain
only a single metaphoric phrase. The corpus con-
sists of around 48k tokens, 12% of which are parts
of metaphors. Removing stopwords and punctua-
tion reduces it to 23k tokens and slightly skews the
distribution, resulting in 15% being metaphors.
We divide the evaluation data into 80% devel-
opment and 20% test data. All reported results are
based on test data. Where training data is required
for model training (see section 5), ten-fold cross
validation is performed on the development set.
5
http://lemurproject.org/clueweb09/
6
http://plg.uwaterloo.ca/
?
gvcormac/
clueweb09spam/
20
Subdomain Seed Terms
Executive administer rule govern lead
Legislative pass law regulate debate parliament
Judicial judge hearing case rule case
sentence
Administr. administer manage issue permits
analyze study facilitate obstruct
Enforcement enforce allow permit require war
make mandate defeat overcome
Economy budget tax spend plan finances
Election vote campaign canvass elect defeat
form party create platform
Acceptance government distrust (de)legitimize
authority reject oppose strike flag
protest pride salute march accept
Table 1: Manually selected seed terms for docu-
ment search queries. The 10k documents with the
highest relevance to the seeds are assigned to the
subdomain cluster.
4 Basic Classification
To gain an impression of the differentiating power
of tf-idf in metaphor detection, we use a basic
threshold classifier (tc) that uses domain relevance
(dr) and common relevance (cr) as its only fea-
tures. Given a word w, a target domain d and two
thresholds ? and ?:
tc(w, d) =
?
?
?
metaphor if dr(w, d) < ?
and cr(w) < ?
literal otherwise
(5)
In cases where a text has more than one literal do-
main or multiple relevant subdomains are avail-
able, a word is only declared a metaphor if it is
not considered literal for any of the (sub)domains.
4.1 Seed Terms
The threshold classifier is evaluated using two dif-
ferent sets of seed terms. The first set is com-
posed of 60 manually chosen terms
7
from eight
governance subdomains. These are shown in table
1. Each subdomain corpus consists of its 10,000
highest ranking documents. We do not subdi-
vide the evaluation corpus into these subdomains.
Rather, we assume that each sentence belongs to
7
Terms were chosen according to human understanding
of typical terms for governance. No optimization of the term
choices was performed thereafter.
principl financi legisl congress crisi
corpor famili middl compani futur
countri global negoti medicaid unit
industri promis polici constitut save
obama health creat capitalist hous
clinton nation dream american busi
nuclear amend great medicar care
econom million feder recoveri job
commun potenti polit freedom law
prosper energi elect program new
Table 2: The fifty stems with the highest tf-idf
score in the gold data. Used as seed terms for doc-
ument search, generating a single governance do-
main. Stems are listed in no particular order.
all eight subdomains
8
, so a word is only consid-
ered a metaphor if it is non-literal for all of them.
Preliminary experiments showed that this provides
better performance than using a single domain cor-
pus with more documents.
As the first set of seeds is chosen without sta-
tistical basis, the resulting clusters might miss im-
portant aspects of the domain. To ensure that our
evaluation is not influenced by this, we also in-
troduce a second seed set, which is directly based
on the development data. As we mentioned in
section 3.2, sentences in the MICS corpus were
not only annotated for metaphoric phrases, but
also for such that are decidedly domain-relevant.
For example in the sentence ?Our economy is the
strongest on earth?, economy is annotated as in-
domain and strongest as metaphor.
Based on these annotations, we divide the en-
tire development data into three bags of words,
one each for metaphor, in-domain and unmarked
words. We then compute TF-IDF values for these
bags, as we did for the domain clusters. The fifty
terms
9
that score highest for the in-domain bag
(i.e. those that make the texts identifiable as gover-
nance texts) are used as the second set of seeds (ta-
ble 2). It should be noted that while the seeds were
based on the evaluation corpus, the resulting term
relevance features were nevertheless computed us-
ing clusters extracted from the web corpus.
8
As our evaluation corpus does not specify secondary do-
mains for its texts (e.g. fishery), we chose not to define any
further domains at this point.
9
Various sizes were tried for the seed set. Using fifty
terms offered the best performance, being neither too specific
nor watering down the cluster quality. It is also close to the
size of our first seed set.
21
F1
Prec Rec
Random 0.222 0.142 0.500
All Metaphor 0.249 0.142 1.000
T-hold: Manual Seeds 0.350 0.276 0.478
T-hold: 50-best Seeds 0.346 0.245 0.591
Table 3: Summary of best performing settings
for each threshold classifier model. Bold num-
bers indicate best performance; slanted bold num-
bers: best threshold classifier recall. All results
are significantly different from the baselines with
p < 0.01.
4.2 Evaluation
We evaluate and optimize our systems for the F
1
metric. In addition we provide precision and re-
call. Accuracy on the other hand proved an inap-
propriate metric, as the prevalence of literal words
in our data resulted in a heavy bias. We eval-
uate on a token-basis, as half of the metaphoric
phrases consist of a single word and less than 15%
are more than three words long (including stop-
words, which are filtered out later). Additionally,
evaluating on a phrase-basis would have required
grouping non-metaphor sections into phrases of a
similar format.
Based on dev set performance, we choose a do-
main relevance threshold ? = 0.02 and a common
relevance threshold ? = 0.1. We provide a ran-
dom baseline, as well as one that labels all words
as metaphors, as they are the most frequently en-
countered baselines in related works. Results are
shown in table 3.
Both seed sets achieve similar F-scores, beating
the baselines by between 39% and 58%, but their
precision and recall performance differs notably.
Both models are significantly better than the base-
line and significantly different from one another
with p < 0.01. Significance was computed for a
two-tailed t-test using sigf (Pad?o, 2006)
10
.
Using manually chosen seed terms results in a
recall rate that is slightly worse than chance, but
it is made up by the highest precision. The fact
that this was achieved without expert knowledge
or term optimization is encouraging.
The classifier using the fifty best governance
terms shows a stronger recall, most likely be-
10
http://www.nlpado.de/
?
sebastian/
software/sigf.shtml
cause the seeds are directly based on the develop-
ment data, resulting in a domain cluster that more
closely resembles the evaluation corpus. Preci-
sion, on the other hand, is slightly below that of the
manual seed classifier. This might be an effect of
the coarser granularity that a single domain score
offers, as opposed to eight subdomain scores.
5 Multi-Feature Classification
Using term relevance as the only factor for
metaphor detection is probably insufficient.
Rather, we anticipate to use it either as a pre-
filtering step or as a feature for a more complex
metaphor detection system. To simulate the latter,
we use an off-the-shelf machine learning classifier
with which we test how term relevance interacts
with other typical word features, such as part of
speech. As we classify all words of a sentence, we
treat the task as a binary sequential labeling task.
Preliminary tests were performed with HMM,
CRF and SVM classifiers. CRF performance was
the most promising. We use CRFsuite (Okazaki,
2007)
11
, an implementation of conditional random
fields that supports continuous values via scaling
factors. Training is performed on the development
set using ten-fold cross validation.
We present results for bigram models. Larger n-
grams were inspected, too, including models with
look-ahead functionality. While they were slightly
more robust with regard to parameter changes,
there was no improvement over the best bigram
model. Also, as metaphor processing still is a low
resource task for which sufficient training data is
hard to come by, bigrams are the most accessible
and representative option.
5.1 Training Features
We experimented with different representations
for the term relevance features. As they are con-
tinuous values, they could be used as continuous
features. Alternatively, they could be represented
as binary features, using a cut-off value as for our
threshold classifier. In the end, we chose a hy-
brid approach where thresholds are used to create
binary features, but are also scaled according to
their score. Thresholds were again determined on
the dev set and set to ? = 0.02 and ? = 0.79.
Each domain receives an individual domain rel-
evance feature. There is only a single common rel-
11
http://www.chokkan.org/software/
crfsuite/
22
F1
Prec Rec
All Metaphor 0.249 0.142 1.000
T-hold: Manual Seeds 0.350 0.276 0.478
CRF: Basic 0.187 0.706 0.108
CRF: Rel 0.219 0.683 0.130
CRF: PosLex 0.340 0.654 0.230
CRF: PosLexRel 0.373 0.640 0.263
Table 4: Summary of best performing settings for
each CRF model. Bold numbers indicate best per-
formance; slanted bold numbers: best CRF re-
call. All results are significantly different from the
baseline with p < 0.01.
evance feature, as it is domain-independent. Sur-
prisingly, we found no noteworthy difference in
performance between the two seed sets (manual
and 50-best). Therefore we only report results for
the manual seeds.
In addition to term relevance, we also provide
part of speech (pos) and lexicographer sense (lex)
as generic features. The part of speech is auto-
matically generated using NLTK?s Maximum En-
tropy POS Tagger, which was trained on the Penn
Treebank. To have a semantic feature to compare
our relevance weights to, we include WordNet?s
lexicographer senses (Fellbaum, 1998), which are
coarse-grained semantic classes. Where a word
has more than one sense, the first was chosen. If no
sense exists for a word, the word is given a sense
unknown placeholder value.
5.2 Performance Evaluation
Performance of the CRF system (see table 4)
seems slightly disappointing at first when com-
pared to our threshold classifier. The best-
performing CRF beats the threshold classifier by
only two points of F-score, despite considerably
richer training input. Precision and recall perfor-
mance are reversed, i.e. the CRF provides a higher
precision of 0.6, but only detects one out of four
metaphor words. All models provide stable results
for all folds, their standard deviation (about 0.01
for F
1
) being almost equal to that of the baseline.
All results are significantly different from the
baseline as well as from each other with p < 0.01,
except for the precision scores of the three non-
basic CRF models, which are significantly differ-
ent from each other with p < 0.05.
 0 0.05 0.1 0.15 0.2
 0.25 0.3 0.35 0.4
200 400 600 800 1000 1200 1400 1600 1800 0 0.05 0.1 0.15 0.2
 0.25 0.3 0.35 0.4F 1 Number of Training Sentences ModelsCRF BasicCRF PosLexThreshold + Relevance+ RelevanceBaseline
Figure 1: Performance curves for various training data sizes. Models with term relevance features (solid
lines) outperform models without term relevance (dashed lines) at 1400 sentences. 1800 sentences rep-
resent the entire training set. Baseline (thin line) and best threshold classifier (dotted line) provided for
reference.
23
Adding term relevance provides a consistent
boost of 0.025 to the F-score. This boost, however,
is rather marginal in comparison to the one pro-
vided by part of speech and lexicographer sense.
A possible reason for this could be that the item
weights learned during training correspond too
closely to our term relevance scores, thus making
them obsolete when enough training data is pro-
vided. The next section explores this possibility
by comparing different amounts of training data.
5.3 Training Size Evaluation
With 2000 metaphoric sentences, the dataset we
used was already among the largest annotated cor-
pora. By reducing the amount of training data we
evaluate whether term relevance is an efficient fea-
ture when data is sparse. To this end, we repeat
our ten-fold cross validations, but withhold some
of the folds from each training set.
Figure 1 compares the performance of CRF fea-
ture configurations with and without term rele-
vance. In both cases adding term relevance out-
performs the standard configuration?s top perfor-
mance with 400 sentences less, saving about a
quarter of the training data.
In figure 2 we also visualize the relative gain
that adding term relevance provides. As one can
see, small datasets profit considerably more from
our metric. Given only 200 sentences, the PosLex
model receives 4.7 times the performance gain
from term relevance it got at at maximum training
size. The basic model has a factor of 6.8. This sup-
ports our assumption that term relevance is similar
to the item weights learned during CRF training.
As labeled training data is considerably more ex-
pensive to create than corpora for term relevance,
this is an encouraging observation.
6 Related Work
For a comprehensive review on computational
metaphor detection, see Shutova (2010). We limit
our discussion to publications that were not cov-
ered by the review. While there are several papers
evaluating on the same domain, direct comparison
proved to be difficult, as many works were either
evaluated on a sentence level (which our data was
inappropriate for, as 80% of sentences contained
metaphors) or did not provide coverage informa-
tion. Another difference was that most evaluations
were performed on balanced datasets, while our
own data was naturally skewed for literal terms.
Strzalkowski et al. (2013) follow a related hy-
pothesis, assuming that metaphors lack topical re-
latedness to in-domain words while being syntac-
tically connected to them. Instead of using the
metaphor candidate?s relevance to a target domain
corpus to judge relatedness, they circumvent the
0 %25 %50 %75 %
100 %125 %150 %
200 400 600 800 1000 1200 1400 1600 18000 %25 %50 %75 %
100 %125 %150 %Relative Gain Number of Training Sentences
ModelsCRF BasicCRF PosLex
Figure 2: Relative performance gain of models obtained from addition of term relevance features.
24
need for pre-existing source data by generating
ad-hoc collocation clusters and check whether the
two highest ranked source clusters share vocab-
ulary with the target domain. Further factors in
their decision process are co-ocurrences in sur-
rounding sentences and psycholinguistic image-
ability scores (i.e. how easy it is to form a men-
tal picture of a word). Evaluating on data in the
governance domain, they achieve an accuracy of
71% against an all metaphor baseline of 46%, but
report no precision or recall.
Mohler et al. (2013) and Heintz et al. (2013)
also evaluate on the governance domain. Rather
than detecting metaphors at a word-level, both de-
tect whether sentences contain metaphors. Mohler
et al. (2013) compare semantic signatures of sen-
tences to signatures of known metaphors. They,
too, face a strong bias against the metaphor label
and show how this can influence the balance be-
tween precision and recall. Heintz et al. (2013)
classify sentences as containing metaphors if their
content is related to both a target and source do-
main. They create clusters via topic modeling and,
like us, use manually chosen seed terms to asso-
ciate them with domains. Unlike our approach,
theirs also requires seeds of all relevant source do-
mains. They observe that identifying metaphors,
even on a sentence level, is difficult even for ex-
perienced annotators, as evidenced by an inter-
annotator agreement of ? = 0.48.
Shutova et al. (2010) use manually annotated
seed sentences to generate source and target do-
main vocabularies via spectral clustering. The re-
sulting domain clusters are used for selectional
preference induction in verb-noun relations. They
report a high precision of 0.79, but have no data on
recall. Target concepts appearing in similar lexico-
syntactic contexts are mapped to the same source
concepts. The resulting mappings are then used to
detect metaphors. This approach is notable for its
combination of distributional clustering and selec-
tional preference induction. Verbs and nouns are
clustered into topics and linked through induction
of selectional preferences, from which metaphoric
mappings are deduced. Other works (S?eaghdha,
2010; Ritter et al., 2010) use topic modeling to di-
rectly induce selectional preferences, but have not
yet been applied to metaphor detection.
Hovy et al. (2013) generalize semantic prefer-
ence violations from verb-noun relations to any
syntactic relation and learn these in a supervised
manner, using SVM and CRF models. The CRF
is not the overall best-performing system, but
achieves the highest precision of 0.74 against an
all-metaphor baseline of 0.49. This is in line
with our own observations. While they argue
that metaphor detection should eventually be per-
formed on every word, their evaluation is limited
to a single expression per sentence.
Our work is also related to that of Sporleder and
Li (2009) and Li and Sporleder (2010), in which
they detect idioms through their lack of seman-
tic cohesiveness with their context. Cohesiveness
is measured via co-occurence of idiom candidates
with other parts of a text in web searches. They
do not make use of domains, basing their measure
entirely on the lexical context instead.
7 Conclusion
We have presented term relevance as a non-
literalness indicator and its use for metaphor de-
tection. We showed that even on its own, term rel-
evance clearly outperforms the baseline by 58%
when detecting metaphors on a word basis.
We also evaluated the utility of term relevance
as a feature in a larger system. Results for this
were mixed, as the general performance of our
system, a sequential CRF classifier, was lower
than anticipated. However, tests on smaller train-
ing sets suggest that term relevance can help when
data is sparse (as it often is for metaphor process-
ing). Also, precision was considerably higher for
CRF, so it might be more useful for cases where
coverage is of secondary importance.
For future work we plan to reimplement the
underlying idea of term relevance with different
means. Domain datasets could be generated via
topic modeling or other clustering means (Shutova
et al., 2010; Heintz et al., 2013) and should also
cover dynamically detected secondary target do-
mains. Instead of using TF-IDF, term relevance
can be modeled using semantic vector spaces
(see Hovy et al. (2013)). While our preliminary
tests showed better performance for CRF than
for SVM, such a change in feature representation
would also justify a re-evaluation of our classifier
choice. To avoid false positives (and thus improve
precision), we could generate ad-hoc source do-
mains, like Strzalkowski et al. (2013) or Shutova
et al. (2010) do, to detect overlooked literal con-
nections between source and target domain.
25
Acknowledgements
We would like to thank the reviewers and proof-
readers for their valuable input.
This research effort was in part supported by
the German Academic Exchange Service (DAAD)
scholarship program PROMOS with funds from
the Federal Ministry of Education and Research
(BMBF), awarded by the International Office of
Saarland University as well as by the Intelligence
Advanced Research Projects Activity (IARPA)
via Department of Defense US Army Research
Laboratory contract number W911NF-12-C-0025.
The U.S. Government is authorized to reproduce
and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions con-
tained herein are those of the authors and should
not be interpreted as necessarily representing the
official policies or endorsements, either expressed
or implied, of DAAD, BMBF, IARPA, DoD/ARL
or the German or U.S. Government.
References
Gordon V. Cormack, Mark D. Smucker, and Charles
L. A. Clarke. 2011. Efficient and effective spam
filtering and re-ranking for large web datasets. In-
formation retrieval, 14(5):441?465.
Christiane Fellbaum. 1998. ed. WordNet: an elec-
tronic lexical database. MIT Press, Cambridge MA,
1:998.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of the HLT/NAACL-06 Workshop on Scal-
able Natural Language Understanding, pages 41?
48.
Ilana Heintz, Ryan Gabbard, Mahesh Srinivasan, David
Barner, Donald S Black, Marjorie Freedman, and
Ralph Weischedel. 2013. Automatic extraction of
linguistic metaphor with lda topic modeling. Pro-
ceedings of the ACL-13 Workshop on Metaphor,
page 58.
Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,
Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-
ney Sanders, and Eduard Hovy. 2013. Identifying
metaphorical word use with tree kernels. Proceed-
ings of the ACL-13 Workshop on Metaphor, page 52.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the HLT/NAACL-07 Workshop on
Computational Approaches to Figurative Language,
pages 13?20. Association for Computational Lin-
guistics.
George Lakoff and Mark Johnson. 1980. Metaphors
we live by, volume 111. University of Chicago
Press.
Linlin Li and Caroline Sporleder. 2010. Using gaus-
sian mixture models to detect figurative language in
context. In Proceedings of NAACL-10, pages 297?
300. Association for Computational Linguistics.
Edward Loper and Steven Bird. 2002. NLTK: The
natural language toolkit. In Proceedings of the
COLING/ACL-02 workshop on Interactive presen-
tation sessions, pages 63?70. Association for Com-
putational Linguistics.
Michael Mohler, David Bracewell, David Hinote, and
Marc Tomlinson. 2013. Semantic signatures for
example-based linguistic metaphor detection. Pro-
ceedings of the ACL-13 Workshop on Metaphor,
page 27.
Naoaki Okazaki, 2007. CRFsuite: a fast implementa-
tion of conditional random fields (CRFs).
Sebastian Pad?o, 2006. User?s guide to sigf: Signifi-
cance testing by approximate randomisation.
Alan Ritter, Oren Etzioni, et al. 2010. A latent dirich-
let allocation method for selectional preferences. In
ACT-10, pages 424?434. Association for Computa-
tional Linguistics.
Diarmuid
?
O S?eaghdha. 2010. Latent variable models
of selectional preference. In Proceedings of ACL-
10, pages 435?444. Association for Computational
Linguistics.
Ekaterina Shutova, Lin Sun, and Anna Korhonen.
2010. Metaphor identification using verb and noun
clustering. In Proceedings of COLING-10, pages
1002?1010. Association for Computational Linguis-
tics.
Ekaterina Shutova. 2010. Models of metaphor in NLP.
In Proceedings of ACL-10, pages 688?697. Associ-
ation for Computational Linguistics.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of EACL09, pages 754?
762. Association for Computational Linguistics.
Tomek Strzalkowski, George Aaron Broadwell, Sarah
Taylor, Laurie Feldman, Boris Yamrom, Samira
Shaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,
et al. 2013. Robust extraction of metaphors from
novel data. Proceedings of the ACL-13 Workshop
on Metaphor, page 67.
Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-
man. 2013. Cross-lingual metaphor detection us-
ing common semantic features. Proceedings of the
ACL-13 Workshop on Metaphor, page 45.
Yorick Wilks. 1978. Making preferences more active.
Artificial Intelligence, 11(3):197?223.
26
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 68?76,
Baltimore, Maryland, USA, June 22-27, 2014.
c
?2014 Association for Computational Linguistics
Evaluation for Partial Event Coreference
Jun Araki Eduard Hovy Teruko Mitamura
Language Technologies Institute
Carnegie Mellon University
Pittsburgh, PA 15213, USA
junaraki@cs.cmu.edu, hovy@cmu.edu, teruko@cs.cmu.edu
Abstract
This paper proposes an evaluation scheme
to measure the performance of a system
that detects hierarchical event structure for
event coreference resolution. We show
that each system output is represented as
a forest of unordered trees, and introduce
the notion of conceptual event hierarchy to
simplify the evaluation process. We enu-
merate the desiderata for a similarity met-
ric to measure the system performance.
We examine three metrics along with the
desiderata, and show that metrics extended
from MUC and BLANC are more ade-
quate than a metric based on Simple Tree
Matching.
1 Introduction
Event coreference resolution is the task to de-
termine whether two event mentions refer to the
same event. This task is important since resolved
event coreference is useful in various tasks such as
topic detection and tracking, information extrac-
tion, question answering, textual entailment, and
contradiction detection.
A key challenge for event coreference resolu-
tion is that one can define several relations be-
tween two events, where some of them exhibit
subtle deviation from perfect event identity. For
clarification, we refer to perfect event identity
as full (event) coreference in this paper. To ad-
dress the subtlety in event identity, Hovy et al.
(2013) focused on two types of partial event iden-
tity: subevent and membership. Subevent relations
form a stereotypical sequence of events, or a script
(Schank and Abelson, 1977; Chambers and Juraf-
sky, 2008). Membership relations represent in-
stances of an event collection. We refer to both
as partial (event) coreference in this paper. Fig-
ure 1 shows some examples of the subevent and
membership relations in the illustrative text be-
low, taken from the Intelligence Community do-
main of violent events. Unlike full coreference,
partial coreference is a directed relation, and forms
hierarchical event structure, as shown in Figure 1.
Detecting partial coreference itself is an important
task because the resulting event structures are ben-
eficial to text comprehension. In addition, such
structures are also useful as background knowl-
edge information to resolve event coreference.
A car bomb that police said was set by Shining Path
guerrillas ripped off(E4) the front of a Lima police
station before dawn Thursday, wounding(E5) 25 peo-
ple. The attack(E6) marked the return to the spotlight
of the feared Maoist group, recently overshadowed by
a smaller rival band of rebels. The pre-dawn bomb-
ing(E7) destroyed(E8) part of the police station and
a municipal office in Lima?s industrial suburb of Ate-
Vitarte, wounding(E9) 8 police officers, one seriously,
Interior Minister Cesar Saucedo told reporters. The
bomb collapsed(E11) the roof of a neighboring hospi-
tal, injuring(E12) 15, and blew out(E13) windows and
doors in a public market, wounding(E14) two guards.
Figure 1: Examples of subevent and member-
ship relations. Solid and dashed arrows represent
subevent and membership relations respectively,
with the direction from a parent to its subevent
or member. For example, we say that E4 is a
subevent of E6. Solid lines without any arrow
heads represent full coreference.
In this paper, we address the problem of evalu-
68
ating the performance of a system that detects par-
tial coreference in the context of event coreference
resolution. This problem is important because, as
with other tasks, a good evaluation method for par-
tial coreference will facilitate future research on
the task in a consistent and comparable manner.
When one introduces a certain evaluation metric
to such a new complex task as partial event coref-
erence, it is often unclear what metric is suitable
to what evaluation scheme for the task under what
assumptions. It is also obscure how effectively and
readily existing algorithms or tools, if any, can be
used in a practical setting of the evaluation. In or-
der to resolve these sub-problems for partial coref-
erence evaluation, we need to formulate an evalu-
ation scheme that defines assumptions to be made
regarding the evaluation, specifies some desider-
ata that an ideal metric should satisfy for the task,
and examines how adequately particular metrics
can satisfy them. For this purpose, we specifi-
cally investigate three existing algorithms MUC,
BLANC, and Simple Tree Matching (STM).
The contributions of this work are as follows:
? We introduce a conceptual tree hierarchy that
simplifies the evaluation process for partial
event coreference.
? We present a way to extend MUC, BLANC,
and STM for the case of unordered trees.
Those metrics are generic and flexible
enough to be used in evaluations involving
data structures based on unordered trees.
? Our experimental results indicate that the ex-
tended MUC and BLANC are better than
Simple Tree Matching for evaluating partial
coreference.
2 Related Work
Recent studies on both entity and event coref-
erence resolution use several metrics to evaluate
system performance (Bejan and Harabagiu, 2010;
Lee et al., 2012; Durrett et al., 2013; Lassalle and
Denis, 2013) since there is no agreement on a sin-
gle metric. Currently, five metrics are widely used:
MUC (Vilain et al., 1995), B-CUBED (Bagga and
Baldwin, 1998), two CEAF metrics CEAF-?
3
and
CEAF-?
4
(Luo, 2005), and BLANC (Recasens
and Hovy, 2011). We can divide these metrics
into two groups: cluster-based metrics, e.g., B-
CUBED and CEAF, and link-based metrics, e.g.,
MUC and BLANC. The former group is not ap-
plicable to evaluate partial coreference because it
is unclear how to define a cluster. The latter is
not readily applicable to the evaluation because it
is unclear how to penalize incorrect directions of
links. We discuss these aspects in Section 4.1 and
Section 4.2.
Tree Edit Distance (TED) is one of the tradi-
tional algorithms for measuring tree similarity. It
has a long history of theoretical studies (Tai, 1979;
Zhang and Shasha, 1989; Klein, 1998; Bille, 2005;
Demaine et al., 2009; Pawlik and Augsten, 2011).
It is also widely studied in many applications, in-
cluding Natural Language Processing (NLP) tasks
(Mehdad, 2009; Wang and Manning, 2010; Heil-
man and Smith, 2010; Yao et al., 2013). However,
TED has a disadvantage: we need to predefine ap-
propriate costs for basic tree-edit operations. In
addition, an implementation of TED for unordered
trees is fairly complex.
Another tree similarity metric is Simple Tree
Matching (STM) (Yang, 1991). STM measures
the similarity of two trees by counting the max-
imum match with dynamic programming. Al-
though this algorithm was also originally devel-
oped for ordered trees, the underlying idea of the
algorithm is simple, making it relatively easy to
extend the algorithm for unordered trees.
Tree kernels have been also widely studied and
applied to NLP tasks, more specifically, to cap-
ture the similarity between parse trees (Collins and
Duffy, 2001; Moschitti et al., 2008) or between
dependency trees (Croce et al., 2011; Srivastava
et al., 2013). This method is based on a super-
vised learning model with training data; hence we
need a number of pairs of trees and associated nu-
meric similarity values between these trees as in-
put. Thus, it is not appropriate for an evaluation
setting.
3 Evaluation Scheme
When one formulates an evaluation scheme for a
new task, it is important to define assumptions for
the evaluation and desiderata that an ideal metric
should satisfy. In this section, we first describe as-
sumptions for partial coreference evaluation, and
introduce the notion of conceptual event hierarchy
to address the challenge posed by one of the as-
sumptions. We then enumerate the desiderata for
a metric.
69
3.1 Assumptions on Partial Coreference
We make the following three assumptions to eval-
uate partial coreference.
Twinless mentions: Twinless mentions (Stoyanov
et al., 2009) are the mentions that exist in the gold
standard but do not in a system response, or vice
versa. In reality, twinless mentions often happen
since an end-to-end system might produce them in
the process of detecting mentions. The assump-
tion regarding twinless mentions has been inves-
tigated in research on entity coreference resolu-
tion. Cluster-based metrics such as B-CUBED and
CEAF assume that a system is given true men-
tions without any twinless mentions in the gold
standard, and then resolves full coreference on
them. Researchers have made different assump-
tions about this issue. Early work such as (Ji et
al., 2005) and (Bengtson and Roth, 2008) simply
ignored such mentions. Rahman and Ng (2009)
removed twinless mentions that are singletons in a
system response. Cai and Strube (2010) proposed
two variants of B-CUBED and CEAF that can deal
with twinless mentions in order to make the evalu-
ation of end-to-end coreference resolution system
consistent.
In evaluation of partial coreference where twin-
less mentions can also exist, we believe that the
value of making evaluation consistent and compa-
rable is the most important, and hypothesize that
it is possible to effectively create a metric to mea-
sure the performance of partial coreference while
dealing with twinless mentions. A potential prob-
lem of making a single metric handle twinless
mentions is that the metric would not be informa-
tive enough to show whether a system is good at
identifying coreference links but poor at identify-
ing mentions, or vice versa (Recasens and Hovy,
2011). However, our intuition is that the prob-
lem is avoidable by showing the performance of
mention identification with metrics such as pre-
cision, recall, and the F-measure simultaneously
with the performance of link identification. In this
work, therefore, we assume that a metric for par-
tial coreference should be able to handle twinless
mentions.
Intransitivity: As described earlier, partial coref-
erence is a directed relation. We assume that par-
tial coreference is not transitive. To illustrate the
intransitivity, let e
i
s
?? e
j
denote a subevent rela-
tion that e
j
is a subevent of e
i
. In Figure 1, we
have E7
s
?? E8 and E8
s
?? E9. In this case,
E9 is not a subevent of E7 due to the intransi-
tivity of subevent relations. One could argue that
the event ?wounding(E9)? is one of stereotypical
events triggered by the event ?bombing(E7)?, and
thus E7
s
?? E9. However, if we allow transitiv-
ity of partial coreference, then we have to measure
all implicit partial coreference links (e.g., the one
between E7 and E9) from hierarchical event struc-
tures. Consequently, this evaluation policy could
result in an unfair scoring scheme biased toward
large event hierarchy.
Link propagation: We assume that partial coref-
erence links can be propagated due to a combi-
nation of full coreference links with them. To il-
lustrate the phenomenon, let e
i
? e
j
denote full
coreference between e
i
and e
j
. In Figure 1, we
have E6 ? E7 and E7
s
?? E8. In this case, E8
is also a subevent of E6, i.e., E6
s
?? E8. The
rationale behind this assumption is that if a sys-
tem identifies E6
s
?? E8 instead of E7
s
?? E8,
then there is no reason to argue that the identified
subevent relation is incorrect given that E6? E7
and E7
s
?? E8. The discussion here also applies
to membership relations.
3.2 Conceptual Event Hierarchy
The assumption of link propagation poses a chal-
lenge in measuring the performance of partial
coreference. We illustrate the challenge with the
example in the discussion on link propagation
above. We focus only on subevent relations to de-
scribe our idea, but one can apply the same dis-
cussion to membership relations. Suppose that a
system detects a subevent link E7
s
?? E8, but not
E6
s
?? E8. Then, is it reasonable to give the
system a double reward for two links E7
s
?? E8
and E6
s
?? E8 due to link propagation, or should
one require a system to perform such link propa-
gation and detect E7
s
?? E8 as well for the system
to achieve the double reward? In the evaluation
scheme based on event trees whose nodes repre-
sent event mentions, we need to predefine how to
deal with link propagation of full and partial coref-
erence in evaluation. In particular, we must pay at-
tention to the potential risk of overcounting partial
corefrence links due to link propagation.
To address the complexity of link propagation,
we introduce a conceptual event tree where each
node represents a conceptual event rather than an
event mention. Figure 2 shows an example of
a conceptual subevent tree constructed from full
70
coreference and subevent relations in Figure 1.
Using set notation, each node of the tree represents
an abstract event. For instance, node {E6, E7}
represents an ?attacking? event which both event
mentions E6 and E7 refer to.
Figure 2: A conceptual subevent tree constructed
from the full coreference and subevent relations in
Figure 1.
The notion of a conceptual event tree obviates
the need to cope with link propagation, thereby
simplifying the evaluation for partial coreference.
Given a conceptual event tree, an evaluation met-
ric is basically just required to measure how many
links in the tree a system successfully detects.
When comparing two conceptual event trees, a
link in a tree is identical to one in the other tree
if there is at least one event mention shared in par-
ent nodes of those links and at least one shared
in child nodes of those links. For example, sup-
pose that system A identifies E6
s
?? E8, system
B E7
s
?? E8, system C both, and all the systems
identify E6 ? E7 in Figure 1. In this case, they
gain the same score since the subevent links that
they identify correspond to one correct subevent
link {E6, E7}
s
?? {E8} in Figure 2. It is pos-
sible to construct the conceptual event hierarchy
for membership relations in the same way as de-
scribed above. This means that the conceptual
event hierarchy allows us to show the performance
of a system on each type of partial coreference
separately, which leads to more informative evalu-
ation output.
One additional note is that the conceptual event
tree representing partial coreference is an un-
ordered tree, as illustrated in Figure 2. Although
we could represent a subevent tree with an or-
dered tree because of the stereotypical sequence of
subevents given by definition, partial coreference
is in general represented with a forest of unordered
trees
1
.
1
For example, it is impossible to intuitively define a se-
3.3 Desiderata for Metrics
In general, a system output of partial event coref-
erence in a document is represented not by a sin-
gle tree but by a forest, i.e., a set of disjoint trees
whose nodes are event mentions that appear in the
document. Let T be a tree, and let F be a forest
F = {T
i
}. Let sim(F
g
, F
r
) ? [0, 1] denote a sim-
ilarity score between the gold standard forest F
g
and a system response forest F
r
. We define the
following properties that an ideal evaluation met-
ric for partial event coreference should satisfy.
P1. Identity: sim(F
1
, F
1
) = 1.
P2. Symmetricity: sim(F
1
, F
2
) = sim(F
2
, F
1
).
P3. Zero: sim(F
1
, F
2
) = 0 if F
1
and F
2
are to-
tally different forests.
P4. Monotonicity: The metric score should in-
crease from 0 to 1 monotonically as two to-
tally different forests approach the identical
one.
P5. Linearity: The metric score should increase
linearly as each single individual correct
piece of information is added to a system re-
sponse.
The first three properties are relatively intuitive.
P4 is important because otherwise a higher score
by the metric does not necessarily mean higher
quality of partial event coreference output. In P5, a
correct piece of information is the addition of one
correct link or the deletion of one incorrect link.
This property is useful for tracking performance
progress over a certain period of time. If the met-
ric score increases nonlinearly, then it is difficult to
compare performance progress such as a 0.1 gain
last year and a 0.1 gain this year, for example.
In addition, one can think of another property
with respect to structural consistency. The moti-
vation for the property is that one might want to
give more reward to partial coreference links that
form hierarchical structures, since they implicitly
form sibling relations among child nodes. For in-
stance, suppose that system A detects two links
{E6, E7}
s
?? {E8} and {E6, E7}
s
?? {E11}, and
system B two links {E8}
s
?? {E9} and {E11}
s
??
{E12} in Figure 2. We can think that system A
performs better since the system successfully de-
tects an implicit subevent sibling relation between
{E8} and {E11} as well. Due to space limita-
tions, however, we do not explore the property in
this work, and leave it for future work.
quence of child nodes in a membership event tree in Figure 1.
71
4 Evaluation Metrics
In this section, we examine three evaluation met-
rics based on MUC, BLANC, and STM respec-
tively under the evaluation scheme described in
Section 3.
4.1 B-CUBED and CEAF
B-CUBED regards a coreference chain as a set of
mentions, and examines the presence and absence
of mentions in a system response that are relative
to each of their corresponding mentions in the gold
standard (Bagga and Baldwin, 1998). Let us call
such set a mention cluster. A problem in applying
B-CUBED to partial coreference is that it is diffi-
cult to properly form a mention cluster for partial
coreference. In Figure 2, for example, we could
form a gold standard cluster containing all nodes
in the tree. We could then form a system response
cluster, given a certain system output. The prob-
lem is that B-CUBED?s way of counting mentions
overlapped in those clusters cannot capture parent-
child relations between the mentions in a cluster.
It is also difficult to extend the counting algorithm
to incorporate such relations in an intuitive man-
ner. Therefore, we observe that B-CUBED is not
appropriate for evaluating partial coreference.
We see the basically same reason for the inade-
quacy of CEAF. It also regards a coreference chain
as a set of mentions, and measures how many men-
tions two clusters share using two similarity met-
rics ?
3
(R,S) = |R ? S| and ?
4
(R,S) =
2|R?S|
|R|+|S|
,
given two clustersR and S. One can extend CEAF
for partial coreference by selecting the most ap-
propriate tree similarity algorithm for ? that works
well with the algorithm to compute maximum bi-
partite matching in CEAF. However, that is an-
other line of work, and due to space limitations
we leave it for future work.
4.2 Extension to MUC and BLANC
MUC relies on the minimum number of links
needed when mapping a system response to the
gold standard (Vilain et al., 1995). Given a set of
key entitiesK and a set of response entitiesR, pre-
cision of MUC is defined as the number of com-
mon links between entities in K and R divided by
the number of links in R, whereas recall of MUC
is defined as the number of common links between
entities inK andR divided by the number of links
inK. After finding a set of mention clusters by re-
solving full coreference, we can compute the num-
ber of correct links by counting all links spanning
in those mention clusters that matched the gold
standard. It is possible to apply the idea of MUC
to the case of partial coreference simply by chang-
ing the definition of a correct link. In the partial
coreference case, we define a correct link as a link
matched with the gold standard including its di-
rection. Let MUC
p
denote such extension to MUC
for partial coreference.
Similarly, it is also possible to define an ex-
tension to BLANC. Let BLANC
p
denote the ex-
tension. BLANC computes precision, recall,
and F1 scores for both coreference and non-
coreference links, and average them for the final
score (Recasens and Hovy, 2011). As with MUC
p
,
BLANC
p
defines a correct link as a link matched
with the gold standard including its direction. An-
other difference between BLANC and BLANC
p
is
the total number of mention pairs, denoted asL. In
the original BLANC, L = N(N ? 1)/2 where N
is the total number of mentions in a document. We
use L
p
= N(N ? 1) instead for BLANC
p
since
we consider two directed links in partial corefer-
ence with respect to each undirected link in full
coreference.
4.3 Extension to Simple Tree Matching
The underlying idea of STM is that if two trees
have more node-matching, then they are more sim-
ilar. The original STM uses a dynamic program-
ming approach to perform recursive node-level
matching in a top-down fashion. In the case of
partial coreference, we cannot readily use the ap-
proach because partial coreference is represented
with unordered trees, and thus time complexity of
node-matching is the exponential order with re-
spect to the number of child nodes. However, par-
tial event coreference is normally given in a small
hierarchy with three levels or less. Taking ad-
vantage of this fact and assuming that each event
mention is uniquely identified in a tree, we ex-
tend STM for the case of unordered trees by using
greedy search. Algorithm 1 shows an extension to
the STM algorithm for unordered trees.
We can also naturally extend STM to take
forests as input. Figure 3 shows how one can con-
vert a forest into a single tree whose subtrees are
the trees in the forest by introducing an additional
dummy root node on top of those tree. The result-
ing tree is also an unordered tree, and thus we can
apply Algorithm 1 to that tree to measure the sim-
72
Algorithm 1 Extended simple tree matching for
unordered trees.
Input: two unordered trees A and B
Output: score
1: procedure SimpleTreeMatching(A, B)
2: if the roots of A and B have different elements then
3: return 0
4: else
5: s := 1 . The initial score for the root match.
6: m := the number of first-level sub-trees of A
7: n := the number of first-level sub-trees of B
8: for i = 1? m do
9: for j = 1? n do
10: if A
i
and B
j
have the same element then
11: s = s + SimpleTreeMatching(A
i
, B
j
)
Figure 3: Conversion from a forest to a single tree
with an additional dummy root.
ilarity of two forests comprising unordered trees.
Let STM
p
denote the extended STM. Finally, we
normalize STM
p
. Let NSTM
p
be a normalized
version of STM
p
as follows: NSTM
p
(F
1
, F
2
) =
STM
p
(F
1
, F
2
)/max(|F
1
|, |F
2
|) where |F | de-
notes the number of nodes in F .
4.4 Flexibility of Metrics
Making assumptions on evaluation for a particular
task and defining desiderata for a metric determine
what evaluation scheme we are going to formulate.
However, this kind of effort tends to make result-
ing evaluation metrics too restrictive to be reusable
in other tasks. Such metrics might be adequate
for that task, but we also value the flexibility of
a metric that can be directly used or be easily ex-
tended to other tasks. To investigate the flexibil-
ity of MUC
p
, BLANC
p
and STM
p
, we will exam-
ine these metrics without making the assumptions
of twinless mentions and intransitivity of partial
coreference against each metric. We consider that
the assumption of link propagation is more funda-
mental and regard it as a basic premise, and thus
we will continue to make that assumption.
MUC was originally designed to deal with re-
sponse links spanning mentions that even key links
do not reach. Thus, it is able to handle twinless
mentions. If we do not assume intransitivity of
partial coreference, we do not see any difficulty in
changing the definition of correct links in MUC
p
and making it capture transitive relations. There-
fore, MUC
p
does not require both assumptions of
twinless mentions and intransitivity.
In contrast, BLANC was originally designed to
handle true mentions in the gold standard. Since
BLANC
p
does not make any modifications on this
aspect, it cannot deal with twinless mentions ei-
ther. As for intransitivity, it is possible to easily
change the definition of correct and incorrect links
in BLANC
p
to detect transitive relations. Thus,
BLANC
p
does not require intransitivity but does
require the assumption of no twinless mentions.
Since STM
p
simply matches elements in nodes
as shown in Algorithm 1, it does not require the as-
sumption of twinless mentions. With respect to in-
transitivity, we can extend STM
p
by adding extra
edges from a parent to grandchild nodes or others
and applying Algorithm 1 to the modified trees.
Hence, it does not require the assumption of in-
transitivity.
5 Experiments
To empirically examine the three metrics de-
scribed in Section 4.2 and Section 4.3, we con-
ducted an experiment using the artificial data
shown in Table 1. Since BLANC
p
cannot han-
dle twinless mentions, we removed twinless men-
tions. We first created the gold standard shown in
the first row of the table. It contains fifty events,
twenty one singleton events, and seven event trees
with three levels or less. We believe this distri-
bution of partial coreference is representative of
that of real data. We then created several system
responses that are ordered toward two extremes.
One extreme is all singletons in which they do not
have correct links. The other is a single big tree
that merges all event trees including singletons in
the gold standard.
Figure 4 shows how the three metrics behave
in two cases: (a) we increase the number of cor-
rect links from all singletons to the perfect output
(equal to the gold standard), and (b) we increase
the incorrect links from the perfect output to a sin-
gle tree merging all trees in the gold standard. In
the former case, we started with System 3 in Ta-
ble 1. Next we added one correct link 28
s
?? 29
shown in System 2. This way, we added cor-
rect links up to the perfect output one by one in
a bottom-up fashion. In the latter case, we started
73
Response
Output
Gold standard
(1(2(6))(3(7))(4)(5)) (8(9(11)(12))(10)) (13(14)(15)(16)(17)(18)) (19(20(21))(22)) (23(24)(25))
(26(27)) (28(29)) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46)
(47) (48) (49) (50)
System 1
(1(4)(5)(2(6))(3(7))) (8(9(11)(12))(10)) (13(18)(14)(15)(16)(17)) (19(22)(20(21))) (23(24)(25))
(26(27)) (28(29)) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45) (46)
(47) (48) (49(50))
System 2
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24)
(25) (26) (27) (28(29)) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45)
(46) (47) (48) (49) (50)
System 3
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15) (16) (17) (18) (19) (20) (21) (22) (23) (24)
(25) (26) (27) (28) (29) (30) (31) (32) (33) (34) (35) (36) (37) (38) (39) (40) (41) (42) (43) (44) (45)
(46) (47) (48) (49) (50)
Table 1: Examples of a system response against a gold standard partial coreference. Each event tree is
shown in the bold font and in the Newick standard format with parentheses.
with the perfect output, and then added one incor-
rect link 49
s
?? 50 shown in System 1. In a manner
similar to case (a), we added incorrect links up to
the merged tree one by one in a bottom-up fashion.
The results indicate that MUC
p
and BLANC
p
meet the desiderata defined in Section 3.3 more
adequately than NSTM
p
. The curve of MUC
p
and
BLANC
p
in Figure 4 are close to the linearity,
which is practically useful as a metric. In contrast,
NSTM
p
fails to meet P4 and P5 in case (a), and
fails to meet P5 in case (b). This is because STM
first checks whether root nodes of two trees have
the same element, and if the root nodes have dif-
ferent elements, STM stops searching the rest of
nodes in the trees.
6 Discussion
In Section 4.4, we observed that MUC
p
and STM
p
are more flexible than BLANC
p
because they can
measure the performance coreference in the case
of twinless mentions as well. The experimental re-
sults in Section 5 show that MUC
p
and BLANC
p
more adequate in terms of the five properties de-
fined in Section 3.3. Putting these together, MUC
p
seems the best metric for partial event coreference.
However, MUC has two disadvantages that (1) it
prefers systems that have more mentions per en-
tity (event), and (2) it ignores recall for singletons
(Pradhan et al., 2011). MUC
p
also has these disad-
vantages. Thus, BLANC
p
might be the best choice
for partial coreference if we could assume that a
system is given true mentions in the gold standard.
Although STM
p
fails to satisfy P4 and P5, it
has potential power to capture structural proper-
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Sc
ore
Ratio of correct links [%]
MUCpBLANCpNSTMp
(a) The number of correct links increases from singletons to
the perfect output (the gold standard) one by one.
 0
 20
 40
 60
 80
 100
 0  20  40  60  80  100
Sc
ore
Ratio of incorrect links [%]
MUCpBLANCpNSTMp
(b) The number of incorrect links increases from the perfect
output to a single tree merging all trees one by one.
Figure 4: Score comparison among MUC
p
,
BLANC
p
, and NSTM
p
.
74
ties of partial coreference described in Section 3.3.
This is because STM?s recursive fashion of node-
counting can be easily extend to counting the num-
ber of correct sibling relations.
7 Conclusion
We proposed an evaluation scheme for partial
event coreference with conceptual event hierar-
chy constructed from mention-based event trees.
We discussed possible assumptions that one can
make, and examined extensions to three existing
metrics. Our experimental results indicate that the
extensions to MUC and BLANC are more ade-
quate than the extension to STM. To our knowl-
edge, this is the first work to argue an evaluation
scheme for partial event coreference. Neverthe-
less, we believe that our scheme is generic and
flexible enough to be applicable to other directed
relations of events (e.g., causality and entailment)
or other related tasks to compare hierarchical data
based on unordered trees (e.g., ontology compari-
son). One future work is to improve the metrics
by incorporating structural consistency of event
trees as an additional property and implementing
the metrics from the perspective of broad contexts
beyond local evaluation by link-based counting.
8 Acknowledgements
This research was supported in part by DARPA
grant FA8750-12-2-0342 funded under the DEFT
program. Any opinions, findings, and conclusion
or recommendations expressed in this material are
those of the authors and do not necessarily reflect
the view of the DARPA or the US government. We
would like to thank anonymous reviewers for their
helpful comments.
References
Amit Bagga and Breck Baldwin. 1998. Algorithms
for Scoring Coreference Chains. In Proceedings of
LREC 1998 Workshop on Linguistics Coreference,
pages 563?566.
Cosmin Bejan and Sanda Harabagiu. 2010. Unsuper-
vised Event Coreference Resolution with Rich Lin-
guistic Features. In Proceedings of ACL 2010, pages
1412?1422.
Eric Bengtson and Dan Roth. 2008. Understanding
the Value of Features for Coreference Resolution. In
Proceedings of EMNLP 2008, pages 294?303.
Philip Bille. 2005. A Survey on Tree Edit Distance and
Related Problems. Theoretical Computer Science,
337(1-3):217?239.
Jie Cai and Michael Strube. 2010. Evaluation Metrics
For End-to-End Coreference Resolution Systems. In
Proceedings of SIGDIAL 2010, pages 28?36.
Nathanael Chambers and Dan Jurafsky. 2008. Un-
supervised Learning of Narrative Event Chains. In
Proceedings of ACL-HLT 2008, pages 789?797.
Michael Collins and Nigel Duffy. 2001. Convolution
Kernels for Natural Language. In Proceedings of
NIPS 2001, pages 625?632.
Danilo Croce, Alessandro Moschitti, and Roberto
Basili. 2011. Structured Lexical Similarity via Con-
volution Kernels on Dependency Trees. In Proceed-
ings of EMNLP 2011, pages 1034?1046.
Erik D. Demaine, Shay Mozes, Benjamin Rossman,
and Oren Weimann. 2009. An Optimal Decomposi-
tion Algorithm for Tree Edit Distance. ACM Trans-
actions on Algorithms (TALG), 6(1):2:1?2:19.
Greg Durrett, David Hall, and Dan Klein. 2013. De-
centralized Entity-Level Modeling for Coreference
Resolution. In Proceedings of ACL 2013, pages
114?124.
Michael Heilman and Noah A. Smith. 2010. Tree Edit
Models for Recognizing Textual Entailments, Para-
phrases, and Answers to Questions. In Proceedings
of NAACL-HLT 2013, pages 1011?1019.
Eduard Hovy, Teruko Mitamura, Felisa Verdejo, Jun
Araki, and Andrew Philpot. 2013. Events are Not
Simple: Identity, Non-Identity, and Quasi-Identity.
In Proceedings of NAACL-HLT 2013 Workshop on
Events: Definition, Detection, Coreference, and
Representation, pages 21?28.
Heng Ji, David Westbrook, and Ralph Grishman. 2005.
Using Semantic Relations to Refine Coreference
Decisions. In Proceedings of EMNLP/HLT 2005,
pages 17?24.
Philip N. Klein. 1998. Computing the Edit-Distance
Between Unrooted Ordered Trees. In Proceed-
ings of the 6th European Symposium on Algorithms
(ESA), pages 91?102.
Emmanuel Lassalle and Pascal Denis. 2013. Im-
proving pairwise coreference models through fea-
ture space hierarchy learning. In Proceedings of
ACL 2013, pages 497?506.
Heeyoung Lee, Marta Recasens, Angel Chang, Mi-
hai Surdeanu, and Dan Jurafsky. 2012. Joint En-
tity and Event Coreference Resolution across Doc-
uments. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 489?500.
75
Xiaoqiang Luo. 2005. On Coreference Resolution Per-
formance Metrics. In Proceedings of EMNLP 2005,
pages 25?32.
Yashar Mehdad. 2009. Automatic Cost Estimation for
Tree Edit Distance Using Particle Swarm Optimiza-
tion. In Proceedings of ACL-IJCNLP 2009, pages
289?292.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree Kernels for Semantic Role La-
beling. Computational Linguistics, 34(2):193?224.
Mateusz Pawlik and Nikolaus Augsten. 2011. RTED:
A Robust Algorithm for the Tree Edit Distance.
Proceedings of the VLDB Endowment (PVLDB),
5(4):334?345.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In Proceed-
ings of CoNLL Shared Task 2011, pages 1?27.
Altaf Rahman and Vincent Ng. 2009. Supervised
Models for Coreference Resolution. In Proceedings
of EMNLP 2009, pages 968?977.
Marta Recasens and Eduard Hovy. 2011. BLANC:
Implementing the Rand index for coreference eval-
uation. Natural Language Engineering, 17(4):485?
510.
Roger C. Schank and Robert P. Abelson. 1977. Scripts,
Plans, Goals, and Understanding: An Inquiry into
Human Knowledge Structures. Lawrence Erlbaum
Associates.
Shashank Srivastava, Dirk Hovy, and Eduard Hovy.
2013. A Walk-Based Semantically Enriched Tree
Kernel Over Distributed Word Representations. In
Proceedings of EMNLP 2013, pages 1411?1416.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in Noun Phrase
Coreference Resolution: Making Sense of the State-
of-the-Art. In Proceedings of ACL/IJCNLP 2009,
pages 656?664.
Kuo-Chung Tai. 1979. The Tree-to-Tree Correction
Problem. Journal of the ACM (JACM), 26(3):422?
433.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A Model-
Theoretic Coreference Scoring Scheme. In Pro-
ceedings of the 6th Message Understanding Confer-
ence (MUC), pages 45?52.
Mengqiu Wang and Christopher D. Manning. 2010.
Probabilistic Tree-Edit Models with Structured La-
tent Variables for Textual Entailment and Question
Answering. In Proceedings of COLING 2010, pages
1164?1172.
Wuu Yang. 1991. Identifying Syntactic Differences
Between Two Programs. Software: Practice and
Experience, 21(7):739?755.
Xuchen Yao, Benjamin Van Durme, Chris Callison-
burch, and Peter Clark. 2013. Answer Extraction
as Sequence Tagging with Tree Edit Distance. In
Proceedings of NAACL-HLT 2013, pages 858?867.
Kaizhong Zhang and Dennis Shasha. 1989. Simple
Fast Algorithms for the Editing Distance Between
Trees and Related Problems. SIAM J. Comput.,
18(6):1245?1262.
76
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 381?386,
Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational Linguistics
Application of Prize based on Sentence Length in Chunk-based
Automatic Evaluation of Machine Translation
Hiroshi Echizen?ya
Hokkai-Gakuen University
S26-Jo, W11-Chome, Chuo-ku,
Sapporo 064-0926 Japan
echi@lst.hokkai-s-u.ac.jp
Kenji Araki
Hokkaido University
N 14-Jo, W 9-Chome, Kita-ku,
Sapporo 060-0814 Japan
araki@ist.hokudai.ac.jp
Eduard Hovy
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213 USA
hovy@cmu.edu
Abstract
As described in this paper, we pro-
pose a new automatic evaluation met-
ric for machine translation. Our met-
ric is based on chunking between the
reference and candidate translation.
Moreover, we apply a prize based on
sentence-length to the metric, dissim-
ilar from penalties in BLEU or NIST.
We designate this metric as Automatic
Evaluation of Machine Translation in
which the Prize is Applied to a Chunk-
based metric (APAC). Through meta-
evaluation experiments and compari-
son with several metrics, we confirmed
that our metric shows stable correla-
tion with human judgment.
1 Introduction
In the field of machine translation, various
automatic evaluation metrics have been pro-
posed. Among them, chunk-based metrics
such as METEOR(A. Lavie and A. Agarwal,
2007), ROUGE-L(Lin and Och, 2004), and
IMPACT(H. Echizen-ya and K. Araki, 2007)
are effective. In general, BLEU(K. Papineni et
al., 2002), NIST(NIST, 2002), and RIBES(H.
Isozaki et al., 2010) use a penalty for calcula-
tion of scores because the high score is often
given extremely when the candidate transla-
tion is short. Therefore, the penalty is effective
to obtain high correlation with human judg-
ment. On the other hand, almost all chunk-
based metrics use the F -measure based on a
precision by candidate translation and a re-
call by reference. Moreover, they assign a
penalty for the difference of chunk order be-
tween the candidate translation and the refer-
ence, not the penalty for the difference of sen-
tence length. Nevertheless, it is also impor-
tant for chunk-based metrics to examine the
sentence length. In chunk-based metrics, each
word?s weight depends on the sentence length.
For example, the weight of each word is 0.2
(=1/5) when the number of words in a sen-
tence is 5; it is 0.1 (=1/10) when the number
of words in a sentence is 10. Therefore, the
weight of the non-matched word in the short
sentence is large.
To resolve this problem, it is effective for
short sentences to give a prize based on the
sentence length in the chunk-based metrics.
Therefore, we propose a new metric using a
prize based on the sentence length. We des-
ignate this metric as Automatic Evaluation
of Machine Translation in which the Prize is
Applied to a Chunk-based metric (APAC). In
our metric, the weight of a non-matched word
becomes small for the short sentence by award-
ing of the prize. It is almost identical to that
for a long sentence by awarding of the prize.
Therefore, our metric does not depend heavily
on sentence length because the weight of non-
matched words is constantly small. We con-
firmed the effectiveness of APAC using meta-
evaluation experiments.
2 Score calculation in APAC
The APAC score is calculated in two phases.
In the first phase, the chunk sequence is
determined between a candidate translation
and the reference. The chunk sequence
381
is determined using the Longest Common
Subsequence (LCS). Generally, several chunk
sequences are obtained using LCS. In that
case, APAC determines only one chunk se-
quence using the number of words in each
chunk and the position of each chunk.
For example, in between the candidate
translation ?In this case, the system power
supply is accessory battery 86.? and ?In this
case, the system power supply is the accessory
power supply battery 86.?, the chunk sequence
is ?in this case, the system power supply is?,
?accessory? and ?battery 86.?, and the chunk
sequence is ony one in these sentences. Only
one chunk sequence is determined using the
number of words in each chunk and the po-
sition of each chunk when several chunk se-
quences are obtained.
The second phase is calculation of the score
based on the determined chunk sequence. The
Ch score in Eq. (3) is calculated using the de-
termined chunk sequence. In Eq. (3), ch de-
notes each chunk and ch num represents the
number of chunks. Moreover, length(ch) is the
word number of each chunk. ? is the weight
parameter for the length of each chunk. For
example, in between the candidate translation
?In this case, the system power supply is ac-
cessory battery 86.? and ?In this case, the
system power supply is the accessory power
supply battery 86.?, ch num is 3 (?in this
case, the system power supply is?, ?accessory?
and ?battery 86.?). Therefore, Ch score is 91
(=9
2.0
+ 1
2.0
+ 3
2.0
) when ? is 2.0.
P =
{(
?
RN?1
i=0
(
?
i
? Ch score
)
m
?
)
1
?
+0.5? Prize m
}
/2.0 (1)
R =
{(
?
RN?1
i=0
(
?
i
? Ch score
)
n
?
)
1
?
+0.5? Prize n
}
/2.0 (2)
Ch score =
?
ch?ch num
length(ch)
?
(3)
Prize m =
1
log(m) + 1
(4)
Prize n =
1
log(n) + 1
(5)
APAC score =
(1 + ?
2
)RP
R + ?
2
P
(6)
The P and R in Eqs. (1) and (2) re-
spectively denote precision by candidate
translation and recall by reference. These
are calculated using the Ch score obtained
using Eq. (3). Therein, m and n respectively
represent the word numbers of the candidate
translation and the reference. Moreover,
the chunk sequence determination process is
repeated recursively to all common words.
The number of determination processes of
the chunk sequence is high when the word
order of the candidate translation differs
from that of the reference. The RN is the
number of determination processes of the
chunk sequence. Here, ? is the parameter for
the chunk order. It is less than 1.0. The value
of the Ch score is small when the chunk order
between the candidate translation and refer-
ences differs because the value of length(ch)
in each chunk becomes small. For example,
in between the candidate translation ?In this
case, the system power supply is accessory
battery 86.? and ?In this case, the system
power supply is the accessory power sup-
ply battery 86.?,
(
?
RN?1
i=0
(
?
i
?Ch score
)
m
?
)
1
?
is 0.773 (=
?
91
169
=
?
?
1?1
i=0
(0.1
0
?91)
13
2.0
)
and
(
?
RN?1
i=0
(
?
i
?Ch score
)
n
?
)
1
?
is 0.596
(=
?
91
256
=
?
?
1?1
i=0
(0.1
0
?91)
16
2.0
) when ? and ?
respectively stand for 0.1 and 2.0. The
value of RN is 1 because there is no more
matching words after the determined chunks
(?in this case, the system power supply is?,
?accessory? and ?battery 86.?) are removed
from the candidate translation ?In this case,
the system power supply is accessory battery
86.? and ?In this case, the system power
supply is the accessory power supply battery
86.?.
Moreover, Prize m and Prize n in Eqs. (1)
and (2) are calculated respectively using Eqs.
382
(4) and (5). Each is less than 1.0. For ex-
ample, in the candidate translation ?In this
case, the system power supply is accessory
battery 86.? and ?In this case, the system
power supply is the accessory power supply
battery 86.?, Prize m and Prize n respec-
tively stand for 0.473 (=
1
1.114+1
=
1
log(13)+1
) and
0.454 (=
1
1.204+1
=
1
log(16)+1
). These values be-
come large in the short sentences. They be-
come small in the long sentences. Therefore,
the weight of each non-matched word is small
in the short sentences. It is kept small in
the long sentences. Finally, the score is cal-
culated using Eq. (6). This equation shows
the f -measure based on P and R. In Eq. (6),
? is determined as P/R(C. J. V. Rijsbergen,
1979). The APAC score is between 0.0 and
1.0. For example, in the candidate transla-
tion ?In this case, the system power supply is
accessory battery 86.? and ?In this case, the
system power supply is the accessory power
supply battery 86.?, P and R respectively
stand for 0.505 (=
0.773+0.5?0.473
2.0
) and 0.412
(=
0.596+0.5?0.454
2.0
). Therefore, APAC score is
0.445 (=
0.521
1.171
=
(1+1.503)?0.412?0.505
0.412+1.503?0.505
) and ? is
1.226 (=
0.505
0.412
)
3 Experiments
3.1 Experimental Procedure
Meta-evaluation experiments are performed
using WMT2012(C. Callison-Burch et al.,
2012) data and WMT2013(O. Bojar et al.,
2013) data, and NTCIR-7(A. Fujii et al., 2008)
data and NTCIR-9(A. Goto et al., 2011) data.
All sentences by NTCIR data are English
patent sentences obtained through Japanese-
to-English translation. The number of refer-
ences is 1. In NTICR-7 data, the average
value in the evaluation results of three hu-
man judgments is used as the scores of 1?
5 from the perspective of adequacy and flu-
ency. In NTCIR-9 data, the evaluation results
of one human judgment is used as the scores
of 1?5 from the view of adequacy and accep-
tance. For this meta-evaluation, we used only
English and Japanese candidate translations
because we can evaluate them in comparison
with other languages correctly.
We calculated the correlation between the
scores by automatic evaluation and the scores
by human judgments at the system level and
the segment level, respectively. Spearman?s
rank correlation coefficient is used at the sys-
tem level. The Kendall tau rank correlation
coefficient is used in the segment level.
Moreover, we used BLEU (ver. 13a),
NIST (ver. 13a), METEOR (ver. 1.4), and
APAC with no prize (APAC no p) as the
automatic evaluation metrics for comparison
with APAC as shown in Eqs. (4) and (5).
In APAC no p,
(
?
RN?1
i=0
(
?
i
?Ch score
)
m
?
)
1
?
as P
and
(
?
RN?1
i=0
(
?
i
?Ch score
)
m
?
)
1
?
as R are used re-
spectively in Eqs. (1) and (2).
3.2 Experimental Results
Tables 1 and 2 respectively present Spear-
man?s rank correlation coefficients of system-
level and Kendall tau rank correlation coef-
ficients of segment-level in WMT2012 data.
Tables 3 and 4 respectively show Spearman?s
rank correlation coefficients of the system-level
and Kendall tau rank correlation coefficients of
segment-level in WMT2013 data. Moreover,
Tables 5 and 6 respectively present Spear-
man?s rank correlation coefficients of system-
level and Kendall tau rank correlation coeffi-
cients of segment-level in NTCIR-7 data. Ta-
bles 7 and 8 respectively show Spearman?s
rank correlation coefficients of system-level
and Kendall tau rank correlation coefficients
of the segment level in NTCIR-9 data.
In APAC, 0.1 and 1.2 were used as the values
of parameters ? and ? by the preliminarily ex-
perimentally obtained results. In Tables 1?8,
?Rank? denotes the ranking based on ?Avg.?
The value of ?()? denotes the number of MT
systems in Tables 1, 3, 5, and 7. The value of
?()? represents the number of sentence pairs
in Tables 2, 4, 6, and 8. These values depend
on the data.
3.3 Discussion
The results presented in Tables 1?8 indicate
that APAC can obtain the most stable corre-
lation coefficients among some metrics. The
ranking of APAC is No. 1 through NTCIR
data in Tables 5?8. In WMT data of Ta-
bles 1?4, the ranking of APAC is the lowest
except for Table 3. However, the difference
383
cs-en(6) de-en(16) es-en(12) fr-en(15) Avg. Rank
APAC 0.886 0.650 0.958 0.811 0.826 5
APAC no p 0.886 0.676 0.958 0.807 0.832 3
METEOR 0.943 0.841 0.979 0.818 0.895 1
BLEU 0.886 0.674 0.958 0.796 0.828 4
NIST 0.943 0.700 0.944 0.779 0.841 2
Table 1: Spearman?s rank correlation coefficient of system-level in WMT2012 data.
cs-en(11,155) de-en(12,042) es-en(9,880) fr-en(11,682) Avg. Rank
APAC 0.185 0.204 0.209 0.226 0.206 3
APAC no p 0.189 0.207 0.208 0.226 0.207 2
METEOR 0.223 0.279 0.248 0.243 0.248 1
Table 2: Kendall tau rank correlation coefficient of the segment level in WMT2012 data.
between the ranking of METEOR, which is
the highest, and that of APAC is not larger
in WMT data. The correlation coefficients of
APAC in NTCIR data of Tables 5?8 are higher
than those of METEOR. In Tables 5 and 6,
underlining in APAC signifies that the differ-
ences between correlation coefficients obtained
using APAC and METEOR are statistically
significant at the 5% significance level. In Ta-
ble 7, the correlation coefficients of METEOR,
BLEU, and NIST are extremely low. Only one
human judgment was used in NTCIR-9 data.
As a result, APAC is fundamentally effective
for various languages independent of the differ-
ences in the grammatical structures between
languages: these experimentally obtained re-
sults indicate that APAC is the most stable
metric.
Moreover, in APAC, the correlation coeffi-
cients of the segment level in NTCIR data were
increased using the prize of Eqs. (4) and (5).
In WMT data, the correlation coefficients are
almost identical using the prize. Therefore,
use of the prize was fundamentally effective
at the segment level. The evaluation quality
of segment level is generally very low in the
automatic evaluation metrics. Therefore, it is
extremely important to improve the correla-
tion coefficient of segment level. Application
of the prize is effective to improve the evalua-
tion quality of the segment level.
4 Conclusion
As described in this paper, we proposed a new
chunk-based automatic evaluation metric us-
ing the prize based on the sentence length.
The experimentally obtained results indicate
that APAC is the most stable metric.
We will improve APAC to obtain higher
correlation coefficients in future studies.
Particularly, we will strive to improve
the correlation coefficients at the segment
level. The APAC software will be re-
leased by http://www.lst.hokkai-s-u.ac.
jp/~echi/automatic_evaluation_mt.html.
Acknowledgments
This work was done as research under the
AAMT/JAPIO Special Interest Group on
Patent Translation. The Japan Patent In-
formation Organization (JAPIO) and the Na-
tional Institute of Information (NII) provided
corpora used in this work. The author grate-
fully acknowledges support from JAPIO and
NII.
References
O. Bojar, C. Buck, C. Callison-Burch, C. Feder-
mann, B. Haddow, P. Koehn, C. Monz, M. Post,
R. Sortcut and L. Specia. 2013. Findings of the
2013 Workshop on Statistical Machine Transla-
tion. Proceedings of the Eighth Workshop on
Statistical Machine Translation. pp.1?44.
C. Callison-Burch, P. Koehn, C. Monz, M. Post,
R. Sortcut and L. Specia. 2012. Findings of the
2012 Workshop on Statistical Machine Transla-
tion. Proceedings of the Seventh Workshop on
Statistical Machine Translation. pp.10?51.
H. Echizen-ya and K. Araki. 2007. Automatic
Evaluation of Machine Translation based on
384
cs-en(11) de-en(17) es-en(12) fr-en(13) ru-en(19) Avg. Rank
APAC 0.900 0.904 0.916 0.934 0.709 0.873 3
APAC no p 0.909 0.909 0.937 0.934 0.721 0.882 2
METEOR 0.982 0.946 0.923 0.967 0.889 0.941 1
BLEU 0.945 0.897 0.853 0.951 0.614 0.852 4
NIST 0.900 0.828 0.804 0.786 0.465 0.757 5
Table 3: Spearman?s rank correlation coefficient of the system level in WMT2013 data.
cs-en de-en es-en fr-en ru-en
Metrics
(85,469) (128,668) (67,832) (80,741) (151,422)
Avg. Rank
APAC 0.144 0.163 0.169 0.139 0.121 0.147 3
APAC no p 0.148 0.167 0.176 0.142 0.123 0.151 2
METEOR 0.222 0.236 0.241 0.194 0.226 0.224 1
Table 4: Kendall tau rank correlation coefficient of the segment level in WMT2013 data.
Recursive Acquisition of an Intuitive Common
Parts Continuum. Proceedings of the Eleventh
Machine Translation Summit. pp.151?158.
A. Fujii, M. Utiyama, M. Yamamoto and T. Ut-
suro. 2008. Overview of the Patent Translation
Task at the NTCIR-7 Workshop. Proceedings
of the Seventh NTCIR Workshop Meeting on
Evaluation of Information Access Technologies:
Information Retrieval, Question Answering and
Cross-lingual Information Access. pp.389?400.
I. Goto, B. Lu, K. P. Chow, E. Sumita and B. K.
Tsou. 2011. Overview of the Patent Translation
Task at the NTCIR-9 Workshop. Proceedings of
the Ninth NTCIR Workshop Meeting. pp.559?
578.
H. Isozaki, T. Hirao, K. Duh, K. Sudoh and
H. Tsukada. 2010. Automatic Evaluation of
Translation Quality for Distant Language Pairs.
Proceedings of the 2010 Conference on Empir-
ical Methods in Natural Language Processing.
pp.944?952.
A. Lavie and A. Agarwal. 2007. Meteor: An Auto-
matic Metric for MT Evaluation with High Lev-
els of Correlation with Human Judgments. Pro-
ceedings of the Second Workshop on Statistical
Machine Translation.
Chin-Yew Lin and F. J. Och. 2004. Automatic
Evaluation of Machine Translation Quality Us-
ing the Longest Common Subsequence and Skip-
Bigram Statistics. In Proc. of ACL?04, 606?613.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurrence Statistics.
http://www.nist.gov/speech/tests/mt/doc/
ngram-study.pdf.
K. Papineni, S. Roukos, T. Ward, and Wei-Jing
Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL). pp.311?
318.
C. J. Van Rijsbergen. 1979. Information Retrieval
(2nd ed.), Butterworths.
385
Adequacy(15) Fluency(15) Avg. Rank
APAC 0.872 0.805 0.839 1
APAC no p 0.872 0.805 0.839 1
METEOR 0.424 0.380 0.402 5
BLEU 0.582 0.586 0.584 3
NIST 0.578 0.568 0.573 4
Table 5: Spearman?s rank correlation coefficient of the system level in NTCIR-7 data.
Adequacy (1,500) Fluency (1,500) Avg. Rank
APAC 0.494 0.489 0.491 1
APAC no p 0.482 0.476 0.479 2
METEOR 0.366 0.383 0.375 3
Table 6: Kendall tau rank correlation coefficient of the segment level in NTCIR-7 data.
Adequacy (19) Acceptance (14) Avg. Rank
APAC 0.182 0.298 0.240 1
APAC no p 0.182 0.298 0.240 1
METEOR -0.081 0.015 -0.033 4
BLEU -0.123 0.059 -0.032 3
NIST -0.344 -0.275 -0.309 5
Table 7: Spearman?s rank correlation coefficient of the system level in NTCIR-9 data.
Adequacy (5,700) Acceptance (5,700) Avg. Rank
APAC 0.250 0.261 0.256 1
APAC no p 0.242 0.250 0.246 2
METEOR 0.167 0.217 0.192 3
Table 8: Kendall tau rank correlation coefficient of segment-level in NTCIR-9 data.
386
