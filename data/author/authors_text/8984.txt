Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 133?136,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Sentiment Vector Space Model for  
Lyric-based Song Sentiment Classification 
 
 
Yunqing Xia Linlin Wang 
Center for Speech and language Tech.  State Key Lab of Intelligent Tech. and Sys. 
RIIT, Tsinghua University Dept. of CST, Tsinghua University 
Beijing 100084, China Beijing 100084, China 
yqxia@tsinghua.edu.cn wangll07@mails.tsinghua.edu.cn 
  
Kam-Fai Wong Mingxing Xu 
Dept. of SE&EM Dept. of CST 
The Chinese University of Hong Kong Tsinghua University 
Shatin, Hong Kong Beijing 100084, China 
kfwong@se.cuhk.edu.hk xumx@tsinghua.edu.cn 
 
 
Abstract 
Lyric-based song sentiment classification 
seeks to assign songs appropriate sentiment 
labels such as light-hearted and heavy-hearted. 
Four problems render vector space model 
(VSM)-based text classification approach in-
effective: 1) Many words within song lyrics 
actually contribute little to sentiment; 2) 
Nouns and verbs used to express sentiment are 
ambiguous; 3) Negations and modifiers 
around the sentiment keywords make particu-
lar contributions to sentiment; 4) Song lyric is 
usually very short. To address these problems, 
the sentiment vector space model (s-VSM) is 
proposed to represent song lyric document. 
The preliminary experiments prove that the s-
VSM model outperforms the VSM model in 
the lyric-based song sentiment classification 
task. 
1 Introduction 
Song sentiment classification nowadays becomes a 
hot research topic due largely to the increasing 
demand of ubiquitous song access, especially via 
mobile phone. In their music phone W910i, Sony 
and Ericsson provide Sense Me component to catch 
owner?s mood and play songs accordingly. Song 
sentiment classification is the key technology for 
song recommendation. Many research works have 
been reported to achieve this goal using audio sig-
nal (Knees et al, 2007). But research efforts on 
lyric-based song classification are very few. 
Preliminary experiments show that VSM-based 
text classification method (Joachims, 2002) is inef-
fective in song sentiment classification (see Sec-
tion 5) due to the following four reasons. Firstly, 
the VSM model considers all content words within 
song lyric as features in text classification. But in 
fact many words in song lyric actually make little 
contribution to sentiment expressing. Using all 
content words as features, the VSM-based classifi-
cation methods perform poorly in song sentiment 
classification. Secondly, observation on lyrics of 
thousands of Chinese pop songs reveals that senti-
ment-related nouns and verbs usually carry multi-
ple senses. Unfortunately, the ambiguity is not 
appropriately handled in the VSM model. Thirdly, 
negations and modifiers are constantly found 
around the sentiment words in song lyric to inverse, 
to strengthen or to weaken the sentiments that the 
sentences carry. But the VSM model is not capable 
of reflecting these functions. Lastly, song lyric is 
usually very short, namely 50 words on average in 
length, rendering serious sparse data problem in 
VSM-based classification. 
To address the aforementioned problems of the 
VSM model, the sentiment vector space model (s-
VSM) is proposed in this work. We adopt the s-
VSM model to extract sentiment features from 
song lyrics and implement the SVM-light 
(Joachims, 2002) classification algorithm to assign 
sentiment labels to given songs. 
133
2 Related Works  
Song sentiment classification has been investigated 
since 1990s in audio signal processing community 
and research works are mostly found relying on 
audio signal to make a decision using machine 
learning algorithms (Li and Ogihara, 2006; Lu et 
al., 2006). Typically, the sentiment classes are de-
fined based on the Thayer?s arousal-valence emo-
tion plane (Thayer, 1989). Instead of assigning 
songs one of the four typical sentiment labels, Lu 
et al (2006) propose the hierarchical framework to 
perform song sentiment classification with two 
steps. In the first step the energy level is detected 
with intensity features and the stress level is de-
termined in the second step with timbre and 
rhythm features. It is proved difficult to detect 
stress level using audio as classification proof. 
Song sentiment classification using lyric as 
proof is recently investigated by Chen et al (2006). 
They adopt the hierarchical framework and make 
use of song lyric to detect stress level in the second 
step. In fact, many literatures have been produced 
to address the sentiment analysis problem in natu-
ral language processing research. Three approaches 
are dominating, i.e. knowledge-based approach 
(Kim and Hovy, 2004), information retrieval-based 
approach (Turney and Littman, 2003) and machine 
learning approach (Pang et al, 2002), in which the 
last approach is found very popular. Pang et al 
(2002) adopt the VSM model to represent product 
reviews and apply text classification algorithms 
such as Na?ve Bayes, maximum entropy and sup-
port vector machines to predict sentiment polarity 
of given product review.  
Chen et al (2006) also apply the VSM model in 
lyric-based song sentiment classification. However, 
our experiments show that song sentiment classifi-
cation with the VSM model delivers disappointing 
quality (see Section 5). Error analysis reveals that 
the VSM model is problematic in representing 
song lyric. It is necessary to design a new lyric rep-
resentation model for song sentiment classification. 
3 Sentiment Vector Space Model 
We propose the sentiment vector space model (s-
VSM) for song sentiment classification. Principles 
of the s-VSM model are listed as follows. 
(1) Only sentiment-related words are used to pro-
duce sentiment features for the s-VSM model.  
(2) The sentiment words are appropriately disam-
biguated with the neighboring negations and 
modifiers.  
(3) Negations and modifiers are included in the s-
VSM model to reflect the functions of invers-
ing, strengthening and weakening.  
Sentiment unit is found the appropriate element 
complying with the above principles.  
To be general, we first present the notation for 
sentiment lexicon as follows. 
 ,...,1},{                         
   ,...,1},{                         
  ,...,1},{  };,,{
LlmM
JjnN
IicCMNCL
l
j
i
==
==
===
 
in which L represents sentiment lexicon, C senti-
ment word set, N negation set and M modifier set. 
These words can be automatically extracted from a 
semantic dictionary and each sentiment word is 
assigned a sentiment label, namely light-hearted or 
heavy-hearted according to its lexical definition.  
Given a piece of song lyric, denoted as follows,  
HhwW h ,...,1},{ ==  
in which W denotes a set of words that appear in 
the song lyric, the semantic lexicon is in turn used 
to locate sentiment units denoted as follows. 
MWmNWnCWc
mncuU
vlvjvi
vlvjviv
??????
==
,,,
,,,
  ;  ;,
 },,{}{
 
Note that sentiment units are unambiguous sen-
timent expressions, each of which contains one 
sentiment word and possibly one modifier and one 
negation. Negations and modifiers are helpful to 
determine the unique meaning of the sentiment 
words within certain context window, e.g. 3 pre-
ceding words and 3 succeeding words in our case.  
Then, the s-VSM model is presented as follows. 
))(),...,(),(( 21 UfUfUfV TS = . 
in which VS represents the sentiment vector for the 
given song lyric and fi(U) sentiment features which 
are usually certain statistics on sentiment units that 
appear in lyric.  
We classify the sentiment units according to oc-
currence of sentiment words, negations and modi-
fiers. If the sentiment word is mandatory for any 
sentiment unit, eight kinds of sentiment units are 
obtained. Let fPSW denote count of positive senti-
134
ment words (PSW), fNSW count of negative senti-
ment words (NSW), fNEG count of negations (NEG) 
and fMOD count of modifiers (MOD). Eight senti-
ment features are defined in Table 1.  
fi Number of sentiment units satisfying ?
f1 fPSW >0, fNSW =fNEG =fMOD =0  
f2 fPSW =0, fNSW >0, fNEG = fMOD =0  
f3 fPSW >0, fNSW =0,  fNEG>0, fMOD =0 
f4 fPSW=0, fNSW >0, fNEG >0, fMOD =0  
f5 fPSW >0, fNSW =0, fNEG =0, fMOD >0  
f6 fPSW=0, fNSW >0, fNEG =0, fMOD >0  
f7 fPSW >0, fNSW =0, fNEG >0, fMOD >0  
f8 fPSW =0, fNSW >0, fNEG >0, fMOD >0  
Table 1. Definition of sentiment features. Note that 
one sentiment unit contains only one sentiment 
word. Thus it is not possible that fPSW and fNSW are 
both bigger than zero. 
Obviously, sparse data problem can be well ad-
dressed using statistics on sentiment units rather 
than on individual words or sentiment units.  
4  Lyric-based Song Sentiment Classifica-
tion 
Song sentiment classification based on lyric can be 
viewed as a text classification task thus can be 
handled by some standard classification algorithms. 
In this work, the SVM-light algorithm is imple-
mented to accomplish this task due to its excel-
lence in text classification.  
Note that song sentiment classification differs 
from the traditional text classification in feature 
extraction. In our case, sentiment units are first 
detected and the sentiment features are then gener-
ated based on sentiment units. As the sentiment 
units carry unambiguous sentiments, it is deemed 
that the s-VSM is model is promising to carry out 
the song sentiment classification task effectively. 
5 Evaluation 
To evaluate the s-VSM model, a song corpus, i.e. 
5SONGS, is created manually. It covers 2,653 Chi-
nese pop songs, in which 1,632 are assigned label 
of light-hearted (positive class) and 1,021 assigned 
heavy-hearted (negative class). We randomly se-
lect 2,001 songs (around 75%) for training and the 
rest for testing. We adopt the standard evaluation 
criteria in text classification, namely precision (p), 
recall (r), f-1 measure (f) and accuracy (a) (Yang 
and Liu, 1999). 
In our experiments, three approaches are imple-
mented in song sentiment classification, i.e. audio-
based (AB) approach, knowledge-based (KB) ap-
proach and machine learning (ML) approach, in 
which the latter two approaches are also referred to 
as text-based (TB) approach. The intentions are 1) 
to compare AB approach against the two TB ap-
proaches, 2) to compare the ML approach against 
the KB approach, and 3) to compare the VSM-
based ML approach against the s-VSM-based one. 
Audio-based (AB) Approach 
We extract 10 timbre features and 2 rhythm fea-
tures (Lu et al, 2006) from audio data of each song. 
Thus each song is represented by a 12-dimension 
vector. We run SVM-light algorithm to learn on the 
training samples and classify test ones.  
Knowledge-based (KB) Approach 
We make use of HowNet (Dong and dong, 
2006), to detect sentiment words, to recognize the 
neighboring negations and modifiers, and finally to 
locate sentiment units within song lyric. Sentiment 
(SM) of the sentiment unit (SU) is determined con-
sidering sentiment words (SW), negation (NEG) 
and modifiers (MOD) using the following rule.  
(1) SM(SU) = label(SW); 
(2) SM(SU) = - SM(SU) iff SU contains NEG; 
(3) SM(SU) = degree(MOD)*SM(SU) iff SU 
contains MOD. 
In the above rule, label(x) is the function to read 
sentiment label(?{1, -1}) of given word in the 
sentiment lexicon and degree(x) to read its modifi-
cation degree(?{1/2, 2}). As the sentiment labels 
are integer numbers, the following formula is 
adopted to obtain label of the given song lyric.  
??
???
?= ?
i
iSUSMsignlabel )(  
Machine Learning (ML) Approach 
The ML approach adopts text classification al-
gorithms to predict sentiment label of given song 
lyric. The SVM-light algorithm is implemented 
based on VSM model and s-VSM model, respec-
tively. For the VSM model, we apply (CHI) algo-
rithm (Yang and Pedersen, 1997) to select effective 
sentiment word features. For the s-VSM model, we 
adopt HowNet as the sentiment lexicon to create 
sentiment vectors.  
Experimental results are presented Table 2.    
135
 p R f-1 a 
Audio-based 0.504 0.701 0.586 0.504
Knowledge-based 0.726 0.584 0.647 0.714
VSM-based 0.587 1.000 0.740 0.587
s-VSM-based 0.783 0.750 0.766 0.732
Table 2. Experimental results 
Table 2 shows that the text-based methods out-
perform the audio-based method. This justifies our 
claim that lyric is better than audio in song senti-
ment detection. The second observation is that ma-
chine learning approach outperforms the 
knowledge-based approach. The third observation 
is that s-VSM-based method outperforms VSM-
based method on f-1 score. Besides, we surpris-
ingly find that VSM-based method assigns all test 
samples light-hearted label thus recall reaches 
100%. This makes results of VSM-based method 
unreliable. We look into the model file created by 
the SVM-light algorithm and find that 1,868 of 
2,001 VSM training vectors are selected as support 
vectors while 1,222 s-VSM support vectors are 
selected. This indicates that the VSM model indeed 
suffers the problems mentioned in Section 1 in 
lyric-based song sentiment classification. As a 
comparison, the s-VSM model produces more dis-
criminative support vectors for the SVM classifier 
thus yields reliable predictions.  
6  Conclusions and Future Works 
The s-VSM model is presented in this paper as a 
document representation model to address the 
problems encountered in song sentiment classifica-
tion. This model considers sentiment units in fea-
ture definition and produces more discriminative 
support vectors for song sentiment classification. 
Some conclusions can be drawn from the prelimi-
nary experiments on song sentiment classification. 
Firstly, text-based methods are more effective than 
the audio-based method. Secondly, the machine 
learning approach outperforms the knowledge-
based approach. Thirdly, s-VSM model is more 
reliable and more accurate than the VSM model. 
We are thus encouraged to carry out more research 
to further refine the s-VSM model in sentiment 
classification. In the future, we will incorporate 
some linguistic rules to improve performance of 
sentiment unit detection. Meanwhile, sentiment 
features in the s-VSM model are currently equally 
weighted. We will adopt some estimation tech-
niques to assess their contributions for the s-VSM 
model. Finally, we will also explore how the s-
VSM model improves quality of polarity classifi-
cation in opinion mining.  
Acknowledgement 
Research work in this paper is partially supported 
by NSFC (No. 60703051) and Tsinghua University 
under the Basic Research Foundation (No. 
JC2007049). 
References  
R.H. Chen, Z.L. Xu, Z.X. Zhang and F.Z. Luo. Content 
Based Music Emotion Analysis and Recognition. 
Proc. of 2006 International Workshop on Computer 
Music and Audio Technology, pp.68-75. 2006.  
Z. Dong and Q. Dong. HowNet and the Computation of 
Meaning. World Scientific Publishing. 2006. 
T. Joachims. Learning to Classify Text Using Support 
Vector Machines, Methods, Theory, and Algorithms. 
Kluwer (2002). 
S.-M. Kim and E. Hovy. Determining the Sentiment of 
Opinions. Proc. COLING?04, pp. 1367-1373. 2004. 
P. Knees, T. Pohle, M. Schedl and G. Widmer. A Music 
Search Engine Built upon Audio-based and Web-
based Similarity Measures. Proc. of SIGIR'07, pp.47-
454. 2007 
T. Li and M. Ogihara. Content-based music similarity 
search and emotion detection. Proc. IEEE Int. Conf. 
Acoustic, Speech, and Signal Processing, pp. 17?21. 
2006. 
L. Lu, D. Liu and H. Zhang. Automatic mood detection 
and tracking of music audio signals. IEEE Transac-
tions on Audio, Speech & Language Processing 
14(1): 5-18 (2006). 
B. Pang, L. Lee and S. Vaithyanathan. Thumbs up? Sen-
timent Classification using Machine Learning Tech-
niques. Proc. of EMNLP-02, pp.79-86. 2002. 
R. E. Thayer, The Biopsychology of Mood and Arousal, 
New York, Oxford University Press. 1989. 
P. D. Turney and M. L. Littman. Measuring praise and 
criticism: Inference of semantic orientation from as-
sociation. ACM Trans. on Information Systems, 
21(4):315?346. 2003. 
Y. Yang and X. Liu. A Re-Examination of Text Catego-
rization Methods. Proc. of SIGIR?99, pp. 42-49. 1999. 
Y. Yang and J. O. Pedersen. A comparative study on 
feature selection in text categorization. Proc. 
ICML?97, pp.412-420. 1997. 
 
136
FASIL Email Summarisation System 
 
 
Angelo Dalli, Yunqing Xia, Yorick Wilks 
NLP Research Group 
Department of Computer Science 
University of Sheffield 
{a.dalli, y.xia, y.wilks}@dcs.shef.ac.uk 
 
 
Abstract 
Email summarisation presents a unique set of 
requirements that are different from general 
text summarisation. This work describes the 
implementation of an email summarisation 
system for use in a voice-based Virtual Per-
sonal Assistant developed for the EU FASiL 
Project. Evaluation results from the first inte-
grated version of the project are presented. 
1 Introduction 
Email is one of the most ubiquitous applications used on 
a daily basis by millions of people world-wide, tradi-
tionally accessed over a fixed terminal or laptop com-
puter. In the past years there has been an increasing 
demand for email access over mobile phones. Our work 
has focused on creating an email summarisation service 
that provides quality summaries adaptively and quickly 
enough to cater for the tight constrains imposed by a 
real time text-to-speech system. 
This work has been done as part of the European 
Union FASiL project, which aims to aims to construct a 
conversationally intelligent Virtual Personal Assistant 
(VPA) designed to manage the user?s personal and 
business information through a voice-based interface 
accessible over mobile phones.  
As the quality of life and productivity is to improved 
in an increasingly information dominated society, peo-
ple need access to information anywhere, anytime. The 
Adaptive Information Management (AIM) service in the 
FASiL VPA seeks to automatically prioritise and pre-
sent information that is most pertinent to the mobile 
users and adapt to different user preferences. The AIM 
service is comprised of three main parts: an email sum-
mariser, email categoriser, calendar scheduling/PIM 
interaction and an adaptive prioritisation service that 
optimizes the sequence in which information is pre-
sented, keeping the overall duration of the voice-based 
dialogue to a minimum. 
2 Email Characteristics 
Email Summarisation techniques share many character-
istics with general text summarisation techniques while 
catering for the unique characteristics of email: 
1. short messages usually between 2 to 800 
words in length (after thread-filtering) 
2. frequently do not obey grammatical or con-
ventional stylistic conventions 
3. are a cross between informal mobile text or 
chat styles and traditional writing formats 
4. display unique thread characteristics with 87% 
containing three previous emails or less 
(Fisher and Moody, 2001) 
All these four main characteristics combined to-
gether mean that most document summarisation tech-
niques simply do not work well for email. The voice-
based system also required that summaries be produced 
on demand, with only a short pause allowed for the 
summariser to output a result ? typically a maximum of 
around 1 second per email.  
Another main constraint imposed in the FASiL VPA 
was the presence of two integer parameters ? the pre-
ferred and maximum length of the summary. The 
maximum length constraint had to be obeyed strictly, 
while striving to fit in the summary into the preferred 
length. These performance and size constraints, coupled 
with the four characteristics of email largely determined 
the design of the FASiL Email Summariser. 
2.1 Short Messages 
Email is a form of short, largely informal, written com-
munication that excludes methods that need large 
amounts of words and phrases to work well.  
The main disadvantage is that sometimes the useful 
content of a whole email message is simply a one word 
in case of a yes/no answer to a question or request. The 
summariser exploits this characteristic by filtering out 
threads and other commonly repeated text at the bottom 
of the email text such as standard email text signatures. 
If the resulting text is very short and falls within the 
preferred length of the summary, the message can be 
output in its entirety to users. The short messages also 
make it easier to achieve relevancy in the summaries. 
Inadvertently context is sometimes lost in the sum-
mary due to replies occurring in threaded emails. Also, 
emails containing lots of question-answer pairs can get 
summarised poorly due to the fixed amount of space 
available for the summary. 
2.2 Stylistic Conventions and Grammar 
Email messages often do not follow formal stylistic 
conventions and are may have a substantial level of 
spelling mistakes, abbreviations and other features that 
make text analysis difficult. 
A simple spellchecker using approximate string 
matching and word frequency/occurrence statistics was 
used to match misspelled names automatically.  
Another problem that was encountered was the iden-
tification of sentence boundaries, since more than 10% 
of the emails seen by the summariser frequently had 
missing punctuation and spurious line breaks inserted 
by various different email programs. A set of hand-
coded heuristics managed to produce acceptable results, 
identifying sentence boundaries correctly more than 
90% of the time. 
2.3 Informal and Formal Styles 
Email can often be classified into three categories: in-
formal short messages ? often sent to people whom are 
directly known or with whom there has been a pro-
longed discussion or interaction about a subject, mixed 
formal/informal emails sent to strangers or when re-
questing information or replying to questions, and for-
mal emails that are generally electronic versions of 
formal letter writing. 
The class of emails that cause most problems for 
summarisation purposes are the first two classes of e-
mails. One of the main determining factors for the style 
adopted by people in replying to emails is the amount of 
time that lapses between replies. Generally email gets 
more formal as the time span between replies increases. 
Informal email can also be recognised by excessive 
use of anaphora that need to be resolved properly before 
summarisation can take place. The summariser thus has 
an anaphora resolver that is capable of resolving ana-
phoric references robustly. 
Linguistic theory indicates that as the formality of a 
text increases, the number of words in the deictic cate-
gory will decrease as the number of words in the non-
deictic category increase (and vice-versa). Deictic (or 
anaphoric) word classes include words that have vari-
able meaning whose meaning needs to be resolved 
through the surrounding (usually preceding) context. 
Non-deictic word classes are those words whose mean-
ing is largely context-independent, analogous to predi-
cates in formal logic.  
2.4 Threaded Emails 
Many emails are composed by replying to an original 
email, often including part or whole of the original 
email together with new content, thus creating a thread 
or chain of emails. The first email in the thread will 
potentially be repeated many times over, which might 
mislead the summarisation process. A thread-detection 
filtering tool is used to eliminate unoriginal content in 
the email by comparing the contents of the current email 
with the content of previous emails. A study of over 57 
user?s incoming and outgoing emails found that around 
30% of all emails are threaded. Around 56% of the 
threaded emails contained only one previous email ? i.e. 
a request and reply, and 87% of all emails contained 
only three previous emails apart from the reply (Fisher 
and Moody, 2001). 
Some reply styles also pose a problem when com-
bined with threads. Emails containing a list of questions 
or requests for comments are often edited by the reply-
ing party and answers inserted directly inside the text of 
the original request, as illustrated in Figure 1. 
 
> ? now coming back to the issue 
> of whether to include support for 
> location names in the recogniser 
> I think that we should include 
> this ? your opinions appreciated. 
I agree with this. 
 
Figure 1 Sample Embedded Answer 
 
Figure 1 illustrates the main two difficulties faced by 
the summariser in this situation. While the threaded 
content from the previous reply should be filtered out to 
identify the reply, the reply on its own is meaningless 
without any form of context. The summariser tries to 
overcome this by identifying this style of embedded 
responses when the original content is split into chunks 
or is only partially included in the reply. The text falling 
before the answer is then treated as part of the reply. 
Although this strategy gives acceptable results in some 
cases, more research is needed into finding the optimal 
strategy to extract the right amount of context from the 
thread without either destroying the context or copying 
too much from the original request back into the sum-
mary. 
3 Summarisation Techniques 
Various summarisation techniques were considered in 
the design of the FASiL email summariser. Few opera-
tional email-specific summarisation systems exist, so 
the emphasis was on extracting the best-of-breed tech-
niques from document summarisation systems that are 
applicable to email summarisation. 
3.1 Previous Work 
Many single-document summarisation systems can be 
split according to whether they are extractive or non-
extractive systems. Extractive systems generate summa-
ries by extracting selected segments from the original 
document that are deemed to be most relevant. Non-
extractive systems try to build an abstract representation 
model and re-generate the summary using this model 
and words found in the original document. 
Previous related work on extractive systems in-
cluded the use of semantic tagging and co-
reference/lexical  chains (Saggion et al, 2003; Barzilay 
and Elhadad, 1997; Azzam et al, 1998), lexical occur-
rence/structural statistics (Mathis et al, 1973), discourse 
structure (Marcu, 1998), cue phrases (Luhn, 1958; 
Paice, 1990; Rau et al, 1994), positional indicators 
(Edmunson, 1964) and other extraction methods (Kui-
pec et al, 1995). 
Non-extractive systems are less common ? previous 
related work included reformulation of extracted models 
(McKeown et al, 1999), gist extraction (Berger and 
Mittal, 2000), machine translation-like approaches 
(Witbrock and Mittal, 1999) and generative models (De 
Jong, 1982; Radev and McKeown, 1998; Fum et al,  
1986; Reihmer and Hahn, 1988; Rau et al,  1989). 
A sentence-extraction system was decided for the 
FASiL summariser, with the capability to have phrase-
level extraction in the future. Non-extractive systems 
were not likely to work as robustly and give the high 
quality results needed by the VPA to work as required. 
Another advantage that extractive systems still pose is 
that in general they are more applicable to a wider range 
of arbitrary domains and are more reliable than non-
extractive systems (Teufel, 2003). 
The FASiL summariser uses named entities as an 
indication of the importance of every sentence, and per-
forms anaphora resolution automatically. Sentences are 
selected according to named entity density and also ac-
cording to their positional ranking. 
3.2 Summariser Architecture 
The FASiL Summariser works in conjunction with a 
number of different components to present real-time 
voice-based summaries to users. Figure 2 shows the 
overall architecture of the summariser and its place in 
the FASiL VPA. 
 
 
Figure 2 Summariser and VPA Architecture 
 
An XML-based protocol is used to communicate 
with the Dialogue Manager enabling the system to be 
loosely coupled but to have high cohesion (Sommer-
ville, 1992). 
3.3 Named Entity Recognition 
One of the most important components in the FASiL 
Summariser is the Named Entity Recogniser (NER) 
system. 
The NER uses a very efficient trie-like structure to 
match sub-parts of every name (Gusfield, 1997; 
Stephen, 1994). An efficient implementation enables the 
NER to confirm or reject a word as being a named en-
tity or not in O(n) time. Named entities are automati-
cally classified according to the following list of 11 
classes: 
? Male proper names (M) 
? Female proper names (F) 
? Places (towns, cities, etc.) (P) 
? Locations (upstairs, boardroom, etc.) (L) 
? Male titles (Mr., Esq., etc.) (Mt) 
? Female titles (Ms., Mrs., etc.) (Ft) 
? Generic titles (t) 
? Date and time references (TIME) 
? Male anaphors (Ma) 
? Female anaphors (Fa) 
? Indeterminate anaphors (a)  
 
The gazetteer list for Locations, Titles, and Ana-
phors were compiled manually. Date and time refer-
ences were compiled from data supplied in the IBM 
International Components for Unicode (ICU) project 
(Davis, 2003). Place names were extracted from data 
available online from the U.S. Geological Survey Geo-
graphic Names Information System and the GEOnet 
Names Server (GNS) of the U.S. National Imagery and 
Mapping Agency (USGS, 2003; NIMA, 2003). 
An innovative approach to gathering names for the 
male and female names was adopted using a small cus-
tom-built information extraction system that crawled 
Internet pages to identify likely proper names in the 
texts. Additional hints were provided by the presence of 
anaphora in the same sentence or the following sentence 
as the suspected proper name. The gender of every title 
and anaphora was manually noted and this information 
was used to keep a count of the number of male or fe-
male titles and anaphors associated with a particular 
name. This information enabled the list of names to be 
organised by gender, enabling a rough probability to be 
assigned to suspect words (Azzam et al, 1998; Mitkov, 
2002).  
An Internet-based method that verified the list and 
filtered out likely spelling mistakes and non-existent 
names was then applied to this list, filtering out incor-
rectly spelt names and other features such as online chat 
nicknames (Dalli, 2004). 
A list of over 592,000 proper names was thus ob-
tained by this method with around 284,000 names being 
identified as male and 308,000 names identified as fe-
male. The large size of this list contributed significantly 
to the NER?s resulting accuracy and compares favoura-
bly with previously compiled lists (Stevenson and Gai-
zauskas, 2000). 
3.4 Anaphora Resolution 
Extracting systems suffer from the problem of dangling 
anaphora in summaries. Anaphora resolution is an effec-
tive way of reducing the incoherence in resulting sum-
maries by replacing anaphors with references to the 
appropriate named entities (Mitkov, 2002). This substi-
tution has the direct effect of making the text less con-
text sensitive and implicitly increases the formality of 
the text. 
Cohesion problems due to semantic discontinuities 
where concepts and agents are not introduced are also 
partially solved by placing emphasis on named entities 
and performing anaphora resolution. The major cohe-
sion problem that still has not been fully addressed is 
the coherence of various events mentioned in the text. 
The anaphora resolver is aided by the gender-
categorised named entity classes, enabling it to perform 
better resolution over a wide variety of names. A simple 
linear model is adopted, where the system focuses 
mainly on nominal and clausal antecedents (Cristea et 
al., 2000). The search scope for candidate antecedents is 
set to the current sentence together with the three pre-
ceding sentences as suggested in (Mitkov, 1998) as em-
pirical studies show that more than 85% of all cases are 
handled correctly with this window size (Mitkov, 2002). 
Candidate antecedents being discarded after ten sen-
tences have been processed without the presence of 
anaphora as suggested in (Kameyama, 1997). 
3.5 Sentence Ranking 
After named entity recognition and anaphora resolution, 
the summariser ranks the various sentences/phrases that 
it identifies and selects the best sentences to extract and 
put in the summary. The summariser takes two parame-
ters apart from the email text itself: a preferred length 
and a maximum length. Typical lengths are 160 charac-
ters preferred with 640 characters maximum, which 
compares to the size a mobile text message. 
Ranking takes into account three parameters: named 
entity density and importance of every class, sentence 
position and the preferred and maximum length parame-
ters. 
0
1
2
3
4
5
6
7
8
1 3 5 7 9 11 13 15 17 19
Number of Sentences
Weight
Series1 Series2 Series3
Series4 Series5
 
Figure 3 Positional sentence weight for varying 
summarisation parameters 
 
Positional importance was found to be significant in 
email text since relevant information was often found to 
be in the first few sentences of the email.  
Figure 3 shows how the quadratic positional weight 
function ? changes with position, giving less importance 
to sentences as they occur further from the start (al-
though the weight is always bigger than zero). Different 
kinds of emails were used to calibrate the weight func-
tion. Series 1 (bottom) represents a typical mobile text 
message length summary with a very long message. 
Series 4 and 5 (middle) represent the weight function 
behaviour when the summary maximum length is long 
(approximately more than 1,000 characters), irrelevant 
of the email message length itself. Series 2 and 3 (top) 
represent email messages that fall within the maximum 
length constraints. 
The following ranking function rank(j), where j is 
the sentence number, is used to rank and select excerpts: 
( ) ( )( ) ( ) ( )( )++?= ?
=
? ????
0
,1
i
c iijjrank  
( )? ?( )???
????
? ??+???
?
???
?
+ ?
???? jlength
j
j
1
max  
 
where ? and ? are empirically determined constants, 
? is the preferred summary length, and jmax is the num-
ber of sentences in the email. The NER function ?c 
represents the number of words of type i in sentence j 
and ?(i) gives the weight associated with that type. In 
our case ? equals 10 since there are 11 named entity 
classes. The NER weights ?(i) for every class have 
been empirically determined and optimized. A third 
parameter ? is used to change the values of ? and ? ac-
cording to the maximum and preferred lengths together 
with the email length as shown in Figure 3. 
The first term handles named entity density, the sec-
ond the sentence position and the third biases the rank-
ing towards the preferred length. The sentences are then 
sorted in rank order and the preferred and maximum 
lengths used to determine which sentences to return in 
the summary. 
4 Experimental Results 
The summariser results quality was evaluated against 
manually produced summaries using precision and re-
call, together with a more useful utility-based evaluation 
that uses a fractional model to cater for varying degrees 
of importance for different sentences. 
4.1 Named Entity Recognition Performance 
The performance of the summariser depends signifi-
cantly on the performance of the NER. Speed tests show 
that the NER consistently processes more than 1 million 
wps on a 1.6 GHz machine while keeping resource us-
age to a manageable 300-400 Mb of memory. 
Precision and recall curves were calculated for 100 
emails chosen at random, separated into 10 random 
sample groups from representative subsets of the three 
main types of emails ? short, normal and long emails as 
explained previously. The samples were manually 
marked according to the 11 different named entity 
classes recognised by the NER to act as a comparative 
standard for relevant results. Figures 4 and 5 respec-
tively show the NER precision and recall results. 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Precision
M F P L
Mt Ft t TIME
Ma Fa a  
Figure 4 Precision by Named Entity Class 
 
It is interesting to note that the NER performed 
worst at anaphora identification with an average preci-
sion of 77.5% for anaphora but 96.7% for the rest of the 
named entity classes. 
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Recall
M F P L
Mt Ft t TIME
Ma Fa a
 
Figure 5 Recall by Named Entity Class 
 
Figure 6 shows the average precision and recall av-
eraged across all the eleven types of named entity 
classes, for the 10 sample email groups. An average 
precision of 93% was achieved throughout, with 97% 
recall. 
0
0.25
0.5
0.75
1
1 2 3 4 5 6 7 8 9 10
Sample Group
Value
Recall Precision
 
Figure 6 Average Precision and Recall 
 
It is interesting to note that the precision and recall 
curves do not exhibit the commonly observed inverse 
trade-off relationship between precision and recall 
(Buckland and Gey, 1994; Alvarez, 2002). This result is 
explained by the fact that the NER, in this case, can 
actually identify most named entities in the text with 
high precision while neither over-selecting irrelevant 
results nor under-selecting relevant results. 
4.2 Summariser Results Quality 
Quality evaluation was performed by selecting 150 
emails at random and splitting the emails up into 15 
groups of 10 emails at random to facilitate multiple per-
son evaluation. Each sentence in every email was then 
manually ranked using a scale of 1 to 10. For recall and 
precision calculation, any sentence ranked ? 5 was de-
fined as relevant. Figure 7 shows the precision and re-
call values with 74% average precision and 71% aver-
age recall. 
0
0.5
1
1.5
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Sample Group
Value
Recall Precision
 
Figure 7 Summaries Recall and Precision 
 
A utility-based evaluation was also used to obtain 
more intuitive results than those given by precision and 
recall using the methods reported in (Jing et al, 1998; 
Goldstein et al, 1999; Radev et al, 2000). The average 
score of each summary was compared to the average 
score over infinity expected to be obtained by extracting 
a combination of the first [1..N] sentences at random. 
The summary average score was also compared to the 
score obtained by an averaged pool of 3 human judges. 
Figure 8 shows a comparison between the summariser 
performance and human performance, with the summar-
iser averaging at 86.5% of the human performance, 
ranging from 60% agreement to 100% agreement with 
the gold standard. 
0
0.5
1
1.5
2
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Sample Group
Value
Summariser Utility Gold Standard Utility
 
Figure 8 Utility Score Comparison 
In Figure 8 a random extraction system is expected 
to get a score of 1 averaged across an infinite amount of 
runs. The average sentence compression factor for the 
summariser was 42%, exactly the same as the human 
judges? results. The selected emails had an average 
length of 14 sentences, varying from 7 to 27 sentences. 
5 Conclusion and Future Work 
The FASiL Email Summarisation system represents a 
compact summarisation system optimised for email 
summarisation in a voice-based system context.  
The excellent performance in both speed and accu-
racy of the NER component makes it ideal for re-use in 
projects that need high quality real-time identification 
and classification of named entities. 
A future improvement will incorporate a fast POS 
analyser to enable phrase-level extraction to take place 
while improving syntactic coherence. An additional 
improvement will be the incorporation of co-reference 
chain methods to verify email subject lines and in some 
cases suggest more appropriate subject lines. 
The FASiL summariser validates the suitability of 
the combined sentence position and NER-driven ap-
proach towards email summarisation with encouraging 
results obtained. 
Acknowledgments 
This research is funded under the EU FASiL Project, an 
EU grant in Human Language Technology (IST-2001-
38685) (Website: www.fasil.co.uk). 
References 
Alvarez, S. 2002. ?An exact analytical relation among 
recall, precision, and classification accuracy in in-
formation retrieval.? Boston College, Boston, Tech-
nical Report BCCS-02-01. 
Azzam, S., Humphreys, K. and Gaizauskas, R. 1998. 
?Coreference resolution in a multilingual information 
extraction?, Proc. Workshop on Linguistic Corefer-
ence. Granada, Spain. 
Barzilay, R. Elhadad, M. 1997. ?Using Lexical Chains 
for Text Summarization.?, Proc. ACL Workshop on 
Intelligent Scaleable Text Summarization, Madrid, 
Spain. 10-17. 
Berger, L. Mittal, V. 2000. ?OCELOT: A system for 
summarizing web pages?. Carnegie Mellon Univer-
sity. Just Research. Pittsburgh, Pennsylvania. 
Buckland, M. Gey, F. 1994. ?The relationship between 
recall and precision.? J. American Society for Infor-
mation Science, 45(1):12-19. 
Cristea, D., Ide, N., Marcu, D., Tablan, V. 2000. ?An 
empirical investigation of the relation between dis-
course structure and coreference.?, Proc. 19th Int. 
Conf. on Comp. Linguistics (COLING-2000), Saar-
br?cken, Germany. 208-214. 
Dalli, A. 2004. ?An Internet-based method for Verifica-
tion of Extracted Proper Names?. CICLING-2004. 
David, C. 2003. Information Society Statistics: PCs, 
Internet and mobile phone usage in the EU. Euro-
pean Community, Report KS-NP-03-015-EN-N. 
Davis, M. 2003. ?An ICU overview?. Proc. 24th Unicode 
Conference, Atlanta. IBM Corporation, California. 
De Jong, G. 1982. ?An overview of the FRUMP sys-
tem.?, in: Lehnert and Ringle eds., Strategies for 
Natural Language Processing, Lawrence Erlbaum 
Associates, Hillsdale, New Jersey. 149-176. 
Edmunson, H.P. 1964. ?Problems in automatic extract-
ing.?, Comm. ACM, 7, 259-263. 
Fisher, D., Moody, P. 2001. Studies of Automated Col-
lection of Email Records. University of California, 
Irvine, Technical Report UCI-ISR-02-4. 
Fum, D. Guida, G. Tasso, C. 1986. ?Tailoring impor-
tance evaluation to reader?s goals: a contribution to 
descriptive text summarization.? Proc. COLING-86, 
256-259. 
Goldstein, J. Kantrowitz, M. Mittal, V. Carbonell, 
Jaime. 1999. ?Summarizing Text Documents: Sen-
tence Selection and Evaluation Metrics?, Proc. ACM-
SIGIR 1999, Berkeley, California. 
Gusfield, D.  1997.  Algorithms on Strings, Trees and 
Sequences.  Cambridge University Press, Cambridge, 
UK. 
Halliday, M.A.K. 1985. Spoken and written language. 
Oxford University Press, Oxford. 
Jing, H. Barzilay, R. McKeown, K. Elhadad, M. 1998. 
?Summarization Evaluation Methods: Experiments 
and Analysis?, AAAI Spring Symposium on Intelligent 
Text Summarisation, Stanford, California. 
Kameyama, M. 1997. ?Recognising referential links: an 
information extraction perspective.?, Proc. EACL-97 
Workshop on Operational Factors in Practical, Ro-
bust, Anaphora Resolution, Madrid, Spain. 46-53. 
Kuipec, J. Pedersen, J. Chen, F. 1995. ?A Trainable 
Document Summarizer.?, Proc. 18th ACM SIGIR 
Conference, Seattle, Washington. 68-73. 
Luhn, P.H. 1958. ?Automatic creation of literature ab-
stracts?. IBM J. 159-165. 
Marcu, D. 1998. ?To Build Text Summaries of High 
Quality, Nuclearity is not Sufficient.?  Proc. AAAI 
Symposium on Intelligent Text Summarisation, Stan-
ford University, Stanford, California. 1-8. 
Mathis, B.A. Rush, J.E. Young, C.E. 1973. ?Improve-
ment of automatic abstracts by the use of structural 
analysis.?, J. American Society for Information Sci-
ence, 24, 101-109. 
McKeown, K. Klavens, J. Hatzivassiloglou, V. Barzi-
lay, R. Eskin, E. 1999. ?Towards Multidocument 
Summarization by Reformulation: Progress and 
Prospects.?, AAAI Symposium on Intelligent Text 
Summarisation. 
Mitkov, R. 1998. ?Robust pronoun resolution with lim-
ited knowledge.?, Proc. 17th International Confer-
ence on Comp. Linguistics (COLING-1998), 
Montreal, Canada. 869-875. 
Mitkov, R. 2002. Anaphora Resolution. London, Long-
man. 
National Imagery and Mapping Agency (NIMA). 2003. 
GEOnet Names Server (GNS). 
Paice, C. 1990. ?Constructing literature abstracts by 
computer: techniques and prospects.?, Information 
Processing and Management, 26:171-186. 
Radev, D. McKeown, K. 1998. ?Generating Natural 
Language Summaries from Multiple On-Line 
Sources.?, Computational Linguistics, 24(3):469-500. 
Radev, D. Jing, H. Budzikowska, M. 2000. ?Centroid-
based summarization of multiple documents: sen-
tence extraction, utility-based evaluation, user stud-
ies.? in Automatic Summarisation: ANLP/NAACL 
2000 Workshop, New Brunswick, New Jersey. 
Rau, L. Jacobs, P. Zernick, U. 1989. ?Information ex-
traction and text summarization using linguistic 
knowledge acquisition.?, Information Processing and 
Management, 25(4):419-428. 
Rau, L. Brandow, R. Mitze, K. 1994. ?Domain-
Independent Summarization of News.?, in: Summa-
rizing Text for Intelligent Communication, Dagstuhl, 
Germany. 71-75. 
Reimer, U. Hahn, U. 1988. ?Text condensation as 
knowledge base abstraction.? Proc. 4th Conference on 
Artificial Intelligence Applications. 338-344. 
Saggion, H. Bontcheva, K. Cunningham, H. 2003. ?Ro-
bust Generic and Query-based Summarisation?. Proc. 
EACL-2003, Budapest. 
Sommerville, I. 1992. Software Engineering. 4th ed. 
Addison-Wesley. 
Stephen, Graham A. 1994. String Searching Algorithms. 
World Scientific Publishing, Bangor, Gwynedd, UK. 
Stevenson, M. Gaizauskas, R. 2000. ?Using Corpus-
derived Name Lists for Named Entity Recognition, 
Proc. ANLP-2000, Seattle. 
Teufel, S. 2003. ?Information Retrieval: Automatic 
Summarisation?, University of Cambridge. 24-25. 
Witbrock, M. Mittal, V. 1999. ?Ultra Summarization: A 
Statistical Approach to Generating Non-Extractive 
Summaries.?, Just Research, Pittsburgh. 
United States Geological Survey (USGS). 2003. Geo-
graphic Names Information System (GNIS). 
http://geonames.usgs.gov/ 
NIL Is Not Nothing: Recognition of Chinese  
Network Informal Language Expressions 
Abstract 
Informal language is actively used in net-
work-mediated communication, e.g. chat 
room, BBS, email and text message. We refer 
the anomalous terms used in such context as 
network informal language (NIL) expres-
sions. For example, ??(ou3)? is used to re-
place ?? (wo3)? in Chinese ICQ. Without 
unconventional resource, knowledge and 
techniques, the existing natural language 
processing approaches exhibit less effective-
ness in dealing with NIL text. We propose to 
study NIL expressions with a NIL corpus and 
investigate techniques in processing NIL ex-
pressions. Two methods for Chinese NIL ex-
pression recognition are designed in NILER 
system. The experimental results show that 
pattern matching method produces higher 
precision and support vector machines 
method higher F-1 measure. These results are 
encouraging and justify our future research 
effort in NIL processing. 
1 Introduction 
The rapid global proliferation of Internet applica-
tions has been showing no deceleration since the 
new millennium. For example, in commerce more 
and more physical customer services/call centers 
are replaced by Internet solutions, e.g. via MSN, 
ICQ, etc. Network informal language (NIL) is ac-
tively used in these applications. Following this 
trend, we forecast that NIL would become a key 
language for human communication via network.  
Today NIL expressions are ubiquitous. They 
appear, for example, in chat rooms, BBS, email, 
text message, etc. There is growing importance in 
understanding NIL expressions from both technol-
ogy and humanity research points of view. For 
instance, comprehension of customer-operator dia-
logues in the aforesaid commercial application 
would facilitate effective Customer Relationship 
Management (CRM).  
Recently, sociologists showed many interests in 
studying impact of network-mediated communica-
tion on language evolution from psychological and 
cognitive perspectives (Danet, 2002; McElhearn, 
2000; Nishimura, 2003). Researchers claim that 
languages have never been changing as fast as to-
day since inception of the Internet; and the lan-
guage for Internet communication, i.e. NIL, gets 
more concise and effective than formal language.  
Processing NIL text requires unconventional 
linguistic knowledge and techniques. Unfortu-
nately, developed to handle formal language text, 
the existing natural language processing (NLP) 
approaches exhibit less effectiveness in dealing 
with NIL text. For example, we use ICTCLAS 
(Zhang et al, 2003) tool to process sentence ???
???????(Is he going to attend 
a meeting?)?. The word segmentation result is 
??|?|?|?|?|??|?|??. In this sentence , ??
?? (xi4 ba1 xi4)? is a NIL expression 
which means ?is he ?.?? in this case. It can be 
concluded that without identifying the expression, 
further Chinese text processing techniques are not 
able to produce reasonable result. 
This problem leads to our recent research in 
?NIL is Not Nothing? project, which aims to pro-
duce techniques for NIL processing, thus  avails 
understanding of change patterns and behaviors in 
language (particularly in Internet language) evolu-
tion. The latter could make us more adaptive to the 
dynamic language environment in the cyber world.  
Recently some linguistic works have been car-
ried out on NIL for English. A shared dictionary 
Yunqing Xia,  Kam-Fai Wong,  Wei Gao
Department of Systems Engineering and Engineering Management 
The Chinese University of Hong Kong, Shatin, Hong Kong 
{yqxia, kfwong, wgao}@se.cuhk.edu.hk 
95
has been compiled and made available online. It 
contains 308 English NIL expressions including 
English abbreviations, acronyms and emoticons. 
Similar efforts for Chinese are rare. This is be-
cause Chinese language has not been widely used 
on the Internet until ten years ago. Moreover, Chi-
nese NIL expression involves processing of Chi-
nese Pinyin and dialects, which results in higher 
complexity in Chinese NIL processing.  
In ?NIL is Not Nothing? project, we develop a 
comprehensive Chinese NIL dictionary. This is a 
difficult task because resource of NIL text is rather 
restricted. We download a collection of BBS text 
from an Internet BBS system and construct a NIL 
corpus by annotating NIL expressions in this col-
lection by hand. An empirical study is conducted 
on the NIL expressions with the NIL corpus and a 
knowledge mining tool is designed to construct the 
NIL dictionary and generate statistical NIL fea-
tures automatically. With these knowledge and 
resources, the NIL processing system, i.e. NILER, 
is developed to extract NIL expressions from NIL 
text by employing state-of-the-art information ex-
traction techniques.  
The remaining sections of this paper are organ-
ized as follow. In Section 2, we observe formation 
of NIL expressions. In Section 3 we present the 
related works. In Section 4, we describe NIL cor-
pus and the knowledge engineering component in 
NIL dictionary construction and NIL features gen-
eration. In Section 5 we present the methods for 
NIL expression recognition. We outline the ex-
periments, discussions and error analysis in Sec-
tion 6, and finally Section 7 concludes the paper. 
2 The Ways NIL Expressions Are Typi-
cally Formed 
NIL expressions were first introduced for expedit-
ing writing or computer input, especially for online 
chat where the input speed is crucial to prompt and 
effective communication. For example, it is rather 
annoying to input full Chinese sentences in text-
based chatting environment, e.g. over the mobile 
phone. Thus abbreviations and acronyms are then 
created by forming words in capital with the first 
letters of a series of either English words or Chi-
nese Pinyin.  
Chinese Pinyin is a popular approach to Chi-
nese character input. Some Pinyin input methods 
incorporate lexical intelligence to support word or 
phrase input. This improves input rate greatly. 
However, Pinyin input is not error free. Firstly, 
options are usually prompted to user and selection 
errors result in homophone, e.g. ???(ban1 
zu2)? and ??? (ban1 zhu3)?. Secondly, 
input with incorrect Pinyin or dialect produces 
wrong Chinese words with similar pronunciation, 
e.g. ???(xi1 fan4)? and ???(xi3 hua-
n1)?. Nonetheless, prompt communication spares 
little time to user to correct such a mistake. The 
same mistake in text is constantly repeated, and the 
wrong word thus becomes accepted by the chat 
community. This, in fact, is one common way that 
a new Chinese NIL expression is created. 
We collect a large number of ?sentences? 
(strictly speaking, not all of them are sentences) 
from a Chinese BBS system and identify NIL ex-
pressions by hand. An empirical study on NIL ex-
pressions in this collection shows that NIL 
expressions can be classified into four classes as 
follow based on their origins. 
1) Abbreviation (A). Many Chinese NIL expres-
sions are derived from abbreviation of Chi-
nese Pinyin. For example, ?PF? equals to ??
?(pei4 fu2)? which means ?admire?.  
2) Foreign expression (F). Popular Informal ex-
pressions from foreign languages such as 
English are adopted, e.g. ?ASAP? is used for 
?as soon as possible?. 
3) Homophone (H). A NIL expression is some-
times generated by borrowing a word with 
similar sound (i.e. similar Pinyin). For exam-
ple ??? ? equals ??? ? which means 
?like?. ???? and ???? hold homophony 
in a Chinese dialect. 
4) Transliteration (T) is a transcription from one 
alphabet to another and a letter-for-letter or 
sound-for-letter spelling is applied to repre-
sent a word in another language. For exam-
ple, ???(bai4 bai4)? is transliteration 
of ?bye-bye?. 
A thorough observation, in turn, reveals that, 
based on the ways NIL expressions are formed 
and/or their part of speech (POS) attributes, we 
observe a NIL expression usually takes one of the 
forms presented in Table 1 and Table 2. 
The above empirical study is essential to NIL 
lexicography and feature definition. 
96
3 Related Works 
NIL expression recognition, in particular, can be 
considered as a subtask of information extraction 
(IE). Named entity recognition (NER) happens to 
hold similar objective with NIL expression recog-
nition, i.e. to extract meaningful text segments 
from unstructured text according to certain pre-
defined criteria. 
NER is a key technology for NLP applications 
such as IE and question & answering. It typically 
aims to recognize names for person, organization, 
location, and expressions of number, time and cur-
rency. The objective is achieved by employing 
either handcrafted knowledge or supervised learn-
ing techniques. The latter is currently dominating 
in NER amongst which the most popular methods 
are decision tree (Sekine et al, 1998; Pailouras et 
al., 2000), Hidden Markov Model (Zhang et al, 
2003; Zhao, 2004), maximum entropy (Chieu and 
Ng, 2002; Bender et al, 2003), and support vector 
machines (Isozaki and Kazawa, 2002; Takeuchi 
and Collier, 2002; Mayfield, 2003). 
From the linguistic perspective, NIL expres-
sions are rather different from named entities in 
nature. Firstly, named entity is typically noun or 
noun phrase (NP), but NIL expression can be any 
kind, e.g. number ?94? in NIL represents ????
which is a verb meaning ?exactly be?. Secondly, 
named entities often have well-defined meanings 
in text and are tractable from a standard dictionary; 
but NIL expressions are either unknown to the dic-
tionary or ambiguous. For example, ???? ap-
pears in conventional dictionary with the meaning 
of Chinese porridge, but in NIL text it represents ?
??? which surprisingly represents ?like?. The 
issue that concerns us is that these expressions like  
???? may also appear in NIL text with their 
formal meaning. This leads to ambiguity and 
makes it more difficult in NIL processing.  
Another notable work is the project of ?Nor-
malization of Non-standard Words? (Sproat et al, 
2001) which aims to detect and normalize the 
?Non-Standard Words (NSW)? such as digit se-
quence; capital word or letter sequence; mixed 
case word; abbreviation; Roman numeral; URL 
and e-mail address. In our work, we consider most 
types of the NSW in English except URL and 
email address. Moreover, we consider Chinese 
NIL expressions that contain same characters as 
the normal words. For example, ??? ? and           
???? both appear in common dictionaries, but 
they carry anomalous meanings in NIL text. Am-
biguity arises and basically brings NIL expressions 
recognition beyond the scope of NSW detection.  
According to the above observations, we pro-
pose to employ the existing IE techniques to han-
dle NIL expressions. Our goal is to develop a NIL 
expression recognition system to facilitate net-
work-mediated communication. For this purpose, 
we first construct the required NIL knowledge re-
sources, namely, a NIL dictionary and n-gram sta-
tistical features. 
Table 2: NIL expression forms based on POS attribute.  
POS
Attribute 
# of NIL  
Expressions Examples 
Number 1 ?W? represents ??(wan4)?and means ?ten thousand?. 
Pronoun 9 ??? represents ??? and means ?I?. 
Noun 29 
?LG? represents ???(lao3 
gong1)? and means ?hus-
band?.
Adjective 250 ?FB? represents ???(fu3 
bai4)? and means ?corrupt?. 
Verb 34 
???(cong1 bai2)? repre-
sents ???(chong3 bai4)?
and means ?adore?. 
Adverb 10 ??(fen3)? represents ??
(hen3)? and means ?very?. 
Exclamation  9 
??(nie0)? represents ??
(ne0)? and equals a descrip-
tive exclamation. 
Phrase 309 ?AFK? represents ?Away From Keyboard?.  
Table 1: NIL expression forms based on word formation. 
Word  
Formation 
# of NIL  
Expressions Examples 
Chinese  
Word or 
Phrase
33 ???? represents ???? and means ?like?. 
Sequence of 
English 
Capitals  
341 ?PF? represents ???? and means ?admire?. 
Number 8
?94(jiu3 si4)? represents
???(jiu4 shi4)? and 
means ?exactly be?. 
Mixture of  
the Above 
Forms
30
?8?(ba1 cuo4)? repre-
sents ???(bu3 cuo4)?
and means ?not bad?. 
Emoticons 239 ?:-(? represents a sad emotion.
97
4 Knowledge Engineering 
Recognition of NIL expressions relies on uncon-
ventional linguistic knowledge such as NIL dic-
tionary and NIL features. We construct a NIL 
corpus and develop a knowledge engineering 
component to obtain these knowledge by running a 
knowledge mining tool on the NIL corpus. The 
knowledge mining tool is a text processing pro-
gram that extracts NIL expressions and their at-
tributes and contextual information, i.e. n-grams, 
from the NIL corpus. Workflow for this compo-
nent is presented in Figure 1. 
4.1  NIL Corpus
The NIL corpus is a collection of network informal 
sentences which provides training data for NIL 
dictionary and statistical NIL features. The NIL 
corpus is constructed by annotating a collection of 
NIL text manually. 
Obtaining real chat text is difficult because of 
the privacy restriction. Fortunately, we find BBS 
text within ????(da4 zui3 qu1)? zone in 
YESKY system (http://bbs.yesky.com/bbs/) re-
flects remarkable colloquial characteristics and 
contains a vast amount of NIL expressions. We 
download BBS text posted from December 2004 
and February 2005 in this zone. Sentences with 
NIL expressions are selected by human annotators, 
and NIL expressions are manually identified and 
annotated with their attributes. We finally col-
lected 22,432 sentences including 451,193 words 
and 22,648 NIL expressions. 
The NIL expressions are marked up with 
SGML. The typical example, i.e. ???????
???? in Section 1, is annotated as follows. 
where NILEX is the SGML tag to label a NIL ex-
pression, which entails NIL linguistic attributes 
including class, normal, pinyin, segments, pos, and 
posseg (see Section 4.2). H is a value of class (see 
Section 2). Value VERB demotes verb, ADJ adjec-
tive, NUM number and AUX auxiliary. 
4.2  NIL Dictionary 
The NIL dictionary is a structured databank that 
contains NIL expression entries. Each entity in 
turn entails nine attributes described as follow. 
1. ID: an unique identification number for the 
NIL expression, e.g. 915800; 
2. string: string of the NIL expression, e.g. ??
???;
3. class: class of the NIL expression (see Sec-
tion 2), e.g. ?H? for homophony;  
4. pinyin: Chinese Pinyin for the NIL expres-
sion, e.g. ?xi4 ba1 xi4?; 
5. normal: corresponding normal text for the 
NIL expression, e.g. ?????;  
6. segments: word segments of the NIL expres-
sion, e.g. ??|?|??; 
7. pos: POS tag associated with the expression, 
e.g. ?VERB? denoting a verb;  
8. posseg: a POS tag list for the word seg-
ments, e.g. ?VERB|AUX|VERB?;  
9. frequency: number of occurrences of the 
NIL expression. 
We run the knowledge mining tool to extract all 
annotated NIL expressions together with their at-
tributes from the NIL corpus. The NIL expressions 
are then each assigned an ID number and inserted 
into an indexed data file, i.e. the NIL dictionary. 
Current NIL dictionary contains 651 NIL entries.  
4.3  NIL Feature Set 
The NIL features are required by support vector 
machines method in NIL expression recognition. 
We define two types of statistical features for NIL 
expressions, i.e. Chinese word n-grams and POS 
tag n-grams. Bigger n leads to more contextual 
?<NILEX string=????? class=?H? normal=??
??? pinyin=?xi4 ba1 xi4? segments=??|?|??
pos=?VERB? posseg=?ADJ|NUM|ADJ?>???
</NILEX>?????
Figure 1: Workflow for NIL knowledge engineering 
component. NILE refers to NIL expression, which is 
identified and annotated by human annotator.  
NILE 
Annotation 
 Original Text  
 Collection 
NIL Corpus 
NIL 
Dictionary 
NIL 
Features 
Extract  
A Sentence 
Knowledge Mining Tool 
Word Segmentation & POS Tagging 
(ICTCLAS) 
98
information, but results in higher computational 
complexity. To compromise, we generate n-grams 
with n = 1, 2, 3, 4. For example,   ???/????
is a bi-gram for ????? in terms of word seg-
mentation, and its POS tag bi-gram is 
?PRONOUN/ VERB?.  
We run the knowledge mining tool on the NIL 
corpus to produce all n-grams for Chinese words 
and their POS tags in which NIL expression ap-
pears. 8379 features were generated including 
7416 word-based n-grams and 963 POS tag-based 
n-grams. These statistical NIL features are linked 
to the corresponding NIL dictionary entries by 
their global NIL expression IDs. 
Besides, we consider some morphological fea-
tures including being/containing a number, some 
English capitals or Chinese characters. These fea-
tures can be extracted by parsing string of the NIL 
expressions. 
5 NILER System 
5.1  Architecture 
We develop NILER system to recognize NIL ex-
pressions in NIL text and convert them to normal 
language text. The latter functionality is discussed 
in other literatures. Architecture of NILER system 
is presented in Figure 2. 
The input chat text is first segmented and POS 
tagged with ICTCLAS tool. Because ICTCLAS is 
not able to identify NIL expressions, some expres-
sions are broken into several segments. NIL ex-
pression recognizer processes the segments and 
POS tags and identifies the NIL expressions.  
5.2  NIL Expression Recognizer 
We implement two methods in NIL expression 
recognition, i.e. pattern matching and support vec-
tor machines. 
5.2.1  Method I: Pattern Matching  
Pattern matching (PM) is a traditional method in 
information extraction systems. It uses a hand-
crafted rule set and dictionary for this purpose. 
Because it?s simple, fast and independent of cor-
pus, this method is widely used in IE tasks. 
By applying NIL dictionary, candidates of NIL 
expressions are first extracted from the input text 
with longest matching. As ambiguity occurs con-
stantly, 24 patterns are produced and employed to 
disambiguate. We first extract those word and POS 
tag n-grams from the NIL corpus and create pat-
terns by generalizing them manually. An illustra-
tive pattern is presented as follows. 
?]_[)_(8]_[ ?!!! anyvunitvnotanyv
where anyv _  and unitv _  are variables denoting 
any word and any unit word respectively;  )(xnot
is the negation operator. The illustrative pattern 
determines ?8? to be a NIL expression if it is suc-
ceeded by a unit word. With this pattern, ?8? 
within sentence ?????  ???? (He has 
been working for eight hours.)? is not recognized 
as a NIL expression.  
5.2.2  Method II: Support Vector Machines  
Support vector machines (SVM) method produces 
high performance in many classification tasks 
(Joachims, 1998; Kudo and Matsumoto, 2001). As 
SVM can handle large numbers of features effi-
ciently, we employ SVM classification method to 
NIL expression recognition. 
Suppose we have a set of training data for a 
two-class classification problem {(x1,y1), (x2,
y2),?,(xN, yN)}, where ),...2,1( NiRx Di  ?  is a fea-
ture vector of the i-th order sample in the training 
set and }1,1{ ?iy  is the label for the sample. 
The goal of SVM is to find a decision function that 
accurately predicts y for unseen x. A non-linear 
SVM classifier gives a decision function 
))(()( xgsignxf   for an input vector x, where  
?
 
 
l
i
ii bzxKxg
1
),()( Y
The szi  are so-called support vectors, and 
represents the training samples. iY  and b  are pa-
rameters for SVM motel. l is number of training 
samples. ),( zxK  is a kernel function that implic-
NIL 
Dictionary 
NIL 
Features 
 Chat Text 
NIL Expression 
 List 
NIL Expression 
Recognizer 
Word Segmentation 
Word POS Tagging 
(ICTCLAS) 
Figure 2: Architecture of NILER system. 
99
itly maps vector x into a higher dimensional space. 
A typical kernel is defined as dot products, i.e.  
)(),( zxkzxK x .
Based on the training process, the SVM algo-
rithm constructs the support vectors and parame-
ters. When text is input for classification, it is first 
converted into feature vector x. The SVM method 
then classifies the vector x by determining sign of 
g(x), in which 1)(  xf  means that word x is posi-
tive and otherwise if 1)(  xf . The SVM algo-
rithm was later extended in SVMmulticlass to predict 
multivariate outputs (Joachims, 1998).  
In NIL expression recognition, we consider 
NIL corpus as training set and the annotated NIL 
expressions as samples. NIL expression recogni-
tion is achieved with the five-class SVM classifi-
cation task, in which four classes are those defined 
in Section 2 and reflected by class attribute within 
NIL annotation scheme. The fifth class is 
NOCLASS, which means the input text is not any 
NIL expression class.  
6 Experiments
6.1  Experiment Description 
We conduct experiments to evaluate the two meth-
ods in performing the task of NIL expression rec-
ognition. In training phase we use NIL corpus to 
construct NIL dictionary and pattern set for PM 
method, and generate statistical NIL features, sup-
port vectors and parameters for SVM methods. To 
observe how performance is influenced by the vol-
ume of training data, we create five NIL corpora, 
i.e. C#1~C#5, with five numbers of NIL sentences, 
i.e. 10,000, 13,000, 16,000, 19,000 and 22,432, by 
randomly selecting sentence from NIL corpus de-
scribed in Section 4.1.  
To generate test set, we download 5,690 sen-
tences from YESKY system which cover BBS text 
in March 2005. We identify and annotate NIL ex-
pressions within these sentences manually and 
consider the annotation results as gold standard.  
We first train the system with the five corpora 
to produce five versions of NIL dictionary, pattern 
set, statistical NIL feature set and SVM model. We 
then run the two methods with each version of the 
above knowledge over the test set to produce rec-
ognition results automatically. We compare these 
results against the gold stand and present experi-
mental results with criteria including precision, 
recall and F1-measure. 
6.2  Experimental Results 
We present experimental results of the two meth-
ods on the five corpora in Table 3. 
Table 3: Experimental results for the two methods on the five 
corpora. PRE denotes precision, REC denotes recall, and F1 
denotes F1-Measure. 
PM SVM Corpus
PRE REC F1 PRE REC F1 
C#1 0.742 0.547 0.630 0.683 0.703 0.693 
C#2 0.815 0.634 0.713 0.761 0.768 0.764 
C#3 0.873 0.709 0.783 0.812 0.824 0.818 
C#4 0.904 0.759 0.825 0.847 0.851 0.849 
C#5 0.915 0.793 0.850 0.867 0.875 0.871 
6.3  Discussion I: The Two Methods 
To compare performance of the two methods, we 
present the experimental results with smoothed 
curves for precision, recall and F1-Mesure in Fig-
ure 3, Figure 4 and Figure 5 respectively. 
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 3: Smoothed precision curves over the five corpora.  
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 4: Smoothed recall curves over the five corpora.   
Figure 3 reveals that PM method produces 
higher precision, i.e. 91.5%, and SVM produces 
higher recall, i.e. 79.3%, and higher F1-Measure, 
i.e. 87.1%, with corpus C#5. It can be inferred that 
PM method is self-restrained. In other words, if a 
NIL expression is identified with this method, it is 
very likely that the decision is right. However, the 
weakness is that more NIL expressions are ne-
glected. On the other hand, SVM method outper-
100
forms PM method regarding overall capability, i.e. 
F1-Measure, according to Figure 5. 
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
Pattern Matching
SVM
Figure 5: Smoothed F1-Measure curves over the five corpora. 
We argue that each method holds strength and 
weakness. Different methods should be adopted to 
cater to different application demands. For exam-
ple, in CRM text processing, we might favor preci-
sion. So PM method may be the better choice. On 
the other hand, to perform the task of chat room 
security monitoring, recall is more important. Then 
SVM method becomes the better option. We claim 
that there exists an optimized approach which 
combines the two methods and yields higher preci-
sion and better robustness at the same time. 
6.4  Discussion II: How Volume Influences Per-
formance 
To observe how training corpus influences per-
formance in the two methods regarding volume, 
we present experimental results with smoothed 
quality curves for the two method in Figure 6 and 
Figure 7 respectively. 
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
0 1 2 3 4 5 6
PRE
REC
F1
Figure 6: Smoothed quality curves for PM method over the 
five corpora.
0.65
0.7
0.75
0.8
0.85
0.9
0 1 2 3 4 5 6
PRE
REC
F1
Figure 7: Smoothed quality curves for SVM method 
over the five corpora. 
The smoothed quality curves in Figure 6 and 
Figure 7 reveal the tendency that bigger volume of 
training data leads to better processing quality. 
Meanwhile, the improvement tends to decrease 
along with increasing of volume. It thus predicts 
that there exists a corpus with a certain volume 
that produces the best quality according to the ten-
dency. Although current corpus is not big enough 
to prove the optimal volume, the tendency re-
vealed by the curves is obvious. 
6.5  Error Analysis 
We present two examples to analyze errors occur 
within our experiments. 
Err.1 Ambiguous NIL Expression 
Example 1:
[Sentence]: ??? 8??
[Meaning]:  I still don?t understand. 
[NIL expression found(Y/N)? ]: Y 
[Normal language text]: ??????
Error in Example 1 is caused by failure in iden-
tifying ? ? ? (mi3 bai2)?. Because ? ?
(mi3)? succeeds ?8(ba1)? in the word seg-
ments, i.e. ??|??|8|?|??, and it can be used as 
a unit word, PM method therefore refuses to iden-
tify ?8(ba)? as a NIL expression according to the 
pattern described in Section 5.2.1. In fact, ????
is an unseen NIL expression. SVM method suc-
cessfully recognizes ??? ? to be ??? (mi3
you3)?, thus recognizes ?8?. In our experiments 
56 errors in PM method suffer the same failure, 
while SVM method identifies 48 of them. This 
demonstrates that PM method is self-restrained 
and SVM method is relatively scalable in process-
ing NIL text. 
Err.2 Unseen NIL expression 
Example 2: 
[Sentence]: ??? 4U??
[Meaning]: Just came back from 4U. 
[NIL expression found (Y/N)?] : N
Actually, there is no NIL expression in example 
2. But because of a same 1-gram with ?4D?, i.e. 
?4?, SVM outputs ?4U? as a NIL expression. In 
fact, it is the name for a mobile dealer. There are 
78 same errors in SVM method in our experi-
ments, which reveals that SVM method is some-
times over-predicting. In other words, some NIL 
expressions are recognized with SVM method by 
mistake, which results in lower precision. 
101
7 Conclusions and Future Works 
Network informal language processing is a new 
NLP research application, which seeks to recog-
nize and normalize NIL expressions automatically 
in a robust and adaptive manner. This research is 
crucial to improve capability of NLP techniques in 
dealing with NIL text.  With empirical study on 
Chinese network informal text and NIL expres-
sions, we propose two NIL expression recognition 
methods, i.e. pattern matching and support vector 
machines. The experimental results show that PM 
method produces higher precision, i.e. 91.5%, and 
SVM method higher F-1 measure, i.e. 87.1%. 
These results are encouraging and justify our fu-
ture research effort in NIL processing. 
Research presented in this paper is preliminary 
but significant. We address future works as follow. 
Firstly, NIL corpus constructed in our work is fun-
damental. Not only will difficulty in seeking for 
text resource be overcome, but a large quantity of 
manpower will be allocated to this laborious and 
significant work. Secondly, new NIL expressions 
will appear constantly with booming of network-
mediated communication. A powerful NIL expres-
sion recognizer will be designed to improve adap-
tivity of the recognition methods and handle the 
unseen NIL expressions effectively. Finally, we 
state that research in this paper targets in special at 
NIL expressions in China mainland. Due to cul-
tural/geographical variance, NIL expressions in 
Hong Kong and Taiwan could be different. Further  
research will be conducted to adapt our methods to 
other NIL communities.  
References 
Bender, O., Och, F. J. and Ney, H. 2003. Maximum En-
tropy Models for Named Entity Recognition,
CoNLL-2003,  pp. 148-151.  
Chieu, H. L. and Ng, H. T. 2002. Named Entity Recog-
nition: A Maximum Entropy Approach Using Global 
Information. COLING-02, pp. 190-196. 
Danet, B. 2002. The Language of Email, European Un-
ion Summer School, University of Rome. 
Isozaki, H. and Kazawa, H. 2002. Efficient Support 
Vector Classifiers for Named Entity Recognition,
COLING-02, pp. 390-396.. 
Joachims, T. 1998. Text categorization with Support 
Vector Machines: Learning with many relevant fea-
tures. ECML?98, pp. 137-142. 
Kudo, T. and Matsumoto, Y. 2001. Chunking with Sup-
port Vector Machines. NAACL 2001, pp.192-199. 
Mayfield, J. 2003. Paul McNamee; Christine Piatko, 
Named Entity Recognition using Hundreds of Thou-
sands of Features, CoNLL-2003, pp. 184-187. 
McElhearn, K. 2000. Writing Conversation - An Analy-
sis of Speech Events in E-mail Mailing Lists,
http://www.mcelhearn.com/cmc.html, Revue Fran-
?aise de Linguistique Appliqu?e, volume V-1.  
Nishimura, Y. 2003. Linguistic Innovations and Inter-
actional Features of Casual Online Communication 
in Japanese, JCMC 9 (1). 
Pailouras, G., Karkaletsis, V. and Spyropoulos, C. D. 
2000. Learning Decision Trees for Named-Entity 
Recognition and Classification. Workshop on Ma-
chine Learning for Information Extraction, 
ECAI(2000).   
Sekine, S., Grishman, R. and Shinnou, H. 1998. A Deci-
sion Tree Method for Finding and Classifying Names 
in Japanese Texts, WVLC 98. 
Snitt, E. N. 2000. The Use of Language on the Internet,
http://www.eng.umu.se/vw2000/Emma/lin-
guistics1.htm. 
Sproat, R., Black, A.,  Chen, S., Kumar, S., Ostendorf, 
M. and Richards, M. 2001. Normalization of Non-
standard Words. Computer Speech and Languages, 
15(3):287- 333. 
Takeuchi, K. and Collier, N. 2002. Use of Support Vec-
tor Machines in Extended Named Entity Recognition.
CoNLL-2002, pp. 119-125.  
Zhang, Z., Yu, H., Xiong, D. and Liu, Q. 2003. HMM-
based Chinese Lexical Analyzer ICTCLAS. In the 2nd
SIGHAN workshop affiliated with ACL?03, pp. 184-
187.  
Zhao, S. 2004. Named Entity Recognition in Biomedical 
Texts Using an HMM model, COLING-04 workshop 
on Natural Language Processing in Biomedicine and 
its Applications.  
102
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 993?1000,
Sydney, July 2006. c?2006 Association for Computational Linguistics
A Phonetic-Based Approach to Chinese Chat Text Normalization 
 
Yunqing Xia, Kam-Fai Wong 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
{yqxia, kfwong}@se.cuhk.edu.hk 
Wenjie Li 
Department of Computing 
The Hong Kong Polytechnic University 
Kowloon, Hong Kong 
cswjli@comp.polyu.edu.hk 
 
 
Abstract 
Chatting is a popular communication 
media on the Internet via ICQ, chat 
rooms, etc. Chat language is different 
from natural language due to its anoma-
lous and dynamic natures, which renders 
conventional NLP tools inapplicable. The 
dynamic problem is enormously trouble-
some because it makes static chat lan-
guage corpus outdated quickly in repre-
senting contemporary chat language. To 
address the dynamic problem, we pro-
pose the phonetic mapping models to 
present mappings between chat terms and 
standard words via phonetic transcrip-
tion, i.e. Chinese Pinyin in our case. Dif-
ferent from character mappings, the pho-
netic mappings can be constructed from 
available standard Chinese corpus. To 
perform the task of dynamic chat lan-
guage term normalization, we extend the 
source channel model by incorporating 
the phonetic mapping models. Experi-
mental results show that this method is 
effective and stable in normalizing dy-
namic chat language terms. 
1 Introduction 
Internet facilitates online chatting by providing 
ICQ, chat rooms, BBS, email, blogs, etc. Chat 
language becomes ubiquitous due to the rapid 
proliferation of Internet applications. Chat lan-
guage text appears frequently in chat logs of 
online education (Heard-White, 2004), customer 
relationship management (Gianforte, 2003), etc. 
On the other hand, wed-based chat rooms and 
BBS systems are often abused by solicitors of 
terrorism, pornography and crime (McCullagh, 
2004). Thus there is a social urgency to under-
stand online chat language text. 
Chat language is anomalous and dynamic. 
Many words in chat text are anomalous to natural 
language. Chat text comprises of ill-edited terms 
and anomalous writing styles. We refer chat 
terms to the anomalous words in chat text. The 
dynamic nature reflects that chat language 
changes more frequently than natural languages. 
For example, many popular chat terms used in 
last year have been discarded and replaced by 
new ones in this year. Details on these two fea-
tures are provided in Section 2.  
The anomalous nature of Chinese chat lan-
guage is investigated in (Xia et al, 2005). Pattern 
matching and SVM are proposed to recognize 
the ambiguous chat terms. Experiments show 
that F-1 measure of recognition reaches 87.1% 
with the biggest training set. However, it is also 
disclosed that quality of both methods drops sig-
nificantly when training set is older. The dy-
namic nature is investigated in (Xia et al, 
2006a), in which an error-driven approach is pro-
posed to detect chat terms in dynamic Chinese 
chat terms by combining standard Chinese cor-
pora and NIL corpus (Xia et al, 2006b). Lan-
guage texts in standard Chinese corpora are used 
as negative samples and chat text pieces in the 
NIL corpus as positive ones. The approach calcu-
lates confidence and entropy values for the input 
text. Then threshold values estimated from the 
training data are applied to identify chat terms. 
Performance equivalent to the methods in exis-
tence is achieved consistently. However, the is-
sue of normalization is addressed in their work. 
Dictionary based chat term normalization is not a 
good solution because the dictionary cannot 
cover new chat terms appearing in the dynamic 
chat language. 
In the early stage of this work, a method based 
on source channel model is implemented for chat 
term normalization. The problem we encounter is 
addressed as follows. To deal with the anoma-
lous nature, a chat language corpus is constructed 
with chat text collected from the Internet. How-
993
ever, the dynamic nature renders the static corpus 
outdated quickly in representing contemporary 
chat language. The dilemma is that timely chat 
language corpus is nearly impossible to obtain. 
The sparse data problem and dynamic problem 
become crucial in chat term normalization. We 
believe that some information beyond character 
should be discovered to help addressing these 
two problems.  
Observation on chat language text reveals that 
most Chinese chat terms are created via phonetic 
transcription, i.e. Chinese Pinyin in our case. A 
more exciting finding is that the phonetic map-
pings between standard Chinese words and chat 
terms remain stable in dynamic chat language. 
We are thus enlightened to make use of the pho-
netic mapping models, in stead of character map-
ping models, to design a normalization algorithm 
to translate chat terms to their standard counter-
parts. Different from the character mapping 
models constructed from chat language corpus, 
the phonetic mapping models are learned from a 
standard language corpus because they attempt to 
model mappings probabilities between any two 
Chinese characters in terms of phonetic tran-
scription. Now the sparse data problem can thus 
be appropriately addressed. To normalize the 
dynamic chat language text, we extend the 
source channel model by incorporating phonetic 
mapping models. We believe that the dynamic 
problem can be resolved effectively and robustly 
because the phonetic mapping models are stable.  
The remaining sections of this paper are or-
ganized as follows. In Section 2, features of chat 
language are analyzed with evidences. In Section 
3, we present methodology and problems of the 
source channel model approach to chat term 
normalization. In Section 4, we present defini-
tion, justification, formalization and parameter 
estimation for the phonetic mapping model. In 
Section 5, we present the extended source chan-
nel model that incorporates the phonetic mapping 
models. Experiments and results are presented in 
Section 6 as well as discussions and error analy-
sis. We conclude this paper in Section 7. 
2 Feature Analysis and Evidences 
Observation on NIL corpus discloses the anoma-
lous and dynamic features of chat language. 
2.1 Anomalous 
Chat language is explicitly anomalous in two 
aspects. Firstly, some chat terms are anomalous 
entries to standard dictionaries. For example, ??
?(here, jie4 li3)? is not a standard word in any 
contemporary Chinese dictionary while it is often 
used to replace ???(here, zhe4 li3)? in chat 
language. Secondly, some chat terms can be 
found in  standard dictionaries while their mean-
ings in chat language are anomalous to the dic-
tionaries. For example, ??(even, ou3)? is often 
used to replace ??(me, wo2)? in chat text. But 
the entry that ??? occupies in standard diction-
ary is used to describe even numbers. The latter 
case is constantly found in chat text, which 
makes chat text understanding fairly ambiguous 
because it is difficult to find out whether these 
terms are used as standard words or chat terms. 
2.2 Dynamic 
Chat text is deemed dynamic due to the fact that 
a large proportion of chat terms used in last year 
may become obsolete in this year. On the other 
hand, ample new chat terms are born. This fea-
ture is not as explicit as the anomalous nature. 
But it is as crucial. Observation on chat text in 
NIL corpus reveals that chat term set changes 
along with time very quickly. 
An empirical study is conducted on five chat 
text collections extracted from YESKY BBS sys-
tem (bbs.yesky.com) within different time peri-
ods, i.e. Jan. 2004, July 2004, Jan. 2005, July 
2005 and Jan. 2006. Chat terms in each collec-
tion are picked out by hand together with their 
frequencies so that five chat term sets are ob-
tained. The top 500 chat terms with biggest fre-
quencies in each set are selected to calculate re-
occurring rates of the earlier chat term sets on the 
later ones.  
Set Jul-04 Jan-05 Jul-05 Jan-06 Avg. 
Jan-04 0.882 0.823 0.769 0.706 0.795
Jul-04 - 0.885 0.805 0.749 0.813
Jan-05 - - 0.891 0.816 0.854
Jul-05 - - - 0.875 0.875
Table 1. Chat term re-occurring rates. The rows 
represent the earlier chat term sets and the col-
umns the later ones. 
The surprising finding in Table 1 is that 29.4% 
of chat terms are replaced with new ones within 
two years and about 18.5% within one year. The 
changing speed is much faster than that in stan-
dard language. This thus proves that chat text is 
dynamic indeed. The dynamic nature renders the 
static corpus outdated quickly. It poses a chal-
lenging issue on chat language processing.  
994
3 Source Channel Model and Problems 
The source channel model is implemented as 
baseline  method in this work for chat term nor-
malization. We brief its methodology and prob-
lems as follows. 
3.1 The Model 
The source channel model (SCM) is a successful 
statistical approach in speech recognition and 
machine translation (Brown, 1990). SCM is 
deemed applicable to chat term normalization 
due to similar task nature. In our case, SCM aims 
to find the character string niicC ,...,2,1}{ ==  that 
the given input chat text njitT ,...,2,1}{ ==  is most 
probably translated to, i.e. ii ct ? , as follows. 
)(
)()|(maxarg)|(maxarg?
Tp
CpCTpTCpC
CC
==     (1) 
Since )(Tp  is a constant for C , so C?  should 
also maximize )()|( CpCTp . Now )|( TCp  is 
decomposed into two components, i.e. chat term 
translation observation model )|( CTp  and lan-
guage model )(Cp . The two models can be both 
estimated with maximum likelihood method us-
ing the trigram model in NIL corpus.  
3.2 Problems 
Two problems are notable in applying SCM in 
chat term normalization. First, data sparseness 
problem is serious because timely chat language 
corpus is expensive thus small due to dynamic 
nature of chat language. NIL corpus contains 
only 12,112 pieces of chat text created in eight 
months, which is far from sufficient to train the 
chat term translation model. Second, training 
effectiveness is poor due to the dynamic nature. 
Trained on static chat text pieces, the SCM ap-
proach would perform poorly in processing chat 
text in the future. Robustness on dynamic chat 
text thus becomes a challenging issue in our re-
search.  
Updating the corpus with recent chat text con-
stantly is obviously not a good solution to the 
above problems. We need to find some informa-
tion beyond character to help addressing the 
sparse data problem and dynamic problem. For-
tunately, observation on chat terms provides us 
convincing evidence that the underlying phonetic 
mappings exist between most chat terms and 
their standard counterparts. The phonetic map-
pings are found promising in resolving the two 
problems.  
4 Phonetic Mapping Model 
4.1 Definition of Phonetic Mapping 
Phonetic mapping is the bridge that connects two 
Chinese characters via phonetic transcription, i.e. 
Chinese Pinyin in our case. For example, ??
???? ?? )56.0,,( jiezhe ?? is the phonetic mapping con-
necting ??(this, zhe4)? and ??(interrupt, jie4)?, 
in which ?zhe? and ?jie? are Chinese Pinyin for  
??? and ??? respectively. 0.56 is phonetic 
similarity between the two Chinese characters. 
Technically, the phonetic mappings can be con-
structed between any two Chinese characters 
within any Chinese corpus. In chat language, any 
Chinese character can be used in chat terms, and 
phonetic mappings are applied to connect chat 
terms to their standard counterparts. Different 
from the dynamic character mappings, the pho-
netic mappings can be produced with standard 
Chinese corpus before hand. They are thus stable 
over time.  
4.2 Justifications on Phonetic Assumption  
To make use of phonetic mappings in normaliza-
tion of chat language terms, an assumption must 
be made that chat terms are mainly formed via 
phonetic mappings. To justify the assumption, 
two questions must be answered. First, how 
many percent of chat terms are created via pho-
netic mappings? Second, why are the phonetic 
mapping models more stable than character map-
ping models in chat language? 
Mapping type Count Percentage 
Chinese word/phrase 9370 83.3% 
English capital 2119 7.9% 
Arabic number 1021 8.0% 
Other  1034 0.8% 
Table 2. Chat term distribution in terms of  map-
ping type. 
To answer the first question, we look into chat 
term distribution in terms of mapping type in 
Table 2. It is revealed that 99.2 percent of chat 
terms in NIL corpus fall into the first four pho-
netic mapping types that make use of phonetic 
mappings. In other words, 99.2 percent of chat 
terms can be represented by phonetic mappings. 
0.8% chat terms come from the OTHER type, 
emoticons for instance. The first question is un-
doubtedly answered with the above statistics.  
To answer the second question, an observation 
is conducted again on the five chat term sets de-
scribed in Section 2.2. We create phonetic map-
995
pings manually for the 500 chat terms in each 
set. Then five phonetic mapping sets are ob-
tained. They are in turn compared against the 
standard phonetic mapping set constructed with 
Chinese Gigaword. Percentage of phonetic map-
pings in each set covered by the standard set is 
presented in Table 3.  
Set Jan-04 Jul-04 Jan-05 Jul-05 Jan-06
percentage 98.7 99.3 98.9 99.3 99.1 
Table 3. Percentages of phonetic mappings in 
each set covered by standard set.  
By comparing Table 1 and Table 3, we find 
that phonetic mappings remain more stable than 
character mappings in chat language text. This 
finding is convincing to justify our intention to 
design effective and robust chat language nor-
malization method by introducing phonetic map-
pings to the source channel model. Note that 
about 1% loss in these percentages comes from 
chat terms that are not formed via phonetic map-
pings, emoticons for example. 
4.3 Formalism 
The phonetic mapping model is a five-tuple, i.e. 
>< )|(Pr),(),(,, CTCptTptCT pm , 
which comprises of chat term character T , stan-
dard counterpart character C , phonetic transcrip-
tion of T  and C , i.e. )(Tpt  and )(Cpt , and the 
mapping probability )|(Pr CTpm  that T  is 
mapped to C  via the  phonetic mapping ( ) CT CTCptTpt pm ??????? ?? )|(Pr),(),(  (hereafter briefed by 
CT M??? ). 
As they manage mappings between any two 
Chinese characters, the phonetic mapping models 
should be constructed with a standard language 
corpus. This results in two advantages. One, 
sparse data problem can be addressed appropri-
ately because standard language corpus is used. 
Two, the phonetic mapping models are as stable 
as standard language. In chat term normalization, 
when the phonetic mapping models are used to 
represent mappings between chat term characters 
and standard counterpart characters, the dynamic 
problem can be addressed in a robust manner.   
Differently, the character mapping model used 
in the SCM (see Section 3.1) connects two Chi-
nese characters directly. It is a three-tuple, i.e.  
>< )|(Pr,, CTCT cm , 
which comprises of chat term character T , stan-
dard counterpart character C  and the mapping 
probability )|(Pr CTcm  that T  is mapped to C  
via this character mapping. As they must be con-
structed from chat language training samples, the 
character mapping models suffer from data 
sparseness problem and dynamic problem.  
4.4 Parameter Estimation 
Two questions should be answered in parameter 
estimation. First, how are the phonetic mapping 
space constructed? Second, how are the phonetic 
mapping probabilities estimated?  
To construct the phonetic mapping models, we 
first extract all Chinese characters from standard 
Chinese corpus and use them to form candidate 
character mapping models. Then we generate 
phonetic transcription for the Chinese characters 
and calculate phonetic probability for each can-
didate character mapping model. We exclude 
those character mapping models holding zero 
probability. Finally, the character mapping mod-
els are converted to phonetic mapping models 
with phonetic transcription and phonetic prob-
ability incorporated.  
The phonetic probability is calculated by 
combining phonetic similarity and character fre-
quencies in standard language as follows.  
( )
( )? ?
?=
i iislc
slc
pm
AApsAfr
AApsAfr
AAob
),()(
),()(
),(Pr    (2) 
In Equation (2) }{ iA  is the character set in 
which each element iA  is similar to character A  
in terms of phonetic transcription. )(cfrslc  is a 
function returning frequency of given character 
c  in standard language corpus and ),( 21 ccps  
phonetic similarity between character 1c  and 2c . 
Phonetic similarity between two Chinese char-
acters is calculated based on Chinese Pinyin as 
follows.  
)))(()),(((        
)))(()),(((       
))(),((),(
ApyfinalApyfinalSim
ApyinitialApyinitialSim
ApyApySimAAps
?
=
=
     (3) 
In Equation (3) )(cpy  is a function that returns 
Chinese Pinyin of given character c , and 
)(xinitial  and )(xfinal  return initial (shengmu) 
and final (yunmu) of given Chinese Pinyin x   
respectively. For example, Chinese Pinyin for the 
Chinese character ??? is ?zhe?, in which ?zh? is 
initial and ?e? is final. When initial or final is 
996
empty for some Chinese characters, we only cal-
culate similarity of the existing parts.  
An algorithm for calculating similarity of ini-
tial pairs and final pairs is proposed in (Li et al, 
2003) based on letter matching. Problem of this 
algorithm is that it always assigns zero similarity 
to those pairs containing no common letter. For 
example, initial similarity between ?ch? and ?q? 
is set to zero with this algorithm. But in fact, 
pronunciations of the two initials are very close 
to each other in Chinese speech. So non-zero 
similarity values should be assigned to these spe-
cial pairs before hand (e.g., similarity between 
?ch? and ?q? is set to 0.8). The similarity values 
are agreed by some native Chinese speakers. 
Thus Li et al?s algorithm is extended to output a 
pre-defined similarity value before letter match-
ing is executed in the original algorithm. For ex-
ample, Pinyin similarity between ?chi? and ?qi? 
is calculated as follows.  
8.018.0),(),()( =?=?= iiSimqchSimchi,qiSim  
5 Extended Source Channel Model 
We extend the source channel model by inserting 
phonetic mapping models niimM ,...,2,1}{ ==  into 
equation (1), in which chat term character it  is 
mapped to standard character ic  via im , i.e. 
i
m
i ct i??? . The extended source channel model 
(XSCM) is mathematically addressed as follows. 
)(
)()|(),|(
maxarg    
),|(maxarg?
,
,
Tp
CpCMpCMTp
TMCpC
MC
MC
=
=
   (4) 
Since )(Tp  is a constant, C?  and M?  should 
also maximize )()|(),|( CpCMpCMTp . Now 
three components are involved in XSCM, i.e. 
chat term normalization observation model 
),|( CMTp , phonetic mapping model )|( CMp  
and language model )(Cp . 
Chat Term Normalization Observation 
Model.  We assume that mappings between chat 
terms and their standard Chinese counterparts are 
independent of each other. Thus chat term nor-
malization probability can be calculated as fol-
lows. 
?= i iii cmtpCMTp ),|(),|(              (5) 
The ),|( iii cmtp ?s are estimated using maxi-
mum likelihood estimation method with Chinese 
character trigram model in NIL corpus.  
Phonetic Mapping Model. We assume that the 
phonetic mapping models depend merely on the 
current observation. Thus the phonetic mapping 
probability is calculated as follows. 
?= i ii cmpCMp )|()|(                 (6) 
in which )|( ii cmp ?s are estimated with equation 
(2) and (3) using a standard Chinese corpus.  
Language Model.  The language model )(Cp ?s 
can be estimated using maximum likelihood es-
timation method with Chinese character trigram 
model on NIL corpus.  
In our implementation, Katz Backoff smooth-
ing technique (Katz, 1987) is used to handle the 
sparse data problem, and Viterbi algorithm is 
employed to find the optimal solution in XSCM.   
6 Evaluation 
6.1 Data Description 
Training Sets 
Two types of training data are used in our ex-
periments. We use news from Xinhua News 
Agency in LDC Chinese Gigaword v.2 
(CNGIGA) (Graf et al, 2005) as standard Chi-
nese corpus to construct phonetic mapping mod-
els because of its excellent coverage of standard 
Simplified Chinese. We use NIL corpus (Xia et 
al., 2006b) as chat language corpus. To evaluate 
our methods on size-varying training data, six 
chat language corpora are created based on NIL 
corpus. We select 6056 sentences from NIL cor-
pus randomly to make the first chat language 
corpus, i.e. C#1. In every next corpus, we add 
extra 1,211 random sentences. So 7,267 sen-
tences are contained in C#2, 8,478 in C#3, 9,689 
in C#4, 10,200 in C#5, and 12,113 in C#6.  
Test Sets 
Test sets are used to prove that chat language is 
dynamic and XSCM is effective and robust in 
normalizing dynamic chat language terms. Six 
time-varying test sets, i.e. T#1 ~ T#6, are created 
in our experiments. They contain chat language 
sentences posted from August 2005 to Jan 2006. 
We randomly extract 1,000 chat language sen-
tences posted in each month. So timestamp of the 
six test sets are in temporal order, in which time-
stamp of T#1 is the earliest and that of T#6 the 
newest.  
The normalized sentences are created by hand 
and used as standard normalization answers. 
997
6.2 Evaluation Criteria 
We evaluate two tasks in our experiments, i.e. 
recognition and normalization. In recognition, 
we use precision (p), recall (r) and f-1 measure 
(f) defined as follows.  
 2        
rp
rpf
zx
xr
yx
xp +
??=+=+=      (7) 
where x denotes the number of true positives, y 
the false positives and z the true negatives.  
For normalization, we use accuracy (a), which 
is commonly accepted by machine translation 
researchers as a standard evaluation criterion. 
Every output of the normalization methods is 
compared to the standard answer so that nor-
malization accuracy on each test set is produced.  
6.3 Experiment I: SCM vs. XSCM Using  
Size-varying Chat Language Corpora 
In this experiment we investigate on quality of 
XSCM and SCM using same size-varying train-
ing data. We intend to prove that chat language is 
dynamic and phonetic mapping models used in 
XSCM are helpful in addressing the dynamic 
problem. As no standard Chinese corpus is used 
in this experiment, we use standard Chinese text 
in chat language corpora to construct phonetic 
mapping models in XSCM. This violates the ba-
sic assumption that the phonetic mapping models 
should be constructed with standard Chinese 
corpus. So results in this experiment should be 
used only for comparison purpose. It would be 
unfair to make any conclusion on general per-
formance of XSCM method based on results in 
this experiments.   
We train the two methods with each of the six 
chat language corpora, i.e. C#1 ~ C#6 and test 
them on six time-varying test sets, i.e. T#1 ~ T#6. 
F-1 measure values produced by SCM and 
XSCM in this experiment are present in Table 3.  
Three tendencies should be pointed out ac-
cording to Table 3. The first tendency is that f-1 
measure in both methods drops on time-varying 
test sets (see Figure 1) using same training chat 
language corpora. For example, both SCM and 
XSCM perform best on the earliest test set T#1 
and worst on newest T#4. We find that the qual-
ity drop is caused by the dynamic nature of chat 
language. It is thus revealed that chat language is 
indeed dynamic. We also find that quality of 
XSCM drops less than that of SCM. This proves 
that phonetic mapping models used in XSCM are 
helpful in addressing the dynamic problem. 
However, quality of XSCM in this experiment 
still drops by 0.05 on the six time-varying test 
sets. This is because chat language text corpus is 
used as standard language corpus to model the 
phonetic mappings. Phonetic mapping models 
constructed with chat language corpus are far 
from sufficient. We will investigate in Experi-
ment-II to prove that stable phonetic mapping 
models can be constructed with real standard 
language corpus, i.e. CNGIGA.  
Test Set T#1 T#2 T#3 T#4 T#5 T#6
C#1 0.829 0.805 0.762 0.701 0.739 0.705
C#2 0.831 0.807 0.767 0.711 0.745 0.715
C#3 0.834 0.811 0.774 0.722 0.751 0.722
C#4 0.835 0.814 0.779 0.729 0.753 0.729
C#5 0.838 0.816 0.784 0.737 0.761 0.737
S
C
M
C#6 0.839 0.819 0.789 0.743 0.765 0.743
C#1 0.849 0.840 0.820 0.790 0.805 0.790
C#2 0.850 0.841 0.824 0.798 0.809 0.796
C#3 0.850 0.843 0.824 0.797 0.815 0.800
C#4 0.851 0.844 0.829 0.805 0.819 0.805
C#5 0.852 0.846 0.833 0.811 0.823 0.811
X
S
C
M
C#6 0.854 0.849 0.837 0.816 0.827 0.816
Table 3. F-1 measure by SCM and XSCM on six 
test sets with six chat language corpora. 
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.87
0.89
0.91
T#1 T#2 T#3 T#4 T#5 T#6
SCM-C#1
SCM-C#2
SCM-C#3
SCM-C#4
SCM-C#5
SCM-C#6
XSCM-C#1
XSCM-C#2
XSCM-C#3
XSCM-C#4
XSCM-C#5
XSCM-C#6
 
Figure 1. Tendency on f-1 measure in SCM and 
XSCM on six test sets with six chat language 
corpora. 
The second tendency is f-1 measure of both 
methods on same test sets drops when trained 
with size-varying chat language corpora. For ex-
ample, both SCM and XSCM perform best on 
the largest training chat language corpus C#6 and 
worst on the smallest corpus C#1. This tendency 
reveals that both methods favor bigger training 
chat language corpus. So extending the chat lan-
guage corpus should be one choice to improve 
quality of chat language term normalization.  
The last tendency is found on quality gap be-
tween SCM and XSCM. We calculate f-1 meas-
ure gaps between two methods using same train-
ing sets on same test sets (see Figure 2). Then the 
tendency is made clear. Quality gap between 
SCM and XSCM becomes bigger when test set 
998
becomes newer. On the oldest test set T#1, the 
gap is smallest, while on the newest test set T#6, 
the gap reaches biggest value, i.e. around 0.09. 
This tendency reveals excellent capability of 
XSCM in addressing dynamic problem using the 
phonetic mapping models.  
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
T#1 T#2 T#3 T#4 T#5 T#6
C#1
C#2
C#3
C#4
C#5
C#6
 
Figure 2. Tendency on f-1 measure gap in SCM 
and XSCM on six test sets with six chat language 
corpora. 
6.4 Experiment II: SCM vs. XSCM Using  
Size-varying Chat Language Corpora 
and CNGIGA 
In this experiment we investigate on quality of 
SCM and XSCM when a real standard Chinese 
language corpus is incorporated. We want to 
prove that the dynamic problem can be addressed 
effectively and robustly when CNGIGA is used 
as standard Chinese corpus.  
We train the two methods on CNGIGA and 
each of the six chat language corpora, i.e. C#1 ~ 
C#6. We then test the two methods on six time-
varying test sets, i.e. T#1 ~ T#6. F-1 measure 
values produced by SCM and XSCM in this ex-
periment are present in Table 4. 
Test Set T#1 T#2 T#3 T#4 T#5 T#6
C#1 0.849 0.840 0.820 0.790 0.735 0.703
C#2 0.850 0.841 0.824 0.798 0.743 0.714
C#3 0.850 0.843 0.824 0.797 0.747 0.720
C#4 0.851 0.844 0.829 0.805 0.748 0.727
C#5 0.852 0.846 0.833 0.811 0.758 0.734
S 
C 
M 
C#6 0.854 0.849 0.837 0.816 0.763 0.740
C#1 0.880 0.878 0.883 0.878 0.881 0.878
C#2 0.883 0.883 0.888 0.882 0.884 0.880
C#3 0.885 0.885 0.890 0.884 0.887 0.883
C#4 0.890 0.888 0.893 0.888 0.893 0.887
C#5 0.893 0.892 0.897 0.892 0.897 0.892
X 
S 
C 
M 
C#6 0.898 0.896 0.900 0.897 0.901 0.896
Table 4. F-1 measure by SCM and XSCM on six 
test sets with six chat language corpora and 
CNGIGA. 
Three observations are conducted on our re-
sults. First, according to Table 4, f-1 measure of 
SCM with same training chat language corpora 
drops on time-varying test sets, but XSCM pro-
duces much better f-1 measure consistently using 
CNGIGA and same training chat language cor-
pora (see Figure 3). This proves that phonetic 
mapping models are helpful in XSCM method. 
The phonetic mapping models contribute in two 
aspects. On the one hand, they improve quality 
of chat term normalization on individual test sets. 
On the other hand, satisfactory robustness is 
achieved consistently.  
0.69
0.71
0.73
0.75
0.77
0.79
0.81
0.83
0.85
0.87
0.89
0.91
T#1 T#2 T#3 T#4 T#5 T#6
SCM-C#1
SCM-C#2
SCM-C#3
SCM-C#4
SCM-C#5
SCM-C#6
XSCM-C#1
XSCM-C#2
XSCM-C#3
XSCM-C#4
XSCM-C#5
XSCM-C#6
`
 
Figure 3. Tendency on f-1 measure in SCM and 
XSCM on six test sets with six chat language 
corpora and CNGIGA. 
The second observation is conducted on pho-
netic mapping models constructed with 
CNGIGA. We find that 4,056,766 phonetic map-
ping models are constructed in this experiment, 
while only 1,303,227 models are constructed 
with NIL corpus in Experiment I. This reveals 
that coverage of standard Chinese corpus is cru-
cial to phonetic mapping modeling. We then 
compare two character lists constructed with two 
corpora. The 100 characters most frequently used 
in NIL corpus are rather different from those ex-
tracted from CNGIGA. We can conclude that 
phonetic mapping models should be constructed 
with a sound corpus that can represent standard 
language.  
The last observation is conducted on f-1 meas-
ure achieved by same methods on same test sets 
using size-varying training chat language corpora. 
Both methods produce best f-1 measure with big-
gest training chat language corpus C#6 on same 
test sets. This again proves that bigger  training 
chat language corpus could be helpful to improve 
quality of chat language term normalization. One 
question might be asked whether quality of 
XSCM converges on size of the training chat 
language corpus. This question remains open due 
to limited chat language corpus available to us.  
6.5 Error Analysis 
Typical errors in our experiments belong mainly 
to the following two types.  
999
Err.1 Ambiguous chat terms 
Example-1: ??? 8?  
In this example, XSCM finds no chat term 
while the correct normalization answer is ???
??? (I still don?t understand)?. Error illus-
trated in Example-1 occurs when chat terms 
?8(eight, ba1)? and ??(meter, mi3)? appear in a 
chat sentence together. In chat language, ??? in 
some cases is used to replace ??(understand, 
ming2)?, while in other cases, it is used to repre-
sent a unit for length, i.e. meter. When number 
?8? appears before ???, it is difficult to tell 
whether they are chat terms within sentential 
context. In our experiments, 93 similar errors 
occurred. We believe this type of errors can be 
addressed within discoursal context.  
Err.2 Chat terms created in manners other 
than phonetic mapping 
Example-2: ?? ing    
In this example, XSCM does not recognize 
?ing? while the correct answer is ?(??)?? 
(I?m worrying)?. This is because chat terms cre-
ated in manners other than phonetic mapping are 
excluded by the phonetic assumption in XSCM 
method. Around 1% chat terms fall out of pho-
netic mapping types. Besides chat terms holding 
same form as showed in Example-2, we find that 
emoticon is another major exception type. Fortu-
nately, dictionary-based method is powerful 
enough to handle the exceptions. So, in a real 
system, the exceptions are handled by an extra 
component.  
7 Conclusions 
To address the sparse data problem and dynamic 
problem in Chinese chat text normalization, the 
phonetic mapping models are proposed in this 
paper to represent mappings between chat terms 
and standard words. Different from character 
mappings, the phonetic mappings are constructed 
from available standard Chinese corpus. We ex-
tend the source channel model by incorporating 
the phonetic mapping models. Three conclusions 
can be made according to our experiments. 
Firstly, XSCM outperforms SCM with same 
training data. Secondly, XSCM produces higher 
performance consistently on time-varying test 
sets.  Thirdly, both SCM and XSCM perform 
best with biggest training chat language corpus.  
Some questions remain open to us regarding 
optimal size of training chat language corpus in 
XSCM.  Does the optimal size exist? Then what 
is it? These questions will be addressed in our 
future work. Moreover, bigger context will be 
considered in chat term normalization, discourse 
for instance.  
Acknowledgement 
Research described in this paper is partially sup-
ported by the Chinese University of Hong Kong 
under the Direct Grant Scheme project 
(2050330) and Strategic Grant Scheme project 
(4410001). 
References 
Brown, P. F., J. Cocke, S. A. D. Pietra, V. J. D. Pietra, 
F. Jelinek, J. D. Lafferty, R. L. Mercer and P. S. 
Roossin. 1990.  A statistical approach to machine 
translation. Computational Linguistics, v.16 n.2, 
p.79-85. 
Gianforte, G.. 2003. From Call Center to Contact 
Center: How to Successfully Blend Phone, Email, 
Web and Chat to Deliver Great Service and Slash 
Costs. RightNow Technologies. 
Graf, D., K. Chen, J.Kong and K. Maeda. 2005. Chi-
nese Gigaword Second Edition. LDC Catalog 
Number LDC2005T14. 
Heard-White, M., Gunter Saunders and Anita Pincas. 
2004.  Report into the use of CHAT in education. 
Final report for project of Effective use of CHAT 
in Online Learning, Institute of Education, Univer-
sity of London. 
James, F.. 2000. Modified Kneser-Ney Smoothing of 
n-gram Models. RIACS Technical Report 00.07. 
Katz, S. M.. Estimation of probabilities from sparse 
data for the language model component of a speech 
recognizer. IEEE Transactions on Acoustics, 
Speech and Signal Processing, 35(3):400-401. 
Li, H., W. He and B. Yuan. 2003. An Kind of Chinese 
Text Strings' Similarity and its Application in 
Speech Recognition. Journal of Chinese Informa-
tion Processing, 2003 Vol.17 No.1 P.60-64.  
McCullagh, D.. 2004. Security officials to spy on chat 
rooms. News provided by CNET Networks. No-
vember 24, 2004. 
Xia, Y., K.-F. Wong and W. Gao. 2005. NIL is not 
Nothing: Recognition of Chinese Network Infor-
mal Language Expressions. 4th SIGHAN Work-
shop at IJCNLP'05, pp.95-102. 
Xia, Y. and K.-F. Wong. 2006a. Anomaly Detecting 
within Dynamic Chinese Chat Text. EACL?06 
NEW TEXT workshop, pp.48-55.  
Xia, Y., K.-F. Wong and W. Li. 2006b. Constructing 
A Chinese Chat Text Corpus with A Two-Stage 
Incremental Annotation Approach. LREC?06. 
1000
Anomaly Detecting within Dynamic Chinese Chat Text 
 
Yunqing Xia 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
yqxia@se.cuhk.edu.hk 
Kam-Fai Wong 
Department of S.E.E.M. 
The Chinese University of Hong Kong 
Shatin, Hong Kong 
kfwong@se.cuhk.edu.hk 
 
 
Abstract 
The problem in processing Chinese chat 
text originates from the anomalous char-
acteristics and dynamic nature of such a 
text genre. That is, it uses ill-edited terms 
and anomalous writing styles in chat text, 
and the anomaly is created and discarded 
very quickly. To handle this problem, 
one solution is to re-train the recognizer 
periodically. This costs a lot of man-
power in producing the timely chat text 
corpus. The new approaches are pro-
posed in this paper to detect the anomaly 
within dynamic Chinese chat text by in-
corporating standard Chinese corpora and 
chat corpus. We first model standard lan-
guage text using standard Chinese cor-
pora and apply these models to detect 
anomalous chat text. To improve detec-
tion quality, we construct anomalous chat 
language model using one static chat text 
corpus and incorporate this model into 
the standard language models. Our ap-
proaches calculate confidence and en-
tropy for the input text and apply thresh-
old values to help make the decisions. 
The experiments prove that performance 
equivalent to the best ones produced by 
the approaches in existence can be 
achieved stably with our approaches.  
1 Introduction 
Network Informal Language (NIL) refers to the 
special human language widely used in the 
community of network communication via plat-
forms such as chat rooms/tools, mobile phone 
short message services (SMS), bulletin board 
systems (BBS), emails, blogs, etc. NIL is ubiqui-
tous due in special to the rapid proliferation of 
Internet applications. As one important type of 
NIL text, chat text appears frequently within in-
creasing volume of chat logs of online education 
(Heard-White, 2004) and customer relationship 
management (Gianforte, 2003) via chat 
rooms/tools. In wed-based chat rooms and BBS a 
large volume of NIL text is abused by (McCul-
lagh, 2004). A survey by the Global System for 
Mobile Communication (GSM) showed that 
Germans send 200 million messages a year 
(German News, 2004). All the facts disclose the 
growing importance in processing NIL text.  
Chat text holds anomalous characteristics in 
forming non-alphabetical characters, words, and 
phrases. It uses ill-edited terms and anomalous 
writing styles. Typical examples of anomalous 
Chinese chat terms can be found in (Xia et. al., 
2005a). Besides the anomalous characteristics, 
our observations reveal remarkable dynamic na-
ture of the chat text. The anomaly is created and 
discarded very quickly. Although there is no idea 
how tomorrow?s chat text would look like, the 
changing will never stop. Instead, the changing 
gets faster and faster.  
The challenging issues originates from the dy-
namic nature are two-fold. On the one hand, 
anomalous chat terms and writing styles are fre-
quently found in chat text. Knowledge about chat 
text is urgently required to understand the anom-
aly. On the other hand, the dynamic nature of the 
chat text makes it nearly impossible to maintain a 
timely chat text knowledge base. This claim has 
been proved by (Xia et. al., 2005a) in which ex-
periments are conducted with an SVM classifier. 
The classifier is trained on chat text created in an 
earlier period and tested on chat text created in a 
later period. In their experiments, performance of 
the SVM classifier becomes lower when the two 
periods are farther. This reveals that chat text is 
written in such a style that changes constantly 
along with time. A straightforward solution to 
this problem is to re-train the SVM classifier pe-
riodically with timely chat text collections. Un-
fortunately, this solution costs a lot of manpower 
in producing new chat text corpora. The super-
48
vised learning technique becomes ineffective in 
processing chat text.  
This paper proposes approaches to detecting 
anomaly in dynamic Chinese chat text by incor-
porating standard Chinese corpora and a static 
chat corpus. The idea is basically error-driven. 
That is, we first create standard language models 
using trigram on standard Chinese corpora. 
These corpora provide negative training samples. 
We then construct anomalous chat language 
model using one static chat text corpus which 
provides positive training samples. We incorpo-
rate the chat language model with the standard 
language models and calculate confidence and 
entropy to help make decisions whether input 
text is anomalous chat text. We investigate two 
types of trigram, i.e. word trigram and part-of-
speech (POS) tag trigram in this work.  
The remaining sections of this paper are or-
ganized as follow.  In Section 2, the works re-
lated to this paper are addressed. In Section 3, 
approaches of anomaly detection in dynamic 
Chinese chat text with standard Chinese corpora 
are presented. In Section 4, we incorporate the 
NIL corpus into our approaches. In section 5, 
experiments are described to estimate threshold 
values and to evaluate performance of the two 
approaches with various configurations. Com-
parisons and discussions are also reported. We 
conclude this paper and address future works in 
Section 6.  
2 Related Works 
Some works had been carried out in (Xia et. al., 
2005a) in which an SVM classifier is imple-
mented to recognize anomalous chat text terms. 
A within-domain open test is conducted on chat 
text posted in March 2005. The SVM classifier is 
trained on five training sets which contain chat 
text posted from December 2004 to February 
2005. The experiments show that performance of 
the SVM classifier increases when the training 
period and test period are closer. This reveals 
that chat text is written in a style that changes 
quickly with time. Many anomalous popular chat 
terms in last year are forgotten today and new 
ones replace them. This makes SVM based pat-
tern learning technique ineffective to reflect the 
changes.  
The solution to this problem in (Xia et. al., 
2005b) is to re-train the SVM classifier periodi-
cally. This costs a lot of manpower in producing 
the timely chat text corpora, in which each piece 
of anomalous chat text should be annotated with 
several attributes manually.  
We argue that the anomalous chat text can be 
identified using negative training samples in 
static Chinese corpora. Our proposal is that we 
model the standard natural language using stan-
dard Chinese corpora. We incorporate a static 
chat text corpus to provide positive training sam-
ples to reflect fundamental characteristics of 
anomalous chat text. We then apply the models 
to detect the anomalous chat text by calculating 
confidence and entropy.  
Regarding the approaches proposed in this pa-
per, our arguments are, 1) the approaches can 
achieve performance equivalent to the best ones 
produced by the approaches in existence; and 2) 
the good performance can be achieved stably. 
We prove these arguments in the following sec-
tions.  
3 Anomaly Detection with Standard 
Chinese Corpora 
Chat text exhibits anomalous characteristics in 
using or forming words. We argue that the 
anomalous chat text, which is referred as anom-
aly in this article, can be identified with language 
models constructed on standard Chinese corpora 
with some statistical language modeling (SLM) 
techniques, e.g. trigram model. 
The problem of anomaly detection can be ad-
dressed as follows. Given a piece of anomalous 
chat text, i.e. },...,,{ 21 nwwwW = , and a language 
model )}({ xpLM = , we attempt to recognize W  
as anomaly by the language model. We propose 
two approaches to tackle this problem. We de-
sign a confidence-based approach to calculate 
how likely that W  fits into the language model. 
Another approach is designed based on entropy 
calculation. Entropy method was originally pro-
posed to estimate how good a language model is. 
In our work we apply this method to estimate 
how much the constructed language models are 
able to reflect the corpora properly based on the 
assumption that the corpora are sound and com-
plete.  
Although there exist numerous statistical 
methods to construct a natural language model, 
the objective of them is one: to construct a prob-
abilistic distribution model )(xp  which fits to the 
most extent into the observed language data in 
the corpus. We implement the trigram model and 
create language models with three Chinese cor-
pora, i.e. People?s Daily corpus, Chinese  Giga-
word and Chinese Pen Treebank. We investigate 
49
quality of the language models produced with 
these corpora. 
3.1 The N-gram Language Models 
N-gram model is the most widely used in statisti-
cal language modeling nowadays. Without loss 
of generality we express the probability of a 
word sequence },...,{ 1 nwwW =  of n words, i.e. 
)(Wp  as  
?
=
?==
n
i
iin wwwwpwwpWp
1
1101 ),...,,|(),...,()(  
(1) 
where 0w  is chosen appropriately to handle the 
initial condition. The probability of the next 
word iw  depends on the history ih  of words 
that have been given so far. With this factoriza-
tion the complexity of the model grows exponen-
tially with the length of the history. 
One of the most successful models of the past 
two decades is the trigram model (n=3) where 
only the most recent two words of the history are 
used to condition the probability of the next 
word. 
Instead of using the actual words, one can use 
a set of word classes. Classes based on the POS 
tags, or the morphological analysis of words, or 
the semantic information have been tried. Also, 
automatically derived classes based on some sta-
tistical models of co-occurrence have been tried 
(Brown et. al., 1990). The class model can be 
generally described as  
?
=
??=
n
i
iiiii cccpcwpWp
1
12 ),|()|()(       (2) 
if the classes are non-overlapping. These tri-class 
models have had higher perplexities than the cor-
responding trigram model. However, they have 
led to a reduction in perplexity when linearly 
combined with the trigram model. 
3.2 The Confidence-based Approach 
Given a piece of chat text },...,,{ 21 nwwwW =  
where each word iw  is obtained with a standard 
Chinese word segmentation tool, e.g. ICTCLAS. 
As ICTCLAS is a segmentation tool based on 
standard vocabulary, it means that some un-
known chat terms (e.g., ????) would be broken 
into several element Chinese words (i.e., ??? 
and ??? in the above case). This does not hurt 
the algorithm because we use trigram in this 
method. A chat term may produce some anoma-
lous word trigrams which are evidences for 
anomaly detection.  
We use non-zero probability for each trigram 
in this calculation. This is very simple but na?ve. 
The calculation seeks to produce a so-called con-
fidence, which reflects how much the given text 
fits into the training corpus in arranging its ele-
ment Chinese words. This is enlightened by the 
observation that the chat terms use element  
words in anomalous manners which can not be 
simulated by the training corpus.  
The confidence-based value is defined as  
( ) KK
i i
TCWC
1
1
)( ??
???
?= ?=                  (3) 
where K denotes the number of trigrams in chat 
text W  and iT  is the i-th order trigram. ( )iTC  is 
confidence of trigram iT . Generally ( )iTC  is as-
signed probability of the trigram iT  in training 
corpus, i.e. ( )iTp . When a trigram is missing, 
linear interpolation is applied to estimate its 
probability.  
We empirically setup a confidence threshold 
value to determine whether the input text con-
tains chat terms, namely, it is a piece of chat text. 
The input is concluded to be stand text if its con-
fidence is bigger than the confidence threshold 
value. Otherwise, the input is concluded to be 
chat text. The confidence threshold value can be 
estimated with a training chat text collection.  
3.3 The Entropy-based Approach 
The idea beneath this approach comes from en-
tropy based language modeling. Given a lan-
guage model, one can use the quantity of entropy 
to get an estimation of how good the language 
model (LM) might be. Denote by p the true dis-
tribution, which is unknown to us, of a segment 
of new text x of k words. Then the entropy on a 
per word basis is defined as 
??= ?>? xk xpxpkH )(ln)(1lim              (4) 
If every word in a vocabulary of size |V| is 
equally likely then the entropy would be 
||log 2 V ; ||ln VH ?  for other distributions of 
the words.  
Enlightened by the estimation method, we 
compute the entropy-based value on a per tri-
gram basis for the input chat text. Given a stan-
dard LM denoted by p~  which is modeled  by 
trigram, the entropy-value is calculate as 
50
?
=
?=
K
i
iiK TpTpK
H
1
)(~ln)(~1~             (5) 
where K denotes number of trigrams the input 
text contains. Our goal is to find how much dif-
ference the input text is compared against the 
LM. Obviously, bigger entropy discloses a piece 
of more anomalous chat text. An empirical en-
tropy threshold is again estimated on a training 
chat text collection. The input is concluded to be 
stand text if its entropy is smaller than the en-
tropy threshold value. Otherwise, the input is 
concluded to be chat text. 
4 Incorporating the Chat Text Corpus 
We argue performance of the approaches can be 
improved when an initial static chat text corpus 
is incorporated. The chat text corpus provides 
some basic forms of the anomalous chat text. 
These forms we observe provide valuable heuris-
tics in the trigram models. Within the chat text 
corpus, we only consider the word trigrams and 
POS tag trigrams in which anomalous chat text 
appears. We thus construct two trigram lists. 
Probabilities are produced for each trigram ac-
cording to its occurrence. One chat text example 
EXP1 is given below.  
EXP1: ?????????? 
SEG1: ? ? ?? ? ?? ? ? ? 
SEG1 presents the word segments produced 
by ICTCLAS. We generate chat text word tri-
grams based on SEG1 as follow. 
TRIGRAM1:   (1)/? ? ??/ 
            (2)/? ?? ?/ 
            (3)/?? ? ?/ 
            (4)/? ? ?/ 
For each input trigram iT , if it appears in the 
chat text corpus, we adjust the confidence and 
entropy values by incorporating its probability in 
chat text corpus.  
4.1 The Refined Confidence 
For each ( )iTC , we assign a weight i? , which is 
calculated as  
)()( icin TpTp
i e ?=?                      (6) 
where )( in Tp  is probability of the trigram iT  in 
standard corpus and )( ic Tp  probability in chat 
text corpus. Equation (3) therefore is re-written 
as  
( )
( ) KK
i in
TpTp
KK
i ii
Tpe
TCWC
icin
1
1
)()(
1
1
'
          
)(
??
???
?=
??
???
?=
?
?
=
?
= ?
         (7) 
The intention of inserting i?  into confidence 
calculation is to decrease confidence of input 
chat text when chat text trigrams are found. 
Normally, when a trigram iT  is found in chat text 
trigram lists, )( in Tp  will be much lower than 
)( ic Tp ; therefore i?  will be much lower than 1 . 
By multiplying such a weight, confidence of in-
put chat text can be decreased so that the text can 
be easily detected.  
4.2 The Refined Entropy 
Instead of assigning a weight, we introduce the 
entropy-based value of the input chat text on the 
chat text corpus, i.e. cKH
~ , to produce a new equa-
tion. We denote nKH
~  the entropy calculated with 
equation (5).  Similar to nKH
~ , cKH
~  is calculated 
with equation (8).  
?
=
?=
K
i
icic
c
K TpTpK
H
1
)(~ln)(~1~              (8) 
We therefore re-write the entropy-based value 
calculation as follows.  
( )?
=
+?=
+=
K
i
icicinin
c
K
n
KK
TpTpTpTp
K
HHH
1
)(~ln)(~)(~ln)(~1
~~~
  (9) 
The intention of introducing cKH
~  in entropy 
calculation is to increase the entropy of input 
chat text when chat text trigrams are found. It 
can be easily proved that KH
~  is never smaller 
than nKH . As bigger entropy discloses a piece of 
more anomalous chat text, we believe more 
anomalous chat texts can be correctly detected 
with equation (9).  
5 Evaluations 
Three experiments are conducted in this work. 
The first experiment aims to estimate threshold 
values from a real text collection. The remaining 
experiments seek to evaluate performance of the 
approaches with various configurations.   
5.1 Data Description 
We use two types of text corpora to train our ap-
proaches in the experiments. The first type is 
51
standard Chinese corpus which is used to con-
struct standard language models. We use Peo-
ple?s Daily corpus, also know as Peking Univer-
sity Corpus (PKU), the Chinese Gigaword 
(CNGIGA) and the Chinese Penn Treebank 
(CNTB) in this work. Considering coverage, 
CNGIGA is the most excellent one. However, 
PKU and CPT provide more syntactic informa-
tion in their annotations. Another type of training 
corpus is chat text corpus. We use NIL corpus 
described in (Xia et. al., 2005b). In NIL corpus 
each anomalous chat text is annotated with their 
attributes.  
We create four test sets in our experiments. 
We use the test set #1 to estimate the threshold 
values of confidence and entropy for our ap-
proaches. The values are estimated on two types 
of trigrams in three corpora. Test set #1 contains 
89 pieces of typical Chinese chat text selected 
from the NIL corpus and 49 pieces of standard 
Chinese sentences selected from online Chinese 
news by hand. There is no special consideration 
that we select different number of chat texts and 
standard sentences in this test set. 
The remaining three test sets are used to com-
pare performance of our approaches on test data 
created in different time periods. The test set #2 
is the earliest one and #4 the latest one according 
to their time stamp. There are 10K sentences in 
total in test set #2, #3 and #4. In this collection, 
chat texts are selected from YESKY BBS system 
(http://bbs.yesky.com/bbs/) which cover BBS 
text in March and April 2005 (later than the chat 
text in the NIL corpus), and standard texts are 
extracted from online Chinese news randomly. 
We describe the four test sets in Table 1. 
Test set # of standard sentences 
# of chat 
sentences 
#1 49 89 
#2 1013 2320 
#3 1013 2320 
#4 1014 2320 
Table 1: Number of sentences in the four test 
sets. 
5.2 Experiment I: Threshold Values Esti-
mation 
5.2.1 Experiment Description 
This experiment seeks to estimate the threshold 
values of confidence and entropy for two types 
of trigrams in three Chinese corpora.  
We first run the two approaches using only 
standard Chinese corpora on the 138 sentences in 
the first test set. We put the calculated values 
(confidence or entropy) into two arrays. Note 
that we already know type of each sentence in 
the first test set. So we are able to select in each 
array a value that produces the lowest error rate. 
In this way we obtain the first group of threshold 
values for our approaches.  
We incorporate the NIL corpus to the two ap-
proaches and run them again. We then produce 
the second group of threshold values in the same 
way to produce the first group of values.  
5.2.2 Results 
The selected threshold values and corresponding 
error rates are presented in Table 2~5.  
Trigram option Threshold Err rate 
word of CNGIGA 1.58E-07 0.092 
word of PKU 7.06E-07 0.098 
word of CNTB 2.09E-06 0.085 
POS tag of CNGIGA 0.0278 0.248 
POS tag of PKU 0.0143 0.263 
POS tag of CNTB 0.0235 0.255 
Table 2: Selected threshold values of confidence 
for the approach using standard Chinese corpora 
and error rates.  
Trigram option Threshold Err rate 
word of CNGIGA 3.762E-056 0.099 
word of PKU 5.683E-048 0.112 
word of CNTB 2.167E-037 0.169 
POS tag of CNGIGA 0.00295 0.234 
POS tag of PKU 0.00150 0.253 
POS tag of CNTB 0.00239 0.299 
Table 3: Selected threshold values of entropy for 
the approach using standard Chinese corpora and 
error rates. 
Trigram option Threshold Err rate 
word of CNGIGA 4.26E-05 0.089 
word of PKU 3.75E-05 0.102 
word of CNTB 6.85E-05 0.092 
POS tag of CNGIGA 0.0398 0.257 
POS tag of PKU 0.0354 0.266 
POS tag of CNTB 0.0451 0.249 
Table 4: Selected threshold values of confidence 
for the approach incorporating the NIL corpus 
and error rates.  
Trigram option Threshold Err rate 
word of CNGIGA 8.368E-027 0.102 
word of PKU 3.134E-019 0.096 
word of CNTB 5.528E-021 0.172 
POS tag of CNGIGA 0.00465 0.241 
POS tag of PKU 0.00341 0.251 
POS tag of CNTB 0.00532 0.282 
Table 5: Selected thresholds values of entropy 
for the approach incorporating the NIL corpus 
and error rates. 
52
We use the selected threshold values in ex-
periment II and III to detect anomalous chat text 
within test set #2, #3 and #4. 
5.3 Experiment II: Anomaly Detection with 
Three Standard Chinese Corpora 
5.3.1 Experiment Description 
In this experiment, we run the two approaches 
using the standard Chinese corpora on test set #2. 
The threshold values estimated in experiment I 
are applied to help make decisions.  
Input text can be detected as either standard 
text or chat text. But we are only interested in 
how correctly the anomalous chat text is de-
tected. Thus we calculate precision (p), recall (r) 
and 1F  measure (f) only for chat text.  
 2        
rp
rpf
ba
ar
ca
ap +
??=+=+=     (10) 
where a is the number of true positives, b the 
false negatives and c the false positives.  
5.3.2 Results 
The experiment results for the approaches using 
the standard Chinese corpora on test set #2 are 
presented in Table 6. 
5.3.3 Discussions 
Table 4 shows that, in most cases, the entropy-
based approach outperforms the confidence-
based approach slightly. It can thus be conclude 
that the entropy-based approach is more effective 
in anomaly detection.  
It is also revealed that both approaches per-
form better with word trigrams than that with 
POS tag trigrams. This is natural for class based 
trigram model when number of class is small. 
Thirty-nine classes are used in ICTCLAS in POS 
tagging Chinese words.  
When the three Chinese corpora are compared, 
the CNGIGA performs best in the confidence-
based approach with word trigram model. How-
ever, it is not the case with POS tag trigram 
model. Results of two approaches on CNTB are 
best amongst the three corpora. Although we are 
able to draw the conclusion that  bigger corpora 
yields better performance with word trigram, the 
same conclusion, however, does not work for 
POS tag trigram. This is very interesting. The 
reason we can address on this issue is that CNTB 
probably provides highest quality POS tag tri-
grams and other corpora contain more noisy POS 
tag trigrams, which eventually decreases the per-
formance. An observation on word/POS tag lists 
for three Chinese corpora verifies such a claim. 
Text in CNTB is best-edited amongst the three.  
5.4 Experiment III: Anomaly Detection 
with NIL Corpus Incorporated 
5.4.1 Experiment Description 
In this experiment, we incorporate one chat text 
corpus, i.e. NIL corpus, to the two approaches. 
We run them on test set #2, #3 and #4 with the 
estimated threshold values. We use precision, 
recall and 1F  measure again to evaluate perform-
ance of the two approaches. 
5.4.2 Results 
The experiment results are presented in Table 7~ 
Table 9 on test set #2,  #3 and #4 respectively. 
5.4.3 Discussions 
We first compare the two approaches with dif-
ferent running configurations. All conclusions 
made in experiment II still work for experiment 
III. They are, i) the entropy-based approach out-
performs the confidence-based approach slightly 
in most cases; ii) both approach perform better 
with word trigram than POS tag trigram; iii) both 
approaches perform best on CNGIGA with word 
trigram model. But with POS tag trigram model, 
CNTB produces the best results.  
An interesting comparison is conducted on 1F  
measure between the approaches in experiment II 
and experiment III on test set #2 in Figure 1 (the 
left two columns). Generally, 1F  measure of 
anomaly detection with both approaches with 
word trigram model is improved when the NIL 
corpus is incorporated. It is revealed in Table 
7~9 that same observation is found with POS tag 
trigram model.  
We compare 1F  measure of the approaches 
with word trigram model in experiment III on 
test set #2, #3 and #4 in Figure 1 (the right three 
columns). The graph in Figure 1 shows that 1F  
measure on three test sets are very close to each 
other. This is also true the approaches with POS 
tag trigram model as showed in Table 7~9. This 
provides evidences for the argument that the ap-
proaches can produce stable performance with 
the NIL corpus. Differently, as reported in (Xia 
et. al., 2005a), performance achieved in SVM 
classifier is rather unstable. It performs poorly 
with training set C#1 which contains BBS text 
posted several months ago, but much better with 
training set C#5 which contains the latest chat 
text.  
53
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.685 0.737 0.710 0.722 0.761 0.741 0.614 0.654 0.633 0.637 0.664 0.650 
PKU 0.699 0.712 0.705 0.701 0.738 0.719 0.619 0.630 0.624 0.625 0.648 0.636 
CNTB 0.653 0.661 0.657 0.692 0.703 0.697 0.651 0.673 0.662 0.684 0.679 0.681 
Table 6: Results of anomaly detection using standard Chinese corpora on test set #2.  
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.821 0.836 0.828 0.857 0.849 0.853 0.653 0.657 0.655 0.672 0.678 0.675 
PKU 0.818 0.821 0.819 0.838 0.839 0.838 0.672 0.672 0.672 0.688 0.679 0.683 
CNTB 0.791 0.787 0.789 0.821 0.811 0.816 0.691 0.679 0.685 0.712 0.688 0.700 
Table 7: Results of anomaly detection incorporating NIL corpus on test set #2 
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.819 0.841 0.830 0.849 0.848 0.848 0.657 0.659 0.658 0.671 0.677 0.674 
PKU 0.812 0.822 0.817 0.835 0.835 0.835 0.663 0.671 0.667 0.687 0.681 0.684 
CNTB 0.801 0.783 0.792 0.822 0.803 0.812 0.689 0.677 0.683 0.717 0.689 0.703 
Table 8: Results of anomaly detection incorporating NIL corpus on test set #3 
Word trigram POS tag trigram 
confidence entropy confidence entropy Corpus 
p r f p r f p r f p r f 
CNGIGA 0.824 0.839 0.831 0.852 0.845 0.848 0.651 0.654 0.652 0.674 0.674 0.674 
PKU 0.815 0.825 0.820 0.836 0.84 0.838 0.668 0.668 0.668 0.692 0.682 0.687 
CNTB 0.796 0.785 0.790 0.817 0.807 0.812 0.694 0.681 0.687 0.713 0.686 0.699 
Table 9: Results of anomaly detection incorporating NIL corpus on test set #4 
0.00
0.10
0.20
0.30
0.40
0.50
0.60
0.70
0.80
0.90
conf-CNGIGA-
word
ent-CNGIGA-
word
conf-PKU-word ent-PKU-word conf-CNTB-word ent-CNTB-word
Exp-II-#2
Exp-III-#2
Exp-III-#3
Exp-III-#4
 
Figure 1: Comparisons on 1F  measure of the approaches with word trigram on test set #2, #3 and #4 in 
experiment II and experiment III. 
 
We finally compare performance of our ap-
proaches against the one described in (Xia, et. 
al., 2005a). The best 1F  measure achieved in our 
work, i.e. 0. 853, is close to the best one in their 
work, i.e. 0.871 with training corpus C#5. This 
proves another argument that our approaches can 
produce equivalent performance to the best ones 
achieved by the approaches in existence. 
6 Conclusions  
The new approaches to detecting anomalous 
Chinese chat text are proposed in this paper. The 
approaches calculate confidence and entropy 
values with the language models constructed on 
negative training samples in three standard Chi-
54
nese corpora. To improve detection quality, we 
incorporate positive training samples in NIL cor-
pus in our approaches. Two conclusions can be 
made based on this work. Firstly, 1F  measure of 
anomaly detection can be improved by around 
0.10 when NIL corpus is incorporated into the 
approaches. Secondly, performance equivalent to 
the best ones produced by the approaches in exis-
tence can be achieved stably by incorporating the 
standard Chinese corpora and the NIL corpus.  
We believe some strong evidences for our 
claims can be obtained by training our ap-
proaches with more chat text corpora which con-
tain chat text created in different time periods. 
We are conducting this experiment seeks to find 
out whether and how our approaches are inde-
pendent of time. This work is still progressing. A 
report on this issue will be available shortly. We 
also plan to investigate how size of chat text cor-
pus influences performance of our approaches. 
The goal is to find the optimal size of chat text 
corpus which can achieve the best performance. 
The readers should also be noted that evaluation 
in this work is a within-domain test. Due to 
shortage of chat text resources, no cross-domain 
test is conducted. In the future cross-domain test, 
we will investigate how our approaches are inde-
pendent of domain.  
Eventual goal of chat text processing is to nor-
malize the anomalous chat text, namely, convert 
it to standard text holding the same meaning. So 
the work carried out in this paper is the first step 
leading to this goal. Approaches will be designed 
to locate the anomalous terms in chat text and 
map them to standard words.  
Acknowledgement 
Research described in this paper is partially sup-
ported by the Chinese University of Hong Kong 
under the Direct Grant Scheme (No: 2050330) 
and Strategic Grant Scheme project (No: 
4410001) respectively. 
Reference 
Brown, P. F., V. J. Della Pietra, P. V. de Souza, J. C. 
Lai, and R. L. Mercer. 1990. Class-based n-gram 
models of natural language. In Proceedings of the 
IBM Natural Language ITL, Paris, France. 
Finkelhor, D., K. J. Mitchell, and J. Wolak. 2000. 
Online Victimization: A Report on the Nation's 
Youth. Alexandria, Virginia: National Center for 
Missing & Ex-ploited Children, page ix. 
German News. 2004.  Germans are world SMS cham-
pions, 8 April 2004, http://www.expatica.com/ 
source/site_article.asp?subchannel_id=52&story_i
d=6469.  
Gianforte, G.. 2003. From Call Center to Contact 
Center: How to Successfully Blend Phone, Email, 
Web and Chat to Deliver Great Service and Slash 
Costs. RightNow Technologies.  
Heard-White, M., Gunter Saunders and Anita Pincas. 
2004. Report into the use of CHAT in education. 
Final report for project of Effective use of CHAT 
in Online Learning, Institute of Education, Univer-
sity of London. 
McCullagh, D.. 2004. Security officials to spy on chat 
rooms. News provided by CNET Networks. No-
vember 24, 2004.  
Xia, Y., K.-F. Wong and W. Gao. 2005a. NIL is not 
Nothing: Recognition of Chinese Network Infor-
mal Language Expressions, 4th SIGHAN Work-
shop on Chinese Language Processing at 
IJCNLP'05, pp95-102. 
Xia, Y., K.-F. Wong and R. Luk. 2005b. A Two-Stage 
Incremental Annotation Approach to Constructing 
A Network Informal Language Corpus. In Proc. of 
NTCIR-5 Meeting, pp. 529-536.  
Zhang, Z., H. Yu, D. Xiong and Q. Liu. 2003. HMM-
based Chinese Lexical Analyzer ICTCLAS. 
SIGHAN?03 within ACL?03, pp. 184-187. 
 
 
55
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 97?102,
Baltimore, Maryland USA, June 23-24, 2014. c?2014 Association for Computational Linguistics
Web Information Mining and Decision Support Platform for the  
Modern Service Industry 
 
Binyang Li1,2, Lanjun Zhou2,3, Zhongyu Wei2,3, Kam-fai Wong2,3,4,  
Ruifeng Xu5, Yunqing Xia6 
 
1 Dept. of Information Science & Technology, University of International Relations, China 
2Dept. of Systems Engineering & Engineering Management, The Chinese University of Hong 
Kong, Shatin, N.T., Hong Kong  
3MoE Key Laboratory of High Confidence Software Technologies, China 
4 Shenzhen Research Institute, The Chinese University of Hong Kong 
5Harbin Institute of Technology Shenzhen Graduate School, Shenzhen, China 
6Department of Computer Science & Technology, TNList, Tsinghua University, China 
{byli,ljzhou,zywei,kfwong}@se.cuhk.edu.hk  
 
 
Abstract 
This demonstration presents an intelligent infor-
mation platform MODEST. MODEST will pro-
vide enterprises with the services of retrieving 
news from websites, extracting commercial in-
formation, exploring customers? opinions, and 
analyzing collaborative/competitive social net-
works. In this way, enterprises can improve the 
competitive abilities and facilitate potential col-
laboration activities. At the meanwhile, MOD-
EST can also help governments to acquire in-
formation about one single company or the entire 
board timely, and make prompt strategies for 
better support. Currently, MODEST is applied to 
the pillar industries of Hong Kong, including 
innovative finance, modem logistics, information 
technology, etc. 
1 Introduction 
With the rapid development of Web 2.0, the 
amount of information is exploding. There are 
millions of events towards companies and bil-
lions of opinions on products generated every 
day (Liu, 2012). Such enormous information 
cannot only facilitate companies to improve their 
competitive abilities, but also help government to 
make prompt decisions for better support or 
timely monitor, e.g. effective risk management. 
For this reason, there is a growing demand of 
Web information mining and intelligent decision 
support services for the industries. Such services 
are collectively referred as modern service, 
which includes the following requirements: 
(1) To efficiently retrieve relevant information 
from the websites; 
(2) To accurately determine the latest business 
news and trends of the company; 
(3) To identify and analyze customers? opinions 
towards the company; 
(4) To explore the collaborative and competitive 
relationship with other companies; 
(5) To leverage the knowledge mined from the 
business news and company social network 
for decision support. 
In this demonstration, we will present a Web 
information mining and decision support plat-
form, MODEST1. The objective of MODEST is 
to provide modern services for both enterprises 
and government, including collecting Web in-
formation, making deep analysis, and providing 
supporting decision. The innovation of MOD-
EST is focusing on deep analysis which incor-
porates the following functions: 
? Topic detection and tracking function is to 
cluster the hot events and capture the rela-
tionship between the relevant events based on 
the collected data from websites (event also 
referred as topic in this paper). In order to re-
alize this function, Web mining techniques 
are adopted, e.g. topic clustering, heuristics 
algorithms, etc. 
? The second function is to identify and analyze 
customers? opinions about the company. 
Opinion mining technology (Zhou et al., 2010) 
is adopted to determine the polarity of those 
news, which can help the company timely and 
appropriately adjust the policy to strengthen 
the dominant position or avoid risks. 
                                                          
1 This work is supported by the Innovation and Technology 
Fund of Hong Kong SAR. 
97
? The third function is to explore and analyze 
social network based on the company centric. 
We utilize social network analysis (SNA) 
technology (Xia et al., 2010) to discover the 
relationships, and we further analyze the con-
tent in fine-grained granularity to identify its 
potential partners or competitors.  
With the help of MODEST, the companies can 
acquire modern service-related information, and 
timely adjust corporate policies and marketing 
plan ahead. Hence, the ability of information ac-
quisition and the competitiveness of the enter-
prises can be improved accordingly. 
In this paper, we will use a practical example 
to illustrate our platform and evaluate the per-
formance of main functions.  
The rest of this paper is organized as follows. 
Section 2 will introduce the system description 
as well as the main functions implementation. 
The practical case study will be illustrated in 
Section 3. The performance of MODEST will be 
evaluated in Section 4.Finally, this paper will be 
concluded in Section 5. 
2 System Description 
In this section, we first outline the system archi-
tecture of MODEST, and then describe the im-
plementation of the main functionality in detail.  
2.1 Architecture and Workflow 
The MODEST system consists of three modules: 
data acquisition, data analysis, and result display. 
The system architecture is shown in Figure 1. 
 
Figure 1: System architecture. (The module in 
blue is data acquisition, the module in orange is 
data analysis, and the module in light green is 
result display) 
(1) The core technique in the data acquisition 
module is the crawler, which is developed to 
collect raw data from websites, e.g. news portals, 
blogosphere. Then the system parse the raw web 
pages and extract information to store in the local 
database for further processing. 
(2) The data analysis module can be divided into 
two parts:  
? NLP pre-processor: utilizes NLP (natural 
language processing) techniques and some 
toolkits to perform the pre-processing on the 
raw data in (1), including word segmenta-
tion, part-of-speech (POS) tagging1, stop-
word removal, and named entity recognition 
(NER)2. We then create knowledgebase for 
individual industry, such as domain-specific 
sentiment word lexicon, name entity collec-
tion, and so on. 
? Miner?makes use of data mining techniques 
to realize four functions, topic detection and 
tracking (TDT), multi-document summari-
zation 3  (MDS), social network analysis 
(SNA), and opinion mining (OM). The re-
sults of data analysis are also stored in the 
database.  
(3) The result display module read out the analy-
sis results from the database and display them to 
users in the form of plain text, charts, figures, as 
well as video. 
2.2 Function Implementation 
Since the innovation of MODEST is focusing on 
the module of data analysis, we will describe its 
main functions in detail, including topic detec-
tion and tracking, opinion mining, and social 
networks analysis. 
2.2.1 Topic Detection and Tracking 
The TDT function targets on detecting and 
tracking the hot topics for each individual com-
pany. Given a period of data collected from web-
sites, there are various discussions about the 
company. In order to extract these topics, clus-
tering methods (Viermetz et al., 2007 and Yoon 
et al., 2009) are implemented to explore the top-
ics. Note that during the period of data collection, 
different topics with respect to the same compa-
ny may have relations. We, therefore, utilize hi-
erarchical clustering methods4to capture the po-
tential relations.  
Due to the large amount of data, it is impossi-
ble to view all the topics at a snapshot. MODEST 
utilizes topic tracking technique (Wang et al., 
2008) to identify related stories with a stream of 
                                                          
1 www.ictclas.org 
2http://ir.hit.edu.cn/demo/ltp 
3http://libots.sourceforge.net/ 
4http://dragon.ischool.drexel.edu/ 
Raw
files
Pre-processed
files
Database
Crawler UI
Word
segmentation
Web
NER
Stopword
removal
POS
tagging
NLP pre-p ocessor
TDT OM
MDS SNS
Miner
Data Layer
98
media. It is convenient for the users to see the 
latest information about the company.  
In summary, TDT function provides the ser-
vices of detecting and tracking the latest and 
emergent topics, analyzing the relationships of 
topics on the dynamics of the company. It meets 
the aforementioned demand, ?to accurately grasp 
the latest business news and trends of the com-
pany?. 
2.2.2 Opinion Mining 
The objective of OM function is to discover 
opinions towards a company and classify the 
opinions into positive, negative, or neutral. 
The opinion mining function is redesigned 
based on our own opinion mining engine (Zhou 
et al., 2010). It separates opinion identification 
and polarity classification into two stages.  
Given a set of documents that are relevant to 
the company, we first split the documents into 
sentences, and then identify whether the sentence 
is opinionated or not. We extract the features 
shown in Table 1 for opinion identification. 
(Zhou et al., 2010) 
Table 1: Features adopted in the opinionated 
sentence classifier 
Punctuation level features 
The presence of direct quote punctuation "?" and "?"  
The presence of other punctuations: "?" and "!" 
Word-Level and entity-level features 
The presence of known opinion operators 
The percentage of known opinion word in sentence 
Presence of a named entity 
Presence of pronoun 
Presence of known opinion indicators 
Presence of known degree adverbs 
Presence of known conjunctions 
Bi-gram features 
Named entities + opinion operators 
Pronouns + opinion operators 
Nouns or named entities + opinion words 
Pronouns + opinion words 
Opinion words (adjective) + opinion words(noun) 
Degree adverbs + opinion words 
Degree adverbs + opinion operators 
These features are then combined using a ra-
dial basis function (RBF) kernel and a support 
vector machine (SVM) classifier (Drucker et al., 
1997) is trained based on the NTCIR 8training 
data for opinion identification (Kando, 2010). 
For those opinionated sentences, we then clas-
sify them into positive, negative, or neutral. In 
addition to the features shown in Table 1, we 
incorporate features of s-VSM (Sentiment Vector 
Space Model) (Xia et al., 2008) to enhance the 
performance. The principles of the s-VSM are 
listed as follows: (1) Only sentiment-related 
words are used to produce sentiment features for 
the s-VSM. (2) The sentiment words are appro-
priately disambiguated with the neighboring ne-
gations and modifiers. (3) Negations and modifi-
ers are included in the s-VSM to reflect the func-
tions of inversing, strengthening and weakening. 
Sentiment unit is the appropriate element com-
plying with the above principles. (Zhou et al., 
2010) 
In addition to polarity classification, opinion 
holder and target are also recognized in OM 
function for further identifying the relationship 
that two companies have, e.g. collaborative or 
competitive. Both of the dependency parser and 
the semantic role labeling1 (SRL) tool are in-
corporated to identify the semantic roles of each 
chunk based on verbs in the sentence. 
The OM function provides the company with 
services of analyzing the social sentimental 
feedback on the dynamics of the company. It 
meets the aforementioned demand, ?to identify 
and analyze customers? opinions towards the 
company?. 
2.2.3 Social Network Analysis  
SNA function aims at producing the commercial 
network of companies that are hidden within the 
articles.  
To achieve this goal, we maintain two lexicons, 
the commercial named entity lexicon and com-
mercial relation lexicon. Commercial named en-
tity are firstly located within the text and then 
recorded in the commercial entity lexicon in the 
pre-processor NER. Commercial relation lexicon 
record the articles/documents that involve the 
commercial relations. Note that the commercial 
relation lexicon (Table 2) is manually compiled. 
In this work, we consider only two general 
commercial relations, namely cooperation and 
competition.  
 
Table 2: Statistics on relation lexicon. 
Type Amount Examples 
Competition 20 ??(challenge), ??
(compete), ? ?
(opponent) 
Collaboration 18 ??(collaborate),??
(coordinate), ? ?
(cooperate) 
SNA function produces the social network of a 
centric company, which can provide the compa-
                                                          
1http://ir.hit.edu.cn/demo/ltp 
99
ny with the impact analysis and decision-making 
chain tracking. It meets the aforementioned de-
mand, ?to explore the collaborative and competi-
tive relationship between companies?. 
3 Practical Example 
In this section, we use a case study to illustrate 
our system and further evaluate the performance 
of the main functions with respect to those com-
panies. Due to the limited space, we just illus-
trate the main functions of topic detection, opin-
ion mining and social network analysis. 
3.1 Topic Detection and Opinion Mining 
Figure 2(a) showed the results of topic detection 
and opinion mining functions for a Hong Kong 
local financial company Sun Hung Kai Proper-
ties (?????). On top of the figure are the 
results of topic detection and tracking function. 
Multi-document summary of the latest news is 
provided for the company and more news with 
the similar topics can be found by pressing the 
button ???? (more). Since there are a lot of 
duplicates of a piece of news on the websites, the 
summary is a direct way to acquire the recent 
news, which can improve the effectiveness of the 
company.  
The results of opinion mining function are 
shown at the bottom of Figure 2(a), where the 
green line indicates negative while the red line 
indicates positive. In order to give a dynamic 
insight of public opinions, we provide the 
amount changes of positive and negative articles 
with time variant. This is very helpful for the 
company to capture the feedback of their mar-
keting policies. As shown in Figure 2(a), there 
were 14 negative articles (????) on Oct. 29, 
2012, which achieved negative peak within the 6 
months. The users would probably read those 14 
articles and adjust the company strategy accord-
ingly.  
3.2 Social Network Analysis 
Figure 2(b) shows the social network based on 
the centric company in yellow, Sun Hung Kai 
Properties (?????). We only list the half 
of the connected companies with collaborative 
relationship from Sun Hung Kai Properties, and 
remove the competitive ones due to limited space. 
The thickness of the line indicates the strength of 
the collaboration between the two companies. 
The social network can explore the potential 
partners/competitors of a company. Furthermore, 
users are allowed to adjust the depth and set the 
nodes count of the network. The above analysis 
can provide a richer insight in to a company.  
In the following section, we will make exper-
iments to investigate the performance of the 
above functions.
 
(a) Topic detection and opinion mining of Sun Hung Kai Properties (?????). (For convenience, 
we translate the texts on the button in English) 
Opinion Mining 
Topic Detection 
100
 (b)Social network of Sun Hung Kai Properties (?????). (The rectangle in yellow is the centric) 
 
Figure 2: Screenshot of the MODEST system. 
4 Experiment and Result 
In our evaluation, the experiments were made 
based on 17692 articles collected from 52 Hong 
Kong websites during 6 months (1/7/2012~ 
31/12/2012). We investigate the performance of 
MODEST based on the standard metrics pro-
posed by NIST1, including precision, recall, and 
F-score. 
Precision (P) is the fraction of detected articles 
(U) that are relevantto the topic (N). 
  
 
 
      
Recall (R) is the fraction of the articles (T) that 
are relevant to the topic that are successfully de-
tected (N). 
  
 
 
      
Usually, there is an inverse relationship be-
tween precision and recall, where it is possible to 
increase one at the cost of reducing the other. 
Therefore, precision and recall scores are not 
discussed in isolation. Instead, F-Score (F) is 
proposed to combine precision and recall, which 
is the harmonic meanof precision and recall. 
  
 
 
 
 
 
 
      
     
   
      
4.1 Topic Detection and Tracking 
We first assess the performance of the topic de-
tection function. The data is divided into 6 parts 
                                                          
1http://trec.nist.gov/ 
according to the time. For different companies, 
the amount of articles vary a lot. Therefore, we 
calculate the metrics for each individual dataset, 
and then compute the weighted mean value. The 
experimental results are shown in Table 3.  
Table 3: Experimental results on topic detection. 
Dataset Recall Precision F-Score 
1/7/12-31/7/12 85.71% 89.52% 85.38% 
1/8/12-31/8/12 93.10% 93.68% 92.49% 
1/9/12-30/9/12 76.50% 83.13% 76.56% 
1/10/12-31/10/12 83.32% 88.53% 85.84% 
1/11/12-30/11/12 86.11% 89.94% 87.98% 
1/12/12-31/12/12 84.26% 87.65% 85.92% 
Average 85.13% 88.78% 85.69% 
From the experimental results, we can find 
that the average F-Score is about 85.69%.The 
dataset in the second row achieves the best per-
formance while the dataset in the third only get 
76.56% in F-Score. It is because that the amount 
of articles is smaller than the others and the re-
call value is very low. As far as we know, the 
best run of topic detection in (Allan et al., 2007) 
achieved 84%. The performance of topic detec-
tion in MODEST is comparable. 
4.2 Opinion Mining 
We then evaluate the performance of opinion 
mining function. We manually annotated 1568 
articles, which is further divided into 8 datasets 
randomly. Precision, recall, and F-score are also 
used as the metrics for the evaluation. The ex-
perimental results are shown in Table 4. 
101
  From Table 4, we can find that the average 
F-Score can reach 74.09%. Note that the opinion 
mining engine of MODEST is the implementa-
tion of (Zhou et al., 2010), which achieved the 
best run in NTCIR. However, the engine is 
trained on NTCIR corpus, which consists of arti-
cles of general domain, while the test set focuses 
on the financial domain. We further train our 
engine on the data from the financial domain and 
the average F-Score improves to over 80%. 
5 Conclusions 
This demonstration presents an intelligent infor-
mation platform designed to mine Web infor-
mation and provide decisions for modern service, 
MODEST. MODEST can provide the services of 
retrieving news from websites, extracting com-
mercial information, exploring customers? opin-
ions about a given company, and analyzing its 
collaborative/competitive social networks. Both 
enterprises and government are the target cus-
tomers. For enterprise, MODEST can improve 
the competitive abilities and facilitate potential 
collaboration. For government, MODEST can 
collect information about the entire industry, and 
make prompt strategies for better support. 
In this paper, we first introduce the system ar-
chitecture design and the main functions imple-
mentation, including topic detection and tracking, 
opinion mining, and social network analysis. 
Then a case study is given to illustrate the func-
tions of MODEST. In order to evaluate the per-
formance of MODEST, we also conduct the ex-
periments based on the data from 52 Hong Kong 
websites, and the results show the effectiveness 
of the above functions. 
In the future, MODEST will be improved in 
two directions: 
? Extend to other languages, e.g. English, 
Simplified Chinese, etc. 
? Enhance the compatibility to implement 
on mobile device.  
The demo of MODEST and the related 
toolkits can be found on the homepage: 
http://sepc111.se.cuhk.edu.hk:8080/adcom_hk/ 
Acknowledgements 
This research is partially supported by General Re-
search Fund of Hong Kong (417112), Shenzhen Fun-
damental Research Program (JCYJ201304011720464 
50, JCYJ20120613152557576), KTO(TBF1ENG007), 
National Natural Science Foundation of China 
(61203378, 61370165), and Shenzhen International 
Cooperation Funding (GJHZ20120613110641217). 
References: 
James Allan, Jaime Carbonell, George Doddington, 
Jonathan Yamron, and Yiming Yang. 1998. Topic 
Detection and Tracking Pilot Study: Final Report. 
Proceedings of the DARPA Broadcast News Tran-
scription and Understanding Workshop. 
Harris Drucker, Chris J.C. Burges, Linda Kaufman, 
Alex Smola, and Vladimir Vpnik. 1997. Support 
Vector Regression Machines. Proceedings of Ad-
vances in Neural Information Processing Systems, 
pp. 155-161. 
Noriko Kando.2010. Overview of the Eighth NTCIR 
Workshop. Proceedings of NTCIR-8 Workshop. 
Bing Liu. 2012. Sentiment Analysis and Opinion 
Mining. Proceedings of Synthesis Lectures on Hu-
man Language Technologies, pp. 1-167. 
Maximilian Viermetz, and Michal Skubacz. 2007. 
Using Topic Discovery to Segment Large Commu-
nication Graphs for Social Network Analysis. Pro-
ceedings of the IEEE/WIC/ACM International 
Conference on Web Intelligence, pp. 95-99. 
Canhui Wang, Min Zhang, Liyun Ru, and Shaoping 
Ma. 2008. Automatic Online News Topic Ranking 
Using Media Focus and User Attention based on 
Aging Theory. Proceedings of the Conference on 
Information and Knowledge Management. 
Yunqing Xia, Nianxing Ji, Weifeng Su, and Yi Liu. 
2010. Mining Commercial Networks from Online 
Financial News. Proceedings of the IEEE Interna-
tional Conference on E-Business Engineering, pp. 
17-23. 
Ruifeng Xu, Kam-fai Wong, and Yunqing Xia. 2008. 
Coarse-Fine Opinion Mining-WIA in NTCIR-7 
MOAT Task. In NTCIR-7 Workshop, pp. 307-313. 
Seok-Ho Yoon, Jung-Hwan Shin, Sang-Wook Kim, 
and Sunju Park. 2009. Extraction of a Latent Blog 
Community based on Subject. Proceeding of the 18th 
ACM Conference on Information and Knowledge 
Management, pp. 1529-1532. 
Lanjun Zhou, Yunqing Xia, Binyang Li, and Kam-fai 
Wong. 2010. WIA-Opinmine System in NTCIR-8 
MOAT Evaluation. Proceedings of NTCIR-8 
Workshop Meeting, pp. 286-292. 
Table 4: Experimental results on opinion mining. 
Dataset Size Precision Recall F-Score 
dataset-1 200 76.57% 78.26% 76.57% 
dataset-2 200 83.55% 89.64% 86.07% 
dataset-3 200 69.12% 69.80% 69.44% 
dataset-4 200 77.13% 75.40% 75.67% 
dataset-5 200 76.21% 77.65% 76.74% 
dataset-6 200 63.76% 66.22% 64.49% 
dataset-7 200 78.56% 78.41% 78.43% 
dataset-8 168 65.72% 65.15% 65.32% 
Average 196 73.83% 75.07% 74.09% 
102
