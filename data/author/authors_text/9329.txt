Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 541?550, Prague, June 2007. c?2007 Association for Computational Linguistics
Identification and Resolution of Chinese Zero Pronouns:
A Machine Learning Approach
Shanheng Zhao and Hwee Tou Ng
Department of Computer Science
National University of Singapore
3 Science Drive 2
Singapore 117543
{zhaosh, nght}@comp.nus.edu.sg
Abstract
In this paper, we present a machine learn-
ing approach to the identification and reso-
lution of Chinese anaphoric zero pronouns.
We perform both identification and resolu-
tion automatically, with two sets of easily
computable features. Experimental results
show that our proposed learning approach
achieves anaphoric zero pronoun resolution
accuracy comparable to a previous state-of-
the-art, heuristic rule-based approach. To
our knowledge, our work is the first to per-
form both identification and resolution of
Chinese anaphoric zero pronouns using a
machine learning approach.
1 Introduction
Coreference resolution is the task of determining
whether two or more noun phrases refer to the same
entity in a text. It is an important task in discourse
analysis, and successful coreference resolution ben-
efits many natural language processing applications
such as information extraction, question answering,
etc.
In the literature, much of the work on corefer-
ence resolution is for English text (Soon et al, 2001;
Ng and Cardie, 2002b; Yang et al, 2003; McCal-
lum and Wellner, 2005). Publicly available cor-
pora for coreference resolution are mostly in En-
glish, e.g., the Message Understanding Conference
tasks (MUC6 and MUC7)1. Relatively less work has
1http://www-nlpir.nist.gov/related_
projects/muc/
been done on coreference resolution for Chinese.
Recently, the ACE Entity Detection and Tracking
(EDT) task2 included annotated Chinese corpora for
coreference resolution. Florian et al (2004) and
Zhou et al (2005) reported research on Chinese
coreference resolution.
A prominent phenomenon in Chinese coreference
resolution is the prevalence of zero pronouns. A zero
pronoun (ZP) is a gap in a sentence which refers
to an entity that supplies the necessary information
for interpreting the gap. An anaphoric zero pro-
noun (AZP) is a zero pronoun that corefers to one
or more overt noun phrases present in the preced-
ing text. Zero pronouns occur much more frequently
in Chinese compared to English, and pose a unique
challenge in coreference resolution for Chinese. For
example, Kim (2000) conducted a study to compare
the use of overt subjects in English, Chinese, and
other languages. He found that the use of overt sub-
jects in English is over 96%, while this percentage is
only 64% for Chinese, indicating that zero pronouns
(lack of overt subjects) are much more prevalent in
Chinese.
Chinese zero pronouns have been studied in lin-
guistics research (Li and Thompson, 1979; Li,
2004), but only a small body of prior work in com-
putational linguistics deals with Chinese zero pro-
noun identification and resolution (Yeh and Chen,
2004; Converse, 2006). To our knowledge, all pre-
vious research on zero pronoun identification and
resolution in Chinese uses hand-engineered rules or
heuristics, and our present work is the first to per-
form both identification and resolution of Chinese
2http://www.nist.gov/speech/tests/ace/
541
anaphoric zero pronouns using a machine learning
approach.
The rest of this paper is organized as follows. In
Section 2, we give the task definition, and describe
the corpus used in our evaluation and the evaluation
metrics. We then give an overview of our approach
in Section 3. Anaphoric zero pronoun identification
and resolution are presented in Section 4 and 5, re-
spectively. We present the experimental results in
Section 6 and related work in Section 7, and con-
clude in Section 8.
2 Task Definition
2.1 Zero Pronouns
As mentioned in the introduction, a zero pronoun
(ZP) is a gap in a sentence which refers to an en-
tity that supplies the necessary information for in-
terpreting the gap. A coreferential zero pronoun is a
zero pronoun that corefers to one or more overt noun
phrases present in the same text.
Just like a coreferential noun phrase, a coreferen-
tial zero pronoun can also corefer to a noun phrase
in the preceding or following text, called anaphoric
or cataphoric, respectively. Most coreferential zero
pronouns in Chinese are anaphoric. In the corpus
used in our evaluation, 98% of the coreferential zero
pronouns have antecedents. Hence, for simplicity,
we only consider anaphoric zero pronouns (AZP)
in this work. That is, we only attempt to resolve a
coreferential zero pronoun to noun phrases preced-
ing it.
Here is an example of an anaphoric zero pronoun
from the Penn Chinese TreeBank (CTB) (Xue et al,
2005) (sentence ID=300):
[?) ?? ?? ??=
[China electronic products import and export
?4]1 ?  ? ?2
trade]1 continues increasing , ?2
3  ??= {
represents total import and export ?s
? ? ? 
ratio continues increasing .
The anaphoric zero pronoun ?2 is coreferring to
noun phrase 1. The corresponding parse tree is
shown in Figure 1. In CTB, IP refers to a simple
clause that does not have complementizers. CP, on
the other hand, refers to a clause introduced by a
complementizer.
Resolving an anaphoric zero pronoun to its cor-
rect antecedent in Chinese is a difficult task. Al-
though gender and number information is available
for an overt pronoun and has proven to be useful
in pronoun resolution in prior research, a zero pro-
noun in Chinese, unlike an overt pronoun, provides
no such gender or number information. At the same
time, identifying zero pronouns in Chinese is also a
difficult task. There are only a few overt pronouns
in English, Chinese, and many other languages, and
state-of-the-art part-of-speech taggers can success-
fully recognize most of these overt pronouns. How-
ever, zero pronouns in Chinese, which are not ex-
plicitly marked in a text, are hard to be identified.
Furthermore, even if a gap is a zero pronoun, it may
not be coreferential. All these difficulties make the
identification and resolution of anaphoric zero pro-
nouns in Chinese a challenging task.
2.2 Corpus
We use an annotated third-person pronoun and zero
pronoun coreference corpus from Converse (2006)3.
The corpus contains 205 texts from CTB 3.0, with
annotations done directly on the parse trees. In
the corpus, coreferential zero pronouns, third-person
pronouns, and noun phrases are annotated as coref-
erence chains. If a noun phrase is not in any coref-
erence chain, it is not annotated. If a coreference
chain does not contain any third-person pronoun or
zero pronoun, the whole chain is not annotated.
A zero pronoun is not always coreferential with
some noun phrases. In the corpus, if a zero pronoun
is not coreferential with any overt noun phrases, it
is assigned one of the following six categories: dis-
course deictic (#DD), existential (#EXT), inferrable
(#INFR), ambiguity between possible referents in
the text (#AMB), arbitrary reference (#ARB), and
unknown (#UNK). For example, in the following
sentence, ?3 refers to an event in the preceding text,
with no corresponding antecedent noun phrase. So
no antecedent is annotated, and ?3 is labeled as
#DD.
&? ?? cL ?T
Hong Kong famous syndicate Cheung Kong
3The data set we obtained is a subset of the one used in Con-
verse (2006).
542
IP1








HH
HH
HH
HH
HH
HH
HH
HH
IP2





HH
HH
HH
HH
H
NP1



HH
HH
H
NP2
NR
?)
NP3
H
NN
??
NN
??
NP4
 H
NN
??=
NN
?4
VP1
H
VV
?
VP2
VV

PU
?
IP3


HH
H
NP5


HH
H
CP1
CP2


HH
H
IP4
VP3
 HH
VV
3
NP6
 H
ADJP
JJ

NP7
NN
??=
DEC
{
NP8
NN
?
VP4
H
VV
?
VP5
VV
?
PU

Figure 1: The parse tree which corresponds to the anaphoric zero pronoun example in Section 2.1.
"  ??- *? 4Qu
Holdings , Peregrine as strategic
=?V . ?? ?
investors already purchased LE
 ? ;?  ?I?
? Shenye Holdings ? twenty percent
{ ?Y ? ?3 ?I 'n ?
?s share , ?3 fully reflects out
=?V { fe 
investors ?s confidence .
Converse (2006) assumed that all correctly identi-
fied AZPs and the gold standard parse trees are given
as input to her system. She applied the Hobbs algo-
rithm (Hobbs, 1978) to resolve antecedents for the
given AZPs.
In our case, we are only interested in zero pro-
nouns with explicit noun phrase referents. If a coref-
erence chain does not contain AZPs, we discard the
chain. We also discard the 6 occurrences of zero
pronouns with split antecedents, i.e., a zero pronoun
with an antecedent that is split into two separate
noun phrases. A total of 383 AZPs remain in the
data set used in our experiments.
Among the 205 texts in the data set, texts 1?155
are reserved for training, while the remaining texts
(156?205) are used for blind test. The statistics of
the data set are shown in Table 1.
Training Test
Doc ID 1?155 156?205
# Docs 155 50
# Characters 96,338 15,710
# Words 55,348 9,183
# ZPs 665 87
# AZPs 343 40
Table 1: Statistics of training and test data sets.
2.3 Evaluation Metrics
As in previous work on pronoun resolution, we eval-
uate the accuracy in terms of recall, precision, and F-
measure. The overall recall and precision on the test
set are computed by micro-averaging over all test in-
stances. The overall F-measure is then computed.
For AZP identification, recall and precision are
543
defined as:
RecallAZP =
# AZP Hit
# AZP in Key
PrecisionAZP =
# AZP Hit
# AZP in Response
An ?AZP Hit? occurs when an AZP as reported in
the response (system output) has a counterpart in the
same position in the gold standard answer key.
For AZP resolution, recall and precision are de-
fined as:
RecallResol =
# Resol Hit
# AZP in Key
PrecisionResol =
# Resol Hit
# AZP in Response
A ?Resol Hit? occurs when an AZP is correctly iden-
tified, and it is correctly resolved to a noun phrase
that is in the same coreference chain as provided in
the answer key.
3 Overview of Our Approach
In this section, we give an overview of our approach
for Chinese AZP identification and resolution.
Typically, the input raw texts need to be pro-
cessed by a Chinese word segmenter, a part-of-
speech (POS) tagger, and a parser sequentially. Al-
though our approach can apply directly to machine-
generated parse trees from raw text, in order to min-
imize errors introduced by preprocessing, and focus
mainly on Chinese zero pronoun resolution, we use
the gold standard word segmentation, POS tags, and
parse trees provided by CTB. However, we remove
all null categories and functional tags from the CTB
gold standard parse trees. Figure 1 shows a parse
tree after such removal.
A set of zero pronoun candidates and a set of noun
phrase candidates are then extracted. If W is the left-
most word in the word sequence that is spanned by
some VP node, the gap G that is immediately to the
left of W qualifies as a ZP candidate. For example,
in Figure 1, gaps immediately to the left of the two
occurrences of?, and,3,? are all ZP
candidates. All noun phrases4 that are either maxi-
mal NPs or modifier NPs qualify as NP candidates.
4A noun phrase can either be NP or QP in CTB. We simply
use NP hereafter.
For example, in Figure 1, NP1, NP2, NP3, NP5, and
NP6 are all NP candidates. With these ZP and NP
candidate extractions, the recalls of ZPs and NPs are
100% and 98.6%, respectively.
After the ZP and NP candidates are determined,
we perform AZP identification and resolution in a
sequential manner. We build two classifiers, the
AZP identification classifier and the AZP resolution
classifier. The AZP identification classifier deter-
mines the position of AZPs, while the AZP resolu-
tion classifier finds an antecedent noun phrase for
each AZP identified by the AZP identification clas-
sifier. Both classifiers are built using machine learn-
ing techniques. The features of both classifiers are
largely syntactic features based on parse trees and
are easily computed.
We perform 5-fold cross validation on the train-
ing data set to tune parameters and to pick the best
model. We then retrain the best model with all data
in the training data set, and apply it to the blind test
set. In the following sections, all accuracies reported
on the training data set are based on 5-fold cross val-
idation.
4 Anaphoric Zero Pronoun Identification
We use machine learning techniques to build the
AZP identification classifier. The features are de-
scribed in Table 2.
In the feature description, Z is the ZP candidate.
Let Wl and Wr be the words immediately to the left
and to the right of Z , respectively, P the parse tree
node that is the lowest common ancestor node of Wl
and Wr, Pl and Pr the child nodes of P that are an-
cestor nodes of Wl and Wr, respectively. If Z is the
first gap of the sentence, Wl, P , Pl, and Pr are all
NA. Furthermore, let V be the highest VP node in
the parse tree that is immediately to the right of Z ,
i.e., the leftmost word in the word sequence that is
spanned by V is Wr. If Z is not the first gap in the
sentence, define the ceiling node C to be P , other-
wise to be the root node of the parse tree. In the
example shown in Figure 1, for the ZP candidate ?2
(which is immediately to the left of 3), Wl, Wr,
P , Pl, Pr, V , and C are ???, 3, IP1, IP2, IP3,
VP3, and IP1, respectively. Its feature values are also
shown in Table 2.
To train an AZP identification classifier, we gen-
544
Feature Description ?2
First Gap If Z is the first gap in the sentence, T; else F. F
Pl Is NP If Z is the first gap in the sentence, NA; otherwise, if Pl is an NP node,
T; else F.
F
Pr Is VP If Z is the first gap in the sentence, NA; otherwise, if Pr is a VP node, T;
else F.
F
Pl Is NP & Pr Is VP If Z is the first gap in the sentence, NA; otherwise, if Pl is an NP node
and Pr is a VP node, T; else F.
F
P Is VP If Z is the first gap in the sentence, NA; otherwise, if P is a VP node, T;
else F.
F
IP-VP If in the path from Wr to C , there is a VP node such that its parent node
is an IP node, T; else F.
T
Has Ancestor NP If V has an NP node as ancestor, T; else F. T
Has Ancestor VP If V has a VP node as ancestor, T; else F. F
Has Ancestor CP If V has a CP node as ancestor, T; else F. T
Left Comma If Z is the first gap, NA; otherwise if Wl is a comma, T; else F. T
Subject Role If the grammatical role of Z is subject, S; else X. X
Clause If V is in a matrix clause, an independent clause, a subordinate clause, or
none of the above, the value is M, I, S, X, respectively.
I
Is In Headline If Z is in the headline of the text, T; else F. F
Table 2: Features for anaphoric zero pronoun identification. The feature values of ?2 are shown in the last
column.
erate training examples from the training data set.
All ZP candidates in the training data set generate
training examples. Whether a training example is
positive or negative depends on whether the ZP can-
didate is an AZP.
After generating all training examples, we train
an AZP identification classifier using the J48 deci-
sion tree learning algorithm in Weka5. During test-
ing, each ZP candidate is presented to the learned
classifier to determine whether it is an AZP. We con-
duct experiments to measure the performance of the
model learned. The results of 5-fold cross validation
on the training data set are shown in Table 3.
Model R P F
Heuristic 99.7 15.0 26.1
AZP Ident 19.8 51.1 28.6
AZP Ident (r = 8) 59.8 44.3 50.9
Table 3: Accuracies of AZP identification on the
training data set under 5-fold cross validation.
We use heuristic rules as a baseline for compar-
5http://www.cs.waikato.ac.nz/ml/weka/
ison. The rules used by the heuristic model are as
follows. For a node T in the parse tree, if
1. T is a VP node; and
2. T ?s parent node is not a VP node; and
3. T has no left sibling, or its left sibling is not an
NP node,
then the gap that is immediately to the left of the
word sequence spanned by T is an AZP. This simple
AZP identification heuristic achieves an F-measure
of 26.1%.
Imbalanced Training Data
From Table 3, one can see that the F-measure
of the machine-learned AZP identification model is
28.6%, which is only slightly higher than baseline
heuristic model. It has a relatively high precision,
but much lower recall. The problem lies in the
highly imbalanced number of positive and negative
training examples. Among all the 155 texts in the
training set, there are 343 positive and 10,098 neg-
ative training examples. The ratio r of the number
545
of negative training examples to the number of pos-
itive training examples is 29.4. A classifier trained
on such highly imbalanced training examples tends
to predict more testing examples as negative exam-
ples. This explains why the precision is high, but the
recall is low.
To overcome this problem, we vary r by varying
the weight of the positive training examples, which
is equivalent to sampling more positive training ex-
amples. The values of r that we have tried are
1, 2, 3, . . . , 29. The larger the value of r, the higher
the precision, and the lower the recall. By tuning
r, we get a balance between precision and recall,
and hence an optimal F-measure. Figure 2 shows
the effect of tuning r on AZP identification. When
r = 8, the optimal F-measure is 50.9%, which is
much higher than the F-measure without tuning r.
0 5 10 15 20 25 30
10
20
30
40
50
60
70
80
90
100
r
Sc
or
e
Recall
Precision
F?measure
Figure 2: Effect of tuning r on AZP identification
Ng and Cardie (2002a) reported that the accura-
cies of their noun phrase anaphoricity determination
classifier were 86.1% and 84.0% for the MUC6 and
MUC7 data sets, respectively. Noun phrases provide
much fruitful information for anaphoricity identifi-
cation. However, useful information such as gen-
der, number, lexical string, etc, is not available in
the case of zero pronouns. This makes AZP identifi-
cation a much more difficult task, and hence it has a
relatively low accuracy.
5 Anaphoric Zero Pronoun Resolution
In anaphoric zero pronoun resolution, we also use
machine learning techniques to build a classifier.
The features are described in Table 4.
In the feature description, Z is the anaphoric zero
pronoun that is under consideration, and A is the po-
tential NP antecedent for Z . V is the same as in AZP
identification. The feature values of the pair NP1 and
?2 (the gap immediately to the left of3) in Figure
1 are shown in Table 4.
To train the AZP resolution classifier, we generate
training examples in the following way. An AZP Z
and its immediately preceding coreferential NP an-
tecedent A in the gold standard coreference chain
form a positive training example. Between A and Z ,
there are other NP candidates. Each one of these NP
candidates, together with Z , form a negative training
example. This is similar to the approach adopted in
Soon et al (2001). We also train the AZP resolution
classifier using the J48 decision tree learning algo-
rithm.
After building both AZP identification and resolu-
tion classifiers, we perform AZP identification and
resolution in a sequential manner. For a ZP candi-
date Z , the AZP identification classifier determines
whether Z is an AZP. If it is an AZP, all NP can-
didates that are to the left of Z in textual order are
considered as potential antecedents. These potential
antecedents are tested from right to left. We start
from the NP candidate A1 that is immediately to the
left of Z . A1 and Z form a pair. If the pair is classi-
fied as positive by the resolution classifier, A1 is the
antecedent for Z . If it is classified as negative, we
proceed to the NP candidate A2 that is immediately
to the left of A1, and test again. The process contin-
ues until we find an antecedent for Z , or there is no
more NP candidate to test.
This right-to-left search attempts to find the clos-
est correct antecedent for an AZP. We do not choose
the best-first search strategy proposed by Ng and
Cardie (2002b). This is because we generate train-
ing examples and build the resolution classifier by
pairing each zero pronoun with its closest preceding
antecedent. In addition, a zero pronoun is typically
not too far away from its antecedent. In our data set,
92.6% of the AZPs have antecedents that are at most
2 sentences apart. Our experiment shows that this
closest-first strategy performs better than the best-
first strategy for Chinese AZP resolution.
Table 5 shows the experimental results of 5-fold
cross validation on the training data set. For com-
546
Feature Description NP1-?2
Features between Z and A
Dist Sentence If Z and A are in the same sentence, 0; if they are one sentence
apart, 1; and so on.
0
Dist Segment If Z and A are in the same segment (where a segment is a se-
quence of words separated by punctuation marks including ???,
???, ??, ???, and ???), 0; if they are one segment apart, 1; and
so on.
1
Sibling NP VP If Z and A are in different sentences, F; Otherwise, if both A and
Z are child nodes of the root node, and they are siblings (or at most
separated by one comma), T; else F.
F
Closest NP If A is the closest preceding NP candidate to Z , T; else F. T
Features on A
A Has Anc NP If A has an ancestor NP node, T; else F. F
A Has Anc NP In IP If A has an ancestor NP node which is a descendant of A?s lowest
ancestor IP node, T; else F.
F
A Has Anc VP If A has an ancestor VP node, T; else F. F
A Has Anc VP In IP If A has an ancestor VP node which is a descendant of A?s lowest
ancestor IP node, T; else F.
F
A Has Anc CP If A has an ancestor CP node, T; else F. F
A Grammatical Role If the grammatical role of A is subject, object, or others, the value is
S, O, or X, respectively.
S
A Clause If A is in a matrix clause, an independent clause, a subordinate
clause, or none of the above, the value is M, I, S, X, respectively.
M
A Is ADV If A is an adverbial NP, T; else F. F
A Is TMP If A is a temporal NP, T; else F. F
A Is Pronoun If A is a pronoun, T; else F. F
A Is NE If A is a named entity, T; else F. F
A In Headline If A is in the headline of the text, T; else F. F
Features on Z
Z Has Anc NP If V has an ancestor NP node, T; else F. T
Z Has Anc NP In IP If V has an ancestor NP node which is a descendant of V?s lowest
ancestor IP node, T; else F.
F
Z Has Anc VP If V has an ancestor VP node, T; else F. F
Z Has Anc VP In IP If V has an ancestor VP node which is a descendant of V?s lowest
ancestor IP node, T; else F.
F
Z Has Anc CP If V has an ancestor CP node, T; else F. T
Z Grammatical Role If the grammatical role of Z is subject, S; else X. X
Z Clause If V is in a matrix clause, an independent clause, a subordinate
clause, or none of the above, the value is M, I, S, X, respectively.
I
Z Is First ZP If Z is the first ZP candidate in the sentence, T; else F. F
Z Is Last ZP If Z is the last ZP candidate in the sentence, T; else F. F
Z In Headline If Z is in the headline of the text, T; else F. F
Table 4: Features for anaphoric zero pronoun resolution. The feature values of the pair NP1 and ?2 are
shown in the last column.
547
parison, we show three baseline systems. In all three
baseline systems, we do not perform AZP identifica-
tion, but directly apply the AZP resolution classifier.
In the first baseline, we apply the AZP resolution
classifier on all ZP candidates. In the second base-
line, we apply the classifier only on ZPs annotated
in the gold standard, instead of all ZP candidates.
In the third baseline, we further restrict it to resolve
only AZPs. The F-measures of the three baselines
are 2.5%, 27.6%, and 40.6% respectively.
Model R P F
All ZP Candidates 40.5 1.3 2.5
Gold ZP 40.5 20.9 27.6
Gold AZP 40.5 40.6 40.6
AZP Ident (r=8 t=0.5) 23.6 17.5 20.1
AZP Ident (r=11 t=0.6) 22.4 20.3 21.3
Table 5: Accuracies of AZP resolution on the train-
ing data set under 5-fold cross validation.
Tuning of Parameters
Ng (2004) showed that an NP anaphoricity iden-
tification classifier with a cut-off threshold t =
0.5 pruned away many correct anaphoric NPs and
harmed the overall recall. By varying t, the overall
resolution F-measure was improved. We adopt the
same tuning strategy and accept a ZP candidate ZPi
as an AZP and proceed to find its antecedent only if
P (ZPi) ? t. The possible values for t that we have
tried are 0, 0.05, 0.1, . . . , 0.95.
In Section 4, we show that r = 8 yields the best
AZP identification F-measure. When we fix r = 8
and vary t, the overall F-measure for AZP resolution
is the best at t = 0.65, as shown in Figure 3. We then
try tuning r and t at the same time. An overall op-
timal F-measure of 21.3% is obtained when r = 11
and t = 0.6. We compare this tuned F-measure with
the F-measure of 20.1% at r = 8 and t = 0.5, ob-
tained without tuning t. Although the improvement
is modest, it is statistically significant (p < 0.05).
6 Experimental Results
In the previous section, we show that when r = 11
and t = 0.6, our sequential AZP identification and
resolution achieves the best F-measure under 5-fold
cross validation on the 155 training texts. In or-
der to utilize all available training data, we generate
0 0.2 0.4 0.6 0.8 1
0
5
10
15
20
25
30
35
40
45
t
Sc
or
e
Recall
Precision
F?measure
Figure 3: Effect of tuning t on AZP resolution
training examples for the AZP identification classi-
fier with r = 11, and generate training examples
for the AZP resolution classifier, on all 155 train-
ing texts. Both classifiers are trained again with the
newly generated training examples. We then apply
both classifiers with anaphoricity identification cut-
off threshold t = 0.6 to the blind test data. The
results are shown in Table 6.
R P F
27.5 24.4 25.9
Table 6: Accuracies of AZP resolution on blind test
data.
By utilizing all available information on the gold
standard parse trees, Converse (2006) finds an an-
tecedent for each AZP given that all AZPs are cor-
rectly input to her system. The accuracy of her rule-
based approach is 43.0%. For comparison, we de-
termine the antecedents for AZPs in the gold stan-
dard annotation, under 5-fold cross validation on all
205 texts in the corpus. The recall, precision, and F-
measure are 42.3%, 42.7%, and 42.5%, respectively.
This shows that our proposed machine learning ap-
proach for Chinese zero pronoun resolution is com-
parable to her state-of-the-art rule-based approach.
7 Related Work
Converse (2006) assumed that the gold standard
Chinese anaphoric zero pronouns and the gold stan-
dard parse trees of the texts in Penn Chinese Tree-
548
Bank (CTB) were given as input to her system,
which performed resolution of the anaphoric zero
pronouns using the Hobbs algorithm (Hobbs, 1978).
Her system did not identify the anaphoric zero pro-
nouns automatically.
Yeh and Chen (2004) proposed an approach for
Chinese zero pronoun resolution based on the Cen-
tering Theory (Grosz et al, 1995). Their system
used a set of hand-engineered rules to perform zero
pronoun identification, and resolved zero pronouns
with a set of hand-engineered resolution rules.
In Iida et al (2006), they proposed a ma-
chine learning approach to resolve zero pronouns in
Japanese using syntactic patterns. Their system also
did not perform zero pronoun identification, and as-
sumed that correctly identified zero pronouns were
given as input to their system.
The probabilistic model of Seki et al (2002) both
identified and resolved Japanese zero pronouns, with
the help of a verb dictionary. Their model needed
large-scale corpora to estimate the probabilities and
to prevent data sparseness.
Ferra?ndez and Peral (2000) proposed a hand-
engineered rule-based approach to identify and re-
solve zero pronouns that are in the subject grammat-
ical position in Spanish.
8 Conclusion
In this paper, we present a machine learning ap-
proach to the identification and resolution of Chi-
nese anaphoric zero pronouns. We perform both
identification and resolution automatically, with two
sets of easily computable features. Experimen-
tal results show that our proposed learning ap-
proach achieves anaphoric zero pronoun resolution
accuracy comparable to a previous state-of-the-art,
heuristic rule-based approach. To our knowledge,
our work is the first to perform both identification
and resolution of Chinese anaphoric zero pronouns
using a machine learning approach.
Obviously, there is much room for improvement.
In future, we plan to apply our model directly on
machine-generated parse trees. We also plan to clas-
sify non-coreferential zero pronouns into the six cat-
egories.
Acknowledgements
We thank Susan Converse and Martha Palmer for
sharing their Chinese third-person pronoun and zero
pronoun coreference corpus.
References
Susan Converse. 2006. Pronominal Anaphora Resolu-
tion in Chinese. Ph.D. thesis, Department of Com-
puter and Information Science, University of Pennsyl-
vania.
Antonio Ferra?ndez and Jesu?s Peral. 2000. A computa-
tional approach to zero-pronouns in Spanish. In Pro-
ceedings of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL2000), pages
166?172.
Radu Florian, Hany Hassan, Abraham Ittycheriah,
Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,
Nicolas Nicolov, and Salim Roukos. 2004. A statisti-
cal model for multilingual entity detection and track-
ing. In Proceedings of the Human Language Tech-
nology Conference and North American Chapter of
the Association for Computational Linguistics Annual
Meeting 2004 (HLT-NAACL2004), pages 1?8.
Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.
1995. Centering: A framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Ryu Iida, Kentaro Inui, and Yuji Matsumoto. 2006. Ex-
ploiting syntactic patterns as clues in zero-anaphora
resolution. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
44th Annual Meeting of the Association for Compu-
tational Linguistics (COLING-ACL2006), pages 625?
632.
Young-Joo Kim. 2000. Subject/object drop in the acqui-
sition of Korean: A cross-linguistic comparison. Jour-
nal of East Asian Linguistics, 9(4):325?351.
Charles N. Li and Sandra A. Thompson. 1979. Third-
person pronouns and zero-anaphora in Chinese dis-
course. Syntax and Semantics, 12:311?335.
Wendan Li. 2004. Topic chains in Chinese discourse.
Discourse Processes, 37(1):25?45.
Andrew McCallum and Ben Wellner. 2005. Conditional
models of identity uncertainty with application to noun
coreference. In Advances in Neural Information Pro-
cessing Systems 17 (NIPS), pages 905?912.
549
Vincent Ng and Claire Cardie. 2002a. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING2002), pages 1?7.
Vincent Ng and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics (ACL2002),
pages 104?111.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings of the 42nd An-
nual Meeting of the Association for Computational
Linguistics (ACL2004), pages 152?159.
Kazuhiro Seki, Atsushi Fujii, and Tetsuya Ishikawa.
2002. A probabilistic method for analyzing Japanese
anaphora integrating zero pronoun detection and reso-
lution. In Proceedings of the 19th International Con-
ference on Computational Linguistics (COLING2002),
pages 911?917.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese TreeBank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew Lim
Tan. 2003. Coreference resolution using competition
learning approach. In Proceedings of the 41st Annual
Meeting of the Association for Computational Linguis-
tics (ACL2003), pages 176?183.
Ching-Long Yeh and Yi-Chun Chen. 2004. Zero
anaphora resolution in Chinese with shallow parsing.
Journal of Chinese Language and Computing.
Yaqian Zhou, Changning Huang, Jianfeng Gao, and Lide
Wu. 2005. Transformation based Chinese entity de-
tection and tracking. In Proceedings of the Second
International Joint Conference on Natural Language
Processing (IJCNLP 2005), pages 232?237.
550
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1308?1316,
Beijing, August 2010
Maximum Metric Score Training for Coreference Resolution
Shanheng Zhao and Hwee Tou Ng
Department of Computer Science
National University of Singapore
{zhaosh,nght}@comp.nus.edu.sg
Abstract
A large body of prior research on coref-
erence resolution recasts the problem as
a two-class classification problem. How-
ever, standard supervised machine learn-
ing algorithms that minimize classifica-
tion errors on the training instances do not
always lead to maximizing the F-measure
of the chosen evaluation metric for coref-
erence resolution. In this paper, we pro-
pose a novel approach comprising the use
of instance weighting and beam search to
maximize the evaluation metric score on
the training corpus during training. Ex-
perimental results show that this approach
achieves significant improvement over the
state-of-the-art. We report results on stan-
dard benchmark corpora (two MUC cor-
pora and three ACE corpora), when evalu-
ated using the link-based MUC metric and
the mention-based B-CUBED metric.
1 Introduction
Coreference resolution refers to the process of
determining whether two or more noun phrases
(NPs) in a text refer to the same entity. Suc-
cessful coreference resolution benefits many nat-
ural language processing tasks. In the literature,
most prior work on coreference resolution recasts
the problem as a two-class classification problem.
Machine learning-based classifiers are applied to
determine whether a candidate anaphor and a po-
tential antecedent are coreferential (Soon et al,
2001; Ng and Cardie, 2002b).
A large body of prior research on corefer-
ence resolution follows the same process: dur-
ing training, they apply standard supervised ma-
chine learning algorithms to minimize the number
of misclassified training instances; during testing,
they maximize either the local or the global proba-
bility of the coreferential relation assignments ac-
cording to the specific chosen resolution method.
However, minimizing the number of misclas-
sified training instances during training does not
guarantee maximizing the F-measure of the cho-
sen evaluation metric for coreference resolution.
First of all, coreference is a rare relation. There
are far fewer positive training instances than neg-
ative ones. Simply minimizing the number of mis-
classified training instances is suboptimal and fa-
vors negative training instances. Secondly, evalu-
ation metrics for coreference resolution are based
on global assignments. Not all errors have the
same impact on the metric score. Furthermore, the
extracted training instances are not equally easy to
be classified.
In this paper, we propose a novel approach
comprising the use of instance weighting and
beam search to address the above issues. Our pro-
posed maximum metric score training (MMST)
approach performs maximization of the chosen
evaluation metric score on the training corpus dur-
ing training. It iteratively assigns higher weights
to the hard-to-classify training instances. The out-
put of training is a standard classifier. Hence,
during testing, MMST is faster than approaches
which optimize the assignment of coreferential re-
lations during testing. Experimental results show
that MMST achieves significant improvements
over the baselines. Unlike most of the previous
work, we report improved results over the state-
of-the-art on all five standard benchmark corpora
1308
(two MUC corpora and three ACE corpora), with
both the link-based MUC metric and the mention-
based B-CUBED metric.
The rest of this paper is organized as follows.
We first review the related work and the evaluation
metrics for coreference resolution in Section 2 and
3, respectively. Section 4 describes the proposed
MMST algorithm. Experimental results and re-
lated discussions are given in Section 5. Finally,
we conclude in Section 6.
2 Related Work
Soon et al (2001) proposed a training and test-
ing framework for coreference resolution. Dur-
ing training, a positive training instance is formed
by a pair of markables, i.e., the anaphor (a noun
phrase) and its closest antecedent (another noun
phrase). Each markable (noun phrase) between
the two, together with the anaphor, form a neg-
ative training instance. A classifier is trained on
all training instances, using a standard supervised
learning algorithm. During testing, all preceding
markables of a candidate anaphor are considered
as potential antecedents, and are tested in a back-
to-front manner. The process stops if either an an-
tecedent is found or the beginning of the text is
reached. This framework has been widely used in
the community of coreference resolution.
Recent work boosted the performance of coref-
erence resolution by exploiting fine-tuned feature
sets under the above framework, or adopting al-
ternative resolution methods during testing (Ng
and Cardie, 2002b; Yang et al, 2003; Denis and
Baldridge, 2007; Versley et al, 2008).
Ng (2005) proposed a ranking model to maxi-
mize F-measure during testing. In the approach, n
different coreference outputs for each test text are
generated, by varying four components in a coref-
erence resolution system, i.e., the learning algo-
rithm, the instance creation method, the feature
set, and the clustering algorithm. An SVM-based
ranker then picks the output that is likely to have
the highest F-measure. However, this approach
is time-consuming during testing, as F-measure
maximization is performed during testing. This
limits its usage on a very large corpus.
In the community of machine learning, re-
searchers have proposed approaches for learning
a model to optimize a chosen evaluation met-
ric other than classification accuracy on all train-
ing instances. Joachims (2005) suggested the use
of support vector machines to optimize nonlinear
evaluation metrics. However, the approach does
not differentiate between the errors in the same
category in the contingency table. Furthermore, it
does not take into account inter-instance relation
(e.g., transitivity), which the evaluation metric for
coreference resolution cares about.
Daume III (2006) proposed a structured learn-
ing framework for coreference resolution to ap-
proximately optimize the ACE metric. Our pro-
posed approach differs in two aspects. First, we
directly optimize the evaluation metric itself, and
not by approximation. Second, unlike the incre-
mental local loss in Daume III (2006), we evaluate
the metric score globally.
In contrast to Ng (2005), Ng and Cardie
(2002a) proposed a rule-induction system with
rule pruning. However, their approach is specific
to rule induction, and is not applicable to other
supervised learning classifiers. Ng (2004) varied
different components of coreference resolution,
choosing the combination of components that re-
sults in a classifier with the highest F-measure on
a held-out development set during training. In
contrast, our proposed approach employs instance
weighting and beam search to maximize the F-
measure of the evaluation metric during training.
Our approach is general and applicable to any su-
pervised learning classifiers.
Recently, Wick and McCallum (2009) pro-
posed a partition-wise model for coreference reso-
lution to maximize a chosen evaluation metric us-
ing the Metropolis-Hastings algorithm (Metropo-
lis et al, 1953; Hastings, 1970). However, they
found that training on classification accuracy, in
most cases, outperformed training on the corefer-
ence evaluation metrics. Furthermore, similar to
Ng (2005), their approach requires the generation
of multiple coreference assignments during test-
ing.
Vemulapalli et al (2009) proposed a document-
level boosting technique for coreference resolu-
tion by re-weighting the documents that have
the lowest F-measures. By combining multiple
classifiers generated in multiple iterations, they
1309
achieved a CEAF score slightly better than the
baseline. Different from them, our approach
works at the instance level, and we output a sin-
gle classifier.
3 Coreference Evaluation Metrics
In this section, we review two commonly used
evaluation metrics for coreference resolution.
First, we introduce the terminology. The gold
standard annotation and the output by a coref-
erence resolution system are called key and re-
sponse, respectively. In both the key and the re-
sponse, a coreference chain is formed by a set of
coreferential mentions. A mention (or markable)
is a noun phrase which satisfies the markable def-
inition in an individual corpus. A link refers to a
pair of coreferential mentions. If a mention has no
links to other mentions, it is called a singleton.
3.1 The MUC Evaluation Metric
Vilain et al (1995) introduced the link-based
MUC evaluation metric for the MUC-6 and MUC-
7 coreference tasks. Let Si be an equivalence
class generated by the key (i.e., Si is a corefer-
ence chain), and p(Si) be a partition of Si relative
to the response. Recall is the number of correctly
identified links over the number of links in the key.
Recall =
?(|Si| ? |p(Si)|)?(|Si| ? 1)
Precision, on the other hand, is defined in the op-
posite way by switching the role of key and re-
sponse. F-measure is a trade-off between recall
and precision.
F = 2 ?Recall ? PrecisionRecall + Precision
3.2 The B-CUBED Evaluation Metric
Bagga and Baldwin (1998) introduced the
mention-based B-CUBED metric. The B-
CUBED metric measures the accuracy of coref-
erence resolution based on individual mentions.
Hence, it also gives credit to the identification of
singletons, which the MUC metric does not. Re-
call is computed as
Recall = 1N
?
d?D
?
m?d
|Om|
|Sm|
where D, d, and m are the set of documents, a
document, and a mention, respectively. Sm is the
equivalence class generated by the key that con-
tains m, while Om is the overlap of Sm and the
equivalence class generated by the response that
contains m. N is the total number of mentions in
D. The precision, again, is computed by switch-
ing the role of key and response. F-measure is
computed in the same way as the MUC metric.
4 Maximum Metric Score Training
Before explaining the algorithm, we describe our
coreference clustering method used during test-
ing. It is the same as most prior work in the lit-
erature, including Soon et al (2001) and Ng and
Cardie (2002b). The individual classification de-
cisions made by the coreference classifier do not
guarantee that transitivity of coreferential NPs is
obeyed. So it can happen that the pair A and B,
and the pair B and C are both classified as coref-
erential, but the pair A and C is not classified
as coreferential by the classifier. After all coref-
erential markable pairs are found (no matter by
closest-first, best-first, or resolving-all strategies
as in different prior work), all coreferential pairs
are clustered together to form the coreference out-
put. By doing so, transitivity is kept: a markable is
in a coreference chain if and only if it is classified
to be coreferential to at least one other markable
in the chain.
4.1 Instance Weighting
Suppose there are mk and mr coreferential links
in the key and the response, respectively, and a
coreference resolution system successfully pre-
dicts n correct links. The recall and the preci-
sion are then nmk and
n
mr , respectively. The learnt
classifier predicts false positive and false negative
instances during testing. For a false positive in-
stance, if we could successfully predict it as neg-
ative, the recall is unchanged, but the precision
will be nmr?1 , which is higher than the original
precision nmr . For a false negative instance, it
is more subtle. If the two markables in the in-
stance are determined to be in the same corefer-
ence chain by the clustering algorithm, it does not
matter whether we predict this instance as posi-
tive or negative, i.e., this false negative does not
1310
change the F-measure of the evaluation metric at
all. If the two markables are not in the same coref-
erence chain under the clustering, in case that we
can predict it as positive, the recall will be n+1mk ,
which is higher than the original recall nmk , and
the precision will be n+1mr+1 , which is higher than
the original precision nmr , as n < mr. In both
cases, the F-measure improves. If we can instruct
the learning algorithm to pay more attention to
these false positive and false negative instances
and to predict them correctly by assigning them
more weight, we should be able to improve the
F-measure.
In the literature, besides the training instance
extraction methods proposed by Soon et al
(2001) and Ng and Cardie (2002b) as discussed
in Section 2, McCarthy and Lehnert (1995) used
all possible pairs of training instances. We also
use all pairs of training instances in our approach
to keep as much information as possible. Initially
all the pairs are equally weighted. We then itera-
tively assign more weights to the hard-to-classify
pairs. The iterative process is conducted by a
beam search algorithm.
4.2 Beam Search
Our proposed MMST algorithm searches for a set
of weights to assign to training instances such
that the classifier trained on the weighted training
instances gives the maximum coreference metric
score when evaluated on the training instances.
Beam search is used to limit the search. Each
search state corresponds to a set of weighted train-
ing instances, a classifier trained on the weighted
training instances minimizing misclassifications,
and the F-measure of the classifier when evalu-
ated on the weighted training instances using the
chosen coreference evaluation metric. The root
of the search tree is the initial search state where
all the training instances have identical weights of
one. Each search state s can expand into two dif-
ferent children search states sl and sr. sl (sr) cor-
responds to assigning higher weights to the false
positive (negative) training instances in s. The
search space thus forms a binary search tree.
Figure 1 shows an example of a binary search
tree. Initially, the tree has only one node: the root
(node 1 in the figure). In each iteration, the algo-
1
2 3
4 5 6 7
8 9 10 11
Figure 1: An example of a binary search tree
rithm expands all the leaf nodes in the beam. For
example, in the first iteration, node 1 is expanded
to generate node 2 and 3, which corresponds to
adding weights to false positive and false nega-
tive training instances, respectively. An expanded
node always has two children in the binary search
tree. All the nodes are then sorted in descending
order of F-measure. Only the top M nodes are
kept, and the remaining nodes are discarded. The
discarded nodes can either be leaf nodes or non-
leaf nodes. For example, if node 5 is discarded
because of low F-measure, it will not be expanded
to generate children in the binary search tree. The
iterative algorithm stops when all the nodes in the
beam are non-leaf nodes, i.e., all the nodes in the
beam have been expanded.
Figure 2 gives the formal description of the
proposed maximum metric score training algo-
rithm. In the algorithm, assume that we have
N texts T1, T2, . . ., TN in the training data set.
mki and mkj are the ith and jth markable in
the text Tk, respectively. Hence, for all i <
j, (mki,mkj , wkij) is a training instance for the
markable pair (mki,mkj), in which wkij is the
weight of the instance. Let Lkij and L?kij be the
true and predicted label of the pair (mki,mkj),
respectively. Let W , C, F , and E be the set of
weights {wkij |1 ? k ? N, i < j}, the classifier,
the F-measure, and a boolean indicator of whether
the search state has been expanded, respectively.
Finally, M is the beam size, and ? controls how
much we update the weights in each iteration.
Since we train the model on all possible pairs,
during testing we also test if a potential anaphor is
coreferential to each preceding antecedent.
1311
INPUT: T1, T2, . . . , TN
OUTPUT: classifier C
wkij ? 1, for all 1 ? k ? N and i < j
C ? train({(mki,mkj , wkij)|1 ? k ? N, i < j})
F ? resolve and evaluate T1, . . . , TN with C
E ? false
BEAM? {(W,C, F,E)}
repeat
BEAM? ? {}
for all (W,C, F,E) in BEAM do
BEAM? ? BEAM?
?
{(W,C, F, true)}
if E=false then
predict all L?kij with C (1 ? k ? N, i < j)
cluster into coreference chains based on L?kij
W ? ? W
for all 1 ? k ? N, i < j do
if Lkij = false and L?kij = true then
w?kij ? w?kij + ?
end if
end for
C? ? train({(mki,mkj , w?kij)|1 ? k ? N, i < j})
F ? ? resolve and evaluate T1, . . . , TN with C?
BEAM? ? BEAM?
?
{(W ?, C?, F ?, false)}
W ?? ? W
for all 1 ? k ? N, i < j do
if Lkij = true and L?kij = false and
Chain(mki) 6= Chain(mkj) then
w??kij ? w??kij + ?
end if
end for
C?? ? train({(mki,mkj , w??kij)|1 ? k ? N, i < j})
F ?? ? resolve and evaluate T1, . . . , TN with C??
BEAM? ? BEAM?
?
{(W ??, C??, F ??, false)}
end if
end for
BEAM? BEAM?
sort BEAM in descending order of F , keep top M elements
until for all E of all elements in BEAM, E = true
return C, from the top element (W,C, F,E) of BEAM
Figure 2: The maximum metric score training
(MMST) algorithm
5 Experiments
5.1 Experimental Setup
In the experiments, we used all the five commonly
used evaluation corpora for coreference resolu-
tion, namely the two MUC corpora (MUC6 and
MUC7) and the three ACE corpora (BNEWS,
NPAPER, and NWIRE). The MUC6 and the
MUC7 corpora were defined in the DARPA Mes-
sage Understanding Conference (MUC-6, 1995;
MUC-7, 1998). The dry-run texts were used as the
training data sets. In both corpora, each training
data set contains 30 texts. The test data sets for
MUC6 and MUC7 consist of the 30 and 20 for-
mal evaluation texts, respectively. The ACE cor-
pora were defined in NIST Automatic Content Ex-
traction phase 2 (ACE-2) (NIST, 2002). The three
data sets are from different news sources: broad-
cast news (BNEWS), newspaper (NPAPER), and
newswire (NWIRE). Each of the three data sets
contains two portions: training and development
test. They were used as our training set and test
set, respectively. The BNEWS, NPAPER, and
NWIRE data sets contain 216, 76, and 130 train-
ing texts, and 51, 17, and 29 test texts, respec-
tively.
Unlike some previous work on coreference res-
olution that assumes that the gold standard mark-
ables are known, we work directly on raw text in-
put. Versley et al (2008) presented the BART
package1, an open source coreference resolution
toolkit, that accepts raw text input and reported
state-of-the-art MUC F-measures on the three
ACE corpora. BART uses an extended feature set
and tree kernel support vector machines (SVM)
under the Soon et al (2001) training and testing
framework. We used the BART package in our ex-
periments, and implemented the proposed MMST
algorithm on top of it. In our experiments reported
in this paper, the features we used are identical to
the features output by the preprocessing code of
BART reported in Versley et al (2008), except
that we did not use their tree-valued and string-
valued features (see the next subsection for de-
tails).
Since we use automatically extracted mark-
ables, it is possible that some extracted markables
and the gold standard markables are unmatched,
or twinless as defined in Stoyanov et al (2009).
How to use the B-CUBED metric for evaluating
twinless markables has been explored recently. In
this paper, we adopt the B3all variation proposed
by Stoyanov et al (2009), which retains all twin-
less markables. We also experimented with their
B30 variation, which gave similar results. Note
that no matter which variant of the B-CUBED
metric is used, it is a fair comparison as long as
the baseline and our proposed MMST algorithm
are compared against each other using the same
B-CUBED variant.
5.2 The Baseline Systems
We include state-of-the-art coreference resolution
systems in the literature for comparison. Since
we use the BART package in our experiments,
1http://www.sfs.uni-tuebingen.de/
?
versley/BART/
1312
we include the results of the original BART sys-
tem (with its extended feature set and SVM-light-
TK (Moschitti, 2006), as reported in Versley et al
(2008)) as the first system for comparison. Vers-
ley et al (2008) reported only the results on the
three ACE data sets with the MUC evaluation met-
ric. Since we used all the five data sets in our
experiments, for fair comparison, we also include
the MUC results reported in Ng (2004). To the
best of our knowledge, Ng (2004) was the only
prior work which reported MUC metric scores on
all the five data sets. The MUC metric scores of
Versley et al (2008) and Ng (2004) are listed in
the row ?Versley et al 08? and ?Ng 04?, respec-
tively, in Table 1. For the B-CUBED metric, we
include Ng (2005) for comparison, although it is
unclear how Ng (2005) interpreted the B-CUBED
metric. The scores are listed in the row ?Ng 05?
in Table 2.
Tree kernel SVM learning is time-consuming.
To reduce the training time needed, instead of us-
ing SVM-light-TK, we used a much faster learn-
ing algorithm, J48, which is the WEKA imple-
mentation of the C4.5 decision tree learning algo-
rithm. (Quinlan, 1993; Witten and Frank, 2005).
As tree-valued features and string-valued features
cannot be used with J48, in our experiments we
excluded them from the extended feature set that
BART used to produce state-of-the-art MUC F-
measures on the three ACE corpora. All our re-
sults in this paper were obtained using this re-
duced feature set and J48 decision tree learn-
ing. However, given sufficient computational re-
sources, our proposed approach is able to apply to
any supervised machine learning algorithms.
Our baselines that follow the Soon et al (2001)
framework, using the reduced feature set and J48
decision tree learning, are shown in the row ?SNL-
Style Baseline? in Table 1 and 2. The results
suggest that our baseline system is comparable
to the state of the art. Although in Table 1, the
performance of the SNL-style baseline is slightly
lower than Versley et al (2008) on the three ACE
corpora, the computational time needed has been
greatly reduced.
Our MMST algorithm trains and tests on all
pairs of markables. To show the effectiveness of
weight updating of MMST, we built another base-
line which trains and tests on all pairs. The per-
formance of this system is shown in the row ?All-
Style Baseline? in Table 1 and 2.
5.3 Results Using Maximum Metric Score
Training
Next, we show the results of using the proposed
maximum metric score training algorithm. From
the description of the algorithm, it can be seen that
there are two parameters in the algorithm. One
parameter is M , the size of the beam. The other
parameter is ?, which controls how much we in-
crease the weight of a training instance in each
iteration.
Since the best M and ? for the MUC evaluation
metric were not known, we used held-out develop-
ment sets to tune the parameters. Specifically, we
trained classifiers with different combinations of
M and ? on a development training set, and eval-
uated their performances on a development test
set. In our experiments, the development training
set contained 2/3 of the texts in the training set
of each individual corpus, while the development
test set contained the remaining 1/3 of the texts.
After having picked the best M and ? values, we
trained a classifier on the entire training set with
the chosen parameters. The learnt classifier was
then applied to the test set.
2 4 6 8 10 12 14 16 18 20
52
54
56
58
60
62
64
66
68
M
F?
me
asu
re
MMST
SNL?Style Baseline
All?Style Baseline
Figure 3: Tuning M on the held-out development
set
To limit the search space, we tuned the two
parameters sequentially. First, we fixed ? =
1, which is equivalent to duplicating each train-
ing instance once in J48, and evaluated M =
2, 4, 6, . . . , 20. After having chosen the best
M that corresponded to the maximum F-measure,
we fixed the value of M , and evaluated ? =
0.1, 0.2, 0.3, . . . , 2.0. Take MUC6 as an exam-
1313
MUC6 MUC7 BNEWS NPAPER NWIRE
Model R P F R P F R P F R P F R P F
Versley et al 08 ? ? 60.7 65.4 63.0 64.1 67.7 65.8 60.4 65.2 62.7
Ng 04 75.8 61.4 67.9 64.2 60.2 62.1 63.1 67.8 65.4 73.5 63.3 68.0 53.1 60.6 56.6
SNL-Style Baseline 67.0 49.2 56.7 63.0 54.2 58.3 57.4 64.3 60.7 61.6 67.3 64.3 58.6 66.1 62.1
All-Style Baseline 56.9 69.2 62.5 51.5 73.4 60.6 53.0 76.7 62.7 56.3 75.4 64.4 53.0 74.5 61.9
MMST 73.3 59.9 65.9???? 66.8 59.8 63.1??? 70.5 61.9 65.9??? 69.9 64.0 66.8? 64.7 64.7 64.7???
M = 6, ? = 1.0 M = 6, ? = 0.7 M = 6, ? = 1.8 M = 6, ? = 0.9 M = 14, ? = 0.7
Table 1: Results for the two MUC and three ACE corpora with MUC evaluation metric
MUC6 MUC7 BNEWS NPAPER NWIRE
Model R P F R P F R P F R P F R P F
Ng 05 ? ? 57.0 77.1 65.6 62.8 71.2 66.7 59.3 75.4 66.4
SNL-Style Baseline 57.8 74.4 65.1 57.6 76.5 65.7 62.0 74.7 67.8 61.8 70.4 65.8 65.8 75.9 70.5
All-Style Baseline 51.6 86.3 64.6 49.1 90.1 63.6 61.6 83.7 71.0 63.9 74.0 68.6 64.8 80.1 71.7
MMST 62.7 81.5 70.9???? 61.8 73.6 67.2?? 61.6 83.7 71.0?? 63.1 76.2 69.1?? 64.3 81.0 71.7
M = 6, ? = 1.0 M = 8, ? = 0.8 M = 6, ? = 0.9 M = 14, ? = 0.5 M = 6, ? = 0.1
Table 2: Results for the two MUC and three ACE corpora with B3 evaluation metric
0 0.2 0.4 0.6 0.8 1 1.2 1.4 1.6 1.8 2
52
54
56
58
60
62
64
66
68
delta
F?
me
asu
re
MMST
SNL?Style Baseline
All?Style Baseline
Figure 4: Tuning ? on the held-out development
set
ple. The results of tuning M on MUC6 are shown
in Figure 3. The maximum F-measure is obtained
when M = 4 and M = 6. On all the different M
values we have tried, MMST outperforms both the
SNL-style baseline and the All-style baseline on
the development test set. We then fixed M = 6,
and evaluated different ? values. The results are
shown in Figure 4. The best F-measure was ob-
tained when ? = 1.0. Again, on all the different
? values we have tried, MMST outperforms both
baselines on the development test set.
The rows ?MMST? in Table 1 and 2 show the
performance of MMST on the test sets, with the
tuned parameters indicated. In our experiments,
the statistical significance test was conducted as
in Chinchor (1995). ? and ?? stand for p < 0.05
and p < 0.01 over the SNL-style baseline, respec-
tively. ? and ?? stand for p < 0.05 and p < 0.01
over the All-style baseline, respectively.
For the MUC metric, when compared to the
All-style baseline, MMST gains 3.4, 2.5, 3.2, 2.4,
and 2.8 improvement in F-measure on MUC6,
MUC7, BNEWS, NPAPER, and NWIRE, respec-
tively. The experimental results clearly show that
MMST gains not only consistent, but also sta-
tistically significant improvement over both the
SNL-style baseline and the All-style baseline in all
combinations (five data sets and two baselines) on
the MUC metric, except that it is not significant
(p = 0.06) over the SNL-style baseline in NPA-
PER. As for the B-CUBED metric, MMST gains
significant improvement in F-measure on MUC6
and MUC7 data sets, while its performance on
the three ACE data sets are comparable to the All-
style baseline.
5.4 Discussion
To see how MMST actually updates the weight,
we use the MUC metric as an example. Under the
experimental settings, it takes 6 ? 9 iterations for
MMST to stop on the five data sets. The number
of explored states in the binary search tree, includ-
ing the root, is 33, 39, 25, 29, and 75 on MUC6,
MUC7, BNEWS, NPAPER, and NWIRE, respec-
tively. It is instructive to find out the final weight
of each instance. Take MUC6 as an example, the
number of positive instances with weight 1, 2, 3,
and 4 are 5,204, 1,568, 1,379, and 1,844, respec-
tively, while the number of negative instances with
weight 1 and 2 are 503,141 and 1,755, respec-
1314
tively. Counting the weighted number of instances
(e.g., an instance with weight 2 is equivalent to 2
instances), we have 19,853 positive and 506,651
negative training instances. This changes the ratio
of the positive instances from 1.9% to 3.8%. As a
by-product, MMST reduces data skewness, while
using all possible NP pairs for training to keep as
much information as possible.
The change of weights of the training instances
is equivalent to the change of distribution of the
training instances. This effectively changes the
classification hypothesis to the one that tends to
yield higher evaluation metric score. Take the fol-
lowing sentence in the MUC6 data set as an ex-
ample:
In a news release, the company said the new
name more accurately reflects its focus on high-
technology communications, including business
and entertainment software, interactive media
and wireless data and voice transmission.
In the above example, the pronoun its is coref-
erential to the antecedent NP the company. The
baseline classifier gives a probability of 0.02 that
the two NPs are coreferential. The pair is clas-
sified wrongly and none of the other pairs in the
article can link the two NPs together through clus-
tering. However, with MMST, this probability in-
creases to 0.54, which leads to the correct classi-
fication. This is because the baseline classifier is
not good at predicting in the case when the sec-
ond markable is a pronoun. In the above exam-
ple, its can have another candidate antecedent the
new name. There are far more negative training
instances than positive ones for this case. In fact,
in the induced decision tree by the baseline, the
leaf node corresponding to the pair the company
? its has 7,782 training instances, out of which
only 175 are positive. With MMST, however,
these numbers decrease to 83 and 45, respectively.
MMST also promotes the Anaphor Is Pronoun
feature to a higher level in the decision tree. Al-
though we use decision tree to illustrate the work-
ing of the algorithm, MMST is not limited to tree
learning, and can make use of any learning algo-
rithms that are able to take advantage of instance
weighting.
It can also be seen that with the B-CUBED
metric, MMST gains improvement on MUC6 and
MUC7, but not on the three ACE corpora. How-
ever, the results of MMST on the three ACE cor-
pora with the B-CUBED evaluation metric are at
least comparable with the All-style baseline. This
is because we always pick the classifier which cor-
responds to the maximum evaluation metric score
on the training set and the classifier correspond-
ing to the All-style baseline is one of the candi-
dates. In addition, our MMST approach improves
upon state-of-the-art results (Ng, 2004; Ng, 2005;
Versley et al, 2008) on most of the five standard
benchmark corpora (two MUC corpora and three
ACE corpora), with both the link-based MUC
metric and the mention-based B-CUBED metric.
Finally, our approach performs all the F-
measure maximization during training, and is very
fast during testing, since the output of the MMST
algorithm is a standard classifier. For example,
on the MUC6 data set with the MUC evaluation
metric, it took 1.6 hours and 31 seconds for train-
ing and testing, respectively, on an Intel Xeon
2.33GHz machine.
6 Conclusion
In this paper, we present a novel maximum met-
ric score training approach comprising the use of
instance weighting and beam search to maximize
the chosen coreference metric score on the train-
ing corpus during training. Experimental results
show that the approach achieves significant im-
provement over the baseline systems. The pro-
posed approach improves upon state-of-the-art re-
sults on most of the five standard benchmark cor-
pora (two MUC corpora and three ACE corpora),
with both the link-based MUC metric and the
mention-based B-CUBED metric.
Acknowledgments
We thank Yannick Versley for providing us
the BART package and the preprocessed data.
This research was done for CSIDM Project No.
CSIDM-200804 partially funded by a grant from
the National Research Foundation (NRF) ad-
ministered by the Media Development Authority
(MDA) of Singapore.
1315
References
Bagga, Amit and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings of
the LREC1998, pages 563?566.
Chinchor, Nancy. 1995. Statistical significance of
MUC-6 results. In Proceedings of the MUC-6,
pages 39?43.
Daume III, Hal. 2006. Practical Structured Learn-
ing for Natural Language Processing. Ph.D. thesis,
University of Southern California.
Denis, Pascal and Jason Baldridge. 2007. Joint deter-
mination of anaphoricity and coreference resolution
using integer programming. In Proceedings of the
NAACL-HLT2007, pages 236?243.
Hastings, W. K. 1970. Monte Carlo sampling meth-
ods using Markov chains and their applications.
Biometrika, 57(1):97?109.
Joachims, Thorsten. 2005. A support vector method
for multivariate performance measures. In Proceed-
ings of the ICML2005, pages 377?384.
McCarthy, Joseph F. and Wendy G. Lehnert. 1995.
Using decision trees for coreference resolution. In
Proceedings of the IJCAI1995, pages 1050?1055.
Metropolis, Nicholas, Arianna W. Rosenbluth, Mar-
shall N. Rosenbluth, Augusta H. Teller, and Edward
Teller. 1953. Equation of state calculations by fast
computing machines. Journal of Chemical Physics,
21(6):1087?1092.
Moschitti, Alessandro. 2006. Making tree kernels
practical for natural language learning. In Proceed-
ings of the EACL2006, pages 113?120.
MUC-6. 1995. Coreference task definition (v2.3, 8
Sep 95). In Proceedings of the MUC-6, pages 335?
344.
MUC-7. 1998. Coreference task definition (v3.0, 13
Jul 97). In Proceedings of the MUC-7.
Ng, Vincent and Claire Cardie. 2002a. Combining
sample selection and error-driven pruning for ma-
chine learning of coreference rules. In Proceedings
of the EMNLP2002, pages 55?62.
Ng, Vincent and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL2002, pages 104?111.
Ng, Vincent. 2004. Improving Machine Learning Ap-
proaches to Noun Phrase Coreference Resolution.
Ph.D. thesis, Cornell University.
Ng, Vincent. 2005. Machine learning for coreference
resolution: From local classification to global rank-
ing. In Proceedings of the ACL2005, pages 157?
164.
NIST. 2002. The ACE 2002 evaluation plan.
ftp://jaguar.ncsl.nist.gov/ace/
doc/ACE-EvalPlan-2002-v06.pdf.
Quinlan, J. Ross. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann.
Soon, Wee Meng, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Stoyanov, Veselin, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the ACL-IJCNLP2009,
pages 656?664.
Vemulapalli, Smita, Xiaoqiang Luo, John F. Pitrelli,
and Imed Zitouni. 2009. Classifier combination
techniques applied to coreference resolution. In
Proceedings of the NAACL-HLT2009 Student Re-
search Workshop and Doctoral Consortium, pages
1?6.
Versley, Yannick, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolu-
tion. In Proceedings of the ACL2008:HLT Demo
Session, pages 9?12.
Vilain, Marc, John Burger, John Aberdeen, Dennis
Connolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the MUC-6, pages 45?52.
Wick, Michael and Andrew McCallum. 2009. Ad-
vances in learning and inference for partition-
wise models of coreference resolution. Techni-
cal Report UM-CS-2009-028, University of Mas-
sachusets, Amherst, USA.
Witten, Ian H. and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
The Morgan Kaufmann Series in Data Management
Systems. Morgan Kaufmann Publishers, second edi-
tion.
Yang, Xiaofeng, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2003. Coreference resolution us-
ing competition learning approach. In Proceedings
of the ACL2003, pages 176?183.
1316
Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21?29,
Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational Linguistics
Domain Adaptation with Active Learning for Coreference Resolution
Shanheng Zhao
Elance
441 Logue Ave
Mountain View, CA 94043, USA
szhao@elance.com
Hwee Tou Ng
Department of Computer Science
National University of Singapore
13 Computing Drive, Singapore 117417
nght@comp.nus.edu.sg
Abstract
In the literature, most prior work on
coreference resolution centered on the
newswire domain. Although a coreference
resolution system trained on the newswire
domain performs well on newswire texts,
there is a huge performance drop when it is
applied to the biomedical domain. In this
paper, we present an approach integrat-
ing domain adaptation with active learning
to adapt coreference resolution from the
newswire domain to the biomedical do-
main. We explore the effect of domain
adaptation, active learning, and target do-
main instance weighting for coreference
resolution. Experimental results show
that domain adaptation with active learn-
ing and target domain instance weighting
achieves performance on MEDLINE ab-
stracts similar to a system trained on coref-
erence annotation of only target domain
training instances, but with a greatly re-
duced number of target domain training
instances that we need to annotate.
1 Introduction
Coreference resolution is the task of determin-
ing whether two or more noun phrases (NPs) in
a text refer to the same entity. Successful coref-
erence resolution benefits many natural language
processing (NLP) tasks, such as information ex-
traction and question answering. In the literature,
most prior work on coreference resolution recasts
the problem as a two-class classification problem.
Machine learning-based classifiers are applied to
determine whether a candidate anaphor and a po-
tential antecedent are coreferential (Soon et al.,
2001; Ng and Cardie, 2002; Stoyanov et al., 2009;
Zhao and Ng, 2010).
In recent years, with the advances in biologi-
cal and life science research, there is a rapidly in-
creasing number of biomedical texts, including re-
search papers, patent documents, etc. This results
in an increasing demand for applying natural lan-
guage processing and information retrieval tech-
niques to efficiently exploit information contained
in these large amounts of texts. However, corefer-
ence resolution, one of the core tasks in NLP, has
only a relatively small body of prior research in
the biomedical domain (Kim et al., 2011a; Kim et
al., 2011b).
A large body of prior research on coreference
resolution focuses on texts in the newswire do-
main. Standardized data sets, such as MUC
(DARPA Message Understanding Conference,
(MUC-6, 1995; MUC-7, 1998)) and ACE (NIST
Automatic Content Extraction Entity Detection
and Tracking task, (NIST, 2002)) data sets are
widely used in the study of coreference resolution.
Traditionally, in order to apply supervised ma-
chine learning approaches to an NLP task in a spe-
cific domain, one needs to collect a text corpus
in the domain and annotate it to serve as training
data. Compared to other NLP tasks, e.g., part-of-
speech (POS) tagging or named entity (NE) tag-
ging, the annotation for coreference resolution is
much more challenging and time-consuming. The
reason is that in tasks like POS tagging, an annota-
tor only needs to focus on each markable (a word,
in the case of POS tagging) and a small window
of its neighboring words. In contrast, to annotate
a coreferential relation, an annotator needs to first
recognize whether a certain text span is a mark-
able, and then scan through the text preceding the
markable (a potential anaphor) to look for the an-
tecedent. It also requires the annotator to under-
stand the text in order to annotate coreferential re-
lations, which are semantic in nature. If a mark-
able is non-anaphoric, the annotator has to scan to
the beginning of the text to realize that. Cohen
et al. (2010) reported that it took an average of 20
hours to annotate coreferential relations in a single
21
document with an average length of 6,155 words,
while an annotator could annotate 3,000 words per
hour in POS tag annotation (Marcus et al., 1993).
The simplest approach to avoid the time-
consuming data annotation in a new domain is
to train a coreference resolution system on a
resource-rich domain and apply it to a different
target domain without any additional data anno-
tation. Although coreference resolution systems
work well on test texts in the same domain as
the training texts, there is a huge performance
drop when they are tested on a different domain.
This motivates the usage of domain adaptation
techniques for coreference resolution: adapting a
coreference resolution system from one source do-
main in which we have a large collection of an-
notated data, to a second target domain in which
we need good performance. It is almost inevitable
that we annotate some data in the target domain to
achieve good coreference resolution performance.
The question is how to minimize the amount of an-
notation needed. In the literature, active learning
has been exploited to reduce the amount of anno-
tation needed (Lewis and Gale, 1994). In contrast
to annotating the entire data set, active learning se-
lects only a subset of the data to annotate in an iter-
ative process. How to apply active learning and in-
tegrate it with domain adaptation remains an open
problem for coreference resolution.
In this paper, we explore domain adaptation
for coreference resolution from the resource-rich
newswire domain to the biomedical domain. Our
approach comprises domain adaptation, active
learning, and target domain instance weighting
to leverage the existing annotated corpora from
the newswire domain, so as to reduce the cost
of developing a coreference resolution system in
the biomedical domain. Our approach achieves
comparable coreference resolution performance
on MEDLINE abstracts, but with a large reduction
in the number of training instances that we need to
annotate. To the best of our knowledge, our work
is the first to combine domain adaptation and ac-
tive learning for coreference resolution.
The rest of this paper is organized as follows.
We first review the related work in Section 2. Then
we describe the coreference resolution system in
Section 3, and the domain adaptation and active
learning techniques in Section 4. Experimental re-
sults are presented in Section 5. Finally, we ana-
lyze the results in Section 6 and conclude in Sec-
tion 7.
2 Related Work
Not only is there a relatively small body of prior
research on coreference resolution in the biomed-
ical domain, there are also fewer annotated cor-
pora in this domain. Castan?o et al. (2002) were
among the first to annotate coreferential relations
in the biomedical domain. Their annotation only
concerned the pronominal and nominal anaphoric
expressions in 46 biomedical abstracts. Gasperin
and Briscoe (2007) annotated coreferential rela-
tions on 5 full articles in the biomedical domain,
but only on noun phrases referring to bio-entities.
Yang et al. (2004) annotated full NP coreferential
relations on biomedical abstracts of the GENIA
corpus. The ongoing project of the CRAFT cor-
pus is expected to annotate all coreferential rela-
tions on full text of biomedical articles (Cohen et
al., 2010).
Unlike the work of (Castan?o et al., 2002),
(Gasperin and Briscoe, 2008), and (Gasperin,
2009) that resolved coreferential relations on cer-
tain restricted entities in the biomedical domain,
we resolve all NP coreferential relations. Al-
though the GENIA corpus contains 1,999 biomed-
ical abstracts, Yang et al. (2004) tested only on 200
abstracts under 5-fold cross validation. In contrast,
we randomly selected 399 abstracts in the 1,999
MEDLINE abstracts of the GENIA-MEDCo cor-
pus as the test set, and as such our evaluation was
carried out on a larger scale.
Domain adaptation has been studied and suc-
cessfully applied to many natural language pro-
cessing tasks (Jiang and Zhai, 2007; Daume III,
2007; Dahlmeier and Ng, 2010; Yang et al., 2012).
On the other hand, active learning has also been
applied to NLP tasks to reduce the need of data an-
notation in the literature (Tang et al., 2002; Laws
et al., 2012; Miller et al., 2012). Unlike the afore-
mentioned work that applied only one of domain
adaptation or active learning to NLP tasks, we
combine both. There is relatively less research
on combining domain adaptation and active learn-
ing together for NLP tasks (Chan and Ng, 2007;
Zhong et al., 2008; Rai et al., 2010). Chan and
Ng (2007) and Zhong et al. (2008) used count
merging and augment, respectively, as their do-
main adaptation techniques whereas we apply and
compare multiple state-of-the-art domain adapta-
tion techniques. Rai et al. (2010) exploited a
22
streaming active learning setting whereas ours is
pool-based.
Dahlmeier and Ng (2010) evaluated the perfor-
mance of three previously proposed domain adap-
tation algorithms for semantic role labeling. They
evaluated the performance of domain adaptation
with different sizes of target domain training data.
In each of their experiments with a certain target
domain training data size, the target domain train-
ing data were added all at once. In contrast, we add
the target domain training instances selectively in
an iterative process. Different from (Dahlmeier
and Ng, 2010), we weight the target domain in-
stances to further boost the performance of do-
main adaptation. Our work is the first system-
atic study of domain adaptation with active learn-
ing for coreference resolution. Although Gasperin
(2009) tried to apply active learning for anaphora
resolution, her results were negative: using ac-
tive learning was not better than randomly select-
ing instances in her work. Miwa et al. (2012)
incorporated a rule-based coreference resolution
system for automatic biomedical event extraction,
and showed that by adding training data from other
domains as supplementary training data and us-
ing domain adaptation, one can achieve a higher
F-measure in event extraction.
3 Coreference Resolution
The gold standard annotation and the output by a
coreference resolution system are called the key
and the response, respectively. In both the key and
the response, a coreference chain is formed by a
set of coreferential markables. A markable is a
noun phrase which satisfies the markable defini-
tion in an individual corpus. Here is an example:
When the same MTHC lines are ex-
posed to TNF-alpha in combination with
IFN-gamma, the cells instead become
DC.
In the above sentence, the same MTHC lines
and the cells are referring to the same entity and
hence are coreferential. It is possible that more
than two markables are coreferential in a text. The
task of coreference resolution is to determine these
relations in a given text.
To evaluate the performance of coreference res-
olution, we follow the MUC evaluation metric in-
troduced by (Vilain et al., 1995). Let S
i
be an
equivalence class generated by the key (i.e., S
i
is a coreference chain), and p(S
i
) be a partition
of S
i
relative to the response. Recall is the num-
ber of correctly identified links over the number of
links in the key: Recall =
?
(|S
i
|?|p(S
i
)|)
?
(|S
i
|?1)
. Pre-
cision, on the other hand, is defined in the oppo-
site way by switching the role of key and response.
F-measure is a trade-off between recall and preci-
sion: F = 2?Recall?Precision
Recall+Precision
.
4 Domain Adaptation with Active
Learning
4.1 Domain Adaptation
Domain adaptation is applicable when one has
a large amount of annotated training data in the
source domain and a small amount or none of
the annotated training data in the target domain.
We evaluate the AUGMENT technique introduced
by (Daume III, 2007), as well as the INSTANCE
WEIGHTING (IW) and the INSTANCE PRUNING
(IP) techniques introduced by (Jiang and Zhai,
2007).
4.1.1 AUGMENT
Daume III (2007) introduced a simple domain
adaptation technique by feature space augmenta-
tion. It maps the feature space of each instance
into a feature space of higher dimension. Suppose
x is the feature vector of an instance. Define ?s
and ?t to be the mappings of an instance from
the original feature space to an augmented feature
space in the source and the target domain, respec-
tively:
?
s
(x) = ?x, x,0? (1)
?
t
(x) = ?x,0, x? (2)
where 0 = ?0, 0, . . . , 0? is a zero vector of length
|x|. The mapping can be treated as taking each
feature in the original feature space and making
three versions of it: a general version, a source-
specific version, and a target-specific version. The
augmented source domain data will contain only
the general and the source-specific versions, while
the augmented target domain data will contain
only the general and the target-specific versions.
4.1.2 INSTANCE WEIGHTING and INSTANCE
PRUNING
Let x and y be the feature vector and the corre-
sponding true label of an instance, respectively.
23
Jiang and Zhai (2007) pointed out that when ap-
plying a classifier trained on a source domain to
a target domain, the joint probability P
t
(x, y) in
the target domain may be different from the joint
probability P
s
(x, y) in the source domain. They
proposed a general framework to use P
s
(x, y) to
estimate P
t
(x, y). The joint probability P (x, y)
can be factored into P (x, y) = P (y|x)P (x). The
adaptation of the first component is labeling adap-
tation, while the adaptation of the second compo-
nent is instance adaptation. We explore only label-
ing adaptation.
To calibrate the conditional probability P (y|x)
from the source domain to the target domain, ide-
ally each source domain training instance (x
i
, y
i
)
should be given a weight Pt(y
s
i
|x
s
i
)
P
s
(y
s
i
|x
s
i
)
. Although
P
s
(y
s
i
|x
s
i
) can be estimated from the source do-
main training data, the estimation of P
t
(y
s
i
|x
s
i
)
is much harder. Jiang and Zhai(2007) proposed
two methods to estimate P
t
(y
s
i
|x
s
i
): INSTANCE
WEIGHTING and INSTANCE PRUNING. Both
methods first train a classifier with a small amount
of target domain training data. Then, INSTANCE
WEIGHTING directly estimates P
t
(y
s
i
|x
s
i
) using
the trained classifier. INSTANCE PRUNING, on the
other hand, removes the top N source domain in-
stances that are predicted wrongly, ranked by the
prediction confidence.
4.1.3 Target Domain Instance Weighting
Both INSTANCE WEIGHTING and INSTANCE
PRUNING set the weights of the source domain
instances. In domain adaptation, there are typi-
cally many more source domain training instances
than target domain training instances. Target do-
main instance weighting can effectively reduce the
imbalance. Unlike INSTANCE WEIGHTING and
INSTANCE PRUNING in which each source do-
main instance is weighted individually, we give
all target domain instances the same weight. This
target domain instance weighting scheme is not
only complementary to INSTANCE WEIGHTING
and INSTANCE PRUNING, but is also applicable
to AUGMENT.
4.2 Active Learning
Active learning iteratively selects the most infor-
mative instances to label, adds them to the train-
ing data pool, and trains a new classifier with the
enlarged data pool. We follow (Lewis and Gale,
1994) and use the uncertainty sampling strategy in
our active learning setting.
D
s
? the set of source domain training instances
D
t
? the set of target domain training instances
D
a
? ?
?? coreference resolution system trained on D
s
T ? number of iterations
for i from 1 to T do
for each d
i
? D
t
do
?
d
i
? prediction of d
i
using ?
p
i
? prediction confidence of ?d
i
end for
D
?
a
? top N instances with the lowest p
i
D
a
? D
a
+D
?
a
D
t
? D
t
?D
?
a
provide correct labels to the unlabeled instances in D?
a
? ? coreference resolution system trained on D
s
and
D
a
using the chosen domain adaptation technique
end for
Figure 1: An algorithm for domain adaptation
with active learning
4.3 Domain Adaptation with Active Learning
Combining domain adaptation and active learning
together, the algorithm we use is shown in Figure
1.
In our domain adaptation setting, there is a pa-
rameter ?
t
for target domain instance weighting.
Because the number of target domain instances is
different in each iteration, the weight should be ad-
justed in each iteration. We give all target domain
training instances an equal weight of ?
t
= N
s
/N
t
,
where N
s
and N
t
are the numbers of instances in
the source domain and the target domain in the
current iteration, respectively. We set N = 10 to
add 10 instances in each iteration to speed up the
active learning process.
To provide the correct labels, the labeling pro-
cess shows the text on the screen, highlights the
two NPs, and asks the annotator to decide if they
are coreferential. In our experiments, this is simu-
lated by providing the gold standard coreferential
information on this NP pair to the active learning
process.
5 Experiments
5.1 The Corpora
We explore domain adaptation from the newswire
domain to the biomedical domain. The newswire
and biomedical domain data that we use are the
ACE Phase-2 corpora and the GENIA-MEDCo
corpus, respectively. The ACE corpora con-
tain 422 and 92 training and test texts, re-
spectively (NIST, 2002). The texts come from
24
three newswire sources: BNEWS, NPAPER, and
NWIRE. The GENIA-MEDCo corpus contains
1,999 MEDLINE abstracts1. We randomly split
the GENIA corpus into a training set and a test
set, containing 1,600 and 399 texts, respectively.
5.2 The Coreference Resolution System
In this study, we use Reconcile, a state-of-the-
art coreference resolution system implemented by
(Stoyanov et al., 2009). The input to the corefer-
ence resolution system is raw text, and we apply a
sequence of preprocessing components to process
it. Following Reconcile, the individual prepro-
cessing steps include: 1) sentence segmentation
(using the OpenNLP toolkit2); 2) tokenization (us-
ing the OpenNLP toolkit); 3) POS tagging (using
the OpenNLP toolkit); 4) syntactic parsing (using
the Berkeley Parser3); and 5) named entity recog-
nition (using the Stanford NER4). Markables are
extracted as defined in each individual corpus. All
possible markable pairs in the training and test set
are extracted to form training and test instances,
respectively. The learning algorithm we use is
maximum entropy modeling, implemented in the
DALR package5 (Jiang and Zhai, 2007). The
coreference resolution system employs a compre-
hensive set of 62 features to represent each train-
ing and test instance, including lexical, proximity,
grammatical, and semantic features (Stoyanov et
al., 2009). We do not introduce additional features
motivated from the biomedical domain, but use the
same feature set for both the source and target do-
mains.
5.3 Preprocessing
For the ACE corpora, all preprocessing compo-
nents use the original models (provided by the
OpenNLP toolkit, the Berkeley Parser, and the
Stanford NER). For the GENIA corpus, since it is
from a very different domain, the original models
do not perform well. However, the GENIA cor-
pus contains multiple layers of annotations. We
use these annotations to re-train each of the pre-
processing components (except tokenization) us-
ing the 1,600 training texts of the GENIA cor-
1http://nlp.i2r.a-star.edu.sg/medco.html
2http://opennlp.sourceforge.net/
3http://code.google.com/p/berkeleyparser/
4http://nlp.stanford.edu/ner/
5http://www.mysmu.edu/faculty/jingjiang/software/
DALR.html
NPAPER NPAPER GENIA GENIA
TRAIN TEST TRAIN TEST
Number of Docs
76 17 1,600 399
Number of Words
Total 68,463 17,350 391,380 95,405
Avg. 900.8 1,020.6 244.6 239.1
Number of Markables
Total 21,492 5,153 99,408 24,397
Avg. 282.8 303.1 62.1 61.1
Number of Instances
Total 3,365,680 871,314 3,335,640 798,844
Avg. 44,285.3 51,253.8 2,084.8 2,002.1
Table 1: Statistics of the NPAPER and GENIA
data sets
pus6. We do not use any texts from the test set
when training these models. Also, we do not use
any NLP toolkits from the biomedical domain, but
only use general toolkits trained with biomedical
training data. These re-trained preprocessing com-
ponents are then applied to process the entire GE-
NIA corpus, including both the training and test
sets.
Instead of using the entire ACE corpora, we
choose the NPAPER portion of the ACE corpora
as the source domain in the experiments, because
it is the best performing one among the three por-
tions. Under these preprocessing settings, the
recall percentages of markable extraction on the
training and test set of the NPAPER corpus are
94.5% and 95.5% respectively, while the recall
percentages of markable extraction on the training
and test set of the GENIA corpus are 87.6% and
86.6% respectively. The statistics of the NPAPER
and the GENIA corpora are listed in Table 1.
5.4 Baseline Results
Under our experimental settings, a coreference
resolution system that is trained on the NPA-
PER training set and tested on the NPAPER test
set achieves recall, precision, and F-measure of
59.0%, 70.6%, and 64.3%, respectively. This
is comparable to the state-of-the-art performance
(Stoyanov et al., 2009). Table 2 compares the per-
formance of testing on the GENIA test set, but
training with the GENIA training set or the NPA-
PER training set. Training with in-domain data
achieves an F-measure that is 9.1% higher than
training with out-of-domain data. Training with
6It turned out that the re-trained tokenization model gave
poorer performance and produced many errors on punctua-
tion symbols. Thus, we stuck to using the original tokeniza-
tion model.
25
Training Set Recall Precision F-measure
GENIA Training Set 37.7 71.9 49.5
NPAPER Training Set 30.3 60.7 40.4
Table 2: MUC F-measures on the GENIA test set
in-domain data is better than training with out-of-
domain data for both recall and precision. This
confirms the impact of domain difference between
the newswire and the biomedical domain.
5.5 Domain Adaptation with Active Learning
In the experiments on domain adaptation with ac-
tive learning for coreference resolution, we as-
sume that the source domain training data are an-
notated. The target domain training data are not
annotated but are used as a data pool for instance
selection. The algorithm selects the instances in
the data pool to annotate and add them to the train-
ing data to update the classifier. The target domain
test set is strictly separated from this data pool, i.e.,
none of the target domain test data are used in the
instance selection process of active learning.
From Table 1, one can see that both training sets
in the NPAPER and the GENIA corpora contain
large numbers of training instances. Instead of us-
ing the entire training sets in the experiments, we
use a smaller subset due to several reasons. First,
to train a coreference resolution classifier, we do
not need so much training data (Soon et al., 2001).
Second, a large number of training instances will
slow the active learning process. Third, a smaller
source domain training corpus suggests a more
modest annotation effort even on the source do-
main. Lastly, a smaller target domain training cor-
pus means that fewer words need to be read by
human annotators to label the data.
We randomly choose 10 NPAPER texts as the
source domain training set. A coreference resolu-
tion system that is trained on these 10 texts and
tested on the entire NPAPER test set achieves re-
call, precision, and F-measure of 60.3%, 70.6%,
and 65.0%, respectively. This is comparable to
(actually slightly better than) a system trained on
the entire NPAPER training set. As for the GE-
NIA training set, we randomly choose 40 texts as
the target domain training data. To avoid selec-
tion bias, we perform 5 random trials, i.e., choos-
ing 5 sets, each containing 40 randomly selected
GENIA training texts. In the rest of this paper, all
performances of using 40 GENIA training texts are
the average scores over 5 runs, each of which uses
a different set of 40 texts.
In the previous section, we have presented the
domain adaptation techniques, the active learning
algorithm, as well as the target domain instance
weighting scheme. In the rest of this section, we
present the experimental results to show how do-
main adaptation, active learning, and target do-
main instance weighting help coreference resolu-
tion in a new domain. We use Augment, IW, and
IP to denote the three domain adaptation tech-
niques: AUGMENT, INSTANCE WEIGHTING, and
INSTANCE PRUNING, respectively. For a further
comparison, we explore another baseline method,
which is simply a concatenation of the source and
target domain data together, called Combine in the
rest of this paper. In all the experiments with ac-
tive learning, we run 100 iterations, which result
in the selection of 1,000 target domain instances.
The first experiment is to measure the effective-
ness of target domain instance weighting. We fix
on the use of uncertainty-based active learning,
and compare weighting and without weighting of
target domain instances (denoted as Weighted and
Unweighted). The learning curves are shown in
Figure 2. For Combine, Augment, and IP, it can be
seen that Weighted is a clear winner. As for IW, at
the beginning of active learning, Unweighted out-
performs Weighted, though it is unstable. At the
end of 100 iterations, Weighted outperforms Un-
weighted.
Since Weighted outperforms Unweighted, we
fix on the use of Weighted and explore the effec-
tiveness of active learning. For comparison, we try
another iterative process that randomly selects 10
instances in each iteration. We found that selection
of instances using active learning achieved better
performance than random selection in all cases.
This is because random selection may select in-
stances that the classifier has very high confidence
in, which will not help in improving the classifier.
In the third experiment, we fix on the use of
Weighted and Uncertainty since they perform the
best, and evaluate the effect of different domain
adaptation techniques. The learning curves are
shown in Figure 3. It can be seen that Augment
is the best performing system. For a closer look,
we tabulate the results in Table 3, with the statisti-
cal significance levels indicated. Statistical signif-
icance tests were conducted following (Chinchor,
2011).
26
0 20 40 60 80 100
20
25
30
35
40
45
50
Iteration
MU
C 
F?
me
as
ure
Weighted + Uncertainty
Unweighted + Uncertainty
(a) Combine
0 20 40 60 80 100
20
25
30
35
40
45
50
Iteration
MU
C 
F?
me
as
ure
Weighted + Uncertainty
Unweighted + Uncertainty
(b) Augment
0 20 40 60 80 100
20
25
30
35
40
45
50
Iteration
MU
C 
F?
me
as
ure
Weighted + Uncertainty
Unweighted + Uncertainty
(c) IW
0 20 40 60 80 100
20
25
30
35
40
45
50
Iteration
MU
C 
F?
me
as
ure
Weighted + Uncertainty
Unweighted + Uncertainty
(d) IP
Figure 2: Learning curves of comparing target domain instances weighted vs. unweighted. All systems
use uncertainty-based active learning.
Iteration 0 10 20 30 40 60 80 100
Combine+Unweighted 39.8 40.7 40.9 41.1 41.4 40.4 41.6 42.1
Combine+Weighted 39.8 40.9 44.0** 44.8** 45.2** 48.0** 47.7** 47.6**
Augment+Weighted 39.8 44.1**?? 46.0**?? 47.0**?? 47.8**?? 49.1**?? 49.1**?? 49.0**??
IW+Weighted 39.8 24.3 33.1 36.8 38.1 45.0** 48.2**?? 48.3**??
IP+Weighted 39.8 34.4 40.7 43.4** 46.2**?? 48.0** 48.5**?? 48.5**??
Table 3: MUC F-measures of different active learning settings on the GENIA test set. All systems use
Uncertainty. Statistical significance is compared against Combine+Unweighted, where * and ** stand
for p < 0.05 and p < 0.01, respectively, and compared against Combine+Weighted, where ?and ??stand
for p < 0.05 and p < 0.01, respectively.
6 Analysis
Using only the source domain training data,
a coreference resolution system achieves an F-
measure of 39.8% on the GENIA test set (the col-
umn of ?Iteration 0? in Table 3). From Figure 3
and Table 3, we can see that in the first few iter-
ations of active learning, domain adaptation does
not perform as well as using only the source do-
main training data. This is because when there
are very limited target domain data, the estima-
tion of the target domain is unreliable. Dahlmeier
and Ng (2010) reported similar findings though
they did not use active learning. With more iter-
ations, i.e., more target domain training data, do-
main adaptation is clearly superior. Among the
three domain adaptation techniques, Augment is
better than IW and IP. It not only achieves a higher
F-measure, but also a faster speed to adapt to a
new domain in active learning. Also, similar to
(Dahlmeier and Ng, 2010), we find that IP is gen-
erally better than IW. All systems (except IW)
with Weighted performs much better than Com-
bine+Unweighted. This shows the effectiveness
of target domain instance weighting. The aver-
age recall, precision, and F-measure of our best
model, Augment+Weighted, after 100 iterations
are 37.3%, 71.5%, and 49.0%, respectively. Com-
pared to training with only the NPAPER training
data, not only the F-measure, but also both the re-
call and precision are greatly improved (cf Table
2).
Among all the target domain instances that were
selected in Augment+Weighted, the average dis-
27
0 20 40 60 80 100
30
35
40
45
50
Iteration
MU
C F
?m
ea
su
re
Combine
Augment
IW
IP
Figure 3: Learning curves of different domain
adaptation methods. All systems use Weighted and
Uncertainty.
tance of the two markables in an instance (mea-
sured in sentence) is 3.4 (averaged over the 5
runs), which means an annotator needs to read 4
sentences on average to annotate an instance.
We also investigate the difference of corefer-
ence resolution between the newswire domain and
the biomedical domain, and the instances that
were selected in active learning which represent
this difference. One of the reasons that corefer-
ence resolution differs in the two domains is that
scientific writing in biomedical texts frequently
compares entities. For example,
In Cushing?s syndrome, the CR of GR
was normal in spite of the fact that the
CR of plasma cortisol was disturbed.
The two CRs refer to different entities and hence
are not coreferential. However, a system trained
on NPAPER predicts them as coreferential. In
the newswire domain, comparisons are less likely,
especially for named entities. For example, in
the newswire domain, London in most cases is
coreferential to other Londons. However, in the
biomedical domain, DNAs as in DNA of human
beings and DNA of monkeys are different enti-
ties. A coreference resolution system trained on
the newswire domain is unable to capture the dif-
ference between these two named entities, hence
predicting them as coreferential. This also jus-
tifies the need for domain adaptation for corefer-
ence resolution. For the above sentence, after ap-
plying our method, the adapted coreference res-
olution system is able to predict the two CRs as
non-coreferential.
Next, we show the effectiveness of our sys-
tem using domain adaptation with active learning
compared to a system trained with full corefer-
ence annotations. Averaged over 5 runs, a system
trained on a single GENIA training text achieves
an F-measure of 25.9%, which is significantly
lower than that achieved by our method. With
more GENIA training texts added, the F-measure
increases. After 80 texts are used, the system
trained on full annotations finally achieves an F-
measure of 49.2%, which is 0.2% higher than Aug-
ment+Weighted after 100 iterations. However, af-
ter 100 iterations, only 1,000 target domain in-
stances are annotated under our framework. Con-
sidering that one single text in the GENIA corpus
contains an average of over 2,000 instances (cf Ta-
ble 1), effectively we annotate only half of a text.
Compared to the 80 training texts needed, this is a
huge reduction. In order to achieve similar perfor-
mance, we only need to annotate 1/160 or 0.63%
of the complete set of training instances under our
framework of domain adaptation with active learn-
ing.
Lastly, although in this paper we reported exper-
imental results with the MUC evaluation metric,
we also evaluated our approach with other evalu-
ation metrics for coreference resolution, e.g., the
B-CUBED metric, and obtained similar findings.
7 Conclusion
In this paper, we presented an approach using
domain adaptation with active learning to adapt
coreference resolution from the newswire domain
to the biomedical domain. We explored the ef-
fect of domain adaptation, active learning, and
target domain instance weighting for coreference
resolution. Experimental results showed that do-
main adaptation with active learning and the tar-
get instance weighting scheme achieved a simi-
lar performance on MEDLINE abstracts but with
a greatly reduced number of annotated training
instances, compared to a system trained on full
coreference annotations.
Acknowledgments
This research is supported by the Singapore Na-
tional Research Foundation under its International
Research Centre @ Singapore Funding Initiative
and administered by the IDM Programme Office.
References
Jose? Castan?o, Jason Zhang, and James Pustejovsky.
2002. Anaphora resolution in biomedical literature.
In Proceedings of the International Symposium on
Reference Resolution.
28
Yee Seng Chan and Hwee Tou Ng. 2007. Domain
adaptation with active learning for word sense dis-
ambiguation. In Proceedings of the ACL2007.
Nancy Chinchor. 2011. Statistical significance of
MUC-6 results. In Proceedings of the Sixth Mes-
sage Understanding Conference.
K. Bretonnel Cohen, Arrick Lanfranchi, William
Corvey, William A. Baumgartner Jr., Christophe
Roeder, Philip V. Ogren, Martha Palmer, and
Lawrence Hunter. 2010. Annotation of all coref-
erence in biomedical text: Guideline selection and
adaptation. In BioTxtM 2010.
Daniel Dahlmeier and Hwee Tou Ng. 2010. Domain
adaptation for semantic role labeling in the biomed-
ical domain. Bioinformatics, 26(8):1098?1104.
Hal Daume III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the ACL2007.
Caroline Gasperin and Ted Briscoe. 2008. Statistical
anaphora resolution in biomedical texts. In Proceed-
ings of the COLING2008.
Caroline Gasperin, Nikiforos Karamanis, and Ruth
Seal. 2007. Annotation of anaphoric relations in
biomedical full-text articles using a domain-relevant
scheme. In Proceedings of the DAARC2007.
Caroline Gasperin. 2009. Active learning for anaphora
resolution. In Proceedings of the NAACL-HLT2009
Workshop on Active Learning for Natural Language
Processing.
Jing Jiang and ChengXiang Zhai. 2007. Instance
weighting for domain adaptation in NLP. In Pro-
ceedings of the ACL2007.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, Ngan Nguyen, and Jun?ichi Tsujii. 2011a.
Overview of BioNLP shared task 2011. In Proceed-
ings of BioNLP Shared Task 2011 Workshop.
Youngjun Kim, Ellen Riloff, and Nathan Gilbert.
2011b. The taming of Reconcile as a biomedical
coreference resolver. In Proceedings of BioNLP
Shared Task 2011 Workshop.
Florian Laws, Florian Heimerl, and Hinrich Schu?tze.
2012. Active learning for coreference resolution. In
Proceedings of the NAACL2012.
David D. Lewis and William A. Gale. 1994. A se-
quential algorithm for training text classifiers. In
Proceedings of the SIGIR1994.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Timothy A. Miller, Dmitriy Dligach, and Guergana K.
Savova. 2012. Active learning for coreference reso-
lution. In Proceedings of the BioNLP2012.
Makoto Miwa, Paul Thompson, and Sophia Ana-
niadou. 2012. Boosting automatic event ex-
traction from the literature using domain adapta-
tion and coreference resolution. Bioinformatics,
28(13):1759?1765.
MUC-6. 1995. Coreference task definition (v2.3, 8
Sep 95). In Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6).
MUC-7. 1998. Coreference task definition (v3.0, 13
Jul 97). In Proceedings of the Seventh Message Un-
derstanding Conference (MUC-7).
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the ACL2002.
NIST. 2002. The ACE 2002 evaluation
plan. ftp://jaguar.ncsl.nist.gov/ace/doc/
ACE-EvalPlan-2002-v06.pdf.
Piyush Rai, Avishek Saha, Hal Daume, and Suresh
Venkatasubramanian. 2010. Domain adapta-
tion meets active learning. In Proceedings of the
NAACL-HLT2010 Workshop on Active Learning for
Natural Language Processing.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the ACL-IJCNLP2009.
Min Tang, Xiaoqiang Luo, and Salim Roukos. 2002.
Active learning for statistical natural language pars-
ing. In Proceedings of the ACL2002.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the MUC-6.
Xiaofeng Yang, Guodong Zhou, Jian Su, and
Chew Lim Tan. 2004. Improving noun phrase
coreference resolution by matching strings. In Pro-
ceedings of the IJCNLP2004.
Jian Bo Yang, Qi Mao, Qiao Liang Xiang, Ivor W.
Tsang, Kian Ming A. Chai, and Hai Leong Chieu.
2012. Domain adaptation for coreference resolu-
tion: An adaptive ensemble approach. In Proceed-
ings of the EMNLP2012.
Shanheng Zhao and Hwee Tou Ng. 2010. Maximum
metric score training for coreference resolution. In
Proceedings of the COLING2010.
Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan. 2008.
Word sense disambiguation using OntoNotes: An
empirical study. In Proceedings of the EMNLP2008.
29
