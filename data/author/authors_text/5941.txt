Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915?932,
Prague, June 2007. c?2007 Association for Computational Linguistics
The CoNLL 2007 Shared Task on Dependency Parsing
Joakim Nivre?? Johan Hall? Sandra Ku?bler? Ryan McDonald??
Jens Nilsson? Sebastian Riedel?? Deniz Yuret??
?Va?xjo? University, School of Mathematics and Systems Engineering, first.last@vxu.se
?Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se
?Indiana University, Department of Linguistics, skuebler@indiana.edu
??Google Inc., ryanmcd@google.com
??University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk
??Koc? University, Dept. of Computer Engineering, dyuret@ku.edu.tr
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, we define the tasks of the
different tracks and describe how the data
sets were created from existing treebanks for
ten languages. In addition, we characterize
the different approaches of the participating
systems, report the test results, and provide
a first analysis of these results.
1 Introduction
Previous shared tasks of the Conference on Compu-
tational Natural Language Learning (CoNLL) have
been devoted to chunking (1999, 2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). In
2006 the shared task was multilingual dependency
parsing, where participants had to train a single
parser on data from thirteen different languages,
which enabled a comparison not only of parsing and
learning methods, but also of the performance that
can be achieved for different languages (Buchholz
and Marsi, 2006).
In dependency-based syntactic parsing, the task is
to derive a syntactic structure for an input sentence
by identifying the syntactic head of each word in the
sentence. This defines a dependency graph, where
the nodes are the words of the input sentence and the
arcs are the binary relations from head to dependent.
Often, but not always, it is assumed that all words
except one have a syntactic head, which means that
the graph will be a tree with the single independent
word as the root. In labeled dependency parsing, we
additionally require the parser to assign a specific
type (or label) to each dependency relation holding
between a head word and a dependent word.
In this year?s shared task, we continue to explore
data-driven methods for multilingual dependency
parsing, but we add a new dimension by also intro-
ducing the problem of domain adaptation. The way
this was done was by having two separate tracks: a
multilingual track using essentially the same setup
as last year, but with partly different languages, and
a domain adaptation track, where the task was to use
machine learning to adapt a parser for a single lan-
guage to a new domain. In total, test results were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). Not everyone submitted papers describ-
ing their system, and some papers describe more
than one system (or the same system in both tracks),
which explains why there are only (!) twenty-one
papers in the proceedings.
In this paper, we provide task definitions for the
two tracks (section 2), describe data sets extracted
from available treebanks (section 3), report results
for all systems in both tracks (section 4), give an
overview of approaches used (section 5), provide a
first analysis of the results (section 6), and conclude
with some future directions (section 7).
915
2 Task Definition
In this section, we provide the task definitions that
were used in the two tracks of the CoNLL 2007
Shard Task, the multilingual track and the domain
adaptation track, together with some background
and motivation for the design choices made. First
of all, we give a brief description of the data format
and evaluation metrics, which were common to the
two tracks.
2.1 Data Format and Evaluation Metrics
The data sets derived from the original treebanks
(section 3) were in the same column-based format
as for the 2006 shared task (Buchholz and Marsi,
2006). In this format, sentences are separated by a
blank line; a sentence consists of one or more to-
kens, each one starting on a new line; and a token
consists of the following ten fields, separated by a
single tab character:
1. ID: Token counter, starting at 1 for each new
sentence.
2. FORM: Word form or punctuation symbol.
3. LEMMA: Lemma or stem of word form, or an
underscore if not available.
4. CPOSTAG: Coarse-grained part-of-speech tag,
where the tagset depends on the language.
5. POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the language, or
identical to the coarse-grained part-of-speech
tag if not available.
6. FEATS: Unordered set of syntactic and/or mor-
phological features (depending on the particu-
lar language), separated by a vertical bar (|), or
an underscore if not available.
7. HEAD: Head of the current token, which is
either a value of ID or zero (0). Note that,
depending on the original treebank annotation,
there may be multiple tokens with HEAD=0.
8. DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on
the particular language. Note that, depending
on the original treebank annotation, the depen-
dency relation when HEAD=0 may be mean-
ingful or simply ROOT.
9. PHEAD: Projective head of current token,
which is either a value of ID or zero (0), or an
underscore if not available.
10. PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
The PHEAD and PDEPREL were not used at all
in this year?s data sets (i.e., they always contained
underscores) but were maintained for compatibility
with last year?s data sets. This means that, in prac-
tice, the first six columns can be considered as input
to the parser, while the HEAD and DEPREL fields
are the output to be produced by the parser. Labeled
training sets contained all ten columns; blind test
sets only contained the first six columns; and gold
standard test sets (released only after the end of the
test period) again contained all ten columns. All data
files were encoded in UTF-8.
The official evaluation metric in both tracks was
the labeled attachment score (LAS), i.e., the per-
centage of tokens for which a system has predicted
the correct HEAD and DEPREL, but results were
also reported for unlabeled attachment score (UAS),
i.e., the percentage of tokens with correct HEAD,
and the label accuracy (LA), i.e., the percentage of
tokens with correct DEPREL. One important differ-
ence compared to the 2006 shared task is that all to-
kens were counted as ?scoring tokens?, including in
particular all punctuation tokens. The official eval-
uation script, eval07.pl, is available from the shared
task website.1
2.2 Multilingual Track
The multilingual track of the shared task was orga-
nized in the same way as the 2006 task, with an-
notated training and test data from a wide range of
languages to be processed with one and the same
parsing system. This system must therefore be able
to learn from training data, to generalize to unseen
test data, and to handle multiple languages, possi-
bly by adjusting a number of hyper-parameters. Par-
ticipants in the multilingual track were expected to
submit parsing results for all languages involved.
1http://depparse.uvt.nl/depparse-wiki/SoftwarePage
916
One of the claimed advantages of dependency
parsing, as opposed to parsing based on constituent
analysis, is that it extends naturally to languages
with free or flexible word order. This explains the
interest in recent years for multilingual evaluation
of dependency parsers. Even before the 2006 shared
task, the parsers of Collins (1997) and Charniak
(2000), originally developed for English, had been
adapted for dependency parsing of Czech, and the
parsing methodology proposed by Kudo and Mat-
sumoto (2002) and Yamada and Matsumoto (2003)
had been evaluated on both Japanese and English.
The parser of McDonald and Pereira (2006) had
been applied to English, Czech and Danish, and the
parser of Nivre et al (2007) to ten different lan-
guages. But by far the largest evaluation of mul-
tilingual dependency parsing systems so far was the
2006 shared task, where nineteen systems were eval-
uated on data from thirteen languages (Buchholz and
Marsi, 2006).
One of the conclusions from the 2006 shared task
was that parsing accuracy differed greatly between
languages and that a deeper analysis of the factors
involved in this variation was an important problem
for future research. In order to provide an extended
empirical foundation for such research, we tried to
select the languages and data sets for this year?s task
based on the following desiderata:
? The selection of languages should be typolog-
ically varied and include both new languages
and old languages (compared to 2006).
? The creation of the data sets should involve as
little conversion as possible from the original
treebank annotation, meaning that preference
should be given to treebanks with dependency
annotation.
? The training data sets should include at least
50,000 tokens and at most 500,000 tokens.2
The final selection included data from Arabic,
Basque, Catalan, Chinese, Czech, English, Greek,
Hungarian, Italian, and Turkish. The treebanks from
2The reason for having an upper bound on the training set
size was the fact that, in 2006, some participants could not train
on all the data for some languages because of time limitations.
Similar considerations also led to the decision to have a smaller
number of languages this year (ten, as opposed to thirteen).
which the data sets were extracted are described in
section 3.
2.3 Domain Adaptation Track
One well known characteristic of data-driven pars-
ing systems is that they typically perform much
worse on data that does not come from the train-
ing domain (Gildea, 2001). Due to the large over-
head in annotating text with deep syntactic parse
trees, the need to adapt parsers from domains with
plentiful resources (e.g., news) to domains with lit-
tle resources is an important problem. This prob-
lem is commonly referred to as domain adaptation,
where the goal is to adapt annotated resources from
a source domain to a target domain of interest.
Almost all prior work on domain adaptation as-
sumes one of two scenarios. In the first scenario,
there are limited annotated resources available in the
target domain, and many studies have shown that
this may lead to substantial improvements. This in-
cludes the work of Roark and Bacchiani (2003), Flo-
rian et al (2004), Chelba and Acero (2004), Daume?
and Marcu (2006), and Titov and Henderson (2006).
Of these, Roark and Bacchiani (2003) and Titov and
Henderson (2006) deal specifically with syntactic
parsing. The second scenario assumes that there are
no annotated resources in the target domain. This is
a more realistic situation and is considerably more
difficult. Recent work by McClosky et al (2006)
and Blitzer et al (2006) have shown that the exis-
tence of a large unlabeled corpus in the new domain
can be leveraged in adaptation. For this shared-task,
we are assuming the latter setting ? no annotated re-
sources in the target domain.
Obtaining adequate annotated syntactic resources
for multiple languages is already a challenging prob-
lem, which is only exacerbated when these resources
must be drawn from multiple and diverse domains.
As a result, the only language that could be feasibly
tested in the domain adaptation track was English.
The setup for the domain adaptation track was as
follows. Participants were provided with a large an-
notated corpus from the source domain, in this case
sentences from the Wall Street Journal. Participants
were also provided with data from three different
target domains: biomedical abstracts (development
data), chemical abstracts (test data 1), and parent-
child dialogues (test data 2). Additionally, a large
917
unlabeled corpus for each data set (training, devel-
opment, test) was provided. The goal of the task was
to use the annotated source data, plus any unlabeled
data, to produce a parser that is accurate for each of
the test sets from the target domains.3
Participants could submit systems in either the
?open? or ?closed? class (or both). The closed class
requires a system to use only those resources pro-
vided as part of the shared task. The open class al-
lows a system to use additional resources provided
those resources are not drawn from the same domain
as the development or test sets. An example might
be a part-of-speech tagger trained on the entire Penn
Treebank and not just the subset provided as train-
ing data, or a parser that has been hand-crafted or
trained on a different training set.
3 Treebanks
In this section, we describe the treebanks used in the
shared task and give relevant information about the
data sets created from them.
3.1 Multilingual Track
Arabic The analytical syntactic annotation
of the Prague Arabic Dependency Treebank
(PADT) (Hajic? et al, 2004) can be considered a
pure dependency annotation. The conversion, done
by Otakar Smrz, from the original format to the
column-based format described in section 2.1 was
therefore relatively straightforward, although not all
the information in the original annotation could be
transfered to the new format. PADT was one of the
treebanks used in the 2006 shared task but then only
contained about 54,000 tokens. Since then, the size
of the treebank has more than doubled, with around
112,000 tokens. In addition, the morphological
annotation has been made more informative. It
is also worth noting that the parsing units in this
treebank are in many cases larger than conventional
sentences, which partly explains the high average
number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
3Note that annotated development data for the target domain
was only provided for the development domain, biomedical ab-
stracts. For the two test domains, chemical abstracts and parent-
child dialogues, the only annotated data sets were the gold stan-
dard test sets, released only after test runs had been submitted.
Basque For Basque, we used the 3LB Basque
treebank (Aduriz et al, 2003). At present, the tree-
bank consists of approximately 3,700 sentences, 334
of which were used as test data. The treebank com-
prises literary and newspaper texts. It is annotated
in a dependency format and was converted to the
CoNLL format by a team led by Koldo Gojenola.
Catalan The Catalan section of the CESS-ECE
Syntactically and Semantically Annotated Cor-
pora (Mart?? et al, 2007) is annotated with, among
other things, constituent structure and grammatical
functions. A head percolation table was used for
automatically converting the constituent trees into
dependency trees. The original data only contains
functions related to the verb, and a function table
was used for deriving the remaining syntactic func-
tions. The conversion was performed by a team led
by Llu??s Ma`rquez and Anto`nia Mart??.
Chinese The Chinese data are taken from the
Sinica treebank (Chen et al, 2003), which con-
tains both syntactic functions and semantic func-
tions. The syntactic head was used in the conversion
to the CoNLL format, carried out by Yu-Ming Hsieh
and the organizers of the 2006 shared task, and the
syntactic functions were used wherever it was pos-
sible. The training data used is basically the same
as for the 2006 shared task, except for a few correc-
tions, but the test data is new for this year?s shared
task. It is worth noting that the parsing units in this
treebank are sometimes smaller than conventional
sentence units, which partly explains the low aver-
age number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
Czech The analytical syntactic annotation of the
Prague Dependency Treebank (PDT) (Bo?hmova? et
al., 2003) is a pure dependency annotation, just as
for PADT. It was also used in the shared task 2006,
but there are two important changes compared to
last year. First, version 2.0 of PDT was used in-
stead of version 1.0, and a conversion script was
created by Zdenek Zabokrtsky, using the new XML-
based format of PDT 2.0. Secondly, due to the upper
bound on training set size, only sections 1?3 of PDT
constitute the training data, which amounts to some
450,000 tokens. The test data is a small subset of the
development test set of PDT.
918
English For English we used the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993). In particular, we used sections 2-11 for train-
ing and a subset of section 23 for testing. As a pre-
processing stage we removed many functions tags
from the non-terminals in the phrase structure repre-
sentation to make the representations more uniform
with out-of-domain test sets for the domain adapta-
tion track (see section 3.2). The resulting data set
was then converted to dependency structures using
the procedure described in Johansson and Nugues
(2007a). This work was done by Ryan McDonald.
Greek The Greek Dependency Treebank
(GDT) (Prokopidis et al, 2005) adopts a de-
pendency structure annotation very similar to those
of PDT and PADT, which means that the conversion
by Prokopis Prokopidis was relatively straightfor-
ward. GDT is one of the smallest treebanks in
this year?s shared task (about 65,000 tokens) and
contains sentences of Modern Greek. Just like PDT
and PADT, the treebank contains more than one
level of annotation, but we only used the analytical
level of GDT.
Hungarian For the Hungarian data, the Szeged
treebank (Csendes et al, 2005) was used. The tree-
bank is based on texts from six different genres,
ranging from legal newspaper texts to fiction. The
original annotation scheme is constituent-based, fol-
lowing generative principles. It was converted into
dependencies by Zo?ltan Alexin based on heuristics.
Italian The data set used for Italian is a subset
of the balanced section of the Italian Syntactic-
Semantic Treebank (ISST) (Montemagni et al,
2003) and consists of texts from the newspaper Cor-
riere della Sera and from periodicals. A team led
by Giuseppe Attardi, Simonetta Montemagni, and
Maria Simi converted the annotation to the CoNLL
format, using information from two different anno-
tation levels, the constituent structure level and the
dependency structure level.
Turkish For Turkish we used the METU-Sabanc?
Turkish Treebank (Oflazer et al, 2003), which was
also used in the 2006 shared task. A new test set of
about 9,000 tokens was provided by Gu?ls?en Eryig?it
(Eryig?it, 2007), who also handled the conversion to
the CoNLL format, which means that we could use
all the approximately 65,000 tokens of the original
treebank for training. The rich morphology of Turk-
ish requires the basic tokens in parsing to be inflec-
tional groups (IGs) rather than words. IGs of a single
word are connected to each other deterministically
using dependency links labeled DERIV, referred to
as word-internal dependencies in the following, and
the FORM and the LEMMA fields may be empty
(they contain underscore characters in the data files).
Sentences do not necessarily have a unique root;
most internal punctuation and a few foreign words
also have HEAD=0.
3.2 Domain Adaptation Track
As mentioned previously, the source data is drawn
from a corpus of news, specifically the Wall Street
Journal section of the Penn Treebank (Marcus et al,
1993). This data set is identical to the English train-
ing set from the multilingual track (see section 3.1).
For the target domains we used three different
labeled data sets. The first two were annotated
as part of the PennBioIE project (Kulick et al,
2004) and consist of sentences drawn from either
biomedical or chemical research abstracts. Like the
source WSJ corpus, this data is annotated using the
Penn Treebank phrase structure scheme. To con-
vert these sets to dependency structures we used the
same procedure as before (Johansson and Nugues,
2007a). Additional care was taken to remove sen-
tences that contained non-WSJ part-of-speech tags
or non-terminals (e.g., HYPH part-of-speech tag in-
dicating a hyphen). Furthermore, the annotation
scheme for gaps and traces was made consistent with
the Penn Treebank wherever possible. As already
mentioned, the biomedical data set was distributed
as a development set for the training phase, while
the chemical data set was only used for final testing.
The third target data set was taken from the
CHILDES database (MacWhinney, 2000), in partic-
ular the EVE corpus (Brown, 1973), which has been
annotated with dependency structures. Unfortu-
nately the dependency labels of the CHILDES data
were inconsistent with those of the WSJ, biomedi-
cal and chemical data sets, and we therefore opted
to only evaluate unlabeled accuracy for this data
set. Furthermore, there was an inconsistency in how
main and auxiliary verbs were annotated for this data
set relative to others. As a result of this, submitting
919
Multilingual Domain adaptation
Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES
Language family Sem. Isol. Rom. Sin. Sla. Ger. Hel. F.-U. Rom. Tur. Ger.
Annotation d d c+f c+f d c+f d c+f c+f d c+f d
Training data Development data
Tokens (k) 112 51 431 337 432 447 65 132 71 65 5
Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2
Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1
LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No
No. CPOSTAG 15 25 17 13 12 31 18 16 14 14 25
No. POSTAG 21 64 54 294 59 45 38 43 28 31 37
No. FEATS 21 359 33 0 71 0 31 50 21 78 0
No. DEPREL 29 35 42 69 46 20 46 49 22 25 18
No. DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1
% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0
% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0
% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0
HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0
% Non-proj. arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4
% Non-proj. sent. 10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0
Punc. attached S S A S S A S A A S A
DEPRELS for punc. 10 13 6 29 16 13 15 1 10 12 8
Test data PCHEM CHILDES
Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999
Sentences 131 334 167 690 286 214 197 390 249 300 195 666
Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9
% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10
% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a
Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development
set and the two test sets of the domain adaptation track.
920
results for the CHILDES data was considered op-
tional. Like the chemical data set, this data set was
only used for final testing.
Finally, a large corpus of unlabeled in-domain
data was provided for each data set and made avail-
able for training. This data was drawn from theWSJ,
PubMed.com (specific to biomedical and chemical
research literature), and the CHILDES data base.
The data was tokenized to be as consistent as pos-
sible with the WSJ training set.
3.3 Overview
Table 1 describes the characteristics of the data sets.
For the multilingual track, we provide statistics over
the training and test sets; for the domain adaptation
track, the statistics were extracted from the develop-
ment set. Following last year?s shared task practice
(Buchholz and Marsi, 2006), we use the following
definition of projectivity: An arc (i, j) is projective
iff all nodes occurring between i and j are dominated
by i (where dominates is the transitive closure of the
arc relation).
In the table, the languages are abbreviated to their
first two letters. Language families are: Semitic,
Isolate, Romance, Sino-Tibetan, Slavic, Germanic,
Hellenic, Finno-Ugric, and Turkic. The type of the
original annotation is either constituents plus (some)
functions (c+f) or dependencies (d). For the train-
ing data, the number of words and sentences are
given in multiples of thousands, and the average
length of a sentence in words (including punctua-
tion tokens). The following rows contain informa-
tion about whether lemmas are available, the num-
ber of coarse- and fine-grained part-of-speech tags,
the number of feature components, and the number
of dependency labels. Then information is given on
how many different dependency labels can co-occur
with HEAD=0, the percentage of HEAD=0 depen-
dencies, and the percentage of heads preceding (left)
or succeeding (right) a token (giving an indication of
whether a language is predominantly head-initial or
head-final). This is followed by the average number
of HEAD=0 dependencies per sentence and the per-
centage of non-projective arcs and sentences. The
last two rows show whether punctuation tokens are
attached as dependents of other tokens (A=Always,
S=Sometimes) and specify the number of depen-
dency labels that exist for punctuation tokens. Note
that punctuation is defined as any token belonging to
the UTF-8 category of punctuation. This means, for
example, that any token having an underscore in the
FORM field (which happens for word-internal IGs
in Turkish) is also counted as punctuation here.
For the test sets, the number of words and sen-
tences as well as the ratio of words per sentence are
listed, followed by the percentage of new words and
lemmas (if applicable). For the domain adaptation
sets, the percentage of new words is computed with
regard to the training set (Penn Treebank).
4 Submissions and Results
As already stated in the introduction, test runs were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). In the result tables below, systems are
identified by the last name of the teammember listed
first when test runs were uploaded for evaluation. In
general, this name is also the first author of a paper
describing the system in the proceedings, but there
are a few exceptions and complications. First of all,
for four out of twenty-seven systems, no paper was
submitted to the proceedings. This is the case for the
systems of Jia, Maes et al, Nash, and Zeman, which
is indicated by the fact that these names appear in
italics in all result tables. Secondly, two teams sub-
mitted two systems each, which are described in a
single paper by each team. Thus, the systems called
?Nilsson? and ?Hall, J.? are both described in Hall et
al. (2007a), while the systems called ?Duan (1)? and
?Duan (2)? are both described in Duan et al (2007).
Finally, please pay attention to the fact that there
are two teams, where the first author?s last name is
Hall. Therefore, we use ?Hall, J.? and ?Hall, K.?,
to disambiguate between the teams involving Johan
Hall (Hall et al, 2007a) and Keith Hall (Hall et al,
2007b), respectively.
Tables 2 and 3 give the scores for the multilingual
track in the CoNLL 2007 shared task. The Average
column contains the average score for all ten lan-
guages, which determines the ranking in this track.
Table 4 presents the results for the domain adapta-
tion track, where the ranking is determined based on
the PCHEM results only, since the CHILDES data
set was optional. Note also that there are no labeled
921
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)
Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)
Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)
Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)
Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)
Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)
Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)
Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)
Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)
Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)
Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)
Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)
Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)
Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)
Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)
Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)
Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)
Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)
Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)
Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)
Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)
Nash 8.65(22)* 86.49(8)
Shimizu 7.20(23) 72.02(20)
Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task. Teams
are denoted by the last name of their first member, with italics indicating that there is no corresponding
paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a score
in the Average column indicates a statistically significant difference with the next lower rank.
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)
Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)
Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)
Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)
Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)
Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)
Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)
Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)
Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)
Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)
Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)
Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)
Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)
Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)
Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)
Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)
Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)
Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)
Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)
Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)
Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)
Nash 8.77(22)* 87.71(9)
Shimizu 7.79(23) 77.91(20)
Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.
Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-
ing paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a
score in the Average column indicates a statistically significant difference with the next lower rank.
922
LAS UAS
Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o
Sagae 81.06(1) 83.42(1)
Attardi 80.40(2) 83.08(3) 58.67(3)
Dredze 80.22(3) 83.38(2) 61.37(1)
Nguyen 79.50(4)* 82.04(4)*
Jia 76.48(5)* 78.92(5)* 57.43(5)
Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)
Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*
Zeman 50.61(8) 54.57(8) 58.89(2)
Schneider 63.01(3)* 66.53(3)* 60.27(2)
Watson 55.47(4) 62.79(4) 45.61(3)
Wu 52.89(6)
Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of
the domain adaptation track in the CoNLL 2007 shared task. Teams are denoted by the last name of their
first member, with italics indicating that there is no corresponding paper in the proceedings. The number
in parentheses next to each score gives the rank. A star next to a score in the PCHEM columns indicates a
statistically significant difference with the next lower rank.
attachment scores for the CHILDES data set, for rea-
sons explained in section 3.2. The number in paren-
theses next to each score gives the rank. A star next
to a score indicates that the difference with the next
lower rank is significant at the 5% level using a z-
test for proportions. A more complete presentation
of the results, including the significance results for
all the tasks and their p-values, can be found on the
shared task website.4
Looking first at the results in the multilingual
track, we note that there are a number of systems
performing at almost the same level at the top of the
ranking. For the average labeled attachment score,
the difference between the top score (Nilsson) and
the fifth score (Hall, J.) is no more than half a per-
centage point, and there are generally very few sig-
nificant differences among the five or six best sys-
tems, regardless of whether we consider labeled or
unlabeled attachment score. For the closed class of
the domain adaptation track, we see a very similar
pattern, with the top system (Sagae) being followed
very closely by two other systems. For the open
class, the results are more spread out, but then there
are very few results in this class. It is also worth not-
ing that the top scores in the closed class, somewhat
unexpectedly, are higher than the top scores in the
4http://nextens.uvt.nl/depparse-wiki/AllScores
open class. But before we proceed to a more detailed
analysis of the results (section 6), we will make an
attempt to characterize the approaches represented
by the different systems.
5 Approaches
In this section we give an overview of the models,
inference methods, and learning methods used in the
participating systems. For obvious reasons the dis-
cussion is limited to systems that are described by
a paper in the proceedings. But instead of describ-
ing the systems one by one, we focus on the basic
methodological building blocks that are often found
in several systems although in different combina-
tions. For descriptions of the individual systems, we
refer to the respective papers in the proceedings.
Section 5.1 is devoted to system architectures. We
then describe the two main paradigms for learning
and inference, in this year?s shared task as well as in
last year?s, which we call transition-based parsers
(section 5.2) and graph-based parsers (section 5.3),
adopting the terminology of McDonald and Nivre
(2007).5 Finally, we give an overview of the domain
adaptation methods that were used (section 5.4).
5This distinction roughly corresponds to the distinction
made by Buchholz and Marsi (2006) between ?stepwise? and
?all-pairs? approaches.
923
5.1 Architectures
Most systems perform some amount of pre- and
post-processing, making the actual parsing compo-
nent part of a sequential workflow of varying length
and complexity. For example, most transition-
based parsers can only build projective dependency
graphs. For languages with non-projective depen-
dencies, graphs therefore need to be projectivized
for training and deprojectivized for testing (Hall et
al., 2007a; Johansson and Nugues, 2007b; Titov and
Henderson, 2007).
Instead of assigning HEAD and DEPREL in a
single step, some systems use a two-stage approach
for attaching and labeling dependencies (Chen et al,
2007; Dredze et al, 2007). In the first step unlabeled
dependencies are generated, in the second step these
are labeled. This is particularly helpful for factored
parsing models, in which label decisions cannot be
easily conditioned on larger parts of the structure
due to the increased complexity of inference. One
system (Hall et al, 2007b) extends this two-stage ap-
proach to a three-stage architecture where the parser
and labeler generate an n-best list of parses which in
turn is reranked.6
In ensemble-based systems several base parsers
provide parsing decisions, which are added together
for a combined score for each potential dependency
arc. The tree that maximizes the sum of these com-
bined scores is taken as the final output parse. This
technique is used by Sagae and Tsujii (2007) and in
the Nilsson system (Hall et al, 2007a). It is worth
noting that both these systems combine transition-
based base parsers with a graph-based method for
parser combination, as first described by Sagae and
Lavie (2006).
Data-driven grammar-based parsers, such as Bick
(2007), Schneider et al (2007), and Watson and
Briscoe (2007), need pre- and post-processing in or-
der to map the dependency graphs provided as train-
ing data to a format compatible with the grammar
used, and vice versa.
5.2 Transition-Based Parsers
Transition-based parsers build dependency graphs
by performing sequences of actions, or transitions.
Both learning and inference is conceptualized in
6They also flip the order of the labeler and the reranker.
terms of predicting the correct transition based on
the current parser state and/or history. We can fur-
ther subclassify parsers with respect to the model (or
transition system) they adopt, the inference method
they use, and the learning method they employ.
5.2.1 Models
The most common model for transition-based
parsers is one inspired by shift-reduce parsing,
where a parser state contains a stack of partially
processed tokens and a queue of remaining input
tokens, and where transitions add dependency arcs
and perform stack and queue operations. This type
of model is used by the majority of transition-based
parsers (Attardi et al, 2007; Duan et al, 2007; Hall
et al, 2007a; Johansson and Nugues, 2007b; Man-
nem, 2007; Titov and Henderson, 2007; Wu et al,
2007). Sometimes it is combined with an explicit
probability model for transition sequences, which
may be conditional (Duan et al, 2007) or generative
(Titov and Henderson, 2007).
An alternative model is based on the list-based
parsing algorithm described by Covington (2001),
which iterates over the input tokens in a sequen-
tial manner and evaluates for each preceding token
whether it can be linked to the current token or not.
This model is used by Marinov (2007) and in com-
ponent parsers of the Nilsson ensemble system (Hall
et al, 2007a). Finally, two systems use models based
on LR parsing (Sagae and Tsujii, 2007; Watson and
Briscoe, 2007).
5.2.2 Inference
The most common inference technique in transition-
based dependency parsing is greedy deterministic
search, guided by a classifier for predicting the next
transition given the current parser state and history,
processing the tokens of the sentence in sequen-
tial left-to-right order7 (Hall et al, 2007a; Mannem,
2007; Marinov, 2007; Wu et al, 2007). Optionally
multiple passes over the input are conducted until no
tokens are left unattached (Attardi et al, 2007).
As an alternative to deterministic parsing, several
parsers use probabilistic models and maintain a heap
or beam of partial transition sequences in order to
pick the most probable one at the end of the sentence
7For diversity in parser ensembles, right-to-left parsers are
also used.
924
(Duan et al, 2007; Johansson and Nugues, 2007b;
Sagae and Tsujii, 2007; Titov and Henderson, 2007).
One system uses as part of their parsing pipeline a
?neighbor-parser? that attaches adjacent words and
a ?root-parser? that identifies the root word(s) of a
sentence (Wu et al, 2007). In the case of grammar-
based parsers, a classifier is used to disambiguate
in cases where the grammar leaves some ambiguity
(Schneider et al, 2007; Watson and Briscoe, 2007)
5.2.3 Learning
Transition-based parsers either maintain a classifier
that predicts the next transition or a global proba-
bilistic model that scores a complete parse. To train
these classifiers and probabilitistic models several
approaches were used: SVMs (Duan et al, 2007;
Hall et al, 2007a; Sagae and Tsujii, 2007), modified
finite Newton SVMs (Wu et al, 2007), maximum
entropy models (Sagae and Tsujii, 2007), multiclass
averaged perceptron (Attardi et al, 2007) and max-
imum likelihood estimation (Watson and Briscoe,
2007).
In order to calculate a global score or probabil-
ity for a transition sequence, two systems used a
Markov chain approach (Duan et al, 2007; Sagae
and Tsujii, 2007). Here probabilities from the output
of a classifier are multiplied over the whole sequence
of actions. This results in a locally normalized
model. Two other entries used MIRA (Mannem,
2007) or online passive-aggressive learning (Johans-
son and Nugues, 2007b) to train a globally normal-
ized model. Titov and Henderson (2007) used an in-
cremental sigmoid Bayesian network to model the
probability of a transition sequence and estimated
model parameters using neural network learning.
5.3 Graph-Based Parsers
While transition-based parsers use training data to
learn a process for deriving dependency graphs,
graph-based parsers learn a model of what it means
to be a good dependency graph given an input sen-
tence. They define a scoring or probability function
over the set of possible parses. At learning time
they estimate parameters of this function; at pars-
ing time they search for the graph that maximizes
this function. These parsers mainly differ in the
type and structure of the scoring function (model),
the search algorithm that finds the best parse (infer-
ence), and the method to estimate the function?s pa-
rameters (learning).
5.3.1 Models
The simplest type of model is based on a sum of
local attachment scores, which themselves are cal-
culated based on the dot product of a weight vector
and a feature representation of the attachment. This
type of scoring function is often referred to as a first-
order model.8 Several systems participating in this
year?s shared task used first-order models (Schiehlen
and Spranger, 2007; Nguyen et al, 2007; Shimizu
and Nakagawa, 2007; Hall et al, 2007b). Canisius
and Tjong Kim Sang (2007) cast the same type of
arc-based factorization as a weighted constraint sat-
isfaction problem.
Carreras (2007) extends the first-order model to
incorporate a sum over scores for pairs of adjacent
arcs in the tree, yielding a second-order model. In
contrast to previous work where this was constrained
to sibling relations of the dependent (McDonald and
Pereira, 2006), here head-grandchild relations can
be taken into account.
In all of the above cases the scoring function is
decomposed into functions that score local proper-
ties (arcs, pairs of adjacent arcs) of the graph. By
contrast, the model of Nakagawa (2007) considers
global properties of the graph that can take multi-
ple arcs into account, such as multiple siblings and
children of a node.
5.3.2 Inference
Searching for the highest scoring graph (usually a
tree) in a model depends on the factorization cho-
sen and whether we are looking for projective or
non-projective trees. Maximum spanning tree al-
gorithms can be used for finding the highest scor-
ing non-projective tree in a first-order model (Hall
et al, 2007b; Nguyen et al, 2007; Canisius and
Tjong Kim Sang, 2007; Shimizu and Nakagawa,
2007), while Eisner?s dynamic programming algo-
rithm solves the problem for a first-order factoriza-
tion in the projective case (Schiehlen and Spranger,
2007). Carreras (2007) employs his own exten-
sion of Eisner?s algorithm for the case of projective
trees and second-order models that include head-
grandparent relations.
8It is also known as an edge-factored model.
925
The methods presented above are mostly efficient
and always exact. However, for models that take
global properties of the tree into account, they can-
not be applied. Instead Nakagawa (2007) uses Gibbs
sampling to obtain marginal probabilities of arcs be-
ing included in the tree using his global model and
then applies a maximum spanning tree algorithm to
maximize the sum of the logs of these marginals and
return a valid cycle-free parse.
5.3.3 Learning
Most of the graph-based parsers were trained using
an online inference-based method such as passive-
aggressive learning (Nguyen et al, 2007; Schiehlen
and Spranger, 2007), averaged perceptron (Carreras,
2007), or MIRA (Shimizu and Nakagawa, 2007),
while some systems instead used methods based on
maximum conditional likelihood (Nakagawa, 2007;
Hall et al, 2007b).
5.4 Domain Adaptation
5.4.1 Feature-Based Approaches
One way of adapting a learner to a new domain with-
out using any unlabeled data is to only include fea-
tures that are expected to transfer well (Dredze et
al., 2007). In structural correspondence learning a
transformation from features in the source domain
to features of the target domain is learnt (Shimizu
and Nakagawa, 2007). The original source features
along with their transformed versions are then used
to train a discriminative parser.
5.4.2 Ensemble-Based Approaches
Dredze et al (2007) trained a diverse set of parsers
in order to improve cross-domain performance by
incorporating their predictions as features for an-
other classifier. Similarly, two parsers trained with
different learners and search directions were used
in the co-learning approach of Sagae and Tsujii
(2007). Unlabeled target data was processed with
both parsers. Sentences that both parsers agreed on
were then added to the original training data. This
combined data set served as training data for one of
the original parsers to produce the final system. In
a similar fashion, Watson and Briscoe (2007) used a
variant of self-training to make use of the unlabeled
target data.
5.4.3 Other Approaches
Attardi et al (2007) learnt tree revision rules for the
target domain by first parsing unlabeled target data
using a strong parser; this data was then combined
with labeled source data; a weak parser was applied
to this new dataset; finally tree correction rules are
collected based on the mistakes of the weak parser
with respect to the gold data and the output of the
strong parser.
Another technique used was to filter sentences of
the out-of-domain corpus based on their similarity
to the target domain, as predicted by a classifier
(Dredze et al, 2007). Only if a sentence was judged
similar to target domain sentences was it included in
the training set.
Bick (2007) used a hybrid approach, where a data-
driven parser trained on the labeled training data was
given access to the output of a Constraint Grammar
parser for English run on the same data. Finally,
Schneider et al (2007) learnt collocations and rela-
tional nouns from the unlabeled target data and used
these in their parsing algorithm.
6 Analysis
Having discussed the major approaches taken in the
two tracks of the shared task, we will now return to
the test results. For the multilingual track, we com-
pare results across data sets and across systems, and
report results from a parser combination experiment
involving all the participating systems (section 6.1).
For the domain adaptation track, we sum up the most
important findings from the test results (section 6.2).
6.1 Multilingual Track
6.1.1 Across Data Sets
The average LAS over all systems varies from 68.07
for Basque to 80.95 for English. Top scores vary
from 76.31 for Greek to 89.61 for English. In gen-
eral, there is a good correlation between the top
scores and the average scores. For Greek, Italian,
and Turkish, the top score is closer to the average
score than the average distance, while for Czech, the
distance is higher. The languages that produced the
most stable results in terms of system ranks with re-
spect to LAS are Hungarian and Italian. For UAS,
Catalan also falls into this group. The language that
926
Setup Arabic Chinese Czech Turkish
2006 without punctuation 66.9 90.0 80.2 65.7
2007 without punctuation 75.5 84.9 80.0 71.6
2006 with punctuation 67.0 90.0 80.2 73.8
2007 with punctuation 76.5 84.7 80.2 79.8
Table 5: A comparison of the LAS top scores from 2006 and 2007. Official scoring conditions in boldface.
For Turkish, scores with punctuation also include word-internal dependencies.
produced the most unstable results with respect to
LAS is Turkish.
In comparison to last year?s languages, the lan-
guages involved in the multilingual track this year
can be more easily separated into three classes with
respect to top scores:
? Low (76.31?76.94):
Arabic, Basque, Greek
? Medium (79.19?80.21):
Czech, Hungarian, Turkish
? High (84.40?89.61):
Catalan, Chinese, English, Italian
It is interesting to see that the classes are more easily
definable via language characteristics than via char-
acteristics of the data sets. The split goes across
training set size, original data format (constituent
vs. dependency), sentence length, percentage of un-
known words, number of dependency labels, and ra-
tio of (C)POSTAGS and dependency labels. The
class with the highest top scores contains languages
with a rather impoverished morphology. Medium
scores are reached by the two agglutinative lan-
guages, Hungarian and Turkish, as well as by Czech.
The most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection. Based on these characteristics, one would
expect to find Czech in the last class. However, the
Czech training set is four times the size of the train-
ing set for Arabic, which is the language with the
largest training set of the difficult languages.
However, it would be wrong to assume that train-
ing set size alone is the deciding factor. A closer
look at table 1 shows that while Basque and Greek
in fact have small training data sets, so do Turk-
ish and Italian. Another factor that may be asso-
ciated with the above classification is the percent-
age of new words (PNW) in the test set. Thus, the
expectation would be that the highly inflecting lan-
guages have a high PNW while the languages with
little morphology have a low PNW. But again, there
is no direct correspondence. Arabic, Basque, Cata-
lan, English, and Greek agree with this assumption:
Catalan and English have the smallest PNW, and
Arabic, Basque, and Greek have a high PNW. But
the PNW for Italian is higher than for Arabic and
Greek, and this is also true for the percentage of
new lemmas. Additionally, the highest PNW can be
found in Hungarian and Turkish, which reach higher
scores than Arabic, Basque, and Greek. These con-
siderations suggest that highly inflected languages
with (relatively) free word order need more training
data, a hypothesis that will have to be investigated
further.
There are four languages which were included in
the shared tasks on multilingual dependency pars-
ing both at CoNLL 2006 and at CoNLL 2007: Ara-
bic, Chinese, Czech, and Turkish. For all four lan-
guages, the same treebanks were used, which allows
a comparison of the results. However, in some cases
the size of the training set changed, and at least one
treebank, Turkish, underwent a thorough correction
phase. Table 5 shows the top scores for LAS. Since
the official scores excluded punctuation in 2006 but
includes it in 2007, we give results both with and
without punctuation for both years.
For Arabic and Turkish, we see a great improve-
ment of approximately 9 and 6 percentage points.
For Arabic, the number of tokens in the training
set doubled, and the morphological annotation was
made more informative. The combined effect of
these changes can probably account for the substan-
tial improvement in parsing accuracy. For Turkish,
the training set grew in size as well, although only by
600 sentences, but part of the improvement for Turk-
ish may also be due to continuing efforts in error cor-
927
rection and consistency checking. We see that the
choice to include punctuation or not makes a large
difference for the Turkish scores, since non-final IGs
of a word are counted as punctuation (because they
have the underscore character as their FORM value),
which means that word-internal dependency links
are included if punctuation is included.9 However,
regardless of whether we compare scores with or
without punctuation, we see a genuine improvement
of approximately 6 percentage points.
For Chinese, the same training set was used.
Therefore, the drop from last year?s top score to this
year?s is surprising. However, last year?s top scor-
ing system for Chinese (Riedel et al, 2006), which
did not participate this year, had a score that was
more than 3 percentage points higher than the sec-
ond best system for Chinese. Thus, if we compare
this year?s results to the second best system, the dif-
ference is approximately 2 percentage points. This
final difference may be attributed to the properties of
the test sets. While last year?s test set was taken from
the treebank, this year?s test set contains texts from
other sources. The selection of the textual basis also
significantly changed average sentence length: The
Chinese training set has an average sentence length
of 5.9. Last year?s test set alo had an average sen-
tence length of 5.9. However, this year, the average
sentence length is 7.5 tokens, which is a significant
increase. Longer sentences are typically harder to
parse due to the increased likelihood of ambiguous
constructions.
Finally, we note that the performance for Czech
is almost exactly the same as last year, despite the
fact that the size of the training set has been reduced
to approximately one third of last year?s training set.
It is likely that this in fact represents a relative im-
provement compared to last year?s results.
6.1.2 Across Systems
The LAS over all languages ranges from 80.32 to
54.55. The comparison of the system ranks aver-
aged over all languages with the ranks for single lan-
9The decision to include word-internal dependencies in this
way can be debated on the grounds that they can be parsed de-
terministically. On the other hand, they typically correspond to
regular dependencies captured by function words in other lan-
guages, which are often easy to parse as well. It is therefore
unclear whether scores are more inflated by including word-
internal dependencies or deflated by excluding them.
guages show considerably more variation than last
year?s systems. Buchholz and Marsi (2006) report
that ?[f]or most parsers, their ranking differs at most
a few places from their overall ranking?. This year,
for all of the ten best performing systems with re-
spect to LAS, there is at least one language for which
their rank is at least 5 places different from their
overall rank. The most extreme case is the top per-
forming Nilsson system (Hall et al, 2007a), which
reached rank 1 for five languages and rank 2 for
two more languages. Their only outlier is for Chi-
nese, where the system occupies rank 14, with a
LAS approximately 9 percentage points below the
top scoring system for Chinese (Sagae and Tsujii,
2007). However, Hall et al (2007a) point out that
the official results for Chinese contained a bug, and
the true performance of their system was actually
much higher. The greatest improvement of a sys-
tem with respect to its average rank occurs for En-
glish, for which the system by Nguyen et al (2007)
improved from the average rank 15 to rank 6. Two
more outliers can be observed in the system of Jo-
hansson and Nugues (2007b), which improves from
its average rank 12 to rank 4 for Basque and Turkish.
The authors attribute this high performance to their
parser?s good performance on small training sets.
However, this hypothesis is contradicted by their re-
sults for Greek and Italian, the other two languages
with small training sets. For these two languages,
the system?s rank is very close to its average rank.
6.1.3 An Experiment in System Combination
Having the outputs of many diverse dependency
parsers for standard data sets opens up the interest-
ing possibility of parser combination. To combine
the outputs of each parser we used the method of
Sagae and Lavie (2006). This technique assigns to
each possible labeled dependency a weight that is
equal to the number of systems that included the de-
pendency in their output. This can be viewed as
an arc-based voting scheme. Using these weights
it is possible to search the space of possible depen-
dency trees using directed maximum spanning tree
algorithms (McDonald et al, 2005). The maximum
spanning tree in this case is equal to the tree that on
average contains the labeled dependencies that most
systems voted for. It is worth noting that variants
of this scheme were used in two of the participating
928
5 10 15 20Number of Systems
80
82
84
86
88
Accu
racy
Unlabeled AccuracyLabeled Accuracy
Figure 1: System Combination
systems, the Nilsson system (Hall et al, 2007a) and
the system of Sagae and Tsujii (2007).
Figure 1 plots the labeled and unlabeled accura-
cies when combining an increasing number of sys-
tems. The data used in the plot was the output of all
competing systems for every language in the mul-
tilingual track. The plot was constructed by sort-
ing the systems based on their average labeled accu-
racy scores over all languages, and then incremen-
tally adding each system in descending order.10 We
can see that both labeled and unlabeled accuracy are
significantly increased, even when just the top three
systems are included. Accuracy begins to degrade
gracefully after about ten different parsers have been
added. Furthermore, the accuracy never falls below
the performance of the top three systems.
6.2 Domain Adaptation Track
For this task, the results are rather surprising. A look
at the LAS and UAS for the chemical research ab-
stracts shows that there are four closed systems that
outperform the best scoring open system. The best
system (Sagae and Tsujii, 2007) reaches an LAS of
81.06 (in comparison to their LAS of 89.01 for the
English data set in the multilingual track). Consider-
ing that approximately one third of the words of the
chemical test set are new, the results are noteworthy.
The next surprise is to be found in the relatively
low UAS for the CHILDES data. At a first glance,
this data set has all the characteristics of an easy
10The reason that there is no data point for two parsers is
that the simple voting scheme adopted only makes sense with at
least three parsers voting.
set; the average sentence is short (12.9 words), and
the percentage of new words is also small (6.10%).
Despite these characteristics, the top UAS reaches
62.49 and is thus more than 10 percentage points
below the top UAS for the chemical data set. One
major reason for this is that auxiliary and main
verb dependencies are annotated differently in the
CHILDES data than in the WSJ training set. As a
result of this discrepancy, participants were not re-
quired to submit results for the CHILDES data. The
best performing system on the CHILDES corpus is
an open system (Bick, 2007), but the distance to
the top closed system is approximately 1 percent-
age point. In this domain, it seems more feasible to
use general language resources than for the chemi-
cal domain. However, the results prove that the extra
effort may be unnecessary.
7 Conclusion
Two years of dependency parsing in the CoNLL
shared task has brought an enormous boost to the
development of dependency parsers for multiple lan-
guages (and to some extent for multiple domains).
But even though nineteen languages have been cov-
ered by almost as many different parsing and learn-
ing approaches, we still have only vague ideas about
the strengths and weaknesses of different methods
for languages with different typological characteris-
tics. Increasing our knowledge of the multi-causal
relationship between language structure, annotation
scheme, and parsing and learning methods probably
remains the most important direction for future re-
search in this area. The outputs of all systems for all
data sets from the two shared tasks are freely avail-
able for research and constitute a potential gold mine
for comparative error analysis across languages and
systems.
For domain adaptation we have barely scratched
the surface so far. But overcoming the bottleneck
of limited annotated resources for specialized do-
mains will be as important for the deployment of
human language technology as being able to handle
multiple languages in the future. One result from
the domain adaptation track that may seem surpris-
ing at first is the fact that closed class systems out-
performed open class systems on the chemical ab-
stracts. However, it seems that the major problem in
929
adapting pre-existing parsers to the new domain was
not the domain as such but the mapping from the
native output of the parser to the kind of annotation
provided in the shared task data sets. Thus, find-
ing ways of reusing already invested development
efforts by adapting the outputs of existing systems
to new requirements, without substantial loss in ac-
curacy, seems to be another line of research that may
be worth pursuing.
Acknowledgments
First and foremost, we want to thank all the peo-
ple and organizations that generously provided us
with treebank data and helped us prepare the data
sets and without whom the shared task would have
been literally impossible: Otakar Smrz, Charles
University, and the LDC (Arabic); Maxux Aranz-
abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-
jenola, and the University of the Basque Coun-
try (Basque); Ma. Anto`nia Mart?? Anton??n, Llu??s
Ma`rquez, Manuel Bertran, Mariona Taule?, Difda
Monterde, Eli Comelles, and CLiC-UB (Cata-
lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming
Hsieh, and Academia Sinica (Chinese); Jan Hajic?,
Zdenek Zabokrtsky, Charles University, and the
LDC (Czech); Brian MacWhinney, Eric Davis, the
CHILDES project, the Penn BioIE project, and
the LDC (English); Prokopis Prokopidis and ILSP
(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-
garian); Giuseppe Attardi, Simonetta Montemagni,
Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril
Ribarov, Alessandro Lenci, Nicoletta Calzolari,
ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal
Oflazer, and Ruket C?ak?c? (Turkish).
Secondly, we want to thank the organizers of last
year?s shared task, Sabine Buchholz, Amit Dubey,
Erwin Marsi, and Yuval Krymolowski, who solved
all the really hard problems for us and answered all
our questions, as well as our colleagues who helped
review papers: Jason Baldridge, Sabine Buchholz,
James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-
lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,
Bea?ta Megyesi, Yannick Versley, and Alexander
Yeh. Special thanks to Bertjan Busser and Erwin
Marsi for help with the CoNLL shared task website
and many other things, and to Richard Johansson for
letting us use his conversion tool for English.
Thirdly, we want to thank the program chairs
for EMNLP-CoNLL 2007, Jason Eisner and Taku
Kudo, the publications chair, Eric Ringger, the
SIGNLL officers, Antal van den Bosch, Hwee Tou
Ng, and Erik Tjong Kim Sang, and members of the
LDC staff, Tony Castelletto and Ilya Ahtaridis, for
great cooperation and support.
Finally, we want to thank the following people,
who in different ways assisted us in the organi-
zation of the CoNLL 2007 shared task: Giuseppe
Attardi, Eckhard Bick, Matthias Buch-Kromann,
Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-
nov, Wolfgang Menzel, Xue Nianwen, Gertjan van
Noord, Petya Osenova, Florian Schiel, Kiril Simov,
Zdenka Uresova, and Heike Zinsmeister.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
E. Bick. 2007. Hybrid ways to improve domain inde-
pendence in an ML dependency parser. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conf. on Computational Natural Language
Learning (CoNLL).
S. Canisius and E. Tjong Kim Sang. 2007. A constraint
satisfaction approach to dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
930
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
W. Chen, Y. Zhang, and H. Isahara. 2007. A two-stage
parser for multilingual dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
H. Daume? and D. Marcu. 2006. Domain adaptation for
statistical classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
M. Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly hard do-
main adaptation for dependency parsing. In Proc. of
the CoNLL 2007 Shared Task. EMNLP-CoNLL.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
G. Eryig?it. 2007. ITU validation set for Metu-Sabanc?
Turkish Treebank. URL: http://www3.itu.edu.tr/
?gulsenc/papers/validationset.pdf.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, A. Luo, N. Nicolov, and S. Roukos. 2004. A
statisical model for multilingual entity detection and
tracking. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Johansson and P. Nugues. 2007a. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conf. on Computational Lin-
guistics (NODALIDA).
R. Johansson and P. Nugues. 2007b. Incremental depen-
dency parsing using online learning. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of the Sixth
Conf. on Computational Language Learning (CoNLL).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
P. R. Mannem. 2007. Online learning for determinis-
tic dependency parsing. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Marinov. 2007. Covington variations. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
931
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of the 11th Conf. of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Methods
in Natural Language Processing (HLT/EMNLP).
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
T. Nakagawa. 2007. Multilingual dependency parsing
using Gibbs sampling. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu. 2007. A
multilingual dependency analysis system using online
passive-aggressive learning. In Proc. of the CoNLL
2007 Shared Task. EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13:95?135.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT).
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-
lingual dependency parsing with incremental integer
linear programming. In Proc. of the Tenth Conf. on
Computational Natural Language Learning (CoNLL).
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
Proc. of the Human Language Technology Conf. and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conf. of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT/NAACL).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
M. Schiehlen and Kristina Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
G. Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.
2007. Pro3Gres parser in the CoNLL domain adap-
tation shared task. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
N. Shimizu and H. Nakagawa. 2007. Structural corre-
spondence learning for dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Watson and T. Briscoe. 2007. Adapting the RASP
system for the CoNLL07 domain-adaptation task. In
Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
Y.-C. Wu, J.-C. Yang, and Y.-S. Lee. 2007. Multi-
lingual deterministic dependency parsing framework
using modified finite Newton method support vector
machines. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT).
932
Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 933?939,
Prague, June 2007. c?2007 Association for Computational Linguistics
Single Malt or Blended? A Study in Multilingual Parser Optimization
Johan Hall? Jens Nilsson? Joakim Nivre??
Gu?ls?en Eryig?it? Bea?ta Megyesi? Mattias Nilsson? Markus Saers?
?Va?xjo? University, School of Mathematics and Systems Engineering
E-mail: firstname.lastname@vxu.se
?Uppsala University, Dept. of Linguistics and Philology
E-mail: firstname.lastname@lingfil.uu.se
?Istanbul Technical University, Computer Engineering Dept.
E-mail: gulsen.cebiroglu@itu.edu.tr
Abstract
We describe a two-stage optimization of the
MaltParser system for the ten languages in
the multilingual track of the CoNLL 2007
shared task on dependency parsing. The
first stage consists in tuning a single-parser
system for each language by optimizing pa-
rameters of the parsing algorithm, the fea-
ture model, and the learning algorithm. The
second stage consists in building an ensem-
ble system that combines six different pars-
ing strategies, extrapolating from the opti-
mal parameters settings for each language.
When evaluated on the official test sets, the
ensemble system significantly outperforms
the single-parser system and achieves the
highest average labeled attachment score.
1 Introduction
In the multilingual track of the CoNLL 2007 shared
task on dependency parsing, a single parser must be
trained to handle data from ten different languages:
Arabic (Hajic? et al, 2004), Basque (Aduriz et al,
2003), Catalan, (Mart?? et al, 2007), Chinese (Chen
et al, 2003), Czech (Bo?hmova? et al, 2003), English
(Marcus et al, 1993; Johansson and Nugues, 2007),
Greek (Prokopidis et al, 2005), Hungarian (Csendes
et al, 2005), Italian (Montemagni et al, 2003), and
Turkish (Oflazer et al, 2003).1 Our contribution is
a study in multilingual parser optimization using the
freely available MaltParser system, which performs
1For more information about the task and the data sets, see
Nivre et al (2007).
deterministic, classifier-based parsing with history-
based feature models and discriminative learning,
and which was one of the top performing systems
in the CoNLL 2006 shared task (Nivre et al, 2006).
In order to maximize parsing accuracy, optimiza-
tion has been carried out in two stages, leading to
two different, but related parsers. The first of these is
a single-parser system, similar to the one described
in Nivre et al (2006), which parses a sentence deter-
ministically in a single left-to-right pass, with post-
processing to recover non-projective dependencies,
and where the parameters of the MaltParser system
have been tuned for each language separately. We
call this system Single Malt, to emphasize the fact
that it consists of a single instance of MaltParser.
The second parser is an ensemble system, which
combines the output of six deterministic parsers,
each of which is a variation of the Single Malt parser
with parameter settings extrapolated from the first
stage of optimization. It seems very natural to call
this system Blended.
Section 2 summarizes the work done to optimize
the Single Malt parser, while section 3 explains how
the Blended parser was constructed from the Single
Malt parser. Section 4 gives a brief analysis of the
experimental results, and section 5 concludes.
2 The Single Malt Parser
The parameters available in the MaltParser system
can be divided into three groups: parsing algorithm
parameters, feature model parameters, and learn-
ing algorithm parameters.2 Our overall optimization
2For a complete documentation of these parameters, see
http://w3.msi.vxu.se/users/nivre/research/MaltParser.html.
933
strategy for the Single Malt parser was as follows:
1. Define a good baseline system with the same
parameter settings for all languages.
2. Tune parsing algorithm parameters once and
for all for each language (with baseline settings
for feature model and learning algorithm pa-
rameters).
3. Optimize feature model and learning algorithm
parameters in an interleaved fashion for each
language.
We used nine-fold cross-validation on 90% of the
training data for all languages with a training set size
smaller than 300,000 tokens and an 80%?10% train-
devtest split for the remaining languages (Catalan,
Chinese, Czech, English). The remaining 10% of
the data was in both cases saved for a final dry run,
where the parser was trained on 90% of the data for
each language and tested on the remaining (fresh)
10%. We consistently used the labeled attachment
score (LAS) as the single optimization criterion.
Below we describe the most important parameters
in each group, define baseline settings, and report
notable improvements for different languages during
development. The improvements for each language
from step 1 (baseline) to step 2 (parsing algorithm)
and step 3 (feature model and learning algorithm)
can be tracked in table 1.3
2.1 Parsing Algorithm
MaltParser implements several parsing algorithms,
but for the Single Malt system we stick to the one
used by Nivre et al (2006), which performs labeled
projective dependency parsing in linear time, using a
stack to store partially processed tokens and an input
queue of remaining tokens. There are three basic
parameters that can be varied for this algorithm:
1. Arc order: The baseline algorithm is arc-
eager, in the sense that right dependents are
attached to their head as soon as possible, but
there is also an arc-standard version, where the
attachment of right dependents has to be post-
poned until they have found all their own de-
pendents. The arc-standard order was found
3Complete specifications of all parameter settings for all
languages, for both Single Malt and Blended, are available at
http://w3.msi.vxu.se/users/jha/conll07/.
to improve parsing accuracy for Chinese, while
the arc-eager order was maintained for all other
languages.
2. Stack initialization: In the baseline version
the parser is initialized with an artificial root
node (with token id 0) on the stack, so that arcs
originating from the root can be added explic-
itly during parsing. But it is also possible to ini-
tialize the parser with an empty stack, in which
case arcs from the root are only added implic-
itly (to any token that remains a root after pars-
ing is completed). Empty stack initialization
(which reduces the amount of nondeterminism
in parsing) led to improved accuracy for Cata-
lan, Chinese, Hungarian, Italian and Turkish.4
3. Post-processing: The baseline parser performs
a single left-to-right pass over the input, but it
is possible to allow a second pass where only
unattached tokens are processed.5 Such post-
processing was found to improve results for
Basque, Catalan, Czech, Greek and Hungarian.
Since the parsing algorithm only produces projective
dependency graphs, we may use pseudo-projective
parsing to recover non-projective dependencies, i.e.,
projectivize training data and encode information
about these transformations in extended arc labels
to support deprojectivization of the parser output
(Nivre and Nilsson, 2005). Pseudo-projective pars-
ing was found to have a positive effect on over-
all parsing accuracy only for Basque, Czech, Greek
and Turkish. This result can probably be explained
in terms of the frequency of non-projective depen-
dencies in the different languages. For Basque,
Czech, Greek and Turkish, more than 20% of the
sentences have non-projective dependency graphs;
for all the remaining languages the corresponding
4For Arabic, Basque, Czech, and Greek, the lack of im-
provement can be explained by the fact that these data sets allow
more than one label for dependencies from the artificial root.
With empty stack initialization all such dependencies are as-
signed a default label, which leads to a drop in labeled attach-
ment score. For English, however, empty stack initialization did
not improve accuracy despite the fact that dependencies from
the artificial root have a unique label.
5This technique is similar to the one used by Yamada and
Matsumoto (2003), but with only a single post-processing pass
parsing complexity remains linear in string length.
934
Attributes
Tokens FORM LEMMA CPOSTAG POSTAG FEATS DEPREL
S: Top + + + + + +
S: Top?1 +
I: Next + + + + +
I: Next+1 + +
I: Next+2 +
I: Next+3 +
G: Head of Top +
G: Leftmost dependent of Top +
G: Rightmost dependent of Top +
G: Leftmost dependent of Next +
Figure 1: Baseline feature model (S = Stack, I = Input, G = Graph).
figure is 10% or less.6
The cumulative improvement after optimization
of parsing algorithm parameters was a modest 0.32
percentage points on average over all ten languages,
with a minimum of 0.00 (Arabic, English) and a
maximum of 0.83 (Czech) (cf. table 1).
2.2 Feature Model
MaltParser uses a history-based feature model for
predicting the next parsing action. Each feature of
this model is an attribute of a token defined relative
to the current stack S, input queue I, or partially built
dependency graph G, where the attribute can be any
of the symbolic input attributes in the CoNLL for-
mat: FORM, LEMMA, CPOSTAG, POSTAG and
FEATS (split into atomic attributes), as well as the
DEPREL attribute of tokens in the graph G. The
baseline feature model is depicted in figure 1, where
rows denote tokens, columns denote attributes, and
each cell containing a plus sign represents a model
feature.7 This model is an extrapolation from many
previous experiments on different languages and
usually represents a good starting point for further
optimization.
The baseline model was tuned for each of the ten
languages using both forward and backward feature
6In fact, for Arabic, which has about 10% sentences with
non-projective dependencies, it was later found that, with an
optimized feature model, it is beneficial to projectivize the train-
ing data without trying to recover non-projective dependencies
in the parser output. This was also the setting that was used for
Arabic in the dry run and final test.
7The names Top and Next refer to the token on top of the
stack S and the first token in the remaining input I, respectively.
selection. The total number of features in the tuned
models varies from 18 (Turkish) to 56 (Hungarian)
but is typically between 20 and 30. This feature se-
lection process constituted the major development
effort for the Single Malt parser and also gave the
greatest improvements in parsing accuracy, but since
feature selection was to some extent interleaved with
learning algorithm optimization, we only report the
cumulative effect of both together in table 1.
2.3 Learning Algorithm
MaltParser supports several learning algorithms but
the best results have so far been obtained with sup-
port vector machines, using the LIBSVM package
(Chang and Lin, 2001). We use a quadratic kernel
K(xi, xj) = (?xTi xj + r)
2 and LIBSVM?s built-
in one-versus-one strategy for multi-class classifica-
tion, converting symbolic features to numerical ones
using the standard technique of binarization. As our
baseline settings, we used ? = 0.2 and r = 0 for
the kernel parameters, C = 0.5 for the penalty para-
meter, and ? = 1.0 for the termination criterion. In
order to reduce training times during development,
we also split the training data for each language into
smaller sets and trained separate multi-class classi-
fiers for each set, using the POSTAG of Next as the
defining feature for the split.
The time spent on optimizing learning algorithm
parameters varies between languages, mainly due
to lack of time. For Arabic, Basque, and Catalan,
the baseline settings were used also in the dry run
and final test. For Chinese, Greek and Hungarian,
935
Development Dry Run Test Test: UAS
Language Base PA F+L SM B SM B SM B
Arabic 70.31 70.31 71.67 70.93 73.09 74.75 76.52 84.21 85.81
Basque 73.86 74.44 76.99 77.18 80.12 74.97 76.92 80.61 82.84
Catalan 85.43 85.51 86.88 86.65 88.00 87.74 88.70 92.20 93.12
Chinese 83.85 84.39 87.64 87.61 88.61 83.51 84.67 87.60 88.70
Czech 75.00 75.83 77.74 77.91 82.17 77.22 77.98 82.35 83.59
English 85.44 85.44 86.35 86.35 88.74 85.81 88.11 86.77 88.93
Greek 72.67 73.04 74.42 74.89 78.17 74.21 74.65 80.66 81.22
Hungarian 74.62 74.64 77.40 77.81 80.04 78.09 80.27 81.71 83.55
Italian 81.42 81.64 82.50 83.37 85.16 82.48 84.40 86.26 87.77
Turkish 75.12 75.80 76.49 75.87 77.09 79.24 79.79 85.04 85.77
Average 77.78 78.10 79.81 79.86 82.12 79.80 81.20 84.74 86.13
Table 1: Development results for Single Malt (Base = baseline, PA = parsing algorithm, F+L = feature model
and learning algorithm); dry run and test results for Single Malt (SM) and Blended (B) (with corrected test
scores for Blended on Chinese). All scores are labeled attachment scores (LAS) except the last two columns,
which report unlabeled attachment scores (UAS) on the test sets.
slightly better results were obtained by not splitting
the training data into smaller sets; for the remain-
ing languages, accuracy was improved by using the
CPOSTAG of Next as the defining feature for the
split (instead of POSTAG). With respect to the SVM
parameters (?, r, C, and ?), Arabic, Basque, Cata-
lan, Greek and Hungarian retain the baseline set-
tings, while the other languages have slightly dif-
ferent values for some parameters.
The cumulative improvement after optimization
of feature model and learning algorithm parameters
was 1.71 percentage points on average over all ten
languages, with a minimum of 0.69 (Turkish) and a
maximum of 3.25 (Chinese) (cf. table 1).
3 The Blended Parser
The Blended parser is an ensemble system based
on the methodology proposed by Sagae and Lavie
(2006). Given the output dependency graphs Gi
(1 ? i ? m) of m different parsers for an input sen-
tence x, we construct a new graph containing all the
labeled dependency arcs proposed by some parser
and weight each arc a by a score s(a) reflecting its
popularity among the m parsers. The output of the
ensemble system for x is the maximum spanning
tree of this graph (rooted at the node 0), which can
be extracted using the Chu-Liu-Edmonds algorithm,
as shown by McDonald et al (2005). Following
Sagae and Lavie (2006), we let s(a) =
?m
i=1 w
c
iai,
where wci is the average labeled attachment score of
parser i for the word class c8 of the dependent of a,
and ai is 1 if a ? Gi and 0 otherwise.
The Blended parser uses six component parsers,
with three different parsing algorithms, each of
which is used to construct one left-to-right parser
and one right-to-left parser. The parsing algorithms
used are the arc-eager baseline algorithm, the arc-
standard variant of the baseline algorithm, and the
incremental, non-projective parsing algorithm first
described by Covington (2001) and recently used
for deterministic classifier-based parsing by Nivre
(2007), all of which are available in MaltParser.
Thus, the six component parsers for each language
were instances of the following:
1. Arc-eager projective left-to-right
2. Arc-eager projective right-to-left
3. Arc-standard projective left-to-right
4. Arc-standard projective right-to-left
5. Covington non-projective left-to-right
6. Covington non-projective right-to-left
8We use CPOSTAG to determine the part of speech.
936
root 1 2 3?6 7+
Parser R P R P R P R P R P
Single Malt 87.01 80.36 95.08 94.87 86.28 86.67 77.97 80.23 68.98 71.06
Blended 92.09 74.20 95.71 94.92 87.55 88.12 78.66 83.02 65.29 78.14
Table 2: Recall (R) and precision (P) of Single Malt and Blended for dependencies of different length,
averaged over all languages (root = dependents of root node, regardless of length).
The final Blended parser was constructed by reusing
the tuned Single Malt parser for each language (arc-
standard left-to-right for Chinese, arc-eager left-to-
right for the remaining languages) and training five
additional parsers with the same parameter settings
except for the following mechanical adjustments:
1. Pseudo-projective parsing was not used for the
two non-projective parsers.
2. Feature models were adjusted with respect to
the most obvious differences in parsing strategy
(e.g., by deleting features that could never be
informative for a given parser).
3. Learning algorithm parameters were adjusted
to speed up training (e.g., by always splitting
the training data into smaller sets).
Having trained all parsers on 90% of the training
data for each language, the weights wci for each
parser i and coarse part of speech c was determined
by the labeled attachment score on the remaining
10% of the data. This means that the results obtained
in the dry run were bound to be overly optimistic for
the Blended parser, since it was then evaluated on
the same data set that was used to tune the weights.
Finally, we want to emphasize that the time for
developing the Blended parser was severely limited,
which means that several shortcuts had to be taken,
such as optimizing learning algorithm parameters
for speed rather than accuracy and using extrapo-
lation, rather than proper tuning, for other impor-
tant parameters. This probably means that the per-
formance of the Blended system can be improved
considerably by optimizing parameters for all six
parsers separately.
4 Results and Discussion
Table 1 shows the labeled attachment score results
from our internal dry run (training on 90% of the
training data, testing on the remaining 10%) and the
official test runs for both of our systems. It should
be pointed out that the test score for the Blended
parser on Chinese is different from the official one
(75.82), which was much lower than expected due
to a corrupted specification file required by Malt-
Parser. Restoring this file and rerunning the parser
on the Chinese test set, without retraining the parser
or changing any parameter settings, resulted in the
score reported here. This also improved the aver-
age score from 80.32 to 81.20, the former being the
highest reported official score.
For the Single Malt parser, the test results are on
average very close to the dry run results, indicating
that models have not been overfitted (although there
is considerably variation between languages). For
the Blended parser, there is a drop of almost one
percentage point, which can be explained by the fact
that weights could not be tuned on held-out data for
the dry run (as explained in section 3).
Comparing the results for different languages, we
see a tendency that languages with rich morphology,
usually accompanied by flexible word order, get
lower scores. Thus, the labeled attachment score is
below 80% for Arabic, Basque, Czech, Greek, Hun-
garian, and Turkish. By comparison, the more con-
figurational languages (Catalan, Chinese, English,
and Italian) all have scores above 80%. Linguis-
tic properties thus seem to be more important than,
for example, training set size, which can be seen by
comparing the results for Italian, with one of the
smallest training sets, and Czech, with one of the
largest. The development of parsing methods that
are better suited for morphologically rich languages
with flexible word order appears as one of the most
important goals for future research in this area.
Comparing the results of our two systems, we
see that the Blended parser outperforms the Single
Malt parser for all languages, with an average im-
937
provement of 1.40 percentage points, a minimum of
0.44 (Greek) and a maximum of 2.40 (English). As
shown by McDonald and Nivre (2007), the Single
Malt parser tends to suffer from two problems: error
propagation due to the deterministic parsing strat-
egy, typically affecting long dependencies more than
short ones, and low precision on dependencies orig-
inating in the artificial root node due to fragmented
parses.9 The question is which of these problems is
alleviated by the multiple views given by the compo-
nent parsers in the Blended system. Table 2 throws
some light on this by giving the precision and re-
call for dependencies of different length, treating de-
pendents of the artificial root node as a special case.
As expected, the Single Malt parser has lower preci-
sion than recall for root dependents, but the Blended
parser has even lower precision (and somewhat bet-
ter recall), indicating that the fragmentation is even
more severe in this case.10 By contrast, we see that
precision and recall for other dependencies improve
across the board, especially for longer dependencies,
which probably means that the effect of error propa-
gation is mitigated by the use of an ensemble system,
even if each of the component parsers is determinis-
tic in itself.
5 Conclusion
We have shown that deterministic, classifier-based
dependency parsing, with careful optimization, can
give highly accurate dependency parsing for a wide
range of languages, as illustrated by the performance
of the Single Malt parser. We have also demon-
strated that an ensemble of deterministic, classifier-
based dependency parsers, built on top of a tuned
single-parser system, can give even higher accuracy,
as shown by the results of the Blended parser, which
has the highest labeled attachment score for five lan-
guages (Arabic, Basque, Catalan, Hungarian, and
9A fragmented parse is a dependency forest, rather than a
tree, and is automatically converted to a tree by attaching all
(other) roots to the artificial root node. Hence, children of the
root node in the final output may not have been predicted as
such by the treebank-induced classifier.
10This conclusion is further supported by the observation
that the single most frequent ?frame confusion? of the Blended
parser, over all languages, is to attach two dependents with the
label ROOT to the root node, instead of only one. The frequency
of this error is more than twice as high for the Blended parser
(180) as for the Single Malt parser (83).
Italian), as well as the highest multilingual average
score.
Acknowledgements
We want to thank all treebank providers for making
the data available for the shared task and the (other)
organizers for their efforts in organizing it. Special
thanks to Ryan McDonald, for fruitful discussions
and assistance with the error analysis, and to Kenji
Sagae, for showing us how to produce a good blend.
Thanks also to two reviewers for useful comments.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT), pages 201?204.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf., pages 95?102.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools,
pages 110?117.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conference on Computational
Linguistics (NODALIDA).
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
938
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Meth-
ods in Natural Language Processing (HLT/EMNLP),
pages 523?530.
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
J. Nivre and J. Nilsson. 2005. Pseudo-projective depen-
dency. In Proc. of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
99?106.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov.
2006. Labeled pseudo-projective dependency parsing
with support vector machines. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL), pages 221?225.
J. Nivre, J. Hall, S. Ku?bler, R. McDonald, J. Nilsson,
S. Riedel, and D. Yuret. 2007. The CoNLL 2007
shared task on dependency parsing. In Proc. of the
Joint Conf. on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL).
J. Nivre. 2007. Incremental non-projective dependency
parsing. In Human Language Technologies: The An-
nual Conf. of the North American Chapter of the Asso-
ciation for Computational Linguistics (NAACL-HLT),
pages 396?403.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT), pages 149?160.
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conference of the NAACL, Companion Volume:
Short Papers, pages 129?132.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT), pages 195?206.
939
Proceedings of the 43rd Annual Meeting of the ACL, pages 99?106,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Pseudo-Projective Dependency Parsing
Joakim Nivre and Jens Nilsson
School of Mathematics and Systems Engineering
Va?xjo? University
SE-35195 Va?xjo?, Sweden
{nivre,jni}@msi.vxu.se
Abstract
In order to realize the full potential of
dependency-based syntactic parsing, it is
desirable to allow non-projective depen-
dency structures. We show how a data-
driven deterministic dependency parser,
in itself restricted to projective structures,
can be combined with graph transforma-
tion techniques to produce non-projective
structures. Experiments using data from
the Prague Dependency Treebank show
that the combined system can handle non-
projective constructions with a precision
sufficient to yield a significant improve-
ment in overall parsing accuracy. This
leads to the best reported performance for
robust non-projective parsing of Czech.
1 Introduction
It is sometimes claimed that one of the advantages
of dependency grammar over approaches based on
constituency is that it allows a more adequate treat-
ment of languages with variable word order, where
discontinuous syntactic constructions are more com-
mon than in languages like English (Mel?c?uk,
1988; Covington, 1990). However, this argument
is only plausible if the formal framework allows
non-projective dependency structures, i.e. structures
where a head and its dependents may correspond
to a discontinuous constituent. From the point of
view of computational implementation this can be
problematic, since the inclusion of non-projective
structures makes the parsing problem more com-
plex and therefore compromises efficiency and in
practice also accuracy and robustness. Thus, most
broad-coverage parsers based on dependency gram-
mar have been restricted to projective structures.
This is true of the widely used link grammar parser
for English (Sleator and Temperley, 1993), which
uses a dependency grammar of sorts, the probabilis-
tic dependency parser of Eisner (1996), and more
recently proposed deterministic dependency parsers
(Yamada and Matsumoto, 2003; Nivre et al, 2004).
It is also true of the adaptation of the Collins parser
for Czech (Collins et al, 1999) and the finite-state
dependency parser for Turkish by Oflazer (2003).
This is in contrast to dependency treebanks, e.g.
Prague Dependency Treebank (Hajic? et al, 2001b),
Danish Dependency Treebank (Kromann, 2003),
and the METU Treebank of Turkish (Oflazer et al,
2003), which generally allow annotations with non-
projective dependency structures. The fact that pro-
jective dependency parsers can never exactly repro-
duce the analyses found in non-projective treebanks
is often neglected because of the relative scarcity of
problematic constructions. While the proportion of
sentences containing non-projective dependencies is
often 15?25%, the total proportion of non-projective
arcs is normally only 1?2%. As long as the main
evaluation metric is dependency accuracy per word,
with state-of-the-art accuracy mostly below 90%,
the penalty for not handling non-projective construc-
tions is almost negligible. Still, from a theoretical
point of view, projective parsing of non-projective
structures has the drawback that it rules out perfect
accuracy even as an asymptotic goal.
99
(?Only one of them concerns quality.?)
R
Z
(Out-of
 
?
AuxP
P
nich
them
 
?
Atr
VB
je
is
T
jen
only
 
?
AuxZ
C
jedna
one-FEM-SG
 
?
Sb
R
na
to
 
?
AuxP
N4
kvalitu
quality
?
 
Adv
Z:
.
.)
 
?
AuxZ
Figure 1: Dependency graph for Czech sentence from the Prague Dependency Treebank1
There exist a few robust broad-coverage parsers
that produce non-projective dependency structures,
notably Tapanainen and Ja?rvinen (1997) and Wang
and Harper (2004) for English, Foth et al (2004)
for German, and Holan (2004) for Czech. In addi-
tion, there are several approaches to non-projective
dependency parsing that are still to be evaluated in
the large (Covington, 1990; Kahane et al, 1998;
Duchier and Debusmann, 2001; Holan et al, 2001;
Hellwig, 2003). Finally, since non-projective con-
structions often involve long-distance dependencies,
the problem is closely related to the recovery of
empty categories and non-local dependencies in
constituency-based parsing (Johnson, 2002; Dienes
and Dubey, 2003; Jijkoun and de Rijke, 2004; Cahill
et al, 2004; Levy and Manning, 2004; Campbell,
2004).
In this paper, we show how non-projective depen-
dency parsing can be achieved by combining a data-
driven projective parser with special graph transfor-
mation techniques. First, the training data for the
parser is projectivized by applying a minimal num-
ber of lifting operations (Kahane et al, 1998) and
encoding information about these lifts in arc labels.
When the parser is trained on the transformed data,
it will ideally learn not only to construct projective
dependency structures but also to assign arc labels
that encode information about lifts. By applying an
inverse transformation to the output of the parser,
arcs with non-standard labels can be lowered to their
proper place in the dependency graph, giving rise
1The dependency graph has been modified to make the final
period a dependent of the main verb instead of being a depen-
dent of a special root node for the sentence.
to non-projective structures. We call this pseudo-
projective dependency parsing, since it is based on a
notion of pseudo-projectivity (Kahane et al, 1998).
The rest of the paper is structured as follows.
In section 2 we introduce the graph transformation
techniques used to projectivize and deprojectivize
dependency graphs, and in section 3 we describe the
data-driven dependency parser that is the core of our
system. We then evaluate the approach in two steps.
First, in section 4, we evaluate the graph transfor-
mation techniques in themselves, with data from the
Prague Dependency Treebank and the Danish De-
pendency Treebank. In section 5, we then evaluate
the entire parsing system by training and evaluating
on data from the Prague Dependency Treebank.
2 Dependency Graph Transformations
We assume that the goal in dependency parsing is to
construct a labeled dependency graph of the kind de-
picted in Figure 1. Formally, we define dependency
graphs as follows:
1. Let R = {r1, . . . , rm} be the set of permissible
dependency types (arc labels).
2. A dependency graph for a string of words
W = w1? ? ?wn is a labeled directed graph
D = (W,A), where
(a) W is the set of nodes, i.e. word tokens in
the input string, ordered by a linear prece-
dence relation <,
(b) A is a set of labeled arcs (wi, r, wj), where
wi, wj ?W , r ? R,
(c) for every wj ?W , there is at most one arc
(wi, r, wj) ? A.
100
(?Only one of them concerns quality.?)
R
Z
(Out-of
 
?
AuxP
P
nich
them
 
?
Atr
VB
je
is
T
jen
only
 
?
AuxZ
C
jedna
one-FEM-SG
 
?
Sb
R
na
to
 
?
AuxP
N4
kvalitu
quality
?
 
Adv
Z:
.
.)
 
?
AuxZ
Figure 2: Projectivized dependency graph for Czech sentence
3. A graph D = (W,A) is well-formed iff it is
acyclic and connected.
If (wi, r, wj) ? A, we say that wi is the head of wj
and wj a dependent of wi. In the following, we use
the notation wi
r? wj to mean that (wi, r, wj) ? A;
we also use wi ? wj to denote an arc with unspeci-
fied label and wi ?? wj for the reflexive and transi-
tive closure of the (unlabeled) arc relation.
The dependency graph in Figure 1 satisfies all the
defining conditions above, but it fails to satisfy the
condition of projectivity (Kahane et al, 1998):
1. An arc wi?wk is projective iff, for every word
wj occurring between wi and wk in the string
(wi<wj<wk or wi>wj>wk), wi ?? wj .
2. A dependency graph D = (W,A) is projective
iff every arc in A is projective.
The arc connecting the head jedna (one) to the de-
pendent Z (out-of) spans the token je (is), which is
not dominated by jedna.
As observed by Kahane et al (1998), any (non-
projective) dependency graph can be transformed
into a projective one by a lifting operation, which
replaces each non-projective arc wj ? wk by a pro-
jective arc wi ? wk such that wi ?? wj holds in
the original graph. Here we use a slightly different
notion of lift, applying to individual arcs and moving
their head upwards one step at a time:
LIFT(wj ? wk) =
{
wi ? wk if wi ? wj
undefined otherwise
Intuitively, lifting an arc makes the word wk depen-
dent on the head wi of its original head wj (which is
unique in a well-formed dependency graph), unless
wj is a root in which case the operation is undefined
(but then wj ? wk is necessarily projective if the
dependency graph is well-formed).
Projectivizing a dependency graph by lifting non-
projective arcs is a nondeterministic operation in the
general case. However, since we want to preserve
as much of the original structure as possible, we
are interested in finding a transformation that in-
volves a minimal number of lifts. Even this may
be nondeterministic, in case the graph contains sev-
eral non-projective arcs whose lifts interact, but we
use the following algorithm to construct a minimal
projective transformation D? = (W,A?) of a (non-
projective) dependency graph D = (W,A):
PROJECTIVIZE(W , A)
1 A? ? A
2 while (W,A?) is non-projective
3 a? SMALLEST-NONP-ARC(A?)
4 A? ? (A? ? {a}) ? {LIFT(a)}
5 return (W,A?)
The function SMALLEST-NONP-ARC returns the
non-projective arc with the shortest distance from
head to dependent (breaking ties from left to right).
Applying the function PROJECTIVIZE to the graph
in Figure 1 yields the graph in Figure 2, where the
problematic arc pointing to Z has been lifted from
the original head jedna to the ancestor je. Using
the terminology of Kahane et al (1998), we say that
jedna is the syntactic head of Z, while je is its linear
head in the projectivized representation.
Unlike Kahane et al (1998), we do not regard a
projectivized representation as the final target of the
parsing process. Instead, we want to apply an in-
101
Lifted arc label Path labels Number of labels
Baseline d p n
Head d?h p n(n+ 1)
Head+Path d?h p? 2n(n+ 1)
Path d? p? 4n
Table 1: Encoding schemes (d = dependent, h = syntactic head, p = path; n = number of dependency types)
verse transformation to recover the underlying (non-
projective) dependency graph. In order to facilitate
this task, we extend the set of arc labels to encode
information about lifting operations. In principle, it
would be possible to encode the exact position of the
syntactic head in the label of the arc from the linear
head, but this would give a potentially infinite set of
arc labels and would make the training of the parser
very hard. In practice, we can therefore expect a
trade-off such that increasing the amount of infor-
mation encoded in arc labels will cause an increase
in the accuracy of the inverse transformation but a
decrease in the accuracy with which the parser can
construct the labeled representations. To explore this
tradeoff, we have performed experiments with three
different encoding schemes (plus a baseline), which
are described schematically in Table 1.
The baseline simply retains the original labels for
all arcs, regardless of whether they have been lifted
or not, and the number of distinct labels is therefore
simply the number n of distinct dependency types.2
In the first encoding scheme, called Head, we use
a new label d?h for each lifted arc, where d is the
dependency relation between the syntactic head and
the dependent in the non-projective representation,
and h is the dependency relation that the syntactic
head has to its own head in the underlying structure.
Using this encoding scheme, the arc from je to Z
in Figure 2 would be assigned the label AuxP?Sb
(signifying an AuxP that has been lifted from a Sb).
In the second scheme, Head+Path, we in addition
modify the label of every arc along the lifting path
from the syntactic to the linear head so that if the
original label is p the new label is p?. Thus, the arc
from je to jedna will be labeled Sb? (to indicate that
there is a syntactic head below it). In the third and
final scheme, denoted Path, we keep the extra infor-
2Note that this is a baseline for the parsing experiment only
(Experiment 2). For Experiment 1 it is meaningless as a base-
line, since it would result in 0% accuracy.
mation on path labels but drop the information about
the syntactic head of the lifted arc, using the label d?
instead of d?h (AuxP? instead of AuxP?Sb).
As can be seen from the last column in Table 1,
both Head and Head+Path may theoretically lead
to a quadratic increase in the number of distinct arc
labels (Head+Path being worse than Head only by
a constant factor), while the increase is only linear in
the case of Path. On the other hand, we can expect
Head+Path to be the most useful representation for
reconstructing the underlying non-projective depen-
dency graph. In approaching this problem, a vari-
ety of different methods are conceivable, including
a more or less sophisticated use of machine learn-
ing. In the present study, we limit ourselves to an
algorithmic approach, using a deterministic breadth-
first search. The details of the transformation proce-
dure are slightly different depending on the encod-
ing schemes:
? Head: For every arc of the form wi
d?h
?? wn,
we search the graph top-down, left-to-right,
breadth-first starting at the head node wi. If we
find an arc wl
h?? wm, called a target arc, we
replace wi
d?h
?? wn by wm
d?? wn; otherwise
we replace wi
d?h
?? wn by wi
d?? wn (i.e. we
let the linear head be the syntactic head).
? Head+Path: Same as Head, but the search
only follows arcs of the form wj
p?
?? wk and a
target arc must have the form wl
h?
?? wm; if no
target arc is found, Head is used as backoff.
? Path: Same as Head+Path, but a target arc
must have the form wl
p?
?? wm and no out-
going arcs of the form wm
p??
?? wo; no backoff.
In section 4 we evaluate these transformations with
respect to projectivized dependency treebanks, and
in section 5 they are applied to parser output. Before
102
Feature type Top?1 Top Next Next+1 Next+2 Next+3
Word form + + + +
Part-of-speech + + + + + +
Dep type of head +
leftmost dep + +
rightmost dep +
Table 2: Features used in predicting the next parser action
we turn to the evaluation, however, we need to intro-
duce the data-driven dependency parser used in the
latter experiments.
3 Memory-Based Dependency Parsing
In the experiments below, we employ a data-driven
deterministic dependency parser producing labeled
projective dependency graphs,3 previously tested on
Swedish (Nivre et al, 2004) and English (Nivre and
Scholz, 2004). The parser builds dependency graphs
by traversing the input from left to right, using a
stack to store tokens that are not yet complete with
respect to their dependents. At each point during the
derivation, the parser has a choice between pushing
the next input token onto the stack ? with or with-
out adding an arc from the token on top of the stack
to the token pushed ? and popping a token from the
stack ? with or without adding an arc from the next
input token to the token popped. More details on the
parsing algorithm can be found in Nivre (2003).
The choice between different actions is in general
nondeterministic, and the parser relies on a memory-
based classifier, trained on treebank data, to pre-
dict the next action based on features of the cur-
rent parser configuration. Table 2 shows the features
used in the current version of the parser. At each
point during the derivation, the prediction is based
on six word tokens, the two topmost tokens on the
stack, and the next four input tokens. For each to-
ken, three types of features may be taken into ac-
count: the word form; the part-of-speech assigned
by an automatic tagger; and labels on previously as-
signed dependency arcs involving the token ? the arc
from its head and the arcs to its leftmost and right-
most dependent, respectively. Except for the left-
3The graphs satisfy all the well-formedness conditions given
in section 2 except (possibly) connectedness. For robustness
reasons, the parser may output a set of dependency trees instead
of a single tree.
most dependent of the next input token, dependency
type features are limited to tokens on the stack.
The prediction based on these features is a k-
nearest neighbor classification, using the IB1 algo-
rithm and k = 5, the modified value difference met-
ric (MVDM) and class voting with inverse distance
weighting, as implemented in the TiMBL software
package (Daelemans et al, 2003). More details on
the memory-based prediction can be found in Nivre
et al (2004) and Nivre and Scholz (2004).
4 Experiment 1: Treebank Transformation
The first experiment uses data from two dependency
treebanks. The Prague Dependency Treebank (PDT)
consists of more than 1M words of newspaper text,
annotated on three levels, the morphological, ana-
lytical and tectogrammatical levels (Hajic?, 1998).
Our experiments all concern the analytical annota-
tion, and the first experiment is based only on the
training part. The Danish Dependency Treebank
(DDT) comprises about 100K words of text selected
from the Danish PAROLE corpus, with annotation
of primary and secondary dependencies (Kromann,
2003). The entire treebank is used in the experiment,
but only primary dependencies are considered.4 In
all experiments, punctuation tokens are included in
the data but omitted in evaluation scores.
In the first part of the experiment, dependency
graphs from the treebanks were projectivized using
the algorithm described in section 2. As shown in
Table 3, the proportion of sentences containing some
non-projective dependency ranges from about 15%
in DDT to almost 25% in PDT. However, the over-
all percentage of non-projective arcs is less than 2%
in PDT and less than 1% in DDT. The last four
4If secondary dependencies had been included, the depen-
dency graphs would not have satisfied the well-formedness con-
ditions formulated in section 2.
103
# Lifts in projectivization
Data set # Sentences % NonP # Tokens % NonP 1 2 3 >3
PDT training 73,088 23.15 1,255,333 1.81 93.79 5.60 0.51 0.11
DDT total 5,512 15.48 100,238 0.94 79.49 13.28 4.36 2.87
Table 3: Non-projective sentences and arcs in PDT and DDT (NonP = non-projective)
Data set Head H+P Path
PDT training (28 labels) 92.3 (230) 99.3 (314) 97.3 (84)
DDT total (54 labels) 92.3 (123) 99.8 (147) 98.3 (99)
Table 4: Percentage of non-projective arcs recovered correctly (number of labels in parentheses)
columns in Table 3 show the distribution of non-
projective arcs with respect to the number of lifts
required. It is worth noting that, although non-
projective constructions are less frequent in DDT
than in PDT, they seem to be more deeply nested,
since only about 80% can be projectivized with a
single lift, while almost 95% of the non-projective
arcs in PDT only require a single lift.
In the second part of the experiment, we applied
the inverse transformation based on breadth-first
search under the three different encoding schemes.
The results are given in Table 4. As expected, the
most informative encoding, Head+Path, gives the
highest accuracy with over 99% of all non-projective
arcs being recovered correctly in both data sets.
However, it can be noted that the results for the least
informative encoding, Path, are almost comparable,
while the third encoding, Head, gives substantially
worse results for both data sets. We also see that
the increase in the size of the label sets for Head
and Head+Path is far below the theoretical upper
bounds given in Table 1. The increase is gener-
ally higher for PDT than for DDT, which indicates a
greater diversity in non-projective constructions.
5 Experiment 2: Memory-Based Parsing
The second experiment is limited to data from PDT.5
The training part of the treebank was projectivized
under different encoding schemes and used to train
memory-based dependency parsers, which were run
on the test part of the treebank, consisting of 7,507
5Preliminary experiments using data from DDT indicated
that the limited size of the treebank creates a severe sparse data
problem with respect to non-projective constructions.
sentences and 125,713 tokens.6 The inverse trans-
formation was applied to the output of the parsers
and the result compared to the gold standard test set.
Table 5 shows the overall parsing accuracy at-
tained with the three different encoding schemes,
compared to the baseline (no special arc labels) and
to training directly on non-projective dependency
graphs. Evaluation metrics used are Attachment
Score (AS), i.e. the proportion of tokens that are at-
tached to the correct head, and Exact Match (EM),
i.e. the proportion of sentences for which the depen-
dency graph exactly matches the gold standard. In
the labeled version of these metrics (L) both heads
and arc labels must be correct, while the unlabeled
version (U) only considers heads.
The first thing to note is that projectivizing helps
in itself, even if no encoding is used, as seen from
the fact that the projective baseline outperforms the
non-projective training condition by more than half
a percentage point on attachment score, although the
gain is much smaller with respect to exact match.
The second main result is that the pseudo-projective
approach to parsing (using special arc labels to guide
an inverse transformation) gives a further improve-
ment of about one percentage point on attachment
score. With respect to exact match, the improvement
is even more noticeable, which shows quite clearly
that even if non-projective dependencies are rare on
the token level, they are nevertheless important for
getting the global syntactic structure correct.
All improvements over the baseline are statisti-
cally significant beyond the 0.01 level (McNemar?s
6The part-of-speech tagging used in both training and testing
was the uncorrected output of an HMM tagger distributed with
the treebank; cf. Hajic? et al (2001a).
104
Encoding UAS LAS UEM LEM
Non-projective 78.5 71.3 28.9 20.6
Baseline 79.1 72.0 29.2 20.7
Head 80.1 72.8 31.6 22.2
Head+Path 80.0 72.8 31.8 22.4
Path 80.0 72.7 31.6 22.0
Table 5: Parsing accuracy (AS = attachment score, EM = exact match; U = unlabeled, L = labeled)
Unlabeled Labeled
Encoding P R F P R F
Head 61.3 54.1 57.5 55.2 49.8 52.4
Head+Path 63.9 54.9 59.0 57.9 50.6 54.0
Path 58.2 49.5 53.4 51.0 45.7 48.2
Table 6: Precision, recall and F-measure for non-projective arcs
test). By contrast, when we turn to a comparison
of the three encoding schemes it is hard to find any
significant differences, and the overall impression is
that it makes little or no difference which encoding
scheme is used, as long as there is some indication
of which words are assigned their linear head instead
of their syntactic head by the projective parser. This
may seem surprising, given the experiments reported
in section 4, but the explanation is probably that the
non-projective dependencies that can be recovered at
all are of the simple kind that only requires a single
lift, where the encoding of path information is often
redundant. It is likely that the more complex cases,
where path information could make a difference, are
beyond the reach of the parser in most cases.
However, if we consider precision, recall and F-
measure on non-projective dependencies only, as
shown in Table 6, some differences begin to emerge.
The most informative scheme, Head+Path, gives
the highest scores, although with respect to Head
the difference is not statistically significant, while
the least informative scheme, Path ? with almost the
same performance on treebank transformation ? is
significantly lower (p < 0.01). On the other hand,
given that all schemes have similar parsing accuracy
overall, this means that the Path scheme is the least
likely to introduce errors on projective arcs.
The overall parsing accuracy obtained with the
pseudo-projective approach is still lower than for the
best projective parsers. Although the best published
results for the Collins parser is 80% UAS (Collins,
1999), this parser reaches 82% when trained on the
entire training data set, and an adapted version of
Charniak?s parser (Charniak, 2000) performs at 84%
(Jan Hajic?, pers. comm.). However, the accuracy is
considerably higher than previously reported results
for robust non-projective parsing of Czech, with a
best performance of 73% UAS (Holan, 2004).
Compared to related work on the recovery of
long-distance dependencies in constituency-based
parsing, our approach is similar to that of Dienes
and Dubey (2003) in that the processing of non-local
dependencies is partly integrated in the parsing pro-
cess, via an extension of the set of syntactic cate-
gories, whereas most other approaches rely on post-
processing only. However, while Dienes and Dubey
recognize empty categories in a pre-processing step
and only let the parser find their antecedents, we use
the parser both to detect dislocated dependents and
to predict either the type or the location of their syn-
tactic head (or both) and use post-processing only to
transform the graph in accordance with the parser?s
analysis.
6 Conclusion
We have presented a new method for non-projective
dependency parsing, based on a combination of
data-driven projective dependency parsing and
graph transformation techniques. The main result is
that the combined system can recover non-projective
dependencies with a precision sufficient to give a
significant improvement in overall parsing accuracy,
105
especially with respect to the exact match criterion,
leading to the best reported performance for robust
non-projective parsing of Czech.
Acknowledgements
This work was supported in part by the Swedish
Research Council (621-2002-4207). Memory-based
classifiers for the experiments were created using
TiMBL (Daelemans et al, 2003). Special thanks to
Jan Hajic? and Matthias Trautner Kromann for assis-
tance with the Czech and Danish data, respectively,
and to Jan Hajic?, Toma?s? Holan, Dan Zeman and
three anonymous reviewers for valuable comments
on a preliminary version of the paper.
References
Cahill, A., Burke, M., O?Donovan, R., Van Genabith, J. and
Way, A. 2004. Long-distance dependency resolution in
automatically acquired wide-coverage PCFG-based LFG ap-
proximations. In Proceedings of ACL.
Campbell, R. 2004. Using linguistic principles to recover
empty categories. In Proceedings of ACL.
Charniak, E. 2000. A maximum-entropy-inspired parser. In
Proceedings of NAACL.
Collins, M., Hajic?, J., Brill, E., Ramshaw, L. and Tillmann, C.
1999. A statistical parser for Czech. In Proceedings of ACL.
Collins, M. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
Covington, M. A. 1990. Parsing discontinuous constituents in
dependency grammar. Computational Linguistics, 16:234?
236.
Daelemans, W., Zavrel, J., van der Sloot, K. and van den Bosch,
A. 2003. TiMBL: Tilburg Memory Based Learner, version
5.0, Reference Guide. Technical Report ILK 03-10, Tilburg
University, ILK.
Dienes, P. and Dubey, A. 2003. Deep syntactic processing by
combining shallow methods. In Proceedings of ACL.
Duchier, D. and Debusmann, R. 2001. Topological dependency
trees: A constraint-based account of linear precedence. In
Proceedings of ACL.
Eisner, J. M. 1996. Three new probabilistic models for depen-
dency parsing: An exploration. In Proceedings of COLING.
Foth, K., Daum, M. and Menzel, W. 2004. A broad-coverage
parser for German based on defeasible constraints. In Pro-
ceedings of KONVENS.
Hajic?, J., Krbec, P., Oliva, K., Kveton, P. and Petkevic, V. 2001.
Serial combination of rules and statistics: A case study in
Czech tagging. In Proceedings of ACL.
Hajic?, J., Vidova Hladka, B., Panevova?, J., Hajic?ova?, E., Sgall,
P. and Pajas, P. 2001. Prague Dependency Treebank 1.0.
LDC, 2001T10.
Hajic?, J. 1998. Building a syntactically annotated corpus:
The Prague Dependency Treebank. In Issues of Valency and
Meaning, pages 106?132. Karolinum.
Hellwig, P. 2003. Dependency unification grammar. In Depen-
dency and Valency, pages 593?635. Walter de Gruyter.
Holan, T., Kubon?, V. and Pla?tek, M. 2001. Word-order re-
laxations and restrictions within a dependency grammar. In
Proceedings of IWPT.
Holan, T. 2004. Tvorba zavislostniho syntaktickeho analyza-
toru. In Proceedings of MIS?2004.
Jijkoun, V. and de Rijke, M. 2004. Enriching the output of
a parser using memory-based learning. In Proceedings of
ACL.
Johnson, M. 2002. A simple pattern-matching algorithm for re-
covering empty nodes and their antecedents. In Proceedings
of ACL.
Kahane, S., Nasr, A. and Rambow, O. 1998. Pseudo-
projectivity: A polynomially parsable non-projective depen-
dency grammar. In Proceedings of ACL-COLING.
Kromann, M. T. 2003. The Danish Dependency Treebank and
the DTAG treebank tool. In Proceedings of TLT 2003.
Levy, R. and Manning, C. 2004. Deep dependencies from
context-free statistical parsers: Correcting the surface depen-
dency approximation. In Proceedings of ACL.
Mel?c?uk, I. 1988. Dependency Syntax: Theory and Practice.
State University of New York Press.
Nivre, J. and Scholz, M. 2004. Deterministic dependency pars-
ing of English text. In Proceedings of COLING.
Nivre, J., Hall, J. and Nilsson, J. 2004. Memory-based depen-
dency parsing. In Proceedings of CoNLL.
Nivre, J. 2003. An efficient algorithm for projective depen-
dency parsing. In Proceedings of IWPT.
Oflazer, K., Say, B., Hakkani-Tu?r, D. Z. and Tu?r, G. 2003.
Building a Turkish treebank. In Treebanks: Building and
Using Parsed Corpora, pages 261?277. Kluwer Academic
Publishers.
Oflazer, K. 2003. Dependency parsing with an extended finite-
state approach. Computational Linguistics, 29:515?544.
Sleator, D. and Temperley, D. 1993. Parsing English with a
link grammar. In Proceedings of IWPT.
Tapanainen, P. and Ja?rvinen, T. 1997. A non-projective depen-
dency parser. In Proceedings of ANLP.
Wang, W. and Harper, M. P. 2004. A statistical constraint
dependency grammar (CDG) parser. In Proceedings of the
Workshop in Incremental Parsing (ACL).
Yamada, H. and Matsumoto, Y. 2003. Statistical dependency
analysis with support vector machines. In Proceedings of
IWPT.
106
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 257?264,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Graph Transformations in Data-Driven Dependency Parsing
Jens Nilsson
Va?xjo? University
jni@msi.vxu.se
Joakim Nivre
Va?xjo? University and
Uppsala University
nivre@msi.vxu.se
Johan Hall
Va?xjo? University
jha@msi.vxu.se
Abstract
Transforming syntactic representations in
order to improve parsing accuracy has
been exploited successfully in statistical
parsing systems using constituency-based
representations. In this paper, we show
that similar transformations can give sub-
stantial improvements also in data-driven
dependency parsing. Experiments on the
Prague Dependency Treebank show that
systematic transformations of coordinate
structures and verb groups result in a
10% error reduction for a deterministic
data-driven dependency parser. Combin-
ing these transformations with previously
proposed techniques for recovering non-
projective dependencies leads to state-of-
the-art accuracy for the given data set.
1 Introduction
It has become increasingly clear that the choice
of suitable internal representations can be a very
important factor in data-driven approaches to syn-
tactic parsing, and that accuracy can often be im-
proved by internal transformations of a given kind
of representation. This is well illustrated by the
Collins parser (Collins, 1997; Collins, 1999), scru-
tinized by Bikel (2004), where several transforma-
tions are applied in order to improve the analy-
sis of noun phrases, coordination and punctuation.
Other examples can be found in the work of John-
son (1998) and Klein and Manning (2003), which
show that well-chosen transformations of syntac-
tic representations can greatly improve the parsing
accuracy obtained with probabilistic context-free
grammars.
In this paper, we apply essentially the same
techniques to data-driven dependency parsing,
specifically targeting the analysis of coordination
and verb groups, two very common constructions
that pose special problems for dependency-based
approaches. The basic idea is that we can facili-
tate learning by transforming the training data for
the parser and that we can subsequently recover
the original representations by applying an inverse
transformation to the parser?s output.
The data used in the experiments come from
the Prague Dependency Treebank (PDT) (Hajic?,
1998; Hajic? et al, 2001), the largest avail-
able dependency treebank, annotated according to
the theory of Functional Generative Description
(FGD) (Sgall et al, 1986). The parser used is
MaltParser (Nivre and Hall, 2005; Nivre et al,
2006), a freely available system that combines a
deterministic parsing strategy with discriminative
classifiers for predicting the next parser action.
The paper is structured as follows. Section 2
provides the necessary background, including a
definition of dependency graphs, a discussion of
different approaches to the analysis of coordina-
tion and verb groups in dependency grammar, as
well as brief descriptions of PDT, MaltParser and
some related work. Section 3 introduces a set
of dependency graph transformations, specifically
defined to deal with the dependency annotation
found in PDT, which are experimentally evaluated
in section 4. While the experiments reported in
section 4.1 deal with pure treebank transforma-
tions, in order to establish an upper bound on what
can be achieved in parsing, the experiments pre-
sented in section 4.2 examine the effects of differ-
ent transformations on parsing accuracy. Finally,
in section 4.3, we combine these transformations
with previously proposed techniques in order to
optimize overall parsing accuracy. We conclude
in section 5.
257
2 Background
2.1 Dependency Graphs
The basic idea in dependency parsing is that the
syntactic analysis consists in establishing typed,
binary relations, called dependencies, between the
words of a sentence. This kind of analysis can be
represented by a labeled directed graph, defined as
follows:
? Let R = {r1, . . . , rm} be a set of dependency
types (arc labels).
? A dependency graph for a string of words
W = w1 . . . wn is a labeled directed graph
G = (W,A), where:
? W is the set of nodes, i.e. word tokens
in the input string, ordered by a linear
precedence relation <.
? A is a set of labeled arcs (wi, r, wj), wi,
wj ? W , r ? R.
? A dependency graph G = (W,A) is well-
formed iff it is acyclic and no node has an
in-degree greater than 1.
We will use the notation wi r? wj to symbolize
that (wi, r, wj) ? A, where wi is referred to as
the head and wj as the dependent. We say that
an arc is projective iff, for every word wj occur-
ring between wi and wk (i.e., wi < wj < wk
or wi > wj > wk), there is a path from wi to
wj . A graph is projective iff all its arcs are pro-
jective. Figure 1 shows a well-formed (projective)
dependency graph for a sentence from the Prague
Dependency Treebank.
2.2 Coordination and Verb Groups
Dependency grammar assumes that syntactic
structure consists of lexical nodes linked by binary
dependencies. Dependency theories are thus best
suited for binary syntactic constructions, where
one element can clearly be distinguished as the
syntactic head. The analysis of coordination is
problematic in this respect, since it normally in-
volves at least one conjunction and two conjuncts.
The verb group, potentially consisting of a whole
chain of verb forms, is another type of construc-
tion where the syntactic relation between elements
is not clear-cut in dependency terms.
Several solutions have been proposed to the
problem of coordination. One alternative is
to avoid creating dependency relations between
the conjuncts, and instead let the conjuncts
have a direct dependency relation to the same
head (Tesnie`re, 1959; Hudson, 1990). Another
approach is to make the conjunction the head and
let the conjuncts depend on the conjunction. This
analysis, which appears well motivated on seman-
tic grounds, is adopted in the FGD framework and
will therefore be called Prague style (PS). It is
exemplified in figure 1, where the conjunction a
(and) is the head of the conjuncts bojovnost?? and
tvrdost??. A different solution is to adopt a more
hierarchical analysis, where the conjunction de-
pends on the first conjunct, while the second con-
junct depends on the conjunction. In cases of
multiple coordination, this can be generalized to a
chain, where each element except the first depends
on the preceding one. This more syntactically
oriented approach has been advocated notably by
Mel?c?uk (1988) and will be called Mel?c?uk style
(MS). It is illustrated in figure 2, which shows a
transformed version of the dependency graph in
figure 1, where the elements of the coordination
form a chain with the first conjunct (bojovnost??) as
the topmost head. Lombardo and Lesmo (1998)
conjecture that MS is more suitable than PS for
incremental dependency parsing.
The difference between the more semantically
oriented PS and the more syntactically oriented
MS is seen also in the analysis of verb groups,
where the former treats the main verb as the head,
since it is the bearer of valency, while the latter
treats the auxiliary verb as the head, since it is the
finite element of the clause. Without questioning
the theoretical validity of either approach, we can
again ask which analysis is best suited to achieve
high accuracy in parsing.
2.3 PDT
PDT (Hajic?, 1998; Hajic? et al, 2001) consists of
1.5M words of newspaper text, annotated in three
layers: morphological, analytical and tectogram-
matical. In this paper, we are only concerned
with the analytical layer, which contains a surface-
syntactic dependency analysis, involving a set of
28 dependency types, and not restricted to projec-
tive dependency graphs.1 The annotation follows
FGD, which means that it involves a PS analysis of
both coordination and verb groups. Whether better
parsing accuracy can be obtained by transforming
1About 2% of all dependencies are non-projective and
about 25% of all sentences have a non-projective dependency
graph (Nivre and Nilsson, 2005).
258
(?The final of the tournament was distinguished by great fighting spirit and unexpected hardness?)
A7
Velkou
great
?
Atr
N7
bojovnost??
fighting-spirit
?
Obj Co
J?
a
and
?
Coord
A7
nec?ekanou
unexpected
?
Atr
N7
tvrdost??
hardness
?
Obj Co
P4
se
itself
?
AuxT
Vp
vyznac?ovalo
distinguished
N2
fina?le
final
?
Sb
N2
turnaje
of-the-tournament
?
Atr
Figure 1: Dependency graph for a Czech sentence from the Prague Dependency Treebank
(?The final of the tournament was distinguished by great fighting spirit and unexpected hardness?)
A7
Velkou
great
?
Atr
N7
bojovnost??
fighting-spirit
?
Obj
J?
a
and
?
Coord
A7
nec?ekanou
unexpected
?
Atr
N7
tvrdost??
hardness
?
Obj
P4
se
itself
?
AuxT
Vp
vyznac?ovalo
distinguished
N2
fina?le
final
?
Sb
N2
turnaje
of-the-tournament
?
Atr
Figure 2: Transformed dependency graph for a Czech sentence from the Prague Dependency Treebank
this to MS is one of the hypotheses explored in the
experimental study below.
2.4 MaltParser
MaltParser (Nivre and Hall, 2005; Nivre et al,
2006) is a data-driven parser-generator, which can
induce a dependency parser from a treebank, and
which supports several parsing algorithms and
learning algorithms. In the experiments below we
use the algorithm of Nivre (2003), which con-
structs a labeled dependency graph in one left-
to-right pass over the input. Classifiers that pre-
dict the next parser action are constructed through
memory-based learning (MBL), using the TIMBL
software package (Daelemans and Van den Bosch,
2005), and support vector machines (SVM), using
LIBSVM (Chang and Lin, 2005).
2.5 Related Work
Other ways of improving parsing accuracy with
respect to coordination include learning patterns
of morphological and semantical information for
the conjuncts (Park and Cho, 2000). More specifi-
cally for PDT, Collins et al (1999) relabel coordi-
nated phrases after converting dependency struc-
tures to phrase structures, and Zeman (2004) uses
a kind of pattern matching, based on frequencies
of the parts-of-speech of conjuncts and conjunc-
tions. Zeman also mentions experiments to trans-
form the dependency structure for coordination
but does not present any results.
Graph transformations in dependency parsing
have also been used in order to recover non-
projective dependencies together with parsers that
are restricted to projective dependency graphs.
Thus, Nivre and Nilsson (2005) improve parsing
accuracy for MaltParser by projectivizing training
data and applying an inverse transformation to the
output of the parser, while Hall and Nova?k (2005)
apply post-processing to the output of Charniak?s
parser (Charniak, 2000). In the final experi-
ments below, we combine these techniques with
the transformations investigated in this paper.
3 Dependency Graph Transformations
In this section, we describe algorithms for trans-
forming dependency graphs in PDT from PS to
MS and back, starting with coordination and con-
tinuing with verb groups.
3.1 Coordination
The PS-to-MS transformation for coordination
will be designated ?c(?), where ? is a data set.
The transformation begins with the identification
of a base conjunction, based on its dependency
type (Coord) and/or its part-of-speech (J?). For
example, the word a (and) in figure 1 is identified
as a base conjunction.
259
Before the actual transformation, the base con-
junction and all its dependents need to be classi-
fied into three different categories. First, the base
conjunction is categorized as a separator (S). If
the coordination consists of more than two con-
juncts, it normally has one or more commas sep-
arating conjuncts, in addition to the base conjunc-
tion. These are identified by looking at their de-
pendency type (mostly AuxX) and are also catego-
rized as S. The coordination in figure 1 contains
no commas, so only the word a will belong to S.
The remaining dependents of the base conjunc-
tion need to be divided into conjuncts (C) and
other dependents (D). To make this distinction,
the algorithm again looks at the dependency type.
In principle, the dependency type of a conjunct
has the suffix Co, although special care has to be
taken for coordinated prepositional cases and em-
bedded clauses (Bo?hmova? et al, 2003). The words
bojovnost?? and tvrdost?? in figure 1, both having the
dependency type Obj Co, belong to the category
C. Since there are no other dependents of a, the
coordination contains no instances of the category
D.
Given this classification of the words involved
in a coordination, the transformation ?c(?) is
straightforward and basically connects all the arcs
in a chain. Let C1, . . . , Cn be the elements of C,
ordered by linear precedence, and let S1i , . . . , Smi
be the separators occurring between Ci and Ci+1.
Then every Ci becomes the head of S1i , . . . , Smi ,
Smi becomes the head of Ci+1, and C1 becomes
the only dependent of the original head of the base
conjunction. The dependency types of the con-
juncts are truncated by removing the suffix Co.2
Also, each word in wd ? D becomes a dependent
of the conjunct closest to its left, and if such a word
does not exist, wd will depend on the leftmost con-
junct. After the transformation ?c(?), every coor-
dination forms a left-headed chain, as illustrated
in figure 2.
This new representation creates a problem,
however. It is no longer possible to distinguish the
dependents in D from other dependents of the con-
juncts. For example, the word Velkou in figure 2
is not distinguishable from a possible dependent
in D, which is an obvious drawback when trans-
forming back to PS. One way of distinguishing D
elements is to extend the set of dependency types.
2Preliminary results indicated that this increases parsing
accuracy.
The dependency type r of each wd ? D can be re-
placed by a completely new dependency type r+
(e.g., Atr+), theoretically increasing the number
of dependency types to 2 ? |R|.
The inverse transformation, ??1c (?), again
starts by identifying base conjunctions, using the
same conditions as before. For each identified
base conjunction, it calls a procedure that per-
forms the inverse transformation by traversing
the chain of conjuncts and separators ?upwards?
(right-to-left), collecting conjuncts (C), separators
(S) and potential conjunction dependents (Dpot).
When this is done, the former head of the left-
most conjunct (C1) becomes the head of the right-
most (base) conjunction (Smn?1). In figure 2,
the leftmost conjunct is bojovnost??, with the head
vyznac?ovalo, and the rightmost (and only) con-
junction is a, which will then have vyznac?ovalo as
its new head. All conjuncts in the chain become
dependents of the rightmost conjunction, which
means that the structure is converted back to the
one depicted in figure 1.
As mentioned above, the original structure in
figure 1 did not have any coordination dependents,
but Velkou ? Dpot. The last step of the inverse
transformation is therefore to sort out conjunction
dependents from conjunct dependents, where the
former will attach to the base conjunction. Four
versions have been implemented, two of which
take into account the fact that the dependency
types AuxG, AuxX, AuxY, and Pred are the only
dependency types that are more frequent as con-
junction dependents (D) than as conjunct depen-
dents in the training data set:
? ?c: Do not extend arc labels in ?c. Leave all
words in Dpot in place in ??1c .
? ?c? : Do not extend arc labels in ?c. Attach all
words with label AuxG, AuxX, AuxY or Pred
to the base conjunction in ??1c .
? ?c+: Extend arc labels from r to r+ for D
elements in ?c. Attach all words with label
r+ to the base conjunction (and change the
label to r) in ??1c .
? ?c+? : Extend arc labels from r to r+ for D
elements in ?c, except for the labels AuxG,
AuxX, AuxY and Pred. Attach all words with
label r+, AuxG, AuxX, AuxY, or Pred to the
base conjunction (and change the label to r if
necessary) in ??1c .
260
3.2 Verb Groups
To transform verb groups from PS to MS, the
transformation algorithm, ?v(?), starts by identi-
fying all auxiliary verbs in a sentence. These will
belong to the set A and are processed from left to
right. A word waux ? A iff wmain AuxV?? waux,
where wmain is the main verb. The transformation
into MS reverses the relation between the verbs,
i.e., waux AuxV?? wmain, and the former head of
wmain becomes the new head of waux. The main
verb can be located on either side of the auxiliary
verb and can have other dependents (whereas aux-
iliary verbs never have dependents), which means
that dependency relations to other dependents of
wmain may become non-projective through the
transformation. To avoid this, all dependents to
the left of the rightmost verb will depend on the
leftmost verb, whereas the others will depend on
the rightmost verb.
Performing the inverse transformation for verb
groups, ??1v (?), is quite simple and essentially
the same procedure inverted. Each sentence is tra-
versed from right to left looking for arcs of the
type waux AuxV?? wmain. For every such arc, the
head of waux will be the new head of wmain, and
wmain the new head of waux. Furthermore, since
waux does not have dependents in PS, all depen-
dents of waux in MS will become dependents of
wmain in PS.
4 Experiments
All experiments are based on PDT 1.0, which is
divided into three data sets, a training set (?t), a
development test set (?d), and an evaluation test
set (?e). Table 1 shows the size of each data set, as
well as the relative frequency of the specific con-
structions that are in focus here. Only 1.3% of all
words in the training data are identified as auxil-
iary verbs (A), whereas coordination (S and C)
is more common in PDT. This implies that coor-
dination transformations are more likely to have
a greater impact on overall accuracy compared to
the verb group transformations.
In the parsing experiments reported in sections
4.1?4.2, we use ?t for training, ?d for tuning, and
?e for the final evaluation. The part-of-speech
tagging used (both in training and testing) is the
HMM tagging distributed with the treebank, with
a tagging accuracy of 94.1%, and with the tagset
compressed to 61 tags as in Collins et al (1999).
Data #S #W %S %C %A
?t 73088 1256k 3.9 7.7 1.3
?d 7319 126k 4.0 7.8 1.4
?e 7507 126k 3.8 7.3 1.4
Table 1: PDT data sets; S = sentence, W = word;
S = separator, C = conjunct, A = auxiliary verb
T AS
?c 97.8
?c? 98.6
?c+ 99.6
?c+? 99.4
?v 100.0
Table 2: Transformations; T = transformation;
AS = attachment score (unlabeled) of ??1(?(?t))
compared to ?t
MaltParser is used with the parsing algorithm of
Nivre (2003) together with the feature model used
for parsing Czech by Nivre and Nilsson (2005).
In section 4.2 we use MBL, again with the same
settings as Nivre and Nilsson (2005),3 and in sec-
tion 4.2 we use SVM with a polynomial kernel of
degree 2.4 The metrics for evaluation are the at-
tachment score (AS) (labeled and unlabeled), i.e.,
the proportion of words that are assigned the cor-
rect head, and the exact match (EM) score (labeled
and unlabeled), i.e., the proportion of sentences
that are assigned a completely correct analysis.
All tokens, including punctuation, are included in
the evaluation scores. Statistical significance is as-
sessed using McNemar?s test.
4.1 Experiment 1: Transformations
The algorithms are fairly simple. In addition, there
will always be a small proportion of syntactic con-
structions that do not follow the expected pattern.
Hence, the transformation and inverse transforma-
tion will inevitably result in some distortion. In
order to estimate the expected reduction in pars-
ing accuracy due to this distortion, we first con-
sider a pure treebank transformation experiment,
where we compare ??1(?(?t)) to ?t, for all the
different transformations ? defined in the previous
section. The results are shown in table 2.
We see that, even though coordination is more
frequent, verb groups are easier to handle.5 The
3TIMBL parameters: -k5 -mM -L3 -w0 -dID.
4LIBSVM parameters: -s0 -t1 -d2 -g0.12 -r0 -c1 -e0.1.
5The result is rounded to 100.0% but the transformed tree-
261
coordination version with the least loss of infor-
mation (?c+) fails to recover the correct head for
0.4% of all words in ?t.
The difference between ?c+ and ?c is expected.
However, in the next section this will be contrasted
with the increased burden on the parser for ?c+,
since it is also responsible for selecting the correct
dependency type for each arc among as many as
2 ? |R| types instead of |R|.
4.2 Experiment 2: Parsing
Parsing experiments are carried out in four steps
(for a given transformation ? ):
1. Transform the training data set into ?(?t).
2. Train a parser p on ?(?t).
3. Parse a test set ? using p with output p(?).
4. Transform the parser output into ??1(p(?)).
Table 3 presents the results for a selection of trans-
formations using MaltParser with MBL, tested on
the evaluation test set ?e with the untransformed
data as baseline. Rows 2?5 show that transform-
ing coordinate structures to MS improves parsing
accuracy compared to the baseline, regardless of
which transformation and inverse transformation
are used. Moreover, the parser benefits from the
verb group transformation, as seen in row 6.
The final row shows the best combination of a
coordination transformation with the verb group
transformation, which amounts to an improvement
of roughly two percentage points, or a ten percent
overall error reduction, for unlabeled accuracy.
All improvements over the baseline are statis-
tically significant (McNemar?s test) with respect
to attachment score (labeled and unlabeled) and
unlabeled exact match, with p < 0.01 except for
the unlabeled exact match score of the verb group
transformation, where 0.01 < p < 0.05. For the
labeled exact match, no differences are significant.
The experimental results indicate that MS is
more suitable than PS as the target representation
for deterministic data-driven dependency parsing.
A relevant question is of course why this is the
case. A partial explanation may be found in the
?short-dependency preference? exhibited by most
parsers (Eisner and Smith, 2005), with MaltParser
being no exception. The first row of table 4 shows
the accuracy of the parser for different arc lengths
under the baseline condition (i.e., with no trans-
formations). We see that it performs very well on
bank contains 19 erroneous heads.
AS EM
T U L U L
None 79.08 72.83 28.99 21.15
?c 80.55 74.06 30.08 21.27
?c? 80.90 74.41 30.56 21.42
?c+ 80.58 74.07 30.42 21.17
?c+? 80.87 74.36 30.89 21.38
?v 79.28 72.97 29.53 21.38
?v??c+? 81.01 74.51 31.02 21.57
Table 3: Parsing accuracy (MBL, ?e); T = trans-
formation; AS = attachment score, EM = exact
match; U = unlabeled, L = labeled
AS ?e 90.1 83.6 70.5 59.5 45.9
Length: 1 2-3 4-6 7-10 11-
?t 51.9 29.4 11.2 4.4 3.0
?c(?t) 54.1 29.1 10.7 3.8 2.4
?v(?t) 52.9 29.2 10.7 4.2 2.9
Table 4: Baseline labeled AS per arc length on ?e
(row 1); proportion of arcs per arc length in ?t
(rows 3?5)
short arcs, but that accuracy drops quite rapidly
as the arcs get longer. This can be related to the
mean arc length in ?t, which is 2.59 in the un-
transformed version, 2.40 in ?c(?t) and 2.54 in
?v(?t). Rows 3-5 in table 4 show the distribution
of arcs for different arc lengths in different ver-
sions of the data set. Both ?c and ?v make arcs
shorter on average, which may facilitate the task
for the parser.
Another possible explanation is that learning is
facilitated if similar constructions are represented
similarly. For instance, it is probable that learning
is made more difficult when a unit has different
heads depending on whether it is part of a coordi-
nation or not.
4.3 Experiment 3: Optimization
In this section we combine the best results from
the previous section with the graph transforma-
tions proposed by Nivre and Nilsson (2005) to re-
cover non-projective dependencies. We write ?p
for the projectivization of training data and ??1p for
the inverse transformation applied to the parser?s
output.6 In addition, we replace MBL with SVM,
a learning algorithm that tends to give higher accu-
racy in classifier-based parsing although it is more
6More precisely, we use the variant called PATH in Nivre
and Nilsson (2005).
262
AS EM
T LA U L U L
None MBL 79.08 72.83 28.99 21.15
?p MBL 80.79 74.39 31.54 22.53
?p??v??c+? MBL 82.93 76.31 34.17 23.01
None SVM 81.09 75.68 32.24 25.02
?p SVM 82.93 77.28 35.99 27.05
?p??v??c+? SVM 84.55 78.82 37.63 27.69
Table 5: Optimized parsing results (SVM, ?e); T = transformation; LA = learning algorithm; AS =
attachment score, EM = exact match; U = unlabeled, L = labeled
T P:S R:S P:C R:C P:A R:A P:M R:M
None 52.63 72.35 55.15 67.03 82.17 82.21 69.95 69.07
?p??v??c+? 63.73 82.10 63.20 75.14 90.89 92.79 80.02 81.40
Table 6: Detailed results for SVM; T = transformation; P = unlabeled precision, R = unlabeled recall
costly to train (Sagae and Lavie, 2005).
Table 5 shows the results, for both MBL and
SVM, of the baseline, the pure pseudo-projective
parsing, and the combination of pseudo-projective
parsing with PS-to-MS transformations. We see
that pseudo-projective parsing brings a very con-
sistent increase in accuracy of at least 1.5 percent-
age points, which is more than that reported by
Nivre and Nilsson (2005), and that the addition
of the PS-to-MS transformations increases accu-
racy with about the same margin. We also see that
SVM outperforms MBL by about two percentage
points across the board, and that the positive effect
of the graph transformations is most pronounced
for the unlabeled exact match score, where the
improvement is more than five percentage points
overall for both MBL and SVM.
Table 6 gives a more detailed analysis of the
parsing results for SVM, comparing the optimal
parser to the baseline, and considering specifically
the (unlabeled) precision and recall of the cate-
gories involved in coordination (separators S and
conjuncts C) and verb groups (auxiliary verbs A
and main verbs M ). All figures indicate, with-
out exception, that the transformations result in
higher precision and recall for all directly involved
words. (All differences are significant beyond the
0.01 level.) It is worth noting that the error reduc-
tion is actually higher for A and M than for S and
C, although the former are less frequent.
With respect to unlabeled attachment score, the
results of the optimized parser are slightly below
the best published results for a single parser. Hall
and Nova?k (2005) report a score of 85.1%, apply-
ing a corrective model to the output of Charniak?s
parser; McDonald and Pereira (2006) achieve a
score of 85.2% using a second-order spanning tree
algorithm. Using ensemble methods and a pool of
different parsers, Zeman and ?Zabokrtsky? (2005)
attain a top score of 87.0%. For unlabeled exact
match, our results are better than any previously
reported results, including those of McDonald and
Pereira (2006). (For the labeled scores, we are not
aware of any comparable results in the literature.)
5 Conclusion
The results presented in this paper confirm that
choosing the right representation is important
in parsing. By systematically transforming the
representation of coordinate structures and verb
groups in PDT, we achieve a 10% error reduc-
tion for a data-driven dependency parser. Adding
graph transformations for non-projective depen-
dency parsing gives a total error reduction of
about 20% (even more for unlabeled exact match).
In this way, we achieve state-of-the-art accuracy
with a deterministic, classifier-based dependency
parser.
Acknowledgements
The research presented in this paper was partially
supported by the Swedish Research Council. We
are grateful to Jan Hajic? and Daniel Zeman for
help with the Czech data and to three anonymous
reviewers for helpful comments and suggestions.
263
References
Daniel M. Bikel. 2004. Intricacies of Collins? parsing
model. Computational Linguistics, 30:479?511.
Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora
Hladka?. 2003. The Prague Dependency Treebank:
A three-level annotation scenario. In Anne Abeille?,
editor, Treebanks: Building and Using Syntactically
Annotated Corpora. Kluwer Academic Publishers.
Chih-Chung Chang and Chih-Jen Lin. 2005. LIB-
SVM: A library for support vector machines.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the First Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics (NAACL), pages
132?139.
Michael Collins, Jan Hajic?, Eric Brill, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical parser
for Czech. In Proceedings of the 37th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 505?512.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annatual Meeting of the Association for Com-
putational Linguistics (ACL), pages 16?23.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Jason Eisner and Noah A. Smith. 2005. Parsing with
soft and hard constraints on dependency length. In
Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT).
Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,
Eva Hajic?ova?, Petr Sgall, and Petr Pajas. 2001.
Prague Dependency Treebank 1.0. LDC, 2001T10.
Jan Hajic?. 1998. Building a Syntactically Annotated
Corpus: The Prague Dependency Treebank. In Is-
sues of Valency and Meaning, pages 12?19. Prague
Karolinum, Charles University Press.
Keith Hall and Vaclav Nova?k. 2005. Corrective mod-
eling for non-projective dependency parsing. In
Proceedings of the 9th International Workshop on
Parsing Technologies (IWPT).
Richard Hudson. 1990. English Word Grammar. Basil
Blackwell.
Mark Johnson. 1998. Pcfg models of linguistic
tree representations. Computational Linguistics,
24:613?632.
Dan Klein and Christopher Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 423?430.
Vincenzo Lombardo and Leonardo Lesmo. 1998.
Unit coordination and gapping in dependency the-
ory. In Proceedings of the Workshop on Processing
of Dependency-Based Grammars, pages 11?20.
Ryan McDonald and Fernando Pereira. 2006. Online
learning of approximate dependency parsing algo-
rithms. In Proceedings of the 11th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL).
Igor Mel?cuk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Joakim Nivre and Johan Hall. 2005. MaltParser: A
language-independent system for data-driven depen-
dency parsing. In Proceedings of the Fourth Work-
shop on Treebanks and Linguistic Theories (TLT),
pages 137?148.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
projective dependency parsing. In Proceedings of
the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL), pages 99?106.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A data-driven parser-generator for de-
pendency parsing. In Proceedings of the 5th In-
ternational Conference on Language Resources and
Evaluation.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Jong C. Park and Hyung Joon Cho. 2000. Informed
parsing for coordination with combinatory catego-
rial grammar. In Proceedings of the 18th Inter-
national Conference on Computational Linguistics
(COLING), pages 593?599.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the 9th International Workshop on Parsing
Technologies (IWPT), pages 125?132.
Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?. 1986.
The Meaning of the Sentence in Its Pragmatic As-
pects. Reidel.
Lucien Tesnie`re. 1959. ?Ele?ments de syntaxe struc-
turale. Editions Klincksieck.
Daniel Zeman and Zdene?k ?Zabokrtsky?. 2005. Improv-
ing parsing accuracy by combining diverse depen-
dency parsers. In Proceedings of the 9th Interna-
tional Workshop on Parsing Technologies (IWPT).
Daniel Zeman. 2004. Parsing with a Statistical De-
pendency Model. Ph.D. thesis, Charles University.
264
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 316?323,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Discriminative Classifiers for Deterministic Dependency Parsing
Johan Hall
Va?xjo? University
jni@msi.vxu.se
Joakim Nivre
Va?xjo? University and
Uppsala University
nivre@msi.vxu.se
Jens Nilsson
Va?xjo? University
jha@msi.vxu.se
Abstract
Deterministic parsing guided by treebank-
induced classifiers has emerged as a
simple and efficient alternative to more
complex models for data-driven parsing.
We present a systematic comparison of
memory-based learning (MBL) and sup-
port vector machines (SVM) for inducing
classifiers for deterministic dependency
parsing, using data from Chinese, English
and Swedish, together with a variety of
different feature models. The comparison
shows that SVM gives higher accuracy for
richly articulated feature models across all
languages, albeit with considerably longer
training times. The results also confirm
that classifier-based deterministic parsing
can achieve parsing accuracy very close to
the best results reported for more complex
parsing models.
1 Introduction
Mainstream approaches in statistical parsing are
based on nondeterministic parsing techniques,
usually employing some kind of dynamic pro-
gramming, in combination with generative prob-
abilistic models that provide an n-best ranking of
the set of candidate analyses derived by the parser
(Collins, 1997; Collins, 1999; Charniak, 2000).
These parsers can be enhanced by using a discrim-
inative model, which reranks the analyses out-
put by the parser (Johnson et al, 1999; Collins
and Duffy, 2005; Charniak and Johnson, 2005).
Alternatively, discriminative models can be used
to search the complete space of possible parses
(Taskar et al, 2004; McDonald et al, 2005).
A radically different approach is to perform
disambiguation deterministically, using a greedy
parsing algorithm that approximates a globally op-
timal solution by making a sequence of locally
optimal choices, guided by a classifier trained on
gold standard derivations from a treebank. This
methodology has emerged as an alternative to
more complex models, especially in dependency-
based parsing. It was first used for unlabeled de-
pendency parsing by Kudo and Matsumoto (2002)
(for Japanese) and Yamada and Matsumoto (2003)
(for English). It was extended to labeled depen-
dency parsing by Nivre et al (2004) (for Swedish)
and Nivre and Scholz (2004) (for English). More
recently, it has been applied with good results to
lexicalized phrase structure parsing by Sagae and
Lavie (2005).
The machine learning methods used to induce
classifiers for deterministic parsing are dominated
by two approaches. Support vector machines
(SVM), which combine the maximum margin
strategy introduced by Vapnik (1995) with the use
of kernel functions to map the original feature
space to a higher-dimensional space, have been
used by Kudo and Matsumoto (2002), Yamada and
Matsumoto (2003), and Sagae and Lavie (2005),
among others. Memory-based learning (MBL),
which is based on the idea that learning is the
simple storage of experiences in memory and that
solving a new problem is achieved by reusing so-
lutions from similar previously solved problems
(Daelemans and Van den Bosch, 2005), has been
used primarily by Nivre et al (2004), Nivre and
Scholz (2004), and Sagae and Lavie (2005).
Comparative studies of learning algorithms are
relatively rare. Cheng et al (2005b) report that
SVM outperforms MaxEnt models in Chinese de-
pendency parsing, using the algorithms of Yamada
and Matsumoto (2003) and Nivre (2003), while
Sagae and Lavie (2005) find that SVM gives better
316
performance than MBL in a constituency-based
shift-reduce parser for English.
In this paper, we present a detailed comparison
of SVM and MBL for dependency parsing using
the deterministic algorithm of Nivre (2003). The
comparison is based on data from three different
languages ? Chinese, English, and Swedish ? and
on five different feature models of varying com-
plexity, with a separate optimization of learning
algorithm parameters for each combination of lan-
guage and feature model. The central importance
of feature selection and parameter optimization in
machine learning research has been shown very
clearly in recent research (Daelemans and Hoste,
2002; Daelemans et al, 2003).
The rest of the paper is structured as follows.
Section 2 presents the parsing framework, includ-
ing the deterministic parsing algorithm and the
history-based feature models. Section 3 discusses
the two learning algorithms used in the experi-
ments, and section 4 describes the experimental
setup, including data sets, feature models, learn-
ing algorithm parameters, and evaluation metrics.
Experimental results are presented and discussed
in section 5, and conclusions in section 6.
2 Inductive Dependency Parsing
The system we use for the experiments uses no
grammar but relies completely on inductive learn-
ing from treebank data. The methodology is based
on three essential components:
1. Deterministic parsing algorithms for building
dependency graphs (Kudo and Matsumoto,
2002; Yamada and Matsumoto, 2003; Nivre,
2003)
2. History-based models for predicting the next
parser action (Black et al, 1992; Magerman,
1995; Ratnaparkhi, 1997; Collins, 1999)
3. Discriminative learning to map histories to
parser actions (Kudo and Matsumoto, 2002;
Yamada and Matsumoto, 2003; Nivre et al,
2004)
In this section we will define dependency graphs,
describe the parsing algorithm used in the experi-
ments and finally explain the extraction of features
for the history-based models.
2.1 Dependency Graphs
A dependency graph is a labeled directed graph,
the nodes of which are indices corresponding to
the tokens of a sentence. Formally:
Definition 1 Given a set R of dependency types
(arc labels), a dependency graph for a sentence
x = (w1, . . . , wn) is a labeled directed graph
G = (V,E, L), where:
1. V = Zn+1
2. E ? V ? V
3. L : E ? R
The set V of nodes (or vertices) is the set Zn+1 =
{0, 1, 2, . . . , n} (n ? Z+), i.e., the set of non-
negative integers up to and including n. This
means that every token index i of the sentence is a
node (1 ? i ? n) and that there is a special node
0, which does not correspond to any token of the
sentence and which will always be a root of the
dependency graph (normally the only root). We
use V + to denote the set of nodes corresponding
to tokens (i.e., V + = V ? {0}), and we use the
term token node for members of V +.
The set E of arcs (or edges) is a set of ordered
pairs (i, j), where i and j are nodes. Since arcs are
used to represent dependency relations, we will
say that i is the head and j is the dependent of
the arc (i, j). As usual, we will use the notation
i ? j to mean that there is an arc connecting i
and j (i.e., (i, j) ? E) and we will use the nota-
tion i ?? j for the reflexive and transitive closure
of the arc relation E (i.e., i ?? j if and only if
i = j or there is a path of arcs connecting i to j).
The function L assigns a dependency type (arc
label) r ? R to every arc e ? E.
Definition 2 A dependency graph G is well-
formed if and only if:
1. The node 0 is a root.
2. Every node has in-degree at most 1.
3. G is connected.1
4. G is acyclic.
5. G is projective.2
Conditions 1?4, which are more or less standard in
dependency parsing, together entail that the graph
is a rooted tree. The condition of projectivity, by
contrast, is somewhat controversial, since the anal-
ysis of certain linguistic constructions appears to
1To be more exact, we require G to be weakly connected,
which entails that the corresponding undirected graph is con-
nected, whereas a strongly connected graph has a directed
path between any pair of nodes.
2An arc (i, j) is projective iff there is a path from i to
every node k such that i < j < k or i > j > k. A graph G
is projective if all its arcs are projective.
317
JJ
Economic
 
?
NMOD
NN
news
 
?
SBJ
VB
had
JJ
little
 
?
NMOD
NN
effect
 
?
OBJ
IN
on
 
?
NMOD
JJ
financial
 
?
NMOD
NN
markets
 
?
PMOD
.
.
?
 
P
Figure 1: Dependency graph for an English sentence from the WSJ section of the Penn Treebank
require non-projective dependency arcs. For the
purpose of this paper, however, this assumption is
unproblematic, given that all the treebanks used in
the experiments are restricted to projective depen-
dency graphs.
Figure 1 shows a well-formed dependency
graph for an English sentence, where each word
of the sentence is tagged with its part-of-speech
and each arc labeled with a dependency type.
2.2 Parsing Algorithm
We begin by defining parser configurations and the
abstract data structures needed for the definition of
history-based feature models.
Definition 3 Given a set R = {r0, r1, . . . rm}
of dependency types and a sentence x =
(w1, . . . , wn), a parser configuration for x is a
quadruple c = (?, ?, h, d), where:
1. ? is a stack of tokens nodes.
2. ? is a sequence of token nodes.
3. h : V +x ? V is a function from token nodes
to nodes.
4. d : V +x ? R is a function from token nodes
to dependency types.
5. For every token node i ? V +x , h(i) = 0 if
and only if d(i) = r0.
The idea is that the sequence ? represents the re-
maining input tokens in a left-to-right pass over
the input sentence x; the stack ? contains partially
processed nodes that are still candidates for de-
pendency arcs, either as heads or dependents; and
the functions h and d represent a (dynamically de-
fined) dependency graph for the input sentence x.
We refer to the token node on top of the stack as
the top token and the first token node of the input
sequence as the next token.
When parsing a sentence x = (w1, . . . , wn),
the parser is initialized to a configuration c0 =
(?, (1, . . . , n), h0, d0) with an empty stack, with
all the token nodes in the input sequence, and with
all token nodes attached to the special root node
0 with a special dependency type r0. The parser
terminates in any configuration cm = (?, ?, h, d)
where the input sequence is empty, which happens
after one left-to-right pass over the input.
There are four possible parser transitions, two
of which are parameterized for a dependency type
r ? R.
1. LEFT-ARC(r) makes the top token i a (left)
dependent of the next token j with depen-
dency type r, i.e., j r? i, and immediately
pops the stack.
2. RIGHT-ARC(r) makes the next token j a
(right) dependent of the top token i with de-
pendency type r, i.e., i r? j, and immediately
pushes j onto the stack.
3. REDUCE pops the stack.
4. SHIFT pushes the next token i onto the stack.
The choice between different transitions is nonde-
terministic in the general case and is resolved by a
classifier induced from a treebank, using features
extracted from the parser configuration.
2.3 Feature Models
The task of the classifier is to predict the next
transition given the current parser configuration,
where the configuration is represented by a fea-
ture vector ?(1,p) = (?1, . . . , ?p). Each feature ?i
is a function of the current configuration, defined
in terms of an address function a?i , which identi-
fies a specific token in the current parser configu-
ration, and an attribute function f?i , which picks
out a specific attribute of the token.
Definition 4 Let c = (?, ?, h, d) be the current
parser configuration.
1. For every i (i ? 0), ?i and ?i are address
functions identifying the ith token of ? and
? , respectively (with indexing starting at 0).
318
2. If ? is an address function, then h(?), l(?),
and r(?) are address functions, identifying
the head (h), the leftmost child (l), and the
rightmost child (r), of the token identified by
? (according to the function h).
3. If ? is an address function, then p(?), w(?)
and d(?) are feature functions, identifying
the part-of-speech (p), word form (w) and de-
pendency type (d) of the token identified by
?. We call p, w and d attribute functions.
A feature model is defined by specifying a vector
of feature functions. In section 4.2 we will define
the feature models used in the experiments.
3 Learning Algorithms
The learning problem for inductive dependency
parsing, defined in the preceding section, is a pure
classification problem, where the input instances
are parser configurations, represented by feature
vectors, and the output classes are parser transi-
tions. In this section, we introduce the two ma-
chine learning methods used to solve this problem
in the experiments.
3.1 MBL
MBL is a lazy learning method, based on the idea
that learning is the simple storage of experiences
in memory and that solving a new problem is
achieved by reusing solutions from similar previ-
ously solved problems (Daelemans and Van den
Bosch, 2005). In essence, this is a k nearest neigh-
bor approach to classification, although a vari-
ety of sophisticated techniques, including different
distance metrics and feature weighting schemes
can be used to improve classification accuracy.
For the experiments reported in this paper we
use the TIMBL software package for memory-
based learning and classification (Daelemans and
Van den Bosch, 2005), which directly handles
multi-valued symbolic features. Based on results
from previous optimization experiments (Nivre et
al., 2004), we use the modified value difference
metric (MVDM) to determine distances between
instances, and distance-weighted class voting for
determining the class of a new instance. The para-
meters varied during experiments are the number
k of nearest neighbors and the frequency threshold
l below which MVDM is replaced by the simple
Overlap metric.
3.2 SVM
SVM in its simplest form is a binary classifier
that tries to separate positive and negative cases in
training data by a hyperplane using a linear kernel
function. The goal is to find the hyperplane that
separates the training data into two classes with
the largest margin. By using other kernel func-
tions, such as polynomial or radial basis function
(RBF), feature vectors are mapped into a higher
dimensional space (Vapnik, 1998; Kudo and Mat-
sumoto, 2001). Multi-class classification with
n classes can be handled by the one-versus-all
method, with n classifiers that each separate one
class from the rest, or the one-versus-one method,
with n(n ? 1)/2 classifiers, one for each pair of
classes (Vural and Dy, 2004). SVM requires all
features to be numerical, which means that sym-
bolic features have to be converted, normally by
introducing one binary feature for each value of
the symbolic feature.
For the experiments reported in this paper
we use the LIBSVM library (Wu et al, 2004;
Chang and Lin, 2005) with the polynomial kernel
K(xi, xj) = (?xTi xj +r)d, ? > 0, where d, ? and
r are kernel parameters. Other parameters that are
varied in experiments are the penalty parameter C,
which defines the tradeoff between training error
and the magnitude of the margin, and the termina-
tion criterion ?, which determines the tolerance of
training errors.
We adopt the standard method for converting
symbolic features to numerical features by bina-
rization, and we use the one-versus-one strategy
for multi-class classification. However, to reduce
training times, we divide the training data into
smaller sets, according to the part-of-speech of
the next token in the current parser configuration,
and train one set of classifiers for each smaller
set. Similar techniques have previously been used
by Yamada and Matsumoto (2003), among others,
without significant loss of accuracy. In order to
avoid too small training sets, we pool together all
parts-of-speech that have a frequency below a cer-
tain threshold t (set to 1000 in all the experiments).
4 Experimental Setup
In this section, we describe the experimental setup,
including data sets, feature models, parameter op-
timization, and evaluation metrics. Experimental
results are presented in section 5.
319
4.1 Data Sets
The data set used for Swedish comes from Tal-
banken (Einarsson, 1976), which contains both
written and spoken Swedish. In the experiments,
the professional prose section is used, consisting
of about 100k words taken from newspapers, text-
books and information brochures. The data has
been manually annotated with a combination of
constituent structure, dependency structure, and
topological fields (Teleman, 1974). This annota-
tion has been converted to dependency graphs and
the original fine-grained classification of gram-
matical functions has been reduced to 17 depen-
dency types. We use a pseudo-randomized data
split, dividing the data into 10 sections by allocat-
ing sentence i to section i mod 10. Sections 1?9
are used for 9-fold cross-validation during devel-
opment and section 0 for final evaluation.
The English data are from the Wall Street Jour-
nal section of the Penn Treebank II (Marcus et al,
1994). We use sections 2?21 for training, sec-
tion 0 for development, and section 23 for the
final evaluation. The head percolation table of
Yamada and Matsumoto (2003) has been used
to convert constituent structures to dependency
graphs, and a variation of the scheme employed
by Collins (1999) has been used to construct arc
labels that can be mapped to a set of 12 depen-
dency types.
The Chinese data are taken from the Penn Chi-
nese Treebank (CTB) version 5.1 (Xue et al,
2005), consisting of about 500k words mostly
from Xinhua newswire, Sinorama news magazine
and Hong Kong News. CTB is annotated with
a combination of constituent structure and gram-
matical functions in the Penn Treebank style, and
has been converted to dependency graphs using es-
sentially the same method as for the English data,
although with a different head percolation table
and mapping scheme. We use the same kind of
pseudo-randomized data split as for Swedish, but
we use section 9 as the development test set (train-
ing on section 1?8) and section 0 as the final test
set (training on section 1?9).
A standard HMM part-of-speech tagger with
suffix smoothing has been used to tag the test data
with an accuracy of 96.5% for English and 95.1%
for Swedish. For the Chinese experiments we have
used the original (gold standard) tags from the
treebank, to facilitate comparison with results pre-
viously reported in the literature.
Feature ?1 ?2 ?3 ?4 ?5
p(?0) + + + + +
p(?0) + + + + +
p(?1) + + + + +
p(?2) + +
p(?3) + +
p(?1) +
d(?0) + + + +
d(l(?0)) + + + +
d(r(?0)) + + + +
d(l(?0)) + + + +
w(?0) + + +
w(?0) + + +
w(?1) +
w(h(?0)) +
Table 1: Feature models
4.2 Feature Models
Table 1 describes the five feature models ?1??5
used in the experiments, with features specified
in column 1 using the functional notation defined
in section 2.3. Thus, p(?0) refers to the part-of-
speech of the top token, while d(l(?0)) picks out
the dependency type of the leftmost child of the
next token. It is worth noting that models ?1??2
are unlexicalized, since they do not contain any
features of the form w(?), while models ?3??5
are all lexicalized to different degrees.
4.3 Optimization
As already noted, optimization of learning algo-
rithm parameters is a prerequisite for meaningful
comparison of different algorithms, although an
exhaustive search of the parameter space is usu-
ally impossible in practice.
For MBL we have used the modified value
difference metric (MVDM) and class voting
weighted by inverse distance (ID) in all experi-
ments, and performed a grid search for the op-
timal values of the number k of nearest neigh-
bors and the frequency threshold l for switching
from MVDM to the simple Overlap metric (cf.
section 3.1). The best values are different for dif-
ferent combinations of data sets and models but
are generally found in the range 3?10 for k and in
the range 1?8 for l.
The polynomial kernel of degree 2 has been
used for all the SVM experiments, but the kernel
parameters ? and r have been optimized together
with the penalty parameter C and the termination
320
Swedish English Chinese
FM LM AS EM AS EM AS EM
U L U L U L U L U L U L
?1 MBL 75.3 68.7 16.0 11.4 *76.5 73.7 9.8 7.7 66.4 63.6 14.3 12.1
SVM 75.4 68.9 16.3 12.1 76.4 73.6 9.8 7.7 66.4 63.6 14.2 12.1
?2 MBL 81.9 74.4 31.4 19.8 81.2 78.2 19.8 14.9 73.0 70.7 22.6 18.8
SVM *83.1 *76.3 *34.3 *24.0 81.3 78.3 19.4 14.9 *73.2 *71.0 22.1 18.6
?3 MBL 85.9 81.4 37.9 28.9 85.5 83.7 26.5 23.7 77.9 76.3 26.3 23.4
SVM 86.2 *82.6 38.7 *32.5 *86.4 *84.8 *28.5 *25.9 *79.7 *78.3 *30.1 *25.9
?4 MBL 86.1 82.1 37.6 30.1 87.0 85.2 29.8 26.0 79.4 77.7 28.0 24.7
SVM 86.0 82.2 37.9 31.2 *88.4 *86.8 *33.2 *30.3 *81.7 *80.1 *31.0 *27.0
?5 MBL 86.6 82.3 39.9 29.9 88.0 86.2 32.8 28.4 81.1 79.2 30.2 25.9
SVM 86.9 *83.2 40.7 *33.7 *89.4 *87.9 *36.4 *33.1 *84.3 *82.7 *34.5 *30.5
Table 2: Parsing accuracy; FM: feature model; LM: learning method; AS: attachment score, EM: exact
match; U: unlabeled, L: labeled
criterion e. The intervals for the parameters are:
?: 0.16?0.40; r: 0?0.6; C: 0.5?1.0; e: 0.1?1.0.
4.4 Evaluation Metrics
The evaluation metrics used for parsing accuracy
are the unlabeled attachment score ASU , which is
the proportion of tokens that are assigned the cor-
rect head (regardless of dependency type), and the
labeled attachment score ASL, which is the pro-
portion of tokens that are assigned the correct head
and the correct dependency type. We also consider
the unlabeled exact match EMU , which is the pro-
portion of sentences that are assigned a completely
correct dependency graph without considering de-
pendency type labels, and the labeled exact match
EML, which also takes dependency type labels
into account. Attachment scores are presented as
mean scores per token, and punctuation tokens are
excluded from all counts. For all experiments we
have performed a McNemar test of significance at
? = 0.01 for differences between the two learning
methods. We also compare learning and parsing
times, as measured on an AMD 64-bit processor
running Linux.
5 Results and Discussion
Table 2 shows the parsing accuracy for the com-
bination of three languages (Swedish, English and
Chinese), two learning methods (MBL and SVM)
and five feature models (?1??5), with algorithm
parameters optimized as described in section 4.3.
For each combination, we measure the attachment
score (AS) and the exact match (EM). A signif-
icant improvement for one learning method over
the other is marked by an asterisk (*).
Independently of language and learning
method, the most complex feature model ?5
gives the highest accuracy across all metrics. Not
surprisingly, the lowest accuracy is obtained with
the simplest feature model ?1. By and large, more
complex feature models give higher accuracy,
with one exception for Swedish and the feature
models ?3 and ?4. It is significant in this context
that the Swedish data set is the smallest of the
three (about 20% of the Chinese data set and
about 10% of the English one).
If we compare MBL and SVM, we see that
SVM outperforms MBL for the three most com-
plex models ?3, ?4 and ?5, both for English and
Chinese. The results for Swedish are less clear,
although the labeled accuracy for ?3 and ?5 are
significantly better. For the ?1 model there is no
significant improvement using SVM. In fact, the
small differences found in the ASU scores are to
the advantage of MBL. By contrast, there is a large
gap between MBL and SVM for the model ?5 and
the languages Chinese and English. For Swedish,
the differences are much smaller (except for the
EML score), which may be due to the smaller size
of the Swedish data set in combination with the
technique of dividing the training data for SVM
(cf. section 3.2).
Another important factor when comparing two
learning methods is the efficiency in terms of time.
Table 3 reports learning and parsing time for the
three languages and the five feature models. The
learning time correlates very well with the com-
plexity of the feature model and MBL, being a lazy
learning method, is much faster than SVM. For the
unlexicalized feature models ?1 and ?2, the pars-
ing time is also considerably lower for MBL, espe-
cially for the large data sets (English and Chinese).
But as model complexity grows, especially with
the addition of lexical features, SVM gradually
gains an advantage over MBL with respect to pars-
ing time. This is especially striking for Swedish,
321
Method Model Swedish English Chinese
LT PT LT PT LT PT
?1 MBL 1 s 2 s 16 s 26 s 7 s 8 s
SVM 40 s 14 s 1.5 h 14 min 1.5 h 17 min
?2 MBL 3 s 5 s 35 s 32 s 13 s 14 s
SVM 40 s 13 s 1 h 11 min 1.5 h 15 min
?3 MBL 6 s 1 min 1.5 min 9.5 min 46 s 10 min
SVM 1 min 15 s 1 h 9 min 2 h 16 min
?4 MBL 8 s 2 min 1.5 min 9 min 45 s 12 min
SVM 2 min 18 s 2 h 12 min 2.5 h 14 min
?5 MBL 10 s 7 min 3 min 41 min 1.5 min 46 min
SVM 2 min 25 s 1.5 h 10 min 6 h 24 min
Table 3: Time efficiency; LT: learning time, PT: parsing time
where the training data set is considerably smaller
than for the other languages.
Compared to the state of the art in dependency
parsing, the unlabeled attachment scores obtained
for Swedish with model ?5, for both MBL and
SVM, are about 1 percentage point higher than the
results reported for MBL by Nivre et al (2004).
For the English data, the result for SVM with
model ?5 is about 3 percentage points below the
results obtained with the parser of Charniak (2000)
and reported by Yamada and Matsumoto (2003).
For Chinese, finally, the accuracy for SVM with
model ?5 is about one percentage point lower than
the best reported results, achieved with a deter-
ministic classifier-based approach using SVM and
preprocessing to detect root nodes (Cheng et al,
2005a), although these results are not based on
exactly the same dependency conversion and data
split as ours.
6 Conclusion
We have performed an empirical comparison of
MBL (TIMBL) and SVM (LIBSVM) as learning
methods for classifier-based deterministic depen-
dency parsing, using data from three languages
and feature models of varying complexity. The
evaluation shows that SVM gives higher parsing
accuracy and comparable or better parsing effi-
ciency for complex, lexicalized feature models
across all languages, whereas MBL is superior
with respect to training efficiency, even if training
data is divided into smaller sets for SVM. The best
accuracy obtained for SVM is close to the state of
the art for all languages involved.
Acknowledgements
The work presented in this paper was partially sup-
ported by the Swedish Research Council. We are
grateful to Hiroyasu Yamada and Yuan Ding for
sharing their head percolation tables for English
and Chinese, respectively, and to three anonymous
reviewers for helpful comments and suggestions.
References
Ezra Black, Frederick Jelinek, John D. Lafferty,
David M. Magerman, Robert L. Mercer, and Salim
Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In
Proceedings of the 5th DARPA Speech and Natural
Language Workshop, pages 31?37.
Chih-Chung Chang and Chih-Jen Lin. 2005. LIB-
SVM: A library for support vector machines.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 173?180.
Eugene Charniak. 2000. A Maximum-Entropy-
Inspired Parser. In Proceedings of the First Annual
Meeting of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL),
pages 132?139.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005a. Chinese deterministic dependency
analyzer: Examining effects of global features and
root node finder. In Proceedings of the Fourth
SIGHAN Workshop on Chinese Language Process-
ing, pages 17?24.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2005b. Machine learning-based depen-
dency analyzer for Chinese. In Proceedings of
the International Conference on Chinese Computing
(ICCC).
Michael Collins and Nigel Duffy. 2005. Discrimina-
tive reranking for natural language parsing. Compu-
tational Linguistics, 31:25?70.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 16?23.
322
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Walter Daelemans and Veronique Hoste. 2002. Eval-
uation of machine learning methods for natural lan-
guage processing tasks. In Proceedings of the Third
International Conference on Language Resources
and Evaluation (LREC), pages 755?760.
Walter Daelemans and Antal Van den Bosch. 2005.
Memory-Based Language Processing. Cambridge
University Press.
Walter Daelemans, Veronique Hoste, Fien De Meulder,
and Bart Naudts. 2003. Combined optimization of
feature selection and algorithm parameter interac-
tion in machine learning of language. In Proceed-
ings of the 14th European Conference on Machine
Learning (ECML), pages 84?95.
Jan Einarsson. 1976. Talbankens skrift-
spra?kskonkordans. Lund University, Department of
Scandinavian Languages.
Mark Johnson, Stuart Geman, Steven Canon, Zhiyi
Chi, and Stefan Riezler. 1999. Estimators for
stochastic ?unification-based? grammars. In Pro-
ceedings of the 37th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
535?541.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with support vector machines. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
Taku Kudo and Yuji Matsumoto. 2002. Japanese de-
pendency analysis using cascaded chunking. In Pro-
ceedings of the Sixth Workshop on Computational
Language Learning (CoNLL), pages 63?69.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 276?283.
Mitchell P. Marcus, Beatrice Santorini, Mary Ann
Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark
Ferguson, Karen Katz, and Britta Schasberger.
1994. The Penn Treebank: Annotating predicate-
argument structure. In Proceedings of the ARPA Hu-
man Language Technology Workshop, pages 114?
119.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), pages 91?98.
Joakim Nivre and Mario Scholz. 2004. Deterministic
dependency parsing of English text. In Proceedings
of the 20th International Conference on Computa-
tional Linguistics (COLING), pages 64?70.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based dependency parsing. In Proceed-
ings of the 8th Conference on Computational Nat-
ural Language Learning (CoNLL), pages 49?56.
Joakim Nivre. 2003. An efficient algorithm for pro-
jective dependency parsing. In Proceedings of the
8th International Workshop on Parsing Technologies
(IWPT), pages 149?160.
Adwait Ratnaparkhi. 1997. A linear observed time
statistical parser based on maximum entropy mod-
els. In Proceedings of the Second Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 1?10.
Kenji Sagae and Alon Lavie. 2005. A classifier-based
parser with linear run-time complexity. In Proceed-
ings of the 9th International Workshop on Parsing
Technologies (IWPT), pages 125?132.
Ben Taskar, Dan Klein, Michael Collins, Daphne
Koller, and Christopher Manning. 2004. Max-
margin parsing. In Proceedings of the Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 1?8.
Ulf Teleman. 1974. Manual fo?r grammatisk beskriv-
ning av talad och skriven svenska. Studentlitteratur.
Vladimir Vapnik. 1995. The Nature of Statistical
Learning Theory. Springer.
Vladimir Vapnik. 1998. Statistical Learning Theory.
John Wiley and Sons, New York.
Volkan Vural and Jennifer G. Dy. 2004. A hierarchi-
cal method for multi-class support vector machines.
ACM International Conference Proceeding Series,
69:105?113.
Ting-Fan Wu, Chih-Jen Lin, and Ruby C. Weng. 2004.
Probability estimates for multi-class classification
by pairwise coupling. Journal of Machine Learning
Research, 5:975?1005.
Nianwen Xue, Fei Xia, Fu-Dong Chiou, and Martha
Palmer. 2005. The Penn Chinese Treebank: Phrase
structure annotation of a large corpus. Natural Lan-
guage Engineering, 11(2):207?238.
Hiroyasu Yamada and Yuji Matsumoto. 2003. Statis-
tical dependency analysis with support vector ma-
chines. In Proceedings of the 8th International
Workshop on Parsing Technologies (IWPT), pages
195?206.
323
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 968?975,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Generalizing Tree Transformations for Inductive Dependency Parsing
Jens Nilsson? Joakim Nivre??
?Va?xjo? University, School of Mathematics and Systems Engineering, Sweden
?Uppsala University, Dept. of Linguistics and Philology, Sweden
{jni,nivre,jha}@msi.vxu.se
Johan Hall?
Abstract
Previous studies in data-driven dependency
parsing have shown that tree transformations
can improve parsing accuracy for specific
parsers and data sets. We investigate to
what extent this can be generalized across
languages/treebanks and parsers, focusing
on pseudo-projective parsing, as a way of
capturing non-projective dependencies, and
transformations used to facilitate parsing of
coordinate structures and verb groups. The
results indicate that the beneficial effect of
pseudo-projective parsing is independent of
parsing strategy but sensitive to language or
treebank specific properties. By contrast, the
construction specific transformations appear
to be more sensitive to parsing strategy but
have a constant positive effect over several
languages.
1 Introduction
Treebank parsers are trained on syntactically anno-
tated sentences and a major part of their success can
be attributed to extensive manipulations of the train-
ing data as well as the output of the parser, usually
in the form of various tree transformations. This
can be seen in state-of-the-art constituency-based
parsers such as Collins (1999), Charniak (2000), and
Petrov et al (2006), and the effects of different trans-
formations have been studied by Johnson (1998),
Klein andManning (2003), and Bikel (2004). Corre-
sponding manipulations in the form of tree transfor-
mations for dependency-based parsers have recently
gained more interest (Nivre and Nilsson, 2005; Hall
and Nova?k, 2005; McDonald and Pereira, 2006;
Nilsson et al, 2006) but are still less studied, partly
because constituency-based parsing has dominated
the field for a long time, and partly because depen-
dency structures have less structure to manipulate
than constituent structures.
Most of the studies in this tradition focus on a par-
ticular parsing model and a particular data set, which
means that it is difficult to say whether the effect
of a given transformation is dependent on a partic-
ular parsing strategy or on properties of a particu-
lar language or treebank, or both. The aim of this
study is to further investigate some tree transforma-
tion techniques previously proposed for data-driven
dependency parsing, with the specific aim of trying
to generalize results across languages/treebanks and
parsers. More precisely, we want to establish, first
of all, whether the transformation as such makes
specific assumptions about the language, treebank
or parser and, secondly, whether the improved pars-
ing accuracy that is due to a given transformation is
constant across different languages, treebanks, and
parsers.
The three types of syntactic phenomena that will
be studied here are non-projectivity, coordination
and verb groups, which in different ways pose prob-
lems for dependency parsers. We will focus on tree
transformations that combine preprocessing with
post-processing, and where the parser is treated as
a black box, such as the pseudo-projective parsing
technique proposed by Nivre and Nilsson (2005)
and the tree transformations investigated in Nils-
son et al (2006). To study the influence of lan-
968
guage and treebank specific properties we will use
data from Arabic, Czech, Dutch, and Slovene, taken
from the CoNLL-X shared task on multilingual de-
pendency parsing (Buchholz and Marsi, 2006). To
study the influence of parsing methodology, we will
compare two different parsers: MaltParser (Nivre et
al., 2004) and MSTParser (McDonald et al, 2005).
Note that, while it is possible in principle to distin-
guish between syntactic properties of a language as
such and properties of a particular syntactic annota-
tion of the language in question, it will be impossi-
ble to tease these apart in the experiments reported
here, since this would require having not only mul-
tiple languages but also multiple treebanks for each
language. In the following, we will therefore speak
about the properties of treebanks (rather than lan-
guages), but it should be understood that these prop-
erties in general depend both on properties of the
language and of the particular syntactic annotation
adopted in the treebank.
The rest of the paper is structured as follows. Sec-
tion 2 surveys tree transformations used in depen-
dency parsing and discusses dependencies between
transformations, on the one hand, and treebanks and
parsers, on the other. Section 3 introduces the four
treebanks used in this study, and section 4 briefly
describes the two parsers. Experimental results are
presented in section 5 and conclusions in section 6.
2 Background
2.1 Non-projectivity
The tree transformations that have attracted most in-
terest in the literature on dependency parsing are
those concerned with recovering non-projectivity.
The definition of non-projectivity can be found in
Kahane et al (1998). Informally, an arc is projec-
tive if all tokens it covers are descendants of the arc?s
head token, and a dependency tree is projective if all
its arcs are projective.1
The full potential of dependency parsing can only
be realized if non-projectivity is allowed, which
pose a problem for projective dependency parsers.
Direct non-projective parsing can be performed with
good accuracy, e.g., using the Chu-Liu-Edmonds al-
1If dependency arcs are drawn above the linearly ordered
sequence of tokens, preceded by a special root node, then a non-
projective dependency tree always has crossing arcs.
gorithm, as proposed byMcDonald et al (2005). On
the other hand, non-projective parsers tend, among
other things, to be slower. In order to maintain the
benefits of projective parsing, tree transformations
techniques to recover non-projectivity while using a
projective parser have been proposed in several stud-
ies, some described below.
In discussing the recovery of empty categories in
data-driven constituency parsing, Campbell (2004)
distinguishes between approaches based on pure
post-processing and approaches based on a combi-
nation of preprocessing and post-processing. The
same division can be made for the recovery of non-
projective dependencies in data-driven dependency
parsing.
Pure Post-processing
Hall and Nova?k (2005) propose a corrective model-
ing approach. The motivation is that the parsers of
Collins et al (1999) and Charniak (2000) adapted
to Czech are not able to create the non-projective
arcs present in the treebank, which is unsatisfac-
tory. They therefore aim to correct erroneous arcs in
the parser?s output (specifically all those arcs which
should be non-projective) by training a classifier that
predicts the most probable head of a token in the
neighborhood of the head assigned by the parser.
Another example is the second-order approximate
spanning tree parser developed by McDonald and
Pereira (2006). It starts by producing the highest
scoring projective dependency tree using Eisner?s al-
gorithm. In the second phase, tree transformations
are performed, replacing lower scoring projective
arcs with higher scoring non-projective ones.
Preprocessing with Post-processing
The training data can also be preprocessed to facili-
tate the recovery of non-projective arcs in the output
of a projective parser. The pseudo-projective trans-
formation proposed by Nivre and Nilsson (2005) is
such an approach, which is compatible with differ-
ent parser engines.
First, the training data is projectivized by making
non-projective arcs projective using a lifting oper-
ation. This is combined with an augmentation of
the dependency labels of projectivized arcs (and/or
surrounding arcs) with information that probably re-
veals their correct non-projective positions. The out-
969
(PS)
C1
 
?
S1
?
C2
 
?
(MS)
C1
?
S1
 
?
C2
 
?
(CS)
C1
?
S1
 
?
C2
 
?
Figure 1: Dependency structure for coordination
put of the parser, trained on the projectivized data,
is then deprojectivized by a heuristic search using
the added information in the dependency labels. The
only assumption made about the parser is therefore
that it can learn to derive labeled dependency struc-
tures with augmented dependency labels.
2.2 Coordination and Verb Groups
The second type of transformation concerns linguis-
tic phenomena that are not impossible for a projec-
tive parser to process but which may be difficult to
learn, given a certain choice of dependency analy-
sis. This study is concerned with two such phe-
nomena, coordination and verb groups, for which
tree transformations have been shown to improve
parsing accuracy for MaltParser on Czech (Nils-
son et al, 2006). The general conclusion of this
study is that coordination and verb groups in the
Prague Dependency Treebank (PDT), based on the-
ories of the Prague school (PS), are annotated in a
way that is difficult for the parser to learn. By trans-
forming coordination and verb groups in the train-
ing data to an annotation similar to that advocated
by Mel?c?uk (1988) and then performing an inverse
transformation on the parser output, parsing accu-
racy can therefore be improved. This is again an
instance of the black-box idea.
Schematically, coordination is annotated in the
Prague school as depicted in PS in figure 1, where
the conjuncts are dependents of the conjunction. In
Mel?c?uk style (MS), on the other hand, conjuncts
and conjunction(s) form a chain going from left to
right. A third way of treating coordination, not dis-
cussed by Nilsson et al (2006), is used by the parser
of Collins (1999), which internally represents coor-
dination as a direct relation between the conjuncts.
This is illustrated in CS in figure 1, where the con-
junction depends on one of the conjuncts, in this
case on the rightmost one.
Nilsson et al (2006) also show that the annotation
of verb groups is not well-suited for parsing PDT
using MaltParser, and that transforming the depen-
dency structure for verb groups has a positive impact
on parsing accuracy. In PDT, auxiliary verbs are de-
pendents of the main verb, whereas it according to
Mel?c?uk is the (finite) auxiliary verb that is the head
of the main verb. Again, the parsing experiments in
this study show that verb groups are more difficult
to parse in PS than in MS.
2.3 Transformations, Parsers, and Treebanks
Pseudo-projective parsing and transformations for
coordination and verb groups are instances of the
same general methodology:
1. Apply a tree transformation to the training data.
2. Train a parser on the transformed data.
3. Parse new sentences.
4. Apply an inverse transformation to the output
of the parser.
In this scheme, the parser is treated as a black
box. All that is assumed is that it is a data-driven
parser designed for (projective) labeled dependency
structures. In this sense, the tree transformations
are independent of parsing methodology. Whether
the beneficial effect of a transformation, if any, is
also independent of parsing methodology is another
question, which will be addressed in the experimen-
tal part of this paper.
The pseudo-projective transformation is indepen-
dent not only of parsing methodology but also of
treebank (and language) specific properties, as long
as the target representation is a (potentially non-
projective) labeled dependency structure. By con-
trast, the coordination and verb group transforma-
tions presuppose not only that the language in ques-
tion contains these constructions but also that the
treebank adopts a PS annotation. In this sense, they
are more limited in their applicability than pseudo-
projective parsing. Again, it is a different question
whether the transformations have a positive effect
for all treebanks (languages) to which they can be
applied.
3 Treebanks
The experiments are mostly conducted using tree-
bank data from the CoNLL shared task 2006. This
970
Slovene Arabic Dutch Czech
SDT PADT Alpino PDT
# T 29 54 195 1249
# S 1.5 1.5 13.3 72.7
%-NPS 22.2 11.2 36.4 23.2
%-NPA 1.8 0.4 5.4 1.9
%-C 9.3 8.5 4.0 8.5
%-A 8.8 - - 1.3
Table 1: Overview of the data sets (ordered by size),
where # S * 1000 = number of sentences, # T * 1000
= number of tokens, %-NPS = percentage of non-
projective sentences, %-NPA = percentage of non-
projective arcs, %-C = percentage of conjuncts, %-A
= percentage of auxiliary verbs.
subsection summarizes some of the important char-
acteristics of these data sets, with an overview in ta-
ble 1. Any details concerning the conversion from
the original formats of the various treebanks to the
CoNLL format, a pure dependency based format, are
found in documentation referred to in Buchholz and
Marsi (2006).
PDT (Hajic? et al, 2001) is the largest manually
annotated treebank, and as already mentioned, it
adopts PS for coordination and verb groups. As
the last four rows reveal, PDT contains a quite high
proportion of non-projectivity, since almost every
fourth dependency graph contains at least one non-
projective arc. The table also shows that coordina-
tion is more common than verb groups in PDT. Only
1.3% of the tokens in the training data are identified
as auxiliary verbs, whereas 8.5% of the tokens are
identified as conjuncts.
Both Slovene Dependency Treebank (Dz?eroski et
al., 2006) (SDT) and Prague Arabic Dependency
Treebank (Hajic? et al, 2004) (PADT) annotate co-
ordination and verb groups as in PDT, since they too
are influenced by the theories of the Prague school.
The proportions of non-projectivity and conjuncts in
SDT are in fact quite similar to the proportions in
PDT. The big difference is the proportion of auxil-
iary verbs, with many more auxiliary verbs in SDT
than in PDT. It is therefore plausible that the trans-
formations for verb groups will have a larger impact
on parser accuracy in SDT.
Arabic is not a Slavic languages such as Czech
and Slovene, and the annotation in PADT is there-
fore more dissimilar to PDT than SDT is. One such
example is that Arabic does not have auxiliary verbs.
Table 1 thus does not give figures verb groups. The
amount of coordination is on the other hand compa-
rable to both PDT and SDT. The table also reveals
that the amount of non-projective arcs is about 25%
of that in PDT and SDT, although the amount of
non-projective sentences is still as large as 50% of
that in PDT and SDT.
Alpino (van der Beek et al, 2002) in the CoNLL
format, the second largest treebank in this study,
is not as closely tied to the theories of the Prague
school as the others, but still treats coordination in
a way similar to PS. The table shows that coor-
dination is less frequent in the CoNLL version of
Alpino than in the three other treebanks. The other
characteristic of Alpino is the high share of non-
projectivity, where more than every third sentence
is non-projective. Finally, the lack of information
about the share of auxiliary verbs is not due to the
non-existence of such verbs in Dutch but to the fact
that Alpino adopts an MS annotation of verb groups
(i.e., treating main verbs as dependents of auxiliary
verbs), which means that the verb group transforma-
tion of Nilsson et al (2006) is not applicable.
4 Parsers
The parsers used in the experiments are Malt-
Parser (Nivre et al, 2004) and MSTParser (Mc-
Donald et al, 2005). These parsers are based on
very different parsing strategies, which makes them
suitable in order to test the parser independence
of different transformations. MaltParser adopts a
greedy, deterministic parsing strategy, deriving a la-
beled dependency structure in a single left-to-right
pass over the input and uses support vector ma-
chines to predict the next parsing action. MST-
Parser instead extracts a maximum spanning tree
from a dense weighted graph containing all possi-
ble dependency arcs between tokens (with Eisner?s
algorithm for projective dependency structures or
the Chu-Liu-Edmonds algorithm for non-projective
structures), using a global discriminative model and
online learning to assign weights to individual arcs.2
2The experiments in this paper are based on the first-order
factorization described in McDonald et al (2005)
971
5 Experiments
The experiments reported in section 5.1?5.2 below
are based on the training sets from the CoNLL-X
shared task, except where noted. The results re-
ported are obtained by a ten-fold cross-validation
(with a pseudo-randomized split) for all treebanks
except PDT, where 80% of the data was used for
training and 20% for development testing (again
with a pseudo-randomized split). In section 5.3, we
give results for the final evaluation on the CoNLL-
X test sets using all three transformations together
with MaltParser.
Parsing accuracy is primarily measured by the un-
labeled attachment score (ASU ), i.e., the propor-
tion of tokens that are assigned the correct head, as
computed by the official CoNLL-X evaluation script
with default settings (thus excluding all punctuation
tokens). In section 5.3 we also include the labeled
attachment score (ASL) (where a token must have
both the correct head and the correct dependency la-
bel to be counted as correct), which was the official
evaluation metric in the CoNLL-X shared task.
5.1 Comparing Treebanks
We start by examining the effect of transformations
on data from different treebanks (languages), using
a single parser: MaltParser.
Non-projectivity
The question in focus here is whether the effect of
the pseudo-projective transformation for MaltParser
varies with the treebank. Table 2 presents the un-
labeled attachment score results (ASU ), compar-
ing the pseudo-projective parsing technique (P-Proj)
with two baselines, obtained by training the strictly
projective parser on the original (non-projective)
training data (N-Proj) and on projectivized train-
ing data with no augmentation of dependency labels
(Proj).
The first thing to note is that pseudo-projective
parsing gives a significant improvement for PDT,
as previously reported by Nivre and Nilsson (2005),
but also for Alpino, where the improvement is even
larger, presumably because of the higher proportion
of non-projective dependencies in the Dutch tree-
bank. By contrast, there is no significant improve-
ment for either SDT or PADT, and even a small drop
N-Proj Proj P-Proj
SDT 77.27 76.63?? 77.11
PADT 76.96 77.07? 77.07?
Alpino 82.75 83.28?? 87.08??
PDT 83.41 83.32?? 84.42??
Table 2: ASU for pseudo-projective parsing with
MaltParser. McNemar?s test: ? = p < .05 and
?? = p < 0.01 compared to N-Proj.
1 2 3 >3
SDT 88.4 9.1 1.7 0.84
PADT 66.5 14.4 5.2 13.9
Alpino 84.6 13.8 1.5 0.07
PDT 93.8 5.6 0.5 0.1
Table 3: The number of lifts for non-projective arcs.
in the accuracy figures for SDT. Finally, in contrast
to the results reported by Nivre and Nilsson (2005),
simply projectivizing the training data (without us-
ing an inverse transformation) is not beneficial at all,
except possibly for Alpino.
But why does not pseudo-projective parsing im-
prove accuracy for SDT and PADT? One possi-
ble factor is the complexity of the non-projective
constructions, which can be measured by counting
the number of lifts that are required to make non-
projective arcs projective. The more deeply nested
a non-projective arc is, the more difficult it is to re-
cover because of parsing errors as well as search er-
rors in the inverse transformation. The figures in ta-
ble 3 shed some interesting light on this factor.
For example, whereas 93.8% of all arcs in PDT
require only one lift before they become projec-
tive (88.4% and 84.6% for SDT and Alpino, respec-
tively), the corresponding figure for PADT is as low
as 66.5%. PADT also has a high proportion of very
deeply nested non-projective arcs (>3) in compari-
son to the other treebanks, making the inverse trans-
formation for PADT more problematic than for the
other treebanks. The absence of a positive effect for
PADT is therefore understandable given the deeply
nested non-projective constructions in PADT.
However, one question that still remains is why
SDT and PDT, which are so similar in terms of both
nesting depth and amount of non-projectivity, be-
972
Figure 2: Learning curves for Alpino measured as
error reduction for ASU .
have differently with respect to pseudo-projective
parsing. Another factor that may be important here
is the amount of training data available. As shown
in table 1, PDT is more than 40 times larger than
SDT. To investigate the influence of training set
size, a learning curve experiment has been per-
formed. Alpino is a suitable data set for this due
to its relatively large amount of both data and non-
projectivity.
Figure 2 shows the learning curve for pseudo-
projective parsing (P-Proj), compared to using only
projectivized training data (Proj), measured as error
reduction in relation to the original non-projective
training data (N-Proj). The experiment was per-
formed by incrementally adding cross-validation
folds 1?8 to the training set, using folds 9?0 as static
test data.
One can note that the error reduction for Proj is
unaffected by the amount of data. While the error
reduction varies slightly, it turns out that the error
reduction is virtually the same for 10% of the train-
ing data as for 80%. That is, there is no correla-
tion if information concerning the lifts are not added
to the labels. However, with a pseudo-projective
transformation, which actively tries to recover non-
projectivity, the learning curve clearly indicates that
the amount of data matters. Alpino, with 36% non-
projective sentences, starts at about 17% and has a
climbing curve up to almost 25%.
Although this experiment shows that there is a
correlation between the amount of data and the accu-
racy for pseudo-projective parsing, it does probably
not tell the whole story. If it did, one would expect
that the error reduction for the pseudo-projective
transformation would be much closer to Proj when
None Coord VG
SDT 77.27 79.33?? 77.92??
PADT 76.96 79.05?? -
Alpino 82.75 83.38?? -
PDT 83.41 85.51?? 83.58??
Table 4: ASU for coordination and verb group trans-
formations with MaltParser (None = N-Proj). Mc-
Nemar?s test: ?? = p < .01 compared to None.
the amount of data is low (to the left in the fig-
ure) than they apparently are. Of course, the dif-
ference is likely to diminish with even less data, but
it should be noted that 10% of Alpino has about half
the size of PADT, for which the positive impact of
pseudo-projective parsing is absent. The absence
of increased accuracy for SDT can partially be ex-
plained by the higher share of non-projective arcs in
Alpino (3 times more).
Coordination and Verb Groups
The corresponding parsing results using MaltParser
with transformations for coordination and verb
groups are shown in table 4. For SDT, PADT and
PDT, the annotation of coordination has been trans-
formed from PS to MS, as described in Nilsson et
al. (2006). For Alpino, the transformation is from
PS to CS (cf. section 2.2), which was found to give
slightly better performance in preliminary experi-
ments. The baseline with no transformation (None)
is the same as N-Proj in table 2.
As the figures indicate, transforming coordination
is beneficial not only for PDT, as reported by Nilsson
et al (2006), but also for SDT, PADT, and Alpino. It
is interesting to note that SDT, PADT and PDT, with
comparable amounts of conjuncts, have compara-
ble increases in accuracy (about 2 percentage points
each), despite the large differences in training set
size. It is therefore not surprising that Alpino, with
a much smaller amount of conjuncts, has a lower in-
crease in accuracy. Taken together, these results in-
dicate that the frequency of the construction is more
important than the size of the training set for this
type of transformation.
The same generalization over treebanks holds for
verb groups too. The last column in table 4 shows
that the expected increase in accuracy for PDT is ac-
973
Algorithm N-Proj Proj P-Proj
Eisner 81.79 83.23 86.45
CLE 86.39
Table 5: Pseudo-projective parsing results (ASU ) for
Alpino with MSTParser.
companied by a even higher increase for SDT. This
can probably be attributed to the higher frequency of
auxiliary verbs in SDT.
5.2 Comparing Parsers
The main question in this section is to what extent
the positive effect of different tree transformations
is dependent on parsing strategy, since all previ-
ous experiments have been performed with a single
parser (MaltParser). For comparison we have per-
formed two experiments with MSTParser, version
0.1, which is based on a very different parsing meth-
dology (cf. section 4). Due to some technical dif-
ficulties (notably the very high memory consump-
tion when using MSTParser for labeled dependency
parsing), we have not been able to replicate the ex-
periments from the preceding section exactly. The
results presented below must therefore be regarded
as a preliminary exploration of the dependencies be-
tween tree transformations and parsing strategy.
Table 5 presents ASU results for MSTParser in
combination with pseudo-projective parsing applied
to the Alpino treebank of Dutch.3 The first row
contains the result for Eisner?s algorithm using no
transformation (N-Proj), projectivized training data
(Proj), and pseudo-projective parsing (P-Proj). The
figures show a pattern very similar to that for Malt-
Parser, with a boost in accuracy for Proj compared
to N-Proj, and with a significantly higher accuracy
for P-Proj over Proj. It is also worth noting that the
error reduction between N-Proj and P-Proj is actu-
ally higher for MSTParser here than for MaltParser
in table 2.
The second row contains the result for the Chu-
Liu-Edmonds algorithm (CLE), which constructs
non-projective structures directly and therefore does
3The figures are not completely comparable to the previ-
ously presented Dutch results for MaltParser, sinceMaltParser?s
feature model has access to all the information in the CoNLL
data format, whereas MSTParser in this experiment only could
handle word forms and part-of-speech tags.
Trans. None Coord VG
ASU 84.5 83.5 84.5
Table 6: Coordination and verb group transforma-
tions for PDT with the CLE algorithm.
Dev Eval Niv McD
SDT ASU 80.40 82.01 78.72 83.17
ASL 71.06 72.44 70.30 73.44
PADT ASU 78.97 78.56 77.52 79.34
ASL 67.63 67.58 66.71 66.91
Alpino ASU 87.63 82.85 81.35 83.57
ASL 84.02 79.73 78.59 79.19
PDT ASU 85.72 85.98 84.80 87.30
ASL 78.56 78.80 78.42 80.18
Table 7: Evaluation on CoNLL-X test data; Malt-
Parser with all transformations (Dev = development,
Eval = CoNLL test set, Niv = Nivre et al (2006),
McD = McDonald et al (2006))
not require the pseudo-projective transformation.
A comparison between Eisner?s algorithm with
pseudo-projective transformation and CLE reveals
that pseudo-projective parsing is at least as accurate
as non-projective parsing for ASU . (The small dif-
ference is not statistically significant.)
By contrast, no positive effect could be detected
for the coordination and verb group transformations
togther with MSTParser. The figures in table 6 are
not based on CoNLL data, but instead on the evalu-
ation test set of the original PDT 1.0, which enables
a direct comparison to McDonald et. al. (2005) (the
None column). We see that there is even a negative
effect for the coordination transformation. These re-
sults clearly indicate that the effect of these transfor-
mations is at least partly dependent on parsing strat-
egy, in contrast to what was found for the pseudo-
projective parsing technique.
5.3 Combining Transformations
In order to assess the combined effect of all three
transformations in relation to the state of the art,
we performed a final evaluation using MaltParser on
the dedicated test sets from the CoNLL-X shared
task. Table 7 gives the results for both develop-
ment (cross-validation for SDT, PADT, and Alpino;
974
development set for PDT) and final test, compared
to the two top performing systems in the shared
task, MSTParser with approximate second-order
non-projective parsing (McDonald et al, 2006) and
MaltParser with pseudo-projective parsing (but no
coordination or verb group transformations) (Nivre
et al, 2006). Looking at the labeled attachment
score (ASL), the official scoring metric of the
CoNLL-X shared task, we see that the combined ef-
fect of the three transformations boosts the perfor-
mance of MaltParser for all treebanks and in two
cases out of four outperforms MSTParser (which
was the top scoring system for all four treebanks).
6 Conclusion
In this paper, we have examined the generality
of tree transformations for data-driven dependency
parsing. The results indicate that the pseudo-
projective parsing technique has a positive effect
on parsing accuracy that is independent of parsing
methodology but sensitive to the amount of training
data as well as to the complexity of non-projective
constructions. By contrast, the construction-specific
transformations targeting coordination and verb
groups appear to have a more language-independent
effect (for languages to which they are applicable)
but do not help for all parsers. More research is
needed in order to know exactly what the dependen-
cies are between parsing strategy and tree transfor-
mations. Regardless of this, however, it is safe to
conclude that pre-processing and post-processing is
important not only in constituency-based parsing, as
previously shown in a number of studies, but also for
inductive dependency parsing.
References
D. Bikel. 2004. Intricacies of Collins? parsing model. Compu-
tational Linguistics, 30:479?511.
S. Buchholz and E. Marsi. 2006. CoNLL-X Shared Task
on Multilingual Dependency Parsing. In Proceedings of
CoNLL, pages 1?17.
R. Campbell. 2004. Using Linguistic Principles to Recover
Empty Categories. In Proceedings of ACL, pages 645?652.
E. Charniak. 2000. A Maximum-Entropy-Inspired Parser. In
Proceedings of NAACL, pages 132?139.
M. Collins, J. Hajic?, L. Ramshaw, and C. Tillmann. 1999. A
statistical parser for Czech. In Proceedings of ACL, pages
100?110.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. Z?abokrtsky, and
A. Z?ele. 2006. Towards a Slovene Dependency Treebank.
In LREC.
J. Hajic?, B. V. Hladka, J. Panevova?, Eva Hajic?ova?, Petr Sgall,
and Petr Pajas. 2001. Prague Dependency Treebank 1.0.
LDC, 2001T10.
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka. 2004.
Prague Arabic Dependency Treebank: Development in Data
and Tools. In NEMLAR, pages 110?117.
K. Hall and V. Nova?k. 2005. Corrective modeling for non-
projective dependency parsing. In Proceedings of IWPT,
pages 42?52.
M. Johnson. 1998. PCFG Models of Linguistic Tree Represen-
tations. Computational Linguistics, 24:613?632.
S. Kahane, A. Nasr, and O. Rambow. 1998. Pseudo-
Projectivity: A Polynomially Parsable Non-Projective De-
pendency Grammar. In Proceedings of COLING/ACL, pages
646?652.
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In Proceedings of ACL, pages 423?430.
R. McDonald and F. Pereira. 2006. Online Learning of Ap-
proximate Dependency Parsing Algorithms. In Proceedings
of EACL, pages 81?88.
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning tree al-
gorithms. In Proceedings of HLT/EMNLP, pages 523?530.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual
dependency analysis with a two-stage discriminative parser.
In Proceedings of CoNLL, pages 216?220.
I. Mel?c?uk. 1988. Dependency Syntax: Theory and Practice.
State University of New York Press.
J. Nilsson, J. Nivre, and J. Hall. 2006. Graph Transforma-
tions in Data-Driven Dependency Parsing. In Proceedings
of COLING/ACL, pages 257?264.
J. Nivre and J. Nilsson. 2005. Pseudo-Projective Dependency
Parsing. In Proceedings of ACL, pages 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based Depen-
dency Parsing. In H. T. Ng and E. Riloff, editors, Proceed-
ings of CoNLL, pages 49?56.
J. Nivre, J. Hall, J. Nilsson, G. Eryig?it, and S. Marinov. 2006.
Labeled Pseudo-Projective Dependency Parsing with Sup-
port Vector Machines. In Proceedings of CoNLL, pages
221?225.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning
Accurate, Compact, and Interpretable Tree Annotation. In
Proceedings of COLING/ACL, pages 433?440.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
975
Memory-Based Dependency Parsing
Joakim Nivre, Johan Hall and Jens Nilsson
School of Mathematics and Systems Engineering
Va?xjo? University
SE-35195 Va?xjo?
Sweden
firstname.lastname@msi.vxu.se
Abstract
This paper reports the results of experiments
using memory-based learning to guide a de-
terministic dependency parser for unrestricted
natural language text. Using data from a small
treebank of Swedish, memory-based classifiers
for predicting the next action of the parser are
constructed. The accuracy of a classifier as
such is evaluated on held-out data derived from
the treebank, and its performance as a parser
guide is evaluated by parsing the held-out por-
tion of the treebank. The evaluation shows that
memory-based learning gives a signficant im-
provement over a previous probabilistic model
based on maximum conditional likelihood esti-
mation and that the inclusion of lexical features
improves the accuracy even further.
1 Introduction
Deterministic dependency parsing has recently been pro-
posed as a robust and efficient method for syntactic pars-
ing of unrestricted natural language text (Yamada and
Matsumoto, 2003; Nivre, 2003). Dependency parsing
means that the goal of the parsing process is to construct
a dependency graph, of the kind depicted in Figure 1. De-
terministic parsing means that we always derive a single
analysis for each input string. Moreover, this single anal-
ysis is derived in a monotonic fashion with no redundancy
or backtracking, which makes it possible to parse natural
language sentences in linear time (Nivre, 2003).
In this paper, we report experiments using memory-
based learning (Daelemans, 1999) to guide the parser de-
scribed in Nivre (2003), using data from a small tree-
bank of Swedish (Einarsson, 1976). Unlike most pre-
vious work on data-driven dependency parsing (Eisner,
1996; Collins et al, 1999; Yamada and Matsumoto, 2003;
Nivre, 2003), we assume that dependency graphs are la-
beled with dependency types, although the evaluation
will give results for both labeled and unlabeled represen-
tations.
The paper is structured as follows. Section 2 gives
the necessary background definitions and introduces the
idea of guided parsing as well as memory-based learning.
Section 3 describes the data used in the experiments, the
evaluation metrics, and the models and algorithms used
in the learning process. Results from the experiments are
given in section 4, while conclusions and suggestions for
further research are presented in section 5.
2 Background
2.1 Dependency Graphs
The linguistic tradition of dependency grammar com-
prises a large and fairly diverse family of theories and for-
malisms that share certain basic assumptions about syn-
tactic structure, in particular the assumption that syntactic
structure consists of lexical nodes linked by binary re-
lations called dependencies (see, e.g., Tesnie`re (1959),
Sgall (1986), Mel?c?uk (1988), Hudson (1990)). Thus,
the common formal property of dependency structures,
as compared to the representations based on constituency
(or phrase structure), is the lack of nonterminal nodes.
In a dependency structure, every word token is depen-
dent on at most one other word token, usually called its
head or regent, which means that the structure can be
represented as a directed graph, with nodes representing
word tokens and arcs representing dependency relations.
In addition, arcs may be labeled with specific dependency
types. Figure 1 shows a labeled dependency graph for a
simple Swedish sentence, where each word of the sen-
tence is labeled with its part of speech and each arc la-
beled with a grammatical function.
Formally, we define dependency graphs in the follow-
ing way:
PP
Pa?
(In
 
?
ADV
NN
60-talet
the-60?s
 
?
PR
VB
ma?lade
painted
PN
han
he
 
?
SUB
JJ
dja?rva
bold
 
?
ATT
NN
tavlor
pictures
 
?
OBJ
HP
som
which
 
?
ATT
VB
retade
annoyed
?
 
SUB
PM
Nikita
Nikita
 
?
OBJ
PM
Chrusjtjov.
Chrustjev.)
 
?
ID
Figure 1: Dependency graph for Swedish sentence
1. Let R = {r1, . . . , rm} be the set of permissible de-
pendency types (arc labels).
2. A dependency graph for a string of words W =
w1? ? ?wn is a labeled directed graph D = (W,A),
where
(a) W is the set of nodes, i.e. word tokens in the
input string,
(b) A is a set of labeled arcs (wi, r, wj) (where
wi, wj ? W and r ? R).
We write wi < wj to express that wi precedes wj
in the string W (i.e., i < j); we write wi r? wj to
say that there is an arc from wi to wj labeled r, and
wi ? wj to say that there is an arc from wi to wj
(regardless of the label); we use ?? to denote the
reflexive and transitive closure of the unlabeled arc
relation; and we use ? and ?? for the correspond-
ing undirected relations, i.e. wi ? wj iff wi ? wj
or wj ? wi.
3. A dependency graph D = (W,A) is well-formed iff
the five conditions given in Figure 2 are satisfied.
For a more detailed discussion of dependency graphs
and well-formedness conditions, the reader is referred to
Nivre (2003).
2.2 Parsing Algorithm
The parsing algorithm presented in Nivre (2003) is in
many ways similar to the basic shift-reduce algorithm for
context-free grammars (Aho et al, 1986), although the
parse actions are different given that no nonterminal sym-
bols are used. Moreover, unlike the algorithm of Yamada
and Matsumoto (2003), the algorithm considered here ac-
tually uses a blend of bottom-up and top-down process-
ing, constructing left-dependencies bottom-up and right-
dependencies top-down, in order to achieve incremental-
ity. For a similar but nondeterministic approach to depen-
dency parsing, see Obrebski (2003).
Parser configurations are represented by triples
?S, I, A?, where S is the stack (represented as a list), I is
the list of (remaining) input tokens, and A is the (current)
arc relation for the dependency graph. Given an input
string W , the parser is initialized to ?nil,W, ?? and termi-
nates when it reaches a configuration ?S,nil, A? (for any
list S and set of arcs A). The input string W is accepted if
the dependency graph D = (W,A) given at termination
is well-formed; otherwise W is rejected. The behavior of
the parser is defined by the transitions defined in Figure
3 (where wi, wj and wk are arbitrary word tokens, and r
and r? are arbitrary dependency relations):
1. The transition Left-Arc (LA) adds an arc wj r?wi
from the next input token wj to the token wi on top
of the stack and reduces (pops) wi from the stack.
2. The transition Right-Arc (RA) adds an arc wi r?wj
from the token wi on top of the stack to the next in-
put token wj , and shifts (pushes) wj onto the stack.
3. The transition Reduce (RE) reduces (pops) the to-
ken wi on top of the stack.
4. The transition Shift (SH) shifts (pushes) the next in-
put token wi onto the stack.
The transitions Left-Arc and Right-Arc are subject to
conditions that ensure that the graph conditions Unique
label and Single head are satisfied. By contrast, the Re-
duce transition can only be applied if the token on top of
the stack already has a head. For Shift, the only condition
is that the input list is non-empty.
As it stands, this transition system is nondeterminis-
tic, since several transitions can often be applied to the
same configuration. Thus, in order to get a deterministic
parser, we need to introduce a mechanism for resolving
transition conflicts. Regardless of which mechanism is
used, the parser is guaranteed to terminate after at most
2n transitions, given an input string of length n (Nivre,
2003). This means that as long as transitions can be per-
formed in constant time, the running time of the parser
will be linear in the length of the input. Moreover, the
parser is guaranteed to produce a dependency graph that
is acyclic and projective (and satisfies the unique-label
and single-head constraints). This means that the depen-
dency graph given at termination is well-formed if and
only if it is connected (Nivre, 2003).
Unique label (wi r?wj ? wi r
?
?wj) ? r = r?
Single head (wi?wj ? wk?wj) ? wi = wk
Acyclic ?(wi?wj ? wj??wi)
Connected wi??wj
Projective (wi?wk ? wi<wj<wk) ? (wi??wj ? wk??wj)
Figure 2: Well-formedness conditions on dependency graphs
Initialization ?nil,W, ??
Termination ?S,nil, A?
Left-Arc ?wi|S,wj |I, A? ? ?S,wj |I, A ? {(wj , r, wi)}? ??wk?r?(wk, r?, wi) ? A
Right-Arc ?wi|S,wj |I, A? ? ?wj |wi|S, I, A ? {(wi, r, wj)}? ??wk?r?(wk, r?, wj) ? A
Reduce ?wi|S, I, A? ? ?S, I, A? ?wj?r(wj , r, wi) ? A
Shift ?S,wi|I, A? ? ?wi|S, I, A?
Figure 3: Parser transitions
2.3 Guided Parsing
One way of turning a nondeterministic parser into a deter-
ministic one is to use a guide (or oracle) that can inform
the parser at each nondeterministic choice point; cf. Kay
(2000), Boullier (2003). Guided parsing is normally used
to improve the efficiency of a nondeterministic parser,
e.g. by letting a simpler (but more efficient) parser con-
struct a first analysis that can be used to guide the choice
of the more complex (but less efficient) parser. This is the
approach taken, for example, in Boullier (2003).
In our case, we rather want to use the guide to im-
prove the accuracy of a deterministic parser, starting from
a baseline of randomized choice. One way of doing this
is to use a treebank, i.e. a corpus of analyzed sentences, to
train a classifier that can predict the next transition (and
dependency type) given the current configuration of the
parser. However, in order to maintain the efficiency of the
parser, the classifier must also be implemented in such a
way that each transition can still be performed in constant
time.
Previous work in this area includes the use of memory-
based learning to guide a standard shift-reduce parser
(Veenstra and Daelemans, 2000) and the use of sup-
port vector machines to guide a deterministic depen-
dency parser (Yamada and Matsumoto, 2003). In the
experiments reported in this paper, we apply memory-
based learning within a deterministic dependency parsing
framework.
2.4 Memory-Based Learning
Memory-based learning and problem solving is based on
two fundamental principles: learning is the simple stor-
age of experiences in memory, and solving a new problem
is achieved by reusing solutions from similar previously
solved problems (Daelemans, 1999). It is inspired by the
nearest neighbor approach in statistical pattern recogni-
tion and artificial intelligence (Fix and Hodges, 1952), as
well as the analogical modeling approach in linguistics
(Skousen, 1989; Skousen, 1992). In machine learning
terms, it can be characterized as a lazy learning method,
since it defers processing of input until needed and pro-
cesses input by combining stored data (Aha, 1997).
Memory-based learning has been successfully applied
to a number of problems in natural language process-
ing, such as grapheme-to-phoneme conversion, part-
of-speech tagging, prepositional-phrase attachment, and
base noun phrase chunking (Daelemans et al, 2002).
Most relevant in the present context is the use of memory-
based learning to predict the actions of a shift-reduce
parser, with promising results reported in Veenstra and
Daelemans (2000).
The main reason for using memory-based learning in
the present context is the flexibility offered by similarity-
based extrapolation when classifying previously unseen
configurations, since previous experiments with a proba-
bilistic model has shown that a fixed back-off sequence
does not work well in this case (Nivre, 2004). Moreover,
the memory-based approach can easily handle multi-class
classification, unlike the support vector machines used by
Yamada and Matsumoto (2003).
For the experiments reported in this paper, we have
used the software package TiMBL (Tilburg Memory
Based Learner), which provides a variety of metrics, al-
gorithms, and extra functions on top of the classical k
nearest neighbor classification kernel, such as value dis-
tance metrics and distance weighted class voting (Daele-
mans et al, 2003).
3 Method
3.1 Target Function and Approximation
The function we want to approximate is a mapping f
from parser configurations to parser actions, where each
action consists of a transition and (unless the transition is
Shift or Reduce) a dependency type:
f : Config ? {LA,RA,RE, SH} ? (R ? {nil})
Here Config is the set of all possible parser configura-
tions and R is the set of dependency types as before.
However, in order to make the problem tractable, we try
to learn a function f? whose domain is a finite space of
parser states, which are abstractions over configurations.
For this purpose we define a number of features that can
be used to define different models of parser state. The
features used in this study are listed in Table 1.
The first five features (TOP?TOP.RIGHT) deal with
properties of the token on top of the stack. In addition to
the word form itself (TOP), we consider its part-of-speech
(as assigned by an automatic part-of-speech tagger in a
preprocessing phase), the dependency type by which it is
related to its head (which may or may not be available in
a given configuration depending on whether the head is
to the left or to the right of the token in question), and
the dependency types by which it is related to its leftmost
and rightmost dependent, respectively (where the current
rightmost dependent may or may not be the rightmost de-
pendent in the complete dependency tree).
The following three features (NEXT?NEXT.LEFT) refer
to properties of the next input token. In this case, there are
no features corresponding to TOP.DEP and TOP.RIGHT,
since the relevant dependencies can never be present at
decision time. The final feature (LOOK) is a simple looka-
head, using the part-of-speech of the next plus one input
token.
In the experiments reported below, we have used
two different parser state models, one called the lexical
model, which includes all nine features, and one called
the non-lexical model, where the two lexical features
TOP and NEXT are omitted. For both these models, we
have used memory-based learning with different parame-
ter settings, as implemented TiMBL.
For comparison, we have included an earlier classifier
that uses the same features as the non-lexical model, but
where prediction is based on maximum conditional likeli-
hood estimation. This classifier always predicts the most
probable transition given the state and the most probable
dependency type given the transition and the state, with
conditional probabilities being estimated by the empiri-
cal distribution in the training data. Smoothing is per-
formed only for zero frequency events, in which case the
classifier backs off to more general models by omitting
first the features TOP.LEFT and LOOK and then the fea-
tures TOP.RIGHT and NEXT.LEFT; if even this does not
help, the classifier predicts Reduce if permissible and
Shift otherwise. This model, which we will refer to as the
MCLE model, is described in more detail in Nivre (2004).
3.2 Data
It is standard practice in data-driven approaches to nat-
ural language parsing to use treebanks both for training
and evaluation. Thus, the Penn Treebank of American
English (Marcus et al, 1993) has been used to train and
evaluate the best available parsers of unrestricted English
text (Collins, 1999; Charniak, 2000). One problem when
developing a parser for Swedish is that there is no com-
parable large-scale treebank available for Swedish.
For the experiments reported in this paper we have
used a manually annotated corpus of written Swedish,
created at Lund University in the 1970?s and consisting
mainly of informative texts from official sources (Einars-
son, 1976). Although the original annotation scheme is
an eclectic combination of constituent structure, depen-
dency structure, and topological fields (Teleman, 1974),
it has proven possible to convert the annotated sentences
to dependency graphs with fairly high accuracy.
In the conversion process, we have reduced the orig-
inal fine-grained classification of grammatical functions
to a more restricted set of 16 dependency types, which
are listed in Table 2. We have also replaced the origi-
nal (manual) part-of-speech annotation by using the same
automatic tagger that is used for preprocessing in the
parser. This is a standard probabilistic tagger trained on
the Stockholm-Umea? Corpus of written Swedish (SUC,
1997) and found to have an accuracy of 95?96% when
tested on held-out data.
Since the function we want to learn is a mapping from
parser states to transitions (and dependency types), the
treebank data cannot be used directly as training and test
Feature Description
TOP The token on top of the stack
TOP.POS The part-of-speech of TOP
TOP.DEP The dependency type of TOP (if any)
TOP.LEFT The dependency type of TOP?s leftmost dependent (if any)
TOP.RIGHT The dependency type of TOP?s rightmost dependent (if any)
NEXT The next input token
NEXT.POS The part-of-speech of NEXT
NEXT.LEFT The dependency type of NEXT?s leftmost dependent (if any)
LOOK.POS The part-of-speech of the next plus one input token
Table 1: Parser state features
data. Instead, we have to simulate the parser on the tree-
bank in order to derive, for each sentence, the transition
sequence corresponding to the correct dependency tree.
Given the result of this simulation, we can construct a
data set consisting of pairs ?s, t?, where s is a parser state
and t is the correct transition from that state (including
a dependency type if applicable). Unlike standard shift-
reduce parsing, the simulation of the current algorithm is
almost deterministic and is guaranteed to be correct if the
input dependency tree is well-formed.
The complete converted treebank contains 6316 sen-
tences and 97623 word tokens, which gives a mean sen-
tence length of 15.5 words. The treebank has been di-
vided into three non-overlapping data sets: 80% for train-
ing 10% for development/validation, and 10% for final
testing (random samples). The results presented below
are all from the validation set. (The final test set has not
been used at all in the experiments reported in this paper.)
When talking about test and validation data, we make
a distinction between the sentence data, which refers to
the original annotated sentences in the treebank, and the
transition data, which refers to the transitions derived by
simulating the parser on these sentences. While the sen-
tence data for validation consists of 631 sentences, the
corresponding transition data contains 15913 instances.
For training, only transition data is relevant and the train-
ing data set contains 371977 instances.
3.3 Evaluation
The output of the memory-based learner is a classifier that
predicts the next transition (including dependency type),
given the current state of the parser. The quality of this
classifier has been evaluated with respect to both predic-
tion accuracy and parsing accuracy.
Prediction accuracy refers to the quality of the clas-
sifier as such, i.e. how well it predicts the next transition
given the correct parser state, and is measured by the clas-
sification accuracy on unseen transition data (using a 0-1
loss function). We use McNemar?s test for statistical sig-
nificance.
Parsing accuracy refers to the quality of the classifier
as a guide for the deterministic parser and is measured
by the accuracy obtained when parsing unseen sentence
data. More precisely, parsing accuracy is measured by
the attachment score, which is a standard measure used
in studies of dependency parsing (Eisner, 1996; Collins
et al, 1999). The attachment score is computed as the
proportion of tokens (excluding punctuation) that are as-
signed the correct head (or no head if the token is a root).
Since parsing is a sentence-level task, we believe that
the overall attachment score should be computed as the
mean attachment score per sentence, which gives an es-
timate of the expected attachment score for an arbitrary
sentence. However, since most previous studies instead
use the mean attachment score per word (Eisner, 1996;
Collins et al, 1999), we will give this measure as well.
In order to measure label accuracy, we also define a la-
beled attachment score, where both the head and the label
must be correct, but which is otherwise computed in the
same way as the ordinary (unlabeled) attachment score.
For parsing accuracy, we use a paired t-test for statistical
significance.
4 Results
Table 3 shows the prediction accuracy achieved with
memory-based learning for the lexical and non-lexical
model, with two different parameter settings for the
learner. The results in the first column were obtained with
the default settings of the TiMBL package, in particular:
? The IB1 classification algorithm (Aha et al, 1991).
? The overlap distance metric.
? Features weighted by Gain Ratio (Quinlan, 1993).
? k = 1, i.e. classification based on a single nearest
neighbor.1
1In TiMBL, the value of k in fact refers to k nearest dis-
tances rather than k nearest neighbors, which means that, even
with k = 1, the nearest neighbor set can contain several in-
Label Dependency Type
ADV Adverbial modifier
APP Apposition
ATT Attribute
CC Coordination (conjunction or second conjunct)
DET Determiner
ID Non-first element of multi-word expression
IM Infinitive dependent on infinitive marker
IP Punctuation mark dependent on lexical head
INF Infinitival complement
OBJ Object
PR Complement of preposition
PRD Predicative complement
SUB Subject
UK Main verb of subordinate clause dependent on complementizer
VC Verb chain (nonfinite verb dependent on other verb)
XX Unclassifiable dependent
Table 2: Dependency types in Swedish treebank
Model Default Maximum
Non-lexical 86.8 87.4
Lexical 88.4 89.7
Table 3: Prediction accuracy for MBL models
The second column shows the accuracy for the best pa-
rameter settings found in the experiments (averaged over
both models), which differ from the default in the follow-
ing respects:
? Overlap metric replaced by the modified value dis-
tance metric (MVDM) (Stanfill and Waltz, 1986;
Cost and Salzberg, 1993).
? No weighting of features.
? k = 5, i.e. classification based on 5 nearest neigh-
bors.
? Distance weighted class voting with inverse distance
weighting (Dudani, 1976).
For more information about the different parameters and
settings, the reader is referred to Daelemans et al (2003).
The results show that the lexical model performs con-
sistently better than the non-lexical model, and that the
difference increases with the optimization of the learning
algorithm (all differences being significant at the .0001
level according to McNemar?s test). This confirms pre-
vious results from statistical parsing indicating that lex-
ical information is crucial for disambiguation (Collins,
stances that are equally distant to the test instance. This is dif-
ferent from the original IB1 algorithm, as described in Aha et
al. (1991).
1999; Charniak, 2000). As regards optimization, we may
note that although there is a significant improvement for
both models, the magnitude of the difference is relatively
small.
Table 4 shows the parsing accuracy obtained with the
optimized versions of the MBL models (lexical and non-
lexical), compared to the MCLE model described in sec-
tion 3. We see that MBL outperforms the MCLE model
even when limited to the same features (all differences
again being significant at the .0001 level according to
a paired t-test). This can probably be explained by the
fact that the similarity-based smoothing built into the
memory-based approach gives a better extrapolation than
the fixed back-off sequence in the MCLE model. We
also see that the lexical MBL model outperforms both
the other models. If we compare the labeled attachment
score to the prediction accuracy (which also takes depen-
dency types into account), we observe a substantial drop
(from 89.7 to 81.7 for the lexical model, from 87.4 to
76.5 for the non-lexical model), which is of course only
to be expected. The unlabeled attachment score is natu-
rally higher, and it is worth noting that the relative differ-
ence between the MBL lexical model and the other two
models is much smaller. This indicates that the advan-
tage of the lexical model mainly concerns the accuracy in
predicting dependency type in addition to transition.
Model Labeled Unlabeled
MCLE 74.7 (72.3) 81.5 (79.7)
MBL non-lexical 76.5 (74.7) 82.9 (81.7)
MBL lexical 81.7 (80.6) 85.7 (84.7)
Table 4: Parsing accuracy for MCLE and MBL models, attachment score per sentence (per word in parentheses)
If we compare the results concerning parsing accuracy
to those obtained for other languages (given that there
are no comparable results available for Swedish), we note
that the best unlabeled attachment score is lower than for
English, where the best results are above 90% (attach-
ment score per word) (Collins et al, 1999; Yamada and
Matsumoto, 2003), but higher than for Czech (Collins et
al., 1999). This is encouraging, given that the size of
the training set in our experiments is fairly small, only
about 10% of the standard training set for the Penn Tree-
bank. One reason why our results nevertheless compare
reasonably well with those obtained with the much larger
training set is probably that the conversion to dependency
trees is more accurate for the Swedish treebank, given the
explicit annotatation of grammatical functions. More-
over, the fact that our parser uses labeled dependencies
is probably also significant, since the possibility of us-
ing information from previously assigned (labeled) de-
pendencies during parsing seems to have a positive effect
on accuracy (Nivre, 2004).
Finally, it may be interesting to consider the accuracy
for individual dependency types. Table 5 gives labeled
precision, labeled recall and unlabeled attachment score
for four of the most important types with the MBL lex-
ical model. The results indicate that subjects have the
highest accuracy, especially when labels are taken into
account. Objects and predicative complements have com-
parable attachment accuracy, but are more often misclas-
sified with respect to dependency type. For adverbial
modifiers, finally, attachment accuracy is lower than for
the other dependency types, which is largely due to the
notorious PP-attachment problem.
5 Conclusion
In this paper we have shown that a combination of
memory-based learning and deterministic dependency
parsing can be used to construct a robust and efficient
parser for unrestricted natural language text, achieving a
parsing accuracy which is close to the state of the art even
with relatively limited amounts of training data. Clas-
sifiers based on memory-based learning achieve higher
parsing accuracy than previous probabilistic models, and
the improvement increases if lexical information is added
to the model.
Suggestions for further research includes the further
exploration of alternative models and parameter settings,
but also the combination of inductive and analytical
learning to impose high-level linguistic constraints, and
the development of new parsing methods (e.g. involving
multiple passes over the data). In addition, it is important
to evaluate the approach with respect to other languages
and corpora in order to increase the comparability with
other approaches.
Acknowledgements
The work presented in this paper was supported by a
grant from the Swedish Research Council (621-2002-
4207). The memory-based classifiers used in the experi-
ments were constructed using the Tilburg Memory-Based
Learner (TiMBL) (Daelemans et al, 2003). We are grate-
ful to three anonymous reviewers for constructive com-
ments on the preliminary version of the paper.
References
D. W. Aha, D. Kibler and M. Albert. 1991. Instance-
based Learning Algorithms. Machine Learning 6, 37?
66.
D. Aha. 1997. Lazy Learning. Dordrecht: Kluwer.
A. V. Aho, R. Sethi and J. D. Ullman. 1986. Compilers:
Principles Techniques, and Tools. Addison Wesley.
P. Boullier. 2003. Guided Earley Parsing. In G. van No-
ord (ed.) Proceedings of the 8th International Work-
shop on Parsing Technologies (IWPT 03), Nancy,
France, pp. 43?54.
E. Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings NAACL-2000.
M. Collins. 1999. Head-Driven Statistical Models for
Natural Language Parsing. PhD Thesis, University of
Pennsylvania.
M. Collins, J. Hajic, E. Brill, L. Ramshaw and C. Till-
mann. 1999. A Statistical Parser of Czech. In Pro-
ceedings of 37th ACL Conference, University of Mary-
land, College Park, USA, pp. 505?512.
S. Cost and S. Salzberg. 1993. A Weighted Nearest
Neighbor Algorithm for Learning with Symbolic Fea-
tures. Machine Learning 10, 57?78.
Dependency type Precision Recall Attachment
SUB 84.3 82.7 89.2
OBJ 74.7 78.8 87.0
PRD 75.1 71.4 84.2
ADV 76.2 74.6 78.3
Table 5: Dependency type accuracy, MBL lexical model; labeled precision, labeled recall, unlabeled attachment score
W. Daelemans. 1999. Memory-Based Language Pro-
cessing. Introduction to the Special Issue. Journal
of Experimental and Theoretical Artificial Intelligence
11(3), 287?292.
W. Daelemans, A. van den Bosch, J. Zavrel. 2002. For-
getting Exceptions is Harmful in Language Learning.
Machine Learning 34, 11?43.
W. Daelemans, J. Zavrel, K. van der Sloot and
A. van den Bosch, . 2003. TiMBL: Tilburg Memory
Based Learner, version 5.0, Reference Guide. Techni-
cal Report ILK 03-10, Tilburg University.
S. A. Dudani. 1976. The Distance-Weighted K-nearest
Neighbor Rule. IEEE Transactions on Systems, Man,
and Cybernetics SMC-6, 325?327.
J. Einarsson. 1976. Talbankens skriftsprkskonkordans.
Lund University.
J. M. Eisner. 1996. Three New Probabilistic Models for
Dependency Parsing: An Exploration. In Proceedings
of COLING-96, Copenhagen.
E. Fix and J. Hodges. 1952. Discriminatory Analy-
sis: Nonparametric Discrimination: Consistency Prop-
erties. Technical Report 21-49-004-11, USAF School
of Aviation Medicine, Randolph Field, Texas.
R. A. Hudson. 1990. English Word Grammar. Black-
well.
M. Kay. 2000. Guides and Oracles for Linear-Time Pars-
ing. In Proceedings of the 6th International Workshop
on Parsing Technologies (IWPT 00), Trento, Italy, pp.
6?9.
M. P. Marcus, B. Santorini and M. A. Marcinkiewics.
1993. Building a Large Annotated Corpus of English:
The Penn Treebank. Computational Linguistics 19,
313?330.
I. Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
J. Nivre. 2003. An Efficient Algorithm for Projective De-
pendency Parsing. In G. van Noord (ed.) Proceedings
of the 8th International Workshop on Parsing Tech-
nologies (IWPT 03), Nancy, France, pp. 149?160.
J. Nivre. 2004. Inductive Dependency Parsing. Techni-
cal Report, Va?xjo? University.
T. Obrebski. 2003. Dependency Parsing Using Depen-
dency Graph. In G. van Noord (ed.) Proceedings of
the 8th International Workshop on Parsing Technolo-
gies (IWPT 03), Nancy, France, pp. 217?218.
J. R. Quinlan. 1993. C4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
P. Sgall, E. Hajicova? and J. Panevova?. 1986. The Mean-
ing of the Sentence in Its Pragmatic Aspects. Reidel.
R. Skousen. 1989. Analogical Modeling of Language.
Dordrecht: Kluwer.
R. Skousen. 1992. Analogy and Structure. Dordrecht:
Kluwer.
C. Stanfill and D. Waltz. 1986. Toward Memory-Based
Reasoning. Communications of the ACM 29(12),
1213?1228.
SUC 1997. Stockholm Umea? Corpus. Version 1.0. Pro-
duced by Department of Linguistics, Umea? University
and Department of Linguistics, Stockholm University.
U. Teleman. 1974. Manual fo?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
L. Tesnie`re. 1959. Ele?ments de syntaxe structurale. Edi-
tions Klincksieck
J. Veenstra and W. Daelemans. 2000. A Memory-Based
Alternative for Connectionist Shift-Reduce Parsing.
Technical Report ILK-0012, University of Tilburg.
H. Yamada and Y. Matsumoto. 2003. Statistical De-
pendency Analysis with Support Vector Machines. In
G. van Noord (ed.) Proceedings of the 8th Interna-
tional Workshop on Parsing Technologies (IWPT 03),
Nancy, France, pp. 195?206.
Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),
pages 221?225, New York City, June 2006. c?2006 Association for Computational Linguistics
Labeled Pseudo-Projective Dependency Parsing
with Support Vector Machines
Joakim Nivre
Johan Hall
Jens Nilsson
School of Mathematics
and Systems Engineering
Va?xjo? University
35195 Va?xjo?, Sweden
{nivre,jha,jni}@msi.vxu.se
Gu?ls?en Eryig?it
Department of
Computer Engineering
Istanbul Technical University
34469 Istanbul, Turkey
gulsen@cs.itu.edu.tr
Svetoslav Marinov
School of Humanities
and Informatics
University of Sko?vde
Box 408
54128 Sko?vde, Sweden
svetoslav.marinov@his.se
Abstract
We use SVM classifiers to predict the next
action of a deterministic parser that builds
labeled projective dependency graphs in
an incremental fashion. Non-projective
dependencies are captured indirectly by
projectivizing the training data for the
classifiers and applying an inverse trans-
formation to the output of the parser. We
present evaluation results and an error
analysis focusing on Swedish and Turkish.
1 Introduction
The CoNLL-X shared task consists in parsing texts
in multiple languages using a single dependency
parser that has the capacity to learn from treebank
data. Our methodology for performing this task is
based on four essential components:
? A deterministic algorithm for building labeled
projective dependency graphs (Nivre, 2006).
? History-based feature models for predicting the
next parser action (Black et al, 1992).
? Support vector machines for mapping histories
to parser actions (Kudo and Matsumoto, 2002).
? Graph transformations for recovering non-
projective structures (Nivre and Nilsson, 2005).
All experiments have been performed using Malt-
Parser (Nivre et al, 2006), version 0.4, which is
made available together with the suite of programs
used for pre- and post-processing.1
1www.msi.vxu.se/users/nivre/research/MaltParser.html
2 Parsing Methodology
2.1 Parsing Algorithm
The parsing algorithm used for all languages is the
deterministic algorithm first proposed for unlabeled
dependency parsing by Nivre (2003) and extended
to labeled dependency parsing by Nivre et al (2004).
The algorithm builds a labeled dependency graph in
one left-to-right pass over the input, using a stack
to store partially processed tokens and adding arcs
using four elementary actions (where top is the token
on top of the stack and next is the next token):
? SHIFT: Push next onto the stack.
? REDUCE: Pop the stack.
? RIGHT-ARC(r): Add an arc labeled r from top
to next; push next onto the stack.
? LEFT-ARC(r): Add an arc labeled r from next
to top; pop the stack.
Although the parser only derives projective graphs,
the fact that graphs are labeled allows non-projective
dependencies to be captured using the pseudo-
projective approach of Nivre and Nilsson (2005) .
Another limitation of the parsing algorithm is that
it does not assign dependency labels to roots, i.e., to
tokens having HEAD=0. To overcome this problem,
we have implemented a variant of the algorithm that
starts by pushing an artificial root token with ID=0
onto the stack. Tokens having HEAD=0 can now
be attached to the artificial root in a RIGHT-ARC(r)
action, which means that they can be assigned any
label. Since this variant of the algorithm increases
the overall nondeterminism, it has only been used
for the data sets that include informative root labels
(Arabic, Czech, Portuguese, Slovene).
221
FO L C P FE D
S: top + + + + + +
S: top?1 +
I: next + + + + +
I: next+1 + +
I: next+2 +
I: next+3 +
G: head of top +
G: leftmost dep of top +
G: rightmost dep of top +
G: leftmost dep of next +
Table 1: Base model; S: stack, I: input, G: graph;
FO: FORM, L: LEMMA , C: CPOS, P: POS,
FE: FEATS, D: DEPREL
2.2 History-Based Feature Models
History-based parsing models rely on features of the
derivation history to predict the next parser action.
The features used in our system are all symbolic
and extracted from the following fields of the data
representation: FORM, LEMMA, CPOSTAG, POSTAG,
FEATS, and DEPREL. Features of the type DEPREL
have a special status in that they are extracted during
parsing from the partially built dependency graph
and may therefore contain errors, whereas all the
other features have gold standard values during both
training and parsing.2
Based on previous research, we defined a base
model to be used as a starting point for language-
specific feature selection. The features of this model
are shown in Table 1, where rows denote tokens in
a parser configuration (defined relative to the stack,
the remaining input, and the partially built depen-
dency graph), and where columns correspond to data
fields. The base model contains twenty features, but
note that the fields LEMMA, CPOS and FEATS are not
available for all languages.
2.3 Support Vector Machines
We use support vector machines3 to predict the next
parser action from a feature vector representing the
history. More specifically, we use LIBSVM (Chang
and Lin, 2001) with a quadratic kernel K(xi, xj) =
(?xTi xj +r)2 and the built-in one-versus-all strategy
for multi-class classification. Symbolic features are
2The fields PHEAD and PDEPREL have not been used at all,
since we rely on pseudo-projective parsing for the treatment of
non-projective structures.
3We also ran preliminary experiments with memory-based
learning but found that this gave consistently lower accuracy.
converted to numerical features using the standard
technique of binarization, and we split values of the
FEATS field into its atomic components.4
For some languages, we divide the training data
into smaller sets, based on some feature s (normally
the CPOS or POS of the next input token), which may
reduce training times without a significant loss in
accuracy (Yamada and Matsumoto, 2003). To avoid
too small training sets, we pool together categories
that have a frequency below a certain threshold t.
2.4 Pseudo-Projective Parsing
Pseudo-projective parsing was proposed by Nivre
and Nilsson (2005) as a way of dealing with
non-projective structures in a projective data-driven
parser. We projectivize training data by a minimal
transformation, lifting non-projective arcs one step
at a time, and extending the arc label of lifted arcs
using the encoding scheme called HEAD by Nivre
and Nilsson (2005), which means that a lifted arc is
assigned the label r?h, where r is the original label
and h is the label of the original head in the non-
projective dependency graph.
Non-projective dependencies can be recovered by
applying an inverse transformation to the output of
the parser, using a left-to-right, top-down, breadth-
first search, guided by the extended arc labels r?h
assigned by the parser. This technique has been used
without exception for all languages.
3 Experiments
Since the projective parsing algorithm and graph
transformation techniques are the same for all data
sets, our optimization efforts have been focused on
feature selection, using a combination of backward
and forward selection starting from the base model
described in section 2.2, and parameter optimization
for the SVM learner, using grid search for an optimal
combination of the kernel parameters ? and r, the
penalty parameter C and the termination criterion ?,
as well as the splitting feature s and the frequency
threshold t. Feature selection and parameter opti-
mization have to some extent been interleaved, but
the amount of work done varies between languages.
4Preliminary experiments showed a slight improvement for
most languages when splitting the FEATS values, as opposed to
taking every combination of atomic values as a distinct value.
222
Ara Bul Chi Cze Dan Dut Ger Jap Por Slo Spa Swe Tur Total
LAS 66.71 87.41 86.92 78.42 84.77 78.59 85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.19
UAS 77.52 91.72 90.54 84.80 89.80 81.35 88.76 93.10 91.22 78.72 84.67 89.50 75.82 85.48
LAcc 80.34 90.44 89.01 85.40 89.16 83.69 91.03 94.34 91.54 80.54 90.06 87.39 78.49 86.75
Table 2: Evaluation on final test set; LAS = labeled attachment score, UAS = unlabeled attachment score,
LAcc = label accuracy score; total score excluding Bulgarian
The main optimization criterion has been labeled
attachment score on held-out data, using ten-fold
cross-validation for all data sets with 100k tokens
or less, and an 80-20 split into training and devtest
sets for larger datasets. The number of features in
the optimized models varies from 16 (Turkish) to 30
(Spanish), but the models use all fields available for
a given language, except that FORM is not used for
Turkish (only LEMMA). The SVM parameters fall
into the following ranges: ?: 0.12?0.20; r: 0.0?0.6;
C: 0.1?0.7; ?: 0.01?1.0. Data has been split on the
POS of the next input token for Czech (t = 200),
German (t = 1000), and Spanish (t = 1000), and
on the CPOS of the next input token for Bulgarian
(t = 1000), Slovene (t = 600), and Turkish (t = 100).
(For the remaining languages, the training data has
not been split at all.)5 A dry run at the end of the
development phase gave a labeled attachment score
of 80.46 over the twelve required languages.
Table 2 shows final test results for each language
and for the twelve required languages together. The
total score is only 0.27 percentage points below the
score from the dry run, which seems to indicate that
models have not been overfitted to the training data.
The labeled attachment score varies from 91.65 to
65.68 but is above average for all languages. We
have the best reported score for Japanese, Swedish
and Turkish, and the score for Arabic, Danish,
Dutch, Portuguese, Spanish, and overall does not
differ significantly from the best one. The unlabeled
score is less competitive, with only Turkish having
the highest reported score, which indirectly indicates
that the integration of labels into the parsing process
primarily benefits labeled accuracy.
4 Error Analysis
An overall error analysis is beyond the scope of this
paper, but we will offer a few general observations
5Detailed specifications of the feature models and learning
algorithm parameters can be found on the MaltParser web page.
before we turn to Swedish and Turkish, focusing on
recall and precision of root nodes, as a reflection of
global syntactic structure, and on attachment score
as a function of arc length. If we start by considering
languages with a labeled attachment score of 85% or
higher, they are characterized by high precision and
recall for root nodes, typically 95/90, and by a grace-
ful degradation of attachment score as arcs grow
longer, typically 95?90?85, for arcs of length 1, 2
and 3?6. Typical examples are Bulgarian (Simov
et al, 2005; Simov and Osenova, 2003), Chinese
(Chen et al, 2003), Danish (Kromann, 2003), and
Swedish (Nilsson et al, 2005). Japanese (Kawata
and Bartels, 2000), despite a very high accuracy, is
different in that attachment score drops from 98%
to 85%, as we go from length 1 to 2, which may
have something to do with the data consisting of
transcribed speech with very short utterances.
A second observation is that a high proportion of
non-projective structures leads to fragmentation in
the parser output, reflected in lower precision for
roots. This is noticeable for German (Brants et al,
2002) and Portuguese (Afonso et al, 2002), which
still have high overall accuracy thanks to very high
attachment scores, but much more conspicuous for
Czech (Bo?hmova? et al, 2003), Dutch (van der Beek
et al, 2002) and Slovene (Dz?eroski et al, 2006),
where root precision drops more drastically to about
69%, 71% and 41%, respectively, and root recall is
also affected negatively. On the other hand, all three
languages behave like high-accuracy languages with
respect to attachment score. A very similar pattern
is found for Spanish (Civit Torruella and Mart?? An-
ton??n, 2002), although this cannot be explained by
a high proportion of non-projective structures. One
possible explanation in this case may be the fact that
dependency graphs in the Spanish data are sparsely
labeled, which may cause problem for a parser that
relies on dependency labels as features.
The results for Arabic (Hajic? et al, 2004; Smrz?
et al, 2002) are characterized by low root accuracy
223
as well as a rapid degradation of attachment score
with arc length (from about 93% for length 1 to 67%
for length 2). By contrast, Turkish (Oflazer et al,
2003; Atalay et al, 2003) exhibits high root accu-
racy but consistently low attachment scores (about
88% for length 1 and 68% for length 2). It is note-
worthy that Arabic and Turkish, being ?typological
outliers?, show patterns that are different both from
each other and from most of the other languages.
4.1 Swedish
A more fine-grained analysis of the Swedish results
reveals a high accuracy for function words, which
is compatible with previous studies (Nivre, 2006).
Thus, the labeled F-score is 100% for infinitive
markers (IM) and subordinating conjunctions (UK),
and above 95% for determiners (DT). In addition,
subjects (SS) have a score above 90%. In all these
cases, the dependent has a configurationally defined
(but not fixed) position with respect to its head.
Arguments of the verb, such as objects (DO, IO)
and predicative complements (SP), have a slightly
lower accuracy (about 85% labeled F-score), which
is due to the fact that they ?compete? in the same
structural positions, whereas adverbials (labels that
end in A) have even lower scores (often below 70%).
The latter result must be related both to the relatively
fine-grained inventory of dependency labels for ad-
verbials and to attachment ambiguities that involve
prepositional phrases. The importance of this kind
of ambiguity is reflected also in the drastic differ-
ence in accuracy between noun pre-modifiers (AT)
(F > 97%) and noun post-modifiers (ET) (F ? 75%).
Finally, it is worth noting that coordination, which
is often problematic in parsing, has high accuracy.
The Swedish treebank annotation treats the second
conjunct as a dependent of the first conjunct and as
the head of the coordinator, which seems to facil-
itate parsing.6 The attachment of the second con-
junct to the first (CC) has a labeled F-score above
80%, while the attachment of the coordinator to the
second conjunct (++) has a score well above 90%.
4.2 Turkish
In Turkish, very essential syntactic information is
contained in the rich morphological structure, where
6The analysis is reminiscent of the treatment of coordination
in the Collins parser (Collins, 1999).
concatenated suffixes carry information that in other
languages may be expressed by separate words. The
Turkish treebank therefore divides word forms into
smaller units, called inflectional groups (IGs), and
the task of the parser is to construct dependencies
between IGs, not (primarily) between word forms
(Eryig?it and Oflazer, 2006). It is then important
to remember that an unlabeled attachment score
of 75.8% corresponds to a word-to-word score of
82.7%, which puts Turkish on a par with languages
like Czech, Dutch and Spanish. Moreover, when
we break down the results according to whether the
head of a dependency is part of a multiple-IG word
or a complete (single-IG) word, we observe a highly
significant difference in accuracy, with only 53.2%
unlabeled attachment score for multiple-IG heads
versus 83.7% for single-IG heads. It is hard to say
at this stage whether this means that our methods
are ill-suited for IG-based parsing, or whether it is
mainly a case of sparse data for multiple-IG words.
When we break down the results by dependency
type, we can distinguish three main groups. The first
consists of determiners and particles, which have
an unlabeled attachment score over 80% and which
are found within a distance of 1?1.4 IGs from their
head.7 The second group mainly contains subjects,
objects and different kinds of adjuncts, with a score
in the range 60?80% and a distance of 1.8?5.2 IGs to
their head. In this group, information about case and
possessive features of nominals is important, which
is found in the FEATS field in the data representation.
We believe that one important explanation for our
relatively good results for Turkish is that we break
down the FEATS information into its atomic com-
ponents, independently of POS and CPOS tags, and
let the classifier decide which one to use in a given
situation. The third group contains distant depen-
dencies, such as sentence modifiers, vocatives and
appositions, which have a much lower accuracy.
5 Conclusion
The evaluation shows that labeled pseudo-projective
dependency parsing, using a deterministic parsing
algorithm and SVM classifiers, gives competitive
parsing accuracy for all languages involved in the
7Given that the average IG count of a word is 1.26 in the
treebank, this means that they are normally adjacent to the head
word.
224
shared task, although the level of accuracy varies
considerably between languages. To analyze in
depth the factors determining this variation, and to
improve our parsing methods accordingly to meet
the challenges posed by the linguistic diversity, will
be an important research goal for years to come.
Acknowledgments
We are grateful for the support from T ?UB?ITAK
(The Scientific and Technical Research Council of
Turkey) and the Swedish Research Council. We also
want to thank Atanas Chanev for assistance with
Slovene, the organizers of the shared task for all
their hard work, and the creators of the treebanks
for making the data available.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora, volume 20 of Text, Speech and Language
Technology. Kluwer Academic Publishers, Dordrecht.
S. Afonso, E. Bick, R. Haber, and D. Santos. 2002. ?Floresta
sinta?(c)tica?: a treebank for Portuguese. In Proc. of LREC-
2002, pages 1698?1703.
N. B. Atalay, K. Oflazer, and B. Say. 2003. The annotation
process in the Turkish treebank. In Proc. of LINC-2003.
E. Black, F. Jelinek, J. D. Lafferty, D. M. Magerman, R. L. Mer-
cer, and S. Roukos. 1992. Towards history-based grammars:
Using richer models for probabilistic parsing. In Proc. of the
5th DARPA Speech and Natural Language Workshop, pages
31?37.
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003. The
PDT: a 3-level annotation scenario. In Abeille? (Abeille?,
2003), chapter 7.
S. Brants, S. Dipper, S. Hansen, W. Lezius, and G. Smith. 2002.
The TIGER treebank. In Proc. of TLT-2002.
C.-C. Chang and C.-J. Lin, 2001. LIBSVM: A Library
for Support Vector Machines. Software available at
http://www.csie.ntu.edu.tw/ cjlin/libsvm.
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang, and
Z. Gao. 2003. Sinica treebank: Design criteria, representa-
tional issues and implementation. In Abeille? (Abeille?, 2003),
chapter 13, pages 231?248.
M. Civit Torruella and Ma A. Mart?? Anton??n. 2002. Design
principles for a Spanish treebank. In Proc. of TLT-2002.
M. Collins. 1999. Head-Driven Statistical Models for Natural
Language Parsing. Ph.D. thesis, University of Pennsylvania.
S. Dz?eroski, T. Erjavec, N. Ledinek, P. Pajas, Z. ?Zabokrtsky, and
A. ?Zele. 2006. Towards a Slovene dependency treebank. In
Proc. of LREC-2006.
G. Eryig?it and K. Oflazer. 2006. Statistical dependency parsing
of Turkish. In Proc. of EACL-2006.
J. Hajic?, O. Smrz?, P. Zema?nek, J. ?Snaidauf, and E. Bes?ka. 2004.
Prague Arabic dependency treebank: Development in data
and tools. In Proc. of NEMLAR-2004, pages 110?117.
Y. Kawata and J. Bartels. 2000. Stylebook for the Japanese
treebank in VERBMOBIL. Verbmobil-Report 240, Seminar
fu?r Sprachwissenschaft, Universita?t Tu?bingen.
M. T. Kromann. 2003. The Danish dependency treebank and
the underlying linguistic theory. In Proc. of TLT-2003.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency anal-
ysis using cascaded chunking. In Proc. of CoNLL-2002,
pages 63?69.
J. Nilsson, J. Hall, and J. Nivre. 2005. MAMBA meets TIGER:
Reconstructing a Swedish treebank from antiquity. In Proc.
of the NODALIDA Special Session on Treebanks.
J. Nivre and J. Nilsson. 2005. Pseudo-projective dependency
parsing. In Proc. of ACL-2005, pages 99?106.
J. Nivre, J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proc. CoNLL-2004, pages 49?56.
J. Nivre, J. Hall, and J. Nilsson. 2006. MaltParser: A data-
driven parser-generator for dependency parsing. In Proc. of
LREC-2006.
J. Nivre. 2003. An efficient algorithm for projective depen-
dency parsing. In Proc. of IWPT-2003, pages 149?160.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r. 2003.
Building a Turkish treebank. In Abeille? (Abeille?, 2003),
chapter 15.
K. Simov and P. Osenova. 2003. Practical annotation scheme
for an HPSG treebank of Bulgarian. In Proc. of LINC-2003,
pages 17?24.
K. Simov, P. Osenova, A. Simov, and M. Kouylekov. 2005.
Design and implementation of the Bulgarian HPSG-based
treebank. In Journal of Research on Language and Com-
putation ? Special Issue, pages 495?522. Kluwer Academic
Publishers.
O. Smrz?, J. ?Snaidauf, and P. Zema?nek. 2002. Prague depen-
dency treebank for Arabic: Multi-level annotation of Arabic
corpus. In Proc. of the Intern. Symposium on Processing of
Arabic, pages 147?155.
L. van der Beek, G. Bouma, R. Malouf, and G. van Noord.
2002. The Alpino dependency treebank. In Computational
Linguistics in the Netherlands (CLIN).
H. Yamada and Y. Matsumoto. 2003. Statistical dependency
analysis with support vector machines. In Proc. of IWPT-
2003, pages 195?206.
225
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 49?60,
Paris, October 2009. c?2009 Association for Computational Linguistics
Parsing Formal Languages using Natural Language Parsing Techniques
Jens Nilsson? Welf Lo?we? Johan Hall?? Joakim Nivre??
?Va?xjo? University, School of Mathematics and Systems Engineering, Sweden
?Uppsala University, Department of Linguistics and Philology, Sweden
{jens.nilsson|welf.lowe|johan.hall|joakim.nivre}@vxu.se
Abstract
Program analysis tools used in software
maintenance must be robust and ought to
be accurate. Many data-driven parsing ap-
proaches developed for natural languages
are robust and have quite high accuracy
when applied to parsing of software. We
show this for the programming languages
Java, C/C++, and Python. Further studies
indicate that post-processing can almost
completely remove the remaining errors.
Finally, the training data for instantiating
the generic data-driven parser can be gen-
erated automatically for formal languages,
as opposed to the manually development
of treebanks for natural languages. Hence,
our approach could improve the robust-
ness of software maintenance tools, proba-
bly without showing a significant negative
effect on their accuracy.
1 Introduction
Software engineering, especially software mainte-
nance, is supported by numerous program anal-
ysis tools. Maintenance tasks include program
comprehension (understanding unknown code for
fixing bugs or further development), quality as-
sessment (judging code, e.g., in code reviews),
and reverse-engineering (reifying the design doc-
uments for given source code). To extract infor-
mation from the programs, the tools first parse the
program code and produce an abstract syntax tree
(AST) for further analysis and abstraction (Strein
et al, 2007). As long as the program conforms
to the syntax of a programming language, clas-
sical parsing techniques known from the field of
compiler construction may be applied. This, how-
ever, cannot be assumed in general, as the pro-
grams to analyze can be incomplete, erroneous, or
conform to a (yet unknown) dialect or version of
the language. Despite error stabilization, classi-
cal parsers then lose a lot of information or simply
break down. This is unsatisfactory for tools sup-
porting maintenance. Therefore, quite some effort
has gone into the development of robust parsers of
programs for these tools (cf. our related work sec-
tion 5). This effort, however, has to be repeated
for every programming language.
The development of robust parsers is of special
interest for languages like C/C++ due to their nu-
merous dialects in use (Anderson, 2008). Also,
tools for languages frequently coming in new ver-
sions, like Java, benefit from robust parsing. Fi-
nally, there are languages like HTML where exist-
ing browsers are forgiving if documents do not ad-
here to the formal standard with the consequence
that there exist many formally erroneous docu-
ments. In such cases, robust parsing is even a pre-
requisite for tool-supported maintenance.
The accuracy of parsing is a secondary goal
in the context of software maintenance. Tasks
like program comprehension, quality assessment,
and reverse-engineering are fuzzy by their nature.
There is no well-defined notion of correctness?
rather an empirical answer to the question: Did
it help the software engineers in fulfilling their
tasks? Moreover, the information provided to the
engineers abstracts anyway from the concrete pro-
gram syntax and semantics, i.e., inaccuracies in
the input may disappear in the output. Finally, pro-
gram analyses are often heuristics themselves, ap-
proximating computationally hard problems like
pattern matching and optimal clustering.
The natural language processing (NLP) com-
munity has for many years developed parsing tech-
nology that is both completely robust and highly
accurate. The present approach applies this tech-
nology to programming languages. It is robust in
the sense that, for each program, the parser always
gives a meaningful model even for slightly incor-
rect and incomplete programs. The approach is,
49
however, not accurate to 100%, i.e., even correct
programs may lead to slightly incorrect models.
As we will show, it is quite accurate when applied
to programming languages.
The data-driven dependency parsing approach
applied here only needs correct examples of the
source and the expected analysis model. Then it
automatically trains and adapts a generic parser.
As we will show, training data for adapting to a
new programming language can even be gener-
ated automatically. Hence, the effort for creating
a parser for a new programming language is quite
small.
The basic idea ? applying natural language pars-
ing to programming languages ? has been pre-
sented to the program maintenance community be-
fore (Nilsson et al, 2009). This paper contributes
with experimental results on
1. data-driven dependency parsing of the pro-
gramming languages C/C++, Java, and
Python,
2. transformations between dependency struc-
ture and phrase structure adapted to program-
ming languages,
3. generic parser model selection and its effect
on parsing accuracy.
Section 2 gives an introduction to the parsing tech-
nology applied here. In section 3, the preparation
of the training examples necessary is described,
while section 4 presents the experimental results.
Section 5 discusses related work in information
extraction for software maintenance. We end with
conclusions and future work in section 6.
2 NLP Background
Dependency structure is one way of representing
the syntax of natural languages. Dependency trees
form labeled, directed and rooted trees, as shown
in figure 1. One essential difference compared to
context-free grammar is the absence of nontermi-
nals. Another difference is that the syntactic struc-
ture is composed of lexical tokens (also called ter-
minals or words) linked by binary and directed re-
lations called dependencies. Each token in the fig-
ure is labeled with a part-of-speech, shown at the
bottom of the figure. Each dependency relation is
also labeled.
The parsing algorithm used in the experiments
of section 4, known as the Nivre?s arc-eager al-
Figure 1: Sentence with a dependency tree.
gorithm (Nivre, 2003), can produce such depen-
dency trees. It bears a resemblance to the shift-
reduce parser for context-free grammars, with the
most apparent difference being that terminals (not
nonterminals) are pushed onto the stack. Parser
configurations are represented by a stack, a list
of (remaining) input tokens, and the (current) set
of arcs for the dependency tree. Similar to the
shift-reduce parser, the construction of syntactic
structure is created by a sequence of transitions.
The parser starts with an empty stack and termi-
nates when the input queue is empty, parsing in-
put from left to right. It has four transitions (Left-
Arc, Right-Arc, Reduce and Shift), manipulating
these data structures. The algorithm has a linear
time complexity as it is guaranteed to terminate
after at most 2n transitions, given that the length
of the input sentence is n.
In contrast to a parser guided by a grammar
(e.g., ordinary shift-reduce parsing for context-
free grammars), this parser is guided by a clas-
sifier induced from empirical data using machine
learning (Nivre et al, 2004). Hence, the parser re-
quires training data containing dependency trees.
In other words, the parser has a training phase
where the training data is used by the training
module in order to learn the correct sequence of
transitions. The training data can contain depen-
dency trees for sentences of any language irrespec-
tively of whether the language is a natural or for-
mal one.
The training module produces the correct tran-
sition sequences using the dependency trees of
the training data. These correct parser configura-
tions and transition sequences are then provided as
training data to a classifier, which predicts the cor-
rect transitions (including a dependency label for
Left-Arc, Right-Arc) given parser configurations.
A parser configuration contains a vast amount of
information located in the data-structures. It is
therefore necessary to abstract it into a set of fea-
tures. Possible features are word forms and parts-
50
of-speech of tokens on the stack and in the list
of input tokens, and dependency labels of depen-
dency arcs created so far.
The parser produces exactly one syntactic anal-
ysis for every input, even if the input does not con-
form to a grammar. The price we have to pay for
this robustness is that any classifier is bound to
commit errors even if the input is acceptable ac-
cording to a grammar.
3 General Approach
In section 2, we presented a parsing algorithm for
producing dependency trees for natural languages.
Here we will show how it can be used to produce
syntactic structures for programming languages.
Since the framework requires training data form-
ing correct dependency trees, we need an approach
for converting source code to dependency trees.
The general approach can be divided into two
phases, training and production. In order to be
able to perform both these phases in this study, we
need to adapt natural language parsing to the needs
of information extraction from programming lan-
guage code, i.e., we need to automatically produce
training data. Therefore, we apply:
(a) Source Code ? Syntax Tree: the classical
approach for generating syntax trees for cor-
rect and complete source code of a program-
ming language.
(b) Syntax Tree ? Dependency Tree: an ap-
proach for encoding the syntax trees as de-
pendency trees adapted to programming lan-
guages.
(c) Dependency Tree ? Syntax Tree: an ap-
proach to convert the dependency trees back
to syntax trees.
These approaches have been accomplished as pre-
sented below. In the training phase, we need to
train and adapt the generic parsing approach to a
specific programming language. Therefore:
(1) Generate training data automatically by
producing syntax trees and then dependency
trees for correct programs using approaches
(a) and (b).
(2) Train the generic parser with the training
data.
This automated training phase needs to be done
for every new programming language we adapt to.
Finally, in the production phase, we extract the in-
formation from (not necessarily correct and com-
plete) programs:
(3) Parse the new source code into dependency
trees.
(4) Convert the dependency trees into syntax
trees using approach (c).
This automated production phase needs to be exe-
cuted for every project we analyze.
Steps (2) and (3) have already been discussed in
section 2 for parsing natural languages. They can
be generalized to parsing programming languages
as described in section 3.1. Both the training phase
and the production phase are complete, once the
steps (a)?(c) have been accomplished. We present
them in sections 3.2, 3.3, and 3.4, respectively.
3.1 Adapting the Input
As mentioned, the parsing algorithm described
in section 2 has been developed for natural lan-
guages, which makes it necessary to resolve a
number of issues that arise when the parser is
adapted for source code as input. First, the parsing
algorithm takes a sequence of words as input, and
for simplicity, we map the tokens in a program-
ming language to words.
One slightly more problematic issue is how to
define a ?sentence? in source code. A natural
language text syntactically decomposes into a se-
quence of sentences in a relatively natural way.
But is there also a natural way of splitting source
code into sentences? The most apparent approach
may be to define a sentence as a compilation unit,
that is, a file of source code. This can however re-
sult in practical problems since a sentence in a nat-
ural language text is usually on average between
15?25 words long, partially depending on the au-
thor and the type of text. The sequence of tokens
in a source file may on the other hand be much
longer. Time complexity is usually in practice of
less importance when the average sentence length
is as low as in natural languages, but that is hardly
the case when there can be several thousands to-
kens in a sentence to parse.
Other approaches could for instance be to let
one method be a sentence. However, then we need
to deal with other types of source code construc-
tions explicitly. We have in this study for sim-
plicity let one compilation unit be one sentence.
This is possible in practice due to the linear time
51
complexity of the parsing algorithm of section 2,
a quite unusual property compared to other NLP
parsers guided by machine learning with state-of-
the-art accuracy.
3.2 Source Code? Syntax Tree
In order to produce training data for the parser
for a programming language, an analyzer that
constructs syntax trees for correct and complete
source code of the programming language is
needed. We are in this study focusing on Java,
Python and C/C++, and consequently need one
such analyzer for each language. For example, fig-
ure 2 shows the concrete syntax tree of the follow-
ing fragments of Java:
Example (1):
public String getName() {
return name;
}
Example (2):
while (count > 0) {
stack[--count]=null;
}
We also map the output of the lexical ana-
lyzer to the parts-of-speech for the words (e.g.,
Identifier for String and getName). All
source code comments and indentation informa-
tion (except for Python where the indentation con-
veys hierarchical information) have been excluded
from the syntax trees. All string and character
literals have also been mapped to ?string? and
?char?, respectively. This does not entail that the
approach is lossy, since all this information can
be retained in a post-processing step, if neces-
sary. As pointed out by, for instance, Collard et
al. (2003), comments and indentation may among
other things be of interest when trying to under-
stand source code.
3.3 Syntax Tree? Dependency Tree
Here we will discuss the conversion of syntax trees
into dependency trees. We use a method that has
been successfully applied for natural languages
for converting syntax trees into a convertible de-
pendency tree that makes it possible to perform
the inverse conversion, meaning that information
about the syntax tree is saved in complex arc la-
bels (Hall and Nivre, 2008). We also present re-
sults in section 4 using the dependency trees that
cannot be used for the inverse conversion, which
we call non-convertible dependency trees.
The conversion is performed in a two-step ap-
proach. First, the algorithm traverses the syntax
tree from the root and identifies the head-child and
the terminal head for all nonterminals in a recur-
sive depth-first search. To identify the head-child
for each nonterminal, the algorithm uses heuristics
called head-finding rules, inspired by, for instance,
Magerman (1995). Three head-finding strategies
have been investigated. For each nonterminal:
1. FREQ: Let the element with the most fre-
quently occurring name be the head, but ex-
clude the token ?;? as a potential head. If two
tokens have the same frequency, let the left-
most occurring element be the head.
2. LEFT: let the leftmost terminal in the entire
subtree of the nonterminal be the head of all
other elements.
3. RIGHT: let the rightmost terminal in the en-
tire subtree of the nonterminal be the head of
all other elements.
The dependency trees in figures 3 and 4 use LEFT
and FREQ. LEFT and RIGHT induce that all arcs
are pointing to the right and left, respectively. The
head-finding rules for FREQ are automatically cre-
ated by counting the children?s names for each
distinct non-terminal name in the syntax trees of
the training data. The priority list is then com-
piled by ordering the elements by descending fre-
quency for each distinct non-terminal name. For
instance, given that the syntax trees are grammati-
cally correct, every non-terminal While will con-
tain the tokens (, ) and while. These tokens
have thus the highest priority, and while there-
fore becomes the head in the lower dependency
tree of figure 4. This is the same as choosing the
left-most mandatory element for each left-hand
side in the grammar. An interesting observation
is that binary operators and the copy assignment
operator become the heads of their operands for
FREQ, which is the case for < and = in figure 4.
Note also that the element names of terminals act
as part-of-speech tags, e.g., the part-of-speech for
String is Identifier.
In the second step, a dependency tree is created
according to the identified terminal heads. The
arcs in the convertible dependency tree are labeled
with complex arc labels, where each complex arc
label consists of two sublabels:
52
Figure 2: Syntax trees for examples (1) and (2).
Figure 3: Non-convertible dependency trees for example (1) using LEFT (upper) and FREQ (lower).
1. Encode the dependent spine, i.e., the se-
quence of nonterminal labels from the de-
pendent terminal to the highest nonterminal
where the dependent terminal is the terminal
head; ?|? separates the nonterminal labels,
2. Encode the attachment point in the head
spine, a non-negative integer value a, which
means that the dependent spine is attached a
steps up in the head spine.
By encoding the arc labels with these two subla-
bels, it is possible to perform the inverse conver-
sion, (see subsection 3.4).
The non-convertible dependency labels allow us
to reduce the complexity of the arc labels, making
the learning problem simpler due to fewer distinct
arc labels. This may result in a higher accuracy
during parsing and can be used as input for fur-
ther processing directly without taking the detour
back to syntax trees. This can be motivated by
the fact that all information in the syntax trees is
usually not needed anyway in many reverse engi-
neering tasks, but labels indicating method calls
and declarations ? the most important information
for most program comprehension tasks ? are pre-
served. This is exemplified by the fact that both
dependency structures in figure 3 contain the la-
bel MethodsDecl.. We thus believe that all the
necessary information is also captured in this less
informative dependency tree. Each dependency la-
bel is the highest nonterminal name of the spine,
that is, the single nonterminal name that is closest
to its head. The non-convertible dependency label
also excludes the attachment point value, making
the learning problem even simpler. Figures 3 and
4 show the non-convertible dependency labels of
the syntax trees (or phrase structure trees) in the
same figures, where each label contains just a sin-
gle nonterminal name of the original syntax trees.
3.4 Dependency Tree? Syntax Tree
The inverse conversion is a bottom-up and top-
down process on the convertible dependency tree
53
Figure 4: Non-convertible dependency trees for example (2) using LEFT (upper) and FREQ (lower).
(must contain complex arc labels). First, the algo-
rithm visits every terminal in the convertible de-
pendency tree and restores the spines of nontermi-
nals with labels for each terminal using the infor-
mation in the first sublabel of the incoming arc.
Thus, the bottom-up process results in a spine of
zero or more arcs from each terminal to the highest
nonterminal of which the terminal is the terminal
head. Secondly, the spines are weaved together ac-
cording to the arcs of the dependency tree. This is
achieved by traversing the dependency tree recur-
sively from the root using a pre-order depth-first
search, where the dependent spine is attached to
its head spine or to the root of the syntax tree. The
attachment point a, given by the second sublabel,
specifies the number of nonterminals between the
terminal head and the attachment nonterminal.
4 Experiments
We will in this section present parsing experiments
and evaluate the accuracy of the syntax trees pro-
duced by the parser. As mentioned in section 2,
the parsing algorithm is robust in the sense that it
always produces a syntactic analysis no matter the
input, but it can commit errors even for correct in-
put. This section investigates the accuracy for cor-
rect input, when varying feature set, head-finding
rules and language. We begin with the experimen-
tal setup.
4.1 Experimental Setup
The open-source software MaltParser (malt-
parser.org) (Nivre et al, 2006) is used in the ex-
periments. It contains an implementation of the
parsing algorithm, as well as an implementation
of the conversion strategy from syntax trees to
dependency trees and back, presented in subsec-
tions 3.3 and 3.4. It comes with the machine
learner LIBSVM (Chang and Lin, 2001), pro-
ducing the most accurate results for parsing nat-
ural languages compared to other evaluated ma-
chine learners (Hall et al, 2006). LIBSVM re-
quires training data. The source files of the follow-
ing projects have been converted into dependency
trees:
? For Java: Recoder 0.83 (Gutzmann et al,
2007), using all source files in the directory
?src? (having 400 source files with 92k LOC
and 335k tokens).
? For C/C++: Elsa 2005.08.22b (McPeak,
2005), where 1389 source files were used,
including the 978 C/C++ benchmark files in
the distribution (thus comprising 1389 source
files with 265k LOC and 691k tokens).
? For Python: Natural Language Toolkit
0.9.5 (Bird et al, 2008), where all source files
in the directory ?nltk? were used (having 160
source files with 65k LOC and 280k tokens).
To construct the syntax tree for the source code
file of Recoder, we have used Recoder. It cre-
ates an abstract syntax tree for a source file, but
we are currently interested in the concrete syntax
tree with all the original tokens. In this first con-
version step, the tokens of the syntax trees are thus
retained. For example, the syntax trees in figure 2
are generated by Recoder.
54
The same strategy was adopted for Elsa with the
difference that CDT 4.0.3, a plug-in to the Eclipse
IDE to produce syntax trees for source code of
C/C++, was used for producing the abstract syntax
trees.1 It produces abstract syntax trees just like
Recoder, so the concrete syntax trees have also
been created by retaining the tokens.
The Python 2.5 interpreter is actually shipped
with an analyzer that produces concrete syn-
tax trees (using the Python imports from
ast import PyCF ONLY AST and import
parser), which we have utilized for the Python
project above. Hence, no additional processing is
needed in order prepare the concrete syntax trees
as training data.
For the experiments, the source files have been
divided into a training set T and a development
test set D, where the former comprises 80% of the
dependency trees and the latter 10%. The remain-
ing 10% (E) has been left untouched for later use.
The source files have been ordered alphabetically
by the file names including the path. The depen-
dency trees have then been distributed into the data
sets in a pseudo-randomized way. Every tenth de-
pendency tree starting at index 9 (i.e. dependency
trees 9, 19, 29, . . . ) will belong to D, and every
tenth dependency trees starting at index 0 to E.
The remaining trees constitute the training set T .
4.2 Metrics
The standard evaluation metric for parse trees for
natural languages based on context-free grammar
is F-score, the harmonic mean of precision and
recall. F-score compares constituents ? defined
by triples ?i, j,XP ? spanning between terminals
i and j ? derived from the test data with those
derived from the parser. A constituent in the
parser output matches a constituent in the test data
when they span over the same terminals in the
input string. Recall is the ratio of matched con-
stituents over all constituents in the test data. Pre-
cision is the ratio of matched constituents over
all constituents found by the parser. F-score
comes in two versions, one unlabeled (FU ) and
one labeled (FL), where each correct constituent
in the latter also must have the correct nontermi-
nal name (i.e., XP ). The metric is implemented
in Evalb (Collins and Sekine, 2008).
1It is worth noting that CDT failed to produce syntax trees
for 2.2% of these source files, which were consequently ex-
cluded from the experiments. This again indicates the diffi-
cult of parsing C/C++ due to its different dialects.
FL FU
FR LE RI FR LE RI
UL 82.1 93.5 74.6 92.3 97.9 90.6
L 89.7 97.7 80.8 95.8 99.3 92.1
Table 1: F-score for various parser models and
head-finding rules for Java, where FR = FREQ, LE
= LEFT and RI = RIGHT.
The standard evaluation metric measuring accu-
racy for dependency parsing for natural language
is, on the other hand, labeled (ASL) and unlabeled
(ASU ) attachment score. ASU is the ratio of to-
kens attached to its correct head. ASL is the same
as ASU with the additional requirement that the
dependency label should be correct as well.
4.3 Results
This section presents the parsing results. The first
experiment was conducted for Java, using the in-
verse transformation back to syntax trees. Two
feature models are evaluated, one unlexicalized
feature sets (UL) containing 13 parts-of-speech
and 4 dependency label features, and one lexical-
ized feature sets (L) containing all these 17 fea-
tures and 13 additional word form features, de-
veloped by manual feature optimization. Table 1
compares these two feature sets, as well as the dif-
ferent head-finding rules discussed previously.
The figures give a clear answer to the question
whether lexical information is beneficial or not.
Every figure in the row L is higher than its cor-
responding figure in the row UL. This means that
names of variables, methods, classes, etc., actu-
ally contain valuable information for the classifier.
This is in contrast to ordinary syntactic parsing us-
ing a grammar of programming languages where
all names are mapped to the same value (e.g. Iden-
tifier), and, e.g., integer constants to IntLiteral, be-
fore the parse. One potential contributing factor
of the difference is the naming conventions that
programmers normally follow. For example, nam-
ing classes, class attributes and local variables, etc.
using typical methods names, such as equals in
Java, is usually avoided by programmers.
It is just as clear that the choice of head-finding
strategy is very important. For both FL and FU ,
the best choice is with a wide margin LEFT, fol-
lowed by FREQ. RIGHT is consequently the least
accurate one. A higher amount of arcs pointing to
the right seems to be beneficial for the strategy of
55
ASL ASU
FR LE RI FR LE RI
CO 87.6 96.6 86.6 90.9 98.2 90.7
NC 91.0 99.1 89.5 92.1 99.7 90.7
Table 2: Attachment score for Java and the lexical
feature set, where CO = convertible and NC = non-
convertible dependency trees.
Python C/C++
FL FU FL FU
UL 91.5 92.1 95.6 96.4
L 99.1 99.2 96.5 96.9
Table 3: F-score for various parser models and
head-finding rules LEFT for Python and C/C++.
parsing from left to right.
Table 1 can be compared to the accuracy on
the parser output before conversion from depen-
dency trees to syntax trees. This is shown in the
first row (CO) of table 2, where all information
in the complex dependency label is concatenated
and placed in the dependency label. The relation-
ships between the head-finding strategies remain
the same, but it is worth noting that the accuracies
for FREQ and RIGHT are closer to each other, en-
tailing a more difficult conversion to syntax trees
for the latter. The first row can also be compared
to the second row (NC) in the same table, show-
ing the accuracies when training and parsing with
non-convertible dependency trees. One observa-
tion is that each figure in NC is higher than its
corresponding figure in CO (even ASU for RIGHT
with more decimals), probably attributed to the
lower burden on the parser. Both ASU and ASL
are above 99% for the non-convertible dependency
trees using LEFT.
We can see that choosing an appropriate repre-
sentation of syntactic structure to be used during
parsing is just as important for programming lan-
guages as for natural languages, when using data-
driven natural language parsers (Bikel, 2004).
The parser output in table 1 can more eas-
ily be used as input to existing program com-
prehension tools, normally requiring abstract syn-
tax trees. However, the highly accurate output
for LEFT using non-convertible dependency trees
could be worth using instead, but it requires some
additional processing.
In order to investigate the language indepen-
dence of our approach, table 3 contains the cor-
responding figures as in table 1 for Python and
C/C++, restricted to LEFT, which is the best
head-finding strategy for these languages as well.
Again, each lexicalized feature set (L) outper-
forms its corresponding unlexicalized feature set
(UL). Python has higher FL and virtually the same
FU as Java, whereas C/C++ has the lowest accu-
racies for L. However, the UL figures are not far
behind the L figures for C/C++, and C/C++ has
in fact higher FL for UL compared to Java and
Python. These results can maybe be explained by
the fact that C/C++ has less verbose syntax than
both Java and Python, making the lexical features
less informative.
The FL figures for Java, Python and C/C++ us-
ing LEFT can also be compared to the correspond-
ing figures in Nilsson et al (2009). They use the
same data sets but a slightly different head-finding
strategy. Instead of selecting the leftmost element
(terminal or non-terminal) as in LEFT, they always
select the leftmost terminal, resulting in FL=99.5
for Java, FL=98.3 for Python and FL=96.5 for
C/C++. That is, our results are slightly lower for
Java, higher for Python, and slightly higher for
C/C++. The same holds for FU as well. That
is, having only arcs pointing to the right results in
high accuracy for all languages (which is the case
for Left described in section 3), but small devia-
tions from this head-finding strategy can in fact be
beneficial for some languages.
We are not aware of any similar studies for
programming languages2 so we compare the re-
sults to natural language parsing. First, the fig-
ures in table 2 for dependency structure are better
than figures reported for natural languages. Some
natural languages are easier to parse than others,
and the parsing results of the CoNLL shared task
2007 (Nivre et al, 2007) for dependency structure
indicate that English and Catalan are relatively
easy, with ASL around 88-89% and ASU around
90-94% for the best dependency parsers.
Secondly, compared to parsing German with
phrase structure with the same approach as here,
with FU = 81.4 and FL = 78.7%, and Swedish,
with FU = 76.8 and FL = 74.0 (Hall and Nivre,
2A comparative experiment using another data-driven
NLP parser for context-free grammar could be of theoreti-
cal interest. However, fast parsing time is important in pro-
gram comprehension tasks, and data-driven NLP parsers for
context-free grammar have worse than a linear time complex-
ity. As, e.g., the Java project has 838 tokens per source file,
linear time complexity is a prerequisite in practice.
56
Correct Label Parsing Label
66 FieldReference VariableReference
25 VariableReference FieldReference
12 MethodDeclaration LocalVariableDeclaration
9 Conditional FieldReference
5 NotEquals MethodReference
4 Plus MethodReference
4 Positive *
4 LessThan FieldReference
4 GreaterOrEquals FieldReference
4 Divide FieldReference
4 Modulo FieldReference
4 LessOrEquals FieldReference
3 Equals NotEquals
3 LessOrEquals Equals
3 NotEquals Equals
Table 4: Confusion matrix for Java using non-
convertible dependency trees with LEFT, ordered
by descending frequency.
2008), the figures reported in tables 1 and 3 are
also much better. It is however worth noting that
natural languages are more complex and less reg-
ular compared to programming languages. Al-
though it remains to be shown, we conjecture that
these figures are sufficiently high for a large num-
ber of program comprehension tasks.
4.4 Error Analysis
This subsection will study the result for Java with
non-convertible dependency trees (NC) and LEFT,
in order to get a deeper insight into the types of
errors that the parser commits. Specifically, the
labeling mistakes caused by the parser are investi-
gated here. This is done by producing a confusion
matrix based on the dependency labels. That is,
how often does a parser confuse label X with la-
bel Y . This is shown in table 4 for the 15 most
common errors.
The two most frequent errors show that the
parser confuses FieldReference and VariableRef-
erence. A FieldReference refers to a class attribute
whereas a VariableReference could refer to either
an attribute or a local variable. The parser mixes a
reference to a class attribute with a reference that
could also be a local variable or vice versa. The
error is understandable, since the parser obviously
has no knowledge about where the variables are
declared. This is an error that type and name anal-
ysis can easily resolve. On the use-occurrence of a
name (reference), analysis looks up for both pos-
sible define-occurrences of the name (declaration),
first a LocalVariableDeclaration and then a Field-
Declaration. It uses the one that is found first.
Another type of confusion involves declara-
tions, where a MethodDeclaration is misinter-
preted as a LocalVariableDeclaration. This type
of error can be resolved by a simple post-
processing step: a LocalVariableDeclaration fol-
lowed by opening parenthesis (always recognized
correctly) is a MethodDeclaration.
Errors that involve binary operators, e.g., Con-
ditional, NotEqual, Plus, are at rank 4 and below
in the list of the most frequent errors. They are
most likely a result of the incremental left-to-right
parsing strategy. The whole expression should be
labeled in accordance with its binary operator (see
count > 0 in figure 4 for LEFT), but is incor-
rectly labeled as either MethodReference, Field-
Reference or some other operator instead. The ref-
erences actually occur in the left-hand side sub-
expression of the binary operators. This means
that subexpressions and bracketing were recog-
nized correctly, but the type of the top expression
node was mixed up. Extending the lookahead in
the list of remaining input tokens, making it pos-
sible for the classifier in the parser to look at even
more yet unparsed tokens, might be one possible
solution. However, these errors are by and large
relatively harmless anyway. Hence, no correction
is in practice needed.
Figure 5 displays some typical mistakes for the
example program fragment
return (fw.unitIndex == unitIndex &&
fw.unitIndex.equals(unitList));
The parser mixes up a ParenthesizedExpression
with a Conditional, a boolean ParenthesizedEx-
pression only occurring in conditional statements
and expressions. Then it incorrectly assigns the
label Equals to the arc between the first left paren-
thesis and the first fw instead of the correct la-
bel LogicalAnd. It mixes up the type of the whole
expression, an Equals- (i.e., ==) is taken for an
LogicalAnd-expression (i.e., &&). Finally, the two
FieldReferences are taken as more general Vari-
ableReferences, which is corrigible as discussed.
In addition to a possible error correction in a
post-processing step, the parsing errors could dis-
appear due to the abstraction of subsequent anal-
yses as commonly used in software maintenance
tools. For instance, without any error correction,
the type reference graphs of our test program, the
correct one and the one constructed using the not
quite correct parsing results, are identical.
57
Correct:
Parsed:
Figure 5: Typical errors for LEFT using by non-convertible dependency trees.
5 Related Work
Classical parsers for formal languages have been
known for many years. They (conventionally) ac-
cept a context-free language defined by a context-
free grammar. For each program, the parsers
produce a phrase structure referred to as an ab-
stract syntax tree (AST) which is also defined by a
context-free language. Parsers including error sta-
bilization and AST-constructors can be generated
from context-free grammars for parsers (Kastens
et al, 2007). A parser for a new language still
requires the development of a complex specifica-
tion. Moreover, error stabilization often throws
away large parts of the source ? it is robust but
does not care about maximizing accuracy.
Breadth-First Parsing (Ophel, 1997) was de-
signed to provide better error stabilization than tra-
ditional parsers and parser generators. It uses a
two phase approach: the first phase identifies high-
level entities ? the second phase parses the struc-
ture with these entities as root nonterminals (ax-
ioms).
Fuzzy Parsing (Koppler, 1997) was designed
to efficiently develop parsers by performing the
analysis on selected parts of the source instead
of the whole input. It is specified by a set of
(sub)grammars each with their own axioms. The
actual approach is then similar to Breadth-First
Parsing: it scans for instances of the axioms and
then parses according to the grammar. It makes
parsing more robust in the sense that it ignores
source fragments ? including missing parts, errors
and deviations therein ? that subsequent analyses
abstract from anyway. A prominent tool using
the fuzzy parsing approach for information extrac-
tion in reverse-engineering tools is Sniff (Bischof-
berger, 1992) for analyzing C++ code.
Island grammars (Moonen, 2001) generalize on
Fuzzy Parsing. Parsing is controlled by two gram-
mar levels (island and sea) where the sea-level is
used when no island-level production applies. The
island-level corresponds to the sub-grammars of
fuzzy parsing. Island grammars have been applied
in reverse-engineering, specifically, to bank soft-
ware (Moonen, 2002).
Syntactic approximation based on lexical anal-
ysis was developed with the same motivation as
our work: when maintenance tools need syntac-
tic information but the documents could not be
parsed for some reason, hierarchies of regular ex-
pression analyses could be used to approximate
the information with high accuracy (Murphy and
Notkin, 1995; Cox and Clarke, 2003). Their in-
formation extraction approach is characterized as
?lightweight? in the sense that it requires little
specification effort.
A similar robust and light-weight approach for
information extraction constructs XML formats
(JavaML and srcML) from C/C++/Java programs
first, before further processing with XML tools
like Xpath (Badros, 2000; Collard et al, 2003). It
combines lexical and context free analyses. Lex-
ical pattern matching is also used in combination
with context free parsing in order to extract facts
from semi-structured specific comments and con-
58
figuration specifications in frameworks (Knodel
and Pinzger, 2003).
TXL is a rule-based language defining informa-
tion extraction and transformation rules for formal
languages (Cordy et al, 1991). It makes it possible
to incrementally extend the rule base and to adapt
to language dialects and extensions. As the rules
are context-sensitive, TXL goes beyond the lexical
and context-free approaches discussed before.
The fundamental difference of our approach
compared to lexical, context-free, and context-
sensitive approaches (and combinations thereof) is
that we use automated machine learning instead of
manual specification for defining and adapting the
information extraction.
General NLP techniques have been applied for
extracting facts from general source code com-
ments to support software maintenance (Etzkorn
et al, 1999). Comments are extracted from source
code using classical lexical analysis; additional in-
formation is extracted (and then added) with clas-
sical compiler front-end technology.
NLP has also been applied to other informa-
tion extraction tasks in software maintenance to
analyze unstructured or very large information
sources, e.g., for analyzing requirement speci-
fications (Sawyer et al, 2002), in clone detec-
tion (Marcus and Maletic, 2001; Grant and Cordy,
2009), and to connect program documentation to
source code (Marcus and Maletic, 2003).
6 Conclusions and Future Work
In this paper, we applied natural language parsing
techniques to programming languages. One ad-
vantage is that it offers robustness, since it always
produces some output even if the input is incorrect
or incomplete. Completely correct analysis can,
however, not be guaranteed even for correct input.
However, the experiments showed that accuracy is
in fact close to 100%.
In contrast to robust information extractors used
so far for formal languages, the approach pre-
sented here is rapidly adaptable to new languages.
We automatically generate the language specific
information extractor using machine learning and
training of a generic parsing, instead of explicitly
specifying the information extractor using gram-
mar and transformation rules. Also the training
data can be generated automatically. This could
increase the development efficiency of parsers,
since no language specification has to be provided,
only examples.
Regarding accuracy, the experiments showed
that selecting the syntactic base representation
used by the parser internally has a major impact.
Incorporating, for instance, class, method and
variable names in the set of features of the parser
improves the accuracy more than expected. The
detailed error analysis showed that many errors
committed by the parser are forgivable, as they
are anyway abstracted in later processing phases.
Other errors are easily corrigible. We can also
see that the best results presented here are much
higher than the best parsing results for natural lan-
guages.
Besides efficient information extractor develop-
ment, efficient parsing itself is important. Applied
to programs which can easily contain several mil-
lion lines of code, a parser with more than linear
time complexity is not acceptable. The data-driven
parser utilized here has linear parsing time.
These results are only the first (promising) step
towards natural language parsing leveraging infor-
mation extraction for software maintenance. How-
ever, the only way to really evaluate the usefulness
of the approach is to use its output as input to client
analyses, e.g., software measurement and archi-
tecture recovery, which we plan to do in the fu-
ture. Another direction for future work is to apply
the approach to more dialects of C/C++, such as
analyzing correct, incomplete, and erroneous pro-
grams for both standard C and its dialects.
References
Paul Anderson. 2008. 90 % Perspiration: Engineering
Static Analysis Techniques for Industrial Applica-
tions. In Proceedings of the 8th IEEE International
Working Conference on Source Code Analysis and
Manipulation, pages 3?12.
Greg J. Badros. 2000. JavaML: a Markup Language
for Java Source Code. In Proceedings of the 9th
International World Wide Web conference on Com-
puter networks : the international journal of com-
puter and telecommunications networking, pages
159?177.
Daniel M. Bikel. 2004. Intricacies of Collins? Parsing
Model. Computational Linguistics, 30(4):479?511.
Steven Bird, Edward Loper, and Ewan Klein.
2008. Natural Language Toolkit (NLTK) 0.9.5.
http://nltk.org/.
Walter R. Bischofberger. 1992. Sniff: A Pragmatic
Approach to a C++ Programming Environment. In
USENIX C++ Conference, pages 67?82.
59
Chih-Chung Chang and Chih-Jen Lin. 2001. LIB-
SVM: A Library for Support Vector Machines.
Michael L. Collard, Huzefa H. Kagdi, and Jonathan I.
Maletic. 2003. An XML-Based Lightweight C++
Fact Extractor. In 11th IEEE International Work-
shop on Program Comprehension, pages 134?143.
Michael Collins and Satoshi Sekine. 2008. Evalb.
http://nlp.cs.nyu.edu/evalb/.
James R. Cordy, Charles D. Halpern-Hamu, and Eric
Promislow. 1991. TXL: a Rapid Prototyping Sys-
tem for Programming Language Dialects. Computer
Languages, 16(1):97?107.
Anthony Cox and Charles L. A. Clarke. 2003. Syntac-
tic Approximation Using Iterative Lexical Analysis.
In Proceedings of the 11th IEEE International Work-
shop on Program Comprehension, pages 154?163.
Letha H. Etzkorn, Lisa L. Bowen, and Carl G. Davis.
1999. An Approach to Program Understanding by
Natural Language Understanding. Natural Lan-
guage Engineering, 5(3):219?236.
Scott Grant and James R. Cordy. 2009. Vector Space
Analysis of Software Clones. In Proceedings of
the IEEE 17th International Conference on Program
Comprehension, pages 233?237.
Tobias Gutzmann, Dirk Heuzeroth, and Mircea Trifu.
2007. Recoder 0.83. http://recoder.sourceforge.net/.
Johan Hall and Joakim Nivre. 2008. Parsing Discon-
tinuous Phrase Structure with Grammatical Func-
tions. In Proceedings of GoTAL, pages 169?180.
Johan Hall, Joakim Nivre, and Jens Nilsson. 2006.
Discriminative Classifiers for Deterministic Depen-
dency Parsing. In Proceedings of COLING-ACL,
pages 316?323.
Uwe Kastens, Anthony M. Sloane, and William M.
Waite. 2007. Generating Software from Specifica-
tions. Jones and Bartlett Publishers.
Jens Knodel and Martin Pinzger. 2003. Improving
Fact Extraction of Framework-Based Software Sys-
tems. In Proceedings of 10th Working Conference
on Reverse Engineering, pages 186?195.
Rainer Koppler. 1997. A Systematic Approach to
Fuzzy Parsing. Software - Practice and Experience,
27(6):637?649.
David M. Magerman. 1995. Statistical Decision-tree
Models for Parsing. In Proceedings of ACL, pages
276?283.
Andrian Marcus and Jonathan I. Maletic. 2001. Iden-
tification of High-Level Concept Clones in Source
Code. In Proceedings of the 16th IEEE interna-
tional conference on Automated software engineer-
ing, page 107.
Andrian Marcus and Jonathan I. Maletic. 2003. Re-
covering Documentation-to-Source-Code Traceabil-
ity Links using Latent Semantic Indexing. In Pro-
ceedings of the 25th International Conference on
Software Engineering, pages 125?135.
Scott McPeak. 2005. Elsa: The
Elkhound-based C/C++ Parser.
http://www.cs.berkeley.edu/?smcpeak.
Leon Moonen. 2001. Generating Robust Parsers using
Island Grammars. In Proceedings of the 8th Work-
ing Conference on Reverse Engineering, pages 13?
22.
Leon Moonen. 2002. Lightweight Impact Analysis us-
ing Island Grammars. In Proceedings of the 10th In-
ternational Workshop on Program Comprehension,
pages 219?228.
Gail C. Murphy and David Notkin. 1995. Lightweight
Source Model Extraction. SIGSOFT Software Engi-
neering Notes, 20(4):116?127.
Jens Nilsson, Welf Lo?we, Johan Hall, and Joakim
Nivre. 2009. Natural Language Parsing for Fact Ex-
traction from Source Code. In Proceedings of 17th
IEEE International Conference on Program Com-
prehension, pages 223?227.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2004.
Memory-based Dependency Parsing. In Proceed-
ings of CoNLL, pages 49?56.
Joakim Nivre, Johan Hall, and Jens Nilsson. 2006.
MaltParser: A Data-Driven Parser-Generator for
Dependency Parsing. In Proceedings of LREC,
pages 2216?2219.
Joakim Nivre, Johan Hall, Sanda Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007. The CoNLL 2007 Shared Task on De-
pendency Parsing. In Proceedings of CoNLL/ACL,
pages 915?932.
Joakim Nivre. 2003. An Efficient Algorithm for
Projective Dependency Parsing. In Proceedings of
IWPT, pages 149?160.
John Ophel. 1997. Breadth-First
Parsing. citeseerx.ist.psu.edu/view-
doc/summary?doi=10.1.1.50.3035.
Pete Sawyer, Paul Rayson, and Roger Garside. 2002.
REVERE: Support for Requirements Synthesis
from Documents. Information Systems Frontiers,
4(11):343?353.
Dennis Strein, Ru?diger Lincke, Jonas Lundberg, and
Welf Lo?we. 2007. An Extensible Meta-Model for
Program Analysis. IEEE Transactions on Software
Engineering, 33(9):592?607.
60
