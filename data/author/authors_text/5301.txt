Improved Word Alignment Using a Symmetric Lexicon Model
Richard Zens and Evgeny Matusov and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{zens,matusov,ney}@cs.rwth-aachen.de
Abstract
Word-aligned bilingual corpora are an
important knowledge source for many
tasks in natural language processing. We
improve the well-known IBM alignment
models, as well as the Hidden-Markov
alignment model using a symmetric lex-
icon model. This symmetrization takes
not only the standard translation direc-
tion from source to target into account,
but also the inverse translation direction
from target to source. We present a the-
oretically sound derivation of these tech-
niques. In addition to the symmetriza-
tion, we introduce a smoothed lexicon
model. The standard lexicon model is
based on full-form words only. We propose
a lexicon smoothing method that takes
the word base forms explicitly into ac-
count. Therefore, it is especially useful
for highly inflected languages such as Ger-
man. We evaluate these methods on the
German?English Verbmobil task and the
French?English Canadian Hansards task.
We show statistically significant improve-
ments of the alignment quality compared
to the best system reported so far. For
the Canadian Hansards task, we achieve
an improvement of more than 30% rela-
tive.
1 Introduction
Word-aligned bilingual corpora are an impor-
tant knowledge source for many tasks in nat-
ural language processing. Obvious applica-
tions are the extraction of bilingual word or
phrase lexica (Melamed, 2000; Och and Ney,
2000). These applications depend heavily on
the quality of the word alignment (Och and
Ney, 2000). Word alignment models were first
introduced in statistical machine translation
(Brown et al, 1993). The alignment describes
the mapping from source sentence words to
target sentence words.
Using the IBM translation models IBM-1
to IBM-5 (Brown et al, 1993), as well as
the Hidden-Markov alignment model (Vogel
et al, 1996), we can produce alignments of
good quality. In (Och and Ney, 2003), it is
shown that the statistical approach performs
very well compared to alternative approaches,
e.g. based on the Dice coefficient or the com-
petitive linking algorithm (Melamed, 2000).
A central component of the statistical trans-
lation models is the lexicon. It models the
word translation probabilities. The standard
training procedure of the statistical models
uses the EM algorithm. Typically, the models
are trained for one translation direction only.
Here, we will perform a simultaneous training
of both translation directions, source-to-target
and target-to-source. After each iteration of
the EM algorithm, we combine the two lexica
to a symmetric lexicon. This symmetric lex-
icon is then used in the next iteration of the
EM algorithm for both translation directions.
We will propose and justify linear and loglin-
ear interpolation methods.
Statistical methods often suffer from the
data sparseness problem. In our case, many
words in the bilingual sentence-aligned texts
are singletons, i.e. they occur only once. This
is especially true for the highly inflected lan-
guages such as German. It is hard to obtain
reliable estimations of the translation proba-
bilities for these rarely occurring words. To
overcome this problem (at least partially), we
will smooth the lexicon probabilities of the
full-form words using a probability distribu-
tion that is estimated using the word base
forms. Thus, we exploit that multiple full-
form words share the same base form and have
similar meanings and translations.
We will evaluate these methods on the
German?English Verbmobil task and the
French?English Canadian Hansards task. We
will show statistically significant improve-
ments compared to state-of-the-art results in
(Och and Ney, 2003). On the Canadian
Hansards task, the symmetrization methods
will result in an improvement of more than
30% relative.
2 Statistical Word Alignment Models
In this section, we will give a short description
of the commonly used statistical word align-
ment models. These alignment models stem
from the source-channel approach to statisti-
cal machine translation (Brown et al, 1993).
We are given a source language sentence fJ1 :=
f1...fj ...fJ which has to be translated into
a target language sentence eI1 := e1...ei...eI .
Among all possible target language sentences,
we will choose the sentence with the highest
probability:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources allows for an independent modeling of
target language model Pr(eI1) and translation
model Pr(fJ1 |eI1). Into the translation model,
the word alignment A is introduced as a hid-
den variable:
Pr(fJ1 |eI1) =
?
A
Pr(fJ1 , A|eI1)
Usually, we use restricted alignments in the
sense that each source word is aligned to at
most one target word, i.e. A = aJ1 . A de-
tailed description of the popular translation
models IBM-1 to IBM-5 (Brown et al, 1993),
as well as the Hidden-Markov alignment model
(HMM) (Vogel et al, 1996) can be found in
(Och and Ney, 2003). All these models include
parameters p(f |e) for the single-word based
lexicon. They differ in the alignment model.
A Viterbi alignment A? of a specific model is
an alignment for which the following equation
holds:
A? = argmax
A
{Pr(fJ1 , A|eI1)
}
We measure the quality of an alignment model
using the quality of the Viterbi alignment com-
pared to a manually produced reference align-
ment.
In Section 3, we will apply the lexicon sym-
metrization methods to the models described
previously. Therefore, we will now sketch the
standard training procedure for the lexicon
model. The EM algorithm is used to train
the free lexicon parameters p(f |e).
In the E-step, the lexical counts for each
sentence pair (fJ1 , eI1) are calculated and then
summed over all sentence pairs in the training
corpus:
N(f, e) =
?
(fJ1 ,eI1)
?
aJ1
p(aJ1 |fJ1 , eI1)
?
i,j
?(f, fj)?(e, ei)
In the M-step the lexicon probabilities are:
p(f |e) = N(f, e)?
f?
N(f? , e)
3 Symmetrized Lexicon Model
During the standard training procedure, the
lexicon parameters p(f |e) and p(e|f) were es-
timated independent of each other in strictly
separate trainings. In this section, we present
two symmetrization methods for the lexicon
model. As a starting point, we use the
joint lexicon probability p(f, e) and determine
the conditional probabilities for the source-
to-target direction p(f |e) and the target-to-
source direction p(e|f) as the corresponding
marginal distribution:
p(f |e) = p(f, e)?
f?
p(f? , e) (1)
p(e|f) = p(f, e)?
e?
p(f, e?) (2)
The nonsymmetric auxiliary Q-functions for
reestimating the lexicon probabilities during
the EM algorithm can be represented as fol-
lows. Here, NST (f, e) and NTS(f, e) denote
the lexicon counts for the source-to-target
(ST ) direction and the target-to-source (TS)
direction, respectively.
QST ({p(f |e)}) =
?
f,e
NST (f, e) ? log p(f, e)?
f?
p(f? , e)
QTS({p(e|f)}) =
?
f,e
NTS(f, e) ? log p(f, e)?
e?
p(f, e?)
3.1 Linear Interpolation
To estimate the joint probability using the EM
algorithm, we define the auxiliary Q-function
as a linear interpolation of the Q-functions for
the source-to-target and the target-to-source
direction:
Q?({p(f, e)}) = ? ?QST ({p(f |e)})
+(1? ?) ?QTS({p(e|f)})
= ? ?
?
f,e
NST (f, e) ? log p(f, e)
+(1? ?) ?
?
f,e
NTS(f, e) ? log p(f, e)
?? ?
?
e
NST (e) ? log
?
f?
p(f? , e)
?(1? ?) ?
?
f
NTS(f) ? log
?
e?
p(f, e?)
The unigram counts N(e) and N(f) are deter-
mined, for each of the two translation direc-
tions, by taking a sum of N(f, e) over f and
over e, respectively. We define the combined
lexicon count N?(f, e):
N?(f, e) := ? ?NST (f, e) + (1? ?) ?NTS(f, e)
Now, we derive the symmetrized Q-function
over p(f, e) for a certain word pair (f, e).
Then, we set this derivative to zero to deter-
mine the reestimation formula for p(f, e) and
obtain the following equation:
N?(f, e)
p(f, e) = ? ?
NST (e)?
f?
p(f? , e) + (1? ?) ?
NTS(f)?
e?
p(f, e?)
We do not know a closed form solution for this
equation. As an approximation, we use the
following term:
p?(f, e) = N?(f, e)?
f? ,e?
N?(f? , e?)
This estimate is an exact solution, if the uni-
gram counts for f and e are independent of the
translation direction, i. e. NST (f) = NTS(f)
and NST (e) = NTS(e). We make this approx-
imation and thus we interpolate the lexicon
counts linear after each iteration of the EM
algorithm. Then, we normalize these counts
(according to Equations 1 and 2) to determine
the lexicon probabilities for each of the two
translation directions.
3.2 Loglinear Interpolation
We will show in Section 5 that the linear in-
terpolation results in significant improvements
over the nonsymmetric system. Motivated by
these experiments, we investigated also the
loglinear interpolation of the lexicon counts of
the two translation directions. The combined
lexicon count N?(f, e) is now defined as:
N?(f, e) = NST (f, e)? ?NTS(f, e)1??
The normalization is done in the same way as
for the linear interpolation. The linear inter-
polation resembles more a union of the two lex-
ica whereas the loglinear interpolation is more
similar to an intersection of both lexica. Thus
for the linear interpolation, a word pair (f, e)
obtains a large combined count, if the count in
at least one direction is large. For the loglin-
ear interpolation, the combined count is large
only if both lexicon counts are large.
In the experiments, we will use the interpo-
lation weight ? = 0.5 for both the linear and
the loglinear interpolation, i. e. both transla-
tion directions are weighted equally.
3.3 Evidence Trimming
Initially, the lexicon contains all word pairs
that cooccur in the bilingual training corpus.
The majority of these word pairs are not trans-
lations of each other. Therefore, we would
like to remove those lexicon entries. Evidence
trimming is one way to do this. The evidence
of a word pair (f, e) is the estimated count
N(f, e). Now, we discard a word pair if its ev-
idence is below a certain threshold ? .1 In the
case of the symmetric lexicon, we can further
refine this method. For estimating the lex-
icon in the source-to-target direction p?(f |e),
the idea is to keep all entries from this di-
rection and to boost the entries that have a
high evidence in the target-to-source direction
NTS(f, e). We obtain the following formula:
N?ST (f, e) =
?
?
?
?NST (f, e) + (1? ?)NTS(f, e)
if NST (f, e) > ?
0 else
The count N?ST (f, e) is now used to estimate
the source-to-target lexicon p?(f |e). With this
method, we do not keep entries in the source-
to-target lexicon p?(f |e) if their evidence is low,
even if their evidence in the target-to-source
1Actually, there is always implicit evidence trim-
ming caused by the limited machine precision.
direction NTS(f, e) is high. For the target-to-
source direction, we apply this method in a
similar way.
4 Lexicon Smoothing
The lexicon model described so far is based on
full-form words. For highly inflected languages
such as German this might cause problems,
because many full-form words occur only a few
times in the training corpus. Compared to En-
glish, the token/type ratio for German is usu-
ally much lower (e.g. Verbmobil: English 99.4,
German 56.3). The information that multiple
full-form words share the same base form is
not used in the lexicon model. To take this in-
formation into account, we smooth the lexicon
model with a backing-off lexicon that is based
on word base forms. The smoothing method
we apply is absolute discounting with interpo-
lation:
p(f |e) = max {N(f, e)? d, 0}N(e) + ?(e) ? ?(f, e?)
This method is well known from language
modeling (Ney et al, 1997). Here, e? de-
notes the generalization, i.e. the base form,
of the word e. The nonnegative value d is
the discounting parameter, ?(e) is a normal-
ization constant and ?(f, e?) is the normalized
backing-off distribution.
The formula for ?(e) is:
?(e) = 1N(e)
?
? ?
f :N(f,e)>d
d+
?
f :N(f,e)?d
N(f, e)
?
?
= 1N(e)
?
f
min{d,N(f, e)}
This formula is a generalization of the one
typically used in publications on language
modeling. This generalization is necessary,
because the lexicon counts may be fractional
whereas in language modeling typically inte-
ger counts are used. Additionally, we want
to allow for discounting values d greater than
one. The backing-off distribution ?(f, e?) is es-
timated using relative frequencies:
?(f, e?) = N(f, e?)?
f?
N(f? , e?)
Here, N(f, e?) denotes the count of the event
that the source language word f and the target
language base form e? occur together. These
counts are computed by summing the lexicon
counts N(f, e) over all full-form words e which
share the same base form e?.
5 Results
5.1 Evaluation Criteria
We use the same evaluation criterion as de-
scribed in (Och and Ney, 2000). The gen-
erated word alignment is compared to a ref-
erence alignment which is produced by hu-
man experts. The annotation scheme explic-
itly takes the ambiguity of the word alignment
into account. There are two different kinds
of alignments: sure alignments (S) which are
used for alignments that are unambiguous and
possible alignments (P ) which are used for
alignments that might or might not exist. The
P relation is used especially to align words
within idiomatic expressions, free translations,
and missing function words. It is guaranteed
that the sure alignments are a subset of the
possible alignments (S ? P ). The obtained
reference alignment may contain many-to-one
and one-to-many relationships.
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER), which is derived from the
well-known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER(S, P ;A) = 1? |A ? S|+ |A ? P ||A|+ |S|
With these definitions a recall error can only
occur if a S(ure) alignment is not found and a
precision error can only occur if a found align-
ment is not even P (ossible).
5.2 Experimental Setup
We evaluated the presented lexicon sym-
metrization methods on the Verbmobil and
the Canadian Hansards task. The German?
English Verbmobil task (Wahlster, 2000) is a
speech translation task in the domain of ap-
pointment scheduling, travel planning and ho-
tel reservation. The French?English Canadian
Hansards task consists of the debates in the
Canadian Parliament.
The corpus statistics are shown in Table 1
and Table 2. The number of running words
and the vocabularies are based on full-form
words including punctuation marks. As in
Table 1: Verbmobil: Corpus statistics.
German English
Train Sentences 34K
Words 329 625 343 076
Vocabulary 5 936 3 505
Singletons 2 600 1 305
Test Sentences 354
Words 3 233 3 109
Table 2: Canadian Hansards: Corpus statistics.
French English
Train Sentences 128K
Words 2.12M 1.93M
Vocabulary 37 542 29 414
Singletons 12 986 9 572
Test Sentences 500
Words 8 749 7 946
(Och and Ney, 2003), the first 100 sentences
of the test corpus are used as a development
corpus to optimize model parameters that are
not trained via the EM algorithm, e.g. the
discounting parameter for lexicon smoothing.
The remaining part of the test corpus is used
to evaluate the models.
We use the same training schemes (model
sequences) as presented in (Och and Ney,
2003). As we use the same training and test-
ing conditions as (Och and Ney, 2003), we will
refer to the results presented in that article as
the baseline results. In (Och and Ney, 2003),
the alignment quality of statistical models is
compared to alternative approaches, e.g. us-
ing the Dice coefficient or the competitive
linking algorithm. The statistical approach
showed the best performance and therefore we
report only the results for the statistical sys-
tems.
5.3 Lexicon Symmetrization
In Table 3 and Table 4, we present the follow-
ing experiments performed for both the Verb-
mobil and the Canadian Hansards task:
? Base: the system taken from (Och and
Ney, 2003) that we use as baseline system.
? Lin.: symmetrized lexicon using a lin-
ear interpolation of the lexicon counts af-
ter each training iteration as described in
Section 3.1.
? Log.: symmetrized lexicon using a log-
linear interpolation of the lexicon counts
after each training iteration as described
in Section 3.2.
Table 3: Comparison of alignment perfor-
mance for the Verbmobil task (S?T: source-
to-target direction, T?S: target-to-source di-
rection; all numbers in percent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 93.5 95.3 5.7 91.4 88.7 9.9
Lin. 96.0 95.4 4.3 93.7 89.6 8.2
Log. 93.6 95.6 5.5 94.5 89.4 7.9
 4
 6
 8
 10
 12
 14
 16
 18
 100  1000  10000  100000
A
E
R
Corpus Size
baselinelinearloglinear
Figure 1: AER[%] of different alignment meth-
ods as a function of the training corpus size
for the Verbmobil task (source-to-target direc-
tion).
In Table 3, we compare both interpolation
variants for the Verbmobil task to (Och and
Ney, 2003). We observe notable improvements
in the alignment error rate using the linear in-
terpolation. For the translation direction from
German to English (S?T), an improvement of
about 25% relative is achieved from an align-
ment error rate of 5.7% for the baseline system
to 4.3% using the linear interpolation. Per-
forming the loglinear interpolation, we observe
a substantial reduction of the alignment error
rate as well. The two symmetrization methods
improve both precision and recall of the result-
ing Viterbi alignment in both translation di-
rections for the Verbmobil task. The improve-
ments with the linear interpolation is for both
translation directions statistically significant
at the 99% level. For the loglinear interpo-
lation, the target-to-source translation direc-
tion is statistically significant at the 99% level.
The statistical significance test were done us-
ing boostrap resampling.
We also performed experiments on sub-
corpora of different sizes. For the Verbmo-
bil task, the results are illustrated in Figure 1.
Table 4: Comparison of alignment perfor-
mance for the Canadian Hansards task (S?T:
source-to-target direction, T?S: target-to-
source direction; all numbers in percent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 85.4 90.6 12.6 85.6 90.9 12.4
Lin. 89.3 91.4 9.9 89.0 92.0 9.8
Log. 91.0 92.0 8.6 91.2 92.1 8.4
We observe that both symmetrization variants
result in improvements for all corpus sizes.
With increasing training corpus size the per-
formance of the linear interpolation becomes
superior to the performance of the loglinear
interpolation.
In Table 4, we compare the symmetriza-
tion methods with the baseline system for the
Canadian Hansards task. Here, the loglin-
ear interpolation performs best. We achieve
a relative improvement over the baseline of
more than 30% for both translation directions.
For instance, the alignment error rate for the
translation direction from French to English
(S?T) improves from 12.6% for the baseline
system to 8.6% for the symmetrized system
with loglinear interpolation. Again, the two
symmetrization methods improve both preci-
sion and recall of the Viterbi alignment.
For the Canadian Hansards task, all the im-
provements of the alignment error rate are sta-
tistically significant at the 99% level.
5.4 Generalized Alignments
In (Och and Ney, 2003) generalized alignments
are used, thus the final Viterbi alignments of
both translation directions are combined us-
ing some heuristic. Experimentally, the best
heuristic for the Canadian Hansards task is
the intersection. For the Verbmobil task, the
refined method of (Och and Ney, 2003) is
used. The results are summarized in Table 5.
We see that both the linear and the loglinear
lexicon symmetrization methods yield an im-
provement with respect to the alignment error
rate. For the Verbmobil task, the improve-
ment with the loglinear interpolation is sta-
tistically significant at the 99% level. For the
Canadian Hansards task, both lexicon sym-
metrization methods result in statistically sig-
nificant improvements at the 95% level. Addi-
tionally, we observe that precision and recall
are more balanced for the symmetrized lexicon
variants, especially for the Canadian Hansards
Table 6: Effect of smoothing the lexicon prob-
abilities on the alignment performance for the
Verbmobil task (S?T: source-to-target direc-
tion, smooth English; T?S: target-to-source
direction, smooth German; all numbers in per-
cent).
S?T T?S
Pre. Rec. AER Pre. Rec. AER
Base 93.5 95.3 5.7 91.4 88.7 9.9
smooth 94.8 94.8 5.2 93.4 88.2 9.1
task.
5.5 Lexicon Smoothing
In Table 6, we present the results for the lex-
icon smoothing as described in Section 4 on
the Verbmobil corpus2. As expected, a no-
table improvement in the AER is reached if
the lexicon smoothing is performed for Ger-
man (i.e. for the target-to-source direction),
because many full-form words with the same
base form are present in this language. These
improvements are statistically significant at
the 95% level.
6 Related Work
The popular IBM models for statistical ma-
chine translation are described in (Brown et
al., 1993). The HMM-based alignment model
was introduced in (Vogel et al, 1996). A
good overview of these models is given in
(Och and Ney, 2003). In that article Model
6 is introduced as the loglinear interpolation
of the other models. Additionally, state-of-
the-art results are presented for the Verbmo-
bil task and the Canadian Hansards task for
various configurations. Therefore, we chose
them as baseline. Compared to our work,
these publications kept the training of the
two translation directions strictly separate
whereas we integrate both directions into one
symmetrized training. Additional linguistic
knowledge sources such as dependency trees
or parse trees were used in (Cherry and Lin,
2003) and (Gildea, 2003). In (Cherry and
Lin, 2003) a probability model Pr(aJ1 |fJ1 , eI1) is
used, which is symmetric per definition. Bilin-
gual bracketing methods were used to produce
a word alignment in (Wu, 1997). (Melamed,
2000) uses an alignment model that enforces
one-to-one alignments for nonempty words. In
2The base forms were determined using LingSoft
tools.
Table 5: Effect of different lexicon symmetrization methods on alignment performance for the
generalized alignments for the Verbmobil task and the Canadian Hansards task.
task: Verbmobil Canadian Hansards
Precision[%] Recall[%] AER[%] Precision[%] Recall[%] AER[%]
Base 93.3 96.0 5.5 96.6 86.0 8.2
Lin. 96.1 94.0 4.9 95.2 88.5 7.7
Loglin. 95.2 95.3 4.7 93.6 90.8 7.5
(Toutanova et al, 2002), extensions to the
HMM-based alignment model are presented.
7 Conclusions
We have addressed the task of automatically
generating word alignments for bilingual cor-
pora. This problem is of great importance for
many tasks in natural language processing, es-
pecially in the field of machine translation.
We have presented lexicon symmetrization
methods for statistical alignment models that
are trained using the EM algorithm, in par-
ticular the five IBM models, the HMM and
Model 6. We have evaluated these meth-
ods on the Verbmobil task and the Cana-
dian Hansards task and compared our results
to the state-of-the-art system of (Och and
Ney, 2003). We have shown that both the
linear and the loglinear interpolation of lexi-
con counts after each iteration of the EM al-
gorithm result in statistically significant im-
provements of the alignment quality. For the
Canadian Hansards task, the AER improved
by about 30% relative; for the Verbmobil task
the improvement was about 25% relative.
Additionally, we have described lexicon
smoothing using the word base forms. Es-
pecially for highly inflected languages such as
German, this smoothing resulted in statisti-
cally significant improvements.
In the future, we plan to optimize the inter-
polation weights to balance the two transla-
tion directions. We will also investigate the
possibility of generating directly an uncon-
strained alignment based on the symmetrized
lexicon probabilities.
Acknowledgment
This work has been partially funded by the
EU project LC-Star, IST-2001-32216.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, June.
C. Cherry and D. Lin. 2003. A probability model
to improve word alignment. In Proc. of the 41th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 88?95, Sap-
poro, Japan, July.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of the 41th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 80?87, Sapporo,
Japan, July.
I. D. Melamed. 2000. Models of translational
equivalence among words. Computational Lin-
guistics, 26(2):221?249.
H. Ney, S. Martin, and F. Wessel. 1997. Statisti-
cal language modeling using leaving-one-out. In
S. Young and G. Bloothooft, editors, Corpus-
Based Methods in Language and Speech Process-
ing, pages 174?207. Kluwer.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 440?447, Hong Kong,
October.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
K. Toutanova, H. T. Ilhan, and C. D. Manning.
2002. Extensions to hmm-based statistical word
alignment models. In Proc. Conf. on Empirical
Methods for Natural Language Processing, pages
87?94, Philadelphia, PA, July.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
W. Wahlster, editor. 2000. Verbmobil: Founda-
tions of speech-to-speech translations. Springer
Verlag, Berlin, Germany, July.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
Symmetric Word Alignments for Statistical Machine Translation
Evgeny Matusov and Richard Zens and Hermann Ney
Lehrstuhl fu?r Informatik VI, Computer Science Department
RWTH Aachen University
D-52056 Aachen, Germany
{matusov,zens,ney}@cs.rwth-aachen.de
Abstract
In this paper, we address the word
alignment problem for statistical machine
translation. We aim at creating a sym-
metric word alignment allowing for reli-
able one-to-many and many-to-one word
relationships. We perform the iterative
alignment training in the source-to-target
and the target-to-source direction with
the well-known IBM and HMM alignment
models. Using these models, we robustly
estimate the local costs of aligning a source
word and a target word in each sentence
pair. Then, we use efficient graph algo-
rithms to determine the symmetric align-
ment with minimal total costs (i. e. max-
imal alignment probability). We evalu-
ate the automatic alignments created in
this way on the German?English Verb-
mobil task and the French?English Cana-
dian Hansards task. We show statistically
significant improvements of the alignment
quality compared to the best results re-
ported so far. On the Verbmobil task,
we achieve an improvement of more than
1% absolute over the baseline error rate of
4.7%.
1 Introduction
Word-aligned bilingual corpora provide im-
portant knowledge for many natural language
processing tasks, such as the extraction of
bilingual word or phrase lexica (Melamed,
2000; Och and Ney, 2000). The solutions of
these problems depend heavily on the quality
of the word alignment (Och and Ney, 2000).
Word alignment models were first introduced
in statistical machine translation (Brown et
al., 1993). An alignment describes a mapping
from source sentence words to target sentence
words.
Using the IBM translation models IBM-1
to IBM-5 (Brown et al, 1993), as well as
the Hidden-Markov alignment model (Vogel et
al., 1996), we can produce alignments of good
quality. However, all these models constrain
the alignments so that a source word can be
aligned to at most one target word. This con-
straint is useful to reduce the computational
complexity of the model training, but makes
it hard to align phrases in the target lan-
guage (English) such as ?the day after tomor-
row? to one word in the source language (Ger-
man) ?u?bermorgen?. We will present a word
alignment algorithm which avoids this con-
straint and produces symmetric word align-
ments. This algorithm considers the align-
ment problem as a task of finding the edge
cover with minimal costs in a bipartite graph.
The parameters of the IBM models and HMM,
in particular the state occupation probabili-
ties, will be used to determine the costs of
aligning a specific source word to a target
word.
We will evaluate the suggested alignment
methods on the German?English Verbmo-
bil task and the French?English Canadian
Hansards task. We will show statistically sig-
nificant improvements compared to state-of-
the-art results in (Och and Ney, 2003).
2 Statistical Word Alignment Models
In this section, we will give an overview of
the commonly used statistical word alignment
techniques. They are based on the source-
channel approach to statistical machine trans-
lation (Brown et al, 1993). We are given
a source language sentence fJ1 := f1...fj ...fJwhich has to be translated into a target lan-
guage sentence eI1 := e1...ei...eI . Among allpossible target language sentences, we will
choose the sentence with the highest proba-
bility:
e?I1 = argmax
eI1
{Pr(eI1|fJ1 )
}
= argmax
eI1
{Pr(eI1) ? Pr(fJ1 |eI1)
}
This decomposition into two knowledge
sources allows for an independent modeling of
target language model Pr(eI1) and translation
model Pr(fJ1 |eI1). Into the translation model,the word alignment A is introduced as a hid-
den variable:
Pr(fJ1 |eI1) =
?
A
Pr(fJ1 , A|eI1)
Usually, the alignment is restricted in the
sense that each source word is aligned to at
most one target word, i.e. A = aJ1 . The align-ment may contain the connection aj = 0 with
the ?empty? word e0 to account for source sen-
tence words that are not aligned to any tar-
get word at all. A detailed description of the
popular translation/alignment models IBM-1
to IBM-5 (Brown et al, 1993), as well as the
Hidden-Markov alignment model (HMM) (Vo-
gel et al, 1996) can be found in (Och and Ney,
2003). Model 6 is a loglinear combination of
the IBM-4, IBM-1, and the HMM alignment
models.
A Viterbi alignment A? of a specific model is
an alignment for which the following equation
holds:
A? = argmax
A
{Pr(fJ1 , A|eI1)
} .
3 State Occupation Probabilities
The training of all alignment models is done
using the EM-algorithm. In the E-step, the
counts for each sentence pair (fJ1 , eI1) are cal-culated. Here, we present this calculation on
the example of the HMM. For its lexicon pa-
rameters, the marginal probability of a target
word ei to occur at the target sentence posi-
tion i as the translation of the source word fj
at the source sentence position j is estimated
with the following sum:
pj(i, fJ1 |eI1) =
?
aJ1 :aj=i
Pr(fJ1 , aJ1 |eI1)
This value represents the likelihood of aligning
fj to ei via every possible alignment A = aJ1that includes the alignment connection aj = i.
By normalizing over the target sentence posi-
tions, we arrive at the state occupation proba-
bility :
pj(i|fJ1 , eI1) =
pj(i, fJ1 |eI1)
I?
i?=1
pj(i?, fJ1 |eI1)
In the M-step of the EM training, the state
occupation probabilities are aggregated for all
words in the source and target vocabularies
by taking the sum over all training sentence
pairs. After proper renormalization the lexi-
con probabilities p(f |e) are determined.
Similarly, the training can be performed
in the inverse (target-to-source) direction,
yielding the state occupation probabilities
pi(j|eI1, fJ1 ).The negated logarithms of the state occu-
pation probabilities
w(i, j; fJ1 , eI1) := ? log pj(i|fJ1 , eI1) (1)
can be viewed as costs of aligning the source
word fj with the target word ei. Thus, the
word alignment task can be formulated as the
task of finding a mapping between the source
and the target words, so that each source and
each target position is covered and the total
costs of the alignment are minimal.
Using state occupation probabilities for
word alignment modeling results in a num-
ber of advantages. First of all, in calculation
of these probabilities with the models IBM-1,
IBM-2 and HMM the EM-algorithm is per-
formed exact, i.e. the summation over all
alignments is efficiently performed in the E-
step. For the HMM this is done using the
Baum-Welch algorithm (Baum, 1972). So far,
an efficient algorithm to compute the sum over
all alignments in the fertility models IBM-3
to IBM-5 is not known. Therefore, this sum
is approximated using a subset of promising
alignments (Och and Ney, 2000). In both
cases, the resulting estimates are more pre-
cise than the ones obtained by the maximum
approximation, i. e. by considering only the
Viterbi alignment.
Instead of using the state occupation prob-
abilities from only one training direction as
costs (Equation 1), we can interpolate the
state occupation probabilities from the source-
to-target and the target-to-source training for
each pair (i,j) of positions in a sentence pair
(fJ1 , eI1). This will improve the estimation ofthe local alignment costs. Having such sym-
metrized costs, we can employ the graph align-
ment algorithms (cf. Section 4) to produce
reliable alignment connections which include
many-to-one and one-to-many alignment re-
lationships. The presence of both relation-
ship types characterizes a symmetric align-
ment that can potentially improve the trans-
lation results (Figure 1 shows an example of a
symmetric alignment).
Another important advantage is the effi-
ciency of the graph algorithms used to deter-
Figure 1: Example of a symmetric alignment
with one-to-many and many-to-one connec-
tions (Verbmobil task, spontaneous speech).
mine the final symmetric alignment. They will
be discussed in Section 4.
4 Alignment Algorithms
In this section, we describe the alignment ex-
traction algorithms. We assume that for each
sentence pair (fJ1 , eI1) we are given a cost ma-trix C.1 The elements of this matrix cij are
the local costs that result from aligning source
word fj to target word ei. For a given align-
ment A ? I ? J , we define the costs of this
alignment c(A) as the sum of the local costs
of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (2)
Now, our task is to find the alignment with the
minimum costs. Obviously, the empty align-
ment has always costs of zero and would be op-
timal. To avoid this, we introduce additional
constraints. The first constraint is source sen-
tence coverage. Thus each source word has
to be aligned to at least one target word or
alternatively to the empty word. The second
constraint is target sentence coverage. Similar
to the source sentence coverage thus each tar-
get word is aligned to at least one source word
or the empty word.
Enforcing only the source sentence cover-
age, the minimum cost alignment is a mapping
from source positions j to target positions aj ,
including zero for the empty word. Each tar-
get position aj can be computed as:
aj = argmin
i
{cij}
This means, in each column we choose the
row with the minimum costs. This method re-
sembles the common IBM models in the sense
1For notational convenience, we omit the depen-
dency on the sentence pair (fJ1 , eI1) in this section.
that the IBM models are also a mapping from
source positions to target positions. There-
fore, this method is comparable to the IBM
models for the source-to-target direction. Sim-
ilarly, if we enforce only the target sentence
coverage, the minimum cost alignment is a
mapping from target positions i to source po-
sitions bi. Here, we have to choose in each
row the column with the minimum costs. The
complexity of these algorithms is in O(I ? J).
The algorithms for determining such a non-
symmetric alignment are rather simple. A
more interesting case arises, if we enforce both
constraints, i.e. each source word as well as
each target word has to be aligned at least
once. Even in this case, we can find the global
optimum in polynomial time.
The task is to find a symmetric alignment
A, for which the costs c(A) are minimal (Equa-
tion 2). This task is equivalent to finding
a minimum-weight edge cover (MWEC) in a
complete bipartite graph2. The two node
sets of this bipartite graph correspond to the
source sentence positions and the target sen-
tence positions, respectively. The costs of an
edge are the elements of the cost matrix C.
To solve the minimum-weight edge cover
problem, we reduce it to the maximum-weight
bipartite matching problem. As described
in (Keijsper and Pendavingh, 1998), this re-
duction is linear in the graph size. For the
maximum-weight bipartite matching problem,
well-known algorithm exist, e.g. the Hungar-
ian method. The complexity of this algorithm
is in O((I + J) ? I ? J). We will call the solu-
tion of the minimum-weight edge cover prob-
lem with the Hungarian method ?the MWEC
algorithm?. In contrary, we will refer to the al-
gorithm enforcing either source sentence cov-
erage or target sentence coverage as the one-
sided minimum-weight edge cover algorithm
(o-MWEC).
The cost matrix of a sentence pair (fJ1 , eI1)can be computed as a weighted linear interpo-
lation of various cost types hm:
cij =
M?
m=1
?m ? hm(i, j)
In our experiments, we will use the negated
logarithm of the state occupation probabilities
as described in Section 3. To obtain a more
symmetric estimate of the costs, we will inter-
polate both the source-to-target direction and
2An edge cover of G is a set of edges E? such that
each node of G is incident to at least one edge in E?.
the target-to-source direction (thus the state
occupation probabilities are interpolated log-
linearly). Because the alignments determined
in the source-to-target training may substan-
tially differ in quality from those produced in
the target-to-source training, we will use an
interpolation weight ?:
cij = ? ?w(i, j; fJ1 , eI1) + (1??) ?w(j, i; eI1, fJ1 )
(3)
Additional feature functions can be included
to compute cij ; for example, one could make
use of a bilingual word or phrase dictionary.
To apply the methods described in this sec-
tion, we made two assumptions: first, the costs
of an alignment can be computed as the sum
of local costs. Second, the features have to be
static in the sense that we have to fix the costs
before aligning any word. Therefore, we can-
not apply dynamic features such as the IBM-
4 distortion model in a straightforward way.
One way to overcome these restrictions lies in
using the state occupation probabilities; e.g.
for IBM-4, they contain the distortion model
to some extent.
5 Results
5.1 Evaluation Criterion
We use the same evaluation criterion as de-
scribed in (Och and Ney, 2000). We compare
the generated word alignment to a reference
alignment produced by human experts. The
annotation scheme explicitly takes the am-
biguity of the word alignment into account.
There are two different kinds of alignments:
sure alignments (S) which are used for unam-
biguous alignments and possible alignments
(P ) which are used for alignments that might
or might not exist. The P relation is used
especially to align words within idiomatic ex-
pressions and free translations. It is guaran-
teed that the sure alignments are a subset of
the possible alignments (S ? P ). The ob-
tained reference alignment may contain many-
to-one and one-to-many relationships.
The quality of an alignment A is computed
as appropriately redefined precision and recall
measures. Additionally, we use the alignment
error rate (AER), which is derived from the
well-known F-measure.
recall = |A ? S||S| , precision =
|A ? P |
|A|
AER(S, P ;A) = 1? |A ? S|+ |A ? P ||A|+ |S|
Table 1: Verbmobil task: corpus statistics.
Source/Target: German English
Train Sentences 34 446
Words 329 625 343 076
Vocabulary 5 936 3 505
Singletons 2 600 1 305
Dictionary Entries 4 404
Test Sentences 354
Words 3 233 3 109
S reference relations 2 559
P reference relations 4 596
Table 2: Canadian Hansards: corpus statistics.
Source/Target: French English
Train Sentences 128K
Words 2.12M 1.93M
Vocabulary 37 542 29 414
Singletons 12 986 9 572
Dictionary Entries 28 701
Test Sentences 500
Words 8 749 7 946
S reference relations 4 443
P reference relations 19 779
With these definitions a recall error can only
occur if a S(ure) alignment is not found and a
precision error can only occur if a found align-
ment is not even P (ossible).
5.2 Experimental Setup
We evaluated the presented lexicon sym-
metrization methods on the Verbmobil and
the Canadian Hansards task. The German?
English Verbmobil task (Wahlster, 2000) is a
speech translation task in the domain of ap-
pointment scheduling, travel planning and ho-
tel reservation. The French?English Canadian
Hansards task consists of the debates in the
Canadian Parliament.
The corpus statistics are shown in Table 1
and Table 2. The number of running words
and the vocabularies are based on full-form
words including punctuation marks. As in
(Och and Ney, 2003), the first 100 sentences
of the test corpus are used as a development
corpus to optimize model parameters that are
not trained via the EM algorithm, e.g. the
interpolation weights. The remaining part of
the test corpus is used to evaluate the models.
We use the same training schemes (model
sequences) as presented in (Och and Ney,
2003): 15H5334363 for the Verbmobil Task ,
i.e. 5 iteration of IBM-1, 5 iterations of the
HMM, 3 iteration of IBM-3, etc.; for the Cana-
dian Hansards task, we use 15H10334363. We
refer to these schemes as the Model 6 schemes.
For comparison, we also perform less sophisti-
cated trainings, to which we refer as the HMM
schemes (15H10 and 15H5, respectively), as
well as the IBM Model 4 schemes (15H103343
and 15H53343).
In all training schemes we use a conventional
dictionary (possibly containing phrases) as ad-
ditional training material. Because we use the
same training and testing conditions as (Och
and Ney, 2003), we will refer to the results pre-
sented in that article as the baseline results.
5.3 Non-symmetric Alignments
In the first experiments, we use the state oc-
cupation probabilities from only one transla-
tion direction to determine the word align-
ment. This allows for a fair comparison with
the Viterbi alignment computed as the result
of the training procedure. In the source-to-
target translation direction, we cannot esti-
mate the probability for the target words with
fertility zero and choose to set it to 0. In this
case, the minimum weight edge cover problem
is solved by the one-sided MWEC algorithm.
Like the Viterbi alignments, the alignments
produced by this algorithm satisfy the con-
straint that multiple source (target) words can
only be aligned to one target (source) word.
Tables 3 and 4 show the performance of
the one-sided MWEC algorithm in compar-
ison with the experiment reported by (Och
and Ney, 2003). We report not only the final
alignment error rates, but also the intermedi-
ate results for the HMM and IBM-4 training
schemes.
For IBM-3 to IBM-5, the Viterbi alignment
and a set of promising alignments are used
to determine the state occupation probabili-
ties. Consequently, we observe similar align-
ment quality when comparing the Viterbi and
the one-sided MWEC alignments.
We also evaluated the alignment quality af-
ter applying alignment generalization meth-
ods, i.e. we combine the alignment of both
translation directions. Experimentally, the
best generalization heuristic for the Canadian
Hansards task is the intersection of the source-
to-target and the target-to-source alignments.
For the Verbmobil task, the refined method
of (Och and Ney, 2003) is used. Again, we
observed similar alignment error rates when
merging either the Viterbi alignments or the
o-MWEC alignments.
Table 3: AER [%] for non-symmetric align-
ment methods and for various models (HMM,
IBM-4, Model 6) on the Canadian Hansards
task.
Alignment method HMM IBM4 M6
Baseline T?S 14.1 12.9 11.9
S?T 14.4 12.8 11.7
intersection 8.4 6.9 7.8
o-MWEC T?S 14.0 13.1 11.9
S?T 14.3 13.0 11.7
intersection 8.2 7.1 7.8
Table 4: AER [%] for non-symmetric align-
ment methods and for various models (HMM,
IBM-4, Model 6) on the Verbmobil task.
Alignment method HMM IBM4 M6
Baseline T?S 7.6 4.8 4.6
S?T 12.1 9.3 8.8
refined 7.1 4.7 4.7
o-MWEC T?S 7.3 4.8 4.5
S?T 12.0 9.3 8.5
refined 6.7 4.6 4.6
5.4 Symmetric Alignments
The heuristically generalized Viterbi align-
ments presented in the previous section can
potentially avoid the alignment constraints3.
However, the choice of the optimal general-
ization heuristic may depend on a particular
language pair and may require extensive man-
ual optimization. In contrast, the symmetric
MWEC algorithm is a systematic and theo-
retically well-founded approach to the task of
producing a symmetric alignment.
In the experiments with the symmetric
MWEC algorithm, the optimal interpolation
parameter ? (see Equation 3) for the Verbmo-
bil corpus was empirically determined as 0.8.
This shows that the model parameters can be
estimated more reliably in the direction from
German to English. In the inverse English-
to-German alignment training, the mappings
of many English words to one German word
are not allowed by the modeling constraints,
although such alignment mappings are signif-
icantly more frequent than mappings of many
German words to one English word.
The experimentally best interpolation pa-
rameter for the Canadian Hansards corpus was
? = 0.5. Thus the model parameters esti-
mated in the translation direction from French
to English are as reliable as the ones estimated
3Consequently, we will use them as baseline for the
experiments with symmetric alignments.
in the direction from English to French.
Lines 2a and 2b of Table 5 show the perfor-
mance of the MWEC algorithm. The align-
ment error rates are slightly lower if the HMM
or the full Model 6 training scheme is used
to train the state occupation probabilities on
the Canadian Hansards task. On the Verbmo-
bil task, the improvement is more significant,
yielding an alignment error rate of 4.1%.
Columns 4 and 5 of Table 5 contain the re-
sults of the experiments, in which the costs
cij were determined as the loglinear interpola-
tion of state occupation probabilities obtained
from the HMM training scheme with those
from IBM-4 (column 4) or from Model 6 (col-
umn 5). We set the interpolation parameters
for the two translation directions proportional
to the optimal values determined in the previ-
ous experiments. On the Verbmobil task, we
obtain a further improvement of 19% relative
over the baseline result reported in (Och and
Ney, 2003), reaching an AER as low as 3.8%.
The improvements of the alignment qual-
ity on the Canadian Hansards task are less
significant. The manual reference alignments
for this task contain many possible connec-
tions and only a few sure connections (cf. Ta-
ble 2). Thus automatic alignments consisting
of only a few reliable alignment points are fa-
vored. Because the differences in the number
of words and word order between French and
English are not as dramatic as e.g. between
German and English, the probability of the
empty word alignment is not very high. There-
fore, plenty of alignment points are produced
by the MWEC algorithm, resulting in a high
recall and low precision. To increase the preci-
sion, we replaced the empty word connection
costs (previously trained as state occupation
probabiliities using the EM algorithm) by the
global, word- and position-independent costs
depending only on one of the involved lan-
guages. The alignment error rates for these
experiments are given in lines 3a and 3b of Ta-
ble 5. The global empty word probability for
the Canadian Hansards task was empirically
set to 0.45 for French and for English, and,
for the Verbmobil task, to 0.6 for German and
0.1 for English. On the Canadian Hansards
task, we achieved further significant reduction
of the AER. In particular, we reached an AER
of 6.6% by performing only the HMM training.
In this case the effectiveness of the MWEC al-
gorithm is combined with the efficiency of the
HMM training, resulting in a fast and robust
alignment training procedure.
We also tested the more simple one-sided
MWEC algorithm. In contrast to the exper-
iments presented in Section 5.3, we used the
loglinear interpolated state occupation prob-
abilities (given by the Equation 3) as costs.
Thus, although the algorithm is not able to
produce a symmetric alignment, it operates
with symmetrized costs. In addition, we used
a combination heuristic to obtain a symmetric
alignment. The results of these experiments
are presented in Table 5, lines 4-6 a/b.
The performance of the one-sided MWEC
algorithm turned out to be quite robust on
both tasks. However, the o-MWEC align-
ments are not symmetric and the achieved low
AER depends heavily on the differences be-
tween the involved languages, which may fa-
vor many-to-one alignments in one translation
direction only. That is why on the Verbmobil
task, when determining the mininum weight in
each row for the translation direction from En-
glish to German, the alignment quality deteri-
orates, because the algorithm cannot produce
alignments which map several English words
to one German word (line 5b of Table 5).
Applying the generalization heuristics
(line 6a/b of Table 5), we achieve an AER of
6.0% on the Canadian Hansards task when
interpolating the state occupation probabil-
ities trained with the HMM and with the
IBM-4 schemes. On the Verbmobil task, the
interpolation of the HMM and the Model 6
schemes yields the best result of 3.7% AER.
In the latter experiment, we reached 97.3%
precision and 95.2% recall.
6 Related Work
A description of the IBM models for statistical
machine translation can be found in (Brown et
al., 1993). The HMM-based alignment model
was introduced in (Vogel et al, 1996). An
overview of these models is given in (Och and
Ney, 2003). That article also introduces the
Model 6; additionally, state-of-the-art results
are presented for the Verbmobil task and the
Canadian Hansards task for various configura-
tions. Therefore, we chose them as baseline.
Additional linguistic knowledge sources such
as dependeny trees or parse trees were used in
(Cherry and Lin, 2003; Gildea, 2003). Bilin-
gual bracketing methods were used to produce
a word alignment in (Wu, 1997). (Melamed,
2000) uses an alignment model that enforces
one-to-one alignments for nonempty words.
Table 5: AER[%] for different alignment symmetrization methods and for various alignment
models on the Canadian Hansards and the Verbmobil tasks (MWEC: minimum weight edge
cover, EW: empty word).
Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6
Canadian 1a. Baseline (intersection) 8.4 6.9 7.8 ? ?
Hansards 2a. MWEC 7.9 9.3 7.5 8.2 7.4
3a. MWEC (global EW costs) 6.6 7.4 6.9 6.4 6.4
4a. o-MWEC T?S 7.3 7.9 7.4 6.7 7.0
5a. S?T 7.7 7.6 7.2 6.9 6.9
6a. S?T (intersection) 7.2 6.6 7.6 6.0 7.1
Symmetrization Method HMM IBM4 M6 HMM + IBM4 HMM + M6
Verbmobil 1b. Baseline (refined) 7.1 4.7 4.7 ? ?
2b. MWEC 6.4 4.4 4.1 4.3 3.8
3b. MWEC (global EW costs) 5.8 5.8 6.6 6.0 6.7
4b. o-MWEC T?S 6.8 4.4 4.1 4.5 3.7
5b. S?T 9.3 7.2 6.8 7.5 6.9
6b. S?T (refined) 6.7 4.3 4.1 4.6 3.7
7 Conclusions
In this paper, we addressed the task of au-
tomatically generating symmetric word align-
ments for statistical machine translation. We
exploited the state occupation probabilties de-
rived from the IBM and HMM translation
models. We used the negated logarithms of
these probabilities as local alignment costs and
reduced the word alignment problem to find-
ing an edge cover with minimal costs in a
bipartite graph. We presented efficient algo-
rithms for the solution of this problem. We
evaluated the performance of these algorithms
by comparing the alignment quality to man-
ual reference alignments. We showed that in-
terpolating the alignment costs of the source-
to-target and the target-to-source translation
directions can result in a significant improve-
ment of the alignment quality.
In the future, we plan to integrate the graph
algorithms into the iterative training proce-
dure. Investigating the usefulness of addi-
tional feature functions might be interesting
as well.
Acknowledgment
This work has been partially funded by the
EU project TransType 2, IST-2001-32091.
References
L. E. Baum. 1972. An inequality and associated
maximization technique in statistical estimation
for probabilistic functions of markov processes.
Inequalities, 3:1?8.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,
and R. L. Mercer. 1993. The mathematics of
statistical machine translation: Parameter esti-
mation. Computational Linguistics, 19(2):263?
311, June.
C. Cherry and D. Lin. 2003. A probability model
to improve word alignment. In Proc. of the 41th
Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 88?95, Sap-
poro, Japan, July.
D. Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proc. of the 41th An-
nual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 80?87, Sapporo,
Japan, July.
J. Keijsper and R. Pendavingh. 1998. An effi-
cient algorithm for minimum-weight bibranch-
ing. Journal of Combinatorial Theory Series B,
73(2):130?145, July.
I. D. Melamed. 2000. Models of translational
equivalence among words. Computational Lin-
guistics, 26(2):221?249.
F. J. Och and H. Ney. 2000. Improved statistical
alignment models. In Proc. of the 38th Annual
Meeting of the Association for Computational
Linguistics (ACL), pages 440?447, Hong Kong,
October.
F. J. Och and H. Ney. 2003. A systematic com-
parison of various statistical alignment models.
Computational Linguistics, 29(1):19?51, March.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation.
In COLING ?96: The 16th Int. Conf. on Com-
putational Linguistics, pages 836?841, Copen-
hagen, Denmark, August.
W. Wahlster, editor. 2000. Verbmobil: Founda-
tions of speech-to-speech translations. Springer
Verlag, Berlin, Germany, July.
D. Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel cor-
pora. Computational Linguistics, 23(3):377?
403, September.
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 913?920
Manchester, August 2008
 
	
	ffProceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 839?847,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Complexity of finding the BLEU-optimal hypothesis in a confusion network
Gregor Leusch and Evgeny Matusov and Hermann Ney
RWTH Aachen University, Germany
{leusch,matusov,ney}@cs.rwth-aachen.de
Abstract
Confusion networks are a simple representa-
tion of multiple speech recognition or transla-
tion hypotheses in a machine translation sys-
tem. A typical operation on a confusion net-
work is to find the path which minimizes or
maximizes a certain evaluation metric. In this
article, we show that this problem is gener-
ally NP-hard for the popular BLEU metric,
as well as for smaller variants of BLEU. This
also holds for more complex representations
like generic word graphs. In addition, we give
an efficient polynomial-time algorithm to cal-
culate unigram BLEU on confusion networks,
but show that even small generalizations of
this data structure render the problem to be
NP-hard again.
Since finding the optimal solution is thus not
always feasible, we introduce an approximat-
ing algorithm based on a multi-stack decoder,
which finds a (not necessarily optimal) solu-
tion for n-gram BLEU in polynomial time.
1 Introduction
In machine translation (MT), confusion networks
(CNs) are commonly used to represent alternative
versions of sentences. Typical applications include
translation of different speech recognition hypothe-
ses (Bertoldi et al, 2007) or system combination
(Fiscus, 1997; Matusov et al, 2006).
A typical operation on a given CN is to find the
path which minimizes or maximizes a certain eval-
uation metric. This operation can be used in ap-
plications like Minimum Error Rate Training (Och,
2003), or optimizing system combination as de-
scribed by Hillard et al (2007). Whereas this is
easily achievable for simple metrics like the Word
Error Rate (WER) as described by Mohri and Riley
(2002), current research in MT uses more sophisti-
cated measures, like the BLEU score (Papineni et
al., 2001). Zens and Ney (2005) first described this
task on general word graphs, and sketched a com-
plete algorithm for calculating the maximum BLEU
score in a word graph. While they do not give an
estimate on the complexity of their algorithm, they
note that already a simpler algorithm for calculating
the Position independent Error Rate (PER) has an
exponential worst-case complexity. The same can
be expected for their BLEU algorithm. Dreyer et
al (2007) examined a special class of word graphs,
namely those that denote constrained reorderings of
single sentences. These word graphs have some
properties which simplify the calculation; for exam-
ple, no edge is labeled with the empty word, and
all paths have the same length and end in the same
node. Even then, their decoder does not optimize
the true BLEU score, but an approximate version
which uses a language-model-like unmodified pre-
cision. We give a very short introduction to CNs and
the BLEU score in Section 2.
In Section 3 we show that finding the best BLEU
score is an NP-hard problem, even for a simplified
variant of BLEU which only scores unigrams and
bigrams. The main reason for this problem to be-
come NP-hard is that by looking at bigrams, we al-
low for one decision to also influence the following
decision, which itself can influence the decisions af-
ter that. We also show that this also holds for uni-
gram BLEU and the position independent error rate
(PER) on a slightly augmented variant of CNs which
allows for edges to carry multiple symbols. The con-
catenation of symbols corresponds to the interde-
pendency of decisions in the case of bigram matches
above.
NP-hard problems are quite common in machine
839
translation; for example, Knight (1999) has shown
that even for a simple form of statistical MT mod-
els, the decoding problem is NP-complete. More
recently, DeNero and Klein (2008) have proven the
NP-completeness of the phrase alignment problem.
But even a simple, common procedure as BLEU
scoring, which can be performed in linear time on
single sentences, becomes a potentially intractable
problem as soon as it has to be performed on a
slightly more powerful representation, such as con-
fusion networks. This rather surprising result is the
motivation of this paper.
The problem of finding the best unigram BLEU
score in an unaugmented variant of CNs is not NP-
complete, as we show in Section 4. We present an
algorithm that finds such a unigram BLEU-best path
in polynomial time.
An important corollary of this work is that calcu-
lating the BLEU-best path on general word graphs
is also NP-complete, as CNs are a true subclass
of word graphs. It is still desirable to calculate a
?good? path in terms of the BLEU score in a CN,
even if calculating the best path is infeasible. In Sec-
tion 5, we present an algorithm which can calculate
?good? solutions for CNs in polynomial time. This
algorithm can easily be extended to handle arbitrary
word graphs. We assess the algorithm experimen-
tally on real-world MT data in Section 6, and draw
some conclusions from the results in this article in
Section 7.
2 Confusion networks
A confusion network (CN) is a word graph where
each edge is labeled with exactly zero or one sym-
bol, and each path from the start node to the end
node visits each node of the graph in canonical or-
der. Usually, we represent unlabeled edges by label-
ing them with the empty word ?.
Within this paper, we represent a CN by a list of
lists of words {wi,j}, where each wi,j corresponds
to a symbol on an edge between nodes i and i + 1.
A path in this CN can be written as a string of inte-
gers, an1 = a1, . . . , an, such that the path is labeled
w1,a1w2,a2 . . . wn,an . Note that there can be a differ-
ent number of possible words, j, for different posi-
tions i.
2.1 BLEU and variants
The BLEU score, as defined by Papineni et al
(2001), is the modified n-gram precision of a hy-
pothesis, with 1 ? n ? N , given a set of reference
translations R. ?Modified precision? here means
that for each n-gram, its maximum number of oc-
currences within the reference sentences is counted,
and only up to that many occurrences in the hypothe-
sis are considered to be correct. The geometric mean
over the precisions for all n is calculated, and mul-
tiplied by a brevity penalty bp. This brevity penalty
is 1.0 if the hypothesis sentence is at least as long as
the reference sentence (special cases occur if multi-
ple reference sentences with different length exists),
and less than 1.0 otherwise. The exact formulation
can be found in the cited paper; for the proofs in
our paper it is enough to note that the BLEU score
is 1.0 exactly if all n-grams in the hypothesis oc-
cur at least that many times in a reference sentence,
and if there is a reference sentence which is as long
as or shorter than the hypothesis. Assuming that
we can always provide a dummy reference sentence
shorter than this length, we do not need to regard
the brevity penalty in these proofs. Within the fol-
lowing proofs of NP-hardness, we will only require
confusion networks (and word graphs) which do not
contain empty words, and where all paths from the
start node to the end node have the same length.
Usually, in the definition of the BLEU score, N is
set to 4; within this article we denote this metric as
4BLEU. We can also restrict the calculations to un-
igrams only, which would be 1BLEU, or to bigrams
and unigrams, which we denote as 2BLEU.
Similar to the 1BLEU metric is the Position in-
dependent Error Rate PER (Tillmann et al, 1997),
which counts the number of substitutions, insertions,
and deletions that have to be performed on the uni-
gram counts to have the hypothesis counts match the
reference counts. Unlike 1BLEU, for PER to be op-
timal (here, 0.0), the reference counts must match
the candidate counts exactly.
Given a CN {wi,j} and a set of reference sen-
tences R, we define the optimization problem
Definition 1 (CN-2BLEU-OPTIMIZE) Among
all paths aI1 through the CN, what is the path with
the highest 2BLEU score?
Related to this is the decision problem
Definition 2 (CN-2BLEU-DECIDE) Among all
paths aI1 through the CN, is there a path with a
2BLEU score of 1.0?
Similarly we define CN-4BLEU-DECIDE, CN-
PER-DECIDE, etc.
840
3 CN-2BLEU-DECIDE is NP-complete
We now show that CN-2BLEU-DECIDE is NP-
complete. It is obvious that the problem is in NP:
Given a path aI1, which is polynomial in size to the
problem, we can decide in polynomial time whether
aI1 is a solution to the problem ? namely by calcu-
lating the BLEU score. We now show that there is
a problem known to be NP-complete which can be
polynomially reduced to CN-2BLEU-DECIDE. For
our proof, we choose 3SAT.
3.1 3SAT
Consider the following problem:
Definition 3 (3SAT) Let X = {x1, . . . , xn}
be a set of Boolean variables, let F =?k
i=1 (Li,1?Li,2?Li,3) be a Boolean formula,
where each literal Li,j is either a variable x or its
negate x. Is there a assignment ? : X ? {0, 1}
such that ? |= F? In other words, if we replace
each x in F by ?(x), and each x by 1? ?(x), does
F become true?
It has been shown by Karp (1972) that 3SAT is
NP-complete. Consequently, if for another problem
in NP there is polynomial-size and -time reduction
of an arbitrary instance of 3SAT to an instance of this
new problem, this new problem is also NP-complete.
3.2 Reduction of 3SAT to
CN-2BLEU-DECIDE
Let F be a Boolean formula in 3CNF, and let k be
its size, as in Definition 3. We will now reduce it to a
corresponding CN-2BLEU-DECIDE problem. This
means that we create an alphabet ?, a confusion net-
work C, and a set of reference sentencesR, such that
there is a path through C with a BLEU score of 1.0
exactly if F is solvable:
Create an alphabet ? based on F as ? :=
{x1, . . . , xn} ? {x1, . . . , xn} ? {}. Here, the xi
and xi symbols will correspond to the variable with
the same name or their negate, respectively, whereas
 will serve as an ?isolator symbol?, to avoid un-
wanted bigram matches or mismatches between sep-
arate parts of the constructed CN or sentences.
Consider the CN C from Figure 1.
Consider the following set of reference sentences:
R := {  (x1)
k(x2)
k . . . (xn)
k
 (x1)
k(x2)
k . . . (xn)
k,
(x1)
k  (x1)
k  . . . (xn)
k  (xn)
k  ()k+n }
where (x)k denotes k subsequent occurrences of x.
Clearly, both C and R are of polynomial size in n
and k, and can be constructed in polynomial time.
Then,
There is an assignment ? such that ? |= F
?
There is a path aI1 through C such that
BLEU(aI1, R) = 1.0.
Proof: ???
Let ? be an assignment under which F becomes
true. Create a path aI1 as follows: Within A, for
each set of edges Li,1, Li,2, Li,3, choose the path
through an x where ?(x) = 1, or through an x
where ?(x) = 0. Note that there must be such an
x, because otherwise the clause Li,1 ? Li,2 ? Li,3
would not be true under ?. Within B, select the path
always through xi if ?(xi) = 0, and through xi if
?(xi) = 1.
Then, aI1 consists of, for each i,
? At most k occurrences of both xi and xi
? At most k occurrences of each of the bigrams
xi, xi, xi, xi, xixi, and xixi
? No other bigram than those listed above.
For all of these unigram and bigram counts, there is
a reference sentence in R which contains at least as
many of those unigrams/bigrams as the path. Thus,
the unigram and bigram precision of aI1 is 1.0. In ad-
dition, there is always a reference sentence whose
length is shorter than that of aI1, such that the brevity
penalty is also 1.0. As a result, BLEU(aI1, R) =
1.0.
???
Let aI1 be a path through C such that
BLEU(aI1, R) = 1.0. Because there is no bi-
gram xixi or xixi in R, we can assume that for each
xi, either only xi edges, or only xi edges appear
in the B part of aI1, each at most k times. As no
unigram xi and xi appears more than k times in R,
we can assume that, if the xi edges are passed in
B, then only the xi edges are passed in A, and vice
versa. Now, create an assignment ? as follows:
? :=
{
0 ifxi edges are passed inB
1 otherwise
Then, ? |= F . Proof: Assume that F? = 0. Then
there must be a clause i such that Li,1?Li,2?Li,3 =
841
A :=          
   




L1,1
L1,2
L1,3
   
   
   
   





   
   
   
   




L2,1
L2,2
L2,3
   
   
   
   




 ? ? ?
   
   
   
   





   
   
   
   




Lk,1
Lk,2
Lk,3
   
   
   
   





   
   
   
   




B :=             
x1
x1
   
   
   
   




x1
x1
? ? ?
   
   
   
   




x1
x1? ?? ?
k times
   
   
   
   





   
   
   
   




x2
x2
? ? ?
? ?? ?
k
   
   
   
   




 ? ? ?
   
   
   
   




xn
xn
? ? ?
? ?? ?
k
   
   
   
   





C := A             

B
Figure 1: CN constructed from a 3SAT formula F . C is the concatenation of the left part A, and the right path B,
separated by an isolating .
0. At least one of the edges Li,j associated with the
literals of this clause must have been passed by aK1
in A. This literal, though, can not have been passed
in B. As a consequence, ?(Li,j) = 1. But this
means that Li,1 ? Li,2 ? Li,3 = 1 ; contra-
diction.
Because CN-2BLEU-DECIDE is in NP, and we
can reduce an NP-complete problem (3SAT) in poly-
nomial time to a CN-2BLEU-DECIDE problem, this
means that CN-2BLEU-DECIDE is NP-complete.
3.3 CN-4BLEU-DECIDE
It is straightforward to modify the construction
above to create an equivalent CN-4BLEU-DECIDE
problem instead: Replace each occurrence of the
isolating symbol  in A,B, C, R by three consecu-
tive isolating symbols . Then, everything said
about unigrams still holds, and bi-, tri- and four-
grams are handled equivalently: Previous unigram
matches on  correspond to uni-, bi-, and trigram
matches on , , . Bigram matches on x corre-
spond to bi-, tri-, and fourgram matches on x, x,
x, and similar holds for bigram matches x, x,
x. Unigram matches x, x, and bigram matches
xx etc. stay the same. Consequently, CN-4BLEU-
DECIDE is also an NP-complete problem.
3.4 CN*-1BLEU-DECIDE
Is it possible to get rid of the necessity for bi-
gram counts in this proof? One possibility might be
to look at slightly more powerful graph structures,
CN*. In these graphs, each edge can be labeled
by arbitrarily many symbols (instead of just zero or
one). Then, consider a CN* graph C? := A            

B?,
B? :=             
(x1)k
(x1)k
   
   
   
   





   
   
   
   




x1
x1
?? ?? ?
k times
? ? ?
   
   
   
   




(xn)k
(xn)k
   
   
   
   




 ? ? ?
Figure 2: Right part of a CN* constructed from a 3SAT
formula F .
with B? as in Figure 2.
With
R? := {(x1)
k(x1)
k . . . (xn)
k(xn)
k()k}
we can again assume that either xi or xi ap-
pears k times in the B?-part of a path aK1 with
1BLEU(aK1 , R
?) = 1.0, and that for every solution
? to F there is a corresponding path aK1 through C
?
and vice versa. In this construction, we also have
exact matches of the counts, so we can also use PER
in the decision problem.
While CN* are generally not word graphs by
themselves due to the multiple symbols on edges,
it is straightforward to create an equivalent word
graph from a given CN*, as demonstrated in Fig-
ure 3. Consequently, deciding unigram BLEU and
unigram PER are NP-complete problems for general
word graphs as well.
4 Solving CN-1BLEU-DECIDE in
polynomial time
It is not a coincidence that we had to resort to
bigrams or to edges with multiple symbols for
NP-completeness: It turns out that CN-1BLEU-
DECIDE, where the order of the words does not
842
CN*: ? ? ?             
(x1)k
(x1)k
   
   
   
   




 ? ? ?
;
WG: ? ? ?          
x1
x1
  
  
  



  
  
  
  




x1
x1
? ? ?   
  
  



  
  
  
  




x1
x1? ?? ?
k times
   
   
   
   




 ? ? ?
Figure 3: Construction of a word graph from a CN* as in
B?.
matter at all, can be decided in polynomial time
using the following algorithm, which disregards a
brevity penalty for the sake of simplicity:
Given a vocabulary X , a CN {wi,j}, and a set of
reference sentences R together with their unigram
BLEU counts c(x) : X ? N and C :=
?
x?X c(x),
1. Remove all parts fromw where there is an edge
labeled with the empty word ?. This step will
always increase unigram precision, and can not
hurt any higher n-gram precision here, because
n = 1. In the example in Figure 4, the edges
labeled very and ? respectively are affected in
this step.
2. Create nodes A0 := {1, . . . , n}, one for each
node with edges in the CN. In the example in
Figure 5, the three leftmost column heads cor-
respond to these nodes.
3. Create nodes B := {x.j |x ? X, 1?j?c(x)}.
In other words, create a unique node for each
?running? word in R ? e.g. if the first and
second reference sentence contain x once each,
and the third reference contains x twice, create
exactly x.1 and x.2. In Figure 5, those are the
row heads to the right.
4. Fill A with empty nodes to match the total
length: A := A0 ? {?.j | 1 ? j ? C ? n}.
If n > C, the BLEU precision can not be 1.0.
The five rightmost columns in Figure 5 corre-
spond to those.
5. Create edges
E := {(i, wi,j .k) | 1? i?n, all j, 1?c(wi,j)}
? {(i, ?.j) | 1 ? i ? n, all j}. These edges are
denoted as ? or ? in Figure 5.
C :=             
on
at
   
   
   
   




the
that
   
   
   
   




very
?
   
   
   
   




day
time
R := { on the same day,
at the time and the day }
Figure 4: Example for CN-1BLEU-DECIDE.
1 2 3 ?.1 ?.2 ?.3 ?.4 ?.5
? ? ? ? ? ? on
? ? ? ? ? ? the.1
? ? ? ? ? ? the.2
? ? ? ? ? same
? ? ? ? ? ? day
? ? ? ? ? ? at
? ? ? ? ? ? time
? ? ? ? ? and
Figure 5: Bipartite graph constructed to find the optimal
1BLEU path in Figure 4. One possible maximum bipar-
tite matching is marked with ?.
6. Find the maximum bipartite matching M be-
tween A and B given E. Figure 5 shows such
a matching with ?.
7. If all nodes in A and B are covered by M ,
then 1BLEU({wi,j}, R) = 1.0. The words that
are matched to A0 then form the solution path
through {wi,j}.
Figure 4 gives an example of a CN and a set of ref-
erences R, for which the best 1BLEU path can be
constructed by the algorithm above. The bipartite
graph constructed in Step 1 to Step 4 for this exam-
ple, given in matrix form, can be found in Figure 5.
Such a solution to Step 6, if found, corresponds
exactly to a path through the confusion network with
1BLEU=1.0, and vice versa: for each position 1 ?
i ? n, the matched word corresponds to the word
that is selected for the position of the path; ?surplus?
counts are matched with ?s.
Step 6 can be performed in polynomial time
(Hopcroft and Karp, 1973) O((C +n)5/2); all other
steps in linear time O(C + n). Consequently, CN-
1BLEU can be decided in polynomial time O((C +
n)5/2). Similarly, an actual optimum 1BLEU score
843
can be calculated in O((C + n)5/2).
It should be noted that the only alterations in the
hypothesis length, and as a result the only alterations
in the brevity penalty, will come from Step 1. Con-
sequently, the brevity penalty can be taken into ac-
count as follows: Consider that there are M nodes
with an empty edge in {wi,j}. Instead of remov-
ing them in Step 1, keep them in, but for each
1 ? m ? M , run through steps 2 to 6, but add
m nodes ?.1, . . . , ?.m to B in Step 3, and add corre-
sponding edges to these nodes to E in Step 5. After
each iteration (which leads to a constant hypothesis
length), calculate precision and brevity penalty. Se-
lect the best product of precision and brevity penalty
in the end. The overall time complexity now is in
M ?O((C + n)5/2).
A PER score can be calculated in a similar fash-
ion.
5 Finding approximating solutions for
CN-4BLEU in polynomial time
Knowing that the problem of finding the BLEU-best
path is an NP-complete problem is an unsatisfactory
answer in practice ? in many cases, having a good,
but not necessarily optimum path is preferable to
having no good path at all.
A simple approach would be to walk the CN from
the start node to the end node, keeping track of n-
grams visited so far, and choosing the word next
which maximizes the n-gram precision up to this
word. Track is kept by keeping n-gram count vec-
tors for the hypothesis path and the reference sen-
tences, and update those in each step.
The main problem with this approach is that of-
ten the local optimum is suboptimal on the global
scale, for example if a word occurs on a later posi-
tion again.
Zens and Ney (2005) on the other hand propose
to keep all n-gram count vectors instead, and only
recombine path hypotheses with identical count vec-
tors. As they suspect, the search space can become
exponentially large.
In this paper, we suggest a compromise between
these two extremes, namely keeping active a suffi-
ciently large number of ?path hypotheses? in terms
of n-gram precision, instead of only the first best,
or of all. But even then, edges with empty words
pose a problem, as stepping along an empty edge
will never decrease the precision of the local path.
In certain cases, steps along empty edges may affect
the n-gram precision for higher n-grams. But this
will only take effect after the next non-empty step, it
does not influence the local decision in a node. Step-
ping along a non-empty edge will often decrease the
local precision, though. As a consequence, a simple
algorithm will prefer paths with shorter hypotheses,
which leads to a suboptimal total BLEU score, be-
cause of the brevity penalty. One can counter this
problem for example by using a brevity penalty al-
ready during the search. But this is problematic as
well, because it is difficult to define a proper partial
reference length in this case.
The approach we propose is to compare only par-
tial path hypotheses with the same number of empty
edges, and ending in the same position in the confu-
sion network. This idea is illustrated in Figure 6: We
compare only the partial precision of path hypothe-
ses ending in the same node. Due to the simple na-
ture of this search graph, it can easily be traversed in
a left-to-right, top-to-bottom manner. With regard to
a node currently being expanded, only the next node
in the same row, and the corresponding columns in
the next row need to be kept active. When imple-
menting this algorithm, Hypotheses should be com-
pared on the modified BLEUS precision by Lin and
Och (2004) because the original BLEU precision
equals zero as long as there are no higher n-gram
matches in the partial hypotheses, which renders
meaningful comparison hard or impossible.
In the rightmost column, all path hypotheses
within a node have the same hypothesis length. Con-
sequently, we can select the hypothesis with the best
(brevity-penalized) BLEU score by multiplying the
appropriate brevity penalty to the precision of the
best path ending in each of these nodes. If we al-
ways expand all possible path hypotheses within the
nodes, and basically run a full search, we will al-
ways find the BLEU-best path this way. From the
proof above, it follows that the number of path hy-
pothesis we would have to keep can become expo-
nentially large. Fortunately, if a ?good? solution is
good enough, we do not have to keep all possible
path hypotheses, but only the S best ones for a given
constant S, or those with a precision not worse than
c times the precision of the best hypothesis within
the node. Assuming that adding and removing an
element to/from a size-limited stack of size S takes
timeO(logS), that we allow at mostE empty edges
in a solution, and that there are j edges in each of the
n positions, this algorithm has a time complexity of
844
Figure 6: Principle of the multi-stack decoder used to find
a path with a good BLEU score. The first row shows
the original confusion network, the following rows show
the search graph. Duplicate edges were removed, but no
word was considered ?unknown?.
O(E ? n ? j ? S logS).
To reduce redundant duplicated path hypotheses,
and by this to speed up the algorithm and reduce the
risk that good path hypotheses are pruned, the confu-
sion network should be simplified before the search,
as shown in Figure 6:
1. Remove all words in the CN which do not ap-
pear in any reference sentence, if there at least
one ?known? non-empty word at the same po-
sition. If there is no such ?known? word, re-
place them all by a single token denoting the
?unknown word?.
2. Remove all duplicate edges in a position, that
is, if there are two or more edges carrying the
same label in one position, remove all but one
for them.
These two steps will keep at least one of the BLEU-
best paths intact. But they can remove the average
branching factor (j) of the CN significantly, which
leads to a significantly lower number of duplicate
path hypotheses during the search.
Table 1: Statistics of the (Chinese?)English MT corpora
used for the experiments
NIST NIST
2003 2006
number of systems 4 4
number of ref. 4 4 per sent.
sentences 919 249
system length 28.4 33.2 words?
ref. length 27.5 34.2 words?
best path 24.4 33.9 words?
CN length 40.7 39.5 nodes?
best single system 29.3 52.5 BLEU
30.5 51.6 BLEUS?
?average per sentence
Our algorithm can easily be extended to handle ar-
bitrary word graphs instead of confusion networks.
In this case, each ?row? in Figure 6 will reflect the
structure of the word graph instead of the ?linear?
structure of the CN.
While this algorithm searches for the best path
for a single sentence only, a common task is to
find the best BLEU score over a whole test set ?
which can mean suboptimal BLEU scores for in-
dividual sentences. This adds an additional com-
binatorial problem over the sentences to the actual
decoding process. Both Zens and Ney (2005) and
Dreyer et al(2007) use a greedy approach here; the
latter estimated the impact of this to be insignifi-
cant in random sampling experiments. In our exper-
iments, we used the per-sentence BLEUS score as
(greedy) decision criterion, as this is also the prun-
ing criterion. One possibility to adapt this approach
to Zens?s/Dreyer?s greedy approach for system-level
BLEU scores might be to initialize n-gram counts
and hypothesis length not to zero at the beginning
of each sentence, but to those of the corpus so far.
But as this diverts from our goal to optimize the
sentence-level scores, we have not implemented it
so far.
6 Experimental assessment of the
algorithm
The question arises how many path hypotheses we
need to retain in each step to obtain optimal paths.
To examine this, we created confusion networks out
of the translations of the four best MT systems of
845
ll l
l l l l l l l l l l l l
5 10 15 20
0.32
0.34
0.36
0.38
0.40
# path hyps
BL
EU
, BL
EU
S
l
sys?BLEU
avg. seg?BLEUS
avg. seg?BLEU
Figure 7: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT03 corpus.
the NIST 2003 and 2006 Chinese?English evalu-
ation campaigns, as available from the Linguistic
Data Consortium (LDC). The hypotheses of the best
single system served as skeleton, those of the three
remaining systems were reordered and aligned to the
skeleton hypothesis. This corpus is described in Ta-
ble 1. Figures 7 and 8 show the measured BLEU
scores in three different definitions, versus the max-
imum number of path hypotheses that are kept in
each node of the search graph. Shown are the av-
erage sentence-wise BLEUS score, which is what
the algorithm actually optimizes, for comparison the
average sentence-wise BLEU score, and the total
document-wise BLEU score.
All scores increase with increasing number of re-
tained hypotheses, but stabilize around a total of 15
hypotheses per node. The difference over a greedy
approach, which corresponds to a maximum of one
hypothesis per node if we leave out the separation by
path length, is quite significant. No further improve-
ments can be expected for a higher number of hy-
potheses, as experiments up to 100 hypotheses show.
7 Conclusions
In this paper, we showed that deciding whether a
given CN contains a path with a BLEU score of 1.0
is an NP-complete problem for n-gram lengths ? 2.
l
l l l
l l l l l l l l l l l l
5 10 15 20 250.
65
0.67
0.69
0.71
# path hyps
BL
EU
, BL
EU
S
l
sys?BLEU
avg. seg?BLEUS
avg. seg?BLEU
Figure 8: Average of the sentence-wise BLEU and
BLEUS score and the system-wide BLEU score versus
the number of path hypotheses kept per node during the
search. NIST MT06 corpus.
The problem is also NP-complete if we only look
at unigram BLEU, but allow for CNs where edges
may contain multiple symbols, or for arbitrary word
graphs. As a corollary, any proposed algorithm to
find the path with an optimal BLEU score in a CN,
even more in an arbitrary word graph, which runs
in worst case polynomial time can only deliver an
approximation1.
We gave an efficient polynomial time algorithm
for the simplest variant, namely deciding on a uni-
gram BLEU score for a CN. This algorithm can eas-
ily be modified to decide on the PER score as well,
or to calculate an actual unigram BLEU score for the
hypothesis CN.
Comparing these results, we conclude that the
ability to take bi- or higher n-grams into account,
be it in the scoring (as in 2BLEU), or in the graph
structure (as in CN*), is the key to render the prob-
lem NP-hard. Doing so creates long-range depen-
dencies, which oppose local decisions.
We also gave an efficient approximating algo-
rithm for higher-order BLEU scores. This algorithm
is based on a multi-stack decoder, taking into ac-
count the empty arcs within a path. Experimental
results on real-world data show that our method is
indeed able to find paths with a significantly better
1provided that P 6= NP , of course.
846
BLEU score than that of a greedy search. The re-
sulting BLEUS score stabilizes already on a quite
restricted search space, showing that despite the
proven NP-hardness of the exact problem, our al-
gorithm can give useful approximations in reason-
able time. It is yet an open problem in how far
the problems of finding the best paths regarding a
sentence-level BLEU score, and regarding a system-
level BLEU score correlate. Our experiments here
suggest a good correspondence.
8 Acknowledgments
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-0023.
The proofs and algorithms in this paper emerged
while the first author was visiting researcher at
the Interactive Language Technologies Group of
the National Research Council (NRC) of Canada,
Gatineau. The author wishes to thank NRC and
Aachen University for the opportunity to jointly
work on this project.
References
Nicola Bertoldi, Richard Zens, and Marcello Federico.
2007. Speech translation by confusion network de-
coding. In IEEE International Conference on Acous-
tics, Speech, and Signal Processing, pages 1297?1300,
Honululu, HI, USA, April.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In Human Language
Technologies 2008: The Conference of the Association
for Computational Linguistics, Short Papers, pages
25?28, Columbus, Ohio, June. Association for Com-
putational Linguistics.
Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. Comparing Reordering Constraints for SMT
Using Efficient BLEU Oracle Computation. In AMTA
Workshop on Syntax and Structure in Statistical Trans-
lation (SSST) at the Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics (NAACL-HLT), pages 103?110,
Rochester, NY, USA, April.
Jonathan G. Fiscus. 1997. A post-processing system to
yield reduced word error rates: Recogniser output vot-
ing error reduction (ROVER). In Proceedings 1997
IEEE Workshop on Automatic Speech Recognition and
Understanding, pages 347?352, Santa Barbara, CA.
Dustin Hillard, Bjo?rn Hoffmeister, Mari Ostendorf, Ralf
Schlu?ter, and Hermann Ney. 2007. iROVER: Improv-
ing system combination with classification. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Companion Volume, Short Pa-
pers, pages 65?68, Rochester, New York, April.
John E. Hopcroft and Richard M. Karp. 1973. An n5/2
algorithm for maximum matchings in bipartite graphs.
SIAM Journal on Computing, 2(4):225?231.
Richard M. Karp. 1972. Reducibility among combina-
torial problems. In R. E. Miller and J. W. Thatcher,
editors, Complexity of Computer Computations, pages
85?103. Plenum Press.
Kevin Knight. 1999. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4):607?615, December.
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluation automatic evaluation metrics for
machine translation. In Proc. COLING 2004, pages
501?507, Geneva, Switzerland, August.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing consensus translation from multiple
machine translation systems using enhanced hypothe-
ses alignment. In Conference of the European Chap-
ter of the Association for Computational Linguistics,
pages 33?40, Trento, Italy, April.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proc. of
the 7th Int. Conf. on Spoken Language Processing (IC-
SLP?02), pages 1313?1316, Denver, CO, September.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proc. of the 41th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 160?167, Sapporo, Japan,
July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. Bleu: a method for auto-
matic evaluation of machine translation. Technical Re-
port RC22176 (W0109-022), IBM Research Division,
Thomas J. Watson Research Center, September.
Christoph Tillmann, Stephan Vogel, Hermann Ney, Alex
Zubiaga, and Hassan Sawaf. 1997. Accelerated
DP based search for statistical translation. In Euro-
pean Conf. on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece, September.
Richard Zens and Hermann Ney. 2005. Word graphs
for statistical machine translation. In 43rd Annual
Meeting of the Assoc. for Computational Linguistics:
Proc. Workshop on Building and Using Parallel Texts:
Data-Driven Machine Translation and Beyond, pages
191?198, Ann Arbor, MI, June.
847
Computing Consensus Translation from Multiple Machine Translation
Systems Using Enhanced Hypotheses Alignment
Evgeny Matusov, Nicola Ueffing, Hermann Ney
Lehrstuhl fu?r Informatik VI - Computer Science Department
RWTH Aachen University, Aachen, Germany.
{matusov,ueffing,ney}@informatik.rwth-aachen.de
Abstract
This paper describes a novel method for
computing a consensus translation from
the outputs of multiple machine trans-
lation (MT) systems. The outputs are
combined and a possibly new transla-
tion hypothesis can be generated. Simi-
larly to the well-established ROVER ap-
proach of (Fiscus, 1997) for combining
speech recognition hypotheses, the con-
sensus translation is computed by voting
on a confusion network. To create the con-
fusion network, we produce pairwise word
alignments of the original machine trans-
lation hypotheses with an enhanced sta-
tistical alignment algorithm that explicitly
models word reordering. The context of a
whole document of translations rather than
a single sentence is taken into account to
produce the alignment.
The proposed alignment and voting ap-
proach was evaluated on several machine
translation tasks, including a large vocab-
ulary task. The method was also tested in
the framework of multi-source and speech
translation. On all tasks and conditions,
we achieved significant improvements in
translation quality, increasing e. g. the
BLEU score by as much as 15% relative.
1 Introduction
In this work we describe a novel technique for
computing a consensus translation from the out-
puts of multiple machine translation systems.
Combining outputs from different systems
was shown to be quite successful in automatic
speech recognition (ASR). Voting schemes like
the ROVER approach of (Fiscus, 1997) use edit
distance alignment and time information to cre-
ate confusion networks from the output of several
ASR systems.
Some research on multi-engine machine trans-
lation has also been performed in recent years.
The most straightforward approaches simply se-
lect, for each sentence, one of the provided hy-
potheses. The selection is made based on the
scores of translation, language, and other mod-
els (Nomoto, 2004; Paul et al, 2005). Other
approaches combine lattices or N -best lists from
several different MT systems (Frederking and
Nirenburg, 1994). To be successful, such ap-
proaches require compatible lattices and compa-
rable scores of the (word) hypotheses in the lat-
tices. However, the scores of most statistical ma-
chine translation (SMT) systems are not normal-
ized and therefore not directly comparable. For
some other MT systems (e.g. knowledge-based
systems), the lattices and/or scores of hypotheses
may not be even available.
(Bangalore et al, 2001) used the edit distance
alignment extended to multiple sequences to con-
struct a confusion network from several transla-
tion hypotheses. This algorithm produces mono-
tone alignments only (i. e. allows insertion, dele-
tion, and substitution of words); it is not able to
align translation hypotheses with significantly dif-
ferent word order. (Jayaraman and Lavie, 2005)
try to overcome this problem. They introduce a
method that allows non-monotone alignments of
words in different translation hypotheses for the
same sentence. However, this approach uses many
heuristics and is based on the alignment that is per-
formed to calculate a specific MT error measure;
the performance improvements are reported only
in terms of this measure.
33
Here, we propose an alignment procedure that
explicitly models reordering of words in the hy-
potheses. In contrast to existing approaches, the
context of the whole document rather than a sin-
gle sentence is considered in this iterative, unsu-
pervised procedure, yielding a more reliable align-
ment.
Based on the alignment, we construct a con-
fusion network from the (possibly reordered)
translation hypotheses, similarly to the approach
of (Bangalore et al, 2001). Using global system
probabilities and other statistical models, the vot-
ing procedure selects the best consensus hypoth-
esis from the confusion network. This consen-
sus translation may be different from the original
translations.
This paper is organized as follows. In Section 2,
we will describe the computation of consensus
translations with our approach. In particular, we
will present details of the enhanced alignment and
reordering procedure. A large set of experimental
results on several machine translation tasks is pre-
sented in Section 3, which is followed by a sum-
mary.
2 Description of the Algorithm
The proposed approach takes advantage of mul-
tiple translations for a whole test corpus to com-
pute a consensus translation for each sentence in
this corpus. Given a single source sentence in the
test corpus, we combine M translation hypothe-
ses E1, . . . , EM from M MT engines. We first
choose one of the hypotheses Em as the primary
one. We consider this primary hypothesis to have
the ?correct? word order. We then align and re-
order the other, secondary hypotheses En(n =
1, ..., M ;n 6= m) to match this word order. Since
each hypothesis may have an acceptable word or-
der, we let every hypothesis play the role of the
primary translation once, and thus align all pairs
of hypotheses (En, Em); n 6= m.
In the following subsections, we will explain
the word alignment procedure, the reordering ap-
proach, and the construction of confusion net-
works.
2.1 Statistical Alignment
The word alignment is performed in analogy to the
training procedure in SMT. The difference is that
the two sentences that have to be aligned are in the
same language. We consider the conditional prob-
ability Pr(En|Em) of the event that, given Em,
another hypothesis En is generated from the Em.
Then, the alignment between the two hypotheses
is introduced as a hidden variable:
Pr(En|Em) =
?
A
Pr(En,A|Em)
This probability is then decomposed into the align-
ment probability Pr(A|Em) and the lexicon prob-
ability Pr(En|A, Em):
Pr(En,A|Em) = Pr(A|Em) ? Pr(En|A, Em)
As in statistical machine translation, we make
modelling assumptions. We use the IBM Model 1
(Brown et al, 1993) (uniform distribution) and the
Hidden Markov Model (HMM, first-order depen-
dency, (Vogel et al, 1996)) to estimate the align-
ment model. The lexicon probability of a sentence
pair is modelled as a product of single-word based
probabilities of the aligned words.
The training corpus for alignment is created
from a test corpus of N sentences (usually a few
hundred) translated by all of the involved MT en-
gines. However, the effective size of the training
corpus is larger than N , since all pairs of different
hypotheses have to be aligned. Thus, the effective
size of the training corpus is M ? (M ?1) ?N . The
single-word based lexicon probabilities p(en|em)
are initialized with normalized lexicon counts col-
lected over the sentence pairs (En, Em) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring equal words, i. e.
if en is the same word as em. In addition, we add
a fraction of a count for words with identical pre-
fixes. The initialization could be furthermore im-
proved by using word classes, part-of-speech tags,
or a list of synonyms.
The model parameters are trained iteratively in
an unsupervised manner with the EM algorithm
using the GIZA++ toolkit (Och and Ney, 2003).
The training is performed in the directions En ?
Em and Em ? En. The updated lexicon tables
from the two directions are interpolated after each
iteration.
The final alignments are determined using cost
matrices defined by the state occupation probabil-
ities of the trained HMM (Matusov et al, 2004).
The alignments are used for reordering each sec-
ondary translation En and for computing the con-
fusion network.
34
Figure 1: Example of creating a confusion network frommonotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an ?-arc in the corresponding part of the confusion network.
1. would you like coffee or tea
original 2. would you have tea or coffee
hypotheses 3. would you like your coffee or
4. I have some coffee tea would you like
alignment would|would you|you have|like coffee|coffee or|or tea|tea
and would|would you|you like|like your|$ coffee|coffee or|or $|tea
reordering I|$ would|would you|you like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would you like $ $ coffee or tea
confusion $ would you have $ $ coffee or tea
network $ would you like your $ coffee or $
I would you like have some coffee $ tea
2.2 Word Reordering
The alignment between En and the primary hy-
pothesis Em used for reordering is computed as a
function of words in the secondary translation En
with minimal costs, with an additional constraint
that identical words in En can not be all aligned to
the same word in Em. This constraint is necessary
to avoid that reordered hypotheses with e. g. multi-
ple consecutive articles ?the? would be produced if
fewer articles were used in the primary hypothesis.
The new word order for En is obtained through
sorting the words in En by the indices of the words
in Em to which they are aligned. Two words in
En which are aligned to the same word in Em are
kept in the original order. After reordering each
secondary hypothesis En, we determine M ? 1
monotone one-to-one alignments between Em and
En, n = 1, . . . ,M ; n 6= m. In case of many-to-
one connections of words in En to a single word in
Em, we only keep the connection with the lowest
alignment costs. The one-to-one alignments are
convenient for constructing a confusion network
in the next step of the algorithm.
2.3 Building Confusion Networks
Given the M?1 monotone one-to-one alignments,
the transformation to a confusion network as de-
scribed by (Bangalore et al, 2001) is straightfor-
ward. It is explained by the example in Figure 1.
Here, the original 4 hypotheses are shown, fol-
lowed by the alignment of the reordered secondary
hypotheses 2-4 with the primary hypothesis 1. The
alignment is shown with the | symbol, and the
words of the primary hypothesis are to the right
of this symbol. The symbol $ denotes a null align-
ment or an ?-arc in the corresponding part of the
confusion network, which is shown at the bottom
of the figure.
Note that the word ?have? in translation 2 is
aligned to the word ?like? in translation 1. This
alignment is acceptable considering the two trans-
lations alone. However, given the presence of the
word ?have? in translation 4, this is not the best
alignment. Yet the problems of this type can in
part be solved by the proposed approach, since ev-
ery translation once plays the role of the primary
translation. For each sentence, we obtain a total of
M confusion networks and unite them in a single
lattice. The consensus translation can be chosen
among different alignment and reordering paths in
this lattice.
The ?voting? on the union of confusion net-
works is straightforward and analogous to the
ROVER system. We sum up the probabilities of
the arcs which are labeled with the same word
and have the same start and the same end state.
These probabilities are the global probabilities as-
signed to the different MT systems. They are man-
ually adjusted based on the performance of the in-
volvedMT systems on a held-out development set.
In general, a better consensus translation can be
produced if the words hypothesized by a better-
performing system get a higher probability. Ad-
ditional scores like word confidence measures can
be used to score the arcs in the lattice.
2.4 Extracting Consensus Translation
In the final step, the consensus translation is ex-
tracted as the best path from the union of confu-
35
Table 1: Corpus statistics of the test corpora.
BTEC IWSLT04 BTEC CSTAR03 EPPS TC-STAR
Chinese Japanese English Italian English Spanish English
Sentences 500 506 1 073
Running Words 3 681 4 131 3 092 3 176 2 942 2 889 18 896 18 289
Distinct Words 893 979 1 125 1 134 1 028 942 3 302 3 742
sion networks. Note that the extracted consensus
translation can be different from the original M
translations. Alternatively, the N -best hypothe-
ses can be extracted for rescoring by additional
models. We performed experiments with both ap-
proaches.
Since M confusion networks are used, the lat-
tice may contain two best paths with the same
probability, the same words, but different word
order. We extended the algorithm to favor more
well-formed word sequences. We assign a higher
probability to each arc of the primary (unre-
ordered) translation in each of the M confusion
networks. Experimentally, this extension im-
proved translation fluency on some tasks.
3 Experimental Results
3.1 Corpus Statistics
The alignment and voting algorithm was evaluated
on both small and large vocabulary tasks. Initial
experiments were performed on the IWSLT 2004
Chinese-English and Japanese-English tasks (Ak-
iba et al, 2004). The data for these tasks come
from the Basic Travel Expression corpus (BTEC),
consisting of tourism-related sentences. We com-
bined the outputs of several MT systems that had
officially been submitted to the IWSLT 2004 eval-
uation. Each system had used 20K sentence pairs
(180K running words) from the BTEC corpus for
training.
Experiments with translations of automatically
recognized speech were performed on the BTEC
Italian-English task (Federico, 2003). Here, the
involved MT systems had used about 60K sen-
tence pairs (420K running words) for training.
Finally, we also computed consensus translation
from some of the submissions to the TC-STAR
2005 evaluation campaign (TC-STAR, 2005). The
TC-STAR participants had submitted translations
of manually transcribed speeches from the Euro-
pean Parliament Plenary Sessions (EPPS). In our
experiments, we used the translations from Span-
Table 2: Improved translation results for the con-
sensus translation computed from 5 translation
outputs on the Chinese-English IWSLT04 task.
BTEC WER PER BLEU
Chinese-English [%] [%] [%]
worst single system ?04 58.3 46.6 34.6
best single system? ?04 54.6 42.6 40.3
consensus of 5 systems
from 2004 47.8 38.0 46.2
system (*) in 2005 50.3 40.5 45.1
ish to English. The MT engines for this task had
been trained on 1.2M sentence pairs (32M running
words).
Table 1 gives an overview of the test corpora,
on which the enhanced hypotheses alignment was
computed, and for which the consensus transla-
tions were determined. The official IWSLT04
test corpus was used for the IWSLT 04 tasks; the
CSTAR03 test corpus was used for the speech
translation task. The March 2005 test corpus of
the TC-STAR evaluation (verbatim condition) was
used for the EPPS task. In Table 1, the number of
running words in English is the average number of
running words in the hypotheses, from which the
consensus translation was computed; the vocabu-
lary of English is the merged vocabulary of these
hypotheses. For the BTEC IWSLT04 corpus, the
statistics for English is given for the experiments
described in Sections 3.3 and 3.5, respectively.
3.2 Evaluation Criteria
Well-established objective evaluation measures
like the word error rate (WER), position-
independent word error rate (PER), and the BLEU
score (Papineni et al, 2002) were used to assess
the translation quality. All measures were com-
puted with respect to multiple reference transla-
tions. The evaluation (as well as the alignment
training) was case-insensitive, without consider-
ing the punctuation marks.
36
3.3 Chinese-English Translation
Different applications of the proposed combina-
tion method have been evaluated. First, we fo-
cused on combining different MT systems which
have the same source and target language. The
initial experiments were performed on the BTEC
Chinese-English task. We combined translations
produced by 5 different MT systems. Table 2
shows the performance of the best and the worst of
these systems in terms of the BLEU score. The re-
sults for the consensus translation show a dramatic
improvement in translation quality. The word er-
ror rate is reduced e. g. from 54.6 to 47.8%. The
research group which had submitted the best trans-
lation in 2004 translated the same test set a year
later with an improved system. We compared
the consensus translation with this new translation
(last line of Table 2). It can be observed that the
consensus translation based on the MT systems
developed in 2004 is still superior to this 2005 sin-
gle system translation in terms of all error mea-
sures.
We also checked how many sentences in the
consensus translation of the test corpus are differ-
ent from the 5 original translations. 185 out of 500
sentences (37%) had new translations. Computing
the error measures on these sentences only, we ob-
served significant improvements in WER and PER
and a small improvement in BLEU with respect
to the original translations. Thus, the quality of
previously unseen consensus translations as gen-
erated from the original translations is acceptable.
In this experiment, the global system proba-
bilities for scoring the confusion networks were
tuned manually on a development set. The distri-
bution was 0.35, 0.25, 0.2, 0.1, 0.1, with 0.35 for
the words of the best single system and 0.1 for the
words of the worst single system. We observed
that the consensus translation did not change sig-
nificantly with small perturbations of these val-
ues. However, the relation between the proba-
bilities is very important for good performance.
No improvement can be achieved with a uniform
probability distribution ? it is necessary to penal-
ize translations of low quality.
3.4 Spanish-English Translation
The improvements in translation quality are
also significant on the TC-STAR EPPS Spanish-
English task. Here, we combined four different
systems which performed best in the TC-STAR
Table 3: Improved translation results for the con-
sensus translation computed from 4 translation
outputs on the Spanish-English TC-STAR task.
EPPS WER PER BLEU
Spanish-English [%] [%] [%]
worst single system 49.1 38.2 39.6
best single system 41.0 30.2 47.7
consensus of 4 systems 39.1 29.1 49.3
+ rescoring 38.8 29.0 50.7
2005 evaluation, see Table 3. Compared to the
best performing single system, the consensus hy-
pothesis reduces the WER from 41.0 to 39.1%.
This result is further improved by rescoring the
N -best lists derived from the confusion networks
(N=1000). For rescoring, a word penalty fea-
ture, the IBM Model 1, and a 4-gram target lan-
guage model were included. The linear interpola-
tion weights of these models and the score from
the confusion network were optimized on a sep-
arate development set with respect to word error
rate.
Table 4 gives examples of improved translation
quality by using the consensus translation as de-
rived from the rescored N -best lists.
3.5 Multi-source Translation
In the IWSLT 2004 evaluation, the English ref-
erence translations for the Chinese-English and
Japanese-English test corpora were the same, ex-
cept for a permutation of the sentences. Thus, we
could combine MT systems which have different
source and the same target language, performing
multi-source machine translation (described e. g.
by (Och and Ney, 2001)). We combined two
Japanese-English and two Chinese-English sys-
tems. The best performing system was a Japanese-
English system with a BLEU score of 44.7%, see
Table 5. By computing the consensus translation,
we improved this score to 49.6%, and also signifi-
cantly reduced the error rates.
To investigate the potential of the proposed ap-
proach, we generated the N -best lists (N = 1000)
of consensus translations. Then, for each sentence,
we selected the hypothesis in the N -best list with
the lowest word error rate with respect to the mul-
tiple reference translations for the sentence. We
then evaluated the quality of these ?oracle? trans-
lations with all error measures. In a contrastive
experiment, for each sentence we simply selected
37
Table 4: Examples of improved translation quality with the consensus translations on the Spanish-English
TC-STAR EPPS task (case-insensitive output).
best system I also authorised to committees to certain reports
consensus I also authorised to certain committees to draw up reports
reference I have also authorised certain committees to prepare reports
best system human rights which therefore has fought the european union
consensus human rights which the european union has fought
reference human rights for which the european union has fought so hard
best system we of the following the agenda
consensus moving on to the next point on the agenda
reference we go on to the next point of the agenda
Table 5: Multi-source translation: improvements
in translation quality when computing consen-
sus translation using the output of two Chinese-
English and two Japanese-English systems on the
IWSLT04 task.
BTEC Chinese-English WER PER BLEU
+ Japanese-English [%] [%] [%]
worst single system 58.0 41.8 39.5
best single system 51.3 38.6 44.7
consensus of 4 systems 44.9 33.9 49.6
Table 6: Consensus-based combination vs. se-
lection: potential for improvement (multi-source
translation, selection/combination of 4 translation
outputs).
BTEC Chinese-English WER PER BLEU
+ Japanese-English [%] [%] [%]
best single system 51.3 38.6 44.7
oracle selection 33.3 29.3 59.2
oracle consensus
(1000-best list) 27.0 22.8 64.2
the translation with the lowest WER from the orig-
inal 4 MT system outputs. Table 6 shows that the
potential for improvement is significantly larger
for the consensus-based combination of transla-
tion outputs than for simple selection of the best
translation1. In our future work, we plan to im-
prove the scoring of hypotheses in the confusion
networks to explore this large potential.
3.6 Speech Translation
Some state-of-the-art speech translation systems
can translate either the first best recognition hy-
1Similar ?oracle? results were observed on other tasks.
potheses or the word lattices of an ASR system. It
has been previously shown that word lattice input
generally improves translation quality. In practice,
however, the translation system may choose, for
some sentences, the paths in the lattice with many
recognition errors and thus produce inferior trans-
lations. These translations can be improved if we
compute a consensus translation from the output
of at least two different speech translation systems.
From each system, we take the translation of the
single best ASR output, and the translation of the
ASR word lattice.
Two different statistical MT systems capable of
translating ASR word lattices have been compared
by (Matusov and Ney, 2005). Both systems pro-
duced translations of better quality on the BTEC
Italian-English speech translation task when using
lattices instead of single best ASR output. We
obtained the output of each of the two systems
under each of these translation scenarios on the
CSTAR03 test corpus. The first-best recognition
word error rate on this corpus is 22.3%. The objec-
tive error measures for the 4 translation hypothe-
ses are given in Table 7. We then computed a con-
sensus translation of the 4 outputs with the pro-
posed method. The better performing word lattice
translations were given higher system probabili-
ties. With the consensus hypothesis, the word er-
ror rate went down from 29.5 to 28.5%. Thus, the
negative effect of recognition errors on the trans-
lation quality was further reduced.
4 Conclusions
In this work, we proposed a novel, theoretically
well-founded procedure for computing a possi-
bly new consensus translation from the outputs of
multiple MT systems. In summary, the main con-
38
Table 7: Improvements in translation quality on
the BTEC Italian-English task through comput-
ing consensus translations from the output of two
speech translation systems with different types of
source language input.
system input WER PER BLEU
[%] [%] [%]
2 correct text 23.3 19.3 65.6
1 a) single best 32.8 28.6 53.9
b) lattice 30.7 26.7 55.9
2 c) single best 31.6 27.5 54.7
d) lattice 29.5 26.1 58.2
consensus a-d 28.5 25.0 58.9
tributions of this work compared to previous ap-
proaches are as follows:
? The words of the original translation hy-
potheses are aligned in order to create a con-
fusion network. The alignment procedure ex-
plicitly models word reordering.
? A test corpus of translations generated by
each of the systems is used for the unsuper-
vised statistical alignment training. Thus, the
decision on how to align two translations of
a sentence takes the whole document context
into account.
? Large and significant gains in translation
quality were obtained on various translation
tasks and conditions.
? A significant improvement of translation
quality was achieved in a multi-source trans-
lation scenario. Here, we combined the
output of MT systems which have different
source and the same target language.
? The proposed method can be effectively ap-
plied in speech translation in order to cope
with the negative impact of speech recogni-
tion errors on translation accuracy.
An important feature of a real-life application of
the proposed alignment technique is that the lex-
icon and alignment probabilities can be updated
with each translated sentence and/or text. Thus,
the correspondence between words in different hy-
potheses and, consequently, the consensus transla-
tion can be improved overtime.
5 Acknowledgement
This paper is based upon work supported by
the Defense Advanced Research Projects Agency
(DARPA) under Contract No. HR0011-06-C-
0023. This work was also in part funded by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa,
M. Paul, and J. Tsujii. 2004. Overview of the
IWSLT04 Evaluation Campaign. Int. Workshop
on Spoken Language Translation, pp. 1?12, Kyoto,
Japan.
S. Bangalore, G. Bordel, G. Riccardi. 2001. Comput-
ing Consensus Translation from Multiple Machine
Translation Systems. IEEE Workshop on Automatic
Speech Recognition and Understanding, Madonna
di Campiglio, Italy.
P. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statisti-
cal Machine Translation. Computational Linguis-
tics, vol. 19(2):263?311.
J. G. Fiscus. 1997. A Post-Processing System to Yield
Reduced Word Error Rates: Recognizer Output Vot-
ing Error Reduction (ROVER). IEEE Workshop on
Automatic Speech Recognition and Understanding.
S. Jayaraman and A. Lavie. 2005. Multi-Engline Ma-
chine Translation Guided by Explicit Word Match-
ing. 10th Conference of the European Association
for Machine Translation, pp. 143-152, Budapest,
Hungary.
M. Federico 2003. Evaluation Frameworks for Speech
Translation Technologies. Proc. of Eurospeech,
pp. 377-380, Geneva, Switzerland.
R. Frederking and S. Nirenburg. 1994. Three Heads
are Better Than One. Fourth Conference on Applied
Natural Language Processing, Stuttgart, Germany.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
Word Alignments for Statistical Machine Transla-
tion. 20th Int. Conf. on Computational Linguistics,
pp. 219?225, Geneva, Switzerland.
E. Matusov and H. Ney. 2005. Phrase-based Trans-
lation of Speech Recognizer Word Lattices Using
Loglinear Model Combination. IEEE Workshop on
Automatic Speech Recognition and Understanding,
pp. 110-115, San Juan, Puerto-Rico.
T. Nomoto. 2004. Multi-Engine Machine Transla-
tion with Voted Language Model. 42nd Confer-
ence of the Association for Computational Linguis-
tics (ACL), pp. 494-501, Barcelona, Spain.
39
F. J. Och and H. Ney. 2001. Statistical Multi-Source
Translation. MT Summit VIII, pp. 253-258, Santi-
ago de Compostela, Spain.
F. J. Och and H. Ney. 2003. A Systematic Comparison
of Various Statistical Alignment Models. Computa-
tional Linguistics, 29(1):19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. Annual Meeting of the ACL, pp.
311?318, Philadelphia, PA, USA.
M. Paul, T. Doi, Y. Hwang, K. Imamura, H. Okuma,
and E. Sumita. 2005. Nobody is Perfect: ATR?s
Hybrid Approach to Spoken Language Translation.
International Workshop on Spoken Language Trans-
lation, pp. 55-62, Pittsburgh, PA, USA.
TC-STAR Spoken Language Translation Progress Re-
port. 2005. http://www.tc-star.org/documents/
deliverable/Deliv D5 Total 21May05.pdf
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
Word Alignment in Statistical Translation. 16th Int.
Conf. on Computational Linguistics, pp. 836?841,
Copenhagen, Denmark.
40
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167?174,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
Novel Reordering Approaches in Phrase-Based Statistical Machine
Translation
Stephan Kanthak, David Vilar, Evgeny Matusov, Richard Zens, and Hermann Ney
The authors are with the Lehrstuhl fu?r Informatik VI,
Computer Science Department, RWTH Aachen University,
D-52056 Aachen, Germany.
E-mail: {kanthak,vilar,matusov,zens,ney}@informatik.rwth-aachen.de.
Abstract
This paper presents novel approaches to
reordering in phrase-based statistical ma-
chine translation. We perform consistent
reordering of source sentences in train-
ing and estimate a statistical translation
model. Using this model, we follow a
phrase-based monotonic machine transla-
tion approach, for which we develop an ef-
ficient and flexible reordering framework
that allows to easily introduce different re-
ordering constraints. In translation, we
apply source sentence reordering on word
level and use a reordering automaton as in-
put. We show how to compute reordering
automata on-demand using IBM or ITG
constraints, and also introduce two new
types of reordering constraints. We further
add weights to the reordering automata.
We present detailed experimental results
and show that reordering significantly im-
proves translation quality.
1 Introduction
Reordering is of crucial importance for machine
translation. Already (Knight et al, 1998) use full un-
weighted permutations on the level of source words
in their early weighted finite-state transducer ap-
proach which implemented single-word based trans-
lation using conditional probabilities. In a refine-
ment with additional phrase-based models, (Kumar
et al, 2003) define a probability distribution over
all possible permutations of source sentence phrases
and prune the resulting automaton to reduce com-
plexity.
A second category of finite-state translation ap-
proaches uses joint instead of conditional probabili-
ties. Many joint probability approaches originate in
speech-to-speech translation as they are the natural
choice in combination with speech recognition mod-
els. The automated transducer inference techniques
OMEGA (Vilar, 2000) and GIATI (Casacuberta et
al., 2004) work on phrase level, but ignore the re-
ordering problem from the view of the model. With-
out reordering both in training and during search,
sentences can only be translated properly into a lan-
guage with similar word order. In (Bangalore et al,
2000) weighted reordering has been applied to tar-
get sentences since defining a permutation model on
the source side is impractical in combination with
speech recognition. In order to reduce the computa-
tional complexity, this approach considers only a set
of plausible reorderings seen on training data.
Most other phrase-based statistical approaches
like the Alignment Template system of Bender
et al (2004) rely on (local) reorderings which are
implicitly memorized with each pair of source and
target phrases in training. Additional reorderings on
phrase level are fully integrated into the decoding
process, which increases the complexity of the sys-
tem and makes it hard to modify. Zens et al (2003)
reviewed two types of reordering constraints for this
type of translation systems.
In our work we follow a phrase-based transla-
tion approach, applying source sentence reordering
on word level. We compute a reordering graph on-
demand and take it as input for monotonic trans-
lation. This approach is modular and allows easy
introduction of different reordering constraints and
probabilistic dependencies. We will show that it per-
forms at least as well as the best statistical machine
translation system at the IWSLT Evaluation.
167
In the next section we briefly review the basic
theory of our translation system based on weighted
finite-state transducers (WFST). In Sec. 3 we in-
troduce new methods for reordering and alignment
monotonization in training. To compare differ-
ent reordering constraints used in the translation
search process we develop an on-demand com-
putable framework for permutation models in Sec. 4.
In the same section we also define and analyze un-
restricted and restricted permutations with some of
them being first published in this paper. We con-
clude the paper by presenting and discussing a rich
set of experimental results.
2 Machine Translation using WFSTs
Let fJ1 and eIi be two sentences from a source and
target language. Assume that we have word level
alignments A of all sentence pairs from a bilingual
training corpus. We denote with e?J1 the segmenta-
tion of a target sentence eI1 into J phrases such that
fJ1 and e?J1 can be aligned to form bilingual tuples
(fj , e?j). If alignments are only functions of target
words A? : {1, . . . , I} ? {1, . . . , J}, the bilingual
tuples (fj , e?j) can be inferred with e. g. the GIATI
method of (Casacuberta et al, 2004), or with our
novel monotonization technique (see Sec. 3). Each
source word will be mapped to a target phrase of one
or more words or an ?empty? phrase ?. In particular,
the source words which will remain non-aligned due
to the alignment functionality restriction are paired
with the empty phrase.
We can then formulate the problem of finding the
best translation e?I1 of a source sentence fJ1 :
e?I1 = argmax
eI1
Pr(fJ1 , e
I
1)
= argmax
e?J1
?
A?A
Pr(fJ1 , e?
J
1 , A)
?= argmax
e?J1
max
A?A
Pr(A) ? Pr(fJ1 , e?
J
1 |A)
?= argmax
e?J1
max
A?A
?
fj :j=1...J
Pr(fj , e?j |f
j?1
1 , e?
j?1
1 , A)
= argmax
e?J1
max
A?A
?
fj :j=1...J
p(fj , e?j |f
j?1
j?m, e?
j?1
j?m, A)
In other words: if we assume a uniform distri-
bution for Pr(A), the translation problem can be
mapped to the problem of estimating an m-gram lan-
guage model over a learned set of bilingual tuples
(fj , e?j). Mapping the bilingual language model to a
WFST T is canonical and it has been shown in (Kan-
thak et al, 2004) that the search problem can then be
rewritten using finite-state terminology:
e?I1 = project-output(best(fJ1 ? T )) .
This implementation of the problem as WFSTs may
be used to efficiently solve the search problem in
machine translation.
3 Reordering in Training
When the alignment function A? is not monotonic,
target language phrases e? can become very long.
For example in a completely non-monotonic align-
ment all target words are paired with the last aligned
source word, whereas all other source words form
tuples with the empty phrase. Therefore, for lan-
guage pairs with big differences in word order, prob-
ability estimates may be poor.
This problem can be solved by reordering either
source or target training sentences such that align-
ments become monotonic for all sentences. We
suggest the following consistent source sentence re-
ordering and alignment monotonization approach in
which we compute optimal, minimum-cost align-
ments.
First, we estimate a cost matrix C for each sen-
tence pair (fJ1 , eI1). The elements of this matrix cij
are the local costs of aligning a source word fj to a
target word ei. Following (Matusov et al, 2004), we
compute these local costs by interpolating state oc-
cupation probabilities from the source-to-target and
target-to-source training of the HMM and IBM-4
models as trained by the GIZA++ toolkit (Och et al,
2003). For a given alignment A ? I ? J , we define
the costs of this alignment c(A) as the sum of the
local costs of all aligned word pairs:
c(A) =
?
(i,j)?A
cij (1)
The goal is to find an alignment with the minimum
costs which fulfills certain constraints.
3.1 Source Sentence Reordering
To reorder a source sentence, we require the
alignment to be a function of source words A1:
{1, . . . , J} ? {1, . . . , I}, easily computed from the
cost matrix C as:
A1(j) = argmini cij (2)
168
We do not allow for non-aligned source words. A1
naturally defines a new order of the source words fJ1
which we denote by f?J1 . By computing this permu-
tation for each pair of sentences in training and ap-
plying it to each source sentence, we create a corpus
of reordered sentences.
3.2 Alignment Monotonization
In order to create a ?sentence? of bilingual tuples
(f?J1 , e?
J
1 ) we required alignments between reordered
source and target words to be a function of target
words A2 : {1, . . . , I} ? {1, . . . , J}. This align-
ment can be computed in analogy to Eq. 2 as:
A2(i) = argminj c?ij (3)
where c?ij are the elements of the new cost matrix
C? which corresponds to the reordered source sen-
tence. We can optionally re-estimate this matrix by
repeating EM training of state occupation probabili-
ties with GIZA++ using the reordered source corpus
and the original target corpus. Alternatively, we can
get the cost matrix C? by reordering the columns of
the cost matrix C according to the permutation given
by alignment A1.
In alignment A2 some target words that were pre-
viously unaligned in A1 (like ?the? in Fig. 1) may
now still violate the alignment monotonicity. The
monotonicity of this alignment can not be guaran-
teed for all words if re-estimation of the cost matri-
ces had been performed using GIZA++.
The general GIATI technique (Casacuberta et al,
2004) is applicable and can be used to monotonize
the alignment A2. However, in our experiments
the following method performs better. We make
use of the cost matrix representation and compute
a monotonic minimum-cost alignment with a dy-
namic programming algorithm similar to the Lev-
enshtein string edit distance algorithm. As costs of
each ?edit? operation we consider the local align-
ment costs. The resulting alignment A3 represents
a minimum-cost monotonic ?path? through the cost
matrix. To make A3 a function of target words we
do not consider the source words non-aligned in A2
and also forbid ?deletions? (?many-to-one? source
word alignments) in the DP search.
An example of such consistent reordering and
monotonization is given in Fig. 1. Here, we re-
order the German source sentence based on the ini-
tial alignment A1, then compute the function of tar-
get words A2, and monotonize this alignment to A3
the very beginning of May would suit me .
the very beginning of May would suit me .
sehr gut Anfang Mai w?rde passen mir .
sehr gut Anfang Mai w?rde passen mir .
the very beginning of May would suit me .
mir sehrw?rde gut Anfang Mai passen .
.Mai|of_May w?rde|would passen|suit mir|me |.sehr|the_very gut|$ Anfang|beginning
A
A
A1
2
3
Figure 1: Example of alignment, source sentence re-
ordering, monotonization, and construction of bilin-
gual tuples.
with the dynamic programming algorithm. Fig. 1
also shows the resulting bilingual tuples (f?j , e?j).
4 Reordering in Search
When searching the best translation e?J1 for a given
source sentence fJ1 , we permute the source sentence
as described in (Knight et al, 1998):
e?I1 = project-output(best(permute(fJ1 ) ? T ))
Permuting an input sequence of J symbols re-
sults in J ! possible permutations and representing
the permutations as a finite-state automaton requires
at least 2J states. Therefore, we opt for computing
the permutation automaton on-demand while apply-
ing beam pruning in the search.
4.1 Lazy Permutation Automata
For on-demand computation of an automaton in the
flavor described in (Kanthak et al, 2004) it is suffi-
cient to specify a state description and an algorithm
that calculates all outgoing arcs of a state from the
state description. In our case, each state represents
a permutation of a subset of the source words fJ1 ,
which are already translated.
This can be described by a bit vector bJ1 (Zens
et al, 2002). Each bit of the state bit vector corre-
sponds to an arc of the linear input automaton and is
set to one if the arc has been used on any path from
the initial to the current state. The bit vectors of two
states connected by an arc differ only in a single bit.
Note that bit vectors elegantly solve the problem of
recombining paths in the automaton as states with
169
the same bit vectors can be merged. As a result, a
fully minimized permutation automaton has only a
single initial and final state.
Even with on-demand computation, complexity
using full permutations is unmanagable for long sen-
tences. We further reduce complexity by addition-
ally constraining permutations. Refer to Figure 2 for
visualizations of the permutation constraints which
we describe in the following.
4.2 IBM Constraints
The IBM reordering constraints are well-known in
the field of machine translation and were first de-
scribed in (Berger et al, 1996). The idea behind
these constraints is to deviate from monotonic trans-
lation by postponing translations of a limited num-
ber of words. More specifically, at each state we
can translate any of the first l yet uncovered word
positions. The implementation using a bit vector is
straightforward. For consistency, we associate win-
dow size with the parameter l for all constraints pre-
sented here.
4.3 Inverse IBM Constraints
The original IBM constraints are useful for a large
number of language pairs where the ability to skip
some words reflects the differences in word order
between the two languages. For some other pairs,
it is beneficial to translate some words at the end of
the sentence first and to translate the rest of the sen-
tence nearly monotonically. Following this idea we
can define the inverse IBM constraints. Let j be the
first uncovered position. We can choose any posi-
tion for translation, unless l ? 1 words on positions
j? > j have been translated. If this is the case we
must translate the word in position j. The inverse
IBM constraints can also be expressed by
invIBM(x) = transpose(IBM(transpose(x))) .
As the transpose operation can not be computed
on-demand, our specialized implementation uses bit
vectors bJ1 similar to the IBM constraints.
4.4 Local Constraints
For some language pairs, e.g. Italian ? English,
words are moved only a few words to the left or
right. The IBM constraints provide too many alter-
native permutations to chose from as each word can
be moved to the end of the sentence. A solution that
allows only for local permutations and therefore has
a)
0000 10001 11002 11103 11114
b)
0000
10001
01002 1100
2
10103
1
0110
3
11103
110141
01114
1111
4
13
2
10114 2
c)
0000
10001
01002
0010
3
0001
4 10014
1010
3 11002
1
1
1
1101
2
11113
11102 4
43
d)
0000
10001
01002 1100
2
10103
1
11103
11014 1111
43
2
Figure 2: Permutations of a) positions j = 1, 2, 3, 4
of a source sentence f1f2f3f4 using a window size
of 2 for b) IBM constraints, c) inverse IBM con-
straints and d) local constraints.
very low complexity is given by the following per-
mutation rule: the next word for translation comes
from the window of l positions1 counting from the
first yet uncovered position. Note, that the local con-
straints define a true subset of the permutations de-
fined by the IBM constraints.
4.5 ITG Constraints
Another type of reordering can be obtained using In-
version Transduction Grammars (ITG) (Wu, 1997).
These constraints are inspired by bilingual bracket-
ing. They proved to be quite useful for machine
translation, e.g. see (Bender et al, 2004). Here,
we interpret the input sentence as a sequence of seg-
ments. In the beginning, each word is a segment of
its own. Longer segments are constructed by recur-
sively combining two adjacent segments. At each
1both covered and uncovered
170
Chinese English Japanese English Italian English
train sentences 20 000 20 000 66107
words 182 904 160 523 209 012 160 427 410 275 427 402
singletons 3 525 2 948 4 108 2 956 6 386 3 974
vocabulary 7 643 6 982 9 277 6 932 15 983 10 971
dev sentences 506 506 500
words 3 515 3 595 4 374 3 595 3 155 3 253
sentence length (avg/max) 6.95 / 24 7.01 / 29 8.64 / 30 7.01 / 29 5.79 / 24 6.51 / 25
test sentences 500 500 506
words 3 794 ? 4 370 ? 2 931 3 595
sentence length (avg/max) 7.59 / 62 7.16 / 71 8.74 / 75 7.16 / 71 6.31 / 27 6.84 / 28
Table 1: Statistics of the Basic Travel Expression (BTEC) corpora.
combination step, we either keep the two segments
in monotonic order or invert the order. This pro-
cess continues until only one segment for the whole
sentence remains. The on-demand computation is
implemented in spirit of Earley parsing.
We can modify the original ITG constraints to
further limit the number of reorderings by forbid-
ding segment inversions which violate IBM con-
straints with a certain window size. Thus, the re-
sulting reordering graph contains the intersection of
the reorderings with IBM and the original ITG con-
straints.
4.6 Weighted Permutations
So far, we have discussed how to generate the per-
mutation graphs under different constraints, but per-
mutations were equally probable. Especially for the
case of nearly monotonic translation it is make sense
to restrict the degree of non-monotonicity that we
allow when translating a sentence. We propose a
simple approach which gives a higher probability
to the monotone transitions and penalizes the non-
monotonic ones.
A state description bJ1 , for which the following
condition holds:
Mon(j) : bj? = ?(j
? ? j) ? 1 ? j? ? J
represents the monotonic path up to the word fj . At
each state we assign the probability ? to that out-
going arc where the target state description fullfills
Mon(j+1) and distribute the remaining probability
mass 1? ? uniformly among the remaining arcs. In
case there is no such arc, all outgoing arcs get the
same uniform probability. This weighting scheme
clearly depends on the state description and the out-
going arcs only and can be computed on-demand.
5 Experimental Results
5.1 Corpus Statistics
The translation experiments were carried out on the
Basic Travel Expression Corpus (BTEC), a multilin-
gual speech corpus which contains tourism-related
sentences usually found in travel phrase books.
We tested our system on the so called Chinese-to-
English (CE) and Japanese-to-English (JE) Supplied
Tasks, the corpora which were provided during the
International Workshop on Spoken Language Trans-
lation (IWSLT 2004) (Akiba et al, 2004). In ad-
dition, we performed experiments on the Italian-to-
English (IE) task, for which a larger corpus was
kindly provided to us by ITC/IRST. The corpus
statistics for the three BTEC corpora are given in
Tab. 1. The development corpus for the Italian-to-
English translation had only one reference transla-
tion of each Italian sentence. A set of 506 source
sentences and 16 reference translations is used as
a development corpus for Chinese-to-English and
Japanese-to-English and as a test corpus for Italian-
to-English tasks. The 500 sentence Chinese and
Japanese test sets of the IWSLT 2004 evaluation
campaign were translated and automatically scored
against 16 reference translations after the end of the
campaign using the IWSLT evaluation server.
5.2 Evaluation Criteria
For the automatic evaluation, we used the crite-
ria from the IWSLT evaluation campaign (Akiba et
al., 2004), namely word error rate (WER), position-
independent word error rate (PER), and the BLEU
and NIST scores (Papineni et al, 2002; Doddington,
2002). The two scores measure accuracy, i. e. larger
scores are better. The error rates and scores were
computed with respect to multiple reference transla-
171
 40
 42
 44
 46
 48
 50
 52
 54
 56
 58
 60
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
 46
 47
 48
 49
 50
 51
 52
 53
 54
 55
 1  2  3  4  5  6  7  8  9
reordering constraints window size
INV-IBMIBMITGLOCAL
Figure 3: Word error rate [%] as a function of the reordering window size for different reordering constraints:
Japanese-to-English (left) and Chinese-to-English (right) translation.
tions, when they were available. To indicate this, we
will label the error rate acronyms with an m. Both
training and evaluation were performed using cor-
pora and references in lowercase and without punc-
tuation marks.
5.3 Experiments
We used reordering and alignment monotonization
in training as described in Sec. 3. To estimate the
matrices of local alignment costs for the sentence
pairs in the training corpus we used the state occupa-
tion probabilities of GIZA++ IBM-4 model training
and interpolated the probabilities of source-to-target
and target-to-source training directions. After that
we estimated a smoothed 4-gram language model on
the level of bilingual tuples fj , e?j and represented it
as a finite-state transducer.
When translating, we applied moderate beam
pruning to the search automaton only when using re-
ordering constraints with window sizes larger than 3.
For very large window sizes we also varied the prun-
ing thresholds depending on the length of the input
sentence. Pruning allowed for fast translations and
reasonable memory consumption without a signifi-
cant negative impact on performance.
In our first experiments, we tested the four re-
ordering constraints with various window sizes. We
aimed at improving the translation results on the de-
velopment corpora and compared the results with
two baselines: reordering only the source training
sentences and translation of the unreordered test sen-
tences; and the GIATI technique for creating bilin-
gual tuples (fj , e?j) without reordering of the source
sentences, neither in training nor during translation.
5.3.1 Highly Non-Monotonic Translation (JE)
Fig. 3 (left) shows word error rate on the
Japanese-to-English task as a function of the win-
dow size for different reordering constraints. For
each of the constraints, good results are achieved
using a window size of 9 and larger. This can be
attributed to the Japanese word order which is very
different from English and often follows a subject-
object-verb structure. For small window sizes, ITG
or IBM constraints are better suited for this task, for
larger window sizes, inverse IBM constraints per-
form best. The local constraints perform worst and
require very large window sizes to capture the main
word order differences between Japanese and En-
glish. However, their computational complexity is
low; for instance, a system with local constraints
and window size of 9 is as fast (25 words per sec-
ond) as the same system with IBM constraints and
window size of 5. Using window sizes larger than
10 is computationally expensive and does not sig-
nificantly improve the translation quality under any
of the constraints.
Tab. 2 presents the overall improvements in trans-
lation quality when using the best setting: inverse
IBM constraints, window size 9. The baseline with-
out reordering in training and testing failed com-
pletely for this task, producing empty translations
for 37 % of the sentences2. Most of the original
alignments in training were non-monotonic which
resulted in mapping of almost all Japanese words to
? when using only the GIATI monotonization tech-
nique. Thus, the proposed reordering methods are of
crucial importance for this task.
2Hence a NIST score of 0 due to the brevity penalty.
172
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
BTEC Japanese-to-English (JE) dev
none 59.7 58.8 13.0 0.00
in training 57.8 39.4 14.7 3.27
+ 9-inv-ibm 40.3 32.1 45.1 8.59
+ rescoring* 39.1 30.9 53.2 9.93
BTEC Chinese-to-English (CE) dev
none 55.2 52.1 24.9 1.34
in training 54.0 42.3 23.0 4.18
+ 7-inv-ibm 47.1 39.4 34.5 6.53
+ rescoring* 48.3 40.7 39.1 8.11
Table 2: Translation results with optimal reorder-
ing constraints and window sizes for the BTEC
Japanese-to-English and Chinese-to-English devel-
opment corpora. *Optimized for the NIST score.
mWER mPER BLEU NIST
[%] [%] [%]
BTEC Japanese-to-English (JE) test
AT 41.9 33.8 45.3 9.49
WFST 42.1 35.6 47.3 9.50
BTEC Chinese-to-English (CE) test
AT 45.6 39.0 40.9 8.55
WFST 46.4 38.8 40.8 8.73
Table 3: Comparison of the IWSLT-2004 automatic
evaluation results for the described system (WFST)
with those of the best submitted system (AT).
Further improvements were obtained with a
rescoring procedure. For rescoring, we produced
a k-best list of translation hypotheses and used the
word penalty and deletion model features, the IBM
Model 1 lexicon score, and target language n-gram
models of the order up to 9. The scaling factors for
all features were optimized on the development cor-
pus for the NIST score, as described in (Bender et
al., 2004).
5.3.2 Moderately Non-Mon. Translation (CE)
Word order in Chinese and English is usually sim-
ilar. However, a few word reorderings over quite
large distances may be necessary. This is especially
true in case of questions, in which question words
like ?where? and ?when? are placed at the end of
a sentence in Chinese. The BTEC corpora contain
many sentences with questions.
The inverse IBM constraints are designed to per-
form this type of reordering (see Sec. 4.3). As shown
in Fig. 3, the system performs well under these con-
mWER mPER BLEU NIST
Reordering: [%] [%] [%]
none 25.6 22.0 62.1 10.46
in training 28.0 22.3 58.1 10.32
+ 4-local 26.3 20.3 62.2 10.81
+ weights 25.3 20.3 62.6 10.79
+ 3-ibm 27.2 20.5 61.4 10.76
+ weights 25.2 20.3 62.9 10.80
+ rescoring* 22.2 19.0 69.2 10.47
Table 4: Translation results with optimal reordering
constraints and window sizes for the test corpus of
the BTEC IE task. *Optimized for WER.
straints already with relatively small window sizes.
Increasing the window size beyond 4 for these con-
straints only marginally improves the translation er-
ror measures for both short (under 8 words) and long
sentences. Thus, a suitable language-pair-specific
choice of reordering constraints can avoid the huge
computational complexity required for permutations
of long sentences.
Tab. 2 includes error measures for the best setup
with inverse IBM constraints with window size of 7,
as well as additional improvements obtained by a k-
best list rescoring.
The best settings for reordering constraints and
model scaling factors on the development corpora
were then used to produce translations of the IWSLT
Japanese and Chinese test corpora. These trans-
lations were evaluated against multiple references
which were unknown to the authors. Our system
(denoted with WFST, see Tab. 3) produced results
competitive with the results of the best system at the
evaluation campaign (denoted with AT (Bender et
al., 2004)) and, according to some of the error mea-
sures, even outperformed this system.
5.3.3 Almost Monotonic Translation (IE)
The word order in the Italian language does not
differ much from the English. Therefore, the abso-
lute translation error rates are quite low and translat-
ing without reordering in training and search already
results in a relatively good performance. This is re-
flected in Tab. 4. However, even for this language
pair it is possible to improve translation quality by
performing reordering both in training and during
translation. The best performance on the develop-
ment corpus is obtained when we constrain the re-
odering with relatively small window sizes of 3 to 4
and use either IBM or local reordering constraints.
173
On the test corpus, as shown in Tab. 4, all error mea-
sures can be improved with these settings.
Especially for languages with similar word order
it is important to use weighted reorderings (Sec. 4.6)
in order to prefer the original word order. Introduc-
tion of reordering weights for this task results in no-
table improvement of most error measures using ei-
ther the IBM or local constraints. The optimal prob-
ability ? for the unreordered path was determined
on the development corpus as 0.5 for both of these
constraints. The results on the test corpus using this
setting are also given in Tab. 4.
6 Conclusion
In this paper, we described a reordering framework
which performs source sentence reordering on word
level. We suggested to use optimal alignment func-
tions for monotonization and improvement of trans-
lation model training. This allowed us to translate
monotonically taking a reordering graph as input.
We then described known and novel reordering con-
straints and their efficient finite-state implementa-
tions in which the reordering graph is computed on-
demand. We also utilized weighted permutations.
We showed that our monotonic phrase-based trans-
lation approach effectively makes use of the reorder-
ing framework to produce quality translations even
from languages with significantly different word or-
der. On the Japanese-to-English and Chinese-to-
English IWSLT tasks, our system performed at least
as well as the best machine translation system.
Acknowledgement
This work was partially funded by the Deutsche
Forschungsgemeinschaft (DFG) under the project
?Statistische Textu?bersetzung? (Ne572/5) and by the
European Union under the integrated project TC-
STAR ? Technology and Corpora for Speech to
Speech Translation (IST-2002-FP6-506738).
References
Y. Akiba, M. Federico, N. Kando, H. Nakaiwa, M. Paul,
and J. Tsujii. 2004. Overview of the IWSLT04 Evalu-
ation Campaign. Proc. Int. Workshop on Spoken Lan-
guage Translation, pp. 1?12, Kyoto, Japan.
S. Bangalore and G. Riccardi. 2000. Stochastic Finite-
State Models for Spoken Language Machine Transla-
tion. Proc. Workshop on Embedded Machine Transla-
tion Systems, pp. 52?59.
O. Bender, R. Zens, E. Matusov, and H. Ney. 2004.
Alignment Templates: the RWTH SMT System. Proc.
Int. Workshop on Spoken Language Translation, pp.
79?84, Kyoto, Japan.
A. L. Berger, P. F. Brown, S. A. Della Pietra, V. J. Della
Pietra, J. R. Gillett, A. S. Kehler, and R. L. Mercer.
1996. Language Translation Apparatus and Method
of Using Context-based Translation Models. United
States Patent 5510981.
F. Casacuberta and E. Vidal. 2004. Machine Transla-
tion with Inferred Stochastic Finite-State Transducers.
Computational Linguistics, vol. 30(2):205-225.
G. Doddington. 2002. Automatic Evaluation of Machine
Translation Quality Using n-gram Co-Occurrence
Statistics. Proc. Human Language Technology Conf.,
San Diego, CA.
S. Kanthak and H. Ney. 2004. FSA: an Efficient and
Flexible C++ Toolkit for Finite State Automata using
On-demand Computation. Proc. 42nd Annual Meet-
ing of the Association for Computational Linguistics,
pp. 510?517, Barcelona, Spain.
K. Knight and Y. Al-Onaizan. 1998. Translation with
Finite-State Devices. Lecture Notes in Artificial Intel-
ligence, Springer-Verlag, vol. 1529, pp. 421?437.
S. Kumar and W. Byrne. 2003. A Weighted Finite State
Transducer Implementation of the Alignment Template
Model for Statistical Machine Translation. Proc. Hu-
man Language Technology Conf. NAACL, pp. 142?
149, Edmonton, Canada.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric Word
Alignments for Statistical Machine Translation. Proc.
20th Int. Conf. on Computational Linguistics, pp. 219?
225, Geneva, Switzerland.
F. J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, vol. 29, number 1, pp. 19?51.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Machine
Translation. Proc. 40th Annual Meeting of the Associ-
ation for Computational Linguistics, Philadelphia, PA,
pp. 311?318.
J. M. Vilar, 2000. Improve the Learning of Sub-
sequential Transducers by Using Alignments and Dic-
tionaries. Lecture Notes in Artificial Intelligence,
Springer-Verlag, vol. 1891, pp. 298?312.
D. Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Corpora.
Computational Linguistics, 23(3):377?403.
R. Zens, F. J. Och and H. Ney. 2002. Phrase-Based Sta-
tistical Machine Translation. In: M. Jarke, J. Koehler,
G. Lakemeyer (Eds.): KI - Conference on AI, KI 2002,
Vol. LNAI 2479, pp. 18-32, Springer Verlag.
R. Zens and H. Ney. 2003. A Comparative Study on
Reordering Constraints in Statistical Machine Trans-
lation. Proc. Annual Meeting of the Association
for Computational Linguistics, pp. 144?151, Sapporo,
Japan.
174
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 51?55,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH System Combination System for WMT 2009
Gregor Leusch, Evgeny Matusov, and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the System Combi-
nation task of the Fourth Workshop on Sta-
tistical Machine Translation (WMT 2009).
Hypotheses from 9 German?English MT
systems were combined into a consen-
sus translation. This consensus transla-
tion scored 2.1% better in BLEU and 2.3%
better in TER (abs.) than the best sin-
gle system. In addition, cross-lingual
output from 10 French, German, and
Spanish?English systems was combined
into a consensus translation, which gave
an improvement of 2.0% in BLEU/3.5% in
TER (abs.) over the best single system.
1 Introduction
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al, 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using the prior prob-
abilities for each system as well as other statistical
models such as a special n-gram language model.
2 System Combination Algorithm
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary hypothesis. From this, we create a confu-
sion network (CN), which we then rescore using
Figure 1: The system combination architecture.
system prior weights and a language model (LM).
The single best path in this CN then constitutes the
consensus translation.
2.1 Word Alignment
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each source sentence F in the test corpus,
we select one of its translations En, n=1, . . . ,M,
as the primary hypothesis. Then we align the sec-
ondary hypotheses Em(m = 1, . . . ,M ;n 6= m)
with En to match the word order in En. Since it is
not clear which hypothesis should be primary, i. e.
has the ?best? word order, we let every hypothesis
play the role of the primary translation, and align
all pairs of hypotheses (En, Em); n 6= m.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al, 1993) and the Hid-
den Markov Model (HMM, (Vogel et al, 1996))
to estimate the alignment model.
The alignment training corpus is created from a
test corpus1 of effectively M ? (M ? 1) ? N sen-
tences translated by the involved MT engines. The
single-word based lexicon probabilities p(e|e?) are
initialized from normalized lexicon counts col-
lected over the sentence pairs (Em, En) on this
corpus. Since all of the hypotheses are in the same
language, we count co-occurring identical words,
i. e. whether em,j is the same word as en,i for some
i and j. In addition, we add a fraction of a count
for words with identical prefixes.
1A test corpus can be used directly because the align-
ment training is unsupervised and only automatically pro-
duced translations are considered.
51
The model parameters are trained iteratively us-
ing the GIZA++ toolkit (Och and Ney, 2003). The
training is performed in the directions Em ? En
and En ? Em. After each iteration, the updated
lexicon tables from the two directions are interpo-
lated. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the ?source-to-target? and ?target-to-
source? training of the HMM model. Two differ-
ent alignments are computed using the cost matrix
C: the alignment a? used for reordering each sec-
ondary translation Em, and the alignment a? used
to build the confusion network.
In addition to the GIZA++ alignments, we have
also conducted preliminary experiments follow-
ing He et al (2008) to exploit character-based
similarity, as well as estimating p(e|e?) :=?
f p(e|f)p(f |e
?) directly from a bilingual lexi-
con. But we were not able to find improvements
over the GIZA++ alignments so far.
2.2 Word Reordering and Confusion
Network Generation
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix according to a?, we determine M?1 mono-
tone one-to-one alignments between En as the pri-
mary translation and Em,m = 1, . . . ,M ;m 6= n.
We then construct the confusion network. In case
of many-to-one connections in a? of words in Em
to a single word from En, we only keep the con-
nection with the lowest alignment costs.
The use of the one-to-one alignment a? implies
that some words in the secondary translation will
not have a correspondence in the primary transla-
tion and vice versa. We consider these words to
have a null alignment with the empty word ?. In
the corresponding confusion network, the empty
word will be transformed to an ?-arc.
M ? 1 monotone one-to-one alignments can
then be transformed into a confusion network. We
follow the approach of Bangalore et al (2001)
with some extensions. Multiple insertions with re-
gard to the primary hypothesis are sub-aligned to
each other, as described by Matusov et al (2008).
Figure 2 gives an example for the alignment.
2.3 Voting in the confusion network
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for all hypotheses as pri-
mary, and unite them into a single lattice. In our
experience, this approach is advantageous in terms
of translation quality, e.g. by 0.7% in BLEU com-
pared to a minimum Bayes risk primary (Rosti et
al., 2007). Weighted majority voting on a single
confusion network is straightforward and analo-
gous to ROVER (Fiscus, 1997). We sum up the
probabilities of the arcs which are labeled with the
same word and have the same start state and the
same end state. To exploit the true casing abilities
of the input MT systems, we sum up the scores of
arcs bearing the same word but in different cases.
Here, we leave the decision about upper or lower
case to the language model.
2.4 Language Models
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we took the system hypotheses for the
same test corpus for which the consensus trans-
lations are to be produced. Using this ?adapted?
LM for lattice rescoring thus gives bonus to n-
grams from the original system hypotheses, in
most cases from the original phrases. Presum-
ably, many of these phrases have a correct word or-
der, since they are extracted from the training data.
Previous experimental results show that using this
LM in rescoring together with a word penalty (to
counteract any bias towards short sentences) no-
tably improves translation quality. This even re-
sults in better translations than using a ?classical?
LM trained on a monolingual training corpus. We
attribute this to the fact that most of the systems
we combine are phrase-based systems, which al-
ready include such general LMs. Since we are us-
ing a true-cased LM trained on the hypotheses, we
can exploit true casing information from the in-
put systems by using this LM to disambiguate be-
tween the separate arcs generated for the variants
(see Section 2.3).
After LM rescoring, we add the probabilities of
identical partial paths to improve the estimation
of the score for the best hypothesis. This is done
through determinization of the lattice.
2.5 Extracting Consensus Translations
To generate our consensus translation, we extract
the single-best path within the rescored confusion
network. With our approach, we could also extract
N -best hypotheses. In a subsequent step, these N -
best lists could be rescored with additional statis-
tical models (Matusov et al, 2008). But as we did
not have the resources in the WMT 2009 evalua-
tion, this step was dropped for our submission.
3 Tuning system weights
System weights, LM factor, and word penalty
need to be tuned to produce good consensus trans-
lations. We optimize these parameters using the
52
0.25 would your like coffee or tea
system 0.35 have you tea or Coffee
hypotheses 0.10 would like your coffee or
0.30 I have some coffee tea would you like
alignment have|would you|your $|like Coffee|coffee or|or tea|tea
and would|would your|your like|like coffee|coffee or|or $|tea
reordering I|$ would|would you|your like|like have|$ some|$ coffee|coffee $|or tea|tea
$ would your like $ $ coffee or tea
confusion $ have you $ $ $ Coffee or tea
network $ would your like $ $ coffee or $
I would you like have some coffee $ tea
$ would you $ $ $ coffee or tea
voting 0.7 0.65 0.65 0.35 0.7 0.7 0.5 0.7 0.9
(normalized) I have your like have some Coffee $ $
0.3 0.35 0.35 0.65 0.3 0.3 0.5 0.3 0.1
consensus translation would you like coffee or tea
Figure 2: Example of creating a confusion network from monotone one-to-one word alignments (denoted
with symbol |). The words of the primary hypothesis are printed in bold. The symbol $ denotes a null
alignment or an ?-arc in the corresponding part of the confusion network.
Table 1: Systems combined for the WMT 2009
task. Systems written in oblique were also used in
the Cross Lingual task (rbmt3 for FR?EN).
DE?EN google, liu, rbmt3, rwth, stutt-
gart, systran, uedin, uka, umd
ES?EN google, nict, rbmt4, rwth,
talp-upc, uedin
FR?EN dcu, google, jhu, limsi, lium-
systran, rbmt4, rwth, uedin, uka
publicly available CONDOR optimization toolkit
(Berghen and Bersini, 2005). For the WMT
2009 Workshop, we selected a linear combina-
tion of BLEU (Papineni et al, 2002) and TER
(Snover et al, 2006) as optimization criterion,
?? := argmax? {(2 ? BLEU)? TER}, based on
previous experience (Mauser et al, 2008). We
used the whole dev set as a tuning set. For more
stable results, we used the case-insensitive variants
for both measures, despite the explicit use of case
information in our approach.
4 Experimental results
Due to the large number of submissions (71 in
total for the language pairs DE?EN, ES?EN,
FR?EN), we had to select a reasonable number
of systems to be able to tune the parameters in
a reliable way. Based on previous experience,
we manually selected the systems with the best
BLEU/TER score, and tried different variations of
this selection, e.g. by removing systems which
had low weights after optimization, or by adding
promising systems, like rule based systems.
Table 1 lists the systems which made it into
our final submission. In our experience, if a large
number of systems is available, using n-best trans-
lations does not give better results than using sin-
gle best translations, but raises optimization time
significantly. Consequently, we only used single
best translations from all systems.
The results also confirm another observation:
Even though rule-based systems by itself may
have significantly lower automatic evaluation
scores (e.g. by 2% or more in BLEU on DE?EN),
they are often very important in system combina-
tion, and can improve the consensus translation
e.g. by 0.5% in BLEU.
Having submitted our translations to the WMT
workshop, we calculated scores on the WMT 2009
test set, to verify the results on the tuning data.
Both the results on the tuning set and on the test
set can be found in the following tables.
4.1 The Google Problem
One particular thing we noticed is that in the lan-
guage pairs of FR?EN and ES?EN, the trans-
lations from one provided single system (Google)
were much better in terms of BLEU and TER than
those of all other systems ? in the former case
by more than 4% in BLEU. In our experience,
our system combination approach requires at least
three ?comparably good? systems to be able to
achieve significant improvements. This was con-
firmed in the WMT 2009 task as well: Neither in
FR?EN nor in ES?EN we were able to achieve
an improvement over the Google system. For this
reason, we did not submit consensus translations
for these two language pairs. On the other hand,
we would have achieved significant improvements
over all (remaining) systems leaving out Google.
4.2 German?English (DE?EN)
Table 2 lists the scores on the tuning and test set
for the DE?EN task. We can see that the best
systems are rather close to each other in terms
of BLEU. Also, the rule-based translation system
(RBMT), here SYSTRAN, scores rather well. As
a consequence, we find a large improvement using
system combination: 2.9%/2.7% abs. on the tun-
ing set, and still 2.1%/2.3% on test, which means
that system combination generalizes well here.
4.3 Spanish?English (ES?EN),
French?English (FR?EN)
In Table 3, we see that on the ES?EN and
FR?EN tasks, a single system ? Google ? scores
significantly better on the TUNE set than any other
53
Table 2: German?English task: case-insensitive
scores. Best single system was Google, second
best UKA, best RBMT Systran. SC stands for sys-
tem combination output.
TUNE TEST
German?English BLEU TER BLEU TER
Best single 23.2 59.5 21.3 61.3
Second best single 23.0 58.8 21.0 61.7
Best RBMT 21.3 61.3 18.9 63.7
SC (9 systems) 26.1 56.8 23.4 59.0
w/o RBMT 24.5 57.3 22.5 59.2
w/o Google 24.9 57.4 23.0 59.1
Table 3: Spanish?English and French?English
task: scores on the tuning set after system combi-
nation weight tuning (case-insensitive). Best sin-
gle system was Google, second best was Uedin
(Spanish) and UKA (French). No results on TEST
were generated.
ES?EN FR?EN
Spanish?English BLEU TER BLEU TER
Best single 29.5 53.6 32.2 50.1
Second best single 26.9 56.1 28.0 54.6
SC (6/9 systems) 28.7 53.6 30.7 52.5
w/o Google 27.5 55.6 30.0 52.8
system, namely by 2.6%/4.2% resp. in BLEU. As
a result, a combination of these systems scores
better than any other system, even when leaving
out the Google system. But it gives worse scores
than the single best system. This is explainable,
because system combination is trying to find a
consensus translation. For example, in one case,
the majority of the systems leave the French term
?wagon-lit? untranslated; spurious translations in-
clude ?baggage car?, ?sleeping car?, and ?alive?.
As a result, the consensus translation also contains
?wagon-lit?, not the correct translation ?sleeper?
which only the Google system provides. Even tun-
ing all other system weights to zero would not re-
sult in pure Google translations, as these weights
neither affect the LM nor the selection of the pri-
mary hypothesis in our approach.
4.4 Cross-Lingual?English (XX?EN)
Finally, we have conducted experiments on cross-
lingual system combination, namely combining
the output from DE?EN, ES?EN, and FR?EN
systems to a single English consensus transla-
tion. Some interesting results can be found in
Table 4. We see that this consensus translation
scores 2.0%/3.5% better than the best single sys-
tem, and 4.4%/5.6% better than the second best
single system. While this is only 0.8%/2.5% bet-
ter than the combination of only the three Google
systems, the combination of the non-Google sys-
Table 4: Cross-lingual task: combination
of German?English, Spanish?English, and
French?English. Case-insensitive scores. Best
single system was Google for all language pairs.
Cross-lingual TUNE TEST
? English BLEU TER BLEU TER
Best single German 23.2 59.5 21.3 61.3
Best single Spanish 29.5 53.6 28.7 53.8
Best single French 32.2 50.1 31.1 51.7
SC (10 systems) 35.5 46.4 33.1 48.2
w/o RBMT 35.1 46.5 32.7 48.3
w/o Google 32.3 48.8 29.9 50.5
3 Google systems 34.2 48.0 32.3 49.2
w/o German 34.0 49.3 31.5 50.9
w/o Spanish 33.4 49.8 31.0 51.9
w/o French 30.5 51.4 28.6 52.3
tems leads to translations that could compete with
the FR?EN Google system. Again, we see that
RBMT systems lead to a small improvement of
0.4% in BLEU, although their scores are signif-
icantly worse than those of the competing SMT
systems.
Regarding languages, we see that despite the
large differences in the quality of the systems (10
points between DE?EN and FR?EN), all lan-
guages seem to provide significant information to
the consensus translation: While FR?EN cer-
tainly has the largest influence (?4.5% in BLEU
when left out), even DE?EN ?contributes? 1.6
BLEU points to the final submission.
5 Conclusions
We have shown that our system combination sys-
tem can lead to significant improvements over
single best MT output where a significant num-
ber of comparably good translations is available
on a single language pair. For cross-lingual sys-
tem combination, we observe even larger improve-
ments, even if the quality in terms of BLEU or
TER between the systems of different language
pairs varies significantly. While the input of high-
quality SMT systems has the largest weight for the
consensus translation quality, we find that RBMT
systems can give important additional information
leading to better translations.
Acknowledgments
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
54
References
S. Bangalore, G. Bordel, and G. Riccardi. 2001.
Computing consensus translation from multiple ma-
chine translation systems. In IEEE Automatic
Speech Recognition and Understanding Workshop,
Madonna di Campiglio, Italy, December.
F. V. Berghen and H. Bersini. 2005. CONDOR,
a new parallel, constrained extension of Powell?s
UOBYQA algorithm: Experimental results and
comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263?311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
X. He, M. Yang, J. Gao, P. Nguyen, and R. Moore.
2008. Indirect-HMM-based hypothesis alignment
for combining outputs from machine translation sys-
tems. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 98?107, Honolulu, Hawaii, October.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143?152, Budapest, Hungary, May.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ?04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219?225, Geneva, Switzer-
land, August.
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40, Trento, Italy, April.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311?318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312?319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223?231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ?96: The 16th Int. Conf. on Computational
Linguistics, pages 836?841, Copenhagen, Denmark,
August.
55
Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 66?69,
Athens, Greece, 30 March ? 31 March 2009. c?2009 Association for Computational Linguistics
The RWTH Machine Translation System for WMT 2009
Maja Popovic?, David Vilar, Daniel Stein, Evgeny Matusov and Hermann Ney
RWTH Aachen University
Aachen, Germany
Abstract
RWTH participated in the shared transla-
tion task of the Fourth Workshop of Sta-
tistical Machine Translation (WMT 2009)
with the German-English, French-English
and Spanish-English pair in each transla-
tion direction. The submissions were gen-
erated using a phrase-based and a hierar-
chical statistical machine translation sys-
tems with appropriate morpho-syntactic
enhancements. POS-based reorderings of
the source language for the phrase-based
systems and splitting of German com-
pounds for both systems were applied. For
some tasks, a system combination was
used to generate a final hypothesis. An ad-
ditional English hypothesis was produced
by combining all three final systems for
translation into English.
1 Introduction
For the WMT 2009 shared task, RWTH submit-
ted translations for the German-English, French-
English and Spanish-English language pair in both
directions. A phrase-based translation system en-
hanced with appropriate morpho-syntactic trans-
formations was used for all translation direc-
tions. Local POS-based word reorderings were ap-
plied for the Spanish-English and French-English
pair, and long range reorderings for the German-
English pair. For this language pair splitting
of German compounds was also applied. Spe-
cial efforts were made for the French-English and
German-English translation, where a hierarchi-
cal system was also used and the final submis-
sions are the result of a system combination. For
translation into English, an additional hypothesis
was produced as a result of combination of the
final German-to-English, French-to-English and
Spanish-to-English systems.
2 Translation models
2.1 Phrase-based model
We used a standard phrase-based system similar to
the one described in (Zens et al, 2002). The pairs
of source and corresponding target phrases are ex-
tracted from the word-aligned bilingual training
corpus. Phrases are defined as non-empty contigu-
ous sequences of words. The phrase translation
probabilities are estimated using relative frequen-
cies. In order to obtain a more symmetric model,
the phrase-based model is used in both directions.
2.2 Hierarchical model
The hierarchical phrase-based approach can be
considered as an extension of the standard phrase-
based model. In this model we allow the phrases
to have ?gaps?, i.e. we allow non-contiguous parts
of the source sentence to be translated into pos-
sibly non-contiguous parts of the target sentence.
The model can be formalized as a synchronous
context-free grammar (Chiang, 2007). The model
also included some additional heuristics which
have shown to be helpful for improving translation
quality, as proposed in (Vilar et al, 2008).
The first step in the hierarchical phrase extrac-
tion is the same as for the phrased-based model.
Having a set of initial phrases, we search for
phrases which contain other smaller sub-phrases
and produce a new phrase with gaps. In our sys-
tem, we restricted the number of non-terminals for
each hierarchical phrase to a maximum of two,
which were also not allowed to be adjacent. The
scores of the phrases are again computed as rela-
tive frequencies.
2.3 Common models
For both translation models, phrase-based and hi-
erarchical, additional common models were used:
word-based lexicon model, phrase penalty, word
penalty and target language model.
66
The target language model was a standard n-
gram language model trained by the SRI language
modeling toolkit (Stolcke, 2002). The smooth-
ing technique we apply was the modified Kneser-
Ney discounting with interpolation. In our case we
used a 4-gram language model.
3 Morpho-syntactic transformations
3.1 POS-based word reorderings
For the phrase-based systems, the local and
long range POS-based reordering rules described
in (Popovic? and Ney, 2006) were applied on the
training and test corpora as a preprocessing step.
Local reorderings were used for the Spanish-
English and French-English language pairs in or-
der to handle differences between the positions of
nouns and adjectives in the two languages. Adjec-
tives in Spanish and French, as in most Romanic
languages, are usually placed after the correspond-
ing noun, whereas for English it is the other way
round. Therefore, for these language pairs local
reorderings of nouns and adjective groups in the
source language were applied. The following se-
quences of words are considered to be an adjective
group: a single adjective, two or more consecutive
adjectives, a sequence of adjectives and coordinate
conjunctions, as well as an adjective along with its
corresponding adverb. If the source language is
Spanish or French, each noun is moved behind the
corresponding adjective group. If the source lan-
guage is English, each adjective group is moved
behind the corresponding noun.
Long range reorderings were applied on the
verb groups for the German-English language pair.
Verbs in the German language can often be placed
at the end of a clause. This is mostly the case
with infinitives and past participles, but there are
many cases when other verb forms also occur at
the clause end. For the translation from German
into English, following verb types were moved to-
wards the beginning of a clause: infinitives, infini-
tives+zu, finite verbs, past participles and negative
particles. For the translation from English to Ger-
man, infinitives and past participles were moved
to the end of a clause, where punctuation marks,
subordinate conjunctions and finite verbs are con-
sidered as the beginning of the next clause.
3.2 German compound words
For the translation from German into English, Ger-
man compounds were split using the frequency-
based method described in (Koehn and Knight,
2003). For the other translation direction, the En-
glish text was first translated into the modified
German language with split compounds. The gen-
erated output was then postprocessed, i.e. the
components were merged using the method de-
scribed in (Popovic? et al, 2006): a list of com-
pounds and a list of components are extracted from
the original German training corpus. If the word
in the generated output is in the component list,
check if this word merged with the next word is in
the compound list. If it is, merge the two words.
4 System combination
For system combination we used the approach de-
scribed in (Matusov et al, 2006). The method is
based on the generation of a consensus transla-
tion out of the output of different translation sys-
tems. The core of the method consists in building
a confusion network for each sentence by align-
ing and combining the (single-best) translation hy-
pothesis from one MT system with the translations
produced by the other MT systems (and the other
translations from the same system, if n-best lists
are used in combination). For each sentence, each
MT system is selected once as ?primary? system,
and the other hypotheses are aligned to this hy-
pothesis. The resulting confusion networks are
combined into a signle word graph, which is then
weighted with system-specific factors, similar to
the approach of (Rosti et al, 2007), and a trigram
LM trained on the MT hypotheses. The translation
with the best total score within this word graph is
selected as consensus translation. The scaling fac-
tors of these models are optimized using the Con-
dor toolkit (Berghen and Bersini, 2005) to achieve
optimal BLEU score on the dev set.
5 Experimental results
5.1 Experimental settings
For all translation directions, we used the provided
EuroParl and News parallel corpora to train the
translation models and the News monolingual cor-
pora to train the language models. All systems
were optimised for the BLEU score on the develop-
ment data (the ?dev-a? part of the 2008 evaluation
data). The other part of the 2008 evaluation set
(?dev-b?) is used as a blind test set. The results re-
ported in the next section will be referring to this
test set. For the tasks including a system combi-
nation, the parameters for the system combination
67
were also trained on the ?dev-b? set. The reported
evaluation metrics are the BLEU score and two
syntax-oriented metrics which have shown a high
correlation with human evaluations: the PBLEU
score (BLEU calculated on POS sequences) and
the POS-F-score PF (similar to the BLEU score but
based on the F-measure instead of precision and
on arithmetic mean instead of geometric mean).
The POS tags used for reorderings and for syn-
tactic evaluation metrics for the English and the
German corpora were generated using the statisti-
cal n-gram-based TnT-tagger (Brants, 2000). The
Spanish corpora are annotated using the FreeLing
analyser (Carreras et al, 2004), and the French
texts using the TreeTagger1.
5.2 Translation results
Table 1 presents the results for the German-
English language pair. For translation from Ger-
man into English, results for the phrase-based sys-
tem with and without verb reordering and com-
pound splitting are shown. The hierarchical sys-
tem was trained with split German compounds.
The final submission was produced by combining
those five systems. The improvement obtained by
system combination on the unseen test data 2009
is similar, i.e. from the systems with BLEU scores
of 17.0%, 17.2%, 17.5%, 17.6% and 17.7% to the
final system with 18.5%.
German?English BLEU PBLEU PF
phrase-based 17.8 31.6 39.7
+reorder verbs 18.2 32.6 40.3
+split compounds 18.0 31.9 40.0
+reord+split 18.4 33.1 40.7
hierarchical+split 18.5 33.5 40.1
system combination 19.2 33.8 40.9
English?German BLEU PBLEU PF
phrase-based 13.6 31.6 39.7
+reorder verbs 13.7 32.4 40.2
+split compounds 13.7 32.3 40.1
+reord+split 13.7 32.3 40.1
system combination 14.0 32.7 40.3
Table 1: Translation results [%] for the German-
English language pair, News2008 dev-b.
The other translation direction is more difficult
and improvements from morpho-syntactic trans-
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/
formations are smaller. No hierarchical system
was trained for this translation direction. The com-
bination of the four phrase-based systems leads
to further improvements (on the unseen test set
as well: contrastive hypotheses have the BLEU
scores in the range from 12.7% to 13.0%, and the
final BLEU score is 13.2%).
The results for the French-English language
pair are shown in Table 2. For the French-to-
English system, we submitted the result of the
combination of three systems: a phrase-based with
and without local reorderings and a hierarchical
system. For the unseen test set, the BLEU score of
the system combination output is 24.4%, whereas
the contrastive hypotheses have 23.2%, 23.4% and
24.1%. For the other translation direction we did
not use the system combination, the submission is
produced by the phrase-based system with local
adjective reorderings.
French?English BLEU PBLEU PF
phrase-based 20.9 37.1 43.2
+reorder adjectives 21.3 38.2 43.6
hierarchical 20.3 36.7 42.6
system combination 21.7 38.5 43.8
English?French BLEU PBLEU PF
phrase-based 20.2 39.5 45.9
+reorder adjectives 20.7 40.6 46.4
Table 2: Translation results [%] for the French-
English language pair, News2008 dev-b.
Table 3 presents the results for the Spanish-
English language pair. As in the English-to-
French translation, the phrase-based system with
adjective reorderings is used to produce the sub-
mitted hypothesis for both translation directions.
Spanish?English BLEU PBLEU PF
phrase-based 22.1 38.5 44.1
+reorder adjectives 22.5 39.2 44.6
English?Spanish BLEU PBLEU PF
phrase-based 20.6 29.3 35.7
+reorder adjectives 21.1 29.7 35.9
Table 3: Translation results [%] for the Spanish-
English language pair, News2008 dev-b.
68
The result of the additional experiment, i.e. for
the multisource translation int English is presented
in Table 4. The English hypothesis is produced by
the combination of the three best systems for each
language pair, and it can be seen that the transla-
tion performance increases in all measures. This
suggests that each language pair poses different
difficulties for the translation task, and the com-
bination of all three can improve performance.
F+S+G?English BLEU PBLEU PF
system combination 25.1 41.0 46.4
Table 4: Multisource translation results [%]:
the English hypothesis is obtained as result of
a system combination of all language pairs,
News2008 dev-b.
6 Conclusions
The RWTH system submitted to the WMT 2009
shared translation task used a phrase-based sys-
tem and a hierarchical system with appropriate
morpho-syntactic extensions, i.e. POS based word
reorderings and splitting of German compounds
were used. System combination produced gains
in BLEU score over phrasal-system baselines in
the German-to-English, English-to-German and
French-to-English tasks.
Acknowledgments
This work was realised as part of the Quaero Pro-
gramme, funded by OSEO, French State agency
for innovation.
References
Frank Vanden Berghen and Hugues Bersini. 2005.
CONDOR, a new parallel, constrained extension of
Powell?s UOBYQA algorithm: Experimental results
and comparison with the DFO algorithm. Journal of
Computational and Applied Mathematics, 181:157?
175.
Thorsten Brants. 2000. Tnt ? a statistical part-of-
speech tagger. In Proceedings of the 6th Applied
Natural Language Processing Conference (ANLP),
pages 224?231, Seattle, WA.
Xavier Carreras, Isaac Chao, Llu??s Padro?, and Muntsa
Padro?. 2004. FreeLing: An Open-Source Suite of
Language Analyzers. In Proceedings 4th Interna-
tional Conference on Language Resources and Eval-
uation (LREC), pages 239?242, Lisbon, Portugal,
May.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, (33):201?228.
Philipp Koehn and Kevin Knight. 2003. Empiri-
cal methods for compound splitting. In Proceed-
ings 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 347?354, Budapest, Hungary, April.
Evgeny Matusov, Nicola Ueffing, and Hermann Ney.
2006. Computing Consensus Translation from Mul-
tiple Machine Translation Systems Using Enhanced
Hypotheses Alignment. In Proceedings of EACL
2006 (11th Conference of the European Chapter
of the Association for Computational Linguistics),
pages 33?40, Trento, Italy, April.
Maja Popovic? and Hermann Ney. 2006. POS-based
Word Reorderings for Statistical Machine Trans-
lation. In Proceedings of the Fifth International
Conference on Language Resources and Evaluation
(LREC), pages 1278?1283, Genoa, Italy, May.
Maja Popovic?, Daniel Stein, and Hermann Ney. 2006.
Statistical machine translation of german compound
words. In Proceedings of the 5th International Con-
ference on Natural Language Processing (FinTAL),
pages 616?624, Turku, Finland, August. Lecture
Notes in Computer Science, Springer Verlag.
Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining Outputs from Multiple Ma-
chine Translation Systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics; Proceedings of the Main Confer-
ence, pages 228?235, Rochester, New York, April.
Association for Computational Linguistics.
Andreas Stolcke. 2002. SRILM ? an extensible lan-
guage modeling toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing
(ICSLP), volume 2, pages 901?904, Denver, CO.
David Vilar, Daniel Stein, and Hermann Ney. 2008.
Analysing soft syntax features and heuristics for hi-
erarchical phrase based machine translation. Inter-
national Workshop on Spoken Language Translation
2008, pages 190?197, October.
Richard Zens, Franz Josef Och, and Hermann Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
25th German Conference on Artificial Intelligence
(KI2002), volume 2479 of Lecture Notes in Artifi-
cial Intelligence (LNAI), pages 18?32, Aachen, Ger-
many, September. Springer Verlag.
69
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412?418,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Language Independent Connectivity Strength Features
for Phrase Pivot Statistical Machine Translation
Ahmed El Kholy, Nizar Habash
Center for Computational Learning Systems, Columbia University
{akholy,habash}@ccls.columbia.edu
Gregor Leusch, Evgeny Matusov
Science Applications International Corporation
{gregor.leusch,evgeny.matusov}@saic.com
Hassan Sawaf
eBay Inc.
hsawaf@ebay.com
Abstract
An important challenge to statistical ma-
chine translation (SMT) is the lack of par-
allel data for many language pairs. One
common solution is to pivot through a
third language for which there exist par-
allel corpora with the source and target
languages. Although pivoting is a robust
technique, it introduces some low quality
translations. In this paper, we present two
language-independent features to improve
the quality of phrase-pivot based SMT.
The features, source connectivity strength
and target connectivity strength reflect the
quality of projected alignments between
the source and target phrases in the pivot
phrase table. We show positive results (0.6
BLEU points) on Persian-Arabic SMT as
a case study.
1 Introduction
One of the main issues in statistical machine trans-
lation (SMT) is the scarcity of parallel data for
many language pairs especially when the source
and target languages are morphologically rich. A
common SMT solution to the lack of parallel data
is to pivot the translation through a third language
(called pivot or bridge language) for which there
exist abundant parallel corpora with the source
and target languages. The literature covers many
pivoting techniques. One of the best performing
techniques, phrase pivoting (Utiyama and Isahara,
2007), builds an induced new phrase table between
the source and target. One of the main issues of
this technique is that the size of the newly cre-
ated pivot phrase table is very large (Utiyama and
Isahara, 2007). Moreover, many of the produced
phrase pairs are of low quality which affects the
translation choices during decoding and the over-
all translation quality. In this paper, we introduce
language independent features to determine the
quality of the pivot phrase pairs between source
and target. We show positive results (0.6 BLEU
points) on Persian-Arabic SMT.
Next, we briefly discuss some related work. We
then review two common pivoting strategies and
how we use them in Section 3. This is followed by
our approach to using connectivity strength fea-
tures in Section 4. We present our experimental
results in Section 5.
2 Related Work
Many researchers have investigated the use of piv-
oting (or bridging) approaches to solve the data
scarcity issue (Utiyama and Isahara, 2007; Wu and
Wang, 2009; Khalilov et al, 2008; Bertoldi et al,
2008; Habash and Hu, 2009). The main idea is to
introduce a pivot language, for which there exist
large source-pivot and pivot-target bilingual cor-
pora. Pivoting has been explored for closely re-
lated languages (Hajic? et al, 2000) as well as un-
related languages (Koehn et al, 2009; Habash and
Hu, 2009). Many different pivot strategies have
been presented in the literature. The following
three are perhaps the most common.
The first strategy is the sentence translation
technique in which we first translate the source
sentence to the pivot language, and then translate
the pivot language sentence to the target language
412
(Khalilov et al, 2008).
The second strategy is based on phrase pivot-
ing (Utiyama and Isahara, 2007; Cohn and Lap-
ata, 2007; Wu and Wang, 2009). In phrase pivot-
ing, a new source-target phrase table (translation
model) is induced from source-pivot and pivot-
target phrase tables. Lexical weights and transla-
tion probabilities are computed from the two trans-
lation models.
The third strategy is to create a synthetic source-
target corpus by translating the pivot side of
source-pivot corpus to the target language using an
existing pivot-target model (Bertoldi et al, 2008).
In this paper, we build on the phrase pivoting
approach, which has been shown to be the best
with comparable settings (Utiyama and Isahara,
2007). We extend phrase table scores with two
other features that are language independent.
Since both Persian and Arabic are morphologi-
cally rich, we should mention that there has been
a lot of work on translation to and from morpho-
logically rich languages (Yeniterzi and Oflazer,
2010; Elming and Habash, 2009; El Kholy and
Habash, 2010a; Habash and Sadat, 2006; Kathol
and Zheng, 2008). Most of these efforts are fo-
cused on syntactic and morphological processing
to improve the quality of translation.
To our knowledge, there hasn?t been a lot of
work on Persian and Arabic as a language pair.
The only effort that we are aware of is based
on improving the reordering models for Persian-
Arabic SMT (Matusov and Ko?pru?, 2010).
3 Pivoting Strategies
In this section, we review the two pivoting strate-
gies that are our baselines. We also discuss how
we overcome the large expansion of source-to-
target phrase pairs in the process of creating a
pivot phrase table.
3.1 Sentence Pivoting
In sentence pivoting, English is used as an inter-
face between two separate phrase-based MT sys-
tems; Persian-English direct system and English-
Arabic direct system. Given a Persian sentence,
we first translate the Persian sentence from Per-
sian to English, and then from English to Arabic.
3.2 Phrase Pivoting
In phrase pivoting (sometimes called triangulation
or phrase table multiplication), we train a Persian-
to-Arabic and an English-Arabic translation mod-
els, such as those used in the sentence pivoting
technique. Based on these two models, we induce
a new Persian-Arabic translation model.
Since we build our models on top of Moses
phrase-based SMT (Koehn et al, 2007), we need
to provide the same set of phrase translation prob-
ability distributions.1 We follow Utiyama and Isa-
hara (2007) in computing the probability distribu-
tions. The following are the set of equations used
to compute the lexical probabilities (?) and the
phrase probabilities (pw)
?(f |a) =?
e
?(f |e)?(e|a)
?(a|f) =?
e
?(a|e)?(e|f)
pw(f |a) =
?
e
pw(f |e)pw(e|a)
pw(a|f) =
?
e
pw(a|e)pw(e|f)
where f is the Persian source phrase. e is
the English pivot phrase that is common in both
Persian-English translation model and English-
Arabic translation model. a is the Arabic target
phrase.
We also build a Persian-Arabic reordering table
using the same technique but we compute the re-
ordering weights in a similar manner to Henriquez
et al (2010).
As discussed earlier, the induced Persian-
Arabic phrase and reordering tables are very large.
Table 1 shows the amount of parallel corpora
used to train the Persian-English and the English-
Arabic and the equivalent phrase table sizes com-
pared to the induced Persian-Arabic phrase table.2
We introduce a basic filtering technique dis-
cussed next to address this issue and present some
baseline experiments to test its performance in
Section 5.3.
3.3 Filtering for Phrase Pivoting
The main idea of the filtering process is to select
the top [n] English candidate phrases for each Per-
sian phrase from the Persian-English phrase ta-
ble and similarly select the top [n] Arabic target
phrases for each English phrase from the English-
Arabic phrase table and then perform the pivot-
ing process described earlier to create a pivoted
1Four different phrase translation scores are computed in
Moses? phrase tables: two lexical weighting scores and two
phrase translation probabilities.
2The size of the induced phrase table size is computed but
not created.
413
Training Corpora Phrase Table
Translation Model Size # Phrase Pairs Size
Persian-English ?4M words 96,04,103 1.1GB
English-Arabic ?60M words 111,702,225 14GB
Pivot Persian-Arabic N/A 39,199,269,195 ?2.5TB
Table 1: Translation Models Phrase Table comparison in terms of number of line and sizes.
Persian-Arabic phrase table. To select the top can-
didates, we first rank all the candidates based on
the log linear scores computed from the phrase
translation probabilities and lexical weights mul-
tiplied by the optimized decoding weights then we
pick the top [n] pairs.
We compare the different pivoting strategies
and various filtering thresholds in Section 5.3.
4 Approach
One of the main challenges in phrase pivoting is
the very large size of the induced phrase table.
It becomes even more challenging if either the
source or target language is morphologically rich.
The number of translation candidates (fanout) in-
creases due to ambiguity and richness (discussed
in more details in Section 5.2) which in return
increases the number of combinations between
source and target phrases. Since the only criteria
of matching between the source and target phrase
is through a pivot phrase, many of the induced
phrase pairs are of low quality. These phrase pairs
unnecessarily increase the search space and hurt
the overall quality of translation.
To solve this problem, we introduce two
language-independent features which are added to
the log linear space of features in order to deter-
mine the quality of the pivot phrase pairs. We call
these features connectivity strength features.
Connectivity Strength Features provide two
scores, Source Connectivity Strength (SCS) and
Target Connectivity Strength (TCS). These two
scores are similar to precision and recall metrics.
They depend on the number of alignment links be-
tween words in the source phrase to words of the
target phrase. SCS and TSC are defined in equa-
tions 1 and 2 where S = {i : 1 ? i ? S} is the
set of source words in a given phrase pair in the
pivot phrase table and T = {j : 1 ? j ? T}
is the set of the equivalent target words. The
word alignment between S and T is defined as
A = {(i, j) : i ? S and j ? T }.
SCS = |A||S| (1)
TCS = |A||T | (2)
We get the alignment links by projecting the
alignments of source-pivot to the pivot-target
phrase pairs used in pivoting. If the source-target
phrase pair are connected through more than one
pivot phrase, we take the union of the alignments.
In contrast to the aggregated values represented
in the lexical weights and the phrase probabilities,
connectivity strength features provide additional
information by counting the actual links between
the source and target phrases. They provide an
independent and direct approach to measure how
good or bad a given phrase pair are connected.
Figure 1 and 2 are two examples (one good, one
bad) Persian-Arabic phrase pairs in a pivot phrase
table induced by pivoting through English.3 In the
first example, each Persian word is aligned to an
Arabic word. The meaning is preserved in both
phrases which is reflected in the SCS and TCS
scores. In the second example, only one Persian
word in aligned to one Arabic word in the equiv-
alent phrase and the two phrases conveys two dif-
ferent meanings. The English phrase is not a good
translation for either, which leads to this bad pair-
ing. This is reflected in the SCS and TCS scores.
5 Experiments
In this section, we present a set of baseline ex-
periments including a simple filtering technique to
overcome the huge expansion of the pivot phrase
table. Then we present our results in using connec-
tivity strength features to improve Persian-Arabic
pivot translation quality.
3We use the Habash-Soudi-Buckwalter Arabic transliter-
ation (Habash et al, 2007) in the figures with extensions for
Persian as suggested by Habash (2010).
414
Persian: "A?tmAd"myAn"dw"k?wr " " "? ?"??"()"?"?%$#"?,-. ?"" " " " " " " " " " " " "?trust"between"the"two"countries?"English: "trust"between"the"two"countries"
Arabic:" "Al?q?"byn"Aldwltyn " " " "? /012?52?2$3"34"? ?"" " " " " " " " " " " " "?the"trust"between"the"two"countries?"
Figure 1: An example of strongly connected Persian-Arabic phrase pair through English. All Persian
words are connected to one or more Arabic words. SCS=1.0 and TCS=1.0.
Persian: "AyjAd"cnd"?rkt"m?trk " " " "? 0/.+?",+*(")'&"?$#"? ?"" " " " " " " " " " " " "?Establish"few"joint"companies?"English: "joint"ventures"
Arabic:" "b?D"?rkAt"AlmqAwlAt"fy"Albld" "? 123"?<=>&";:"?89"?6?",+5"? ?"" " " " " " " " " " " " "?Some"construcBon"companies"in"the"country?"
Figure 2: An example of weakly connected Persian-Arabic phrase pairs through English. Only one
Persian word is connected to an Arabic word. SCS=0.25 and TCS=0.2.
5.1 Experimental Setup
In our pivoting experiments, we build two SMT
models. One model to translate from Persian to
English and another model to translate from En-
glish to Arabic. The English-Arabic parallel cor-
pus is about 2.8M sentences (?60M words) avail-
able from LDC4 and GALE5 constrained data. We
use an in-house Persian-English parallel corpus of
about 170K sentences and 4M words.
Word alignment is done using GIZA++ (Och
and Ney, 2003). For Arabic language model-
ing, we use 200M words from the Arabic Giga-
word Corpus (Graff, 2007) together with the Ara-
bic side of our training data. We use 5-grams
for all language models (LMs) implemented us-
ing the SRILM toolkit (Stolcke, 2002). For En-
glish language modeling, we use English Giga-
word Corpus with 5-gram LM using the KenLM
toolkit (Heafield, 2011).
All experiments are conducted using the Moses
phrase-based SMT system (Koehn et al, 2007).
We use MERT (Och, 2003) for decoding weight
4LDC Catalog IDs: LDC2005E83, LDC2006E24,
LDC2006E34, LDC2006E85, LDC2006E92, LDC2006G05,
LDC2007E06, LDC2007E101, LDC2007E103,
LDC2007E46, LDC2007E86, LDC2008E40, LDC2008E56,
LDC2008G05, LDC2009E16, LDC2009G01.
5Global Autonomous Language Exploitation, or GALE,
is a DARPA-funded research project.
optimization. For Persian-English translation
model, weights are optimized using a set 1000 sen-
tences randomly sampled from the parallel cor-
pus while the English-Arabic translation model
weights are optimized using a set of 500 sen-
tences from the 2004 NIST MT evaluation test
set (MT04). The optimized weights are used for
ranking and filtering (discussed in Section 3.3).
We use a maximum phrase length of size 8
across all models. We report results on an in-
house Persian-Arabic evaluation set of 536 sen-
tences with three references. We evaluate using
BLEU-4 (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007).
5.2 Linguistic Preprocessing
In this section we present our motivation and
choice for preprocessing Arabic, Persian, English
data. Both Arabic and Persian are morphologi-
cally complex languages but they belong to two
different language families. They both express
richness and linguistic complexities in different
ways.
One aspect of Arabic?s complexity is its vari-
ous attachable clitics and numerous morphologi-
cal features (Habash, 2010). We follow El
Kholy and Habash (2010a) and use the PATB to-
kenization scheme (Maamouri et al, 2004) in our
415
experiments. We use MADA v3.1 (Habash and
Rambow, 2005; Habash et al, 2009) to tokenize
the Arabic text. We only evaluate on detokenized
and orthographically correct (enriched) output fol-
lowing the work of El Kholy and Habash (2010b).
Persian on the other hand has a relatively sim-
ple nominal system. There is no case system and
words do not inflect with gender except for a few
animate Arabic loanwords. Unlike Arabic, Persian
shows only two values for number, just singular
and plural (no dual), which are usually marked by
either the suffix A?+ +hA and sometimes 	?@+ +An,
or one of the Arabic plural markers. Verbal mor-
phology is very complex in Persian. Each verb
has a past and present root and many verbs have
attached prefix that is regarded part of the root.
A verb in Persian inflects for 14 different tense,
mood, aspect, person, number and voice combina-
tion values (Rasooli et al, 2013). We use Perstem
(Jadidinejad et al, 2010) for segmenting Persian
text.
English, our pivot language, is quite different
from both Arabic and Persian. English is poor
in morphology and barely inflects for number and
tense, and for person in a limited context. English
preprocessing simply includes down-casing, sepa-
rating punctuation and splitting off ??s?.
5.3 Baseline Evaluation
We compare the performance of sentence pivot-
ing against phrase pivoting with different filtering
thresholds. The results are presented in Table 2. In
general, the phrase pivoting outperforms the sen-
tence pivoting even when we use a small filtering
threshold of size 100. Moreover, the higher the
threshold the better the performance but with a di-
minishing gain.
Pivot Scheme BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F100 19.4 37.4
Phrase Pivot F500 20.1 38.1
Phrase Pivot F1K 20.5 38.6
Table 2: Sentence pivoting versus phrase pivoting
with different filtering thresholds (100/500/1000).
We use the best performing setup across the rest
of the experiments.
5.4 Connectivity Strength Features
Evaluation
In this experiment, we test the performance of
adding the connectivity strength features (+Conn)
to the best performing phrase pivoting model
(Phrase Pivot F1K).
Model BLEU METEOR
Sentence Pivoting 19.2 36.4
Phrase Pivot F1K 20.5 38.6
Phrase Pivot F1K+Conn 21.1 38.9
Table 3: Connectivity strength features experi-
ment result.
The results in Table 3 show that we get a
nice improvement of ?0.6/0.5 (BLEU/METEOR)
points by adding the connectivity strength fea-
tures. The differences in BLEU scores between
this setup and all other systems are statistically
significant above the 95% level. Statistical signif-
icance is computed using paired bootstrap resam-
pling (Koehn, 2004).
6 Conclusion and Future Work
We presented an experiment showing the effect of
using two language independent features, source
connectivity score and target connectivity score,
to improve the quality of pivot-based SMT. We
showed that these features help improving the
overall translation quality. In the future, we plan
to explore other features, e.g., the number of the
pivot phases used in connecting the source and tar-
get phrase pair and the similarity between these
pivot phrases. We also plan to explore language
specific features which could be extracted from
some seed parallel data, e.g., syntactic and mor-
phological compatibility of the source and target
phrase pairs.
Acknowledgments
The work presented in this paper was possible
thanks to a generous research grant from Science
Applications International Corporation (SAIC).
The last author (Sawaf) contributed to the effort
while he was at SAIC. We would like to thank M.
Sadegh Rasooli and Jon Dehdari for helpful dis-
cussions and insights into Persian. We also thank
the anonymous reviewers for their insightful com-
ments.
416
References
Nicola Bertoldi, Madalina Barbaiani, Marcello Fed-
erico, and Roldano Cattoni. 2008. Phrase-based
statistical machine translation with pivot languages.
Proceeding of IWSLT, pages 143?149.
Trevor Cohn and Mirella Lapata. 2007. Ma-
chine translation by triangulation: Making ef-
fective use of multi-parallel corpora. In AN-
NUAL MEETING-ASSOCIATION FOR COMPU-
TATIONAL LINGUISTICS, volume 45, page 728.
Ahmed El Kholy and Nizar Habash. 2010a. Ortho-
graphic and Morphological Processing for English-
Arabic Statistical Machine Translation. In Proceed-
ings of Traitement Automatique du Langage Naturel
(TALN-10). Montre?al, Canada.
Ahmed El Kholy and Nizar Habash. 2010b. Tech-
niques for Arabic Morphological Detokenization
and Orthographic Denormalization. In Proceed-
ings of the seventh International Conference on Lan-
guage Resources and Evaluation (LREC), Valletta,
Malta.
Jakob Elming and Nizar Habash. 2009. Syntactic
Reordering for English-Arabic Phrase-Based Ma-
chine Translation. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages, pages 69?77, Athens, Greece, March.
David Graff. 2007. Arabic Gigaword 3, LDC Cat-
alog No.: LDC2003T40. Linguistic Data Consor-
tium, University of Pennsylvania.
Nizar Habash and Jun Hu. 2009. Improving Arabic-
Chinese Statistical Machine Translation using En-
glish as Pivot Language. In Proceedings of the
Fourth Workshop on Statistical Machine Transla-
tion, pages 173?181, Athens, Greece, March.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphologi-
cal Disambiguation in One Fell Swoop. In Proceed-
ings of the 43rd Annual Meeting of the Association
for Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan.
Nizar Habash and Fatiha Sadat. 2006. Arabic Pre-
processing Schemes for Statistical Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Companion Vol-
ume: Short Papers, pages 49?52, New York City,
USA.
Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.
2007. On Arabic Transliteration. In A. van den
Bosch and A. Soudi, editors, Arabic Computa-
tional Morphology: Knowledge-based and Empiri-
cal Methods. Springer.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS
tagging, stemming and lemmatization. In Khalid
Choukri and Bente Maegaard, editors, Proceedings
of the Second International Conference on Arabic
Language Resources and Tools. The MEDAR Con-
sortium, April.
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publish-
ers.
Jan Hajic?, Jan Hric, and Vladislav Kubon. 2000. Ma-
chine Translation of Very Close Languages. In Pro-
ceedings of the 6th Applied Natural Language Pro-
cessing Conference (ANLP?2000), pages 7?12, Seat-
tle.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, UK.
Carlos Henriquez, Rafael E. Banchs, and Jose? B.
Marin?o. 2010. Learning reordering models for sta-
tistical machine translation with a pivot language.
Amir Hossein Jadidinejad, Fariborz Mahmoudi, and
Jon Dehdari. 2010. Evaluation of PerStem: a sim-
ple and efficient stemming algorithm for Persian. In
Multilingual Information Access Evaluation I. Text
Retrieval Experiments, pages 98?101.
Andreas Kathol and Jing Zheng. 2008. Strategies for
building a Farsi-English smt system from limited re-
sources. In Proceedings of the 9th Annual Confer-
ence of the International Speech Communication As-
sociation (INTERSPEECH2008), pages 2731?2734,
Brisbane, Australia.
M. Khalilov, Marta R. Costa-juss, Jos A. R. Fonollosa,
Rafael E. Banchs, B. Chen, M. Zhang, A. Aw, H. Li,
Jos B. Mario, Adolfo Hernndez, and Carlos A. Hen-
rquez Q. 2008. The talp & i2r smt systems for iwslt
2008. In International Workshop on Spoken Lan-
guage Translation. IWSLT 2008, pg. 116?123.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Christo-
pher Callison-Burch, Marcello Federico, Nicola
Bertoldi, Brooke Cowan, Wade Shen, Christine
Moran, Richard Zens, Christopher Dyer, Ondrej Bo-
jar, Alexandra Constantin, and Evan Herbst. 2007.
Moses: open source toolkit for statistical machine
translation. In Proceedings of the 45th Annual Meet-
ing of the Association for Computational Linguistics
Companion Volume Proceedings of the Demo and
Poster Sessions, pages 177?180, Prague, Czech Re-
public.
Philipp Koehn, Alexandra Birch, and Ralf Steinberger.
2009. 462 machine translation systems for europe.
Proceedings of MT Summit XII, pages 65?72.
Philipp Koehn. 2004. Statistical significance tests for-
machine translation evaluation. In Proceedings of
the Empirical Methods in Natural Language Pro-
cessing Conference (EMNLP?04), Barcelona, Spain.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In Proceed-
ings of the Second Workshop on Statistical Machine
Translation, pages 228?231, Prague, Czech Repub-
lic.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic Treebank:
Building a Large-Scale Annotated Arabic Corpus.
417
In NEMLAR Conference on Arabic Language Re-
sources and Tools, pages 102?109, Cairo, Egypt.
Evgeny Matusov and Selc?uk Ko?pru?. 2010. Improv-
ing reordering in statistical machine translation from
farsi. In AMTA The Ninth Conference of the Associ-
ation for Machine Translation in the Americas, Den-
ver, Colorado, USA.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?52.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics-Volume 1, pages 160?167. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceed-
ings of the 40th Annual Meeting of the Associa-
tion for Computational Linguistics, pages 311?318,
Philadelphia, PA.
Mohammad Sadegh Rasooli, Manouchehr Kouhestani,
and Amirsaeid Moloodi. 2013. Development of
a Persian syntactic dependency treebank. In The
2013 Conference of the North American Chapter
of the Association for Computational Linguistics:
Human Language Technologies (NAACL HLT), At-
lanta, USA.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), volume 2, pages 901?904, Denver,
CO.
Masao Utiyama and Hitoshi Isahara. 2007. A com-
parison of pivot methods for phrase-based statistical
machine translation. In Human Language Technolo-
gies 2007: The Conference of the North American
Chapter of the Association for Computational Lin-
guistics; Proceedings of the Main Conference, pages
484?491, Rochester, New York, April. Association
for Computational Linguistics.
Hua Wu and Haifeng Wang. 2009. Revisiting pivot
language approach for machine translation. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Joint
Conference on Natural Language Processing of the
AFNLP, pages 154?162, Suntec, Singapore, August.
Association for Computational Linguistics.
Reyyan Yeniterzi and Kemal Oflazer. 2010. Syntax-to-
morphology mapping in factored phrase-based sta-
tistical machine translation from english to turkish.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, pages 454?
464, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
418
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 158?163,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
OmnifluentTM English-to-French and Russian-to-English Systems for the
2013 Workshop on Statistical Machine Translation
Evgeny Matusov, Gregor Leusch
Science Applications International Corporation (SAIC)
7990 Science Applications Ct.
Vienna, VA, USA
{evgeny.matusov,gregor.leusch}@saic.com
Abstract
This paper describes OmnifluentTM Trans-
late ? a state-of-the-art hybrid MT sys-
tem capable of high-quality, high-speed
translations of text and speech. The sys-
tem participated in the English-to-French
and Russian-to-English WMT evaluation
tasks with competitive results. The
features which contributed the most to
high translation quality were training data
sub-sampling methods, document-specific
models, as well as rule-based morpholog-
ical normalization for Russian. The latter
improved the baseline Russian-to-English
BLEU score from 30.1 to 31.3% on a held-
out test set.
1 Introduction
Omnifluent Translate is a comprehensive multilin-
gual translation platform developed at SAIC that
automatically translates both text and audio con-
tent. SAIC?s technology leverages hybrid machine
translation, combining features of both rule-based
machine and statistical machine translation for im-
proved consistency, fluency, and accuracy of trans-
lation output.
In the WMT 2013 evaluation campaign, we
trained and tested the Omnifluent system on the
English-to-French and Russian-to-English tasks.
We chose the En?Fr task because Omnifluent En?
Fr systems are already extensively used by SAIC?s
commercial customers: large human translation
service providers, as well as a leading fashion de-
signer company (Matusov, 2012). Our Russian-to-
English system also produces high-quality transla-
tions and is currently used by a US federal govern-
ment customer of SAIC.
Our experimental efforts focused mainly on the
effective use of the provided parallel and monolin-
gual data, document-level models, as well using
rules to cope with the morphological complexity
of the Russian language. While striving for the
best possible translation quality, our goal was to
avoid those steps in the translation pipeline which
would make a real-time use of the Omnifluent sys-
tem impossible. For example, we did not integrate
re-scoring of N-best lists with huge computation-
ally expensive models, nor did we perform system
combination of different system variants. This al-
lowed us to create a MT system that produced our
primary evaluation submission with the translation
speed of 18 words per second1. This submission
had a BLEU score of 24.2% on the Russian-to-
English task2, and 27.3% on the English-to-French
task. In contrast to many other submissions from
university research groups, our evaluation system
can be turned into a fully functional, commer-
cially deployable on-line system with the same
high level of translation quality and speed within
a single work day.
The rest of the paper is organized as follows. In
the next section, we describe the core capabilities
of the Omnifluent Translate systems. Section 3
explains our data selection and filtering strategy.
In Section 4 we present the document-level trans-
lation and language models. Section 5 describes
morphological transformations of Russian. In sec-
tions 6 we present an extension to the system that
allows for automatic spelling correction. In Sec-
tion 7, we discuss the experiments and their evalu-
ation. Finally, we conclude the paper in Section 8.
2 Core System Capabilities
The Omnifluent system is a state-of-the-art hybrid
MT system that originates from the AppTek tech-
nology acquired by SAIC (Matusov and Ko?pru?,
2010a). The core of the system is a statistical
search that employs a combination of multiple
1Using a single core of a 2.8 GHz Intel Xeon CPU.
2The highest score obtained in the evaluation was 25.9%
158
probabilistic translation models, including phrase-
based and word-based lexicons, as well as reorder-
ing models and target n-gram language models.
The retrieval of matching phrase pairs given an
input sentence is done efficiently using an algo-
rithm based on the work of (Zens, 2008). The
main search algorithm is the source cardinality-
synchronous search. The goal of the search is to
find the most probable segmentation of the source
sentence into non-empty non-overlapping contigu-
ous blocks, select the most probable permutation
of those blocks, and choose the best phrasal trans-
lations for each of the blocks at the same time. The
concatenation of the translations of the permuted
blocks yields a translation of the whole sentence.
In practice, the permutations are limited to allow
for a maximum of M ?gaps? (contiguous regions
of uncovered word positions) at any time during
the translation process. We set M to 2 for the
English-to-French translation to model the most
frequent type of reordering which is the reorder-
ing of an adjective-noun group. The value of M
for the Russian-to-English translation is 3.
The main differences of Omnifluent Trans-
late as compared to the open-source MT sys-
tem Moses (Koehn et al, 2007) is a reordering
model that penalizes each deviation from mono-
tonic translation instead of assigning costs propor-
tional to the jump distance (4 features as described
by Matusov and Ko?pru? (2010b)) and a lexicaliza-
tion of this model when such deviations depend on
words or part-of-speech (POS) tags of the last cov-
ered and current word (2 features, see (Matusov
and Ko?pru?, 2010a)). Also, the whole input doc-
ument is always visible to the system, which al-
lows the use of document-specific translation and
language models. In translation, multiple phrase
tables can be interpolated linearly on the count
level, as the phrasal probabilities are computed
on-the-fly. Finally, various novel phrase-level fea-
tures have been implemented, including binary
topic/genre/phrase type indicators and translation
memory match features (Matusov, 2012).
The Omnifluent system also allows for partial
or full rule-based translations. Specific source lan-
guage entities can be identified prior to the search,
and rule-based translations of these entities can
be either forced to be chosen by the MT system,
or can compete with phrase translation candidates
from the phrase translation model. In both cases,
the language model context at the boundaries of
the rule-based translations is taken into account.
Omnifluent Translate identifies numbers, dates,
URLs, e-mail addresses, smileys, etc. with manu-
ally crafted regular expressions and uses rules to
convert them to the appropriate target language
form. In addition, it is possible to add manual
translation rules to the statistical phrase table of
the system.
3 Training Data Selection and Filtering
We participated in the constrained data track of
the evaluation in order to obtain results which are
comparable to the majority of the other submis-
sions. This means that we trained our systems only
on the provided parallel and monolingual data.
3.1 TrueCasing
Instead of using a separate truecasing module, we
apply an algorithm for finding the true case of the
first word of each sentence in the target training
data and train truecased phrase tables and a true-
cased language model3. Thus, the MT search de-
cides on the right case of a word when ambiguities
exist. Also, the Omnifluent Translate system has
an optional feature to transfer the case of an input
source word to the word in the translation output
to which it is aligned. Although this approach is
not always error-free, there is an advantage to it
when the input contains previously unseen named
entities which use common words that have to be
capitalized. We used this feature for our English-
to-French submission only.
3.2 Monolingual Data
For the French language model, we trained sepa-
rate 5-gram models on the two GigaWord corpora
AFP and APW, on the provided StatMT data for
2007?2012 (3 models), on the EuroParl data, and
on the French side of the bilingual data. LMs were
estimated and pruned using the IRSTLM toolkit
(Federico et al, 2008). We then tuned a linear
combination of these seven individual parts to op-
timum perplexity on WMT test sets 2009 and 2010
and converted them for use with the KenLM li-
brary (Heafield, 2011). Similarly, our English LM
was a linear combination of separate LMs built for
GigaWord AFP, APW, NYT, and the other parts,
StatMT 2007?2012, Europarl/News Commentary,
and the Yandex data, which was tuned for best per-
plexity on the WMT 2010-2013 test sets.
3Source sentences were lowercased.
159
3.3 Parallel Data
Since the provided parallel corpora had differ-
ent levels of noise and quality of sentence align-
ment, we followed a two-step procedure for fil-
tering the data. First, we trained a baseline sys-
tem on the ?good-quality? data (Europarl and
News Commentary corpora) and used it to trans-
late the French side of the Common Crawl data
into English. Then, we computed the position-
independent word error rate (PER) between the
automatic translation and the target side on the
segment level and only kept those original seg-
ment pairs, the PER for which was between 10%
and 60%. With this criterion, we kept 48% of the
original 3.2M sentence pairs of the common-crawl
data.
To leverage the significantly larger Multi-UN
parallel corpus, we performed perplexity-based
data sub-sampling, similarly to the method de-
scribed e. g. by Axelrod et al (2011). First, we
trained a relatively small 4-gram LM on the source
(English) side of our development data and evalu-
ation data. Then, we used this model to compute
the perplexity of each Multi-UN source segment.
We kept the 700K segments with the lowest per-
plexity (normalized by the segment length), so that
the size of the Multi-UN corpus does not exceed
30% of the total parallel corpus size. This proce-
dure is the only part of the translation pipeline for
which we currently do not have a real-time solu-
tion. Yet such a real-time algorithm can be imple-
mented without problems: we word-align the orig-
inal corpora using GIZA++ahead of time, so that af-
ter sub-sampling we only need to perform a quick
phrase extraction. To obtain additional data for
the document-level models only (see Section 4),
we also applied this procedure to the even larger
Gigaword corpus and thus selected 1M sentence
pairs from this corpus.
We used the PER-based procedure as described
above to filter the Russian-English Common-
crawl corpus to 47% of its original size. The base-
line system used to obtain automatic translation
for the PER-based filtering was trained on News
Commentary, Yandex, and Wiki headlines data.
4 Document-level Models
As mentioned in the introduction, the Omnifluent
system loads a whole source document at once.
Thus, it is possible to leverage document context
by using document-level models which score the
phrasal translations of sentences from a specific
document only and are unloaded after processing
of this document.
To train a document-level model for a specific
document from the development, test, or evalua-
tion data, we automatically extract those source
sentences from the background parallel training
data which have (many) n-grams (n=2...7) in com-
mon with the source sentences of the document.
Then, to train the document-level LM we take the
target language counterparts of the extracted sen-
tences and train a standard 3-gram LM on them.
To train the document-level phrase table, we take
the corresponding word alignments for the ex-
tracted source sentences and their target counter-
parts, and extract the phrase table as usual. To
keep the additional computational overhead min-
imal yet have enough data for model estimation,
we set the parameters of the n-gram matching
in such a way that the number of sentences ex-
tracted for document-level training is around 20K
for document-level phrase tables and 100K for
document-level LMs.
In the search, the counts from the document-
level phrase table are linearly combined with the
counts from the background phrase table trained
on the whole training data. The document-level
LM is combined log-linearly with the general LM
and all the other models and features. The scal-
ing factors for the document-level LMs and phrase
tables are not document-specific; neither is the
linear interpolation factor for a document-level
phrase table which we tuned manually on a devel-
opment set. The scaling factor for the document-
level LM was optimized together with the other
scaling factors using Minimum Error Rate Train-
ing (MERT, see (Och, 2003)).
For English-to-French translation, we used both
document-level phrase tables and document-level
LMs; the background data for them contained the
sub-sampled Gigaword corpus (see Section 3.3).
We used only the document-level LMs for the
Russian-to-English translation. They were ex-
tracted from the same data that was used to train
the background phrase table.
5 Morphological Transformations of
Russian
Russian is a morphologically rich language. Even
for large vocabulary MT systems this leads to data
sparseness and high out-of-vocabulary rate. To
160
mitigate this problem, we developed rules for re-
ducing the morphological complexity of the lan-
guage, making it closer to English in terms of the
used word forms. Another goal was to ease the
translation of some morphological and syntactic
phenomena in Russian by simplifying them; this
included adding artificial function words.
We used the pymorphy morphological analyzer4
to analyze Russian words in the input text. The
output of pymorphy is one or more alternative
analyses for each word, each of which includes
the POS tag plus morphological categories such
as gender, tense, etc. The analyses are generated
based on a manual dictionary, do not depend on
the context, and are not ordered by probability of
any kind. However, to make some functional mod-
ifications to the input sentences, we applied the
tool not to the vocabulary, but to the actual input
text; thus, in some cases, we introduced a context
dependency. To deterministically select one of the
pymorphy?s analyses, we defined a POS priority
list. Nouns had a higher priority than adjectives,
and adjectives higher priority than verbs. Other-
wise we relied on the first analysis for each POS.
The main idea behind our hand-crafted rules
was to normalize any ending/suffix which does not
carry information necessary for correct translation
into English. Under normalization we mean the
restoration of some ?base? form. The pymorphy
analyzer API provides inflection functions so that
each word could be changed into a particular form
(case, tense, etc.). We came up with the following
normalization rules:
? convert all adjectives and participles to first-
person masculine singular, nominative case;
? convert all nouns to the nominative case
keeping the plural/singular distinction;
? for nouns in genitive case, add the artificial
function word ?of ? after the last noun before
the current one, if the last noun is not more
than 4 positions away;
? for each verb infinitive, add the artificial
function word ?to ? in front of it;
? convert all present-tense verbs to their infini-
tive form;
? convert all past-tense verbs to their past-tense
first-person masculine singular form;
? convert all future-tense verbs to the artificial
function word ?will ? + the infinitive;
4https://bitbucket.org/kmike/pymorphy
? For verbs ending with reflexive suffixes
??/??, add the artificial function word ?sya ?
in front of the verb and remove the suf-
fix. This is done to model the reflexion (e.g.
??? ????????? ??? sya_ ??????? ? ?he
washed himself?, here ?sya ? corresonds to
?himself?), as well as, in other cases, the pas-
sive mood (e.g. ??? ???????????  ??
sya_ ?????????? ?it is inserted?).
An example that is characteristic of all these mod-
ifications is given in Figure 1.
It is worth noting that not all of these transfor-
mations are error-free because the analysis is also
not always error-free. Also, sometimes there is in-
formation loss (as in case of the instrumental noun
case, for example, which we currently drop instead
of finding the right artificial preposition to express
it). Nevertheless, our experiments show that this is
a successful morphological normalization strategy
for a statistical MT system.
6 Automatic Spelling Correction
Machine translation input texts, even if prepared
for evaluations such as WMT, still contain spelling
errors, which lead to serious translation errors. We
extended the Omnifluent system by a spelling cor-
rection module based on Hunspell5 ? an open-
source spelling correction software and dictionar-
ies. For each input word that is unknown both to
the Omnifluent MT system and to Hunspell, we
add those Hunspell?s spelling correction sugges-
tions to the input which are in the vocabulary of
the MT system. They are encoded in a lattice and
assigned weights. The weight of a suggestion is
inversely proportional to its rank in the Hunspell?s
list (the first suggestions are considered to be more
probable) and proportional to the unigram proba-
bility of the word(s) in the suggestion. To avoid
errors related to unknown names, we do not apply
spelling correction to words which begin with an
uppercase letter.
The lattice is translated by the decoder using
the method described in (Matusov et al, 2008);
the globally optimal suggestion is selected in the
translation process. On the English-to-French
task, 77 out of 3000 evaluation data sentences
were translated differently because of automatic
spelling correction. The BLEU score on these
sentences improved from 22.4 to 22.6%. Man-
ual analysis of the results shows that in around
5http://hunspell.sourceforge.net
161
source ???? ?????????? ? ????? ????????? ?????? ????????? ????? ????? ????????? ???? ?? ????
prep ???? sya_ ???????? ? ????? ????????? ?????? ????????? ???? ????? ????????? of_ ??? ?? ????
ref The dinner was held at a Washington hotel a few hours after the conference of the court over the case
Figure 1: Example of the proposed morphological normalization rules and insertion of artificial function
words for Russian.
System BLEU PER
[%] [%]
baseline 31.3 41.1
+ extended features 31.7 41.0
+ alignment combination 32.1 40.6
+ doc-level models 32.7 39.3
+ common-crawl/UN data 33.0 39.9
Table 1: English-to-French translation results
(newstest-2012-part2 progress test set).
70% of the cases the MT system picks the right
or almost right correction. We applied automatic
spelling correction also to the Russian-to-English
evaluation submissions. Here, the spelling correc-
tion was applied to words which remained out-of-
vocabulary after applying the morphological nor-
malization rules.
7 Experiments
7.1 Development Data and Evaluation
Criteria
For our experiments, we divided the 3000-
sentence newstest-2012 test set from the WMT
2012 evaluation in two roughly equal parts, re-
specting document boundaries. The first part we
used as a tuning set for N-best list MERT opti-
mization (Och, 2003). We used the second part
as a test set to measure progress; the results on it
are reported below. We computed case-insensitive
BLEU score (Papineni et al, 2002) for optimiza-
tion and evaluation. Only one reference translation
was available.
7.2 English-to-French System
The baseline system for the English-to-French
translation direction was trained on Europarl and
News Commentary corpora. The word align-
ment was obtained by training HMM and IBM
Model 3 alignment models and combining their
two directions using the ?grow-diag-final? heuris-
tic (Koehn, 2004). The first line in Table 1 shows
the result for this system when we only use the
standard features (phrase translation and word lex-
icon costs in both directions, the base reorder-
System BLEU PER
[%] [%]
baseline (full forms) 30.1 38.9
morph. reduction 31.3 38.1
+ extended features 32.4 37.3
+ doc-level LMs 32.3 37.4
+ common-crawl data 32.9 37.1
Table 2: Russian-to-English translation results
(newstest-2012-part2 progress test set).
ing features as described in (Matusov and Ko?pru?,
2010b) and the 5-gram target LM). When we
also optimize the scaling factors for extended fea-
tures, including the word-based and POS-based
lexicalized reordering models described in (Ma-
tusov and Ko?pru?, 2010a), we improve the BLEU
score by 0.4% absolute. Extracting phrase pairs
from three different, equally weighted alignment
heuristics improves the score by another 0.3%.
The next big improvement comes from using
document-level language models and phrase ta-
bles, which include Gigaword data. Especially the
PER decreases significantly, which indicates that
the document-level models help, in most cases, to
select the right word translations. Another signifi-
cant improvement comes from adding parts of the
Common-crawl and Multi-UN data, sub-sampled
with the perplexity-based method as described in
Section 3.3. The settings corresponding to the last
line of Table 1 were used to produce the Omniflu-
ent primary submission, which resulted in a BLEU
score of 27.3 on the WMT 2013 test set.
After the deadline for submission, we discov-
ered a bug in the extraction of the phrase table
which had reduced the positive impact of the ex-
tended phrase-level features. We re-ran the opti-
mization on our tuning set and obtained a BLEU
score of 27.7% on the WMT 2013 evaluation set.
7.3 Russian-to-English System
The first experiment with the Russian-to-English
system was to show the positive effect of the
morphological transformations described in Sec-
tion 5. Table 2 shows the result of the baseline
system, trained using full forms of the Russian
162
words on the News Commentary, truecased Yan-
dex and Wiki Headlines data. When applying the
morphological transformations described in Sec-
tion 5 both in training and translation, we obtain
a significant improvement in BLEU of 1.3% ab-
solute. The out-of-vocabulary rate was reduced
from 0.9 to 0.5%. This shows that the morpholog-
ical reduction actually helps to alleviate the data
sparseness problem and translate structurally com-
plex constructs in Russian.
Significant improvements are obtained for Ru?
En through the use of extended features, including
the lexicalized and ?POS?-based reordering mod-
els. As the ?POS? tags for the Russian words we
used the pymorphy POS tag selected deterministi-
cally based on our priority list, together with the
codes for additional morphological features such
as tense, case, and gender. In contrast to the En?
Fr task, document-level models did not help here,
most probably because we used only LMs and
only trained on sub-sampled data that was already
part of the background phrase table. The last boost
in translation quality was obtained by adding those
segments of the cleaned Common-crawl data to
the phrase table training which are similar to the
development and evaluation data in terms of LM
perplexity. The BLEU score in the last line of Ta-
ble 2 corresponds to Omnifluent?s BLEU score of
24.2% on the WMT 2013 evaluation data. This is
only 1.7% less than the score of the best BLEU-
ranked system in the evaluation.
8 Summary and Future Work
In this paper we described the Omnifluent hybrid
MT system and its use for the English-to-French
and Russian-to-English WMT tasks. We showed
that it is important for good translation quality to
perform careful data filtering and selection, as well
as use document-specific phrase tables and LMs.
We also proposed and evaluated rule-based mor-
phological normalizations for Russian. They sig-
nificantly improved the Russian-to-English trans-
lation quality. In contrast to some evaluation par-
ticipants, the presented high-quality system is fast
and can be quickly turned into a real-time system.
In the future, we intend to improve the rule-based
component of the system, allowing users to add
and delete translation rules on-the-fly.
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain Adaptation via Pseudo In-Domain
Data Selection. In International Conference on Em-
perical Methods in Natural Language Processing,
Edinburgh, UK, July.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In Proceed-
ings of Interspeech, pages 1618?1621.
Kenneth Heafield. 2011. KenLM: faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197, Edinburgh, Scotland, United Kingdom,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Annual Meeting of the Association for Com-
putational Linguistics (ACL), Prague, Czech Repub-
lic. Association for Computational Linguistics.
Philipp Koehn. 2004. Pharaoh: a beam search
decoder for phrase-based statistical machine trans-
lation models. In 6th Conference of the As-
sociation for Machine Translation in the Ameri-
cas (AMTA 04), pages 115?124, Washington DC,
September/October.
Evgeny Matusov and Selc?uk Ko?pru?. 2010a. AppTek?s
APT Machine Translation System for IWSLT 2010.
In Proc. of the International Workshop on Spoken
Language Translation, Paris, France, December.
Evgeny Matusov and Selc?uk Ko?pru?. 2010b. Improv-
ing Reordering in Statistical Machine Translation
from Farsi. In AMTA 2010: The Ninth Conference
of the Association for Machine Translation in the
Americas, Denver, Colorado, USA, November.
Evgeny Matusov, Bjo?rn Hoffmeister, and Hermann
Ney. 2008. ASR word lattice translation with
exhaustive reordering is possible. In Interspeech,
pages 2342?2345, Brisbane, Australia, September.
Evgeny Matusov. 2012. Incremental Re-training of a
Hybrid English-French MT System with Customer
Translation Memory Data. In 10th Conference of the
Association for Machine Translation in the Amer-
icas (AMTA 12), San Diego, CA, USA, October-
November.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In 41st Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02: Pro-
ceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 311?318,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Richard Zens. 2008. Phrase-based Statistical Machine
Translation: Models, Search, Training. Ph.D. the-
sis, RWTH Aachen University, Aachen, Germany,
February.
163
