Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13?18,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Measuring the Semantic Similarity of Texts
Courtney Corley and Rada Mihalcea
Department of Computer Science
University of North Texas
{corley,rada}@cs.unt.edu
Abstract
This paper presents a knowledge-based
method for measuring the semantic-
similarity of texts. While there is a large
body of previous work focused on find-
ing the semantic similarity of concepts
and words, the application of these word-
oriented methods to text similarity has not
been yet explored. In this paper, we in-
troduce a method that combines word-
to-word similarity metrics into a text-to-
text metric, and we show that this method
outperforms the traditional text similarity
metrics based on lexical matching.
1 Introduction
Measures of text similarity have been used for a
long time in applications in natural language pro-
cessing and related areas. One of the earliest ap-
plications of text similarity is perhaps the vectorial
model in information retrieval, where the document
most relevant to an input query is determined by
ranking documents in a collection in reversed or-
der of their similarity to the given query (Salton and
Lesk, 1971). Text similarity has been also used for
relevance feedback and text classification (Rocchio,
1971), word sense disambiguation (Lesk, 1986), and
more recently for extractive summarization (Salton
et al, 1997b), and methods for automatic evaluation
of machine translation (Papineni et al, 2002) or text
summarization (Lin and Hovy, 2003).
The typical approach to finding the similarity be-
tween two text segments is to use a simple lexical
matching method, and produce a similarity score
based on the number of lexical units that occur in
both input segments. Improvements to this simple
method have considered stemming, stop-word re-
moval, part-of-speech tagging, longest subsequence
matching, as well as various weighting and normal-
ization factors (Salton et al, 1997a). While success-
ful to a certain degree, these lexical matching simi-
larity methods fail to identify the semantic similarity
of texts. For instance, there is an obvious similarity
between the text segments I own a dog and I have
an animal, but most of the current text similarity
metrics will fail in identifying any kind of connec-
tion between these texts. The only exception to this
trend is perhaps the latent semantic analysis (LSA)
method (Landauer et al, 1998), which represents
an improvement over earlier attempts to use mea-
sures of semantic similarity for information retrieval
(Voorhees, 1993), (Xu and Croft, 1996). LSA aims
to find similar terms in large text collections, and
measure similarity between texts by including these
additional related words. However, to date LSA has
not been used on a large scale, due to the complex-
ity and computational cost associated with the algo-
rithm, and perhaps also due to the ?black-box? ef-
fect that does not allow for any deep insights into
why some terms are selected as similar during the
singular value decomposition process.
In this paper, we explore a knowledge-based
method for measuring the semantic similarity of
texts. While there are several methods previ-
ously proposed for finding the semantic similar-
ity of words, to our knowledge the application of
these word-oriented methods to text similarity has
not been yet explored. We introduce an algorithm
13
that combines the word-to-word similarity metrics
into a text-to-text semantic similarity metric, and we
show that this method outperforms the simpler lex-
ical matching similarity approach, as measured in a
paraphrase identification application.
2 Measuring Text Semantic Similarity
Given two input text segments, we want to auto-
matically derive a score that indicates their similar-
ity at semantic level, thus going beyond the simple
lexical matching methods traditionally used for this
task. Although we acknowledge the fact that a com-
prehensive metric of text semantic similarity should
take into account the relations between words, as
well as the role played by the various entities in-
volved in the interactions described by each of the
two texts, we take a first rough cut at this problem
and attempt to model the semantic similarity of texts
as a function of the semantic similarity of the com-
ponent words. We do this by combining metrics of
word-to-word similarity and language models into
a formula that is a potentially good indicator of the
semantic similarity of the two input texts.
2.1 Semantic Similarity of Words
There is a relatively large number of word-to-word
similarity metrics that were previously proposed in
the literature, ranging from distance-oriented mea-
sures computed on semantic networks, to metrics
based on models of distributional similarity learned
from large text collections. From these, we chose to
focus our attention on six different metrics, selected
mainly for their observed performance in natural
language processing applications, e.g. malapropism
detection (Budanitsky and Hirst, 2001) and word
sense disambiguation (Patwardhan et al, 2003), and
for their relatively high computational efficiency.
We conduct our evaluation using the following
word similarity metrics: Leacock & Chodorow,
Lesk, Wu & Palmer, Resnik, Lin, and Jiang & Con-
rath. Note that all these metrics are defined be-
tween concepts, rather than words, but they can be
easily turned into a word-to-word similarity metric
by selecting for any given pair of words those two
meanings that lead to the highest concept-to-concept
similarity. We use the WordNet-based implemen-
tation of these metrics, as available in the Word-
Net::Similarity package (Patwardhan et al, 2003).
We provide below a short description for each of
these six metrics.
The Leacock & Chodorow (Leacock and
Chodorow, 1998) similarity is determined as:
Simlch = ? log
length
2 ? D (1)
where length is the length of the shortest path be-
tween two concepts using node-counting, and D is
the maximum depth of the taxonomy.
The Lesk similarity of two concepts is defined as a
function of the overlap between the corresponding
definitions, as provided by a dictionary. It is based
on an algorithm proposed in (Lesk, 1986) as a solu-
tion for word sense disambiguation.
The Wu and Palmer (Wu and Palmer, 1994) simi-
larity metric measures the depth of the two concepts
in the WordNet taxonomy, and the depth of the least
common subsumer (LCS), and combines these fig-
ures into a similarity score:
Simwup =
2 ? depth(LCS)
depth(concept1) + depth(concept2)
(2)
The measure introduced by Resnik (Resnik, 1995)
returns the information content (IC) of the LCS of
two concepts:
Simres = IC(LCS) (3)
where IC is defined as:
IC(c) = ? log P (c) (4)
and P (c) is the probability of encountering an in-
stance of concept c in a large corpus.
The next measure we use in our experiments is the
metric introduced by Lin (Lin, 1998), which builds
on Resnik?s measure of similarity, and adds a nor-
malization factor consisting of the information con-
tent of the two input concepts:
Simlin =
2 ? IC(LCS)
IC(concept1) + IC(concept2)
(5)
Finally, the last similarity metric we consider is
Jiang & Conrath (Jiang and Conrath, 1997), which
returns a score determined by:
Simjnc =
1
IC(concept1) + IC(concept2) ? 2 ? IC(LCS)
(6)
14
2.2 Language Models
In addition to the semantic similarity of words, we
also want to take into account the specificity of
words, so that we can give a higher weight to a se-
mantic matching identified between two very spe-
cific words (e.g. collie and sheepdog), and give less
importance to the similarity score measured between
generic concepts (e.g. go and be). While the speci-
ficity of words is already measured to some extent
by their depth in the semantic hierarchy, we are re-
inforcing this factor with a corpus-based measure of
word specificity, based on distributional information
learned from large corpora.
Language models are frequently used in natural
language processing applications to account for the
distribution of words in language. While word fre-
quency does not always constitute a good measure of
word importance, the distribution of words across an
entire collection can be a good indicator of the speci-
ficity of the words. Terms that occur in a few docu-
ments with high frequency contain a greater amount
of discriminatory ability, while terms that occur in
numerous documents across a collection with a high
frequency have inherently less meaning to a docu-
ment. We determine the specificity of a word us-
ing the inverse document frequency introduced in
(Sparck-Jones, 1972), which is defined as the total
number of documents in the corpus, divided by the
total number of documents that include that word.
In the experiments reported in this paper, we use the
British National Corpus to derive the document fre-
quency counts, but other corpora could be used to
the same effect.
2.3 Semantic Similarity of Texts
Provided a measure of semantic similarity between
words, and an indication of the word specificity, we
combine them into a measure of text semantic sim-
ilarity, by pairing up those words that are found to
be most similar to each other, and weighting their
similarity with the corresponding specificity score.
We define a directional measure of similarity,
which indicates the semantic similarity of a text seg-
ment Ti with respect to a text segment Tj . This def-
inition provides us with the flexibility we need to
handle applications where the directional knowledge
is useful (e.g. entailment), and at the same time it
gives us the means to handle bidirectional similarity
through a simple combination of two unidirectional
metrics.
For a given pair of text segments, we start by cre-
ating sets of open-class words, with a separate set
created for nouns, verbs, adjectives, and adverbs.
In addition, we also create a set for cardinals, since
numbers can also play an important role in the un-
derstanding of a text. Next, we try to determine pairs
of similar words across the sets corresponding to the
same open-class in the two text segments. For nouns
and verbs, we use a measure of semantic similarity
based on WordNet, while for the other word classes
we apply lexical matching1.
For each noun (verb) in the set of nouns (verbs)
belonging to one of the text segments, we try to iden-
tify the noun (verb) in the other text segment that has
the highest semantic similarity (maxSim), accord-
ing to one of the six measures of similarity described
in Section 2.1. If this similarity measure results in a
score greater than 0, then the word is added to the set
of similar words for the corresponding word class
WSpos2. The remaining word classes: adjectives,
adverbs, and cardinals, are checked for lexical sim-
ilarity with their counter-parts and included in the
corresponding word class set if a match is found.
The similarity between the input text segments Ti
and Tj is then determined using a scoring function
that combines the word-to-word similarities and the
word specificity:
sim(Ti, Tj)Ti =
?
pos
(
?
wk?{WSpos}
(maxSim(wk) ? idfwk ))
?
wk?{Tipos}
idfwk
(7)
This score, which has a value between 0 and 1, is
a measure of the directional similarity, in this case
computed with respect to Ti. The scores from both
directions can be combined into a bidirectional sim-
ilarity using a simple average function:
sim(Ti, Tj) =
sim(Ti, Tj)Ti + sim(Ti, Tj)Tj
2 (8)
1The reason behind this decision is the fact that most of the
semantic similarity measures apply only to nouns and verbs, and
there are only one or two relatedness metrics that can be applied
to adjectives and adverbs.
2All similarity scores have a value between 0 and 1. The
similarity threshold can be also set to a value larger than 0,
which would result in tighter measures of similarity.
15
Text Segment 1: The jurors were taken into the courtroom in
groups of 40 and asked to fill out a questionnaire.
? SetNN = {juror, courtroom, group, questionnaire}
SetV B = {be, take, ask, fill}
SetRB = {out}
SetCD = {40}
Text Segment 2: About 120 potential jurors were being asked
to complete a lengthy questionnaire.
? SetNN = {juror, questionnaire}
SetV B = {be, ask, complete}
SetJJ = {potential, lengthy}
SetCD = {120}
Figure 1: Two text segments and their corresponding
word class sets
3 A Walk-Through Example
We illustrate the application of the text similarity
measure with an example. Given two text segments,
as shown in Figure 1, we want to determine a score
that reflects their semantic similarity. For illustration
purposes, we restrict our attention to one measure of
word-to-word similarity, the Wu & Palmer metric.
First, the text segments are tokenized, part-of-
speech tagged, and the words are inserted into their
corresponding word class sets. The sets obtained for
the given text segments are illustrated in Figure 1.
Starting with each of the two text segments, and
for each word in its word class sets, we determine
the most similar word from the corresponding set in
the other text segment. As mentioned earlier, we
seek a WordNet-based semantic similarity for nouns
and verbs, and only lexical matching for adjectives,
adverbs, and cardinals. The word semantic similar-
ity scores computed starting with the first text seg-
ment are shown in Table 3.
Text 1 Text 2 maxSim IDF
jurors jurors 1.00 5.80
courtroom jurors 0.30 5.23
questionnaire questionnaire 1.00 3.57
groups questionnaire 0.29 0.85
were were 1.00 0.09
taken asked 1.00 0.28
asked asked 1.00 0.45
fill complete 0.86 1.29
out ? 0 0.06
40 ? 0 1.39
Table 1: Wu & Palmer word similarity scores for
computing text similarity with respect to text 1
Next, we use equation 7 and determine the seman-
tic similarity of the two text segments with respect
to text 1 as 0.6702, and with respect to text 2 as
0.7202. Finally, the two figures are combined into
a bidirectional measure of similarity, calculated as
0.6952 based on equation 8.
Although there are a few words that occur in both
text segments (e.g. juror, questionnaire), there are
also words that are not identical, but closely related,
e.g. courtroom found similar to juror, or fill which
is related to complete. Unlike traditional similar-
ity measures based on lexical matching, our metric
takes into account the semantic similarity of these
words, resulting in a more precise measure of text
similarity.
4 Evaluation
To test the effectiveness of the text semantic simi-
larity metric, we use this measure to automatically
identify if two text segments are paraphrases of
each other. We use the Microsoft paraphrase cor-
pus (Dolan et al, 2004), consisting of 4,076 training
pairs and 1,725 test pairs, and determine the number
of correctly identified paraphrase pairs in the cor-
pus using the text semantic similarity measure as the
only indicator of paraphrasing. In addition, we also
evaluate the measure using the PASCAL corpus (Da-
gan et al, 2005), consisting of 1,380 test?hypothesis
pairs with a directional entailment (580 development
pairs and 800 test pairs).
For each of the two data sets, we conduct two
evaluations, under two different settings: (1) An un-
supervised setting, where the decision on what con-
stitutes a paraphrase (entailment) is made using a
constant similarity threshold of 0.5 across all exper-
iments; and (2) A supervised setting, where the op-
timal threshold and weights associated with various
similarity metrics are determined through learning
on training data. In this case, we use a voted percep-
tron algorithm (Freund and Schapire, 1998)3.
We evaluate the text similarity metric built on top
of the various word-to-word metrics introduced in
Section 2.1. For comparison, we also compute three
baselines: (1) A random baseline created by ran-
domly choosing a true or false value for each text
pair; (2) A lexical matching baseline, which only
3Classification using this algorithm was determined optimal
empirically through experiments.
16
counts the number of matching words between the
two text segments, while still applying the weighting
and normalization factors from equation 7; and (3)
A vectorial similarity baseline, using a cosine sim-
ilarity measure as traditionally used in information
retrieval, with tf.idf term weighting. For compari-
son, we also evaluated the corpus-based similarity
obtained through LSA; however, the results obtained
were below the lexical matching baseline and are not
reported here.
For paraphrase identification, we use the bidirec-
tional similarity measure, and determine the sim-
ilarity with respect to each of the two text seg-
ments in turn, and then combine them into a bidi-
rectional similarity metric. For entailment identifi-
cation, since this is a directional relation, we only
measure the semantic similarity with respect to the
hypothesis (the text that is entailed).
We evaluate the results in terms of accuracy, rep-
resenting the number of correctly identified true or
false classifications in the test data set. We also mea-
sure precision, recall and F-measure, calculated with
respect to the true values in each of the test data sets.
Tables 2 and 3 show the results obtained in the
unsupervised setting, when a text semantic similar-
ity larger than 0.5 was considered to be an indica-
tor of paraphrasing (entailment). We also evaluate a
metric that combines all the similarity measures us-
ing a simple average, with results indicated in the
Combined row.
The results obtained in the supervised setting are
shown in Tables 4 and 5. The optimal combination
of similarity metrics and optimal threshold are now
determined in a learning process performed on the
training set. Under this setting, we also compute an
additional baseline, consisting of the most frequent
label, as determined from the training data.
5 Discussion and Conclusions
For the task of paraphrase recognition, incorporating
semantic information into the text similarity mea-
sure increases the likelihood of recognition signifi-
cantly over the random baseline and over the lexi-
cal matching baseline. In the unsupervised setting,
the best performance is achieved using a method that
combines several similarity metrics into one, for an
overall accuracy of 68.8%. When learning is used to
find the optimal combination of metrics and optimal
threshold, the highest accuracy of 71.5% is obtained
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
J & C 0.683 0.724 0.846 0.780
L & C 0.680 0.724 0.838 0.777
Lesk 0.680 0.724 0.838 0.777
Lin 0.679 0.717 0.855 0.780
W & P 0.674 0.722 0.831 0.773
Resnik 0.672 0.725 0.815 0.768
Combined 0.688 0.741 0.817 0.777
Baselines
LexMatch 0.661 0.722 0.798 0.758
Vectorial 0.654 0.716 0.795 0.753
Random 0.513 0.683 0.500 0.578
Table 2: Text semantic similarity for paraphrase
identification (unsupervised)
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
J & C 0.573 0.543 0.908 0.680
L & C 0.569 0.543 0.870 0.669
Lesk 0.568 0.542 0.875 0.669
Resnik 0.565 0.541 0.850 0.662
Lin 0.563 0.538 0.878 0.667
W & P 0.558 0.534 0.895 0.669
Combined 0.583 0.561 0.755 0.644
Baselines
LexMatch 0.545 0.530 0.795 0.636
Vectorial 0.528 0.525 0.588 0.555
Random 0.486 0.486 0.493 0.489
Table 3: Text semantic similarity for entailment
identification (unsupervised)
by combining the similarity metrics and the lexical
matching baseline together.
For the entailment data set, although we do not
explicitly check for entailment, the directional sim-
ilarity computed for textual entailment recognition
does improve over the random and lexical matching
baselines. Once again, the combination of similar-
ity metrics gives the highest accuracy, measured at
58.3%, with a slight improvement observed in the
supervised setting, where the highest accuracy was
measured at 58.9%. Both these figures are compet-
itive with the best results achieved during the PAS-
CAL entailment evaluation (Dagan et al, 2005).
Although our method relies on a bag-of-words ap-
proach, as it turns out the use of measures of seman-
tic similarity improves significantly over the tradi-
tional lexical matching metrics4. We are nonetheless
4The improvement of the combined semantic similarity met-
ric over the simpler lexical matching measure was found to be
statistically significant in all experiments, using a paired t-test
(p < 0.001).
17
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
Lin 0.702 0.706 0.947 0.809
W & P 0.699 0.705 0.941 0.806
L & C 0.699 0.708 0.931 0.804
J & C 0.699 0.707 0.935 0.805
Lesk 0.695 0.702 0.929 0.800
Resnik 0.692 0.705 0.921 0.799
Combined 0.715 0.723 0.925 0.812
Baselines
LexMatch 0.671 0.693 0.908 0.786
Vectorial 0.665 0.665 1.000 0.799
Most frequent 0.665 0.665 1.000 0.799
Table 4: Text semantic similarity for paraphrase
identification (supervised)
Metric Acc. Prec. Rec. F
Semantic similarity (knowledge-based)
L & C 0.583 0.573 0.650 0.609
W & P 0.580 0.570 0.648 0.607
Resnik 0.579 0.572 0.628 0.598
Lin 0.574 0.568 0.620 0.593
J & C 0.575 0.566 0.643 0.602
Lesk 0.573 0.566 0.633 0.597
Combined 0.589 0.579 0.650 0.612
Baselines
LexMatch 0.568 0.573 0.530 0.551
Most frequent 0.500 0.500 1.000 0.667
Vectorial 0.479 0.484 0.645 0.553
Table 5: Text semantic similarity for entailment
identification (supervised)
aware that a bag-of-words approach ignores many of
important relationships in sentence structure, such as
dependencies between words, or roles played by the
various arguments in the sentence. Future work will
consider the investigation of more sophisticated rep-
resentations of sentence structure, such as first order
predicate logic or semantic parse trees, which should
allow for the implementation of more effective mea-
sures of text semantic similarity.
References
A. Budanitsky and G. Hirst. 2001. Semantic distance in word-
net: An experimental, application-oriented evaluation of five
measures. In Proceedings of the NAACL Workshop on Word-
Net and Other Lexical Resources, Pittsburgh, June.
I. Dagan, O. Glickman, and B. Magnini. 2005. The PASCAL
recognising textual entailment challenge. In Proceedings of
the PASCAL Workshop.
W.B. Dolan, C. Quirk, and C. Brockett. 2004. Unsuper-
vised construction of large paraphrase corpora: Exploiting
massively parallel news sources. In Proceedings of the
20th International Conference on Computational Linguis-
tics, Geneva, Switzerland.
Y. Freund and R.E. Schapire. 1998. Large margin classifica-
tion using the perceptron algorithm. In Proceedings of the
11th Annual Conference on Computational Learning The-
ory, pages 209?217, New York, NY. ACM Press.
J. Jiang and D. Conrath. 1997. Semantic similarity based on
corpus statistics and lexical taxonomy. In Proceedings of
the International Conference on Research in Computational
Linguistics, Taiwan.
T. K. Landauer, P. Foltz, and D. Laham. 1998. Introduction to
latent semantic analysis. Discourse Processes, 25.
C. Leacock and M. Chodorow. 1998. Combining local context
and WordNet sense similiarity for word sense disambigua-
tion. In WordNet, An Electronic Lexical Database. The MIT
Press.
M.E. Lesk. 1986. Automatic sense disambiguation using ma-
chine readable dictionaries: How to tell a pine cone from an
ice cream cone. In Proceedings of the SIGDOC Conference
1986, Toronto, June.
C.Y. Lin and E.H. Hovy. 2003. Automatic evaluation of
summaries using n-gram co-occurrence statistics. In Pro-
ceedings of Human Language Technology Conference (HLT-
NAACL 2003), Edmonton, Canada, May.
D. Lin. 1998. An information-theoretic definition of similar-
ity. In Proceedings of the 15th International Conference on
Machine Learning, Madison, WI.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. Bleu:
a method for automatic evaluation of machine translation.
In Proceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics (ACL 2002), Philadel-
phia, PA, July.
S. Patwardhan, S. Banerjee, and T. Pedersen. 2003. Using mea-
sures of semantic relatedness for word sense disambiguation.
In Proceedings of the Fourth International Conference on
Intelligent Text Processing and Computational Linguistics,
Mexico City, February.
P. Resnik. 1995. Using information content to evaluate seman-
tic similarity. In Proceedings of the 14th International Joint
Conference on Artificial Intelligence, Montreal, Canada.
J. Rocchio, 1971. Relevance feedback in information retrieval.
Prentice Hall, Ing. Englewood Cliffs, New Jersey.
G. Salton and M.E. Lesk, 1971. Computer evaluation of index-
ing and text processing, pages 143?180. Prentice Hall, Ing.
Englewood Cliffs, New Jersey.
G. Salton, , and A. Bukley. 1997a. Term weighting approaches
in automatic text retrieval. In Readings in Information Re-
trieval. Morgan Kaufmann Publishers, San Francisco, CA.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997b. Auto-
matic text structuring and summarization. Information Pro-
cessing and Management, 2(32).
K. Sparck-Jones. 1972. A statistical interpretation of term
specificity and its applicatoin in retrieval. Journal of Doc-
umentation, 28(1):11?21.
E. Voorhees. 1993. Using wordnet to disambiguate word
senses for text retrieval. In Proceedings of the 16th annual
international ACM SIGIR conference, Pittsburgh, PA.
Z. Wu and M. Palmer. 1994. Verb semantics and lexical se-
lection. In Proceedings of the 32nd Annual Meeting of the
Association for Computational Linguistics, Las Cruces, New
Mexico.
J. Xu and W. B. Croft. 1996. Query expansion using local and
global document analysis. In Proceedings of the 19th annual
international ACM SIGIR conference, Zurich, Switzerland.
18
Proceedings of BioNLP Shared Task 2011 Workshop, pages 130?137,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Complex Biological Event Extraction from Full Text using Signatures of
Linguistic and Semantic Features
Liam R. McGrath and Kelly Domico and Courtney D. Corley and Bobbie-Jo Webb-Robertson
Pacific Northwest National Laboratory
902 Battelle BLVD, PO BOX 999
Richland, WA 99352
{liam | kelly.domico | court | bj}@pnl.gov
Abstract
Building on technical advances from the
BioNLP 2009 Shared Task Challenge, the
2011 challenge sets forth to generalize tech-
niques to other complex biological event ex-
traction tasks. In this paper, we present the
implementation and evaluation of a signature-
based machine-learning technique to predict
events from full texts of infectious disease
documents. Specifically, our approach uses
novel signatures composed of traditional lin-
guistic features and semantic knowledge to
predict event triggers and their candidate argu-
ments. Using a leave-one out analysis, we re-
port the contribution of linguistic and shallow
semantic features in the trigger prediction and
candidate argument extraction. Lastly, we ex-
amine evaluations and posit causes for errors
in our complex biological event extraction.
1 Introduction
The BioNLP 2009 Shared Task (Kim et al, 2009)
was the first shared task to address fine-grained in-
formation extraction for the bio-molecular domain,
by defining a task involving extraction of event
types from the GENIA ontology. The BioNLP 2011
Shared Task ( (Kim et al, 2011)) series generalized
this defining a series of tasks involving more text
types, domains and target event types. Among the
tasks for the new series is the Infection Disease task,
proposed and investigated by (Pyysalo et al, 2011;
Pyysalo et al, 2010; Bjorne et al, 2010).
Like the other tasks for the BioNLP Shared Task
series, the goal is to extract mentions of relevant
events from biomedical publications. To extract
an event, the event trigger and all arguments must
be identified in the text by exact offset and typed
according to a given set of event and argument
classes (Miwa et al, 2010). Entity annotations are
given for a set of entity types that fill many of the
arguments.
Here we describe Pacific Northwest National Lab-
oratory?s (PNNL) submission to the BioNLP 2011
Infectious Disease shared task. We describe the ap-
proach and then discuss results, including an analy-
sis of errors and contribution of various features.
2 Approach
Our system uses a signature-based machine-learning
approach. The system is domain-independent,
using a primary task description vocabulary and
training data to learn the task, but domain re-
sources can be incorporated as additional features
when available, as described here. The approach
can be broken down into 4 components: an au-
tomated annotation pipeline to provide the basis
for features, classification-based trigger identifica-
tion and argument identification components, and a
post-processing component to apply semantic con-
straints. The UIMA framework1 is used to integrate
the components into a pipeline architecture.
2.1 Primary Tasks
A definition of the events to be extracted is used to
define candidates for classification and post-process
the results of the classification. First a list of
domain-specific entity classes is given. Entities of
1http://uima.apache.org/
130
Event Class Arguments
Gene expression Theme(Protein|Regulon-operon)
Transcription Theme(Protein|Regulon-operon)
Protein catabolism Theme(Protein)
Phosphorylation Theme(Protein), Site(entity)?
Localization Theme(core entity), AtLoc(entity)?, ToLoc(entity)?
Binding Theme(core entity)+, Site(entity)*
Regulation Theme(core entity|event), Cause(core entity|event)?, Site(entity)?, CSite(entity)?
Positive regulation Theme(core entity|event), Cause(core entity|event)?, Site(entity)?, CSite(entity)?
Negative regulation Theme(core entity|event), Cause(core entity|event)?, Site(entity)?, CSite(entity)?
Process Participant(core entity)?
Table 1: Summary of the target events. Type restrictions on fillers of each argument type are shown in parenthesis.
Multiplicity of each argument type is also marked (+ = one-to-many, ? = zero-to-one, * = zero-to-many, otherwise =
one).
these classes are assumed to be annotated in the data,
as is the case for the Infectious Disease task. Then,
each event class is given, with a list of argument
types for each. Each argument is marked with its
multiplicity, indicating how many of this argument
type is valid for each event, either: one ? exactly one
is required, one-to-many ? one or more is required,
zero-to-one ? one is optional, and zero-to-many ?
one or many are optional. Also, restrictions on the
classes of entities that can fill each argument are
given, by listing: one or more class names ? indicat-
ing the valid domain-specific entity classes from the
definition, core entity ? indicating that any domain-
specific entity in the definition is valid, event ? indi-
cating that any event in the definition is valid, or en-
tity ? indicating that any span from the text is valid.
Table 1 shows the summary of the event extraction
tasks for the Infectious Disease track.
2.2 Annotation
Linguistic and domain annotations are automatically
applied to the document to be used for trigger and
argument identification in framing the tasks for clas-
sification and generating features for each instance.
Linguistic annotations include sentence splits, to-
kens, parts of speech, tree parses, typed dependen-
cies (deMarneffe et al, 2006; MacKinlay et al,
2009), and stems. For the Infectious Disease task,
the parses from the Stanford Parser (Klein and Man-
ning, 2003) provided by the Supporting Analysis
(Stenetorp et al, 2011) was used to obtain all of
these linguistic annotations, except for the stems,
which were obtained from the Porter Stemmer (van
Rijsbergen et al, 1980).
For the Infectious Disease task, two sets of do-
main specific annotations are included: known
trigger words for each event class and semantic
tags from the Unified Medical Language System
(UMLS) (Bodenreider, 2004). Annotations for
known trigger words are created using a dictionary
of word stem-event class pairs created from anno-
tated training data. An entry is created in the dictio-
nary every time a new stem is seen as a trigger for
an event class. When a word with one of these stems
is seen during processing, it is annotated as a typical
trigger word for that event class.
Semantic tags are calculating using MetaMap
2010 (Aronson and Lang, 2010). MetaMap provides
semantic tags for terms in a document with up to
three levels of specificity, from most to least spe-
cific: concept, type and group (Torii et al, 2011).
Word sense disambiguation is used to identify the
best tags for each term. For example, consider the
tags identified by MetaMap for the phrase Human
peripheral B cells:
Human
concept: Homo sapiens
type: Human
group: Living Beings
Peripheral
type: Spatial Concept
group: Concepts & Ideas
B-Cells
concept: B-Lymphocytes
type: Cell
131
group: Anatomy
In this example, semantic mappings were found for
three terms: Human, Peripheral and B-Cells. Hu-
man and B-Cells were mapped to specific concepts,
but Peripheral was mapped to a more general group.
Entities are also annotated at this point. For the
Infectious Disease task, annotations for five entity
types are given: Protein, Two-component system,
Chemical, Organism, or Regulon/Operon.
2.3 Trigger Identification
Triggers are identified using an SVM classifier (Vap-
nik, 1995; Joachims, 1999). Candidate triggers are
chosen from the words in the text by part-of-speech.
Based on known triggers seen in the training data, all
nouns, verbs, adjectives, prepositions and adverbs
are selected as candidates. A binary model is trained
for each event type, and candidate triggers are tested
against each classifier.
The following features are used to classify candi-
date event triggers:
? term ? the candidate trigger
? stem ? the stem of the term
? part of speech ? the part of speech of the term
? capitalization ? capitalization of the term
? punctuation ? individual features for the pres-
ence of different punctuation types
? numerics ? the presence of a number in the
term
? ngrams ? 4-grams of characters from the term
? known trigger types ? tags from list of known
trigger terms for each event type
? lexical context ? terms in the same sentence
? syntactic dependencies ? the type and role
(governor or dependent) of typed dependencies
involving the trigger
? semantic type ? type mapping from MetaMap
? semantic group ? group mapping from
MetaMap
For training data, both the Infectious Disease
training set and the GENIA training set were used.
Although the GENIA training set represents a dif-
ferent genre and is annotated with a slightly differ-
ent vocabulary than the Infectious Disease task data,
it is similar enough to provide some beneficial su-
pervision. The Infectious Disease training data is
relatively small at 154 documents so including the
larger GENIA training set at 910 documents results
in a much more larger training set. Testing on the
Infectious Disease development data, a 1 point im-
provement in fscore in overall results is seen with
the additional training data.
2.4 Argument Identification
Arguments are also identified using an SVM classi-
fier. For each predicted trigger, candidate arguments
are selected based on the argument types. For ar-
guments that are restricted to being filled by some
set of specific entity and event types, each anno-
tated entity and predicted event is selected as a can-
didate. For arguments that can be filled by any span
of text, each span corresponding to a constituent of
the tree parse is selected as a candidate. Each pair
of an event trigger and a candidate argument serves
as an instance for the classification. A binary model
is trained for each event type, and each pair is tested
against each classifier.
Many of the features used are inspired by those
used in semantic role labeling systems (Gildea and
Jurafsky, 2002). Given an event trigger and a can-
didate argument, the following features are used to
classify event arguments:
? trigger type ? the predicted event type of the
trigger
? argument terms ? the text of the argument
? argument type ? entity or event type annota-
tion on the argument
? argument super-type ? core entity or core ar-
gument
? trigger and argument stems ? the stems of
each
? trigger and argument parts of speech ? the
part of speech of each
? parse tree path ? from the trigger to argument
via least common ancestor in tree parse, as a
list of phrase types
? voice of sentence ? active or passive
? trigger and argument partial paths ? from
the trigger or argument to the least common an-
cestor in tree parse, as a list of phrase types
132
? relative position of argument to trigger ? be-
fore or after
? trigger sub-categorization ? representation of
the phrase structure rule that describes the rela-
tionship between the trigger, its parent and its
siblings.
The training data used is the same as for trig-
ger identification: the Infectious Disease training set
plus the Genia training set.
2.5 Post-processing
A post-processing component is used to turn output
from the various classifiers into semantically valid
output according to the target task. For each pre-
dicted trigger, the positive predictions for each argu-
ment model are collected, and the set is compared to
the argument restrictions in the target task descrip-
tion.
For example, the types on argument predictions
are compared to the argument restrictions in the
target task, and non-conforming ones are dropped.
Then the multiplicity of the arguments for each pre-
dicted event is checked against the task vocabulary.
Where there were not sufficient positive argument
predictions to make a full event, the best negative
predictions from the model are tried. When a com-
pliant set of arguments can not be created for a pre-
dicted event, it is dropped.
3 Results and Discussion
Results for the system on both the development data
and the official test data for the task are shown in
Table 2 and Table 5, respectively. For the develop-
ment data, a system using gold-standard event trig-
gers is included, to isolate the performance of argu-
ment identification. In all cases, the total fscore for
non-regulation events were much higher than regula-
tion events. On the official test data, the system per-
formed the best in predicting Phosphorylation (fs-
core = 71.43), Gene Expression (fscore = 53.33) and
Process events (fscore = 51.04), but was unable to
find any Transcription and Regulation events. This
is also evident in the results on the development data
using predicted triggers; additionally, no matches
were found for localization and binding events. The
total fscore on the development data using gold trig-
gers was 55.33, more than 13 points higher than
when using predicted triggers. In the discussion that
follows, we detail the importance of individual fea-
tures and their contribution to evaluation fscores.
3.1 Feature Importance
The effect of each argument and trigger feature type
on the Infectious Disease development data was de-
termined using a leave-one-out approach. The ar-
gument and trigger feature effect results are shown
in Table 3 and Table 4, respectively. In a series of
experiments, each feature type is left out of the full
feature set one-by-one. The difference in fscore be-
tween each of these systems and the full feature set
system is the effect of the feature type; a high nega-
tive effect indicates a significant contribution to the
system since the removal of the feature resulted in a
lower fscore.
Features fscore effect
all features 41.66
w/o argument terms 36.16 -5.50
w/o argument type 39.50 -2.16
w/o trigger partial path 40.65 -1.01
w/o argument part of speech 40.98 -0.68
w/o argument partial path 41.16 -0.50
w/o trigger sub-categorization 41.45 -0.21
w/o argument stem 41.48 -0.18
w/o argument super-type 41.63 -0.03
w/o trigger type 41.63 -0.03
w/o trigger part of speech 41.81 0.15
w/o trigger stem 41.81 0.15
w/o voice of sentence 41.85 0.19
w/o relative position 42.21 0.55
w/o parse tree path 42.67 1.01
Table 3: Effect of each argument feature type on Infec-
tious Disease development data.
Within the argument feature set system, the parse
tree path feature had a notable positive effect of
1.01. The features providing the greatest contribu-
tion were argument terms and argument type with
effects of -5.50 and -2.16, respectively. Within the
trigger feature set system, the lexical context and
syntactic dependencies features showed the highest
negative effect signifying positive contribution to the
system. The text and known trigger types features
showed a negative contribution to the system.
133
Using Gold Triggers Using Predicted Triggers
Event Class gold/ans./match recall prec. fscore gold/ans./match recall prec. fscore
Gene expression 134 / 110 / 100 74.63 90.00 81.60 134 / 132 / 85 64.18 64.39 64.29
Transcription 35 / 26 / 23 65.71 88.46 75.41 25 / 0 / 0 0.00 0.00 0.00
Protein catabolism 0 / 0 / 0 0.00 0.00 0.00 0 / 0 / 0 0.00 0.00 0.00
Phosphorylation 13 / 13 / 13 100.00 100.00 100.00 13 / 14 / 13 100.00 92.86 96.30
Localization 1 / 1 / 0 0.00 0.00 0.00 1 / 10 / 0 0.00 0.00 0.00
Binding 17 / 6 / 0 0.00 0.00 0.00 17 / 3 / 0 0.00 0.00 0.00
Process 206 / 180 / 122 59.22 67.78 63.21 207 / 184 / 108 52.17 58.70 55.24
Regulation 81 / 61 / 20 24.69 32.79 28.17 80 / 0 / 0 0.00 0.00 0.00
Positive regulation 113 / 91 / 36 31.86 39.56 35.29 113 / 42 / 13 11.50 30.95 16.77
Negative regulation 90 / 71 / 32 35.56 45.07 39.75 90 / 42 / 11 12.22 26.19 16.67
TOTAL 690 / 559 / 346 50.14 61.72 55.33 680 / 427 / 230 33.97 53.86 41.66
Table 2: Results on Infectious Disease development data. The system is compared to a system using gold standard
triggers to isolate performance of argument identification.
Features fscore effect
all features 41.66
w/o lexical context 40.14 -1.52
w/o syntactic dependencies 40.28 -1.38
w/o ngrams 40.88 -0.78
w/o part of speech 41.48 -0.18
w/o capitalization 41.51 -0.15
w/o numerics 41.51 -0.15
w/o semantic group 41.55 -0.11
w/o punctuation 41.59 -0.07
w/o stem 41.74 0.08
w/o semantic type 41.82 0.16
w/o known trigger types 42.11 0.45
w/o text 42.31 0.65
Table 4: Effect of each trigger feature type on Infectious
Disease development data.
3.2 Transcription and Regulation events
Lastly, we present representative examples of errors
(e.g., false positive, false negative, poor recall) pro-
duced by our system in the Infectious Disease track
core tasks. The discussion herein will cover eval-
uations where our system did not correctly predict
(transcription and regulation) any events or partially
predicted (binding and +/- regulation) event triggers
and arguments. In the text examples that follow, trig-
gers are underlined and arguments are italicized.
The following are transcription events from the
document PMC1804205-02-Results-03 in the devel-
opment data.
? In contrast to the phenotype of the pta ackA
double mutant, pbgP transcription was reduced
in the pmrD mutant (Fig. 3).
? Growth at pH 5.8 resulted in pmrD
transcript levels that were approximately3.5-
fold higher than in organisms grown at pH 7.7
(Fig. 4A).
In both the development and test data evaluations,
our system did not predict any transcription events,
resulting in a 0.0 fscore; however, the system
achieved 75.41 fscore when the gold-standard trig-
gers were provided to the evaluation. Because ar-
gument prediction performed well, the system will
benefit most by improving transcription event trig-
ger prediction.
The following are regulation events from the doc-
ument PMC1804205-02-Results-01in the develop-
ment data.
? . . . we grew Salmonella cells harbouring chro-
mosomal lacZYA transcriptional fusions to the
PmrA-regulated genes pbgP, pmrC and ugd
(Wosten and Groisman, 1999) in N-minimal
media buffered at pH 5.8 or 7.7.
? We determined that Chelex 100 was effective at
chelating iron because expression of the pmrA-
independent iron-repressed iroA gene . . .
Similar to the transcription task, our system did not
predict any regulation events, resulting in a 0.0 fs-
core. Unlike transcription events though, our sys-
tem performed poorly on both argument identifica-
tion and trigger prediction. The system achieved a
28.17 fscore when gold-standard triggers were used
134
Event Class gold (match) answer (match) recall prec. fscore
Gene expression 152 80 148 80 52.63 54.05 53.33
Transcription 50 0 0 0 0.00 0.00 0.00
Protein catabolism 5 1 12 1 20.00 8.33 11.76
Phosphorylation 16 10 12 10 62.50 83.33 71.43
Localization 7 4 22 4 57.14 18.18 27.59
Binding 56 7 14 7 12.50 50.00 20.00
Regulation 193 0 0 0 0.00 0.00 0.00
Positive regulation 193 34 87 34 17.62 39.08 24.29
Negative regulation 181 32 68 32 17.68 47.06 25.70
Process 516 234 401 234 45.35 58.35 51.04
TOTAL 1369 402 764 402 29.36 52.62 37.69
Table 5: Official results on Infectious Disease test data
in the evaluation. Hypotheses for poor performance
on candidate argument prediction are addressed in
the following sections.
We posit that false negative trigger identifications
are due to the limited full text training data (i.e. tran-
scription events) and the inability of our system to
predict non-verb triggers (i.e. second transcription
example above). The SVM classifier was unable
to distinguish between true transcription event trig-
gers and transcription-related terms and ultimately,
did not predict any transcription event in the devel-
opment or test evaluations. To improve transcrip-
tion event prediction, immediate effort should fo-
cus on 1) providing additional training data (e.g.,
BioCreativec?iteBioCreative) and 2) introduce a trig-
ger word filter that defines a subset of event triggers
that have the best hit rate in the corpus. The hit rate
is the number of occurrences of the word in a sen-
tence per event type, divided by the total count in the
gold standard (Nguyen et al, 2010).
3.3 +/-Regulation and Binding
The following positive regulation event is from doc-
ument PMC1874608-03-RESULTS-03 in the devel-
opment data.
? Invasiveness for HEp-2 cells was reduced to
39.1% of the wild-type level by mlc mutation,
whereas it was increased by 1.57-fold by hilE
mutation (Figure 3B).
In the preceding example, our system correctly
predicted the +regulation trigger and the theme hilE;
however, the correct argument was a gene expres-
sion event, not the entity. Many errors in the positive
and negative regulation events were of this type; the
predicted argument was a theme and not an event.
Evaluation of our system?s binding event predic-
tions resulted in low recall (12.50 or 0.0) in the
test and development evaluations. The proceeding
binding events are from document PMC1874608-
03-RESULTS-05 in the development data. In both
of the examples, our system correctly predicted the
trigger binding; however, no arguments were pre-
dicted. Evaluation on the development data with
gold standard triggers also resulted in an fscore of
0.0; thus, further algorithm refinement is needed to
improve binding scores.
? Mlc directly represses hilE by binding to the P3
promoter
? These results clearly demonstrate that Mlc
can regulate directly the hilE P3 promoter by
binding to the promoter.
The following binding event is from document
PMC1874608-01-INTRODUCTION in the devel-
opment data and is representative of errors across
many of the tasks. Here, the trigger is correctly pre-
dicted; however, the candidate arguments did not
match with the reference data. Upon closer look,
the arguments were drawn from the entire sentence,
rather than an independent clause. The syntactic
parse feature was not sufficient to prevent over-
predicting arguments for the trigger, a potential so-
lution is to add the arguments syntactic dependency
135
to the trigger as a feature to the candidate argument
selection.
? Using two-hybrid analysis, it has been shown
that HilE interacts with HilD, which suggests
that HilE represses hilA expression by inhibit-
ing the activity of HilD through a protein-
protein interaction (19,20).
4 Summary
This article reports Pacific Northwest National Lab-
oratory?s entry to the BioNLP Shared Task 2011 In-
fectious Disease track competition. Our system uses
a signature-based machine-learning approach incor-
porating traditional linguistic features and shallow
semantic concepts from NIH?s METAMAP The-
saurus. We examine the contribution of each of
the linguistic and semantic features to the over-
all fscore for our system. This approach performs
well on gene expression, process and phosphoryla-
tion event prediction. Transcription, regulation and
binding events each achieve low fscores and war-
rant further research to improve their effectiveness.
Lastly, we present a performance analysis of the
transcription, regulation and binding tasks. Future
work to improve our system?s performance could in-
clude pre-processing using simple patterns (Nguyen
et al, 2010), information extraction from figure cap-
tions (Kim and Yu, 2011) and text-to-text event ex-
traction. The last suggested improvement is to add
semantic features to the candidate argument predic-
tion algorithm in addition to using rich features, such
as semantic roles (Torii et al, 2011).
Acknowledgements
The authors thank the Signature Discovery Initia-
tive, part of the Laboratory Directed Research and
Development Program at Pacific Northwest National
Laboratory (PNNL). PNNL is operated by Battelle
for the U.S. Department of Energy under contract
DE-ACO5-76RLO 1830.
References
Alan R Aronson and Franc?ois-Michel Lang. 2010. An
overview of metamap: historical perspective and re-
cent advances. J AmMed Inform Assoc, 17(3):229?36,
May.
J Bjorne, F Ginter, S Pyysalo, J Tsujii, and T Salakoski.
2010. Complex event extraction at pubmed scale.
Bioinformatics, 26(12):i382?i390, Jun.
O. Bodenreider. 2004. The unified medical language
system (UMLS): integrating biomedical terminology.
Nucleic acids research, 32(suppl 1):D267.
M.C. deMarneffe, B. MacCartney, and C.D. Manning.
2006. Generating typed dependency parses from
phrase structure parses. In Proceedings of LREC 2006.
D. Gildea and D. Jurafsky. 2002. Automatic label-
ing of semantic roles. Computational Linguistics,
28(3):245?288.
T. Joachims. 1999. Making large scale SVM learning
practical. Advances in Kernel Methods ? Support Vec-
tor Learning.
Daehyun Kim and Hong Yu. 2011. Figure text extraction
in biomedical literature. PLoS ONE, 6(1):e15338, Jan.
JD Kim, T Ohta, S Pyysalo, Y Kano, and J Tsujii. 2009.
Overview of bionlp?09 shared task on event extraction.
Proceedings of the Workshop on BioNLP: Shared Task,
pages 1?9.
Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert
Bossy, and Jun?ichi Tsujii. 2011. Overview of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
D. Klein and C.D. Manning. 2003. Accurate unlexical-
ized parsing. In Proceedings of the 41st Annual Meet-
ing on Association for Computational Linguistics-
Volume 1, pages 423?430. Association for Computa-
tional Linguistics.
A MacKinlay, D Martinez, and T Baldwin. 2009.
Biomedical event annotation with crfs and precision
grammars. Proceedings of the Workshop on BioNLP:
Shared Task, pages 77?85.
Makoto Miwa, Rune Saetre, Jin-Dong Kim, and Jun?ichi
Tsujii. 2010. Event extraction with complex event
classification using rich features. J. Bioinform. Com-
put. Biol., 8(1):131?46, Feb.
Quang Long Nguyen, Domonkos Tikk, and Ulf Leser.
2010. Simple tricks for improving pattern-based in-
formation extraction from the biomedical literature. J
Biomed Semantics, 1(1):9, Jan.
S. Pyysalo, T. Ohta, H.C. Cho, D. Sullivan, C. Mao,
B. Sobral, J. Tsujii, and S. Ananiadou. 2010. Towards
event extraction from full texts on infectious diseases.
In Proceedings of the 2010 Workshop on Biomedical
Natural Language Processing, pages 132?140. Asso-
ciation for Computational Linguistics.
Sampo Pyysalo, Tomoko Ohta, Rafal Rak, Dan Sul-
livan, Chunhong Mao, Chunxia Wang, Bruno So-
bral, Jun?ichi Tsujii, and Sophia Ananiadou. 2011.
136
Overview of the Infectious Diseases (ID) task of
BioNLP Shared Task 2011. In Proceedings of
the BioNLP 2011 Workshop Companion Volume for
Shared Task, Portland, Oregon, June. Association for
Computational Linguistics.
Pontus Stenetorp, Goran Topic?, Sampo Pyysalo, Tomoko
Ohta, Jin-Dong Kim, and Jun?ichi Tsujii. 2011.
BioNLP Shared Task 2011: Supporting Resources. In
Proceedings of the BioNLP 2011 Workshop Compan-
ion Volume for Shared Task, Portland, Oregon, June.
Association for Computational Linguistics.
Manabu Torii, Lanlan Yin, Thang Nguyen, Chand T
Mazumdar, Hongfang Liu, David M Hartley, and
Noele P Nelson. 2011. An exploratory study of a text
classification framework for internet-based surveil-
lance of emerging epidemics. International Journal
of Medical Informatics, 80(1):56?66, Jan.
C.J. van Rijsbergen, S.E. Robertson, and M.F. Porter.
1980. New models in probabilistic information re-
trieval.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer, New York.
137
