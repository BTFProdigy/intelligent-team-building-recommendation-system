Proceedings of ACL-08: HLT, pages 1039?1047,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Finding Contradictions in Text
Marie-Catherine de Marneffe,
Linguistics Department
Stanford University
Stanford, CA 94305
mcdm@stanford.edu
Anna N. Rafferty and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{rafferty,manning}@stanford.edu
Abstract
Detecting conflicting statements is a foun-
dational text understanding task with appli-
cations in information analysis. We pro-
pose an appropriate definition of contradiction
for NLP tasks and develop available corpora,
from which we construct a typology of con-
tradictions. We demonstrate that a system for
contradiction needs to make more fine-grained
distinctions than the common systems for en-
tailment. In particular, we argue for the cen-
trality of event coreference and therefore in-
corporate such a component based on topical-
ity. We present the first detailed breakdown
of performance on this task. Detecting some
types of contradiction requires deeper inferen-
tial paths than our system is capable of, but
we achieve good performance on types arising
from negation and antonymy.
1 Introduction
In this paper, we seek to understand the ways con-
tradictions occur across texts and describe a system
for automatically detecting such constructions. As a
foundational task in text understanding (Condoravdi
et al, 2003), contradiction detection has many possi-
ble applications. Consider applying a contradiction
detection system to political candidate debates: by
drawing attention to topics in which candidates have
conflicting positions, the system could enable voters
to make more informed choices between candidates
and sift through the amount of available informa-
tion. Contradiction detection could also be applied
to intelligence reports, demonstrating which infor-
mation may need further verification. In bioinfor-
matics where protein-protein interaction is widely
studied, automatically finding conflicting facts about
such interactions would be beneficial.
Here, we shed light on the complex picture of con-
tradiction in text. We provide a definition of contra-
diction suitable for NLP tasks, as well as a collec-
tion of contradiction corpora. Analyzing these data,
we find contradiction is a rare phenomenon that may
be created in different ways; we propose a typol-
ogy of contradiction classes and tabulate their fre-
quencies. Contradictions arise from relatively obvi-
ous features such as antonymy, negation, or numeric
mismatches. They also arise from complex differ-
ences in the structure of assertions, discrepancies
based on world-knowledge, and lexical contrasts.
(1) Police specializing in explosives defused the rock-
ets. Some 100 people were working inside the plant.
(2) 100 people were injured.
This pair is contradictory: defused rockets cannot go
off, and thus cannot injure anyone. Detecting con-
tradictions appears to be a harder task than detecting
entailments. Here, it is relatively easy to identify the
lack of entailment: the first sentence involves no in-
juries, so the second is unlikely to be entailed. Most
entailment systems function as weak proof theory
(Hickl et al, 2006; MacCartney et al, 2006; Zan-
zotto et al, 2007), but contradictions require deeper
inferences and model building. While mismatch-
ing information between sentences is often a good
cue of non-entailment (Vanderwende et al, 2006),
it is not sufficient for contradiction detection which
requires more precise comprehension of the conse-
quences of sentences. Assessing event coreference
is also essential: for texts to contradict, they must
1039
refer to the same event. The importance of event
coreference was recognized in the MUC information
extraction tasks in which it was key to identify sce-
narios related to the same event (Humphreys et al,
1997). Recent work in text understanding has not
focused on this issue, but it must be tackled in a suc-
cessful contradiction system. Our system includes
event coreference, and we present the first detailed
examination of contradiction detection performance,
on the basis of our typology.
2 Related work
Little work has been done on contradiction detec-
tion. The PASCAL Recognizing Textual Entailment
(RTE) Challenges (Dagan et al, 2006; Bar-Haim
et al, 2006; Giampiccolo et al, 2007) focused on
textual inference in any domain. Condoravdi et al
(2003) first recognized the importance of handling
entailment and contradiction for text understanding,
but they rely on a strict logical definition of these
phenomena and do not report empirical results. To
our knowledge, Harabagiu et al (2006) provide the
first empirical results for contradiction detection, but
they focus on specific kinds of contradiction: those
featuring negation and those formed by paraphrases.
They constructed two corpora for evaluating their
system. One was created by overtly negating each
entailment in the RTE2 data, producing a bal-
anced dataset (LCC negation). To avoid overtrain-
ing, negative markers were also added to each non-
entailment, ensuring that they did not create con-
tradictions. The other was produced by paraphras-
ing the hypothesis sentences from LCC negation, re-
moving the negation (LCC paraphrase): A hunger
strike was not attempted ? A hunger strike was
called off. They achieved very good performance:
accuracies of 75.63% on LCC negation and 62.55%
on LCC paraphrase. Yet, contradictions are not lim-
ited to these constructions; to be practically useful,
any system must provide broader coverage.
3 Contradictions
3.1 What is a contradiction?
One standard is to adopt a strict logical definition of
contradiction: sentences A and B are contradictory
if there is no possible world in which A and B are
both true. However, for contradiction detection to be
useful, a looser definition that more closely matches
human intuitions is necessary; contradiction occurs
when two sentences are extremely unlikely to be true
simultaneously. Pairs such as Sally sold a boat to
John and John sold a boat to Sally are tagged as con-
tradictory even though it could be that each sold a
boat to the other. This definition captures intuitions
of incompatiblity, and perfectly fits applications that
seek to highlight discrepancies in descriptions of the
same event. Examples of contradiction are given in
table 1. For texts to be contradictory, they must in-
volve the same event. Two phenomena must be con-
sidered in this determination: implied coreference
and embedded texts. Given limited context, whether
two entities are coreferent may be probable rather
than certain. To match human intuitions, compatible
noun phrases between sentences are assumed to be
coreferent in the absence of clear countervailing ev-
idence. In the following example, it is not necessary
that the woman in the first and second sentences is
the same, but one would likely assume it is if the two
sentences appeared together:
(1) Passions surrounding Germany?s final match turned
violent when a woman stabbed her partner because
she didn?t want to watch the game.
(2) A woman passionately wanted to watch the game.
We also mark as contradictions pairs reporting con-
tradictory statements. The following sentences refer
to the same event (de Menezes in a subway station),
and display incompatible views of this event:
(1) Eyewitnesses said de Menezes had jumped over the
turnstile at Stockwell subway station.
(2) The documents leaked to ITV News suggest that
Menezes walked casually into the subway station.
This example contains an ?embedded contradic-
tion.? Contrary to Zaenen et al (2005), we argue
that recognizing embedded contradictions is impor-
tant for the application of a contradiction detection
system: if John thinks that he is incompetent, and his
boss believes that John is not being given a chance,
one would like to detect that the targeted information
in the two sentences is contradictory, even though
the two sentences can be true simultaneously.
3.2 Typology of contradictions
Contradictions may arise from a number of different
constructions, some overt and others that are com-
1040
ID Type Text Hypothesis
1 Antonym Capital punishment is a catalyst for more crime. Capital punishment is a deterrent to
crime.
2 Negation A closely divided Supreme Court said that juries and
not judges must impose a death sentence.
The Supreme Court decided that only
judges can impose the death sentence.
3 Numeric The tragedy of the explosion in Qana that killed more
than 50 civilians has presented Israel with a dilemma.
An investigation into the strike in Qana
found 28 confirmed dead thus far.
4 Factive Prime Minister John Howard says he will not be
swayed by a warning that Australia faces more terror-
ism attacks unless it withdraws its troops from Iraq.
Australia withdraws from Iraq.
5 Factive The bombers had not managed to enter the embassy. The bombers entered the embassy.
6 Structure Jacques Santer succeeded Jacques Delors as president
of the European Commission in 1995.
Delors succeeded Santer in the presi-
dency of the European Commission.
7 Structure The Channel Tunnel stretches from England to
France. It is the second-longest rail tunnel in the
world, the longest being a tunnel in Japan.
The Channel Tunnel connects France
and Japan.
8 Lexical The Canadian parliament?s Ethics Commission said
former immigration minister, Judy Sgro, did nothing
wrong and her staff had put her into a conflict of in-
terest.
The Canadian parliament?s Ethics
Commission accuses Judy Sgro.
9 Lexical In the election, Bush called for U.S. troops to be with-
drawn from the peacekeeping mission in the Balkans.
He cites such missions as an example of
how America must ?stay the course.?
10 WK Microsoft Israel, one of the first Microsoft branches
outside the USA, was founded in 1989.
Microsoft was established in 1989.
Table 1: Examples of contradiction types.
plex even for humans to detect. Analyzing contra-
diction corpora (see section 3.3), we find two pri-
mary categories of contradiction: (1) those occur-
ring via antonymy, negation, and date/number mis-
match, which are relatively simple to detect, and
(2) contradictions arising from the use of factive or
modal words, structural and subtle lexical contrasts,
as well as world knowledge (WK).
We consider contradictions in category (1) ?easy?
because they can often be automatically detected
without full sentence comprehension. For exam-
ple, if words in the two passages are antonyms and
the sentences are reasonably similar, especially in
polarity, a contradiction occurs. Additionally, little
external information is needed to gain broad cover-
age of antonymy, negation, and numeric mismatch
contradictions; each involves only a closed set of
words or data that can be obtained using existing
resources and techniques (e.g., WordNet (Fellbaum,
1998), VerbOcean (Chklovski and Pantel, 2004)).
However, contradictions in category (2) are more
difficult to detect automatically because they require
precise models of sentence meaning. For instance,
to find the contradiction in example 8 (table 1),
it is necessary to learn that X said Y did nothing
wrong and X accuses Y are incompatible. Presently,
there exist methods for learning oppositional terms
(Marcu and Echihabi, 2002) and paraphrase learn-
ing has been thoroughly studied, but successfully
extending these techniques to learn incompatible
phrases poses difficulties because of the data dis-
tribution. Example 9 provides an even more dif-
ficult instance of contradiction created by a lexical
discrepancy. Structural issues also create contradic-
tions (examples 6 and 7). Lexical complexities and
variations in the function of arguments across verbs
can make recognizing these contradictions compli-
cated. Even when similar verbs are used and ar-
gument differences exist, structural differences may
indicate non-entailment or contradiction, and distin-
guishing the two automatically is problematic. Con-
sider contradiction 7 in table 1 and the following
non-contradiction:
(1) The CFAP purchases food stamps from the govern-
ment and distributes them to eligible recipients.
(2) A government purchases food.
1041
Data # contradictions # total pairs
RTE1 dev1 48 287
RTE1 dev2 55 280
RTE1 test 149 800
RTE2 dev 111 800
RTE3 dev 80 800
RTE3 test 72 800
Table 2: Number of contradictions in the RTE datasets.
In both cases, the first sentence discusses one en-
tity (CFAP, The Channel Tunnel) with a relationship
(purchase, stretch) to other entities. The second sen-
tence posits a similar relationship that includes one
of the entities involved in the original relationship
as well as an entity that was not involved. However,
different outcomes result because a tunnel connects
only two unique locations whereas more than one
entity may purchase food. These frequent interac-
tions between world-knowledge and structure make
it hard to ensure that any particular instance of struc-
tural mismatch is a contradiction.
3.3 Contradiction corpora
Following the guidelines above, we annotated the
RTE datasets for contradiction. These datasets con-
tain pairs consisting of a short text and a one-
sentence hypothesis. Table 2 gives the number of
contradictions in each dataset. The RTE datasets are
balanced between entailments and non-entailments,
and even in these datasets targeting inference, there
are few contradictions. Using our guidelines,
RTE3 test was annotated by NIST as part of the
RTE3 Pilot task in which systems made a 3-way de-
cision as to whether pairs of sentences were entailed,
contradictory, or neither (Voorhees, 2008).1
Our annotations and those of NIST were per-
formed on the original RTE datasets, contrary to
Harabagiu et al (2006). Because their corpora are
constructed using negation and paraphrase, they are
unlikely to cover all types of contradictions in sec-
tion 3.2. We might hypothesize that rewriting ex-
plicit negations commonly occurs via the substitu-
tion of antonyms. Imagine, e.g.:
H: Bill has finished his math.
1Information about this task as well as data can be found at
http://nlp.stanford.edu/RTE3-pilot/.
Type RTE sets ?Real? corpus
1 Antonym 15.0 9.2
Negation 8.8 17.6
Numeric 8.8 29.0
2 Factive/Modal 5.0 6.9
Structure 16.3 3.1
Lexical 18.8 21.4
WK 27.5 13.0
Table 3: Percentages of contradiction types in the
RTE3 dev dataset and the real contradiction corpus.
Neg-H: Bill hasn?t finished his math.
Para-Neg-H: Bill is still working on his math.
The rewriting in both the negated and the para-
phrased corpora is likely to leave one in the space of
?easy? contradictions and addresses fewer than 30%
of contradictions (table 3). We contacted the LCC
authors to obtain their datasets, but they were unable
to make them available to us. Thus, we simulated the
LCC negation corpus, adding negative markers to
the RTE2 test data (Neg test), and to a development
set (Neg dev) constructed by randomly sampling 50
pairs of entailments and 50 pairs of non-entailments
from the RTE2 development set.
Since the RTE datasets were constructed for tex-
tual inference, these corpora do not reflect ?real-life?
contradictions. We therefore collected contradic-
tions ?in the wild.? The resulting corpus contains
131 contradictory pairs: 19 from newswire, mainly
looking at related articles in Google News, 51 from
Wikipedia, 10 from the Lexis Nexis database, and
51 from the data prepared by LDC for the distillation
task of the DARPA GALE program. Despite the ran-
domness of the collection, we argue that this corpus
best reflects naturally occurring contradictions.2
Table 3 gives the distribution of contradiction
types for RTE3 dev and the real contradiction cor-
pus. Globally, we see that contradictions in category
(2) occur frequently and dominate the RTE develop-
ment set. In the real contradiction corpus, there is a
much higher rate of the negation, numeric and lex-
ical contradictions. This supports the intuition that
in the real world, contradictions primarily occur for
two reasons: information is updated as knowledge
2Our corpora?the simulation of the LLC negation corpus,
the RTE datasets and the real contradictions?are available at
http://nlp.stanford.edu/projects/contradiction.
1042
of an event is acquired over time (e.g., a rising death
toll) or various parties have divergent views of an
event (e.g., example 9 in table 1).
4 System overview
Our system is based on the stage architecture of the
Stanford RTE system (MacCartney et al, 2006), but
adds a stage for event coreference decision.
4.1 Linguistic analysis
The first stage computes linguistic representations
containing information about the semantic content
of the passages. The text and hypothesis are con-
verted to typed dependency graphs produced by
the Stanford parser (Klein and Manning, 2003; de
Marneffe et al, 2006). To improve the dependency
graph as a pseudo-semantic representation, colloca-
tions in WordNet and named entities are collapsed,
causing entities and multiword relations to become
single nodes.
4.2 Alignment between graphs
The second stage provides an alignment between
text and hypothesis graphs, consisting of a mapping
from each node in the hypothesis to a unique node
in the text or to null. The scoring measure uses
node similarity (irrespective of polarity) and struc-
tural information based on the dependency graphs.
Similarity measures and structural information are
combined via weights learned using the passive-
aggressive online learning algorithm MIRA (Cram-
mer and Singer, 2001). Alignment weights were
learned using manually annotated RTE development
sets (see Chambers et al, 2007).
4.3 Filtering non-coreferent events
Contradiction features are extracted based on mis-
matches between the text and hypothesis. Therefore,
we must first remove pairs of sentences which do not
describe the same event, and thus cannot be contra-
dictory to one another. In the following example, it
is necessary to recognize that Pluto?s moon is not the
same as the moon Titan; otherwise conflicting diam-
eters result in labeling the pair a contradiction.
T: Pluto?s moon, which is only about 25 miles in di-
ameter, was photographed 13 years ago.
H: The moon Titan has a diameter of 5100 kms.
This issue does not arise for textual entailment: el-
ements in the hypothesis not supported by the text
lead to non-entailment, regardless of whether the
same event is described. For contradiction, however,
it is critical to filter unrelated sentences to avoid
finding false evidence of contradiction when there
is contrasting information about different events.
Given the structure of RTE data, in which the
hypotheses are shorter and simpler than the texts,
one straightforward strategy for detecting coreferent
events is to check whether the root of the hypothesis
graph is aligned in the text graph. However, some
RTE hypotheses are testing systems? abilities to de-
tect relations between entities (e.g., John of IBM . . .
? John works for IBM). Thus, we do not filter verb
roots that are indicative of such relations. As shown
in table 4, this strategy improves results on RTE
data. For real world data, however, the assumption
of directionality made in this strategy is unfounded,
and we cannot assume that one sentence will be
short and the other more complex. Assuming two
sentences of comparable complexity, we hypothe-
size that modeling topicality could be used to assess
whether the sentences describe the same event.
There is a continuum of topicality from the start to
the end of a sentence (Firbas, 1971). We thus orig-
inally defined the topicality of an NP by nw where
n is the nth NP in the sentence. Additionally, we
accounted for multiple clauses by weighting each
clause equally; in example 4 in table 1, Australia
receives the same weight as Prime Minister because
each begins a clause. However, this weighting was
not supported empirically, and we thus use a sim-
pler, unweighted model. The topicality score of a
sentence is calculated as a normalized score across
all aligned NPs.3 The text and hypothesis are topi-
cally related if either sentence score is above a tuned
threshold. Modeling topicality provides an addi-
tional improvement in precision (table 4).
While filtering provides improvements in perfor-
mance, some examples of non-coreferent events are
still not filtered, such as:
T: Also Friday, five Iraqi soldiers were killed and nine
3Since dates can often be viewed as scene setting rather than
what the sentence is about, we ignore these in the model. How-
ever, ignoring or including dates in the model creates no signif-
icant differences in performance on RTE data.
1043
Strategy Precision Recall
No filter 55.10 32.93
Root 61.36 32.93
Root + topic 61.90 31.71
Table 4: Precision and recall for contradiction detection
on RTE3 dev using different filtering strategies.
wounded in a bombing, targeting their convoy near
Beiji, 150 miles north of Baghdad.
H: Three Iraqi soldiers also died Saturday when their
convoy was attacked by gunmen near Adhaim.
It seems that the real world frequency of events
needs to be taken into account. In this case, attacks
in Iraq are unfortunately frequent enough to assert
that it is unlikely that the two sentences present mis-
matching information (i.e., different location) about
the same event. But compare the following example:
T: President Kennedy was assassinated in Texas.
H: Kennedy?s murder occurred in Washington.
The two sentences refer to one unique event, and the
location mismatch renders them contradictory.
4.4 Extraction of contradiction features
In the final stage, we extract contradiction features
on which we apply logistic regression to classify the
pair as contradictory or not. The feature weights are
hand-set, guided by linguistic intuition.
5 Features for contradiction detection
In this section, we define each of the feature sets
used to capture salient patterns of contradiction.
Polarity features. Polarity difference between the
text and hypothesis is often a good indicator of con-
tradiction, provided there is a good alignment (see
example 2 in table 1). The polarity features cap-
ture the presence (or absence) of linguistic mark-
ers of negative polarity contexts. These markers are
scoped such that words are considered negated if
they have a negation dependency in the graph or are
an explicit linguistic marker of negation (e.g., sim-
ple negation (not), downward-monotone quantifiers
(no, few), or restricting prepositions). If one word is
negated and the other is not, we may have a polarity
difference. This difference is confirmed by checking
that the words are not antonyms and that they lack
unaligned prepositions or other context that suggests
they do not refer to the same thing. In some cases,
negations are propagated onto the governor, which
allows one to see that no bullet penetrated and a bul-
let did not penetrate have the same polarity.
Number, date and time features. Numeric mis-
matches can indicate contradiction (example 3
in table 1). The numeric features recognize
(mis-)matches between numbers, dates, and times.
We normalize date and time expressions, and rep-
resent numbers as ranges. This includes expression
matching (e.g., over 100 and 200 is not a mismatch).
Aligned numbers are marked as mismatches when
they are incompatible and surrounding words match
well, indicating the numbers refer to the same entity.
Antonymy features. Aligned antonyms are a very
good cue for contradiction. Our list of antonyms
and contrasting words comes from WordNet, from
which we extract words with direct antonymy links
and expand the list by adding words from the same
synset as the antonyms. We also use oppositional
verbs from VerbOcean. We check whether an
aligned pair of words appears in the list, as well as
checking for common antonym prefixes (e.g., anti,
un). The polarity of the context is used to determine
if the antonyms create a contradiction.
Structural features. These features aim to deter-
mine whether the syntactic structures of the text and
hypothesis create contradictory statements. For ex-
ample, we compare the subjects and objects for each
aligned verb. If the subject in the text overlaps with
the object in the hypothesis, we find evidence for a
contradiction. Consider example 6 in table 1. In the
text, the subject of succeed is Jacques Santer while
in the hypothesis, Santer is the object of succeed,
suggesting that the two sentences are incompatible.
Factivity features. The context in which a verb
phrase is embedded may give rise to contradiction,
as in example 5 (table 1). Negation influences some
factivity patterns: Bill forgot to take his wallet con-
tradicts Bill took his wallet while Bill did not forget
to take his wallet does not contradict Bill took his
wallet. For each text/hypothesis pair, we check the
(grand)parent of the text word aligned to the hypoth-
esis verb, and generate a feature based on its factiv-
1044
ity class. Factivity classes are formed by clustering
our expansion of the PARC lists of factive, implica-
tive and non-factive verbs (Nairn et al, 2006) ac-
cording to how they create contradiction.
Modality features. Simple patterns of modal rea-
soning are captured by mapping the text and hy-
pothesis to one of six modalities ((not )possible,
(not )actual, (not )necessary), according to the
presence of predefined modality markers such as
can or maybe. A feature is produced if the
text/hypothesis modality pair gives rise to a con-
tradiction. For instance, the following pair will
be mapped to the contradiction judgment (possible,
not possible):
T: The trial court may allow the prevailing party rea-
sonable attorney fees as part of costs.
H: The prevailing party may not recover attorney fees.
Relational features. A large proportion of the
RTE data is derived from information extraction
tasks where the hypothesis captures a relation be-
tween elements in the text. Using Semgrex, a pat-
tern matching language for dependency graphs, we
find such relations and ensure that the arguments be-
tween the text and the hypothesis match. In the fol-
lowing example, we detect that Fernandez works for
FEMA, and that because of the negation, a contra-
diction arises.
T: Fernandez, of FEMA, was on scene when Martin
arrived at a FEMA base camp.
H: Fernandez doesn?t work for FEMA.
Relational features provide accurate information but
are difficult to extend for broad coverage.
6 Results
Our contradiction detection system was developed
on all datasets listed in the first part of table 5. As
test sets, we used RTE1 test, the independently an-
notated RTE3 test, and Neg test. We focused on at-
taining high precision. In a real world setting, it is
likely that the contradiction rate is extremely low;
rather than overwhelming true positives with false
positives, rendering the system impractical, we mark
contradictions conservatively. We found reasonable
inter-annotator agreement between NIST and our
post-hoc annotation of RTE3 test (? = 0.81), show-
ing that, even with limited context, humans tend to
Precision Recall Accuracy
RTE1 dev1 70.37 40.43 ?
RTE1 dev2 72.41 38.18 ?
RTE2 dev 64.00 28.83 ?
RTE3 dev 61.90 31.71 ?
Neg dev 74.07 78.43 75.49
Neg test 62.97 62.50 62.74
LCC negation ? ? 75.63
RTE1 test 42.22 26.21 ?
RTE3 test 22.95 19.44 ?
Avg. RTE3 test 10.72 11.69 ?
Table 5: Precision and recall figures for contradiction de-
tection. Accuracy is given for balanced datasets only.
?LCC negation? refers to performance of Harabagiu et al
(2006); ?Avg. RTE3 test? refers to mean performance of
the 12 submissions to the RTE3 Pilot.
agree on contradictions.4 The results on the test sets
show that performance drops on new data, highlight-
ing the difficulty in generalizing from a small corpus
of positive contradiction examples, as well as under-
lining the complexity of building a broad coverage
system. This drop in accuracy on the test sets is
greater than that of many RTE systems, suggesting
that generalizing for contradiction is more difficult
than for entailment. Particularly when addressing
contradictions that require lexical and world knowl-
edge, we are only able to add coverage in a piece-
meal fashion, resulting in improved performance on
the development sets but only small gains for the
test sets. Thus, as shown in table 6, we achieve
13.3% recall on lexical contradictions in RTE3 dev
but are unable to identify any such contradictions in
RTE3 test. Additionally, we found that the preci-
sion of category (2) features was less than that of
category (1) features. Structural features, for exam-
ple, caused us to tag 36 non-contradictions as con-
tradictions in RTE3 test, over 75% of the precision
errors. Despite these issues, we achieve much higher
precision and recall than the average submission to
the RTE3 Pilot task on detecting contradictions, as
shown in the last two lines of table 5.
4This stands in contrast with the low inter-annotator agree-
ment reported by Sanchez-Graillet and Poesio (2007) for con-
tradictions in protein-protein interactions. The only hypothesis
we have to explain this contrast is the difficulty of scientific ma-
terial.
1045
Type RTE3 dev RTE3 test
1 Antonym 25.0 (3/12) 42.9 (3/7)
Negation 71.4 (5/7) 60.0 (3/5)
Numeric 71.4 (5/7) 28.6 (2/7)
2 Factive/Modal 25.0 (1/4) 10.0 (1/10)
Structure 46.2 (6/13) 21.1 (4/19)
Lexical 13.3 (2/15) 0.0 (0/12)
WK 18.2 (4/22) 8.3 (1/12)
Table 6: Recall by contradiction type.
7 Error analysis and discussion
One significant issue in contradiction detection is
lack of feature generalization. This problem is es-
pecially apparent for items in category (2) requiring
lexical and world knowledge, which proved to be
the most difficult contradictions to detect on a broad
scale. While we are able to find certain specific re-
lationships in the development sets, these features
attained only limited coverage. Many contradictions
in this category require multiple inferences and re-
main beyond our capabilities:
T: The Auburn High School Athletic Hall of Fame re-
cently introduced its Class of 2005 which includes
10 members.
H: The Auburn High School Athletic Hall of Fame has
ten members.
Of the types of contradictions in category (2), we are
best at addressing those formed via structural differ-
ences and factive/modal constructions as shown in
table 6. For instance, we detect examples 5 and 6 in
table 1. However, creating features with sufficient
precision is an issue for these types of contradic-
tions. Intuitively, two sentences that have aligned
verbs with the same subject and different objects (or
vice versa) are contradictory. This indeed indicates
a contradiction 55% of the time on our development
sets, but this is not high enough precision given the
rarity of contradictions.
Another type of contradiction where precision fal-
ters is numeric mismatch. We obtain high recall for
this type (table 6), as it is relatively simple to deter-
mine if two numbers are compatible, but high preci-
sion is difficult to achieve due to differences in what
numbers may mean. Consider:
T: Nike Inc. said that its profit grew 32 percent, as the
company posted broad gains in sales and orders.
H: Nike said orders for footwear totaled $4.9 billion,
including a 12 percent increase in U.S. orders.
Our system detects a mismatch between 32 percent
and 12 percent, ignoring the fact that one refers to
profit and the other to orders. Accounting for con-
text requires extensive text comprehension; it is not
enough to simply look at whether the two numbers
are headed by similar words (grew and increase).
This emphasizes the fact that mismatching informa-
tion is not sufficient to indicate contradiction.
As demonstrated by our 63% accuracy on
Neg test, we are reasonably good at detecting nega-
tion and correctly ascertaining whether it is a symp-
tom of contradiction. Similarly, we handle single
word antonymy with high precision (78.9%). Never-
theless, Harabagiu et al?s performance demonstrates
that further improvement on these types is possible;
indeed, they use more sophisticated techniques to
extract oppositional terms and detect polarity differ-
ences. Thus, detecting category (1) contradictions is
feasible with current systems.
While these contradictions are only a third of
those in the RTE datasets, detecting such contra-
dictions accurately would solve half of the prob-
lems found in the real corpus. This suggests that
we may be able to gain sufficient traction on contra-
diction detection for real world applications. Even
so, category (2) contradictions must be targeted to
detect many of the most interesting examples and to
solve the entire problem of contradiction detection.
Some types of these contradictions, such as lexi-
cal and world knowledge, are currently beyond our
grasp, but we have demonstrated that progress may
be made on the structure and factive/modal types.
Despite being rare, contradiction is foundational
in text comprehension. Our detailed investigation
demonstrates which aspects of it can be resolved and
where further research must be directed.
Acknowledgments
This paper is based on work funded in part by
the Defense Advanced Research Projects Agency
through IBM and by the Disruptive Technology
Office (DTO) Phase III Program for Advanced
Question Answering for Intelligence (AQUAINT)
through Broad Agency Announcement (BAA)
N61339-06-R-0034.
1046
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second PASCAL recognising textual en-
tailment challenge. In Proceedings of the Second
PASCAL Challenges Workshop on Recognising Tex-
tual Entailment, Venice, Italy.
Nathanael Chambers, Daniel Cer, Trond Grenager,
David Hall, Chloe Kiddon, Bill MacCartney, Marie-
Catherine de Marneffe, Daniel Ramage, Eric Yeh, and
Christopher D. Manning. 2007. Learning alignments
and leveraging natural logic. In Proceedings of the
ACL-PASCAL Workshop on Textual Entailment and
Paraphrasing.
Timothy Chklovski and Patrick Pantel. 2004. Verbo-
cean: Mining the web for fine-grained semantic verb
relations. In Proceedings of EMNLP-04.
Cleo Condoravdi, Dick Crouch, Valeria de Pavia, Rein-
hard Stolle, and Daniel G. Bobrow. 2003. Entailment,
intensionality and text understanding. Workshop on
Text Meaning (2003 May 31).
Koby Crammer and Yoram Singer. 2001. Ultraconser-
vative online algorithms for multiclass problems. In
Proceedings of COLT-2001.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL recognising textual entailment
challenge. In Quinonero-Candela et al, editor, MLCW
2005, LNAI Volume 3944, pages 177?190. Springer-
Verlag.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of the 5th International Conference on Lan-
guage Resources and Evaluation (LREC-06).
Christiane Fellbaum. 1998. WordNet: an electronic lexi-
cal database. MIT Press.
Jan Firbas. 1971. On the concept of communicative dy-
namism in the theory of functional sentence perspec-
tive. Brno Studies in English, 7:23?47.
Danilo Giampiccolo, Ido Dagan, Bernardo Magnini, and
Bill Dolan. 2007. The third PASCAL recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing.
Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.
2006. Negation, contrast, and contradiction in text
processing. In Proceedings of the Twenty-First Na-
tional Conference on Artificial Intelligence (AAAI-06).
Andrew Hickl, John Williams, Jeremy Bensley, Kirk
Roberts, Bryan Rink, and Ying Shi. 2006. Recog-
nizing textual entailment with LCC?s GROUNDHOG
system. In Proceedings of the Second PASCAL Chal-
lenges Workshop on Recognising Textual Entailment.
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information extrac-
tion. In Proceedings of the Workshop on Operational
Factors in Pratical, Robust Anaphora Resolution for
Unrestricted Texts, 35th ACL meeting.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Annual Meeting of the Association of Computational
Linguistics.
Bill MacCartney, Trond Grenager, Marie-Catherine de
Marneffe, Daniel Cer, and Christopher D. Manning.
2006. Learning to recognize features of valid textual
entailments. In Proceedings of the North American
Association of Computational Linguistics (NAACL-
06).
Daniel Marcu and Abdessamad Echihabi. 2002. An
unsupervised approach to recognizing discourse rela-
tions. In Proceedings of the 40th Annual Meeting of
the Association for Computational Linguistics.
Rowan Nairn, Cleo Condoravdi, and Lauri Karttunen.
2006. Computing relative polarity for textual infer-
ence. In Proceedings of ICoS-5.
Olivia Sanchez-Graillet and Massimo Poesio. 2007. Dis-
covering contradiction protein-protein interactions in
text. In Proceedings of BioNLP 2007: Biological,
translational, and clinical language processing.
Lucy Vanderwende, Arul Menezes, and Rion Snow.
2006. Microsoft research at rte-2: Syntactic contri-
butions in the entailment task: an implementation. In
Proceedings of the Second PASCAL Challenges Work-
shop on Recognising Textual Entailment.
Ellen Voorhees. 2008. Contradictions and justifications:
Extensions to the textual entailment task. In Proceed-
ings of the 46th Annual Meeting of the Association for
Computational Linguistics.
Annie Zaenen, Lauri Karttunen, and Richard S. Crouch.
2005. Local textual inference: can it be defined or
circumscribed? In ACL 2005 Workshop on Empirical
Modeling of Semantic Equivalence and Entailment.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Alessandro Moschitti. 2007. Shallow semantics in
fast textual entailment rule learners. In Proceedings
of the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing.
1047
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 40?46,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Parsing Three German Treebanks: Lexicalized and Unlexicalized Baselines
Anna N. Rafferty and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{rafferty,manning}@stanford.edu
Abstract
Previous work on German parsing has pro-
vided confusing and conflicting results con-
cerning the difficulty of the task and whether
techniques that are useful for English, such
as lexicalization, are effective for German.
This paper aims to provide some understand-
ing and solid baseline numbers for the task.
We examine the performance of three tech-
niques on three treebanks (Negra, Tiger, and
Tu?Ba-D/Z): (i) Markovization, (ii) lexicaliza-
tion, and (iii) state splitting. We additionally
explore parsing with the inclusion of gram-
matical function information. Explicit gram-
matical functions are important to German
language understanding, but they are numer-
ous, and na??vely incorporating them into a
parser which assumes a small phrasal category
inventory causes large performance reductions
due to increasing sparsity.
1 Introduction
Recent papers provide mixed evidence as to whether
techniques that increase statistical parsing perfor-
mance for English also improve German parsing
performance (Dubey and Keller, 2003; Ku?bler et al,
2006). We provide a systematic exploration of this
topic to shed light on what techniques might bene-
fit German parsing and show general trends in the
relative performance increases for each technique.
While these results vary across treebanks, due to
differences in annotation schemes as discussed by
Ku?bler (2005), we also find similarities and provide
explanations for the trend differences based on the
annotation schemes.
We address three parsing techniques:
(i) Markovization, (ii) lexicalization, and (iii) state
splitting (i.e., subcategorization). These techniques
are not independent, and we thus examine how
lexicalization and Markovization interact, since
lexicalization for German has been the most
contentious area in the literature. Many of these
techniques have been investigated in other work
(Schiehlen, 2004; Dubey, 2004; Dubey, 2005),
but, we hope that by consolidating, replicating,
improving, and clarifying previous results we can
contribute to the re-evaluation of German proba-
bilistic parsing after a somewhat confusing start to
initial literature in this area.
One feature of German that differs markedly from
English is substantial free word order. This requires
the marking of grammatical functions on phrases to
indicate their syntactic function in sentences (sub-
ject, object, etc.), whereas for English these func-
tions can be derived from configurations (Chomsky,
1965; de Marneffe et al, 2006). While some simi-
lar functions are present in English treebanks, they
are used more frequently in German treebanks and
many more unique functions and category-function
pairings exist. Because of the relatively free word
ordering in German, the usefulness of parses is sub-
stantially increased by generating them with this in-
formation. We demonstrate the difficulties intro-
duced by na??vely concatenating these functions to
categories and how this treatment interacts with the
other parsing techniques. There are several avenues
for improving this situation in future work. The ver-
sions of the treebanks we use here do not include
case information in part-of-speech tags and we do
40
Treebank Train Dev ? 40 Test ? 40
Tiger 20894 2611 2535 2611 2525
Tu?Ba-D/Z 20894 2611 2611 2611 2611
Negra v2 18602 1000 975 1000 968
Table 1: Size in sentences of treebanks used in this paper.
?Tiger? and ?Tu?Ba-D/Z? refer to the corpora prepared for
the ACL-08 workshop shared task; the full Tiger corpus
is much larger. Our Negra results are on the test set.
not use any morphological analyzer; this should be
rectified in future work. A new parsing model could
be written to treat separate grammatical functions
for nodes as first class objects, rather than just con-
catenating phrasal categories and functions. Finally,
assignment of grammatical functions could be left
to a separate post-processing phase, which could ex-
ploit not only case information inside noun phrases
but joint information across the subcategorization
frames of predicates.
2 Methodology
We use the Stanford Parser (Klein and Manning,
2003b) for all experiments. An advantage of this
parser for baseline experiments is that it provides
clean, simple implementations of component mod-
els, with many configuration options. We show re-
sults in most instances for evaluations both with and
without grammatical functions and with and without
gold tags. When training and parsing with the inclu-
sion of grammatical functions, we treat each pair-
ing of basic category and grammatical function as
one new category. Rules are learned for each such
category with a separate orthographic form, with
no attempt to learn general rules for nodes with the
same basic category but different functions. Clearly,
more sophisticated methods of handling grammat-
ical functions exist, but our focus is on providing
baseline results that are easily replicable by others.
We focus primarily on the Tu?Ba-D/Z and Tiger
corpora, training on the training sets for the ACL
2008 Workshop on Parsing German shared task and
providing ablation results based on development set
performance. Additionally, we show a limited num-
ber of results on the Negra corpus, using the standard
training/development/test splits, defined in (Dubey
and Keller, 2003). The sizes of these data sets are
shown in table 1.
3 Markovization
Previous work has shown that adding vertical
Markovization ((grand-)parent annotation) and us-
ing horizontal Markovization can greatly improve
English parsing performance (Klein and Manning,
2003a). Several papers have already reported par-
tially corresponding results on German: Schiehlen
(2004) and Dubey (2004) reported gains of several
percent for unlexicalized parsing on Negra; Ku?bler
et al (2006) agreed with these results for Negra, but
suggests that they do not hold for Tu?Ba-D/Z. We ex-
tend these results by examining a variety of com-
binations of Markovization parameters for all three
corpora (Tu?Ba-D/Z, Tiger, and Negra) in table 2. No
results presented here do include grammatical func-
tions; we present results on the interaction between
these functions and Markovization in section 4.
For Tu?Ba-D/Z, we see that adding vertical
Markovization provides a substantial performance
gain of about 2% (vertical Markovization = 2) for
all levels of horizontal Markovization; increasing
vertical Markovization improves performance only
slightly further. Decreasing horizontal Markoviza-
tion from the default of infinity for a standard
PCFG also provides marginal gains, and decreases
the number of rules learned by the parser, cre-
ating a more compact grammar. The results of
Markovization on the Tiger and Negra corpora il-
lustrate the problems of a large grammar. While a
modest improvement is found by using parent anno-
tation (vertical Markovization = 2) when horizontal
Markovization is small, increasing either horizontal
or vertical Markovization past this point decreases
performance due to sparsity. Thus, while the gen-
eral results concerning Markovization from English
hold, the size of performance increase is affected ap-
preciably by the annotation strategy.
In table 3, we show a subset of the results of var-
ious Markovization parameters when gold part-of-
speech tags are used, focusing on models that per-
formed well without gold tags and that produce rel-
atively compact grammars. Gold tags provide 2?3%
absolute improvement in F1 over tagging while pars-
ing; slightly greater improvements are seen when the
PCFG model is used individually (3?4% absolute
improvement), and absolute improvement does not
vary greatly between treebanks. These results are
41
Tu?Ba-D/Z Tiger Negra
Horiz. Vertical Markov Order Vertical Markov Order Vertical Markov Order
Order 1 2 3 1 2 3 1 2 3
1 86.50 88.60 88.71 76.69 77.40 76.46 76.63 77.20 75.91
(+2.76) (+1.21) (+0.89) (+3.54) (+3.57) (+3.27) (+2.39) (+2.06) (+2.08)
2 86.55 88.61 88.84 75.91 75.30 74.20 76.39 75.39 73.77
(+2.63) (+1.22) (+0.90) (+3.22) (+3.09) (+3.10) (+3.40) (+2.20) (+2.16)
3 86.47 88.56 88.74 75.27 74.08 72.88 75.30 74.22 72.53
(+2.63) (+1.18) (+0.90) (+3.36) (+3.41) (+2.85) (+3.74) (+2.12) (+2.60)
? 86.04 88.41 88.67 74.44 73.26 71.96 74.48 73.50 71.84
(+2.17) (+1.07) (+0.91) (+3.10) (+3.02) (+2.51) (+3.31) (+1.97) (+3.02)
Table 2: Factored parsing results for Tu?Ba-D/Z, Tiger, and Negra when tagging is done by the parser. Numbers in
italics show difference between factored parser and PCFG, where improvements over the PCFG are positive.
comparable to Maier (2006), which found 3?6% im-
provement using an unlexicalized PCFG; these ab-
solute improvements hold despite the fact that the
Maier (2006) parser has results with 2?4% absolute
lower F1 than those in this paper.
4 Inclusion of Grammatical Functions
In this section we examine how the addition of gram-
matical functions for training and evaluation affects
performance. As noted previously, we add gram-
matical functions simply by concatenating them to
the dependent phrasal categories and calling each
unique symbol a PCFG nonterminal; this is an ob-
vious way to adapt an existing PCFG parser, but not
a sophisticated model of grammatical functions. We
also present our shared task results (table 6).
4.1 Effects on Evaluation
As shown in table 4, the inclusion of grammati-
cal functions decreases performance by 10?15% for
both treebanks. This is partially due to the increase
in grammar size, creating less supporting evidence
for each rule, and the fact that the parser must now
discriminate amongst more categories. The larger
grammar is particularly problematic for Tiger due to
its flat annotation style. Adding gold tags (table 5)
increases performace by 2?3%, a similar gain to that
for the parsers without grammatical functions. We
also see that lexicalization provides smaller gains
when grammatical functions are included; we dis-
cuss this further in section 5. Finally, especially for
the Tiger corpus, vertical Markovization diminishes
Tu?Ba-D/Z Vertical Markov Order
Horizontal Order 1 2
1 89.66 91.69
(+1.82) (+0.54)
2 89.72 91.71
(+1.56) (+0.43)
? 89.34 91.43
(+1.39) (+0.29)
Tiger Vertical Markov Order
Horizontal Order 1 2
1 79.39 79.67
(+2.83) (+2.53)
2 78.60 77.40
(+2.74) (+2.22)
? 76.65 75.29
(+2.50) (+1.94)
Negra Vertical Markov Order
Horizontal Order 1 2
1 78.80 79.51
(+2.39) (+1.55)
2 77.92 77.43
(+2.15) (+1.81)
? 74.44 73.26
(+3.10) (+3.02)
Table 3: Factored parsing results for Tu?Ba-D/Z, Tiger,
and Negra when gold tags are provided as input to the
parser. Numbers in italics show difference between fac-
tored parser and PCFG, where improvements over the
PCFG are positive.
42
TueBa-D/Z Tiger
Horiz. Vertical Vertical
Order 1 2 1 2
1 75.97 77.21 60.48 58.00
(+2.69) (+1.49) (+2.69) (+2.24)
2 76.96 53.68
(+1.44) (+2.22)
? 75.24 76.66 55.36 50.94
(+2.18) (+1.22) (+2.50) (+1.94)
Table 4: Results for Tu?Ba-D/Z and Tiger when gram-
matical functions are included and tagging is done by
the parser. Numbers in italics show difference between
factored parser and PCFG, where improvements over the
PCFG are positive.
Tu?Ba-D/Z Tiger
Horiz. Vertical Vertical
Order 1 2 1 2
1 78.91 80.64 67.72 64.93
(+1.60) (+0.81) (+1.16) (+0.77)
2 80.32 59.60
(+0.69) (+0.67)
? 78.38 80.01 60.36 56.77
(+1.33) (+0.59) (+0.89) (+0.18)
Table 5: Results for Tu?Ba-D/Z and Tiger when gram-
matical functions are included and gold tags (including
grammatical functions) are given to the parser.
Tu?Ba-D/Z Tiger
Petrov & Klein 83.97 69.81
Rafferty & Manning 79.24 59.44
Hall 75.37 65.18
Rafferty & Manning -gf 73.36 49.03
Table 6: Shared task results (F1) for Tu?Ba-D/Z and Tiger
when grammatical functions are included and gold tags
are given to the parser. Gold tags include grammatical
functions except in the case of ?Rafferty & Manning -gf?.
performance. Sparsity becomes too great of an is-
sue for increased vertical annotations to be effective:
the grammar grows from 11,170 rules with horizon-
tal Markovization = 1, vertical Markovization = 1
to 39,435 rules with horizontal Markovization = ?,
vertical Markovization = 2.
Tu?Ba-D/Z Fact. PCFG
Configuration F1 ? F1 ?
H = 1, V = 1 87.63 +1.63 85.32 +1.58
H = 1, V = 2 88.47 ?0.13 87.31 ?0.08
H = 2, V = 2 88.30 ?0.31 87.13 ?0.26
H = ?, V = 1 87.23 +1.17 85.27 +1.40
H = ?, V = 2 88.18 ?0.23 87.09 ?0.25
Tiger Fact. PCFG
Configuration F1 ? F1 ?
H = 1, V = 1 72.09 ?4.60 69.09 ?4.06
H = 1, V = 2 69.25 ?8.15 67.24 ?6.59
H = 2, V = 2 66.08 ?9.22 64.42 ?7.79
H = ?, V = 1 67.58 ?9.07 64.85 ?6.49
H = ?, V = 2 63.54 ?11.75 62.21 ?8.03
Table 7: Effect of adding grammatical functions infor-
mation to the training data only. The difference (?) is
from a parser with same Markovization parameters but
not trained with grammatical functions.
4.2 Effects on Training Only
While training and testing with grammatical func-
tions significantly reduces our performance, this
does not necessarily mean that we cannot benefit
from grammatical functions. We explored whether
training with grammatical functions could improve
the parser?s test time performance on syntactic cat-
egories (ignoring grammatical functions), hypothe-
sizing that the functions could provide additional in-
formation for disambiguating which rule should be
applied. This test also provides evidence of whether
decreased performance with grammatical functions
is due to sparseness caused by the large grammar
or simply that more categorization needs to be done
when grammatical functions are included.
We found, as shown in table 7, that grammatical
functions provide limited gains for basic categories
but have no extra utility once vertical Markoviza-
tion is added. These results suggest that adding
grammatical functions is not only problematic due to
increased categorization but because of sparseness
(this task has the same categorization demands as
parsing without grammatical functions considered
in section 3). The Stanford Parser was initially de-
signed under the assumption of a small phrasal cat-
egory set, and makes no attempts to smooth gram-
mar rule probabilities (smoothing only probabilities
43
of words having a certain tag and probabilities of de-
pendencies). While this approach is in general not
optimal when many category splits are used inside
the parser ? smoothing helps, cf. Petrov et al (2006)
? it becomes untenable as the category set grows
large, multi-faceted, and sparse. This is particularly
evident given the results in table 7 that show the pre-
cipitous decline in F1 on the Tiger corpus, where
the general problems are exacerbated by the flatter
annotation style of Tiger.
5 Lexicalization
In the tables in section 3, we showed the utility
of lexicalization for German parsing when gram-
matical functions are not required. This contrasts
strongly with the results of (Dubey and Keller, 2003;
Dubey, 2004) where no performance increases (in-
deed, performance decreases) are reported from lex-
icalization. Lexicalization shows fairly consistent
2?3% gains on the Negra and Tiger treebanks. As
the number of tags increases, however, such as when
grammatical functions are included, gains from lex-
icalization are limited due to sparseness. While use-
ful category splits lessen the need for lexicaliza-
tion, we think the diminishing gain is primarily due
to problems resulting from the unsmoothed PCFG
model. As the grammar becomes sparser, there are
limited opportunities for the lexical dependencies
to correct the output of the PCFG grammar under
the factored parsing model of Klein and Manning
(2003b). Indeed, as shown in table 8, the grammar
becomes sufficiently sparse that for many sentences
there is no tree on which the PCFG and dependency
grammar can agree, and the parser falls back to sim-
ply returning the best PCFG parse. This falloff, in
addition to overall issues of sparsity, helps explain
the drop in performance with the addition of gram-
matical functions: our possible gain from lexicalized
parsing is decreased by the increasing rate of fail-
ure for the factored parser. Thus, for future German
work to gain from lexicalization, it may be necessary
to explore smoothing the grammar or working with
a diminished tagset without grammatical functions.
Lexicalized parsing focuses on identifying depen-
dencies. As recognized by Collins (2003), identi-
fying dependencies between words allows for bet-
ter evaluation of attachment accuracy, diminishing
Total Parseable
Dataset Sent. w.o. GFs with GFs
Tu?Ba-D/Z 2611 2610 2197
Tiger 2535 2534 1592
Table 8: Number of sentences parseable by the factored
lexicalized parser. If the factored model fails to return
a parse, the parser returns the best PCFG parse, so the
parser maintains 100% coverage.
Tu?Ba-D/Z Tiger
Gold Tags 91.00 90.21
Auto. Tags 86.90 83.39
Gold Tags -gf 89.89 88.97
Auto. Tags -gf 86.89 85.86
Table 9: Performance (F1) on identifying dependencies
in Tu?Ba-D/Z and Tiger. Tags were either provided (?Gold
Tags?) or generated during parsing (?Auto. Tags?); gram-
matical functions were used for the first two results and
omitted for the final two (?-gf?).
spurious effects on labeled bracketing F1 of differ-
ent annotation schemes. In particular, Rehbein and
van Genabith (2007) correctly emphasize how F1
scores are very dependent on the amount of branch-
ing structure in a treebank, and are hence not validly
comparable across annotation styles. We evaluate
performance on identifying unlabeled dependencies
between heads and modifiers, extracting dependen-
cies automatically from the parse trees. Most heads
in the Tu?Ba-D/Z and Tiger treebanks are marked,
and we use marked heads when possible for train-
ing and evaluation. When heads were not marked,
we used heuristic rules to identify the likely head.
Broadly consistent with the results of Rehbein and
van Genabith (2007), Table 9 shows that the dis-
parity in performance between Tu?Ba-D/Z and Tiger
is much smaller when measuring dependency accu-
racy rather than labeled bracketing F1, especially
when using gold tags. These results also reverse the
trend in our other results that adding grammatical
functions greatly reduces F1. While F1 decreases
or remains constant when grammatical functions are
used with automatic tags, probably reflecting a de-
crease in accuracy on tags when using grammatical
functions, they increase F1 given gold tags. These
results suggest both that useful information may be
gained from grammatical functions and that the dif-
44
ferences between the annotation schemes of Tu?Ba-
D/Z and Tiger may not cause as large a fundamen-
tal difference in parser performance as suggested in
Ku?bler et al (2006).
6 Feature Splits
Another technique shown to improve accuracy in
English parsing is state splits (Klein and Manning,
2003a). We experimented with such splits in an
attempt to show similar utility for German. How-
ever, despite trying a number of splits that leveraged
observations of useful splits for English as well as
information from grammatical functions, we were
unable to find any splits that caused significant im-
provement for German parsing performance. Some-
what more positive results are reported by Schiehlen
(2004) ? in particular, his relative clause marking
adds significantly to performance ? although many
of the other features he explores also yield little.
7 Errors by Category
In this section, we examine which categories have
the most parsing errors and possible reasons for
these biases. Two types of error patterns are con-
sidered: errors on particularly salient grammatical
functions and overall category errors.
7.1 Grammatical Function Errors
A subset of grammatical functions was recognized
by Ku?bler et al (2006) as particularly important for
using parsing results, so we investigated training
and testing with the inclusion of these grammatical
functions but without any others. These functions
were the subject, dative object, and accusative object
functions. We found that the three categories had
distinctively different patterns of errors, although we
unfortunately still do not achieve particularly high
F1 for any of the individual pairings of node label
and grammatical function. Note that this analysis
differs from that of Ku?bler et al (2006) due to our
analysis of the accuracy of node labels and gram-
matical functions, rather than only performance on
identifying these three grammatical functions (with-
out regards to the correctness of the original node
label). Overall, dative objects occur much less fre-
quently than either of the other two types, and ac-
cusative objects occur less frequently than subjects.
Consistent with sparsity causing degradations in per-
formance, for both Tiger and Tu?Ba-D/Z, we show
the best performance on subjects, followed by ac-
cusative objects and then dative objects. For all cat-
egories, we find that these functions occur most fre-
quently with noun phrases, and we achieve higher
performance when pairing tthem with a noun phrase
than with any other basic category. While Ku?bler
et al (2006) suggests these functions are particu-
larly important for parsing, our low performance on
dative objects (F1 between 0.00 and 0.06) may not
matter a great deal given that dative objects consist
of only 0.42% of development set nodes in Tu?Ba-
D/Z and 0.76% of such nodes in Tiger.
7.2 Overall Errors
One limiting factor for overall parsing accuracy is
roughly defined by the number of local (one-level)
trees in the test set that are present in the training set.
While changes such as Markovization may allow
rules to be learned that do not correspond directly to
such local trees, it is unlikely that many such rules
will be created. Thus, if a local tree in the test set
is not represented in the training set, it is unlikely
we will be able to correctly parse this sentence. The
number of such local trees and the amount of test set
coverage they provide varies widely between Tu?Ba-
D/Z and Tiger. Without grammatical functions, the
training set for Tu?Ba-D/Z contains 4,532 unique lo-
cal trees, whereas the training set for Tiger con-
tains 20,957; both have 20,894 complete trees. Lo-
cal trees from the training set represent 79.6% of
the unique local trees in the development set for
Tu?Ba-D/Z, whereas they represent 61.8% of unique
local trees in Tiger?s development set. This trans-
lates to 99.3% of total local trees in the develop-
ment set represented in the training set for Tu?Ba-
D/Z versus 92.3% for Tiger. With grammatical func-
tions, the number of unique local trees increases for
both Tu?Ba-D/Z and Tiger (10,464 and 32,614 trees
in training, respectively), and total coverage in the
development sets drop to 98.6% (Tu?Ba-D/Z) and
87.7% (Tiger). Part of the reason for this decrease
in coverage with the addition of grammatical func-
tions, and the disparity between corpora, is a large
increase in the number of possible categories for
each node: from 26 to 139 categories for Tu?Ba-D/Z
and from 24 to 192 categories for Tiger.
45
References
Noam Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press, Cambridge, MA.
Michael Collins. 2003. Head-driven statistical models
for natural language parsing. In Computational Lin-
guistics, pages 589?638.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In 5th
International Conference on Language Resources and
Evaluation (LREC 2006), pages 449?454.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In
ACL 41, pages 96?103.
Amit Dubey. 2004. Statistical Parsing for German:
Modeling Syntactic Properties and Annotation Differ-
ences. Ph.D. thesis, Universitaet des Saarlandes.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In ACL 43, pages 314?21.
Dan Klein and Christopher D. Manning. 2003a. Accu-
rate unlexicalized parsing. In ACL 41, pages 423?430.
Dan Klein and Christopher D. Manning. 2003b. Fast
exact inference with a factored model for natural lan-
guage parsing. Advances in Neural Information Pro-
cessing Systems, 15:3?10.
Sandra Ku?bler, Erward W. Hinrichs, and Wolfgang
Maier? 2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of RANLP
2005.
Wolfgang Maier. 2006. Annotation schemes and their in-
uence on parsing results. In Proceedings of the COL-
ING/ACL 2006 Student Research Workshop, pages 19?
24.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In ACL 44, pages 433?440.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 630?639.
Michael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In Proceedings of the
20th International Conference on Computational Lin-
guistics, pages 390?96.
46
Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 23?31,
Suntec, Singapore, 7 August 2009.
c
?2009 ACL and AFNLP
Random Walks for Text Semantic Similarity
Daniel Ramage, Anna N. Rafferty, and Christopher D. Manning
Computer Science Department
Stanford University
Stanford, CA 94305
{dramage,manning}@cs.stanford.edu
rafferty@eecs.berkeley.edu
Abstract
Many tasks in NLP stand to benefit from
robust measures of semantic similarity for
units above the level of individual words.
Rich semantic resources such as WordNet
provide local semantic information at the
lexical level. However, effectively com-
bining this information to compute scores
for phrases or sentences is an open prob-
lem. Our algorithm aggregates local re-
latedness information via a random walk
over a graph constructed from an underly-
ing lexical resource. The stationary dis-
tribution of the graph walk forms a ?se-
mantic signature? that can be compared
to another such distribution to get a relat-
edness score for texts. On a paraphrase
recognition task, the algorithm achieves an
18.5% relative reduction in error rate over
a vector-space baseline. We also show that
the graph walk similarity between texts
has complementary value as a feature for
recognizing textual entailment, improving
on a competitive baseline system.
1 Introduction
Many natural language processing applications
must directly or indirectly assess the semantic sim-
ilarity of text passages. Modern approaches to
information retrieval, summarization, and textual
entailment, among others, require robust numeric
relevance judgments when a pair of texts is pro-
vided as input. Although each task demands its
own scoring criteria, a simple lexical overlap mea-
sure such as cosine similarity of document vectors
can often serve as a surprisingly powerful base-
line. We argue that there is room to improve these
general-purpose similarity measures, particularly
for short text passages.
Most approaches fall under one of two cate-
gories. One set of approaches attempts to explic-
itly account for fine-grained structure of the two
passages, e.g. by aligning trees or constructing
logical forms for theorem proving. While these
approaches have the potential for high precision
on many examples, errors in alignment judgments
or formula construction are often insurmountable.
More broadly, it?s not always clear that there is a
correct alignment or logical form that is most ap-
propriate for a particular sentence pair. The other
approach tends to ignore structure, as canonically
represented by the vector space model, where any
lexical item in common between the two passages
contributes to their similarity score. While these
approaches often fail to capture distinctions im-
posed by, e.g. negation, they do correctly capture
a broad notion of similarity or aboutness.
This paper presents a novel variant of the vec-
tor space model of text similarity based on a ran-
dom walk algorithm. Instead of comparing two
bags-of-words directly, we compare the distribu-
tion each text induces when used as the seed of
a random walk over a graph derived from Word-
Net and corpus statistics. The walk posits the ex-
istence of a distributional particle that roams the
graph, biased toward the neighborhood surround-
ing an input bag of words. Eventually, the walk
reaches a stationary distribution over all nodes in
the graph, smoothing the peaked input distribution
over a much larger semantic space. Two such sta-
tionary distributions can be compared using con-
ventional measures of vector similarity, producing
a final relatedness score.
This paper makes the following contributions.
We present a novel random graph walk algorithm
23
Word Step 1 Step 2 Step 3 Conv.
eat 3 8 9 9
corrode 10 33 53 >100
pasta ? 2 3 5
dish ? 4 5 6
food ? ? 21 12
solid ? ? ? 26
Table 1: Ranks of sample words in the distribu-
tion for I ate a salad and spaghetti after a given
number of steps and at convergence. Words in the
vector are ordered by probability at time step t; the
word with the highest probability in the vector has
rank 1. ??? indicates that node had not yet been
reached.
for semantic similarity of texts, demonstrating its
efficiency as compared to a much slower but math-
ematically equivalent model based on summed
similarity judgments of individual words. We
show that walks effectively aggregate information
over multiple types of links and multiple input
words on an unsupervised paraphrase recognition
task. Furthermore, when used as a feature, the
walk?s semantic similarity score can improve the
performance of an existing, competitive textual
entailment system. Finally, we provide empiri-
cal results demonstrating that indeed, each step of
the random walk contributes to its ability to assess
paraphrase judgments.
2 A random walk example
To provide some intuition about the behavior of
the random walk on text passages, consider the
following example sentence: I ate a salad and
spaghetti.
No measure based solely on lexical identity
would detect overlap between this sentence and
another input consisting of only the word food.
But if each text is provided as input to the random
walk, local relatedness links from one word to an-
other allow the distributional particle to explore
nearby parts of the semantic space. The number of
non-zero elements in both vectors increases, even-
tually converging to a stationary distribution for
which both vectors have many shared non-zero en-
tries.
Table 1 ranks elements of the sentence vector
based on their relative weights. Observe that at the
beginning of the walk, corrode has a high rank due
to its association with the WordNet sense of eat
corresponding to eating away at something. How-
ever, because this concept is not closely linked
with other words in the sentence, its relative rank
drops as the distribution converges and other word
senses more related to food are pushed up. The
random walk allows the meanings of words to re-
inforce one another. If the sentence above had
ended with drank wine rather than spaghetti, the
final weight on the food node would be smaller
since fewer input words would be as closely linked
to food. This matches the intuition that the first
sentence has more to do with food than does the
second, although both walks should and do give
some weight to this node.
3 Related work
Semantic relatedness for individual words has
been thoroughly investigated in previous work.
Budanitsky and Hirst (2006) provide an overview
of many of the knowledge-based measures derived
from WordNet, although other data sources have
been used as well. Hughes and Ramage (2007) is
one such measure based on random graph walks.
Prior work has considered random walks on var-
ious text graphs, with applications to query expan-
sion (Collins-Thompson and Callan, 2005), email
address resolution (Minkov and Cohen, 2007), and
word-sense disambiguation (Agirre and Soroa,
2009), among others.
Measures of similarity have also been proposed
for sentence or paragraph length text passages.
Mihalcea et al (2006) present an algorithm for
the general problem of deciding the similarity of
meaning in two text passages, coining the name
?text semantic similarity? for the task. Corley
and Mihalcea (2005) apply this algorithm to para-
phrase recognition.
Previous work has shown that similarity mea-
sures can have some success as a measure of tex-
tual entailment. Glickman et al (2005) showed
that many entailment problems can be answered
using only a bag-of-words representation and web
co-occurrence statistics. Many systems integrate
lexical relatedness and overlap measures with
deeper semantic and syntactic features to create
improved results upon relatedness alone, as in
Montejo-R?aez et al (2007).
4 Random walks on lexical graphs
In this section, we describe the mechanics of
computing semantic relatedness for text passages
24
based on the random graph walk framework. The
algorithm underlying these computations is related
to topic-sensitive PageRank (Haveliwala, 2002);
see Berkhin (2005) for a survey of related algo-
rithms.
To compute semantic relatedness for a pair of
passages, we compare the stationary distributions
of two Markov chains, each with a state space de-
fined over all lexical items in an underlying corpus
or database. Formally, we define the probability of
finding the particle at a node n
i
at time t as:
n
(t)
i
=
?
n
j
?V
n
(t?1)
j
P (n
i
| n
j
)
where P (n
i
| n
j
) is the probability of transition-
ing from n
j
to n
i
at any time step. If those transi-
tions bias the particle to the neighborhood around
the words in a text, the particle?s distribution can
be used as a lexical signature.
To compute relatedness for a pair of texts, we
first define the graph nodes and transition proba-
bilities for the random walk Markov chain from
an underlying lexical resource. Next, we deter-
mine an initial distribution over that state space for
a particular input passage of text. Then, we sim-
ulate a random walk in the state space, biased to-
ward the initial distribution, resulting in a passage-
specific distribution over the graph. Finally, we
compare the resulting stationary distributions from
two such walks using a measure of distributional
similarity. The remainder of this section discusses
each stage in more detail.
4.1 Graph construction
We construct a graph G = (V,E) with vertices V
and edges E extracted from WordNet 3.0. Word-
Net (Fellbaum, 1998) is an annotated graph of
synsets, each representing one concept, that are
populated by one or more words. The set of ver-
tices extracted from the graph is all synsets present
in WordNet (e.g. foot#n#1 meaning the part of
the human leg below the ankle), all part-of-speech
tagged words participating in those synsets (e.g.
foot#n linking to foot#n#1 and foot#n#2 etc.), and
all untagged words (e.g. foot linking to foot#n and
foot#v). The set of edges connecting synset nodes
is all inter-synset edges contained in WordNet,
such as hyponymy, synonomy, antonymy, etc., ex-
cept for regional and usage links. All WordNet
relational edges are given uniform weight. Edges
also connect each part-of-speech tagged word to
all synsets it takes part in, and from each word to
all its part-of-speech. These edge weights are de-
rived from corpus counts as in Hughes and Ram-
age (2007). We also included a low-weight self-
loop for each node.
Our graph has 420,253 nodes connected by
1,064,464 edges. Because synset nodes do not link
outward to part-of-speech tagged nodes or word
nodes in this graph, only the 117,659 synset nodes
have non-zero probability in every random walk?
i.e. the stationary distribution will always be non-
zero for these 117,659 nodes, but will be non-zero
for only a subset of the remainder.
4.2 Initial distribution construction
The next step is to seed the random walk with an
initial distribution over lexical nodes specific to
the given sentence. To do so, we first tag the in-
put sentence with parts-of-speech and lemmatize
each word based on the finite state transducer of
Minnen et al (2001). We search over consecu-
tive words to match multi-word collocation nodes
found in the graph. If the word or its lemma is
part of a sequence that makes a complete colloca-
tion, that collocation is used. If not, the word or
its lemma with its part of speech tag is used if it
is present as a graph node. Finally, we fall back
to the surface word form or underlying lemma
form without part-of-speech information if neces-
sary. For example, the input sentence: The boy
went with his dog to the store, would result in mass
being assigned to underlying graph nodes boy#n,
go with, he, dog#n, store#n.
Term weights are set with tf.idf and then nor-
malized. Each term?s weight is proportional to the
number of occurrences in the sentence times the
log of the number of documents in some corpus
divided by the number of documents containing
that term. Our idf counts were derived from the
English Gigaword corpus 1994-1999.
4.3 Computing the stationary distribution
We use the power iteration method to compute the
stationary distribution for the Markov chain. Let
the distribution over the N states at time step t of
the random walk be denoted ~v
(t)
? R
N
, where
~v
(0)
is the initial distribution as defined above. We
denote the column-normalized state-transition ma-
trix as M ? R
N?N
. We compute the stationary
distribution of the Markov chain with probability
? of returning to the initial distribution at each
25
time step as the limit as t?? of:
~v
(t)
= ?~v
(0)
+ (1? ?)M~v
(t?1)
In practice, we test for convergence by examining
if
?
N
i=1
?v
(t)
i
? v
(t?1)
i
? < 10
?6
, which in our ex-
periments was usually after about 50 iterations.
Note that the resulting stationary distribution
can be factored as the weighted sum of the sta-
tionary distributions of each word represented in
the initial distribution. Because the initial distri-
bution ~v
(0)
is a normalized weighted sum, it can
be re-written as ~v
(0)
=
?
k
?
k
? ~w
(0)
k
for ~w
k
hav-
ing a point mass at some underlying node in the
graph and with ?
k
positive such that
?
k
?
k
= 1.
A simple proof by induction shows that the sta-
tionary distribution ~v
(?)
is itself the weighted sum
of the stationary distribution of each underlying
word, i.e. ~v
?
=
?
k
?
k
? ~w
(?)
k
.
In practice, the stationary distribution for a
passage of text can be computed from a single
specially-constructed Markov chain. The process
is equivalent to taking the weighted sum of every
word type in the passage computed independently.
Because the time needed to compute the station-
ary distribution is dominated by the sparsity pat-
tern of the walk?s transition matrix, the computa-
tion of the stationary distribution for the passage
takes a fraction of the time needed if the station-
ary distribution for each word were computed in-
dependently.
4.4 Comparing stationary distributions
In order to get a final relatedness score for a pair
of texts, we must compare the stationary distribu-
tion from the first walk with the distribution from
the second walk. There exist many measures for
computing a final similarity (or divergence) mea-
sure from a pair of distributions, including geo-
metric measures, information theoretic measures,
and probabilistic measures. See, for instance, the
overview of measures provided in Lee (2001).
In system development on training data, we
found that most measures were reasonably effec-
tive. For the rest of this paper, we report num-
bers using cosine similarity, a standard measure in
information retrieval; Jensen-Shannon divergence,
a commonly used symmetric measure based on
KL-divergence; and the dice measure extended to
weighted features (Curran, 2004). A summary of
these measures is shown in Table 2. Justification
Cosine
~x?~y
?~x?
2
?~y?
2
Jensen-Shannon
1
2
D(x?
x+y
2
) +
1
2
D(y?
x+y
2
)
Dice
2
P
i
min(x
i
,y
i
)
P
i
x
i
+
P
i
y
i
Table 2: Three measures of distributional similar-
ity between vectors ~x and ~y used to compare the
stationary distributions from passage-specific ran-
dom walks. D(p?q) is KL-divergence, defined as
?
i
p
i
log
p
i
q
i
.
for the choice of these three measures is discussed
in Section 6.
5 Evaluation
We evaluate the system on two tasks that might
benefit from semantic similarity judgments: para-
phrase recognition and recognizing textual entail-
ment. A complete solution to either task will cer-
tainly require tools more tuned to linguistic struc-
ture; the paraphrase detection evaluation argues
that the walk captures a useful notion of semantics
at the sentence level. The entailment system eval-
uation demonstrates that the walk score can im-
prove a larger system that does make use of more
fine-grained linguistic knowledge.
5.1 Paraphrase recognition
The Microsoft Research (MSR) paraphrase data
set (Dolan et al, 2004) is a collection of 5801
pairs of sentences automatically collected from
newswire over 18 months. Each pair was hand-
annotated by at least two judges with a binary
yes/no judgment as to whether one sentence was
a valid paraphrase of the other. Annotators were
asked to judge whether the meanings of each
sentence pair were reasonably equivalent. Inter-
annotator agreement was 83%. However, 67% of
the pairs were judged to be paraphrases, so the cor-
pus does not reflect the rarity of paraphrases in the
wild. The data set comes pre-split into 4076 train-
ing pairs and 1725 test pairs.
Because annotators were asked to judge if the
meanings of two sentences were equivalent, the
paraphrase corpus is a natural evaluation testbed
for measures of semantic similarity. Mihalcea et
al. (2006) defines a measure of text semantic sim-
ilarity and evaluates it in an unsupervised para-
phrase detector on this data set. We present their
26
algorithm here as a strong reference point for se-
mantic similarity between text passages, based on
similar underlying lexical resources.
The Mihalcea et al (2006) algorithm is a wrap-
per method that works with any underlying mea-
sure of lexical similarity. The similarity of a pair
of texts T
1
and T
2
, denoted as sim
m
(T
1
, T
2
), is
computed as:
sim
m
(T
1
, T
2
) =
1
2
f(T
1
, T
2
) +
1
2
f(T
2
, T
1
)
f(T
a
, T
b
) =
P
w?T
a
maxSim(w, T
b
) ? idf(w)
P
w?T
a
idf(w)
where the maxSim(w, T ) function is defined as
the maximum similarity of the word w within the
text T as determined by an underlying measure of
lexical semantic relatedness. Here, idf(w) is de-
fined as the number of documents in a background
corpus divided by the number of documents con-
taining the term. maxSim compares only within
the same WordNet part-of-speech labeling in or-
der to support evaluation with lexical relatedness
measures that cannot cross part-of-speech bound-
aries.
Mihalcea et al (2006) presents results for sev-
eral underlying measures of lexical semantic re-
latedness. These are subdivided into corpus-based
measures (using Latent Semantic Analysis (Lan-
dauer et al, 1998) and a pointwise-mutual infor-
mation measure) and knowledge-based resources
driven by WordNet. The latter include the methods
of Jiang and Conrath (1997), Lesk (1986), Resnik
(1999), and others.
In this unsupervised experimental setting, we
consider using only a thresholded similarity value
from our system and from the Mihalcea algorithm
to determine the paraphrase or non-paraphrase
judgment. For consistency with previous work, we
threshold at 0.5. Note that this threshold could be
tuned on the training data in a supervised setting.
Informally, we observed that on the training data a
threshold of near 0.5 was often a good choice for
this task.
Table 3 shows the results of our system and
a representative subset of those reported in (Mi-
halcea et al, 2006). All the reported measures
from both systems do a reasonable job of para-
phrase detection ? the majority of pairs in the cor-
pus are deemed paraphrases when the similarity
measure is thresholded at 0.5, and indeed this is
reasonable given the way in which the data were
System Acc. F
1
: c
1
F
1
: c
0
Macro F
1
Random Graph Walk
Walk (Cosine) 0.687 0.787 0.413 0.617
Walk (Dice) 0.708 0.801 0.453 0.645
Walk (JS) 0.688 0.805 0.225 0.609
Mihalcea et. al., Corpus-based
PMI-IR 0.699 0.810 0.301 0.625
LSA 0.684 0.805 0.170 0.560
Mihalcea et. al., WordNet-based
J&C 0.693 0.790 0.433 0.629
Lesk 0.693 0.789 0.439 0.629
Resnik 0.690 0.804 0.254 0.618
Baselines
Vector-based 0.654 0.753 0.420 0.591
Random 0.513 0.578 0.425 0.518
Majority (c
1
) 0.665 0.799 ? 0.399
Table 3: System performance on 1725 examples of
the MSR paraphrase detection test set. Accuracy
(micro-averaged F
1
), F
1
for c
1
?paraphrase? and
c
0
?non-paraphrase? classes, and macro-averaged
F
1
are reported.
collected. The first three rows are the perfor-
mance of the similarity judgments output by our
walk under three different distributional similar-
ity measures (cosine, dice, and Jensen-Shannon),
with the walk score using the dice measure outper-
forming all other systems on both accuracy and
macro-averaged F
1
. The output of the Mihalcea
system using a representative subset of underly-
ing lexical measures is reported in the second and
third segments. The fourth segment reports the re-
sults of baseline methods?the vector space simi-
larity measure is cosine similarity among vectors
using tf.idf weighting, and the random baseline
chooses uniformly at random, both as reported in
(Mihalcea et al, 2006). We add the additional
baseline of always guessing the majority class la-
bel because the data set is skewed toward ?para-
phrase.?
In an unbalanced data setting, it is important to
consider more than just accuracy and F
1
on the
majority class. We report accuracy, F
1
for each
class label, and the macro-averaged F
1
on all sys-
tems. F
1
: c
0
and Macro-F
1
are inferred for the sys-
tem variants reported in (Mihalcea et al, 2006).
Micro-averaged F
1
in this context is equivalent to
accuracy (Manning et al, 2008).
Mihalcea also reports a combined classifier
which thresholds on the simple average of the in-
dividual classifiers, resulting in the highest num-
bers reported in that work, with accuracy of 0.703,
?paraphrase? class F
1
: c
1
= 0.813, and inferred
Macro F
1
= 0.648. We believe that the scores
27
Data Set Cosine Dice Jensen-Shannon
RTE2 dev 55.00 51.75 55.50
RTE2 test 57.00 54.25 57.50
RTE3 dev 59.00 57.25 59.00
RTE3 test 55.75 55.75 56.75
Table 4: Accuracy of entailment detection when
thresholding the text similarity score output by the
random walk.
from the various walk measures might also im-
prove performance when in a combination clas-
sifier, but without access to the individual judg-
ments in that system we are unable to evaluate
the claim directly. However, we did create an up-
per bound reference by combining the walk scores
with easily computable simple surface statistics.
We trained a support vector classifier on the MSR
paraphrase training set with a feature space con-
sisting of the walk score under each distributional
similarity measure, the length of each text, the dif-
ference between those lengths, and the number of
unigram, bigram, trigram, and four-gram overlaps
between the two texts. The resulting classifier
achieved accuracy of 0.719 with F
1
: c
1
= 0.807
and F
1
: c
0
= 0.487 and Macro F
1
= 0.661. This
is a substantial improvement, roughly on the same
order of magnitude as from switching to the best
performing distributional similarity function.
Note that the running time of the Mihalcea et
al. algorithm for comparing texts T
1
and T
2
re-
quires |T
1
| ? |T
2
| individual similarity judgments.
By contrast, this work allows semantic profiles to
be constructed and evaluated for each text in a sin-
gle pass, independent of the number of terms in
the texts.
The performance of this unsupervised applica-
tion of walks to paraphrase recognition suggests
that the framework captures important intuitions
about similarity in text passages. In the next sec-
tion, we examine the performance of the measure
embedded in a larger system that seeks to make
fine-grained entailment judgments.
5.2 Textual entailment
The Recognizing Textual Entailment Challenge
(Dagan et al, 2005) is a task in which systems as-
sess whether a sentence is entailed by a short pas-
sage or sentence. Participants have used a variety
of strategies beyond lexical relatedness or overlap
for the task, but some have also used only rela-
tively simple similarity metrics. Many systems
Data Set Baseline Cosine Dice JS
RTE2 dev 66.00 66.75 65.75 66.25
RTE2 test 63.62 64.50 63.12 63.25
RTE3 dev 70.25 70.50 70.62 70.38
RTE3 test 65.44 65.82 65.44 65.44
Table 5: Accuracy when the random walk is
added as a feature of an existing RTE system
(left column) under various distance metrics (right
columns).
incorporate a number of these strategies, so we
experimented with using the random walk to im-
prove an existing RTE system. This addresses the
fact that using similarity alone to detect entailment
is impoverished: entailment is an asymmetric de-
cision while similarity is necessarily symmetric.
However, we also experiment with thresholding
random walk scores as a measure of entailment to
compare to other systems and provide a baseline
for whether the walk could be useful for entail-
ment detection.
We tested performance on the development and
test sets for the Second and Third PASCAL RTE
Challenges (Bar-Haim et al, 2006; Giampiccolo
et al, 2007). Each of these data sets contains 800
pairs of texts for which to determine entailment.
In some cases, no words from a passage appear
in WordNet, leading to an empty vector. In this
case, we use the Levenshtein string similarity mea-
sure between the two texts; this fallback is used in
fewer than five examples in any of our data sets
(Levenshtein, 1966).
Table 4 shows the results of using the simi-
larity measure alone to determine entailment; the
system?s ability to recognize entailment is above
chance on all data sets. Since the RTE data sets are
balanced, we used the median of the random walk
scores for each data set as the threshold rather than
using an absolute threshold. While the measure
does not outperform most RTE systems, it does
outperform some systems that used only lexical
overlap such as the Katrenko system from the sec-
ond challenge (Bar-Haim et al, 2006). These re-
sults show that the measure is somewhat sensitive
to the distance metric chosen, and that the best dis-
tance metric may vary by application.
To test the random walk?s value for improv-
ing an existing RTE system, we incorporated the
walk as a feature of the Stanford RTE system
(Chambers et al, 2007). This system computes
28
a weighted sum of a variety of features to make
an entailment decision. We added the random
walk score as one of these features and scaled it
to have a magnitude comparable to the other fea-
tures; other than scaling, there was no system-
specific engineering to add this feature.
As shown in Table 5, adding the random walk
feature improves the original RTE system. Thus,
the random walk score provides meaningful ev-
idence for detecting entailment that is not sub-
sumed by other information, even in a system with
several years of feature engineering and competi-
tive performance. In particular, this RTE system
contains features representing the alignment score
between two passages; this score is composed of a
combination of lexical relatedness scores between
words in each text. The ability of the random walk
to add value to the system even given this score,
which contains many common lexical relatedness
measures, suggests we are able to extract text sim-
ilarity information that is distinct from other mea-
sures. To put the gain we achieve in perspective,
an increase in the Stanford RTE system?s score of
the same magnitude would have moved the sys-
tem?s two challenge entries from 7th and 25th
to 6th and 17th, respectively, in the second RTE
Challenge. It is likely the gain from this feature
could be increased by closer integration with the
system and optimizing the initial distribution cre-
ation for this task.
By using the score as a feature, the system is
able to take advantage of properties of the score
distribution. While Table 4 shows performance
when a threshold is picked a priori, experiment-
ing with that threshold increases performance by
over two percent. By lowering the threshold (clas-
sifying more passages as entailments), we increase
recall of entailed pairs without losing as much pre-
cision in non-entailed pairs since many have very
low scores. As a feature, this aspect of the score
distribution can be incorporated by the system, but
it cannot be used in a simple thresholding design.
6 Discussion
The random walk framework smoothes an initial
distribution of words into a much larger lexical
space. In one sense, this is similar to the technique
of query expansion used in information retrieval.
A traditional query expansion model extends a bag
of words (usually a query) with additional related
words. In the case of pseudo-relevance feedback,
Figure 1: Impact of number of walk steps on cor-
relation with MSR paraphrase judgments. The
left column shows absolute correlation across ten
resampled runs (y-axis) versus number of steps
taken (x-axis). The right column plots the mean
ratio of performance at step t (x-axis) versus per-
formance at convergence.
29
these words come from the first documents re-
turned by the search engine, but other modes of se-
lecting additional words exist. In the random walk
framework, this expansion is analogous to taking
only a single step of the random walk. Indeed,
in the case of the translation model introduced in
(Berger and Lafferty, 1999), they are mathemati-
cally equivalent. However, we have argued that the
walk is an effective global aggregator of related-
ness information. We can formulate the question
as an empirical one?does simulating the walk un-
til convergence really improve our representation
of the text document?
To answer this question, we extracted a 200
items subset of the MSR training data and trun-
cated the walk at each time step up until our con-
vergence threshold was reached at around 50 it-
erations. We then evaluated the correlation of
the walk score with the correct label from the
MSR data for 10 random resamplings of 66 doc-
uments each. Figure 1 plots this result for dif-
ferent distributional similarity measures. We ob-
serve that as the number of steps increases, per-
formance under most of the distributional similar-
ity measures improves, with the exception of the
asymmetric skew-divergence measure introduced
in (Lee, 2001).
This plot also gives some insight into the qual-
itative nature of the stability of the various distri-
butional measures for the paraphrase task. For in-
stance, we observe that the Jensen-Shannon score
and dice score tend to be the most consistent be-
tween runs, but the dice score has a slightly higher
mean. This explains in part why the dice score was
the best performing measure for the task. In con-
trast, cosine similarity was observed to perform
poorly here, although it was found to be the best
measure when combined with our textual entail-
ment system. We believe this discrepancy is due
in part to the feature scaling issues described in
section 5.2.
7 Final remarks
Notions of similarity have many levels of gran-
ularity, from general metrics for lexical related-
ness to application-specific measures between text
passages. While lexical relatedness is well stud-
ied, it is not directly applicable to text passages
without some surrounding environment. Because
this work represents words and passages as in-
terchangeable mathematical objects (teleport vec-
tors), our approach holds promise as a general
framework for aggregating local relatedness infor-
mation between words into reliable measures be-
tween text passages.
The random walk framework can be used to
evaluate changes to lexical resources because it
covers the entire scope of a resource: the whole
graph is leveraged to construct the final distribu-
tion, so changes to any part of the graph are re-
flected in each walk. This means that the meaning-
fulness of changes in the graph can be evaluated
according to how they affect these text similarity
scores; this provides a more semantically relevant
evaluation of updates to a resource than, for ex-
ample, counting how many new words or links be-
tween words have been added. As shown in Jar-
masz and Szpakowicz (2003), an updated resource
may have many more links and concepts but still
have similar performance on applications as the
original. Evaluations of WordNet extensions, such
as those in Navigli and Velardi (2005) and Snow et
al. (2006), are easily conducted within the frame-
work of the random walk.
The presented framework for text semantic sim-
ilarity with random graph walks is more general
than the WordNet-based instantiation explored
here. Transition matrices from alternative linguis-
tic resources such as corpus co-occurrence statis-
tics or larger knowledge bases such as Wikipedia
may very well add value as a lexical resource un-
derlying the walk. One might also consider tailor-
ing the output of the walk with machine learning
techniques like those presented in (Minkov and
Cohen, 2007).
References
E. Agirre and A. Soroa. 2009. Personalizing pagerank
for word sense disambiguation. In EACL, Athens,
Greece.
R. Bar-Haim, I. Dagan, B. Dolan, L. Ferro, D. Gi-
ampiccolo, B. Magnini, and I. Szpektor. 2006. The
2nd PASCAL recognizing textual entailment chal-
lenge. In PASCAL Challenges Workshop on RTE.
A. Berger and J. Lafferty. 1999. Information retrieval
as statistical translation. SIGIR 1999, pages 222?
229.
P. Berkhin. 2005. A survey on pagerank computing.
Internet Mathematics, 2(1):73?120.
A. Budanitsky and G. Hirst. 2006. Evaluating
wordnet-based measures of lexical semantic related-
ness. Computational Linguistics, 32(1):13?47.
30
N. Chambers, D. Cer, T. Grenager, D. Hall, C. Kiddon,
B. MacCartney, M. de Marneffe, D. Ramage, E. Yeh,
and C. D. Manning. 2007. Learning alignments and
leveraging natural logic. In ACL-PASCAL Workshop
on Textual Entailment and Paraphrasing.
K. Collins-Thompson and J. Callan. 2005. Query ex-
pansion using random walk models. In CIKM ?05,
pages 704?711, New York, NY, USA. ACM Press.
C. Corley and R. Mihalcea. 2005. Measuring the se-
mantic similarity of texts. In ACL Workshop on Em-
pirical Modeling of Semantic Equivalence and En-
tailment, pages 13?18, Ann Arbor, Michigan, June.
ACL.
J. R. Curran. 2004. From Distributional to Semantic
Similarity. Ph.D. thesis, University of Edinburgh.
I. Dagan, O. Glickman, and B. Magnini. 2005.
The PASCAL recognizing textual entailment chal-
lenge. In Quinonero-Candela et al, editor, MLCW
2005, LNAI Volume 3944, pages 177?190. Springer-
Verlag.
B. Dolan, C. Quirk, and C. Brockett. 2004. Unsu-
pervised construction of large paraphrase corpora:
Exploiting massively parallel news sources. In Col-
ing 2004, pages 350?356, Geneva, Switzerland, Aug
23?Aug 27. COLING.
C. Fellbaum. 1998. WordNet: An electronic lexical
database. MIT Press.
D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan.
2007. The 3rd PASCAL Recognizing Textual En-
tailment Challenge. In ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 1?9,
Prague, June.
O. Glickman, I. Dagan, and M. Koppel. 2005. Web
based probabilistic textual entailment. In PASCAL
Challenges Workshop on RTE.
T. H. Haveliwala. 2002. Topic-sensitive pagerank. In
WWW ?02, pages 517?526, New York, NY, USA.
ACM.
T. Hughes and D. Ramage. 2007. Lexical semantic
relatedness with random graph walks. In EMNLP-
CoNLL, pages 581?589.
M. Jarmasz and S. Szpakowicz. 2003. Roget?s the-
saurus and semantic similarity. In Proceedings of
RANLP-03, pages 212?219.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In ROCLING X, pages 19?33.
T.K. Landauer, P.W. Foltz, and D. Laham. 1998. An
introduction to latent semantic analysis. Discourse
Processes, 25(2-3):259?284.
L. Lee. 2001. On the effectiveness of the skew diver-
gence for statistical language analysis. In Artificial
Intelligence and Statistics 2001, pages 65?72.
M. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. ACM SIGDOC: Pro-
ceedings of the 5th Annual International Conference
on Systems Documentation, 1986:24?26.
V. I. Levenshtein. 1966. Binary Codes Capable
of Correcting Deletions, Insertions, and Reversals.
Ph.D. thesis, Soviet Physics Doklady.
C. Manning, P. Raghavan, and H. Schutze, 2008. In-
troduction to information retrieval, pages 258?263.
Cambridge University Press.
R. Mihalcea, C. Corley, and C. Strapparava. 2006.
Corpus-based and knowledge-based measures of
text semantic similarity. AAAI 2006, 6.
E. Minkov and W. W. Cohen. 2007. Learning to rank
typed graph walks: Local and global approaches. In
WebKDD and SNA-KDD joint workshop 2007.
G. Minnen, J. Carroll, and D. Pearce. 2001. Applied
morphological processing of English. Natural Lan-
guage Engineering, 7(03):207?223.
A. Montejo-R?aez, J.M. Perea, F. Mart??nez-Santiago,
M. A. Garc??a-Cumbreras, M. M. Valdivia, and
A. Ure?na L?opez. 2007. Combining lexical-syntactic
information with machine learning for recognizing
textual entailment. In ACL-PASCAL Workshop on
Textual Entailment and Paraphrasing, pages 78?82,
Prague, June. ACL.
R. Navigli and P. Velardi. 2005. Structural seman-
tic interconnections: A knowledge-based approach
to word sense disambiguation. IEEE Trans. Pattern
Anal. Mach. Intell., 27(7):1075?1086.
P. Resnik. 1999. Semantic similarity in a taxonomy:
An information-based measure and its application to
problems of ambiguity in natural language. JAIR,
(11):95?130.
R. Snow, D. Jurafsky, and A. Y. Ng. 2006. Semantic
taxonomy induction from heterogenous evidence. In
ACL, pages 801?808.
31
Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 49?57,
Portland, Oregon, June 2011. c?2011 Association for Computational Linguistics
Exploring the relationship between learnability and linguistic universals
Anna N. Rafferty (rafferty@cs.berkeley.edu)
Computer Science Division, University of California, Berkeley, CA 94720 USA
Thomas L. Griffiths (tom griffiths@berkeley.edu)
Department of Psychology, University of California, Berkeley, CA 94720 USA
Marc Ettlinger (marc@northwestern.edu)
Department of Communication Sciences and Disorders
Northwestern University, Evanston, IL 60208 USA
Abstract
Greater learnability has been offered as an ex-
planation as to why certain properties appear
in human languages more frequently than oth-
ers. Languages with greater learnability are
more likely to be accurately transmitted from
one generation of learners to the next. We ex-
plore whether such a learnability bias is suffi-
cient to result in a property becoming preva-
lent across languages by formalizing language
transmission using a linear model. We then
examine the outcome of repeated transmission
of languages using a mathematical analysis, a
computer simulation, and an experiment with
human participants, and show several ways in
which greater learnability may not result in a
property becoming prevalent. Both the ways
in which transmission failures occur and the
relative number of languages with and with-
out a property can affect whether the rela-
tionship between learnability and prevalence
holds. Our results show that simply finding
a learnability bias is not sufficient to explain
why a particular property is a linguistic univer-
sal, or even frequent among human languages.
1 Introduction
A comparison of languages around the world reveals
that certain properties are far more frequent than
others, which are taken to reflect linguistic univer-
sals (Greenberg, 1963; Comrie, 1981; Croft, 2002).
Understanding the origins of linguistic universals is
an important project for linguistics, and understand-
ing how they relate to human cognitive processes
is an important project for cognitive science. One
prominent explanation for the existence of these pat-
terns is the presence of cognitive biases that make
certain properties of language more easily learned
than others (Slobin, 1973; Wilson, 2003; Finley &
Badecker, 2007; Wilson, 2006). Under this hypothe-
sis, certain properties are common across languages
because they are more easily learned than others (a
learnability bias) and are therefore more likely to be
maintained when a language is passed from one gen-
eration to the next. These universals generally reflect
tendencies, rather than properties that are present in
each and every language (Croft, 2002).
Recent work in psycholinguistics has provided
support for a relationship between learnability bi-
ases and the properties that are prevalent in human
languages. A number of studies have shown that cer-
tain common phonological patterns, such as vowel
harmony, voicing agreement and final devoicing are,
indeed, more learnable than other unattested patterns
(Finley & Badecker, 2007; Moreton, 2008; Becker,
Ketrez, & Nevins, 2011). Based on these findings,
it is tempting to argue that learnability biases alone
might account for the prevalence of these proper-
ties in human languages. However, this argument
assumes that more accurate learning of a language
with a certain property is sufficient for that property
to become widespread across languages and does
not account for why a property might be prevalent
but not universal across languages.
In this paper, we examine the assumption that
greater learnability is sufficient for a property to be-
come prevalent. We formalize language transmis-
sion using a simple linear model, and then show two
basic scenarios in which greater learnability for a
particular language does not result in that language
becoming prevalent. We first perform a mathemat-
ical analysis to show that one way this can occur
is for errors in transmission to favor particular lan-
49
guages over others. We next use a simulation to
show another scenario in which greater learnabil-
ity can fail to result in a dominant pattern: when
the number of alternative languages is large. We
conduct two experiments with human participants to
illustrate the occurrence of this second scenario in
the case of a particular property of human language,
vowel harmony.
2 Linking Learnability and Transmission
Languages change over time due to transmission
from generation to generation (e.g., Labov, 2001).
Our goal is to understand how long-term trends of
language change are related to cognitive, perceptual,
and production biases observed in a single instance
of transmission. We begin by formalizing transmis-
sion using a general mathematical model in order
to uncover what long term trends emerge given that
certain languages are more likely to be accurately
transmitted than others.
We use a linear model of cultural transmission,
in which it is assumed that each person learns a lan-
guage from utterances produced by one person in the
previous generation. This linear model of transmis-
sion has many specific instantiations in the literature
on language evolution, such as the iterated learn-
ing model (Kirby, 2001; Griffiths & Kalish, 2007)
or the replicator dynamics (Schuster & Sigmund,
1983; Komarova & Nowak, 2003). To specify this
model, we first define the set of possible languages,
denoted H. Each element h ? H is one possible lan-
guage. Transmission occurs when a new member of
the population receives linguistic data (a set of utter-
ances) from another member of the population and
learns a language h ? H. We assume transmission
occurs only from one person to another person, and
that each person learns only one language. For ex-
ample, someone who knows language j might speak
to another member of the population, and based on
hearing those utterances, the learner might also learn
the language j. Alternatively, the learner might learn
another language: The learner might not have heard
enough language to fully specify j as the language
or might have misheard something, and thus simply
infers another language i that is consistent with the
data she or he heard. More generally, we assume
that for all i, j ? H, qi j is the probability that some-
Q =
?
?
0.90 0.05 0.07
0.04 0.76 0.08
0.06 0.19 0.85
?
?
(a)
(b) (c)
? =
?
?
0.39
0.20
0.41
?
?
This process may continue indefinitely, with the tth learner re-
ceiving the output of the (t ?1)th learner. The iterated learn-
ing models we analyze make the simplifying assumptions
that language evolution occurs in only one direction (previ-
ous generations do not change their hypotheses based on the
data produced by future generations) and that each learner re-
ceives input from only one previous learner. We first charac-
terize how learning occurs, independent of specific represen-
tation, and then give a more detailed d scription of the form
of these hypotheses and data.
Our models assume that learners represent (or act as if they
represent) the degree to which const ints predispose them to
certain hypotheses about language through a probability dis-
tribution over hypotheses, and that they combine these pre-
dispositions with information from the dat using Bayesian
inference. Starting with a prior distribution over hypotheses
p(h) for all hypotheses h in a hypothesis space H, the pos-
terior distribution over hypotheses given data d is giv n by
Bayes? rule,
p(h|d) =
p(d|h)p(h)
?h??H p(d|h?)p(h?)
(1)
where the likelihood p(d|h) indicates the probability of see-
ing d under hypothesis h. The learners thus shape the lan-
guage they are learning through their own bias s in the form
of the prior probabilities: the prior p(h) incorporates the hu-
man learning constraints. These probabilities might, for ex-
ample, tend to favor lword forms wi h alternating c so ant-
vowel phonemes. We assume that learners? expectations
about the distribution of the data given the hypothesis are
consistent with the actual distribution (i.e. that the probabil-
ity of the previous learner generating data d from hypothesis
h matches the likelihood function p(d|h)). Finally, we as-
sume that learners choose a hypothesis by sampling from the
posterior distribution (although we consider other ways of se-
lecting hypotheses in the Discussion section).1
The analyses we present in this paper are based on the ob-
servation that iterated learning defines a Markov chain. A
Markov chain is a sequence of random variables Xt such that
each Xt is independent of all preceding variables when condi-
tioned on the immediately preceding variable, Xt?1. Thus,
p(xt |x1, . . . ,xt?1) = p(xt |xt?1). There are several ways of
reducing iterated learning to a Markov chain (Griffiths &
Kalish, 2007). We will focus on the Markov chain on hy-
potheses, where transitions from one state to another occur
each generation: the tth learner assumes the data were gen-
erated by ht , where these data are dependent only on the
hypothesis ht?1 chosen by the previous learner. The transi-
tion probabilities for this Markov chain are obtained by sum-
ming over the data from the previous time step di?1, with
p(ht |ht?1) = ?di?1 p(ht |di?1)p(di?1|ht?1) (see Figure 1).
Identifying iterated learning as a Markov chain allows us to
draw on mathematical results concerning the convergence of
1Note that these various probabilities form our model of the
learners. Learners need not actually hold them explicitly, nor per-
form the exact computations, provided that they act as if they do.
data ...
(a)
(c)
...(b)
...
data
hypothesis hypothesis
data
?d p(h|d)p(d|h)
d0 h2d1h1
p(h|d) p(d|h) p(h|d) p(d|h)
d2
h2h1
?d p(h|d)p(d|h)
Figure 1: Language evolution by iterated learning. (a) Each
learner sees data, forms a hypothesis, and generates the data
provided to the next learner. (b) The underlying stochastic
process, with dt and ht being the data generated by the tth
learner and the hypothesis selected by that learner respec-
tively. (c) We consider the Markov chain over hypotheses
formed by summing over the data variables. All learners
share the same prior p(h), and each learner assumes the input
data were created using the same p(d|h).
Markov chains. In particular, Markov chains can converge to
a stationary distribution, meaning that after some number of
generations t, the marginal probability that a variable Xt takes
value xt becomes fixed and independent of the value of the
first variable in the chain (Norris, 1997). Intuitively, the sta-
tionary distribution is a distribution over states in which the
probability of each state is not affected by further iterations
of the Markov chain; in our case, the probability that a learner
learns a specific grammar at time t is equal to the probability
of any future learner learning that grammar. The stationary
distribution is thus an equilibrium state that iterated learn-
ing will eventually reach, regardless of the hypothesis of the
first ancestral learner, provided simple technical conditions
are satisfied (see Griffiths & Kalish, 2007, for details).
Previous work has shown that the stationary distribution
of the Markov chain defined by Bayesian learners sampling
from the posterior is the learners? prior distribution over hy-
potheses, p(h) (Griffiths & Kalish, 2007). These results illus-
trate how constraints on learning can influence the languages
that people come to speak, indicating that it is possible for
iterated learning to converge to an equilibrium that is deter-
mined by these constraints and independent of the language
spoken by the first learner in the chain.
However, characterizing the stationary distribution of iter-
ated learning still leaves open the question of whether enough
generations of learning have occurred for convergence to this
distribution to have taken place in human languages. To un-
derstand the degree to which linguistic universals reflect con-
straints on learning rather than descent from a common ances-
tor, it is necessary to establish bounds on convergence time.
Previous work has identified factors influencing the rate of
convergence in very simple settings (e.g., Griffiths & Kalish,
2007). Our contribution is to provide analytic upper bounds
on the convergence time of iterated learning with relatively
complex representations of the structure of a language that
are consistent with linguistic theories.
Q
language language
t
Figure 1: (a) A general model of the cultural transmis-
sion of languages. A language is passed from one learner
to another, and the matrix Q encodes the probability a
learner will learn a particular language i from someone
who knows language j. (b) An example transition matrix
Q with three states. (c) The solution to the eigenvector
equation Qpi = pi for this transition matrix. pi gives the
equilibrium probability that a learner will learn a particu-
lar language when languages are transmitted via a process
that has transition matrix Q.
one will learn language i from someone who knows
language j. These can be encoded in a transition
matrix Q where the (i, j)th entry of the matrix cor-
responds to qi j (see Figure 1).
Using this framework, we can formally define
learnability biases and determine whether learn-
ability bias for some property necessarily implies
that this property will be present in the majority
of languages. As mentioned previously, we define
learnability bias to mean that one type of language
is more likely to be transmitted accurately to the next
generation than another; this is similar to the notion
of ?cognitive bias? discussed in Wilson (2003) and
is what is tested in experiments. Formally, a learn-
ability bias for some language i over some other
language j means that qii > q j j. For example, one
might expose one group of learners to language i and
another group to language j. If more le rners in the
first group accurately learned the language they were
exposed to, this would indicate a learnability bias for
language i over language j.
We can extend the idea of a learnability bias to
a property of a language, rather than a specific lan-
guage, by applying a similar definition to sets of lan-
guages. Imagine there are two sets of languages, H1
and H2. These sets might be defined by classifying
50
all languages with a particular property in H1 and
all languages without the property in H2. One way
of defining a learnability bias that favors a particular
property is for each language with that property to be
more likely to be transmitted successfully than each
language without that property. That is, for all pos-
sible pairs i ? H1 and j ? H2, qii > q j j. This would
indicate a general learnability bias for languages in
H1 over languages in H2.
Using this definition of a learnability bias, we can
determine whether such a bias is sufficient to estab-
lish that the property will be present in the majority
of languages. That is, if H1 denotes the languages
with the property of interest, we want to determine
whether a learnability bias for languages in H1 im-
plies that after many generations, the majority of the
languages in the population will be in H1 and not
in H2. We can determine the consequences of many
instances of language transmission in this model by
appealing to existing results on the equilibrium of
this linear dynamical system. As mentioned above,
this linear transmission model is related to two kinds
of models that have been used to study language
evolution: If we assume that learners are organized
in a chain, this linear model is called iterated learn-
ing (Kirby, 2001); alternatively, if we assume that
there are an infinite number of learners in the pop-
ulation, the model is called the replicator dynam-
ics (Schuster & Sigmund, 1983). In either case, the
probability that a learner will learn language h, as-
suming the population has reached equilibrium, is
given by the solution to the eigenvector equation
Qpi = pi, normalized such that ?ni=1pii = 1 (for de-
tails, see Griffiths & Kalish, 2007). For languages in
H1 to occur the majority of the time, it thus must be
the case that ?h?H1 pih > ?h?H2 pih.
We can now identify one context in which a learn-
ability bias is not sufficient to ensure that a property
will appear in the majority of languages. Consider
the example transition matrix Q shown in Figure 1
(b). Let H1 = {s1} and H2 = {s2,s3}, where each
state si represents a distinct language. We have that
q11 > qii for all i ? H2: each state in H2 has a lower
self transition probability than state s1, the only state
in H1. Thus, we have a learnability bias for state
s1 over all states in H2. However, the eigenvector
pi shown in Figure 1 (c) indicates that the equilib-
rium of this system, which will be reached after lan-
guages are transmitted from person to person many
times, favors state s3 over the other states. Overall,
?h?H1 pih = 0.39 while ?h?H2 pih = 0.61: most of thelearners will learn a language in H2.1
Intuitively, this result comes from the fact that
transmission failures tend to favor languages in H2.
A learner who learns from someone who speaks a
language i in H2 will rarely learn the language in
H1, although she may learn a different language than
i in H2. This pattern of transmission failures over-
whelms the learnability bias that the language in H1
has over the languages in H2. Note that this pattern
holds even given that q1i > qi1 for all i?H2, another
common criterion for a learnability bias.
This result implies that if the linear transmission
model is an accurate model for understanding hu-
man language evolution, then it is not sufficient to
compare how accurately languages are maintained
over a single generation in order to predict what
trends will emerge after many generations. Instead,
one must also look at what happens when languages
are not maintained accurately. The ways in which
mutations occur may be as important as the relative
fidelities of transmission in determining long term
trends. When one only looks for a learnability bias,
the rate of different mutations is not accounted for,
leaving open the possibility that predictions about
long term trends will be incorrect.
3 Simulating Language Transmission
In the previous section, we used a simple linear
transmission model to identify one context in which
a learnability bias is not sufficient for languages with
a certain property to become prevalent. We now ex-
plore a second context in which a learnability bias
is not sufficient to guarantee that languages with a
particular property become prevalent, using a sim-
ulation of language transmission. We use an iter-
ated learning model in which our representation of
language is inspired by the principles and parame-
ters approach (Chomsky & Lasnik, 1993). Rafferty,
Griffiths, and Klein (2009) present a model similar
to the one we consider here and show that compa-
1While one might try to resolve this issue by collapsing all
languages in H2 into a single state in the Markov chain, such
a transformation is possible only in cases where qi j = qik for
all languages j,k ? H2 and i /? H2 (Burke & Rosenblatt, 1958;
Kemeny & Snell, 1960).
51
0 200 400 600 800 10000
512
1024
Lang
uage
Samples From Transition Matrix
0 256 512 768 10240
3500
7000
Freq
uenc
y
Language Frequency
Other Target0
5000
10000
Freq
uenc
y
Relative Frequency of Target Language
0 200 400 600 800 10000
512
1024
Sample Iteration
Lang
uage
0 256 512 768 1024
3500
0
7000
Language
Freq
uenc
y
Other Target0
5000
10000
Freq
uenc
y
? = 0.6
? = 0.1
Figure 2: Model results for the frequency of the target language based on adjusting the bias towards that hypothesis.
The rows in the above figure correspond to two possible values of ?; larger ? results in a higher prior probability on
the target language. The leftmost column shows 1,000 samples from the transition matrix, with black x marks corre-
sponding to occurrences of the target language. The middle column corresponds to the frequency of each language in
the full 10,000 samples; the rightmost bar in each figure corresponds to the target language. The rightmost column
shows the frequency of the target language versus all other languages for the same 10,000 samples.
rable results hold using other representations of lan-
guage, such as those based on optimality theory.
In order to define the transition matrix Q, we need
to specify the process by which learners select a lan-
guage. We assume that learners are Bayesian, mean-
ing that they infer a language h based on the data d
that they receive according to Bayes? rule. The pos-
terior probability assigned to h after observing d is
p(h|d) ? p(d|h)p(h), where p(d|h) (the likelihood)
indicates the probability of d being generated from
h, and p(h) (the prior) indicates the extent to which
the learner was biased towards h before observing d.
If we assume learners select hypotheses with proba-
bility equal to their posterior probability, we obtain
a transition matrix Q with entries
qi j = p(h(t+1) = i|h(t) = j)
=?
d
p(h(t+1) = i|d)p(d|h(t) = j)
where h(t) and h(t+1) are the languages of learners at
iterations t and t +1 respectively.
To represent languages, we use binary vectors of
length N. Each place corresponds to the setting for
a particular parameter. We consider one particular
setting of the parameters to be the target language
and include a learnability bias for this language in
the model; we then look at whether this language
is more prevalent than other languages after many
transmissions. In the iterated learning model that we
use, learners are organized into a chain, with each
learner learning from data generated by the previous
learner (Kirby, 2001). The previous learner gener-
ates k pieces of data that match her or his language.
These pieces of data each specify the correct param-
eter setting for one of the properties represented by
the binary vector. The other N?k properties are left
unspecified in the data given to the next learner.
In order to define the transition probability be-
tween languages, we need to define the two terms
in Bayes rule: the prior p(h) and the likelihood
p(d|h). Intuitively, the prior probability distribution
over languages corresponds to how much evidence
is required for the learner to learn each hypothesis.
If one hypothesis has a very high prior probability,
only a small amount of evidence will be required to
convince the learner that that hypothesis is the cor-
rect one. By controlling the prior probability of the
target language versus the other languages, we can
manipulate the learnability bias for the target lan-
guage. We thus set the prior probability of the target
language to ? and then divide the remaining proba-
bility mass of 1?? uniformly across all of the lan-
guages (including the target language). The param-
eter ? thus controls the strength of the learnability
bias for the target language, but this language is al-
ways favored for any ? greater than 0.
The likelihood p(d|h) reflects the probability that
a given hypothesis h would produce data d. We as-
sume d is a string of length N that contains 0s, 1s,
and ?s. A ??? in the ith position means that no in-
formation was given about the ith property. We also
assume there is a probability ? that the chosen lan-
guage will not match the data at each position; that
is, with probability ?, the language chosen by the
52
learner will have a 1 in the ith spot if the data had a
0 in that spot. This gives:
p(d|h) = ?Ni=1,di 6=? ?I(h`di)(1? ?)I(h0di)
where h ` di means that h has the same setting of the
ith property as di.
Given these specifications for the prior and the
likelihood, we can calculate the 2N ? 2N transition
matrix and sample from this matrix to simulate a
sequence of learners each learning a language from
the utterances produced by the previous learner. We
let N = 10 and k = 5. As shown in Griffiths and
Kalish (2007), in this model ? iterated learning with
Bayesian learners ? the equilibrium pi is simply the
prior distribution p(h). The distribution over lan-
guages is thus unaffected by the error parameter ?;
this parameter only affects the time to reach equilib-
rium (Rafferty et al, 2009). We present results using
? = 0.25. Figure 2 shows how relative frequency of
the target language is affected by changing the pa-
rameter ?, using ? = 0.6 and ? = 0.1. Frequencies
are based on taking 11,000 samples from the ma-
trix and discarding the first 1,000 to ensure that the
population had reached equilibrium.
The middle column of Figure 2 shows that the
frequency with which learners chose the target lan-
guage was greater than that of the other languages
for both values of ?. This is consistent with the
target language having a higher prior probability
than other languages. However, depending on the
strength of the bias, this language may still not be
chosen the majority of the time, as shown in the
rightmost column of Figure 2. When ? is large,
its probability overwhelms that of its competitors.
However, if ? is relatively small, the combined fre-
quencies of all other languages exceed that of the
target language. Thus, despite being favored by a
learnability bias, the target language is not chosen by
the majority of learners. Like the previous example,
this simulation demonstrates that learnability biases
may not always lead to accurate prediction of long
term trends. More specifically, it highlights that one
must consider the size of the comparison set: If there
are many alternate possible languages, learners may
tend to learn one of these languages even if some
particular language with a learnability bias is more
frequent than any other given individual language.
4 Language Transmission in the Lab
While we have shown two scenarios in which a sim-
ple linear transmission model does not predict that
learnability biases will necessarily lead to linguistic
universals, human learners are not necessarily con-
sistent with this model and could follow a differ-
ent pattern. Thus, we conducted two experiments
to determine if the same dissociation between in-
dividual bias and long-term change can be shown
when teaching human learners an artificial gram-
mar. In Experiment 1, we establish a learnability
bias for a linguistic pattern that is common in the
world?s languages over an arbitrary pattern. In Ex-
periment 2, we explore what happens when a lan-
guage with the common pattern is transmitted mul-
tiple times among learners in the lab. Each learner
learns a language and then produces data from this
language to teach the next learner. By examining the
languages that emerge after several transmissions,
we will show that the learnability bias in Experi-
ment 1 does not translate to the pattern becoming
widespread across the learned languages in Experi-
ment 2. This pattern is an instance of the scenario
in which the many alternative languages overwhelm
the language with the learnability bias.
In our experiments, we use the property of vowel
harmony. Relatively common across the world?s
languages (van der Hulst & van de Weijer, 1995),
vowel harmony is a linguistic pattern wherein the
vowels in words in a language must share some
phonological feature. For example, in Turkish, the
plural suffix is -lar in bash-lar ?heads?, but -ler in
bebek-ler ?babies? so as to adhere to the requirement
that words are front-back harmonic. In the former,
both vowels are back vowels and in the latter, both
vowels are front vowels. Harmony is well-suited for
use in this case because English speakers have no fa-
miliarity with vowel harmony from their native lan-
guage input and because previous work has shown
that typologically attested vowel harmony patterns
are generally more easily learned (Moreton, 2008;
Finley & Badecker, 2009).
5 Experiment 1: Establishing a Bias
5.1 Methods
Participants. There were 40 participants who
received either monetary compensation or course
53
credit for their participation. All were native speak-
ers of English.
Stimuli. A native speaker of English was recorded
saying 160 CVCVC words. Each word began with
one of 80 CVC stems, twenty each with the vow-
els /i/, /e/, /u/ and /o/ and random consonants.
Each stem was recorded with both variants, or al-
lomorphs, of a suffix, [it] and [ut]. Thus, half the
words were front-harmonic (e.g., pel-it, bis-it) and
half were front-disharmonic (e.g., pel-ut, bis-ut).
Procedure. The procedure followed a modified arti-
ficial grammar paradigm. Participants were assigned
to one of two conditions: the harmonic condition
or the height-front dependency condition, which is
unattested. In both conditions, participants were ex-
posed in training to 40 words from the language
they were learning. In the harmonic condition, 40
harmonic words were selected. In the height-front
dependency condition, words were selected such
that mid-vowel stems received the front vowel suffix
(e.g., pel-it, bod-it) and high-vowel stems received
the back-vowel suffix (e.g., bis-ut, tug-ut). This rule
was chosen arbitrarily from the space of possible
languages to test the hypothesis that vowel harmony
would have a learnability bias over other patterns.
Participants were familiarized with the words in
the same way regardless of condition. They were
given alternating blocks of passive listening and
blocks in which for each trial, two words were
played and they were required to choose which word
they had previously heard. In the forced choice tri-
als, the choice was between a word that had been
played in the passive listening section and a word
with the same prefix and the alternate allomorph. A
total of five blocks of 40 trials each were included in
training: three passive listening blocks with a forced
choice block in between each.
Following the training trials, participants com-
pleted one block of 80 test trials. On each test
trial, participants were asked to choose which of
two words they thought was from the language they
had learned in the training trials. In each trial, the
two words both had the same stem and differed in
the suffix. 40 of the test trials included words from
training, and 40 were generalization trials involving
novel words.
Height?Frontness Harmony0
0.5
1
Proporti
on of Ge
neraliza
tions
Proportion of Generalizations Following Training Set Rule   
Height?Frontness Harmony0
0.5
1
Test Ac
curacy
Test Accuracy by Training Set Rule
Figure 3: Results for harmonic versus height-frontness
rule conditions. By condition, there are significant dif-
ferences in the proportion of generalizations following
the rule (0.70 for harmony rule versus 0.57 for height-
frontness rule, t(38) = 2.05, p < 0.05; left) and in test
accuracy (0.80 for harmony rule versus 0.68 for height-
frontness rule, t(38) = 2.23, p< 0.05; right).
5.2 Results
As shown in Figure 3, we found a learnability bias
for the harmonic language. Learners had signifi-
cantly greater accuracy in test when they learned the
vowel harmonic language than when they learned
the height-front dependency language (80% correct
for learners of the harmony rule versus 68% cor-
rect for the height-frontness rule, t(38) = 2.23, p <
0.05). Additionally, 70% of generalizations made
by learners in the harmony rule condition followed
the harmonic rule while only 57% of generaliza-
tions made by learners in the height-front depen-
dency condition followed the height-frontness rule
(t(38) = 2.05, p < 0.05).2 The result of these two
phenomena was that the final languages produced by
the learners in the harmony condition had a greater
prevalence of harmonic words than the final lan-
guages of learners in the height-frontness depen-
dency had of adhering words.
These results establish that the probability of tran-
sitioning from a harmonic language to another lan-
guage with a high proportion of harmonic words
is higher than the probability of transitioning from
a height-front dependency language to another lan-
guage with a high proportion of adhering words. In
2For the second experiment, participants who had low ac-
curacy (< 62.5% of previously heard words chosen in test as
?from the language?) were excluded. Performing this exclusion
in this experiment preserves the same results: Mean accuracy
of 87% for the harmonic condition versus 73% for the height-
front dependency condition (t(28) = 2.74, p< 0.025), and 77%
mean proportion of generalizations following the rule for the
harmonic condition versus 58% for the height-front dependency
condition (t(28) = 2.43, p< 0.025). This exclusion criterion re-
sulted in removing five participants from each condition.
54
terms of the transition matrix, this corresponds to
q`harm,`harm > q`h-f,`h-f , where `harm is the set of lan-
guages with a high proportion of harmonic words
and `h-f is the set of languages with a high propor-
tion of words that follow the height-frontness rule.
In other words, the harmonic language is easier to
learn than the height-front dependency language.
6 Experiment 2: Language Transmission
6.1 Methods
Participants. There were a total of 104 partici-
pants who received either monetary compensation or
course credit for their participation. All were native
speakers of English.
Stimuli. The same stimuli were used as in Experi-
ment 1.
Procedure. The procedure for this experiment was
similar to Experiment 1, but the way that words were
chosen for training differed. For the first subject in
each chain, a total of 40 prefixes were selected at
random, and based on the starting condition of the
chain, the allophone for each prefix was selected.
For example, for the 50% harmonic starting con-
dition, 40 prefixes were chosen and of those pre-
fixes, half were chosen to have the appropriate al-
lophone to make the word harmonic and half were
chosen to have the allophone to make the word non-
harmonic. For subsequent subjects in each chain,
40 words were chosen at random from those words
which the previous subject had said was in the lan-
guage. In order to exclude subjects who had not ac-
tually learned the language in training, subjects were
not included in the chain if their accuracy in test on
previously seen words was below 62.5%; this is the
lowest level of accuracy that is significantly differ-
ent (binomial test, p < 0.05) from chance guessing.
Chains were started at 100%, 75%, 50%, 25%, and
0% harmonic. One chain with 10 subjects was run
for each starting point except for 100%. Four chains
of 10 subjects each were run at this starting point as
this is the point of most interest: given a learnabil-
ity bias, does the percentage of harmonic words in a
language remain consistently large?
6.2 Results
While Experiment 1 showed a learnability bias for
the harmonic language over an arbitrarily chosen
language, the iterated learning chains in Experiment
2 did not favor the harmonic language. As shown
in Figure 4, all chains tended toward languages with
approximately 50% harmonic words, and after sev-
eral generations, the chains that began with 100%
harmonic words did not differ significantly from the
other chains. There is also no difference in accuracy
on the harmonic items over time, as shown in Figure
5. This is empirical evidence that the pattern shown
in simulation can also occur with human learners:
One language is more accurately transmitted than
others, but due to the large number of other possi-
ble languages, this language does not predominate
after many transmissions.
7 General Discussion
In this paper, we formalized language transmission
using a linear model in order to examine whether
a learnability bias for some property of language is
sufficient for that property to become prevalent in
human languages. We showed two ways in which
a learnability bias for a property can exist but not
cause that property to become prevalent. First, using
a mathematical analysis, we showed that this can oc-
cur when transmission failures favor languages other
than those that have greater learnability. This illus-
trates the importance of considering the entire trans-
mission matrix, not just the probabilities of accurate
transmissions that are considered when establishing
a learnability bias.
Second, we showed that it is possible for the sheer
number of other possible languages to overwhelm
greater learnability for a particular language. We
then illustrated that this second scenario might lead
to incorrect predictions in an experimental context.
In artificial language experiments, greater learnabil-
ity is often established by comparing the accuracy
of transmission for a language with the property of
interest to an arbitrary language. However, in our
experiment, we established such a learnability bias
for vowel harmony, but this did not result in vowel
harmony being maintained after many instances of
transmission. This result seems to be due to the fact
that numerous languages other than harmonic lan-
guages were possible, so learners tended to learn one
of these many other languages.
One limitation of our analysis is the use of the
55
1 2 3 4 5 6 7 8 9 100.2
0.4
0.6
0.8
1.0
Prop
ortio
n Ha
rmon
ic Proportion of Harmonic Words Chosen in Test
 
 100%75%50%25%0%
1 2 3 4 5 6 7 8 9 100.2
0.4
0.6
0.8
1.0 Proportion of Generalizations in Test that were Harmonic
Iteration in ChainP
ropo
rtion
 Har
mon
ic
 
 
100%75%50%25%0%
Figure 4: Iterated learning chain results. Dotted lines show the two-tailed 95% confidence interval for chance re-
sponding; confidence intervals differ between the two graphs because there are 40 opportunities to generalize versus
80 opportunities to choose harmonic words.
1 2 3 4 5 6 7 8 9 100.2
0.40.6
0.81.0
Accu
racy
Accuracy for Harmonic Words
 
 
1 2 3 4 5 6 7 8 9 100.2
0.40.6
0.81.0
Iteration in Chain
Accu
racy
Accuracy for Non?Harmonic Words
 
 100%75%50%25%0%
100%75%50%25%0%
Figure 5: Accuracy on harmonic versus non-harmonic words by iteration. Overall, there is no difference in accuracy.
simple linear transmission model, in which each
learner learns from one member of the previous gen-
eration. It is easy to imagine variants on this model
that make more realistic assumptions about cultural
transmission of languages. However, we suspect
that these more complex models would not alter the
conclusions that we have drawn here. For exam-
ple, learning from multiple members of the previous
generation tends to dilute the effects of learnability
on the languages produced by a population (Smith,
2009; Burkett & Griffiths, 2010).
Overall, the result of a more complicated relation-
ship between learnability biases and linguistic uni-
versals is congruent with the evidence that all lan-
guages do not exhibit all properties for which learn-
ability biases have been found. Indeed, in histori-
cal linguistics, the general principle is one of lan-
guage divergence, rather than convergence on some
universal language (e.g., Greenberg, 1971). Given
this relationship, one must rethink using experimen-
tal evidence for particular learnability biases to ex-
plain linguistic tendencies. Instead, one must either
estimate all of the values in the transmission matrix,
or actually simulate the process of multiple trans-
missions in the lab to establish whether a particu-
lar property with a learnability bias is actually main-
tained over many generations. While this process is
dependent on assuming a particular model of how
transmission occurs in populations, such as the lin-
ear iterated learning paradigm we used in our exper-
iments, it provides a way of understanding what mu-
tations are likely to occur and of exploring the long
term trends that result from particular learnability bi-
ases. As we showed for vowel harmony, long term
trends may not match what one predicted based on a
learnability bias. Given such a result, one must look
to factors other than the learnability bias to explain
why a property is common across languages.
Acknowledgements. This work was supported by an
NSF Graduate Research Fellowship to ANR, grant num-
ber BCS-0704034 from the NSF to TLG, and grant num-
ber T32 NS047987 from the NIH to ME.
56
References
Becker, M., Ketrez, N., & Nevins, A. (2011). The surfeit
of the stimulus: Analytic biases filter lexical statistics
in turkish laryngeal alternations. Language, 87(1), 84-
125.
Burke, C. J., & Rosenblatt, M. (1958). A Markovian
function of a Markov chain. The Annals of Mathemat-
ical Statistics, 29(4), 1112?1122.
Burkett, D., & Griffiths, T. (2010). Iterated learning
of multiple languages from multiple teachers. In The
Evolution of Language: Proceedings of the 8th Inter-
national Conference (EVOLANG8).
Chomsky, N., & Lasnik, H. (1993). The theory of prin-
ciples and parameters. In J. Jacobs, A. von Stechow,
W. Sternefeld, & T. Vannemann (Eds.), Syntax: An in-
ternational handbook of contemporary research (pp.
506?569). Berlin: Walter de Gruyter.
Comrie, B. (1981). Language universals and linguistic
typology. Chicago: University of Chicago Press.
Croft, W. (2002). Typology and universals. Cambridge
University Press.
Finley, S., & Badecker, W. (2007). Towards a substan-
tively biased theory of learning. Berkeley Linguistics
Society, 33.
Finley, S., & Badecker, W. (2009). Artificial language
learning and feature-based generalization. Journal of
Memory and Language, 61, 423?437.
Greenberg, J. (Ed.). (1963). Universals of language.
Cambridge, MA: MIT Press.
Greenberg, J. (1971). Language, culture, and communi-
cation. Stanford: Stanford University Press.
Griffiths, T. L., & Kalish, M. L. (2007). A Bayesian view
of language evolution by iterated learning. Cognitive
Science, 31, 441-480.
Kemeny, J., & Snell, J. (1960). Finite markov chains.
Princeton, NJ: van Nostrand.
Kirby, S. (2001). Spontaneous evolution of linguistic
structure: An iterated learning model of the emergence
of regularity and irregularity. IEEE Journal of Evolu-
tionary Computation, 5, 102-110.
Komarova, N. L., & Nowak, M. A. (2003). Language
dynamics in finite populations. Journal of Theoretical
Biology, 221, 445-457.
Labov, W. (2001). Principles of linguistic change. Vol-
ume II: Social Factors. Blackwell.
Moreton, E. (2008). Analytic bias and phonological ty-
pology. Phonology, 25(1), 83?127.
Rafferty, A. N., Griffiths, T. L., & Klein, D. (2009).
Convergence bounds for language evolution by iterated
learning. Proceedings of the Thirty-First Annual Con-
ference of the Cognitive Science Society.
Schuster, P., & Sigmund, K. (1983). Replicator dynam-
ics. Journal of Theoretical Biology, 100(3), 533 - 538.
Slobin, D. (1973). Cognitive prerequisites for the acqui-
sition of grammar. In C. Ferguson & D. Slobin (Eds.),
Studies of child language development (pp. 173?208).
Smith, K. (2009). Iterated learning in populations of
Bayesian agents. In Proceedings of the 31st Annual
Conference of the Cognitive Science Society.
van der Hulst, H., & van de Weijer, J. (1995). Vowel har-
mony. In J. Goldsmith (Ed.), The Handbook of Phono-
logical Theory (pp. 495?534). Blackwell.
Wilson, C. (2003). Experimental investigation of phono-
logical naturalness. Proceedings of the 22nd West
Coast Conference on Formal Linguistics.
Wilson, C. (2006). Learning phonology with substan-
tive bias: An experimental and computational study of
velar palatalization. Cognitive Science, 30, 945?982.
57
