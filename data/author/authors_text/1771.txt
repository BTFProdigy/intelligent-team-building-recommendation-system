Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 314?323,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Sampling Alignment Structure under a Bayesian Translation Model
John DeNero, Alexandre Bouchard-Co?te? and Dan Klein
Computer Science Department
University of California, Berkeley
{denero, bouchard, klein}@cs.berkeley.edu
Abstract
We describe the first tractable Gibbs sam-
pling procedure for estimating phrase pair
frequencies under a probabilistic model of
phrase alignment. We propose and evalu-
ate two nonparametric priors that successfully
avoid the degenerate behavior noted in previ-
ous work, where overly large phrases mem-
orize the training data. Phrase table weights
learned under our model yield an increase in
BLEU score over the word-alignment based
heuristic estimates used regularly in phrase-
based translation systems.
1 Introduction
In phrase-based translation, statistical knowledge
of translation equivalence is primarily captured by
counts of how frequently various phrase pairs occur
in training bitexts. Since bitexts do not come seg-
mented and aligned into phrase pairs, these counts
are typically gathered by fixing a word alignment
and applying phrase extraction heuristics to this
word-aligned training corpus. Alternatively, phrase
pair frequencies can be learned via a probabilistic
model of phrase alignment, but this approach has
presented several practical challenges.
In this paper, we address the two most signifi-
cant challenges in phrase alignment modeling. The
first challenge is with inference: computing align-
ment expectations under general phrase models is
#P-hard (DeNero and Klein, 2008). Previous phrase
alignment work has sacrificed consistency for effi-
ciency, employing greedy hill-climbing algorithms
and constraining inference with word alignments
(Marcu and Wong, 2002; DeNero et al, 2006; Birch
et al, 2006). We describe a Gibbs sampler that con-
sistently and efficiently approximates expectations,
using only polynomial-time computable operators.
Despite the combinatorial complexity of the phrase
alignment space, our sampled phrase pair expecta-
tions are guaranteed to converge to the true poste-
rior distributions under the model (in theory) and do
converge to effective values (in practice).
The second challenge in learning phrase align-
ments is avoiding a degenerate behavior of the gen-
eral model class: as with many models which can
choose between large and small structures, the larger
structures win out in maximum likelihood estima-
tion. Indeed, the maximum likelihood estimate of
a joint phrase alignment model analyzes each sen-
tence pair as one large phrase with no internal struc-
ture (Marcu andWong, 2002). We describe two non-
parametric priors that empirically avoid this degen-
erate solution.
Fixed word alignments are used in virtually ev-
ery statistical machine translation system, if not to
extract phrase pairs or rules directly, then at least
to constrain the inference procedure for higher-level
models. We estimate phrase translation features
consistently using an inference procedure that is not
constrained by word alignments, or any other heuris-
tic. Despite this substantial change in approach, we
report translation improvements over the standard
word-alignment-based heuristic estimates of phrase
table weights. We view this result as an important
step toward building fully model-based translation
systems that rely on fewer procedural heuristics.
2 Phrase Alignment Model
While state-of-the-art phrase-based translation sys-
tems include an increasing number of features,
translation behavior is largely driven by the phrase
pair count ratios ?(e|f) and ?(f |e). These features
are typically estimated heuristically using the counts
c(?e, f?) of all phrase pairs in a training corpus that
are licensed by word alignments:
?(e|f) =
c(?e, f?)
?
e? c(?e
?, f?)
.
314
Gracias
,
lo
har?
de
muy
buen
grado
.
you
do so
Thank , I shall
gladly
.
you
do so
Thank , I shall
gladly
.
Gracias
,
lo
har?
de
muy
buen
grado
.
(a) example word alignment (b) example phrase alignment
Figure 1: In this corpus example, the phrase
alignment model found the non-literal translation
pair ?gladly, de muy buen grado? while heuristically-
combined word alignment models did not. (a) is a grow-
diag-final-and combined IBM Model 4 word alignment;
(b) is a phrase alignment under our model.
In contrast, a generative model that explicitly
aligns pairs of phrases ?e, f? gives us well-founded
alternatives for estimating phrase pair scores. For
instance, we could use the model?s parameters as
translation features. In this paper, we compute the
expected counts of phrase pairs in the training data
according to our model, and derive features from
these expected counts. This approach endows phrase
pair scores with well-defined semantics relative to a
probabilistic model. Practically, phrase models can
discover high-quality phrase pairs that often elude
heuristics, as in Figure 1. In addition, the model-
based approach fits neatly into the framework of sta-
tistical learning theory for unsupervised problems.
2.1 Generative Model Description
We first describe the symmetric joint model of
Marcu and Wong (2002), which we will extend. A
two-step generative process constructs an ordered
set of English phrases e1:m, an ordered set of for-
eign phrases f1:n, and a phrase-to-phrase alignment
between them, a = {(j, k)} indicating that ?ej , fk?
is an aligned pair.
1. Choose a number of components ` and generate
each of ` phrase pairs independently.
2. Choose an ordering for the phrases in the for-
eign language; the ordering for English is fixed
by the generation order.1
1We choose the foreign to reorder without loss of generality.
In this process, m = n = |a|; all phrases in both
sentences are aligned one-to-one.
We parameterize the choice of ` using a geometric
distribution, denoted PG, with stop parameter p$:
P (`) = PG(`; p$) = p$ ? (1 ? p$)
`?1 .
Each aligned phrase pair ?e, f? is drawn from a
multinomial distribution ?J which is unknown. We
fix a simple distortion model, setting the probability
of a permutation of the foreign phrases proportional
to the product of position-based distortion penalties
for each phrase:
P (a|{?e, f?}) ?
?
a?a
?(a)
?(a = (j, k)) = b|pos(ej)?pos(fk)?s| ,
where pos(?) denotes the word position of the start
of a phrase, and s the ratio of the length of the En-
glish to the length of the foreign sentence. This po-
sitional distortion model was deemed to work best
by Marcu and Wong (2002).
We can now state the joint probability for a
phrase-aligned sentence consisting of ` phrase pairs:
P ({?e, f?}, a) = PG(`; p$)P (a|{?e, f?})
?
?e,f?
?J(?e, f?) .
While this model has several free parameters in ad-
dition to ?J, we fix them to reasonable values to fo-
cus learning on the phrase pair distribution.2
2.2 Unaligned Phrases
Sentence pairs do not always contain equal informa-
tion on both sides, and so we revise the generative
story to include unaligned phrases in both sentences.
When generating each component of a sentence pair,
we first decide whether to generate an aligned phrase
pair or, with probability p?, an unaligned phrase.3
Then, we either generate an aligned phrase pair from
?J or an unaligned phrase from ?N, where ?N is a
multinomial over phrases. Now, when generating
e1:m, f1:n and alignment a, the number of phrases
m+ n can be greater than 2 ? |a|.
2Parameters were chosen by hand during development on a
small training corpus. p$ = 0.1, b = 0.85 in experiments.
3We strongly discouraged unaligned phrases in order to
align as much of the corpus as possible: p? = 10?10 in ex-
periments.
315
To unify notation, we denote unaligned phrases as
phrase pairs with one side equal to null: ?e, null? or
?null, f?. Then, the revised model takes the form:
P ({?e, f?},a) = PG(`; p$)P (a|{?e, f?})
?
?e,f?
PM(?e, f?)
PM(?e, f?) = p??N(?e, f?) + (1 ? p?)?J(?e, f?) .
In this definition, the distribution ?N gives non-
zero weight only to unaligned phrases of the form
?e, null? or ?null, f?, while ?J gives non-zero
weight only to aligned phrase pairs.
3 Model Training and Expectations
Our model involves observed sentence pairs, which
in aggregate we can call x, latent phrase segmenta-
tions and alignments, which we can call z, and pa-
rameters ?J and ?N, which together we can call ?.
A model such as ours could be used either for the
learning of the key phrase pair parameters in ?, or
to compute expected counts of phrase pairs in our
data. These two uses are very closely related, but
we focus on the computation of phrase pair expecta-
tions. For exposition purposes, we describe a Gibbs
sampling algorithm for computing expected counts
of phrases under P (z|x, ?) for fixed ?. Such ex-
pectations would be used, for example, to compute
maximum likelihood estimates in the E-step of EM.
In Section 4, we instead compute expectations under
P (z|x), with ? marginalized out entirely.
In a Gibbs sampler, we start with a complete
phrase segmentation and alignment, state z0, which
sets all latent variables to some initial configuration.
We then produce a sequence of sample states zi,
each of which differs from the last by some small
local change. The samples zi are guaranteed (in the
limit) to consistently approximate the conditional
distribution P (z|x, ?) (or P (z|x) later). Therefore,
the average counts of phrase pairs in the samples
converge to expected counts under the model. Nor-
malizing these expected counts yields estimates for
the features ?(e|f) and ?(f |e).
Gibbs sampling is not new to the natural language
processing community (Teh, 2006; Johnson et al,
2007). However, it is usually used as a search pro-
cedure akin to simulated annealing, rather than for
approximating expectations (Goldwater et al, 2006;
Finkel et al, 2007). Our application is also atypical
for an NLP application in that we use an approxi-
mate sampler not only to include Bayesian prior in-
formation (section 4), but also because computing
phrase alignment expectations exactly is a #P-hard
problem (DeNero and Klein, 2008). That is, we
could not run EM exactly, even if we wanted maxi-
mum likelihood estimates.
3.1 Related Work
Expected phrase pair counts under P (z|x, ?) have
been approximated before in order to run EM.
Marcu and Wong (2002) employed local search
from a heuristic initialization and collected align-
ment counts during a hill climb through the align-
ment space. DeNero et al (2006) instead proposed
an exponential-time dynamic program pruned using
word alignments. Subsequent work has relied heav-
ily on word alignments to constrain inference, even
under reordering models that admit polynomial-time
E-steps (Cherry and Lin, 2007; Zhang et al, 2008).
None of these approximations are consistent, and
they offer no method of measuring their biases.
Gibbs sampling is not only consistent in the limit,
but also allows us to add Bayesian priors conve-
niently (section 4). Of course, sampling has liabili-
ties as well: we do not know in advance how long we
need to run the sampler to approximate the desired
expectations ?closely enough.?
Snyder and Barzilay (2008) describe a Gibbs sam-
pler for a bilingual morphology model very similar
in structure to ours. However, the basic sampling
step they propose ? resampling all segmentations
and alignments for a sequence at once ? requires a
#P-hard computation. While this asymptotic com-
plexity was apparently not prohibitive in the case of
morphological alignment, where the sequences are
short, it is prohibitive in phrase alignment, where the
sentences are often very long.
3.2 Sampling with the SWAP Operator
Our Gibbs sampler repeatedly applies each of five
operators to each position in each training sentence
pair. Each operator freezes all of the current state zi
except a small local region, determines all the ways
that region can be reconfigured, and then chooses a
(possibly) slightly different zi+1 from among those
outcomes according to the conditional probability of
each, given the frozen remainder of the state. This
316
frozen region of the state is called a Markov blanket
(denoted m), and plays a critical role in proving the
correctness of the sampler.
The first operator we consider is SWAP, which
changes alignments but not segmentations. It freezes
the set of phrases, then picks two English phrases e1
and e2 (or two foreign phrases, but we focus on the
English case). All alignments are frozen except the
phrase pairs ?e1, f1? and ?e2, f2?. SWAP chooses be-
tween keeping ?e1, f1? and ?e2, f2? aligned as they
are (outcome o0), or swapping their alignments to
create ?e1, f2? and ?e2, f1? (outcome o1).
SWAP chooses stochastically in proportion to
each outcome?s posterior probability: P (o0|m,x, ?)
and P (o1|m,x, ?). Each phrase pair in each out-
come contributes to these posteriors the probability
of adding a new pair, deciding whether it is null, and
generating the phrase pair along with its contribu-
tion to the distortion probability. This is all captured
in a succinct potential function ?(?e, f?) =
{
(1?p$) (1?p?) ?J(?e, f?) ?(?e, f?) e & f non-null
(1?p$) ? p? ? ?N(?e, f?) otherwise
.
Thus, outcome o0 is chosen with probability
P (o0|m,x, ?) =
?(?e1, f1?)?(?e2, f2?)
?(?e1, f1?)?(?e2, f2?) + ?(?e1, f2?)?(?e2, f1?)
.
Operators in a Gibbs sampler require certain con-
ditions to guarantee the correctness of the sampler.
First, they must choose among all possible configu-
rations of the unfrozen local state. Second, imme-
diately re-applying the operator from any outcome
must yield the same set of outcome options as be-
fore.4 If these conditions are not met, the sampler
may no longer be guaranteed to yield consistent ap-
proximations of the posterior distribution.
A subtle issue arises with SWAP as defined:
should it also consider an outcome o2 of ?e1, null?
and ?e2, null? that removes alignments? No part
of the frozen state is changed by removing these
alignments, so the first Gibbs condition dictates that
we must include o2. However, after choosing o2,
when we reapply the operator to positions e1 and
4These are two sufficient conditions to guarantee that the
Metropolis-Hastings acceptance ratio of the sampling step is 1.
(b) FLIP(a) SWAP
(c) TOGGLE
(d) FLIP TWO
(e) MOVE
Figure 2: Each local operator manipulates a small portion
of a single alignment. Relevant phrases are exaggerated
for clarity. The outcome sets (depicted by arrows) of each
possible configuration are fully connected. Certain con-
figurations cannot be altered by certain operators, such as
the final configuration in SWAP. Unalterable configura-
tions for TOGGLE have been omitted for space.
e2, we freeze all alignments except ?e1, null? and
?e2, null?, which prevents us from returning to o0.
Thus, we fail to satisfy the second condition. This
point is worth emphasizing because some prior work
has treated Gibbs sampling as randomized search
and, intentionally or otherwise, proposed inconsis-
tent operators.
Luckily, the problem is not with SWAP, but with
our justification of it: we can salvage SWAP by aug-
menting its Markov blanket. Given that we have se-
lected ?e1, f1? and ?e2, f2?, we not only freeze all
other alignments and phrase boundaries, but also the
number of aligned phrase pairs. With this count held
invariant, o2 is not among the possible outcomes of
SWAP given m. Moreover, regardless of the out-
come chosen, SWAP can immediately be reapplied
at the same location with the same set of outcomes.
All the possible starting configurations and out-
come sets for SWAP appear in Figure 2(a).
317
The boys are
Ellos
comen
Current State
Includes segmentations
and alignments for all
sentence pairs
Markov Blanket
Freezes most of the
segmentations and 
alignments, along with 
the alignment count
Outcomes
An exhaustive set of 
possibilities given 
the Markov blanket
eating
? ?
Apply the FLIP operator 
to English position 1
1
Compute the conditional 
probability of each outcome
2
Finally, select a new state proportional 
to its conditional probability
3
?
Figure 3: The three steps involved in applying the FLIP
operator. The Markov blanket freezes all segmentations
except English position 1 and all alignments except those
for Ellos and The boys. The blanket alo freezes the num-
ber of alignments, which disallows the lower right out-
come.
3.3 The FLIP operator
SWAP can arbitrarily shuffle alignments, but we
need a second operator to change the actual phrase
boundaries. The FLIP operator changes the status of
a single segmentation position5 to be either a phrase
boundary or not. In this sense FLIP is a bilingual
analog of the segmentation boundary flipping oper-
ator of Goldwater et al (2006).
Figure 3 diagrams the operator and its Markov
blanket. First, FLIP chooses any between-word po-
sition in either sentence. The outcome sets for FLIP
vary based on the current segmentation and adjacent
alignments, and are depicted in Figure 2.
Again, for FLIP to satisfy the Gibbs conditions,
we must augment its Markov blanket to freeze not
only all other segmentation points and alignments,
but also the number of aligned phrase pairs. Oth-
erwise, we end up allowing outcomes from which
5A segmentation position is a position between two words
that is also potentially a boundary between two phrases in an
aligned sentence pair.
we cannot return to the original state by reapply-
ing FLIP. Consequently, when a position is already
segmented and both adjacent phrases are currently
aligned, FLIP cannot unsegment the point because
it can?t create two aligned phrase pairs with the one
larger phrase that results (see bottom of Figure 2(b)).
3.4 The TOGGLE operator
Both SWAP and FLIP freeze the number of align-
ments in a sentence. The TOGGLE operator, on the
other hand, can add or remove individual alignment
links. In TOGGLE, we first choose an e1 and f1. If
?e1, f1? ? a or both e1 and f1 are null, we freeze
all segmentations and the rest of the alignments, and
choose between including ?e1, f1? in the alignment
or leaving both e1 and f1 unaligned. If only one of
e1 and f1 are aligned, or they are not aligned to each
other, then TOGGLE does nothing.
3.5 A Complete Sampler
Together, FLIP, SWAP and TOGGLE constitute a
complete Gibbs sampler that consistently samples
from the posterior P (z|x, ?). Not only are these
operators valid Gibbs steps, but they also can form
a path of positive probability from any source state
to any target state in the space of phrase alignments
(formally, the induced Markov chain is irreducible).
Such a path can at worst be constructed by unalign-
ing all phrases in the source state with TOGGLE,
composing applications of FLIP to match the target
phrase boundaries, then applying TOGGLE to match
the target algnments.
We include two more local operators to speed up
the rate at which the sampler explores the hypothesis
space. In short, FLIP TWO simultaneously flips an
English and a foreign segmentation point (to make a
large phrase out of two smaller ones or vice versa),
while MOVE shifts an aligned phrase boundary to
the left or right. We omit details for lack of space.
3.6 Phrase Pair Count Estimation
With our sampling procedure in place, we can now
estimate the expected number of times a given
phrase pair occurs in our data, for fixed ?, using a
Monte-Carlo average,
1
N
N?
i=1
count?e,f?(x, zi)
a.s.
?? E
[
count?e,f?(x, ?)
]
.
318
The left hand side is simple to compute; we count
aligned phrase pairs in each sample we generate.
In practice, we only count phrase pairs after apply-
ing every operator to every position in every sen-
tence (one iteration).6 Appropriate normalizations
of these expected counts can be used either in an M-
step as maximum likelihood estimates, or to com-
pute values for features ?(f |e) and ?(e|f).
4 Nonparametric Bayesian Priors
The Gibbs sampler we presented addresses the infer-
ence challenges of learning phrase alignment mod-
els. With slight modifications, it also enables us to
include prior information into the model. In this sec-
tion, we treat ? as a random variable and shape its
prior distribution in order to correct the well-known
degenerate behavior of the model.
4.1 Model Degeneracy
The structure of our joint model penalizes explana-
tions that use many small phrase pairs. Each phrase
pair token incurs the additional expense of genera-
tion and distortion. In fact, the maximum likelihood
estimate of the model puts mass on ?e, f? pairs that
span entire sentences, explaining the training corpus
with one phrase pair per sentence.
Previous phrase alignment work has primarily
mitigated this tendency by constraining the in-
ference procedure, for example with word align-
ments and linguistic features (Birch et al, 2006),
or by disallowing large phrase pairs using a non-
compositional constraint (Cherry and Lin, 2007;
Zhang et al, 2008). However, the problem lies with
the model, and therefore should be corrected in the
model, rather than the inference procedure.
Model-based solutions appear in the literature as
well, though typically combined with word align-
ment constraints on inference. A sparse Dirichlet
prior coupled with variational EM was explored by
Zhang et al (2008), but it did not avoid the degen-
erate solution. Moore and Quirk (2007) proposed a
new conditional model structure that does not cause
large and small phrases to compete for probabil-
ity mass. May and Knight (2007) added additional
model terms to balance the cost of long and short
derivations in a syntactic alignment model.
6For experiments, we ran the sampler for 100 iterations.
4.2 A Dirichlet Process Prior
We control this degenerate behavior by placing a
Dirichlet process (DP) prior over ?J, the distribution
over aligned phrase pairs (Ferguson, 1973).
If we were to assume a maximum number K of
phrase pair types, a (finite) Dirichlet distribution
would be an appropriate prior. A draw from a K-
dimensional Dirichlet distribution is a list of K real
numbers in [0, 1] that sum to one, which can be in-
terpreted as a distribution overK phrase pair types.
However, since the event space of possible phrase
pairs is in principle unbounded, we instead use a
Dirichlet process. A draw from a DP is a countably
infinite list of real numbers in [0, 1] that sum to one,
which we interpret as a distribution over a countably
infinite list of phrase pair types.7
The Dirichlet distribution and the DP distribution
have similar parameterizations. A K-dimensional
Dirichlet can be parameterized with a concentration
parameter ? > 0 and a base distribution M0 =
(?1, . . . , ?K?1), with ?i ? (0, 1).8 This parameteri-
zation has an intuitive interpretation: under these pa-
rameters, the average of independent samples from
the Dirichlet will converge toM0. That is, the aver-
age of the ith element of the samples will converge
to ?i. Hence, the base distributionM0 characterizes
the sample mean. The concentration parameter ?
only affects the variance of the draws.
Similarly, we can parameterize the Dirichlet pro-
cess with a concentration parameter ? (that affects
only the variance) and a base distribution M0 that
determines the mean of the samples. Just as in the
finite Dirichlet case,M0 is simply a probability dis-
tribution, but now with countably infinite support:
all possible phrase pairs in our case. In practice, we
can use an unnormalized M0 (a base measure) by
appropriately rescaling ?.
In our model, we select a base measure that
strongly prefers shorter phrases, encouraging the
model to use large phrases only when it has suffi-
cient evidence for them. We continue the model:
7Technical note: to simplify exposition, we restrict the dis-
cussion to settings such as ours where the base measure of the
DP has countable support.
8This parametrization is equivalent to the standard pseudo-
counts parametrization of K positive real numbers. The bi-
jection is given by ? =
PK
i=1 ??i and ?i = ??i/?, where
(??1, . . . , ??K) are the pseudo-counts.
319
?J ? DP (M0, ?)
M0(?e, f?) = [Pf (f)PWA(e|f) ? Pe(e)PWA(f |e)]
1
2
Pf (f) = PG(|f |; ps) ?
(
1
nf
)|f |
Pe(e) = PG(|e|; ps) ?
(
1
ne
)|e|
.
.
PWA is the IBM model 1 likelihood of one phrase
conditioned on the other (Brown et al, 1994). Pf
and Pe are uniform over types for each phrase
length: the constants nf and ne denote the vocab-
ulary size of the foreign and English languages, re-
spectively, and PG is a geometric distribution.
Above, ?J is drawn from a DP centered on the ge-
ometric mean of two joint distributions over phrase
pairs, each of which is composed of a monolingual
unigram model and a lexical translation component.
This prior has two advantages. First, we pressure
the model to use smaller phrases by increasing ps
(ps = 0.8 in experiments). Second, we encour-
age good phrase pairs by incorporating IBM Model
1 distributions. This use of word alignment distri-
butions is notably different from lexical weighting
or word alignment constraints: we are supplying
prior knowledge that phrases will generally follow
word alignments, though with enough corpus evi-
dence they need not (and often do not) do so in the
posterior samples. The model proved largely insen-
sitive to changes in the sparsity parameter ?, which
we set to 100 for experiments.
4.3 Unaligned phrases and the DP Prior
Introducing unaligned phrases invites further degen-
erate megaphrase behavior: a sentence pair can be
generated cheaply as two unaligned phrases that
each span an entire sentence. We attempted to place
a similar DP prior over ?N, but surprisingly, this
modeling choice invoked yet another degenerate be-
havior. The DP prior imposes a rich-get-richer prop-
erty over the phrase pair distribution, strongly en-
couraging the model to reuse existing pairs rather
than generate new ones. As a result, common
words consistently aligned to null, even while suit-
able translations were present, simply because each
null alignment reinforced the next. For instance, the
was always unaligned.
Instead, we fix ?N to a simple unigram model that
is uniform over word types. This way, we discour-
age unaligned phrases while focusing learning on ?J.
For simplicity, we reuse Pf (f) and Pe(e) from the
prior over ?J.
?N(?e, f?) =
{
1
2 ? Pe(e) if f = null
1
2 ? Pf (f) if e = null .
The 12 represents a choice of whether the aligned
phrase is in the foreign or English sentence.
4.4 Collapsed Sampling with a DP Prior
Our entire model now has the general form
P (x, z, ?J); all other model parameters have been
fixed. Instead of searching for a suitable ?J,9 we
sample from the posterior distribution P (z|x) with
?J marginalized out.
To this end, we convert our Gibbs sampler into
a collapsed Gibbs sampler10 using the Chinese
Restaurant Process (CRP) representation of the DP
(Aldous, 1985). With the CRP, we avoid the prob-
lem of explicitely representing samples from the
DP. CRP-based samplers have served the commu-
nity well in related language tasks, such as word seg-
mentation and coreference resolution (Goldwater et
al., 2006; Haghighi and Klein, 2007).
Under this representation, the probability of each
sampling outcome is a simple expression in terms
of the state of the rest of the training corpus (the
Markov blanket), rather than explicitly using ?J.
Let zm be the set of aligned phrase pair tokens ob-
served in the rest of the corpus. Then, when ?e, f? is
aligned (that is, neither e nor f are null), the condi-
tional probability for a pair ?e, f? takes the form:
?(?e, f?|zm) =
count?e,f?(zm) + ? ?M0(?e, f?)
|zm| + ?
,
where count?e,f?(zm) is the number of times that
?e, f? appears in zm. We can write this expression
thanks to the exchangeability of the model. For fur-
ther exposition of this collapsed sampler posterior,
9For instance, using approximate MAP EM.
10A collapsed sampler is simply one in which the model pa-
rameters have been marginalized out.
320
025
50
75
100
2007 2008
1 x 1
1 x 2, 2 x 1
2 x 2
2 x 3, 3 x 2
3+ x 3+ 
0
25
50
75
100
1x1 1x2 & 2x1 1x3 & 3x1 2x2 2x3 & 3x2 3x3 and up
Minimal extracted phrases
Sampled phrases
All extracted phrases
Figure 4: The distribution of phrase pair sizes (denoted
English length x foreign length) favors small phrases un-
der the model.
see Goldwater et al (2006).11
The sampler remains exactly the same as de-
scribed in Section 3, except that the posterior con-
ditional probability of each outcome uses a revised
potential function ?DP(?e, f?) =
{
(1?p$) (1?p?) ?(?e, f?) ?(?e, f?) e & f non-null
(1?p$) ? p? ? ?N(?e, f?) otherwise .
?DP is like ?, but the fixed ?J is replaced with the
constantly-updated ? function.
4.5 Degeneracy Analysis
Figure 4 shows a histogram of phrase pair sizes in
the distribution of expected counts under the model.
As reference, we show the size distribution of both
minimal and all phrase pairs extracted from word
alignments using the standard heuristic. Our model
tends to select minimal phrases, only using larger
phrases when well motivated.12
This result alone is important: a model-based
solution with no inference constraint has yielded
a non-degenerate distribution over phrase lengths.
Note that our sampler does find the degenerate solu-
tion quickly under a uniform prior, confirming that
the model, and not the inference procedure, is select-
ing these small phrases.
11Note that the expression for ? changes slightly under con-
ditions where two phrase pairs being changed simultaneously
coincidentally share the same lexical content. Details of these
fringe conditions have been omitted for space, but were in-
cluded in our implementation.
12The largest phrase pair found was 13 English words by 7
Spanish words.
4.6 A Hierarchical Dirichlet Process Prior
We also evaluate a hierarchical Dirichlet process
(HDP) prior over ?J, which draws monolingual dis-
tributions ?E and ?F from a DP and ?J from their
cross-product:
?J ? DP (M
?
0, ?)
M ?0(?e, f?) = [?F(f)PWA(e|f) ? ?E(e)PWA(f |e)]
1
2
?F ? DP (Pf , ?
?)
?E ? DP (Pe, ?
?) .
This prior encourages novel phrase pairs to be com-
posed of phrases that have been used before. In the
sampler, we approximate table counts for ?E and
?F with their expectations, which can be computed
from phrase pair counts (see the appendix of Gold-
water et al (2006) for details). The HDP prior gives
a similar distribution over phrase sizes.
5 Translation Results
We evaluate our new estimates using the baseline
translation pipeline from the 2007 Statistical Ma-
chine Translation Workshop shared task.
5.1 Baseline System
We trained Moses on all Spanish-English Europarl
sentences up to length 20 (177k sentences) using
GIZA++ Model 4 word alignments and the grow-
diag-final-and combination heuristic (Koehn et al,
2007; Och and Ney, 2003; Koehn, 2002), which
performed better than any alternative combination
heuristic.13 The baseline estimates (Heuristic) come
from extracting phrases up to length 7 from the word
alignment. We used a bidirectional lexicalized dis-
tortion model that conditions on both foreign and
English phrases, along with their orientations. Our
5-gram language model was trained on 38.3 million
words of Europarl using Kneser-Ney smoothing. We
report results with and without lexical weighting,
denoted lex.
We tuned and tested on development corpora for
the 2006 translation workshop. The parameters for
each phrase table were tuned separately using min-
imum error rate training (Och, 2003). Results are
13Sampling iteration time scales quadratically with sentence
length. Short sentences were chosen to speed up our experiment
cycle.
321
Phrase Exact
Pair NIST Match
Estimate Count BLEU METEOR
Heuristic 4.4M 29.8 52.4
DP 0.6M 28.8 51.7
HDP 0.3M 29.1 52.0
DP-composed 3.7M 30.1 52.7
HDP-composed 3.1M 30.1 52.6
DP-smooth 4.8M 30.1 52.5
HDP-smooth 4.6M 30.2 52.7
Heuristic + lex 4.4M 30.5 52.9
DP-smooth + lex 4.8M 30.4 53.0
HDP-smooth + lex 4.6M 30.7 53.2
Table 1: BLEU results for learned distributions improve
over a heuristic baseline. Estimate labels are described
fully in section 5.3. The label lex indicates the addition
of a lexical weighting feature.
scored with lowercased, tokenized NIST BLEU, and
exact match METEOR (Papineni et al, 2002; Lavie
and Agarwal, 2007).
The baseline system gives a BLEU score of 29.8,
which increases to 30.5 with lex, as shown in Table
1. For reference, training on all sentences of length
less than 40 (the shared task baseline default) gives
32.4 BLEU with lex.
5.2 Learned Distribution Performance
We initialized the sampler with a configuration de-
rived from the word alignments generated by the
baseline. We greedily constructed a phrase align-
ment from the word alignment by identifying min-
imal phrase pairs consistent with the word align-
ment in each region of the sentence. We then ran
the sampler for 100 iterations through the training
data. Each iteration required 12 minutes under the
DP prior, and 30 minutes under the HDP prior. Total
running time for the HDP model neared two days on
an eight-processor machine with 16 Gb of RAM.
Estimating phrase counts under the DP prior de-
creases BLEU to 28.8, or 29.1 under the HDP prior.
This gap is not surprising: heuristic extraction dis-
covers many more phrase pairs than sampling. Note
that sacrificing only 0.7 BLEU while shrinking the
phrase table by 92% is an appealing trade-off in
resource-constrained settings.
5.3 Increasing Phrase Pair Coverage
The estimates DP-composed and HDP-composed in
Table 1 take expectations of a more liberal count
function. While sampling, we count not only aligned
phrase pairs, but also larger ones composed of two or
more contiguous aligned pairs. This count function
is similar to the phrase pair extraction heuristic, but
never includes unaligned phrases in any way. Expec-
tations of these composite phrases still have a proba-
bilistic interpretation, but they are not the structures
we are directly modeling. Notably, these estimates
outperform the baseline by 0.3 BLEU without ever
extracting phrases from word alignments, and per-
formance increases despite a reduction in table size.
We can instead increase coverage by smooth-
ing the learned estimates with the heuristic counts.
The estimates DP-smooth and HDP-smooth add
counts extracted from word alignments to the sam-
pler?s running totals, which improves performance
by 0.4 BLEU over the baseline. This smoothing bal-
ances the lower-bias sampler counts with the lower-
variance heuristics ones.
6 Conclusion
Our novel Gibbs sampler and nonparametric pri-
ors together address two open problems in learn-
ing phrase alignment models, approximating infer-
ence consistently and efficiently while avoiding de-
generate solutions. While improvements are mod-
est relative to the highly developed word-alignment-
centered baseline, we show for the first time com-
petitive results from a system that uses word align-
ments only for model initialization and smoothing,
rather than inference and estimation. We view this
milestone as critical to eventually developing a clean
probabilistic approach to machine translation that
unifies model structure across both estimation and
decoding, and decreases the use of heuristics.
References
David Aldous. 1985. Exchangeability and related topics.
In E?cole d?e?te? de probabilitie?s de Saint-Flour, Berlin.
Springer.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In The Con-
322
ference for the Association for Machine Translation in
the Americas.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1994. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263?311.
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling. In
The Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics
Workshop on Syntax and Structure in Statistical Trans-
lation.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In The Annual Confer-
ence of the Association for Computational Linguistics:
Short Paper Track.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In The Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics Workshop on Statistical Ma-
chine Translation.
Thomas S Ferguson. 1973. A bayesian analysis of some
nonparametric problems. In Annals of Statistics.
Jenny Rose Finkel, Trond Grenager, and Christopher D.
Manning. 2007. The infinite tree. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2006. Contextual dependencies in unsupervised
word segmentation. In The Annual Conference of the
Association for Computational Linguistics.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric bayesian
model. In The Annual Conference of the Association
for Computational Linguistics.
Mark Johnson, Thomas Griffiths, and Sharon Goldwa-
ter. 2007. Bayesian inference for PCFGs via Markov
chain Monte Carlo. In The Annual Conference of the
Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In The An-
nual Conference of the Association for Computational
Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: An
automatic metric for mt evaluation with high levels
of correlation with human judgments. In The Annual
Conference of the Association for Computational Lin-
guistics Workshop on Statistical Machine Translation.
Daniel Marcu and Daniel Wong. 2002. A phrase-based,
joint probability model for statistical machine trans-
lation. In The Conference on Empirical Methods in
Natural Language Processing.
Jonathan May and Kevin Knight. 2007. Syntactic re-
alignment models for machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Robert Moore and Chris Quirk. 2007. An iteratively-
trained segmentation-free phrase translation model for
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics
Workshop on Statistical Machine Translation.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In The Annual Confer-
ence of the Association for Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised multilingual learning for morphological segmen-
tation. In The Annual Conference of the Association
for Computational Linguistics.
Yee Whye Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In The Annual
Conference of the Association for Computational Lin-
guistics.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
The Annual Conference of the Association for Compu-
tational Linguistics.
323
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1418?1427,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
Consensus Training for Consensus Decoding in Machine Translation
Adam Pauls, John DeNero and Dan Klein
Computer Science Division
University of California at Berkeley
{adpauls,denero,klein}@cs.berkeley.edu
Abstract
We propose a novel objective function for dis-
criminatively tuning log-linear machine trans-
lation models. Our objective explicitly op-
timizes the BLEU score of expected n-gram
counts, the same quantities that arise in forest-
based consensus and minimum Bayes risk de-
coding methods. Our continuous objective
can be optimized using simple gradient as-
cent. However, computing critical quantities
in the gradient necessitates a novel dynamic
program, which we also present here. As-
suming BLEU as an evaluation measure, our
objective function has two principle advan-
tages over standard max BLEU tuning. First,
it specifically optimizes model weights for
downstream consensus decoding procedures.
An unexpected second benefit is that it reduces
overfitting, which can improve test set BLEU
scores when using standard Viterbi decoding.
1 Introduction
Increasing evidence suggests that machine trans-
lation decoders should not search for a single
top scoring Viterbi derivation, but should instead
choose a translation that is sensitive to the model?s
entire predictive distribution. Several recent con-
sensus decoding methods leverage compact repre-
sentations of this distribution by choosing transla-
tions according to n-gram posteriors and expected
counts (Tromble et al, 2008; DeNero et al, 2009;
Li et al, 2009; Kumar et al, 2009). This change
in decoding objective suggests a complementary
change in tuning objective, to one that optimizes
expected n-gram counts directly. The ubiquitous
minimum error rate training (MERT) approach op-
timizes Viterbi predictions, but does not explicitly
boost the aggregated posterior probability of de-
sirable n-grams (Och, 2003).
We therefore propose an alternative objective
function for parameter tuning, which we call con-
sensus BLEU or CoBLEU, that is designed to
maximize the expected counts of the n-grams that
appear in reference translations. To maintain con-
sistency across the translation pipeline, we for-
mulate CoBLEU to share the functional form of
BLEU used for evaluation. As a result, CoBLEU
optimizes exactly the quantities that drive efficient
consensus decoding techniques and precisely mir-
rors the objective used for fast consensus decoding
in DeNero et al (2009).
CoBLEU is a continuous and (mostly) differ-
entiable function that we optimize using gradient
ascent. We show that this function and its gradient
are efficiently computable over packed forests of
translations generated by machine translation sys-
tems. The gradient includes expectations of prod-
ucts of features and n-gram counts, a quantity that
has not appeared in previous work. We present a
new dynamic program which allows the efficient
computation of these quantities over translation
forests. The resulting gradient ascent procedure
does not require any k-best approximations. Op-
timizing over translation forests gives similar sta-
bility benefits to recent work on lattice-based min-
imum error rate training (Macherey et al, 2008)
and large-margin training (Chiang et al, 2008).
We developed CoBLEU primarily to comple-
ment consensus decoding, which it does; it pro-
duces higher BLEU scores than coupling MERT
with consensus decoding. However, we found
an additional empirical benefit: CoBLEU is less
prone to overfitting than MERT, even when using
Viterbi decoding. In experiments, models trained
to maximize tuning set BLEU using MERT con-
sistently degraded in performance from tuning to
test set, while CoBLEU-trained models general-
ized more robustly. As a result, we found that op-
timizing CoBLEU improved test set performance
reliably using consensus decoding and occasion-
ally using Viterbi decoding.
1418
Once upon a rhyme
H
1
) Once on a rhyme
H
3
) Once upon a time
H
2
) Once upon a rhyme
Il ?tait une rime
(a) Tuning set sentence and translation
(a) Hypotheses ranked by ?
TM 
= ?
LM 
= 1
(a)  Model score as a function of ?
LM
 
Reference r:
Sentence f:
TM LM
-3 -7 0.67
-5 -6 0.24
-9 -3 0.09
Pr
(b)  Objectives as functions of ?
LM
(b) Computing Consensus Bigram Precision
-18
-12
-6
0
0 2
H
3
H
1
H
2
Parameter: ?
LM
M
o
d
e
l
:
 
T
M
 
+
 
?
L
M
 
?
 
L
M
 
V
i
t
e
r
b
i
 
&
 
C
o
n
s
e
n
s
u
s
 
O
b
j
e
c
t
i
v
e
s
Parameter: ?
LM
E
?
[c(?Once upon?, d)|f ] = 0.24 + 0.09 = 0.33
E
?
[c(?upon a?, d)|f ] = 0.24 + 0.09 = 0.33
E
?
[c(?a rhyme?, d)|f ] = 0.67 + 0.24 = 0.91
?
g
E
?
[c(g, d)|f ] = 3[0.67 + 0.24 + 0.09]
?
g
min{E
?
[c(g, d)|f ], c(g, r)}
?
g
E
?
[c(g, d)|f ]
=
0.33 + 0.33 + 0.91
3
Figure 1: (a) A simple hypothesis space of translations
for a single sentence containing three alternatives, each
with two features. The hypotheses are scored under a
log-linear model with parameters ? equal to the identity
vector. (b) The expected counts of all bigrams that ap-
pear in the computation of consensus bigram precision.
2 Consensus Objective Functions
Our proposed objective function maximizes n-
gram precision by adapting the BLEU evaluation
metric as a tuning objective (Papineni et al, 2002).
To simplify exposition, we begin by adapting a
simpler metric: bigram precision.
2.1 Bigram Precision Tuning
Let the tuning corpus consist of source sentences
F = f
1
. . . f
m
and human-generated references
R = r
1
. . . r
m
, one reference for each source
sentence. Let e
i
be a translation of f
i
, and let
E = e
1
. . . e
m
be a corpus of translations, one for
each source sentence. A simple evaluation score
for E is its bigram precision BP(R,E):
BP(R,E) =
?
m
i=1
?
g
2
min{c(g
2
, e
i
), c(g
2
, r
i
)}
?
m
i=1
?
g
2
c(g
2
, e
i
)
where g
2
iterates over the set of bigrams in the tar-
get language, and c(g
2
, e) is the count of bigram
g
2
in translation e. As in BLEU, we ?clip? the bi-
gram counts of e in the numerator using counts of
bigrams in the reference sentence.
Modern machine translation systems are typi-
cally tuned to maximize the evaluation score of
Viterbi derivations
1
under a log-linear model with
parameters ?. Let d
?
?
(f
i
) = arg max
d
P
?
(d|f
i
) be
the highest scoring derivation d of f
i
. For a system
employing Viterbi decoding and evaluated by bi-
gram precision, we would want to select ? to max-
imize MaxBP(R,F, ?):
?
m
i=1
?
g
2
min{c(g
2
, d
?
?
(f
i
)), c(g
2
, r
i
)}
?
m
i=1
?
g
2
c(g
2
, d
?
?
(f
i
))
On the other hand, for a system that uses ex-
pected bigram counts for decoding, we would pre-
fer to choose ? such that expected bigram counts
match bigrams in the reference sentence. To this
end, we can evaluate an entire posterior distri-
bution over derivations by computing the same
clipped precision for expected bigram counts us-
ing CoBP(R,F, ?):
?
m
i=1
?
g
2
min{E
?
[c(g
2
, d)|f
i
], c(g
2
, r
i
)}
?
m
i=1
?
g
2
E
?
[c(g
2
, d)|f
i
]
(1)
where
E
?
[c(g
2
, d)|f
i
] =
?
d
P
?
(d|f
i
)c(g
2
, d)
is the expected count of bigram g
2
in all deriva-
tions d of f
i
. We define the precise parametric
form of P
?
(d|f
i
) in Section 3. Figure 1 shows pro-
posed translations for a single sentence along with
the bigram expectations needed to compute CoBP.
Equation 1 constitutes an objective function for
tuning the parameters of a machine translation
model. Figure 2 contrasts the properties of CoBP
and MaxBP as tuning objectives, using the simple
example from Figure 1.
Consensus bigram precision is an instance of a
general recipe for converting n-gram based eval-
uation metrics into consensus objective functions
for model tuning. For the remainder of this pa-
per, we focus on consensus BLEU. However, the
techniques herein, including the optimization ap-
proach of Section 3, are applicable to many differ-
entiable functions of expected n-gram counts.
1
By derivation, we mean a translation of a foreign sen-
tence along with any latent structure assumed by the model.
Each derivation corresponds to a particular English transla-
tion, but many derivations may yield the same translation.
1419
1.0 1.5 2.0 2.5 3.0
-16
-14
-12
-10
?
LM
Log M
odel 
Score
H
1
H
2
H
3
(a)
0 2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
1.0
?
LM
Valu
e of O
bject
ive
CoBP
MaxBP
H
1
H
3
H
1
H
2
H
3
(b)
Figure 2: These plots illustrate two properties of the objectives max bigram precision (MaxBP) and consensus
bigram precision (CoBP) on the simple example from Figure 1. (a) MaxBP is only sensitive to the convex hull (the
solid line) of model scores. When varying the single parameter ?
LM
, it entirely disregards the correct translation
H
2
becauseH
2
never attains a maximal model score. (b) A plot of both objectives shows their differing characteris-
tics. The horizontal segmented line at the top of the plot indicates the range over which consensus decoding would
select each hypothesis, while the segmented line at the bottom indicates the same for Viterbi decoding. MaxBP
is only sensitive to the single point of discontinuity between H
1
and H
3
, and disregards H
2
entirely. CoBP peaks
when the distribution most heavily favorsH
2
while suppressingH
1
. ThoughH
2
never has a maximal model score,
if ?
LM
is in the indicated range, consensus decoding would select H
2
, the desired translation.
2.2 CoBLEU
The logarithm of the single-reference
2
BLEU met-
ric (Papineni et al, 2002) has the following form:
ln BLEU(R,E) =
(
1?
|R|
?
m
i=1
?
g
1
c(g
1
, e
i
)
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{c(g
n
, e
i
), c(g
n
, r
i
)}
?
m
i=1
?
g
n
c(g
n
, e
i
)
Above, |R| denotes the number of words in the
reference corpus. The notation (?)
?
is shorthand
for min(?, 0). In the inner sums, g
n
iterates over
all n-grams of order n. In order to adapt BLEU
to be a consensus tuning objective, we follow the
recipe of Section 2.1: we replace n-gram counts
from a candidate translation with expected n-gram
counts under the model.
CoBLEU(R,F, ?)=
(
1?
|R|
?
m
i=1
?
g
1
E
?
[c(g
1
, d)|f
i
]
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(g
n
, r
i
)}
?
m
i=1
?
g
n
E
?
[c(g
n
, d)|f
i
]
The brevity penalty term in BLEU is calculated
using the expected length of the corpus, which
2
Throughout this paper, we use only a single reference,
but our objective readily extends to multiple references.
equals the sum of all expected unigram counts.
We call this objective function consensus BLEU,
or CoBLEU for short.
3 Optimizing CoBLEU
Unlike the more common MaxBLEU tuning ob-
jective optimized by MERT, CoBLEU is con-
tinuous. For distributions P
?
(d|f
i
) that factor
over synchronous grammar rules and n-grams, we
show below that it is also analytically differen-
tiable, permitting a straightforward gradient ascent
optimization procedure.
3
In order to perform gra-
dient ascent, we require methods for efficiently
computing the gradient of the objective function
for a given parameter setting ?. Once we have the
gradient, we can perform an update at iteration t
of the form
?
(t+1)
? ?
(t)
+ ?
t
?
?
CoBLEU(R,F, ?
(t)
)
where ?
t
is an adaptive step size.
4
3
Technically, CoBLEU is non-differentiable at some
points because of clipping. At these points, we must com-
pute a sub-gradient, and so our optimization is formally sub-
gradient ascent. See the Appendix for details.
4
After each successful step, we grow the step size by a
constant factor. Whenever the objective does not decrease
after a step, we shrink the step size by a constant factor and
try again until a decrease is attained.
1420
head(h)
tail(h)
u=
Once
S
rhyme
v
1
=
Once
RB
Once
v
2
=
upon
IN
upon
v
3
=
a
NP
rhyme
c(?Once upon?, h)
c(?upon a?, h)
= 1
= 1
!
2
(h) = 2
Figure 3: A hyperedge h represents a ?rule? used in
syntactic machine translation. tail(h) refers to the ?chil-
dren? of the rule, while head(h) refers to the ?head? or
?parent?. A forest of translations is built by combining
the nodes v
i
using h to form a new node u = head(h).
Each forest node consists of a grammar symbol and tar-
get language boundary words used to track n-grams. In
the above, we keep one boundary word for each node,
which allows us to track bigrams.
In this section, we develop an analytical expres-
sion for the gradient of CoBLEU, then discuss
how to efficiently compute the value of the objec-
tive function and gradient.
3.1 Translation Model Form
We first assume the general hypergraph setting of
Huang and Chiang (2007), namely, that deriva-
tions under our translation model form a hyper-
graph. This framework allows us to speak about
both phrase-based and syntax-based translation in
a unified framework.
We define a probability distribution over deriva-
tions d via ? as:
P
?
(d|f
i
) =
w(d)
Z(f
i
)
with
Z(f
i
) =
?
d
?
w(d
?
)
where w(d) = exp(?
>
?(d, f
i
)) is the weight of a
derivation and ?(d, f
i
) is a featurized representa-
tion of the derivation d of f
i
. We further assume
that these features decompose over hyperedges in
the hypergraph, like the one in Figure 3. That is,
?(d, f
i
) =
?
h?d
?(h, f
i
).
In this setting, we can analytically compute the
gradient of CoBLEU. We provide a sketch of the
derivation of this gradient in the Appendix. In
computing this gradient, we must calculate the fol-
lowing expectations:
E
?
[c(?
k
, d)|f
i
] (2)
E
?
[`
n
(d)|f
i
] (3)
E
?
[c(?
k
, d) ? `
n
(d)|f
i
] (4)
where `
n
(d) =
?
g
n
c(g
n
, d) is the sum of all n-
grams on derivation d (its ?length?). The first ex-
pectation is an expected count of the kth feature
?
k
over all derivations of f
i
. The second is an ex-
pected length, the total expected count of all n-
grams in derivations of f
i
. We call the final ex-
pectation an expected product of counts. We now
present the computation of each of these expecta-
tions in turn.
3.2 Computing Feature Expectations
The expected feature counts E
?
[c(?
k
, d)|f
i
] can be
written as
E
?
[c(?
k
, d)|f
i
] =
?
d
P
?
(d|f
i
)c(?
k
, d)
=
?
h
P
?
(h|f
i
)c(?
k
, h)
We can justify the second step since fea-
ture counts are local to hyperedges, i.e.
c(?
k
, d) =
?
h?d
c(?
k
, h). The posterior
probability P
?
(h|f
i
) can be efficiently computed
with inside-outside scores. Let I(u) and O(u) be
the standard inside and outside scores for a node
u in the forest.
5
P
?
(h|f
i
) =
1
Z(f)
w(h) O(head(h))
?
v?tail(h)
I(v)
where w(h) is the weight of hyperedge h, given
by exp(?
>
?(h)), and Z(f) = I(root) is the in-
side score of the root of the forest. Computing
these inside-outside quantities takes time linear in
the number of hyperedges in the forest.
3.3 Computing n-gram Expectations
We can compute the expectations of any specific
n-grams, or of total n-gram counts `, in the same
way as feature expectations, provided that target-
side n-grams are also localized to hyperedges (e.g.
consider ` to be a feature of a hyperedge whose
value is the number of n-grams on h). If the
nodes in our forests are annotated with target-side
5
Appendix Figure 7 gives recursions for I(u) and O(u).
1421
boundary words as in Figure 3, then this will be the
case. Note that this is the same approach used by
decoders which integrate a target language model
(e.g. Chiang (2007)). Other work has computed
n-gram expectations in the same way (DeNero et
al., 2009; Li et al, 2009).
3.4 Computing Expectations of Products of
Counts
While the previous two expectations can be com-
puted using techniques known in the literature, the
expected product of counts E
?
[c(?
k
, d) ? `
n
(d)|f
i
]
is a novel quantity. Fortunately, an efficient dy-
namic program exists for computing this expec-
tation as well. We present this dynamic program
here as one of the contributions of this paper,
though we omit a full derivation due to space re-
strictions.
To see why this expectation cannot be computed
in the same way as the expected feature or n-gram
counts, we expand the definition of the expectation
above to get
?
d
P
?
(d|f
i
) [c(?
k
, d)`
n
(d)]
Unlike feature and n-gram counts, the product of
counts in brackets above does not decompose over
hyperedges, at least not in an obvious way. We
can, however, still decompose the feature counts
c(?
k
, d) over hyperedges. After this decomposi-
tion and a little re-arranging, we get
=
?
h
c(?
k
, h)
?
d:h?d
P
?
(d|f
i
)`
n
(d)
=
1
Z(f
i
)
?
h
c(?
k
, h)
[
?
d:h?d
w(d)`
n
(d)
]
=
1
Z(f
i
)
?
h
c(?
k
, h)
?
D
n
?
(h|f
i
)
The quantity
?
D
n
?
(h|f
i
) =
?
d:h?d
w(d)`
n
(d) is the
sum of the weight-length products of all deriva-
tions d containing hyperedge h. In the same
way that P
?
(h|f
i
) can be efficiently computed
from inside and outside probabilities, this quan-
tity
?
D
n
?
(h|f
i
) can be efficiently computed with two
new inside and outside quantities, which we call
?
I
n
(u) and
?
O
n
(u). We provide recursions for these
quantities in Figure 4. Like the standard inside and
outside computations, these recursions run in time
linear in the number of hyperedges in the forest.
While a full exposition of the algorithm is not
possible in the available space, we give some brief
intuition behind this dynamic program. We first
define
?
I
n
(u):
?
I
n
(u) =
?
d
u
w(d
u
)`
n
(d)
where d
u
is a derivation rooted at node u. This is
a sum of weight-length products similar to
?
D. To
give a recurrence for
?
I, we rewrite it:
?
I
n
(u) =
?
d
u
?
h?d
u
[w(d
u
)`
n
(h)]
Here, we have broken up the total value of `
n
(d)
across hyperedges in d. The bracketed quantity
is a score of a marked derivation pair (d, h) where
the edge h is some specific element of d. The score
of a marked derivation includes the weight of the
derivation and the factor `
n
(h) for the marked hy-
peredge.
This sum over marked derivations gives the in-
side recurrence in Figure 4 by the following de-
composition. For
?
I
n
(u) to sum over all marked
derivation pairs rooted at u, we must consider two
cases. First, the marked hyperedge could be at the
root, in which case we must choose child deriva-
tions from regular inside scores and multiply in the
local `
n
, giving the first summand of
?
I
n
(u). Alter-
natively, the marked hyperedge is in exactly one
of the children; for each possibility we recursively
choose a marked derivation for one child, while
the other children choose regular derivations. The
second summand of
?
I
n
(u) compactly expresses
a sum over instances of this case.
?
O
n
(u) de-
composes similarly: the marked hyperedge could
be local (first summand), under a sibling (second
summand), or higher in the tree (third summand).
Once we have these new inside-outside quanti-
ties, we can compute
?
D as in Figure 5. This com-
bination states that marked derivations containing
h are either marked at h, below h, or above h.
As a final detail, computing the gradient
?C
clip
n
(?) (see the Appendix) involves a clipped
version of the expected product of counts, for
which a clipped
?
D is required. This quantity can
be computed with the same dynamic program with
a slight modification. In Figure 4, we show the dif-
ference as a choice point when computing `
n
(h).
3.5 Implementation Details
As stated, the runtime of computing the required
expectations for the objective and gradient is lin-
ear in the number of hyperedges in the forest. The
1422
?I
n
(u) =
?
h?IN(u)
w(h)
?
?
`
n
(h)
?
v?tail(h)
I(v) +
?
v?tail(h)
?
I
n
(v)
?
w 6=v
I(w)
?
?
?
O
n
(u) =
?
h?OUT(u)
w(h)
?
?
?
?
?
?
`
n
(h) O(head(h))
?
v?tail(h)
v 6=u
I(v) + O(head(h))
?
v?tail(h)
v 6=u
?
I
n
(v)
?
w?tail(h)
w 6=v
w 6=u
I(w) +
?
O
n
(head(h))
?
w?tail(h)
w 6=u
I(w)
?
?
?
?
?
?
`
n
(h) =
{
?
g
n
c(g
n
, h) computing unclipped counts
?
g
n
c(g
n
, h)1 [E
?
[c(g
n
, d)] ? c(g
n
, r
i
)] computing clipped counts
Figure 4: Inside and Outside recursions for
?
I
n
(u) and
?
O
n
(u). IN(u) and OUT(u) refer to the incoming and
outgoing hyperedges of u, respectively. I(?) and O(?) refer to standard inside and outside quantities, defined in
Appendix Figure 7. We initialize with
?
I
n
(u) = 0 for all terminal forest nodes u and
?
O
n
(root) = 0 for the root
node. `
n
(h) computes the sum of all n-grams of order n on a hyperedge h.
?
D
n
?
(h|f
i
) =
w(h)
?
?
?
?
`
n
(h)O(head(h))
?
v?tail(h)
I(v) + O(head(h))
?
v?tail(h)
?
I
n
(v)
?
v?tail(h)
w 6=v
I(w) +
?
O
n
(head(h))
?
w?tail(h)
I(w)
?
?
?
?
Figure 5: Calculation of
?
D
n
?
(h|f
i
) after
?
I
n
(u) and
?
O
n
(u) have been computed.
number of hyperedges is very large, however, be-
cause we must track n-gram contexts in the nodes,
just as we would in an integrated language model
decoder. These contexts are required both to cor-
rectly compute the model score of derivations and
to compute clipped n-gram counts. To speed our
computations, we use the cube pruning method of
Huang and Chiang (2007) with a fixed beam size.
For regularization, we added an L
2
penalty on
the size of ? to the CoBLEU objective, a simple
addition for gradient ascent. We did not find that
our performance varied very much for moderate
levels of regularization.
3.6 Related Work
The calculation of expected counts can be for-
mulated using the expectation semiring frame-
work of Eisner (2002), though that work does
not show how to compute expected products of
counts which are needed for our gradient calcu-
lations. Concurrently with this work, Li and Eis-
ner (2009) have generalized Eisner (2002) to com-
pute expected products of counts on translation
forests. The training algorithm of Kakade et al
(2002) makes use of a dynamic program similar to
ours, though specialized to the case of sequence
models.
4 Consensus Decoding
Once model parameters ? are learned, we must
select an appropriate decoding objective. Sev-
eral new decoding approaches have been proposed
recently that leverage some notion of consensus
over the many weighted derivations in a transla-
tion forest. In this paper, we adopt the fast consen-
sus decoding procedure of DeNero et al (2009),
which directly complements CoBLEU tuning. For
a source sentence f , we first build a translation
forest, then compute the expected count of each
n-gram in the translation of f under the model.
We extract a k-best list from the forest, then select
the translation that yields the highest BLEU score
relative to the forest?s expected n-gram counts.
Specifically, let BLEU(e; r) compute the simi-
larity of a sentence e to a reference r based on
the n-gram counts of each. When training with
CoBLEU, we replace e with expected counts and
maximize ?. In consensus decoding, we replace r
with expected counts and maximize e.
Several other efficient consensus decoding pro-
1423
cedures would similarly benefit from a tuning pro-
cedure that aggregates over derivations. For in-
stance, Blunsom and Osborne (2008) select the
translation sentence with highest posterior proba-
bility under the model, summing over derivations.
Li et al (2009) propose a variational approxima-
tion maximizing sentence probability that decom-
poses over n-grams. Tromble et al (2008) min-
imize risk under a loss function based on the lin-
ear Taylor approximation to BLEU, which decom-
poses over n-gram posterior probabilities.
5 Experiments
We compared CoBLEU training with an imple-
mentation of minimum error rate training on two
language pairs.
5.1 Model
Our optimization procedure is in principle
tractable for any syntactic translation system. For
simplicity, we evaluate the objective using an In-
version Transduction Grammar (ITG) (Wu, 1997)
that emits phrases as terminal productions, as in
(Cherry and Lin, 2007). Phrasal ITG models have
been shown to perform comparably to the state-of-
the art phrase-based system Moses (Koehn et al,
2007) when using the same phrase table (Petrov et
al., 2008).
We extract a phrase table using the Moses
pipeline, based on Model 4 word alignments gen-
erated from GIZA++ (Och and Ney, 2003). Our fi-
nal ITG grammar includes the five standard Moses
features, an n-gram language model, a length fea-
ture that counts the number of target words, a fea-
ture that counts the number of monotonic ITG
rewrites, and a feature that counts the number of
inverted ITG rewrites.
5.2 Data
We extracted phrase tables from the Spanish-
English and French-English sections of the Eu-
roparl corpus, which include approximately 8.5
million words of bitext for each of the language
pairs (Koehn, 2002). We used a trigram lan-
guage model trained on the entire corpus of En-
glish parliamentary proceedings provided with the
Europarl distribution and generated according to
the ACL 2008 SMT shared task specifications.
6
For tuning, we used all sentences from the 2007
SMT shared task up to length 25 (880 sentences
6
See http://www.statmt.org/wmt08 for details.
2 4 6 8 10
0.00
.20.
40.6
0.81
.0
Iterations
Fractio
n of Va
lue at C
onverge
nce
CoBLEU
MERT
Figure 6: Trajectories of MERT and CoBLEU dur-
ing optimization show that MERT is initially unstable,
while CoBLEU training follows a smooth path to con-
vergence. Because these two training procedures op-
timize different functions, we have normalized each
trajectory by the final objective value at convergence.
Therefore, the absolute values of this plot do not re-
flect the performance of either objective, but rather
the smoothness with which the final objective is ap-
proached. The rates of convergence shown in this plot
are not directly comparable. Each iteration for MERT
above includes 10 iterations of coordinate ascent, fol-
lowed by a decoding pass through the training set. Each
iteration of CoBLEU training involves only one gradi-
ent step.
for Spanish and 923 for French), and we tested on
the subset of the first 1000 development set sen-
tences which had length at most 25 words (447
sentences for Spanish and 512 for French).
5.3 Tuning Optimization
We compared two techniques for tuning the nine
log-linear model parameters of our ITG grammar.
We maximized CoBLEU using gradient ascent, as
described above. As a baseline, we maximized
BLEU of the Viterbi translation derivations using
minimum error rate training. To improve opti-
mization stability, MERT used a cumulative k-best
list that included all translations generated during
the tuning process.
One of the benefits of CoBLEU training is that
we compute expectations efficiently over an entire
forest of translations. This has substantial stabil-
ity benefits over methods based on k-best lists. In
Figure 6, we show the progress of CoBLEU as
compared to MERT. Both models are initialized
from 0 and use the same features. This plot ex-
hibits a known issue with MERT training: because
new k-best lists are generated at each iteration,
the objective function can change drastically be-
tween iterations. In contrast, CoBLEU converges
1424
Consensus Decoding
Spanish
Tune Test ? Br.
MERT 32.5 30.2 -2.3 0.992
CoBLEU 31.4 30.4 -1.0 0.992
MERT?CoBLEU 31.7 30.8 -0.9 0.992
French
Tune Test ? Br.
MERT 32.5 31.1* -1.4 0.972
CoBLEU 31.9 30.9 -1.0 0.954
MERT?CoBLEU 32.4 31.2* -0.8 0.953
Table 1: Performance measured by BLEU using a con-
sensus decoding method over translation forests shows
an improvement over MERT when using CoBLEU
training. The first two conditions were initialized by
0 vectors. The third condition was initialized by the
final parameters of MERT training. Br. indicates the
brevity penalty on the test set. The * indicates differ-
ences which are not statistically significant.
smoothly to its final objective because the forests
do not change substantially between iterations, de-
spite the pruning needed to track n-grams. Similar
stability benefits have been observed for lattice-
based MERT (Macherey et al, 2008).
5.4 Results
We performed experiments from both French and
Spanish into English under three conditions. In the
first two, we initialized both MERT and CoBLEU
training uniformly with zero weights and trained
until convergence. In the third condition, we ini-
tialized CoBLEU with the final parameters from
MERT training, denoted MERT?CoBLEU in the
results tables. We evaluated each of these condi-
tions on both the tuning and test sets using the con-
sensus decoding method of DeNero et al (2009).
The results appear in Table 1.
In Spanish-English, CoBLEU slightly outper-
formed MERT under the same initialization, while
the opposite pattern appears for French-English.
The best test set performance in both language
pairs was the third condition, in which CoBLEU
training was initialized with MERT. This con-
dition also gave the highest CoBLEU objective
value. This pattern indicates that CoBLEU is a
useful objective for translation with consensus de-
coding, but that the gradient ascent optimization is
getting stuck in local maxima during tuning. This
issue can likely be addressed with annealing, as
described in (Smith and Eisner, 2006).
Interestingly, the brevity penatly results in
French indicate that, even though CoBLEU did
Viterbi Decoding
Spanish
Tune Test ?
MERT 32.5 30.2 -2.3
MERT?CoBLEU 30.5 30.9 +0.4
French
Tune Test ?
MERT 32.0 31.0 -1.0
MERT?CoBLEU 31.7 30.9 -0.8
Table 2: Performance measured by BLEU using Viterbi
decoding indicates that CoBLEU is less prone to over-
fitting than MERT.
not outperform MERT in a statistically significant
way, CoBLEU tends to find shorter sentences with
higher n-gram precision than MERT.
Table 1 displays a second benefit of CoBLEU
training: compared to MERT training, CoBLEU
performance degrades less from tuning to test
set. In Spanish, initializing with MERT-trained
weights and then training with CoBLEU actually
decreases BLEU on the tuning set by 0.8 points.
However, this drop in tuning performance comes
with a corresponding increase of 0.6 on the test
set, relative to MERT training. We see the same
pattern in French, albeit to a smaller degree.
While CoBLEU ought to outperform MERT us-
ing consensus decoding, we expected that MERT
would give better performance under Viterbi de-
coding. Surprisingly, we found that CoBLEU
training actually outperformed MERT in Spanish-
English and performed equally well in French-
English. Table 2 shows the results. In these ex-
periments, we again see that CoBLEU overfit the
training set to a lesser degree than MERT, as evi-
denced by a smaller drop in performance from tun-
ing to test set. In fact, test set performance actually
improved for Spanish-English CoBLEU training
while dropping by 2.3 BLEU for MERT.
6 Conclusion
CoBLEU takes a fundamental quantity used in
consensus decoding, expected n-grams, and trains
to optimize a function of those expectations.
While CoBLEU can therefore be expected to in-
crease test set BLEU under consensus decoding, it
is more surprising that it seems to better regularize
learning even for the Viterbi decoding condition.
It is also worth emphasizing that the CoBLEU ap-
proach is applicable to functions of expected n-
gram counts other than BLEU.
1425
Appendix: The Gradient of CoBLEU
We would like to compute the gradient of
(
1?
|R|
?
m
i=1
?
g
1
E
?
[c(g
1
, d)|f
i
]
)
?
+
1
4
4
?
n=1
ln
?
m
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(g
n
, r
i
)}
?
m
i=1
?
g
n
E
?
[c(g
n
, d)|f
i
]
To simplify notation, we introduce the functions
C
n
(?) =
m
?
i=1
?
g
n
E
?
[c(g
n
, e)|f
i
]
C
clip
n
(?) =
m
?
i=1
?
g
n
min{E
?
[c(g
n
, d)|f
i
], c(r, g
n
)}
C
n
(?) represents the sum of the expected counts
of all n-grams or order n in all translations of
the source corpus F , while C
clip
n
(?) represents the
sum of the same expected counts, but clipped with
reference counts c(g
n
, r
i
).
With this notation, we can write our objective
function CoBLEU(R,F, ?) in three terms:
(
1?
|R|
C
1
(?)
)
?
+
1
4
4
?
n=1
lnC
clip
n
(?)?
1
4
4
?
n=1
lnC
n
(?)
We first state an identity:
?
g
n
?
??
k
E
?
[c(g
n
, d)|f
i
] =
E
?
[c(?
k
, d) ? `
n
(d)|f
i
]
?E
?
[`
n
(d)|f
i
] ? E
?
[c(?
k
, d)|f
i
]
which can be derived by expanding the expectation on
the left-hand side
?
g
n
?
d
?
??
k
P
?
(d|f
i
)c(g
n
, d)
and substituting
?
??
k
P
?
(d|f
i
) =
P
?
(d|f
i
)c(?
k
, d)? P
?
(d|f
i
)
?
d
?
P
?
(d
?
|f
i
)c(?
k
, d
?
)
Using this identity and some basic calculus, the
gradient?C
n
(?) is
m
?
i=1
E
?
[c(?
k
, d) ? `
n
(d)|f
i
]? C
n
(?)E
?
[c(?
k
, d)|f
i
]
I(u) =
?
h?IN(u)
w(h)
?
?
?
v?tail(h)
I(v)
?
?
O(u) =
?
h?OUT (u)
w(h)
?
?
?
?
O(head(h))
?
v?tail(h)
v 6=u
I(v)
?
?
?
?
Figure 7: Standard Inside-Outside recursions which
compute I(u) and O(u). IN(u) and OUT(u) refer to the
incoming and outgoing hyperedges of u, respectively.
We initialize with I(u) = 1 for all terminal forest nodes
u and O(root) = 1 for the root node. These quantities
are referenced in Figure 4.
and the gradient?C
clip
n
(?) is given by
m
?
i=1
?
g
n
[
E
?
[c(g
n
, d) ? c(?
k
, d)|f
i
]
?1
[
E
?
[c(g
n
, d)|f
i
] ? c(g
n
, r
i
)
]
]
?C
clip
n
(?)E
?
[c(?
k
, d) + f
i
]
where 1 denotes an indicator function. At the top
level, the gradient of the first term (the brevity
penalty) is
|R|?C
1
(?)
C
1
(?)
2
1
[
C
1
(?) ? |R|
]
The gradient of the second term is
1
4
4
?
n=1
?C
clip
n
(?)
C
clip
n
(?)
and the gradient of the third term is
?
1
4
4
?
n=1
?C
n
(?)
C
n
(?)
Note that, because of the indicator func-
tions, CoBLEU is non-differentiable when
E
?
[c(g
n
, d)|f
i
] = c(g
n
, r
i
) or C
n
(?) = |R|.
Formally, we must compute a sub-gradient at
these points. In practice, we can choose between
the gradients calculated assuming the indicator
function is 0 or 1; we always choose the latter.
1426
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proceedings
of the Conference on Emprical Methods for Natural
Language Processing.
Colin Cherry and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In The Annual Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics Workshop on Syntax and Structure in
Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In The Conference on Em-
pirical Methods in Natural Language Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
The Annual Conference of the Association for Com-
putational Linguistics.
Jason Eisner. 2002. Parameter estimation for prob-
abilistic finite-state transducers. In Proceedings of
the 40th Annual Meeting on Association for Compu-
tational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In The Annual Conference of the Association for
Computational Linguistics.
Sham Kakade, Yee Whye Teh, and Sam T. Roweis.
2002. An alternate objective function for markovian
fields. In Proceedings of ICML.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
The Annual Conference of the Association for Com-
putational Linguistics.
Philipp Koehn. 2002. Europarl: A multilingual corpus
for evaluation of machine translation.
Shankar Kumar, Wolfgang Macherey, Chris Dyer,
and Franz Och. 2009. Efficient minimum error
rate training and minimum Bayes-risk decoding for
translation hypergraphs and lattices. In The Annual
Conference of the Association for Computational
Linguistics.
Zhifei Li and Jason Eisner. 2009. First- and second-
order expectation semirings with applications to
minimum-risk training on translation forests. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In The Annual Conference of the Association
for Computational Linguistics.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit.
2008. Lattice-based minimum error rate training for
statistical machine translation. In In Proceedings of
Empirical Methods in Natural Language Process-
ing.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19?51.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics (ACL), pages 160?167, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In The Annual
Conference of the Association for Computational
Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation us-
ing language projections. In Proceedings of the
2008 Conference on Empirical Methods in Nat-
ural Language Processing, pages 108?116, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
David Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In In Pro-
ceedings of the Association for Computational Lin-
guistics.
Roy Tromble, Shankar Kumar, Franz Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In The
Conference on Empirical Methods in Natural Lan-
guage Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
1427
Proceedings of NAACL HLT 2007, pages 412?419,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Approximate Factoring for A? Search
Aria Haghighi, John DeNero, Dan Klein
Computer Science Division
University of California Berkeley
{aria42, denero, klein}@cs.berkeley.edu
Abstract
We present a novel method for creating A? esti-
mates for structured search problems. In our ap-
proach, we project a complex model onto multiple
simpler models for which exact inference is effi-
cient. We use an optimization framework to es-
timate parameters for these projections in a way
which bounds the true costs. Similar to Klein and
Manning (2003), we then combine completion es-
timates from the simpler models to guide search
in the original complex model. We apply our ap-
proach to bitext parsing and lexicalized parsing,
demonstrating its effectiveness in these domains.
1 Introduction
Inference tasks in NLP often involve searching for
an optimal output from a large set of structured out-
puts. For many complex models, selecting the high-
est scoring output for a given observation is slow or
even intractable. One general technique to increase
efficiency while preserving optimality is A? search
(Hart et al, 1968); however, successfully using A?
search is challenging in practice. The design of ad-
missible (or nearly admissible) heuristics which are
both effective (close to actual completion costs) and
also efficient to compute is a difficult, open prob-
lem in most domains. As a result, most work on
search has focused on non-optimal methods, such
as beam search or pruning based on approximate
models (Collins, 1999), though in certain cases ad-
missible heuristics are known (Och and Ney, 2000;
Zhang and Gildea, 2006). For example, Klein and
Manning (2003) show a class of projection-based A?
estimates, but their application is limited to models
which have a very restrictive kind of score decom-
position. In this work, we broaden their projection-
based technique to give A? estimates for models
which do not factor in this restricted way.
Like Klein and Manning (2003), we focus on
search problems where there are multiple projec-
tions or ?views? of the structure, for example lexical
parsing, in which trees can be projected onto either
their CFG backbone or their lexical attachments. We
use general optimization techniques (Boyd and Van-
denberghe, 2005) to approximately factor a model
over these projections. Solutions to the projected
problems yield heuristics for the original model.
This approach is flexible, providing either admissi-
ble or nearly admissible heuristics, depending on the
details of the optimization problem solved. Further-
more, our approach allows a modeler explicit control
over the trade-off between the tightness of a heuris-
tic and its degree of inadmissibility (if any). We de-
scribe our technique in general and then apply it to
two concrete NLP search tasks: bitext parsing and
lexicalized monolingual parsing.
2 General Approach
Many inference problems in NLP can be solved
with agenda-based methods, in which we incremen-
tally build hypotheses for larger items by combining
smaller ones with some local configurational struc-
ture. We can formalize such tasks as graph search
problems, where states encapsulate partial hypothe-
ses and edges combine or extend them locally.1 For
example, in HMM decoding, the states are anchored
labels, e.g. VBD[5], and edges correspond to hidden
transitions, e.g. VBD[5] ? DT[6].
The search problem is to find a minimal cost path
from the start state to a goal state, where the path
cost is the sum of the costs of the edges in the path.
1In most complex tasks, we will in fact have a hypergraph,
but the extension is trivial and not worth the added notation.
412
( a
a?
)
?
( b
b?
)
( b
a?
)
?
( c
b?
)
1
( a
a?
)
?
( b
b?
)
( b
b?
)
?
( c
c?
)
1
( a
a?
)
?
( b
b?
)
( a
b?
)
?
( b
c?
)
1
( a
a?
)
?
( b
b?
)
( a
b?
)
?
( b
c?
)
1
Local Configurations
a' ? b' b'  c'
a ? b
b ? c
3.0 4.0
3.02.0
2.01.0
2.0
1.0
Factored Cost Matrix
Original Cost Matrix
3.0 4.0
3.02.0a ? b
b ? c
a' ? b' b' ? c'
c(a ? b)
c(b ? c)
c(a' ? b')
c(a' ? b')
3.0 4.0
3.02.0
2.01.0
2.0
1.0
Factored Cost Matrix
Original Cost Matrix
3.0 5.0
3.02.0a ? b
b ? c
a' ? b' b' ? c'
c(a ? b)
c(b ? c)
c(a' ? b')
c(a' ? b')
(a) (b) (c)
Figure 1: Example cost factoring: In (a), each cell of the matrix is a local configuration composed of two projections (the row and
column of the cell). In (b), the top matrix is an example cost matrix, which specifies the cost of each local configuration. The
bottom matrix represents our factored estimates, where each entry is the sum of configuration projections. For this example, the
actual cost matrix can be decomposed exactly into two projections. In (c), the top cost matrix cannot be exactly decomposed along
two dimensions. Our factored cost matrix has the property that each factored cost estimate is below the actual configuration cost.
Although our factorization is no longer tight, it still can be used to produce an admissible heuristic.
For probabilistic inference problems, the cost of an
edge is typically a negative log probability which de-
pends only on some local configuration type. For
instance, in PCFG parsing, the (hyper)edges refer-
ence anchored spans X[i, j], but the edge costs de-
pend only on the local rule type X ? Y Z. We will
use a to refer to a local configuration and use c(a)
to refer to its cost. Because edge costs are sensi-
tive only to local configurations, the cost of a path
is
?
a c(a). A? search requires a heuristic function,
which is an estimate h(s) of the completion cost, the
cost of a best path from state s to a goal.
In this work, following Klein and Manning
(2003), we consider problems with projections or
?views,? which define mappings to simpler state and
configuration spaces. For instance, suppose that we
are using an HMM to jointly model part-of-speech
(POS) and named-entity-recognition (NER) tagging.
There might be one projection onto the NER com-
ponent and another onto the POS component. For-
mally, a projection pi is a mapping from states to
some coarser domain. A state projection induces
projections of edges and of the entire graph pi(G).
We are particularly interested in search problems
with multiple projections {pi1, . . . , pi`} where each
projection, pii, has the following properties: its state
projections induce well-defined projections of the
local configurations pii(a) used for scoring, and the
projected search problem admits a simpler infer-
ence. For instance, the POS projection in our NER-
POS HMM is a simpler HMM, though the gains
from this method are greater when inference in the
projections have lower asymptotic complexity than
the original problem (see sections 3 and 4).
In defining projections, we have not yet dealt with
the projected scoring function. Suppose that the
cost of local configurations decomposes along pro-
jections as well. In this case,
c (a) =
?`
i=1
ci(a) , ?a ? A (1)
where A is the set of local configurations and ci(a)
represents the cost of configuration a under projec-
tion pii. A toy example of such a cost decomposi-
tion in the context of a Markov process over two-part
states is shown in figure 1(b), where the costs of the
joint transitions equal the sum of costs of their pro-
jections. Under the strong assumption of equation
(1), Klein and Manning (2003) give an admissible
A? bound. They note that the cost of a path decom-
poses as a sum of projected path costs. Hence, the
following is an admissible additive heuristic (Felner
et al, 2004),
h(s) =
?`
i=1
h?i (s) (2)
where h?i (s) denote the optimal completion costs in
the projected search graph pii(G). That is, the com-
pletion cost of a state bounds the sum of the comple-
tion costs in each projection.
In virtually all cases, however, configuration costs
will not decompose over projections, nor would we
expect them to. For instance, in our joint POS-NER
task, this assumption requires that the POS and NER
413
transitions and observations be generated indepen-
dently. This independence assumption undermines
the motivation for assuming a joint model. In the
central contribution of this work, we exploit the pro-
jection structure of our search problem without mak-
ing any assumption about cost decomposition.
Rather than assuming decomposition, we propose
to find scores ? for the projected configurations
which are pointwise admissible:
?`
i=1
?i(a) ? c(a), ?a ? A (3)
Here, ?i(a) represents a factored projection cost of
pii(a), the pii projection of configuration a. Given
pointwise admissible ?i?s we can again apply the
heuristic recipe of equation (2). An example of
factored projection costs are shown in figure 1(c),
where no exact decomposition exists, but a point-
wise admissible lower bound is easy to find.
Claim. If a set of factored projection costs
{?1, . . . , ?`} satisfy pointwise admissibility, then
the heuristic from (2) is an admissible A? heuristic.
Proof. Assume a1, . . . , ak are configurations used
to optimally reach the goal from state s. Then,
h?(s) =
kX
j=1
c(aj) ?
kX
j=1
X`
i=1
?i(aj)
=
X`
i=1
 
kX
j=1
?i(aj)
!
?
X`
i=1
h?i (s) = h(s)
The first inequality follows from pointwise admis-
sibility. The second inequality follows because each
inner sum is a completion cost for projected problem
pii and therefore h?i (s) lower bounds it. Intuitively,
we can see two sources of slack in such projection
heuristics. First, there may be slack in the pointwise
admissible scores. Second, the best paths in the pro-
jections will be overly optimistic because they have
been decoupled (see figure 5 for an example of de-
coupled best paths in projections).
2.1 Finding Factored Projections for
Non-Factored Costs
We can find factored costs ?i(a) which are point-
wise admissible by solving an optimization problem.
We think of our unknown factored costs as a block
vector ? = [?1, .., ?`], where vector ?i is composed
of the factored costs, ?i(a), for each configuration
a ? A. We can then find admissible factored costs
by solving the following optimization problem,
minimize
?
??? (4)
such that, ?a = c(a)?
?`
i=1
?i(a), ?a ? A
?a ? 0, ?a ? A
We can think of each ?a as the amount by which
the cost of configuration a exceeds the factored pro-
jection estimates (the pointwise A? gap). Requiring
?a ? 0 insures pointwise admissibility. Minimiz-
ing the norm of the ?a variables encourages tighter
bounds; indeed if ??? = 0, the solution corresponds
to an exact factoring of the search problem. In the
case where we minimize the 1-norm or ?-norm, the
problem above reduces to a linear program, which
can be solved efficiently for a large number of vari-
ables and constraints.2
Viewing our procedure decision-theoretically, by
minimizing the norm of the pointwise gaps we are
effectively choosing a loss function which decom-
poses along configuration types and takes the form
of the norm (i.e. linear or squared losses). A com-
plete investigation of the alternatives is beyond the
scope of this work, but it is worth pointing out that
in the end we will care only about the gap on entire
structures, not configurations, and individual config-
uration factored costs need not even be pointwise ad-
missible for the overall heuristic to be admissible.
Notice that the number of constraints is |A|, the
number of possible local configurations. For many
search problems, enumerating the possible configu-
rations is not feasible, and therefore neither is solv-
ing an optimization problem with all of these con-
straints. We deal with this situation in applying our
technique to lexicalized parsing models (section 4).
Sometimes, we might be willing to trade search
optimality for efficiency. In our approach, we can
explicitly make this trade-off by designing an alter-
native optimization problem which allows for slack
2We used the MOSEK package (Andersen and Andersen,
2000).
414
in the admissibility constraints. We solve the follow-
ing soft version of problem (4):
minimize
?
??+?+ C???? (5)
such that, ?a = c(a)?
?`
i=1
?i(a), ?a ? A
where ?+ = max{0, ?} and ?? = max{0,??}
represent the componentwise positive and negative
elements of ? respectively. Each ??a > 0 represents
a configuration where our factored projection esti-
mate is not pointwise admissible. Since this situa-
tion may result in our heuristic becoming inadmis-
sible if used in the projected completion costs, we
more heavily penalize overestimating the cost by the
constant C.
2.2 Bounding Search Error
In the case where we allow pointwise inadmissibil-
ity, i.e. variables ??a , we can bound our search er-
ror. Suppose ??max = maxa?A ??a and that L? is
the length of the longest optimal solution for the
original problem. Then, h(s) ? h?(s) + L???max,
?s ? S. This ?-admissible heuristic (Ghallab and
Allard, 1982) bounds our search error by L???max.3
3 Bitext Parsing
In bitext parsing, one jointly infers a synchronous
phrase structure tree over a sentence ws and its
translation wt (Melamed et al, 2004; Wu, 1997).
Bitext parsing is a natural candidate task for our
approximate factoring technique. A synchronous
tree projects monolingual phrase structure trees onto
each sentence. However, the costs assigned by
a weighted synchronous grammar (WSG) G do
not typically factor into independent monolingual
WCFGs. We can, however, produce a useful surro-
gate: a pair of monolingual WCFGs with structures
projected by G and weights that, when combined,
underestimate the costs of G.
Parsing optimally relative to a synchronous gram-
mar using a dynamic program requires time O(n6)
in the length of the sentence (Wu, 1997). This high
degree of complexity makes exhaustive bitext pars-
ing infeasible for all but the shortest sentences. In
3This bound may be very loose if L is large.
contrast, monolingual CFG parsing requires time
O(n3) in the length of the sentence.
3.1 A? Parsing
Alternatively, we can search for an optimal parse
guided by a heuristic. The states in A? bitext pars-
ing are rooted bispans, denoted X [i, j] :: Y [k, l].
States represent a joint parse over subspans [i, j] of
ws and [k, l] of wt rooted by the nonterminals X and
Y respectively.
Given a WSG G, the algorithm prioritizes a state
(or edge) e by the sum of its inside cost ?G(e) (the
negative log of its inside probability) and its outside
estimate h(e), or completion cost.4 We are guaran-
teed the optimal parse if our heuristic h(e) is never
greater than ?G(e), the true outside cost of e.
We now consider a heuristic combining the com-
pletion costs of the monolingual projections of G,
and guarantee admissibility by enforcing point-wise
admissibility. Each state e = X [i, j] :: Y [k, l]
projects a pair of monolingual rooted spans. The
heuristic we propose sums independent outside costs
of these spans in each monolingual projection.
h(e) = ?s(X [i, j]) + ?t(Y [k, l])
These monolingual outside scores are computed rel-
ative to a pair of monolingual WCFG grammars Gs
and Gt given by splitting each synchronous rule
r =
(
X(s)
Y(t)
)
?
(
? ?
? ?
)
into its components pis(r) = X? ?? and pit(r) =
Y??? and weighting them via optimized ?s(r) and
?t(r), respectively.5
To learn pointwise admissible costs for the mono-
lingual grammars, we formulate the following opti-
mization problem:6
minimize
?,?s,?t
???1
such that, ?r = c(r)? [?s(r) + ?t(r)]
for all synchronous rules r ? G
?s ? 0, ?t ? 0, ? ? 0
4All inside and outside costs are Viterbi, not summed.
5Note that we need only parse each sentence (monolin-
gually) once to compute the outside probabilities for every span.
6The stated objective is merely one reasonable choice
among many possibilities which require pointwise admissibil-
ity and encourage tight estimates.
415
ij
k
l
S
o
u
r
c
e
T
a
r
g
e
t
i
j
k
l
S
o
u
r
c
e
T
a
r
g
e
t
i
j
k
l
S
o
u
r
c
e
T
a
r
g
e
t
? ?
Cost under Gt Cost under G
Synchronized completion 
scored by original model
Synchronized completion 
scored by factored model
Monolingual completions 
scored by factored model
Cost under Gs
Figure 2: The gap between the heuristic (left) and true comple-
tion cost (right) comes from relaxing the synchronized problem
to independent subproblems and slack in the factored models.
Figure 2 diagrams the two bounds that enforce the
admissibility of h(e). For any outside cost ?G(e),
there is a corresponding optimal completion struc-
ture o under G, which is an outer shell of a syn-
chronous tree. o projects monolingual completions
os and ot which have well-defined costs cs(os) and
ct(ot) under Gs and Gt respectively. Their sum
cs(os) + ct(ot) will underestimate ?G(e) by point-
wise admissibility.
Furthermore, the heuristic we compute underesti-
mates this sum. Recall that the monolingual outside
score ?s(X [i, j]) is the minimal costs for any com-
pletion of the edge. Hence, ?s(X [i, j]) ? cs(os)
and ?t(X [k, l]) ? ct(ot). Admissibility follows.
3.2 Experiments
We demonstrate our technique using the syn-
chronous grammar formalism of tree-to-tree trans-
ducers (Knight and Graehl, 2004). In each weighted
rule, an aligned pair of nonterminals generates two
ordered lists of children. The non-terminals in each
list must align one-to-one to the non-terminals in the
other, while the terminals are placed freely on either
side. Figure 3(a) shows an example rule.
Following Galley et al (2004), we learn a gram-
mar by projecting English syntax onto a foreign lan-
guage via word-level alignments, as in figure 3(b).7
We parsed 1200 English-Spanish sentences using
a grammar learned from 40,000 sentence pairs of
the English-Spanish Europarl corpus.8 Figure 4(a)
shows that A? expands substantially fewer states
while searching for the optimal parse with our op-
7The bilingual corpus consists of translation pairs with fixed
English parses and word alignments. Rules were scored by their
relative frequencies.
8Rare words were replaced with their parts of speech to limit
the memory consumption of the parser.
(a)
?
NP(s)
NP(t)
?
?
 
NN(s)1 NNS
(s)
2
NNS(t)2 de NN
(t)
1
!
(b)
T
r
a
n
s
l
a
t
i
o
n
s
y
s
t
e
m
s
s
o
m
e
t
i
m
e
s
w
o
r
k
sistemas
traduccion
funcionan
a
veces
de
NNS
NN
NP
NNSNN
NP
RB VB
S
Figure 3: (a) A tree-to-tree transducer rule. (b) An example
training sentence pair that yields rule (a).
timization heuristic. The exhaustive curve shows
edge expansions using the null heuristic. The in-
termediate result, labeled English only, used only
the English monolingual outside score as a heuris-
tic. Similar results using only Spanish demonstrate
that both projections contribute to parsing efficiency.
All three curves in figure 4 represent running times
for finding the optimal parse.
Zhang and Gildea (2006) offer a different heuris-
tic for A? parsing of ITG grammars that provides a
forward estimate of the cost of aligning the unparsed
words in both sentences. We cannot directly apply
this technique to our grammar because tree-to-tree
transducers only align non-terminals. Instead, we
can augment our synchronous grammar model to in-
clude a lexical alignment component, then employ
both heuristics. We learned the following two-stage
generative model: a tree-to-tree transducer generates
trees whose leaves are parts of speech. Then, the
words of each sentence are generated, either jointly
from aligned parts of speech or independently given
a null alignment. The cost of a complete parse un-
der this new model decomposes into the cost of the
synchronous tree over parts of speech and the cost
of generating the lexical items.
Given such a model, both our optimization heuris-
tic and the lexical heuristic of Zhang and Gildea
(2006) can be computed independently. Crucially,
the sum of these heuristics is still admissible. Re-
sults appear in figure 4(b). Both heuristics (lexi-
cal and optimization) alone improve parsing perfor-
mance, but their sum opt+lex substantially improves
upon either one.
416
(a) 050
100150
200
5 7 9 11 13 15Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveLexicalOptimizationOpt+Lex
050
100150
200
5 7 9 11 13 15Sentence lengthAv
g. Edges 
Popped
(in thous
ands) ExhaustiveEnglish OnlyOptimization
(b) 050
100150
200
5 7 9 1 13 15Sent ce lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveLexicalOptimizationOpt+Lex
050
100150
200
5 7 9 1 13 15Sent ce lengthAv
g. Edges 
Popped
(in thous
ands) ExhaustiveEnglish OnlyOptimization
Figure 4: (a) Parsing efficiency results with optimization heuristics show that both component projections constrain the problem.
(b) Including a lexical model and corresponding heuristic further increases parsing efficiency.
4 Lexicalized Parsing
We next apply our technique to lexicalized pars-
ing (Charniak, 1997; Collins, 1999). In lexical-
ized parsing, the local configurations are lexicalized
rules of the form X[h, t] ? Y [h?, t?] Z[h, t], where
h, t, h?, and t? are the head word, head tag, ar-
gument word, and argument tag, respectively. We
will use r = X ? Y Z to refer to the CFG back-
bone of a lexicalized rule. As in Klein and Man-
ning (2003), we view each lexicalized rule, `, as
having a CFG projection, pic(`) = r, and a de-
pendency projection, pid(`) = (h, t, h?, t?)(see fig-
ure 5).9 Broadly, the CFG projection encodes con-
stituency structure, while the dependency projection
encodes lexical selection, and both projections are
asymptotically more efficient than the original prob-
lem. Klein and Manning (2003) present a factored
model where the CFG and dependency projections
are generated independently (though with compati-
ble bracketing):
P (Y [h, t]Z[h?, t?] | X[h, t]) = (6)
P (Y Z|X)P (h?, t?|t, h)
In this work, we explore the following non-factored
model, which allows correlations between the CFG
and dependency projections:
P (Y [h, t]Z[h?, t?] | X[h, t]) = P (Y Z|X, t, h) (7)
P (t?|t, Z, h?, h) P (h?|t?, t, Z, h?, h)
This model is broadly representative of the suc-
cessful lexicalized models of Charniak (1997) and
9We assume information about the distance and direction of
the dependency is encoded in the dependency tuple, but we omit
it from the notation for compactness.
Collins (1999), though simpler.10
4.1 Choosing Constraints and Handling
Unseen Dependencies
Ideally we would like to be able to solve the op-
timization problem in (4) for this task. Unfortu-
nately, exhaustively listing all possible configura-
tions (lexical rules) yields an impractical number of
constraints. We therefore solve a relaxed problem in
which we enforce the constraints for only a subset
of the possible configurations, A? ? A. Once we
start dropping constraints, we can no longer guaran-
tee pointwise admissibility, and therefore there is no
reason not to also allow penalized violations of the
constraints we do list, so we solve (5) instead.
To generate the set of enforced constraints, we
first include all configurations observed in the gold
training trees. We then sample novel configurations
by choosing (X,h, t) from the training distribution
and then using the model to generate the rest of the
configuration. In our experiments, we ended up with
434,329 observed configurations, and sampled the
same number of novel configurations. Our penalty
multiplier C was 10.
Even if we supplement our training set with many
sample configurations, we will still see new pro-
jected dependency configurations at test time. It is
therefore necessary to generalize scores from train-
ing configurations to unseen ones. We enrich our
procedure by expressing the projected configuration
costs as linear functions of features. Specifically, we
define feature vectors fc(r) and fd(h, t, h?t?) over
the CFG and dependency projections, and intro-
10All probability distributions for the non-factored model are
estimated by Witten-Bell smoothing (Witten and Bell, 1991)
where conditioning lexical items are backed off first.
417
SXXXXXXNPSaaa!!!NPNP
DT
These
PPNPHHHNNS
stocks
NPPP
RB
eventually
VPS
VBD
reopened
reopened-VBDhhhhhhhh""((((((((These-DT
These
stocks-NNS
stocks
reopened-VBDPPPP
eventually-RB
eventually
reopened-VBD
reopened
S, reopened-VBDhhhhhhhhhh
((((((((((NPS , stocks-NNSbb""DT
These
NNS
stocks
ADVPS , eventually-RB
RB
eventually
VPS , reopened-VBD
VBD
reopened
Actual Cost: 18.7
Best Projected CFG Cost: 4.1 Best Projected Dep. Cost: 9.5 CFG Projection Cost : 6.9
Dep. Projection Cost: 11.1(a) (b) (c)
Figure 5: Lexicalized parsing projections. The figure in (a) is the optimal CFG projection solution and the figure in (b) is the
optimal dependency projection solution. The tree in (c) is the optimal solution for the original problem. Note that the sum of the
CFG and dependency projections is a lower bound (albeit a fairly tight one) on actual solution cost.
duce corresponding weight vectors wc and wd. The
weight vectors are learned by solving the following
optimization problem:
minimize
?,wc,wd
??+?2 + C????2 (8)
such that, wc ? 0, wd ? 0
?` = c(`)? [w
T
c fc(r) + w
T
d fd(h, t, h
?, t?)]
for ` = (r, h, t, h?, t?) ? A?
Our CFG feature vector has only indicator features
for the specific rule. However, our dependency fea-
ture vector consists of an indicator feature of the tu-
ple (h, t, h?, t?) (including direction), an indicator of
the part-of-speech type (t, t?) (also including direc-
tion), as well as a bias feature.
4.2 Experimental Results
We tested our approximate projection heuristic on
two lexicalized parsing models. The first is the fac-
tored model of Klein and Manning (2003), given
by equation (6), and the second is the non-factored
model described in equation (7). Both models
use the same parent-annotated head-binarized CFG
backbone and a basic dependency projection which
models direction, but not distance or valence.11
In each case, we compared A? using our approxi-
mate projection heuristics to exhaustive search. We
measure efficiency in terms of the number of ex-
panded hypotheses (edges popped); see figure 6.12
In both settings, the factored A? approach substan-
tially outperforms exhaustive search. For the fac-
11The CFG and dependency projections correspond to the
PCFG-PA and DEP-BASIC settings in Klein and Manning
(2003).
12All models are trained on section 2 through 21 of the En-
glish Penn treebank, and tested on section 23.
tored model of Klein and Manning (2003), we can
also compare our reconstructed bound to the known
tight bound which would result from solving the
pointwise admissible problem in (4) with all con-
straints. As figure 6 shows, the exact factored
heuristic does outperform our approximate factored
heuristic, primarily because of many looser, backed-
off cost estimates for unseen dependency tuples. For
the non-factored model, we compared our approxi-
mate factored heuristic to one which only bounds the
CFG projection as suggested by Klein and Manning
(2003). They suggest,
?c(r) = min
`?A:pic(`)=r
c(`)
where we obtain factored CFG costs by minimizing
over dependency projections. As figure 6 illustrates,
this CFG only heuristic is substantially less efficient
than our heuristic which bounds both projections.
Since our heuristic is no longer guaranteed to be
admissible, we evaluated its effect on search in sev-
eral ways. The first is to check for search errors,
where the model-optimal parse is not found. In the
case of the factored model, we can find the optimal
parse using the exact factored heuristic and compare
it to the parse found by our learned heuristic. In our
test set, the approximate projection heuristic failed
to return the model optimal parse in less than 1% of
sentences. Of these search errors, none of the costs
were more than 0.1% greater than the model optimal
cost in negative log-likelihood. For the non-factored
model, the model optimal parse is known only for
shorter sentences which can be parsed exhaustively.
For these sentences up to length 15, there were no
search errors. We can also check for violations of
pointwise admissibility for configurations encoun-
418
(a)
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveCFG OnlyApprox. Factored
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveApprox. FactoredExact Factored
(b)
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveCFG OnlyApprox. Factored
050
100150
200
5 10 15 20 25 30 35 40Sentence lengthA
vg. Edges
 Popped
(in thous
ands) ExhaustiveApprox. FactoredExact Factored
Figure 6: Edges popped by exhaustive versus factored A? search. The chart in (a) is using the factored lexicalized model from
Klein and Manning (2003). The chart in (b) is using the non-factored lexicalized model described in section 4.
tered during search. For both the factored and non-
factored model, less than 2% of the configurations
scored by the approximate projection heuristic dur-
ing search violated pointwise admissibility.
While this is a paper about inference, we also
measured the accuracy in the standard way, on sen-
tences of length up to 40, using EVALB. The fac-
tored model with the approximate projection heuris-
tic achieves an F1 of 82.2, matching the performance
with the exact factored heuristic, though slower. The
non-factored model, using the approximate projec-
tion heuristic, achieves an F1 of 83.8 on the test set,
which is slightly better than the factored model.13
We note that the CFG and dependency projections
are as similar as possible across models, so the in-
crease in accuracy is likely due in part to the non-
factored model?s coupling of CFG and dependency
projections.
5 Conclusion
We have presented a technique for creating A? es-
timates for inference in complex models. Our tech-
nique can be used to generate provably admissible
estimates when all search transitions can be enumer-
ated, and an effective heuristic even for problems
where all transitions cannot be efficiently enumer-
ated. In the future, we plan to investigate alterna-
tive objective functions and error-driven methods for
learning heuristic bounds.
Acknowledgments We would like to thank the
anonymous reviewers for their comments. This
work is supported by a DHS fellowship to the first
13Since we cannot exhaustively parse with this model, we
cannot compare our F1 to an exact search method.
author and a Microsoft new faculty fellowship to the
third author.
References
E. D. Andersen and K. D. Andersen. 2000. The MOSEK in-
terior point optimizer for linear programming. In H. Frenk
et al, editor, High Performance Optimization. Kluwer Aca-
demic Publishers.
Stephen Boyd and Lieven Vandenberghe. 2005. Convex Opti-
mization. Cambridge University Press.
Eugene Charniak. 1997. Statistical parsing with a context-free
grammar and word statistics. In National Conference on Ar-
tificial Intelligence.
Michael Collins. 1999. Head-driven statistical models for nat-
ural language parsing.
Ariel Felner, Richard Korf, and Sarit Hanan. 2004. Additive
pattern database heuristics. JAIR.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-NAACL.
Malik Ghallab and Dennis G. Allard. 1982. A?? - an efficient
near admissible heuristic search algorithm. In IJCAI.
P. Hart, N. Nilsson, and B. Raphael. 1968. A formal basis for
the heuristic determination of minimum cost paths. In IEEE
Transactions on Systems Science and Cybernetics. IEEE.
Dan Klein and Christopher D. Manning. 2003. Factored A*
search for models over sequences and trees. In IJCAI.
Kevin Knight and Jonathan Graehl. 2004. Training tree trans-
ducers. In HLT-NAACL.
I. Dan Melamed, Giorgio Satta, and Ben Wellington. 2004.
Generalized multitext grammars. In ACL.
F. J. Och and H. Ney. 2000. Improved statistical alignment
models. In ACL.
Ian H. Witten and Timothy C. Bell. 1991. The zero-frequency
problem: Estimating the probabilities of novel events in
adaptive text compression. IEEE.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Comput. Linguist.
Hao Zhang and Daniel Gildea. 2006. Efficient search for inver-
sion transduction grammar. In EMNLP.
419
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 227?235,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Efficient Parsing for Transducer Grammars
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, mbansal, adpauls, klein}@cs.berkeley.edu
Abstract
The tree-transducer grammars that arise in
current syntactic machine translation systems
are large, flat, and highly lexicalized. We ad-
dress the problem of parsing efficiently with
such grammars in three ways. First, we
present a pair of grammar transformations
that admit an efficient cubic-time CKY-style
parsing algorithm despite leaving most of the
grammar in n-ary form. Second, we show
how the number of intermediate symbols gen-
erated by this transformation can be substan-
tially reduced through binarization choices.
Finally, we describe a two-pass coarse-to-fine
parsing approach that prunes the search space
using predictions from a subset of the origi-
nal grammar. In all, parsing time reduces by
81%. We also describe a coarse-to-fine prun-
ing scheme for forest-based language model
reranking that allows a 100-fold increase in
beam size while reducing decoding time. The
resulting translations improve by 1.3 BLEU.
1 Introduction
Current approaches to syntactic machine translation
typically include two statistical models: a syntac-
tic transfer model and an n-gram language model.
Recent innovations have greatly improved the effi-
ciency of language model integration through multi-
pass techniques, such as forest reranking (Huang
and Chiang, 2007), local search (Venugopal et al,
2007), and coarse-to-fine pruning (Petrov et al,
2008; Zhang and Gildea, 2008). Meanwhile, trans-
lation grammars have grown in complexity from
simple inversion transduction grammars (Wu, 1997)
to general tree-to-string transducers (Galley et al,
2004) and have increased in size by including more
synchronous tree fragments (Galley et al, 2006;
Marcu et al, 2006; DeNeefe et al, 2007). As a result
of these trends, the syntactic component of machine
translation decoding can now account for a substan-
tial portion of total decoding time. In this paper,
we focus on efficient methods for parsing with very
large tree-to-string grammars, which have flat n-ary
rules with many adjacent non-terminals, as in Fig-
ure 1. These grammars are sufficiently complex that
the purely syntactic pass of our multi-pass decoder is
the compute-time bottleneck under some conditions.
Given that parsing is well-studied in the mono-
lingual case, it is worth asking why MT grammars
are not simply like those used for syntactic analy-
sis. There are several good reasons. The most im-
portant is that MT grammars must do both analysis
and generation. To generate, it is natural to mem-
orize larger lexical chunks, and so rules are highly
lexicalized. Second, syntax diverges between lan-
guages, and each divergence expands the minimal
domain of translation rules, so rules are large and
flat. Finally, we see most rules very few times, so
it is challenging to subcategorize non-terminals to
the degree done in analytic parsing. This paper de-
velops encodings, algorithms, and pruning strategies
for such grammars.
We first investigate the qualitative properties of
MT grammars, then present a sequence of parsing
methods adapted to their broad characteristics. We
give normal forms which are more appropriate than
Chomsky normal form, leaving the rules mostly flat.
We then describe a CKY-like algorithm which ap-
plies such rules efficiently, working directly over the
n-ary forms in cubic time. We show how thoughtful
227
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical normal form (LNF) transformation
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF transformation
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
NP ? DT+NN NNS NP ? DT NN+NNSor
Type-minimizing binarization
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT,NN
DT,NN,NNS 
Minimal binary rules for LNF
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
Figure 1: (a) A synchronous transducer rule has co-
indexed non-terminals on the source and target side. In-
ternal grammatical structure of the target side has been
omitted. (b) The source-side projection of the rule is a
monolingual source-language rule with target-side gram-
mar symbols. (c) A training sentence pair is annotated
with a target-side parse tree and a word alignment, which
license this rule to be extracted.
binarization can further increase parsing speed, and
we present a new coarse-to-fine scheme that uses
rule subsets rather than symbol clustering to build
a coarse grammar projection. These techniques re-
duce parsing time by 81% in aggregate. Finally,
we demonstrate that we can accelerate forest-based
reranking with a language model by pruning with
information from the parsing pass. This approach
enables a 100-fold increase in maximum beam size,
improving translation quality by 1.3 BLEU while
decreasing total decoding time.
2 Tree Transducer Grammars
Tree-to-string transducer grammars consist of
weighted rules like the one depicted in Figure 1.
Each n-ary rule consists of a root symbol, a se-
quence of lexical items and non-terminals on the
source-side, and a fragment of a syntax tree on
the target side. Each non-terminal on the source
side corresponds to a unique one on the target side.
Aligned non-terminals share a grammar symbol de-
rived from a target-side monolingual grammar.
These grammars are learned from word-aligned
sentence pairs annotated with target-side phrase
structure trees. Extraction proceeds by using word
alignments to find correspondences between target-
side constituents and source-side word spans, then
discovering transducer rules that match these con-
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
Figure 2: Transducer grammars are composed of very flat
rules. Above, the histogram shows rule counts for each
rule size among the 332,000 rules that apply to an indi-
vidual 30-word sentence. The size of a rule is the total
number of non-terminals and lexical items in its source-
side yield.
stituent alignments (Galley et al, 2004). Given this
correspondence, an array of extraction procedures
yields rules that are well-suited to machine trans-
lation (Galley et al, 2006; DeNeefe et al, 2007;
Marcu et al, 2006). Rule weights are estimated
by discriminatively combining relative frequency
counts and other rule features.
A transducer grammarG can be projected onto its
source language, inducing a monolingual grammar.
If we weight each rule by the maximumweight of its
projecting synchronous rules, then parsing with this
projected grammar maximizes the translation model
score for a source sentence. We need not even con-
sider the target side of transducer rules until integrat-
ing an n-gram language model or other non-local
features of the target language.
We conduct experiments with a grammar ex-
tracted from 220 million words of Arabic-English
bitext, extracting rules with up to 6 non-terminals. A
histogram of the size of rules applicable to a typical
30-word sentence appears in Figure 2. The grammar
includes 149 grammatical symbols, an augmentation
of the Penn Treebank symbol set. To evaluate, we
decoded 300 sentences of up to 40 words in length
from the NIST05 Arabic-English test set.
3 Efficient Grammar Encodings
Monolingual parsing with a source-projected trans-
ducer grammar is a natural first pass in multi-pass
decoding. These grammars are qualitatively dif-
ferent from syntactic analysis grammars, such as
the lexicalized grammars of Charniak (1997) or the
heavily state-split grammars of Petrov et al (2006).
228
In this section, we develop an appropriate grammar
encoding that enables efficient parsing.
It is problematic to convert these grammars into
Chomsky normal form, which CKY requires. Be-
cause transducer rules are very flat and contain spe-
cific lexical items, binarization introduces a large
number of intermediate grammar symbols. Rule size
and lexicalization affect parsing complexity whether
the grammar is binarized explicitly (Zhang et al,
2006) or implicitly binarized using Early-style inter-
mediate symbols (Zollmann et al, 2006). Moreover,
the resulting binary rules cannot be Markovized to
merge symbols, as in Klein andManning (2003), be-
cause each rule is associated with a target-side tree
that cannot be abstracted.
We also do not restrict the form of rules in the
grammar, a common technique in syntactic machine
translation. For instance, Zollmann et al (2006)
follow Chiang (2005) in disallowing adjacent non-
terminals. Watanabe et al (2006) limit grammars
to Griebach-Normal form. However, general tree
transducer grammars provide excellent translation
performance (Galley et al, 2006), and so we focus
on parsing with all available rules.
3.1 Lexical Normal Form
Sequences of consecutive non-terminals complicate
parsing because they require a search over non-
terminal boundaries when applied to a sentence
span. We transform the grammar to ensure that all
rules containing lexical items (lexical rules) do not
contain sequences of non-terminals. We allow both
unary and binary non-lexical rules.
Let L be the set of lexical items and V the set
of non-terminal symbols in the original grammar.
Then, lexical normal form (LNF) limits productions
to two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? (X1)?(X2)
? = w+(Xiw+)?
Above, all Xi ? V and w+ ? L+. Symbols in
parentheses are optional. The nucleus ? of lexical
rules is a mixed sequence that has lexical items on
each end and no adjacent non-terminals.
Converting a grammar into LNF requires two
steps. In the sequence elimination step, for every
NNP
1
 no d ba una bofetada  DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verd
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
LNF replaces non-terminal sequences in lexical rules
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Non-lexical rules before binarization:
Equivalent binary rules, minimizing symbol count:
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+
DT+NN ? DT NN
NP ? DT NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
NP ? DT+NN NNS DT+NN ? DT NN
Figure 3: We transform the original grammar by first
eliminating non-terminal sequences in lexical rules.
Next, we binarize, adding a minimal number of inter-
mediate grammar symbols and binary non-lexical rules.
Finally, anchored LNF further transforms lexical rules
to begin and end with lexical items by introducing ad-
ditional symbols.
lexical rule we replace each sequence of consecutive
non-terminalsX1 . . . Xn with the intermediate sym-
bol X1+. . .+Xn (abbreviated X1:n) and introduce a
non-lexical rule X1+. . .+Xn ? X1 . . . Xn. In the
binarization step, we introduce further intermediate
symbols and rules to binarize all non-lexical rules
in the grammar, including those added by sequence
elimination.
3.2 Non-terminal Binarization
Exactly howwe binarize non-lexical rules affects the
total number of intermediate symbols introduced by
the LNF transformation.
Binarization involves selecting a set of symbols
that will allow us to assemble the right-hand side
X1 . . . Xn of every non-lexical rule using binary
productions. This symbol set must at least include
the left-hand side of every rule in the grammar
(lexical and non-lexical), including the intermediate
229
NNP
1
 no daba una bofetada a DT
2
 NN
3
 verde
S ? NNP no daba una bofetada a DT NN verde
DT+NN ? DT NN
Maria no daba una bofetada a la bruja verde
Mary did not slap the green witch
S
NNP NNDT
NNP
1
 did not slap DT
2
 green NN
3
S ?
Lexical rules cannot contain adjacent non-terminals
S ? NNP no daba una bofetada a DT NN verde
S\NNP ? no daba una bofetada a DT+NN verde
S ? NNP S\NNP
Anchored LNF rules are bounded by lexical items
DT+NN ? DT NN
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
0
17,500
35,000
52,500
70,000
1 2 3 4 5 6+
Original grammar rules are flat and lexical
NP ? DT NN NNS
S ? NNP no daba una bofetada a DT+NN verde
Non-lexical rules are binarized using few symbols
Required symbols Sequences to build
DT+NN DT NN
NNS NNP NP S
DT  NN
DT  NN  NNS 
Binary rules for LNF that minimize symbol count
NP ? DT+NN NNS
NP ? DT+NN NNS
X
no
la
X
daba
bruja
daba
VP ? no daba
NP ? la bruja
... Maria daba ...
S ? NP   daba
S ? NP daba
(a)
(b)
(c)
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7+
0
22,500
45,000
67,500
90,000
1 2 3 4 5 6 7 8 9 10+
Right-branching
Left-branching
Greedy
Optimal (ILP)
0 3,000 6,000 9,000
443
1,101
5,871
8,095
0
30,000
60,000
90,000
1 2 3 4 5 6 7+ Figure 4: The number of non-terminal symbols intro-
duced to the grammar through LNF binarization depends
upon the policy for binarizing type sequences. This ex-
periment shows results from transforming a grammar that
has already been filtered for a particular short sentence.
Both the greedy and optimal binarizations use far fewer
symbols than naive binarizations.
symbols X1:n introduced by sequence elimination.
To ensure that a symbol sequence X1 . . . Xn can
be constructed, we select a split point k and add in-
termediate types X1:k and Xk+1:n to the grammar.
We must also ensure that the sequences X1 . . . Xk
and Xk+1 . . . Xn can be constructed. As baselines,
we used left-branching (where k = 1 always) and
right-branching (where k = n? 1) binarizations.
We also tested a greedy binarization approach,
choosing k to minimize the number of grammar
symbols introduced. We first try to select k such that
both X1:k and Xk+1:n are already in the grammar.
If no such k exists, we select k such that one of the
intermediate types generated is already used. If no
such k exists again, we choose k = ?12n
?. This pol-
icy only creates new intermediate types when nec-
essary. Song et al (2008) propose a similar greedy
approach to binarization that uses corpus statistics to
select common types rather than explicitly reusing
types that have already been introduced.
Finally, we computed an optimal binarization that
explicitly minimizes the number of symbols in the
resulting grammar. We cast the minimization as an
integer linear program (ILP). Let V be the set of
all base non-terminal symbols in the grammar. We
introduce an indicator variable TY for each symbol
Y ? V + to indicate that Y is used in the grammar.
Y can be either a base non-terminal symbol Xi or
an intermediate symbol X1:n. We also introduce in-
dicators AY,Z for each pairs of symbols, indicating
that both Y and Z are used in the grammar. Let
L ? V + be the set of left-hand side symbols for
all lexical and non-lexical rules already in the gram-
mar. Let R be the set of symbol sequences on the
right-hand side of all non-lexical rules. Then, the
ILP takes the form:
min ?
Y ?V +
TY (1)
s.t. TY = 1 ? Y ? L (2)
1 ??
k
AX1:k,Xk+1:n ? X1 . . . Xn ? R (3)
TX1:n ?
?
k
AX1:k,Xk+1:n ? X1:n (4)
AY,Z ? TY , AY,Z ? TZ ? Y, Z (5)
The solution to this ILP indicates which symbols
appear in a minimal binarization. Equation 1 explic-
itly minimizes the number of symbols. Equation 2
ensures that all symbols already in the grammar re-
main in the grammar.
Equation 3 does not require that a symbol repre-
sent the entire right-hand side of each non-lexical
rule, but does ensure that each right-hand side se-
quence can be built from two subsequence symbols.
Equation 4 ensures that any included intermediate
type can also be built from two subsequence types.
Finally, Equation 5 ensures that if a pair is used, each
member of the pair is included. This program can be
optimized with an off-the-shelf ILP solver.1
Figure 4 shows the number of intermediate gram-
mar symbols needed for the four binarization poli-
cies described above for a short sentence. Our ILP
solver could only find optimal solutions for very
short sentences (which have small grammars after
relativization). Because greedy requires very little
time to compute and generates symbol counts that
are close to optimal when both can be computed, we
use it for our remaining experiments.
3.3 Anchored Lexical Normal Form
We also consider a further grammar transformation,
anchored lexical normal form (ALNF), in which the
yield of lexical rules must begin and end with a lex-
ical item. As shown in the following section, ALNF
improves parsing performance over LNF by shifting
work from lexical rule applications to non-lexical
1We used lp solve: http://sourceforge.net/projects/lpsolve.
230
rule applications. ALNF consists of rules with the
following two forms:
Non-lexical: X ? X1(X2)
Lexical: X ? w+(Xiw+)?
To convert a grammar into ALNF, we first transform
it into LNF, then introduce additional binary rules
that split off non-terminal symbols from the ends of
lexical rules, as shown in Figure 3.
4 Efficient CKY Parsing
We now describe a CKY-style parsing algorithm for
grammars in LNF. The dynamic program is orga-
nized into spans Sij and computes the Viterbi score
w(i, j,X) for each edge Sij [X], the weight of the
maximum parse over words i+1 to j, rooted at sym-
bol X . For each Sij , computation proceeds in three
phases: binary, lexical, and unary.
4.1 Applying Non-lexical Binary Rules
For a span Sij , we first apply the binary non-lexical
rules just as in standard CKY, computing an interme-
diate Viterbi score wb(i, j,X). Let ?r be the weight
of rule r. Then, wb(i, j,X) =
max
r=X?X1X2
?r
j?1max
k=i+1
w(i, k,X1) ? w(k, j,X2).
The quantitiesw(i, k,X1) andw(k, j,X2) will have
already been computed by the dynamic program.
The work in this phase is cubic in sentence length.
4.2 Applying Lexical Rules
On the other hand, lexical rules in LNF can be ap-
plied without binarization, because they only apply
to particular spans that contain the appropriate lexi-
cal items. For a given Sij , we first compute all the le-
gal mappings of each rule onto the span. A mapping
consists of a correspondence between non-terminals
in the rule and subspans of Sij . In practice, there
is typically only one way that a lexical rule in LNF
can map onto a span, because most lexical items will
appear only once in the span.
Let m be a legal mapping and r its corresponding
rule. Let S(i)k` [X] be the edge mapped to the ith non-terminal of r underm, and ?r the weight of r. Then,
wl(i, j,X) = maxm ?r
?
S(i)k` [X]
w(k, `,X).
Again, w(k, `,X) will have been computed by the
dynamic program. Assuming only a constant num-
ber of mappings per rule per span, the work in this
phase is quadratic. We can then merge wl and wb:
w(i, j,X) = max(wl(i, j,X), wb(i, j,X)).
To efficiently compute mappings, we store lexi-
cal rules in a trie (or suffix array) ? a searchable
graph that indexes rules according to their sequence
of lexical items and non-terminals. This data struc-
ture has been used similarly to index whole training
sentences for efficient retrieval (Lopez, 2007). To
find all rules that map onto a span, we traverse the
trie using depth-first search.
4.3 Applying Unary Rules
Unary non-lexical rules are applied after lexical
rules and non-lexical binary rules.
w(i, j,X) = max
r:r=X?X1
?rw(i, j,X1).
While this definition is recursive, we allow only one
unary rule application per symbol X at each span
to prevent infinite derivations. This choice does not
limit the generality of our algorithm: chains of unar-
ies can always be collapsed via a unary closure.
4.4 Bounding Split Points for Binary Rules
Non-lexical binary rules can in principle apply to
any span Sij where j ? i ? 2, using any split point
k such that i < k < j. In practice, however, many
rules cannot apply to many (i, k, j) triples because
the symbols for their children have not been con-
structed successfully over the subspans Sik and Skj .
Therefore, the precise looping order over rules and
split points can influence computation time.
We found the following nested looping order for
the binary phase of processing an edge Sij [X] gave
the fastest parsing times for these grammars:
1. Loop over symbols X1 for the left child
2. Loop over all rules X ? X1X2 containing X1
3. Loop over split points k : i < k < j
4. Update wb(i, j,X) as necessary
This looping order allows for early stopping via
additional bookkeeping in the algorithm. We track
the following statistics as we parse:
231
Grammar Bound checks Parsing time
LNF no 264
LNF yes 181
ALNF yes 104
Table 1: Adding bound checks to CKY and transforming
the grammar from LNF to anchored LNF reduce parsing
time by 61% for 300 sentences of length 40 or less. No
approximations have been applied, so all three scenarios
produce no search errors. Parsing time is in minutes.
minEND(i,X), maxEND(i,X): The minimum and
maximum position k for which symbol X was
successfully built over Sik.
minSTART(j,X), maxSTART(j,X): The minimum
and maximum position k for which symbol X
was successfully built over Skj .
We then bound k by mink and maxk in the inner
loop using these statistics. If ever mink > maxk,
then the loop is terminated early.
1. set mink = i+ 1,maxk = j ? 1
2. loop over symbols X1 for the left child
mink = max(mink,minEND(i,X1))
maxk = min(maxk,maxEND(i,X1))
3. loop over rules X ? X1X2
mink = max(mink,minSTART(j,X2))
maxk = min(maxk,maxSTART(j,X2))
4. loop over split points k : mink ? k ? maxk
5. update wb(i, j,X) as necessary
In this way, we eliminate unnecessary work by
avoiding split points that we know beforehand can-
not contribute to wb(i, j,X).
4.5 Parsing Time Results
Table 1 shows the decrease in parsing time from in-
cluding these bound checks, as well as switching
from lexical normal form to anchored LNF.
Using ALNF rather than LNF increases the num-
ber of grammar symbols and non-lexical binary
rules, but makes parsing more efficient in three
ways. First, it decreases the number of spans for
which a lexical rule has a legal mapping. In this way,
ALNF effectively shifts work from the lexical phase
to the binary phase. Second, ALNF reduces the time
spent searching the trie for mappings, because the
first transition into the trie must use an edge with a
lexical item. Finally, ALNF improves the frequency
that, when a lexical rule matches a span, we have
successfully built every edge Sk`[X] in the mapping
for that rule. This frequency increases from 45% to
96% with ALNF.
5 Coarse-to-Fine Search
We now consider two coarse-to-fine approximate
search procedures for parsing with these grammars.
Our first approach clusters grammar symbols to-
gether during the coarse parsing pass, following
work in analytic parsing (Charniak and Caraballo,
1998; Petrov and Klein, 2007). We collapse all
intermediate non-terminal grammar symbols (e.g.,
NP) to a single coarse symbol X, while pre-terminal
symbols (e.g., NN) are hand-clustered into 7 classes
(nouns, verbals, adjectives, punctuation, etc.). We
then project the rules of the original grammar into
this simplified symbol set, weighting each rule of
the coarse grammar by the maximum weight of any
rule that mapped onto it.
In our second and more successful approach, we
select a subset of grammar symbols. We then in-
clude only and all rules that can be built using those
symbols. Because the grammar includes many rules
that are compositions of smaller rules, parsing with
a subset of the grammar still provides meaningful
scores that can be used to prune base grammar sym-
bols while parsing under the full grammar.
5.1 Symbol Selection
To compress the grammar, we select a small sub-
set of symbols that allow us to retain as much of
the original grammar as possible. We use a voting
scheme to select the symbol subset. After conver-
sion to LNF (or ALNF), each lexical rule in the orig-
inal grammar votes for the symbols that are required
to build it. A rule votes as many times as it was ob-
served in the training data to promote frequent rules.
We then select the top nl symbols by vote count and
include them in the coarse grammar C.
We would also like to retain as many non-lexical
rules from the original grammar as possible, but the
right-hand side of each rule can be binarized in many
ways. We again use voting, but this time each non-
232
Pruning Minutes Model score BLEU
No pruning 104 60,179 44.84
Clustering 79 60,179 44.84
Subsets 50 60,163 44.82
Table 2: Coarse-to-fine pruning speeds up parsing time
with minimal effect on either model score or translation
quality. The coarse grammar built using symbol subsets
outperforms clustering grammar symbols, reducing pars-
ing time by 52%. These experiments do not include a
language model.
lexical rule votes for its yield, a sequence of sym-
bols. We select the top nu symbol sequences as the
set R of right-hand sides.
Finally, we augment the symbol set of C with in-
termediate symbols that can construct all sequences
in R, using only binary rules. This step again re-
quires choosing a binarization for each sequence,
such that a minimal number of additional symbols is
introduced. We use the greedy approach from Sec-
tion 3.2. We then include in C all rules from the
original grammar that can be built from the symbols
we have chosen. Surprisingly, we are able to re-
tain 76% of the grammar rules while excluding 92%
of the grammar symbols2, which speeds up parsing
substantially.
5.2 Max Marginal Thresholding
We parse first with the coarse grammar to find the
Viterbi derivation score for each edge Sij [X]. We
then perform a Viterbi outside pass over the chart,
like a standard outside pass but replacing ? with
max (Goodman, 1999). The product of an edge?s
Viterbi score and its Viterbi outside score gives a
max marginal, the score of the maximal parse that
uses the edge.
We then prune away regions of the chart that de-
viate in their coarse max marginal from the global
Viterbi score by a fixed margin tuned on a develop-
ment set. Table 2 shows that both methods of con-
structing a coarse grammar are effective in pruning,
but selecting symbol subsets outperformed the more
typical clustering approach, reducing parsing time
by an additional factor of 2.
2We used nl of 500 and nu of 4000 for experiments. These
parameters were tuned on a development set.
6 Language Model Integration
Large n-gram language models (LMs) are critical
to the performance of machine translation systems.
Recent innovations have managed the complexity
of LM integration using multi-pass architectures.
Zhang and Gildea (2008) describes a coarse-to-fine
approach that iteratively increases the order of the
LM. Petrov et al (2008) describes an additional
coarse-to-fine hierarchy over language projections.
Both of these approaches integrate LMs via bottom-
up dynamic programs that employ beam search. As
an alternative, Huang and Chiang (2007) describes a
forest-based reranking algorithm called cube grow-
ing, which also employs beam search, but focuses
computation only where necessary in a top-down
pass through a parse forest.
In this section, we show that the coarse-to-fine
idea of constraining each pass using marginal pre-
dictions of the previous pass also applies effectively
to cube growing. Max marginal predictions from the
parse can substantially reduce LM integration time.
6.1 Language Model Forest Reranking
Parsing produces a forest of derivations, where each
edge in the forest holds its Viterbi (or one-best)
derivation under the transducer grammar. In forest
reranking via cube growing, edges in the forest pro-
duce k-best lists of derivations that are scored by
both the grammar and an n-gram language model.
Using ALNF, each edge must first generate a k-best
list of derivations that are not scored by the language
model. These derivations are then flattened to re-
move the binarization introduced by ALNF, so that
the resulting derivations are each rooted by an n-
ary rule r from the original grammar. The leaves of
r correspond to sub-edges in the chart, which are
recursively queried for their best language-model-
scored derivations. These sub-derivations are com-
bined by r, and new n-grams at the edges of these
derivations are scored by the language model.
The language-model-scored derivations for the
edge are placed on a priority queue. The top of
the priority queue is repeatedly removed, and its
successors added back on to the queue, until k
language-model-scored derivations have been dis-
covered. These k derivations are then sorted and
233
Pruning Max TM LM Total Inside Outside LM Total
strategy beam BLEU score score score time time time time
No pruning 20 57.67 58,570 -17,202 41,368 99 0 247 346
CTF parsing 200 58.43 58,495 -16,929 41,556 53 0 186 239
CTF reranking 200 58.63 58,582 -16,998 41,584 98 64 79 241
CTF parse + rerank 2000 58.90 58,602 -16,980 41,622 53 52 148 253
Table 3: Time in minutes and performance for 300 sentences. We used a trigram language model trained on 220
million words of English text. The no pruning baseline used a fix beam size for forest-based language model reranking.
Coarse-to-fine parsing included a coarse pruning pass using a symbol subset grammar. Coarse-to-fine reranking used
max marginals to constrain the reranking pass. Coarse-to-fine parse + rerank employed both of these approximations.
supplied to parent edges upon request.3
6.2 Coarse-to-Fine Parsing
Even with this efficient reranking algorithm, inte-
grating a language model substantially increased de-
coding time and memory use. As a baseline, we
reranked using a small fixed-size beam of 20 deriva-
tions at each edge. Larger beams exceeded the mem-
ory of our hardware. Results appear in Table 3.
Coarse-to-fine parsing before LM integration sub-
stantially improved language model reranking time.
By pruning the chart with max marginals from the
coarse symbol subset grammar from Section 5, we
were able to rerank with beams of length 200, lead-
ing to a 0.8 BLEU increase and a 31% reduction in
total decoding time.
6.3 Coarse-to-Fine Forest Reranking
We realized similar performance and speed bene-
fits by instead pruning with max marginals from the
full grammar. We found that LM reranking explored
many edges with low max marginals, but used few
of them in the final decoder output. Following the
coarse-to-fine paradigm, we restricted the reranker
to edges with a max marginal above a fixed thresh-
old. Furthermore, we varied the beam size of each
edge based on the parse. Let ?m be the ratio of
the max marginal for edge m to the global Viterbi
derivation for the sentence. We used a beam of size?
k ? 2ln?m? for each edge.
Computing max marginals under the full gram-
mar required an additional outside pass over the full
parse forest, adding substantially to parsing time.
3Huang and Chiang (2007) describes the cube growing al-
gorithm in further detail, including the precise form of the suc-
cessor function for derivations.
However, soft coarse-to-fine pruning based on these
max marginals also allowed for beams up to length
200, yielding a 1.0 BLEU increase over the baseline
and a 30% reduction in total decoding time.
We also combined the coarse-to-fine parsing ap-
proach with this soft coarse-to-fine reranker. Tiling
these approximate search methods allowed another
10-fold increase in beam size, further improving
BLEU while only slightly increasing decoding time.
7 Conclusion
As translation grammars increase in complexity
while innovations drive down the computational cost
of language model integration, the efficiency of the
parsing phase of machine translation decoding is be-
coming increasingly important. Our grammar nor-
mal form, CKY improvements, and symbol subset
coarse-to-fine procedure reduced parsing time for
large transducer grammars by 81%.
These techniques also improved forest-based lan-
guage model reranking. A full decoding pass with-
out any of our innovations required 511 minutes us-
ing only small beams. Coarse-to-fine pruning in
both the parsing and language model passes allowed
a 100-fold increase in beam size, giving a perfor-
mance improvement of 1.3 BLEU while decreasing
total decoding time by 50%.
Acknowledgements
This work was enabled by the Information Sci-
ences Institute Natural Language Group, primarily
through the invaluable assistance of Jens Voeckler,
and was supported by the National Science Founda-
tion (NSF) under grant IIS-0643742.
234
References
Eugene Charniak and Sharon Caraballo. 1998. New fig-
ures of merit for best-first probabilistic chart parsing.
In Computational Linguistics.
Eugene Charniak. 1997. Statistical techniques for natu-
ral language parsing. In National Conference on Arti-
ficial Intelligence.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In The Annual Con-
ference of the Association for Computational Linguis-
tics.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn from
phrase-based MT? In Proceedings of the Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Hu-
man Language Technologies: The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In The An-
nual Conference of the Association for Computational
Linguistics.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
The Annual Conference of the Association for Compu-
tational Linguistics.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proceedings of the Association for
Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based transla-
tion with suffix arrays. In The Conference on Empiri-
cal Methods in Natural Language Processing.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In The Annual Conference
of the North American Chapter of the Association for
Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In The Annual Conference of
the Association for Computational Linguistics.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using lan-
guage projections. In The Conference on Empirical
Methods in Natural Language Processing.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In The Con-
ference on Empirical Methods in Natural Language
Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan Vo-
gel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In In Pro-
ceedings of the Human Language Technology and
North American Association for Computational Lin-
guistics Conference.
Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.
2006. Left-to-right target generation for hierarchical
phrase-based translation. In The Annual Conference
of the Association for Computational Linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free grammars.
In The Annual Conference of the Association for Com-
putational Linguistics.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for machine
translation. In North American Chapter of the Associ-
ation for Computational Linguistics.
Andreas Zollmann, Ashish Venugopal, and Stephan Vo-
gel. 2006. Syntax augmented machine translation via
chart parsing. In The Statistical Machine Translation
Workshop at the North American Association for Com-
putational Linguistics Conference.
235
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 17?24,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Tailoring Word Alignments to Syntactic Machine Translation
John DeNero
Computer Science Division
University of California, Berkeley
denero@berkeley.edu
Dan Klein
Computer Science Division
University of California, Berkeley
klein@cs.berkeley.edu
Abstract
Extracting tree transducer rules for syntac-
tic MT systems can be hindered by word
alignment errors that violate syntactic corre-
spondences. We propose a novel model for
unsupervised word alignment which explic-
itly takes into account target language con-
stituent structure, while retaining the robust-
ness and efficiency of the HMM alignment
model. Our model?s predictions improve the
yield of a tree transducer extraction system,
without sacrificing alignment quality. We
also discuss the impact of various posterior-
based methods of reconciling bidirectional
alignments.
1 Introduction
Syntactic methods are an increasingly promising ap-
proach to statistical machine translation, being both
algorithmically appealing (Melamed, 2004; Wu,
1997) and empirically successful (Chiang, 2005;
Galley et al, 2006). However, despite recent
progress, almost all syntactic MT systems, indeed
statistical MT systems in general, build upon crude
legacy models of word alignment. This dependence
runs deep; for example, Galley et al (2006) requires
word alignments to project trees from the target lan-
guage to the source, while Chiang (2005) requires
alignments to induce grammar rules.
Word alignment models have not stood still in re-
cent years. Unsupervised methods have seen sub-
stantial reductions in alignment error (Liang et al,
2006) as measured by the now much-maligned AER
metric. A host of discriminative methods have been
introduced (Taskar et al, 2005; Moore, 2005; Ayan
and Dorr, 2006). However, few of these methods
have explicitly addressed the tension between word
alignments and the syntactic processes that employ
them (Cherry and Lin, 2006; Daume? III and Marcu,
2005; Lopez and Resnik, 2005).
We are particularly motivated by systems like the
one described in Galley et al (2006), which con-
structs translations using tree-to-string transducer
rules. These rules are extracted from a bitext anno-
tated with both English (target side) parses and word
alignments. Rules are extracted from target side
constituents that can be projected onto contiguous
spans of the source sentence via the word alignment.
Constituents that project onto non-contiguous spans
of the source sentence do not yield transducer rules
themselves, and can only be incorporated into larger
transducer rules. Thus, if the word alignment of a
sentence pair does not respect the constituent struc-
ture of the target sentence, then the minimal transla-
tion units must span large tree fragments, which do
not generalize well.
We present and evaluate an unsupervised word
alignment model similar in character and compu-
tation to the HMM model (Ney and Vogel, 1996),
but which incorporates a novel, syntax-aware distor-
tion component which conditions on target language
parse trees. These trees, while automatically gener-
ated and therefore imperfect, are nonetheless (1) a
useful source of structural bias and (2) the same trees
which constrain future stages of processing anyway.
In our model, the trees do not rule out any align-
ments, but rather softly influence the probability of
transitioning between alignment positions. In par-
ticular, transition probabilities condition upon paths
through the target parse tree, allowing the model to
prefer distortions which respect the tree structure.
17
Our model generates word alignments that better
respect the parse trees upon which they are condi-
tioned, without sacrificing alignment quality. Using
the joint training technique of Liang et al (2006)
to initialize the model parameters, we achieve an
AER superior to the GIZA++ implementation of
IBM model 4 (Och and Ney, 2003) and a reduc-
tion of 56.3% in aligned interior nodes, a measure
of agreement between alignments and parses. As a
result, our alignments yield more rules, which better
match those we would extract had we used manual
alignments.
2 Translation with Tree Transducers
In a tree transducer system, as in phrase-based sys-
tems, the coverage and generality of the transducer
inventory is strongly related to the effectiveness of
the translation model (Galley et al, 2006). We will
demonstrate that this coverage, in turn, is related to
the degree to which initial word alignments respect
syntactic correspondences.
2.1 Rule Extraction
Galley et al (2004) proposes a method for extracting
tree transducer rules from a parallel corpus. Given a
source language sentence s, a target language parse
tree t of its translation, and a word-level alignment,
their algorithm identifies the constituents in t which
map onto contiguous substrings of s via the align-
ment. The root nodes of such constituents ? denoted
frontier nodes ? serve as the roots and leaves of tree
fragments that form minimal transducer rules.
Frontier nodes are distinguished by their compat-
ibility with the word alignment. For a constituent c
of t, we consider the set of source words sc that are
aligned to c. If none of the source words in the lin-
ear closure s?c (the words between the leftmost and
rightmost members of sc) aligns to a target word out-
side of c, then the root of c is a frontier node. The
remaining interior nodes do not generate rules, but
can play a secondary role in a translation system.1
The roots of null-aligned constituents are not fron-
tier nodes, but can attach productively to multiple
minimal rules.
1Interior nodes can be used, for instance, in evaluating
syntax-based language models. They also serve to differentiate
transducer rules that have the same frontier nodes but different
internal structure.
Two transducer rules, t1 ? s1 and t2 ? s2,
can be combined to form larger translation units
by composing t1 and t2 at a shared frontier node
and appropriately concatenating s1 and s2. How-
ever, no technique has yet been shown to robustly
extract smaller component rules from a large trans-
ducer rule. Thus, for the purpose of maximizing the
coverage of the extracted translation model, we pre-
fer to extract many small, minimal rules and gen-
erate larger rules via composition. Maximizing the
number of frontier nodes supports this goal, while
inducing many aligned interior nodes hinders it.
2.2 Word Alignment Interactions
We now turn to the interaction between word align-
ments and the transducer extraction algorithm. Con-
sider the example sentence in figure 1A, which
demonstrates how a particular type of alignment er-
ror prevents the extraction of many useful transducer
rules. The mistaken link [la ? the] intervenes be-
tween axe?s and carrie`r, which both align within an
English adjective phrase, while la aligns to a distant
subspan of the English parse tree. In this way, the
alignment violates the constituent structure of the
English parse.
While alignment errors are undesirable in gen-
eral, this error is particularly problematic for a
syntax-based translation system. In a phrase-based
system, this link would block extraction of the
phrases [axe?s sur la carrie`r ? career oriented] and
[les emplois ? the jobs] because the error overlaps
with both. However, the intervening phrase [em-
plois sont ? jobs are] would still be extracted, at
least capturing the transfer of subject-verb agree-
ment. By contrast, the tree transducer extraction
method fails to extract any of these fragments: the
alignment error causes all non-terminal nodes in
the parse tree to be interior nodes, excluding pre-
terminals and the root. Figure 1B exposes the conse-
quences: a wide array of desired rules are lost during
extraction.
The degree to which a word alignment respects
the constituent structure of a parse tree can be quan-
tified by the frequency of interior nodes, which indi-
cate alignment patterns that cross constituent bound-
aries. To achieve maximum coverage of the trans-
lation model, we hope to infer tree-violating align-
ments only when syntactic structures truly diverge.
18
.(A)
(B) (i)
(ii)
S
NP
VP
ADJP
NN VBN
NNS
DT
AUX
T
h
e
j
o
b
s
a
r
e
c
a
r
e
e
r
o
r
i
e
n
t
e
d
.
les
emplois
sont
ax?s
sur
la
carri?re
.
.
Legend
Correct proposed word alignment consistent with 
human annotation.
Proposed word alignment error inconsistent with 
human annotation.
Word alignment constellation that renders the 
root of the relevant constituent to be an interior 
node.
Word alignment constellation that would allow a 
phrase extraction in a phrase-based translation 
system, but which does not correspond to an 
English constituent.
Bold
Italic
Frontier node (agrees with alignment)
Interior node (inconsistent with alignment)
(S (NP (DT[0] NNS[1]) (VP AUX[2] (ADJV NN[3] VBN[4]) .[5]) ? [0] [1] [2] [3] [4] [5]
(S (NP (DT[0] (NNS jobs)) (VP AUX[1] (ADJV NN[2] VBN[3]) .[4]) ? [0] sont [1] [2] [3] [4]
(S (NP (DT[0] (NNS jobs)) (VP (AUX are) (ADJV NN[1] VBN[2]) .[3]) ? [0] emplois sont [1] [2] [3]
(S NP[0] VP[1] .[2]) ? [0] [1] [2]
(S (NP (DT[0] NNS[1]) VP[2] .[3]) ? [0] [1] [2] [3]
(S (NP (DT[0] (NNS jobs)) VP[2] .[3]) ? [0] emplois [2] [3]
(S (NP (DT[0] (NNS jobs)) (VP AUX[1] ADJV[2]) .[3]) ? [0] emplois [1] [2] [3]
(S (NP (DT[0] (NNS jobs)) (VP (AUX are) ADJV[1]) .[2]) ? [0] emplois sont [1] [2]
Figure 1: In this transducer extraction example, (A) shows a proposed alignment from our test set with
an alignment error that violates the constituent structure of the English sentence. The resulting frontier
nodes are printed in bold; all nodes would be frontier nodes under a correct alignment. (B) shows a small
sample of the rules extracted under the proposed alignment, (ii), and the correct alignment, (i) and (ii). The
single alignment error prevents the extraction of all rules in (i) and many more. This alignment pattern was
observed in our test set and corrected by our model.
3 Unsupervised Word Alignment
To allow for this preference, we present a novel con-
ditional alignment model of a foreign (source) sen-
tence f = {f1, ..., fJ} given an English (target) sen-
tence e = {e1, ..., eI} and a target tree structure t.
Like the classic IBM models (Brown et al, 1994),
our model will introduce a latent alignment vector
a = {a1, ..., aJ} that specifies the position of an
aligned target word for each source word. Formally,
our model describes p(a, f|e, t), but otherwise bor-
rows heavily from the HMM alignment model of
Ney and Vogel (1996).
The HMM model captures the intuition that the
alignment vector a will in general progress across
the sentence e in a pattern which is mostly local, per-
haps with a few large jumps. That is, alignments are
locally monotonic more often than not.
Formally, the HMM model factors as:
p(a, f|e) =
J?
j=1
pd(aj |aj? , j)p`(fj |eaj )
where j? is the position of the last non-null-aligned
source word before position j, p` is a lexical transfer
model, and pd is a local distortion model. As in all
such models, the lexical component p` is a collec-
tion of unsmoothed multinomial distributions over
19
foreign words.
The distortion model pd(aj |aj? , j) is a distribu-
tion over the signed distance aj ? aj? , typically
parameterized as a multinomial, Gaussian or expo-
nential distribution. The implementation that serves
as our baseline uses a multinomial distribution with
separate parameters for j = 1, j = J and shared
parameters for all 1 < j < J . Null alignments have
fixed probability at any position. Inference over a
requires only the standard forward-backward algo-
rithm.
3.1 Syntax-Sensitive Distortion
The broad and robust success of the HMM align-
ment model underscores the utility of its assump-
tions: that word-level translations can be usefully
modeled via first-degree Markov transitions and in-
dependent lexical productions. However, its distor-
tion model considers only string distance, disregard-
ing the constituent structure of the English sentence.
To allow syntax-sensitive distortion, we consider
a new distortion model of the form pd(aj |aj? , j, t).
We condition on t via a generative process that tran-
sitions between two English positions by traversing
the unique shortest path ?(aj? ,aj ,t) through t from
aj? to aj . We constrain ourselves to this shortest
path using a staged generative process.
Stage 1 (POP(n?), STOP(n?)): Starting in the leaf
node at aj? , we choose whether to STOP or
POP from child to parent, conditioning on the
type of the parent node n?. Upon choosing
STOP, we transition to stage 2.
Stage 2 (MOVE(n?, d)): Again, conditioning on the
type of the parent n? of the current node n, we
choose a sibling n? based on the signed distance
d = ?n?(n) ? ?n?(n?), where ?n?(n) is the index
of n in the child list of n?. Zero distance moves
are disallowed. After exactly one MOVE, we
transition to stage 3.
Stage 3 (PUSH(n, ?n(n?))): Given the current node
n, we select one of its children n?, conditioning
on the type of n and the position of the child
?n(n?). We continue to PUSH until reaching a
leaf.
This process is a first-degree Markov walk
through the tree, conditioning on the current node
Stage 1: { Pop(VBN), Pop(ADJP), Pop(VP), Stop(S) }
Stage 2: { Move(S, -1) }
Stage 3: { Push(NP, 1), Push(DT, 1) }
S
NP
VP
ADJP
NN VBN
NNS
DT
AUX
The jobs are career oriented .
.
Figure 2: An example sequence of staged tree tran-
sitions implied by the unique shortest path from the
word oriented (aj? = 5) to the word the (aj = 1).
and its immediate surroundings at each step. We en-
force the property that ?(aj? ,aj ,t) be unique by stag-
ing the process and disallowing zero distance moves
in stage 2. Figure 2 gives an example sequence of
tree transitions for a small parse tree.
The parameterization of this distortion model fol-
lows directly from its generative process. Given a
path ?(aj? ,aj ,t) with r = k +m+3 nodes including
the two leaves, the nearest common ancestor, k in-
tervening nodes on the ascent and m on the descent,
we express it as a triple of staged tree transitions that
include k POPs, a STOP, a MOVE, and m PUSHes:
?
?
{POP(n2), ..., POP(nk+1), STOP(nk+2)}
{MOVE (nk+2, ?(nk+3)? ?(nk+1))}
{PUSH (nk+3, ?(nk+4)) , ..., PUSH (nr?1, ?(nr))}
?
?
Next, we assign probabilities to each tree transi-
tion in each stage. In selecting these distributions,
we aim to maintain the original HMM?s sensitivity
to target word order:
? Selecting POP or STOP is a simple Bernoulli
distribution conditioned upon a node type.
? We model both MOVE and PUSH as multino-
mial distributions over the signed distance in
positions (assuming a starting position of 0 for
PUSH), echoing the parameterization popular
in implementations of the HMM model.
This model reduces to the classic HMM distor-
tion model given minimal English trees of only uni-
formly labeled pre-terminals and a root node. The
classic 0-distance distortion would correspond to the
20
00.2
0.40.6
-2 -1 0 1 2 3 4 5Li
kelihood HMMSyntactic
T
h
i
s
w
o
u
l
d
r
e
l
i
e
v
e
t
h
e
p
r
e
s
s
u
r
e
o
n
o
i
l
.
S
VB
DT .
MD VP
VP
NP PP
DT NN IN NN
Figure 3: For this example sentence, the learned dis-
tortion distribution of pd(aj |aj? , j, t) resembles its
counterpart pd(aj |aj? , j) of the HMM model but re-
flects the constituent structure of the English tree t.
For instance, the short path from relieve to on gives
a high transition likelihood.
STOP probability of the pre-terminal label; all other
distances would correspond to MOVE probabilities
conditioned on the root label, and the probability of
transitioning to the terminal state would correspond
to the POP probability of the root label.
As in a multinomial-distortion implementation of
the classic HMM model, we must sometimes artifi-
cially normalize these distributions in the deficient
case that certain jumps extend beyond the ends of
the local rules. For this reason, MOVE and PUSH
are actually parameterized by three values: a node
type, a signed distance, and a range of options that
dictates a normalization adjustment.
Once each tree transition generates a score, their
product gives the probability of the entire path, and
thereby the cost of the transition between string po-
sitions. Figure 3 shows an example learned distribu-
tion that reflects the structure of the given parse.
With these derivation steps in place, we must ad-
dress a handful of special cases to complete the gen-
erative model. We require that the Markov walk
from leaf to leaf of the English tree must start and
end at the root, using the following assumptions.
1. Given no previous alignment, we forego stages
1 and 2 and begin with a series of PUSHes from
the root of the tree to the desired leaf.
2. Given no subsequent alignments, we skip
stages 2 and 3 after a series of POPs including
a pop conditioned on the root node.
3. If the first choice in stage 1 is to STOP at the
current leaf, then stage 2 and 3 are unneces-
sary. Hence, a choice to STOP immediately is
a choice to emit another foreign word from the
current English word.
4. We flatten unary transitions from the tree when
computing distortion probabilities.
5. Null alignments are treated just as in the HMM
model, incurring a fixed cost from any position.
This model can be simplified by removing all con-
ditioning on node types. However, we found this
variant to slightly underperform the full model de-
scribed above. Intuitively, types carry information
about cross-linguistic ordering preferences.
3.2 Training Approach
Because our model largely mirrors the genera-
tive process and structure of the original HMM
model, we apply a nearly identical training proce-
dure to fit the parameters to the training data via the
Expectation-Maximization algorithm. Och and Ney
(2003) gives a detailed exposition of the technique.
In the E-step, we employ the forward-backward
algorithm and current parameters to find expected
counts for each potential pair of links in each train-
ing pair. In this familiar dynamic programming ap-
proach, we must compute the distortion probabilities
for each pair of English positions.
The minimal path between two leaves in a tree can
be computed efficiently by first finding the path from
the root to each leaf, then comparing those paths to
find the nearest common ancestor and a path through
it ? requiring time linear in the height of the tree.
Computing distortion costs independently for each
pair of words in the sentence imposed a computa-
tional overhead of roughly 50% over the original
HMM model. The bulk of this increase arises from
the fact that distortion probabilities in this model
must be computed for each unique tree, in contrast
21
to the original HMM which has the same distortion
probabilities for all sentences of a given length.
In the M-step, we re-estimate the parameters of
the model using the expected counts collected dur-
ing the E-step. All of the component distributions
of our lexical and distortion models are multinomi-
als. Thus, upon assuming these expectations as val-
ues for the hidden alignment vectors, we maximize
likelihood of the training data simply by comput-
ing relative frequencies for each component multi-
nomial. For the distortion model, an expected count
c(aj , aj?) is allocated to all tree transitions along the
path ?(aj? ,aj ,t). These allocations are summed and
normalized for each tree transition type to complete
re-estimation. The method of re-estimating the lexi-
cal model remains unchanged.
Initialization of the lexical model affects perfor-
mance dramatically. Using the simple but effective
joint training technique of Liang et al (2006), we
initialized the model with lexical parameters from a
jointly trained implementation of IBM Model 1.
3.3 Improved Posterior Inference
Liang et al (2006) shows that thresholding the pos-
terior probabilities of alignments improves AER rel-
ative to computing Viterbi alignments. That is, we
choose a threshold ? (typically ? = 0.5), and take
a = {(i, j) : p(aj = i|f, e) > ?}.
Posterior thresholding provides computationally
convenient ways to combine multiple alignments,
and bidirectional combination often corrects for
errors in individual directional alignment models.
Liang et al (2006) suggests a soft intersection of a
model m with a reverse model r (foreign to English)
that thresholds the product of their posteriors at each
position:
a = {(i, j) : pm(aj = i|f, e) ? pr(ai = j|f, e) > ?} .
These intersected alignments can be quite sparse,
boosting precision at the expense of recall. We
explore a generalized version to this approach by
varying the function c that combines pm and pr:
a = {(i, j) : c(pm, pr) > ?}. If c is the max func-
tion, we recover the (hard) union of the forward and
reverse posterior alignments. If c is the min func-
tion, we recover the (hard) intersection. A novel,
high performing alternative is the soft union, which
we evaluate in the next section:
c(pm, pr) =
pm(aj = i|f, e) + pr(ai = j|f, e)
2
.
Syntax-alignment compatibility can be further
promoted with a simple posterior decoding heuristic
we call competitive thresholding. Given a threshold
and a matrix c of combined weights for each pos-
sible link in an alignment, we include a link (i, j)
only if its weight cij is above-threshold and it is con-
nected to the maximum weighted link in both row i
and column j. That is, only the maximum in each
column and row and a contiguous enclosing span of
above-threshold links are included in the alignment.
3.4 Related Work
This proposed model is not the first variant of the
HMM model that incorporates syntax-based distor-
tion. Lopez and Resnik (2005) considers a sim-
pler tree distance distortion model. Daume? III and
Marcu (2005) employs a syntax-aware distortion
model for aligning summaries to documents, but
condition upon the roots of the constituents that are
jumped over during a transition, instead of those that
are visited during a walk through the tree. In the case
of syntactic machine translation, we want to condi-
tion on crossing constituent boundaries, even if no
constituents are skipped in the process.
4 Experimental Results
To understand the behavior of this model, we com-
puted the standard alignment error rate (AER) per-
formance metric.2 We also investigated extraction-
specific metrics: the frequency of interior nodes ? a
measure of how often the alignments violate the con-
stituent structure of English parses ? and a variant of
the CPER metric of Ayan and Dorr (2006).
We evaluated the performance of our model on
both French-English and Chinese-English manually
aligned data sets. For Chinese, we trained on the
FBIS corpus and the LDC bilingual dictionary, then
tested on 491 hand-aligned sentences from the 2002
2The hand-aligned test data has been annotated with both
sure alignments S and possible alignments P , with S ? P , ac-
cording to the specifications described in Och and Ney (2003).
With these alignments, we compute AER for a proposed align-
ment A as:
?
1? |A?S|+|A?P ||A|+|S|
?
? 100%.
22
French Precision Recall AER
Classic HMM 93.9 93.0 6.5
Syntactic HMM 95.2 91.5 6.4
GIZA++ 96.0 86.1 8.6
Chinese Precision Recall AER
Classic HMM 81.6 78.8 19.8
Syntactic HMM 82.2 76.8 20.5
GIZA++? 61.9 82.6 29.7
Table 1: Alignment error rates (AER) for 100k train-
ing sentences. The evaluated alignments are a soft
union for French and a hard union for Chinese, both
using competitive thresholding decoding. ?From
Ayan and Dorr (2006), grow-diag-final heuristic.
NIST MT evaluation set. For French, we used the
Hansards data from the NAACL 2003 Shared Task.3
We trained on 100k sentences for each language.
4.1 Alignment Error Rate
We compared our model to the original HMM
model, identical in implementation to our syntac-
tic HMM model save the distortion component.
Both models were initialized using the same jointly
trained Model 1 parameters (5 iterations), then
trained independently for 5 iterations. Both models
were then combined with an independently trained
HMM model in the opposite direction: f ? e.4 Ta-
ble 1 summarizes the results; the two models per-
form similarly. The main benefit of our model is the
effect on rule extraction, discussed below.
We also compared our French results to the pub-
lic baseline GIZA++ using the script published for
the NAACL 2006 Machine Translation Workshop
Shared Task.5 Similarly, we compared our Chi-
nese results to the GIZA++ results in Ayan and
Dorr (2006). Our models substantially outperform
GIZA++, confirming results in Liang et al (2006).
Table 2 shows the effect on AER of competitive
thresholding and different combination functions.
3Following previous work, we developed our system on the
37 provided validation sentences and the first 100 sentences of
the corpus test set. We used the remainder as a test set.
4Null emission probabilities were fixed to 1|e| , inversely pro-
portional to the length of the English sentence. The decoding
threshold was held fixed at ? = 0.5.
5Training includes 16 iterations of various IBM models and
a fixed null emission probability of .01. The output of running
GIZA++ in both directions was combined via intersection.
French w/o CT with CT
Hard Intersection (Min) 8.4 8.4
Hard Union (Max) 12.3 7.7
Soft Intersection (Product) 6.9 7.1
Soft Union (Average) 6.7 6.4
Chinese w/o CT with CT
Hard Intersection (Min) 27.4 27.4
Hard Union (Max) 25.0 20.5
Soft Intersection (Product) 25.0 25.2
Soft Union (Average) 21.1 21.6
Table 2: Alignment error rates (AER) by decoding
method for the syntactic HMM model. The compet-
itive thresholding heuristic (CT) is particularly help-
ful for the hard union combination method.
The most dramatic effect of competitive threshold-
ing is to improve alignment quality for hard unions.
It also impacts rule extraction substantially.
4.2 Rule Extraction Results
While its competitive AER certainly speaks to the
potential utility of our syntactic distortion model, we
proposed the model for a different purpose: to mini-
mize the particularly troubling alignment errors that
cross constituent boundaries and violate the struc-
ture of English parse trees. We found that while the
HMM and Syntactic models have very similar AER,
they make substantially different errors.
To investigate the differences, we measured the
degree to which each set of alignments violated the
supplied parse trees, by counting the frequency of
interior nodes that are not null aligned. Figure 4
summarizes the results of the experiment for French:
the Syntactic distortion with competitive threshold-
ing reduces tree violations substantially. Interior
node frequency is reduced by 56% overall, with
the most dramatic improvement observed for clausal
constituents. We observed a similar 50% reduction
for the Chinese data.
Additionally, we evaluated our model with the
transducer analog to the consistent phrase error rate
(CPER) metric of Ayan and Dorr (2006). This evalu-
ation computes precision, recall, and F1 of the rules
extracted under a proposed alignment, relative to the
rules extracted under the gold-standard sure align-
ments. Table 3 shows improvements in F1 by using
23
Reduction 
(percent)
NP
54.1
14.6
VP
46.3
10.3
PP
52.4
6.3
S
77.5
4.8
SBAR
58.0
1.9
Non-
Terminals
53.1
41.1
All
56.3
100.0
Corpus
Frequency
0.05.010.0
15.020.025.0
30.0
Interior 
Node Fr
equency
(percent
) HMM Model Syntactic Model + CT
Corpus frequency:
Reduction (percent): 38.9 47.2 45.3 54.8 59.7 43.7 45.1
14.6 10.3 6.3 4.8 1.9 41.1 100
Figure 4: The syntactic distortion model with com-
petitive thresholding decreases the frequency of in-
terior nodes for each type and the whole corpus.
the syntactic HMM model and competitive thresh-
olding together. Individually, each of these changes
contributes substantially to this increase. Together,
their benefits are partially, but not fully, additive.
5 Conclusion
In light of the need to reconcile word alignments
with phrase structure trees for syntactic MT, we have
proposed an HMM-like model whose distortion is
sensitive to such trees. Our model substantially re-
duces the number of interior nodes in the aligned
corpus and improves rule extraction while nearly
retaining the speed and alignment accuracy of the
HMM model. While it remains to be seen whether
these improvements impact final translation accu-
racy, it is reasonable to hope that, all else equal,
alignments which better respect syntactic correspon-
dences will be superior for syntactic MT.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going beyond aer:
An extensive analysis of word alignments and their impact
on mt. In ACL.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,
and Robert L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computational
Linguistics, 19:263?311.
Colin Cherry and Dekang Lin. 2006. Soft syntactic constraints
for word alignment through discriminative training. In ACL.
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In ACL.
Hal Daume? III and Daniel Marcu. 2005. Induction of word and
phrase alignments for automatic document summarization.
Computational Linguistics, 31(4):505?530, December.
French Prec. Recall F1
Classic HMM Baseline 40.9 17.6 24.6
Syntactic HMM + CT 33.9 22.4 27.0
Relative change -17% 27% 10%
Chinese Prec. Recall F1
HMM Baseline (hard) 66.1 14.5 23.7
HMM Baseline (soft) 36.7 39.1 37.8
Syntactic + CT (hard) 48.0 41.6 44.6
Syntactic + CT (soft) 32.9 48.7 39.2
Relative change? 31% 6% 18%
Table 3: Relative to the classic HMM baseline, our
syntactic distortion model with competitive thresh-
olding improves the tradeoff between precision and
recall of extracted transducer rules. Both French
aligners were decoded using the best-performing
soft union combiner. For Chinese, we show aligners
under both soft and hard union combiners. ?Denotes
relative change from the second line to the third line.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In HLT-NAACL.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu,
Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scal-
able inference and training of context-rich syntactic transla-
tion models. In ACL.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by
agreement. In HLT-NAACL.
A. Lopez and P. Resnik. 2005. Improved hmm alignment mod-
els for languages with scarce resources. In ACL WPT-05.
I. Dan Melamed. 2004. Algorithms for syntax-aware statistical
machine translation. In Proceedings of the Conference on
Theoretical and Methodological Issues in Machine Transla-
tion.
Robert C. Moore. 2005. A discriminative framework for bilin-
gual word alignment. In EMNLP.
Hermann Ney and Stephan Vogel. 1996. Hmm-based word
alignment in statistical translation. In COLING.
Franz Josef Och and Hermann Ney. 2003. A systematic com-
parison of various statistical alignment models. Computa-
tional Linguistics, 29:19?51.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A
discriminative matching approach to word alignment. In
EMNLP.
Dekai Wu. 1997. Stochastic inversion transduction grammars
and bilingual parsing of parallel corpora. Computational
Linguistics, 23:377?404.
24
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 25?28,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The Complexity of Phrase Alignment Problems
John DeNero and Dan Klein
Computer Science Division, EECS Department
University of California at Berkeley
{denero, klein}@cs.berkeley.edu
Abstract
Many phrase alignment models operate over
the combinatorial space of bijective phrase
alignments. We prove that finding an optimal
alignment in this space is NP-hard, while com-
puting alignment expectations is #P-hard. On
the other hand, we show that the problem of
finding an optimal alignment can be cast as
an integer linear program, which provides a
simple, declarative approach to Viterbi infer-
ence for phrase alignment models that is em-
pirically quite efficient.
1 Introduction
Learning in phrase alignment models generally re-
quires computing either Viterbi phrase alignments
or expectations of alignment links. For some re-
stricted combinatorial spaces of alignments?those
that arise in ITG-based phrase models (Cherry and
Lin, 2007) or local distortion models (Zens et al,
2004)?inference can be accomplished using poly-
nomial time dynamic programs. However, for more
permissive models such as Marcu and Wong (2002)
and DeNero et al (2006), which operate over the full
space of bijective phrase alignments (see below), no
polynomial time algorithms for exact inference have
been exhibited. Indeed, Marcu and Wong (2002)
conjectures that none exist. In this paper, we show
that Viterbi inference in this full space is NP-hard,
while computing expectations is #P-hard.
On the other hand, we give a compact formula-
tion of Viterbi inference as an integer linear program
(ILP). Using this formulation, exact solutions to the
Viterbi search problem can be found by highly op-
timized, general purpose ILP solvers. While ILP
is of course also NP-hard, we show that, empir-
ically, exact solutions are found very quickly for
most problem instances. In an experiment intended
to illustrate the practicality of the ILP approach, we
show speed and search accuracy results for aligning
phrases under a standard phrase translation model.
2 Phrase Alignment Problems
Rather than focus on a particular model, we describe
four problems that arise in training phrase alignment
models.
2.1 Weighted Sentence Pairs
A sentence pair consists of two word sequences, e
and f. A set of phrases {eij} contains all spans eij
from between-word positions i to j of e. A link is an
aligned pair of phrases, denoted (eij , fkl).1
Let a weighted sentence pair additionally include
a real-valued function ? : {eij}?{fkl} ? R, which
scores links. ?(eij , fkl) can be sentence-specific, for
example encoding the product of a translation model
and a distortion model for (eij , fkl). We impose no
additional restrictions on ? for our analysis.
2.2 Bijective Phrase Alignments
An alignment is a set of links. Given a weighted
sentence pair, we will consider the space of bijective
phrase alignments A: those a ? {eij} ? {fkl} that
use each word token in exactly one link. We first
define the notion of a partition: unionsqiSi = T means Si
are pairwise disjoint and cover T . Then, we can for-
mally define the set of bijective phrase alignments:
A =
?
?
?
a :
?
(eij ,fkl)?a
eij = e ;
?
(eij ,fkl)?a
fkl = f
?
?
?
1As in parsing, the position between each word is assigned
an index, where 0 is to the left of the first word. In this paper,
we assume all phrases have length at least one: j > i and l > k.
25
Both the conditional model of DeNero et al
(2006) and the joint model of Marcu and Wong
(2002) operate in A, as does the phrase-based de-
coding framework of Koehn et al (2003).
2.3 Problem Definitions
For a weighted sentence pair (e, f, ?), let the score
of an alignment be the product of its link scores:
?(a) =
?
(eij ,fkl)?a
?(eij , fkl).
Four related problems involving scored alignments
arise when training phrase alignment models.
OPTIMIZATION, O: Given (e, f, ?), find the high-
est scoring alignment a.
DECISION, D: Given (e, f, ?), decide if there is an
alignment a with ?(a) ? 1.
O arises in the popular Viterbi approximation to
EM (Hard EM) that assumes probability mass is
concentrated at the mode of the posterior distribu-
tion over alignments. D is the corresponding deci-
sion problem for O, useful in analysis.
EXPECTATION, E: Given a weighted sentence pair
(e, f, ?) and indices i, j, k, l, compute
?
a ?(a)
over all a ? A such that (eij , fkl) ? a.
SUM, S: Given (e, f, ?), compute
?
a?A ?(a).
E arises in computing sufficient statistics for
re-estimating phrase translation probabilities (E-
step) when training models. The existence of a
polynomial time algorithm for E implies a poly-
nomial time algorithm for S , because A =
?|e|
j=1
?|f|?1
k=0
?|f|
l=k+1 {a : (e0j , fkl) ? a,a ? A} .
3 Complexity of Inference in A
For the space A of bijective alignments, problems E
and O have long been suspected of being NP-hard,
first asserted but not proven in Marcu and Wong
(2002). We give a novel proof that O is NP-hard,
showing that D is NP-complete by reduction from
SAT, the boolean satisfiability problem. This re-
sult holds despite the fact that the related problem of
finding an optimal matching in a weighted bipartite
graph (the ASSIGNMENT problem) is polynomial-
time solvable using the Hungarian algorithm.
3.1 Reducing Satisfiability to D
A reduction proof of NP-completeness gives a con-
struction by which a known NP-complete problem
can be solved via a newly proposed problem. From a
SAT instance, we construct a weighted sentence pair
for which alignments with positive score correspond
exactly to the SAT solutions. Since SAT is NP-
complete and our construction requires only poly-
nomial time, we conclude that D is NP-complete.2
SAT: Given vectors of boolean variables v = (v)
and propositional clauses3 C = (C), decide
whether there exists an assignment to v that si-
multaneously satisfies each clause in C.
For a SAT instance (v,C), we construct f to con-
tain one word for each clause, and e to contain sev-
eral copies of the literals that appear in those clauses.
? scores only alignments from clauses to literals that
satisfy the clauses. The crux of the construction lies
in ensuring that no variable is assigned both true and
false. The details of constructing such a weighted
sentence pair wsp(v,C) = (e, f, ?), described be-
low, are also depicted in figure 1.
1. f contains a word for each C, followed by an
assignment word for each variable, assign(v).
2. e contains c(`) consecutive words for each lit-
eral `, where c(`) is the number of times that `
appears in the clauses.
Then, we set ?(?, ?) = 0 everywhere except:
3. For all clauses C and each satisfying literal `,
and each one-word phrase e in e containing `,
?(e, fC) = 1. fC is the one-word phrase con-
taining C in f.
4. The assign(v) words in f align to longer phrases
of literals and serve to consistently assign each
variable by using up inconsistent literals. They
also align to unused literals to yield a bijection.
Let ek[`] be the phrase in e containing all literals
` and k negations of `. fassign(v) is the one-word
phrase for assign(v). Then, ?(ek[`], fassign(v)) =
1 for ` ? {v, v?} and all applicable k.
2Note that D is trivially in NP: given an alignment a, it is
easy to determine whether or not ?(a) ? 1.
3A clause is a disjunction of literals. A literal is a bare vari-
able vn or its negation v?n. For instance, v2? v?7? v?9 is a clause.
26
v1
? v
2
? v
3
v?
1
? v
2
? v?
3
v?
1
? v?
2
? v?
3
v?
1
? v?
2
? v
3
v
1
v?
1
v?
2
v?
3
v
3
v
2
v?
1
v?
1
v
2
v?
2
v
3
v?
3
v
1
v?
1
v?
2
v?
3
v
3
v
2
v?
1
v?
1
v
2
v?
2
v
3
v?
3
(a) (b) (c)
assign(v
1
)
assign(v
2
)
assign(v
3
)
(d)
v
1
is true
v
2
is false
v
3
is false
Figure 1: (a) The clauses of an example SAT instance with v = (v
1
, v
2
, v
3
). (b) The weighted sentence pair wsp(v,C)
constructed from the SAT instance. All links that have ? = 1 are marked with a blue horizontal stripe. Stripes in the
last three rows demarcate the alignment options for each assign(vn), which consume all words for some literal. (c) A
bijective alignment with score 1. (d) The corresponding satisfying assignment for the original SAT instance.
Claim 1. If wsp(v,C) has an alignment a with
?(a) ? 1, then (v,C) is satisfiable.
Proof. The score implies that f aligns using all one-
word phrases and ?ai ? a, ?(ai) = 1. By condition
4, each fassign(v) aligns to all v? or all v in e. Then,
assign each v to true if fassign(v) aligns to all v?, and
false otherwise. By condition 3, each C must align
to a satisfying literal, while condition 4 assures that
all available literals are consistent with this assign-
ment to v, which therefore satisfies C.
Claim 2. If (v,C) is satisfiable, then wsp(v,C) has
an alignment a with ?(a) = 1.
Proof. We construct such an alignment a from the
satisfying assignment v. For each C, we choose a
satisfying literal ` consistent with the assignment.
Align fC to the first available ` token in e if the cor-
responding v is true, or the last if v is false. Align
each fassign(v) to all remaining literals for v.
Claims 1 and 2 together show that D is NP-
complete, and therefore that O is NP-hard.
3.2 Reducing Perfect Matching to S
With another construction, we can show that S is #P-
hard, meaning that it is at least as hard as any #P-
complete problem. #P is a class of counting prob-
lems related to NP, and #P-hard problems are NP-
hard as well.
COUNTING PERFECT MATCHINGS, CPM
Given a bipartite graph G with 2n vertices,
count the number of matchings of size n.
For a bipartite graphGwith edge setE = {(vj , vl)},
we construct e and f with n words each, and set
?(ej?1 j , fl?1 l) = 1 and 0 otherwise. The num-
ber of perfect matchings in G is the sum S for
this weighted sentence pair. CPM is #P-complete
(Valiant, 1979), so S (and hence E) is #P-hard.
4 Solving the Optimization Problem
Although O is NP-hard, we present an approach to
solving it using integer linear programming (ILP).
4.1 Previous Inference Approaches
Marcu and Wong (2002) describes an approximation
to O. Given a weighted sentence pair, high scoring
phrases are linked together greedily to reach an ini-
tial alignment. Then, local operators are applied to
hill-climb A in search of the maximum a. This pro-
cedure also approximates E by collecting weighted
counts as the space is traversed.
DeNero et al (2006) instead proposes an
exponential-time dynamic program to systemati-
cally explore A, which can in principle solve either
O or E. In practice, however, the space of align-
ments has to be pruned severely using word align-
ments to control the running time of EM.
Notably, neither of these inference approaches of-
fers any test to know if the optimal alignment is ever
found. Furthermore, they both require small data
sets due to computational expense.
4.2 Alignment via an Integer Program
We cast O as an ILP problem, for which many opti-
mization techniques are well known. First, we in-
27
troduce binary indicator variables ai,j,k,l denoting
whether (eij , fkl) ? a. Furthermore, we introduce
binary indicators ei,j and fk,l that denote whether
some (eij , ?) or (?, fkl) appears in a, respectively. Fi-
nally, we represent the weight function ? as a weight
vector in the program: wi,j,k,l = log ?(eij , fkl).
Now, we can express an integer program that,
when optimized, will yield the optimal alignment of
our weighted sentence pair.
max
?
i,j,k,l
wi,j,k,l ? ai,j,k,l
s.t.
?
i,j:i<x?j
ei,j = 1 ?x : 1 ? x ? |e| (1)
?
k,l:k<y?l
fk,l = 1 ?y : 1 ? y ? |f | (2)
ei,j =
?
k,l
ai,j,k,l ?i, j (3)
fk,l =
?
i,j
ai,j,k,l ?k, l (4)
with the following constraints on index variables:
0 ? i < |e|, 0 < j ? |e|, i < j
0 ? k < |f |, 0 < l ? |f |, k < l .
The objective function is log ?(a) for a implied
by {ai,j,k,l = 1}. Constraint equation 1 ensures that
the English phrases form a partition of e ? each word
in e appears in exactly one phrase ? as does equa-
tion 2 for f. Constraint equation 3 ensures that each
phrase in the chosen partition of e appears in exactly
one link, and that phrases not in the partition are not
aligned (and likewise constraint 4 for f).
5 Applications
The need to find an optimal phrase alignment for a
weighted sentence pair arises in at least two appli-
cations. First, a generative phrase alignment model
can be trained with Viterbi EM by finding optimal
phrase alignments of a training corpus (approximate
E-step), then re-estimating phrase translation param-
eters from those alignments (M-step).
Second, this is an algorithm for forced decoding:
finding the optimal phrase-based derivation of a par-
ticular target sentence. Forced decoding arises in
online discriminative training, where model updates
are made toward the most likely derivation of a gold
translation (Liang et al, 2006).
Sentences per hour on a four-core server 20,000
Frequency of optimal solutions found 93.4%
Frequency of -optimal solutions found 99.2%
Table 1: The solver, tuned for speed, regularly reports
solutions that are within 10?5 of optimal.
Using an off-the-shelf ILP solver,4 we were able
to quickly and reliably find the globally optimal
phrase alignment under ?(eij , fkl) derived from the
Moses pipeline (Koehn et al, 2007).5 Table 1 shows
that finding the optimal phrase alignment is accurate
and efficient.6 Hence, this simple search technique
effectively addresses the intractability challenges in-
herent in evaluating new phrase alignment ideas.
References
Colin Cherry and Dekang Lin. 2007. Inversion transduc-
tion grammar for joint phrasal translation modeling.
In NAACL-HLT Workshop on Syntax and Structure in
Statistical Translation.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In NAACL Workshop on Statistical
Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In HLT-
NAACL.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In ACL.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In ACL.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In EMNLP.
Leslie G. Valiant. 1979. The complexity of computing
the permanent. In Theoretical Computer Science 8.
Richard Zens, Hermann Ney, Taro Watanabeand, and
E. Sumita. 2004. Reordering constraints for phrase
based statistical machine translation. In Coling.
4We used Mosek: www.mosek.com.
5?(eij , fkl) was estimated using the relative frequency of
phrases extracted by the default Moses training script. We eval-
uated on English-Spanish Europarl, sentences up to length 25.
6ILP solvers include many parameters that trade off speed
for accuracy. Substantial speed gains also follow from explicitly
pruning the values of ILP variables based on prior information.
28
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567?575,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Fast Consensus Decoding over Translation Forests
John DeNero David Chiang and Kevin Knight
Computer Science Division Information Sciences Institute
University of California, Berkeley University of Southern California
denero@cs.berkeley.edu {chiang, knight}@isi.edu
Abstract
The minimum Bayes risk (MBR) decoding ob-
jective improves BLEU scores for machine trans-
lation output relative to the standard Viterbi ob-
jective of maximizing model score. However,
MBR targeting BLEU is prohibitively slow to op-
timize over k-best lists for large k. In this pa-
per, we introduce and analyze an alternative to
MBR that is equally effective at improving per-
formance, yet is asymptotically faster ? running
80 times faster than MBR in experiments with
1000-best lists. Furthermore, our fast decoding
procedure can select output sentences based on
distributions over entire forests of translations, in
addition to k-best lists. We evaluate our proce-
dure on translation forests from two large-scale,
state-of-the-art hierarchical machine translation
systems. Our forest-based decoding objective
consistently outperforms k-best list MBR, giving
improvements of up to 1.0 BLEU.
1 Introduction
In statistical machine translation, output transla-
tions are evaluated by their similarity to human
reference translations, where similarity is most of-
ten measured by BLEU (Papineni et al, 2002).
A decoding objective specifies how to derive final
translations from a system?s underlying statistical
model. The Bayes optimal decoding objective is
to minimize risk based on the similarity measure
used for evaluation. The corresponding minimum
Bayes risk (MBR) procedure maximizes the ex-
pected similarity score of a system?s translations
relative to the model?s distribution over possible
translations (Kumar and Byrne, 2004). Unfortu-
nately, with a non-linear similarity measure like
BLEU, we must resort to approximating the ex-
pected loss using a k-best list, which accounts for
only a tiny fraction of a model?s full posterior dis-
tribution. In this paper, we introduce a variant
of the MBR decoding procedure that applies effi-
ciently to translation forests. Instead of maximiz-
ing expected similarity, we express similarity in
terms of features of sentences, and choose transla-
tions that are similar to expected feature values.
Our exposition begins with algorithms over k-
best lists. A na??ve algorithm for finding MBR
translations computes the similarity between every
pair of k sentences, entailing O(k2) comparisons.
We show that if the similarity measure is linear in
features of a sentence, then computing expected
similarity for all k sentences requires only k sim-
ilarity evaluations. Specific instances of this gen-
eral algorithm have recently been proposed for two
linear similarity measures (Tromble et al, 2008;
Zhang and Gildea, 2008).
However, the sentence similarity measures we
want to optimize in MT are not linear functions,
and so this fast algorithm for MBR does not ap-
ply. For this reason, we propose a new objective
that retains the benefits of MBR, but can be op-
timized efficiently, even for non-linear similarity
measures. In experiments using BLEU over 1000-
best lists, we found that our objective provided
benefits very similar to MBR, only much faster.
This same decoding objective can also be com-
puted efficiently from forest-based expectations.
Translation forests compactly encode distributions
over much larger sets of derivations and arise nat-
urally in chart-based decoding for a wide variety
of hierarchical translation systems (Chiang, 2007;
Galley et al, 2006; Mi et al, 2008; Venugopal
et al, 2007). The resulting forest-based decoding
procedure compares favorably in both complexity
and performance to the recently proposed lattice-
based MBR (Tromble et al, 2008).
The contributions of this paper include a linear-
time algorithm for MBR using linear similarities,
a linear-time alternative to MBR using non-linear
similarity measures, and a forest-based extension
to this procedure for similarities based on n-gram
counts. In experiments, we show that our fast pro-
cedure is on average 80 times faster than MBR
using 1000-best lists. We also show that using
forests outperforms using k-best lists consistently
across language pairs. Finally, in the first pub-
lished multi-system experiments on consensus de-
567
coding for translation, we demonstrate that bene-
fits can differ substantially across systems. In all,
we show improvements of up to 1.0 BLEU from
consensus approaches for state-of-the-art large-
scale hierarchical translation systems.
2 Consensus Decoding Algorithms
Let e be a candidate translation for a sentence f ,
where e may stand for a sentence or its derivation
as appropriate. Modern statistical machine trans-
lation systems take as input some f and score each
derivation e according to a linear model of fea-
tures:
?
i ?i ??i(f, e). The standard Viterbi decod-
ing objective is to find e? = arg maxe ? ? ?(f, e).
For MBR decoding, we instead leverage a sim-
ilarity measure S(e; e?) to choose a translation us-
ing the model?s probability distribution P(e|f),
which has support over a set of possible transla-
tions E. The Viterbi derivation e? is the mode of
this distribution. MBR is meant to choose a trans-
lation that will be similar, on expectation, to any
possible reference translation. To this end, MBR
chooses e? that maximizes expected similarity to
the sentences in E under P(e|f):1
e? = arg maxe EP(e?|f)
[
S(e; e?)
]
= arg maxe
?
e??E
P(e?|f) ? S(e; e?)
MBR can also be interpreted as a consensus de-
coding procedure: it chooses a translation similar
to other high-posterior translations. Minimizing
risk has been shown to improve performance for
MT (Kumar and Byrne, 2004), as well as other
language processing tasks (Goodman, 1996; Goel
and Byrne, 2000; Kumar and Byrne, 2002; Titov
and Henderson, 2006; Smith and Smith, 2007).
The distribution P(e|f) can be induced from a
translation system?s features and weights by expo-
nentiating with base b to form a log-linear model:
P (e|f) =
b???(f,e)
?
e??E b
???(f,e?)
We follow Ehling et al (2007) in choosing b using
a held-out tuning set. For algorithms in this sec-
tion, we assume that E is a k-best list and b has
been chosen already, so P(e|f) is fully specified.
1Typically, MBR is defined as arg mine?EE[L(e; e
?)] for
some loss function L, for example 1 ? BLEU(e; e?). These
definitions are equivalent.
2.1 Minimum Bayes Risk over Sentence Pairs
Given any similarity measure S and a k-best
list E, the minimum Bayes risk translation can
be found by computing the similarity between all
pairs of sentences in E, as in Algorithm 1.
Algorithm 1 MBR over Sentence Pairs
1: A? ??
2: for e ? E do
3: Ae ? 0
4: for e? ? E do
5: Ae ? Ae + P (e?|f) ? S(e; e?)
6: if Ae > A then A, e?? Ae, e
7: return e?
We can sometimes exit the inner for loop early,
whenever Ae can never become larger than A
(Ehling et al, 2007). Even with this shortcut, the
running time of Algorithm 1 is O(k2 ? n), where
n is the maximum sentence length, assuming that
S(e; e?) can be computed in O(n) time.
2.2 Minimum Bayes Risk over Features
We now consider the case when S(e; e?) is a lin-
ear function of sentence features. Let S(e; e?) be
a function of the form
?
j ?j(e) ? ?j(e
?), where
?j(e?) are real-valued features of e?, and ?j(e) are
sentence-specific weights on those features. Then,
the MBR objective can be re-written as
arg maxe?E EP(e?|f)
[
S(e; e?)
]
= arg maxe
?
e??E
P (e?|f) ?
?
j
?j(e) ? ?j(e
?)
= arg maxe
?
j
?j(e)
[
?
e??E
P (e?|f) ? ?j(e
?)
]
= arg maxe
?
j
?j(e) ? EP(e?|f)
[
?j(e
?)
]
. (1)
Equation 1 implies that we can find MBR trans-
lations by first computing all feature expectations,
then applying S only once for each e. Algorithm 2
proceduralizes this idea: lines 1-4 compute feature
expectations, and lines 5-11 find the translation
with highest S relative to those expectations. The
time complexity is O(k ?n), assuming the number
of non-zero features ?(e?) and weights ?(e) grow
linearly in sentence length n and all features and
weights can be computed in constant time.
568
Algorithm 2 MBR over Features
1: ??? [0 for j ? J ]
2: for e? ? E do
3: for j ? J such that ?j(e?) 6= 0 do
4: ??j ? ??j + P (e?|f) ? ?j(e?)
5: A? ??
6: for e ? E do
7: Ae ? 0
8: for j ? J such that ?j(e) 6= 0 do
9: Ae ? Ae + ?j(e) ? ??j
10: if Ae > A then A, e?? Ae, e
11: return e?
An example of a linear similarity measure is
bag-of-words precision, which can be written as:
U(e; e?) =
?
t?T1
?(e, t)
|e|
? ?(e?, t)
where T1 is the set of unigrams in the language,
and ?(e, t) is an indicator function that equals 1
if t appears in e and 0 otherwise. Figure 1 com-
pares Algorithms 1 and 2 using U(e; e?). Other
linear functions have been explored for MBR, in-
cluding Taylor approximations to the logarithm of
BLEU (Tromble et al, 2008) and counts of match-
ing constituents (Zhang and Gildea, 2008), which
are discussed further in Section 3.3.
2.3 Fast Consensus Decoding using
Non-Linear Similarity Measures
Most similarity measures of interest for machine
translation are not linear, and so Algorithm 2 does
not apply. Computing MBR even with simple
non-linear measures such as BLEU, NIST or bag-
of-words F1 seems to require O(k2) computation
time. However, these measures are all functions
of features of e?. That is, they can be expressed as
S(e;?(e?)) for a feature mapping ? : E ? Rn.
For example, we can express BLEU(e; e?) =
exp
"?
1 ?
|e?|
|e|
?
?
+
1
4
4X
n=1
ln
P
t?Tn
min(c(e, t), c(e?, t))
P
t?Tn
c(e, t)
#
In this expression, BLEU(e; e?) references e? only
via its n-gram count features c(e?, t).2
2The length penalty
?
1 ? |e
?|
|e|
?
?
is also a function of n-
gram counts: |e?| =
P
t?T1
c(e?, t). The negative part oper-
ator (?)? is equivalent to min(?, 0).
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [r(man with)] = 0.4 + 0.6 ? 1.0
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
22.5
45.0
67.5
90.0
Hiero SBMT
70.2
84.6
56.6
61.4
51.1
50.5
Viterbi n-gram precision
Forest n-gram precision at Viterbi recall
Forest n-gram precision for Er(t) ? 1
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+ 13)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) = 0.6+0.7+0.73
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
Figure 1: For the linear similarity measure U(e; e?), which
computes unigram precision, the MBR translation can be
found by iterating either over s ntence pairs (Algorithm 1) or
over features (Algorithm 2). These two algorithms take the
same input (step 1), but diverge in their consensus computa-
tions (steps 2 & 3). However, they produce identical results
for U and any other linear similarity measure.
Following the structure of Equation 1, we can
choose a translation e based on the feature expec-
tations of e?. In particular, we can choose
e? = arg maxe?ES(e;EP(e?|f)
[
?(e?)
]
). (2)
This objective differs from MBR, but has a simi-
lar consensus-building structure. We have simply
moved the expectation inside the similarity func-
tion, just as we did in Equation 1. This new ob-
jective can be optimized by Algorithm 3, a pro-
cedure that runs in O(k ? n) time if the count of
non-zero features in e? and the computation time
of S(e;?(e?)) are both linear in sentence length n.
This fast consensus decoding procedure shares
the same structure as linear MBR: first we com-
pute feature expectations, then we choose the sen-
tence that is most similar to those expectations. In
fact, Algorithm 2 is a special case of Algorithm 3.
Lines 7-9 of the former and line 7 of the latter are
equivalent for linear S(e; e?). Thus, for any linear
similarity measure, Algorithm 3 is an algorithm
for minimum Bayes risk decoding.
569
Algorithm 3 Fast Consensus Decoding
1: ??? [0 for j ? J ]
2: for e? ? E do
3: for j ? J such that ?j(e?) 6= 0 do
4: ??j ? ??j + P (e?|f) ? ?j(e?)
5: A? ??
6: for e ? E do
7: Ae ? S(e; ??)
8: if Ae > A then A, e?? Ae, e
9: return e?
As described, Algorithm 3 can use any sim-
ilarity measure that is defined in terms of real-
valued features of e?. There are some nuances
of this procedure, however. First, the precise
form of S(e;?(e?)) will affect the output, but
S(e;E[?(e?)]) is often an input point for which a
sentence similarity measure S was not originally
defined. For example, our definition of BLEU
above will have integer valued ?(e?) for any real
sentence e?, butE[?(e?)]will not be integer valued.
As a result, we are extending the domain of BLEU
beyond its original intent. One could imagine dif-
ferent feature-based expressions that also produce
BLEU scores for real sentences, but produce dif-
ferent values for fractional features. Some care
must be taken to define S(e;?(e?)) to extend nat-
urally from integer-valued to real-valued features.
Second, while any similarity measure can in
principle be expressed as S(e;?(e?)) for a suffi-
ciently rich feature space, fast consensus decoding
will not apply effectively to all functions. For in-
stance, we cannot naturally use functions that in-
clude alignments or matchings between e and e?,
such as METEOR (Agarwal and Lavie, 2007) and
TER (Snover et al, 2006). Though these functions
can in principle be expressed in terms of features
of e? (for instance with indicator features for whole
sentences), fast consensus decoding will only be
effective if different sentences share many fea-
tures, so that the feature expectations effectively
capture trends in the underlying distribution.
3 Computing Feature Expectations
We now turn our focus to efficiently comput-
ing feature expectations, in service of our fast
consensus decoding procedure. Computing fea-
ture expectations from k-best lists is trivial, but
k-best lists capture very little of the underlying
model?s posterior distribution. In place of k-best
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
20
40
60
80
Hiero SBMT
56.6
61.4
51.1
50.5
N-grams from baseline translation
N-grams with high expected count
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+ 13)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) = 0.6+0.7+0.73
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [c(e, ?man with?)] =
?
h
P (h|f) ? c(h, ?man with?)
= 0.4 ? 1 + (0.6 ? 1.0) ? 1
Figure 2: This translation forest for a Spanish sentence en-
codes two English parse trees. Hyper-edges (boxes) are an-
notated with normalized transition probabilities, as well as
the bigrams produced by each rule application. The expected
count of the bigram ?man with? is the sum of posterior prob-
abilities of the two hyper-edges that produce it. In this exam-
ple, we normalized inside scores at all nodes to 1 for clarity.
lists, compact encodings of translation distribu-
tions have proven effective for MBR (Zhang and
Gildea, 2008; Tromble et al, 2008). In this sec-
tion, we consider BLEU in particular, for which
the relevant features ?(e) are n-gram counts up to
length n = 4. We show how to compute expec-
tations of these counts efficiently from translation
forests.
3.1 Translation Forests
Translation forests compactly encode an exponen-
tial number of output translations for an input
sentence, along with their model scores. Forests
arise naturally in chart-based decoding procedures
for many hierarchical translation systems (Chiang,
2007). Exploiting forests has proven a fruitful av-
enue of research in both parsing (Huang, 2008)
and machine translation (Mi et al, 2008).
Formally, translation forests are weighted
acyclic hyper-graphs. The nodes are states in the
decoding process that include the span (i, j) of the
sentence to be translated, the grammar symbol s
over that span, and the left and right context words
of the translation relevant for computing n-gram
language model scores.3 Each hyper-edge h rep-
resents the application of a synchronous rule r that
combines nodes corresponding to non-terminals in
3Decoder states can include additional information as
well, such as local configurations for dependency language
model scoring.
570
r into a node spanning the union of the child spans
and perhaps some additional portion of the input
sentence covered directly by r?s lexical items. The
weight of h is the incremental score contributed
to all translations containing the rule application,
including translation model features on r and lan-
guage model features that depend on both r and
the English contexts of the child nodes. Figure 2
depicts a forest.
Each n-gram that appears in a translation e is as-
sociated with some h in its derivation: the h corre-
sponding to the rule that produces the n-gram. Un-
igrams are produced by lexical rules, while higher-
order n-grams can be produced either directly by
lexical rules, or by combining constituents. The
n-gram language model score of e similarly de-
composes over the h in e that produce n-grams.
3.2 Computing Expected N-Gram Counts
We can compute expected n-gram counts effi-
ciently from a translation forest by appealing to
the linearity of expectations. Let ?(e) be a vector
of n-gram counts for a sentence e. Then, ?(e) is
the sum of hyper-edge-specific n-gram count vec-
tors ?(h) for all h in e. Therefore, E[?(e)] =
?
h?e E[?(h)].
To compute n-gram expectations for a hyper-
edge, we first compute the posterior probability of
each h, conditioned on the input sentence f :
P(h|f) =
(
?
e:h?e
b???(f,e)
)(
?
e
b???(f,e)
)?1
,
where e iterates over translations in the forest. We
compute the numerator using the inside-outside al-
gorithm, while the denominator is the inside score
of the root node. Note that many possible deriva-
tions of f are pruned from the forest during decod-
ing, and so this posterior is approximate.
The expected n-gram count vector for a hyper-
edge is E[?(h)] = P(h|f) ? ?(h). Hence, after
computing P (h|f) for every h, we need only sum
P(h|f) ? ?(h) for all h to compute E[?(e)]. This
entire procedure is a linear-time computation in
the number of hyper-edges in the forest.
To complete forest-based fast consensus de-
coding, we then extract a k-best list of unique
translations from the forest (Huang et al, 2006)
and continue Algorithm 3 from line 5, which
chooses the e? from the k-best list that maximizes
BLEU(e;E[?(e?)]).
3.3 Comparison to Related Work
Zhang and Gildea (2008) embed a consensus de-
coding procedure into a larger multi-pass decoding
framework. They focus on inversion transduction
grammars, but their ideas apply to richer models as
well. They propose an MBR decoding objective
of maximizing the expected number of matching
constituent counts relative to the model?s distri-
bution. The corresponding constituent-matching
similarity measure can be expressed as a linear
function of features of e?, which are indicators of
constituents. Expectations of constituent indicator
features are the same as posterior constituent prob-
abilities, which can be computed from a transla-
tion forest using the inside-outside algorithm. This
forest-based MBR approach improved translation
output relative to Viterbi translations.
Tromble et al (2008) describe a similar ap-
proach using MBR with a linear similarity mea-
sure. They derive a first-order Taylor approxima-
tion to the logarithm of a slightly modified defini-
tion of corpus BLEU4, which is linear in n-gram
indicator features ?(e?, t) of e?. These features are
weighted by n-gram counts c(e, t) and constants
? that are estimated from held-out data. The lin-
ear similarity measure takes the following form,
where Tn is the set of n-grams:
G(e; e?) = ?0|e|+
4?
n=1
?
t?Tn
?t ? c(e, t) ? ?(e
?, t).
Using G, Tromble et al (2008) extend MBR to
word lattices, which improves performance over
k-best list MBR.
Our approach differs from Tromble et al (2008)
primarily in that we propose decoding with an al-
ternative to MBR using BLEU, while they propose
decoding with MBR using a linear alternative to
BLEU. The specifics of our approaches also differ
in important ways.
First, word lattices are a subclass of forests that
have only one source node for each edge (i.e., a
graph, rather than a hyper-graph). While forests
are more general, the techniques for computing
posterior edge probabilities in lattices and forests
are similar. One practical difference is that the
forests needed for fast consensus decoding are
4The log-BLEU function must be modified slightly to
yield a linear Taylor approximation: Tromble et al (2008)
replace the clipped n-gram count with the product of an n-
gram count and an n-gram indicator function.
571
generated already by the decoder of a syntactic
translation system.
Second, rather than use BLEU as a sentence-
level similarity measure directly, Tromble et al
(2008) approximate corpus BLEU with G above.
The parameters ? of the approximation must be es-
timated on a held-out data set, while our approach
requires no such estimation step.
Third, our approach is also simpler computa-
tionally. The features required to compute G are
indicators ?(e?, t); the features relevant to us are
counts c(e?, t). Tromble et al (2008) compute ex-
pected feature values by intersecting the transla-
tion lattice with a lattices for each n-gram t. By
contrast, expectations of c(e?, t) can all be com-
puted with a single pass over the forest. This con-
trast implies a complexity difference. LetH be the
number of hyper-edges in the forest or lattice, and
T the number of n-grams that can potentially ap-
pear in a translation. Computing indicator expec-
tations seems to require O(H ? T ) time because of
automata intersections. Computing count expec-
tations requires O(H) time, because only a con-
stant number of n-grams can be produced by each
hyper-edge.
Our approaches also differ in the space of trans-
lations from which e? is chosen. A linear similar-
ity measure like G allows for efficient search over
the lattice or forest, whereas fast consensus decod-
ing restricts this search to a k-best list. However,
Tromble et al (2008) showed that most of the im-
provement from lattice-based consensus decoding
comes from lattice-based expectations, not search:
searching over lattices instead of k-best lists did
not change results for two language pairs, and im-
proved a third language pair by 0.3 BLEU. Thus,
we do not consider our use of k-best lists to be a
substantial liability of our approach.
Fast consensus decoding is also similar in char-
acter to the concurrently developed variational de-
coding approach of Li et al (2009). Using BLEU,
both approaches choose outputs that match ex-
pected n-gram counts from forests, though differ
in the details. It is possible to define a similar-
ity measure under which the two approaches are
equivalent.5
5For example, decoding under a variational approxima-
tion to the model?s posterior that decomposes over bigram
probabilities is equivalent to fast consensus decoding with
the similarity measure B(e; e?) =
Q
t?T2
h
c(e?,t)
c(e?,h(t))
ic(e,t)
,
where h(t) is the unigram prefix of bigram t.
4 Experimental Results
We evaluate these consensus decoding techniques
on two different full-scale state-of-the-art hierar-
chical machine translation systems. Both systems
were trained for 2008 GALE evaluations, in which
they outperformed a phrase-based system trained
on identical data.
4.1 Hiero: a Hierarchical MT Pipeline
Hiero is a hierarchical system that expresses its
translation model as a synchronous context-free
grammar (Chiang, 2007). No explicit syntactic in-
formation appears in the core model. A phrase
discovery procedure over word-aligned sentence
pairs provides rule frequency counts, which are
normalized to estimate features on rules.
The grammar rules of Hiero all share a single
non-terminal symbol X , and have at most two
non-terminals and six total items (non-terminals
and lexical items), for example:
my X2 ?s X1 ? X1 de mi X2
We extracted the grammar from training data using
standard parameters. Rules were allowed to span
at most 15 words in the training data.
The log-linear model weights were trained us-
ing MIRA, a margin-based optimization proce-
dure that accommodates many features (Crammer
and Singer, 2003; Chiang et al, 2008). In addition
to standard rule frequency features, we included
the distortion and syntactic features described in
Chiang et al (2008).
4.2 SBMT: a Syntax-Based MT Pipeline
SBMT is a string-to-tree translation system with
rich target-side syntactic information encoded in
the translation model. The synchronous grammar
rules are extracted from word aligned sentence
pairs where the target sentence is annotated with
a syntactic parse (Galley et al, 2004). Rules map
source-side strings to target-side parse tree frag-
ments, and non-terminal symbols correspond to
target-side grammatical categories:
(NP (NP (PRP$ my) NN2 (POS ?s)) NNS1)?
NNS1 de mi NN2
We extracted the grammar via an array of criteria
(Galley et al, 2006; DeNeefe et al, 2007; Marcu
et al, 2006). The model was trained using min-
imum error rate training for Arabic (Och, 2003)
and MIRA for Chinese (Chiang et al, 2008).
572
Arabic-English
Objective Hiero SBMT
Min. Bayes Risk (Alg 1) 2h 47m 12h 42m
Fast Consensus (Alg 3) 5m 49s 5m 22s
Speed Ratio 29 142
Chinese-English
Objective Hiero SBMT
Min. Bayes Risk (Alg 1) 10h 24m 3h 52m
Fast Consensus (Alg 3) 4m 52s 6m 32s
Speed Ratio 128 36
Table 1: Fast consensus decoding is orders of magnitude
faster than MBR when using BLEU as a similarity measure.
Times only include reranking, not k-best list extraction.
4.3 Data Conditions
We evaluated on both Chinese-English and
Arabic-English translation tasks. Both Arabic-
English systems were trained on 220 million
words of word-aligned parallel text. For the
Chinese-English experiments, we used 260 mil-
lion words of word-aligned parallel text; the hi-
erarchical system used all of this data, and the
syntax-based system used a 65-million word sub-
set. All four systems used two language models:
one trained from the combined English sides of
both parallel texts, and another, larger, language
model trained on 2 billion words of English text
(1 billion for Chinese-English SBMT).
All systems were tuned on held-out data (1994
sentences for Arabic-English, 2010 sentences for
Chinese-English) and tested on another dataset
(2118 sentences for Arabic-English, 1994 sen-
tences for Chinese-English). These datasets were
drawn from the NIST 2004 and 2005 evaluation
data, plus some additional data from the GALE
program. There was no overlap at the segment or
document level between the tuning and test sets.
We tuned b, the base of the log-linear model,
to optimize consensus decoding performance. In-
terestingly, we found that tuning b on the same
dataset used for tuning ?was as effective as tuning
b on an additional held-out dataset.
4.4 Results over K-Best Lists
Taking expectations over 1000-best lists6 and us-
ing BLEU7 as a similarity measure, both MBR
6We ensured that k-best lists contained no duplicates.
7To prevent zero similarity scores, we also used a standard
smoothed version of BLEU that added 1 to the numerator and
denominator of all n-gram precisions. Performance results
Arabic-English
Expectations Similarity Hiero SBMT
Baseline - 52.0 53.9
104-best BLEU 52.2 53.9
Forest BLEU 53.0 54.0
Forest Linear G 52.3 54.0
Chinese-English
Expectations Similarity Hiero SBMT
Baseline - 37.8 40.6
104-best BLEU 38.0 40.7
Forest BLEU 38.2 40.8
Forest Linear G 38.1 40.8
Table 2: Translation performance improves when computing
expected sentences from translation forests rather than 104-
best lists, which in turn improve over Viterbi translations. We
also contrasted forest-based consensus decoding with BLEU
and its linear approximation, G. Both similarity measures are
effective, but BLEU outperforms G.
and our variant provided consistent small gains of
0.0?0.2 BLEU. Algorithms 1 and 3 gave the same
small BLEU improvements in each data condition
up to three significant figures.
The two algorithms differed greatly in speed,
as shown in Table 1. For Algorithm 1, we ter-
minated the computation of E[BLEU(e; e?)] for
each e whenever e could not become the maxi-
mal hypothesis. MBR speed depended on how
often this shortcut applied, which varied by lan-
guage and system. Despite this optimization, our
new Algorithm 3 was an average of 80 times faster
across systems and language pairs.
4.5 Results for Forest-Based Decoding
Table 2 contrasts Algorithm 3 over 104-best lists
and forests. Computing E[?(e?)] from a transla-
tion forest rather than a 104-best list improved Hi-
ero by an additional 0.8 BLEU (1.0 over the base-
line). Forest-based expectations always outper-
formed k-best lists, but curiously the magnitude
of benefit was not consistent across systems. We
believe the difference is in part due to more ag-
gressive forest pruning within the SBMT decoder.
For forest-based decoding, we compared two
similarity measures: BLEU and its linear Taylor
approximationG from section 3.3.8 Table 2 shows
were identical to standard BLEU.
8We did not estimate the ? parameters of G ourselves;
instead we used the parameters listed in Tromble et al
(2008), which were also estimated for GALE data. We
also approximated E[?(e?, t)] with a clipped expected count
573
Choose a distribution P over a set of translations E
MBR over Sentence Pairs
Compute pairwise similarity
Compute expectations
Max expected similarity Max feature similarity
3/3 1/4 2/5
1/3 4/4 0/5
2/3 0/4 5/5
MBR over Features
E [?(efficient)] = 0.6
E [?(forest)] = 0.7
E [?(decoding)] = 0.7
E [?(for)] = 0.3
E [?(rusty)] = 0.3
E [?(coating)] = 0.3
E [?(a)] = 0.4
E [?(fish)] = 0.4
E [?(ain?t)] = 0.4
c1 c2 c3
r1
r2
r3
1
2
3
2
3
50.0
50.2
50.4
50.6
50.8
511,660 513,245 514,830
Total model score for 1000 translations
C
o
r
p
u
s
 
B
L
E
U
0
20
40
60
80
Hiero SBMT
56.6
61.4
51.1
50.5
N-grams from baseline translations
N-grams with high expected count
Forest samples (b?2)
Forest samples (b?5)
Viterbi translations
U(e2; e1) =
|efficient|
|efficient for rusty coating|
EU(e1; e?) = 0.3(1+
1
3)+0.4?
2
3
= 0.667
EU(e2; e?) = 0.375
EU(e3; e?) = 0.520
U(e1;E?) =
0.6+0.7+0.7
3
= 0.667
U(e2;E?) = 0.375
U(e3;E?) = 0.520
P (e1|f) = 0.3 ; e1 = efficient forest decoding
P (e2|f) = 0.3 ; e2 = efficient for rusty coating
P (e3|f) = 0.4 ; e3 = A fish ain?t forest decoding
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
E [c(e, ?man with?)] =
?
h
P (h|f) ? c(h, ?man with?)
= 0.4 ? 1 + (0.6 ? 1.0) ? 1
N
-
g
r
a
m
 
P
r
e
c
i
s
i
o
n
Figure 3: N -grams with high expected count are more likely
to appear in the reference translation that n-grams in the
translation model?s Viterbi translation, e?. Above, we com-
pare the precision, relative to reference translations, of sets of
n-grams chosen in two ways. The left bar is the precision of
the n-grams in e?. The right bar is the precision of n-grams
with E[c(e, t)] > ?. To justify this comparison, we chose ?
so that both methods of choosing n-grams gave the same n-
gram recall: the fraction of n-grams in reference translations
that also appeared in e? or had E[c(e, t)] > ?.
that both similarities were effective, but BLEU
outperformed its linear approximation.
4.6 Analysis
Forest-based consensus decoding leverages infor-
mation about the correct translation from the en-
tire forest. In particular, consensus decoding
with BLEU chooses translations using n-gram
count expectations E[c(e, t)]. Improvements in
translation quality should therefore be directly at-
tributable to information in these expected counts.
We endeavored to test the hypothesis that ex-
pected n-gram counts under the forest distribution
carry more predictive information than the base-
line Viterbi derivation e?, which is the mode of the
distribution. To this end, we first tested the pre-
dictive accuracy of the n-grams proposed by e?:
the fraction of the n-grams in e? that appear in a
reference translation. We compared this n-gram
precision to a similar measure of predictive accu-
racy for expected n-gram counts: the fraction of
the n-grams t with E[c(e, t)] ? ? that appear in
a reference. To make these two precisions com-
parable, we chose ? such that the recall of ref-
erence n-grams was equal. Figure 3 shows that
computing n-gram expectations?which sum over
translations?improves the model?s ability to pre-
dict which n-grams will appear in the reference.
min(1,E[c(e?, t)]). Assuming an n-gram appears at most
once per sentence, these expressions are equivalent, and this
assumption holds for most n-grams.
Reference translation:
Mubarak said that he received a telephone call from
Sharon in which he said he was ?ready (to resume ne-
gotiations) but the Palestinians are hesitant.?
Baseline translation:
Mubarak said he had received a telephone call from
Sharon told him he was ready to resume talks with the
Palestinians.
Fast forest-based consensus translation:
Mubarak said that he had received a telephone call from
Sharon told him that he ?was ready to resume the nego-
tiations) , but the Palestinians are hesitant.?
Figure 4: Three translations of an example Arabic sentence:
its human-generated reference, the translation with the high-
est model score under Hiero (Viterbi), and the translation
chosen by forest-based consensus decoding. The consensus
translation reconstructs content lost in the Viterbi translation.
We attribute gains from fast consensus decoding
to this increased predictive accuracy.
Examining the translations chosen by fast con-
sensus decoding, we found that gains in BLEU of-
ten arose from improved lexical choice. However,
in our hierarchical systems, consensus decoding
did occasionally trigger large reordering. We also
found examples where the translation quality im-
proved by recovering content that was missing
from the baseline translation, as in Figure 4.
5 Conclusion
We have demonstrated substantial speed increases
in k-best consensus decoding through a new pro-
cedure inspired by MBR under linear similarity
measures. To further improve this approach, we
computed expected n-gram counts from transla-
tion forests instead of k-best lists. Fast consensus
decoding using forest-based n-gram expectations
and BLEU as a similarity measure yielded con-
sistent improvements over MBR with k-best lists,
yet required only simple computations that scale
linearly with the size of the translation forest.
The space of similarity measures is large and
relatively unexplored, and the feature expectations
that can be computed from forests extend beyond
n-gram counts. Therefore, future work may show
additional benefits from fast consensus decoding.
Acknowledgements
This work was supported under DARPA GALE,
Contract No. HR0011-06-C-0022.
574
References
Abhaya Agarwal and Alon Lavie. 2007. METEOR:
An automatic metric for MT evaluation with high
levels of correlation with human judgments. In Pro-
ceedings of the Workshop on Statistical Machine
Translation for the Association of Computational
Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
Steve DeNeefe, Kevin Knight, Wei Wang, and Daniel
Marcu. 2007. What can syntax-based MT learn
from phrase-based MT? In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing and CoNLL.
Nicola Ehling, Richard Zens, and Hermann Ney. 2007.
Minimum Bayes risk decoding for BLEU. In Pro-
ceedings of the Association for Computational Lin-
guistics: Short Paper Track.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of HLT: the North American Chapter
of the Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Vaibhava Goel and William Byrne. 2000. Minimum
Bayes-risk automatic speech recognition. In Com-
puter, Speech and Language.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the Association for Compu-
tational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the Associa-
tion for Machine Translation in the Americas.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
the Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2002. Minimum
Bayes-risk word alignments of bilingual texts. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical machine
translation with syntactified target language phrases.
In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of the Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings
of the Association for Computational Linguistics.
David Smith and Noah Smith. 2007. Probabilistic
models of nonprojective dependency trees. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing and CoNLL.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proceedings of Association for Machine Transla-
tion in the Americas.
Ivan Titov and James Henderson. 2006. Loss mini-
mization in parse reranking. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Roy Tromble, Shankar Kumar, Franz Josef Och, and
Wolfgang Macherey. 2008. Lattice minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Ashish Venugopal, Andreas Zollmann, and Stephan
Vogel. 2007. An efficient two-pass approach to
synchronous-CFG driven statistical MT. In Pro-
ceedings of HLT: the North American Association
for Computational Linguistics Conference.
Hao Zhang and Daniel Gildea. 2008. Efficient multi-
pass decoding for synchronous context free gram-
mars. In Proceedings of the Association for Compu-
tational Linguistics.
575
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 923?931,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Better Word Alignments with Supervised ITG Models
Aria Haghighi, John Blitzer, John DeNero and Dan Klein
Computer Science Division, University of California at Berkeley
{ aria42,blitzer,denero,klein }@cs.berkeley.edu
Abstract
This work investigates supervised word align-
ment methods that exploit inversion transduc-
tion grammar (ITG) constraints. We con-
sider maximum margin and conditional like-
lihood objectives, including the presentation
of a new normal form grammar for canoni-
calizing derivations. Even for non-ITG sen-
tence pairs, we show that it is possible learn
ITG alignment models by simple relaxations
of structured discriminative learning objec-
tives. For efficiency, we describe a set of prun-
ing techniques that together allow us to align
sentences two orders of magnitude faster than
naive bitext CKY parsing. Finally, we intro-
duce many-to-one block alignment features,
which significantly improve our ITG models.
Altogether, our method results in the best re-
ported AER numbers for Chinese-English and
a performance improvement of 1.1 BLEU over
GIZA++ alignments.
1 Introduction
Inversion transduction grammar (ITG) con-
straints (Wu, 1997) provide coherent structural
constraints on the relationship between a sentence
and its translation. ITG has been extensively
explored in unsupervised statistical word align-
ment (Zhang and Gildea, 2005; Cherry and
Lin, 2007a; Zhang et al, 2008) and machine
translation decoding (Cherry and Lin, 2007b;
Petrov et al, 2008). In this work, we investigate
large-scale, discriminative ITG word alignment.
Past work on discriminative word alignment
has focused on the family of at-most-one-to-one
matchings (Melamed, 2000; Taskar et al, 2005;
Moore et al, 2006). An exception to this is the
work of Cherry and Lin (2006), who discrim-
inatively trained one-to-one ITG models, albeit
with limited feature sets. As they found, ITG
approaches offer several advantages over general
matchings. First, the additional structural con-
straint can result in superior alignments. We con-
firm and extend this result, showing that one-to-
one ITG models can perform as well as, or better
than, general one-to-one matching models, either
using heuristic weights or using rich, learned fea-
tures.
A second advantage of ITG approaches is that
they admit a range of training options. As with
general one-to-one matchings, we can optimize
margin-based objectives. However, unlike with
general matchings, we can also efficiently com-
pute expectations over the set of ITG derivations,
enabling the training of conditional likelihood
models. A major challenge in both cases is that
our training alignments are often not one-to-one
ITG alignments. Under such conditions, directly
training to maximize margin is unstable, and train-
ing to maximize likelihood is ill-defined, since the
target algnment derivations don?t exist in our hy-
pothesis class. We show how to adapt both margin
and likelihood objectives to learn good ITG align-
ers.
In the case of likelihood training, two innova-
tions are presented. The simple, two-rule ITG
grammar exponentially over-counts certain align-
ment structures relative to others. Because of this,
Wu (1997) and Zens and Ney (2003) introduced a
normal form ITG which avoids this over-counting.
We extend this normal form to null productions
and give the first extensive empirical comparison
of simple and normal form ITGs, for posterior de-
coding under our likelihood models. Additionally,
we show how to deal with training instances where
the gold alignments are outside of the hypothesis
class by instead optimizing the likelihood of a set
of minimum-loss alignments.
Perhaps the greatest advantage of ITG mod-
els is that they straightforwardly permit block-
923
structured alignments (i.e. phrases), which gen-
eral matchings cannot efficiently do. The need for
block alignments is especially acute in Chinese-
English data, where oracle AERs drop from 10.2
without blocks to around 1.2 with them. Indeed,
blocks are the primary reason for gold alignments
being outside the space of one-to-one ITG align-
ments. We show that placing linear potential func-
tions on many-to-one blocks can substantially im-
prove performance.
Finally, to scale up our system, we give a com-
bination of pruning techniques that allows us to
sum ITG alignments two orders of magnitude
faster than naive inside-outside parsing.
All in all, our discriminatively trained, block
ITG models produce alignments which exhibit
the best AER on the NIST 2002 Chinese-English
alignment data set. Furthermore, they result in
a 1.1 BLEU-point improvement over GIZA++
alignments in an end-to-end Hiero (Chiang, 2007)
machine translation system.
2 Alignment Families
In order to structurally restrict attention to rea-
sonable alignments, word alignment models must
constrain the set of alignments considered. In this
section, we discuss and compare alignment fami-
lies used to train our discriminative models.
Initially, as in Taskar et al (2005) and Moore
et al (2006), we assume the score a of a potential
alignment a) decomposes as
s(a) = ?
(i,j)?a
sij +
?
i/?a
si +
?
j /?a
sj (1)
where sij are word-to-word potentials and si and
sj represent English null and foreign null poten-
tials, respectively.
We evaluate our proposed alignments (a)
against hand-annotated alignments, which are
marked with sure (s) and possible (p) alignments.
The alignment error rate (AER) is given by,
AER(a, s,p) = 1? |a ? s|+ |a ? p||a|+ |s|
2.1 1-to-1 Matchings
The class of at most 1-to-1 alignment match-
ings, A1-1, has been considered in several works
(Melamed, 2000; Taskar et al, 2005; Moore et al,
2006). The alignment that maximizes a set of po-
tentials factored as in Equation (1) can be found
in O(n3) time using a bipartite matching algo-
rithm (Kuhn, 1955).1 On the other hand, summing
over A1-1 is #P -hard (Valiant, 1979).
Initially, we consider heuristic alignment poten-
tials given by Dice coefficients
Dice(e, f) = 2CefCe + Cf
where Cef is the joint count of words (e, f) ap-
pearing in aligned sentence pairs, and Ce and Cf
are monolingual unigram counts.
We extracted such counts from 1.1 million
French-English aligned sentence pairs of Hansards
data (see Section 6.1). For each sentence pair in
the Hansards test set, we predicted the alignment
from A1-1 which maximized the sum of Dice po-
tentials. This yielded 30.6 AER.
2.2 Inversion Transduction Grammar
Wu (1997)?s inversion transduction grammar
(ITG) is a synchronous grammar formalism in
which derivations of sentence pairs correspond to
alignments. In its original formulation, there is a
single non-terminal X spanning a bitext cell with
an English and foreign span. There are three rule
types: Terminal unary productions X ? ?e, f?,
where e and f are an aligned English and for-
eign word pair (possibly with one being null);
normal binary rules X ? X(L)X(R), where the
English and foreign spans are constructed from
the children as ?X(L)X(R), X(L)X(R)?; and in-
verted binary rules X ; X(L)X(R), where the
foreign span inverts the order of the children
?X(L)X(R), X(R)X(L)?.2 In general, we will call
a bitext cell a normal cell if it was constructed with
a normal rule and inverted if constructed with an
inverted rule.
Each ITG derivation yields some alignment.
The set of such ITG alignments,AITG, are a strict
subset of A1-1 (Wu, 1997). Thus, we will view
ITG as a constraint on A1-1 which we will ar-
gue is generally beneficial. The maximum scor-
ing alignment from AITG can be found in O(n6)
time with synchronous CFG parsing; in practice,
we can make ITG parsing efficient using a variety
of pruning techniques. One computational advan-
tage of AITG over A1-1 alignments is that sum-
mation overAITG is tractable. The corresponding
1We shall use n throughout to refer to the maximum of
foreign and English sentence lengths.
2The superscripts on non-terminals are added only to in-
dicate correspondence of child symbols.
924
In
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
 ?
 ?
 ??
 ??
 ??
 ??
 ?
 ?
 ??
 ??
 ??
 ??
I
n
d
o
n
e
s
i
a
'
s
p
a
r
l
i
a
m
e
n
t
s
p
e
a
k
e
r
a
r
r
a
i
g
n
e
d
i
n
c
o
u
r
t
(a) Max-Matching Alignment (b) Block ITG Alignment
Figure 1: Best alignments from (a) 1-1 matchings and (b) block ITG (BITG) families respectively. The 1-1
matching is the best possible alignment in the model family, but cannot capture the fact that Indonesia is rendered
as two words in Chinese or that in court is rendered as a single word in Chinese.
dynamic program allows us to utilize likelihood-
based objectives for learning alignment models
(see Section 4).
Using the same heuristic Dice potentials on
the Hansards test set, the maximal scoring align-
ment from AITG yields 28.4 AER?2.4 better
than A1-1 ?indicating that ITG can be beneficial
as a constraint on heuristic alignments.
2.3 Block ITG
An important alignment pattern disallowed by
A1-1 is the many-to-one alignment block. While
not prevalent in our hand-aligned French Hansards
dataset, blocks occur frequently in our hand-
aligned Chinese-English NIST data. Figure 1
contains an example. Extending A1-1 to include
blocks is problematic, because finding a maximal
1-1 matching over phrases is NP-hard (DeNero
and Klein, 2008).
With ITG, it is relatively easy to allow contigu-
ous many-to-one alignment blocks without added
complexity.3 This is accomplished by adding ad-
ditional unary terminal productions aligning a for-
eign phrase to a single English terminal or vice
versa. We will use BITG to refer to this block
ITG variant and ABITG to refer to the alignment
family, which is neither contained in nor contains
A1-1. For this alignment family, we expand the
alignment potential decomposition in Equation (1)
to incorporate block potentials sef and sef which
represent English and foreign many-to-one align-
ment blocks, respectively.
One way to evaluate alignment families is to
3In our experiments we limited the block size to 4.
consider their oracle AER. In the 2002 NIST
Chinese-English hand-aligned data (see Sec-
tion 6.2), we constructed oracle alignment poten-
tials as follows: sij is set to +1 if (i, j) is a sure
or possible alignment in the hand-aligned data, -
1 otherwise. All null potentials (si and sj) are
set to 0. A max-matching under these potentials is
generally a minimal loss alignment in the family.
The oracle AER computed in this was is 10.1 for
A1-1 and 10.2 for AITG. The ABITG alignment
family has an oracle AER of 1.2. These basic ex-
periments show that AITG outperforms A1-1 for
heuristic alignments, and ABITG provide a much
closer fit to true Chinese-English alignments than
A1-1.
3 Margin-Based Training
In this and the next section, we discuss learning
alignment potentials. As input, we have a training
set D = (x1,a?1), . . . , (xn,a?n) of hand-aligned
data, where x refers to a sentence pair. We will as-
sume the score of a alignment is given as a linear
function of a feature vector ?(x,a). We will fur-
ther assume the feature representation of an align-
ment, ?(x,a) decomposes as in Equation (1),
?
(i,j)?a
?ij(x) +
?
i/?a
?i(x) +
?
j /?a
?j(x)
In the framework of loss-augmented margin
learning, we seek a w such that w ? ?(x,a?) is
larger than w ? ?(x,a) + L(a,a?) for all a in an
alignment family, where L(a,a?) is the loss be-
tween a proposed alignment a and the gold align-
ment a?. As in Taskar et al (2005), we utilize a
925
loss that decomposes across alignments. Specif-
ically, for each alignment cell (i, j) which is not
a possible alignment in a?, we incur a loss of 1
when aij 6= a?ij ; note that if (i, j) is a possible
alignment, our loss is indifferent to its presence in
the proposal alignment.
A simple loss-augmented learning pro-
cedure is the margin infused relaxed algo-
rithm (MIRA) (Crammer et al, 2006). MIRA
is an online procedure, where at each time step
t+ 1, we update our weights as follows:
wt+1 = argminw||w ?wt||22 (2)
s.t. w ? ?(x,a?) ? w ? ?(x, a?) + L(a?,a?)
where a? = argmax
a?A
wt ? ?(x,a)
In our data sets, many a? are not in A1-1 (and
thus not in AITG), implying the minimum in-
family loss must exceed 0. Since MIRA oper-
ates in an online fashion, this can cause severe
stability problems. On the Hansards data, the
simple averaging technique described by Collins
(2002) yields a reasonable model. On the Chinese
NIST data, however, where almost no alignment
is in A1-1, the update rule from Equation (2) is
completely unstable, and even the averaged model
does not yield high-quality results.
We instead use a variant of MIRA similar to
Chiang et al (2008). First, rather than update
towards the hand-labeled alignment a?, we up-
date towards an alignment which achieves mini-
mal loss within the family.4 We call this best-
in-class alignment a?p. Second, we perform loss-
augmented inference to obtain a?. This yields the
modified QP,
wt+1 = argminw||w ?wt||22 (3)
s.t. w ? ?(x,a?p) ? w ? ?(x, a?) + L(a,a?p)
where a? = argmax
a?A
wt ? ?(x,a) + ?L(a,a?p)
By setting ? = 0, we recover the MIRA update
from Equation (2). As ? grows, we increase our
preference that a? have high loss (relative to a?p)
rather than high model score. With this change,
MIRA is stable, but still performs suboptimally.
The reason is that initially the score for all align-
ments is low, so we are biased toward only using
very high loss alignments in our constraint. This
slows learning and prevents us from finding a use-
ful weight vector. Instead, in all the experiments
4There might be several alignments which achieve this
minimal loss; we choose arbitrarily among them.
we report here, we begin with ? = 0 and slowly
increase it to ? = 0.5.
4 Likelihood Objective
An alternative to margin-based training is a likeli-
hood objective, which learns a conditional align-
ment distribution Pw(a|x) parametrized as fol-
lows,
logPw(a|x)=w??(x,a)?log
?
a??A
exp(w??(x,a?))
where the log-denominator represents a sum over
the alignment family A. This alignment probabil-
ity only places mass on members ofA. The likeli-
hood objective is given by,
max
w
?
(x,a?)?A
logPw(a?|x)
Optimizing this objective with gradient methods
requires summing over alignments. ForAITG and
ABITG, we can efficiently sum over the set of ITG
derivations inO(n6) time using the inside-outside
algorithm. However, for the ITG grammar pre-
sented in Section 2.2, each alignment has multiple
grammar derivations. In order to correctly sum
over the set of ITG alignments, we need to alter
the grammar to ensure a bijective correspondence
between alignments and derivations.
4.1 ITG Normal Form
There are two ways in which ITG derivations dou-
ble count alignments. First, n-ary productions are
not binarized to remove ambiguity; this results in
an exponential number of derivations for diagonal
alignments. This source of overcounting is con-
sidered and fixed by Wu (1997) and Zens and Ney
(2003), which we briefly review here. The result-
ing grammar, which does not handle null align-
ments, consists of a symbol N to represent a bi-
text cell produced by a normal rule and I for a cell
formed by an inverted rule; alignment terminals
can be either N or I . In order to ensure unique
derivations, we stipulate that a N cell can be con-
structed only from a sequence of smaller inverted
cells I . Binarizing the rule N ? I2+ introduces
the intermediary symbolN (see Figure 2(a)). Sim-
ilarly for inverse cells, we insist an I cell only be
built by an inverted combination of N cells; bina-
rization of I ; N2+ requires the introduction of
the intermediary symbol I (see Figure 2(b)).
Null productions are also a source of double
counting, as there are many possible orders in
926
N ? I2+N ? IN
N ? I
}
N ? IN
I
I
I
N
N
N
(a) Normal Domain Rules
} I ! N2+
I ! NI
I ! NI
I ! N N
N
N
I
I
I
(b) Inverted Domain Rules
N11 ? ??, f?N11
N11 ? N10
N10 ? N10?e, ??
N10 ? N00
}N11 ? ??, f?
?N10
}N10 ? N00?e, ???
}
N00 ? I11N
N ? I11N
N ? I00
N00 ? I+11I00
N00 N10 N10
N11
N
NI11
I11
I00
N00
N11
(c) Normal Domain with Null Rules
}
}
}
I11 ! ??, f?I11
I11 ! I10 I11 ! ??, f?
?I10
I10 ! I10?e, ??
I10 ! I00 I10 ! I00?e, ??
?
I00 ! N+11N00 I
I
N00
N11
N11
I00 ! N11I
I ! N11I
I ! N00
I00
I00 I10 I10
I11
I11
(d) Inverted Domain with Null Rules
Figure 2: Illustration of two unambiguous forms of ITG grammars: In (a) and (b), we illustrate the normal grammar
without nulls (presented in Wu (1997) and Zens and Ney (2003)). In (c) and (d), we present a normal form grammar
that accounts for null alignments.
which to attach null alignments to a bitext cell;
we address this by adapting the grammar to force
a null attachment order. We introduce symbols
N00, N10, and N11 to represent whether a normal
cell has taken no nulls, is accepting foreign nulls,
or is accepting English nulls, respectively. We also
introduce symbols I00, I10, and I11 to represent
inverse cells at analogous stages of taking nulls.
As Figures 2 (c) and (d) illustrate, the directions
in which nulls are attached to normal and inverse
cells differ. The N00 symbol is constructed by
one or more ?complete? inverted cells I11 termi-
nated by a no-null I00. By placing I00 in the lower
right hand corner, we allow the larger N00 to un-
ambiguously attach nulls. N00 transitions to the
N10 symbol and accepts any number of ?e, ?? En-
glish terminal alignments. Then N10 transitions to
N11 and accepts any number of ??, f? foreign ter-
minal alignments. An analogous set of grammar
rules exists for the inverted case (see Figure 2(d)
for an illustration). Given this normal form, we
can efficiently compute model expectations over
ITG alignments without double counting.5 To our
knowledge, the alteration of the normal form to
accommodate null emissions is novel to this work.
5The complete grammar adds sentinel symbols to the up-
per left and lower right, and the root symbol is constrained to
be a N00.
4.2 Relaxing the Single Target Assumption
A crucial obstacle for using the likelihood objec-
tive is that a given a? may not be in the alignment
family. As in our alteration to MIRA (Section 3),
we could replace a? with a minimal loss in-class
alignment a?p. However, in contrast to MIRA, the
likelihood objective will implicitly penalize pro-
posed alignments which have loss equal to a?p. We
opt instead to maximize the probability of the set
of alignmentsM(a?) which achieve the same op-
timal in-class loss. Concretely, let m? be the min-
imal loss achievable relative to a? in A. Then,
M(a?) = {a ? A|L(a,a?) = m?}
When a? is an ITG alignment (i.e., m? is 0),
M(a?) consists only of alignments which have all
the sure alignments in a?, but may have some sub-
set of the possible alignments in a?. See Figure 3
for a specific example where m? = 1.
Our modified likelihood objective is given by,
max
w
?
(x,a?)?D
log ?
a?M(a?)
Pw(a|x)
Note that this objective is no longer convex, as it
involves a logarithm of a summation, however we
still utilize gradient-based optimization. Summing
and obtaining feature expectations over M(a?)
can be done efficiently using a constrained variant
927
MIRA Likelihood
1-1 ITG ITG-S ITG-N
Features P R AER P R AER P R AER P R AER
Dice,dist 85.9 82.6 15.6 86.7 82.9 15.0 89.2 85.2 12.6 87.8 82.6 14.6
+lex,ortho 89.3 86.0 12.2 90.1 86.4 11.5 92.0 90.6 8.6 90.3 88.8 10.4
+joint HMM 95.8 93.8 5.0 96.0 93.2 5.2 95.5 94.2 5.0 95.6 94.0 5.1
Table 1: Results on the French Hansards dataset. Columns indicate models and training methods. The rows
indicate the feature sets used. ITG-S uses the simple grammar (Section 2.2). ITG-N uses the normal form grammar
(Section 4.1). For MIRA (Viterbi inference), the highest-scoring alignment is the same, regardless of grammar.
T
h
a
t
i
s
n
o
t
g
o
o
d
e
n
o
u
g
h
 Se
ne
est
pas
 suffisant
a?Gold Alignment Target AlignmentsM(a?)
Figure 3: Often, the gold alignment a? isn?t in our
alignment family, here ABITG. For the likelihood ob-
jective (Section 4.2), we maximize the probability of
the setM(a?) consisting of alignments ABITG which
achieve minimal loss relative to a?. In this example,
the minimal loss is 1, and we have a choice of remov-
ing either of the sure alignments to the English word
not. We also have the choice of whether to include the
possible alignment, yielding 4 alignments inM(a?).
of the inside-outside algorithm where sure align-
ments not present in a? are disallowed, and the
number of missing sure alignments is appended to
the state of the bitext cell.6
One advantage of the likelihood-based objec-
tive is that we can obtain posteriors over individual
alignment cells,
Pw((i, j)|x) =
?
a?A:(i,j)?a
Pw(a|x)
We obtain posterior ITG alignments by including
all alignment cells (i, j) such that Pw((i, j)|x) ex-
ceeds a fixed threshold t. Posterior thresholding
allows us to easily trade-off precision and recall in
our alignments by raising or lowering t.
5 Dynamic Program Pruning
Both discriminative methods require repeated
model inference: MIRA depends upon loss-
augmented Viterbi parsing, while conditional like-
6Note that alignments that achieve the minimal loss would
not introduce any alignments not either sure or possible, so it
suffices to keep track only of the number of sure recall errors.
lihood uses the inside-outside algorithm for com-
puting cell posteriors. Exhaustive computation
of these quantities requires an O(n6) dynamic
program that is prohibitively slow even on small
supervised training sets. However, most of the
search space can safely be pruned using posterior
predictions from a simpler alignment models. We
use posteriors from two jointly estimated HMM
models to make pruning decisions during ITG in-
ference (Liang et al, 2006). Our first pruning tech-
nique is broadly similar to Cherry and Lin (2007a).
We select high-precision alignment links from the
HMM models: those word pairs that have a pos-
terior greater than 0.9 in either model. Then, we
prune all bitext cells that would invalidate more
than 8 of these high-precision alignments.
Our second pruning technique is to prune all
one-by-one (word-to-word) bitext cells that have a
posterior below 10?4 in both HMM models. Prun-
ing a one-by-one cell also indirectly prunes larger
cells containing it. To take maximal advantage of
this indirect pruning, we avoid explicitly attempt-
ing to build each cell in the dynamic program. In-
stead, we track bounds on the spans for which we
have successfully built ITG cells, and we only iter-
ate over larger spans that fall within those bounds.
The details of a similar bounding approach appear
in DeNero et al (2009).
In all, pruning reduces MIRA iteration time
from 175 to 5 minutes on the NIST Chinese-
English dataset with negligible performance loss.
Likelihood training time is reduced by nearly two
orders of magnitude.
6 Alignment Quality Experiments
We present results which measure the quality of
our models on two hand-aligned data sets. Our
first is the English-French Hansards data set from
the 2003 NAACL shared task (Mihalcea and Ped-
ersen, 2003). Here we use the same 337/100
train/test split of the labeled data as Taskar et al
928
MIRA Likelihood
1-1 ITG BITG BITG-S BITG-N
Features P R AER P R AER P R AER P R AER P R AER
Dice, dist,
blcks, dict, lex 85.7 63.7 26.8 86.2 65.8 25.2 85.0 73.3 21.1 85.7 73.7 20.6 85.3 74.8 20.1
+HMM 90.5 69.4 21.2 91.2 70.1 20.3 90.2 80.1 15.0 87.3 82.8 14.9 88.2 83.0 14.4
Table 2: Word alignment results on Chinese-English. Each column is a learning objective paired with an alignment
family. The first row represents our best model without external alignment models and the second row includes
features from the jointly trained HMM. Under likelihood, BITG-S uses the simple grammar (Section 2.2). BITG-N
uses the normal form grammar (Section 4.1).
(2005); we compute external features from the
same unlabeled data, 1.1 million sentence pairs.
Our second is the Chinese-English hand-aligned
portion of the 2002 NIST MT evaluation set. This
dataset has 491 sentences, which we split into a
training set of 150 and a test set of 191. When we
trained external Chinese models, we used the same
unlabeled data set as DeNero and Klein (2007), in-
cluding the bilingual dictionary.
For likelihood based models, we set the L2 reg-
ularization parameter, ?2, to 100 and the thresh-
old for posterior decoding to 0.33. We report re-
sults using the simple ITG grammar (ITG-S, Sec-
tion 2.2) where summing over derivations dou-
ble counts alignments, as well as the normal form
ITG grammar (ITG-N,Section 4.1) which does
not double count. We ran our annealed loss-
augmented MIRA for 15 iterations, beginning
with ? at 0 and increasing it linearly to 0.5. We
compute Viterbi alignments using the averaged
weight vector from this procedure.
6.1 French Hansards Results
The French Hansards data are well-studied data
sets for discriminative word alignment (Taskar et
al., 2005; Cherry and Lin, 2006; Lacoste-Julien
et al, 2006). For this data set, it is not clear
that improving alignment error rate beyond that of
GIZA++ is useful for translation (Ganchev et al,
2008). Table 1 illustrates results for the Hansards
data set. The first row uses dice and the same dis-
tance features as Taskar et al (2005). The first
two rows repeat the experiments of Taskar et al
(2005) and Cherry and Lin (2006), but adding ITG
models that are trained to maximize conditional
likelihood. The last row includes the posterior of
the jointly-trained HMM of Liang et al (2006)
as a feature. This model alone achieves an AER
of 5.4. No model significantly improves over the
HMM alone, which is consistent with the results
of Taskar et al (2005).
6.2 Chinese NIST Results
Chinese-English alignment is a much harder task
than French-English alignment. For example, the
HMM aligner achieves an AER of 20.7 when us-
ing the competitive thresholding heuristic of DeN-
ero and Klein (2007). On this data set, our block
ITG models make substantial performance im-
provements over the HMM, and moreover these
results do translate into downstream improve-
ments in BLEU score for the Chinese-English lan-
guage pair. Because of this, we will briefly de-
scribe the features used for these models in de-
tail. For features on one-by-one cells, we con-
sider Dice, the distance features from (Taskar et
al., 2005), dictionary features, and features for the
50 most frequent lexical pairs. We also trained an
HMM aligner as described in DeNero and Klein
(2007) and used the posteriors of this model as fea-
tures. The first two columns of Table 2 illustrate
these features for ITG and one-to-one matchings.
For our block ITG models, we include all of
these features, along with variants designed for
many-to-one blocks. For example, we include the
average Dice of all the cells in a block. In addi-
tion, we also created three new block-specific fea-
tures types. The first type comprises bias features
for each block length. The second type comprises
features computed from N-gram statistics gathered
from a large monolingual corpus. These include
features such as the number of occurrences of the
phrasal (multi-word) side of a many-to-one block,
as well as pointwise mutual information statistics
for the multi-word parts of many-to-one blocks.
These features capture roughly how ?coherent? the
multi-word side of a block is.
The final block feature type consists of phrase
shape features. These are designed as follows: For
each word in a potential many-to-one block align-
ment, we map an individual word to X if it is not
one of the 25 most frequent words. Some example
features of this type are,
929
? English Block: [the X, X], [in X of, X]
? Chinese Block: [ X, X] [X|, X]
For English blocks, for example, these features
capture the behavior of phrases such as in spite
of or in front of that are rendered as one word in
Chinese. For Chinese blocks, these features cap-
ture the behavior of phrases containing classifier
phrases like? orP, which are rendered as
English indefinite determiners.
The right-hand three columns in Table 2 present
supervised results on our Chinese English data set
using block features. We note that almost all of
our performance gains (relative to both the HMM
and 1-1 matchings) come from BITG and block
features. The maximum likelihood-trained nor-
mal form ITG model outperforms the HMM, even
without including any features derived from the
unlabeled data. Once we include the posteriors
of the HMM as a feature, the AER decreases to
14.4. The previous best AER result on this data set
is 15.9 from Ayan and Dorr (2006), who trained
stacked neural networks based on GIZA++ align-
ments. Our results are not directly comparable
(they used more labeled data, but did not have the
HMM posteriors as an input feature).
6.3 End-To-End MT Experiments
We further evaluated our alignments in an end-to-
end Chinese to English translation task using the
publicly available hierarchical pipeline JosHUa
(Li and Khudanpur, 2008). The pipeline extracts
a Hiero-style synchronous context-free grammar
(Chiang, 2007), employs suffix-array based rule
extraction (Lopez, 2007), and tunes model pa-
rameters with minimum error rate training (Och,
2003). We trained on the FBIS corpus using sen-
tences up to length 40, which includes 2.7 million
English words. We used a 5-gram language model
trained on 126 million words of the Xinhua section
of the English Gigaword corpus, estimated with
SRILM (Stolcke, 2002). We tuned on 300 sen-
tences of the NIST MT04 test set.
Results on the NIST MT05 test set appear in
Table 3. We compared four sets of alignments.
The GIZA++ alignments7 are combined across di-
rections with the grow-diag-final heuristic, which
outperformed the union. The joint HMM align-
ments are generated from competitive posterior
7We used a standard training regimen: 5 iterations of
model 1, 5 iterations of HMM, 3 iterations of Model 3, and 3
iterations of Model 4.
Alignments Translations
Model Prec Rec Rules BLEU
GIZA++ 62 84 1.9M 23.22
Joint HMM 79 77 4.0M 23.05
Viterbi ITG 90 80 3.8M 24.28
Posterior ITG 81 83 4.2M 24.32
Table 3: Results on the NIST MT05 Chinese-English
test set show that our ITG alignments yield improve-
ments in translation quality.
thresholding (DeNero and Klein, 2007). The ITG
Viterbi alignments are the Viterbi output of the
ITG model with all features, trained to maximize
log likelihood. The ITG Posterior alignments
result from applying competitive thresholding to
alignment posteriors under the ITG model. Our
supervised ITG model gave a 1.1 BLEU increase
over GIZA++.
7 Conclusion
This work presented the first large-scale applica-
tion of ITG to discriminative word alignment. We
empirically investigated the performance of con-
ditional likelihood training of ITG word aligners
under simple and normal form grammars. We
showed that through the combination of relaxed
learning objectives, many-to-one block alignment
potential, and efficient pruning, ITG models can
yield state-of-the art word alignments, even when
the underlying gold alignments are highly non-
ITG. Our models yielded the lowest published er-
ror for Chinese-English alignment and an increase
in downstream translation performance.
References
Necip Fazil Ayan and Bonnie Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In ACL.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In ACL.
Colin Cherry and Dekang Lin. 2007a. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In NAACL-HLT 2007.
Colin Cherry and Dekang Lin. 2007b. A scalable in-
version transduction grammar for joint phrasal trans-
lation modeling. In SSST Workshop at ACL.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In EMNLP.
930
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In EMNLP.
Koby Crammer, Ofer Dekel, Shai S. Shwartz, and
Yoram Singer. 2006. Online passive-aggressive al-
gorithms. Journal of Machine Learning Research.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In ACL.
John DeNero and Dan Klein. 2008. The complexity
of phrase alignment problems. In ACL Short Paper
Track.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In NAACL.
Kuzman Ganchev, Joao Graca, and Ben Taskar. 2008.
Better alignments = better translations? In ACL.
H. W. Kuhn. 1955. The Hungarian method for the as-
signment problem. Naval Research Logistic Quar-
terly.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael Jordan. 2006. Word alignment via
quadratic assignment. In NAACL.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In
SSST Workshop at ACL.
Percy Liang, Dan Klein, and Dan Klein. 2006. Align-
ment by agreement. In NAACL-HLT.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In EMNLP.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Rada Mihalcea and Ted Pedersen. 2003. An evalua-
tion exercise for word alignment. In HLT/NAACL
Workshop on Building and Using Parallel Texts.
Robert C. Moore, Wen tau Yih, and Andreas Bode.
2006. Improved discriminative bilingual word
alignment. In ACL-COLING.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL.
Slav Petrov, Aria Haghighi, and Dan Klein. 2008.
Coarse-to-fine syntactic machine translation using
language projections. In Empirical Methods in Nat-
ural Language Processing.
Andreas Stolcke. 2002. Srilm: An extensible language
modeling toolkit. In ICSLP 2002.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In NAACL-HLT.
L. G. Valiant. 1979. The complexity of computing the
permanent. Theoretical Computer Science, 8:189?
201.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23.
Richard Zens and Hermann Ney. 2003. A comparative
study on reordering constraints in statistical machine
translation. In ACL.
Hao Zhang and Dan Gildea. 2005. Stochastic lexical-
ized inversion transduction grammar for alignment.
In ACL.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
ACL.
931
Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 141?144,
Suntec, Singapore, 4 August 2009.
c?2009 ACL and AFNLP
Asynchronous Binarization for Synchronous Grammars
John DeNero, Adam Pauls, and Dan Klein
Computer Science Division
University of California, Berkeley
{denero, adpauls, klein}@cs.berkeley.edu
Abstract
Binarization of n-ary rules is critical for the effi-
ciency of syntactic machine translation decoding.
Because the target side of a rule will generally
reorder the source side, it is complex (and some-
times impossible) to find synchronous rule bina-
rizations. However, we show that synchronous
binarizations are not necessary in a two-stage de-
coder. Instead, the grammar can be binarized one
way for the parsing stage, then rebinarized in a
different way for the reranking stage. Each indi-
vidual binarization considers only one monolin-
gual projection of the grammar, entirely avoid-
ing the constraints of synchronous binarization
and allowing binarizations that are separately op-
timized for each stage. Compared to n-ary for-
est reranking, even simple target-side binariza-
tion schemes improve overall decoding accuracy.
1 Introduction
Syntactic machine translation decoders search
over a space of synchronous derivations, scoring
them according to both a weighted synchronous
grammar and an n-gram language model. The
rewrites of the synchronous translation gram-
mar are typically flat, n-ary rules. Past work
has synchronously binarized such rules for effi-
ciency (Zhang et al, 2006; Huang et al, 2008).
Unfortunately, because source and target orders
differ, synchronous binarizations can be highly
constrained and sometimes impossible to find.
Recent work has explored two-stage decoding,
which explicitly decouples decoding into a source
parsing stage and a target language model inte-
gration stage (Huang and Chiang, 2007). Be-
cause translation grammars continue to increase
in size and complexity, both decoding stages re-
quire efficient approaches (DeNero et al, 2009).
In this paper, we show how two-stage decoding
enables independent binarizations for each stage.
The source-side binarization guarantees cubic-
time construction of a derivation forest, while an
entirely different target-side binarization leads to
efficient forest reranking with a language model.
Binarizing a synchronous grammar twice inde-
pendently has two principal advantages over syn-
chronous binarization. First, each binarization can
be fully tailored to its decoding stage, optimiz-
ing the efficiency of both parsing and language
model reranking. Second, the ITG constraint on
non-terminal reordering patterns is circumvented,
allowing the efficient application of synchronous
rules that do not have a synchronous binarization.
The primary contribution of this paper is to es-
tablish that binarization of synchronous grammars
need not be constrained by cross-lingual reorder-
ing patterns. We also demonstrate that even sim-
ple target-side binarization schemes improve the
search accuracy of forest reranking with a lan-
guage model, relative to n-ary forest reranking.
2 Asynchronous Binarization
Two-stage decoding consists of parsing and lan-
guage model integration. The parsing stage builds
a pruned forest of derivations scored by the trans-
lation grammar only. In the second stage, this for-
est is reranked by an n-gram language model. We
rerank derivations with cube growing, a lazy beam
search algorithm (Huang and Chiang, 2007).
In this paper, we focus on syntactic translation
with tree-transducer rules (Galley et al, 2006).
These synchronous rules allow multiple adjacent
non-terminals and place no restrictions on rule size
or lexicalization. Two example unlexicalized rules
appear in Figure 1, along with aligned and parsed
training sentences that would have licensed them.
2.1 Constructing Translation Forests
The parsing stage builds a forest of derivations by
parsing with the source-side projection of the syn-
chronous grammar. Each forest node P
ij
com-
pactly encodes all parse derivations rooted by
grammar symbol P and spanning the source sen-
tence from positions i to j. Each derivation of P
ij
is rooted by a rule with non-terminals that each
141
?PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
   PP
4
   NN
2
S ?
yo    ayer    com?    en casa
I       ate     at home   yesterday 
PRP  VBD       PP           NN
S
(a)
(b)
PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
   PP
4
   NN
2
S ?
yo    ayer    com?    en casa
I       ate     at home   yesterday 
PRP  VBD       PP           NN
S
yo    ayer    com?    en casa
I       ate   yesterday   at home 
PRP  VBD       NN           PP
S
PRP
1
   NN
2    
VBD
3
   PP
4
PRP
1
   VBD
3
  NN
2
   PP
4
S ?
Figure 1: Two unlexicalized transducer rules (top) and
aligned, parsed training sentences from which they could be
extracted (bottom). The internal structure of English parses
has been omitted, as it is irrelevant to our decoding problem.
anchor to some child nodeC
(t)
k`
, where the symbol
C
(t)
is the tth child in the source side of the rule,
and i ? k < ` ? j.
We build this forest with a CKY-style algorithm.
For each span (i, j) from small to large, and each
symbol P , we iterate over all ways of building a
node P
ij
, first considering all grammar rules with
parent symbol P and then, for each rule, consider-
ing all ways of anchoring its non-terminals to ex-
isting forest nodes. Because we do not incorporate
a language model in this stage, we need only oper-
ate over the source-side projection of the grammar.
Of course, the number of possible anchorings
for a rule is exponential in the number of non-
terminals it contains. The purpose of binarization
during the parsing pass is to make this exponential
algorithm polynomial by reducing rule branching
to at most two non-terminals. Binarization reduces
algorithmic complexity by eliminating redundant
work: the shared substructures of n-ary rules are
scored only once, cached, and reused. Caching is
also commonplace in Early-style parsers that im-
plicitly binarize when applying n-ary rules.
While any binarization of the source side will
give a cubic-time algorithm, the particulars of a
grammar transformation can affect parsing speed
substantially. For instance, DeNero et al (2009)
describe normal forms particularly suited to trans-
ducer grammars, demonstrating that well-chosen
binarizations admit cubic-time parsing algorithms
while introducing very few intermediate grammar
symbols. Binarization choice can also improve
monolingual parsing efficiency (Song et al, 2008).
The parsing stage of our decoder proceeds
by first converting the source-side projection of
the translation grammar into lexical normal form
(DeNero et al, 2009), which allows each rule to
be applied to any span in linear time, then build-
ing a binary-branching translation forest, as shown
in Figure 2(a). The intermediate nodes introduced
during this transformation do not have a target-
side projection or interpretation. They only exist
for the sake of source-side parsing efficiency.
2.2 Collapsing Binarization
To facilitate a change in binarization, we transform
the translation forest into n-ary form. In the n-ary
forest, each hyperedge corresponds to an original
grammar rule, and all nodes correspond to original
grammar symbols, rather than those introduced
during binarizaiton. Transforming the entire for-
est to n-ary form is intractable, however, because
the number of hyperedges would be exponential in
n. Instead, we include only the top k n-ary back-
traces for each forest node. These backtraces can
be enumerated efficiently from the binary forest.
Figure 2(b) illustrates the result.
For efficiency, we follow DeNero et al (2009)
in pruning low-scoring nodes in the n-ary for-
est under the weighted translation grammar. We
use a max-marginal threshold to prune unlikely
nodes, which can be computed through a max-
sum semiring variant of inside-outside (Goodman,
1996; Petrov and Klein, 2007).
Forest reranking with a language model can be
performed over this n-ary forest using the cube
growing algorithm of Huang and Chiang (2007).
Cube growing lazily builds k-best lists of deriva-
tions at each node in the forest by filling a node-
specific priority queue upon request from the par-
ent. N -ary forest reranking serves as our baseline.
2.3 Reranking with Target-Side Binarization
Zhang et al (2006) demonstrate that reranking
over binarized derivations improves search accu-
racy by better exploring the space of translations
within the strict confines of beam search. Binariz-
ing the forest during reranking permits pairs of ad-
jacent non-terminals in the target-side projection
of rules to be rescored at intermediate forest nodes.
This target-side binarization can be performed on-
the-fly: when a node P
ij
is queried for its k-best
list, we binarize its n-ary backtraces.
Suppose P
ij
can be constructed from a rule r
with target-side projection
P ? `
0
C
1
`
1
C
2
`
2
. . . C
n
`
n
where C
1
, . . . , C
n
are non-terminal symbols that
are each anchored to a nodeC
(i)
kl
in the forest, and
`
i
are (possibly empty) sequences of lexical items.
142
yo ayer com? en casa
S
PRP+NN+VBD
PRP+NN
PRP NN VBD PP
yo ayer com? en casa
S
PRP NN VBD PP
yo ayer com? en casa
S
PRP NN VBD PP
PRP+VBD+NN
PRP+VBD
?I ate?
[[PRP
1
     NN
2
]
     
VBD
3
]  PP
4
PRP
1
     VBD
3
     NN
2
    PP
4
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
PRP
1
     VBD
3
     NN
2
    PP
4
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
[[PRP
1
     VBD
3
]    NN
2
]  PP
4
S ?
[[PRP
1
     NN
2
]
     
VBD
3
]  PP
4
PRP
1
     VBD
3
    PP
4
     NN
2
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
PRP
1
     VBD
3
    PP
4
     NN
2
S ?
PRP
1
     NN
2      
VBD
3
     PP
4
[[PRP
1
     VBD
3
]    PP
4
]  NN
2
S ?
(a) Parsing stage binarization (b) Collapsed n-ary forest (c) Reranking stage binarization
PRP+VBD+PP
Figure 2: A translation forest as it evolves during two-stage decoding, along with two n-ary rules in the forest that are rebi-
narized. (a) A source-binarized forest constructed while parsing the source sentence with the translation grammar. (b) A flat
n-ary forest constructed by collapsing out the source-side binarization. (c) A target-binarized forest containing two derivations
of the root symbol?the second is dashed for clarity. Both derivations share the node PRP+VBD, which will contain a single
k-best list of translations during language model reranking. One such translation of PRP+VBD is shown: ?I ate?.
We apply a simple left-branching binarization to
r, though in principle any binarization is possible.
We construct a new symbol B and two new rules:
r
1
: B ? `
0
C
1
`
1
C
2
`
2
r
2
: P ? B C
3
`
3
. . . C
n
`
n
These rules are also anchored to forest nodes. Any
C
i
remains anchored to the same node as it was in
the n-ary forest. For the new symbol B, we intro-
duce a new forest nodeB that does not correspond
to any particular span of the source sentence. We
likewise transform the resulting r
2
until all rules
have at most two non-terminal items. The original
rule r from the n-ary forest is replaced by binary
rules. Figure 2(c) illustrates the rebinarized forest.
Language model reranking treats the newly in-
troduced forest nodeB as any other node: building
a k-best derivation list by combining derivations
from C
(1)
and C
(2)
using rule r
1
. These deriva-
tions are made available to the parent of B, which
may be another introduced node (if more binariza-
tion were required) or the original root P
ij
.
Crucially, the ordering of non-terminals in the
source-side projection of r does not play a role
in this binarization process. The intermediate
nodes B may comprise translations of discontigu-
ous parts of the source sentence, as long as those
parts are contained within the span (i, j).
2.4 Reusing Intermediate Nodes
The binarization we describe transforms the for-
est on a rule-by-rule basis. We must consider in-
dividual rules because they may contain different
lexical items and non-terminal orderings. How-
ever, two different rules that can build a node often
share some substructures. For instance, the two
rules in Figure 2 both begin with PRP followed by
VBD. In addition, these symbols are anchored to
the same source-side spans. Thus, binarizing both
rules yields the same intermediate forest node B.
In the case where two intermediate nodes share
the same intermediate rule anchored to the same
forest nodes, they can be shared. That is, we need
only generate one k-best list of derivations, then
use it in derivations rooted by both rules. Sharing
derivation lists in this way provides an additional
advantage of binarization over n-ary forest rerank-
ing. Not only do we assess language model penal-
ties over smaller partial derivations, but repeated
language model evaluations are cached and reused
across rules with common substructure.
3 Experiments
The utility of binarization for parsing is well
known, and plays an important role in the effi-
ciency of the parsing stage of decoding (DeNero et
al., 2009). The benefit of binarization for language
143
Forest Reranked BLEU Model Score
N -ary baseline 58.2 41,543
Left-branching binary 58.5 41,556
Table 1: Reranking a binarized forest improves BLEU by 0.3
and model score by 13 relative to an n-ary forest baseline by
reducing search errors during forest rescoring.
model reranking has also been established, both
for synchronous binarization (Zhang et al, 2006)
and for target-only binarization (Huang, 2007). In
our experiment, we evaluate the benefit of target-
side forest re-binarization in the two-stage decoder
of DeNero et al (2009), relative to reranking n-ary
forests directly.
We translated 300 NIST 2005 Arabic sentences
to English with a large grammar learned from a
220 million word bitext, using rules with up to 6
non-terminals. We used a trigram language model
trained on the English side of this bitext. Model
parameters were tuned withMERT. Beam size was
limited to 200 derivations per forest node.
Table 1 shows a modest increase in model
and BLEU score from left-branching binarization
during language model reranking. We used the
same pruned n-ary forest from an identical parsing
stage in both conditions. Binarization did increase
reranking time by 25% because more k-best lists
are constructed. However, reusing intermediate
edges during reranking binarization reduced bina-
rized reranking time by 37%. We found that on
average, intermediate nodes introduced in the for-
est are used in 4.5 different rules, which accounts
for the speed increase.
4 Discussion
Asynchronous binarization in two-stage decoding
allows us to select an appropriate grammar trans-
formation for each language. The source trans-
formation can optimize specifically for the parsing
stage of translation, while the target-side binariza-
tion can optimize for the reranking stage.
Synchronous binarization is of course a way to
get the benefits of binarizing both grammar pro-
jections; it is a special case of asynchronous bi-
narization. However, synchronous binarization is
constrained by the non-terminal reordering, lim-
iting the possible binarization options. For in-
stance, none of the binarization choices used in
Figure 2 on either side would be possible in a
synchronous binarization. There are rules, though
rare, that cannot be binarized synchronously at all
(Wu, 1997), but can be incorporated in two-stage
decoding with asynchronous binarization.
On the source side, these limited binarization
options may, for example, prevent a binarization
that minimizes intermediate symbols (DeNero et
al., 2009). On the target side, the speed of for-
est reranking depends upon the degree of reuse
of intermediate k-best lists, which in turn depends
upon the manner in which the target-side grammar
projection is binarized. Limiting options may pre-
vent a binarization that allows intermediate nodes
to be maximally reused. In future work, we look
forward to evaluating the wide array of forest bi-
narization strategies that are enabled by our asyn-
chronous approach.
References
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein.
2009. Efficient parsing for transducer grammars. In Pro-
ceedings of the Annual Conference of the North American
Association for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.
2006. Scalable inference and training of context-rich syn-
tactic translation models. In Proceedings of the Annual
Conference of the Association for Computational Linguis-
tics.
Joshua Goodman. 1996. Parsing algorithms and metrics. In
Proceedings of the Annual Meeting of the Association for
Computational Linguistics.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In Pro-
ceedings of the Annual Conference of the Association for
Computational Linguistics.
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin Knight.
2008. Binarization of synchronous context-free gram-
mars. Computational Linguistics.
Liang Huang. 2007. Binarization, synchronous binarization,
and target-side binarization. In Proceedings of the HLT-
NAACL Workshop on Syntax and Structure in Statistical
Translation (SSST).
Slav Petrov and Dan Klein. 2007. Improved inference for un-
lexicalized parsing. In Proceedings of the North American
Chapter of the Association for Computational Linguistics.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008. Better
binarization for the CKY parsing. In Proceedings of the
Conference on Empirical Methods in Natural Language
Processing.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377?404.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight.
2006. Synchronous binarization for machine translation.
In Proceedings of the North American Chapter of the As-
sociation for Computational Linguistics.
144
Proceedings of the Workshop on Statistical Machine Translation, pages 31?38,
New York City, June 2006. c?2006 Association for Computational Linguistics
Why Generative Phrase Models Underperform Surface Heuristics
John DeNero, Dan Gillick, James Zhang, Dan Klein
Department of Electrical Engineering and Computer Science
University of California, Berkeley
Berkeley, CA 94705
{denero, dgillick, jyzhang, klein}@eecs.berkeley.edu
Abstract
We investigate why weights from generative mod-
els underperform heuristic estimates in phrase-
based machine translation. We first propose a sim-
ple generative, phrase-based model and verify that
its estimates are inferior to those given by surface
statistics. The performance gap stems primarily
from the addition of a hidden segmentation vari-
able, which increases the capacity for overfitting
during maximum likelihood training with EM. In
particular, while word level models benefit greatly
from re-estimation, phrase-level models do not: the
crucial difference is that distinct word alignments
cannot all be correct, while distinct segmentations
can. Alternate segmentations rather than alternate
alignments compete, resulting in increased deter-
minization of the phrase table, decreased general-
ization, and decreased final BLEU score. We also
show that interpolation of the two methods can re-
sult in a modest increase in BLEU score.
1 Introduction
At the core of a phrase-based statistical machine
translation system is a phrase table containing
pairs of source and target language phrases, each
weighted by a conditional translation probability.
Koehn et al (2003a) showed that translation qual-
ity is very sensitive to how this table is extracted
from the training data. One particularly surprising
result is that a simple heuristic extraction algorithm
based on surface statistics of a word-aligned training
set outperformed the phrase-based generative model
proposed by Marcu and Wong (2002).
This result is surprising in light of the reverse sit-
uation for word-based statistical translation. Specif-
ically, in the task of word alignment, heuristic ap-
proaches such as the Dice coefficient consistently
underperform their re-estimated counterparts, such
as the IBM word alignment models (Brown et al,
1993). This well-known result is unsurprising: re-
estimation introduces an element of competition into
the learning process. The key virtue of competition
in word alignment is that, to a first approximation,
only one source word should generate each target
word. If a good alignment for a word token is found,
other plausible alignments are explained away and
should be discounted as incorrect for that token.
As we show in this paper, this effect does not pre-
vail for phrase-level alignments. The central differ-
ence is that phrase-based models, such as the ones
presented in section 2 or Marcu and Wong (2002),
contain an element of segmentation. That is, they do
not merely learn correspondences between phrases,
but also segmentations of the source and target sen-
tences. However, while it is reasonable to sup-
pose that if one alignment is right, others must be
wrong, the situation is more complex for segmenta-
tions. For example, if one segmentation subsumes
another, they are not necessarily incompatible: both
may be equally valid. While in some cases, such
as idiomatic vs. literal translations, two segmenta-
tions may be in true competition, we show that the
most common result is for different segmentations
to be recruited for different examples, overfitting the
training data and overly determinizing the phrase
translation estimates.
In this work, we first define a novel (but not rad-
ical) generative phrase-based model analogous to
IBM Model 3. While its exact training is intractable,
we describe a training regime which uses word-
level alignments to constrain the space of feasible
segmentations down to a manageable number. We
demonstrate that the phrase analogue of the Dice co-
efficient is superior to our generative model (a re-
sult also echoing previous work). In the primary
contribution of the paper, we present a series of ex-
periments designed to elucidate what re-estimation
learns in this context. We show that estimates are
overly determinized because segmentations are used
31
in unintuitive ways for the sake of data likelihood.
We comment on both the beneficial instances of seg-
ment competition (idioms) as well as the harmful
ones (most everything else). Finally, we demon-
strate that interpolation of the two estimates can
provide a modest increase in BLEU score over the
heuristic baseline.
2 Approach and Evaluation Methodology
The generative model defined below is evaluated
based on the BLEU score it produces in an end-
to-end machine translation system from English to
French. The top-performing diag-and extraction
heuristic (Zens et al, 2002) serves as the baseline for
evaluation.1 Each approach ? the generative model
and heuristic baseline ? produces an estimated con-
ditional distribution of English phrases given French
phrases. We will refer to the distribution derived
from the baseline heuristic as ?H . The distribution
learned via the generative model, denoted ?EM , is
described in detail below.
2.1 A Generative Phrase Model
While our model for computing ?EM is novel, it
is meant to exemplify a class of models that are
not only clear extensions to generative word align-
ment models, but also compatible with the statistical
framework assumed during phrase-based decoding.
The generative process we modeled produces a
phrase-aligned English sentence from a French sen-
tence where the former is a translation of the lat-
ter. Note that this generative process is opposite to
the translation direction of the larger system because
of the standard noisy-channel decomposition. The
learned parameters from this model will be used to
translate sentences from English to French. The gen-
erative process modeled has four steps:2
1. Begin with a French sentence f.
1This well-known heuristic extracts phrases from a sentence
pair by computing a word-level alignment for the sentence and
then enumerating all phrases compatible with that alignment.
The word alignment is computed by first intersecting the direc-
tional alignments produced by a generative IBM model (e.g.,
model 4 with minor enhancements) in each translation direc-
tion, then adding certain alignments from the union of the di-
rectional alignments based on local growth rules.
2Our notation matches the literature for phrase-based trans-
lation: e is an English word, e? is an English phrase, and e?I1 is a
sequence of I English phrases, and e is an English sentence.
2. Segment f into a sequence of I multi-word
phrases that span the sentence, f? I1 .
3. For each phrase f?i ? f? I1 , choose a correspond-
ing position j in the English sentence and es-
tablish the alignment aj = i, then generate ex-
actly one English phrase e?j from f?i.
4. The sequence e?j ordered by a describes an En-
glish sentence e.
The corresponding probabilistic model for this gen-
erative process is:
P (e|f) =
?
f?I1 ,e?
I
1,a
P (e, f? I1 , e?
I
1, a|f)
=
?
f?I1 ,e?
I
1,a
?(f? I1 |f)
?
f?i?f?I1
?(e?j |f?i)d(aj = i|f)
where P (e, f? I1 , e?
I
1, a|f) factors into a segmentation
model ?, a translation model ? and a distortion
model d. The parameters for each component of this
model are estimated differently:
? The segmentation model ?(f? I1 |f) is assumed to
be uniform over all possible segmentations for
a sentence.3
? The phrase translation model ?(e?j |f?i) is pa-
rameterized by a large table of phrase transla-
tion probabilities.
? The distortion model d(aj = i|f) is a discount-
ing function based on absolute sentence posi-
tion akin to the one used in IBM model 3.
While similar to the joint model in Marcu and Wong
(2002), our model takes a conditional form com-
patible with the statistical assumptions used by the
Pharaoh decoder. Thus, after training, the param-
eters of the phrase translation model ?EM can be
used directly for decoding.
2.2 Training
Significant approximation and pruning is required
to train a generative phrase model and table ? such
as ?EM ? with hidden segmentation and alignment
variables using the expectation maximization algo-
rithm (EM). Computing the likelihood of the data
3This segmentation model is deficient given a maximum
phrase length: many segmentations are disallowed in practice.
32
for a set of parameters (the e-step) involves summing
over exponentially many possible segmentations for
each training sentence. Unlike previous attempts to
train a similar model (Marcu and Wong, 2002), we
allow information from a word-alignment model to
inform our approximation. This approach allowed
us to directly estimate translation probabilities even
for rare phrase pairs, which were estimated heuristi-
cally in previous work.
In each iteration of EM, we re-estimate each
phrase translation probability by summing fractional
phrase counts (soft counts) from the data given the
current model parameters.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i)
=
?
(f,e)
?
f?I1 :f?i?f?
I
1
?
e?I1:e?j?e?
I
1
?
a:aj=i P (e, f?
I
1 , e?
I
1, a|f)
?
f?I1 :f?i?f?
I
1
?
e?I1
?
a P (e, f?
I
1 , e?
I
1, a|f)
This training loop necessitates approximation be-
cause summing over all possible segmentations and
alignments for each sentence is intractable, requiring
time exponential in the length of the sentences. Ad-
ditionally, the set of possible phrase pairs grows too
large to fit in memory. Using word alignments, we
can address both problems.4 In particular, we can
determine for any aligned segmentation (f? I1 , e?
I
1, a)
whether it is compatible with the word-level align-
ment for the sentence pair. We define a phrase pair
to be compatible with a word-alignment if no word
in either phrase is aligned with a word outside the
other phrase (Zens et al, 2002). Then, (f? I1 , e?
I
1, a)
is compatible with the word-alignment if each of its
aligned phrases is a compatible phrase pair.
The training process is then constrained such that,
when evaluating the above sum, only compatible
aligned segmentations are considered. That is, we
allow P (e, f? I1 , e?
I
1, a|f) > 0 only for aligned seg-
mentations (f? I1 , e?
I
1, a) such that a provides a one-
to-one mapping from f? I1 to e?
I
1 where all phrase pairs
(f?aj , e?j) are compatible with the word alignment.
This constraint has two important effects. First,
we force P (e?j |f?i) = 0 for all phrase pairs not com-
patible with the word-level alignment for some sen-
tence pair. This restriction successfully reduced the
4The word alignments used in approximating the e-step
were the same as those used to create the heuristic diag-and
baseline.
total legal phrase pair types from approximately 250
million to 17 million for 100,000 training sentences.
However, some desirable phrases were eliminated
because of errors in the word alignments.
Second, the time to compute the e-step is reduced.
While in principle it is still intractable, in practice
we can compute most sentence pairs? contributions
in under a second each. However, some spurious
word alignments can disallow all segmentations for
a sentence pair, rendering it unusable for training.
Several factors including errors in the word-level
alignments, sparse word alignments and non-literal
translations cause our constraint to rule out approx-
imately 54% of the training set. Thus, the reduced
size of the usable training set accounts for some of
the degraded performance of ?EM relative to ?H .
However, the results in figure 1 of the following sec-
tion show that ?EM trained on twice as much data
as ?H still underperforms the heuristic, indicating a
larger issue than decreased training set size.
2.3 Experimental Design
To test the relative performance of ?EM and ?H ,
we evaluated each using an end-to-end translation
system from English to French. We chose this non-
standard translation direction so that the examples
in this paper would be more accessible to a primar-
ily English-speaking audience. All training and test
data were drawn from the French/English section of
the Europarl sentence-aligned corpus. We tested on
the first 1,000 unique sentences of length 5 to 15 in
the corpus and trained on sentences of length 1 to 60
starting after the first 10,000.
The system follows the structure proposed in
the documentation for the Pharaoh decoder and
uses many publicly available components (Koehn,
2003b). The language model was generated from
the Europarl corpus using the SRI Language Model-
ing Toolkit (Stolcke, 2002). Pharaoh performed de-
coding using a set of default parameters for weight-
ing the relative influence of the language, translation
and distortion models (Koehn, 2003b). A maximum
phrase length of three was used for all experiments.
To properly compare ?EM to ?H , all aspects of
the translation pipeline were held constant except for
the parameters of the phrase translation table. In par-
ticular, we did not tune the decoding hyperparame-
ters for the different phrase tables.
33
Source 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743Iteration 2 0.3735 0.3851 0.3814iteration 3 0.3705 0.384 0.3827Iteration 4 0.3695 0.285 0.3801iteration 5 0.3705 0.284 0.3774interpSource 25k 50k 100kHeuristic 0.3853 0.3883 0.3897Iteration 1 0.3724 0.3775 0.3743iteration 3 0.3705 0.384 0.3827iteration 3 0.3705 0.384 0.3827
0.360.37
0.380.39
0.40
25k 50k 100kTraining sentences
BLEU
HeuristicIteration 1iteration 3
0%20%
40%60%
80%100%
0 10 20 30 40 50 60Sentence Length
Senten
ces Sk
ipped
Figure 1: Statistical re-estimation using a generative
phrase model degrades BLEU score relative to its
heuristic initialization.
3 Results
Having generated ?H heuristically and ?EM with
EM, we now compare their performance. While the
model and training regimen for ?EM differ from the
model from Marcu and Wong (2002), we achieved
results similar to Koehn et al (2003a): ?EM slightly
underperformed ?H . Figure 1 compares the BLEU
scores using each estimate. Note that the expecta-
tion maximization algorithm for training ?EM was
initialized with the heuristic parameters ?H , so the
heuristic curve can be equivalently labeled as itera-
tion 0.
Thus, the first iteration of EM increases the ob-
served likelihood of the training sentences while si-
multaneously degrading translation performance on
the test set. As training proceeds, performance on
the test set levels off after three iterations of EM. The
system never achieves the performance of its initial-
ization parameters. The pruning of our training regi-
men accounts for part of this degradation, but not all;
augmenting ?EM by adding back in all phrase pairs
that were dropped during training does not close the
performance gap between ?EM and ?H .
3.1 Analysis
Learning ?EM degrades translation quality in large
part because EM learns overly determinized seg-
mentations and translation parameters, overfitting
the training data and failing to generalize. The pri-
mary increase in richness from generative word-
level models to generative phrase-level models is
due to the additional latent segmentation variable.
Although we impose a uniform distribution over
segmentations, it nonetheless plays a crucial role
during training. We will characterize this phe-
nomenon through aggregate statistics and transla-
tion examples shortly, but begin by demonstrating
the model?s capacity to overfit the training data.
Let us first return to the motivation behind in-
troducing and learning phrases in machine transla-
tion. For any language pair, there are contiguous
strings of words whose collocational translation is
non-compositional; that is, they translate together
differently than they would in isolation. For in-
stance, chat in French generally translates to cat in
English, but appeler un chat un chat is an idiom
which translates to call a spade a spade. Introduc-
ing phrases allows us to translate chat un chat atom-
ically to spade a spade and vice versa.
While introducing phrases and parameterizing
their translation probabilities with a surface heuris-
tic allows for this possibility, statistical re-estimation
would be required to learn that chat should never be
translated to spade in isolation. Hence, translating I
have a spade with ?H could yield an error.
But enforcing competition among segmentations
introduces a new problem: true translation ambigu-
ity can also be spuriously explained by the segmen-
tation. Consider the french fragment carte sur la
table, which could translate to map on the table or
notice on the chart. Using these two sentence pairs
as training, one would hope to capture the ambiguity
in the parameter table as:
French English ?(e|f)
carte map 0.5
carte notice 0.5
carte sur map on 0.5
carte sur notice on 0.5
sur on 1.0
... ... ...
table table 0.5
table chart 0.5
Assuming we only allow non-degenerate seg-
mentations and disallow non-monotonic alignments,
this parameter table yields a marginal likelihood
P (f|e) = 0.25 for both sentence pairs ? the intu-
itive result given two independent lexical ambigu-
34
ities. However, the following table yields a likeli-
hood of 0.28 for both sentences:5
French English ?(e|f)
carte map 1.0
carte sur notice on 1.0
carte sur la notice on the 1.0
sur on 1.0
sur la table on the table 1.0
la the 1.0
la table the table 1.0
table chart 1.0
Hence, a higher likelihood can be achieved by al-
locating some phrases to certain translations while
reserving overlapping phrases for others, thereby
failing to model the real ambiguity that exists across
the language pair. Also, notice that the phrase sur
la can take on an arbitrary distribution over any en-
glish phrases without affecting the likelihood of ei-
ther sentence pair. Not only does this counterintu-
itive parameterization give a high data likelihood,
but it is also a fixed point of the EM algorithm.
The phenomenon demonstrated above poses a
problem for generative phrase models in general.
The ambiguous process of translation can be mod-
eled either by the latent segmentation variable or the
phrase translation probabilities. In some cases, opti-
mizing the likelihood of the training corpus adjusts
for the former when we would prefer the latter. We
next investigate how this problem manifests in ?EM
and its effect on translation quality.
3.2 Learned parameters
The parameters of ?EM differ from the heuristically
extracted parameters ?H in that the conditional dis-
tributions over English translations for some French
words are sharply peaked for ?EM compared to flat-
ter distributions generated by ?H . This determinism
? predicted by the previous section?s example ? is
not atypical of EM training for other tasks.
To quantify the notion of peaked distributions
over phrase translations, we compute the entropy of
the distribution for each French phrase according to
5For example, summing over the first translation ex-
pands to 17 (?(map | carte)?(on the table | sur la table)
+?(map | carte)?(on | sur)?(the table | la table)).
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
0 10 20 30 400 - .01
.01 - .5.5 - 1
1 - 1.51.5 - 2
> 2
Entrop
y
% Phrase TranslationsLearnedHeuristic
1E-04 1E-02 1E+00 1E+02',de.lall 'leetlesMost
 Comm
on Fre
nch Ph
rases
EntropyLearned Heuristic
Figure 2: Many more French phrases have very low
entropy under the learned parameterization.
the standard definition.
H(?(e?|f?)) =
?
e?
?(e?|f?) log2 ?(e?|f?)
The average entropy, weighted by frequency, for the
most common 10,000 phrases in the learned table
was 1.55, comparable to 3.76 for the heuristic table.
The difference between the tables becomes much
more striking when we consider the histogram of
entropies for phrases in figure 2. In particular, the
learned table has many more phrases with entropy
near zero. The most pronounced entropy differences
often appear for common phrases. Ten of the most
common phrases in the French corpus are shown in
figure 3.
As more probability mass is reserved for fewer
translations, many of the alternative translations un-
der ?H are assigned prohibitively small probabili-
ties. In translating 1,000 test sentences, for example,
no phrase translation with ?(e?|f?) less than 10?5 was
used by the decoder. Given this empirical threshold,
nearly 60% of entries in ?EM are unusable, com-
pared with 1% in ?H .
3.3 Effects on Translation
While this determinism of ?EM may be desirable
in some circumstances, we found that the ambi-
guity in ?H is often preferable at decoding time.
35
it 2.76E-08 as there are 0.073952202code 2.29E-08 the 0.002670946to 1.98E-12 less helpful 6.22E-05it be 1.11E-14 please stop messing 1.12E-05
01020
3040
0 - .01 .01 - .5 .5 - 1 1 - 1.5 1.5 - 2 > 2Entropy
% Phr
ase 
Transl
ations HeuristicLearned
1E-04 1E-02 1E+00 1E+02 ',.ll 'n 'quequiplusl ' unionC
ommo
n Fren
ch Phr
ases
EntropyLearned Heuristic
Figure 3: Entropy of 10 common French phrases.
Several learned distributions have very low entropy.
In particular, the pattern of translation-ambiguous
phrases receiving spuriously peaked distributions (as
described in section 3.1) introduces new tra slation
errors relative to the baseline. We now investigate
both positive and negative effects of the learning
process.
The issue that motivated training a generative
model is sometimes resolved correctly: for a word
that translates differently alone than in the context
of an idiom, the translation probabilities can more
accurately reflect this. Returning to the previous ex-
ample, the phrase table for chat has been corrected
through the learning process. The heuristic process
gives the incorrect translation spade with 61% prob-
ability, while the statistical learning approach gives
cat with 95% probability.
While such examples of improvement are en-
couraging, the trend of spurious determinism over-
whelms this benefit by introducing errors in four re-
lated ways, each of which will be explored in turn.
1. Useful phrase pairs can be assigned very low
probabilities and therefore become unusable.
2. A proper translation for a phrase can be over-
ridden by another translation with spuriously
high probability.
3. Error-prone, common, ambiguous phrases be-
come active during decoding.
4. The language model cannot distinguish be-
tween different translation options as effec-
tively due to deterministic translation model
distributions.
The first effect follows from our observation in
section 3.2 that many phrase pairs are unusable due
to vanishingly small probabilities. Some of the en-
tries that are made unusable by re-estimation are
helpful at decoding time, evidenced by the fact
that pruning the set of ?EM ?s low-scoring learned
phrases from the original heuristic table reduces
BLEU score by 0.02 for 25k training sentences (be-
low the score for ?EM ).
The second effect is more subtle. Consider the
sentence in figure 4, which to a first approxima-
tion can be translated as a series of cognates, as
demonstrated by the decoding that follows from the
heuristic parameterization ?H .6 Notice also that the
translation probabilities from heuristic extraction are
non-deterministic. On the other hand, the translation
system makes a significant lexical error on this sim-
ple sentence when parameterized by ?EM : the use
of caracte?rise in this context is incorrect. This error
arises from a sharply peaked distribution over En-
glish phrases for caracte?rise.
This example illustrates a recurring problem: er-
rors do not necessarily arise because a correct trans-
lation is not available. Notice that a preferable trans-
lation of degree as degre? is available under both pa-
rameterizations. Degre? is not used, however, be-
cause of the peaked distribution of a competing
translation candidate. In this way, very high prob-
ability translations can effectively block the use of
more appropriate translations at decoding time.
What is furthermore surprising and noteworthy in
this example is that the learned, near-deterministic
translation for caracte?rise is not a common trans-
lation for the word. Not only does the statistical
learning process yield low-entropy translation dis-
tributions, but occasionally the translation with un-
desirably high conditional probability does not have
a strong surface correlation with the source phrase.
This example is not unique; during different initial-
izations of the EM algorithm, we noticed such pat-
6While there is some agreement error and awkwardness, the
heuristic translation is comprehensible to native speakers. The
learned translation incorrectly translates degree, degrading the
translation quality.
36
the situation varies to an
la situation varie d ' une
Heuristically Extracted Phrase Table
Learned Phrase Table
enormous
immense
degree
degr?
situation varies to
la varie d '
an enormous
une immense
degree
caract?rise
the
situation
caracte?rise
English ?(e|f)
degree 0.998
characterises 0.001
characterised 0.001
caracte?rise
English ?(e|f)
characterises 0.49
characterised 0.21
permeate 0.05
features 0.05
typifies 0.05
degr e?
English ?(e|f)
degree 0.49
level 0.38
extent 0.02
amount 0.02
how 0.01
degre?
English ?(e|f)
degree 0.64
level 0.26
extent 0.10
Figure 4: Spurious determinism in the learned phrase parameters degrades translation quality.
terns even for common French phrases such as de
and ne.
The third source of errors is closely related: com-
mon phrases that translate in many ways depending
on the context can introduce errors if they have a
spuriously peaked distribution. For instance, con-
sider the lone apostrophe, which is treated as a sin-
gle token in our data set (figure 5). The shape of
the heuristic translation distribution for the phrase is
intuitively appealing, showing a relatively flat dis-
tribution among many possible translations. Such
a distribution has very high entropy. On the other
hand, the learned table translates the apostrophe to
the with probability very near 1.
Heuristic
English ?H(e|f)
our 0.10
that 0.09
is 0.06
we 0.05
next 0.05
Learned
English ?EM (e|f)
the 0.99
, 4.1 ? 10?3
is 6.5 ? 10?4
to 6.3 ? 10?4
in 5.3 ? 10?4
Figure 5: Translation probabilities for an apostro-
phe, the most common french phrase. The learned
table contains a highly peaked distribution.
Such common phrases whose translation depends
highly on the context are ripe for producing transla-
tion errors. The flatness of the distribution of ?H en-
sures that the single apostrophe will rarely be used
during decoding because no one phrase table entry
has high enough probability to promote its use. On
the other hand, using the peaked entry ?EM (the|?)
incurs virtually no cost to the score of a translation.
The final kind of errors stems from interactions
between the language and translation models. The
selection among translation choices via a language
model ? a key virtue of the noisy channel frame-
work ? is hindered by the determinism of the transla-
tion model. This effect appears to be less significant
than the previous three. We should note, however,
that adjusting the language and translation model
weights during decoding does not close the perfor-
mance gap between ?H and ?EM .
3.4 Improvements
In light of the low entropy of ?EM , we could hope to
improve translations by retaining entropy. There are
several strategies we have considered to achieve this.
Broadly, we have tried two approaches: combin-
ing ?EM and ?H via heuristic interpolation methods
and modifying the training loop to limit determin-
ism.
The simplest strategy to increase entropy is to
interpolate the heuristic and learned phrase tables.
Varying the weight of interpolation showed an im-
provement over the heuristic of up to 0.01 for 100k
sentences. A more modest improvement of 0.003 for
25k training sentences appears in table 1.
In another experiment, we interpolated the out-
put of each iteration of EM with its input, thereby
maintaining some entropy from the initialization pa-
rameters. BLEU score increased to a maximum of
0.394 using this technique with 100k training sen-
tences, outperforming the heuristic by a slim margin
of 0.005.
We might address the determinization in ?EM
without resorting to interpolation by modifying the
37
training procedure to retain entropy. By imposing a
non-uniform segmentation model that favors shorter
phrases over longer ones, we hope to prevent the
error-causing effects of EM training outlined above.
In principle, this change will encourage EM to ex-
plain training sentences with shorter sentences. In
practice, however, this approach has not led to an
improvement in BLEU.
Another approach to maintaining entropy during
the training process is to smooth the probabilities
generated by EM. In particular, we can use the fol-
lowing smoothed update equation during the train-
ing loop, which reserves a portion of probability
mass for unseen translations.
?new(e?j |f?i) =
c(f?i, e?j)
c(f?i) + kl?1
In the equation above, l is the length of the French
phrase and k is a tuning parameter. This formula-
tion not only serves to reduce very spiked probabili-
ties in ?EM , but also boosts the probability of short
phrases to encourage their use. With k = 2.5, this
smoothing approach improves BLEU by .007 using
25k training sentences, nearly equaling the heuristic
(table 1).
4 Conclusion
Re-estimating phrase translation probabilities using
a generative model holds the promise of improving
upon heuristic techniques. However, the combina-
torial properties of a phrase-based generative model
have unfortunate side effects. In cases of true ambi-
guity in the language pair to be translated, parameter
estimates that explain the ambiguity using segmen-
tation variables can in some cases yield higher data
likelihoods by determinizing phrase translation esti-
mates. However, this behavior in turn leads to errors
at decoding time.
We have also shown that some modest benefit can
be obtained from re-estimation through the blunt in-
strument of interpolation. A remaining challenge is
to design more appropriate statistical models which
tie segmentations together unless sufficient evidence
of true non-compositionality is present; perhaps
such models could properly combine the benefits of
both current approaches.
Estimate BLEU
?H 0.385
?H phrase pairs that also appear in ?EM 0.365
?EM 0.374
?EM with a non-uniform segmentation model 0.374
?EM with smoothing 0.381
?EM with gaps filled in by ?H 0.374
?EM interpolated with ?H 0.388
Table 1: BLEU results for 25k training sentences.
5 Acknowledgments
We would like to thank the anonymous reviewers for
their valuable feedback on this paper.
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. The mathematics of
statistical machine translation: Parameter estimation.
Computational Linguistics, 19(2), 1993.
Philipp Koehn. Europarl: A Multilingual Corpus for
Evaluation of Machine Translation. USC Information
Sciences Institute, 2002.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. Sta-
tistical phrase-based translation. HLT-NAACL, 2003.
Philipp Koehn. Pharaoh: A Beam Search Decoder for
Phrase-Based Statisical Machine Translation Models.
USC Information Sciences Institute, 2003.
Daniel Marcu and William Wong. A phrase-based, joint
probability model for statistical machine translation.
Conference on Empirical Methods in Natual Language
Processing, 2002.
Franz Josef Och and Hermann Ney. A systematic com-
parison of various statistical alignment models. Com-
putational Linguistics, 29(1):19?51, 2003.
Franz Josef Och, Christoph Tillmann, and Hermann Ney.
Improved alignment models for statistical machine
translation. ACL Workshops, 1999.
Andreas Stolcke. Srilm ? an extensible language model-
ing toolkit. Proceedings of the International Confer-
ence on Statistical Language Processing, 2002.
Richard Zens, Franz Josef Och and Hermann Ney.
Phrase-Based Statistical Machine Translation. Annual
German Conference on AI, 2002.
38
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 193?203,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Inducing Sentence Structure from Parallel Corpora for Reordering
John DeNero
Google Research
denero@google.com
Jakob Uszkoreit
Google Research
uszkoreit@google.com
Abstract
When translating among languages that differ
substantially in word order, machine transla-
tion (MT) systems benefit from syntactic pre-
ordering?an approach that uses features from
a syntactic parse to permute source words
into a target-language-like order. This paper
presents a method for inducing parse trees au-
tomatically from a parallel corpus, instead of
using a supervised parser trained on a tree-
bank. These induced parses are used to pre-
order source sentences. We demonstrate that
our induced parser is effective: it not only
improves a state-of-the-art phrase-based sys-
tem with integrated reordering, but also ap-
proaches the performance of a recent pre-
ordering method based on a supervised parser.
These results show that the syntactic structure
which is relevant to MT pre-ordering can be
learned automatically from parallel text, thus
establishing a new application for unsuper-
vised grammar induction.
1 Introduction
Recent work in statistical machine translation (MT)
has demonstrated the effectiveness of syntactic pre-
ordering: an approach that permutes source sen-
tences into a target-like order as a pre-processing
step, using features of a source-side syntactic parse
(Collins et al, 2005; Xu et al, 2009). Syntac-
tic pre-ordering is particularly effective at apply-
ing structural transformations, such as the order-
ing change from a subject-verb-object (SVO) lan-
guage like English to a subject-object-verb (SOV)
language like Japanese. However, state-of-the-art
pre-ordering methods require a supervised syntac-
tic parser to provide structural information about
each sentence. We propose a method that learns
both a parsing model and a reordering model di-
rectly from a word-aligned parallel corpus. Our ap-
proach, which we call Structure Induction for Re-
ordering (STIR), requires no syntactic annotations
to train, but approaches the performance of a re-
cent syntactic pre-ordering method in a large-scale
English-Japanese MT system.
STIR predicts a pre-ordering via two pipelined
models: (1) parsing and (2) tree reordering. The
first model induces a binary parse, which defines
the space of possible reorderings. In particular, only
trees that properly separate verbs from their object
noun phrases will license an SVO to SOV trans-
formation. The second model locally permutes this
tree. Our approach resembles work with binary syn-
chronous grammars (Wu, 1997), but is distinct in its
emphasis on monolingual parsing as a first phase,
and in selecting reorderings without the aid of a
target-side language model.
The parsing model is trained to maximize the
conditional likelihood of trees that license the re-
orderings implied by observed word alignments in
a parallel corpus. This objective differs from those
of previous grammar induction models, which typ-
ically focus on succinctly explaining the observed
source language corpus via latent hierarchical struc-
ture (Pereira and Schabes, 1992; Klein and Man-
ning, 2002). Our convex objective allows us to train
a feature-rich log-linear parsing model, even without
supervised treebank data.
Focusing on pre-ordering for MT leads to a new
193
perspective on the canonical NLP task of grammar
induction?one which marries the wide-spread sci-
entific interest in unsupervised parsing models with
a clear application and extrinsic evaluation method-
ology. To support this perspective, we highlight sev-
eral avenues of future research throughout the paper.
We evaluate STIR in a large-scale English-
Japanese machine translation system. We measure
how closely our predicted reorderings match those
implied by hand-annotated word alignments. STIR
approaches the performance of the state-of-the-art
pre-ordering method described in Genzel (2010),
which learns reordering rules for supervised tree-
bank parses. STIR gives a translation improvement
of 3.84 BLEU over a standard phrase-based system
with an integrated reordering model.
2 Parsing and Reordering Models
STIR consists of two pipelined log-linear models for
parsing and reordering, as well as a third model for
inducing trees from parallel corpora, trees that serve
to train the first two models. This section describes
the domain and structure of each model, while Sec-
tion 3 describes features and learning objectives.
Figure 1 depicts the relationship between the three
models. For each aligned sentence pair in a paral-
lel corpus, the parallel parsing model selects a bi-
nary tree t over the source sentence, such that t li-
censes the reordering pattern implied by the word
alignment (Section 2.2). The monolingual parsing
model is trained to generate t without inspecting the
alignments or target sentences (Section 2.3). The
tree reordering model is trained to locally permute t
to produce the target order (Section 2.4). In the con-
text of an MT system, the monolingual parser and
tree reorderer are applied in sequence to pre-order
source sentences.
2.1 Unlabeled Binary Trees
Unlabeled binary trees are central to the STIR
pipeline. We represent trees via their constituent
spans. Let [k, `) denote a span of indices of a 0-
indexed word sequence e, where i ? [k, `) if k ?
i < `. [0, n) denotes the root span covering the
whole sequence, where n = |e|.
A tree t = (T ,N ) consists of a set of termi-
nal spans T and non-terminal spans N . Each non-
?? ? ?? ? ?? ?????
pair [subj] list to add to was
0 1 2 3 4 5
pair added to the lexicon
[0,2) [4,6) [3,4) [2,3)
?
Target
Alignment
Projections
Parallel
Parse
Source
Induced
Parse
Gloss
Reference
Order
Induced
Order
pair addedtothe lexicon
pair addedtothe lexicon
[ ] ][ ][ [ ]
Positions
S V O
s o v
Parallel
corpus
Parallel
parsing 
model
S V O
Trees &
rotations
Monolingual
parsing 
model
Tree 
reordering 
model
S V O
S V O
S O V
Figure 1: The training and reordering pipeline for STIR
contains three models. The inputs and outputs of each
model are indicated by solid arrows, while dashed arrows
indicate the source of training examples. The parallel
parsing model provides tree and reordering examples that
are used to train the other models. In an MT system, the
trained reordering pipeline (shaded) pre-orders a source
sentence without target-side or alignment information.
terminal span [k, `) ? N has a split point m, where
k < m < ` splits the span into child spans [k,m)
and [m, `). Formally, a pair (T ,N ) is a well-formed
tree over [0, n) if:
? The root span [0, n) ? T ? N .
? For each [k, `) ? N , there exists exactly one m
such that {[k,m), [m, `)} ? T ? N .
? Terminal spans T are disjoint, but cover [0, n).
These trees include multi-word terminal spans. It
is often convenient to refer to a split non-terminal
triple (k,m, `) that include a non-terminal span
[k, `) and its split point m. We denote the set of
these triples as
N+= {(k,m, `) : {[k, `), [k,m), [m, `)}? T ? N} .
2.2 Parallel Parsing Model
The first step in the STIR pipeline is to select a bi-
nary parse of each source sentence in a parallel cor-
pus, one which licenses the reordering implied by
a word alignment. Let the triple (e, f ,A) be an
aligned sentence pair, where e and f are word se-
quences and A is a set of links (i, j) indicating that
ei aligns to fj .
The set A provides ordering information over e.
To simplify definitions below, we first adjust A to
194
ignore all unaligned words in f .
A? = {(i, c(j)) : (i, j) ? A}
c(j) = |{j? : j? < j ? ?i such that (i, j?) ? A}| .
c(j) is the number of aligned words in f prior to
position j. Next, we define a projection function:
?(i) =
[
min
j?Ji
j,max
j?Ji
j + 1
)
Ji = {j : (i, j) ? A?} ,
and let ?(i) = ? if ei is unaligned. We can extend
this projection function to spans [k, `) of e via union:
?(k, `) =
?
k?i<`
?(i) .
We say that a span [k, `) aligns contiguously if
?(i, j) ? A?, j ? ?(k, `) implies i ? [k, `) ,
which corresponds to the familiar definition that
[k, `) is one side of an extractable phrase pair. Un-
aligned spans do not align contiguously.
Given this notion of projection, we can relate
trees to alignments. A tree (T ,N ) over e respects
an alignment A? if all [k, `) ? T ? N align con-
tiguously, and for every (k,m, `), the projections
?(k,m) and ?(m, `) are adjacent. Projections are
adjacent if the left bound of one is the right bound
of the other, or if either is empty.
The parallel parsing model is a linear model over
trees that respect A?, which factors over spans.
s(t) =
?
[k,`)?T
wT?T (k, `) +
?
(k,m,`)?N+
wN?N (k,m, `)
where the weight vector w = (wT wN ) scores fea-
tures ?T on terminal spans and ?N on non-terminal
spans and their split points.
Exact inference under this model can be per-
formed via a dynamic program that exploits the fol-
lowing recurrence. Let s(k, `) be the score of the
highest scoring binary tree over the span [k, `) that
respects A?. Then,
sT (k, `) =
?
??
??
wT?T (k, `) if [k, `) aligns
contiguously
?? otherwise
f(k,m, `) = s(k,m) + s(m, `) + wN?N (k,m, `)
sN (k, `) = maxm:k<m<`
?
???
???
f(k,m, `) if ?(k,m) is
adjacent
to ?(m, `)
?? otherwise
s(k, `) = max [sT (k, `), sN (k, `)]
Above, sT scores terminal spans while filtering out
those which are not contiguous. The function f
scores non-terminal spans by the sum of their child
scores and additional features ?N of the parent
span. The recursive function sN maximizes over
split points while filtering out non-adjacent children.
The recurrence will assign a score of ?? to any
tree that does not respect A?. Section 3 describes
the features of this model. s(k, `) can be computed
efficiently using the CKY algorithm.
2.3 Monolingual Parsing Model
The monolingual parsing model is trained to select
the same trees as the parallel model, but without
any features or constraints that reference word align-
ments. Hence, it can be applied to a source sentence
before its translation is known.
This model also scores untyped binary trees ac-
cording to a linear model parameterized by some
w = (wT wN ) that weights features on terminal and
non-terminal spans, respectively. We impose a max-
imum terminal length of L, but otherwise allow any
binary tree. The score s(k, `) of the maximal tree
over a span [k, `) satisfies the familiar recurrence:
sM (k, `) =
{
wT?T (k, `) if `? k ? L
?? otherwise
s(k, `) = max
[
sL(k, `), maxm:k<m<` f(k,m, `)
]
Inference under this recurrence can also be per-
formed using the CKY algorithm. Section 3 de-
scribes the feature functions and training method.
195
2.4 Tree Reordering Model
Given a binary tree (T ,N ) over a sentence e, we
can reorder e by (a) permuting the children of non-
terminals and (b) permuting the words of terminal
spans. Formally, a reordering r assigns each termi-
nal [k, `) ? T a permutation ?(k, `) of its words
and each split non-terminal (k,m, `) a permutation
b(k,m, `) of its subspans, which can be either mono-
tone or inverted, in the case of a binary tree. The per-
mutation ?(k, `) of a non-terminal span [k, `) /? T
is defined recursively as:
{
?(k,m) ?(m, `) if b(k,m, `) is monotone
?(m, `) ?(k,m) if b(k,m, `) is inverted
In this paper, we use a reordering model that
selects each terminal ?(k, `) and each split non-
terminal b(k,m, `) independently, conditioned on
the sentence e. While the sub-problems of choos-
ing ?(k, `) and b(k,m, `) are formally similar, we
consider and evaluate them separately because the
former deals only with local reordering, while the
latter involves long-distance structural reordering.
Because our trees are binary, selecting b(k,m, `)
is a binary classification problem. Selecting ?(k, `)
for a terminal is a multiclass prediction problem that
chooses among the (` ? k)! permutations of ter-
minal [k, `). Development experiments in English-
Japanese yielded the best results with a maximum
terminal span length L = 2. Hence, in experiments,
terminal reordering is also binary classification.
Because each permutation is independent of all
the others, reordering inference via a single pass
through the tree is optimal. However, a more com-
plex search procedure would be necessary to main-
tain optimality if the decision of b(k,m, `) ref-
erenced other permutations, such as ?([k,m)) or
?([m, `)). Coupling together inference in this way
represents a possible area of future study.
3 Features and Training Objectives
Each of these linear models factors over features
on either terminal spans [k, `) or split non-terminals
(k,m, `). Features vary in concert with the learning
objectives and search spaces of each model.
Figure 2 shows an example sentence from our de-
velopment corpus, including the target (Japanese)
?? ? ?? ? ?? ?????
pair [subj] list to add to was
0 1 2 3 4 5
pair added to the lexicon
[0,2) [4,6) [3,4) [2,3)
?
Target
Alignment
Projections
Parallel
Parse
Source
Induced
Parse
Gloss
Reference
Order
Induced
Order
pair addedtothe lexicon
pair addedtothe lexicon
[ ] ][ ][ [ ]
Positions
Figure 2: An example from our development corpus, an-
notated with the information flow (left) and annotations
and predictions (right). Alignments inform projections,
which are spans of the target associated with each source
word. The parallel parse may only include contiguous
spans. On the other hand, the induced parse may only
condition on the source sentence. The induced order
is restricted by the induced parse. In this example, the
induced order is incorrect because the subject and verb
form a constituent in the induced parse that cannot be sep-
arated correctly by the reordering model. This example
demonstrates the important role of the induced parser in
the STIR pipeline.
sentence, alignment, projections, parallel parser pre-
diction, monolingual parser prediction, and pre-
dicted permutation. The feature descriptions below
reference this example.
3.1 Tree Reordering Features
The tree reordering model consists of two local clas-
sifiers: the first can invert the two children of a
non-terminal span, while the second can permute the
words of a terminal span. The non-terminal classi-
fier is trained on the trees that are selected by the
parallel parsing model; the weights are chosen to
minimize log loss of the correct permutation of each
span (i.e., a maximum entropy model).
The terminal model is a multi-class maximum en-
tropy model over the n! possible permutations of the
words in a terminal span. To make reordering more
robust to monolingual parsing errors, the terminal
196
model is trained on all contiguous spans of each sen-
tence up to length L, not just the terminal spans in-
cluded in the parallel parsing tree.
The feature templates we apply to each span can
be divided into the following five categories. Most
features are shared across the two models.
Statistics. From a large aligned parallel corpus, we
compute two statistics.
? PC(e) = count(e aligns contiguously)count(e) is the frac-tion of the time that a phrase e aligns con-
tiguously to some target phrase, for all
phrases up to length 4.
? PD(ei, ej) is the fraction of the time that
two co-occuring source words ei and ej
align to adjacent positions in the target.
The first statistic indicates whether a contigu-
ous phrase in the source should stay contiguous
after reordering. Features based on this statistic
apply to both terminal and short non-terminal
spans. The second statistic indicates when a
possibly discontiguous pair of words should be
adjacent after reordering. This statistic is ap-
plied to pairs of words that would end up ad-
jacent after an inversion: ek and e`?1 for span
[k, `). For instance, PC(added to) = 0.68 and
PD(lexicon, to) = 0.19.
Cluster. All source word types are clustered into
word classes, which together maximize likeli-
hood of the source side of a large parallel cor-
pus under a hidden Markov model, as in Uszko-
reit and Brants (2008). Indicator features based
on clusterings over c classes are defined over
words ek, em?1, em and e`?1, as well as word
sequences for spans up to length 4. Features are
included for a variety of clusterings with sizes
c ? {23, 24, . . . , 211}.
POS. A supervised part-of-speech (POS) tagger
provides coarse tags drawn from a 12 tag set
T = {Verb, Noun, Pronoun, Conjunction,
Adjective, Adverb, Adposition, Determiner,
Number, Particle/Function word, Punctuation,
Other} (Petrov et al, 2011). Features based on
these tags are computed identically to the fea-
tures based on word classes.
Lexical. For a list of very common words in the
source language, we include lexical indicator
features for the boundary words ek and e`?1.
For instance, the word ?to? triggers a reorder-
ing, as do prepositions in general.
Length. Length computed as `?k, length as a frac-
tion of sentence length, and quantized length
features all contribute structural information.
All features except POS are computed directly
from aligned parallel corpora. The Cluster and POS
features play a similar role of expressing reordering
patterns over collections of similar words. The ab-
lation study in Section 5 compares these two feature
sets directly.
3.2 Monolingual Parsing Features
The monolingual parsing model is also trained dis-
criminatively, but involves structured prediction, as
in a conditional random field (Lafferty et al, 2001).
Conditional likelihood objectives have proven ef-
fective for supervised parsers (Finkel et al, 2008;
Petrov and Klein, 2008). Recall that the score of a
tree t = (T ,N ) factors over spans.
s(t) =
?
[k,`)?T
wT?T (k, `) +
?
[k,`)?N
wN?N (k,m, `)
P(t|e) = exp [s(t)]?
(t?)?B(e) exp [s(t?)]
where B(e) is the set of well-formed trees over e.
The parallel parsing model (Section 2.2) produces
a tree over the source sentence of each aligned sen-
tence pair; these trees serve as our training exam-
ples. We can maximize their conditional likelihood
according to this model via gradient methods. Each
tree t over sentence e has a cumulative feature vector
of dimension |w| = |wT |+|wN |, formed by stacking
the terminal and non-terminal vectors:
?(t, e) =
?
? ?
[k,`)?T
?T (k, `)
?
[k,`)?N
?N (k,m, `)
?
?
The contribution to the gradient objective from a tree
t for a sentence e is the difference between observed
197
and expected feature vectors.
L(w) =
?
(t,e)
log P(t|e)
?L(w) =
?
(t,e)
?
??(t, e)?
?
t??B(e)
P(t?|e) ? ?(t?, e)
?
?
The second term in the gradient?the expected
feature vector?can be computed efficiently because
the feature vector ?(t?) decomposes over the spans
of t?. In particular, the inside-outside algorithm pro-
vides the quantities needed to compute the poste-
rior probability of each terminal span [k, `) and each
split non-terminal (k,m, `). Let, ?(k, `) and ?(k, `)
be the outside and inside scores of a span, respec-
tively, computed using a log-sum semiring. Then,
the log probablility that a terminal span [k, `) ap-
pears in the tree for e under the posterior distribu-
tion P(t|e) is ?(k, `) + wT?T (k, `) . Note that this
terminal posterior does not include the inside score
of the span.
The log probability that a non-terminal span [k, `)
appears with split point m is
?(k, `) + ?(k,m) + ?(m, `) + wN?N (k,m, `)
By the linearity of expectations, the expected feature
vector for e can be computed by averaging the fea-
ture vectors of each terminal and split non-terminal
span, weighted by their posterior probabilities.
In future work, one may consider training this
model to maximize the likelihood of an entire forest
of trees, in order to maintain uncertainty over which
tree licensed a particular alignment.
We are currently using l-BFGS to optimize this
objective over a relatively small training corpus, for
35 iterations. For this reason, we only include lexi-
cal features for very common words. Distributed or
online training algorithms would perhaps allow for
more training data (and therefore more lexicalized
features) to be used in the future.
The features of this parsing model share the same
types as the tree reordering models, but vary in their
definition. The differences stem primarily from the
different purpose of the model: here, features are
not meant to decide how to reorder the sentence, but
instead how to bracket the sentence hierarchically so
that it can be reordered.
In particular, terminal spans have features on the
sequence of POS tags and word clusters they con-
tain, while a split non-terminal (k,m, `) is scored
based on the tags/clusters of the following words and
word pairs: ek, em?1, em, e`?1, (ek, em), (ek, e`?1),
and (em?1, em). The head word of a constituent of-
ten appears at one of its boundary positions, and so
these features provide a proxy for explicitly tracking
constituent heads in a parser.
Context features also appear, inspired by the
constituent-context model of Klein and Manning
(2001). For a span [k, `), we add indicator fea-
tures on the POS tags and word clusters of the words
(ek?1, e`) which directly surround the constituent.
Features based on the statistic PC(e) are also
scored in the parsing model on all spans of length
up to 4.
Length features score various structural aspects of
each non-terminal (k,m, `), such as m?k`?k , m?kk?m , etc.
One particularly interesting direction for future
work is to train a single parsing model that licenses
the reordering for several different languages. We
might expect that a reasonable syntactic bracket-
ing of English would simultaneously license the
head-final transformations necessary to produce a
Japanese or Korean ordering, and also the verb-
subject-object ordering of formal Arabic.1
3.3 Parallel Parsing Features
The parallel parsing model does not run at transla-
tion time, but instead provides training examples to
the other two models. Hence, defining an appropri-
ate learning objective for this model is more chal-
lenging.
In the end, we are interested in selecting trees that
we can learn to reproduce without an alignment (via
the monolingual parsing model) and which can be
reordered reliably (via the tree reordering model).
Note that by construction, any tree selected by the
parallel parsing model can be reordered perfectly.
However, some of those trees will be easier to re-
produce and reorder than others.
1An astute reviewer pointed out that no binary tree over an
S-V-O sentence can license both S-O-V and V-S-O orderings.
Hence, parse trees that are induced for multilingual reordering
will need n-ary branches.
198
3.3.1 Reordering Loss Function
In order to measure the effectiveness of a reorder-
ing pipeline, we would like a metric over permu-
tations. Fortunately, permutation loss for machine
translation is already an established component of
the METEOR metric, called a fragmentation penalty
(Lavie and Agarwal, 2007). We define a slight vari-
ant of METEOR?s fragmentation penalty that ranges
from 0 to 1.
Given a sentence e, a reference permutation ??
of (0, ? ? ? , |e| ? 1), and a hypothesized permuta-
tion ??, let chunks(??, ??) be the minimum number
of ?chunks? in ??: the number of elements in a par-
tition of ?? such that each contiguous subsequence is
also contiguous in ??.
We can define the reordering score between two
permutations in terms of chunks.
R(??, ??) = |?
?| ? chunks(??, ??)
|??| ? 1 (1)
If ?? = ??, then chunks(??, ??) = 1. If no
two adjacent elements of ?? are adjacent in ??, then
chunks(??, ??) = |?|. Hence, the metric defined by
Equation 1 ranges from 0 to 1.
The reference permutation ?? of a source sen-
tence e can be defined from an aligned sentence pair
(e, f ,A) by sorting the words ei of e by the left
bound of their projection ?(i). Null-aligned words
are placed to the left of the next aligned word to their
right in the original order.
The reordering-specific loss functions defined in
Equation 1 has been shown to correlate with human
judgements of translation quality, especially for lan-
guage pairs with substantial reordering like English-
Japanese (Talbot et al, 2011). Other reordering-
specific loss functions also correlate with human
judgements (Birch et al, 2010). Future research
could experiment with alternative reordering-based
loss functions, such as Kendall?s Tau, as suggested
by Birch and Osborne (2011).
3.3.2 Parallel Parsing Objective
We can train our reordering pipeline by dividing
an aligned parallel corpus into two halves, A and B,
where the monolingual parsing and tree reordering
models are trained on A, and their effectiveness is
evaluated on held-out set B. Then, the effectiveness
of the parallel parsing model is best measured on B,
given fully trained parsing and reordering models.
?
(e,??)?B
R
(
?
(
arg max
t?B(e)
[w ? ?(t)]
)
, ??
)
(2)
Evaluating this objective involves training the
other two models. Therefore, we can only hope to
optimize this objective directly over a small dimen-
sional space, for instance using a grid search. For
this reason, we currently only include 4 features in
the parallel parsing model for a tree t:
1. The sum of log PC(e) for all terminals e in t
with length greater than 1.
2. The count of length-1 terminal spans in t.
3. The count of terminals of length greater than k.
4. An indicator feature of whether parentheses
and brackets are balanced in each span.
The model weights of features 3 and 4 above are
fixed to large negative constants to prefer terminal
spans of length up to k and spans with balanced
punctuation. The weight of feature 1 is fixed to 1,
and weight 2 was set via line search to 0.3. Ties
among trees were broken randomly.
Of course, the problem of selecting training trees
need not be directly tied to the end task of reorder-
ing, as in Equation 2. Instead, we might consider se-
lecting trees according to a likelihood objective on
the source side of a parallel corpus, similar to how
monolingual grammar induction models often opti-
mize corpus likelihood. In such a case, we could
imagine training models with far more parameters,
but we leave this research direction to future work.
4 Related Work
Our approach to inducing hierarchical structure for
pre-ordering relates to several areas of previous
work, including other pre-ordering methods, re-
ordering models more generally, and models for the
unsupervised induction of syntactic structure.
4.1 Pre-Ordering Models
Our reordering pipeline is intentionally similar to
approaches that use a treebank-trained supervised
199
parser to reorder source sentences at training and
translation time (Xia and McCord, 2004; Collins
et al, 2005; Lee et al, 2010). Given a supervised
parser, a rule-based pre-ordering procedure can ei-
ther be specified by hand (Xu et al, 2009) or learned
automatically (Genzel, 2010). We consider our ap-
proach to be a direct extension of these approaches,
but one which induces structure from parallel cor-
pora rather than relying on a treebank.
Tromble (2009) show that some pre-ordering ben-
efits can be realized without a parsing step at all, by
instead casting pre-ordering as a permutation mod-
eling problem. While not splitting the task of pre-
ordering into parsing and tree rordering, that work
shows that pre-ordering models can be learned di-
rectly from parallel corpora.
4.2 Integrated Reordering Models
Distortion models have been primary components
in machine translation models since the advent of
statistical MT (Brown et al, 1993). In modern
systems, reordering models are integrated into de-
coders as additional features in a discriminative log-
linear model, which also includes a language model,
translation features, etc. In these cases, reordering
models interact with the strong signal of a target-
side language model. Because ordering prediciton
is conflated with target-side generation, evaluations
are conducted on the entire generated output, which
cannot isolate reordering errors from other sorts of
errors, like lexical selection.
Despite these differences, certain integrated re-
ordering models are similar in character to syntactic
pre-ordering models. In particular, the tree rotation
model of Yamada and Knight (2001) posited that re-
ordering decisions involve rotations of a source-side
syntax tree. The parameters of such a model can be
trained by treating tree rotations as latent variables
in a factored translation model, which parameterizes
reordering and transfer separately but performs joint
inference (Dyer and Resnik, 2010). Syntactic re-
ordering and transfer can also be modeled jointly,
for instance in a tree-to-string translation system pa-
rameterized by a transducer grammar.
While the success of integrated reordering models
certainly highlights the importance of reordering in
machine translation systems, we see several advan-
tages to a pipelined, pre-ordering approach. First,
the pre-ordering model can be trained and evaluated
directly. Second, pre-ordering models need not fac-
tor according to the same dynamic program as the
translation model. Third, the same reordering can be
applied during training (for word alignment and rule
extraction) and translation time without adding com-
plexity to the extraction and decoding algorithms.
Of course, integrating our model into translation in-
ference represents a potentially fruitful avenue of fu-
ture research.
4.3 Grammar Induction
The language processing community actively works
on the problem of automatically inducing grammat-
ical structure from a corpus of text (Pereira and
Schabes, 1992). Some success in this area has
been demonstrated via generative models (Klein and
Manning, 2002), which often benefit from well-
chosen priors (Cohen and Smith, 2009) or poste-
rior constraints (Ganchev et al, 2009). In princi-
ple, these models must discover the syntactic pat-
terns that govern a language from the sequences of
word tokens alone. These models are often evalu-
ated relative to reference treebank annotations.
Grammar induction in the context of machine
translation reordering offers different properties.
The alignment patterns in a parallel corpus pro-
vide an additional signal to models that is strongly
tied to syntactic properties of the aligned languages.
Also, the evaluation is straightforward?any syntac-
tic structure that supports the prediction of reorder-
ing is rewarded.
Kuhn (2004) applied alignment-based constraints
to the problem of inducing probabilistic context-free
grammars, and showed an improvement with respect
to Penn Treebank annotations over monolingual in-
duction. Their work is distinct from ours because it
focused on projecting distituents across languages,
but mirrors ours in demonstrating that there is a role
for aligned parallel corpora in grammar induction.
Snyder et al (2009) also demonstrated that paral-
lel corpora can play a role in improving the quality
of grammar induction models. Their work differs
from ours in that it focuses on multilingual lexical
statistics and dependency relationships, rather than
reordering patterns.
200
Parsing Tree Reordering Pipeline
Prec Rec F1 accN accT RO R
All features 82.0 87.8 84.8 97.3 93.6 87.7 80.5
Annotated word All but POS 81.3 87.7 84.4 97.0 92.6 86.6 79.4
alignments All but Cluster 81.2 87.9 84.4 95.9 93.2 83.8 77.8
All but POS & Cluster 75.4 82.0 78.5 89.2 89.7 66.8 49.7
Learned alignments All features 72.5 61.0 66.3 91.6 83.3 72.0 49.5
Monotone order 34.9
Inverted order 30.8
Syntactic pre-ordering (Genzel, 2010) 66.0
Table 1: Accuracy of individual monolingual parsing and reordering models, as well as complete pipelines trained on
annotated and learned word alignments.
4.4 Bilingual Grammar Induction
Also related to STIR is previous work on bilingual
grammar induction from parallel corpora using ITG
(Blunsom et al, 2009). These models have focused
on learning phrasal translations ? which are the ter-
minal productions of a synchronous ITG ? rather
than reordering patterns that occur higher in the tree.
Hence, while this paper shares formal machinery
and data sources with that line of work, the models
themselves target orthogonal aspects of the transla-
tion problem.
5 Experimental Results
As training data for our models we used 14,000 En-
glish sentences that were sampled from the web,
translated into Japanese, and manually annotated
with word alignments. The annotation was carried
out by the original translators to promote consis-
tency of analysis. Talbot et al (2011) describes this
corpus in further detail. A held-out test set of 396
manually aligned sentence pairs was used to evalu-
ate reordering accuracy. Statistics used for features
were computed from the full, unreordered, automat-
ically word aligned, parallel training corpus used for
the translation experiments described below.
5.1 Individual Model Accuracy
We evaluate the accuracy of the monolingual parsing
models by their span F1, relative to the trees induced
by the parallel parsing model on the held-out set.
The first row of Table 1 shows that the model was
able to reliably replicate the parses induced from
alignments, at 84.8% F1. The following three lines
show that removing either POS or cluster features
degrades performance by only 0.4% F1, indicating
that POS features are largely redundant in the pres-
ence of automatically induced word class features.
Hence, no syntactic annotations are necessary at all
to train the model.
We report two accuracy measures for the tree re-
ordering model, one for non-terminal spans (accN )
and one for terminal spans (accT ). The following
column, labeled RO, is the reordering score of the
tree reordering model applied to the oracle parallel
parser tree. This score is independent of the mono-
lingual parsing model.
The fifth line, labeled learned alignments, shows
the impact of replacing manual alignment anno-
tations with learned Model 1 alignments, trained
in both directions and combined with the refined
heuristic (Brown et al, 1993; Och et al, 1999).
The pipeline column shows the reordering score
of the full STIR pipeline compared to two simple
baselines: Monotone applies no reordering, while
inverted simply inverts the word order. STIR out-
performs all three other systems.
In the final line, we compare to the syntax-based
pre-ordering system described in Genzel (2010).
This approach first parses source sentences with a
supervised parser, then learns reordering rules that
permute those trees.
5.2 Translation Quality
We apply STIR as a pre-ordering step in a state-
of-the-art phrase-based translation system from En-
glish to Japanese (Koehn et al, 2003). At training
201
time, pre-ordering is applied to the source side of ev-
ery sentence pair in the training corpus before word
alignment and phrase extraction. Likewise, every in-
put sentence is pre-ordered at translation time.
Our baseline is the same system, but without pre-
ordering. Our implementation?s integrated distor-
tion model is expressed as a negative exponential
function of the distance between the current and pre-
vious source phrase, with a maximum jump width
of four words. Our in-house decoder is based on the
alignment template approach to translation and uses
a small set of standard feature functions during de-
coding (Och and Ney, 2004).
We compare to using an integrated lexicalized re-
ordering model (Koehn and Monz, 2005), a forest-
to-string translation model (Zhang et al, 2011) and
finally the syntactic pre-ordering technique of Gen-
zel (2010) applied to the phrase-based baseline. We
evaluate the impact of the proposed approach on
translation quality as measured by the BLEU score
on the token level (Papineni et al, 2002).
The translation model is trained on 700 million
tokens of parallel text, primarily extracted from the
web using automated parallel document identifica-
tion (Uszkoreit et al, 2010). Alignments were
learned using two iterations of Model 1 and two it-
erations of the HMM alignment model (Vogel et al,
1996). Our dev and test data sets consist of 3100
and 1000 English sentences, respectively, that were
randomly sampled from the web and translated into
Japanese. The eval set is a larger, heterogenous
set containing 12,784 sentences. In all cases, the
final log-linear models were optimized on the dev
set using lattice-based Minimum Error Rate Train-
ing (Macherey et al, 2008).
Table 2 shows that STIR improves over the base-
line system by a large margin of 3.84% BLEU (test).
These gains are comparable in magnitude to those
reported in Genzel (2010). Our induced parses are
competitive with both systems that use syntactic
parsers and substantially outperform lexicalized re-
ordering.
6 Conclusion
We have demonstrated that induced parses suffice
for pre-ordering. We hope that future work in gram-
mar induction will also consider pre-ordering as an
BLEU %
dev test eval
Baseline 18.65 19.02 13.60
Lexicalized Reordering 19.45 18.92 13.99
Forest-to-String 23.08 22.85 16.60
Syntactic Pre-ordering 22.59 23.28 16.31
STIR: annotated 22.46 22.86 16.39
STIR: learned 20.28 20.66 14.64
Table 2: Translation quality, measured by BLEU, for En-
glish to Japanese. STIR results use both manually anno-
tated and learned alignments.
extrinsic evaluation.
References
Alexandra Birch and Miles Osborne. 2011. Reordering
metrics for MT. In Proceedings of the Association for
Computational Linguistics.
Alexandra Birch, Phil Blunsom, and Miles Osborne.
2010. Metrics for MT evaluation: Evaluating reorder-
ing. Machine Translation.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Association for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Shay Cohen and Noah Smith. 2009. Shared logistic
normal distributions for soft parameter tying in unsu-
pervised grammar induction. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the Association for
Computational Linguistics.
Chris Dyer and Philip Resnik. 2010. Context-free re-
ordering, finite-state translation. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of the Associa-
tion for Computational Linguistics.
Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.
2009. Dependency grammar induction via bitext pro-
jection constraints. In Proceedings of the Association
for Computational Linguistics.
202
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the Conference on Computa-
tional Linguistics.
Dan Klein and Christopher D. Manning. 2001. Natu-
ral language grammar induction using a constituent-
context model. In Proceedings of Neural Information
Processing Systems.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of the Association for Com-
putational Linguistics.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In Proceedings of the International Workshop
on Spoken Language Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Jonas Kuhn. 2004. Experiments in parallel-text based
grammar induction. In Proceedings of the Association
for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of the International Conference on Machine
Learning.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of
ACL Workshop on Statistical Machine Translation.
Young-Suk Lee, Bing Zhao, and Xiaoqiang Luo. 2010.
Constituent reordering and syntax models for English-
to-Japanese statistical machine translation. In Pro-
ceedings of the Conference on Computational Linguis-
tics.
Wolfgang Macherey, Franz Och, Ignacio Thayer, and
Jakob Uszkoreit. 2008. Lattice-based minimum er-
ror rate training for statistical machine translation. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och, Christopher Tillman, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the Association for Computational
Linguistics.
Slav Petrov and Dan Klein. 2008. Sparse multi-scale
grammars for discriminative latent variable parsing. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. Technical report.
Benjamin Snyder, Tahira Naseem, and Regina Barzilay.
2009. Unsupervised multilingual grammar induction.
In Proceedings of the Association for Computational
Linguistics.
David Talbot, Hideto Kazawa, Hiroshi Ichikawa, Ja-
son Katz-Brown, Masakazu Seno, and Franz J. Och.
2011. A lightweight evaluation framework for ma-
chine translation reordering. In Proceedings of the
Sixth Workshop on Statistical Machine Translation.
Roy Tromble. 2009. Learning linear ordering problems
for better translation. In Proceedings of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
the Association for Computational Linguistics.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of the Conference
on Computational Linguistics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics.
Fei Xia and Michael McCord. 2004. Improving a sta-
tistical mt system with automatically learned rewrite
patterns. In Proceedings of the Conference on Com-
putational Linguistics.
Peng Xu, Jaeho Kang, Michael Ringgard, and Franz Och.
2009. Using a dependency parser to improve smt for
subject-object-verb languages. In Proceedings of the
North American Chapter of the Association for Com-
putational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the As-
sociation for Computational Linguistics.
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu.
2011. Binarized forest to string translation. In Pro-
ceedings of the Association for Computational Lin-
guistics.
203
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 636?646,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Identifying Phrasal Verbs Using Many Bilingual Corpora
Karl Pichotta?
Department of Computer Science
University of Texas at Austin
pichotta@cs.utexas.edu
John DeNero
Google, Inc.
denero@google.com
Abstract
We address the problem of identifying mul-
tiword expressions in a language, focus-
ing on English phrasal verbs. Our poly-
glot ranking approach integrates frequency
statistics from translated corpora in 50 dif-
ferent languages. Our experimental eval-
uation demonstrates that combining statisti-
cal evidence from many parallel corpora us-
ing a novel ranking-oriented boosting algo-
rithm produces a comprehensive set of English
phrasal verbs, achieving performance compa-
rable to a human-curated set.
1 Introduction
A multiword expression (MWE), or noncomposi-
tional compound, is a sequence of words whose
meaning cannot be composed directly from the
meanings of its constituent words. These idiosyn-
cratic phrases are prevalent in the lexicon of a lan-
guage; Jackendoff (1993) estimates that their num-
ber is on the same order of magnitude as that of sin-
gle words, and Sag et al (2002) suggest that they
are much more common, though quantifying them
is challenging (Church, 2011). The task of identify-
ing MWEs is relevant not only to lexical semantics
applications, but also machine translation (Koehn et
al., 2003; Ren et al, 2009; Pal et al, 2010), informa-
tion retrieval (Xu et al, 2010; Acosta et al, 2011),
and syntactic parsing (Sag et al, 2002). Awareness
of MWEs has empirically proven useful in a num-
ber of domains: Finlayson and Kulkarni (2011), for
example, use MWEs to attain a significant perfor-
mance improvement in word sense disambiguation;
Venkatapathy and Joshi (2006) use features associ-
ated with MWEs to improve word alignment.
?Research conducted during an internship at Google.
We focus on a particular subset of MWEs, English
phrasal verbs. A phrasal verb consists of a head
verb followed by one or more particles, such that
the meaning of the phrase cannot be determined by
combining the simplex meanings of its constituent
words (Baldwin and Villavicencio, 2002; Dixon,
1982; Bannard et al, 2003).1 Examples of phrasal
verbs include count on [rely], look after [tend], or
take off [remove], the meanings of which do not in-
volve counting, looking, or taking. In contrast, there
are verbs followed by particles that are not phrasal
verbs, because their meaning is compositional, such
as walk towards, sit behind, or paint on.
We identify phrasal verbs by using frequency
statistics calculated from parallel corpora, consist-
ing of bilingual pairs of documents such that one
is a translation of the other, with one document in
English. We leverage the observation that a verb
will translate in an atypical way when occurring as
the head of a phrasal verb. For example, the word
look in the context of look after will tend to trans-
late differently from how look translates generally.
In order to characterize this difference, we calculate
a frequency distribution over translations of look,
then compare it to the distribution of translations of
look when followed by the word after. We expect
that idiomatic phrasal verbs will tend to have unex-
pected translation of their head verbs, measured by
the Kullback-Leibler divergence between those dis-
tributions.
Our polyglot ranking approach is motivated by the
hypothesis that using many parallel corpora of dif-
ferent languages will help determine the degree of
semantic idiomaticity of a phrase. In order to com-
1Nomenclature varies: the term verb-particle construction
is also used to denote what we call phrasal verbs; further, the
term phrasal verb is sometimes used to denote a broader class
of constructions.
636
bine evidence from multiple languages, we develop
a novel boosting algorithm tailored to the task of
ranking multiword expressions by their degree of id-
iomaticity. We train and evaluate on disjoint subsets
of the phrasal verbs in English Wiktionary2. In our
experiments, the set of phrasal verbs identified au-
tomatically by our method achieves held-out recall
that nears the performance of the phrasal verbs in
WordNet 3.0, a human-curated set. Our approach
strongly outperforms a monolingual system, and
continues to improve when incrementally adding
translation statistics for 50 different languages.
2 Identifying Phrasal Verbs
The task of identifying phrasal verbs using corpus
information raises several issues of experimental de-
sign. We consider four central issues below in moti-
vating our approach.
Types vs. Tokens. When a phrase is used in con-
text, it takes a particular meaning among its pos-
sible senses. Many phrasal verbs admit composi-
tional senses in addition to idiomatic ones?contrast
idiomatic ?look down on him for his politics? with
compositional ?look down on him from the balcony.?
In this paper, we focus on the task of determining
whether a phrase type is a phrasal verb, meaning that
it frequently expresses an idiomatic meaning across
its many token usages in a corpus. We do not at-
tempt to distinguish which individual phrase tokens
in the corpus have idiomatic senses.
Ranking vs. Classification. Identifying phrasal
verbs involves relative, rather than categorical, judg-
ments: some phrasal verbs are more compositional
than others, but retain a degree of noncomposition-
ality (McCarthy et al, 2003). Moreover, a poly-
semous phrasal verb may express an idiosyncratic
sense more or less often than a compositional sense
in a particular corpus. Therefore, we should expect
a corpus-driven system not to classify phrases as
strictly idiomatic or compositional, but instead as-
sign a ranking or relative scoring to a set of candi-
dates.
Candidate Phrases. We distinguish between the
task of identifying candidate multiword expressions
2http://en.wiktionary.org
Feature Description
?L (?50) KL Divergence for each language L
?1 frequency of phrase given verb
?2 PMI of verb and particles
?3 ?1 with interposed pronouns
Table 1: Features used by the polyglot ranking system.
and the task of ranking those candidates by their se-
mantic idiosyncracy. With English phrasal verbs, it
is straightforward to enumerate all desired verbs fol-
lowed by one or more particles, and rank the entire
set.
Using Parallel Corpora. There have been a num-
ber of approaches proposed for the use of multilin-
gual resources for MWE identification (Melamed,
1997; Villada Moiro?n and Tiedemann, 2006; Caseli
et al, 2010; Tsvetkov and Wintner, 2012; Salehi
and Cook, 2013). Our approach differs from pre-
vious work in that we identify MWEs using transla-
tion distributions of verbs, as opposed to 1?1, 1?m,
or m?n word alignments, most-likely translations,
bilingual dictionaries, or distributional entropy. To
the best of our knowledge, ours is the first approach
to use translational distributions to leverage the ob-
servation that a verb typically translates differently
when it heads a phrasal verb.
3 The Polyglot Ranking Approach
Our approach uses bilingual and monolingual statis-
tics as features, computed over unlabeled corpora.
Each statistic characterizes the degree of idiosyn-
crasy of a candidate phrasal verb, using a single
monolingual or bilingual corpus. We combine fea-
tures for many language pairs using a boosting algo-
rithm that optimizes a ranking objective using a su-
pervised training set of English phrasal verbs. Each
of these aspects of our approach is described in de-
tail below; for reference, Table 1 provides a list of
the features used.
3.1 Bilingual Statistics
One of the intuitive properties of an MWE is that
its individual words likely do not translate literally
when the whole expression is translated into another
language (Melamed, 1997). We capture this effect
637
by measuring the divergence between how a verb
translates generally and how it translates when head-
ing a candidate phrasal verb.
A parallel corpus is a collection of document
pairs ?DE , DF ?, where DE is in English, DF is in
another language, one document is a translation of
the other, and all documents DF are in the same
language. A phrase-aligned parallel corpus aligns
those documents at a sentence, phrase, and word
level. A phrase e aligns to another phrase f if some
word in e aligns to some word in f and no word in
e or f aligns outside of f or e, respectively. As a
result of this definition, the words within an aligned
phrase pair are themselves connected by word-level
alignments.
Given an English phrase e, define F (e) to be the
set of all foreign phrases observed aligned to e in a
parallel corpus. For any f ? F (e), let P (f |e) be the
conditional probability of the phrase e translating to
the phrase f . This probability is estimated as the
relative frequency of observing f and e as an aligned
phrase pair, conditioned on observing e aligned to
any phrase in the corpus:
P (f |e) = N(e, f)?
f ? N(e, f ?)
with N(e, f) the number of times e and f are ob-
served occurring as an aligned phrase pair.
Next, we assign statistics to individual verbs
within phrases. The first word of a candidate phrasal
verb e is a verb. For a candidate phrasal verb e and
a foreign phrase f , let pi1(e, f) be the subphrase of
f that is most commonly word-aligned to the first
word of e. As an example, consider the phrase pair
e = talk down to and f = hablar con menosprecio.
Suppose that when e is aligned to f , the word talk is
most frequently aligned to hablar. Then pi1(e, f) =
hablar.
For a phrase e and its set F (e) of aligned trans-
lations, we define the constituent translation proba-
bility of a foreign subphrase x as:
Pe(x) =
?
f?F (e)
P (f |e) ? ? (pi1(e, f), x) (1)
where ? is the Kronecker delta function, taking value
1 if its arguments are equal and 0 otherwise. Intu-
itively, Pe assigns the probability mass for every f
to its subphrase most commonly aligned to the verb
in e. It expresses how this verb is translated in the
context of a phrasal verb construction.3 Equation (1)
defines a distribution over all phrases x of a foreign
language.
We also assign statistics to verbs as they are trans-
lated outside of the context of a phrase. Let v(e)
be the verb of a phrasal verb candidate e, which
is always its first word. For a single-word verb
phrase v(e), we can compute the constituent transla-
tion probability Pv(e)(x), again using Equation (1).
The difference between Pe(x) and Pv(e)(x) is that
the latter sums over all translations of the verb v(e),
regardless of whether it appears in the context of e:
Pv(e)(x) =
?
f?F (v(e))
P (f |v(e)) ? ? (pi1(v(e), f), x)
For a one-word phrase such as v(e), pi1(v(e), f)
is the subphrase of f that most commonly directly
word-aligns to the one word of v(e).
Finally, for a phrase e and its verb v(e), we calcu-
late the Kullback-Leibler (KL) divergence between
the translation distribution of v(e) and e:
DKL
(
Pv(e)?Pe
)
=
?
x
Pv(e)(x) ln
Pv(e)(x)
Pe(x)
(2)
where the sum ranges over all x such that Pv(e)(x) >
0. This quantifies the difference between the trans-
lations of e?s verb when it occurs in e, and when it
occurs in general. Figure 1 illustrates this computa-
tion on a toy corpus.
Smoothing. Equation (2) is defined only if, for ev-
ery x such that Pv(e)(x) > 0, it is also the case
that Pe(x) > 0. In order to ensure that this con-
dition holds, we smooth the translation distributions
toward uniform. Let D be the set of phrases with
non-zero probability under either distribution:
D = {x : Pv(e)(x) > 0 or Pe(x) > 0}
Then, let UD be the uniform distribution over D:
UD(x) =
{
1/|D| if x ? D
0 if x /? D
3To extend this statistic to other types of multiword expres-
sions, one could compute a similar distribution for other content
words in the phrase.
638
0%
10%
20%
30%
40%
50%
0 5 10 15 20 25 30 35 40 45 50
T
e
s
t
 
s
e
t
 
r
e
c
a
l
l
-
a
t
-
1
2
2
0
Number of languages (k)
Combined with AdaBoost
Individual Bilingual Statistics
looking forward to
mirando adelante a
looking forward to
deseando
looking
mirando
looking
buscando  a
3
1
5
3
Aligned Phrase Pair
N(e, f) ?1(e, f)
mirando
deseando
mirando
buscando
\begin{tabular}{rrrr}
         &\textit{mirando}   &\textit{deseando} &\textit{buscando} \\ [2ex]
$P_{v(e)}(x)$ &$\frac{5}{8}=0.625 $  &$0$       &$\frac{3}{8}=0.375 $ \\ [1ex]
\hline \\ [-1ex]
$P'_{v(e)}(x)$&$0.610     $         &$0.02$     &$0.373$ \\ [1ex]
\hline \\ [-1ex]
$P_e(x)$ &$\frac{3}{4}=0.75 $  &$\frac{1}{4}=0.25 $ &$0$ \\ [1.5ex]
\hline \\ [-1ex]
$P'_e(x)$&$0.729     $         &$0.254    $     &$0.02$ \\ [1ex]
\hline \\ [-1ex]
\end{tabular}
DKL(P 0v(ei)kP 0ei) =  0.109 + 0.045 + 1.159 = 1.005
D_{KL} (P'_{v(e_i)} \| P'_{e_i}) = -0.109 + -0.045 + 1.159 = 1.005
mirando deseando buscando
Pv(e)(x) 58 = 0.625 0 38 = 0.375
P 0v(e)(x) 0.610 0.02 0.373
Pe(x) 34 = 0.75 14 = 0.25 0
P 0e(x) 0.729 0.254 0.02
Figure 1: The computation of DKL(P ?v(ei)?P
?
ei) using a
toy corpus, for e = looking forward to. Note that the sec-
ond aligned phrase pair contains the third, so the second?s
count of 3 must be included in the third?s count of 5.
When computing divergence in Equation (2), we use
the smoothed distributions P ?e and P ?v(e):
P ?e(x) = ?Pe(x) + (1? ?)UD(x)
P ?v(e)(x) = ?Pv(e)(x) + (1? ?)UD(x).
We use ? = 0.95, which distributes 5% of the total
probability mass evenly among all events in D.
Morphology. We calculate statistics for morpho-
logical variants of an English phrase. For a candi-
date English phrasal verb e (for example, look up),
letE denote the set of inflections of that phrasal verb
(for look up, this will be [look|looks|looked|looking]
up). We extract the variants in E from the verb en-
tries in English Wiktionary. The final score com-
puted from a phrase-aligned parallel corpus translat-
ing English sentences into a language L is the aver-
age KL divergence of smoothed constituent transla-
tion distributions for any inflected form ei ? E:
?L(e) =
1
|E|
?
ei?E
DKL
(
P ?v(ei)?P
?
ei
)
3.2 Monolingual Statistics
We also collect a number of monolingual statistics
for each phrasal verb candidate, motivated by the
considerable body of previous work on the topic
(Church and Hanks, 1990; Lin, 1999; McCarthy et
al., 2003). The monolingual statistics are designed
to identify frequent collocations in a language. This
set of monolingual features is not comprehensive, as
we focus our attention primarily on bilingual fea-
tures in this paper.
As above, define E to be the set of morpholog-
ically inflected variants of a candidate e, and let
V be the set of inflected variants of the head verb
v(e) of e. We define three statistics calculated from
the phrase counts of a monolingual English corpus.
First, we define ?1(e) to be the relative frequency of
the candidate e, given e?s head verb, summed over
morphological variants:
?1(e) = lnP (E|V )
= ln
?
ei?E
N(ei)
?
vi?V
N(vi)
where N(x) is the number of times phrase x was
observed in the monolingual corpus.
Second, define ?2(e) to be the pointwise mutual
information (PMI) between V (the event that one of
the inflections of the verb in e is observed) and R,
the event of observing the rest of the phrase:
?2(e)
= PMI(V,R)
= lgP (V,R)? lg (P (V )P (R))
= lgP (E)? lg (P (V )P (R))
= lg
?
ei?E
N(ei)?lg
?
vi?V
N(vi)?lgN(r)+lgN
where N is the total number of tokens in the corpus,
and logarithms are base-2. This statistic character-
izes the degree of association between a verb and
its phrasal extension. We only calculate ?2 for two-
word phrases, as it did not prove helpful for longer
phrases.
639
Finally, define ?3(e) to be the relative frequency
of the phrasal verb e augmented by an accusative
pronoun, conditioned on the verb. Let A be the
set of phrases in E with an accusative pronoun (it,
them, him, her, me, you) optionally inserted either at
the end of the phrase or directly after the verb. For
e = look up, A = {look up, look X up, look up X,
looks up, looks X up, looks up X, . . . }, with X an
accusative pronoun. The ?3 statistic is similar to ?1,
but allows for an intervening or following pronoun:
?3(e) = lnP (A|V )
= ln
?
ei?A
N(ei)
?
vi?V
N(vi)
.
This statistic is designed to exploit the intuition that
phrasal verbs frequently have accusative pronouns
either inserted into the middle (e.g. look it up) or at
the end (e.g. look down on him).
3.3 Ranking Phrasal Verb Candidates
Our goal is to assign a single real-valued score to
each candidate e, by which we can rank candidates
according to semantic idiosyncrasy. For each lan-
guage L for which we have a parallel corpus, we
defined, in section 3.1, a function ?L(e) assigning
real values to candidate phrasal verbs e, which we
hypothesize is higher on average for more idiomatic
compounds. Further, in section 3.2, we defined real-
valued monolingual functions ?1, ?2, and ?3 for
which we hypothesize the same trend holds. Be-
cause each score individually ranks all candidates,
it is natural to view each ?L and ?i as a weak rank-
ing function that we can combine with a supervised
boosting objective. We use a modified version of
AdaBoost (Freund and Schapire, 1995) that opti-
mizes for recall.
For each ?L and ?i, we compute a ranked list
of candidate phrasal verbs, ordered from highest to
lowest value. To simplify learning, we consider only
the top 5000 candidate phrasal verbs according to
?1, ?2, and ?3. This pruning procedure excludes
candidates that do not appear in our monolingual
corpus.
We optimize the ranker using an unranked, in-
complete training set of phrasal verbs. We can eval-
uate the quality of the ranker by outputting the top
N ranked candidates and measuring recall relative
Algorithm 1 Recall-Oriented Ranking AdaBoost
1: for i = 1 : |X| do
2: w[i]? 1/|X|
3: end for
4: for t = 1 : T do
5: for all h ? H do
6: h ? 0
7: for i = 1 : |X| do
8: if xi 6? h then
9: h ? h + w[i]
10: end if
11: end for
12: end for
13: ht ? argmaxh?H |B ? h|
14: ?t ? ln(B/ht)
15: for i = 1 : |X| do
16: if xi ? ht then
17: w[i]? 1Zw[i] exp (??t)
18: else
19: w[i]? 1Zw[i] exp (?t)
20: end if
21: end for
22: end for
to this gold-standard training set. We choose this
recall-at-N metric so as to not directly penalize pre-
cision errors, as our training set is incomplete.
DefineH to be the set of N -element sets contain-
ing the top proposals for each weak ranker (we use
N = 2000). That is, each element ofH is a set con-
taining the 2000 highest values for some ?L or ?i.
We define the baseline error B to be 1?E[R], with
R the recall-at-N of a ranker ordering the candidate
phrases in the set ?H at random. The value E[R] is
estimated by averaging the recall-at-N of 1000 ran-
dom orderings of ?H.
Algorithm 1 gives the formulation of the Ada-
Boost training algorithm that we use to combine
weak rankers. The algorithm maintains a weight
vector w (summing to 1) which contains a positive
real number for each gold standard phrasal verb in
the training set X . Initially, w is uniformly set to
1/|X|. At each iteration of the algorithm, w is mod-
ified to take higher values for recently misclassi-
fied examples. We repeatedly choose weak rankers
ht ? H (and corresponding real-valued coefficients
?t) that correctly rank examples with high w values.
640
Lines 5?12 of Algorithm 1 calculate the weighted
error values h for every weak ranker set h ? H.
The error h will be 1 if h contains none of X and 0
if h contains all of X , as w always sums to 1. Line
13 picks the ranker ht ? H whose weighted error is
as far as possible from the random baseline error B .
Line 14 calculates a coefficient ?t for ht, which will
be positive if ht < B and negative if ht > B .
Intuitively, ?t encodes the importance of ht?it will
be high if ht performs well, and low if it performs
poorly. The Z in lines 17 and 19 is the normalizing
constant ensuring the vector w sums to 1.
After termination of Algorithm 1, we have
weights ?1, . . . , ?T and lists h1, . . . , hT . Define ft
as the function that generated the list ht (each ft will
be some ?L or ?i). Now, we define a final combined
function ?, taking a phrase e and returning a real
number:
?(e) =
T?
t=1
?tft(e).
We standardize the scores of individual weak
rankers to have mean 0 and variance 1, so that their
scores are comparable.
The final learned ranker outputs a real value, in-
stead of the class labels frequently found in Ada-
Boost. This follows previous work using boosting
for learning to rank (Freund et al, 2003; Xu and Li,
2007). Our algorithm differs from previous methods
because we are seeking to optimize for Recall-at-N ,
rather than a ranking loss.
4 Experimental Evaluation
4.1 Training and Test Set
In order to train and evaluate our system, we con-
struct a gold-standard list of phrasal verbs from
the freely available English Wiktionary. We gather
phrasal verbs from three sources within Wiktionary:
1. Entries labeled as English phrasal verbs4,
2. Entries labeled as English idioms5, and
3. The derived terms6 of English verb entries.
4http://en.wiktionary.org/wiki/Category:
English_phrasal_verbs
5http://en.wiktionary.org/wiki/Category:
English_idioms
6For example, see http://en.wiktionary.org/
wiki/take#Derived_terms
about across after against along
among around at before behind
between beyond by down for
from in into like off
on onto outside over past
round through to towards under
up upon with within without
Table 2: Particles and prepositions allowed in phrasal
verbs gathered from Wiktionary.
Many of the idioms and derived terms are not
phrasal verbs (e.g. kick the bucket, make-or-break).
We filter out any phrases not of the form V P+, with
V a verb, and P+ denoting one or more occurrences
of particles and prepositions from the list in Table 2.
We omit prepositions that do not productively form
English phrasal verbs, such as amid and as. This
process also omits some compounds that are some-
times called phrasal verbs, such as light verb con-
structions, e.g. have a go (Butt, 2003), and noncom-
positional verb-adverb collocations, e.g. look for-
ward.
There are a number of extant phrasal verb cor-
pora. For example, McCarthy et al (2003) present
graded human compositionality judgments for 116
phrasal verbs, and Baldwin (2008) presents a large
set of candidates produced by an automated system,
with false positives manually removed. We use Wik-
tionary instead, in an attempt to construct a maxi-
mally comprehensive data set that is free from any
possible biases introduced by automatic extraction
processes.
4.2 Filtering and Data Partition
The merged list of phrasal verbs extracted from Wik-
tionary included some common collocations that
have compositional semantics (e.g. know about), as
well as some very rare constructions (e.g. cheese
down). We removed these spurious results system-
atically by filtering out very frequent and very infre-
quent entries. First, we calculated the log probability
of each phrase, according to a language model built
from a large monolingual corpus of news documents
and web documents, smoothed with stupid back-
off (Brants et al, 2007). We sorted all Wiktionary
phrasal verbs according to this value. Then, we se-
lected the contiguous 75% of the sorted phrases that
minimize the variance of this statistic. This method
641
Recall-at-1220
Dev Test
Frequent Candidates 17.0 19.3
B
as
el
in
e
WordNet 3.0 Frequent 41.6 43.7
WordNet 3.0 Filtered 49.4 48.8
Monolingual Only 30.1 30.2
B
oo
st
ed
Bilingual Only 47.1 43.9
Monolingual+Bilingual 50.8 47.9
Table 3: Our boosted ranker combining monolingual
and bilingual features (bottom) compared to three base-
lines (top) gives comparable performance to the human-
curated upper bound.
removed a few very frequent phrases and a large
number of rare phrases. The remaining phrases were
split randomly into a development set of 694 items
and a held-out test set of 695 items.
4.3 Corpora
Our monolingual English corpus consists of news ar-
ticles and documents collected from the web. Our
parallel corpora from English to each of 50 lan-
guages also consist of documents collected from
the web via distributed data mining of parallel doc-
uments based on the text content of web pages
(Uszkoreit et al, 2010).
The parallel corpora were segmented into aligned
sentence pairs and word-aligned using two iterations
of IBM Model 1 (Brown et al, 1993) and two iter-
ations of the HMM-based alignment model (Vogel
et al, 1996) with posterior symmetrization (Liang et
al., 2006). This training recipe is common in large-
scale machine translation systems.
4.4 Generating Candidates
To generate the set of candidate phrasal verbs con-
sidered during evaluation, we exhaustively enumer-
ated the Cartesian product of all verbs present in the
previously described Wiktionary set (V), all parti-
cles in Table 2 (P) and a small set of second parti-
cles T = {with, to, on, }, with  the empty string.
The set of candidate phrasal verbs we consider dur-
ing evaluation is the product V ?P ?T , which con-
tains 96,880 items.
4.5 Results
We optimize a ranker using the boosting algorithm
described in section 3.3, using the features from Ta-
ble 1, optimizing performance on the Wiktionary de-
velopment set described in section 4.2. Monolingual
and bilingual statistics are calculated using the cor-
pora described in section 4.3, with candidate phrasal
verbs being drawn from the set described in section
4.4.
We evaluate our method of identifying phrasal
verbs by computing recall-at-N . This statistic is the
fraction of the Wiktionary test set that appears in the
top N proposed phrasal verbs by the method, where
N is an arbitrary number of top-ranked candidates
held constant when comparing different approaches
(we use N = 1220). We do not compute precision,
because the test set to which we compare is not an
exhaustive list of phrasal verbs, due to the develop-
ment/test split, frequency filtering, and omissions in
the original lexical resource. Proposing a phrasal
verb not in the test set is not necessarily an error, but
identifying many phrasal verbs from the test set is an
indication of an effective method. Recall-at-N is a
natural way to evaluate a ranking system where the
gold-standard data is an incomplete, unranked set.
Table 3 compares our approach to three baselines
using the Recall-at-1220 metric evaluated on both
the development and test sets. As a lower bound, we
evaluated the 1220 most frequent candidates in our
Monolingual corpus (Frequent Candidates).
As a competitive baseline, we evaluated the set of
phrasal verbs in WordNet 3.0 (Fellbaum, 1998). We
selected the most frequent 1220 out of 1781 verb-
particle constructions in WordNet (WordNet 3.0 Fre-
quent). A stronger baseline resulted from apply-
ing the same filtering procedure to WordNet that
we did to Wiktionary: sorting all verb-particle en-
tries by their language model score and retaining the
1220 consecutive entries that minimized language
model variance (WordNet 3.0 Filtered). WordNet
is a human-curated resource, and yet its recall-at-N
compared to our Wiktionary test set is only 48.8%,
indicating substantial divergence between the two
resources. Such divergence is typical: lexical re-
sources often disagree about what multiword expres-
sions to include (Lin, 1999).
The three final lines in Table 3 evaluate our
642
0%
10%
20%
30%
40%
50%
0 5 10 15 20 25 30 35 40 45 50
T
e
s
t
 
s
e
t
 
r
e
c
a
l
l
-
a
t
-
1
2
2
0
Number of languages (k)
Combined with AdaBoost
Individual Bilingual Statistics
Figure 2: The solid line shows recall-at-1220 when com-
bining the k best-performing bilingual statistics and three
monolingual statistics. The dotted line shows the indi-
vidual performance of the kth best-performing bilingual
statistic, when applied in isolation to rank candidates.
boosted ranker. Automatically detecting phrasal
verbs using monolingual features alone strongly out-
performed the frequency-based lower bound, but un-
derperformed the WordNet baseline. Bilingual fea-
tures, using features from 50 languages, proved sub-
stantially more effective. The combination of both
types of features yielded the best performance, out-
performing the human-curated WordNet baseline on
the development set (on which our ranker was opti-
mized) and approaching its performance on the held-
out test set.
4.6 Feature Analysis
The solid line in Figure 2 shows the recall-at-1220
for a boosted ranker using all monolingual statistics
and k bilingual statistics, for increasing k. Bilin-
gual statistics are added according to their individual
recall, from best-performing to worst. That is, the
point at k = 0 uses only ?1, ?2, and ?3, the point at
k = 1 adds the best individually-performing bilin-
gual statistic (Spanish) as a weak ranker, the next
point adds the second-best bilingual statistic (Ger-
man), etc. Boosting maximizes performance on the
development set, and evaluation is performed on the
test set. We use T = 53 (equal to the total number
of weak rankers).
Recall-at-1220
Dev Test
Bilingual only 47.1 43.9
Bilingual+?1 48.1 46.9
Bilingual+?2 50.1 48.3
Bilingual+?3 48.4 46.3
Bilingual+?1 + ?2 50.2 47.9
Bilingual+?1 + ?3 49.0 47.4
Bilingual+?2 + ?3 50.4 49.4
Bilingual+?1 + ?2 + ?3 50.8 47.9
Table 4: An ablation of monolingual statistics shows that
they are useful in addition to the 50 bilingual statistics
combined, and no single statistic provides maximal per-
formance.
The dotted line in Figure 2 shows that individual
bilingual statistics have recall-at-1220 ranging from
34.4% to 5.0%. This difference reflects the differ-
ent sizes of parallel corpora and usefulness of dif-
ferent languages in identifying English semantic id-
iosyncrasy. Combining together the signal of mul-
tiple languages is clearly beneficial, and including
many low-performing languages still offers overall
improvements.
Table 4 shows the effect of adding different sub-
sets of the monolingual statistics to the set of all
50 bilingual statistics. Monolingual statistics give
a performance improvement of up to 5.5% recall
on the test set, but the comparative behavior of the
various combinations of the ?i is somewhat unpre-
dictable when training on the development set and
evaluating on the test set. The pointwise mutual in-
formation of a verb and its particles (?2) appears to
be the most useful feature. In fact, the test set per-
formance of using ?2 alone outperforms the combi-
nation of all three. The best combination even out-
performs the WordNet 3.0 baseline on the test set,
though optimizing on the development set would not
select this model.
4.7 Error Analysis
Table 5 shows the 100 highest ranked phrasal verb
candidates by our system that do not appear in either
the development or test sets. Most of these candi-
dates are in fact English phrasal verbs that happened
to be missing from Wiktionary; some are present
in Wiktionary but were removed from the reference
643
pick up pat on tap into fit for charge with suit against
catch up burst into muck up haul up give up get off
get through get up get in tack on buzz about do like
plump for haul in keep up with strap on catch up with suck into
get round chop off slap on pitch into get into inquire into
drop behind get on catch up on pass on cue from carry around
get around get over shoot at pick over shoot by shoot in
make up to get past cast down set up with rule off hand round
piss on hit by break down move for lead off pluck off
flip through edge over strike off plug into keep up go past
set off pull round see about stay on put up sidle up to
buzz around take off set up slap in head towards shoot past
inquire for tuck up lie with well before go on with reel from
drive along snap off barge into whip on put down instance through
bar from cut down on let in tune in to move off suit in
lean against well beyond get down to go across sail into lie over
hit with chow down on look after catch at
Table 5: The highest ranked phrasal verb candidates from our full system that do not appear in either Wiktionary set.
Candidates are presented in decreasing rank; ?pat on? is the second highest ranked candidate.
sets during filtering, and the remainder are in fact
not phrasal verbs (true precision errors).
These errors fall largely into two categories.
Some candidates are compositional, but contain pol-
ysemous verbs, such as hit by, drive along, and head
towards. In these cases, prepositions disambiguate
the verb, which naturally affects translation distri-
butions. Other candidates are not phrasal verbs, but
instead phrases that tend to have a different syntac-
tic role, such as suit against, instance through, fit
for, and lie over (conjugated as lay over). A care-
ful treatment of part-of-speech tags when computing
corpus statistics might address this issue.
5 Related Work
The idea of using word-aligned parallel corpora
to identify idiomatic expressions has been pur-
sued in a number of different ways. Melamed
(1997) tests candidate MWEs by collapsing them
into single tokens, training a new translation model
with these tokens, and using the performance of
the new model to judge candidates? noncomposi-
tionality. Villada Moiro?n and Tiedemann (2006)
use word-aligned parallel corpora to identify Dutch
MWEs, testing the assumption that the distributions
of alignments of MWEs will generally have higher
entropies than those of fully compositional com-
pounds. Caseli et al (2010) generate candidate mul-
tiword expressions by picking out sufficiently com-
mon phrases that align to single target-side tokens.
Tsvetkov and Wintner (2012) generate candidate
MWEs by finding one-to-one alignments in paral-
lel corpora which are not in a bilingual dictionary,
and ranking them based on monolingual statistics.
The system of Salehi and Cook (2013) is perhaps
the closest to the current work, judging noncompo-
sitionality using string edit distance between a can-
didate phrase?s automatic translation and its com-
ponents? individual translations. Unlike the current
work, their method does not use distributions over
translations or combine individual bilingual values
with boosting; however, they find, as we do, that in-
corporating many languages is beneficial to MWE
identification.
A large body of work has investigated the identifi-
cation of noncompositional compounds from mono-
lingual sources (Lin, 1999; Schone and Jurafsky,
2001; Fazly and Stevenson, 2006; McCarthy et
al., 2003; Baldwin et al, 2003; Villavicencio,
2003). Many of these monolingual statistics could
be viewed as weak rankers and fruitfully incorpo-
rated into our framework.
There has also been a substantial amount of work
addressing the problem of differentiating between
literal and idiomatic instances of phrases in con-
text (Katz and Giesbrecht, 2006; Li et al, 2010;
644
Sporleder and Li, 2009; Birke and Sarkar, 2006;
Diab and Bhutada, 2009). We do not attempt this
task; however, techniques for token identification
could be used to improve type identification (Bald-
win, 2005).
6 Conclusion
We have presented the polyglot ranking approach
to phrasal verb identification, using parallel corpora
from many languages to identify phrasal verbs. We
proposed an evaluation metric that acknowledges the
inherent incompleteness of reference sets, but dis-
tinguishes among competing systems in a manner
aligned to the goals of the task. We developed a
recall-oriented learning method that integrates mul-
tiple weak ranking signals, and demonstrated exper-
imentally that combining statistical evidence from a
large number of bilingual corpora, as well as from
monolingual corpora, produces the most effective
system overall. We look forward to generalizing
our approach to other types of noncompositional
phrases.
Acknowledgments
Special thanks to Ivan Sag, who argued for the
importance of handling multi-word expressions in
natural language processing applications, and who
taught the authors about natural language syntax
once upon a time. We would also like to thank the
anonymous reviewers for their helpful suggestions.
References
Otavio Acosta, Aline Villavicencio, and Viviane Moreira.
2011. Identification and treatment of multiword ex-
pressions applied to information retrieval. In Proceed-
ings of the ACL Workshop on Multiword Expressions.
Timothy Baldwin and Aline Villavicencio. 2002. Ex-
tracting the unextractable: A case study on verb-
particles. In Proceedings of the Sixth Conference on
Natural Language Learning.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL Workshop on Multiword Expressions.
Timothy Baldwin. 2005. Deep lexical acquisition of
verb-particle constructions. Computer Speech & Lan-
guage, Special Issue on Multiword Expressions.
Timothy Baldwin. 2008. A resource for evaluating the
deep lexical acquisition of english verb-particle con-
structions. In Proceedings of the LREC Workshop To-
wards a Shared Task for Multiword Expressions.
Colin Bannard, Timothy Baldwin, and Alex Lascarides.
2003. A statistical approach to the semantics of verb-
particles. In Proceedings of the ACL Workshop on
Multiword Expressions.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for the nearly unsupervised recognition of non-
literal language. In Proceedings of European Chapter
of the Association for Computational Linguistics.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.
Och, and Jeffrey Dean. 2007. Large language mod-
els in machine translation. In Proceedings of the Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Miriam Butt. 2003. The light verb jungle. In Proceed-
ings of the Workshop on Multi-Verb Constructions.
Helena de Medeiros Caseli, Carlos Ramisch, Maria das
Grac?as Volpe Nunes, and Aline Villavicencio. 2010.
Alignment-based extraction of multiword expressions.
Language Resources and Evaluation.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational Linguistics, 16(1).
Kenneth Church. 2011. How many multiword expres-
sions do people know? In Proceedings of the ACL
Workshop on Multiword Expressions.
Mona T. Diab and Pravin Bhutada. 2009. Verb noun
construction MWE token supervised classification. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Robert Dixon. 1982. The grammar of english phrasal
verbs. Australian Journal of Linguistics.
Afsaneh Fazly and Suzanne Stevenson. 2006. Automat-
ically constructing a lexicon of verb phrase idiomatic
combinations. In Proceedings of the European Chap-
ter of the Association for Computational Linguistics.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. The MIT Press.
Mark Finlayson and Nidhi Kulkarni. 2011. Detecting
multiword expressions improves word sense disam-
biguation. In Proceedings of the ACL Workshop on
Multiword Expressions.
Yoav Freund and Robert E. Schapire. 1995. A decision-
theoretic generalization of on-line learning and an ap-
plication to boosting. In Proceedings of the Confer-
ence on Computational Learning Theory.
645
Yoav Freund, Raj Iyer, Robert E Schapire, and Yoram
Singer. 2003. An efficient boosting algorithm for
combining preferences. The Journal of Machine
Learning Research.
Ray Jackendoff. 1993. The Architecture of the Language
Faculty. MIT Press.
Graham Katz and Eugenie Giesbrecht. 2006. Auto-
matic identification of non-compositional multi-word
expressions using latent semantic analysis. In Pro-
ceedings of the ACL Workshop on Multiword Expres-
sions.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Linlin Li, Benjamin Roth, and Caroline Sporleder. 2010.
Topic models for word sense disambiguation and
token-based idiom detection. In Proceedings of the
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In Proceedings of the Asso-
ciation for Computational Linguistics.
Diana McCarthy, Bill Keller, and John Carroll. 2003.
Detecting a continuum of compositionality in phrasal
verbs. In Proceedings of the ACL Workshop on Multi-
word Expressions.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional compounds in parallel data. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Santanu Pal, Sudip Kumar Naskar, Pavel Pecina, Sivaji
Bandyopadhyay, and Andy Way. 2010. Handling
named entities and compound verbs in phrase-based
statistical machine translation. In Proceedings of the
COLING 2010 Workshop on Multiword Expressions.
Zhixiang Ren, Yajuan Lu, Jie Cao, Qun Liu, and Yun
Huang. 2009. Improving statistical machine transla-
tion using domain bilingual multiword expressions. In
Proceedings of the ACL Workshop on Multiword Ex-
pressions.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann A.
Copestake, and Dan Flickinger. 2002. Multiword ex-
pressions: A pain in the neck for NLP. In Proceedings
of the CICLING Conference on Intelligent Text Pro-
cessing and Computational Linguistics.
Bahar Salehi and Paul Cook. 2013. Predicting the com-
positionality of multiword expressions using transla-
tions in multiple languages. In Second Joint Confer-
ence on Lexical and Computational Semantics.
Patrick Schone and Daniel Jurafsky. 2001. Is
knowledge-free induction of multiword unit dictionary
headwords a solved problem? In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Caroline Sporleder and Linlin Li. 2009. Unsupervised
recognition of literal and non-literal use of idiomatic
expressions. In Proceedings of the European Chapter
of the Association for Computational Linguistics.
Yulia Tsvetkov and Shuly Wintner. 2012. Extraction
of multi-word expressions from small parallel corpora.
In Natural Language Engineering.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of the Conference
on Computational Linguistics.
Sriram Venkatapathy and Aravind K Joshi. 2006. Us-
ing information about multi-word expressions for the
word-alignment task. In Proceedings of the ACL
Workshop on Multiword Expressions.
Begon?a Villada Moiro?n and Jo?rg Tiedemann. 2006.
Identifying idiomatic expressions using automatic
word-alignment. In Proceedings of the EACL Work-
shop on Multiword Expressions in a Multilingual Con-
text.
Aline Villavicencio. 2003. Verb-particle constructions
and lexical resources. In Proceedings of the ACL
workshop on Multiword expressions.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
Jun Xu and Hang Li. 2007. AdaRank: a boosting al-
gorithm for information retrieval. In Proceedings of
the SIGIR Conference on Research and Development
in Information Retrieval.
Ying Xu, Randy Goebel, Christoph Ringlstetter, and
Grzegorz Kondrak. 2010. Application of the tightness
continuum measure to chinese information retrieval.
In Proceedings of the COLING Workshop on Multi-
word Expressions.
646
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582?590,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Painless Unsupervised Learning with Features
Taylor Berg-Kirkpatrick Alexandre Bouchard-Co?te? John DeNero Dan Klein
Computer Science Division
University of California at Berkeley
{tberg, bouchard, denero, klein}@cs.berkeley.edu
Abstract
We show how features can easily be added
to standard generative models for unsuper-
vised learning, without requiring complex
new training methods. In particular, each
component multinomial of a generative model
can be turned into a miniature logistic regres-
sion model if feature locality permits. The in-
tuitive EM algorithm still applies, but with a
gradient-based M-step familiar from discrim-
inative training of logistic regression mod-
els. We apply this technique to part-of-speech
induction, grammar induction, word align-
ment, and word segmentation, incorporating
a few linguistically-motivated features into
the standard generative model for each task.
These feature-enhanced models each outper-
form their basic counterparts by a substantial
margin, and even compete with and surpass
more complex state-of-the-art models.
1 Introduction
Unsupervised learning methods have been increas-
ingly successful in recent NLP research. The rea-
sons are varied: increased supplies of unlabeled
data, improved understanding of modeling methods,
additional choices of optimization algorithms, and,
perhaps most importantly for the present work, in-
corporation of richer domain knowledge into struc-
tured models. Unfortunately, that knowledge has
generally been encoded in the form of conditional
independence structure, which means that injecting
it is both tricky (because the connection between
independence and knowledge is subtle) and time-
consuming (because new structure often necessitates
new inference algorithms).
In this paper, we present a range of experiments
wherein we improve existing unsupervised models
by declaratively adding richer features. In particu-
lar, we parameterize the local multinomials of exist-
ing generative models using features, in a way which
does not require complex new machinery but which
still provides substantial flexibility. In the feature-
engineering paradigm, one can worry less about the
backbone structure and instead use hand-designed
features to declaratively inject domain knowledge
into a model. While feature engineering has his-
torically been associated with discriminative, super-
vised learning settings, we argue that it can and
should be applied more broadly to the unsupervised
setting.
The idea of using features in unsupervised learn-
ing is neither new nor even controversial. Many
top unsupervised results use feature-based mod-
els (Smith and Eisner, 2005; Haghighi and Klein,
2006). However, such approaches have presented
their own barriers, from challenging normalization
problems, to neighborhood design, to the need for
complex optimization procedures. As a result, most
work still focuses on the stable and intuitive ap-
proach of using the EM algorithm to optimize data
likelihood in locally normalized, generative models.
The primary contribution of this paper is to
demonstrate the clear empirical success of a sim-
ple and accessible approach to unsupervised learn-
ing with features, which can be optimized by us-
ing standard NLP building blocks. We consider
the same generative, locally-normalized models that
dominate past work on a range of tasks. However,
we follow Chen (2003), Bisani and Ney (2008), and
Bouchard-Co?te? et al (2008), and allow each com-
ponent multinomial of the model to be a miniature
multi-class logistic regression model. In this case,
the EM algorithm still applies with the E-step un-
changed. The M-step involves gradient-based train-
ing familiar from standard supervised logistic re-
gression (i.e., maximum entropy models). By inte-
grating these two familiar learning techniques, we
add features to unsupervised models without any
582
specialized learning or inference.
A second contribution of this work is to show that
further gains can be achieved by directly optimiz-
ing data likelihood with LBFGS (Liu et al, 1989).
This alternative optimization procedure requires no
additional machinery beyond what EM uses. This
approach is still very simple to implement, and we
found that it empirically outperforms EM.
This paper is largely empirical; the underlying op-
timization techniques are known, even if the overall
approach will be novel to many readers. As an em-
pirical demonstration, our results span an array of
unsupervised learning tasks: part-of-speech induc-
tion, grammar induction, word alignment, and word
segmentation. In each task, we show that declaring a
few linguistically motivated feature templates yields
state-of-the-art results.
2 Models
We start by explaining our feature-enhanced model
for part-of-speech (POS) induction. This particular
example illustrates our approach to adding features
to unsupervised models in a well-known NLP task.
We then explain how the technique applies more
generally.
2.1 Example: Part-of-Speech Induction
POS induction consists of labeling words in text
with POS tags. A hidden Markov model (HMM) is a
standard model for this task, used in both a frequen-
tist setting (Merialdo, 1994; Elworthy, 1994) and in
a Bayesian setting (Goldwater and Griffiths, 2007;
Johnson, 2007).
A POS HMM generates a sequence of words in
order. In each generation step, an observed word
emission yi and a hidden successor POS tag zi+1 are
generated independently, conditioned on the current
POS tag zi . This process continues until an absorb-
ing stop state is generated by the transition model.
There are two types of conditional distributions in
the model?emission and transition probabilities?
that are both multinomial probability distributions.
The joint likelihood factors into these distributions:
P?(Y = y,Z = z) = P?(Z1 = z1) ?
|z|
?
i=1
P?(Yi = yi|Zi = zi) ? P?(Zi+1 = zi+1|Zi = zi)
The emission distribution P?(Yi = yi|Zi = zi) is
parameterized by conditional probabilities ?y,z,EMIT
for each word y given tag z. Alternatively, we can
express this emission distribution as the output of a
logistic regression model, replacing the explicit con-
ditional probability table by a logistic function pa-
rameterized by weights and features:
?y,z,EMIT(w) =
exp ?w, f(y, z, EMIT)?
?
y? exp ?w, f(y?, z, EMIT)?
This feature-based logistic expression is equivalent
to the flat multinomial in the case that the feature
function f(y, z, EMIT) consists of all indicator fea-
tures on tuples (y, z, EMIT), which we call BASIC
features. The equivalence follows by setting weight
wy,z,EMIT = log(?y,z,EMIT).1 This formulation is
known as the natural parameterization of the multi-
nomial distribution.
In order to enhance this emission distribution, we
include coarse features in f(y, z, EMIT), in addi-
tion to the BASIC features. Crucially, these features
can be active across multiple (y, z) values. In this
way, the model can abstract general patterns, such
as a POS tag co-occurring with an inflectional mor-
pheme. We discuss specific POS features in Sec-
tion 4.
2.2 General Directed Models
Like the HMM, all of the models we propose are
based on locally normalized generative decisions
that condition on some context. In general, let X =
(Z,Y) denote the sequence of generation steps (ran-
dom variables) where Z contains all hidden random
variables and Y contains all observed random vari-
ables. The joint probability of this directed model
factors as:
Pw(X = x) =
?
i?I
Pw
(
Xi = xi
?
?Xpi(i) = xpi(i)
)
,
where Xpi(i) denotes the parents of Xi and I is the
index set of the variables in X.
In the models that we use, each factor in the above
expression is the output of a local logistic regression
1As long as no transition or emission probabilities are equal
to zero. When zeros are present, for instance to model that an
absorbing stop state can only transition to itself, it is often possi-
ble to absorb these zeros into a base measure. All the arguments
in this paper carry with a structured base measure; we drop it for
simplicity.
583
model parameterized by w:
Pw
`
Xi = d
?
?Xpi(i) = c
?
=
exp?w, f(d, c, t)?
P
d? exp?w, f(d?, c, t)?
Above, d is the generative decision value for Xi
picked by the model, c is the conditioning context
tuple of values for the parents of Xi, and t is the
type of decision being made. For instance, the POS
HMM has two types of decisions: transitions and
emissions. In the emission model, the type t is EMIT,
the decision d is a word and the context c is a tag.
The denominator normalizes the factor to be a prob-
ability distribution over decisions.
The objective function we derive from this model
is the marginal likelihood of the observations y,
along with a regularization term:
L(w) = log Pw(Y = y)? ?||w||22 (1)
This model has two advantages over the more preva-
lent form of a feature-rich unsupervised model, the
globally normalized Markov random field.2 First,
as we explain in Section 3, optimizing our objec-
tive does not require computing expectations over
the joint distribution. In the case of the POS HMM,
for example, we do not need to enumerate an in-
finite sum of products of potentials when optimiz-
ing, in contrast to Haghighi and Klein (2006). Sec-
ond, we found that locally normalized models em-
pirically outperform their globally normalized coun-
terparts, despite their efficiency and simplicity.
3 Optimization
3.1 Optimizing with Expectation Maximization
In this section, we describe the EM algorithm ap-
plied to our feature-rich, locally normalized models.
For models parameterized by standard multinomi-
als, EM optimizes L(?) = log P?(Y = y) (Demp-
ster et al, 1977). The E-step computes expected
counts for each tuple of decision d, context c, and
multinomial type t:
ed,c,t?E?
[
?
i?I
 (Xi =d, Xpi(i) =c, t)
?
?
?
?
Y = y
]
(2)
2The locally normalized model class is actually equivalent
to its globally normalized counterpart when the former meets
the following three conditions: (1) The graphical model is a
directed tree. (2) The BASIC features are included in f . (3) We
do not include regularization in the model (? = 0). This follows
from Smith and Johnson (2007).
These expected counts are then normalized in the
M-step to re-estimate ?:
?d,c,t ?
ed,c,t
?
d? ed?,c,t
Normalizing expected counts in this way maximizes
the expected complete log likelihood with respect to
the current model parameters.
EM can likewise optimize L(w) for our locally
normalized models with logistic parameterizations.
The E-step first precomputes multinomial parame-
ters from w for each decision, context, and type:
?d,c,t(w)?
exp?w, f(d, c, t)?
?
d? exp?w, f(d?, c, t)?
Then, expected counts e are computed accord-
ing to Equation 2. In the case of POS induction,
expected counts are computed with the forward-
backward algorithm in both the standard and logistic
parameterizations. The only change is that the con-
ditional probabilities ? are now functions of w.
The M-step changes more substantially, but still
relies on canonical NLP learning methods. We wish
to choose w to optimize the regularized expected
complete log likelihood:
?(w, e) =
?
d,c,t
ed,c,t log ?d,c,t(w)? ?||w||22 (3)
We optimize this objective via a gradient-based
search algorithm like LBFGS. The gradient with re-
spect to w takes the form
??(w, e) =
?
d,c,t
ed,c,t ??d,c,t(w)? 2? ?w (4)
?d,c,t(w) = f(d, c, t)?
?
d?
?d?,c,t(w)f(d?, c, t)
This gradient matches that of regularized logis-
tic regression in a supervised model: the differ-
ence ? between the observed and expected features,
summed over every decision and context. In the su-
pervised case, we would observe the count of occur-
rences of (d, c, t), but in the unsupervised M-step,
we instead substitute expected counts ed,c,t.
This gradient-based M-step is an iterative proce-
dure. For each different value of w considered dur-
ing the search, we must recompute ?(w), which re-
quires computation in proportion to the size of the
584
parameter space. However, e stays fixed throughout
the M-step. Algorithm 1 outlines EM in its entirety.
The subroutine climb(?, ?, ?) represents a generic op-
timization step such as an LBFGS iteration.
Algorithm 1 Feature-enhanced EM
repeat
Compute expected counts e  Eq. 2
repeat
Compute ?(w, e)  Eq. 3
Compute??(w, e)  Eq. 4
w ? climb(w, ?(w, e),??(w, e))
until convergence
until convergence
3.2 Direct Marginal Likelihood Optimization
Another approach to optimizing Equation 1 is to
compute the gradient of the log marginal likelihood
directly (Salakhutdinov et al, 2003). The gradient
turns out to have the same form as Equation 4, with
the key difference that ed,c,t is recomputed for every
different value of w. Algorithm 2 outlines the proce-
dure. Justification for this algorithm appears in the
Appendix.
Algorithm 2 Feature-enhanced direct gradient
repeat
Compute expected counts e  Eq. 2
Compute L(w)  Eq. 1
Compute ??(w, e)  Eq. 4
w ? climb(w, L(w),??(w, e))
until convergence
In practice, we find that this optimization ap-
proach leads to higher task accuracy for several
models. However, in cases where computing ed,c,t
is expensive, EM can be a more efficient alternative.
4 Part-of-Speech Induction
We now describe experiments that demonstrate the
effectiveness of locally normalized logistic models.
We first use the bigram HMM described in Sec-
tion 2.1 for POS induction, which has two types of
multinomials. For type EMIT, the decisions d are
words and contexts c are tags. For type TRANS, the
decisions and contexts are both tags.
4.1 POS Induction Features
We use the same set of features used by Haghighi
and Klein (2006) in their baseline globally normal-
ized Markov random field (MRF) model. These are
all coarse features on emission contexts that activate
for words with certain orthographic properties. We
use only the BASIC features for transitions. For
an emission with word y and tag z, we use the
following feature templates:
BASIC:  (y = ?, z = ?)
CONTAINS-DIGIT: Check if y contains digit and conjoin
with z:
 (containsDigit(y) = ?, z = ?)
CONTAINS-HYPHEN:  (containsHyphen(x) = ?, z = ?)
INITIAL-CAP: Check if the first letter of y is
capitalized:  (isCap(y) = ?, z = ?)
N-GRAM: Indicator functions for character n-
grams of up to length 3 present in y.
4.2 POS Induction Data and Evaluation
We train and test on the entire WSJ tag corpus (Mar-
cus et al, 1993). We attempt the most difficult ver-
sion of this task where the only information our sys-
tem can make use of is the unlabeled text itself. In
particular, we do not make use of a tagging dictio-
nary. We use 45 tag clusters, the number of POS tags
that appear in the WSJ corpus. There is an identifi-
ability issue when evaluating inferred tags. In or-
der to measure accuracy on the hand-labeled corpus,
we map each cluster to the tag that gives the highest
accuracy, the many-1 evaluation approach (Johnson,
2007). We run all POS induction models for 1000
iterations, with 10 random initializations. The mean
and standard deviation of many-1 accuracy appears
in Table 1.
4.3 POS Induction Results
We compare our model to the basic HMM and a bi-
gram version of the feature-enhanced MRF model of
Haghighi and Klein (2006). Using EM, we achieve
a many-1 accuracy of 68.1. This outperforms the
basic HMM baseline by a 5.0 margin. The same
model, trained using the direct gradient approach,
achieves a many-1 accuracy of 75.5, outperforming
the basic HMM baseline by a margin of 12.4. These
results show that the direct gradient approach can of-
fer additional boosts in performance when used with
a feature-enhanced model. We also outperform the
585
globally normalized MRF, which uses the same set
of features and which we train using a direct gradi-
ent approach.
To the best of our knowledge, our system achieves
the best performance to date on the WSJ corpus for
totally unsupervised POS tagging.3
5 Grammar Induction
We next apply our technique to a grammar induction
task: the unsupervised learning of dependency parse
trees via the dependency model with valence (DMV)
(Klein and Manning, 2004). A dependency parse is
a directed tree over tokens in a sentence. Each edge
of the tree specifies a directed dependency from a
head token to a dependent, or argument token. Thus,
the number of dependencies in a parse is exactly the
number of tokens in the sentence, not counting the
artificial root token.
5.1 Dependency Model with Valence
The DMV defines a probability distribution over de-
pendency parse trees. In this head-outward attach-
ment model, a parse and the word tokens are derived
together through a recursive generative process. For
each token generated so far, starting with the root, a
set of left dependents is generated, followed by a set
of right dependents.
There are two types of multinomial distributions
in this model. The Bernoulli STOP probabilities
?d,c,STOP capture the valence of a particular head. For
this type, the decision d is whether or not to stop
generating arguments, and the context c contains the
current head h, direction ? and adjacency adj. If
a head?s stop probability is high, it will be encour-
aged to accept few arguments. The ATTACH multi-
nomial probability distributions ?d,c,ATTACH capture
attachment preferences of heads. For this type, a de-
cision d is an argument token a, and the context c
consists of a head h and a direction ?.
We take the same approach as previous work
(Klein and Manning, 2004; Cohen and Smith, 2009)
and use gold POS tags in place of words.
3Haghighi and Klein (2006) achieve higher accuracies by
making use of labeled prototypes. We do not use any external
information.
5.2 Grammar Induction Features
One way to inject knowledge into a dependency
model is to encode the similarity between the vari-
ous morphological variants of nouns and verbs. We
encode this similarity by incorporating features into
both the STOP and the ATTACH probabilities. The
attachment features appear below; the stop feature
templates are similar and are therefore omitted.
BASIC:  (a = ?, h = ?, ? = ?)
NOUN: Generalize the morphological variants of
nouns by using isNoun(?):
 (a = ?, isNoun(h) = ?, ? = ?)
 (isNoun(a) = ?, h = ?, ? = ?)
 (isNoun(a) = ?, isNoun(h) = ?, ? = ?)
VERB: Same as above, generalizing verbs instead
of nouns by using isVerb(?)
NOUN-VERB: Same as above, generalizing with
isVerbOrNoun(?) = isVerb(?)? isNoun(?)
BACK-OFF: We add versions of all other features that
ignore direction or adjacency.
While the model has the expressive power to al-
low specific morphological variants to have their
own behaviors, the existence of coarse features en-
courages uniform analyses, which in turn gives bet-
ter accuracies.
Cohen and Smith?s (2009) method has similar
characteristics. They add a shared logistic-normal
prior (SLN) to the DMV in order to tie multinomial
parameters across related derivation events. They
achieve their best results by only tying parame-
ters between different multinomials when the cor-
responding contexts are headed by nouns and verbs.
This observation motivates the features we choose to
incorporate into the DMV.
5.3 Grammar Induction Data and Evaluation
For our English experiments we train and report di-
rected attachment accuracy on portions of the WSJ
corpus. We work with a standard, reduced version of
WSJ, WSJ10, that contains only sentences of length
10 or less after punctuation has been removed. We
train on sections 2-21, and use section 22 as a de-
velopment set. We report accuracy on section 23.
These are the same training, development, and test
sets used by Cohen and Smith (2009). The regular-
ization parameter (?) is tuned on the development
set to maximize accuracy.
For our Chinese experiments, we use the same
corpus and training/test split as Cohen and Smith
586
(2009). We train on sections 1-270 of the Penn Chi-
nese Treebank (Xue et al, 2002), similarly reduced
(CTB10). We test on sections 271-300 of CTB10,
and use sections 400-454 as a development set.
The DMV is known to be sensitive to initializa-
tion. We use the deterministic harmonic initializer
from Klein and Manning (2004). We ran each op-
timization procedure for 100 iterations. The results
are reported in Table 1.
5.4 Grammar Induction Results
We are able to outperform Cohen and Smith?s (2009)
best system, which requires a more complicated
variational inference method, on both English and
Chinese data sets. Their system achieves an accu-
racy of 61.3 for English and an accuracy of 51.9 for
Chinese.4 Our feature-enhanced model, trained us-
ing the direct gradient approach, achieves an accu-
racy of 63.0 for English, and an accuracy of 53.6 for
Chinese. To our knowledge, our method for feature-
based dependency parse induction outperforms all
existing methods that make the same set of condi-
tional independence assumptions as the DMV.
6 Word Alignment
Word alignment is a core machine learning com-
ponent of statistical machine translation systems,
and one of the few NLP tasks that is dominantly
solved using unsupervised techniques. The pur-
pose of word alignment models is to induce a cor-
respondence between the words of a sentence and
the words of its translation.
6.1 Word Alignment Models
We consider two classic generative alignment mod-
els that are both used heavily today, IBM Model 1
(Brown et al, 1994) and the HMM alignment model
(Ney and Vogel, 1996). These models generate a
hidden alignment vector z and an observed foreign
sentence y, all conditioned on an observed English
sentence e. The likelihood of both models takes the
form:
P (y, z|e) =
?
j
p(zj = i|zj?1) ? ?yj ,ei,ALIGN
4Using additional bilingual data, Cohen and Smith (2009)
achieve an accuracy of 62.0 for English, and an accuracy of
52.0 for Chinese, still below our results.
Model Inference Reg Eval
POS Induction ? Many-1
W
SJ
Basic-HMM EM ? 63.1 (1.3)
Feature-MRF LBFGS 0.1 59.6 (6.9)
Feature-HMM EM 1.0 68.1 (1.7)
LBFGS 1.0 75.5 (1.1)
Grammar Induction ? Dir
W
SJ
10
Basic-DMV EM ? 47.8
Feature-DMV EM 0.05 48.3
LBFGS 10.0 63.0
(Cohen and Smith, 2009) 61.3
C
T
B
10
Basic-DMV EM ? 42.5
Feature-DMV EM 1.0 49.9
LBFGS 5.0 53.6
(Cohen and Smith, 2009) 51.9
Word Alignment ? AER
N
IS
T
C
hE
n Basic-Model 1 EM ? 38.0
Feature-Model 1 EM ? 35.6
Basic-HMM EM ? 33.8
Feature-HMM EM ? 30.0
Word Segmentation ? F1
B
R
Basic-Unigram EM ? 76.9 (0.1)
Feature-Unigram EM 0.2 84.5 (0.5)
LBFGS 0.2 88.0 (0.1)
(Johnson and Goldwater, 2009) 87
Table 1: Locally normalized feature-based models outperform
all proposed baselines for all four tasks. LBFGS outperformed
EM in all cases where the algorithm was sufficiently fast to run.
Details of each experiment appear in the main text.
The distortion term p(zj = i|zj?1) is uniform in
Model 1, and Markovian in the HMM. See Liang et
al. (2006) for details on the specific variant of the
distortion model of the HMM that we used. We use
these standard distortion models in both the baseline
and feature-enhanced word alignment systems.
The bilexical emission model ?y,e,ALIGN differen-
tiates our feature-enhanced system from the base-
line system. In the former, the emission model is a
standard conditional multinomial that represents the
probability that decision word y is generated from
context word e, while in our system, the emission
model is re-parameterized as a logistic regression
model and feature-enhanced.
Many supervised feature-based alignment models
have been developed. In fact, this logistic parame-
terization of the HMM has been proposed before and
yielded alignment improvements, but was trained
using supervised estimation techniques (Varea et al,
2002).5 However, most full translation systems to-
5Varea et al (2002) describes unsupervised EM optimiza-
tion with logistic regression models at a high level?their dy-
namic training approach?but provides no experiments.
587
day rely on unsupervised learning so that the models
may be applied easily to many language pairs. Our
approach provides efficient and consistent unsuper-
vised estimation for feature-rich alignment models.
6.2 Word Alignment Features
The BASIC features on pairs of lexical items
provide strong baseline performance. We add
coarse features to the model in order to inject
prior knowledge and tie together lexical items with
similar characteristics.
BASIC:  (e = ?, y = ?)
EDIT-DISTANCE:  (dist(y, e) = ?)
DICTIONARY:  ((y, e) ? D) for dictionary D.
STEM:  (stem(e) = ?, y = ?) for Porter stemmer.
PREFIX:  (prefix(e) = ?, y = ?) for prefixes of
length 4.
CHARACTER:  (e = ?, charAt(y, i) = ?) for index i in
the Chinese word.
These features correspond to several common
augmentations of word alignment models, such as
adding dictionary priors and truncating long words,
but here we integrate them all coherently into a sin-
gle model.
6.3 Word Alignment Data and Evaluation
We evaluate on the standard hand-aligned portion
of the NIST 2002 Chinese-English development set
(Ayan et al, 2005). The set is annotated with sure S
and possible P alignments. We measure alignment
quality using alignment error rate (AER) (Och and
Ney, 2000).
We train the models on 10,000 sentences of FBIS
Chinese-English newswire. This is not a large-scale
experiment, but large enough to be relevant for low-
resource languages. LBFGS experiments are not
provided because computing expectations in these
models is too computationally intensive to run for
many iterations. Hence, EM training is a more ap-
propriate optimization approach: computing the M-
step gradient requires only summing over word type
pairs, while the marginal likelihood gradient needed
for LBFGS requires summing over training sentence
alignments. The final alignments, in both the base-
line and the feature-enhanced models, are computed
by training the generative models in both directions,
combining the result with hard union competitive
thresholding (DeNero and Klein, 2007), and us-
ing agreement training for the HMM (Liang et al,
2006). The combination of these techniques yields
a state-of-the-art unsupervised baseline for Chinese-
English.
6.4 Word Alignment Results
For both IBM Model 1 and the HMM alignment
model, EM training with feature-enhanced models
outperforms the standard multinomial models, by
2.4 and 3.8 AER respectively.6 As expected, large
positive weights are assigned to both the dictionary
and edit distance features. Stem and character fea-
tures also contribute to the performance gain.
7 Word Segmentation
Finally, we show that it is possible to improve upon
the simple and effective word segmentation model
presented in Liang and Klein (2009) by adding
phonological features. Unsupervised word segmen-
tation is the task of identifying word boundaries in
sentences where spaces have been removed. For a
sequence of characters y = (y1, ..., yn), a segmen-
tation is a sequence of segments z = (z1, ..., z|z|)
such that z is a partition of y and each zi is a con-
tiguous subsequence of y. Unsupervised models for
this task infer word boundaries from corpora of sen-
tences of characters without ever seeing examples of
well-formed words.
7.1 Unigram Double-Exponential Model
Liang and Klein?s (2009) unigram double-
exponential model corresponds to a simple
derivational process where sentences of characters
x are generated a word at a time, drawn from a
multinomial over all possible strings ?z,SEGMENT.
For this type, there is no context and the decision is
the particular string generated. In order to avoid the
degenerate MLE that assigns mass only to single
segment sentences it is helpful to independently
generate a length for each segment from a fixed
distribution. Liang and Klein (2009) constrain in-
dividual segments to have maximum length 10 and
generate lengths from the following distribution:
?l,LENGTH = exp(?l1.6) when 1 ? l ? 10. Their
model is deficient since it is possible to generate
6The best published results for this dataset are supervised,
and trained on 17 times more data (Haghighi et al, 2009).
588
lengths that are inconsistent with the actual lengths
of the generated segments. The likelihood equation
is given by:
P (Y = y,Z = z) =
?STOP
|z|
?
i=1
[
(1? ?STOP) ?zi,SEGMENT exp(?|zi|1.6)
]
7.2 Segmentation Data and Evaluation
We train and test on the phonetic version of the
Bernstein-Ratner corpus (1987). This is the same
set-up used by Liang and Klein (2009), Goldwater
et al (2006), and Johnson and Goldwater (2009).
This corpus consists of 9790 child-directed utter-
ances transcribed using a phonetic representation.
We measure segment F1 score on the entire corpus.
We run all word segmentation models for 300 iter-
ations with 10 random initializations and report the
mean and standard deviation of F1 in Table 1.
7.3 Segmentation Features
The SEGMENT multinomial is the important distri-
bution in this model. We use the following features:
BASIC:  (z = ?)
LENGTH:  (length(z) = ?)
NUMBER-VOWELS:  (numVowels(z) = ?)
PHONO-CLASS-PREF:  (prefix(coarsePhonemes(z)) = ?)
PHONO-CLASS-PREF:  (suffix(coarsePhonemes(z)) = ?)
The phonological class prefix and suffix features
project each phoneme of a string to a coarser class
and then take prefix and suffix indicators on the
string of projected characters. We include two ver-
sions of these features that use projections with dif-
ferent levels of coarseness. The goal of these fea-
tures is to help the model learn general phonetic
shapes that correspond to well-formed word bound-
aries.
As is the case in general for our method, the
feature-enhanced unigram model still respects the
conditional independence assumptions that the stan-
dard unigram model makes, and inference is still
performed using a simple dynamic program to com-
pute expected sufficient statistics, which are just seg-
ment counts.
7.4 Segmentation Results
To our knowledge our system achieves the best per-
formance to date on the Bernstein-Ratner corpus,
with an F1 of 88.0. It is substantially simpler than
the non-parametric Bayesian models proposed by
Johnson et al (2007), which require sampling pro-
cedures to perform inference and achieve an F1 of
87 (Johnson and Goldwater, 2009). Similar to our
other results, the direct gradient approach outper-
forms EM for feature-enhanced models, and both
approaches outperform the baseline, which achieves
an F1 of 76.9.
8 Conclusion
We have shown that simple, locally normalized
models can effectively incorporate features into un-
supervised models. These enriched models can
be easily optimized using standard NLP build-
ing blocks. Beyond the four tasks explored in
this paper?POS tagging, DMV grammar induc-
tion, word alignment, and word segmentation?the
method can be applied to many other tasks, for ex-
ample grounded semantics, unsupervised PCFG in-
duction, document clustering, and anaphora resolu-
tion.
Acknowledgements
We thank Percy Liang for making his word segmen-
tation code available to us, and the anonymous re-
viewers for their comments.
Appendix: Optimization
In this section, we derive the gradient of the log marginal likeli-
hood needed for the direct gradient approach. Let w0 be the cur-
rent weights in Algorithm 2 and e = e(w0) be the expectations
under these weights as computed in Equation 2. In order to jus-
tify Algorithm 2, we need to prove that ?L(w0) = ??(w0, e).
We use the following simple lemma: if ?, ? are real-valued
functions such that: (1) ?(w0) = ?(w0) for some w0; (2)
?(w) ? ?(w) on an open set containing w0; and (3), ? and ?
are differentiable at w0; then ??(w0) = ??(w0).
We set ?(w) = L(w) and ?(w) = ?(w, e)?P
z
Pw0(Z =
z|Y = y) log Pw0(Z = z|Y = y). If we can show that ?, ?
satisfy the conditions of the lemma we are done since the second
term of ? depends on w0, but not on w.
Property (3) can be easily checked, and property (2) follows
from Jensen?s inequality. Finally, property (1) follows from
Lemma 2 of Neal and Hinton (1998).
589
References
N. F. Ayan, B. Dorr, and C. Monz. 2005. Combining
word alignments using neural networks. In Empirical
Methods in Natural Language Processing.
N. Bernstein-Ratner. 1987. The phonology of parent-
child speech. K. Nelson and A. van Kleeck.
M. Bisani and H. Ney. 2008. Joint-sequence models for
grapheme-to-phoneme conversion.
A. Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.
2008. A probabilistic approach to language change.
In Neural Information Processing Systems.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1994. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics.
S. F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
S. B. Cohen and N. A. Smith. 2009. Shared logistic nor-
mal distributions for soft parameter tying in unsuper-
vised grammar induction. In North American Chapter
of the Association for Computational Linguistics.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological).
J. DeNero and D. Klein. 2007. Tailoring word align-
ments to syntactic machine translation. In Association
for Computational Linguistics.
D. Elworthy. 1994. Does Baum-Welch re-estimation
help taggers? In Association for Computational Lin-
guistics.
S. Goldwater and T. L. Griffiths. 2007. A fully Bayesian
approach to unsupervised part-of-speech tagging. In
Association for Computational Linguistics.
S. Goldwater, T. L. Griffiths, and M. Johnson. 2006.
Contextual dependencies in unsupervised word seg-
mentation. In International Conference on Computa-
tional Linguistics/Association for Computational Lin-
guistics.
A. Haghighi and D. Klein. 2006. Prototype-driven learn-
ing for sequence models. In Association for Computa-
tional Linguistics.
A. Haghighi, J. Blitzer, J. DeNero, and D. Klein. 2009.
Better word alignments with supervised ITG models.
In Association for Computational Linguistics.
M. Johnson and S. Goldwater. 2009. Improving non-
parametric Bayesian inference: Experiments on unsu-
pervised word segmentation with adaptor grammars.
In North American Chapter of the Association for
Computational Linguistics.
M. Johnson, T. L. Griffiths, and S. Goldwater. 2007.
Adaptor grammars: a framework for specifying com-
positional nonparametric Bayesian models. In Neural
Information Processing Systems.
M. Johnson. 2007. Why doesnt EM find good HMM
POS-taggers? In Empirical Methods in Natural Lan-
guage Processing/Computational Natural Language
Learning.
D. Klein and C. D. Manning. 2004. Corpus-based in-
duction of syntactic structure: Models of dependency
and constituency. In Association for Computational
Linguistics.
P. Liang and D. Klein. 2009. Online EM for unsuper-
vised models. In North American Chapter of the As-
sociation for Computational Linguistics.
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In North American Chapter of the Associ-
ation for Computational Linguistics.
D. C. Liu, J. Nocedal, and C. Dong. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of English:
the penn treebank. Computational Linguistics.
B. Merialdo. 1994. Tagging English text with a proba-
bilistic model. Computational Linguistics.
R. Neal and G. E. Hinton. 1998. A view of the EM
algorithm that justifies incremental, sparse, and other
variants. In Learning in Graphical Models. Kluwer
Academic Publishers.
H. Ney and S. Vogel. 1996. HMM-based word alignment
in statistical translation. In International Conference
on Computational Linguistics.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Association for Computational Lin-
guistics.
R. Salakhutdinov, S. Roweis, and Z. Ghahramani. 2003.
Optimization with EM and expectation-conjugate-
gradient. In International Conference on Machine
Learning.
N. A. Smith and J. Eisner. 2005. Contrastive estimation:
Training log-linear models on unlabeled data. In As-
sociation for Computational Linguistics.
N. A. Smith and M. Johnson. 2007. Weighted and prob-
abilistic context-free grammars are equally expressive.
Computational Linguistics.
I. G. Varea, F. J. Och, H. Ney, and F. Casacuberta. 2002.
Refined lexicon models for statistical machine transla-
tion using a maximum entropy approach. In Associa-
tion for Computational Linguistics.
N. Xue, F-D Chiou, and M. Palmer. 2002. Building a
large-scale annotated Chinese corpus. In International
Conference on Computational Linguistics.
590
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 975?983,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Model Combination for Machine Translation
John DeNero, Shankar Kumar, Ciprian Chelba, and Franz Och
UC Berkeley Google, Inc.
denero@berkeley.edu {shankarkumar,ciprianchelba,och}@google.com
Abstract
Machine translation benefits from two types
of decoding techniques: consensus decoding
over multiple hypotheses under a single model
and system combination over hypotheses from
different models. We present model combina-
tion, a method that integrates consensus de-
coding and system combination into a uni-
fied, forest-based technique. Our approach
makes few assumptions about the underly-
ing component models, enabling us to com-
bine systems with heterogenous structure. Un-
like most system combination techniques, we
reuse the search space of component models,
which entirely avoids the need to align trans-
lation hypotheses. Despite its relative sim-
plicity, model combination improves trans-
lation quality over a pipelined approach of
first applying consensus decoding to individ-
ual systems, and then applying system combi-
nation to their output. We demonstrate BLEU
improvements across data sets and language
pairs in large-scale experiments.
1 Introduction
Once statistical translation models are trained, a de-
coding approach determines what translations are fi-
nally selected. Two parallel lines of research have
shown consistent improvements over the standard
max-derivation decoding objective, which selects
the highest probability derivation. Consensus de-
coding procedures select translations for a single
system by optimizing for model predictions about
n-grams, motivated either as minimizing Bayes risk
(Kumar and Byrne, 2004), maximizing sentence
similarity (DeNero et al, 2009), or approximating a
max-translation objective (Li et al, 2009b). System
combination procedures, on the other hand, generate
translations from the output of multiple component
systems (Frederking and Nirenburg, 1994). In this
paper, we present model combination, a technique
that unifies these two approaches by learning a con-
sensus model over the n-gram features of multiple
underlying component models.
Model combination operates over the compo-
nent models? posterior distributions over translation
derivations, encoded as a forest of derivations.1 We
combine these components by constructing a linear
consensus model that includes features from each
component. We then optimize this consensus model
over the space of all translation derivations in the
support of all component models? posterior distribu-
tions. By reusing the components? search spaces,
we entirely avoid the hypothesis alignment problem
that is central to standard system combination ap-
proaches (Rosti et al, 2007).
Forest-based consensus decoding techniques dif-
fer in whether they capture model predictions
through n-gram posteriors (Tromble et al, 2008;
Kumar et al, 2009) or expected n-gram counts
(DeNero et al, 2009; Li et al, 2009b). We evaluate
both in controlled experiments, demonstrating their
empirical similarity. We also describe algorithms for
expanding translation forests to ensure that n-grams
are local to a forest?s hyperedges, and for exactly
computing n-gram posteriors efficiently.
Model combination assumes only that each trans-
lation model can produce expectations of n-gram
features; the latent derivation structures of compo-
nent systems can differ arbitrarily. This flexibility
allows us to combine phrase-based, hierarchical, and
syntax-augmented translation models. We evaluate
by combining three large-scale systems on Chinese-
English and Arabic-English NIST data sets, demon-
strating improvements of up to 1.4 BLEU over the
1In this paper, we use the terms translation forest and hyper-
graph interchangeably.
975
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Single-Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?:
[
v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?:
[
v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 1: An example translation forest encoding two
synchronous derivations for a Spanish sentence: one solid
and one dotted. Nodes are annotated with their left and
right unigram contexts, and hyperedges are annotated
with scores ? ? ?(r) and the bigrams they introduce.
best single systemmax-derivation baseline, and con-
sistent improvements over a more complex multi-
system pipeline that includes independent consensus
decoding and system combination.
2 Model Combination
Model combination is a model-based approach to se-
lecting translations using information from multiple
component systems. Each system provides its poste-
rior distributions over derivations Pi(d|f), encoded
as a weighted translation forest (i.e., translation hy-
pergraph) in which hyperedges correspond to trans-
lation rule applications r.2 The conditional distribu-
tion over derivations takes the form:
Pi(d|f) =
exp
[?
r?d ?i ? ?i(r)
]
?
d??D(f) exp
[?
r?d? ?i ? ?i(r)
]
whereD(f) is the set of synchronous derivations en-
coded in the forest, r iterates over rule applications
in d, and ?i is the parameter vector for system i. The
feature vector ?i is system specific and includes both
translation model and language model features. Fig-
ure 1 depicts an example forest.
Model combination includes four steps, described
below. The entire sequence is illustrated in Figure 2.
2Phrase-based systems produce phrase lattices, which are in-
stances of forests with arity 1.
2.1 Computing Combination Features
The first step in model combination is to com-
pute n-gram expectations from component system
posteriors?the same quantities found in MBR, con-
sensus, and variational decoding techniques. For an
n-gram g and system i, the expectation
vni (g) = EPi(d|f) [h(d, g)]
can be either an n-gram expected count, if h(d, g)
is the count of g in d, or the posterior probability
that d contains g, if h(d, g) is an indicator function.
Section 3 describes how to compute these features
efficiently.
2.2 Constructing a Search Space
The second step in model combination constructs a
hypothesis space of translation derivations, which
includes all derivations present in the forests con-
tributed by each component system. This search
space D is also a translation forest, and consists of
the conjoined union of the component forests. Let
Ri be the root node of component hypergraph Di.
For all i, we include all of Di in D, along with an
edge from Ri to R, the root of D. D may contain
derivations from different types of translation sys-
tems. However, D only contains derivations (and
therefore translations) that appeared in the hypothe-
sis space of some component system. We do not in-
termingle the component search spaces in any way.
2.3 Features for the Combination Model
The third step defines a new combination model over
all of the derivations in the search space D, and then
annotates D with features that allow for efficient
model inference. We use a linear model over four
types of feature functions of a derivation:
1. Combination feature functions on n-grams
vni (d) =
?
g?Ngrams(d) v
n
i (g) score a deriva-
tion according to the n-grams it contains.
2. Model score feature function b gives the model
score ?i ? ?i(d) of a derivation d under the sys-
tem i that d is from.
3. A length feature ` computes the word length of
the target-side yield of a derivation.
4. A system indicator feature ?i is 1 if the deriva-
tion came from system i, and 0 otherwise.
976
All of these features are local to rule applications
(hyperedges) in D. The combination features pro-
vide information sharing across the derivations of
different systems, but are functions of n-grams, and
so can be scored on any translation forest. Model
score features are already local to rule applications.
The length feature is scored in the standard way.
System indicator features are scored only on the hy-
peredges fromRi toR that link each component for-
est to the common root.
Scoring the joint search space D with these fea-
tures involves annotating each rule application r (i.e.
hyperedge) with the value of each feature.
2.4 Model Training and Inference
We have defined the following combination model
sw(d) with weights w over derivations d from I dif-
ferent component models:
I?
i=1
[
4?
n=1
wni v
n
i (d) + w
?
i ?i(d)
]
+wb?b(d)+w`?`(d)
Because we have assessed all of these features on
local rule applications, we can find the highest scor-
ing derivation d? = arg max
d?D
sw(d) using standard
max-sum (Viterbi) inference over D.
We learn the weights of this consensus model us-
ing hypergraph-based minimum-error-rate training
(Kumar et al, 2009). This procedure maximizes the
translation quality of d? on a held-out set, according
to a corpus-level evaluation metric B(?; e) that com-
pares to a reference set e. We used BLEU, choosing
w to maximize the BLEU score of the set of transla-
tions predicted by the combination model.
3 Computing Combination Features
The combination features vni (d) score derivations
from each model with the n-gram predictions of the
others. These predictions sum over all derivations
under a single component model to compute a pos-
terior belief about each n-gram. In this paper, we
compare two kinds of combination features, poste-
rior probabilities and expected counts.3
3The model combination framework could incorporate ar-
bitrary features on the common output space of the models, but
we focus on features that have previously proven useful for con-
sensus decoding.
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Singl -Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?:
[
v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?:
[
v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 2: Model combination applied to a phrase-based
(pb) and a hierarchical model (h) includes four steps. (1)
shows an excerpt of the bigram feature function for each
component, (2) depicts the result of conjoining a phrase
lattice with a hierarchical forest, (3) shows example hy-
peredge features of the combination model, including bi-
gram features vni and system indicators ?i, and (4) gives
training and decoding objectives.
Posterior probabilities represent a model?s be-
lief that the translation will contain a particular n-
gram at least once. They can be expressed as
EP (d|f) [?(d, g)] for an indicator function ?(d, g)
that is 1 if n-gram g appears in derivation d. These
quantities arise in approximating BLEU for lattice-
based and hypergraph-based minimum Bayes risk
decoding (Tromble et al, 2008; Kumar et al, 2009).
Expected n-gram counts EP (d|f) [c(d, g)] represent
the model?s belief of how many times an n-gram g
will appear in the translation. These quantities ap-
pear in forest-based consensus decoding (DeNero et
al., 2009) and variational decoding (Li et al, 2009b).
977
Methods for computing both of these quantities ap-
pear in the literature. However, we address two out-
standing issues below. In Section 5, we also com-
pare the two quantities experimentally.
3.1 Computing N -gram Posteriors Exactly
Kumar et al (2009) describes an efficient approx-
imate algorithm for computing n-gram posterior
probabilities. Algorithm 1 is an exact algorithm that
computes all n-gram posteriors from a forest in a
single inside pass. The algorithm tracks two quanti-
ties at each node n: regular inside scores ?(n) and
n-gram inside scores ??(n, g) that sum the scores of
all derivations rooted at n that contain n-gram g.
For each hyperedge, we compute b?(g), the sum of
scores for derivations that do not contain g (Lines 8-
11). We then use that quantity to compute the score
of derivations that do contain g (Line 17).
Algorithm 1 Computing n-gram posteriors
1: for n ? N in topological order do
2: ?(n)? 0
3: ??(n, g)? 0, ?g ? Ngrams(n)
4: for r ? Rules(n) do
5: w ? exp [? ? ?(r)]
6: b? w
7: b?(g)? w, ?g ? Ngrams(n)
8: for ` ? Leaves(r) do
9: b? b? ?(`)
10: for g ? Ngrams(n) do
11: b?(g)? b?(g)?
(
?(`)? ??(`, g)
)
12: ?(n)? ?(n) + b
13: for g ? Ngrams(n) do
14: if g ? Ngrams(r) then
15: ??(n, g)? ??(n, g)+b
16: else
17: ??(n, g)? ??(n, g)+b? b?(g)
18: for g ? Ngrams(root) (all g in the HG) do
19: P (g|f)? ??(root,g)?(root)
This algorithm can in principle compute the pos-
terior probability of any indicator function on local
features of a derivation. More generally, this algo-
rithm demonstrates how vector-backed inside passes
can compute quantities beyond expectations of local
features (Li and Eisner, 2009).4 Chelba and Maha-
jan (2009) developed a similar algorithm for lattices.
4Indicator functions on derivations are not locally additive
3.2 Ensuring N -gram Locality
DeNero et al (2009) describes an efficient algorithm
for computing n-gram expected counts from a trans-
lation forest. This method assumes n-gram local-
ity of the forest, the property that any n-gram intro-
duced by a hyperedge appears in all derivations that
include the hyperedge. However, decoders may re-
combine forest nodes whenever the language model
does not distinguish between n-grams due to back-
off (Li and Khudanpur, 2008). In this case, a forest
encoding of a posterior distribution may not exhibit
n-gram locality in all regions of the search space.
Figure 3 shows a hypergraph which contains non-
local trigrams, along with its local expansion.
Algorithm 2 expands a forest to ensure n-gram lo-
cality while preserving the encoded distribution over
derivations. Let a forest (N,R) consist of nodes N
and hyperedges R, which correspond to rule appli-
cations. Let Rules(n) be the subset of R rooted by
n, and Leaves(r) be the leaf nodes of rule applica-
tion r. The expanded forest (Ne, Re) is constructed
by a function Reapply(r, L) that applies the rule of r
to a new set of leavesL ? Ne, forming a pair (r?, n?)
consisting of a new rule application r? rooted by n?.
P is a map from nodes in N to subsets of Ne which
tracks how N projects to Ne. Two nodes in Ne are
identical if they have the same (n?1)-gram left and
right contexts and are projections of the same node
in N . The symbol
?
denotes a set cross-product.
Algorithm 2 Expanding for n-gram locality
1: Ne ? {}; Re ? {}
2: for n ? N in topological order do
3: P (n)? {}
4: for r ? Rules(n) do
5: for L ?
?
`?Leaves(r) [P (`)] do
6: r?, n? ? Reapply(r, L)
7: P (n)? P (n) ? {n?}
8: Ne ? Ne ? {n?}
9: Re ? Re ? {r?}
This transformation preserves the original distri-
bution over derivations by splitting states, but main-
taining continuations from those split states by du-
plicating rule applications. The process is analogous
over the rules of a derivation, even if the features they indicate
are local. Therefore, Algorithm 1 is not an instance of an ex-
pectation semiring computation.
978
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.4
?saw the?
?man with?
0.6
?saw the?
1.0
?man with?
Step 1: Compute Single-Model N-gram Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based system Hierarchical system
...
...
R
R
pb
R
h
R
R
pb
R
h
?saw the?: [v2pb = 0.7, v2h = 1.0
]
[?pb = 1] [?h = 1]
v2pb(?saw the?) = 0.7
green witch
was here
blue witch
green witch was here
blue witch was here
was here
green witch was here
blue witch was here
green witch blue witch
v2h(?saw the?) = 1.0
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
I ... telescope
Yo vi al hombre con el telescopio
I ... saw the ... man with ... telescope
the ... telescope
0.6
?saw the?
1.0
?man with?
I ... man
0.4
?telescope the?
0.3
?saw with?
I ... telescope
?I saw the man with the telescope?
?I saw with the telescope the man? 
Step 1: Compute Combination Features 
Step 2: Construct a Search Space
Step 3: Add Features for the Combination Model
Step 4: Model Training and Inference
Phrase-based model Hierarchical model
...
...
R
R
pb
R
h
[?pb = 1] [?h = 1]
w = arg max
w
BLEU
({
arg max
d?D(f)
sw(d)
}
; e
)
d? = arg max
d?D
sw(d)
v2h(?saw the?) = 0.7v2pb(?saw the?) = 0.9
?saw the?: [v2pb = 0.9, v2h = 0.7
]
applied rule
rule root
rule leaves
n !? P (n)
Figure 3: Hypergraph expansion ensures n-gram locality
without affecting the distribution over derivations. In the
left example, trigrams ?green witch was? and ?blue witch
was? are non-local due to language model back-off. On
the right, states are split to enforce trigram locality.
to expanding bigram lattices to e code a trigram his-
tory at each lattice node (Weng et al, 1998).
4 Relationship to Prior Work
Model combination is a multi-system generaliza-
tion of consensus or minimum Bayes risk decod-
ing. When only one component system is included,
model combination is identical to minimum Bayes
risk decoding over hypergraphs, as described in Ku-
mar et al (2009).5
4.1 System Combination
System combination techniques in machine trans-
lat on take as input the outputs {e1, ? ? ? , ek} of k
translation systems, where ei is a structured transla-
tion object (or k-best lists thereof), typically viewed
as a sequence of words. The dominant approach in
the field chooses a primary translation ep as a back-
bone, then finds an alignment ai to the backbone for
each ei. A new search space is constructed from
these backbone-aligned outputs, and then a voting
procedure or feature-based model predicts a final
consensus translation (Rosti et al, 2007). Model
combination entirely avoids this alignment problem
by viewing hypotheses as n-gram occurrence vec-
tors rather than word sequences.
Model combination also requires less total com-
putation than applying system combination to
5We do not refer to model combination as a minimum Bayes
risk decoding procedure despite this similarity because risk im-
plies a belief distribution over outputs, and we now have mul-
tiple output distributions that are not necessarily calibrated.
Moreover, our generalized, multi-model objective (Section 2.4)
is motivated by BLEU, but not a direct approximation to it.
consensus-decoded outputs. The best consensus de-
coding methods for individual systems already re-
quire the computation-intensive steps of model com-
bination: producing lattices or forests, computing n-
gram feature expectations, and re-decoding to max-
imize a secondary consensus objective. Hence, to
maximize the performance of system combination,
these steps must be performed for each system,
whereas model combination requires only one for-
est rescoring pass over all systems.
Model combination also leverages aggregate
statistics from the components? posteriors, whereas
system combiners typically do not. Zhao and He
(2009) showed that n-gram posterior features are
useful in the context of a system combination model,
even when computed from k-best lists.
Despite these advantages, system combination
may be more appropriate in some settings. In par-
ticular, model combination is designed primarily for
st tistical systems that generate hypergraph outputs.
Model combination can in principle integrate a non-
statisti al system that generates either a single hy-
pothesis or an unweighted forest.6 Likewise, the pro-
cedure c uld be applied to statistical systems that
only generate k-best lists. However, we would not
expect the same strong performance from model
combination in these constrained settings.
4.2 Joint Decoding and Collaborative Decoding
Liu et al (2009) describes two techniques for com-
bining multiple synchronous grammars, which the
authors characterize as joint decoding. Joint de-
coding does not involve a consensus or minimum-
Bayes-risk decoding objective; indeed, their best
results come from standard max-derivation decod-
ing (with a multi-system grammar). More impor-
tantly, their computations rely on a correspondence
between nodes in the hypergraph outputs of differ-
ent systems, and so they can only joint decode over
models with similar search strategies. We combine a
phrase-based model that uses left-to-right decoding
with two hierarchical systems that use bottom-up de-
coding ? a scenario to which joint decoding is not
applicable. Though Liu et al (2009) rightly point
out that most models can be decoded either left-to-
6A single hypothesis can be represented as a forest, while an
unweighted forest could be assigned a uniform distribution.
979
right or bottom-up, such changes can have substan-
tial implications for search efficiency and search er-
ror. We prefer to maintain the flexibility of using dif-
ferent search strategies in each component system.
Li et al (2009a) is another related technique for
combining translation systems by leveraging model
predictions of n-gram features. K-best lists of par-
tial translations are iteratively reranked using n-
gram features from the predictions of other mod-
els (which are also iteratively updated). Our tech-
nique differs in that we use no k-best approxima-
tions, have fewer parameters to learn (one consensus
weight vector rather than one for each collaborating
decoder) and produce only one output, avoiding an
additional system combination step at the end.
5 Experiments
We report results on the constrained data track of the
NIST 2008 Arabic-to-English (ar-en) and Chinese-
to-English (zh-en) translation tasks.7 We train on all
parallel and monolingual data allowed in the track.
We use the NIST 2004 eval set (dev) for optimiz-
ing parameters in model combination and test on
the NIST 2008 evaluation set. We report results
using the IBM implementation of the BLEU score
which computes the brevity penalty using the clos-
est reference translation for each segment (Papineni
et al, 2002). We measure statistical significance us-
ing 95% confidence intervals computed using paired
bootstrap resampling. In all table cells (except for
Table 3) systems without statistically significant dif-
ferences are marked with the same superscript.
5.1 Base Systems
We combine outputs from three systems. Our
phrase-based system is similar to the alignment tem-
plate system described by Och and Ney (2004).
Translation is performed using a standard left-
to-right beam-search decoder. Our hierarchical
systems consist of a syntax-augmented system
(SAMT) that includes target-language syntactic cat-
egories (Zollmann and Venugopal, 2006) and a
Hiero-style system with a single non-terminal (Chi-
ang, 2007). Each base system yields state-of-the-art
translation performance, summarized in Table 1.
7http://www.nist.gov/speech/tests/mt
BLEU (%)
ar-en zh-en
Sys Base dev nist08 dev nist08
PB MAX 51.6 43.9 37.7 25.4
PB MBR 52.4? 44.6? 38.6? 27.3?
PB CON 52.4? 44.6? 38.7? 27.2?
Hiero MAX 50.9 43.3 40.0 27.2
Hiero MBR 51.4? 43.8? 40.6? 27.8
Hiero CON 51.5? 43.8? 40.5? 28.2
SAMT MAX 51.7 43.8 40.8? 28.4
SAMT MBR 52.7? 44.5? 41.1? 28.8?
SAMT CON 52.6? 44.4? 41.1? 28.7?
Table 1: Performance of baseline systems.
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
Best MAX system 51.7 43.9 40.8 28.4
Best MBR system 52.7 44.5 41.1 28.8?
MC Conjoin/SI 53.5 45.3 41.6 29.0?
Table 2: Performance from the best single system for
each language pair without consensus decoding (Best
MAX system), the best system with minimum Bayes risk
decoding (Best MBR system), and model combination
across three systems.
For each system, we report the performance of
max-derivation decoding (MAX), hypergraph-based
MBR (Kumar et al, 2009), and a linear version of
forest-based consensus decoding (CON) (DeNero et
al., 2009). MBR and CON differ only in that the first
uses n-gram posteriors, while the second uses ex-
pected n-gram counts. The two consensus decoding
approaches yield comparable performance. Hence,
we report performance for hypergraph-based MBR
in our comparison to model combination below.
5.2 Experimental Results
Table 2 compares model combination (MC) to the
best MAX and MBR systems. Model combination
uses a conjoined search space wherein each hyper-
edge is annotated with 21 features: 12 n-gram poste-
rior features vni computed from the PB/Hiero/SAMT
forests for n ? 4; 4 n-gram posterior features vn
computed from the conjoined forest; 1 length fea-
ture `; 1 feature b for the score assigned by the base
model; and 3 system indicator (SI) features ?i that
select which base system a derivation came from.
We refer to this model combination approach as MC
980
BLEU (%)
ar-en zh-en
Strategy dev nist08 dev nist08
Best MBR system 52.7 44.5 41.1 28.8
MBR Conjoin 52.3 44.5 40.5 28.3
MBR Conjoin/feats-best 52.7 44.9 41.2 28.8
MBR Conjoin/SI 53.1 44.9 41.2 28.9
MC 1-best HG 52.7 44.6 41.1 28.7
MC Conjoin 52.9 44.6 40.3 28.1
MC Conjoin/base/SI 53.5 45.1 41.2 28.9
MC Conjoin/SI 53.5 45.3 41.6 29.0
Table 3: Model Combination experiments.
Conjoin/SI. Model combination improves over the
single best MAX system by 1.4 BLEU in ar-en and
0.6 BLEU in zh-en, and always improves over MBR.
This improvement could arise due to multiple rea-
sons: a bigger search space, the consensus features
from constituent systems, or the system indicator
features. Table 3 teases apart these contributions.
We first perform MBR on the conjoined hyper-
graph (MBR-Conjoin). In this case, each edge is
tagged with 4 conjoined n-gram features vn, along
with length and base model features. MBR-Conjoin
is worse than MBR on the hypergraph from the
single best system. This could imply that either
the larger search space introduces poor hypotheses
or that the n-gram posteriors obtained are weaker.
When we now restrict the n-gram features to those
from the best system (MBR Conjoin/feats-best),
BLEU scores increase relative to MBR-Conjoin.
This implies that the n-gram features computed over
the conjoined hypergraph are weaker than the corre-
sponding features from the best system.
Adding system indicator features (MBR Con-
join+SI) helps the MBR-Conjoin system consider-
ably; the resulting system is better than the best
MBR system. This could mean that the SI features
guide search towards stronger parts of the larger
search space. In addition, these features provide a
normalization of scores across systems.
We next do several model-combination experi-
ments. We perform model combination using the
search space of only the best MBR system (MC
1best HG). Here, the hypergraph is annotated with
n-gram features from the 3 base systems, as well as
length and base model features. A total of 3 ? 4 +
1 + 1 = 14 features are added to each edge. Sur-
BLEU (%)
ar-en zh-en
Approach Base dev nist08 dev nist08
Sent-level MAX 51.8? 44.4? 40.8? 28.2?
Word-level MAX 52.0? 44.4? 40.8? 28.1?
Sent-level MBR 52.7+ 44.6? 41.2 28.8+
Word-level MBR 52.5+ 44.7? 40.9 28.8+
MC-conjoin-SI 53.5 45.3 41.6 29.0+
Table 4: BLEU performance for different system and
model combination approaches. Sentence-level and
word-level system combination operate over the sentence
output of the base systems, which are either decoded to
maximize derivation score (MAX) or to minimize Bayes
risk (MBR).
prisingly, n-gram features from the additional sys-
tems did not help select a better hypothesis within
the search space of a single system.
When we expand the search space to the con-
joined hypergraph (MC Conjoin), it performs worse
relative to MC 1-best. Since these two systems are
identical in their feature set, we hypothesize that
the larger search space has introduced erroneous hy-
potheses. This is similar to the scenario where MBR
Conjoin is worse than MBR 1-best. As in the MBR
case, adding system indicator features helps (MC
Conjoin/base/SI). The result is comparable to MBR
on the conjoined hypergraph with SI features.
We finally add extra n-gram features which are
computed from the conjoined hypergraph (MC Con-
join + SI). This gives the best performance although
the gains over MC Conjoin/base/SI are quite small.
Note that these added features are the same n-gram
features used in MBR Conjoin. Although they are
not strong by themselves, they provide additional
discriminative power by providing a consensus score
across all 3 base systems.
5.3 Comparison to System Combination
Table 4 compares model combination to two sys-
tem combination algorithms. The first, which we
call sentence-level combination, chooses among the
base systems? three translations the sentence that
has the highest consensus score. The second, word-
level combination, builds a ?word sausage? from
the outputs of the three systems and chooses a path
through the sausage with the highest score under
a similar model (Macherey and Och, 2007). Nei-
981
BLEU (%)
ar-en zh-en
Approach dev nist08 dev nist08
HG-expand 52.7? 44.5? 41.1? 28.8?
HG-noexpand 52.7? 44.5? 41.1? 28.8?
Table 5: MBR decoding on the syntax augmented system,
with and without hypergraph expansion.
ther system combination technique provides much
benefit, presumably because the underlying systems
all share the same data, pre-processing, language
model, alignments, and code base.
Comparing system combination when no consen-
sus (i.e., minimum Bayes risk) decoding is utilized
at all, we find that model combination improves
upon the result by up to 1.1 BLEU points. Model
combination also performs slightly better relative to
system combination over MBR-decoded systems. In
the latter case, system combination actually requires
more computation compared to model combination;
consensus decoding is performed for each system
rather than only once for model combination. This
experiment validates our approach. Model combina-
tion outperforms system combination while avoid-
ing the challenge of aligning translation hypotheses.
5.4 Algorithmic Improvements
Section 3 describes two improvements to comput-
ing n-gram posteriors: hypergraph expansion for n-
gram locality and exact posterior computation. Ta-
ble 5 shows MBR decoding with and without expan-
sion (Algorithm 2) in a decoder that collapses nodes
due to language model back-off. These results show
that while expansion is necessary for correctness, it
does not affect performance.
Table 6 compares exact n-gram posterior compu-
tation (Algorithm 1) to the approximation described
by Kumar et al (2009). Both methods yield identical
results. Again, while the exact method guarantees
correctness of the computation, the approximation
suffices in practice.
6 Conclusion
Model combination is a consensus decoding strat-
egy over a collection of forests produced by multi-
ple machine translation systems. These systems can
BLEU (%)
ar-en zh-en
Posteriors dev nist08 dev nist08
Exact 52.4? 44.6? 38.6? 27.3?
Approximate 52.5? 44.6? 38.6? 27.2?
Table 6: MBR decoding on the phrase-based system with
either exact or approximate posteriors.
have varied decoding strategies; we only require that
each system produce a forest (or a lattice) of trans-
lations. This flexibility allows the technique to be
applied quite broadly. For instance, de Gispert et al
(2009) describe combining systems based on mul-
tiple source representations using minimum Bayes
risk decoding?likewise, they could be combined
via model combination.
Model combination has two significant advan-
tages over current approaches to system combina-
tion. First, it does not rely on hypothesis alignment
between outputs of individual systems. Aligning
translation hypotheses accurately can be challeng-
ing, and has a substantial effect on combination per-
formance (He et al, 2008). Instead of aligning hy-
potheses, we compute expectations of local features
of n-grams. This is analogous to how BLEU score is
computed, which also views sentences as vectors of
n-gram counts (Papineni et al, 2002) . Second, we
do not need to pick a backbone system for combina-
tion. Choosing a backbone system can also be chal-
lenging, and also affects system combination perfor-
mance (He and Toutanova, 2009). Model combina-
tion sidesteps this issue by working with the con-
joined forest produced by the union of the compo-
nent forests, and allows the consensus model to ex-
press system preferences via weights on system in-
dicator features.
Despite its simplicity, model combination pro-
vides strong performance by leveraging existing
consensus, search, and training techniques. The
technique outperforms MBR and consensus decod-
ing on each of the component systems. In addition,
it performs better than standard sentence-based or
word-based system combination techniques applied
to either max-derivation or MBR outputs of the indi-
vidual systems. In sum, it is a natural and effective
model-based approach to multi-system decoding.
982
References
Ciprian Chelba and M. Mahajan. 2009. A dynamic
programming algorithm for computing the posterior
probability of n-gram occurrences in automatic speech
recognition lattices. Personal communication.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics.
A. de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.
2009. Minimum bayes risk combination of translation
hypotheses from alternative morphological decompo-
sitions. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Robert Frederking and Sergei Nirenburg. 1994. Three
heads are better than one. In Proceedings of the Con-
ference on Applied Natural Language Processing.
Xiaodong He and Kristina Toutanova. 2009. Joint opti-
mization for machine translation system combination.
In Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,
and Robert Moore. 2008. Indirect-hmm-based hy-
pothesis alignment for combining outputs from ma-
chine translation systems. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Shankar Kumar and William Byrne. 2004. Minimum
Bayes-risk decoding for statistical machine transla-
tion. In Proceedings of the North American Chapter
of the Association for Computational Linguistics.
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Och. 2009. Efficient minimum error rate train-
ing and minimum bayes-risk decoding for translation
hypergraphs and lattices. In Proceedings of the Asso-
ciation for Computational Linguistics and IJCNLP.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proceedings of
the Conference on Empirical Methods in Natural Lan-
guage Processing.
Zhifei Li and Sanjeev Khudanpur. 2008. A scalable
decoder for parsing-based machine translation with
equivalent language model state maintenance. In ACL
Workshop on Syntax and Structure in Statistical Trans-
lation.
Mu Li, Nan Duan, Dongdong Zhang, Chi-Ho Li, and
Ming Zhou. 2009a. Collaborative decoding: Partial
hypothesis re-ranking using translation consensus be-
tween decoders. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009b.
Variational decoding for statistical machine transla-
tion. In Proceedings of the Association for Compu-
tational Linguistics and IJCNLP.
Yang Liu, Haitao Mi, Yang Feng, and Qun Liu. 2009.
Joint decoding with multiple translation models. In
Proceedings of the Association for Computational Lin-
guistics and IJCNLP.
Wolfgang Macherey and Franz Och. 2007. An empirical
study on computing consensus translations from mul-
tiple machine translation systems. In EMNLP, Prague,
Czech Republic.
Franz J. Och and Hermann Ney. 2004. The Alignment
Template Approach to Statistical Machine Translation.
Computational Linguistics, 30(4):417 ? 449.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Antti-Veikko I. Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie J.
Dorr. 2007. Combining outputs from multiple ma-
chine translation systems. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Roy Tromble, Shankar Kumar, Franz J. Och, and Wolf-
gang Macherey. 2008. Lattice minimum Bayes-risk
decoding for statistical machine translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Fuliang Weng, Andreas Stolcke, and Ananth Sankar.
1998. Efficient lattice representation and generation.
In Intl. Conf. on Spoken Language Processing.
Yong Zhao and Xiaodong He. 2009. Using n-gram based
features for machine translation system combination.
In Proceedings of the North American Chapter of the
Association for Computational Linguistics.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the NAACL 2006 Workshop on statisti-
cal machine translation.
983
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 773?782,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Unsupervised Translation Sense Clustering
Mohit Bansal?
UC Berkeley
mbansal@cs.berkeley.edu
John DeNero
Google
denero@google.com
Dekang Lin
Google
lindek@google.com
Abstract
We propose an unsupervised method for clus-
tering the translations of a word, such that
the translations in each cluster share a com-
mon semantic sense. Words are assigned to
clusters based on their usage distribution in
large monolingual and parallel corpora using
the softK-Means algorithm. In addition to de-
scribing our approach, we formalize the task
of translation sense clustering and describe a
procedure that leverages WordNet for evalu-
ation. By comparing our induced clusters to
reference clusters generated from WordNet,
we demonstrate that our method effectively
identifies sense-based translation clusters and
benefits from both monolingual and parallel
corpora. Finally, we describe a method for an-
notating clusters with usage examples.
1 Introduction
The ability to learn a bilingual lexicon from a
parallel corpus was an early and influential area
of success for statistical modeling techniques in
natural language processing. Probabilistic word
alignment models can induce bilexical distributions
over target-language translations of source-language
words (Brown et al, 1993). However, word-to-word
correspondences do not capture the full structure of
a bilingual lexicon. Consider the example bilingual
dictionary entry in Figure 1; in addition to enumerat-
ing the translations of a word, the dictionary author
has grouped those translations into three sense clus-
ters. Inducing such a clustering would prove use-
ful in generating bilingual dictionaries automatically
or building tools to assist bilingual lexicographers.
?Author was a summer intern with Google Research while
conducting this research project.
Colocar [co?lo?car?], va. 1. To arrange, to put in
due place or order. 2. To place, to put in any place,
rank condition or office, to provide a place or em-
ployment. 3. To collocate, to locate, to lay.
Figure 1: This excerpt from a bilingual dictionary groups
English translations of the polysemous Spanish word colocar
into three clusters that correspond to different word senses
(Vela?zquez de la Cadena et al, 1965).
This paper formalizes the task of clustering a set
of translations by sense, as might appear in a pub-
lished bilingual dictionary, and proposes an unsu-
pervised method for inducing such clusters. We also
show how to add usage examples for the translation
sense clusters, hence providing complete structure
to a bilingual dictionary.
The input to this task is a set of source words and
a set of target translations for each source word. Our
proposed method clusters these translations in two
steps. First, we induce a global clustering of the en-
tire target vocabulary using the soft K-Means algo-
rithm, which identifies groups of words that appear
in similar contexts (in a monolingual corpus) and are
translated in similar ways (in a parallel corpus). Sec-
ond, we derive clusters over the translations of each
source word by projecting the global clusters.
We evaluate these clusters by comparing them to
reference clusters with the overlapping BCubed met-
ric (Amigo et al, 2009). We propose a clustering cri-
terion that allows us to derive reference clusters from
the synonym groups of WordNet R? (Miller, 1995).1
Our experiments using Spanish-English and
Japanese-English datasets demonstrate that the au-
tomatically generated clusters produced by our
method are substantially more similar to the
1WordNet is used only for evaluation; our sense clustering
method is fully unsupervised and language-independent.
773
Sense cluster WordNet sense description Usage example
collocate group or chunk together in a certain
order or place side by side
colocar juntas todas los libros
collocate all the books
invest, place, put make an investment capitales para colocar
capital to invest
locate, place assign a location to colocar el nu?mero de serie
locate the serial number
place, position, put put into a certain place or abstract
location
colocar en un lugar
put in a place
Figure 2: Correct sense clusters for the translations of Spanish verb s = colocar, assuming that it has translation set Ts =
{collocate, invest, locate, place, position, put}. Only the sense clusters are outputs of the translation sense clustering task; the
additional columns are presented for clarity.
WordNet-based reference clusters than naive base-
lines. Moreover, we show that bilingual features
collected from parallel corpora improve clustering
accuracy over monolingual distributional similarity
features alone.
Finally, we present a method for annotating clus-
ters with usage examples, which enrich our automat-
ically generated bilingual dictionary entries.
2 Task Description
We consider a three-step pipeline for generating
structured bilingual dictionary entries automatically.
(1) The first step is to identify a set of high-quality
target-side translations for source lexical items. In
our experiments, we ask bilingual human annota-
tors to create these translation sets.2 We restrict our
present study to word-level translations, disallowing
multi-word phrases, in order to leverage existing lex-
ical resources for evaluation.
(2) The second step is to cluster translations of each
word according to common word senses. This clus-
tering task is the primary focus of the paper, and we
formalize it in this section.
(3) The final step annotates clusters with usage ex-
amples to enrich the structure of the output. Sec-
tion 7 describes a method of identifying cluster-
specific usage examples.
In the task of translation sense clustering, the
second step, we assume a fixed set of source lexi-
cal items of interest S, each with a single part of
2We do not use automatically extracted translation sets in
our experiments, in order to isolate the clustering task on clean
input.
speech3, and for each s ? S a set Ts of target trans-
lations. Moreover, we assume that each target word
t ? Ts has a set of senses in common with s. These
senses may also be shared among different target
words. That is, each target word may have multiple
senses and each sense may be expressed by multiple
words.
Given a translation set Ts, we define a clusterG ?
Ts to be a correct sense cluster if it is both coherent
and complete.
? A sense cluster G is coherent if and only if
there exists some sense B shared by all of the
target words in G.
? A sense clusterG is complete if and only if, for
every sense B shared by all words in G, there
is no other word in Ts but not in G that also
shares that sense.
The full set of correct clusters for a set of translations
consists of all sense clusters that are both coherent
and complete.
The example translation set for the Spanish word
colocar in Figure 2 is shown with four correct sense
clusters. For descriptive purposes, these clusters are
annotated by WordNet senses and bilingual usage
examples. However, the task we have defined does
not require the WordNet sense or usage example
to be identified: we must only produce the correct
sense clusters within a set of translations. In fact, a
cluster may correspond to more than one sense.
Our definition of correct sense clusters has sev-
eral appealing properties. First, we do not attempt
to enumerate all senses of the source word. Sense
3A noun and verb that share the same word form would con-
stitute two different source lexical items.
774
Notation
Ts : The set of target-language translations (given)
Dt : The set of synsets in which t appears (given)
C : A synset; a set of target-language words
B : A source-specific synset; a subset of Ts
B : A set of source-specific synsets
G : A set of correct sense clusters for Ts
The Cluster Projection Algorithm:
B ?
{
C ? Ts : C ?
?
t?Ts
Dt
}
G ? ?
for B ? B do
if @B? ? B such that B ? B? then
add B to G
return G
Figure 3: The Cluster Projection (CP) algorithm projects
language-level synsets (C) to source-specific synsets (B) and
then filters the set of synsets for redundant subsets to produce
the complete set of source-specific synsets that are both coher-
ent and complete (G).
distinctions are only made when they affect cross-
lingual lexical choice. If a source word has many
fine-grained senses but translates in the same way
regardless of the sense intended, then there is only
one correct sense cluster for that translation.
Second, no correct sense cluster can be a super-
set of another, because the subset would violate the
completeness condition. This criterion encourages
larger clusters that are easier to interpret, as their
unifying senses can be identified as the intersection
of senses of the translations in the cluster.
Third, the correct clusters need not form a parti-
tion of the input translations. It is common in pub-
lished bilingual dictionaries for a translation to ap-
pear in multiple sense clusters. In our example, the
polysemous English verbs place and put appear in
multiple clusters.
3 Generating Reference Clusters
To construct a reference set for the translation
sense clustering task, we first collected English
translations of Spanish and Japanese nouns, verbs,
and adverbs. Translation sets were curated by hu-
man annotators to keep only high-quality single-
word translations.
Rather than gathering reference clusters via an ad-
ditional annotation effort, we leverage WordNet, a
large database of English lexical semantics (Miller,
1995). WordNet groups words into sets of cogni-
Synsets 
 collocate collocate, lump, chunk 
 invest, put, commit, place invest, clothe, adorn invest, vest, enthrone ? 
 locate, turn up situate, locate locate, place, site ? 
 put, set, place, pose, position, lay rate, rank, range, order, grade, place locate, place, site invest, put, commit, place 
? 
 position put, set, place, pose, position, lay 
 put, set, place, pose, position, lay put frame, redact, cast, put, couch invest, put, commit, place ? 
Words 
 collocate 
  
invest 
  
 locate 
 
  
 place 
  
 
 position   
put 
Sense Clusters 
 collocate 
 invest, place, put 
 
locate, place  
place, position, put 
Figure 4: An example of cluster projection on WordNet, for the
Spanish source word colocar. We show the target translation
words to be clustered, their WordNet synsets (with words not in
the translation set grayed out), and the final set of correct sense
clusters.
tive synonyms called synsets, each expressing a dis-
tinct concept. We use WordNet version 2.1, which
has wide coverage of nouns, verbs, and adverbs, but
sparser coverage of adjectives and prepositions.4
Reference clusters for the set of translations Ts
of some source word s are generated algorithmi-
cally from WordNet synsets via the Cluster Projec-
tion (CP) algorithm defined in Figure 3. An input
to the CP algorithm is the translation set Ts of some
source word s. Also, each translation t ? Ts be-
longs to some set of synsets Dt, where each synset
C ? Dt contains target-language words that may
or may not be translations of s. First, the CP algo-
rithm constructs a source-specific synset B for each
C, which contains only translations of s. Second,
it identifies all correct sense clusters G that are both
coherent and complete with respect to the source-
specific senses B. A sense cluster must correspond
to some synset B ? B to be coherent, and it must
4WordNet version 2.1 is almost identical to ver-
sion 3.0, for Unix-like systems, as described in
http://wordnetcode.princeton.edu/3.0/CHANGES. The lat-
est version 3.1 is not yet available for download.
775
not have a proper superset in B to be complete.5
Figure 4 illustrates the CP algorithm for the trans-
lations of the Spanish source word colocar that ap-
pear in our input dataset.
4 Clustering with K-Means
In this section, we describe an unsupervised method
for inducing translation sense clusters from the us-
age statistics of words in large monolingual and par-
allel corpora. Our method is language independent.
4.1 Distributed SoftK-Means Clustering
As a first step, we cluster all words in the target-
language vocabulary in a way that relates words that
have similar distributional features. Several methods
exist for this task, such as the K-Means algorithm
(MacQueen, 1967), the Brown algorithm (Brown
et al, 1992) and the exchange algorithm (Kneser
and Ney, 1993; Martin et al, 1998; Uszkoreit and
Brants, 2008). We use a distributed implementa-
tion of the ?soft? K-Means clustering algorithm de-
scribed in Lin and Wu (2009). Given a feature vec-
tor for each element (a word type) and the number
of desired clusters K, the K-Means algorithm pro-
ceeds as follows:
1. Select K elements as the initial centroids for
K clusters.
repeat
2. Assign each element to the top M clusters
with the nearest centroid, according to a simi-
larity function in feature space.
3. Recompute each cluster?s centroid by aver-
aging the feature vectors of the elements in that
cluster.
until convergence
4.2 Monolingual Features
Following Lin and Wu (2009), each word to be clus-
tered is represented as a feature vector describing the
distributional context of that word. In our setup, the
5One possible shortcoming of our approach to constructing
reference sets for translation sense clustering is that a cluster
may correspond to a sense that is not shared by the original
source word used to generate the translation set. All translations
must share some sense with the source word, but they may not
share all senses with the source word. It is possible that two
translations are synonymous in a sense that is not shared by the
source. However, we did not observe this problem in practice.
context of a word w consists of the words immedi-
ately to the left and right of w. The context feature
vector of w is constructed by first aggregating the
frequency counts of each word f in the context of
each w. We then compute point-wise mutual infor-
mation (PMI) features from the frequency counts:
PMI(w, f) = log
c(w, f)
c(w)c(f)
where w is a word, f is a neighboring word, and
c(?) is the count of a word or word pair in the cor-
pus.6 A feature vector for w contains a PMI feature
for each word type f (with relative position left or
right) for all words that appears a sufficient number
of times as a neighbor of w. The similarity of two
feature vectors is the cosine of the angle between the
vectors. We follow Lin and Wu (2009) in applying
various thresholds during K-Means, such as a fre-
quency threshold for the initial vocabulary, a total-
count threshold for the feature vectors, and a thresh-
old for PMI scores.
4.3 Bilingual Features
In addition to the features described in Lin and Wu
(2009), we introduce features from a bilingual par-
allel corpus that encode reverse-translation informa-
tion from the source-language (Spanish or Japanese
in our experiments). We have two types of bilin-
gual features: unigram features capture source-side
reverse-translations ofw, while bigram features cap-
ture both the reverse-translations and source-side
neighboring context words to the left and right. Fea-
tures are expressed again as PMI computed from
frequency counts of aligned phrase pairs in a par-
allel corpus. For example, one unigram feature for
place would be the PMI computed from the number
of times that place was in the target side of a phrase
pair whose source side was the unigram lugar. Sim-
ilarly, a bigram feature for place would be the PMI
computed from the number of times that place was
in the target side of a phrase pair whose source side
was the bigram lugar de. These features characterize
the way in which a word is translated, an indication
of its meaning.
6PMI is typically defined in terms of probabilities, but has
proven effective previously when defined in terms of counts.
776
4.4 Predicting Translation Clusters
As a result of softK-Means clustering, each word in
the target-language vocabulary is assigned to a list of
up to M clusters. To predict the sense clusters for a
set of translations of a source word, we apply the CP
algorithm (Figure 3), treating the K-Means clusters
as synsets (Dt).
5 Related Work
To our knowledge, the translation sense clustering
task has not been explored previously. However,
much prior work has explored the related task of
monolingual word and phrase clustering. Uszkor-
eit and Brants (2008) uses an exchange algorithm
to cluster words in a language model, Lin and Wu
(2009) uses distributed K-Means to cluster phrases
for various discriminative classification tasks, Vla-
chos et al (2009) uses Dirichlet Process Mixture
Models for verb clustering, and Sun and Korhonen
(2011) uses a hierarchical Levin-style clustering to
cluster verbs.
Previous word sense induction work (Diab and
Resnik, 2002; Kaji, 2003; Ng et al, 2003; Tufis
et al, 2004; Apidianaki, 2009) relates to our work
in that these approaches discover word senses au-
tomatically through clustering, even using multilin-
gual parallel corpora. However, our task of clus-
tering multiple words produces a different type of
output from the standard word sense induction task
of clustering in-context uses of a single word. The
underlying notion of ?sense? is shared across these
tasks, but the way in which we use and evaluate in-
duced senses is novel.
6 Experiments
The purpose of our experiments is to assess whether
our unsupervised soft K-Means clustering method
can effectively recover the reference sense clusters
derived from WordNet.
6.1 Datasets
We conduct experiments using two bilingual
datasets: Spanish-to-English (S?E) and Japanese-
to-English (J?E). Table 1 shows, for each dataset,
the number of source words and the total number
of target words in their translation sets. The datasets
Dataset No. of src-words Total no. of tgt-words
S?E 52 230
J?E 369 1639
Table 1: Sizes of the Spanish-to-English (S?E) and Japanese-
to-English (J?E) datasets.
are limited in size because we solicited human anno-
tators to filter the set of translations for each source
word. The S?E dataset has 52 source-words with a
part-of-speech-tag distribution of 38 nouns, 10 verbs
and 4 adverbs. The J?E dataset has 369 source-
words with 319 nouns, 38 verbs and 12 adverbs. We
included only these parts of speech because Word-
Net version 2.1 has adequate coverage for them.
Most source words have 3 to 5 translations each.
Monolingual features for K-Means clustering
were computed from an English corpus of Web
documents with 700 billion tokens of text. Bilin-
gual features were computed from 0.78 (S?E) and
1.04 (J?E) billion tokens of parallel text, primar-
ily extracted from the Web using automated paral-
lel document identification (Uszkoreit et al, 2010).
Word alignments were induced from the HMM-
based alignment model (Vogel et al, 1996), initial-
ized with the bilexical parameters of IBM Model 1
(Brown et al, 1993). Both models were trained us-
ing 2 iterations of the expectation maximization al-
gorithm. Phrase pairs were extracted from aligned
sentence pairs in the same manner used in phrase-
based machine translation (Koehn et al, 2003).
6.2 Clustering Evaluation Metrics
The quality of text clustering algorithms can be eval-
uated using a wide set of metrics. For evaluation
by set matching, the popular measures are Purity
(Zhao and Karypis, 2001) and Inverse Purity and
their harmonic mean (F measure, see Van Rijsber-
gen (1974)). For evaluation by counting pairs, the
popular metrics are the Rand Statistic and Jaccard
Coefficient (Halkidi et al, 2001; Meila, 2003).
Metrics based on entropy include Cluster Entropy
(Steinbach et al, 2000), Class Entropy (Bakus et al,
2002), VI-measure (Meila, 2003), Q0 (Dom, 2001),
V-measure (Rosenberg and Hirschberg, 2007) and
Mutual Information (Xu et al, 2003). Lastly, there
exist the BCubed metrics (Bagga and Baldwin,
1998), a family of metrics that decompose the clus-
777
tering evaluation by estimating precision and recall
for each item in the distribution.
Amigo et al (2009) compares the various clus-
tering metrics mentioned above and their properties.
They define four formal but intuitive constraints on
such metrics that explain which aspects of clustering
quality are captured by the different metric families.
Their analysis shows that of the wide range of met-
rics, only BCubed satisfies those constraints. After
defining each constraint below, we briefly describe
its relevance to the translation sense clustering task.
Homogeneity: In a cluster, we should not mix items
belonging to different categories.
Relevance: All words in a proposed cluster should
share some common WordNet sense.
Completeness: Items belonging to the same cate-
gory should be grouped in the same cluster.
Relevance: All words that share some common
WordNet sense should appear in the same cluster.
Rag Bag: Introducing disorder into a disordered
cluster is less harmful than introducing disorder into
a clean cluster.
Relevance: We prefer to maximize the number of
error-free clusters, because these are most easily in-
terpreted and therefore most useful.
Cluster Size vs. Quantity: A small error in a big
cluster is preferable to a large number of small er-
rors in small clusters.
Relevance: We prefer to minimize the total number
of erroneous clusters in a dictionary.
Amigo et al (2009) also show that BCubed ex-
tends cleanly to settings with overlapping clusters,
where an element can simultaneously belong to
more than one cluster. For these reasons, we focus
on BCubed for cluster similarity evaluation.7
The BCubed metric for scoring overlapping clus-
ters is computed from the pair-wise precision and
recall between pairs of items:
P(e, e?) =
min(|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|C(e) ? C(e?)|
R(e, e?) =
min(|C(e) ? C(e?)|, |L(e) ? L(e?)|)
|L(e) ? L(e?)|
where e and e? are two items, L(e) is the set of ref-
erence clusters for e and C(e) is the set of predicted
7An evaluation using purity and inverse purity (extended to
overlapping clusters) has been omitted for space, but leads to
the same conclusions as the evaluation using BCubed.
clusters for e (i.e., clusters to which e belongs). Note
that P(e, e?) is defined only when e and e? share
some predicted cluster, and R(e, e?) when e and e?
share some reference cluster.
The BCubed precision associated to one item is its
averaged pair-wise precision over other items shar-
ing some of its predicted clusters, and likewise for
recall8; and the overall BCubed precision (or recall)
is the averaged precision (or recall) of all items:
PB3 = Avge[Avge?s.t.C(e)?C(e?)6=?[P(e, e
?)]]
RB3 = Avge[Avge?s.t.L(e)?L(e?)6=?[R(e, e
?)]]
6.3 Results
Figure 5 shows the F?-score for various ? values:
F? =
(1 + ?2) ? PB3 ? RB3
?2 ? PB3 + RB3
This graph gives us a trade-off between precision
and recall (? = 0 is exact precision and ? ? ?
tends to exact recall).9
Each curve in Figure 5 represents a particular
clustering method. We include three naive baselines:
ewnc: Each word in its own cluster
aw1c: All words in one cluster
Random: Each target word is assigned M random
cluster id?s in the range 1 to K, then translation
sets are clustered with the CP algorithm.
The curves for K-Means clustering include one
condition with monolingual features alone and two
curves that include bilingual features as well.10 The
bilingual curves correspond to two different feature
sets: the first includes only unigram features (t1),
while the second includes both unigram and bigram
features (t1t2).
Each point on an F? curve in Figure 5 (including
the baseline curves) represents a maximum over two
8The metric does include in this computation the relation of
each item with itself.
9Note that we use the micro-averaged version of F-score
where we first compute PB3 and RB3 for each source-word,
then compute the average PB3 and RB3 over all source-words,
and finally compute the F-score using these averaged PB3 and
RB3.
10All bilingual K-Means experiments include monolingual
features also. K-Means with only bilingual features does not
produce accurate clusters.
778
0.65 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
F ! s
core
 
!  
Spanish-English BCubed Results 
ewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 
0.7 
0.75 
0.8 
0.85 
0.9 
0.95 
0.5 1 1.5 2 2.5 3 3.5 4 4.5 5 
F ! s
core
 
!  
Japanese-English BCubed Results 
ewnc aw1c Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 
Figure 5: BCubed F? plot for the Spanish-English dataset (top) and Japanese-English dataset (bottom).
Source word: ayudar
Monolingual [[aid], [assist, help]] P=1.0, R=0.56
Bilingual [[aid, assist, help]] P=1.0, R=1.0
Source word: concurso
Monolingual [[competition, contest, match], [concourse], [contest, meeting]] P=0.58, R=1.0
Bilingual [[competition, contest], [concourse], [match], [meeting]] P=1.0, R=1.0
Table 2: Examples showing improvements in clustering when we move from K-Means clustering with only monolingual features
to clustering with additional bilingual features.
779
0.79 
0.84 
0.89 
0.94 
0.99 
0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 
Reca
ll 
Precision 
Japanese-English BCubed Results 
Random Kmeans-monolingual Kmeans-bilingual-t1 Kmeans-bilingual-t1t2 ewnc aw1c 
Figure 6: BCubed Precision-Recall scatter plot for the Japanese-English dataset. Each point represents a particular choice of cluster
count K and clusters per word M .
parameters: K, the number of clusters created in the
whole corpus andM , the number of clusters allowed
per word (in M -best soft K-Means). As both the
random baseline and proposed clustering methods
can be tuned to favor precision or recall, we show
the best result from each technique across this spec-
trum of F? metrics. We vary ? to highlight different
potential objectives of translation sense clustering.
An application that focuses on synonym discovery
would favor recall, while an application portraying
highly granular sense distinctions would favor pre-
cision.
Clustering accuracy improves over the baselines
with monolingual features alone, and it improves
further with the addition of bilingual features, for a
wide range of ? values. Our unsupervised approach
with bilingual features achieves up to 6-8% absolute
improvement over the random baseline, and is par-
ticularly effective for recall-weighted metrics.11 As
an example, in a S?E experiment with a K-Means
setting ofK = 4096 : M = 3, the overall F1.5 score
11It is not surprising that a naive baseline like random clus-
tering can achieve a high precision: BCubed counts each word
itself as correctly clustered, and so even trivial techniques that
create many singleton clusters will have high precision. High
recall (without very low precision) is harder to achieve, because
it requires positing larger clusters, and it is for recall-focused
objectives that our technique substantially outperforms the ran-
dom baseline.
increases from 80.58% to 86.12% upon adding bilin-
gual features. Table 2 shows two examples from that
experiment for which bilingual features improve the
output clusters.
The parameter values we use in our experiments
are K ? {23, 24, . . . , 212} and M ? {1, 2, 3, 4, 5}.
To provide additional detail, Figure 6 shows the
BCubed precision and recall for each induced clus-
tering, as the values of K and M vary, for Japanese-
English.12 Each point in this scatter plot represents a
clustering methodology and a particular value for K
and M . Soft K-Means with bilingual features pro-
vides the strongest performance across a broad range
of cluster parameters.
6.4 Evaluation Details
Certain special cases needed to be addressed in order
to complete this evaluation.
Target words not in WordNet: Words that did not
have any synset in WordNet were each assigned to a
singleton reference cluster.13 The S?E dataset has
only 2 out of 225 target types missing in WordNet
and the J?E dataset has only 55 out of 1351 target
12Spanish-English precision-recall results are omitted due to
space constraints, but depict similar trends.
13Note that certain words with WordNet synsets also end up
in their own singleton cluster because all other words in their
cluster are not in the translation set.
780
types missing.
Target words not clustered by K-Means: The K-
Means algorithm applies various thresholds during
different parts of the process. As a result, there
are some target word types that are not assigned
any cluster at the end of the algorithm. For ex-
ample, in the J?E experiment with K = 4096
and with bilingual (t1 only) features, only 49 out
of 1351 target-types are not assigned any cluster by
K-Means. These unclustered words were each as-
signed to a singleton cluster in post-processing.
7 Identifying Usage Examples
We now briefly consider the task of automatically
extracting usage examples for each predicted clus-
ter. We identify these examples among the extracted
phrase pairs of a parallel corpus.
Let Ps be the set of source phrases containing
source word s, and letAt be the set of source phrases
that align to target phrases containing target word
t. For a source word s and target sense cluster G,
we identify source phrases that contain s and trans-
late to all words in G. That is, we collect the set
of phrases Ps ?
?
t?GAt. We use the same parallel
corpus as we used to compute bilingual features.
For example, if we consider the cluster [place, po-
sition, put] for the Spanish word colocar, then we
find Spanish phrases that contain colocar and also
align to English phrases containing place, position,
and put somewhere in the parallel corpus. Sample
usage examples extracted by this approach appear in
Figure 7. We have not performed a quantitative eval-
uation of these extracted examples, although quali-
tatively we have found that the technique surfaces
useful phrases. We look forward to future research
that further explores this important sub-task of auto-
matically generating bilingual dictionaries.
8 Conclusion
We presented the task of translation sense clustering,
a critical second step to follow translation extraction
in a pipeline for generating well-structured bilingual
dictionaries automatically. We introduced a method
of projecting language-level clusters into clusters for
specific translation sets using the CP algorithm. We
used this technique both for constructing reference
clusters, via WordNet synsets, and constructing pre-
debajo
["below","beneath"]    ? debajo de la superficie (below the surface)
["below","under"]     ? debajo de la l?nea (below the line)
["underneath"]     ? debajo de la piel (under the skin)
??
["break"]     ? ???? ?? ? ?? ?? ?? ? ? ?? ?? . 
(I worked hard and I deserve a good break.)
["recreation"]     ? ?? ? ?? ? ?? ?? 
(Traditional healing and recreation activities)
["rest"]     ? ??? ? ?? ?? ?? ? ?? ?? . 
(Bed rest is the only treatment required.)
??
["application"]     ? ??????? ?? ?? 
(Computer-aided technique)
["use","utilization"]     ? ?? ? ?? ?? ? ?? ?? 
(Promote effective use of land)
??
["draw","pull"]     ? ???? ? ?? 
(Draw the curtain)
["subtract"]     ? A ?? B ? ?? 
(Subtract B from A)
["tug"]     ? ? ? ??? ?? 
(Tug at someone's sleeve)
Figure 7: Usage examples for Spanish and Japanese words and
their English sense clusters. Our approach extracts multiple
examples per cluster, but we show only one. We also show
the translation of the examples back into English produced by
Google Translate.
dicted clusters from the output of a vocabulary-level
clustering algorithm.
Our experiments demonstrated that the soft K-
Means clustering algorithm, trained using distribu-
tional features from very large monolingual and
bilingual corpora, recovered a substantial portion of
the structure of reference clusters, as measured by
the BCubed clustering metric. The addition of bilin-
gual features improved clustering results over mono-
lingual features alone; these features could prove
useful for other clustering tasks as well. Finally, we
annotated our clusters with usage examples.
In future work, we hope to combine our cluster-
ing method with a system for automatically gen-
erating translation sets. In doing so, we will de-
velop a system that can automatically induce high-
quality, human-readable bilingual dictionaries from
large corpora using unsupervised learning methods.
Acknowledgments
We would like to thank Jakob Uszkoreit, Adam
Pauls, and the anonymous reviewers for their helpful
suggestions.
781
References
Enrique Amigo, Julio Gonzalo, Javier Artiles, and Felisa
Verdejo. 2009. A comparison of extrinsic clustering
evaluation metrics based on formal constraints. Infor-
mation Retrieval, 12(4):461486.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of EACL.
A. Bagga and B. Baldwin. 1998. Entity-based cross-
document coreferencing using the vector space model.
In Proceedings of COLING-ACL.
J. Bakus, M. F. Hussin, and M. Kamel. 2002. A SOM-
based document clustering using phrases. In Proceed-
ings of ICONIP.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,
Jenifer C. Lai, and Robert L. Mercer. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467479.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Mona Diab and Philip Resnik. 2002. An unsupervised
method for word sense tagging using parallel corpora.
In Proceedings of ACL.
B.E. Dom. 2001. An information-theoretic external
cluster-validity measure. In IBM Technical Report RJ-
10219.
M. Halkidi, Y. Batistakis, and M. Vazirgiannis. 2001. On
clustering validation techniques. Journal of Intelligent
Information Systems, 17(2-3):107?145.
Hiroyuki Kaji. 2003. Word sense acquisition from bilin-
gual comparable corpora. In Proceedings of NAACL.
Reinherd Kneser and Hermann Ney. 1993. Improved
clustering techniques for class-based statistical lan-
guage modelling. In Proceedings of the 3rd European
Conference on Speech Communication and Technol-
ogy.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of NAACL.
Dekang Lin and Xiaoyun Wu. 2009. Phrase clustering
for discriminative learning. In Proceedings of ACL.
J. B. MacQueen. 1967. Some methods for classifica-
tion and analysis of multivariate observations. In Pro-
ceedings of 5th Berkeley Symposium on Mathematical
Statistics and Probability.
Sven Martin, Jorg Liermann, and Hermann Ney. 1998.
Algorithms for bigram and trigram word clustering.
Speech Communication, 24:19?37.
M. Meila. 2003. Comparing clusterings by the variation
of information. In Proceedings of COLT.
George A. Miller. 1995. Wordnet: A lexical database for
English. In Communications of the ACM.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL.
Andrew Rosenberg and Julia Hirschberg. 2007. V-
measure: A conditional entropy-based external cluster
evaluation measure. In Proceedings of EMNLP.
Michael Steinbach, George Karypis, and Vipin Kumar.
2000. A comparison of document clustering tech-
niques. In Proceedings of KDD Workshop on Text
Mining.
Lin Sun and Anna Korhonen. 2011. Hierarchical verb
clustering using graph factorization. In Proceedings
of EMNLP.
Dan Tufis, Radu Ion, and Nancy Ide. 2004. Fine-grained
word sense disambiguation based on parallel corpora,
word alignment, word clustering and aligned word-
nets. In Proceedings of COLING.
Jakob Uszkoreit and Thorsten Brants. 2008. Distributed
word clustering for large scale class-based language
modeling in machine translation. In Proceedings of
ACL.
Jakob Uszkoreit, Jay Ponte, Ashok Popat, and Moshe Du-
biner. 2010. Large scale parallel document mining for
machine translation. In Proceedings of COLING.
C. Van Rijsbergen. 1974. Foundation of evaluation.
Journal of Documentation, 30(4):365?373.
Mariano Vela?zquez de la Cadena, Edward Gray, and
Juan L. Iribas. 1965. New Revised Vela?zques Spanish
and English Dictionary. Follet Publishing Company.
Andreas Vlachos, Anna Korhonen, and Zoubin Ghahra-
mani. 2009. Unsupervised and constrained Dirichlet
process mixture models for verb clustering. In Pro-
ceedings of the Workshop on Geometrical Models of
Natural Language Semantics.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
W. Xu, X. Liu, and Y. Gong. 2003. Document-clustering
based on non-negative matrix factorization. In Pro-
ceedings of SIGIR.
Y. Zhao and G. Karypis. 2001. Criterion functions for
document clustering: Experiments and analysis. In
Technical Report TR 01-40, Department of Computer
Science, University of Minnesota, Minneapolis, MN.
782
Proceedings of NAACL-HLT 2013, pages 1185?1195,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Supervised Learning of Complete Morphological Paradigms
Greg Durrett?
Computer Science Division
University of California, Berkeley
gdurrett@cs.berkeley.edu
John DeNero
Google, Inc.
denero@google.com
Abstract
We describe a supervised approach to predict-
ing the set of all inflected forms of a lexical
item. Our system automatically acquires the
orthographic transformation rules of morpho-
logical paradigms from labeled examples, and
then learns the contexts in which those trans-
formations apply using a discriminative se-
quence model. Because our approach is com-
pletely data-driven and the model is trained
on examples extracted from Wiktionary, our
method can extend to new languages without
change. Our end-to-end system is able to pre-
dict complete paradigms with 86.1% accuracy
and individual inflected forms with 94.9% ac-
curacy, averaged across three languages and
two parts of speech.
1 Introduction
For natural languages with rich morphology, knowl-
edge of how to inflect base forms is critical for both
text generation and analysis. Hand-engineered, rule-
based methods for predicting inflections can offer
extremely high accuracy, but they are laborious to
construct and do not exist with full lexical cover-
age in all languages. By contrast, a large number
of example inflections are freely available in a semi-
structured format on the Web. The English Wik-
tionary1 is a crowd-sourced lexical resource that in-
cludes complete inflection tables for many lexical
items in many languages. We present a supervised
?Research conducted during an internship at Google.
1http://en.wiktionary.org
system that, given only data from Wiktionary, au-
tomatically discovers and learns to apply the ortho-
graphic transformations governing a language?s in-
flectional morphology.2
Our data-driven approach is designed to extend to
any language for which we have a substantial num-
ber of example inflection tables. The design of our
model is guided by three structural assumptions:
1. The inflections of many lexical items are
governed by a few repeated morphological
paradigms.
2. A morphological paradigm can be decom-
posed into independent orthographic transfor-
mation rules (including prefix, suffix, and stem
changes), which are triggered by orthographic
context.
3. A base form is transformed in consistent, cor-
related ways to produce its inflected variants.
Learning proceeds in two stages that both utilize
the same training set of labeled inflection tables.
First, an inventory of interpretable transformation
rules is generated by aligning each base form to all
of its inflected forms. Second, a semi-Markov con-
ditional random field (CRF) (Sarawagi and Cohen,
2004) is trained to apply these rules correctly to un-
seen base forms. As we demonstrate experimentally,
the CRF is most effective when jointly predicting all
inflected forms of a lexical item together, forcing the
system to adopt a single consistent analysis of each
base form.
2See http://eecs.berkeley.edu/~gdurrett for
our datasets and code.
1185
Previous work has also described supervised and
semi-supervised approaches to predicting inflec-
tional morphology (Yarowsky and Wicentowski,
2000; Wicentowski, 2004; Dreyer and Eisner, 2011).
Our approach differs primarily in its use of auto-
matically extracted morphological rules and our dis-
criminative prediction method which jointly mod-
els entire inflection tables. These modeling choices
are directly inspired by the data setting: Wiktionary
contains complete inflection tables for many lexical
items in each of a large number of languages, so it
is natural to make full use of this information with a
joint model of all inflected forms.
We evaluate our predictions on held-out Wik-
tionary inflection tables for three languages and two
parts of speech. Our language-independent method
predicts inflections for unseen base forms with ac-
curacies ranging from 88.9% (German nouns) to
99.7% (Spanish verbs). For comparability with pre-
vious work, we also evaluate our approach on Ger-
man verb forms in the CELEX lexical database
(Baayen et al, 1995). Our approach outperforms
the semi-supervised hierarchical Bayesian model of
Dreyer and Eisner (2011), while employing scal-
able exact inference and interpretable transforma-
tion rules.
2 Background: Inflectional Morphology
Among the valid words W and parts of speech P
in a language, the base forms B ? W ? P are the
canonical forms of the language?s lexical items. A
base form relates to an inflected form via an inflec-
tional relation (b, w, a), where b ? B is a base form,
w ? W is the inflected form, and a is a vector of
morphological attributes. An inflection table T (b) is
the set of all such relations for a base form b.
Two partial inflection tables are shown in Table 1,
for the base forms (infinitives) of the German verbs
machen and schleichen, containing such inflec-
tional relations as (machen, mache, [1P,PRES,SING])
and (machen, gemacht, [PAST PART.]). Only a
small sample of the valid attribute combinations are
shown; a full inflection table for a German verb in
our Wiktionary dataset contains 27 relations.
The goal of this paper is to learn how to map b
to T (b). We generate candidate inflection tables by
applying compact, interpretable orthographic trans-
INFINITIVE machen schleichen
1P,PRES,SING mache schleiche
2P,PRES,SING machst schleichst
3P,PRES,SING macht schleicht
PAST PART. gemacht geschlichen
... ... ...
Table 1: Two partial inflection tables for the German
verbs machen (to make) and schleichen (to crawl).
formation rules that have been extracted from ex-
ample tables. As an example of our rule applica-
tion process, to inflect machen appropriately in the
forms listed in Table 1, one could apply the follow-
ing rules:
1. Replace a suffix -en with -e for first person, -st
for second person, -t for third person, and -t for
the past participle.
2. Add a prefix ge- for the past participle.
To inflect schleichen, one could apply a larger set of
three rules:
1. Replace a suffix -en with -e for first person, -st
for second person, -t for third person, and -en
for the past participle.
2. Add a prefix ge- for the past participle.
3. Delete the first e for the past participle.
The inflection tables of other German verbs can be
generated using precisely the same rules above, and
different inflection patterns may share rules, such as
the repeated rule 2. This example illustrates one of
our chief assumptions, that the inflections of many
base forms can be modeled with a small number of
such rules, applied in various combinations.
3 Learning Transformation Rules
From a training set of inflection tables
{T (b1), ..., T (bn)}, our system learns a set of
orthographic transformation rules. A rule is a func-
tion R : s, a? s? that takes as input a substring s of
a base form and an attribute vector a and outputs a
replacement substring s?. The suffix transformation
from Section 2 for machen can be described using a
1186
Algorithm 1 Learning rules from examples.
Input: n training instances T (b1), . . . , T (bn)
Rule setR ? {}
for i? 1 to n do
Changed source spans C ? {}
for all a ? A do
Ca ? PROJECTSPANS(ALIGN(bi, Ta(bi)))
C ? UNIONSPANS(C,Ca)
end for
for all c ? C do
R ? R? {EXTRACTRULE(c)}
end for
end for
return R
rule with four entries:
R(en, [1P,PRES,SING]) = e
R(en, [2P,PRES,SING]) = st
R(en, [3P,PRES,SING]) = t
R(en, [PAST PART.]) = t
Our method for learning rules from examples is
described in Algorithm 1 and depicted in Figure 1.
We extract rules from each observed inflection table
T (bi) independently, and the final set of rules is sim-
ply the union of the sets of rules learned from each
example. The procedure for a single inflection table
has three steps:
Alignment: Align each inflected form to the base
form with an iterated edit-distance algorithm.
Span Merging: Extract the set of spans of the
base form that changed to produce the inflected
form, and take their union across all attribute vec-
tors to identify maximal changed spans.
Rule Extraction: Extract a rule for each maxi-
mal changed span.
Alignment. For each setting of attributes a, we
find the lowest-cost transformation of the base form
b into the corresponding inflected form Ta(b) using
single-character insertions, deletions, and substitu-
tions. This minimum edit distance calculation is
computed via the following recurrence, where i is
an index into the base form b and j is an index into
s c h l e i c h e  n
s c h l e i c h e
s c h l     i c h
g e s c h l     i c h e n
s c h l  e  i c h e n
s c h l e i c h e n
s c h l e i c h e
s c h l  i c h
g e s c h l  i c h e n
...
...
Alignment
Span Merging
s c h l  e  i c h  e n
||||||||| D
||| ||||
D D D
|||| |||||
I I D
Rule Extraction
...
s c h l
i c h
e n
es c h l
s c h l
s c h l
e
e e ni c h
g e
i c h
i c h
Figure 1: Demonstration of the rule extraction algorithm
with the base form schleichen and three inflected forms:
schleiche (first person singular present), schlich (first per-
son singular past), and geschlichen (past participle). We
ideally want to extract appropriate transformation rules
like those described in Section 2. In the alignment step,
we minimize the edit distance between each inflected
form and the base form to identify changed spans. In
the span merging step, we project changes onto the base
form and take the union of adjacent or overlapping spans.
In the rule extraction step, we project these spans back
onto the inflected forms to identify transformation rules.
an inflected form Ta(b):
L(i, j) = min{L(i, j ? 1) + I,
L(i? 1, j) +D,
L(i? 1, j ? 1) + S(i, j)}
I , D, and S are insertion, deletion, and substi-
tion costs, respectively. Tracing the computation of
L(len(b), len(Ta(b))) yields an optimal sequence of
edit operations. The alignments output by this pro-
cedure are depicted in the first panel of Figure 1.
The most typical cost scheme sets I = 1, D = 1,
and S(i, j) = (1 ? I[match(i, j)]), i.e. 0 if the ith
character of b is the same as the jth character of
Ta(b), and 1 otherwise. However, this cost scheme
did not yield intuitive alignments for some of our
training instances. For example, in the case of the
verb denken aligning to its past participle gedacht,
1187
the initial d and g will be aligned and the follow-
ing e?s will be aligned, preventing the algorithm
from recognizing the addition of the prefix ge-. To
solve this problem, we use a dynamic edit distance
cost scheme in which I , D, and unmatched substi-
tutions all have a cost of 0. Matched substitutions
have a negative cost ?ci, where i is the index in the
base form and ci is the number of other inflected
forms for which i is aligned to a matching char-
acter. The inflected forms are iteratively realigned
with the base form until the ci converge (Eisner,
2002; Oncina and Sebban, 2006). This cost scheme
encourages a single consistent analysis of the base
form as it aligns to all of its inflected forms.
Span Merging. From each aligned pair of words,
the PROJECTSPANS procedure identifies sequences
of character edit operations with contiguous spans of
the base form. We construct a set of changed spans
Ca of b as follows: include the span (i, j) if and
only if no characters between i and j were aligned
to matching characters in Ta(b) and no smaller span
captures the same set of changes. Projected spans
for the inflected forms of schleichen are shown in
the ?Span Merging? panel of Figure 1.
The UNIONSPANS procedure combines two sets
of spans by iteratively merging any two spans that
are overlapping or adjacent. Repeating this proce-
dure to accumulate spans for each setting of a yields
the set C of maximal changed spans for a base form.
Any span inC is bordered either by word boundaries
or by characters that are match-aligned in every in-
flected form, meaning that we have isolated a region
characterized by a particular orthographic transfor-
mation.
Rule Extraction. The final step of Algorithm 1
extracts one rule for each maximal changed span of
the base form. The Rule Extraction panel of Figure 1
depicts how maximal changed spans in the base
form correspond to transformation rules. Because
UNIONSPANS guarantees that match-aligned char-
acters border each maximal changed span, there is
no ambiguity about the segmentation of transforma-
tions. The EXTRACTRULE procedure produces one
rule R(s, a) corresponding to each changed span.
Table 2 contains examples of the transformation
rules we extract from German verbs. The extracted
Attributes Suffix Stem Pre.
INFINITIVE en en en n e
1P,PRES,SING e e e e e
1P,PAST,SING te te te
2P,PRES,SING st t st st e
2P,PAST,SING test test st test
3P,PRES,SING t t t t e
3P,PAST,SING te te te
PAST PART. t t en t ge
... ... ... ... ... ... ...
Label Rsuf,1 Rsuf,2 Rsuf,3 Rsuf,4 Rst,1 Rpre,1
Table 2: Each column is an example of a morphological
transformation rule extracted by our approach. The first
four are suffix changes; these apply to, in order, regular
verbs such as machen, verbs ending in -zen or -sen such as
setzen, verbs such as schleichen and beheben, and verbs
ending in -ern or -eln such as sprenkeln. The stem change
occurs in strong verbs of the first class such as schleichen,
greifen, and streiten. Finally, we learn that ge- can be
added as a prefix to indicate the past participle.
rules are interpretable descriptions of common in-
flection patterns.
4 Applying Transformation Rules
For a novel base form b, the inventory of learned
transformation rules R = {R(s, a)} can typically
generate many candidate inflection tables T (b) for
us to choose between. A rule can potentially apply
to a base form in a number of places; we define an
anchored rule A = (R, i, j, b) to be the application
of R to a span (i, j) in b. A is only a valid anchoring
if the substring of b between i and j matches the
input of rule R.
Given a set A of non-overlapping anchored rules
for b, each entry of T (b) can be deterministically
produced by rewriting each anchored rule?s span
(i, j) using the ruleR. Therefore, the task of predict-
ing T (b) is equivalent to selecting a coherent subset
A of anchored rules from the set of all possible an-
chored rules for this base form. By coherent, we
mean that the selected rules are anchored to non-
overlapping, non-adjacent3 spans of b. Figure 2a
shows two coherent anchored rule subsets for schle-
ichen (the top one being correct). Underlining indi-
3During rule extraction, any adjacent changed spans are
merged into a single rule. Disallowing adjacent spans here
therefore prevents us from synthesizing new rules.
1188
cates length-one spans S = (i, i + 1, b) that are not
part of any anchored rule in A. We denote the set of
such spans by S(A); this set is uniquely defined for
the given base form by the selected anchored rules.
We use a log-linear model to place a conditional
distribution over valid anchored rule subsetsA given
the base form b:
pw(A|b) ? expwT
?
?
?
A?A
?(A) +
?
S?S(A)
?(S)
?
?
where w is a weight vector, ?(A) computes a fea-
ture vector for anchored rule A, and ?(S) computes
a feature vector for preserved spans S. We train
this model to maximize the regularized conditional
log-likelihood of the training data, which consists of
base forms bi and gold subsets of anchored rulesA?i
derived using Algorithm 1 on the gold inflection ta-
bles.
L(w) =
n?
i=1
log p(A?i |bi) +
?
2
?w?2.
We find w? = argmaxw L(w) using L-BFGS (Liu
and Nocedal, 1989), which requires computing ?L?w .This gradient takes the standard form of the differ-
ence between gold feature counts and expected fea-
ture counts under the model:
?L
?w =
n?
i=1
?
?
?
?
?
A?A?i
?(A) +
?
S?S(A?i )
?(S)
?
? ?
?
?
?
A?A(R,b)
Epw?(A) +
?
S?S(b)
Epw?(S)
?
?
?
?? ?w
where, by a slight abuse of notation, S(b) is the set
of all length-one spans of b.
In general, the normalizer of pw and the expec-
tation over pw cannot be computed directly, since
there may be exponentially many coherent subsets
of anchored rules. However, we note that A and
its corresponding S(A) form a segmentation of the
base form b, with features decomposing over indi-
vidual segments. Our model can therefore be viewed
a semi-Markov model over b (Sarawagi and Co-
hen, 2004); more precisely, a zeroth-order semi-
Markov model, since we do not include features on
state transitions. At training time, we can use the
s c h l e i c h e n
s c h l e i c h e n
a)
b)
s c h l e i c h e n
Rpre,1
Rst,1
Rst,1
Rpre,1
Rst,1:l[e]
Rst,1:[e]i
Rsuf,3
S:c[h]
S:[h]e
 
 
 
 
Figure 2: a) Two possible anchored rule sets for schle-
ichen. The indicated rules are prefix, stem, and suffix
rules as found in Table 2. The top anchoring is correct,
while the bottom misplaces the stem change and does not
include a suffix change. Underlined letters indicate pre-
served spans S. b) Bigram context features computed by
?(Rst,1), where the stem change is applied to the high-
lighted e, and similar features computed by ?(S) for the
underlined h, which is unchanged by the applied rules.
forward-backward algorithm for semi-Markov mod-
els to compute the gradient of pw, and at test time,
the Viterbi algorithm can exactly find the best rule
subset under the model: A? = argmaxA pw(A|b).
Features. The feature function ? captures contex-
tual information in the base form surrounding the
site of the anchored rule application. It is well under-
stood that different morphological rules may require
examining different amounts of context to apply cor-
rectly (Kohonen, 1986; Torkkola, 1993; Shalonova
and Gole?nia, 2010); to this end, we will use local
character n-gram features, which have been success-
fully applied to related problems (Jiampojamarn et
al., 2008; Dinu et al, 2012).
A sketch of our feature computation scheme is
shown in Figure 2b. Our basic feature template is
an indicator on a character n-gram with some off-
set from the rule application site, conjoined with the
identity of the rule R being applied. Our features
look at variable amounts of context: we include fea-
tures on unigrams through 4-grams, starting up to
five letters behind the anchored rule span and end-
ing up to five letters past the anchored rule span.
These features can model most hand-coded morpho-
logical rules, but are in many cases more numerous
than necessary. However, we find that regularization
is effective at balancing high model capacity with
generalization, and reducing the size of the feature
set empirically harms overall accuracy.
We also employ factored features that only look at
predictions over particular inflected forms; these are
1189
coarser features that are shared between two rules
when they predict the same orthographic change for
a particular setting of attributes. These features are
indicators onRa (the restriction ofR to attributes a),
the context n-gram, and its offset from the span.
The feature function ? is almost identical to ?,
but instead of indicating a rule appearing in some
context, it instead indicates that a particular length-
one span is being preserved in its n-gram context.
Examples of ? features are shown in Figure 2b.
Pruning. Thus far, the only requirement on an an-
choring A is that the source side of its rule R must
match the span it is anchored to in the base form
b. We further filter the set of possible A as follows:
if every occurrence of R in the training set is pre-
ceded by the same character (including a start-of-
word character) or followed by the same character
(including an end-of-word character), any anchoring
A must be preceded or followed accordingly. This
stipulation is most useful in restricting prefixing or
suffixing insertions, which have an empty source
side, to apply only at the beginnings or ends of base
forms (rather than at arbitrary points throughout). In
doing so, we prune out many erroneous anchored
rules and speed up inference substantially without
prohibiting correct rule applications.
5 Wiktionary Morphology Data
Our primary source of supervised inflection table
data is English Wiktionary. The collective editors
of English Wiktionary have created complete, con-
sistent inflection tables for many lexical items in
many languages. Previous work has successfully
parsed other information from Wiktionary, such as
parts of speech, glosses, and etymology (Zesch et
al., 2008; Li et al, 2012); however, to our knowl-
edge, inflection tables have not previously been ex-
tracted in a format easily amenable to natural lan-
guage processing applications. These inflection ta-
bles are challenging to extract because the layout of
tables varies substantially by language (beyond the
expected changes due to differing sets of relevant
morphological attributes), and some tables contain
annotations in addition to word forms.
In order to extract this data, we built a Wiktionary
scraper which generates fully structured output by
interpreting the templates that generate the rendered
Lang/POS Base forms Infl. forms per base
DE-NOUNS 2764 8
DE-VERBS 2027 27
ES-VERBS 4055 57
FI-NOUNS 40589 28
FI-VERBS 7249 53
Table 3: Number of full morphology tables extracted
from Wiktionary for each language and part of speech
pair that we considered, as well as the number of inflected
forms associated with each base form.
inflection tables. Table 3 gives statistics for the num-
ber of base forms and inflected forms extracted from
Wikitionary. When multiple forms were listed in an
inflection table for the same base form and attribute
vector, we selected the first in linear order; applying
the same principle, we also kept only the first inflec-
tion table when more than one was listed for a given
base form. Furthermore, base forms and inflected
forms separated by spaces, hyphens, or colons were
discarded. As a result, we discarded German verb-
preposition compounds such as ablehnen4 and Span-
ish reflexives such as lavarse.
6 Experiments
We evaluate our model under two experimental con-
ditions. First, we use the German verb lexicon in
the CELEX lexical database (Baayen et al, 1995)
with the same train/test splits as Dreyer and Eisner
(2011). Second, we train on our Wiktionary data de-
scribed in Section 5 and evaluate on held-out forms
from this same dataset.
In each case, we evaluate two variants of our
model in order to examine the importance of jointly
modeling the production of the entire inflection ta-
ble. Our JOINT model is exactly as defined in Sec-
tion 4. For our FACTORED model, the dictionary of
rules is extracted separately for each setting of the
attributes a; i.e., we run the entire procedure in Sec-
tion 3 with only one inflected form at a time and
forego the UNIONSPANS step. A separate predic-
tion model is trained for each a and so features are
not shared across multiple predictions as they are in
the JOINT case. Note that this FACTORED approach
4This class of verbs was also ignored by Dreyer and Eisner
(2011).
1190
No. of training examples
50 100 200
NAI?VE 87.61 87.70 87.70
FACTORED 89.61 91.40 92.64
JOINT 90.47 92.31 93.18
DE11 89.9 91.5
DE11+CORPUS 90.9 92.2
ORACLE 95.47 96.09 96.77
Table 4: Accuracies on reconstructing individual in-
flected forms in CELEX, averaged over the 5415 inflec-
tion tables in each of 10 test sets. Three training set
sizes are reported. DE11 indicates a reported result from
Dreyer and Eisner (2011), with blank results unreported
in that work. Our FACTORED model is able to do approx-
imately as well as the DE11 baseline method, and our
JOINT model performs better yet, performing compara-
bly to DE11+CORPUS, which uses additional monolin-
gual text. All models substantially outperform the NAI?VE
suffixing baseline. The relatively low ORACLE accuracy
indicates that some errors arise from failing to apply rules
that are not attested in these small training sets.
can produce inflection tables that the JOINT model
cannot, due to its ability to ?mix and match? ortho-
graphic changes in the same inflection table.
We also evaluate a NAI?VE method for applying
the joint rules which selects the most common suffix
rule available after pruning.5 Finally, we report the
ORACLE accuracy attainable with the morphologi-
cal rule dictionary of the JOINT model.
For our conditional likelihood objective, we use
? = 0.0002; this parameter and the feature set were
tuned on a small development set and held fixed for
all experiments.
6.1 CELEX Experiments
Dreyer and Eisner (2011) construct ten train/test
splits of the 5615 German verb forms in the CELEX
lexical database, keeping 200 forms for training in
each case, which they further subsample. These ran-
dom splits serve to control for instability due to the
small training set sizes. Each infinitive verb form
has 22 corresponding inflected forms capturing vari-
ation such as person, number, mood, and tense.
5For example, for German verbs ending in -en, this applies
the most regular -en suffix change, that exhibited by machen
and many other verbs.
Table 4 shows our results compared to those of
Dreyer and Eisner (2011). The FACTORED model
performs on par with the DE11 baseline model, but
the stronger performance of the JOINT model in-
dicates that making joint predictions is important.
With 100 training examples, our model is able to
equal the performance of DE11+CORPUS, which
additionally uses ten million tokens of monolingual
German text.
We emphasize that this is not the data condition
for which our model was designed. It is unfavor-
able for two reasons: first, feature-rich models can
be learned more stably on larger training sets, and
second, the train/test splits are chosen randomly, and
therefore the test sets may contain completely irreg-
ular verbs using morphological rules that we have
never observed. As can be seen from the ORA-
CLE results in Table 4, a substantial fraction of the
missed test examples cannot be produced using our
extracted rules simply because we have not seen the
relevant examples; in many cases, even a human
could not generalize correctly from the given ex-
amples without exploiting external knowledge of the
German language.
6.2 Held-Out Wiktionary Data
Our algorithm was designed with the fundamental
assumption that the training set should be a com-
prehensive description of the morphology of a given
language, which is not true for the CELEX data. In
order to evaluate on a broader set of languages under
these training conditions, we turn to our Wiktionary
data. For each language and part of speech, we train
on all but 400 inflection tables, holding back 200 ex-
amples as a development set and 200 examples as a
blind test set.6 The forms selected for the develop-
ment and test data were purposely chosen not to be
among the 200 most frequently occurring forms in
the language, since these common cases can be eas-
ily memorized from Wiktionary.
Results are shown in Table 5. As with the CELEX
results, we see that the joint prediction improves ac-
curacy over the factored model, obtaining a 9% er-
ror reduction on individual forms and a 35% error
reduction on exact match. The more pronounced
6For Finnish nouns, because there were so many inflection
tables, we trained only on the first 6000 examples. Using more
examples did not significantly change performance.
1191
Exact table match Individual form accuracy
Lang/POS NAI?VE FACT. JOINT ORACLE NAI?VE FACT. JOINT ORACLE
DE-VERBS 42.0 74.5 85.0 99.5 89.13 94.76 96.19 99.98
DE-NOUNS 12.0 74.0 79.5 98.5 49.06 88.31 88.94 99.25
ES-VERBS 81.5 93.5 95.0 99.5 97.20 99.61 99.67 99.99
FI-VERBS 33.5 82.0 87.5 99.5 75.32 97.23 96.43 99.86
FI-NOUNS 31.0 69.0 83.5 100.0 61.23 92.14 93.41 100.00
AVG 40.0 78.6 86.1 99.4 74.39 94.41 94.93 99.81
Table 5: Accuracies on reconstructing complete inflection tables and individual inflected forms for held-out base forms
in our Wiktionary dataset. Results are shown for our fully JOINT model, a FACTORED model that predicts individual
inflected forms independently, a NAI?VE baseline that picks the most common applicable suffix rule, and an ORACLE
that selects the best inflection table within our model?s capacity. For each language and part of speech, regardless of
training set size, evaluation is based on a blind test set of 200 held-out forms.
improvement on exact match is unsurprising, since
we expect that the joint predictions should get in-
flection tables correct in an ?all-or-nothing? fashion,
whereas factored predictions are more likely to re-
flect divergent feature weights of the different com-
ponent models. The NAI?VE baseline performs rather
poorly overall, indicating our algorithm is being so-
phisticated about applying more than just the most
common changes. Finally, we note that the ORA-
CLE performance is much higher in this case than
on the CELEX data, confirming our intuition that
with the appropriate level of supervision our model
at least has the capacity to make correct predictions
in almost every case.
6.3 Error Analysis
We conducted an error analysis on the output of
our JOINT model on German nouns. From 2364
paradigms, we learn 53 different orthographic trans-
formation rules, of which our 200-example develop-
ment set exhibits 14.7
On our development set, 196 inflection tables are
within the capacity of our model. Of those 196, 159
are exactly correct. In Table 6, we show the top
six rules by frequency in the development set, along
7Nineteen of our 53 extracted rules only occur on one ex-
ample; this suggests a few reasons that fewer rules are applied
than are extracted. First, very common base forms with irreg-
ular morphology may give rise to completely irregular rules.
Second, our edit distance alignment procedure can sometimes
merge two adjacent rules if the orthographic context is such that
there are multiple minimum-cost analyses. Finally, errors and
inconsistencies in Wiktionary can yield nonsense rules that are
never applied elsewhere.
NOM,SING a
NOM,PL n e a? en
ACC,SING a
ACC,PL n e a? en
DAT,SING a
DAT,PL n en a? n en
GEN,SING es a s
GEN,PL n e a? en
Example Klasse Krieg Haus Nutzer Frau
Gold 49 48 26 26 20
Prec 95.7 72.9 88.0 82.8 87.0
Rec 91.8 89.6 84.6 92.3 100.0
F1 93.8 80.4 86.3 87.3 93.0
Table 6: Breakdown of errors by morphological rule be-
ing applied by the JOINT model on the DE-NOUNS devel-
opment set. We show the rule itself, treating the nomina-
tive singular as the base form, an example of a German
word using that rule, and then the model?s accuracy at
predicting applications of that rule. Errors are spread out
over many rules, but it generally appears that common
rules are to blame for the errors that are made, due in
large part to gender confusion in this case.
with the precision, recall, and F-measure that our
model attains for each rule.8 These rules are mostly
interpretable: for example, the first two columns
correspond to common suffix rules for feminine and
masculine nouns, respectively. Our model?s per-
formance is consistently high for each of the rules
shown, including a stem change (a changing to a?
in plural forms), providing further evidence that our
model is useful for modeling rarer morphological
8Gold rules are obtained by running our rule extraction pro-
cedure over the examples in question.
1192
paradigms as well as more common ones.
As a concrete example of an error our model does
make, Lo?we (lion) is incorrectly predicted to have
the first suffix, instead of the correct suffix (not
shown) which adds an -n for accusative, genitive,
and dative singular as well. However, making this
prediction correctly is essentially beyond the capac-
ity of a model based purely on orthography. Words
ending in -e are commonly feminine, and none of
our other training examples end in -we, so guess-
ing that Lo?we follows a common feminine inflec-
tion pattern is reasonable (though Lo?we is, in fact,
masculine). Disambiguating this case requires ei-
ther features on observed genders, a more complex
model of the German language, or observing the
word in a large corpus. Generally, when the model
fails, as in this case, it is because of a fundamental
linguistic information source that it does not have
access to.
7 Related Work
Much of the past work on morphology has focused
on concatenative morphology using unsupervised
methods (Goldsmith, 2001; Creutz and Lagus, 2007;
Monson, 2008; Poon et al, 2009; Goldwater et al,
2009) or weak forms of supervision (Snyder and
Barzilay, 2008). These methods can handle aspects
of derivational morphology that we cannot, such as
compounding, but we can handle a much larger sub-
set of inflectional morphology, including more com-
plex prefix and suffix rules, stem changes, and ir-
regular forms. Some unsupervised work has specifi-
cally targeted these sorts of phenomena by, for ex-
ample, learning spelling rules for mildly noncon-
catenative cases (Dasgupta and Ng, 2007; Narad-
owsky and Goldwater, 2009) or mining lemma-base
form pairs from a corpus (Schone and Jurafsky,
2001), but it is extremely difficult to make unsu-
pervised methods perform as well as supervised ap-
proaches like ours.
Past supervised work on nonconcatenative inflec-
tional morphology has typically targeted individual
pairs of base forms and inflected forms for the pur-
poses of inflection (Clark, 2001) or lemmatization
(Yarowsky and Wicentowski, 2000; Wicentowski,
2004; Linde?n, 2008; Toutanova and Cherry, 2009).
Some of these methods may use analysis (Linde?n,
2008) or decoding (Toutanova and Cherry, 2009)
steps similar to those of our model, but none attempt
to jointly predict a complete inflection table based
on automatically extracted rules.
Some previous work has addressed the joint anal-
ysis (Zajac, 2001; Monson, 2008) or prediction
(Linde?n and Tuovila, 2009; Dinu et al, 2012) of
whole inflection tables, as we do, but rarely are
both aspects addressed simultaneously and most ap-
proaches are tuned to one particular language or
use language-specific, curated resources. In over-
all setup, our work most closely resembles that of
Dreyer and Eisner (2011), but they focus on incor-
porating large amounts of raw text data rather than
using large training sets effectively.
Broadly similar techniques are also employed in
systems to filter candidate rules and aid in human an-
notation of paradigms (Zajac, 2001; Forsberg et al,
2006; De?trez and Ranta, 2012) for resources such as
Grammatical Framework (Ranta, 2011).
8 Conclusion
In this work, we presented a method for inflecting
base forms in morphologically rich languages: we
first extract orthographic transformation rules from
observed inflection tables, then learn to apply these
rules to new base forms based on orthographic fea-
tures. Training examples for our supervised method
can be collected from Wiktionary for a large number
of languages and parts of speech. The changes we
extract are interpretable and can be associated with
particular classes of words. Moreover, our model
can successfully apply these changes to unseen base
forms with high accuracy, allowing us to rapidly
generate lexicons for new languages of interest.
Our Wiktionary datasets and an open-
source version of our code are available at
http://eecs.berkeley.edu/~gdurrett
Acknowledgments
We are grateful to Klaus Macherey and David Talbot
for assistance with the examples and helpful discus-
sions throughout the course of this work. We would
also like to thank the three anonymous reviewers for
their useful comments.
1193
References
R. H. Baayen, R. Piepenbrock, and L. Gulikers. 1995.
The CELEX Lexical Database (Release 2). Linguistic
Data Consortium, University of Pennsylvania.
Alexander Clark. 2001. Partially Supervised Learning
of Morphology with Stochastic Transducers. In Pro-
ceedings of Natural Language Processing Pacific Rim
Symposium, pages 341?348, Tokyo, Japan.
Mathias Creutz and Krista Lagus. 2007. Unsupervised
Models for Morpheme Segmentation and Morphology
Learning. ACM Transactions on Speech and Lan-
guage Processing, 4(1):3:1?3:34, Feb.
Sajib Dasgupta and Vincent Ng. 2007. High Per-
formance, Language-Independent Morphological Seg-
mentation. In Proceedings of the North American
Chapter of the Association for Computational Linguis-
tics.
Gre?goire De?trez and Aarne Ranta. 2012. Smart
Paradigms and the Predictability and Complexity of
Inflectional Morphology. In Proceedings of the Eu-
ropean Chapter of the Association for Computational
Linguistics.
Liviu P. Dinu, Vlad Niculae, and Octavia-Maria S?ulea.
2012. Learning How to Conjugate the Romanian
Verb: Rules for Regular and Partially Irregular Verbs.
In Proceedings of the European Chapter of the Asso-
ciation for Computational Linguistics.
Markus Dreyer and Jason Eisner. 2011. Discovering
Morphological Paradigms from Plain Text Using a
Dirichlet Process Mixture Model. In Proceedings of
Empirical Methods in Natural Language Processing,
pages 616?627, Edinburgh, Scotland, UK.
Jason Eisner. 2002. Parameter Estimation for Probabilis-
tic Finite-State Transducers. In Proceedings of the As-
sociation for Computational Linguistics.
Markus Forsberg, Harald Hammarstro?m, and Aarne
Ranta. 2006. Morphological Lexicon Extraction from
Raw Text Data. In Proceedings of Advances in Natu-
ral Language Processing.
John Goldsmith. 2001. Unsupervised Learning of the
Morphology of a Natural Language. Computational
Linguistics, 27(2):153?198, June.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian Framework for Word Segmen-
tation: Exploring the Effects of Context. Cognition,
112(1):21?54.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2008. Joint Processing and Discrimina-
tive Training for Letter-to-Phoneme Conversion. In
Proceedings of the Association for Computational Lin-
guistics.
Teuvo Kohonen. 1986. Dynamically Expanding Con-
text, With Application to the Correction of Symbol
Strings in the Recognition of Continuous Speech. In
Proceedings of the International Conference on Pat-
tern Recognition.
Shen Li, Joa?o V. Grac?a, and Ben Taskar. 2012. Wiki-ly
Supervised Part-of-speech Tagging. In Proceedings of
Empirical Methods in Natural Language Processing.
Krister Linde?n and Jussi Tuovila. 2009. Corpus-based
Paradigm Selection for Morphological Entries. In
Proceedings of the Nordic Conference of Computa-
tional Linguistics.
Krister Linde?n. 2008. A Probabilistic Model for Guess-
ing Base Forms of New Words by Analogy. In Pro-
ceedings of Computational Linguistics and Intelligent
Text Processing.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45(3):503?528, Decem-
ber.
Christian Monson. 2008. ParaMor: From Paradigm
Structure to Natural Language Morphology Induction.
Ph.D. thesis, Carnegie Mellon University.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving Morphology Induction by Learning Spelling
Rules. In Proceedings of the International Joint Con-
ferences on Artificial Intelligence.
Jose Oncina and Marc Sebban. 2006. Learning Stochas-
tic Edit Distance: Application in Handwritten Char-
acter Recognition. Pattern Recognition, 39(9):1575?
1587, September.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised Morphological Segmentation
with Log-Linear Models. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Aarne Ranta. 2011. Grammatical Framework: Pro-
gramming with Multilingual Grammars. CSLI Pub-
lications, Stanford.
Sunita Sarawagi and William W. Cohen. 2004. Semi-
Markov Conditional Random Fields for Information
Extraction. In Advances in Neural Information Pro-
cessing Systems 17.
Patrick Schone and Daniel Jurafsky. 2001. Knowledge-
Free Induction of Inflectional Morphologies. In Pro-
ceedings of the North American Chapter of the Asso-
ciation for Computational Linguistics.
Ksenia Shalonova and Bruno Gole?nia. 2010. Weakly Su-
pervised Morphology Learning for Agglutinating Lan-
guages Using Small Training Sets. In Proceedings of
the Conference on Computational Linguistics.
Benjamin Snyder and Regina Barzilay. 2008. Unsuper-
vised Multilingual Learning for Morphological Seg-
mentation. In Proceedings of the Association for Com-
putational Linguistics.
1194
Kari Torkkola. 1993. An Efficient Way to Learn English
Grapheme-to-Phoneme Rules Automatically. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing: Speech Processing -
Volume II.
Kristina Toutanova and Colin Cherry. 2009. A Global
Model for Joint Lemmatization and Part-of-Speech
Prediction. In Proceedings of the Association for
Computational Linguistics.
Richard Wicentowski. 2004. Multilingual Noise-Robust
Supervised Morphological Analysis Using the Word-
Frame Model. In Proceedings of the ACL Special In-
terest Group in Computational Phonology.
David Yarowsky and Richard Wicentowski. 2000. Min-
imally Supervised Morphological Analysis by Multi-
modal Alignment. In Proceedings of the Association
for Computational Linguistics.
Re?mi Zajac. 2001. Morpholog: Constrained and Super-
vised Learning of Morphology. In Proceedings of the
Conference on Natural Language Learning.
Torsten Zesch, Christof Mu?ller, and Iryna Gurevych.
2008. Extracting Lexical Semantic Knowledge from
Wikipedia and Wiktionary. In Proceedings of Lan-
guage Resources and Evaluation.
1195
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1453?1463,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Discriminative Modeling of Extraction Sets for Machine Translation
John DeNero and Dan Klein
Computer Science Division
University of California, Berkeley
{denero,klein}@cs.berkeley.edu
Abstract
We present a discriminative model that di-
rectly predicts which set of phrasal transla-
tion rules should be extracted from a sen-
tence pair. Our model scores extraction
sets: nested collections of all the overlap-
ping phrase pairs consistent with an under-
lying word alignment. Extraction set mod-
els provide two principle advantages over
word-factored alignment models. First,
we can incorporate features on phrase
pairs, in addition to word links. Second,
we can optimize for an extraction-based
loss function that relates directly to the
end task of generating translations. Our
model gives improvements in alignment
quality relative to state-of-the-art unsuper-
vised and supervised baselines, as well
as providing up to a 1.4 improvement in
BLEU score in Chinese-to-English trans-
lation experiments.
1 Introduction
In the last decade, the field of statistical machine
translation has shifted from generating sentences
word by word to systems that recycle whole frag-
ments of training examples, expressed as transla-
tion rules. This general paradigm was first pur-
sued using contiguous phrases (Och et al, 1999;
Koehn et al, 2003), and has since been general-
ized to a wide variety of hierarchical and syntactic
formalisms. The training stage of statistical sys-
tems focuses primarily on discovering translation
rules in parallel corpora.
Most systems discover translation rules via a
two-stage pipeline: a parallel corpus is aligned at
the word level, and then a second procedure ex-
tracts fragment-level rules from word-aligned sen-
tence pairs. This paper offers a model-based alter-
native to phrasal rule extraction, which merges this
two-stage pipeline into a single step. We present a
discriminative model that directly predicts which
set of phrasal translation rules should be extracted
from a sentence pair. Our model predicts extrac-
tion sets: combinatorial objects that include the
set of all overlapping phrasal translation rules con-
sistent with an underlying word-level alignment.
This approach provides additional discriminative
power relative to word aligners because extraction
sets are scored based on the phrasal rules they con-
tain in addition to word-to-word alignment links.
Moreover, the structure of our model directly re-
flects the purpose of alignment models in general,
which is to discover translation rules.
We address several challenges to training and
applying an extraction set model. First, we would
like to leverage existing word-level alignment re-
sources. To do so, we define a deterministic map-
ping from word alignments to extraction sets, in-
spired by existing extraction procedures. In our
mapping, possible alignment links have a precise
interpretation that dictates what phrasal translation
rules can be extracted from a sentence pair. This
mapping allows us to train with existing annotated
data sets and use the predictions from word-level
aligners as features in our extraction set model.
Second, our model solves a structured predic-
tion problem, and the choice of loss function dur-
ing training affects model performance. We opti-
mize for a phrase-level F-measure in order to fo-
cus learning on the task of predicting phrasal rules
rather than word alignment links.
Third, our discriminative approach requires that
we perform inference in the space of extraction
sets. Our model does not factor over disjoint word-
to-word links or minimal phrase pairs, and so ex-
isting inference procedures do not directly apply.
However, we show that the dynamic program for
a block ITG aligner can be augmented to score ex-
traction sets that are indexed by underlying ITG
word alignments (Wu, 1997). We also describe a
1453
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
(a)
(b)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent word pairs 
that are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
(a)
(b)
Figure 1: A word alignment A (shaded grid cells)
defines projections ?(ei) and ?(fj), shown as dot-
ted lines for each word in each sentence. The ex-
traction set R3(A) includes all bispans licensed by
these projections, shown as rounded rectangles.
coarse-to-fine inference approach that allows us to
scale our method to long sentences.
Our extraction set model outperforms both un-
supervised and supervised word aligners at pre-
dicting word alignments and extraction sets. We
also demonstrate that extraction sets are useful for
end-to-end machine translation. Our model im-
proves translation quality relative to state-of-the-
art Chinese-to-English baselines across two pub-
licly available systems, providing total BLEU im-
provements of 1.2 in Moses, a phrase-based sys-
tem, and 1.4 in a Joshua, a hierarchical system
(Koehn et al, 2007; Li et al, 2009)
2 Extraction Set Models
The input to our model is an unaligned sentence
pair, and the output is an extraction set of phrasal
translation rules. Word-level alignments are gen-
erated as a byproduct of inference. We first spec-
ify the relationship between word alignments and
extraction sets, then define our model.
2.1 Extraction Sets from Word Alignments
Rule extraction is a standard concept in machine
translation: word alignment constellations license
particular sets of overlapping rules, from which
subsets are selected according to limits on phrase
length (Koehn et al, 2003), number of gaps (Chi-
ang, 2007), count of internal tree nodes (Galley et
al., 2006), etc. In this paper, we focus on phrasal
rule extraction (i.e., phrase pair extraction), upon
which most other extraction procedures are based.
Given a sentence pair (e, f), phrasal rule extrac-
tion defines a mapping from a set of word-to-word
k
l
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =2
l =4
g =1
h =3
Figure 2: Examples of two types of possible align-
ment links (striped). These types account for 96%
of the possible alignment links in our data set.
alignment links A = {(i, j)} to an extraction set
of bispans Rn(A) = {[g, h) ? [k, `)}, where
each bispan links target span [g, h) to source span
[k, `).1 The maximum phrase length n ensures that
max(h? g, `? k) ? n.
We can describe this mapping via word-to-
phrase projections, as illustrated in Figure 1. Let
word ei project to the phrasal span ?(ei), where
?(ei) =
[
min
j?Ji
j , max
j?Ji
j + 1
)
(1)
Ji = {j : (i, j) ? A}
and likewise each word fj projects to a span of e.
Then, Rn(A) includes a bispan [g, h)? [k, `) iff
?(ei) ? [k, `) ?i ? [g, h)
?(fj) ? [g, h) ?j ? [k, `)
That is, every word in one of the phrasal spans
must project within the other. This mapping is de-
terministic, and so we can interpret a word-level
alignment A as also specifying the phrasal rules
that should be extracted from a sentence pair.
2.2 Possible and Null Alignment Links
We have not yet accounted for two special cases
in annotated corpora: possible alignments and null
alignments. To analyze these annotations, we con-
sider a particular data set: a hand-aligned portion
1We use the fencepost indexing scheme used commonly
for parsing. Words are 0-indexed. Spans are inclusive on the
lower bound and exclusive on the upper bound. For example,
the span [0, 2) includes the first two words of a sentence.
1454
of the NIST MT02 Chinese-to-English test set,
which has been used in previous alignment experi-
ments (Ayan et al, 2005; DeNero and Klein, 2007;
Haghighi et al, 2009).
Possible links account for 22% of all alignment
links in these data, and we found that most of
these links fall into two categories. First, possible
links are used to align function words that have no
equivalent in the other language, but colocate with
aligned content words, such as English determin-
ers. Second, they are used to mark pairs of words
or short phrases that are not lexical equivalents,
but which play equivalent roles in each sentence.
Figure 2 shows examples of these two use cases,
along with their corpus frequencies.2
On the other hand, null alignments are used
sparingly in our annotated data. More than 90%
of words participate in some alignment link. The
unaligned words typically express content in one
sentence that is absent in its translation.
Figure 3 illustrates how we interpret possible
and null links in our projection. Possible links are
typically not included in extraction procedures be-
cause most aligners predict only sure links. How-
ever, we see a natural interpretation for possible
links in rule extraction: they license phrasal rules
that both include and exclude them. We exclude
null alignments from extracted phrases because
they often indicate a mismatch in content.
We achieve these effects by redefining the pro-
jection operator ?. Let A(s) be the subset of A
that are sure links, then let the index set Ji used
for projection ? in Equation 1 be
Ji =
?
??
??
{
j : (i, j) ? A(s)
}
if ?j : (i, j) ? A(s)
{?1, |f|} if @j : (i, j) ? A
{j : (i, j) ? A} otherwise
Here, Ji is a set of integers, and ?(ei) for null
aligned ei will be [?1, |f|+ 1) by Equation 1.
Of course, the characteristics of our aligned cor-
pus may not hold for other annotated corpora or
other language pairs. However, we hope that the
overall effectiveness of our modeling approach
will influence future annotation efforts to build
corpora that are consistent with this interpretation.
2.3 A Linear Model of Extraction Sets
We now define a linear model that scores extrac-
tion sets. We restrict our model to score only co-
2We collected corpus frequencies of possible alignment
link types ourselves on a sample of the hand-aligned data set.
k
l
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent word pairs 
that are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
(past)
k =2
l =4
g =1
h =3
Figure 3: Possible links constrain the word-to-
phrase projection of otherwise unaligned words,
which in turn license overlapping phrases. In this
example, ?(f2) = [1, 2) does not include the
possible link at (1, 0) because of the sure link at
(1, 1), but ?(e1) = [1, 2) does use the possible
link because it would otherwise be unaligned. The
word ?PDT? is null aligned, and so its projection
?(e4) = [?1, 4) extends beyond the bounds of the
sentence, excluding ?PDT? from all phrase pairs.
herent extraction sets Rn(A), those that are li-
censed by an underlying word alignment A with
sure alignments A(s) ? A. Conditioned on a
sentence pair (e, f) and maximum phrase length
n, we score extraction sets via a feature vec-
tor ?(A(s), Rn(A)) that includes features on sure
links (i, j) ? A(s) and features on the bispans in
Rn(A) that link [g, h) in e to [k, `) in f :
?(A(s), Rn(A)) =
?
(i,j)?A(s)
?a(i, j) +
?
[g,h)?[k,`)?Rn(A)
?b(g, h, k, `)
Because the projection operator Rn(?) is a
deterministic function, we can abbreviate
?(A(s), Rn(A)) as ?(A) without loss of infor-
mation, although we emphasize that A is a set
of sure and possible alignments, and ?(A) does
not decompose as a sum of vectors on individual
word-level alignment links. Our model is param-
eterized by a weight vector ?, which scores an
extraction set Rn(A) as ? ? ?(A).
To further limit the space of extraction sets we
are willing to consider, we restrict A to block
inverse transduction grammar (ITG) alignments,
a space that allows many-to-many alignments
through phrasal terminal productions, but other-
wise enforces at-most-one-to-one phrase match-
ings with ITG reordering patterns (Cherry and Lin,
2007; Zhang et al, 2008). The ITG constraint
1455
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =2
l =4
g =1
h =3
Figure 4: Above, we show a representative sub-
set of the block alignment patterns that serve as
terminal productions of the ITG that restricts the
output space of our model. These terminal pro-
ductions cover up to n = 3 words in each sentence
and include a mixture of sure (filled) and possible
(striped) word-level alignment links.
is more computationally convenient than arbitrar-
ily ordered phrase matchings (Wu, 1997; DeNero
and Klein, 2008). However, the space of block
ITG alignments is expressive enough to include
the vast majority of patterns observed in hand-
annotated parallel corpora (Haghighi et al, 2009).
In summary, our model scores all Rn(A) for
A ? ITG(e, f) where A can include block termi-
nals of size up to n. In our experiments, n = 3.
Unlike previous work, we allow possible align-
ment links to appear in the block terminals, as de-
picted in Figure 4.
3 Model Estimation
We estimate the weights ? of our extraction set
model discriminatively using the margin-infused
relaxed algorithm (MIRA) of Crammer and Singer
(2003)?a large-margin, perceptron-style, online
learning algorithm. MIRA has been used suc-
cessfully in MT to estimate both alignment mod-
els (Haghighi et al, 2009) and translation models
(Chiang et al, 2008).
For each training example, MIRA requires that
we find the alignment Am corresponding to the
highest scoring extraction set Rn(Am) under the
current model,
Am = arg maxA?ITG(e,f)? ? ?(A) (2)
Section 4 describes our approach to solving this
search problem for model inference.
MIRA updates away from Rn(Am) and to-
ward a gold extraction set Rn(Ag). Some hand-
annotated alignments are outside of the block ITG
model class. Hence, we update toward the ex-
traction set for a pseudo-gold alignment Ag ?
ITG(e, f) with minimal distance from the true ref-
erence alignment At.
Ag = arg minA?ITG(e,f)|A ? At ?A ?At| (3)
Inference details appear in Section 4.3.
GivenAg andAm, we update the model param-
eters away from Am and toward Ag.
? ? ? + ? ? (?(Ag)? ?(Am))
where ? is the minimal step size that will ensure
we prefer Ag to Am by a margin greater than
the loss L(Am;Ag), capped at some maximum
update size C to provide regularization. We use
C = 0.01 in experiments. The step size is a closed
form function of the loss and feature vectors: ? =
min
(
C,
L(Am;Ag)? ? ? (?(Ag)? ?(Am))
||?(Ag)? ?(Am)||22
)
We train the model for 30 iterations over the
training set, shuffling the order each time, and we
average the weight vectors observed after each it-
eration to estimate our final model.
3.1 Extraction Set Loss Function
In order to focus learning on predicting the
right bispans, we use an extraction-level loss
L(Am;Ag): an F-measure of the overlap between
bispans in Rn(Am) and Rn(Ag). This measure
has been proposed previously to evaluate align-
ment systems (Ayan and Dorr, 2006). Based
on preliminary translation results during develop-
ment, we chose bispan F5 as our loss:
Pr(Am) = |Rn(Am) ?Rn(Ag)|/|Rn(Am)|
Rc(Am) = |Rn(Am) ?Rn(Ag)|/|Rn(Ag)|
F5(Am;Ag) =
(1 + 52) ? Pr(Am) ? Rc(Am)
52 ? Pr(Am) + Rc(Am)
L(Am;Ag) = 1? F5(Am;Ag)
F5 favors recall over precision. Previous align-
ment work has shown improvements from adjust-
ing the F-measure parameter (Fraser and Marcu,
2006). In particular, Lacoste-Julien et al (2006)
also chose a recall-biased objective.
Optimizing for a bispan F-measure penalizes
alignment mistakes in proportion to their rule ex-
traction consequences. That is, adding a word
link that prevents the extraction of many correct
phrasal rules, or which licenses many incorrect
rules, is strongly discouraged by this loss.
1456
3.2 Features on Extraction Sets
The discriminative power of our model is driven
by the features on sure word alignment links
?a(i, j) and bispans ?b(g, h, k, `). In both cases,
the most important features come from the pre-
dictions of unsupervised models trained on large
parallel corpora, which provide frequency and co-
occurrence information.
To score word-to-word links, we use the poste-
rior predictions of a jointly trained HMM align-
ment model (Liang et al, 2006). The remaining
features include a dictionary feature, an identical
word feature, an absolute position distortion fea-
ture, and features for numbers and punctuation.
To score phrasal translation rules in an extrac-
tion set, we use a mixture of feature types. Ex-
traction set models allow us to incorporate the
same phrasal relative frequency statistics that drive
phrase-based translation performance (Koehn et
al., 2003). To implement these frequency features,
we extract a phrase table from the alignment pre-
dictions of a jointly trained unsupervised HMM
model using Moses (Koehn et al, 2007), and score
bispans using the resulting features. We also in-
clude indicator features on lexical templates for
the 50 most common words in each language, as
in Haghighi et al (2009). We include indicators
for the number of words and Chinese characters
in rules. One useful indicator feature exploits the
fact that capitalized terms in English tend to align
to Chinese words with three or more characters.
On 1-by-n or n-by-1 phrasal rules, we include in-
dicator features of fertility for common words.3
We also include monolingual phrase features
that expose useful information to the model. For
instance, English bigrams beginning with ?the?
are often extractable phrases. English trigrams
with a hyphen as the second word are typically ex-
tractable, meaning that the first and third words
align to consecutive Chinese words. When any
conjugation of the word ?to be? is followed by a
verb, indicating passive voice or progressive tense,
the two words tend to align together.
Our feature set alo includes bias features on
phrasal rules and links, which control the num-
ber of null-aligned words and number of rules li-
censed. In total, our final model includes 4,249
individual features, dominated by various instanti-
ations of lexical templates.
3Limiting lexicalized features to common words helps
prevent overfitting.
k
l
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =2
l =4
g =1
h =3
or
Figure 5: Both possible ITG decompositions of
this example alignment will split one of the two
highlighted bispans across constituents.
4 Model Inference
Equation 2 asks for the highest scoring extraction
set under our model, Rn(Am), which we also re-
quire at test time. Although we have restricted
Am ? ITG(e, f), our extraction set model does not
factor over ITG productions, and so the dynamic
program for a vanilla block ITG will not suffice to
find Rn(Am). To see this, consider the extraction
set in Figure 5. An ITG decomposition of the un-
derlying alignment imposes a hierarchical brack-
eting on each sentence, and some bispan in the ex-
traction set for this alignment will cross any such
bracketing. Hence, the score of some licensed bis-
pan will be non-local to the ITG decomposition.
4.1 A Dynamic Program for Extraction Sets
If we treat the maximum phrase length n as a fixed
constant, then we can define a dynamic program to
search the space of extraction sets. An ITG deriva-
tion for some alignment A decomposes into two
sub-derivations forAL andAR.4 The model score
of A, which scores extraction set Rn(A), decom-
poses over AL and AR, along with any phrasal
bispans licensed by adjoining AL and AR.
? ? ?(A) = ? ? ?(AL) + ? ? ?(AR) + I(AL,AR)
where I(AL,AR) is ? ?
?
?(g, h, k, l) summed
over licensed bispans [g, h) ? [k, `) that overlap
the boundary between AL and AR.5
4We abuse notation in conflating an alignment A with its
derivation. All derivations of the same alignment receive the
same score, and we only compute the max, not the sum.
5We focus on the case of adjoining two aligned bispans.
Our algorithm easily extends to include null alignments, but
we focus on the non-null setting for simplicity.
1457
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent word pairs 
that are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
(past)
k =2
l =4
g =1
h =3
Figure 6: Augmenting the ITG grammar states
with the alignment configuration in an n? 1 deep
perimeter of the bispan allows us to score all over-
lapping phrasal rules introduced by adjoining two
bispans. The state must encode whether a sure link
appears in each edge column or row, but the spe-
cific location of edge links is not required.
In order to compute I(AL,AR), we need cer-
tain information about the alignment configura-
tions of AL and AR where they adjoin at a corner.
The state must represent (a) the specific alignment
links in the n ? 1 deep corner of each A, and (b)
whether any sure alignments appear in the rows or
columns extending from those corners.6 With this
information, we can infer the bispans licensed by
adjoining AL and AR, as in Figure 6.
Applying our score recurrence yields a
polynomial-time dynamic program. This dynamic
program is an instance of ITG bitext parsing,
where the grammar uses symbols to encode
the alignment contexts described above. This
context-as-symbol augmentation of the grammar
is similar in character to augmenting symbols with
lexical items to score language models during
hierarchical decoding (Chiang, 2007).
4.2 Coarse-to-Fine Inference and Pruning
Exhaustive inference under an ITG requiresO(k6)
time in sentence length k, and is prohibitively slow
when there is no sparsity in the grammar. Main-
taining the context necessary to score non-local
bispans further increases running time. That is,
ITG inference is organized around search states
associated with a grammar symbol and a bispan;
augmenting grammar symbols also augments this
state space.
To parse quickly, we prune away search states
using predictions from the more efficient HMM
6The number of configuration states does not depend on
the size ofA because corners have fixed size, and because the
position of links within rows or columns is not needed.
alignment model (Ney and Vogel, 1996). We dis-
card all states corresponding to bispans that are
incompatible with 3 or more alignment links un-
der an intersected HMM?a proven approach to
pruning the space of ITG alignments (Zhang and
Gildea, 2006; Haghighi et al, 2009). Pruning in
this way reduces the search space dramatically, but
only rarely prohibits correct alignments. The ora-
cle alignment error rate for the block ITG model
class is 1.4%; the oracle alignment error rate for
this pruned subset of ITG is 2.0%.
To take advantage of the sparsity that results
from pruning, we use an agenda-based parser that
orders search states from small to large, where we
define the size of a bispan as the total number of
words contained within it. For each size, we main-
tain a separate agenda. Only when the agenda for
size k is exhausted does the parser proceed to pro-
cess the agenda for size k + 1.
We also employ coarse-to-fine search to speed
up inference (Charniak and Caraballo, 1998). In
the coarse pass, we search over the space of ITG
alignments, but score only features on alignment
links and bispans that are local to terminal blocks.
This simplification eliminates the need to augment
grammar symbols, and so we can exhaustively ex-
plore the (pruned) space. We then compute out-
side scores for bispans under a max-sum semir-
ing (Goodman, 1996). In the fine pass with the
full extraction set model, we impose a maximum
size of 10,000 for each agenda. We order states on
agendas by the sum of their inside score under the
full model and the outside score computed in the
coarse pass, pruning all states not within the fixed
agenda beam size.
Search states that are popped off agendas are
indexed by their corner locations for fast look-
up when constructing new states. For each cor-
ner and size combination, built states are main-
tained in sorted order according to their inside
score. This ordering allows us to stop combin-
ing states early when the results are falling off the
agenda beams. Similar search and beaming strate-
gies appear in many decoders for machine trans-
lation (Huang and Chiang, 2007; Koehn and Had-
dow, 2009; Moore and Quirk, 2007).
4.3 Finding Pseudo-Gold ITG Alignments
Equation 3 asks for the block ITG alignment
Ag that is closest to a reference alignment At,
which may not lie in ITG(e,f). We search for
1458
kl
g
h
2?
15?
2010?
On February 15 2010
2?
15?
2010?
On February 15 2010
?(ei)
?(f2)
?(e1)
Type 1: Language-specific function 
words omitted in the other language 
Type 2: Role-equivalent pairs that 
are not lexical equivalents
?
??
[go over]
[Earth]
over the Earth
65%
31%
?
??
[passive marker]
[discover]
was discovered
Distribution over 
possible link types
?(fj)
?
??
?
?
In the
past two
years
[past]
[two]
[year]
[in]
PDT
After
dinner I slept
?
??
?
?
?
?
[after]
[dinner]
[after]
[I]
[sleep]
[past tense]
k =1
l =4
g =0
h =3
or
Figure 7: A* search for pseudo-gold ITG align-
ments uses an admissible heuristic for bispans that
counts the number of gold links outside of [k, `)
but within [g, h). Above, the heuristic is 1, which
is also the minimal number of alignment errors
that an ITG alignment will incur using this bispan.
Ag using A* bitext parsing (Klein and Manning,
2003). Search states, which correspond to bispans
[g, h)? [k, `), are scored by the number of errors
within the bispan plus the number of (i, j) ? At
such that j ? [k, `) but i /? [g, h) (recall errors).
As an admissible heuristic for the future cost of
a bispan [g, h) ? [k, `), we count the number of
(i, j) ? At such that i ? [g, h) but j /? [k, `), as
depicted in Figure 7. These links will become re-
call errors eventually. A* search with this heuristic
makes no errors, and the time required to compute
pseudo-gold alignments is negligible.
5 Relationship to Previous Work
Our model is certainly not the first alignment ap-
proach to include structures larger than words.
Model-based phrase-to-phrase alignment was pro-
posed early in the history of phrase-based trans-
lation as a method for training translation models
(Marcu and Wong, 2002). A variety of unsuper-
vised models refined this initial work with priors
(DeNero et al, 2008; Blunsom et al, 2009) and
inference constraints (DeNero et al, 2006; Birch
et al, 2006; Cherry and Lin, 2007; Zhang et al,
2008). These models fundamentally differ from
ours in that they stipulate a segmentation of the
sentence pair into phrases, and only align the min-
imal phrases in that segmentation. Our model
scores the larger overlapping phrases that result
from composing these minimal phrases.
Discriminative alignment is also a well-
explored area. Most work has focused on pre-
dicting word alignments via partial matching in-
ference algorithms (Melamed, 2000; Taskar et al,
2005; Moore, 2005; Lacoste-Julien et al, 2006).
Work in semi-supervised estimation has also con-
tributed evidence that hand-annotations are useful
for training alignment models (Fraser and Marcu,
2006; Fraser and Marcu, 2007). The ITG gram-
mar formalism, the corresponding word alignment
class, and inference procedures for the class have
also been explored extensively (Wu, 1997; Zhang
and Gildea, 2005; Cherry and Lin, 2007; Zhang
et al, 2008). At the intersection of these lines of
work, discriminative ITG models have also been
proposed, including one-to-one alignment mod-
els (Cherry and Lin, 2006) and block models
(Haghighi et al, 2009). Our model directly ex-
tends this research agenda with first-class possi-
ble links, overlapping phrasal rule features, and an
extraction-level loss function.
Ka?a?ria?inen (2009) trains a translation model
discriminatively using features on overlapping
phrase pairs. That work differs from ours in
that it uses fixed word alignments and focuses on
translation model estimation, while we focus on
alignment and translate using standard relative fre-
quency estimators.
Deng and Zhou (2009) present an alignment
combination technique that uses phrasal features.
Our approach differs in two ways. First, their ap-
proach is tightly coupled to the input alignments,
while we perform a full search over the space of
ITG alignments. Also, their approach uses greedy
search, while our search is optimal aside from
pruning and beaming. Despite these differences,
their strong results reinforce our claim that phrase-
level information is useful for alignment.
6 Experiments
We evaluate our extraction set model by the bis-
pans it predicts, the word alignments it generates,
and the translations generated by two end-to-end
systems. Table 1 compares the five systems de-
scribed below, including three baselines. All su-
pervised aligners were optimized for bispan F5.
Unsupervised Baseline: GIZA++. We trained
GIZA++ (Och and Ney, 2003) using the default
parameters included with the Moses training script
(Koehn et al, 2007). The designated regimen con-
cludes by Viterbi aligning under Model 4 in both
directions. We combined these alignments with
1459
the grow-diag heuristic (Koehn et al, 2003).
Unsupervised Baseline: Joint HMM. We
trained and combined two HMM alignment mod-
els (Ney and Vogel, 1996) using the Berkeley
Aligner.7 We initialized the HMM model pa-
rameters with jointly trained Model 1 param-
eters (Liang et al, 2006), combined word-to-
word posteriors by averaging (soft union), and de-
coded with the competitive thresholding heuristic
of DeNero and Klein (2007), yielding a state-of-
the-art unsupervised baseline.
Supervised Baseline: Block ITG. We discrimi-
natively trained a block ITG aligner with only sure
links, using block terminal productions up to 3
words by 3 words in size. This supervised base-
line is a reimplementation of the MIRA-trained
model of Haghighi et al (2009). We use the same
features and parser implementation for this model
as we do for our extraction set model to ensure a
clean comparison. To remain within the alignment
class, MIRA updates this model toward a pseudo-
gold alignment with only sure links. This model
does not score any overlapping bispans.
Extraction Set Coarse Pass. We add possible
links to the output of the block ITG model by
adding the mixed terminal block productions de-
scribed in Section 2.3. This model scores over-
lapping phrasal rules contained within terminal
blocks that result from including or excluding pos-
sible links. However, this model does not score
bispans that cross bracketing of ITG derivations.
Full Extraction Set Model. Our full model in-
cludes possible links and features on extraction
sets for phrasal bispans with a maximum size of
3. Model inference is performed using the coarse-
to-fine scheme described in Section 4.2.
6.1 Data
In this paper, we focus exclusively on Chinese-to-
English translation. We performed our discrimi-
native training and alignment evaluations using a
hand-aligned portion of the NIST MT02 test set,
which consists of 150 training and 191 test sen-
tences (Ayan and Dorr, 2006). We trained the
baseline HMM on 11.3 million words of FBIS
newswire data, a comparable dataset to those used
in previous alignment evaluations on our test set
(DeNero and Klein, 2007; Haghighi et al, 2009).
7http://code.google.com/p/berkeleyaligner
Our end-to-end translation experiments were
tuned and evaluated on sentences up to length 40
from the NIST MT04 and MT05 test sets. For
these experiments, we trained on a 22.1 million
word parallel corpus consisting of sentences up to
length 40 of newswire data from the GALE pro-
gram, subsampled from a larger data set to pro-
mote overlap with the tune and test sets. This cor-
pus also includes a bilingual dictionary. To im-
prove performance, we retrained our aligner on a
retokenized version of the hand-annotated data to
match the tokenization of our corpus.8 We trained
a language model with Kneser-Ney smoothing
on 262 million words of newswire using SRILM
(Stolcke, 2002).
6.2 Word and Phrase Alignment
The first panel of Table 1 gives a word-level eval-
uation of all five aligners. We use the alignment
error rate (AER) measure: precision is the frac-
tion of sure links in the system output that are sure
or possible in the reference, and recall is the frac-
tion of sure links in the reference that the system
outputs as sure. For this evaluation, possible links
produced by our extraction set models are ignored.
The full extraction set model performs the best by
a small margin, although it was not tuned for word
alignment.
The second panel gives a phrasal rule-level
evaluation, which measures the degree to which
these aligners matched the extraction sets of hand-
annotated alignments, R3(At).9 To compete
fairly, all models were evaluated on the full ex-
traction sets induced by the word alignments they
predicted. Again, the extraction set model outper-
formed the baselines, particularly on the F5 mea-
sure for which these systems were trained.
Our coarse pass extraction set model performed
nearly as well as the full model. We believe
these models perform similarly for two reasons.
First, most of the information needed to predict
an extraction set can be inferred from word links
and phrasal rules contained within ITG terminal
productions. Second, the coarse-to-fine inference
may be constraining the full phrasal model to pre-
dict similar output to the coarse model. This simi-
larity persists in translation experiments.
8All alignment results are reported under the annotated
data set?s original tokenization.
9While pseudo-gold approximations to the annotation
were used for training, the evaluation is always performed
relative to the original human annotation.
1460
Word Bispan BLEU
Pr Rc AER Pr Rc F1 F5 Joshua Moses
Baseline GIZA++ 72.5 71.8 27.8 69.4 45.4 54.9 46.0 33.8 32.6
models Joint HMM 84.0 76.9 19.6 69.5 59.5 64.1 59.9 34.5 33.2
Block ITG 83.4 83.8 16.4 75.8 62.3 68.4 62.8 34.7 33.6
Extraction Coarse Pass 82.2 84.2 16.9 70.0 72.9 71.4 72.8 35.7 34.2
set models Full Model 84.7 84.0 15.6 69.0 74.2 71.6 74.0 35.9 34.4
Table 1: Experimental results demonstrate that the full extraction set model outperforms supervised and
unsupervised baselines in evaluations of word alignment quality, extraction set quality, and translation.
In word and bispan evaluations, GIZA++ did not have access to a dictionary while all other methods
did. In the BLEU evaluation, all systems used a bilingual dictionary included in the training corpus. The
BLEU evaluation of supervised systems also included rule counts from the Joint HMM to compensate
for parse failures.
6.3 Translation Experiments
We evaluate the alignments predicted by our
model using two publicly available, open-source,
state-of-the-art translation systems. Moses is a
phrase-based system with lexicalized reordering
(Koehn et al, 2007). Joshua (Li et al, 2009) is
an implementation of Hiero (Chiang, 2007) using
a suffix-array-based grammar extraction approach
(Lopez, 2007).
Both of these systems take word alignments as
input, and neither of these systems accepts possi-
ble links in the alignments they consume. To inter-
face with our extraction set models, we produced
three sets of sure-only alignments from our model
predictions: one that omitted possible links, one
that converted all possible links to sure links, and
one that includes each possible link with 0.5 prob-
ability. These three sets were aggregated and rules
were extracted from all three.
The training set we used for MT experiments
is quite heterogenous and noisy compared to our
alignment test sets, and the supervised aligners
did not handle certain sentence pairs in our par-
allel corpus well. In some cases, pruning based
on consistency with the HMM caused parse fail-
ures, which in turn caused training sentences to be
skipped. To account for these issues, we added
counts of phrasal rules extracted from the baseline
HMM to the counts produced by supervised align-
ers.
In Moses, our extraction set model predicts the
set of phrases extracted by the system, and so the
estimation techniques for the alignment model and
translation model both share a common underly-
ing representation: extraction sets. Empirically,
we observe a BLEU score improvement of 1.2
over the best unsupervised baseline and 0.8 over
the block ITG supervised baseline (Papineni et al,
2002).
In Joshua, hierarchical rule extraction is based
upon phrasal rule extraction, but abstracts away
sub-phrases to create a grammar. Hence, the ex-
traction sets we predict are closely linked to the
representation that this system uses to translate.
The extraction model again outperformed both un-
supervised and supervised baselines, by 1.4 BLEU
and 1.2 BLEU respectively.
7 Conclusion
Our extraction set model serves to coordinate the
alignment and translation model components of a
statistical translation system by unifying their rep-
resentations. Moreover, our model provides an ef-
fective alternative to phrase alignment models that
choose a particular phrase segmentation; instead,
we predict many overlapping phrases, both large
and small, that are mutually consistent. In future
work, we look forward to developing extraction
set models for richer formalisms, including hier-
archical grammars.
Acknowledgments
This project is funded in part by BBN under
DARPA contract HR0011-06-C-0022 and by the
NSF under grant 0643742. We thank the anony-
mous reviewers for their helpful comments.
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going
beyond AER: An extensive analysis of word align-
ments and their impact on MT. In Proceedings of
1461
the Annual Conference of the Association for Com-
putational Linguistics.
Necip Fazil Ayan, Bonnie J. Dorr, and Christof Monz.
2005. Neuralign: combining word alignments us-
ing neural networks. In Proceedings of the Confer-
ence on Human Language Technology and Empiri-
cal Methods in Natural Language Processing.
Alexandra Birch, Chris Callison-Burch, and Miles Os-
borne. 2006. Constraining the phrase-based, joint
probability statistical translation model. In Proceed-
ings of the Conference for the Association for Ma-
chine Translation in the Americas.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics.
Eugene Charniak and Sharon Caraballo. 1998. New
figures of merit for best-first probabilistic chart pars-
ing. In Computational Linguistics.
Colin Cherry and Dekang Lin. 2006. Soft syntactic
constraints for word alignment through discrimina-
tive training. In Proceedings of the Annual Confer-
ence of the Association for Computational Linguis-
tics.
Colin Cherry and Dekang Lin. 2007. Inversion trans-
duction grammar for joint phrasal translation mod-
eling. In Proceedings of the Annual Conference of
the North American Chapter of the Association for
Computational Linguistics Workshop on Syntax and
Structure in Statistical Translation.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultracon-
servative online algorithms for multiclass problems.
Journal of Machine Learning Research, 3:951?991.
John DeNero and Dan Klein. 2007. Tailoring word
alignments to syntactic machine translation. In Pro-
ceedings of the Annual Conference of the Associa-
tion for Computational Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the
Annual Conference of the Association for Computa-
tional Linguistics: Short Paper Track.
John DeNero, Dan Gillick, James Zhang, and Dan
Klein. 2006. Why generative phrase models un-
derperform surface heuristics. In Proceedings of the
NAACL Workshop on Statistical Machine Transla-
tion.
John DeNero, Alexandre Bouchard-Cote, and Dan
Klein. 2008. Sampling alignment structure under
a bayesian translation model. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table train-
ing. In Proceedings of the Annual Conference of the
Association for Computational Linguistics: Short
Paper Track.
Alexander Fraser and Daniel Marcu. 2006. Semi-
supervised training for statistical word alignment. In
Proceedings of the Annual Conference of the Asso-
ciation for Computational Linguistics.
Alexander Fraser and Daniel Marcu. 2007. Getting
the structure right for word alignment: Leaf. In Pro-
ceedings of the Joint Conference on Empirical Meth-
ods in Natural Language Processing and Computa-
tional Natural Language Learning.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of the Annual Conference of the Associa-
tion for Computational Linguistics.
Joshua Goodman. 1996. Parsing algorithms and met-
rics. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with super-
vised ITG models. In Proceedings of the Annual
Conference of the Association for Computational
Linguistics.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language mod-
els. In Proceedings of the Annual Conference of the
Association for Computational Linguistics.
Matti Ka?a?ria?inen. 2009. Sinuhe?statistical machine
translation using a globally trained conditional ex-
ponential family translation model. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Dan Klein and Chris Manning. 2003. A* parsing: Fast
exact Viterbi parse selection. In Proceedings of the
Conference of the North American Chapter of the
Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2009. Edinburghs
submission to all tracks of the WMT2009 shared
task with reordering and speed improvements to
Moses. In Proceedings of the Workshop on Statis-
tical Machine Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics.
1462
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the Annual Conference of the Associ-
ation for Computational Linguistics: Demonstration
track.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I. Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the Annual
Conference of the North American Chapter of the
Association for Computational Linguistics.
Adam Lopez. 2007. Hierarchical phrase-based trans-
lation with suffix arrays. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Daniel Marcu and Daniel Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
I. Dan Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics.
Robert Moore and Chris Quirk. 2007. Faster
beam-search decoding for phrasal statistical ma-
chine translation. In Proceedings of MT Summit XI.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing.
Hermann Ney and Stephan Vogel. 1996. HMM-based
word alignment in statistical translation. In Pro-
ceedings of the Conference on Computational lin-
guistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29:19?51.
Franz Josef Och, Christoph Tillmann, and Hermann
Ney. 1999. Improved alignment models for statisti-
cal machine translation. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic
evaluation of machine translation. In Proceedings of
the Annual Conference of the Association for Com-
putational Linguistics.
Andreas Stolcke. 2002. Srilm an extensible language
modeling toolkit. In Proceedings of the Interna-
tional Conference on Spoken Language Processing.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A discriminative matching approach to word
alignment. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23:377?404.
Hao Zhang and Daniel Gildea. 2005. Stochastic lex-
icalized inversion transduction grammar for align-
ment. In Proceedings of the Annual Conference of
the Association for Computational Linguistics.
Hao Zhang and Daniel Gildea. 2006. Efficient search
for inversion transduction grammar. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Hao Zhang, Chris Quirk, Robert C. Moore, and
Daniel Gildea. 2008. Bayesian learning of non-
compositional phrases with synchronous parsing. In
Proceedings of the Annual Conference of the Asso-
ciation for Computational Linguistics.
1463
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 420?429,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Model-Based Aligner Combination Using Dual Decomposition
John DeNero
Google Research
denero@google.com
Klaus Macherey
Google Research
kmach@google.com
Abstract
Unsupervised word alignment is most often
modeled as a Markov process that generates a
sentence f conditioned on its translation e. A
similar model generating e from f will make
different alignment predictions. Statistical
machine translation systems combine the pre-
dictions of two directional models, typically
using heuristic combination procedures like
grow-diag-final. This paper presents a graph-
ical model that embeds two directional align-
ers into a single model. Inference can be per-
formed via dual decomposition, which reuses
the efficient inference algorithms of the direc-
tional models. Our bidirectional model en-
forces a one-to-one phrase constraint while ac-
counting for the uncertainty in the underlying
directional models. The resulting alignments
improve upon baseline combination heuristics
in word-level and phrase-level evaluations.
1 Introduction
Word alignment is the task of identifying corre-
sponding words in sentence pairs. The standard
approach to word alignment employs directional
Markov models that align the words of a sentence
f to those of its translation e, such as IBM Model 4
(Brown et al, 1993) or the HMM-based alignment
model (Vogel et al, 1996).
Machine translation systems typically combine
the predictions of two directional models, one which
aligns f to e and the other e to f (Och et al,
1999). Combination can reduce errors and relax
the one-to-many structural restriction of directional
models. Common combination methods include the
union or intersection of directional alignments, as
well as heuristic interpolations between the union
and intersection like grow-diag-final (Koehn et al,
2003). This paper presents a model-based alterna-
tive to aligner combination. Inference in a prob-
abilistic model resolves the conflicting predictions
of two directional models, while taking into account
each model?s uncertainty over its output.
This result is achieved by embedding two direc-
tional HMM-based alignment models into a larger
bidirectional graphical model. The full model struc-
ture and potentials allow the two embedded direc-
tional models to disagree to some extent, but reward
agreement. Moreover, the bidirectional model en-
forces a one-to-one phrase alignment structure, sim-
ilar to the output of phrase alignment models (Marcu
and Wong, 2002; DeNero et al, 2008), unsuper-
vised inversion transduction grammar (ITG) models
(Blunsom et al, 2009), and supervised ITG models
(Haghighi et al, 2009; DeNero and Klein, 2010).
Inference in our combined model is not tractable
because of numerous edge cycles in the model
graph. However, we can employ dual decomposi-
tion as an approximate inference technique (Rush et
al., 2010). In this approach, we iteratively apply the
same efficient sequence algorithms for the underly-
ing directional models, and thereby optimize a dual
bound on the model objective. In cases where our
algorithm converges, we have a certificate of opti-
mality under the full model. Early stopping before
convergence still yields useful outputs.
Our model-based approach to aligner combina-
tion yields improvements in alignment quality and
phrase extraction quality in Chinese-English exper-
iments, relative to typical heuristic combinations
methods applied to the predictions of independent
directional models.
420
2 Model Definition
Our bidirectional model G = (V,D) is a globally
normalized, undirected graphical model of the word
alignment for a fixed sentence pair (e,f). Each ver-
tex in the vertex set V corresponds to a model vari-
able Vi, and each undirected edge in the edge set D
corresponds to a pair of variables (Vi, Vj). Each ver-
tex has an associated potential function ?i(vi) that
assigns a real-valued potential to each possible value
vi of Vi.1 Likewise, each edge has an associated po-
tential function ?ij(vi, vj) that scores pairs of val-
ues. The probability under the model of any full as-
signment v to the model variables, indexed by V ,
factors over vertex and edge potentials.
P(v) ?
?
vi?V
?i(vi) ?
?
(vi,vj)?D
?ij(vi, vj)
Our model contains two directional hidden
Markov alignment models, which we review in Sec-
tion 2.1, along with additional structure that that we
introduce in Section 2.2.
2.1 HMM-Based Alignment Model
This section describes the classic hidden Markov
model (HMM) based alignment model (Vogel et al,
1996). The model generates a sequence of words f
conditioned on a word sequence e. We convention-
ally index the words of e by i and f by j. P(f |e)
is defined in terms of a latent alignment vector a,
where aj = i indicates that word position i of e
aligns to word position j of f .
P(f |e) =
?
a
P(f ,a|e)
P(f ,a|e) =
|f |?
j=1
D(aj |aj?1)M(fj |eaj ) . (1)
In Equation 1 above, the emission model M is
a learned multinomial distribution over word types.
The transition model D is a multinomial over tran-
sition distances, which treats null alignments as a
special case.
D(aj = 0|aj?1 = i) = po
D(aj = i
? 6= 0|aj?1 = i) = (1? po) ? c(i
? ? i) ,
1Potentials in an undirected model play the same role as con-
ditional probabilities in a directed model, but do not need to be
locally normalized.
where c(i? ? i) is a learned distribution over signed
distances, normalized over the possible transitions
from i. The parameters of the conditional multino-
mial M and the transition model c can be learned
from a sentence aligned corpus via the expectation
maximization algorithm. The null parameter po is
typically fixed.2
The highest probability word alignment vector
under the model for a given sentence pair (e,f) can
be computed exactly using the standard Viterbi al-
gorithm for HMMs in O(|e|2 ? |f |) time.
An alignment vector a can be converted trivially
into a set of word alignment links A:
Aa = {(i, j) : aj = i, i 6= 0} .
Aa is constrained to be many-to-one from f to e;
many positions j can align to the same i, but each j
appears at most once.
We have defined a directional model that gener-
ates f from e. An identically structured model can
be defined that generates e from f . Let b be a vector
of alignments where bi = j indicates that word po-
sition j of f aligns to word position i of e. Then,
P(e,b|f) is defined similarly to Equation 1, but
with e and f swapped. We can distinguish the tran-
sition and emission distributions of the two models
by subscripting them with their generative direction.
P(e,b|f) =
|e|?
j=1
Df?e(bi|bi?1)Mf?e(ei|fbi) .
The vector b can be interpreted as a set of align-
ment links that is one-to-many: each value i appears
at most once in the set.
Ab = {(i, j) : bi = j, j 6= 0} .
2.2 A Bidirectional Alignment Model
We can combine two HMM-based directional align-
ment models by embedding them in a larger model
2In experiments, we set po = 10
?6. Transitions from a null-
aligned state aj?1 = 0 are also drawn from a fixed distribution,
where D(aj = 0|aj?1 = 0) = 10?4 and for i? ? 1,
D(aj = i
?|aj?1 = 0) ? 0.8
(
?
?
?
?i??
|f|
|e|?j
?
?
?
)
.
With small po, the shape of this distribution has little effect on
the alignment outcome.
421
How are
you
?
?
How are
you
?
?
How are
you
a
1
c
11
b
2
b
2
c
22
(a)
c
22
(b)
a
2
a
3
b
1
c
12
c
13
c
21
c
22
c
23
a
1
a
2
a
3
b
1
c
21
(a)
c
21
(b)
c
23
(a)
c
23
(b)
c
13
(a)
c
13
(b)
c
12
(a)
c
12
(b)
c
11
(a)
c
11
(b)
c
22
(a)
a
1
a
2
a
3
c
21
(a)
c
23
(a)
c
13
(a)
c
12
(a)
c
11
(a)
Figure 1: The structure of our graphical model for a sim-
ple sentence pair. The variables a are blue, b are red, and
c are green.
that includes all of the random variables of two di-
rectional models, along with additional structure that
promotes agreement and resolves discrepancies.
The original directional models include observed
word sequences e and f , along with the two latent
alignment vectors a and b defined in Section 2.1.
Because the word types and lengths of e and f are
always fixed by the observed sentence pair, we can
define our model only over a and b, where the edge
potentials between any aj , fj , and e are compiled
into a vertex potential function ?(a)j on aj , defined
in terms of f and e, and likewise for any bi.
?(a)j (i) = Me?f (fj |ei)
?(b)i (j) = Mf?e(ei|fj)
The edge potentials between a and b encode the
transition model in Equation 1.
?(a)j?1,j(i, i
?) = De?f (aj = i
?|aj?1 = i)
?(b)i?1,i(j, j
?) = Df?e(bi = j
?|bi?1 = j)
In addition, we include in our model a latent
boolean matrix c that encodes the output of the com-
bined aligners:
c ? {0, 1}|e|?|f | .
This matrix encodes the alignment links proposed
by the bidirectional model:
Ac = {(i, j) : cij = 1} .
Each model node for an element cij ? {0, 1} is
connected to aj and bi via coherence edges. These
edges allow the model to ensure that the three sets
of variables, a, b, and c, together encode a coher-
ent alignment analysis of the sentence pair. Figure 1
depicts the graph structure of the model.
2.3 Coherence Potentials
The potentials on coherence edges are not learned
and do not express any patterns in the data. Instead,
they are fixed functions that promote consistency be-
tween the integer-valued directional alignment vec-
tors a and b and the boolean-valued matrix c.
Consider the assignment aj = i, where i = 0
indicates that word fj is null-aligned, and i ? 1 in-
dicates that fj aligns to ei. The coherence potential
ensures the following relationship between the vari-
able assignment aj = i and the variables ci?j , for
any i? ? [1, |e|].
? If i = 0 (null-aligned), then all ci?j = 0.
? If i > 0, then cij = 1.
? ci?j = 1 only if i? ? {i? 1, i, i+ 1}.
? Assigning ci?j = 1 for i? 6= i incurs a cost e??.
Collectively, the list of cases above enforce an intu-
itive correspondence: an alignment aj = i ensures
that cij must be 1, adjacent neighbors may be 1 but
incur a cost, and all other elements are 0.
This pattern of effects can be encoded in a poten-
tial function ?(c) for each coherence edge. These
edge potential functions takes an integer value i for
some variable aj and a binary value k for some ci?j .
?(c)(aj ,ci?j)
(i, k) =
?
????????????
????????????
1 i = 0 ? k = 0
0 i = 0 ? k = 1
1 i = i? ? k = 1
0 i = i? ? k = 0
1 i 6= i? ? k = 0
e?? |i? i?| = 1 ? k = 1
0 |i? i?| > 1 ? k = 1
(2)
Above, potentials of 0 effectively disallow certain
cases because a full assignment to (a,b, c) is scored
by the product of all model potentials. The poten-
tial function ?(c)(bi,cij? )
(j, k) for a coherence edge be-
tween b and c is defined similarly.
422
2.4 Model Properties
We interpret c as the final alignment produced by the
model, ignoring a and b. In this way, we relax the
one-to-many constraints of the directional models.
However, all of the information about how words
align is expressed by the vertex and edge potentials
on a and b. The coherence edges and the link ma-
trix c only serve to resolve conflicts between the di-
rectional models and communicate information be-
tween them.
Because directional alignments are preserved in-
tact as components of our model, extensions or
refinements to the underlying directional Markov
alignment model could be integrated cleanly into
our model as well, including lexicalized transition
models (He, 2007), extended conditioning contexts
(Brunning et al, 2009), and external information
(Shindo et al, 2010).
For any assignment to (a,b, c) with non-zero
probability, c must encode a one-to-one phrase
alignment with a maximum phrase length of 3. That
is, any word in either sentence can align to at most
three words in the opposite sentence, and those
words must be contiguous. This restriction is di-
rectly enforced by the edge potential in Equation 2.
3 Model Inference
In general, graphical models admit efficient, exact
inference algorithms if they do not contain cycles.
Unfortunately, our model contains numerous cycles.
For every pair of indices (i, j) and (i?, j?), the fol-
lowing cycle exists in the graph:
cij ? bi ? cij? ? aj? ?
ci?j? ? bi? ? ci?j ? aj ? cij
Additional cycles also exist in the graph through
the edges between aj?1 and aj and between bi?1
and bi. The general phrase alignment problem under
an arbitrary model is known to be NP-hard (DeNero
and Klein, 2008).
3.1 Dual Decomposition
While the entire graphical model has loops, there are
two overlapping subgraphs that are cycle-free. One
subgraph Ga includes all of the vertices correspond-
ing to variables a and c. The other subgraph Gb in-
cludes vertices for variables b and c. Every edge in
the graph belongs to exactly one of these two sub-
graphs.
The dual decomposition inference approach al-
lows us to exploit this sub-graph structure (Rush et
al., 2010). In particular, we can iteratively apply
exact inference to the subgraph problems, adjusting
their potentials to reflect the constraints of the full
problem. The technique of dual decomposition has
recently been shown to yield state-of-the-art perfor-
mance in dependency parsing (Koo et al, 2010).
3.2 Dual Problem Formulation
To describe a dual decomposition inference proce-
dure for our model, we first restate the inference
problem under our graphical model in terms of the
two overlapping subgraphs that admit tractable in-
ference. Let c(a) be a copy of c associated with Ga,
and c(b) with Gb. Also, let f(a, c(a)) be the un-
normalized log-probability of an assignment to Ga
and g(b, c(b)) be the unnormalized log-probability
of an assignment to Gb. Finally, let I be the index
set of all (i, j) for c. Then, the maximum likelihood
assignment to our original model can be found by
optimizing
max
a,b,c(a),c(b)
f(a, c(a)) + g(b, c(b)) (3)
such that: c(a)ij = c
(b)
ij ? (i, j) ? I .
The Lagrangian relaxation of this optimization
problem is L(a,b, c(a), c(b),u) =
f(a, c(a))+ g(b, c(b))+
?
(i,j)?I
u(i, j)(c(a)i,j ?c
(b)
i,j ) .
Hence, we can rewrite the original problem as
max
a,b,c(a),c(b)
min
u
L(a,b, c(a), c(b),u) .
We can form a dual problem that is an up-
per bound on the original optimization problem by
swapping the order of min and max. In this case,
the dual problem decomposes into two terms that are
each local to an acyclic subgraph.
min
u
?
?max
a,c(a)
?
?f(a, c(a)) +
?
i,j
u(i, j)c(a)ij
?
?
+ max
b,c(b)
?
?g(b, c(b))?
?
i,j
u(i, j)c(b)ij
?
?
?
? (4)
423
How are
you
?
?
How are
you
?
?
How are
you
a
1
c
11
b
2
b
2
c
22
(a)
c
22
(b)
a
2
a
3
b
1
c
12
c
13
c
21
c
22
c
23
a
1
a
2
a
3
b
1
c
21
(a)
c
21
(b)
c
23
(a)
c
23
(b)
c
13
(a)
c
13
(b)
c
12
(a)
c
12
(b)
c
11
(a)
c
11
(b)
c
22
(a)
a
1
a
2
a
3
c
21
(a)
c
23
(a)
c
13
(a)
c
12
(a)
c
11
(a)
Figure 2: Our combined model decomposes into two
acyclic models that each contain a copy of c.
The decomposed model is depicted in Figure 2.
As in previous work, we solve for the dual variable
u by repeatedly performing inference in the two de-
coupled maximization problems.
3.3 Sub-Graph Inference
We now address the problem of evaluating Equa-
tion 4 for fixed u. Consider the first line of Equa-
tion 4, which includes variables a and c(a).
max
a,c(a)
?
?f(a, c(a)) +
?
i,j
u(i, j)c(a)ij
?
? (5)
Because the graph Ga is tree-structured, Equa-
tion 5 can be evaluated in polynomial time. In fact,
we can make a stronger claim: we can reuse the
Viterbi inference algorithm for linear chain graph-
ical models that applies to the embedded directional
HMM models. That is, we can cast the optimization
of Equation 5 as
max
a
?
?
|f |?
j=1
De?f (aj |aj?1) ?M
?
j(aj = i)
?
? .
In the original HMM-based aligner, the vertex po-
tentials correspond to bilexical probabilities. Those
quantities appear in f(a, c(a)), and therefore will be
a part of M?j(?) above. The additional terms of Equa-
tion 5 can also be factored into the vertex poten-
tials of this linear chain model, because the optimal
How are
you
?
?
How are
you
?
?
How are
you
a
1
c
11
b
2
b
2
c
22
(a)
c
22
(b)
a
2
a
3
b
1
c
12
c
13
c
21
c
22
c
23
a
1
a
2
a
3
b
1
c
21
(a)
c
21
(b)
c
23
(a)
c
23
(b)
c
13
(a)
c
13
(b)
c
12
(a)
c
12
(b)
c
11
(a)
c
11
(b)
c
22
(a)
a
1
a
2
a
3
c
21
(a)
c
23
(a)
c
13
(a)
c
12
(a)
c
11
(a)
Figure 3: The tree-structured subgraph Ga can be mapped
to an equivalent chain-structured model by optimizing
over ci?j for aj = i.
choice of each cij can be determined from aj and the
model parameters. If aj = i, then cij = 1 according
to our edge potential defined in Equation 2. Hence,
setting aj = i requires the inclusion of the corre-
sponding vertex potential ?(a)j (i), as well as u(i, j).
For i? 6= i, either ci?j = 0, which contributes noth-
ing to Equation 5, or ci?j = 1, which contributes
u(i?, j)??, according to our edge potential between
aj and ci?j .
Thus, we can capture the net effect of assigning
aj and then optimally assigning all ci?j in a single
potential M?j(aj = i) =
?(a)j (i) + exp
?
?u(i, j) +
?
j?:|j??j|=1
max(0, u(i, j?)? ?)
?
?
Note that Equation 5 and f are sums of terms in
log space, while Viterbi inference for linear chains
assumes a product of terms in probability space,
which introduces the exponentiation above.
Defining this potential allows us to collapse the
source-side sub-graph inference problem defined
by Equation 5, into a simple linear chain model
that only includes potential functions M?j and ?
(a).
Hence, we can use a highly optimized linear chain
inference implementation rather than a solver for
general tree-structured graphical models. Figure 3
depicts this transformation.
An equivalent approach allows us to evaluate the
424
Algorithm 1 Dual decomposition inference algo-
rithm for the bidirectional model
for t = 1 to max iterations do
r ? 1t . Learning rate
c(a) ? argmax f(a, c(a)) +
?
i,j u(i, j)c
(a)
ij
c(b) ? argmax g(b, c(b))?
?
i,j u(i, j)c
(b)
ij
if c(a) = c(b) then
return c(a) . Converged
u? u + r ? (c(b) ? c(a)) . Dual update
return combine(c(a), c(b)) . Stop early
second line of Equation 4 for fixed u:
max
b,c(b)
?
?g(b, c(b)) +
?
i,j
u(i, j)c(b)ij
?
? . (6)
3.4 Dual Decomposition Algorithm
Now that we have the means to efficiently evalu-
ate Equation 4 for fixed u, we can define the full
dual decomposition algorithm for our model, which
searches for a u that optimizes Equation 4. We can
iteratively search for such a u via sub-gradient de-
scent. We use a learning rate 1t that decays with the
number of iterations t. The full dual decomposition
optimization procedure appears in Algorithm 1.
If Algorithm 1 converges, then we have found a u
such that the value of c(a) that optimizes Equation 5
is identical to the value of c(b) that optimizes Equa-
tion 6. Hence, it is also a solution to our original
optimization problem: Equation 3. Since the dual
problem is an upper bound on the original problem,
this solution must be optimal for Equation 3.
3.5 Convergence and Early Stopping
Our dual decomposition algorithm provides an infer-
ence method that is exact upon convergence.3 When
Algorithm 1 does not converge, the two alignments
c(a) and c(b) can still be used. While these align-
ments may differ, they will likely be more similar
than the alignments of independent aligners.
These alignments will still need to be combined
procedurally (e.g., taking their union), but because
3This certificate of optimality is not provided by other ap-
proximate inference algorithms, such as belief propagation,
sampling, or simulated annealing.
they are more similar, the importance of the combi-
nation procedure is reduced. We analyze the behav-
ior of early stopping experimentally in Section 5.
3.6 Inference Properties
Because we set a maximum number of iterations
n in the dual decomposition algorithm, and each
iteration only involves optimization in a sequence
model, our entire inference procedure is only a con-
stant multiple n more computationally expensive
than evaluating the original directional aligners.
Moreover, the value of u is specific to a sen-
tence pair. Therefore, our approach does not require
any additional communication overhead relative to
the independent directional models in a distributed
aligner implementation. Memory requirements are
virtually identical to the baseline: only u must be
stored for each sentence pair as it is being processed,
but can then be immediately discarded once align-
ments are inferred.
Other approaches to generating one-to-one phrase
alignments are generally more expensive. In par-
ticular, an ITG model requires O(|e|3 ? |f |3) time,
whereas our algorithm requires only
O(n ? (|f ||e|2 + |e||f |2)) .
Moreover, our approach allows Markov distortion
potentials, while standard ITG models are restricted
to only hierarchical distortion.
4 Related Work
Alignment combination normally involves selecting
some A from the output of two directional models.
Common approaches include forming the union or
intersection of the directional sets.
A? = Aa ? Ab
A? = Aa ? Ab .
More complex combiners, such as the grow-diag-
final heuristic (Koehn et al, 2003), produce align-
ment link sets that include all of A? and some sub-
set ofA? based on the relationship of multiple links
(Och et al, 1999).
In addition, supervised word alignment models
often use the output of directional unsupervised
aligners as features or pruning signals. In the case
425
that a supervised model is restricted to proposing
alignment links that appear in the output of a di-
rectional aligner, these models can be interpreted as
a combination technique (Deng and Zhou, 2009).
Such a model-based approach differs from ours in
that it requires a supervised dataset and treats the di-
rectional aligners? output as fixed.
Combination is also related to agreement-based
learning (Liang et al, 2006). This approach to
jointly learning two directional alignment mod-
els yields state-of-the-art unsupervised performance.
Our method is complementary to agreement-based
learning, as it applies to Viterbi inference under the
model rather than computing expectations. In fact,
we employ agreement-based training to estimate the
parameters of the directional aligners in our experi-
ments.
A parallel idea that closely relates to our bidi-
rectional model is posterior regularization, which
has also been applied to the word alignment prob-
lem (Grac?a et al, 2008). One form of posterior
regularization stipulates that the posterior probabil-
ity of alignments from two models must agree, and
enforces this agreement through an iterative proce-
dure similar to Algorithm 1. This approach also
yields state-of-the-art unsupervised alignment per-
formance on some datasets, along with improve-
ments in end-to-end translation quality (Ganchev et
al., 2008).
Our method differs from this posterior regulariza-
tion work in two ways. First, we iterate over Viterbi
predictions rather than posteriors. More importantly,
we have changed the output space of the model to
be a one-to-one phrase alignment via the coherence
edge potential functions.
Another similar line of work applies belief prop-
agation to factor graphs that enforce a one-to-one
word alignment (Cromie`res and Kurohashi, 2009).
The details of our models differ: we employ
distance-based distortion, while they add structural
correspondence terms based on syntactic parse trees.
Also, our model training is identical to the HMM-
based baseline training, while they employ belief
propagation for both training and Viterbi inference.
Although differing in both model and inference, our
work and theirs both find improvements from defin-
ing graphical models for alignment that do not admit
exact polynomial-time inference algorithms.
Aligner Intersection Union Agreement
Model |A?| |A?| |A?|/|A?|
Baseline 5,554 10,998 50.5%
Bidirectional 7,620 10,262 74.3%
Table 1: The bidirectional model?s dual decomposition
algorithm substantially increases the overlap between the
predictions of the directional models, measured by the
number of links in their intersection.
5 Experimental Results
We evaluated our bidirectional model by comparing
its output to the annotations of a hand-aligned cor-
pus. In this way, we can show that the bidirectional
model improves alignment quality and enables the
extraction of more correct phrase pairs.
5.1 Data Conditions
We evaluated alignment quality on a hand-aligned
portion of the NIST 2002 Chinese-English test set
(Ayan and Dorr, 2006). We trained the model on a
portion of FBIS data that has been used previously
for alignment model evaluation (Ayan and Dorr,
2006; Haghighi et al, 2009; DeNero and Klein,
2010). We conducted our evaluation on the first 150
sentences of the dataset, following previous work.
This portion of the dataset is commonly used to train
supervised models.
We trained the parameters of the directional mod-
els using the agreement training variant of the expec-
tation maximization algorithm (Liang et al, 2006).
Agreement-trained IBM Model 1 was used to ini-
tialize the parameters of the HMM-based alignment
models (Brown et al, 1993). Both IBM Model 1
and the HMM alignment models were trained for
5 iterations on a 6.2 million word parallel corpus
of FBIS newswire. This training regimen on this
data set has provided state-of-the-art unsupervised
results that outperform IBM Model 4 (Haghighi et
al., 2009).
5.2 Convergence Analysis
With n = 250 maximum iterations, our dual decom-
position inference algorithm only converges 6.2%
of the time, perhaps largely due to the fact that the
two directional models have different one-to-many
structural constraints. However, the dual decompo-
426
Model Combiner Prec Rec AER
union 57.6 80.0 33.4
Baseline intersect 86.2 62.7 27.2
grow-diag 60.1 78.8 32.1
union 63.3 81.5 29.1
Bidirectional intersect 77.5 75.1 23.6
grow-diag 65.6 80.6 28.0
Table 2: Alignment error rate results for the bidirectional
model versus the baseline directional models. ?grow-
diag? denotes the grow-diag-final heuristic.
Model Combiner Prec Rec F1
union 75.1 33.5 46.3
Baseline intersect 64.3 43.4 51.8
grow-diag 68.3 37.5 48.4
union 63.2 44.9 52.5
Bidirectional intersect 57.1 53.6 55.3
grow-diag 60.2 47.4 53.0
Table 3: Phrase pair extraction accuracy for phrase pairs
up to length 5. ?grow-diag? denotes the grow-diag-final
heuristic.
sition algorithm does promote agreement between
the two models. We can measure the agreement
between models as the fraction of alignment links
in the union A? that also appear in the intersection
A? of the two directional models. Table 1 shows
a 47% relative increase in the fraction of links that
both models agree on by running dual decomposi-
tion (bidirectional), relative to independent direc-
tional inference (baseline). Improving convergence
rates represents an important area of future work.
5.3 Alignment Error Evaluation
To evaluate alignment error of the baseline direc-
tional aligners, we must apply a combination pro-
cedure such as union or intersection to Aa and Ab.
Likewise, in order to evaluate alignment error for
our combined model in cases where the inference
algorithm does not converge, we must apply combi-
nation to c(a) and c(b). In cases where the algorithm
does converge, c(a) = c(b) and so no further combi-
nation is necessary.
We evaluate alignments relative to hand-aligned
data using two metrics. First, we measure align-
ment error rate (AER), which compares the pro-
posed alignment setA to the sure set S and possible
set P in the annotation, where S ? P .
Prec(A,P) =
|A ? P|
|A|
Rec(A,S) =
|A ? S|
|S|
AER(A,S,P) = 1?
|A ? S|+ |A ? P|
|A|+ |S|
AER results for Chinese-English are reported in
Table 2. The bidirectional model improves both pre-
cision and recall relative to all heuristic combination
techniques, including grow-diag-final (Koehn et al,
2003). Intersected alignments, which are one-to-one
phrase alignments, achieve the best AER.
Second, we measure phrase extraction accuracy.
Extraction-based evaluations of alignment better co-
incide with the role of word aligners in machine
translation systems (Ayan and Dorr, 2006). Let
R5(S,P) be the set of phrases up to length 5 ex-
tracted from the sure link set S and possible link set
P . Possible links are both included and excluded
from phrase pairs during extraction, as in DeNero
and Klein (2010). Null aligned words are never in-
cluded in phrase pairs for evaluation. Phrase ex-
traction precision, recall, and F1 for R5(A,A) are
reported in Table 3. Correct phrase pair recall in-
creases from 43.4% to 53.6% (a 23.5% relative in-
crease) for the bidirectional model, relative to the
best baseline.
Finally, we evaluated our bidirectional model in a
large-scale end-to-end phrase-based machine trans-
lation system from Chinese to English, based on
the alignment template approach (Och and Ney,
2004). The translation model weights were tuned for
both the baseline and bidirectional alignments using
lattice-based minimum error rate training (Kumar et
al., 2009). In both cases, union alignments outper-
formed other combination heuristics. Bidirectional
alignments yielded a modest improvement of 0.2%
BLEU4 on a single-reference evaluation set of sen-
tences sampled from the web (Papineni et al, 2002).
4BLEU improved from 29.59% to 29.82% after training
IBM Model 1 for 3 iterations and training the HMM-based
alignment model for 3 iterations. During training, link poste-
riors were symmetrized by pointwise linear interpolation.
427
As our model only provides small improvements in
alignment precision and recall for the union com-
biner, the magnitude of the BLEU improvement is
not surprising.
6 Conclusion
We have presented a graphical model that combines
two classical HMM-based alignment models. Our
bidirectional model, which requires no additional
learning and no supervised data, can be applied us-
ing dual decomposition with only a constant factor
additional computation relative to independent di-
rectional inference. The resulting predictions im-
prove the precision and recall of both alignment
links and extraced phrase pairs in Chinese-English
experiments. The best results follow from combina-
tion via intersection.
Because our technique is defined declaratively in
terms of a graphical model, it can be extended in a
straightforward manner, for instance with additional
potentials on c or improvements to the component
directional models. We also look forward to dis-
covering the best way to take advantage of these
new alignments in downstream applications like ma-
chine translation, supervised word alignment, bilin-
gual parsing (Burkett et al, 2010), part-of-speech
tag induction (Naseem et al, 2009), or cross-lingual
model projection (Smith and Eisner, 2009; Das and
Petrov, 2011).
References
Necip Fazil Ayan and Bonnie J. Dorr. 2006. Going be-
yond AER: An extensive analysis of word alignments
and their impact on MT. In Proceedings of the Asso-
ciation for Computational Linguistics.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A Gibbs sampler for phrasal syn-
chronous grammar induction. In Proceedings of the
Association for Computational Linguistics.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics.
Jamie Brunning, Adria de Gispert, and William Byrne.
2009. Context-dependent alignment models for statis-
tical machine translation. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
David Burkett, John Blitzer, and Dan Klein. 2010.
Joint parsing and alignment with weakly synchronized
grammars. In Proceedings of the North American As-
sociation for Computational Linguistics and IJCNLP.
Fabien Cromie`res and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings of
the European Chapter of the Association for Compu-
tational Linguistics and IJCNLP.
Dipanjan Das and Slav Petrov. 2011. Unsupervised part-
of-speech tagging with bilingual graph-based projec-
tions. In Proceedings of the Association for Computa-
tional Linguistics.
John DeNero and Dan Klein. 2008. The complexity of
phrase alignment problems. In Proceedings of the As-
sociation for Computational Linguistics.
John DeNero and Dan Klein. 2010. Discriminative mod-
eling of extraction sets for machine translation. In
Proceedings of the Association for Computational Lin-
guistics.
John DeNero, Alexandre Bouchard-Co?te?, and Dan Klein.
2008. Sampling alignment structure under a Bayesian
translation model. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing.
Yonggang Deng and Bowen Zhou. 2009. Optimizing
word alignment combination for phrase table training.
In Proceedings of the Association for Computational
Linguistics.
Kuzman Ganchev, Joao Grac?a, and Ben Taskar. 2008.
Better alignments = better translations? In Proceed-
ings of the Association for Computational Linguistics.
Joao Grac?a, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In Proceedings of Neural Information Processing Sys-
tems.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of the Association for
Computational Linguistics.
Xiaodong He. 2007. Using word-dependent transition
models in HMM-based word alignment for statistical
machine. In ACL Workshop on Statistical Machine
Translation.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the North American Chapter of the Association
for Computational Linguistics.
Terry Koo, Alexander M. Rush, Michael Collins, Tommi
Jaakkola, and David Sontag. 2010. Dual decomposi-
tion for parsing with non-projective head automata. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing.
428
Shankar Kumar, Wolfgang Macherey, Chris Dyer, and
Franz Josef Och. 2009. Efficient minimum error rate
training and minimum bayes-risk decoding for trans-
lation hypergraphs and lattices. In Proceedings of the
Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the North
American Chapter of the Association for Computa-
tional Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based,
joint probability model for statistical machine transla-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Tahira Naseem, Benjamin Snyder, Jacob Eisenstein, and
Regina Barzilay. 2009. Multilingual part-of-speech
tagging: Two unsupervised approaches. Journal of Ar-
tificial Intelligence Research.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics.
Franz Josef Och, Christopher Tillman, and Hermann Ney.
1999. Improved alignment models for statistical ma-
chine translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A method for automatic eval-
uation of machine translation. In Proceedings of the
Association for Computational Linguistics.
Alexander M. Rush, David Sontag, Michael Collins, and
Tommi Jaakkola. 2010. On dual decomposition and
linear programming relaxations for natural language
processing. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.
2010. Word alignment with synonym regularization.
In Proceedings of the Association for Computational
Linguistics.
David A. Smith and Jason Eisner. 2009. Parser adapta-
tion and projection with quasi-synchronous grammar
features. In Proceedings of the Conference on Empir-
ical Methods in Natural Language Processing.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the Conference on Computa-
tional linguistics.
429
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 146?155,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Class-Based Agreement Model for
Generating Accurately Inflected Translations
Spence Green
Computer Science Department, Stanford University
spenceg@stanford.edu
John DeNero
Google
denero@google.com
Abstract
When automatically translating from a weakly
inflected source language like English to a tar-
get language with richer grammatical features
such as gender and dual number, the output
commonly contains morpho-syntactic agree-
ment errors. To address this issue, we present
a target-side, class-based agreement model.
Agreement is promoted by scoring a sequence
of fine-grained morpho-syntactic classes that
are predicted during decoding for each transla-
tion hypothesis. For English-to-Arabic transla-
tion, our model yields a +1.04 BLEU average
improvement over a state-of-the-art baseline.
The model does not require bitext or phrase ta-
ble annotations and can be easily implemented
as a feature in many phrase-based decoders.
1 Introduction
Languages vary in the degree to which surface forms
reflect grammatical relations. English is a weakly in-
flected language: it has a narrow verbal paradigm, re-
stricted nominal inflection (plurals), and only the ves-
tiges of a case system. Consequently, translation into
English?which accounts for much of the machine
translation (MT) literature (Lopez, 2008)?often in-
volves some amount of morpho-syntactic dimension-
ality reduction. Less attention has been paid to what
happens during translation from English: richer gram-
matical features such as gender, dual number, and
overt case are effectively latent variables that must
be inferred during decoding. Consider the output of
Google Translate for the simple English sentence in
Fig. 1. The correct translation is a monotone mapping
of the input. However, in Arabic, SVO word order
requires both gender and number agreement between
the subject

?PAJ
??@ ?the car? and verb I. ?
	
YK
 ?go?. The
MT system selects the correct verb stem, but with
masculine inflection. Although the translation has
(1)

?PAJ
??@
the-carsg.def.fem
I. ?
	
YK

gosg.masc

??Q??.
with-speedsg.fem
The car goes quickly
Figure 1: Ungrammatical Arabic output of Google Trans-
late for the English input The car goes quickly. The subject
should agree with the verb in both gender and number, but
the verb has masculine inflection. For clarity, the Arabic
tokens are arranged left-to-right.
the correct semantics, it is ultimately ungrammatical.
This paper addresses the problem of generating text
that conforms to morpho-syntactic agreement rules.
Agreement relations that cross statistical phrase
boundaries are not explicitly modeled in most phrase-
based MT systems (Avramidis and Koehn, 2008).
We address this shortcoming with an agreement
model that scores sequences of fine-grained morpho-
syntactic classes. First, bound morphemes in transla-
tion hypotheses are segmented. Next, the segments
are labeled with classes that encode both syntactic
category information (i.e., parts of speech) and gram-
matical features such as number and gender. Finally,
agreement is promoted by scoring the predicted class
sequences with a generative Markov model.
Our model scores hypotheses during decoding. Un-
like previous models for scoring syntactic relations,
our model does not require bitext annotations, phrase
table features, or decoder modifications. The model
can be implemented using the feature APIs of popular
phrase-based decoders such as Moses (Koehn et al,
2007) and Phrasal (Cer et al, 2010).
Intuition might suggest that the standard n-gram
language model (LM) is sufficient to handle agree-
ment phenomena. However, LM statistics are sparse,
and they are made sparser by morphological varia-
tion. For English-to-Arabic translation, we achieve
a +1.04 BLEU average improvement by tiling our
model on top of a large LM.
146
It has also been suggested that this setting requires
morphological generation because the bitext may not
contain all inflected variants (Minkov et al, 2007;
Toutanova et al, 2008; Fraser et al, 2012). However,
using lexical coverage experiments, we show that
there is ample room for translation quality improve-
ments through better selection of forms that already
exist in the translation model.
2 A Class-based Model of Agreement
2.1 Morpho-syntactic Agreement
Morpho-syntactic agreement refers to a relationship
between two sentence elements a and b that must
have at least one matching grammatical feature.1
Agreement relations tend to be defined for partic-
ular syntactic configurations such as verb-subject,
noun-adjective, and pronoun-antecedent. In some
languages, agreement affects the surface forms of the
words. For example, from the perspective of gener-
ative grammatical theory, the lexicon entry for the
Arabic nominal

?PAJ
??@ ?the car? contains a feminine
gender feature. When this nominal appears in the sub-
ject argument position, the verb-subject agreement
relationship triggers feminine inflection of the verb.
Our model treats agreement as a sequence of
scored, pairwise relations between adjacent words.
Of course, this assumption excludes some agreement
phenomena, but it is sufficient for many common
cases. We focus on English-Arabic translation as
an example of a translation direction that expresses
substantially more morphological information in the
target. These relations are best captured in a target-
side model because they are mostly unobserved (from
lexical clues) in the English source.
The agreement model scores sequences of morpho-
syntactic word classes, which express grammatical
features relevant to agreement. The model has three
components: a segmenter, a tagger, and a scorer.
2.2 Morphological Segmentation
Segmentation is a procedure for converting raw sur-
face forms to component morphemes. In some lan-
guages, agreement relations exist between bound
morphemes, which are syntactically independent yet
phonologically dependent morphemes. For example,
1We use morpho-syntactic and grammatical agreement inter-
changeably, as is common in the literature.
	



Pron+Fem+Sg Verb+Masc+3+Pl Prt Conj
andwillthey writeit
Figure 2: Segmentation and tagging of the Arabic token
A?
	
E?J.

J?J
?? ?and they will write it?. This token has four seg-
ments with conflicting grammatical features. For example,
the number feature is singular for the pronominal object
and plural for the verb. Our model segments the raw to-
ken, tags each segment with a morpho-syntactic class (e.g.,
?Pron+Fem+Sg?), and then scores the class sequences.
the single raw token in Fig. 2 contains at least four
grammatically independent morphemes. Because the
morphemes bear conflicting grammatical features and
basic parts of speech (POS), we need to segment the
token before we can evaluate agreement relations.2
Segmentation is typically applied as a bitext pre-
processing step, and there is a rich literature on the
effect of different segmentation schemata on transla-
tion quality (Koehn and Knight, 2003; Habash and
Sadat, 2006; El Kholy and Habash, 2012). Unlike pre-
vious work, we segment each translation hypothesis
as it is generated (i.e., during decoding). This permits
greater modeling flexibility. For example, it may be
useful to count tokens with bound morphemes as a
unit during phrase extraction, but to score segmented
morphemes separately for agreement.
We treat segmentation as a character-level se-
quence modeling problem and train a linear-chain
conditional random field (CRF) model (Lafferty et
al., 2001). As a pre-processing step, we group con-
tiguous non-native characters (e.g., Latin characters
in Arabic text). The model assigns four labels:
? I: Continuation of a morpheme
? O: Outside morpheme (whitespace)
? B: Beginning of a morpheme
? F: Non-native character(s)
2Segmentation also improves translation of compounding
languages such as German (Dyer, 2009) and Finnish (Macherey
et al, 2011).
147
Translation Model
e Target sequence of I words
f Source sequence of J words
a Sequence ofK phrase alignments for ?e, f?
? Permutation of the alignments for target word order e
h Sequence ofM feature functions
? Sequence of learned weights for theM features
H A priority queue of hypotheses
Class-based Agreement Model
t ? T Set of morpho-syntactic classes
s ? S Set of all word segments
?seg Learned weights for the CRF-based segmenter
?tag Learned weights for the CRF-based tagger
?o, ?t CRF potential functions (emission and transition)
? Sequence of I target-side predicted classes
pi T dimensional (log) prior distribution over classes
s? Sequence of l word segments
? Model state: a tagged segment ?s, t?
Figure 3: Notation used in this paper. The convention eIi
indicates a subsequence of a length I sequence.
The features are indicators for (character, position,
label) triples for a five character window and bigram
label transition indicators.
This formulation is inspired by the classic ?IOB?
text chunking model (Ramshaw and Marcus, 1995),
which has been previously applied to Chinese seg-
mentation (Peng et al, 2004). It can be learned from
gold-segmented data, generally applies to languages
with bound morphemes, and does not require a hand-
compiled lexicon.3 Moreover, it has only four labels,
so Viterbi decoding is very fast. We learn the param-
eters ?seg using a quasi-Newton (QN) procedure with
l1 (lasso) regularization (Andrew and Gao, 2007).
2.3 Morpho-syntactic Tagging
After segmentation, we tag each segment with a fine-
grained morpho-syntactic class. For this task we also
train a standard CRF model on full sentences with
gold classes and segmentation. We use the same QN
procedure as before to obtain ?tag.
A translation derivation is a tuple ?e, f, a? where
e is the target, f is the source, and a is an alignment
between the two. The CRF tagging model predicts a
target-side class sequence ??
?? = arg max
?
I?
i=1
?tag ? {?o(?i, i, e) + ?t(?i, ?i?1)}
where further notation is defined in Fig. 3.
3Mada, the standard tool for Arabic segmentation (Habash
and Rambow, 2005), relies on a manually compiled lexicon.
Set of Classes The tagger assignsmorpho-syntactic
classes, which are coarse POS categories refined with
grammatical features such as gender and definiteness.
The coarse categories are the universal POS tag set
described by Petrov et al (2012). More than 25 tree-
banks (in 22 languages) can be automatically mapped
to this tag set, which includes ?Noun? (nominals),
?Verb? (verbs), ?Adj? (adjectives), and ?ADP? (pre-
and post-positions). Many of these treebanks also
contain per-token morphological annotations. It is
easy to combine the coarse categories with selected
grammatical annotations.
For Arabic, we used the coarse POS tags plus
definiteness and the so-called phi features (gender,
number, and person).4 For example,

?PAJ
??@ ?the
car? would be tagged ?Noun+Def+Sg+Fem?. We
restricted the set of classes to observed combinations
in the training data, so the model implicitly disallows
incoherent classes like ?Verb+Def?.
Features The tagging CRF includes emission fea-
tures ?o that indicate a class ?i appearing with various
orthographic characteristics of the word sequence
being tagged. In typical CRF inference, the entire
observation sequence is available throughout infer-
ence, so these features can be scored on observed
words in an arbitrary neighborhood around the cur-
rent position i. However, we conduct CRF inference
in tandem with the translation decoding procedure
(?3), creating an environment in which subsequent
words of the observation are not available; the MT
system has yet to generate the rest of the translation
when the tagging features for a position are scored.
Therefore, we only define emission features on the
observed words at the current and previous positions
of a class: ?o(?i, ei, ei?1).
The emission features are word types, prefixes and
suffixes of up to three characters, and indicators for
digits and punctuation. None of these features are
language specific.
Bigram transition features ?t encode local agree-
ment relations. For example, the model learns that the
Arabic class ?Noun+Fem? is followed by ?Adj+Fem?
and not ?Adj+Masc? (noun-adjective gender agree-
ment).
4Case is also relevant to agreement in Arabic, but it is mostly
indicated by diacritics, which are absent in unvocalized text.
148
2.4 Word Class Sequence Scoring
The CRF tagger model defines a conditional distribu-
tion p(? |e; ?tag) for a class sequence ? given a sen-
tence e and model parameters ?tag. That is, the sam-
ple space is over class?not word?sequences. How-
ever, in MT, we seek a measure of sentence quality
q(e) that is comparable across different hypotheses
on the beam (much like the n-gram language model
score). Discriminative model scores have been used
as MT features (Galley and Manning, 2009), but we
obtained better results by scoring the 1-best class se-
quences with a generative model. We trained a simple
add-1 smoothed bigram language model over gold
class sequences in the same treebank training data:
q(e) = p(?) =
I?
i=1
p(?i|?i?1)
We chose a bigram model due to the aggressive
recombination strategy in our phrase-based decoder.
For contexts in which the LM is guaranteed to back
off (for instance, after an unseen bigram), our decoder
maintains only theminimal state needed (perhaps only
a single word). In less restrictive decoders, higher
order scoring models could be used to score longer-
distance agreement relations.
We integrate the segmentation, tagging, and scor-
ing models into a self-contained component in the
translation decoder.
3 Inference during Translation Decoding
Scoring the agreement model as part of translation
decoding requires a novel inference procedure. Cru-
cially, the inference procedure does not measurably
affect total MT decoding time.
3.1 Phrase-based Translation Decoding
We consider the standard phrase-based approach to
MT (Och and Ney, 2004). The distribution p(e|f) is
modeled directly using a log-linear model, yielding
the following decision rule:
e? = arg max
e,a,?
{
M?
m=1
?mhm(e, f, a,?)
}
(1)
This decoding problem is NP-hard, thus a beam search
is often used (Fig. 4). The beam search relies on three
operations, two of which affect the agreement model:
Input: implicitly defined search space
generate initial hypotheses and add toH
setHfinal to ?
whileH is not empty:
setHext to ?
for each hypothesis ? inH:
if ? is a goal hypothesis:
add ? toHfinal
else Extend ? and add toHext IScore agreement
Recombine and PruneHext
setH toHext
Output: argmax ofHfinal
Figure 4: Breadth-first beam search algorithm of Och and
Ney (2004). Typically, a hypothesis stackH is maintained
for each unique source coverage set.
Input: (eI1, n, is_goal)
run segmenter on attachment eIn+1 to get s?
L
1
get model state ? = ?s, t? for translation prefix en1
initialize pi to ??
set pi(t) = 0
compute ?? from parameters ?s, s?L1 , pi, is_goal?
compute q(eIn+1) = p(?
?) under the generative LM
set model state ?new = ?s?L, ??L? for prefix e
I
1
Output: q(eIn+1)
Figure 5: Procedure for scoring agreement for each hy-
pothesis generated during the search algorithm of Fig. 4.
In the extended hypothesis eI1, the index n+ 1 indicates
the start of the new attachment.
? Extend a hypothesis with a new phrase pair
? Recombine hypotheses with identical states
We assume familiarity with these operations, which
are described in detail in (Och and Ney, 2004).
3.2 Agreement Model Inference
The class-based agreement model is implemented as
a feature function hm in Eq. (1). Specifically, when
Extend generates a new hypothesis, we run the algo-
rithm shown in Fig. 5. The inputs are a translation
hypothesis eI1, an index n distinguishing the prefix
from the attachment, and a flag indicating if their
concatenation is a goal hypothesis.
The beam search maintains state for each deriva-
tion, the score of which is a linear combination of
the feature values. States in this program depend on
some amount of lexical history. With a trigram lan-
guage model, the state might be the last two words
of the translation prefix. Recombine can be applied
to any two hypotheses with equivalent states. As a
149
result, two hypotheses with different full prefixes?
and thus potentially different sequences of agreement
relations?can be recombined.
Incremental Greedy Decoding Decoding with
the CRF-based tagger model in this setting requires
some slight modifications to the Viterbi algorithm.
We make a greedy approximation that permits recom-
bination and works well in practice. The agreement
model state is the last tagged segment ?s, t? of the
concatenated hypothesis. We tag a new attachment by
assuming a prior distribution pi over the starting posi-
tion such that pi(t) = 0 and ?? for all other classes,
a deterministic distribution in the tropical semiring.
This forces the Viterbi path to go through t. We only
tag the final boundary symbol for goal hypotheses.
To accelerate tagger decoding in our experiments,
we also used tagging dictionaries for frequently ob-
served word types. For each word type observed more
than 100 times in the training data, we restricted the
set of possible classes to the set of observed classes.
3.3 Translation Model Features
The agreement model score is one decoder feature
function. The output of the procedure in Fig. 5 is the
log probability of the class sequence of each attach-
ment. Summed over all attachments, this gives the
log probability of the whole class sequence.
We also add a new length penalty feature. To dis-
criminate between hypotheses that might have the
same number of raw tokens, but different underlying
segmentations, we add a penalty equal to the length
difference between the segmented and unsegmented
attachments |s?L1 | ? |e
I
n+1|.
4 Related Work
We compare our class-based model to previous ap-
proaches to scoring syntactic relations in MT.
Unification-based Formalisms Agreement rules
impose syntactic and semantic constraints on the
structure of sentences. A principled way to model
these constraints is with a unification-based gram-
mar (UBG). Johnson (2003) presented algorithms for
learning and parsing with stochastic UBGs. However,
training data for these formalisms remains extremely
limited, and it is unclear how to learn such knowledge-
rich representations from unlabeled data. One partial
solution is to manually extract unification rules from
phrase-structure trees. Williams and Koehn (2011)
annotated German trees, and extracted translation
rules from them. They then specified manual unifi-
cation rules, and applied a penalty according to the
number of unification failures in a hypothesis. In
contrast, our class-based model does not require any
manual rules and scores similar agreement phenom-
ena as probabilistic sequences.
Factored Translation Models Factored transla-
tion models (Koehn and Hoang, 2007) facilitate a
more data-oriented approach to agreement modeling.
Words are represented as a vector of features such as
lemma and POS. The bitext is annotated with separate
models, and the annotations are saved during phrase
extraction. Hassan et al (2007) noticed that the target-
side POS sequences could be scored, much as we do
in this work. They used a target-side LM over Combi-
natorial Categorial Grammar (CCG) supertags, along
with a penalty for the number of operator violations,
and also modified the phrase probabilities based on
the tags. However, Birch et al (2007) showed that
this approach captures the same re-ordering phenom-
ena as lexicalized re-ordering models, which were
not included in the baseline. Birch et al (2007) then
investigated source-side CCG supertag features, but
did not show an improvement for Dutch-English.
Subotin (2011) recently extended factored transla-
tion models to hierarchical phrase-based translation
and developed a discriminative model for predicting
target-side morphology in English-Czech. His model
benefited from gold morphological annotations on
the target-side of the 8M sentence bitext.
In contrast to these methods, our model does not af-
fect phrase extraction and does not require annotated
translation rules.
Class-based LMs Class-based LMs (Brown et al,
1992) reduce lexical sparsity by placing words in
equivalence classes. They have been widely used
for speech recognition, but not for MT. Och (1999)
showed a method for inducing bilingual word classes
that placed each phrase pair into a two-dimensional
equivalence class. To our knowledge, Uszkoreit and
Brants (2008) are the only recent authors to show an
improvement in a state-of-the-art MT system using
class-based LMs. They used a classical exchange al-
gorithm for clustering, and learned 512 classes from
150
a large monolingual corpus. Then they mixed the
classes into a word-based LM. However, both Och
(1999) and Uszkoreit and Brants (2008) relied on
automatically induced classes. It is unclear if their
classes captured agreement information.
Monz (2011) recently investigated parameter es-
timation for POS-based language models, but his
classes did not include inflectional features.
Target-Side Syntactic LMs Our agreement model
is a form of syntactic LM, of which there is a long
history of research, especially in speech processing.5
Syntactic LMs have traditionally been too slow for
scoring during MT decoding. One exception was
the quadratic-time dependency language model pre-
sented by Galley and Manning (2009). They applied
a quadratic time dependency parser to every hypothe-
sis during decoding. However, to achieve quadratic
running time, they permitted ill-formed trees (e.g.,
parses with multiple roots). More recently, Schwartz
et al (2011) integrated a right-corner, incremental
parser into Moses. They showed a large improve-
ment for Urdu-English, but decoding slowed by three
orders of magnitude.6 In contrast, our class-based
model encodes shallow syntactic information without
a noticeable effect on decoding time.
Our model can be viewed as a way to score local
syntactic relations without extensive decoder modifi-
cations. For long-distance relations, Shen et al (2010)
proposed a new decoder that generates target-side
dependency trees. The target-side structure enables
scoring hypotheses with a trigram dependency LM.
5 Experiments
We first evaluate the Arabic segmenter and tagger
components independently, then provide English-
Arabic translation quality results.
5.1 Intrinsic Evaluation of Components
Experimental Setup All experiments use the Penn
Arabic Treebank (ATB) (Maamouri et al, 2004) parts
1?3 divided into training/dev/test sections according
to the canonical split (Rambow et al, 2005).7
5See (Zhang, 2009) for a comprehensive survey.
6In principle, their parser should run in linear time. An imple-
mentation issue may account for the decoding slowdown. (p.c.)
7LDC catalog numbers: LDC2008E61 (ATBp1v4),
LDC2008E62 (ATBp2v3), and LDC2008E22 (ATBp3v3.1).
Full (%) Incremental (%)
Segmenter 98.6 ?
Tagger 96.3 96.2
Table 1: Intrinsic evaluation accuracy [%] (development
set) for Arabic segmentation and tagging.
The ATB contains clitic-segmented text with per-
segment morphological analyses (in addition to
phrase-structure trees, which we discard). For train-
ing the segmenter, we used markers in the vocalized
section to construct the IOB character sequences. For
training the tagger, we automatically converted the
ATB morphological analyses to the fine-grained class
set. This procedure resulted in 89 classes.
For the segmentation evaluation, we report per-
character labeling accuracy.8 For the tagger, we re-
port per-token accuracy.
Results Tbl. 1 shows development set accuracy for
two settings. Full is a standard evaluation in which
features may be defined over the whole sentence. This
includes next-character segmenter features and next-
word tagger features. Incremental emulates the MT
setting in which the models are restricted to current
and previous observation features. Since the seg-
menter operates at the character level, we can use
the same feature set. However, next-observation fea-
tures must be removed from the tagger. Nonetheless,
tagging accuracy only decreases by 0.1%.
5.2 Translation Quality
Experimental Setup Our decoder is based on the
phrase-based approach to translation (Och and Ney,
2004) and contains various feature functions includ-
ing phrase relative frequency, word-level alignment
statistics, and lexicalized re-ordering models (Till-
mann, 2004; Och et al, 2004). We tuned the feature
weights on a development set using lattice-based min-
imum error rate training (MERT) (Macherey et al,
The data was pre-processed with packages from the Stanford
Arabic parser (Green and Manning, 2010). The corpus split is
available at http://nlp.stanford.edu/projects/arabic.shtml.
8We ignore orthographic re-normalization performed by the
annotators. For example, they converted the contraction ???? ll
back to ??@'?? l Al. As a result, we can report accuracy since
the guess and gold segmentations have equal numbers of non-
whitespace characters.
151
MT04 (tune) MT02 MT03 MT05 Avg
Baseline 18.14 23.87 18.88 22.60
+POS 18.11 ?0.03 23.65 ?0.22 18.99 +0.11 22.29 ?0.31 ?0.17
+POS+Agr 18.86 +0.72 24.84 +0.97 20.26 +1.38 23.48 +0.88 +1.04
genres nw nw nw nw
#sentences 1353 728 663 1056 2447
Table 2: Translation quality results (BLEU-4 [%]) for newswire (nw) sets. Avg is the weighted averaged (by number of
sentences) of the individual test set gains. All improvements are statistically significant at p ? 0.01.
MT06 MT08 Avg
Baseline 14.68 14.30
+POS 14.57 ?0.11 14.30 +0.0 ?0.06
+POS+Agr 15.04 +0.36 14.49 +0.19 +0.29
genres nw,bn,ng nw,ng,wb
#sentences 1797 1360 3157
Table 3: Mixed genre test set results (BLEU-4 [%]). The
MT06 result is statistically significant at p ? 0.01; MT08
is significant at p ? 0.02. The genres are: nw, broadcast
news (bn), newsgroups (ng), and weblog (wb).
2008). For each set of results, we initialized MERT
with uniform feature weights.
We trained the translation model on 502 million
words of parallel text collected from a variety of
sources, including theWeb. Word alignments were in-
duced using a hidden Markov model based alignment
model (Vogel et al, 1996) initialized with bilexical
parameters from IBM Model 1 (Brown et al, 1993).
Both alignment models were trained using two itera-
tions of the expectation maximization algorithm. Our
distributed 4-gram language model was trained on
600 million words of Arabic text, also collected from
many sources including the Web (Brants et al, 2007).
For development and evaluation, we used the NIST
Arabic-English data sets, each of which contains one
set of Arabic sentences and multiple English refer-
ences. To reverse the translation direction for each
data set, we chose the first English reference as the
source and the Arabic as the reference.
The NIST sets come in two varieties: newswire
(MT02-05) and mixed genre (MT06,08). Newswire
contains primarily Modern Standard Arabic (MSA),
while the mixed genre data sets also contain tran-
scribed speech and web text. Since the ATB contains
MSA, and significant lexical and syntactic differences
may exist between MSA and the mixed genres, we
achieved best results by tuning on MT04, the largest
newswire set.
We evaluated translation quality with BLEU-4 (Pa-
pineni et al, 2002) and computed statistical signifi-
cance with the approximate randomization method
of Riezler and Maxwell (2005).9
6 Discussion of Translation Results
Tbl. 2 shows translation quality results on newswire,
while Tbl. 3 contains results for mixed genres. The
baseline is our standard system feature set. For
comparison, +POS indicates our class-based model
trained on the 11 coarse POS tags only (e.g., ?Noun?).
Finally, +POS+Agr shows the class-based model
with the fine-grained classes (e.g., ?Noun+Fem+Sg?).
The best result?a +1.04 BLEU average gain?
was achieved when the class-based model training
data, MT tuning set, and MT evaluation set contained
the same genre. We realized smaller, yet statistically
significant, gains on the mixed genre data sets. We
tried tuning on both MT06 and MT08, but obtained
insignificant gains. In the next section, we investigate
this issue further.
Tuning with a Treebank-Trained Feature The
class-based model is trained on the ATB, which is pre-
dominantly MSA text. This data set is syntactically
regular, meaning that it does not have highly dialectal
content, foreign scripts, disfluencies, etc. Conversely,
the mixed genre data sets contain more irregulari-
ties. For example, 57.4% of MT06 comes from non-
newswire genres. Of the 764 newsgroup sentences,
112 contain some Latin script tokens, while others
contain very little morphology:
9With the implementation of Clark et al (2011), available at:
http://github.com/jhclark/multeval.
152
(2) ?


??
	
g@
mix
1/2
1/2
H. ??
cup
?
	
g
vinegar
hA
	
?

K
apple
Mix 1/2 cup apple vinegar
(3)

@YK.
start
l .
?A
	
KQK.
program
? 	P?J
?
miozik
?

A?
maatsh
MusicMatch
MusicMatch
Start the program music match (MusicMatch)
In these imperatives, there are no lexically marked
agreement relations to score. Ex. (2) is an excerpt
from a recipe that appears in full in MT06. Ex. (3)
is part of usage instructions for the MusicMatch soft-
ware. The ATB contains few examples like these, so
our class-based model probably does not effectively
discriminate between alternative hypotheses for these
types of sentences.
Phrase Table Coverage In a standard phrase-
based system, effective translation into a highly in-
flected target language requires that the phrase table
contain the inflected word forms necessary to con-
struct an output with correct agreement. If the requi-
site words are not present in the search space of the
decoder, then no feature function would be sufficient
to enforce morpho-syntactic agreement.
During development, we observed that the phrase
table of our large-scale English-Arabic system did
often contain the inflected forms that we desired the
system to select. In fact, correctly agreeing alterna-
tives often appeared in n-best translation lists. To
verify this observation, we computed the lexical cov-
erage of the MT05 reference sentences in the decoder
search space. The statistics below report the token-
level recall of reference unigrams:10
? Baseline system translation output: 44.6%
? Phrase pairs matching source n-grams: 67.8%
The bottom category includes all lexical items that
the decoder could produce in a translation of the
source. This large gap between the unigram recall
of the actual translation output (top) and the lexical
coverage of the phrase-based model (bottom) indi-
cates that translation performance can be improved
dramatically by altering the translation model through
features such as ours, without expanding the search
space of the decoder.
10To focus on possibly inflected word forms, we excluded
numbers and punctuation from this analysis.
Human Evaluation We also manually evaluated
the MT05 output for improvements in agreement.11
Our system produced different output from the base-
line for 785 (74.3%) sentences. We randomly sam-
pled 100 of these sentences and counted agreement
errors of all types. The baseline contained 78 errors,
while our system produced 66 errors, a statistically
significant 15.4% error reduction at p ? 0.01 accord-
ing to a paired t-test.
In our output, a frequent source of remaining errors
was the case of so-called ?deflected agreement?: inan-
imate plural nouns require feminine singular agree-
ment with modifiers. On the other hand, animate
plural nouns require the sound plural, which is indi-
cated by an appropriate masculine or feminine suffix.
For example, the inanimate plural HAK
B??@ ?states? re-
quires the singular feminine adjective

?Yj

J?? @ ?united?,
not the sound plural H@YjJ?? @. The ATB does not con-
tain animacy annotations, so our agreement model
cannot discriminate between these two cases. How-
ever, Alkuhlani and Habash (2011) have recently
started annotating the ATB for animacy, and our
model could benefit as more data is released.
7 Conclusion and Outlook
Our class-based agreement model improves transla-
tion quality by promoting local agreement, but with
a minimal increase in decoding time and no addi-
tional storage requirements for the phrase table. The
model can be implemented with a standard CRF pack-
age, trained on existing treebanks for many languages,
and integrated easily with many MT feature APIs.
We achieved best results when the model training
data, MT tuning set, and MT evaluation set con-
tained roughly the same genre. Nevertheless, we also
showed an improvement, albeit less significant, on
mixed genre evaluation sets.
In principle, our class-based model should be more
robust to unseen word types and other phenomena that
make non-newswire genres challenging. However,
our analysis has shown that for Arabic, these genres
typically contain more Latin script and transliterated
words, and thus there is less morphology to score.
One potential avenue of future work would be to adapt
our component models to new genres by self-training
them on the target side of a large bitext.
11The annotator was the first author.
153
AcknowledgmentsWe thank Zhifei Li and ChrisManning
for helpful discussions, and Klaus Macherey, Wolfgang
Macherey, Daisy Stanton, and Richard Zens for engineer-
ing support. This work was conducted while the first au-
thor was an intern at Google. At Stanford, the first author
is supported by a National Science Foundation Graduate
Research Fellowship.
References
S. Alkuhlani and N. Habash. 2011. A corpus for modeling
morpho-syntactic agreement in Arabic: Gender, number and
rationality. In ACL-HLT.
G. Andrew and J. Gao. 2007. Scalable training of l1-regularized
log-linear models. In ICML.
E. Avramidis and P. Koehn. 2008. Enriching morphologically
poor languages for statistical machine translation. In ACL.
A. Birch, M. Osborne, and P. Koehn. 2007. CCG supertags in
factored statistical machine translation. In WMT.
T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large
language models in machine translation. In EMNLP-CoNLL.
P. F. Brown, P. V. deSouza, R. L. Mercer, V. J. Della Pietra,
and J. C. Lai. 1992. Class-based n-gram models of natural
language. Computational Linguistics, 18:467?479.
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993. The mathematics of statistical machine translation:
Parameter estimation. Computational Linguistics, 19(2):263?
313.
D. Cer, M. Galley, D. Jurafsky, and C. D.Manning. 2010. Phrasal:
A statistical machine translation toolkit for exploring new
model features. In HLT-NAACL, Demonstration Session.
J. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Better hy-
pothesis testing for statistical machine translation: Controlling
for optimizer instability. In ACL.
C. Dyer. 2009. Using a maximum entropy model to build seg-
mentation lattices for MT. In NAACL.
A. El Kholy and N. Habash. 2012. Orthographic and mor-
phological processing for English-Arabic statistical machine
translation. Machine Translation, 26(1-2):25?45.
A. Fraser, M. Weller, A. Cahill, and F. Cap. 2012. Modeling
inflection and word-formation in SMT. In EACL.
M. Galley and C. D. Manning. 2009. Quadratic-time dependency
parsing for machine translation. In ACL-IJCNLP.
S. Green and C. D. Manning. 2010. Better Arabic parsing:
baselines, evaluations, and analysis. In COLING.
N. Habash and O. Rambow. 2005. Arabic tokenization, part-of-
speech tagging and morphological disambiguation in one fell
swoop. In ACL.
N. Habash and F. Sadat. 2006. Arabic preprocessing schemes
for statistical machine translation. In NAACL.
H. Hassan, K. Sima?an, and A. Way. 2007. Supertagged phrase-
based statistical machine translation. In ACL.
M. Johnson. 2003. Learning and parsing stochastic unification-
based grammars. In COLT.
P. Koehn and H. Hoang. 2007. Factored translation models. In
EMNLP-CoNLL.
P. Koehn and K. Knight. 2003. Empirical methods for compound
splitting. In EACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch, M. Federico,
N. Bertoldi, et al 2007. Moses: Open source toolkit for sta-
tistical machine translation. In ACL, Demonstration Session.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional ran-
dom fields: Probablistic models for segmenting and labeling
sequence data. In ICML.
A. Lopez. 2008. Statistical machine translation. ACMComputing
Surveys, 40(8):1?49.
M. Maamouri, A. Bies, T. Buckwalter, and W. Mekki. 2004.
The Penn Arabic Treebank: Building a large-scale annotated
Arabic corpus. In NEMLAR.
W. Macherey, F. Och, I. Thayer, and J. Uszkoreit. 2008. Lattice-
basedminimum error rate training for statistical machine trans-
lation. In EMNLP.
K. Macherey, A. Dai, D. Talbot, A. Popat, and F. Och. 2011.
Language-independent compound splitting with morphologi-
cal operations. In ACL.
E. Minkov, K. Toutanova, and H. Suzuki. 2007. Generating
complex morphology for machine translation. In ACL.
C. Monz. 2011. Statistical machine translation with local lan-
guage models. In EMNLP.
F. J. Och and H. Ney. 2004. The alignment template approach
to statistical machine translation. Computational Linguistics,
30(4):417?449.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada,
A. Fraser, et al 2004. A smorgasbord of features for sta-
tistical machine translation. In HLT-NAACL.
F. J. Och. 1999. An efficient method for determining bilingual
word classes. In EACL.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: a
method for automatic evaluation of machine translation. In
ACL.
F. Peng, F. Feng, and A. McCallum. 2004. Chinese segmentation
and new word detection using conditional random fields. In
COLING.
S. Petrov, D. Das, and R. McDonald. 2012. A universal part-of-
speech tagset. In LREC.
O. Rambow, D. Chiang, M. Diab, N. Habash, R. Hwa, et al 2005.
Parsing Arabic dialects. Technical report, Johns Hopkins
University.
L. A. Ramshaw and M. Marcus. 1995. Text chunking using
transformation-based learning. In Proc. of the ThirdWorkshop
on Very Large Corpora.
S. Riezler and J. T. Maxwell. 2005. On some pitfalls in auto-
matic evaluation and significance testing in MT. In ACL-05
Workshop on Intrinsic and Extrinsic Evaluation Measures for
Machine Translation and/or Summarization (MTSE).
L. Schwartz, C. Callison-Burch, W. Schuler, and S. Wu. 2011.
Incremental syntactic language models for phrase-based trans-
lation. In ACL-HLT.
L. Shen, J. Xu, and R. Weischedel. 2010. String-to-dependency
statistical machine translation. Computational Linguistics,
36(4):649?671.
154
M. Subotin. 2011. An exponential translation model for target
language morphology. In ACL-HLT.
C. Tillmann. 2004. A unigram orientation model for statistical
machine translation. In NAACL.
K. Toutanova, H. Suzuki, and A. Ruopp. 2008. Applying mor-
phology generation models to machine translation. In ACL-
HLT.
J. Uszkoreit and T. Brants. 2008. Distributed word clustering
for large scale class-based language modeling in machine
translation. In ACL-HLT.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word
alignment in statistical translation. In COLING.
P. Williams and P. Koehn. 2011. Agreement constraints for
statistical machine translation into German. In WMT.
Y. Zhang. 2009. Structured Language Models for Statistical Ma-
chine Translation. Ph.D. thesis, Carnegie Mellon University.
155
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 17?22,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Feature-Rich Constituent Context Model for Grammar Induction
Dave Golland
University of California, Berkeley
dsg@cs.berkeley.edu
John DeNero
Google
denero@google.com
Jakob Uszkoreit
Google
uszkoreit@google.com
Abstract
We present LLCCM, a log-linear variant of the
constituent context model (CCM) of grammar
induction. LLCCM retains the simplicity of
the original CCM but extends robustly to long
sentences. On sentences of up to length 40,
LLCCM outperforms CCM by 13.9% brack-
eting F1 and outperforms a right-branching
baseline in regimes where CCM does not.
1 Introduction
Unsupervised grammar induction is a fundamental
challenge of statistical natural language processing
(Lari and Young, 1990; Pereira and Schabes, 1992;
Carroll and Charniak, 1992). The constituent con-
text model (CCM) for inducing constituency parses
(Klein and Manning, 2002) was the first unsuper-
vised approach to surpass a right-branching base-
line. However, the CCM only effectively models
short sentences. This paper shows that a simple re-
parameterization of the model, which ties together
the probabilities of related events, allows the CCM
to extend robustly to long sentences.
Much recent research has explored dependency
grammar induction. For instance, the dependency
model with valence (DMV) of Klein and Manning
(2004) has been extended to utilize multilingual in-
formation (Berg-Kirkpatrick and Klein, 2010; Co-
hen et al, 2011), lexical information (Headden III et
al., 2009), and linguistic universals (Naseem et al,
2010). Nevertheless, simplistic dependency models
like the DMV do not contain information present in
a constituency parse, such as the attachment order of
object and subject to a verb.
Unsupervised constituency parsing is also an ac-
tive research area. Several studies (Seginer, 2007;
Reichart and Rappoport, 2010; Ponvert et al, 2011)
have considered the problem of inducing parses
over raw lexical items rather than part-of-speech
(POS) tags. Additional advances have come from
more complex models, such as combining CCM
and DMV (Klein and Manning, 2004) and model-
ing large tree fragments (Bod, 2006).
The CCM scores each parse as a product of prob-
abilities of span and context subsequences. It was
originally evaluated only on unpunctuated sentences
up to length 10 (Klein and Manning, 2002), which
account for only 15% of the WSJ corpus; our exper-
iments confirm the observation in (Klein, 2005) that
performance degrades dramatically on longer sen-
tences. This problem is unsurprising: CCM scores
each constituent type by a single, isolated multino-
mial parameter.
Our work leverages the idea that sharing infor-
mation between local probabilities in a structured
unsupervised model can lead to substantial accu-
racy gains, previously demonstrated for dependency
grammar induction (Cohen and Smith, 2009; Berg-
Kirkpatrick et al, 2010). Our model, Log-Linear
CCM (LLCCM), shares information between the
probabilities of related constituents by expressing
them as a log-linear combination of features trained
using the gradient-based learning procedure of Berg-
Kirkpatrick et al (2010). In this way, the probabil-
ity of generating a constituent is informed by related
constituents.
Our model improves unsupervised constituency
parsing of sentences longer than 10 words. On sen-
tences of up to length 40 (96% of all sentences in
the Penn Treebank), LLCCM outperforms CCM by
13.9% (unlabeled) bracketing F1 and, unlike CCM,
outperforms a right-branching baseline on sentences
longer than 15 words.
17
2 Model
The CCM is a generative model for the unsuper-
vised induction of binary constituency parses over
sequences of part-of-speech (POS) tags (Klein and
Manning, 2002). Conditioned on the constituency or
distituency of each span in the parse, CCM generates
both the complete sequence of terminals it contains
and the terminals in the surrounding context.
Formally, the CCM is a probabilistic model that
jointly generates a sentence, s, and a bracketing,
B, specifying whether each contiguous subsequence
is a constituent or not, in which case the span is
called a distituent. Each subsequence of POS tags,
or SPAN, ?, occurs in a CONTEXT, ?, which is an
ordered pair of preceding and following tags. A
bracketing is a boolean matrix B, indicating which
spans (i, j) are constituents (Bij = true) and which
are distituents (Bij = false). A bracketing is con-
sidered legal if its constituents are nested and form a
binary tree T (B).
The joint distribution is given by:
P(s,B) = PT (B) ?
?
i,j?T (B)
PS (?(i, j, s)|true) PC (?(i, j, s)|true) ?
?
i,j 6?T (B)
PS (?(i, j, s)|false) PC (?(i, j, s)|false)
The prior over unobserved bracketings PT (B) is
fixed to be the uniform distribution over all legal
bracketings. The other distributions, PS (?) and
PC (?), are multinomials whose isolated parameters
are estimated to maximize the likelihood of a set of
observed sentences {sn} using EM (Dempster et al,
1977).1
2.1 The Log-Linear CCM
A fundamental limitation of the CCM is that it con-
tains a single isolated parameter for every span. The
number of different possible span types increases ex-
ponentially in span length, leading to data sparsity as
the sentence length increases.
1As mentioned in (Klein and Manning, 2002), the CCM
model is deficient because it assigns probability mass to yields
and spans that cannot consistently combine to form a valid sen-
tence. Our model does not address this issue, and hence it is
similarly deficient.
The Log-Linear CCM (LLCCM) reparameterizes
the distributions in the CCM using intuitive features
to address the limitations of CCM while retaining
its predictive power. The set of proposed features
includes a BASIC feature for each parameter of the
original CCM, enabling the LLCCM to retain the
full expressive power of the CCM. In addition, LL-
CCM contains a set of coarse features that activate
across distinct spans.
To introduce features into the CCM, we express
each of its local conditional distributions as a multi-
class logistic regression model. Each local distri-
bution, Pt(y|x) for t ? {SPAN,CONTEXT}, condi-
tions on label x ? {true, false} and generates an
event (span or context) y. We can define each lo-
cal distribution in terms of a weight vector, w, and
feature vector, fxyt, using a log-linear model:
Pt(y|x) =
exp ?w, fxyt?
?
y? exp
?
w, fxy?t
? (1)
This technique for parameter transformation was
shown to be effective in unsupervised models for
part-of-speech induction, dependency grammar in-
duction, word alignment, and word segmentation
(Berg-Kirkpatrick et al, 2010). In our case, replac-
ing multinomials via featurized models not only im-
proves model accuracy, but also lets the model apply
effectively to a new regime of long sentences.
2.2 Feature Templates
In the SPAN model, for each span y = [?1, . . . , ?n]
and label x, we use the following feature templates:
BASIC: I [y = ? ? x = ?]
BOUNDARY: I [?1 = ? ? ?n = ? ? x = ?]
PREFIX: I [?1 = ? ? x = ?]
SUFFIX: I [?n = ? ? x = ?]
Just as the external CONTEXT is a signal of con-
stituency, so too is the internal ?context.? For exam-
ple, there are many distinct noun phrases with differ-
ent spans that all begin with DT and end with NN; a
fact expressed by the BOUNDARY feature (Table 1).
In the CONTEXT model, for each context y =
[?1, ?2] and constituent/distituent decision x, we use
the following feature templates:
BASIC: I [y = ? ? x = ?]
L-CONTEXT: I [?1 = ? ? x = ?]
R-CONTEXT: I [?2 = ? ? x = ?]
18
Consider the following example extracted from
the WSJ:
0 The 1
DT
Venezuelan 2
JJ
currency 3
NN
NP-SBJ
plummeted 4
VBD
this 5
DT
year 6
NN
NP-TMP
VP
S
Both spans (0, 3) and (4, 6) are constituents corre-
sponding to noun phrases whose features are shown
in Table 1:
Feature Name (0,3) (4, 6)
sp
an
BASIC-DT-JJ-NN: 1 0
BASIC-DT-NN: 0 1
BOUNDARY-DT-NN: 1 1
PREFIX-DT: 1 1
SUFFIX-NN: 1 1
co
nt
ex
t
BASIC--VBD: 1 0
BASIC-VBD-: 0 1
L-CONTEXT-: 1 0
L-CONTEXT-VBD: 0 1
R-CONTEXT-VBD: 1 0
R-CONTEXT-: 0 1
Table 1: Span and context features for constituent spans (0, 3)
and (4, 6). The symbol  indicates a sentence boundary.
Notice that although the BASIC span features are
active for at most one span, the remaining features
fire for both spans, effectively sharing information
between the local probabilities of these events.
The coarser CONTEXT features factor the context
pair into its components, which allow the LLCCM
to more easily learn, for example, that a constituent
is unlikely to immediately follow a determiner.
3 Training
In the EM algorithm for estimating CCM parame-
ters, the E-Step computes posteriors over bracket-
ings using the Inside-Outside algorithm. The M-
Step chooses parameters that maximize the expected
complete log likelihood of the data.
The weights, w, of LLCCM are estimated to max-
imize the data log likelihood of the training sen-
tences {sn}, summing out all possible bracketings
B for each sentence:
L(w) =
?
sn
log
?
B
Pw(sn, B)
We optimize this objective via L-BFGS (Liu and
Nocedal, 1989), which requires us to compute the
objective gradient. Berg-Kirkpatrick et al (2010)
showed that the data log likelihood gradient is equiv-
alent to the gradient of the expected complete log
likelihood (the objective maximized in the M-step of
EM) at the point from which expectations are com-
puted. This gradient can be computed in three steps.
First, we compute the local probabilities of the
CCM, Pt(y|x), from the current w using Equa-
tion (1). We approximate the normalization over an
exponential number of terms by only summing over
spans that appeared in the training corpus.
Second, we compute posteriors over bracketings,
P(i, j|sn), just as in the E-step of CCM training,2 in
order to determine the expected counts:
exy,SPAN =
?
sn
?
ij
I [?(i, j, sn) = y] ?(x)
exy,CONTEXT =
?
sn
?
ij
I [?(i, j, sn) = y] ?(x)
where ?(true) = P(i, j|sn), and ?(false) = 1 ?
?(true).
We summarize these expected count quantities as:
exyt =
{
exy,SPAN if t = SPAN
exy,CONTEXT if t = CONTEXT
Finally, we compute the gradient with respect to
w, expressed in terms of these expected counts and
conditional probabilities:
?L(w) =
?
xyt
exytfxyt ?G(w)
G(w) =
?
xt
(
?
y
exyt
)
?
y?
Pt(y|x)fxy?t
Following (Klein and Manning, 2002), we initialize
the model weights by optimizing against posterior
probabilities fixed to the split-uniform distribution,
which generates binary trees by randomly choosing
a split point and recursing on each side of the split.3
2We follow the dynamic program presented in Appendix A.1
of (Klein, 2005).
3In Appendix B.2, Klein (2005) shows this posterior can be
expressed in closed form. As in previous work, we start the ini-
tialization optimization with the zero vector, and terminate after
10 iterations to regularize against achieving a local maximum.
19
3.1 Efficiently Computing the Gradient
The following quantity appears in G(w):
?t(x) =
?
y
exyt
Which expands as follows depending on t:
?SPAN(x) =
?
y
?
sn
?
ij
I [?(i, j, sn) = y] ?(x)
?CONTEXT(x) =
?
y
?
sn
?
ij
I [?(i, j, sn) = y] ?(x)
In each of these expressions, the ?(x) term can
be factored outside the sum over y. Each fixed
(i, j) and sn pair has exactly one span and con-
text, hence the quantities
?
y I [?(i, j, sn) = y] and?
y I [?(i, j, sn) = y] are both equal to 1.
?t(x) =
?
sn
?
ij
?(x)
This expression further simplifies to a constant.
The sum of the posterior probabilities, ?(true), over
all positions is equal to the total number of con-
stituents in the tree. Any binary tree over N ter-
minals contains exactly 2N ? 1 constituents and
1
2(N ? 2)(N ? 1) distituents.
?t(x) =
{?
sn (2|sn| ? 1) if x = true
1
2
?
sn(|sn| ? 2)(|sn| ? 1) if x = false
where |sn| denotes the length of sentence sn.
Thus, G(w) can be precomputed once for the en-
tire dataset at each minimization step. Moreover,
?t(x) can be precomputed once before all iterations.
3.2 Relationship to Smoothing
The original CCM uses additive smoothing in its M-
step to capture the fact that distituents outnumber
constituents. For each span or context, CCM adds
10 counts: 2 as a constituent and 8 as a distituent.4
We note that these smoothing parameters are tai-
lored to short sentences: in a binary tree, the number
of constituents grows linearly with sentence length,
whereas the number of distituents grows quadrati-
cally. Therefore, the ratio of constituents to dis-
tituents is not constant across sentence lengths. In
contrast, by virtue of the log-linear model, LLCCM
assigns positive probability to all spans or contexts
without explicit smoothing.
4These counts are specified in (Klein, 2005); Klein and
Manning (2002) added 10 constituent and 50 distituent counts.
Length
Baseline 
CCM
LLCCM Right 
branching
Upper 
bound
Initialization
10
15
20
25
30
35
40
71.9 72.0 61.7 88.1 49.8
53.0 64.6 53.1 86.8 39.8
46.6 60.0 48.2 86.3 34.2
42.7 56.2 44.9 85.9 30.6
39.9 50.3 42.6 85.7 28.5
37.5 49.2 41.3 85.6 27.3
33.7 47.6 40.5 85.5 26.8
0
25
50
75
100
10 15 20 25 30 35 40
72.0
64.6
60.0
56.2
50.3
49.2
47.6
71.9
53.0
46.6
42.7
39.9
37.5
33.7
Binary branching upper bound
Log-linear CCM
Standard CCM
Right branching
Maximum sentence length
B
r
a
c
k
e
t
i
n
g
 
F
1
Figure 1: CCM and LLCCM trained and tested on sentences of
a fixed length. LLCCM performs well on longer sentences. The
binary branching upper bound correponds to UBOUND from
(Klein and Manning, 2002).
4 Experiments
We train our models on gold POS sequences from
all sections (0-24) of the WSJ (Marcus et al, 1993)
with punctuation removed. We report bracketing
F1 scores between the binary trees predicted by the
models on these sequences and the treebank parses.
We train and evaluate both a CCM implementa-
tion (Luque, 2011) and our LLCCM on sentences up
to a fixed length n, for n ? {10, 15, . . . , 40}. Fig-
ure 1 shows that LLCCM substantially outperforms
the CCM on longer sentences. After length 15,
CCM accuracy falls below the right branching base-
line, whereas LLCCM remains significantly better
than right-branching through length 40.
5 Conclusion
Our log-linear variant of the CCM extends robustly
to long sentences, enabling constituent grammar in-
duction to be used in settings that typically include
long sentences, such as machine translation reorder-
ing (Chiang, 2005; DeNero and Uszkoreit, 2011;
Dyer et al, 2011).
Acknowledgments
We thank Taylor Berg-Kirkpatrick and Dan Klein
for helpful discussions regarding the work on which
this paper is based. This work was partially sup-
ported by the National Science Foundation through
a Graduate Research Fellowship to the first author.
20
References
Taylor Berg-Kirkpatrick and Dan Klein. 2010. Phyloge-
netic grammar induction. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics, pages 1288?1297, Uppsala, Sweden, July.
Association for Computational Linguistics.
Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,
John DeNero, and Dan Klein. 2010. Painless unsu-
pervised learning with features. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 582?590, Los Angeles,
California, June. Association for Computational Lin-
guistics.
Rens Bod. 2006. Unsupervised parsing with U-DOP.
In Proceedings of the Conference on Computational
Natural Language Learning.
Glenn Carroll and Eugene Charniak. 1992. Two experi-
ments on learning probabilistic dependency grammars
from corpora. In Workshop Notes for Statistically-
Based NLP Techniques, AAAI, pages 1?13.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Shay B. Cohen and Noah A. Smith. 2009. Shared logis-
tic normal distributions for soft parameter tying in un-
supervised grammar induction. In Proceedings of Hu-
man Language Technologies: The 2009 Annual Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics, pages 74?82,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Shay B. Cohen, Dipanjan Das, and Noah A. Smith. 2011.
Unsupervised structure prediction with non-parallel
multilingual guidance. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 50?61, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Arthur Dempster, Nan Laird, and Donald Rubin. 1977.
Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society. Se-
ries B (Methodological), 39(1):1?38.
John DeNero and Jakob Uszkoreit. 2011. Inducing sen-
tence structure from parallel corpora for reordering.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pages 193?
203, Edinburgh, Scotland, UK., July. Association for
Computational Linguistics.
Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and
Noah A. Smith. 2011. The CMU-ARK German-
English translation system. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
337?343, Edinburgh, Scotland, July. Association for
Computational Linguistics.
William P. Headden III, Mark Johnson, and David Mc-
Closky. 2009. Improving unsupervised dependency
parsing with richer contexts and smoothing. In Pro-
ceedings of Human Language Technologies: The 2009
Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages
101?109, Boulder, Colorado, June. Association for
Computational Linguistics.
Dan Klein and Christopher D. Manning. 2002. A gener-
ative constituent-context model for improved grammar
induction. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
128?135, Philadelphia, Pennsylvania, USA, July. As-
sociation for Computational Linguistics.
Dan Klein and Christopher D. Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In Proceedings of the
42nd Meeting of the Association for Computational
Linguistics, Main Volume, pages 478?485, Barcelona,
Spain, July.
Dan Klein. 2005. The Unsupervised Learning of Natural
Language Structure. Ph.D. thesis.
Karim Lari and Steve J. Young. 1990. The estimation
of stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35?56.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory method for large scale optimization. Mathe-
matical Programming B, 45(3):503?528.
Franco Luque. 2011. Una implementacio?n del mod-
elo DMV+CCM para parsing no supervisado. In 2do
Workshop Argentino en Procesamiento de Lenguaje
Natural.
Mitchell P. Marcus, Beatrice Santorini, and Mary A.
Marcinkiewicz. 1993. Building a Large Annotated
Corpus of English: The Penn Treebank. Computa-
tional Linguistics, 19(2):313?330.
Tahira Naseem and Regina Barzilay. 2011. Using se-
mantic cues to learn syntax. In AAAI.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1234?1244, Cambridge,
MA, October. Association for Computational Linguis-
tics.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
21
In Proceedings of the 30th Annual Meeting of the As-
sociation for Computational Linguistics, pages 128?
135, Newark, Delaware, USA, June. Association for
Computational Linguistics.
Elias Ponvert, Jason Baldridge, and Katrin Erk. 2011.
Simple unsupervised grammar induction from raw text
with cascaded finite state models. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 1077?1086, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Roi Reichart and Ari Rappoport. 2010. Improved fully
unsupervised parsing with zoomed learning. In Pro-
ceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 684?693,
Cambridge, MA, October. Association for Computa-
tional Linguistics.
Yoav Seginer. 2007. Fast unsupervised incremental pars-
ing. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 384?
391, Prague, Czech Republic, June. Association for
Computational Linguistics.
22
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1481?1490,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
A Constrained Viterbi Relaxation for Bidirectional Word Alignment
Yin-Wen Chang Alexander M. Rush
MIT CSAIL,
Cambridge, MA 02139
{yinwen,srush}@
csail.mit.edu
John DeNero
UC Berkeley,
Berkeley, CA 94720
denero@
cs.berkeley.edu
Michael Collins
Columbia University,
New York, NY 10027
mcollins@
cs.columbia.edu
Abstract
Bidirectional models of word alignment
are an appealing alternative to post-hoc
combinations of directional word align-
ers. Unfortunately, most bidirectional
formulations are NP-Hard to solve, and
a previous attempt to use a relaxation-
based decoder yielded few exact solu-
tions (6%). We present a novel relax-
ation for decoding the bidirectional model
of DeNero and Macherey (2011). The
relaxation can be solved with a mod-
ified version of the Viterbi algorithm.
To find optimal solutions on difficult
instances, we alternate between incre-
mentally adding constraints and applying
optimality-preserving coarse-to-fine prun-
ing. The algorithm finds provably ex-
act solutions on 86% of sentence pairs
and shows improvements over directional
models.
1 Introduction
Word alignment is a critical first step for build-
ing statistical machine translation systems. In or-
der to ensure accurate word alignments, most sys-
tems employ a post-hoc symmetrization step to
combine directional word aligners, such as IBM
Model 4 (Brown et al, 1993) or hidden Markov
model (HMM) based aligners (Vogel et al, 1996).
Several authors have proposed bidirectional mod-
els that incorporate this step directly, but decoding
under many bidirectional models is NP-Hard and
finding exact solutions has proven difficult.
In this paper, we describe a novel Lagrangian-
relaxation based decoder for the bidirectional
model proposed by DeNero and Macherey (2011),
with the goal of improving search accuracy.
In that work, the authors implement a dual
decomposition-based decoder for the problem, but
are only able to find exact solutions for around 6%
of instances.
Our decoder uses a simple variant of the Viterbi
algorithm for solving a relaxed version of this
model. The algorithm makes it easy to re-
introduce constraints for difficult instances, at the
cost of increasing run-time complexity. To offset
this cost, we employ optimality-preserving coarse-
to-fine pruning to reduce the search space. The
pruning method utilizes lower bounds on the cost
of valid bidirectional alignments, which we obtain
from a fast, greedy decoder.
The method has the following properties:
? It is based on a novel relaxation for the model
of DeNero and Macherey (2011), solvable
with a variant of the Viterbi algorithm.
? To find optimal solutions, it employs an effi-
cient strategy that alternates between adding
constraints and applying pruning.
? Empirically, it is able to find exact solutions
on 86% of sentence pairs and is significantly
faster than general-purpose solvers.
We begin in Section 2 by formally describing
the directional word alignment problem. Section 3
describes a preliminary bidirectional model us-
ing full agreement constraints and a Lagrangian
relaxation-based solver. Section 4 modifies this
model to include adjacency constraints. Section 5
describes an extension to the relaxed algorithm to
explicitly enforce constraints, and Section 6 gives
a pruning method for improving the efficiency of
the algorithm.
Experiments compare the search error and accu-
racy of the new bidirectional algorithm to several
directional combiners and other bidirectional al-
gorithms. Results show that the new relaxation is
much more effective at finding exact solutions and
is able to produce comparable alignment accuracy.
1481
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
Figure 1: An example e?f directional alignment for the sen-
tences let us see the documents and montrez -
nous les documents, with I = 5 and J = 5. The in-
dices i ? [I]
0
are rows, and the indices j ? [J ]
0
are columns.
The HMM alignment shown has transitions x(0, 1, 1) =
x(1, 2, 3) = x(3, 3, 1) = x(1, 4, 4) = x(4, 5, 5) = 1.
Notation We use lower- and upper-case letters
for scalars and vectors, and script-case for sets
e.g. X . For vectors, such as v ? {0, 1}
(I?J )?J
,
where I and J are finite sets, we use the notation
v(i, j) and v(j) to represent elements of the vec-
tor. Define d = ?(i) to be the indicator vector with
d(i) = 1 and d(i
?
) = 0 for all i
?
6= i. Finally de-
fine the notation [J ] to refer to {1 . . . J} and [J ]
0
to refer to {0 . . . J}.
2 Background
The focus of this work is on the word alignment
decoding problem. Given a sentence e of length
|e| = I and a sentence f of length |f | = J , our
goal is to find the best bidirectional alignment be-
tween the two sentences under a given objective
function. Before turning to the model of interest,
we first introduce directional word alignment.
2.1 Word Alignment
In the e?f word alignment problem, each word
in e is aligned to a word in f or to the null word .
This alignment is a mapping from each index i ?
[I] to an index j ? [J ]
0
(where j = 0 represents
alignment to ). We refer to a single word align-
ment as a link.
A first-order HMM alignment model (Vogel et
al., 1996) is an HMM of length I + 1 where the
hidden state at position i ? [I]
0
is the aligned in-
dex j ? [J ]
0
, and the transition score takes into
account the previously aligned index j
?
? [J ]
0
.
1
Formally, define the set of possible HMM align-
ments as X ? {0, 1}
([I]
0
?[J ]
0
)?([I]?[J ]
0
?[J ]
0
)
with
1
Our definition differs slightly from other HMM-based
aligners in that it does not track the last  alignment.
X =
?
?
?
?
?
?
?
?
?
x : x(0, 0) = 1,
x(i, j) =
J?
j
?
=0
x(j
?
, i, j) ?i ? [I], j ? [J ]
0
,
x(i, j) =
J?
j
?
=0
x(j, i+ 1, j
?
) ?i ? [I ? 1]
0
, j ? [J ]
0
where x(i, j) = 1 indicates that there is a link
between index i and index j, and x(j
?
, i, j) = 1
indicates that index i ? 1 aligns to index j
?
and
index i aligns to j. Figure 1 shows an example
member of X .
The constraints of X enforce backward and for-
ward consistency respectively. If x(i, j) = 1,
backward consistency enforces that there is a tran-
sition from (i? 1, j
?
) to (i, j) for some j
?
? [J ]
0
,
whereas forward consistency enforces a transition
from (i, j) to (i+ 1, j
?
) for some j
?
? [J ]
0
. Infor-
mally the constraints ?chain? together the links.
The HMM objective function f : X ? R can
be written as a linear function of x
f(x; ?) =
I
?
i=1
J
?
j=0
J
?
j
?
=0
?(j
?
, i, j)x(j
?
, i, j)
where the vector ? ? R
[I]?[J ]
0
?[J ]
0
includes the
transition and alignment scores. For a generative
model of alignment, we might define ?(j
?
, i, j) =
log(p(e
i
|f
j
)p(j|j
?
)). For a discriminative model
of alignment, we might define ?(j
?
, i, j) = w ?
?(i, j
?
, j, f , e) for a feature function ? and weights
w (Moore, 2005; Lacoste-Julien et al, 2006).
Now reverse the direction of the model and
consider the f?e alignment problem. An f?e
alignment is a binary vector y ? Y where for
each j ? [J ], y(i, j) = 1 for exactly one i ?
[I]
0
. Define the set of HMM alignments Y ?
{0, 1}
([I]
0
?[J ]
0
)?([I]
0
?[I]
0
?[J ])
as
Y =
?
?
?
?
?
?
?
?
?
y : y(0, 0) = 1,
y(i, j) =
I?
i
?
=0
y(i
?
, i, j) ?i ? [I]
0
, j ? [J ],
y(i, j) =
I?
i
?
=0
y(i, i
?
, j + 1) ?i ? [I]
0
, j ? [J ? 1]
0
Similarly define the objective function
g(y;?) =
J
?
j=1
I
?
i=0
I
?
i
?
=0
?(i
?
, i, j)y(i
?
, i, j)
with vector ? ? R
[I]
0
?[I]
0
?[J ]
.
1482
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(a)
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(b)
Figure 2: (a) An example alignment pair (x, y) satisfying the
full agreement conditions. The x alignment is represented
with circles and the y alignment with triangles. (b) An exam-
ple f?e alignment y ? Y
?
with relaxed forward constraints.
Note that unlike an alignment from Y multiple words may
be aligned in a column and words may transition from non-
aligned positions.
Note that for both of these models we can solve
the optimization problem exactly using the stan-
dard Viterbi algorithm for HMM decoding. The
first can be solved in O(IJ
2
) time and the second
in O(I
2
J) time.
3 Bidirectional Alignment
The directional bias of the e?f and f?e align-
ment models may cause them to produce differing
alignments. To obtain the best single alignment,
it is common practice to use a post-hoc algorithm
to merge these directional alignments (Och et al,
1999). First, a directional alignment is found from
each word in e to a word f . Next an alignment is
produced in the reverse direction from f to e. Fi-
nally, these alignments are merged, either through
intersection, union, or with an interpolation algo-
rithm such as grow-diag-final (Koehn et al, 2003).
In this work, we instead consider a bidirectional
alignment model that jointly considers both direc-
tional models. We begin in this section by in-
troducing a simple bidirectional model that en-
forces full agreement between directional models
and giving a relaxation for decoding. Section 4
loosens this model to adjacent agreement.
3.1 Enforcing Full Agreement
Perhaps the simplest post-hoc merging strategy is
to retain the intersection of the two directional
models. The analogous bidirectional model en-
forces full agreement to ensure the two alignments
select the same non-null links i.e.
x
?
, y
?
= argmax
x?X ,y?Y
f(x) + g(y) s.t.
x(i, j) = y(i, j) ?i ? [I], j ? [J ]
We refer to the optimal alignments for this prob-
lem as x
?
and y
?
.
Unfortunately this bidirectional decoding
model is NP-Hard (a proof is given in Ap-
pendix A). As it is common for alignment pairs to
have |f | or |e| over 40, exact decoding algorithms
are intractable in the worst-case.
Instead we will use Lagrangian relaxation for
this model. At a high level, we will remove a
subset of the constraints from the original problem
and replace them with Lagrange multipliers. If we
can solve this new problem efficiently, we may be
able to get optimal solutions to the original prob-
lem. (See the tutorial by Rush and Collins (2012)
describing the method.)
There are many possible subsets of constraints
to consider relaxing. The relaxation we use pre-
serves the agreement constraints while relaxing
the Markov structure of the f?e alignment. This
relaxation will make it simple to later re-introduce
constraints in Section 5.
We relax the forward constraints of set Y . With-
out these constraints the y links are no longer
chained together. This has two consequences: (1)
for index j there may be any number of indices i,
such that y(i, j) = 1, (2) if y(i
?
, i, j) = 1 it is no
longer required that y(i
?
, j ? 1) = 1. This gives a
set Y
?
which is a superset of Y
Y
?
=
{
y : y(0, 0) = 1,
y(i, j) =
?
I
i
?
=0
y(i
?
, i, j) ?i ? [I]
0
, j ? [J ]
Figure 2(b) shows a possible y ? Y
?
and a valid
unchained structure.
To form the Lagrangian dual with relaxed for-
ward constraints, we introduce a vector of La-
grange multipliers, ? ? R
[I?1]
0
?[J ]
0
, with one
multiplier for each original constraint. The La-
grangian dual L(?) is defined as
max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) +
I?
i=1
J?
j=0
I?
i
?
=0
y(i
?
, i, j)?(i
?
, i, j) (1)
?
I?
i=0
J?1?
j=0
?(i, j)
(
y(i, j)?
I?
i
?
=0
y(i, i
?
, j + 1)
)
= max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) +
I?
i=1
J?
j=0
I?
i
?
=0
y(i
?
, i, j)?
?
(i
?
, i, j)(2)
= max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) +
I?
i=1
J?
j=0
y(i, j) max
i
?
?[I]
0
?
?
(i
?
, i, j)(3)
= max
x?X ,y?Y
?
,
x(i,j)=y(i,j)
f(x) + g
?
(y;?, ?) (4)
1483
Line 2 distributes the ??s and introduces a modi-
fied potential vector ?
?
defined as
?
?
(i
?
, i, j) = ?(i
?
, i, j)? ?(i, j) + ?(i
?
, j ? 1)
for all i
?
? [I]
0
, i ? [I]
0
, j ? [J ]. Line 3 uti-
lizes the relaxed set Y
?
which allows each y(i, j)
to select the best possible previous link (i
?
, j ? 1).
Line 4 introduces the modified directional objec-
tive
g
?
(y;?, ?) =
I
?
i=1
J
?
j=0
y(i, j) max
i
?
?[I]
0
?
?
(i
?
, i, j)
The Lagrangian dual is guaranteed to be an up-
per bound on the optimal solution, i.e. for all ?,
L(?) ? f(x
?
) + g(y
?
). Lagrangian relaxation
attempts to find the tighest possible upper bound
by minimizing the Lagrangian dual, min
?
L(?),
using subgradient descent. Briefly, subgradient
descent is an iterative algorithm, with two steps.
Starting with ? = 0, we iteratively
1. Set (x, y) to the argmax of L(?).
2. Update ?(i, j) for all i ? [I ? 1]
0
, j ? [J ]
0
,
?(i, j)? ?(i, j)? ?
t
(
y(i, j)?
I?
i
?
=0
y(i, i
?
, j + 1)
)
.
where ?
t
> 0 is a step size for the t?th update. If
at any iteration of the algorithm the forward con-
straints are satisfied for (x, y), then f(x)+g(y) =
f(x
?
) + g(x
?
) and we say this gives a certificate
of optimality for the underlying problem.
To run this algorithm, we need to be able to effi-
ciently compute the (x, y) pair that is the argmax
of L(?) for any value of ?. Fortunately, since the y
alignments are no longer constrained to valid tran-
sitions, we can compute these alignments by first
picking the best f?e transitions for each possible
link, and then running an e?f Viterbi-style algo-
rithm to find the bidirectional alignment.
The max version of this algorithm is shown in
Figure 3. It consists of two steps. We first compute
the score for each y(i, j) variable. We then use the
standard Viterbi update for computing the x vari-
ables, adding in the score of the y(i, j) necessary
to satisfy the constraints.
procedure VITERBIFULL(?, ?
?
)
Let pi, ? be dynamic programming charts.
?[i, j]? max
i
?
?[I]
0
?
?
(i
?
, i, j) ? i ? [I], j ? [J ]
0
pi[0, 0]?
?
J
j=1
max{0, ?[0, j]}
for i ? [I], j ? [J ]
0
in order do
pi[i, j]? max
j
?
?[J]
0
?(j
?
, i, j) + pi[i? 1, j
?
]
if j 6= 0 then pi[i, j]? pi[i, j] + ?[i, j]
return max
j?[J]
0
pi[I, j]
Figure 3: Viterbi-style algorithm for computing L(?). For
simplicity the algorithm shows the max version of the algo-
rithm, argmax can be computed with back-pointers.
4 Adjacent Agreement
Enforcing full agreement can be too strict an align-
ment criteria. DeNero and Macherey (2011) in-
stead propose a model that allows near matches,
which we call adjacent agreement. Adjacent
agreement allows links from one direction to agree
with adjacent links from the reverse alignment for
a small penalty. Figure 4(a) shows an example
of a valid bidirectional alignment under adjacent
agreement.
In this section we formally introduce adjacent
agreement, and propose a relaxation algorithm for
this model. The key algorithmic idea is to extend
the Viterbi algorithm in order to consider possible
adjacent links in the reverse direction.
4.1 Enforcing Adjacency
Define the adjacency set K = {?1, 0, 1}. A bidi-
rectional alignment satisfies adjacency if for all
i ? [I], j ? [J ],
? If x(i, j) = 1, it is required that y(i+k, j) =
1 for exactly one k ? K (i.e. either above,
center, or below). We indicate which position
with variables z
l
i,j
? {0, 1}
K
? If x(i, j) = 1, it is allowed that y(i, j + k) =
1 for any k ? K (i.e. either left, center, or
right) and all other y(i, j
?
) = 0. We indicate
which positions with variables z
?
i,j
? {0, 1}
K
Formally for x ? X and y ? Y , the pair (x, y) is
feasible if there exists a z from the set Z(x, y) ?
{0, 1}
K
2
?[I]?[J ]
defined as
Z(x, y) =
?
?
?
?
?
?
?
?
?
z : ?i ? [I], j ? [J ]
z
l
i,j
? {0, 1}
K
, z
?
i,j
? {0, 1}
K
x(i, j) =
?
k?K
z
l
i,j
(k),
?
k?K
z
?
i,j
(k) = y(i, j),
z
l
i,j
(k) ? y(i+ k, j) ?k ? K : i+ k > 0,
x(i, j) ? z
?
i,j?k
(k) ?k ? K : j + k > 0
1484
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(a)
 m
o
n
t
r
e
z
- n
o
u
s
l
e
s
d
o
c
u
m
e
n
t
s

let
us
see
the
documents
(b)
Figure 4: (a) An alignment satisfying the adjacency con-
straints. Note that x(2, 1) = 1 is allowed because of
y(1, 1) = 1, x(4, 3) = 1 because of y(3, 3), and y(3, 1)
because of x(3, 2). (b) An adjacent bidirectional alignment
in progress. Currently x(2, 2) = 1 with z
l
(?1) = 1 and
z
?
(?1) = 1. The last transition was from x(1, 3) with
z
??
(?1) = 1, z
??
(0) = 1, z
l?
(0) = 1.
Additionally adjacent, non-overlapping
matches are assessed a penalty ? calculated as
h(z) =
I
?
i=1
J
?
j=1
?
k?K
?|k|(z
l
i,j
(k) + z
?
i,j
(k))
where ? ? 0 is a parameter of the model. The
example in Figure 4(a) includes a 3? penalty.
Adding these penalties gives the complete adja-
cent agreement problem
argmax
z?Z(x,y)
x?X ,y?Y
f(x) + g(y) + h(z)
Next, apply the same relaxation from Sec-
tion 3.1, i.e. we relax the forward constraints of
the f?e set. This yields the following Lagrangian
dual
L(?) = max
z?Z(x,y)
x?X ,y?Y
?
f(x) + g
?
(y;?, ?) + h(z)
Despite the new constraints, we can still com-
pute L(?) in O(IJ(I + J)) time using a variant
of the Viterbi algorithm. The main idea will be to
consider possible adjacent settings for each link.
Since each z
l
i,j
and z
?
i,j
only have a constant num-
ber of settings, this does not increase the asymp-
totic complexity of the algorithm.
Figure 5 shows the algorithm for computing
L(?). The main loop of the algorithm is similar to
Figure 3. It proceeds row-by-row, picking the best
alignment x(i, j) = 1. The major change is that
the chart pi also stores a value z ? {0, 1}
K?K
rep-
resenting a possible z
l
i,j
, z
?
i,j
pair. Since we have
procedure VITERBIADJ(?, ?
?
)
?[i, j]? max
i
?
?[I]
0
?
?
(i
?
, i, j) ? i ? [I], j ? [J ]
0
pi[0, 0]?
?
J
j=1
max{0, ?[0, j]}
for i ? [I], j ? [J ]
0
, z
l
, z
?
? {0, 1}
|K|
do
pi[i, j, z]?
max
j
?
?[J]
0
,
z
?
?N (z,j?j
?
)
?(j
?
, i, j) + pi[i? 1, j
?
, z
?
]
+
?
k?K
z
?
(k)(?[i, j + k] + ?|k|)
+z
l
(k)?|k|
return max
j?[J]
0
,z?{0,1}
|K?K|
pi[I, j, z]
Figure 5: Modified Viterbi algorithm for computing the adja-
cent agreement L(?).
the proposed z
i,j
in the inner loop, we can include
the scores of the adjacent y alignments that are
in neighboring columns, as well as the possible
penalty for matching x(i, j) to a y(i + k, j) in a
different row. Figure 4(b) gives an example set-
ting of z.
In the dynamic program, we need to ensure that
the transitions between the z?s are consistent. The
vector z
?
indicates the y links adjacent to x(i ?
1, j
?
). If j
?
is near to j, z
?
may overlap with z
and vice-versa. The transition setN ensures these
indicators match up
N (z, k
?
) =
?
?
?
z
?
: (z
l
(?1) ? k
?
? K)? z
??
(k
?
),
(z
l?
(1) ? k
?
? K)? z
?
(?k
?
),
?
k?K
z
l
(k) = 1
5 Adding Back Constraints
In general, it can be shown that Lagrangian relax-
ation is only guaranteed to solve a linear program-
ming relaxation of the underlying combinatorial
problem. For difficult instances, we will see that
this relaxation often does not yield provably exact
solutions. However, it is possible to ?tighten? the
relaxation by re-introducing constraints from the
original problem.
In this section, we extend the algorithm to al-
low incrementally re-introducing constraints. In
particular we track which constraints are most of-
ten violated in order to explicitly enforce them in
the algorithm.
Define a binary vector p ? {0, 1}
[I?1]
0
?[J ]
0
where p(i, j) = 1 indicates a previously re-
laxed constraint on link y(i, j) that should be re-
introduced into the problem. Let the new partially
1485
constrained Lagrangian dual be defined as
L(?; p) = max
z?Z(x,y)
x?X ,y?Y
?
f(x) + g
?
(y;?, ?) + h(z)
y(i, j) =
?
i
?
y(i, i
?
, j + 1) ?i, j : p(i, j) = 1
If p =
~
1, the problem includes all of the original
constraints, whereas p =
~
0 gives our original La-
grangian dual. In between we have progressively
more constrained variants.
In order to compute the argmax of this op-
timization problem, we need to satisfy the con-
straints within the Viterbi algorithm. We augment
the Viterbi chart with a count vector d ? D where
D ? Z
||p||
1
and d(i, j) is a count for the (i, j)?th
constraint, i.e. d(i, j) = y(i, j) ?
?
i
?
y(i
?
, i, j).
Only solutions with count 0 at the final position
satisfy the active constraints. Additionally de-
fine a helper function [?]
D
as the projection from
Z
[I?1]
0
?[J ]
? D, which truncates dimensions
without constraints.
Figure 6 shows this constrained Viterbi relax-
ation approach. It takes p as an argument and en-
forces the active constraints. For simplicity, we
show the full agreement version, but the adjacent
agreement version is similar. The main new addi-
tion is that the inner loop of the algorithm ensures
that the count vector d is the sum of the counts of
its children d
?
and d? d
?
.
Since each additional constraint adds a dimen-
sion to d, adding constraints has a multiplicative
impact on running time. Asymptotically the new
algorithm requires O(2
||p||
1
IJ(I + J)) time. This
is a problem in practice as even adding a few con-
straints can make the problem intractable. We ad-
dress this issue in the next section.
6 Pruning
Re-introducing constraints can lead to an expo-
nential blow-up in the search space of the Viterbi
algorithm. In practice though, many alignments
in this space are far from optimal, e.g. align-
ing a common word like the to nous instead
of les. Since Lagrangian relaxation re-computes
the alignment many times, it would be preferable
to skip these links in later rounds, particularly after
re-introducing constraints.
In this section we describe an optimality pre-
serving coarse-to-fine algorithm for pruning. Ap-
proximate coarse-to-fine pruning algorithms are
procedure CONSVITERBIFULL(?, ?
?
, p)
for i ? [I], j ? [J ]
0
, i
?
? [I] do
d? |?(i, j)? ?(i
?
, j ? 1)|
D
?[i, j, d]? ?
?
(i
?
, i, j)
for j ? [J ], d ? D do
pi[0, 0, d]? max
d
?
?D
pi[0, 0, d
?
] + ?[0, j, d? d
?
]
for i ? [I], j ? [J ]
0
, d ? D do
if j = 0 then
pi[i, j, d]? max
j
?
?[J]
0
?(j
?
, i, j) + pi[i? 1, j
?
, d]
else
pi[i, j, d]?
max
j
?
?[J]
0
,d
?
?D
?(j
?
, i, j) + pi[i? 1, j
?
, d
?
]
+?[i, j, d? d
?
]
return max
j?[J]
0
pi[I, j,0]
Figure 6: Constrained Viterbi algorithm for finding partially-
constrained, full-agreement alignments. The argument p in-
dicates which constraints to enforce.
widely used within NLP, but exact pruning is
less common. Our method differs in that it
only eliminates non-optimal transitions based on
a lower-bound score. After introducing the prun-
ing method, we present an algorithm to make this
method effective in practice by producing high-
scoring lower bounds for adjacent agreement.
6.1 Thresholding Max-Marginals
Our pruning method is based on removing transi-
tions with low max-marginal values. Define the
max-marginal value of an e?f transition in our
Lagrangian dual as
M(j
?
, i, j;?) = max
z?Z(x,y),
x?X ,y?Y
?
f(x) + g
?
(y;?) + h(z)
s.t. x(j
?
, i, j) = 1
where M gives the value of the best dual align-
ment that transitions from (i ? 1, j
?
) to (i, j).
These max-marginals can be computed by running
a forward-backward variant of any of the algo-
rithms described thus far.
We make the following claim about max-
marginal values and any lower-bound score
Lemma 1 (Safe Pruning). For any valid con-
strained alignment x ? X , y ? Y, z ? Z(x, y)
and for any dual vector ? ? R
[I?1]
0
?[J ]
0
, if there
exists a transition j
?
, i, j with max-marginal value
M(j
?
, i, j;?) < f(x)+g(y)+h(z) then the tran-
sition will not be in the optimal alignment, i.e.
x
?
(j
?
, i, j) = 0.
This lemma tells us that we can prune transi-
tions whose dual max-marginal value falls below
1486
a threshold without pruning possibly optimal tran-
sitions. Pruning these transitions can speed up La-
grangian relaxation without altering its properties.
Furthermore, the threshold is determined by any
feasible lower bound on the optimal score, which
means that better bounds can lead to more pruning.
6.2 Finding Lower Bounds
Since the effectiveness of pruning is dependent on
the lower bound, it is crucial to be able to produce
high-scoring alignments that satisfy the agreement
constraints. Unfortunately, this problem is non-
trivial. For instance, taking the union of direc-
tional alignments does not guarantee a feasible so-
lution; whereas taking the intersection is trivially
feasible but often not high-scoring.
To produce higher-scoring feasible bidirectional
alignments we introduce a greedy heuristic al-
gorithm. The algorithm starts with any feasible
alignment (x, y, z). It runs the following greedy
loop:
1. Repeat until there exists no x(i, 0) = 1 or
y(0, j) = 1, or there is no score increase.
(a) For each i ? [I], j ? [J ]
0
, k ? K :
x(i, 0) = 1, check if x(i, j) ? 1 and
y(i, j + k) ? 1 is feasible, remember
score.
(b) For each i ? [I]
0
, j ? [J ], k ? K :
y(0, j) = 1, check if y(i, j) ? 1 and
x(i + k, j) ? 1 is feasible, remember
score.
(c) Let (x, y, z) be the highest-scoring fea-
sible solution produced.
This algorithm produces feasible alignments with
monotonically increasing score, starting from the
intersection of the alignments. It has run-time of
O(IJ(I + J)) since each inner loop enumerates
IJ possible updates and assigns at least one index
a non-zero value, limiting the outer loop to I + J
iterations.
In practice we initialize the heuristic based on
the intersection of x and y at the current round
of Lagrangian relaxation. Experiments show that
running this algorithm significantly improves the
lower bound compared to just taking the intersec-
tion, and consequently helps pruning significantly.
7 Related Work
The most common techniques for bidirectional
alignment are post-hoc combinations, such as
union or intersection, of directional models, (Och
et al, 1999), or more complex heuristic combiners
such as grow-diag-final (Koehn et al, 2003).
Several authors have explored explicit bidirec-
tional models in the literature. Cromieres and
Kurohashi (2009) use belief propagation on a fac-
tor graph to train and decode a one-to-one word
alignment problem. Qualitatively this method is
similar to ours, although the model and decoding
algorithm are different, and their method is not
able to provide certificates of optimality.
A series of papers by Ganchev et al (2010),
Graca et al (2008), and Ganchev et al (2008) use
posterior regularization to constrain the posterior
probability of the word alignment problem to be
symmetric and bijective. This work acheives state-
of-the-art performance for alignment. Instead of
utilizing posteriors our model tries to decode a sin-
gle best one-to-one word alignment.
A different approach is to use constraints at
training time to obtain models that favor bidi-
rectional properties. Liang et al (2006) propose
agreement-based learning, which jointly learns
probabilities by maximizing a combination of
likelihood and agreement between two directional
models.
General linear programming approaches have
also been applied to word alignment problems.
Lacoste-Julien et al (2006) formulate the word
alignment problem as quadratic assignment prob-
lem and solve it using an integer linear program-
ming solver.
Our work is most similar to DeNero and
Macherey (2011), which uses dual decomposition
to encourage agreement between two directional
HMM aligners during decoding time.
8 Experiments
Our experimental results compare the accuracy
and optimality of our decoding algorithm to direc-
tional alignment models and previous work on this
bidirectional model.
Data and Setup The experimental setup is iden-
tical to DeNero and Macherey (2011). Evalu-
ation is performed on a hand-aligned subset of
the NIST 2002 Chinese-English dataset (Ayan and
Dorr, 2006). Following past work, the first 150
sentence pairs of the training section are used for
evaluation. The potential parameters ? and ? are
set based on unsupervised HMM models trained
on the LDC FBIS corpus (6.2 million words).
1487
1-20 (28%) 21-40 (45%) 41-60 (27%) all
time cert exact time cert exact time cert exact time cert exact
ILP 15.12 100.0 100.0 364.94 100.0 100.0 2,829.64 100.0 100.0 924.24 100.0 100.0
LR 0.55 97.6 97.6 4.76 55.9 55.9 15.06 7.5 7.5 6.33 54.7 54.7
CONS 0.43 100.0 100.0 9.86 95.6 95.6 61.86 55.0 62.5 21.08 86.0 88.0
D&M - 6.2 - - 0.0 - - 0.0 - - 6.2 -
Table 1: Experimental results for model accuracy of bilingual alignment. Column time is the mean time per sentence pair in
seconds; cert is the percentage of sentence pairs solved with a certificate of optimality; exact is the percentage of sentence pairs
solved exactly. Results are grouped by sentence length. The percentage of sentence pairs in each group is shown in parentheses.
Training is performed using the agreement-based
learning method which encourages the directional
models to overlap (Liang et al, 2006). This direc-
tional model has been shown produce state-of-the-
art results with this setup (Haghighi et al, 2009).
Baselines We compare the algorithm described
in this paper with several baseline methods. DIR
includes post-hoc combinations of the e?f and
f?e HMM-based aligners. Variants include
union, intersection, and grow-diag-final. D&M is
the dual decomposition algorithm for bidirectional
alignment as presented by DeNero and Macherey
(2011) with different final combinations. LR is the
Lagrangian relaxation algorithm applied to the ad-
jacent agreement problem without the additional
constraints described in Section 5. CONS is our
full Lagrangian relaxation algorithm including in-
cremental constraint addition. ILP uses a highly-
optimized general-purpose integer linear program-
ming solver to solve the lattice with the constraints
described (Gurobi Optimization, 2013).
Implementation The main task of the decoder
is to repeatedly compute the argmax of L(?).
To speed up decoding, our implementation fully
instantiates the Viterbi lattice for a problem in-
stance. This approach has several benefits: each
iteration can reuse the same lattice structure; max-
marginals can be easily computed with a gen-
eral forward-backward algorithm; pruning corre-
sponds to removing lattice edges; and adding con-
straints can be done through lattice intersection.
For consistency, we implement each baseline (ex-
cept for D&M) through the same lattice.
Parameter Settings We run 400 iterations of
the subgradient algorithm using the rate schedule
?
t
= 0.95
t
?
where t
?
is the count of updates for
which the dual value did not improve. Every 10
iterations we run the greedy decoder to compute
a lower bound. If the gap between our current
dual value L(?) and the lower bound improves
significantly we run coarse-to-fine pruning as de-
scribed in Section 6 with the best lower bound. For
Model Combiner
alignment phrase pair
Prec Rec AER Prec Rec F1
DIR
union 57.6 80.0 33.4 75.1 33.5 46.3
intersection 86.2 62.9 27.0 64.3 43.5 51.9
grow-diag 59.7 79.5 32.1 70.1 36.9 48.4
D&M
union 63.3 81.5 29.1 63.2 44.9 52.5
intersection 77.5 75.1 23.6 57.1 53.6 55.3
grow-diag 65.6 80.6 28.0 60.2 47.4 53.0
CONS 72.5 74.9 26.4 53.0 52.4 52.7
Table 2: Alignment accuracy and phrase pair extraction ac-
curacy for directional and bidirectional models. Prec is the
precision. Rec is the recall. AER is alignment error rate and
F1 is the phrase pair extraction F1 score.
CONS, if the algorithm does not find an optimal
solution we run 400 more iterations and incremen-
tally add the 5 most violated constraints every 25
iterations.
Results Our first set of experiments looks at the
model accuracy and the decoding time of various
methods that can produce optimal solutions. Re-
sults are shown in Table 1. D&M is only able to
find the optimal solution with certificate on 6% of
instances. The relaxation algorithm used in this
work is able to increase that number to 54.7%.
With incremental constraints and pruning, we are
able to solve over 86% of sentence pairs includ-
ing many longer and more difficult pairs. Addi-
tionally the method finds these solutions with only
a small increase in running time over Lagrangian
relaxation, and is significantly faster than using an
ILP solver.
Next we compare the models in terms of align-
ment accuracy. Table 2 shows the precision, recall
and alignment error rate (AER) for word align-
ment. We consider union, intersection and grow-
diag-final as combination procedures. The com-
bination procedures are applied to D&M in the
case when the algorithm does not converge. For
CONS, we use the optimal solution for the 86%
of instances that converge and the highest-scoring
greedy solution for those that do not. The pro-
posed method has an AER of 26.4, which outper-
forms each of the directional models. However,
although CONS achieves a higher model score
than D&M, it performs worse in accuracy. Ta-
1488
1-20 21-40 41-60 all
# cons. 20.0 32.1 39.5 35.9
Table 3: The average number of constraints added for sen-
tence pairs where Lagrangian relaxation is not able to find an
exact solution.
ble 2 also compares the models in terms of phrase-
extraction accuracy (Ayan and Dorr, 2006). We
use the phrase extraction algorithm described by
DeNero and Klein (2010), accounting for possi-
ble links and  alignments. CONS performs bet-
ter than each of the directional models, but worse
than the best D&M model.
Finally we consider the impact of constraint ad-
dition, pruning, and use of a lower bound. Table 3
gives the average number of constraints added for
sentence pairs for which Lagrangian relaxation
alone does not produce a certificate. Figure 7(a)
shows the average over all sentence pairs of the
best dual and best primal scores. The graph com-
pares the use of the greedy algorithm from Sec-
tion 6.2 with the simple intersection of x and y.
The difference between these curves illustrates the
benefit of the greedy algorithm. This is reflected
in Figure 7(b) which shows the effectiveness of
coarse-to-fine pruning over time. On average, the
pruning reduces the search space of each sentence
pair to 20% of the initial search space after 200
iterations.
9 Conclusion
We have introduced a novel Lagrangian relaxation
algorithm for a bidirectional alignment model that
uses incremental constraint addition and coarse-
to-fine pruning to find exact solutions. The algo-
rithm increases the number of exact solution found
on the model of DeNero and Macherey (2011)
from 6% to 86%.
Unfortunately despite achieving higher model
score, this approach does not produce more accu-
rate alignments than the previous algorithm. This
suggests that the adjacent agreement model may
still be too constrained for this underlying task.
Implicitly, an approach with fewer exact solu-
tions may allow for useful violations of these con-
straints. In future work, we hope to explore bidi-
rectional models with soft-penalties to explicitly
permit these violations.
A Proof of NP-Hardness
We can show that the bidirectional alignment
problem is NP-hard by reduction from the trav-
0 50 100 150 200 250 300 350 400iteration
100
50
0
50
100
sco
re 
rel
ati
ve 
to 
opt
ima
l best dualbest primal
intersection
(a) The best dual and the best primal score, relative to the
optimal score, averaged over all sentence pairs. The best
primal curve uses a feasible greedy algorithm, whereas the
intersection curve is calculated by taking the intersec-
tion of x and y.
0 50 100 150 200 250 300 350 400number of iterations
0.0
0.2
0.4
0.6
0.8
1.0
rel
ati
ve 
sea
rch
 sp
ace
 siz
e
(b) A graph showing the effectiveness of coarse-to-fine prun-
ing. Relative search space size is the size of the pruned lattice
compared to the initial size. The plot shows an average over
all sentence pairs.
Figure 7
eling salesman problem (TSP). A TSP instance
with N cities has distance c(i
?
, i) for each (i
?
, i) ?
[N ]
2
. We can construct a sentence pair in which
I = J = N and -alignments have infinite cost.
?(i
?
, i, j) = ?c(i
?
, i) ?i
?
? [N ]
0
, i ? [N ], j ? [N ]
?(j
?
, i, j) = 0 ?j
?
? [N ]
0
, i ? [N ], j ? [N ]
?(i
?
, 0, j) = ?? ?i
?
? [N ]
0
, j ? [N ]
?(j
?
, i, 0) = ?? ?j
?
? [N ]
0
, i ? [N ]
Every bidirectional alignment with finite objec-
tive score must align exactly one word in e to each
word in f, encoding a permutation a. Moreover,
each possible permutation has a finite score: the
negation of the total distance to traverse the N
cities in order a under distance c. Therefore, solv-
ing such a bidirectional alignment problem would
find a minimal Hamiltonian path of the TSP en-
coded in this way, concluding the reduction.
Acknowledgments Alexander Rush, Yin-Wen
Chang and Michael Collins were all supported
by NSF grant IIS-1161814. Alexander Rush was
partially supported by an NSF Graduate Research
Fellowship.
1489
References
Necip Fazil Ayan and Bonnie J Dorr. 2006. Going
beyond aer: An extensive analysis of word align-
ments and their impact on mt. In Proceedings of
the 21st International Conference on Computational
Linguistics and the 44th annual meeting of the Asso-
ciation for Computational Linguistics, pages 9?16.
Association for Computational Linguistics.
Peter F Brown, Vincent J Della Pietra, Stephen A Della
Pietra, and Robert L Mercer. 1993. The mathemat-
ics of statistical machine translation: Parameter esti-
mation. Computational linguistics, 19(2):263?311.
Fabien Cromieres and Sadao Kurohashi. 2009. An
alignment algorithm using belief propagation and a
structure-based distortion model. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 166?174. Association for Computational Lin-
guistics.
John DeNero and Dan Klein. 2010. Discriminative
modeling of extraction sets for machine translation.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
1453?1463. Association for Computational Linguis-
tics.
John DeNero and Klaus Macherey. 2011. Model-
based aligner combination using dual decomposi-
tion. In ACL, pages 420?429.
Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.
2008. Better alignments = better translations?
In Proceedings of ACL-08: HLT, pages 986?993,
Columbus, Ohio, June. Association for Computa-
tional Linguistics.
K. Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.
2010. Posterior Regularization for Structured La-
tent Variable Models. Journal of Machine Learning
Research, 11:2001?2049.
Joao Graca, Kuzman Ganchev, and Ben Taskar. 2008.
Expectation maximization and posterior constraints.
In J.C. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20, pages 569?576. MIT Press, Cambridge,
MA.
Inc. Gurobi Optimization. 2013. Gurobi optimizer ref-
erence manual.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with su-
pervised itg models. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Natu-
ral Language Processing of the AFNLP: Volume 2-
Volume 2, pages 923?931. Association for Compu-
tational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In
Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology-
Volume 1, pages 48?54. Association for Computa-
tional Linguistics.
Simon Lacoste-Julien, Ben Taskar, Dan Klein, and
Michael I Jordan. 2006. Word alignment via
quadratic assignment. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 112?
119. Association for Computational Linguistics.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In Proceedings of the main
conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104?
111. Association for Computational Linguistics.
Robert C Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Process-
ing, pages 81?88. Association for Computational
Linguistics.
Franz Josef Och, Christoph Tillmann, Hermann Ney,
et al 1999. Improved alignment models for statis-
tical machine translation. In Proc. of the Joint SIG-
DAT Conf. on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, pages
20?28.
Alexander M Rush and Michael Collins. 2012. A tuto-
rial on dual decomposition and lagrangian relaxation
for inference in natural language processing. Jour-
nal of Artificial Intelligence Research, 45:305?362.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based word alignment in statistical
translation. In Proceedings of the 16th conference
on Computational linguistics-Volume 2, pages 836?
841. Association for Computational Linguistics.
1490
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 816?821,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Observational Initialization of Type-Supervised Taggers
Hui Zhang
?
Department of Computer Science
University of Southern California
hzhang@isi.edu
John DeNero
Google, Inc.
denero@google.com
Abstract
Recent work has sparked new interest
in type-supervised part-of-speech tagging,
a data setting in which no labeled sen-
tences are available, but the set of allowed
tags is known for each word type. This
paper describes observational initializa-
tion, a novel technique for initializing EM
when training a type-supervised HMM
tagger. Our initializer allocates probabil-
ity mass to unambiguous transitions in an
unlabeled corpus, generating token-level
observations from type-level supervision.
Experimentally, observational initializa-
tion gives state-of-the-art type-supervised
tagging accuracy, providing an error re-
duction of 56% over uniform initialization
on the Penn English Treebank.
1 Introduction
For many languages, there exist comprehensive
dictionaries that list the possible parts-of-speech
for each word type, but there are no corpora la-
beled with the part-of-speech of each token in con-
text. Type-supervised tagging (Merialdo, 1994)
explores this scenario; a model is provided with
type-level information, such as the fact that ?only?
can be an adjective, adverb, or conjunction, but
not any token-level information about which in-
stances of ?only? in a corpus are adjectives. Re-
cent research has focused on using type-level su-
pervision to infer token-level tags. For instance,
Li et al (2012) derive type-level supervision from
Wiktionary, Das and Petrov (2011) and T?ackstr?om
et al (2013) project type-level tag sets across lan-
guages, and Garrette and Baldridge (2013) solicit
type-level annotations directly from speakers. In
all of these efforts, a probabilistic sequence model
is trained to disambiguate token-level tags that are
?
Research conducted during an internship at Google.
constrained to match type-level tag restrictions.
This paper describes observational initialization,
a simple but effective learning technique for train-
ing type-supervised taggers.
A hidden Markov model (HMM) can be used
to disambiguate tags of individual tokens by max-
imizing corpus likelihood using the expectation
maximization (EM) algorithm. Our approach is
motivated by a suite of oracle experiments that
demonstrate the effect of initialization on the fi-
nal tagging accuracy of an EM-trained HMM tag-
ger. We show that initializing EM with accurate
transition model parameters is sufficient to guide
learning toward a high-accuracy final model.
Inspired by this finding, we introduce obser-
vational initialization, which is a simple method
to heuristically estimate transition parameters for
a corpus using type-level supervision. Transi-
tion probabilities are estimated from unambiguous
consecutive tag pairs that arise when two consec-
utive words each have only a single allowed tag.
These unambiguous word pairs can be tagged cor-
rectly without any statistical inference. Initializing
EM with the relative frequency of these unambigu-
ous pairs improves tagging accuracy dramatically
over uniform initialization, reducing errors by
56% in English and 29% in German. This efficient
and data-driven approach gives the best reported
tagging accuracy for type-supervised sequence
models, outperforming the minimized model of
Ravi and Knight (2009), the Bayesian LDA-based
model of Toutanova and Johnson (2008), and an
HMM trained with language-specific initialization
described by Goldberg et al (2008).
2 Type-Supervised Tagging
A first-order Markov model for part-of-speech
tagging defines a distribution over sentences for
which a single tag is given to each word token.
Let w
i
?W refer to the ith word in a sentence w,
drawn from language vocabulary W . Likewise,
816
ti
? T is the tag in tag sequence t of the ith word,
drawn from tag inventory T . The joint probabil-
ity of a sentence can be expressed in terms of two
sets of parameters for conditional multinomial dis-
tributions: ? defines the probability of a tag given
its previous tag and ? defines the probability of a
word given its tag.
P
?,?
(w, t) =
|w|
?
i=1
P
?
(t
i
|t
i?1
) ? P
?
(w
i
|t
i
)
Above, t
0
is a fixed start-of-sentence tag.
For a set of sentences S, the EM algorithm can
be used to iteratively find a local maximum of the
corpus log-likelihood:
`(?, ?;S) =
?
w?S
ln
[
?
t
P
?,?
(w, t)
]
The parameters ? and ? can then be used to predict
the most likely sequence of tags for each sentence
under the model:
?
t(w) = arg max
t
P
?,?
(w, t)
Tagging accuracy is the fraction of these tags in
?
t(w) that match hand-labeled oracle tags t
?
(w).
Type Supervision. In addition to an unlabeled
corpus of sentences, type-supervised models also
have access to a tag dictionary D ? W ? T that
contains all allowed word-tag pairs. For an EM-
trained HMM, initially setting P
?
(w|t) = 0 for all
(w, t) /? D ensures that all words will be labeled
with allowed tags.
Tag dictionaries can be derived from various
sources, such as lexicographic resources (Li et
al., 2012) and cross-lingual projections (Das and
Petrov, 2011). In this paper, we will follow pre-
vious work in deriving the tag dictionary from
a labeled corpus (Smith and Eisner, 2005); this
synthetic setting maximizes experiment repeata-
bility and allows for direct comparison of type-
supervised learning techniques.
Transductive Applications. We consider a
transductive data setting in which the test set is
available during training. In this case, the model
is not required to generalize to unseen examples or
unknown words, as in the typical inductive setting.
Transductive learning arises in document clus-
tering and corpus analysis applications. For ex-
ample, before running a document clustering al-
gorithm on a fixed corpus of documents, it may be
useful to tag each word with its most likely part-
of-speech in context, disambiguating the lexical
features in a bag-of-words representation. In cor-
pus analysis or genre detection, it may be useful
to determine for a fixed corpus the most common
part-of-speech for each word type, which could be
inferred by tagging each word with its most likely
part-of-speech. In both cases, the set of sentences
to tag is known in advance of learning.
3 Initializing HMM Taggers
The EM algorithm is sensitive to initialization. In
a latent variable model, different parameter values
may yield similar data likelihoods but very differ-
ent predictions. We explore this issue via exper-
iments on the Wall Street Journal section of the
English Penn Treebank (Marcus et al, 1993). We
adopt the transductive data setting introduced by
Smith and Eisner (2005) and used by Goldwa-
ter and Griffiths (2007), Toutanova and Johnson
(2008) and Ravi and Knight (2009); models are
trained on all sections 00-24, the tag dictionary D
is constructed by allowing all word-tag pairs ap-
pearing in the entire labeled corpus, and the tag-
ging accuracy is evaluated on a 1005 sentence sub-
set sampled from the corpus.
The degree of variation in tagging accuracy due
to initialization can be observed most clearly by
two contrasting initializations. UNIFORM initial-
izes the model with uniform distributions over al-
lowed outcomes:
P
?
(t|t
?
) =
1
|T |
P
?
(w|t) =
1
|{w : (w, t) ? D}|
SUPERVISED is an oracle setting that initializes
the model with the relative frequency of observed
pairs in a labeled corpus:
P
?
(t|t
?
) ?
?
(w,t
?
)
|w|
?
i=1
?((t
?
i
, t
?
i?1
), (t, t
?
))
P
?
(w|t) ?
?
(w,t
?
)
|w|
?
i=1
?((w
i
, t
?
i
), (w, t))
where the Kronecker ?(x, y) function is 1 if x and
y are equal and 0 otherwise.
Figure 1 shows that while UNIFORM and
SUPERVISED achieve nearly identical data log-
likelihoods, their final tagging accuracy differs by
817
70
80
90
100
0 5 10 15 20 25 30
-13
-11
-9
-7
SUPERVISED
UNIFORM
70
80
90
100
0 5 10 15 20 25 30
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
94.1%
82.1%
96.7%
72.0%
93.7%
91.0%
92.8%
93.5%
Number of Iterations of Expectation Maximization
-13
-11
-9
-7
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
70
80
90
100
0 5 10 15 20 25 30
93.7%
92.1%
89.2%
93.5%
-13
-11
-9
-7
SUPERVISED TRANSITIONS
OBSERVATIONAL
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
Figure 1: The data log-likelihood (top) and tag-
ging accuracy (bottom) of two contrasting initial-
izers, UNIFORM and SUPERVISED, compared on
the Penn Treebank.
70
80
90
100
0 5 10 15 20 25 30
-13
-11
-9
-7
SUPERVISED
UNIFORM
70
80
90
100
0 5 10 15 20 25 30
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
94.1%
82.1%
96.7%
72.0%
93.7%
91.0%
92.8%
93.5%
Number of Iterations of Expectation Maximization
-13
-11
-9
-7
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
70
80
90
100
0 5 10 15 20 25 30
93.7%
92.1%
89.2%
93.5%
-13
-11
-9
-7
SUPERVISED TRANSITIONS
OBSERVATIONAL
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
Figure 2: The data log-likelihood (top) and tag-
ging accuracy (bottom) of two partially supervised
initializers, one with SUPERVISED TRANSITIONS
and one with SUPERVISED EMISSIONS, compared
on the Penn Treebank.
12%. Accuracy degrades somewhat from the SU-
PERVISED initialization, since the data likelihood
objective differs from the objective of maximizing
tagging accuracy. However, the final SUPERVISED
performance of 94.1% shows that there is substan-
tial room for improvement over the UNIFORM ini-
tializer.
Figure 2 compares two partially supervised ini-
tializations. SUPERVISED TRANSITIONS initial-
izes the transition model with oracle counts, but
the emission model uniformly. Conversely, SU-
PERVISED EMISSIONS initializes the emission pa-
rameters from oracle counts, but initializes the
transition model uniformly. There are many more
emission parameters (57,390) than transition pa-
rameters (1,858). Thus, it is not surprising that
SUPERVISED EMISSIONS gives a higher initial
likelihood. Again, both initializers lead to solu-
tions with nearly the same likelihood as SUPER-
VISED and UNIFORM.
Figure 2 shows that SUPERVISED TRANSI-
TIONS outperforms SUPERVISED EMISSIONS in
tagging accuracy, despite the fact that fewer pa-
rameters are set with supervision. With fixed D,
an accurate initialization of the transition distribu-
tions leads to accurate tagging after EM training.
We therefore concentrate on developing an effec-
tive initialization for the transition distribution.
4 Observational Initialization
The SUPERVISED TRANSITIONS initialization is
estimated from observations of consecutive tags in
a labeled corpus. Our OBSERVATIONAL initializer
is likewise estimated from the relative frequency
of consecutive tags, taking advantage of the struc-
ture of the tag dictionary D. However, it does not
require a labeled corpus.
Let D(w, ?) = {t : (w, t) ? D} denote the
allowed tags for word w. The set
U = {w : |D(w, ?)| = 1}
contains all words that have only one allowed tag.
When a token of some w ? U is observed in a
corpus, its tag is unambiguous. Therefore, its tag
is observed as well, and a portion of the tag se-
quence is known. When consecutive pairs of to-
kens are both in U , we can observe a transition in
the latent tag sequence. The OBSERVATIONAL ini-
tializer simply estimates a transition distribution
from the relative frequency of these unambiguous
observations that occur whenever two consecutive
tokens both have a unique tag.
We now formally define the observational ini-
tializer. Let g(w, t) = ?(D(w, ?), {t}) be an indi-
cator function that is 1 whenever w ? U and its
single allowed tag is t, and 0 otherwise. Then, we
initialize ? such that:
P
?
(t|t
?
) ?
?
w?S
|w|
?
i=1
g(w
i
, t) ? g(w
i?1
, t
?
)
The emission parameters ? are set to be uniform
over allowed words for each tag, as in UNIFORM
initialization.
Figure 3 compares the OBSERVATIONAL ini-
tializer to the SUPERVISED TRANSITIONS initial-
izer, and the top of Table 1 summarizes the perfor-
mance of all initializers discussed so far for the
818
70
80
90
100
0 5 10 15 20 25 30
-13
-11
-9
-7
SUPERVISED
UNIFORM
70
80
90
100
0 5 10 15 20 25 30
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
94.1%
82.1%
96.7%
72.0%
93.7%
91.0%
92.8%
93.5%
Number of Iterations of Expectation Maximization
-13
-11
-9
-7
SUPERVISED TRANSITIONS
SUPERVISED EMISSIONS
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
70
80
90
100
0 5 10 15 20 25 30
93.7%
92.1%
89.2%
93.5%
-13
-11
-9
-7
SUPERVISED TRANSITIONS
OBSERVATIONAL
Data Log-Likelihood (10
6
)
Tagging Accuracy (%)
Number of Iterations of Expectation Maximization
Figure 3: The data log-likelihood (top) and tag-
ging accuracy (bottom) of initializing with SU-
PERVISED TRANSITIONS compared to the unsu-
pervised OBSERVATIONAL initialization that re-
quires only a tag dictionary and an unlabeled train-
ing corpus.
English Penn Treebank. The OBSERVATIONAL
initializer provides an error reduction over UNI-
FORM of 56%, surpassing the performance of an
initially supervised emission model and nearing
the performance of a supervised transition model.
The bottom of Table 1 shows a similar compar-
ison on the T?ubingen treebank of spoken German
(Telljohann et al, 2006). Both training and test-
ing were performed on the entire treebank. The
observational initializer provides an error reduc-
tion over UNIFORM of 29%, and again outper-
forms SUPERVISED EMISSIONS. On this dataset
OBSERVATIONAL initialization matches the final
performance of SUPERVISED TRANSITIONS.
5 Discussion
The fact that observations and prior knowledge are
useful for part-of-speech tagging is well under-
stood (Brill, 1995), but the approach of estimating
an initial transition model only from unambiguous
word pairs is novel.
Our experiments show that for EM-trained
HMM taggers in a type-supervised transductive
data setting, observational initialization is an ef-
fective technique for guiding training toward high-
accuracy solutions, approaching the oracle accu-
racy of SUPERVISED TRANSITIONS initialization.
The fact that models with similar data likeli-
hood can vary dramatically in accuracy has been
observed in other learning problems. For instance,
Toutanova and Galley (2011) show that optimal
English Initial EM-trained
UNIFORM 72.0 82.1
OBSERVATIONAL 89.2 92.1
SUP. EMISSIONS 92.8 91.0
SUP. TRANSITIONS 93.5 93.7
FULLY SUPERVISED 96.7 94.1
German Initial EM-trained
UNIFORM 77.2 88.8
OBSERVATIONAL 92.7 92.1
SUP. EMISSIONS 90.7 89.0
SUP. TRANSITIONS 94.8 92.0
FULLY SUPERVISED 97.0 92.9
Table 1: Accuracy of English (top) and German
(bottom) tagging models at initialization (left) and
after 30 iterations of EM training (right) using var-
ious initializers.
parameters for IBM Model 1 are not unique, and
alignments predicted from different optimal pa-
rameters vary significantly in accuracy.
However, the effectiveness of observational ini-
tialization is somewhat surprising because EM
training includes these unambiguous tag pairs in
its expected counts, even with uniform initializa-
tion. Our experiments indicate that this signal is
not used effectively unless explicitly encoded in
the initialization.
In our English data, 48% of tokens and 74% of
word types have only one allowed tag. 28% of
pairs of adjacent tokens have only one allowed tag
pair and contribute to observational initialization.
In German, 49% of tokens and 87% of word types
are unambiguous, and 26% of adjacent token pairs
are unambiguous.
6 Related Work
We now compare with several previous published
results on type-supervised part-of-speech tagging
trained using the same data setting on the English
WSJ Penn Treebank, introduced by Smith and Eis-
ner (2005).
Contrastive estimation (Smith and Eisner, 2005)
is a learning technique that approximates the par-
tition function of the EM objective in a log-linear
model by considering a neighborhood around ob-
served training examples. The Bayesian HMM
of Goldwater and Griffiths (2007) is a second-
order HMM (i.e., likelihood factors over triples
of tags) that is estimated using a prior distribu-
tion that promotes sparsity. Sparse priors have
819
45 tag set 17 tag set
All train 973k train All train 973k train
Observational initialization (this work) 92.1 92.8 93.9 94.8
Contrastive Estimation (Smith and Eisner, 2005) ? ? 88.7 ?
Bayesian HMM (Goldwater and Griffiths, 2007) 86.8 ? 87.3 ?
Bayesian LDA-HMM (Toutanova and Johnson, 2008) ? ? 93.4 ?
Linguistic initialization (Goldberg et al, 2008) 91.4 ? 93.8 ?
Minimal models (Ravi and Knight, 2009) ? 92.3 ? 96.8
Table 2: Tagging accuracy of different approaches on English Penn Treebank. Columns labeled 973k
train describe models trained on the subset of 973k tokens used by Ravi and Knight (2009).
been motivated empirically for this task (Johnson,
2007). The Bayesian HMM model predicts tag se-
quences via Gibbs sampling, integrating out model
parameters. The Bayesian LDA-based model of
Toutanova and Johnson (2008) models ambiguity
classes of words, which allows information shar-
ing among words in the tag dictionary. In addition,
it incorporates morphology features and a sparse
prior of tags for a word. Inference approximations
are required to predict tags, integrating out model
parameters.
Ravi and Knight (2009) employs integer linear
programming to select a minimal set of parame-
ters that can generate the test sentences, followed
by EM to set parameter values. This technique
requires the additional information of which sen-
tences will be used for evaluation, and its scalabil-
ity is limited. In addition, this work used a sub-
set of the WSJ Penn Treebank for training and se-
lecting a tag dictionary. This restriction actually
tends to improve performance, because a smaller
tag dictionary further constrains model optimiza-
tion. We compare directly to their training set,
kindly provided to us by the authors.
The linguistic initialization of Goldberg et al
(2008) is most similar to the current work, in
that it estimates maximum likelihood parameters
of an HMM using EM, but starting with a well-
chosen initialization with language specific lin-
guistic knowledge. That work estimates emission
distributions using a combination of suffix mor-
phology rules and corpus context counts.
Table 2 compares our results to these related
techniques. Each column represents a variant of
the experimental setting used in prior work. Smith
and Eisner (2005) introduced a mapping from the
full 45 tag set of the Penn Treebank to 17 coarse
tags. We report results on this coarse set by pro-
jecting from the full set after learning and infer-
ence.
1
Using the full tag set or the full training
data, our method offers the best published perfor-
mance without language-specific assumptions or
approximate inference.
7 Future Work
This paper has demonstrated a simple and effec-
tive learning method for type-supervised, trans-
ductive part-of-speech tagging. However, it is an
open question whether the technique is as effec-
tive for tag dictionaries derived from more natural
sources than the labels of an existing treebank.
All of the methods to which we compare ex-
cept Goldberg et al (2008) focus on learning and
modeling techniques, while our method only ad-
dresses initialization. We look forward to inves-
tigating whether our technique can be used as an
initialization or prior for these other methods.
References
Eric Brill. 1995. Unsupervised learning of disam-
biguation rules for part of speech tagging. In In Nat-
ural Language Processing Using Very Large Cor-
pora, pages 1?13. Kluwer Academic Press.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In Proceedings of the Assocation for
Computational Linguistics.
Dan Garrette and Jason Baldridge. 2013. Learning a
part-of-speech tagger from two hours of annotation.
In Proceedings of the North American Chapter of
the Assocation for Computational Linguistics.
Yoav Goldberg, Meni Adler, and Michael Elhadad.
2008. EM can find pretty good HMM POS-taggers
1
Training with the reduced tag set led to lower perfor-
mance of 91.0% accuracy, likely because the coarse projec-
tion drops critical information about allowable English tran-
sitions, such as what verb forms can follow to be (Goldberg
et al, 2008).
820
(when given a good start). In Proceedings of the As-
sociation for Computational Linguistics.
Sharon Goldwater and Tom Griffiths. 2007. A fully
Bayesian approach to unsupervised part-of-speech
tagging. In Proceedings of the Association for Com-
putational Linguistics.
Mark Johnson. 2007. Why doesnt EM nd good HMM
POS-taggers? In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing.
Shen Li, Jo?ao V. Grac?a, and Ben Taskar. 2012. Wiki-ly
supervised part-of-speech tagging. In Proceedings
of the Conference on Empirical Methods in Natural
Language Processing.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of english: The penn treebank. Computa-
tional Linguistics.
Bernard Merialdo. 1994. Tagging English text with a
probabilistic model. Computational Linguistics.
Sujith Ravi and Kevin Knight. 2009. Minimized mod-
els for unsupervised part-of-speech tagging. In Pro-
ceedings of the Association for Computational Lin-
guistics.
Noah A. Smith and Jason Eisner. 2005. Contrastive
estimation: Training log-linear models on unlabeled
data. In Proceedings of the Association for Compu-
tational Linguistics.
Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan
McDonald, and Joakim Nivre. 2013. Token and
type constraints for cross-lingual part-of-speech tag-
ging. Transactions of the Association for Computa-
tional Linguistics.
Heike Telljohann, Erhard Hinrichs, Sandra K?ubler, and
Heike Zinsmeister. 2006. Stylebook for the tbingen
treebank of written german.
Kristina Toutanova and Michel Galley. 2011. Why
initialization matters for ibm model 1: Multiple op-
tima and non-strict convexity. In Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 461?466, Portland, Oregon, USA, June.
Association for Computational Linguistics.
Kristina Toutanova and Mark Johnson. 2008. A
Bayesian LDA-based model for semi-supervised
part-of-speech tagging. In Proceedings of Neural
and Information Processing Systems.
821
