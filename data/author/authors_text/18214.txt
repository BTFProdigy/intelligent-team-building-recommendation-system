Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1246?1258,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Understanding and Quantifying Creativity in Lexical Composition
Polina Kuznetsova Jianfu Chen Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,jianchen,ychoi}@cs.stonybrook.edu
Abstract
Why do certain combinations of words such
as ?disadvantageous peace? or ?metal to the
petal? appeal to our minds as interesting ex-
pressions with a sense of creativity, while
other phrases such as ?quiet teenager?, or
?geometrical base? not as much? We present
statistical explorations to understand the char-
acteristics of lexical compositions that give
rise to the perception of being original, inter-
esting, and at times even artistic. We first ex-
amine various correlates of perceived creativ-
ity based on information theoretic measures
and the connotation of words, then present ex-
periments based on supervised learning that
give us further insights on how different as-
pects of lexical composition collectively con-
tribute to the perceived creativity.
1 Introduction
An essential property of natural language is the gen-
erative capacity that makes it possible for people to
express indefinitely many thoughts through indef-
initely many different ways of composing phrases
and sentences (Chomsky, 1965). The possibility of
novel, creative expressions never seems to exhaust.
Various types of writers, such as novelists, journal-
ists, movie script writers, and creatives in adver-
tising, continue creating novel phrases and expres-
sions that are original while befitting in expressing
the desired meaning in the given situation. Consider
unique phrases such as ?geological split personal-
ity?, or ?intoxicating Shangri-La of shoes?,1 that
1Examples from New York Times articles in 2013.
continue flowing into the online text drawing atten-
tion from readers.
Writers put significant effort in choosing the per-
fect words in completing their compositions, as a
well-chosen combination of words is impactful in
readers? minds for rendering the precise intended
meaning, as well as stimulating an increased level
of cognitive responses and attention. Metaphors in
particular, one of the quintessential forms of lin-
guistic creativity, have been discussed extensively
by studies across multiple disciplines, e.g., Cog-
nitive Science, Psychology, Linguistics, and Liter-
ature (e.g., Lakoff and Johnson (1980), McCurry
and Hayes (1992), Goatly (1997)). Moreover, re-
cent studies based on fMRI begin to discover bio-
logical evidences that support the impact of creative
phrases on people?s minds. These studies report that
unconventional metaphoric expressions elicit signif-
icantly increased involvement of brain processing
when compared against the effect of conventional
metaphors or literal expressions (e.g., Mashal et al
(2007), Mashal et al (2009)).
Several linguistic elements, e.g., syntax, seman-
tics, and pragmatics, are likely to be working to-
gether in order to lead to the perception of creativ-
ity. However, their underlying mechanisms by and
large are yet to be investigated. In this paper, as a
small step toward quantitative understanding of lin-
guistic creativity, we present a focused study on lex-
ical composition two content words.
Being creative, by definition, implies qualities
such as being unique, novel, unfamiliar or uncon-
ventional. But not every unfamiliar combination of
words would appeal as creative. For example, unfa-
1246
miliar biomedical terms, e.g., ?cardiac glycosides?,
are only informative without appreciable creativity.
Similarly, less frequent combinations of words, e.g.,
?rotten detergent? or ?quiet teenager?, though de-
scribing situations that are certainly uncommon, do
not bring about the sense of creativity. Finally, some
unique combinations of words can be just nonsensi-
cal , e.g., ?elegant glycosides?.
Different studies assumed different definitions of
linguistic creativity depending on their context and
end goals (e.g., Chomsky (1976), Zhu et al (2009),
Gerva?s (2010), Maybin and Swann (2007), Carter
and McCarthy (2004)). In this paper, as an opera-
tional definition, we consider a phrase creative if it
is (a) unconventional or uncommon, and (b) expres-
sive in an interesting, imaginative, or inspirational
way.
A system that can recognize creative expressions
could be of practical use for many aspiring writers
who are often in need of inspirational help in search-
ing for the optimal choice of words. Such a system
can also be integrated into automatic assessment of
writing styles and quality, and utilized to automat-
ically construct a collection of interesting expres-
sions from the web, which may be potentially useful
for enriching natural language generation systems.
With these practical goals in mind, we aim to un-
derstand phrases with linguistic creativity in a broad
scope. Similarly as the work of Zhu et al (2009),
our study encompasses phrases that evoke the sense
of interestingness and creativity in readers? minds,
rather than focusing exclusively on clearly but nar-
rowly defined figure of speeches such as metaphors
(e.g., Shutova (2010)), similes (e.g., Veale et al
(2008), Hao and Veale (2010)), and humors (e.g.,
Mihalcea and Strapparava (2005), Purandare and
Litman (2006)). Unlike the study of Zhu et al
(2009), however, we concentrate specifically on how
combinations of different words give rise to the
sense of creativity, as this is an angle that has not
been directly studied before. We leave the roles of
syntactic elements as future research.
We first examine various correlates of perceived
creativity based on information theoretic measures
and the connotation of words, then present experi-
ments based on supervised learning that give us fur-
ther insights on how different aspects of lexical com-
position collectively contribute to the perceived cre-
ativity.
2 Theories of Creativity and Hypotheses
Many researchers, from the ancient philosophers to
the modern time scientists, have proposed theories
that attempt to explain the mechanism of creative
process. In this section, we draw connections from
some of these theories developed for general human
creativity to the problem of quantitatively interpret-
ing linguistic creativity in lexical composition.
2.1 Divergent Thinking and Composition
Divergent thinking (e.g., McCrae (1987)), which
seeks to generate multiple unstereotypical solutions
to an open ended problem has been considered as
the key element in creative process, which contrasts
with convergent thinking that find a single, cor-
rect solution (e.g., Cropley (2006)). Applying the
same high-level idea to lexical composition, diver-
gent composition that explores an unusual, uncon-
ventional set of words is more likely to be creative.
Note that the key novelty then lies in the composi-
tional operation itself, i.e., the act of putting together
a set of words in an unexpected way, rather than the
rareness of individual words being used. In recent
years there has been a swell of work on composi-
tional distributional semantics that captures the com-
positional aspects of language understanding, such
as sentiment analysis (e.g., Yessenalina and Cardie
(2011), Socher et al (2011)) and language model-
ing (e.g., Mitchell and Lapata (2009), Baroni and
Zamparelli (2010), Guevara (2011), Clarke (2012),
Rudolph and Giesbrecht (2010)). However, none
has examined the compositional nature in quantify-
ing creativity in lexical composition.
We consider two computational approaches to
capture the notion of creative composition. The first
is via various information theoretic measures, e.g.,
relative entropy reduction, to measure the surprisal
of seeing the next word given the previous word.
The second is via supervised learning, where we ex-
plore different modeling techniques to capture the
statistical regularities in creative compositional op-
erations. In particular, we will explore (1) compo-
sitional operations of vector space models, (2) ker-
nels capturing the non-linear composition of differ-
ent dimensions in the meaning space, (3) the use of
1247
neural networks as an alternative to incorporate non-
linearity in vector composition. (See ?5).
2.2 Latent Memory and Creative Semantic
Subspace
Although we expect that unconventional composi-
tion has a connection to creativeness of resulting
phrases, that alone does not explain many counter
examples where the composition itself is uncommon
but the resulting expression is not creative due to
lack of interestingness or imagination, e.g., ?room
and water?.2 Therefore, we must consider addi-
tional conditions that give rise to creative phrases.
Let S represent the semantic space, i.e., the set of
all possible semantic representation that can be ex-
pressed by a phrase that is composed of two content
words.3 Then we hypothesize that some subsets of
semantic space {Si|Si ? S} are semantically futile
regions for appreciable linguistic creativity, regard-
less of how novel the composition in itself might be.
Such regions may include technical domains such as
law or pharmacology. Similarly, we expect seman-
tically fruitful subsets of semantic space where cre-
ative expressions are more frequently found. For in-
stance, phrases such as ?guns and roses? and ?metal
to the petal? are semantically close to each other and
yet both can be considered as interesting and cre-
ative (as opposed to one of them losing the sense of
creativity due to its semantic proximity to the other).
This notion of creative semantic subspace con-
nects to theories that suggest that latent memories
serve as motives for creative ideas and that one?s
creativity is largely depending on prior experience
and knowledge one has been exposed to (e.g., Freud
(1908), Necka (1999), Glaskin (2011), Cohen and
Levinthal (1990), Amabile (1997)), a point also
made by Einstein: ?The secret to creativity is know-
ing how to hide your sources.?
Figure 5 presents visualized supports for creative
semantic subspace,4 where we observe that phrases
in the neighborhood of legal terms are generally
not creative, while the semantic neighborhood of
2With additional context this example may turn into a cre-
ative one, but for simplicity we focus on phrases with two con-
tent words considered out of context.
3Investigation on recursive composition of more than two
content words and the influence of syntactic packaging is left as
future research.
4See ?6 for more detailed discussion.
Source # of # of Avg
uniq sent sent Entropy
words len
QUOTESraw 29498 49402 28 173.05
GLOSSESraw 20869 7745 53 96.79
Table 1: Entropy of word distribution in datasets
Dataset
# of word pairs percentage
total #(-) #(+) #(+)/total %
GLOSSES 1912 149 18 0.94
QUOTES 3298 204 35 1.06
Table 2: Distribution of creative(+)/common(-) word
pairs over GLOSSES and QUOTES dataset.
?kingdom? and ?power? is relatively more fruitful
for composing creative (i.e., unique and uncommon
while being imaginative and interesting, per our op-
erational definition of creativity given in ?1) word
pairs, e.g., invisible empire?. In our empirical in-
vestigation, this notion of semantically fruitful and
futile semantic subspaces are captured using dis-
tributional semantic space models under supervised
learning framework (?5).
2.3 Affective Language
Another angle we probe is the connection between
creative expressions and the use of affective lan-
guage. This idea is supported in part by previ-
ous research that explored the connection between
figurative languages such as metaphors and senti-
ment (e.g., Fussell and Moss (1998), Rumbell et
al. (2008), Rentoumi et al (2012)). The focus of
previous work was either on interpretation of the
sentiment in metaphors, or the use of metaphors
in the description of affect. In contrast, we aim
to quantify the correlation between creative expres-
sions (beyond metaphors) and the use of sentiment-
laden words in a more systematic way. This explo-
ration has a connection to the creative semantic sub-
space discussed earlier (?2.2), but pays a more direct
attention to the aspect of sentiment and connotation.
3 Creative Language Dataset
We start our investigation by considering two types
of naturally existing collection of sentences: (1)
quotes and (2) dictionary glosses. We expect that
quotes are likely to be rich in creative expressions,
while dictionary glosses stand in the opposite spec-
1248
less	 ?freq	 ? more	 ?freq	 ?
(a)	 ? (b)	 ? (c)	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
30	 ?
1	 ? 5	 ? 9	 ? 13	 ? 17	 ? 21	 ? 25	 ? 29	 ? 33	 ? 37	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
1	 ? 9	 ? 17	 ? 25	 ? 33	 ? 41	 ? 49	 ? 57	 ? 65	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
1	 ? 11	 ? 21	 ? 31	 ? 41	 ? 51	 ? 61	 ? 71	 ? 81	 ? 91	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
less	 ?freq	 ? more	 ?freq	 ? less	 ?freq	 ? more	 ?freq	 ?
Glosses	 ? Quotes	 ? All	 ?
Figure 1: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of frequencies (x-axis) for GLOSSES, QUOTES and both datasets combined.
lower	 ?val	 ? higher	 ?val	 ?
(a)	 ? (b)	 ? (c)	 ?
lower	 ?val	 ? higher	 ?val	 ? lower	 ?val	 ? higher	 ?val	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
1	 ? 5	 ? 9	 ? 13	 ? 17	 ? 21	 ? 25	 ? 29	 ? 33	 ? 37	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
5	 ?
10	 ?
15	 ?
20	 ?
25	 ?
1	 ? 9	 ? 17	 ? 25	 ? 33	 ? 41	 ? 49	 ? 57	 ? 65	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
0	 ?
10	 ?
20	 ?
30	 ?
40	 ?
50	 ?
1	 ? 11	 ? 21	 ? 31	 ? 41	 ? 51	 ? 61	 ? 71	 ? 81	 ? 91	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
Glosses	 ? Quotes	 ? All	 ?
Figure 2: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of PMI values (x-axis) for GLOSSES, QUOTES and both datasets combined.
trum of being creative.
QUOTESraw: We crawled inspirational quotes
from ?Brainy Quote?.5
GLOSSESraw: We collected glosses from Ox-
ford Dictionary and Merriam-Webster Dictionary.6
Overall we crawled about 8K definitions. Table 1
shows statistics of the dataset.7
Entropy of word distribution We conjecture that
QUOTES and GLOSSES are different in terms of
word variety, which can be quantified by the entropy
5http://www.brainyquote.com/
6http://oxforddictionaries.com/ and http://www.merriam-
webster.com/. We only consider words appearing in both dic-
tionaries to avoid unusual words such as compound words, e.g.,
?zero-base?.
7QUOTESraw contain 30K unique words and GLOSSESraw
has 20K unique words. QUOTESraw have much arger number
of sentences, while its average sentence is shorter.
of word distributions. To compute the entropy for
each dataset, we use ngram statistics from the corre-
sponding dataset to measure the probability of each
word. As expected, QUOTES dataset has higher
entropy than GLOSSES in Table 1.
3.1 Creative Word Pairs
We extract word pairs corresponding to the follow-
ing syntactic patterns: [NN NN], [JJ NN], [NN JJ]
and [JJ JJ]. Not all pairs from QUOTESraw are cre-
ative, and likewise, not all pairs from GLOSSESraw
are uncreative. Therefore, we perform manual an-
notations to a subset of the collected pairs as fol-
lows. We obtain a small subset of pairs by apply-
ing stratified sampling based on bigram frequency
buckets: first we sort word pairs by their bigram
frequencies obtained from Web 1T corpus (Brants
and Franz (2006)), group them into consecutive fre-
1249
quency buckets each of which containing 400 word
pairs, then sample 40 word pairs from each bucket.
We label word pairs using Amazon Mechnical
Turk (AMT) (e.g., Snow et al (2008)). We ask three
turkers to score each pair in 1-5 scale, where 1 is the
least creative and 5 is the most creative. We then
obtain the final creativity scale score by averaging
the scores over 3 users. In addition, we ask turkers
a series of yes/no questions to help turkers to deter-
mine whether the given pair is creative or not.8 We
determine the final label of a word pair based on two
scores, creativity scale score and yes/no question-
based score. If creativity scale score is 4 or 5 and
question-based score is positive, we label the pair as
creative. Similarly, if creativity scale score is 1 or
2 and question-based score is negative, we label the
pair as common. We discard the rest from the final
dataset. This filtering process is akin to the removal
of neural sentiment in the early work of sentiment
analysis (e.g., Pang et al (2002)).9 Table 2 shows
the statistics of the resulting dataset.
Creative Pairs and their Frequencies: To gain
insights on the stratified sample of word pairs, we
plot the label (? {creative, common}) distribution
of word pairs as a function of simple statistics, such
as a range (bucket) of bigram frequencies or PMI
values of the given pair of words. Both bigram fre-
quencies and PMI scores are computed based on
Google Web 1T corpus Brants and Franz (2006).
Figure 1 shows the results for word frequencies. As
expected, word pairs with high frequencies are much
more likely to be common, while word pairs with
low frequencies can be either of the two. Also as ex-
pected, pairs extracted from QUOTES are relatively
more likely to be creative than those from GLOSSES.
In any case, it is clear that not all rare pairs are cre-
ative.
Creative Pairs and their PMI Scores: Similarly
as above, Figure 2 plots the relation between the
distribution of labels of word pairs and their corre-
sponding PMI. As expected, pairs with high PMI are
more likely to be common, though the trend is not as
8E.g., ?is this word combination boring and not original??
or ?does it provoke unusual imagination??.
9Cohen?s Kappa and Pearson Correlation on the filtered data
are 0.69 and 0.72 respectively. Corresponding scores for the un-
filtered data drop to 0.26 and 0.29 respectively. All the experi-
ments are performed on the filtered data.
Common Creative
quiet teenager inglorious success
constant longitude thorny existence
watery juice relaxed symmetry
noble political sardonic destiny
diet cooking dispassionate history
verbal interpretation poetical enthusiasm
unwelcome situation verbal beauty
migratory tuna earth breathe
lousy businessman disadvantageous peace
terrific marriage alchemical marriage
solved issue deep nonsense
Table 3: Sample Creative / Common Word Pairs
skewed as before.
Final Dataset: From our initial annotation study,
it became apparent to us that creative pairs are very
rare, perhaps not surprisingly, even among infre-
quent pairs. In order to build the word pair corpus
with as many creative pairs as possible, we focus on
infrequent word pairs for further annotation, from
which we construct a larger and balanced set of cre-
ative and common word pairs, with 394 word pairs
for each class. The specific construction procedure
is as follows: first combine all of the word pairs
extracted from both QUOTESraw and GLOSSESraw
as a single dataset, sort them by bigram frequency,
group them into consecutive frequency buckets each
of which has 40 word pairs; finally balance each fre-
quency bucket, by discarding word pairs with higher
frequency value from the larger class in that bucket.
Examples of labeled word pairs are shown in Ta-
ble 3. Hereafter we use this balanced dataset of word
pairs for all experiments.10
4 Creativity Measures
4.1 Information Measures
In this section we explore information theoretic
measures to quantify the surprisal aspect of creative
word pairs, relating to the divergent, compositional
nature of creativity discussed in ?2.1.
Entropy of Context Seeing a word w changes our
expectation on what might follow next. Some words
have stronger selective preference (higher entropy)
than others.
10The resulting dataset is available at http://www.cs.
stonybrook.edu/?pkuznetsova/creativity/
1250
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
MI(w1,w2)	 ?	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?
%	 ?
of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
Lconn(w1,w2)	 ?
20	 ?
30	 ?
40	 ?
50	 ?
60	 ?
70	 ?
80	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
Lsubj(w1,w2)	 ?
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?%
	 ?of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?
H(w1w2)	 ?
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?
%	 ?
of
	 ?w
or
d	 ?p
air
s	 ?
bucket	 ?	 ?
RH(w1,w2)	 ?
10	 ?
30	 ?
50	 ?
70	 ?
90	 ?
0	 ? 2	 ? 4	 ? 6	 ? 8	 ? 10	 ? 12	 ? 14	 ? 16	 ? 18	 ? 20	 ? 22	 ?
%	 ?
of
	 ?w
or
dp
air
s	 ?
bucket	 ?
KL(w1w2,w2)	 ?	 ?
(a)	 ? (b)	 ? (c)	 ?
(d)	 ? (e)	 ? (f)	 ?
diff	 ? diff	 ?
Figure 3: Distribution of creative (double lines in blue) versus common (single lines in red) word pairs with varying
ranges of information or polarity measures (x-axis).
0	 ?
0.01	 ?
0.02	 ?
0.03	 ?
0.04	 ?
0.05	 ?
0.06	 ?
Mi
lto
n	 ? as	 ? bes
t	 ?
bu
sy	 ?
cle
ar	 ?
com
mo
n	 ?
coo
l	 ?
dee
p	 ?
ear
ly	 ? end
	 ?
exp
ens
ive
	 ?
fin
al	 ? gla
d	 ?
har
d	 ? ho
t	 ?
ing
ot	 ? las
t	 ?
like
ly	 ?
ma
ny	 ? nex
t	 ?
ow
n	 ?
ple
ase
d	 ?
pro
fes
sio
nal
	 ?
rar
ely
	 ?
rol
e	 ?
ser
iou
sly
	 ?
slo
w	 ?
spe
cia
l	 ?
suc
ces
s	 ? to	 ?
val
uab
le	 ?
we
lco
me
	 ?
Figure 4: Conditional probability of neighboring words
for ?inglorious? (filled / red) and ?very? (unfilled / blue).
For instance, the entropy after seeing ?very?
would be higher than that after seeing ?inglorious?,
as the former can be used in a wider variety of con-
text than the later. Figure 4 visualizes relatively
more skewed distribution of ?inglorious?. We com-
pute the entropy of future context conditioning on
w1, w2 and w1w2, which we denote as H(w1),
H(w2), H(w1w2) respectively, latter is shown in
Figure 3 ? a.11
11As before, language models are drawn from Google Web
Relative Entropy Transformation In order to fo-
cus more directly on the relative change of entropy
as a result of composition, we compute Relative En-
tropy Transformation:
RH(w1, w2) =
|H(w1)?H(w1w2)|
H(w1) +H(w1w2)
(1)
As expected (Figure 3 ? b and Table 4), this relative
quantity captures creativity better than the absolute
measure H(w1w2) computed above. The idea be-
hind this measure has a connection to uncertainty
reduction in psycholinguistic literature (e.g., Frank
(2010), Hale (2003), Hale (2006)).
KL divergence To capture unusual combinations
of words, we compare the difference between the
distributional contexts of w1 and w1w2 so that
KL(w1w2, w1) =
?
wi?V
P (wi|w1, w2) log
P (wi|w1, w2)
P (wi|w1)
(2)
Figure (3 ? c) shows thatKL(w1w2, w1)12 is among
1T corpus Brants and Franz (2006).
12We also compute KL(w1, w2) in a similar manner as
KL(w1w2, w1)
1251
the effective measures in capturing creative pairs.
Mutual Information Finally, we consider mutual
information (Figure 3 ? d):
MI(w1, w2) =
?
wi?V
P (wi|w1, w2)?
log
P (wi|w1, w2)
P (wi|w1) ? P (wi|w2)
(3)
Correlation coefficients Pearson coefficients for
all measures are shown in Table 4. Interestingly, in-
formation theoretic measures that compare the dis-
tribution of word?s context, such as RH(w1, w2),
KL(w1w2, w1) and MI(w1, w2), capture the sur-
prisal aspect of creativity better than simple frequen-
cies or PMI scores that do not consider contextual
changes. But even for those cases when the corre-
lation is statistically significant, the values are not
too high. We conjecture that there are two reasons
for this. First, Pearson assumes linear correlations,
hence not sensitive enough to capture non-linear cor-
relations that are evident in graphs shown in Fig-
ure 3. Second, these measures only capture the sur-
prisal aspect of creativity, missing the other impor-
tant qualities: interestingness or imaginativeness.
4.2 Sentiment and Connotation
Next we investigate the connection between creativ-
ity and sentiment, as illustrated in ?2.3. We con-
sider both sentiment (more explicit) and connotation
(more implicit) words,13 and consider them with
or without distinguishing the polarity (i.e., positive,
negative). To determine sentiment and connotation,
we use lexicons provided by OpinionFinder (Wilson
et al (2005)) and Feng et al (2013) respectively. We
denote polarity of a word wi as L(wi).14 When wi
has a negative polarity L(wi) is assigned a value of
-1, and when wi is positive L(wi) is equal to 1. We
assume that a word is neutral when it is not in the
lexicon, assigning 0 to L(wi). For a word pair w1w2
we compute absolute difference Ldiff (w1, w2) be-
tween polarities of tokens in a word pair in order to
catch examples such as ?inglorious success?.
13E.g., expressions such as ?blue sky? or ?white sand? are
not sentiment-laden, but do have positive connotation.
14We denote polarity from OpinionFinder as Lsubj and con-
notation as Lconn
Measure Corr Coeff p-value? adj p-value??
pointwise, noncontextual
Freq(w1w2) 0.014 0.67 0.86
PMI(w1, w2) 0.011 0.75 0.86
information theoretic, contextual
E(w1) -0.038 0.26 0.49
E(w2) -0.126 0.00019 0.00083
E(w1, w2) 0.013 0.71 0.86
RH(w1, w2) 0.113 0.00081 0.0024
KL(w1w2, w1) 0.134 7.152-05 0.00054
KL(w1, w2) -0.080 0.018 0.039
MI(w1, w2) 0.125 0.00022 0.00083
sentiment & connotation
Lsubj(w1) 0.006 0.87 0.87
Lsubj(w2) 0.031 0.36 0.60
Ldiffsubj (w1, w2) 0.168 6.67e-07 1.00e-05
Lconn(w1) 0.023 0.49 0.74
Lconn(w2) 0.008 0.80 0.86
Ldiffconn(w1, w2) 0.082 0.015 0.038
Table 4: Pearson correlation between various measures
and creativity of word pairs. Boldface denotes statistical
significance (p ? 0.05).
note *: Two-tailed p-value, 394 word pairs per class
note **: We used Benjamini-Hochberg method to adjust
p-values for multiple tests
Table 4 shows Pearson coefficient for sentiment
and connotation based measures. It turns out that
polarity of each word on its own does not have a
high impact on the creativity of a word pair. Rather,
it is the difference between the two words that gives
rise the sense of creativity.
4.3 Learning to Recognize Creativity
Now we put together all measures explored in ?4.1
and 4.2 in a supervised-learning framework. As ex-
pected, rather than either one alone, the combination
of various measures leads to the best performance:
~F12 = [RH(w1, w2);KL(w1, w2);H(w1w2);
Ldiffconn(w1, w2);PMI(w1, w2);
H(w2);KL(w1w2, w1);KL(w2, w1);
Ldiffsubj (w1, w2);MI(w1, w2);
Freq(w1w2);H(w1)]
Table 5 shows the performance of the above fea-
ture vector with 12 features using libsvm (Chang and
Lin, 2011). We use C-Support Vector Classification
(C-SVC). Performance is reported in accuracy using
5-fold cross validation.15
15Among these 12 features, the feature selection algorithm
1252
5 Learning Creative Pairs with
Distributional Semantic Vectors
The measures explored in ?4 were largely unin-
formed of distributional semantic dimensions of
each word. However, in order to pursue the concep-
tual aspect of creativity illustrated in ?2.2, that is, the
notion of semantic subspaces that are inherently fu-
tile or fruitful for creativity, we need to incorporate
semantic representations more directly. We there-
fore explore the use of distributional vector space
models. Another goal of this section will be addi-
tional learning-based investigation to the composi-
tional nature of creative word pairs, complementing
the investigation in ?4, which focused on the com-
positional aspect of creativity described in ?2.1.
With above goals in mind, in what follows, we ex-
plore three different ways to learn compositional as-
pect of creative word pairs: (1) learning with explicit
compositional vector operations (?5.1), (2) learning
nonlinear composition via kernels (?5.2), (3) learn-
ing nonlinear composition via deep learning (?5.3).
Note that in all these approaches, the notion of cre-
ative semantic subspace is integrated indirectly, as
the feature representation always incorporates the
resulting (composed) vector representations.
Baseline & Configuration We consider the con-
catenation of two word vectors [~w1; ~w2] as the base-
line, since it can be viewed as what simple bag-of-
word features would be. Since the size of creative
pair dataset is not at scale yet, we choose to work
with vector space models that are in reduced dimen-
sions. We experimented with both Non-Negative
Sparse Embedding (Murphy et al (2012)) and neu-
ral semantic vectors of Huang et al (2012), but re-
port experiments with the latter only as those gave
us slightly better results.
5.1 Compositional Vector Operations
We consider the following compositional vector op-
erations inspired by recent studies for composi-
tional distributional semantics (e.g., Guevara (2011),
Clarke (2012), Mitchell and Lapata (2008), Wid-
dows (2008)).
? ADD: ~w1 + ~w2
? DIFF: abs(~w1 ? ~w2)
of Chen and Lin (2005) determines that the most two important
ones are RH(w1, w2) and KL(w1, w2).
? MULT: ~w1 .? ~w2
? MIN: min{~w1, ~w2}
? MAX: max{~w1, ~w2}
All operations take two input vectors ? Rn, and
output a vector ? Rn. Each operation is applied
element-wise. We then perform binary classifica-
tion over the composed vectors using linear SVM.
Besides using features based on the composed vec-
tors, we also experiment with features based on con-
catenating multiple composed vectors, in the hope to
capture more diverse compositional operations. See
Table 5 for more details and experimental results.
5.2 Learning Nonlinear Composition via
Kernels
As an alternative to explicit vector compositions, we
also probe implicit operations based on non-linear
combinations of semantic dimensions using kernels
(e.g., Scho?lkopf and Smola (2002), Shawe-Taylor
and Cristianini (2004)), in particular:
? Polynomial: K(x, y) = (?xT y + r)d, ? > 0
? RBF: K(x, y) = exp(?? ?x? y?2), ? > 0
? Laplacian: K(x, y) = exp(?? ?x? y?), ? > 0
5.3 Learning Non-linear Composition via Deep
Learning
Yet another alternative to model non-linear com-
position is deep learning. To learn the non-linear
transformation of a pair of semantic vectors, we ex-
plore the use of autoencoders (e.g., Pollack (1990),
Voegtlin and Dominey (2005)). We follow the for-
mulation of vector composition proposed by Socher
et al (2011) except that we do not stack autoen-
coders for recursion. More specifically, given the
two input words ~w1, ~w2 ? Rn, we want to learn
a vector space representation of their combination
~p ? Rn. The recursive auto encoder (RAE) of
Socher et al (2011) models the composition of a
word pair as a non-linear transformation of their
concatenation [~w1; ~w2]:
~p = f(M1[~w1; ~w2] +~b1) (4)
where M1 ? Rn?2n. After adding a bias term
~b1 ? Rn, a nonlinear element-wise function f such
as tanh is applied to the resulting vector. The repre-
sentation ~p of the word pair is then fed into a recon-
struction layer to reconstruct the two input vectors,
1253
Methods Accuracy
Creativity measures (?4.3)
~F12 62.30
Baseline: vector concatenation (no composition)
[~w1; ~w2] 67.51
Explicit vector composition (?5.1)
~w1 + ~w2 66.62
abs(~w1 ? ~w2) 60.03
min{~w1, ~w2} 66.08
max{~w1, ~w2} 64.97
~w1 .? ~w2 56.34
[abs(~w1 ? ~w2); ~w1; ~w2] 69.54
[max{~w1, ~w2}; ~w1; ~w2] 68.02
Non-linear composition via kernels (?5.2)
Polynomial 65.86
RBF 69.16
Laplacian 68.15
Non-linear composition via deep learning (?5.3)
f(M1[~w1; ~w2] +~b1) 67.25
Table 5: Performance comparison of creativity classifiers.
Incorrectly predicted y? Semantically close y?
word pairs word pairs
CONFUSION DUE TO WORD SIMILARITY (20/42)
?entire carton? - ?whole angst? +
?outdated tax? - ?graconian tax? +
?dismissive way? - ?amorous way? +
?insidious part? + ?leather part? -
CONFUSION DUE TO SUBJECTIVE LABELING (8/42)
?independent + ?wonderful -
religion? religion?
WORD SENSE DISAMBIGUATION PROBLEMS (2/42)
?fiscal cliff? - ?winding lake? +
?opera window? + ?work-shop floor? -
Table 6: Error analysis: y? denotes the true label. For
each incorrectly predicted word pair (left column), we
show an example of semantically close word pairs (right
column) with the opposite true label that might have con-
fused learning.
and a softmax layer to predict the probability of the
word pair being creative and not creative. We ini-
tialize the word vectors using the pre-learned vector
space representations in Huang et al (2012).
5.4 Experimental Results
Table 5 shows the performance comparison of dif-
ferent features sets and algorithms. In all cases,
parameters are tuned from the training portion of
the data. We see that simple vector composition
alone does not perform better than vector concate-
nation [~w1; ~w2]. However, combining abs(~w1? ~w2)
or max{~w1; ~w2} with [~w1; ~w2] perform better than
concatenation. Kernels with non-linear transforma-
tion of feature space generally improve performance
over linear SVM, suggesting that kernels capture
some of the interesting compositional aspect of cre-
ativity that is not covered by some of the explicit
vector compositions considered in ?5.1. We also ex-
perimented with additional features driven from the
creativity measures explored in ?4, but we omit their
results as those did not help improving the perfor-
mance. Unfortunately learning nonlinear composi-
tion with deep learning did not yield better results.
We conjecture that it is due to the small dataset we
were able to obtain for this study, which may have
not been enough to learn the rich parameter space of
the nonlinear transformation matrix.
6 Analysis and Insight
Error analysis We manually inspected a ran-
domly chosen 42 error cases, and characterize the
potential causes of those errors. Examples of three
types of errors are shown in Table 6. For each incor-
rectly predicted word pair, we also show a seman-
tically close word pair with the opposite true label
that might have confused the learning algorithm.
Visualization To gain additional insight, we
project word pairs represented in their vec-
tor concatenations onto 2-dimensional space us-
ing t-Distributed Stochastic Neighbor Embedding
(van der Maaten and Hinton (2008)). Figure 5
shows some of the interesting regions of the pro-
jection: some regions are relatively futile in hav-
ing creative phrases (e.g., regions involving simple
adjectives such as ?good?, ?bad?, regions corre-
sponding to legal terms), while some regions are rel-
atively more fruitful (e.g., regions involving abstract
adjectives such as ?infinite?, ?universal?, ?funda-
mental?). There are also many other regions (e.g., in
the vicinity of ?true?, ?perfect? or ?intelligent? in
Figure 5) where the separation between creative and
noncreative phrases are not as prominent. In those
regions, compositional aspects would play a bigger
role in determining creativity than memorizing fruit-
ful semantic subspaces.
1254
?? infinite promise 
?? infinite leisure 
?? universal aspiration 
?? perfect disorder 
?? absolute barbarism 
?? fundamental soul 
?? theoretical wisdom 
?? fundamental key 
?? technological refinement 
?? good marathon 
?? good custodian 
?? universal anguish 
?? pure phenomenology 
?? logical market 
?? true perversion 
?? perfect fire 
?? perfect land 
?? true ambition 
?? true golfer 
?? normal professor 
?? normal adulthood 
?? bad profession 
?? bad motivation 
?? intelligent manipulation 
?? intelligent vocabulary 
?? honest coward 
?? human spark ?? human architecture 
?? human masterpiece 
?? human incompetence 
?? legal slavery ?? legal corporation 
?? legal trading 
?? legal progress 
?? judicial verdict 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- ?? -------- -------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? invisible empire 
?? omnipotent realm 
?? finite realm 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- -------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- --------- 
?? -------- -------- 
?? -------- -------- 
Figure 5: Creative (blue bold) and not creative (red italic) word pairs graph.
7 Related Work
Among computational approaches that touch on lin-
guistic creativity, many focused on metaphor (e.g.,
Dunn (2013), Krishnakumaran and Zhu (2007),
Mashal et al (2007), Rumbell et al (2008), Ren-
toumi et al (2012), Mashal et al (2009)). Other lin-
guistic devices and phenomena related to creativity
include irony (e.g., Davidov et al (2010), Gonza?lez-
Iba?n?ez et al (2011), Filatova (2012)), neologism
(e.g., Cartoni (2008)), humor (e.g., Mihalcea and
Strapparava (2005), Purandare and Litman (2006)),
and similes (e.g., Hao and Veale (2010)).
Veale (2011) proposed the new task of creative
text retrieval to harvest expressions that potentially
convey the same meaning as the query phrase in
a fresh or unusual way. Our work contributes to
the retrieval process of recognizing more creative
phrases. Ozbal and Strapparava (2012) explored
automatic creative naming of commercial products
and services, focusing on the generation of creative
phrases within a specific domain. Costello (2002)
investigated the cognitive process that guides peo-
ple?s choice of words when making up a novel noun-
noun compound. In contrast, we present a data-
driven investigation to quantifying creativity in lex-
ical composition. Memorability is loosely related to
linguistic creativity (Danescu-Niculescu-Mizil et al
(2012)) as some of the creative quotes may be more
memorable, but not all creative phrases are memo-
rable and vice versa.
8 Conclusion
We presented the first study that focuses on learn-
ing and quantifying creativity in lexical composi-
tions, exploring statistical techniques motivated by
three different theories and hypotheses of creativ-
ity, ranging from divergent thinking, compositional
structure, creative semantic subspace, and the con-
nection to sentiment and connotation. Our experi-
mental results suggest the viability of learning cre-
ative language, and point to promising directions for
future research.
Acknowledgments This research was supported
in part by the Stony Brook University Office of the
Vice President for Research, and in part by gift from
Google. We thank anonymous reviewers for insight-
ful comments and suggestions.
References
T. Amabile. 1997. Motivating creativity in organiza-
tions: On doing what you love and loving what you
do. California Management Review, 40(1):39?58.
1255
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: representing
adjective-noun constructions in semantic space. In
Proceedings of the 2010 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1183?1193, Stroudsburg, PA, USA.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1. Google Inc.
Ronald Carter and Michael McCarthy. 2004. Talking,
creating: interactional language, creativity, and con-
text. Applied Linguistics, 25(1):62?88.
Bruno Cartoni. 2008. Lexical resources for automatic
translation of constructed neologisms: the case study
of relational adjectives. In LREC.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm:
a library for support vector machines. ACM Trans-
actions on Intelligent Systems and Technology (TIST),
2(3):27.
Yi-Wei Chen and Chih-Jen Lin. 2005. Combining svms
with various feature selection strategies. In Taiwan
University. Springer-Verlag.
Noam Chomsky. 1965. Aspects of the Theory of Syntax,
volume 11. The MIT press.
Carol Chomsky. 1976. Creativity and innovation in child
language. Journal of Education, Boston.
Daoud Clarke. 2012. A context-theoretic framework for
compositionality in distributional semantics. Compu-
tational Linguistics, 38(1):41?71.
W.M. Cohen and D.A. Levinthal. 1990. Absorptive Ca-
pacity: A New Perspective on Learning and Innova-
tion. Administrative Science Quarterly, 35(1).
Fintan J. Costello. 2002. Investigating creative language:
People?s choice of words in the production of novel
noun-noun compounds. In Proceedings of the 24th
Annual Conference of the Cognitive Science Society.
Arthur Cropley. 2006. In praise of convergent thinking.
Creativity Research Journal, 18(3):391?404.
Cristian Danescu-Niculescu-Mizil, Justin Cheng, Jon
Kleinberg, and Lillian Lee. 2012. You had me at
hello: How phrasing affects memorability. In Pro-
ceedings of the 50th Annual Meeting of the Association
for Computational Linguistics: Long Papers-Volume
1, pages 892?901. Association for Computational Lin-
guistics.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences in
twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, pages 107?116. Association for
Computational Linguistics.
Jonathan Dunn. 2013. What metaphor identifica-
tion systems can tell us about metaphor-in-language.
Meta4NLP 2013, page 1.
Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin
Choi. 2013. Connotation lexicon: A dash of senti-
ment beneath the surface meaning. In Proceedings of
the 51th Annual Meeting of the Association for Com-
putational Linguistics (Volume 2: Short Papers), Sofia,
Bulgaria, Angust. Association for Computational Lin-
guistics.
Elena Filatova. 2012. Irony and sarcasm: Corpus gen-
eration and analysis using crowdsourcing. In LREC,
pages 392?398.
Stefan L Frank. 2010. Uncertainty reduction as a mea-
sure of cognitive processing effort. In Proceedings of
the 2010 workshop on cognitive modeling and com-
putational linguistics, pages 81?89. Association for
Computational Linguistics.
Sigmund Freud. 1908. Creative writers and day-
dreaming. Standard edition, 9:143?153.
Susan R. Fussell and Mallie M. Moss. 1998. Figurative
language in descriptions of emotional states. In Social
and cognitive approaches to interpersonal communi-
cation.
Pablo Gerva?s. 2010. Engineering linguistic creativ-
ity: Bird flight and jet planes. In Proceedings of
the NAACL HLT 2010 Second Workshop on Computa-
tional Approaches to Linguistic Creativity, pages 23?
30. Association for Computational Linguistics.
Katie Glaskin. 2011. Dreams, memory, and the ances-
tors: creativity, culture, and the science of sleep. Jour-
nal of the royal anthropological institute, 17(1):44?62.
Andrew Goatly. 1997. The language of metaphors.
Routledge.
Roberto Gonza?lez-Iba?n?ez, Smaranda Muresan, and Nina
Wacholder. 2011. Identifying sarcasm in twitter: A
closer look. In ACL (Short Papers), pages 581?586.
Citeseer.
Emiliano Guevara. 2011. Computing semantic composi-
tionality in distributional semantics. In Proceedings of
the Ninth International Conference on Computational
Semantics (IWCS 2011), pages 135?144. Citeseer.
John Hale. 2003. The information conveyed by words
in sentences. Journal of Psycholinguistic Research,
32(2):101?123.
John Hale. 2006. Uncertainty about the rest of the sen-
tence. Cognitive Science, 30(4):643?672.
Yanfen Hao and Tony Veale. 2010. An ironic fist
in a velvet glove: Creative mis-representation in the
construction of ironic similes. Minds and Machines,
20(4):635?650.
Eric H. Huang, Richard Socher, Christopher D. Manning,
and Andrew Y. Ng. 2012. Improving Word Represen-
tations via Global Context and Multiple Word Proto-
types. In Annual Meeting of the Association for Com-
putational Linguistics (ACL).
1256
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources. In
Proceedings of the Workshop on Computational ap-
proaches to Figurative Language, pages 13?20. Asso-
ciation for Computational Linguistics.
George Lakoff and Mark Johnson. 1980. Metaphors we
Live by. University of Chicago Press, Chicago.
N. Mashal, M. Faust, T Hendler, and M. Jung-Beeman.
2007. An fmri investigation of the neural correlates
underlying the processing of novel metaphoric expres-
sions. Brain and Language, pages 115 ? 126.
N Mashal, M Faust, T Hendler, and M Jung-Beeman.
2009. An fmri study of processing novel metaphoric
sentences. Laterality, (1):30?54.
Janet Maybin and Joan Swann. 2007. Everyday creativ-
ity in language: Textuality, contextuality, and critique.
Applied Linguistics, 28(4):497?517.
Robert R McCrae. 1987. Creativity, divergent thinking,
and openness to experience. Journal of personality
and social psychology, 52(6):1258.
Susan M. McCurry and Steven C. Hayes. 1992. Clinical
and experimental perspectives on metaphorical talk.
Clinical Psychology Review, 12(7):763 ? 785.
Rada Mihalcea and Carlo Strapparava. 2005. Mak-
ing computers laugh: Investigations in automatic hu-
mor recognition. In Proceedings of Human Language
Technology Conference and Conference on Empirical
Methods in Natural Language Processing, pages 531?
538, Vancouver, British Columbia, Canada, October.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In In Proceedings of
ACL-08: HLT, pages 236?244.
Jeff Mitchell and Mirella Lapata. 2009. Language mod-
els based on semantic composition. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing: Volume 1-Volume 1, pages
430?439. Association for Computational Linguistics.
Brian Murphy, Partha Pratim Talukdar, and Tom
Mitchell. 2012. Learning effective and interpretable
semantic models using non-negative sparse embed-
ding. In COLING, pages 1933?1950.
Edward Necka. 1999. Memory and creativity. Ency-
clopedia of creativity, ed. by MA Runco, SR Pritzker,
2:193?99.
Gozde Ozbal and Carlo Strapparava. 2012. A compu-
tational approach to the automation of creative nam-
ing. In Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Volume 1:
Long Papers), pages 703?711, Jeju Island, Korea, July.
Association for Computational Linguistics.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79?86. Associa-
tion for Computational Linguistics.
J. B. Pollack. 1990. Recursive distributed representation.
Artificial Intelligence, 46:77?105.
Amruta Purandare and Diane Litman. 2006. Humor:
Prosody analysis and automatic recognition for f* r* i*
e* n* d* s*. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 208?215. Association for Computational Lin-
guistics.
Vassiliki Rentoumi, George A. Vouros, Vangelis
Karkaletsis, and Amalia Moser. 2012. Investigating
metaphorical language in sentiment analysis: A sense-
to-sentiment perspective. ACM Trans. Speech Lang.
Process., 9(3):6:1?6:31, November.
Sebastian Rudolph and Eugenie Giesbrecht. 2010. Com-
positional matrix-space models of language. In Pro-
ceedings of the 48th Annual Meeting of the Association
for Computational Linguistics, pages 907?916. Asso-
ciation for Computational Linguistics.
Tim Rumbell, John Barnden, Mark Lee, and Alan
Wallington. 2008. Affect in metaphor: Developments
with wordnet.
Bernhard Scho?lkopf and Alexander J Smola. 2002.
Learning with kernels. The MIT Press.
John Shawe-Taylor and Nello Cristianini. 2004. Kernel
methods for pattern analysis. Cambridge university
press.
Ekaterina Shutova. 2010. Models of metaphor in nlp.
In Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, ACL ?10,
pages 688?697, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y Ng. 2008. Cheap and fast?but is it good?:
evaluating non-expert annotations for natural language
tasks. In Proceedings of the conference on empirical
methods in natural language processing, pages 254?
263. Association for Computational Linguistics.
Richard Socher, Jeffrey Pennington, Eric H Huang, An-
drew Y Ng, and Christopher D Manning. 2011. Semi-
supervised recursive autoencoders for predicting sen-
timent distributions. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, pages 151?161. Association for Computational
Linguistics.
L.J.P. van der Maaten and G.E. Hinton. 2008. Visualiz-
ing high-dimensional data using t-sne.
Tony Veale, Yanfen Hao, and Guofu Li. 2008. Multilin-
gual harvesting of cross-cultural stereotypes. In ACL,
pages 523?531.
1257
Tony Veale. 2011. Creative language retrieval: A ro-
bust hybrid of information retrieval and linguistic cre-
ativity. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Hu-
man Language Technologies, pages 278?287, Port-
land, Oregon, USA, June. Association for Computa-
tional Linguistics.
Thomas Voegtlin and Peter F. Dominey. 2005. Linear
recursive distributed representations. Neural Netw.,
18(7):878?895, September.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Proceedings of the Sec-
ond AAAI Symposium on Quantum Interaction.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi, Claire
Cardie, Ellen Riloff, and Siddharth Patwardhan. 2005.
Opinionfinder: A system for subjectivity analysis. In
Proceedings of HLT/EMNLP on Interactive Demon-
strations, pages 34?35. Association for Computational
Linguistics.
Ainur Yessenalina and Claire Cardie. 2011. Composi-
tional matrix-space models for sentiment analysis. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, pages 172?182. As-
sociation for Computational Linguistics.
Xiaojin Zhu, Zhiting Xu, and Tushar Khot. 2009. How
creative is your writing? a linguistic creativity mea-
sure from computer science and cognitive psychology
perspectives. In Proceedings of the Workshop on Com-
putational Approaches to Linguistic Creativity, pages
87?93. Association for Computational Linguistics.
1258
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1443?1448,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Where Not to Eat? Improving Public Policy by Predicting Hygiene
Inspections Using Online Reviews
Jun Seok Kang? Polina Kuznetsova?
?Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{junkang,pkuznetsova,ychoi}
@cs.stonybrook.edu
Michael Luca? Yejin Choi?
?Harvard Business School
Soldiers Field Road
Boston, MA 02163
mluca@hbs.edu
Abstract
This paper offers an approach for governments
to harness the information contained in social
media in order to make public inspections and
disclosure more efficient. As a case study, we
turn to restaurant hygiene inspections ? which
are done for restaurants throughout the United
States and in most of the world and are a fre-
quently cited example of public inspections
and disclosure. We present the first empiri-
cal study that shows the viability of statistical
models that learn the mapping between tex-
tual signals in restaurant reviews and the hy-
giene inspection records from the Department
of Public Health. The learned model achieves
over 82% accuracy in discriminating severe
offenders from places with no violation, and
provides insights into salient cues in reviews
that are indicative of the restaurant?s sanitary
conditions. Our study suggests that public
disclosure policy can be improved by mining
public opinions from social media to target in-
spections and to provide alternative forms of
disclosure to customers.
1 Introduction
Public health inspection records help customers to
be wary of restaurants that have violated health
codes. In some counties and cities, e.g., LA, NYC,
it is required for restaurants to post their inspec-
tion grades at their premises, which have shown
to affect the revenue of the business substantially
(e.g., Jin and Leslie (2005), Henson et al (2006)),
thereby motivating restaurants to improve their sani-
tary practice. Other studies have reported correlation
between the frequency of unannounced inspections
per year, and the average violation scores, confirm-
ing the regulatory role of inspections in improving
the hygiene quality of the restaurants and decreasing
food-borne illness risks (e.g., Jin and Leslie (2003),
Jin and Leslie (2009), Filion and Powell (2009),
NYC-DoHMH (2012)).
However, one practical challenge in the current
inspection system is that the department of health
has only limited resources to dispatch inspectors,
leaving out a large number of restaurants with un-
known hygiene grades. We postulate that online re-
views written by the very citizens who have visited
those restaurants can serve as a proxy for predicting
the likely outcome of the health inspection of any
given restaurant. Such a prediction model can com-
plement the current inspection system by enlight-
ening the department of health to make a more in-
formed decision when allocating inspectors, and by
guiding customers when choosing restaurants.
Our work shares the spirit of recently emerging
studies that explores social media analysis for pub-
lic health surveillance, in particular, monitoring in-
fluenza or food-poisoning outbreaks from micro-
blogs (e.g., Aramaki et al (2011), Sadilek et al
(2012b), Sadilek et al (2012a), Sadilek et al (2013),
Lamb et al (2013), Dredze et al (2013), von Etter
et al (2010)). However, no prior work has examined
the utility of review analysis as a predictive tool for
accessing hygiene of restaurants, perhaps because
the connection is not entirely conspicuous: after all,
customers are neither familiar with inspection codes,
nor have the full access to the kitchen, nor have been
asked to report on the hygiene aspects of their expe-
1443
review count*review count (filtered)*
Coe
fficie
nt
0.05
0.10
(a)0 10 20 30 40 50
np review count*np review count (filtered)*
  
0.05
0.10
(b)0 10 20 30 40 50
avg review rating*avg review rating (filtered)*
Coe
fficie
nt
?0.05
?0.03
(c)0 10 20 30 40 50
avg review length*avg review length(filtered)*
  
0
0.05
(d)0 10 20 30 40 50Inspection Penalty Score Threshold
Figure 1: Spearman?s coefficients of factors & inspection
penalty scores. ?*?: statistically significant (p ? 0.05)
rience.
In this work, we report the first empirical study
demonstrating the utility of review analysis for pre-
dicting health inspections, achieving over 82% accu-
racy in discriminating severe offenders from places
with no violation, and find predictive cues in reviews
that correlate with the inspection results.
2 Data
We scraped entire reviews written for restaurants in
Seattle from Yelp over the period of 2006 to 2013.1
The inspection records of Seattle is publicly avail-
able at www.datakc.org. More than 50% of the
restaurants listed under Yelp did not have inspection
records, implying the limited coverage of inspec-
tions. We converted street addresses into canonical
forms when matching restaurants between Yelp and
inspection database. After integrating reviews with
inspection records, we obtained about 13k inspec-
1Available at http://www.cs.stonybrook.edu/
?junkang/hygiene/
bimodality*bimodality (filtered)*
Coe
fficie
nt
0
0.05
0.10
(a)0 10 20 30 40 50
fake review count*fake review count (filtered)
   
0
0.05
0.10
(b)0 10 20 30 40 50
Inspection Penalty Score Threshold
Figure 2: Spearman?s coefficients of factors & inspection
penalty scores. ?*?: statistically significant (p ? 0.05)
tions over 1,756 restaurants with 152k reviews. For
each restaurant, there are typically several inspec-
tion records. We defined an ?inspection period? of
each inspection record as the period of time start-
ing from the day after the previous inspection to the
day of the current inspection. If there is no previ-
ous inspection, then the period stretches to the past
6 months in time. Each inspection period corre-
sponds to an instance in the training or test set. We
merge all reviews within an inspection period into
one document when creating the feature vector.
Note that non-zero penalty scores may not nec-
essarily indicate alarming hygiene issues. For ex-
ample, violating codes such as ?proper labeling? or
?proper consumer advisory posted for raw or under-
cooked foods? seem relatively minor, and unlikely to
be noted and mentioned by reviewers. Therefore, we
focus on restaurants with severe violations, as they
are exactly the set of restaurants that inspectors and
customers need to pay the most attention to. To de-
fine restaurants with ?severe violations? we experi-
ment with a varying threshold t, such that restaurants
with score ? t are labeled as ?unhygienic?.2
3 Correlates of Inspection Penalty Scores
We examine correlation between penalty scores and
several statistics of reviews:
I. Volume of Reviews:
2For restaurants with ?hygienic? labels, we only consider
those without violation, as there are enough number of such
restaurants to keep balanced distribution between two classes.
1444
61.42 61.46 66.61
70.83 77.16
81.37
Acc
urac
y (%
)
60
80
Inspection Penalty Score Threshold
0 10 20 30 40 50
Figure 3: Trend of penalty score thresholds & accuracies.
? count of all reviews
? average length of all reviews
II. Sentiment of Reviews: We examine whether
the overall sentiment of the customers correlates
with the hygiene of the restaurants based on follow-
ing measures:
? average review rating
? count of negative (? 3) reviews
III. Deceptiveness of Reviews: Restaurants with
bad hygiene status are more likely to attract negative
reviews, which would then motivate the restaurants
to solicit fake reviews. But it is also possible that
some of the most assiduous restaurants that abide
by health codes strictly are also diligent in solicit-
ing fake positive reviews. We therefore examine the
correlation between hygiene violations and the de-
gree of deception as follows.
? bimodal distribution of review ratings
The work of Feng et al (2012) has shown
that the shape of the distribution of opinions,
overtly skewed bimodal distributions in partic-
ular, can be a telltale sign of deceptive review-
ing activities. We approximately measure this
by computing the variance of review ratings.
? volume of deceptive reviews based on linguistic
patterns
We also explore the use of deception classifiers
based on linguistic patterns (Ott et al, 2011)
to measure the degree of deception. Since no
deception corpus is available in the restaurant
domain, we collected a set of fake reviews and
truthful reviews (250 reviews for each class),
following Ott et al (2011).3
310 fold cross validation on this dataset yields 79.2% accu-
racy based on unigram and bigram features.
Features Acc. MSE SCC
- *50.00 0.500 -
review count *50.00 0.489 0.0005
np review count *52.94 0.522 0.0017
cuisine *66.18 0.227 0.1530
zip code *67.32 0.209 0.1669
avrg. rating *57.52 0.248 0.0091
inspection history *72.22 0.202 0.1961
unigram 78.43 0.461 0.1027
bigram *76.63 0.476 0.0523
unigram + bigram 82.68 0.442 0.0979
all 81.37 0.190 0.2642
Table 1: Feature Compositions & Respective Accuracies,
Respective Mean Squared Errors(MSE) & Squared Cor-
relation Coefficients (SCC), np=non-positive
Filtering Reviews: When computing above statis-
tics over the set of reviews corresponding to each
restaurant, we also consider removing a subset of re-
views that might be dubious or just noise. In partic-
ular, we remove reviews that are too far away (delta
? 2) from the average review rating. Another filter-
ing rule can be removing all reviews that are clas-
sified as deceptive by the deception classifier ex-
plained above. For brevity, we only show results
based on the first filtering rule, as we did not find
notable differences in different filtering strategies.
Results: Fig 1 and 2 show Spearman?s rank corre-
lation coefficient with respect to the statistics listed
above, with and without filtering, computed at dif-
ferent threshold cutoffs ? {0, 10, 20, 30, 40, 50} of
inspection scores. Although coefficients are not
strong,4 they are mostly statistically significant with
p ? 0.05 (marked with ?*?), and show interesting
contrastive trends as highlighted below.
In Fig 1, as expected, average review rating is neg-
atively correlated with the inspection penalty scores.
Interestingly, all three statistics corresponding to the
volume of customer reviews are positively corre-
lated with inspection penalty. What is more inter-
esting is that if potentially deceptive reviews are fil-
tered, then the correlation gets stronger, which sug-
gests the existence of deceptive reviews covering up
unhappy customers. Also notice that correlation is
4Spearman?s coefficient assumes monotonic correlation. We
suspect that the actual correlation of these factors and inspection
scores are not entirely monotonic.
1445
Hygienic gross, mess, sticky, smell, restroom, dirty
Basic Ingredients: beef, pork, noodle, egg, soy,
ramen, pho,
Cuisines Vietnamese, Dim Sum, Thai, Mexican,
Japanese, Chinese, American, Pizza, Sushi, Indian,
Italian, Asian
Sentiment: cheap, never,
Service & Atmosphere cash, worth, district, delivery,
think, really, thing, parking, always, usually, definitely
- door: ?The wait is always out the door when I
actually want to go there?,
- sticker: ?I had sticker shock when I saw the prices.?,
- student: ?heap, large portions and tasty = the perfect
student food!?,
- the size: ?i was pretty astonished at the size of all the
plates for the money.?,
- was dry: ?The beef was dry, the sweet soy and
anise-like sauce was TOO salty (almost inedible).?,
- pool: ?There are pool tables, TV airing soccer games
from around the globe and of course - great drinks!?
Table 2: Lexical Cues & Examples - Unhygienic (dirty)
generally stronger when higher cutoffs are used (x-
axis), as expected. Fig 2 looks at the relation be-
tween the deception level and the inspection scores
more directly. As suspected, restaurants with high
penalty scores show increased level of deceptive re-
views.
Although various correlates of hygiene scores ex-
amined so far are insightful, these alone are not in-
formative enough to be used as a predictive tool,
hence we explore content-based classification next.
4 Content-based Prediction
We examine the utility of the following features:
Features based on customers? opinion:
1. Aggregated opinion: average review rating
2. Content of the reviews: unigram, bigram
Features based on restaurant?s metadata:
3. Cuisine: e.g., Thai, Italian, as listed under Yelp
4. Location: first 5 digits of zip code
5. Inspection History: a boolean feature (?hy-
gienic? or ?unhygienic?), a numerical feature
(previous penalty score rescaled ? [0, 1]), a nu-
meric feature (average penalty score over all
previous inspections)
Hygienic:
Cooking Method & Garnish: brew, frosting, grill,
crush, crust, taco, burrito, toast
Healthy or Fancier Ingredients: celery, calamity,
wine, broccoli, salad, flatbread, olive, pesto
Cuisines : Breakfast, Fish & Chips, Fast Food,
German, Diner, Belgian, European, Sandwiches,
Vegetarian
Whom & When: date, weekend, our, husband,
evening, night
Sentiment: lovely, yummy, generous, friendly, great,
nice
Service & Atmosphere: selection, attitude,
atmosphere, ambiance, pretentious
Table 3: Lexical Cues & Examples - Hygienic (clean)
6. Review Count
7. Non-positive Review Count
Classification Results We use liblinear?s SVM
(Fan et al, 2008) with L1 regularization and 10 fold
cross validation. We filter reviews that are farther
than 2 from the average rating. We also run Sup-
port Vector Regression (SVR) using liblinear. Fig 3
shows the results. As we increase the threshold, the
accuracy also goes up in most cases. Table 1 shows
feature ablation at threshold t = 50, and ?*? denotes
statistically significant (p?0.05) difference over the
performance with all features based on student t-test.
We find that metadata information of restaurants
such as location and cuisine alone show good predic-
tive power, both above 66%, which are significantly
higher than the expected accuracy of random guess-
ing (50%).
Somewhat unexpected outcome is aggregated
opinion, which is the average review rating during
the corresponding inspection period, as it performs
not much better than chance (57.52%). This result
suggest that the task of hygiene prediction from re-
views differs from the task of sentiment classifica-
tion of reviews.
Interestingly, the inspection history feature alone
is highly informative, reaching accuracy upto 72%,
suggesting that the past performance is a good pre-
dictor of the future performance.
Textual content of the reviews (unigram+bigram)
turns out to be the most effective features, reaching
upto 82.68% accuracy. Lastly, when all the features
1446
are combined together, the performance decreases
slightly to 81.37%, perhaps because n-gram features
perform drastically better than all others.
4.1 Insightful Cues
Table 2 and 3 shows representative lexical cues for
each class with example sentences excerpted from
actual reviews when context can be helpful.
Hygiene: Interestingly, hygiene related words are
overwhelmingly negative, e.g., ?gross?, ?mess?,
?sticky?. What this suggests is that reviewers do
complain when the restaurants are noticeably dirty,
but do not seem to feel the need to complement on
cleanliness as often. Instead, they seem to focus on
other positive aspects of their experience, e.g., de-
tails of food, atmosphere, and their social occasions.
Service and Atmosphere: Discriminative fea-
tures reveal that it is not just the hygiene related
words that are predictive of the inspection results of
restaurants. It turns out that there are other quali-
ties of restaurants, such as service and atmosphere,
that also correlate with the likely outcome of inspec-
tions. For example, when reviewers feel the need
to talk about ?door?, ?student?, ?sticker?, or ?the
size? (see Table 2 and 3), one can extrapolate that
the overall experience probably was not glorious. In
contrast, words such as ?selection?, ?atmosphere?,
?ambiance? are predictive of hygienic restaurants,
even including those with slightly negative connota-
tion such as ?attitude? or ?pretentious?.
Whom and When: If reviewers talk about details
of their social occasions such as ?date?, ?husband?,
it seems to be a good sign.
The way food items are described: Another in-
teresting aspect of discriminative words are the way
food items are described by reviewers. In general,
mentions of basic ingredients of dishes, e.g., ?noo-
dle?, ?egg?, ?soy? do not seem like a good sign. In
contrast, words that help describing the way dish is
prepared or decorated, e.g., ?grill?, ?toast?, ?frost-
ing?, ?bento box? ?sugar? (as in ?sugar coated?)
are good signs of satisfied customers.
Cuisines: Finally, cuisines have clear correlations
with inspection outcome, as shown in Table 2 and 3.
5 Related Work
There have been several recent studies that probe the
viability of public health surveillance by measuring
relevant textual signals in social media, in particu-
lar, micro-blogs (e.g., Aramaki et al (2011), Sadilek
et al (2012b), Sadilek et al (2012a), Sadilek et al
(2013), Lamb et al (2013), Dredze et al (2013), von
Etter et al (2010)). Our work joins this line of re-
search but differs in two distinct ways. First, most
prior work aims to monitor a specific illness, e.g.,
influenza or food-poisoning by paying attention to
a relatively small set of keywords that are directly
relevant to the corresponding sickness. In contrast,
we examine all words people use in online reviews,
and draw insights on correlating terms and concepts
that may not seem immediately relevant to the hy-
giene status of restaurants, but nonetheless are pre-
dictive of the outcome of the inspections. Second,
our work is the first to examine online reviews in the
context of improving public policy, suggesting addi-
tional source of information for public policy mak-
ers to pay attention to.
Our work draws from the rich body of research
that studies online reviews for sentiment analysis
(e.g., Pang and Lee (2008)) and deception detec-
tion (e.g., Mihalcea and Strapparava (2009), Ott et
al. (2011), Feng et al (2012)), while introducing
the new task of public hygiene prediction. We ex-
pect that previous studies for aspect-based sentiment
analysis (e.g., Titov and McDonald (2008), Brody
and Elhadad (2010), Wang et al (2010)) would be a
fruitful venue for further investigation.
6 Conclusion
We have reported the first empirical study demon-
strating the promise of review analysis for predicting
health inspections, introducing a task that has poten-
tially significant societal benefits, while being rele-
vant to much research in NLP for opinion analysis
based on customer reviews.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research, and in part by gift from Google. We thank
anonymous reviewers and Adam Sadilek for helpful
comments and suggestions.
1447
References
Eiji Aramaki, Sachiko Maskawa, and Mizuki Morita.
2011. Twitter catches the flu: Detecting influenza epi-
demics using twitter. In Proceedings of the 2011 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1568?1576, Edinburgh, Scotland,
UK., July. Association for Computational Linguistics.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, HLT ?10,
pages 804?812, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Mark Dredze, Michael J. Paul, Shane Bergsma, and Hieu
Tran. 2013. Carmen: A twitter geolocation system
with applications to public health. In AAAI Workshop
on Expanding the Boundaries of Health Informatics
Using AI (HIAI).
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Song Feng, Longfei Xing, Anupam Gogar, and Yejin
Choi. 2012. Distributional footprints of deceptive
product reviews. In ICWSM.
Katie Filion and Douglas A Powell. 2009. The use of
restaurant inspection disclosure systems as a means of
communicating food safety information. Journal of
Foodservice, 20(6):287?297.
Spencer Henson, Shannon Majowicz, Oliver Masakure,
Paul Sockett, Anria Johnes, Robert Hart, Debora Carr,
and Lewinda Knowles. 2006. Consumer assessment
of the safety of restaurants: The role of inspection
notices and other information cues. Journal of Food
Safety, 26(4):275?301.
Ginger Zhe Jin and Phillip Leslie. 2003. The effect of
information on product quality: Evidence from restau-
rant hygiene grade cards. The Quarterly Journal of
Economics, 118(2):409?451.
Ginger Zhe Jin and Phillip Leslie. 2005. The case in
support of restaurant hygiene grade cards.
Ginger Zhe Jin and Phillip Leslie. 2009. Reputational
incentives for restaurant hygiene. American Economic
Journal: Microeconomics, pages 237?267.
Alex Lamb, Michael J. Paul, and Mark Dredze. 2013.
Separating fact from fear: Tracking flu infections on
twitter. In the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies (NAACL-HLT).
Rada Mihalcea and Carlo Strapparava. 2009. The lie
detector: Explorations in the automatic recognition
of deceptive language. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, pages 309?
312, Suntec, Singapore, August. Association for Com-
putational Linguistics.
NYC-DoHMH. 2012. Restaurant grading in new york
city at 18 months. New York City Department of
Health and Mental Hygiene.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
309?319, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and trends in infor-
mation retrieval, 2(1-2):1?135.
Adam Sadilek, Henry Kautz, and Vincent Silenzio.
2012a. Predicting disease transmission from geo-
tagged micro-blog data. In Twenty-Sixth AAAI Con-
ference on Artificial Intelligence.
Adam Sadilek, Henry A. Kautz, and Vincent Silenzio.
2012b. Modeling spread of disease from social in-
teractions. In John G. Breslin, Nicole B. Ellison,
James G. Shanahan, and Zeynep Tufekci, editors,
ICWSM. The AAAI Press.
Adam Sadilek, Sean Brennan, Henry Kautz, and Vincent
Silenzio. 2013. nemesis: Which restaurants should
you avoid today? First AAAI Conference on Human
Computation and Crowdsourcing.
Ivan Titov and Ryan McDonald. 2008. A joint model
of text and aspect ratings for sentiment summariza-
tion. In Proceedings of ACL-08: HLT, pages 308?316,
Columbus, Ohio, June. Association for Computational
Linguistics.
Peter von Etter, Silja Huttunen, Arto Vihavainen, Matti
Vuorinen, and Roman Yangarber. 2010. Assess-
ment of utility in web mining for the domain of pub-
lic health. In Proceedings of the NAACL HLT 2010
Second Louhi Workshop on Text and Data Mining of
Health Documents, pages 29?37, Los Angeles, Cal-
ifornia, USA, June. Association for Computational
Linguistics.
Hongning Wang, Yue Lu, and Chengxiang Zhai. 2010.
Latent aspect rating analysis on review text data: a rat-
ing regression approach. In Proceedings of the 16th
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 783?792.
ACM.
1448
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 359?368,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Collective Generation of Natural Image Descriptions
Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,
Tamara L. Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
We present a holistic data-driven approach
to image description generation, exploit-
ing the vast amount of (noisy) parallel im-
age data and associated natural language
descriptions available on the web. More
specifically, given a query image, we re-
trieve existing human-composed phrases
used to describe visually similar images,
then selectively combine those phrases
to generate a novel description for the
query image. We cast the generation pro-
cess as constraint optimization problems,
collectively incorporating multiple inter-
connected aspects of language composition
for content planning, surface realization
and discourse structure. Evaluation by hu-
man annotators indicates that our final
system generates more semantically cor-
rect and linguistically appealing descrip-
tions than two nontrivial baselines.
1 Introduction
Automatically describing images in natural lan-
guage is an intriguing, but complex AI task, re-
quiring accurate computational visual recogni-
tion, comprehensive world knowledge, and natu-
ral language generation. Some past research has
simplified the general image description goal by
assuming that relevant text for an image is pro-
vided (e.g., Aker and Gaizauskas (2010), Feng
and Lapata (2010)). This allows descriptions to
be generated using effective summarization tech-
niques with relatively surface level image under-
standing. However, such text (e.g., news articles
or encyclopedic text) is often only loosely related
to an image?s specific content and many natu-
ral images do not come with associated text for
summarization.
In contrast, other recent work has focused
more on the visual recognition aspect by de-
tecting content elements (e.g., scenes, objects,
attributes, actions, etc) and then composing de-
scriptions from scratch (e.g., Yao et al (2010),
Kulkarni et al (2011), Yang et al (2011), Li
et al (2011)), or by retrieving existing whole
descriptions from visually similar images (e.g.,
Farhadi et al (2010), Ordonez et al (2011)). For
the latter approaches, it is unrealistic to expect
that there will always exist a single complete de-
scription for retrieval that is pertinent to a given
query image. For the former approaches, visual
recognition first generates an intermediate rep-
resentation of image content using a set of En-
glish words, then language generation constructs
a full description by adding function words and
optionally applying simple re-ordering. Because
the generation process sticks relatively closely
to the recognized content, the resulting descrip-
tions often lack the kind of coverage, creativ-
ity, and complexity typically found in human-
written text.
In this paper, we propose a holistic data-
driven approach that combines and extends the
best aspects of these previous approaches ? a)
using visual recognition to directly predict indi-
vidual image content elements, and b) using re-
trieval from existing human-composed descrip-
tions to generate natural, creative, and inter-
359
esting captions. We also lift the restriction of
retrieving existing whole descriptions by gather-
ing visually relevant phrases which we combine
to produce novel and query-image specific de-
scriptions. By judiciously exploiting the corre-
spondence between image content elements and
phrases, it is possible to generate natural lan-
guage descriptions that are substantially richer
in content and more linguistically interesting
than previous work.
At a high level, our approach can be moti-
vated by linguistic theories about the connection
between reading activities and writing skills,
i.e., substantial reading enriches writing skills,
(e.g., Hafiz and Tudor (1989), Tsang (1996)).
Analogously, our generation algorithm attains a
higher level of linguistic sophistication by read-
ing large amounts of descriptive text available
online. Our approach is also motivated by lan-
guage grounding by visual worlds (e.g., Roy
(2002), Dindo and Zambuto (2010), Monner and
Reggia (2011)), as in our approach the mean-
ing of a phrase in a description is implicitly
grounded by the relevant content of the image.
Another important thrust of this work is col-
lective image-level content-planning, integrating
saliency, content relations, and discourse struc-
ture based on statistics drawn from a large
image-text parallel corpus. This contrasts with
previous approaches that generate multiple sen-
tences without considering discourse flow or re-
dundancy (e.g., Li et al (2011)). For example,
for an image showing a flock of birds, generating
a large number of sentences stating the relative
position of each bird is probably not useful.
Content planning and phrase synthesis can
be naturally viewed as constraint optimization
problems. We employ Integer Linear Program-
ming (ILP) as an optimization framework that
has been used successfully in other generation
tasks (e.g., Clarke and Lapata (2006), Mar-
tins and Smith (2009), Woodsend and Lapata
(2010)). Our ILP formulation encodes a rich
set of linguistically motivated constraints and
weights that incorporate multiple aspects of the
generation process. Empirical results demon-
strate that our final system generates linguisti-
cally more appealing and semantically more cor-
rect descriptions than two nontrivial baselines.
1.1 System Overview
Our system consists of two parts. For a query
image, we first retrieve candidate descriptive
phrases from a large image-caption database us-
ing measures of visual similarity (?2). We then
generate a coherent description from these can-
didates using ILP formulations for content plan-
ning (?4) and surface realization (?5).
2 Vision & Phrase Retrieval
For a query image, we retrieve relevant candi-
date natural language phrases by visually com-
paring the query image to database images from
the SBU Captioned Photo Collection (Ordonez
et al, 2011) (1 million photographs with asso-
ciated human-composed descriptions). Visual
similarity for several kinds of image content are
used to compare the query image to images from
the database, including: 1) object detections for
89 common object categories (Felzenszwalb et
al., 2010), 2) scene classifications for 26 com-
mon scene categories (Xiao et al, 2010), and
3) region based detections for stuff categories
(e.g. grass, road, sky) (Ordonez et al, 2011).
All content types are pre-computed on the mil-
lion database photos, and caption parsing is per-
formed using the Berkeley PCFG parser (Petrov
et al, 2006; Petrov and Klein, 2007).
Given a query image, we identify content el-
ements present using the above classifiers and
detectors and then retrieve phrases referring to
those content elements from the database. For
example, if we detect a horse in a query im-
age, then we retrieve phrases referring to vi-
sually similar horses in the database by com-
paring the color, texture (Leung and Malik,
1999), or shape (Dalal and Triggs, 2005; Lowe,
2004) of the detected horse to detected horses
in the database images. We collect four types of
phrases for each query image as follows:
[1] NPs We retrieve noun phrases for each
query object detection (e.g., ?the brown cow?)
from database captions using visual similar-
ity between object detections computed as an
equally weighted linear combination of L2 dis-
360
tances on histograms of color, texton (Leung and
Malik, 1999), HoG (Dalal and Triggs, 2005) and
SIFT (Lowe, 2004) features.
[2] VPs We retrieve verb phrases for each
query object detection (e.g. ?boy running?)
from database captions using the same mea-
sure of visual similarity as for NPs, but restrict-
ing the search to only those database instances
whose captions contain a verb phrase referring
to the object category.
[3] Region/Stuff PPs We collect preposi-
tional phrases for each query stuff detection (e.g.
?in the sky?, ?on the road?) by measuring visual
similarity of appearance (color, texton, HoG)
and geometric configuration (object-stuff rela-
tive location and distance) between query and
database detections.
[4] Scene PPs We also collect prepositonal
phrases referring to general image scene context
(e.g. ?at the market?, ?on hot summer days?,
?in Sweden?) based on global scene similarity
computed using L2 distance between scene clas-
sification score vectors (Xiao et al, 2010) com-
puted on the query and database images.
3 Overview of ILP Formulation
For each image, we aim to generate multiple
sentences, each sentence corresponding to a sin-
gle distinct object detected in the given image.
Each sentence comprises of the NP for the main
object, and a subset of the corresponding VP,
region/stuff PP, and scene PP retrieved in ?2.
We consider four different types of operations
to generate the final description for each image:
T1. Selecting the set of objects to describe (one
object per sentence).
T2. Re-ordering sentences (i.e., re-ordering ob-
jects).
T3. Selecting the set of phrases for each sen-
tence.
T4. Re-ordering phrases within each sentence.
The ILP formulation of ?4 addresses T1 & T2,
i.e., content-planning, and the ILP of ?5 ad-
dresses T3 & T4, i.e., surface realization.1
1It is possible to create one conjoined ILP formulation
to address all four operations T1?T4 at once. For com-
4 Image-level Content Planning
First we describe image-level content planning,
i.e., abstract generation. The goals are to (1) se-
lect a subset of the objects based on saliency and
semantically compatibility, and (2) order the se-
lected objects based on their content relations.
4.1 Variables and Objective Function
The following set of indicator variables encodes
the selection of objects and ordering:
ysk =
?
?
?
1, if object s is selected
for position k
0, otherwise
(1)
where k = 1, ..., S encodes the position (order)
of the selected objects, and s indexes one of the
objects. In addition, we define a set of variables
indicating specific pairs of adjacent objects:
yskt(k+1) =
{
1, if ysk = yt(k+1) = 1
0, otherwise
(2)
The objective function, F , that we will maxi-
mize is a weighted linear combination of these
indicator variables and can be optimized using
integer linear programming:
F =
?
s
Fs ?
S?
k=1
ysk ?
?
st
Fst ?
S?1?
k=1
yskt(k+1) (3)
where Fs quantifies the salience/confidence of
the object s, and Fst quantifies the seman-
tic compatibility between the objects s and t.
These coefficients (weights) will be described in
?4.3 and ?4.4. We use IBM CPLEX to optimize
this objective function subject to the constraints
introduced next in ?4.2.
4.2 Constraints
Consistency Constraints: We enforce consis-
tency between indicator variables for indivisual
objects (Eq. 1) and consecutive objects (Eq. 2)
so that yskt(k+1) = 1 iff ysk = 1 and yt(k+1) = 1:
?stk, yskt(k+1) ? ysk (4)
yskt(k+1) ? yt(k+1) (5)
yskt(k+1) + (1? ysk) + (1? yt(k+1)) ? 1 (6)
putational and implementation efficiency however, we opt
for the two-step approach.
361
To avoid empty descriptions, we enforce that the
result includes at least one object:
?
s
ys1 = 1 (7)
To enforce contiguous positions be selected:
?k = 2, ..., S ? 1,
?
s
ys(k+1) ?
?
s
ysk (8)
Discourse constraints: To avoid spurious de-
scriptions, we allow at most two objects of the
same type, where cs is the type of object s:
?c ? objTypes,
?
{s: cs=c}
S?
k=1
ysk ? 2 (9)
4.3 Weight Fs: Object Detection
Confidence
In order to quantify the confidence of the object
detector for the object s, we define 0 ? Fs ? 1
as the mean of the detector scores for that object
type in the image.
4.4 Weight Fst: Ordering and
Compatibility
The weight 0 ? Fst ? 1 quantifies the compat-
ibility of the object pairing (s, t). Note that in
the objective function, we subtract this quan-
tity from the function to be maximized. This
way, we create a competing tension between the
single object selection scores and the pairwise
compatibility scores, so that variable number of
objects can be selected.
Object Ordering Statistics: People have bi-
ases on the order of topic or content flow. We
measure these biases by collecting statistics on
ordering of object names from the 1 million im-
age descriptions in the SBU Captioned Dataset
(Ordonez et al, 2011). Let ford(w1, w2) be
the number of times w1 appeared before w2.
For instance, ford(window, house) = 2895 and
ford(house, window) = 1250, suggesting that
people are more likely to mention a window be-
fore mentioning a house/building2. We use these
ordering statistics to enhance content flow. We
define score for the order of objects using Z-score
for normalization as follows:
F?st =
ford(cs, ct)?mean(ford)
std dev(ford)
(10)
2We take into account synonyms.
We then transform F?st so that F?st ? [0,1], and
then set Fst = 1 ? F?st so that smaller values
correspond to better choices.
5 Surface Realization
Recall that for each image, the computer vi-
sion system identifies phrases from descriptions
of images that are similar in a variety of aspects.
The result is a set of phrases representing four
different types of information (?2). From this
assortment of phrases, we aim to select a subset
and glue them together to compose a complete
sentence that is linguistically plausible and se-
mantically truthful to the content of the image.
5.1 Variables and Objective Function
The following set of variables encodes the selec-
tion of phrases and their ordering in construct-
ing S? sentences.
xsijk =
?
?????
?????
1, if phrase i of type j
is selected
for position k
in sentence s
0, otherwise
(11)
where k = 1, ..., N encodes the ordering of the
selected phrases, and j indexes one of the four
phrases types (object-NPs, action-VPs, region-
PPs, scene-PPs), i = 1, ...,M indexes one of
the M candidate phrases of each phrase type,
and s = 1, ..., S? encodes the sentence (object).
In addition, we define indicator variables for
adjacent pairs of phrases: xsijkpq(k+1) = 1 if
xsijk = xspq(k+1) = 1 and 0 otherwise. Finally,
we define the objective function F as:
F =
?
sij
Fsij ?
N?
k=1
xsijk
?
?
sijpq
Fsijpq ?
N?1?
k=1
xsijkpq(k+1) (12)
where Fsij weights individual phrase goodness
and Fsijpq adjacent phrase goodness. All coeffi-
cients (weights) will be described in Section 5.3
and 5.4.
We optionally prepend the first sentence in a
generated description with a cognitive phrase.3
3We collect most frequent 200 phrases of length 1-
7 that start a caption from the SBU Captioned Photo
Collection.
362
ILP: I think this is a boy?s bike 
lied in saltwater for quite a 
while. 
 
HMM: I liked the way bicycles 
leaning against a wall in 
Copenhagen Denmark in a 
windy sky in a Singapore 
bathroom. Boy?s bike lied in 
saltwater for quite a while in a 
windy sky in a Singapore 
bathroom. Fruit rubbing his 
face in the encrusted snow in a 
windy sky in a Singapore 
bathroom. 
 
Human: You re nobody in 
Oxford, unless you have a old 
bike with a basket 
 
ILP:  
This is a photo of this little flower sprouted up in defiance against grass.  
Bright yellow flowers growing in a rock garden at Volcan Mombacho. 
 
HMM: These was taken on the flowers growing in a rock garden in the field in two sorts. This 
little flower sprouted up in defiance in the field in two sorts. A full open flower sprouted up in 
defiance in the field in gardens. Bright yellow flowers growing in a rock garden in the field. 
 
Human: Yellow flower in my field 
 
ILP: Found trucks parked on first avenue in the east village. 
 
HMM: This is the first cellar door left back bedroom in center 
and clothes dryer to the right to the building in the house. 
This HUGE screen hanging on the wall outside a burned down 
building in the house. My truck parked on first avenue in the 
east village by the glass buildings in the house. 
 
Human: Flat bed Chisholms truck on display at the vintage 
vehicle rall y at Astley Green Colliery near Leigh Lancs 
 
Figure 1: ILP & HMM generated captions. In HMM generated captions, underlined phrases show redundancy
across different objects (due to lack of discourse constraints), and phrases in boldface show awkward topic
flow (due to lack of content planning). Note that in the bicycle image, the visual recognizer detected two
separate bicycles and some fruits, as can be seen in the HMM result. Via collective image-level content
planning (see ?4), some of these erroneous detection can be corrected, as shown in the ILP result. Spurious
and redundant phrases can be suppressed via discourse constraints (see ?5).
These are generic constructs that are often used
to start a description about an image, for in-
stance, ?This is an image of...?. We treat these
phrases as an additional type, but omit corre-
sponding variables and constraints for brevity.
5.2 Constraints
Consistency Constraints: First we enforce
consistency between the unary variables (Eq.
11) and the pairwise variables so that xsijkpqm =
1 iff xsijk = 1 and xspqm = 1:
?ijkpqm, xsijkpqm ? xsijk (13)
xsijkpqm ? xspqm (14)
xsijkpqm + (1? xsijk) + (1? xspqm) ? 1 (15)
Next we include constraints similar to Eq. 8
(contiguous slots are filled), but omit them for
brevity. Finally, we add constraints to ensure at
least two phrases are selected for each sentence,
to promote informative descriptions.
Linguistic constraints: We include linguisti-
cally motivated constraints to generate syntacti-
cally and semantically plausible sentences. First
we enforce a noun-phrase to be selected to en-
sure semantic relevance to the image:
?s,
?
ik
xsiNPk = 1 (16)
Also, to avoid content redundancy, we allow at
most one phrase of each type:
?sj,
?
i
N?
k=1
xsijk ? 1 (17)
Discourse constraints: We allow at most
one prepositional scene phrase for the whole de-
scription to avoid redundancy:
For j = PPscene,
?
sik
xsijk ? 1 (18)
We add constraints that prevent the inclusion of
more than one phrase with identical head words:
?s, ij, pq with the same heads,
N?
k=1
xsijk +
N?
k=1
xspqk ? 1 (19)
5.3 Unary Phrase Selection
Let Msij be the confidence score for phrase
xsij given by the image?phrase matching al-
gorithm (?2). To make the scores across dif-
ferent phrase types comparable, we normalize
them using Z-score: Fsij = norm?(Msij) =
(Msij ? meanj)/devj , and then transform the
values into the range of [0,1].
5.4 Pairwise Phrase Cohesion
In this section, we describe the pairwise phrase
cohesion score Fsijpq defined for each xsijpq in
363
ILP: I like the way the clouds hanging down by 
the ground in Dupnitsa of Avikwalal. 
 
Human: Car was raised on the wall over a bridge 
facing traffic..paramedics were attending the 
driver on the ground 
ILP: This is a photo of this bird hopping 
around eating things off of the ground by 
river. 
Human: IMG_6892 Lookn up in the sky its a 
bird its a plane its ah..... you 
ILP: This is a sporty little red convertible made for 
a great day in Key West FL. This car was in the 4th 
parade of the apartment buildings. 
 
Human: Hard rock casino exotic car show in June 
ILP: Taken in front of my cat sitting in a shoe 
box. Cat likes hanging around in my recliner. 
 
Human: H happily rests his armpit on a 
warm Gatorade bottle of water (a small 
bottle wrapped in a rag) 
Figure 2: In some cases (16%), ILP generated captions were preferred over human written ones!
the objective function (Eq. 12). Via Fsijpq,
we aim to quantify the degree of syntactic and
semantic cohesion across two phrases xsij and
xspq. Note that we subtract this cohesion score
from the objective function. This trick helps the
ILP solver to generate sentences with varying
number of phrases, rather than always selecting
the maximum number of phrases allowed.
N-gram Cohesion Score: We use n-gram
statistics from the Google Web 1-T dataset
(Brants and Franz., 2006) Let Lsijpq be the set
of all n-grams (2 ? n ? 5) across xsij and xspq.
Then the n-gram cohesion score is computed as:
FNGRAMsijpq = 1?
?
l?Lsijpq
NPMI(l)
size(Lsijpq)
(20)
NPMI(ngr) =
PMI(ngr)? PMImin
PMImax ? PMImin
(21)
Where NPMI is the normalized point-wise mu-
tual information.4
Co-occurrence Cohesion Score: To cap-
ture long-distance cohesion, we introduce a co-
occurrence-based score, which measures order-
preserved co-occurrence statistics between the
head words hsij and hspq 5. Let f?(hsij , hspq)
be the sum frequency of all n-grams that start
with hsij , end with hspq and contain a prepo-
sition prep(spq) of the phrase spq. Then the
4We include the n-gram cohesion for the sentence
boundaries as well, by approximating statistics for sen-
tence boundaries with punctuation marks in the Google
Web 1-T data.
5For simplicity, we use the last word of a phrase as
the head word, except VPs where we take the main verb.
co-occurrence cohesion is computed as:
FCOsijpq =
max(f?)? f?(hsij , hspq)
max(f?)?min(f?)
(22)
Final Cohesion Score: Finally, the pairwise
phrase cohesion score Fijpq is a weighted sum of
n-gram and co-occurrence cohesion scores:
Fsijpq =
? ? FNGRAMsijpq + ? ? F
CO
sijpq
?+ ?
(23)
where ? and ? can be tuned via grid search,
and FNGRAMijpq and F
CO
ijpq are normalized ? [0, 1]
for comparability. Notice that Fsijpq is in the
range [0,1] as well.
6 Evaluation
TestSet: Because computer vision is a challeng-
ing and unsolved problem, we restrict our query
set to images where we have high confidence that
visual recognition algorithms perform well. We
collect 1000 test images by running a large num-
ber (89) of object detectors on 20,000 images
and selecting images that receive confident ob-
ject detection scores, with some preference for
images with multiple object detections to obtain
good examples for testing discourse constraints.
Baselines: We compare our ILP approaches
with two nontrivial baselines: the first is an
HMM approach (comparable to Yang et al
(2011)), which takes as input the same set of
candidate phrases described in ?2, but for de-
coding, we fix the ordering of phrases as [ NP
? VP ? Region PP ? Scene PP] and find the
best combination of phrases using the Viterbi
algorithm. We use the same rich set of pairwise
364
Hmm Hmm Ilp Ilp
cognitive phrases: with w/o with w/o
0.111 0.114 0.114 0.116
Table 1: Automatic Evaluation
ILP selection rate
ILP V.S. HMM (w/o cogn) 67.2%
ILP V.S. HMM (with cogn) 66.3%
Table 2: Human Evaluation (without images)
ILP selection rate
ILP V.S. HMM (w/o cogn) 53.17%
ILP V.S. HMM (with cogn) 54.5%
ILP V.S. Retrieval 71.8%
ILP V.S. Human 16%
Table 3: Human Evaluation (with images)
phrase cohesion scores (?5.4) used for the ILP
formulation, producing a strong baseline6.
The second baseline is a recent Retrieval
based description method (Ordonez et al, 2011),
that searches the large parallel corpus of im-
ages and captions, and transfers a caption from
a visually similar database image to the query.
This again is a very strong baseline, as it ex-
ploits the vast amount of image-caption data,
and produces a description high in linguistic
quality (since the captions were written by hu-
man annotators).
Automatic Evaluation: Automatically quan-
tifying the quality of machine generated sen-
tences is known to be difficult. BLEU score
(Papineni et al, 2002), despite its simplicity
and limitations, has been one of the common
choices for automatic evaluation of image de-
scriptions (Farhadi et al, 2010; Kulkarni et al,
2011; Li et al, 2011; Ordonez et al, 2011), as
it correlates reasonably well with human evalu-
ation (Belz and Reiter, 2006).
Table 1 shows the the BLEU @1 against the
original caption of 1000 images. We see that the
ILP improves the score over HMM consistently,
with or without the use of cognitive phrases.
6Including other long-distance scores in HMM decod-
ing would make the problem NP-hard and require more
sophisticated decoding, e.g. ILP.
Grammar Cognitive Relevance
HMM 3.40(?=.82) 3.40(?=.88) 2.25(?=1.37)
ILP 3.56(?=.90) 3.60(?=.98) 2.37(?=1.49)
Hum. 4.36(?=.79) 4.77(?=.66) 3.86(?=1.60)
Table 4: Human Evaluation: Multi-Aspect Rating
(? is a standard deviation)
Human Evaluation I ? Ranking: We com-
plement the automatic evaluation with Mechan-
ical Turk evaluation. In ranking evaluation, we
ask raters to choose a better caption between
two choices7. We do this rating with and with-
out showing the images, as summarized in Ta-
ble 2 & 3. When images are shown, raters evalu-
ate content relevance as well as linguistic quality
of the captions. Without images, raters evaluate
only linguistic quality.
We found that raters generally prefer ILP gen-
erated captions over HMM generated ones, twice
as much (67.2% ILP V.S. 32.8% HMM), if im-
ages are not presented. However the difference is
less pronounced when images are shown. There
could be two possible reasons. The first is that
when images are shown, the Turkers do not try
as hard to tell apart the subtle difference be-
tween the two imperfect captions. The second
is that the relative content relevance of ILP gen-
erated captions is negating the superiority in lin-
guistic quality. We explore this question using
multi-aspect rating, described below.
Note that ILP generated captions are exceed-
ingly (71.8 %) preferred over the Retrieval
baseline (Ordonez et al, 2011), despite the gen-
erated captions tendency to be more prone to
grammatical and cognitive errors than retrieved
ones. This indicates that the generated captions
must have substantially better content relevance
to the query image, supporting the direction of
this research. Finally, notice that as much as
16% of the time, ILP generated captions are pre-
ferred over the original human generated ones
(examples in Figure 2).
Human Evaluation II ? Multi-Aspect Rat-
ing: Table 4 presents rating in the 1?5 scale (5:
perfect, 4: almost perfect, 3: 70?80% good, 2:
7We present two captions in a randomized order.
365
Found MIT boy 
gave me this 
quizical expression. 
One of the most shirt 
in the wall of the 
house. 
Grammar Problems 
Here you can see a 
bright red flower taken 
near our apartment in 
Torremolinos the Costa 
Del Sol. 
Content Irrelevance 
This is a shoulder bag with 
a blended rainbow effect. 
Cognitive Absurdity 
Here you can see a cross 
by the frog in the sky. 
Figure 3: Examples with different aspects of prob-
lems in the ILP generated captions.
50?70% good, 1: totally bad) in three different
aspects: grammar, cognitive correctness,8 and
relevance. We find that ILP improves over HMM
in all aspects, however, the relevance score is no-
ticeably worse than scores of two other criteria.
It turns out human raters are generally more
critical against the relevance aspect, as can be
seen in the ratings given to the original human
generated captions.
Discussion with Examples: Figure 1 shows
contrastive examples of HMM vs ILP gener-
ated captions. Notice that HMM captions
look robotic, containing spurious and redundant
phrases due to lack of discourse constraints, and
often discussing an awkward set of objects due
to lack of image-level content planning. Also
notice how image-level content planning under-
pinned by language statistics helps correct some
of the erroneous vision detections. Figure 3
shows some example mistakes in the ILP gen-
erated captions.
7 Related Work & Discussion
Although not directly focused on image descrip-
tion generation, some previous work in the realm
of summarization shares the similar problem of
content planning and surface realization. There
8E.g., ?A desk on top of a cat? is grammatically cor-
rect, but cognitively absurd.
are subtle, but important differences however.
First, sentence compression is hardly the goal
of image description generation, as human writ-
ten descriptions are not necessarily succinct.9
Second, unlike summarization, we are not given
with a set of coherent text snippet to begin with,
and the level of noise coming from the visual
recognition errors is much higher than that of
starting with clean text. As a result, choosing
an additional phrase in the image description is
much riskier than it is in summarization.
Some recent research proposed very elegant
approaches to summarization using ILP for col-
lective content planning and/or surface realiza-
tion (e.g., Martins and Smith (2009), Woodsend
and Lapata (2010), Woodsend et al (2010)).
Perhaps the most important difference in our
approach is the use of negative weights in the
objective function to create the necessary ten-
sion between selection (salience) and compatibil-
ity, which makes it possible for ILP to generate
variable length descriptions, effectively correct-
ing some of the erroneous vision detections. In
contrast, all previous work operates with a pre-
defined upper limit in length, hence the ILP was
formulated to include as many textual units as
possible modulo constraints.
To conclude, we have presented a collective
approach to generating natural image descrip-
tions. Our approach is the first to systematically
incorporate state of the art computer vision
to retrieve visually relevant candidate phrases,
then produce images descriptions that are sub-
stantially more complex and human-like than
previous attempts.
Acknowledgments T. L. Berg is supported
in part by NSF CAREER award #1054133; A.
C. Berg and Y. Choi are partially supported by
the Stony Brook University Office of the Vice
President for Research. We thank K. Yam-
aguchi, X. Han, M. Mitchell, H. Daume III, A.
Goyal, K. Stratos, A. Mensch, J. Dodge for data
pre-processing and useful initial discussions.
9On a related note, the notion of saliency also differs
in that human written captions often digress on details
that might be tangential to the visible content of the
image. E.g., ?This is a dress my mom made.?, where the
picture does not show a woman making the dress.
366
References
Ahmet Aker and Robert Gaizauskas. 2010. Gen-
erating image descriptions using dependency rela-
tional patterns. In ACL.
Anja Belz and Ehud Reiter. 2006. Comparing au-
tomatic and human evaluation of nlg systems.
In EACL 2006, 11st Conference of the European
Chapter of the Association for Computational Lin-
guistics, Proceedings of the Conference, April 3-7,
2006, Trento, Italy. The Association for Computer
Linguistics.
Thorsten Brants and Alex Franz. 2006. Web 1t 5-
gram version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Associa-
tion for Computational Linguistics.
Navneet Dalal and Bill Triggs. 2005. Histograms of
oriented gradients for human detection. In Pro-
ceedings of the 2005 IEEE Computer Society Con-
ference on Computer Vision and Pattern Recogni-
tion (CVPR?05) - Volume 1 - Volume 01, CVPR
?05, pages 886?893, Washington, DC, USA. IEEE
Computer Society.
Haris Dindo and Daniele Zambuto. 2010. A prob-
abilistic approach to learning a visually grounded
language model through human-robot interaction.
In IROS, pages 790?796. IEEE.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every
picture tells a story: generating sentences for im-
ages. In ECCV.
Pedro F. Felzenszwalb, Ross B. Girshick, David
McAllester, and Deva Ramanan. 2010. Object
detection with discriminatively trained part based
models. tPAMI, Sept.
Yansong Feng and Mirella Lapata. 2010. How many
words is a picture worth? automatic caption gen-
eration for news images. In ACL.
Fateh Muhammad Hafiz and Ian Tudor. 1989. Ex-
tensive reading and the development of language
skills. ELT Journal, 43(1):4?13.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar,
Siming Li, Yejin Choi, Alexander C Berg, and
Tamara L Berg. 2011. Babytalk: Understand-
ing and generating simple image descriptions. In
CVPR.
Thomas K. Leung and Jitendra Malik. 1999. Rec-
ognizing surfaces using three-dimensional textons.
In ICCV.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Compos-
ing simple image descriptions using web-scale n-
grams. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learn-
ing, pages 220?228, Portland, Oregon, USA, June.
Association for Computational Linguistics.
David G. Lowe. 2004. Distinctive image features
from scale-invariant keypoints. Int. J. Comput.
Vision, 60:91?110, November.
Andre Martins and Noah A. Smith. 2009. Summa-
rization with a joint model for sentence extraction
and compression. In Proceedings of the Workshop
on Integer Linear Programming for Natural Lan-
guage Processing, pages 1?9, Boulder, Colorado,
June. Association for Computational Linguistics.
Derek D. Monner and James A. Reggia. 2011. Sys-
tematically grounding language through vision in
a deep, recurrent neural network. In Proceed-
ings of the 4th international conference on Arti-
ficial general intelligence, AGI?11, pages 112?121,
Berlin, Heidelberg. Springer-Verlag.
Vicente Ordonez, Girish Kulkarni, and Tamara L.
Berg. 2011. Im2text: Describing images using 1
million captioned photographs. In Neural Infor-
mation Processing Systems (NIPS).
Kishore Papineni, Salim Roukos, Todd Ward, and
Wei jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In ACL.
Slav Petrov and Dan Klein. 2007. Improved infer-
ence for unlexicalized parsing. In HLT-NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, com-
pact, and interpretable tree annotation. In COL-
ING/ACL.
Deb K. Roy. 2002. Learning visually-grounded
words and syntax for a scene description task.
Computer Speech and Language, In review.
Wai-King Tsang. 1996. Comparing the effects of
reading and writing on writing performance. Ap-
plied Linguistics, 17(2):210?233.
Kristian Woodsend and Mirella Lapata. 2010. Au-
tomatic generation of story highlights. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 565?
574, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Kristian Woodsend, Yansong Feng, and Mirella
Lapata. 2010. Title generation with quasi-
synchronous grammar. In Proceedings of the 2010
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?10, pages 513?523,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
367
Jianxiong Xiao, James Hays, Krista A. Ehinger,
Aude Oliva, and Antonio Torralba. 2010. Sun
database: Large-scale scene recognition from
abbey to zoo. In CVPR.
Yezhou Yang, Ching Teo, Hal Daume III, and Yian-
nis Aloimonos. 2011. Corpus-guided sentence gen-
eration of natural images. In Proceedings of the
2011 Conference on Empirical Methods in Nat-
ural Language Processing, pages 444?454, Edin-
burgh, Scotland, UK., July. Association for Com-
putational Linguistics.
Benjamin Z. Yao, Xiong Yang, Liang Lin, Mun Wai
Lee, and Song-Chun Zhu. 2010. I2t: Image pars-
ing to text description. Proc. IEEE, 98(8).
368
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1774?1784,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Connotation Lexicon:
A Dash of Sentiment Beneath the Surface Meaning
Song Feng Jun Seok Kang Polina Kuznetsova Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
songfeng, junkang, pkuznetsova, ychoi@cs.stonybrook.edu
Abstract
Understanding the connotation of words
plays an important role in interpreting sub-
tle shades of sentiment beyond denotative
or surface meaning of text, as seemingly
objective statements often allude nuanced
sentiment of the writer, and even purpose-
fully conjure emotion from the readers?
minds. The focus of this paper is draw-
ing nuanced, connotative sentiments from
even those words that are objective on the
surface, such as ?intelligence?, ?human?,
and ?cheesecake?. We propose induction
algorithms encoding a diverse set of lin-
guistic insights (semantic prosody, distri-
butional similarity, semantic parallelism of
coordination) and prior knowledge drawn
from lexical resources, resulting in the first
broad-coverage connotation lexicon.
1 Introduction
There has been a substantial body of research
in sentiment analysis over the last decade (Pang
and Lee, 2008), where a considerable amount of
work has focused on recognizing sentiment that is
generally explicit and pronounced rather than im-
plied and subdued. However in many real-world
texts, even seemingly objective statements can be
opinion-laden in that they often allude nuanced
sentiment of the writer (Greene and Resnik, 2009),
or purposefully conjure emotion from the readers?
minds (Mohammad and Turney, 2010). Although
some researchers have explored formal and statis-
tical treatments of those implicit and implied sen-
timents (e.g. Wiebe et al (2005), Esuli and Sebas-
tiani (2006), Greene and Resnik (2009), Davidov
et al (2010)), automatic analysis of them largely
remains as a big challenge.
In this paper, we concentrate on understanding
the connotative sentiments of words, as they play
an important role in interpreting subtle shades of
sentiment beyond denotative or surface meaning
of text. For instance, consider the following:
Geothermal replaces oil-heating; it helps re-
ducing greenhouse emissions.1
Although this sentence could be considered as a
factual statement from the general standpoint, the
subtle effect of this sentence may not be entirely
objective: this sentence is likely to have an influ-
ence on readers? minds in regard to their opinion
toward ?geothermal?. In order to sense the subtle
overtone of sentiments, one needs to know that the
word ?emissions? has generally negative connota-
tion, which geothermal reduces. In fact, depend-
ing on the pragmatic contexts, it could be precisely
the intention of the author to transfer his opinion
into the readers? minds.
The main contribution of this paper is a broad-
coverage connotation lexicon that determines the
connotative polarity of even those words with ever
so subtle connotation beneath their surface mean-
ing, such as ?Literature?, ?Mediterranean?, and
?wine?. Although there has been a number of
previous work that constructed sentiment lexicons
(e.g., Esuli and Sebastiani (2006), Wilson et al
(2005a), Kaji and Kitsuregawa (2007), Qiu et
al. (2009)), which seem to be increasingly and
inevitably expanding over words with (strongly)
connotative sentiments rather than explicit senti-
ments alone (e.g., ?gun?), little prior work has di-
rectly tackled this problem of learning connota-
tion,2 and much of the subtle connotation of many
seemingly objective words is yet to be determined.
1Our learned lexicon correctly assigns negative polarity to
emission.
2A notable exception would be the work of Feng et al
1774
POSITIVE NEGATIVE
FEMA, Mandela, Intel, Google, Python, Sony, Pulitzer,
Harvard, Duke, Einstein, Shakespeare, Elizabeth, Clooney,
Hoover, Goldman, Swarovski, Hawaii, Yellowstone
Katrina, Monsanto, Halliburton, Enron, Teflon, Hi-
roshima, Holocaust, Afghanistan, Mugabe, Hutu, Sad-
dam, Osama, Qaeda, Kosovo, Helicobacter, HIV
Table 1: Example Named Entities (Proper Nouns) with Polar Connotation.
A central premise to our approach is that it is
collocational statistics of words that affect and
shape the polarity of connotation. Indeed, the ety-
mology of ?connotation? is from the Latin ?com-
? (?together or with?) and ?notare? (?to mark?).
It is important to clarify, however, that we do not
simply assume that words that collocate share the
same polarity of connotation. Although such an
assumption played a key role in previous work for
the analogous task of learning sentiment lexicon
(Velikovich et al, 2010), we expect that the same
assumption would be less reliable in drawing sub-
tle connotative sentiments of words. As one ex-
ample, the predicate ?cure?, which has a positive
connotation typically takes arguments with nega-
tive connotation, e.g., ?disease?, when used as the
?relieve? sense.3
Therefore, in order to attain a broad cover-
age lexicon while maintaining good precision, we
guide the induction algorithm with multiple, care-
fully selected linguistic insights: [1] distributional
similarity, [2] semantic parallelism of coordina-
tion, [3] selectional preference, and [4] seman-
tic prosody (e.g., Sinclair (1991), Louw (1993),
Stubbs (1995), Stefanowitsch and Gries (2003))),
and also exploit existing lexical resources as an ad-
ditional inductive bias.
We cast the connotation lexicon induction task
as a collective inference problem, and consider ap-
proaches based on three distinct types of algorith-
mic framework that have been shown successful
for conventional sentiment lexicon induction:
Random walk based on HITS/PageRank (e.g.,
Kleinberg (1999), Page et al (1999), Feng
et al (2011) Heerschop et al (2011),
Montejo-Ra?ez et al (2012))
Label/Graph propagation (e.g., Zhu and Ghahra-
(2011) but with practical limitations. See ?3 for detailed dis-
cussion.
3Note that when ?cure? is used as the ?preserve? sense, it
expects objects with non-negative connotation. Hence word-
sense-disambiguation (WSD) presents a challenge, though
not unexpectedly. In this work, we assume the general conno-
tation of each word over statistically prevailing senses, leav-
ing a more cautious handling of WSD as future work.
mani (2002), Velikovich et al (2010))
Constraint optimization (e.g., Roth and Yih
(2004), Choi and Cardie (2009), Lu et al
(2011)).
We provide comparative empirical results over
several variants of these approaches with compre-
hensive evaluations including lexicon-based, hu-
man judgments, and extrinsic evaluations.
It is worthwhile to note that not all words have
connotative meanings that are distinct from deno-
tational meanings, and in some cases, it can be dif-
ficult to determine whether the overall sentiment is
drawn from denotational or connotative meanings
exclusively, or both. Therefore, we encompass any
sentiment from either type of meanings into the
lexicon, where non-neutral polarity prevails over
neutral one if some meanings lead to neutral while
others to non-neutral.4
Our work results in the first broad-coverage
connotation lexicon,5 significantly improving both
the coverage and the precision of Feng et al
(2011). As an interesting by-product, our algo-
rithm can be also used as a proxy to measure the
general connotation of real-world named entities
based on their collocational statistics. Table 1
highlights some example proper nouns included in
the final lexicon.
The rest of the paper is structured as follows.
In ?2 we describe three types of induction algo-
rithms followed by evaluation in ?3. Then we re-
visit the induction algorithms based on constraint
optimization in ?4 to enhance quality and scala-
bility. ?5 presents comprehensive evaluation with
human judges and extrinsic evaluations. Related
work and conclusion are in ?6 and ?7.
4In general, polysemous words do not seem to have con-
flicting non-neutral polarities over different senses, though
there are many exceptions, e.g., ?heat?, or ?fine?. We treat
each word in each part-of-speech as a separate word to reduce
such cases, otherwise aim to learn the most prevalent polar-
ity in the corpus with respect to each part-of-speech of each
word.
5Available at http://www.cs.stonybrook.edu/
?ychoi/connotation.
1775
? 
Pred-Arg 
Arg-Arg 
pred-arg distr sim 
enjoy 
thank 
writing 
profit 
help 
investment 
aid 
reading 
Figure 1: Graph for Graph Propagation (?2.2).
? 
? 
synonyms antonyms 
prevent 
suffer  
enjoy 
thank 
tax  
loss 
writing 
profit 
preventing 
gain 
investment 
bonus  
pred-arg distr sim 
flu  
cold  
Figure 2: Graph for ILP/LP (?2.3, ?4.2).
2 Connotation Induction Algorithms
We develop induction algorithms based on three
distinct types of algorithmic framework that have
been shown successful for the analogous task of
sentiment lexicon induction: HITS & PageRank
(?2.1), Label/Graph Propagation (?2.2), and Con-
straint Optimization via Integer Linear Program-
ming (?2.3). As will be shown, each of these ap-
proaches will incorporate additional, more diverse
linguistic insights.
2.1 HITS & PageRank
The work of Feng et al (2011) explored the use
of HITS (Kleinberg, 1999) and PageRank (Page
et al, 1999) to induce the general connotation
of words hinging on the linguistic phenomena of
selectional preference and semantic prosody, i.e.,
connotative predicates influencing the connotation
of their arguments. For example, the object of
a negative connotative predicate ?cure? is likely
to have negative connotation, e.g., ?disease? or
?cancer?. The bipartite graph structure for this
approach corresponds to the left-most box (labeled
as ?pred-arg?) in Figure 1.
2.2 Label Propagation
With the goal of obtaining a broad-coverage lexi-
con in mind, we find that relying only on the struc-
ture of semantic prosody is limiting, due to rel-
atively small sets of connotative predicates avail-
able.6 Therefore, we extend the graph structure
as an overlay of two sub-graphs (Figure 1) as de-
scribed below:
6For connotative predicates, we use the seed predicate set
of Feng et al (2011), which comprises of 20 positive and 20
negative predicates.
Sub-graph #1: Predicate?Argument Graph
This sub-graph is the bipartite graph that encodes
the selectional preference of connotative predi-
cates over their arguments. In this graph, conno-
tative predicates p reside on one side of the graph
and their co-occurring arguments a reside on the
other side of the graph based on Google Web 1T
corpus.7 The weight on the edges between the
predicates p and arguments a are defined using
Point-wise Mutual Information (PMI) as follows:
w(p? a) := PMI(p, a) = log2
P (p, a)
P (p)P (a)
PMI scores have been widely used in previous
studies to measure association between words
(e.g., Turney (2001), Church and Hanks (1990)).
Sub-graph #2: Argument?Argument Graph
The second sub-graph is based on the distribu-
tional similarities among the arguments. One pos-
sible way of constructing such a graph is simply
connecting all nodes and assign edge weights pro-
portionate to the word association scores, such as
PMI, or distributional similarity. However, such a
completely connected graph can be susceptible to
propagating noise, and does not scale well over a
very large set of vocabulary.
We therefore reduce the graph connectivity by
exploiting semantic parallelism of coordination
(Bock (1986), Hatzivassiloglou and McKeown
7We restrict predicte-argument pairs to verb-object pairs
in this study. Note that Google Web 1T dataset consists of
n-grams upto n = 5. Since n-gram sequences are too short
to apply a parser, we extract verb-object pairs approximately
by matching part-of-speech tags. Empirically, when overlaid
with the second sub-graph, we found that it is better to keep
the connectivity of this sub-graph as uni-directional. That is,
we only allow edges to go from a predicate to an argument.
1776
POSITIVE NEGATIVE NEUTRAL
n. avatar, adrenaline, keynote, debut,
stakeholder, sunshine, cooperation
unbeliever, delay, shortfall, gun-
shot, misdemeanor, mutiny, rigor
header, mark, clothing, outline,
grid, gasoline, course, preview
v. handcraft, volunteer, party, ac-
credit, personalize, nurse, google
sentence, cough, trap, scratch, de-
bunk, rip, misspell, overcharge
state, edit, send, put, arrive, type,
drill, name, stay, echo, register
a. floral, vegetarian, prepared, age-
less, funded, contemporary
debilitating, impaired, swollen,
intentional, jarring, unearned
same, cerebral, west, uncut, auto-
matic, hydrated, unheated, routine
Table 2: Example Words with Learned Connotation: Nouns(n), Verbs(v), Adjectives(a).
(1997), Pickering and Branigan (1998)). In par-
ticular, we consider an undirected edge between a
pair of arguments a1 and a2 only if they occurred
together in the ?a1 and a2? or ?a2 and a1? coor-
dination, and assign edge weights as:
w(a1 ? a2) = CosineSim(??a1,??a2) =
??a1 ? ??a2
||??a1|| ||??a2||
where ??a1 and ??a2 are co-occurrence vectors for a1
and a2 respectively. The co-occurrence vector for
each word is computed using PMI scores with re-
spect to the top n co-occurring words.8 n (=50)
is selected empirically. The edge weights in two
sub-graphs are normalized so that they are in the
comparable range.9
Limitations of Graph-based Algorithms
Although graph-based algorithms (?2.1, ?2.2) pro-
vide an intuitive framework to incorporate various
lexical relations, limitations include:
1. They allow only non-negative edge weights.
Therefore, we can encode only positive (sup-
portive) relations among words (e.g., distri-
butionally similar words will endorse each
other with the same polarity), while miss-
ing on exploiting negative relations (e.g.,
antonyms may drive each other into the op-
posite polarity).
2. They induce positive and negative polarities
in isolation via separate graphs. However, we
expect that a more effective algorithm should
induce both polarities simultaneously.
3. The framework does not readily allow incor-
porating a diverse set of soft and hard con-
straints.
8We discard edges with cosine similarity ? 0, as those
indicate either independence or the opposite of similarity.
9Note that cosine similarity does not make sense for the
first sub-graph as there is no reason why a predicate and an ar-
gument should be distributionally similar. We experimented
with many different variations on the graph structure and
edge weights, including ones that include any word pairs that
occurred frequently enough together. For brevity, we present
the version that achieved the best results here.
2.3 Constraint Optimization
Addressing limitations of graph-based algorithms
(?2.2), we propose an induction algorithm based
on Integer Linear Programming (ILP). Figure 2
provides the pictorial overview. In comparison to
Figure 1, two new components are: (1) dictionary-
driven relations targeting enhanced precision, and
(2) dictionary-driven words (i.e., unseen words
with respect to those relations explored in Figure
1) targeting enhanced coverage. We formulate in-
sights in Figure 2 using ILP as follows:
Definition of sets of words:
1. P+: the set of positive seed predicates.
P?: the set of negative seed predicates.
2. S: the set of seed sentiment words.
3. Rsyn: word pairs in synonyms relation.
Rant: word pairs in antonyms relation.
Rcoord: word pairs in coordination relation.
Rpred: word pairs in pred-arg relation.
Rpred+(?) : Rpred based on P+ (P?).
Definition of variables: For each word i, we
define binary variables xi, yi, zi ? {0, 1}, where
xi = 1 (yi = 1, zi = 1) if and only if i has a pos-
itive (negative, neutral) connotation respectively.
For every pair of word i and j, we define binary
variables dpqij where p, q ? {+,?, 0} and dpqij = 1
if and only if the polarity of i and j are p and q
respectively.
Objective function: We aim to maximize:
F = ?prosody + ?coord + ?neu
where ?prosody is the scores based on semantic
prosody, ?coord captures the distributional similar-
ity over coordination, and ?neu controls the sen-
sitivity of connotation detection between positive
(negative) and neutral. In particular,
?prosody =
Rpred?
i,j
wpredi,j (d++i,j + d??i,j ? d+?i,j ? d?+i,j )
?coord =
Rcoord?
i,j
wcoordi,j (d++i,j + d??i,j + d00i,j)
1777
?neu = ?
Rpred?
i,j
wpredi,j ? zj
Soft constraints (edge weights): The weights in
the objective function are set as follows:
wpred(p, a) = freq(p, a)?
(p,x)?Rpred
freq(p, x)
wcoord(a1, a2) = CosSim(??a1,??a2) =
??a1 ? ??a2
||??a1|| ||??a2||
Note that the same wcoord(a1, a2) has been used
in graph propagation described in Section 2.2. ?
controls the sensitivity of connotation detection
such that higher value of ? will promote neutral
connotation over polar ones.
Hard constrains for variable consistency:
1. Each word i has one of {+,?, ?} as polarity:
?i, xi + yi + zi = 1
2. Variable consistency between dpqij and
xi, yi, zi:
xi + xj ? 1 ? 2d++i,j ? xi + xj
yi + yj ? 1 ? 2d??i,j ? yi + yj
zi + zj ? 1 ? 2d00i,j ? zi + zj
xi + yj ? 1 ? 2d+?i,j ? xi + yj
yi + xj ? 1 ? 2d?+i,j ? yi + xj
Hard constrains for WordNet relations:
1. Cant: Antonym pairs will not have the same
positive or negative polarity:
?(i, j) ? Rant, xi + xj ? 1, yi + yj ? 1
For this constraint, we only consider
antonym pairs that share the same root, e.g.,
?sufficient? and ?insufficient?, as those pairs
are more likely to have the opposite polarities
than pairs without sharing the same root, e.g.,
?east? and ?west?.
2. Csyn: Synonym pairs will not have the oppo-
site polarity:
?(i, j) ? Rsyn, xi + yj ? 1, xj + yi ? 1
3 Experimental Result I
We provide comprehensive comparisons over vari-
ants of three types of algorithms proposed in ?2.
We use the Google Web 1T data (Brants and Franz
(2006)), and POS-tagged ngrams using Stanford
POS Tagger (Toutanova and Manning (2000)). We
filter out the ngrams with punctuations and other
special characters to reduce the noise.
3.1 Comparison against Conventional
Sentiment Lexicon
Note that we consider the connotation lexicon to
be inclusive of a sentiment lexicon for two prac-
tical reasons: first, it is highly unlikely that any
word with non-neutral sentiment (i.e., positive or
negative) would carry connotation of the oppo-
site, i.e., conflicting10 polarity. Second, for some
words with distinct sentiment or strong connota-
tion, it can be difficult or even unnatural to draw a
precise distinction between connotation and senti-
ment, e.g., ?efficient?. Therefore, sentiment lexi-
cons can serve as a surrogate to measure a subset
of connotation words induced by the algorithms,
as shown in Table 3 with respect to General In-
quirer (Stone and Hunt (1963)) and MPQA (Wil-
son et al (2005b)).11
Discussion Table 3 shows the agreement statis-
tics with respect to two conventional sentiment
lexicons. We find that the use of label propaga-
tion alone [PRED-ARG (CP)] improves the per-
formance substantially over the comparable graph
construction with different graph analysis algo-
rithms, in particular, HITS and PageRank ap-
proaches of Feng et al (2011). The two com-
pletely connected variants of the graph propa-
gation on the Pred-Arg graph, [? PRED-ARG
(PMI)] and [? PRED-ARG (CP)], do not neces-
sarily improve the performance over the simpler
and computationally lighter alternative, [PRED-
ARG (CP)]. The [OVERLAY], which is based
on both Pred-Arg and Arg-Arg subgraphs (?2.2),
achieves the best performance among graph-based
algorithms, significantly improving the precision
over all other baselines. This result suggests:
1 The sub-graph #2, based on the semantic par-
allelism of coordination, is simple and yet
very powerful as an inductive bias.
2 The performance of graph propagation varies
significantly depending on the graph topol-
ogy and the corresponding edge weights.
Note that a direct comparison against ILP for top
N words is tricky, as ILP does not rank results.
Only for comparison purposes however, we assign
10We consider ?positive? and ?negative? polarities conflict,
but ?neutral? polarity does not conflict with any.
11In the case of General Inquirer, we use words in POSITIV
and NEGATIV sets as words with positive and negative labels
respectively.
1778
GENINQ EVAL MPQA EVAL
100 1,000 5,000 10,000 ALL 100 1,000 5,000 10,000 ALL
ILP 97.6 94.5 84.5 80.8 80.4 98.0 89.7 84.6 81.2 78.4
OVERLAY 97.0 95.1 78.8 (78.3) 78.3 98.0 93.4 82.1 77.7 77.7? PRED-ARG (PMI) 91.0 91.4 76.1 (76.1) 76.1 88.0 89.1 78.8 75.1 75.1?PRED-ARG (CP) 88.0 85.4 76.2 (76.2) 76.2 87.0 82.6 78.0 76.3 76.3
PRED-ARG (CP) 91.0 91.0 81.0 (81.0) 81.0 88.0 91.5 80.0 78.3 78.3
HITS-ASYMT 77.0 68.8 - - 66.5 86.3 81.3 - - 72.2
PAGERANK-ASYMF 77.0 68.5 - - 65.7 87.2 80.3 - - 72.3
Table 3: Evaluation of Induction Algorithms (?2) with respect to Sentiment Lexicons (precision%).
ranks based on the frequency of words for ILP. Be-
cause of this issue, the performance of top ?1k
words of ILP should be considered only as a con-
servative measure. Importantly, when evaluated
over more than top 5k words, ILP is overall the
top performer considering both precision (shown
in Table 3) and coverage (omitted for brevity).12
4 Precision, Coverage, and Efficiency
In this section, we address three important aspects
of an ideal induction algorithm: precision, cover-
age, and efficiency. For brevity, the remainder of
the paper will focus on the algorithms based on
constraint optimization, as it turned out to be the
most effective one from the empirical results in ?3.
Precision In order to see the effectiveness of the
induction algorithms more sharply, we had used a
limited set of seed words in ?3. However to build a
lexicon with substantially enhanced precision, we
will use as a large seed set as possible, e.g., entire
sentiment lexicons13.
Broad coverage Although statistics in Google
1T corpus represent a very large amount of text,
words that appear in pred-arg and coordination re-
lations are still limited. To substantially increase
the coverage, we will leverage dictionary words
(that are not in the corpus) as described in ?2.3
and Figure 2.
Efficiency One practical problem with ILP is ef-
ficiency and scalability. In particular, we found
that it becomes nearly impractical to run the ILP
formulation including all words in WordNet plus
all words in the argument position in Google Web
1T. We therefore explore an alternative approach
based on Linear Programming in what follows.
12In fact, the performance of PRED-ARG variants for top
10K w.r.t. GENINQ is not meaningful as no additional word
was matched beyond top 5k words.
13Note that doing so will prevent us from evaluating
against the same sentiment lexicon used as a seed set.
4.1 Induction using Linear Programming
One straightforward option for Linear Program-
ming formulation may seem like using the same
Integer Linear Programming formulation intro-
duced in ?2.3, only changing the variable defini-
tions to be real values ? [0, 1] rather than integers.
However, because the hard constraints in ?2.3 are
defined based on the assumption that all the vari-
ables are binary integers, those constraints are not
as meaningful when considered for real numbers.
Therefore we revise those hard constraints to en-
code various semantic relations (WordNet and se-
mantic coordination) more directly.
Definition of variables: For each word i, we de-
fine variables xi, yi, zi ? [0, 1]. i has a positive
(negative) connotation if and only if the xi (yi) is
assigned the greatest value among the three vari-
ables; otherwise, i is neutral.
Objective function: We aim to maximize:
F = ?prosody + ?coord + ?syn + ?ant + ?neu
?prosody =
Rpred+?
i,j
wpred
+
i,j ? xj +
Rpred??
i,j
wpred
?
i,j ? yj
?coord =
Rcoord?
i,j
wcoordi,j ? (dc++i,j + dc??i,j )
?syn = W syn
Rsyn?
i,j
(ds++i,j + ds??i,j )
?ant = W ant
Rant?
i,j
(da++i,j + da??i,j )
?neu = ?
Rpred?
i,j
wpredi,j ? zj
Hard constraints We add penalties to the
objective function if the polarity of a pair of words
is not consistent with its corresponding semantic
relations. For example, for synonyms i and j, we
introduce a penalty W syn (a positive constant) for
ds++i,j , ds??i,j ? [?1, 0], where we set the upper
bound of ds++i,j (ds??i,j ) as the signed distance of
1779
FORMULA POSITIVE NEGATIVE ALLR P F R P F R P F
ILP ?prosody + Csyn + Cant 51.4 85.7 64.3 44.7 87.9 59.3 48.0 86.8 61.8
?prosody + Csyn + Cant + CS 61.2 93.3 73.9 52.4 92.2 66.8 56.8 92.8 70.5
?prosody + ?coord + Csyn + Cant 67.3 75.0 70.9 53.7 84.4 65.6 60.5 79.7 68.8
?prosody + ?coord + Csyn + Cant + CS 62.2 96.0 75.5 51.5 89.5 65.4 56.9 92.8 70.5
LP ?prosody + ?syn + ?ant 24.4 76.0 36.9 23.6 78.8 36.3 24.0 77.4 36.6
?prosody + ?syn + ?ant + ?S 71.6 87.8 78.9 68.8 84.6 75.9 70.2 86.2 77.4
?prosody + ?coord + ?syn + ?ant 67.9 92.6 78.3 64.6 89.1 74.9 66.3 90.8 76.6
?prosody + ?coord + ?syn + ?ant + ?S 78.6 90.5 84.1 73.3 87.1 79.6 75.9 88.8 81.8
Table 4: ILP/LP Comparison on MQPA? (%).
xi and xj (yi and yj) as shown below:
For (i, j) ? Rsyn,
ds++i,j ? xi ? xj , ds++i,j ? xj ? xi
ds??i,j ? yi ? yj , ds??i,j ? yj ? yi
Notice that ds++i,j , ds??i,j satisfying above inequal-
ities will be always of negative values, hence in
order to maximize the objective function, the LP
solver will try to minimize the absolute values of
ds++i,j , ds??i,j , effectively pushing i and j toward
the same polarity. Constraints for semantic coor-
dination Rcoord can be defined similarly. Lastly,
following constraints encode antonym relations:
For (i, j) ? Rant ,
da++i,j ? xi ? (1? xj), da++i,j ? (1? xj)? xi
da??i,j ? yi ? (1? yj), da??i,j ? (1? yj)? yi
Interpretation Unlike ILP, some of the vari-
ables result in fractional values. We consider a
word has positive or negative polarity only if the
assignment indicates 1 for the corresponding po-
larity and 0 for the rest. In other words, we treat
all words with fractional assignments over differ-
ent polarities as neutral. Because the optimal so-
lutions of LP correspond to extreme points in the
convex polytope formed by the constraints, we ob-
tain a large portion of words with non-fractional
assignments toward non-neutral polarities. Alter-
natively, one can round up fractional values.
4.2 Empirical Comparisons: ILP v.s. LP
To solve the ILP/LP, we run ILOG CPLEX Opti-
mizer (CPLEX, 2009)) on a 3.5GHz 6 core CPU
machine with 96GB RAM. Efficiency-wise, LP
runs within 10 minutes while ILP takes several
hours. Table 4 shows the results evaluated against
MPQA for different variations of ILP and LP.
We find that LP variants much better recall and
F-score, while maintaining comparable precision.
Therefore, we choose the connotation lexicon by
LP (C-LP) in the following evaluations in ?5.
5 Experimental Results II
In this section, we present comprehensive intrin-
sic ?5.1 and extrinsic ?5.2 evaluations comparing
three representative lexicons from ?2 & ?4: C-
LP, OVERLAY, PRED-ARG (CP), and two popular
sentiment lexicons: SentiWordNet (Baccianella et
al., 2010) and GI+MPQA.14 Note that C-LP is the
largest among all connotation lexicons, including
?70,000 polar words.15
5.1 Intrinsic Evaluation: Human Judgements
We evaluate 4000 words16 using Amazon Me-
chanical Turk (AMT). Because we expect that
judging a connotation can be dependent on one?s
cultural background, personality and value sys-
tems, we gather judgements from 5 people for
each word, from which we hope to draw a more
general judgement of connotative polarity. About
300 unique Turkers participated the evaluation
tasks. We gather gold standard only for those
words for which more than half of the judges
agreed on the same polarity. Otherwise we treat
them as ambiguous cases.17 Figure 3 shows a part
of the AMT task, where Turkers are presented with
questions that help judges to determine the subtle
connotative polarity of each word, then asked to
rate the degree of connotation on a scale from -
5 (most negative) and 5 (most positive). To draw
14GI+MPQA is the union of General Inquirer and MPQA.
The GI, we use words in the ?Positiv? & ?Negativ? set. For
SentiWordNet, to retrieve the polarity of a given word, we
sum over the polarity scores over all senses, where positive
(negative) values correspond to positive (negative) polarity.
15?13k adj, ?6k verbs, ?28k nouns, ?22k proper nouns.
16We choose words that are not already in GI+MPQA and
obtain most frequent 10,000 words based on the unigram fre-
quency in Google-Ngram, then randomly select 4000 words.
17We allow Turkers to mark words that can be used with
both positive and negative connotation, which results in about
7% of words that are excluded from the gold standard set.
1780
Figure 3: A Part of AMT Task Design.
YES NO
QUESTION % Avg % Avg
?Enjoyable or pleasant? 43.3 2.9 16.3 -2.4
?Of a good quality? 56.7 2.5 6.1 -2.7
?Respectable / honourable? 21.0 3.3 14.0 -1.1
?Would like to do or have? 52.5 2.8 11.5 -2.4
Table 5: Distribution of Answers from AMT.
the gold standard, we consider two different voting
schemes:
? ?V ote: The judgement of each Turker is
mapped to neutral for ?1 ? score ? 1, pos-
itive for score ? 2, negative for score ? 2,
then we take the majority vote.
? ?Score: Let ?(i) be the sum (weighted vote)
of the scores given by 5 judges for word i.
Then we determine the polarity label l(i) of i
as:
l(i) =
?
?
?
positive if ?(i) > 1
negative if ?(i) < ?1
neutral if ?1 ? ?(i) ? 1
The resulting distribution of judgements is shown
in Table 5 & 6. Interestingly, we observe
that among the relatively frequently used English
words, there are overwhelmingly more positively
connotative words than negative ones.
In Table 7, we show the percentage of words
with the same label over the mutual words by the
two lexicon. The highest agreement is 77% by
C-LP and the gold standard by AMTV ote. How
good is this? It depends on what is the natural de-
gree of agreement over subtle connotation among
people. Therefore, we also report the degree of
agreement among human judges in Table 7, where
we compute the agreement of one Turker with re-
spect to the gold standard drawn from the rest of
the Turkers, and take the average across over all
five Turkers18. Interestingly, the performance of
18In order to draw the gold standard from the 4 remaining
Turkers, we consider adjusted versions of ?V ote and ?Score
schemes described above.
POS NEG NEU UNDETERMINED
?V ote 50.4 14.6 24.1 10.9
?Score 67.9 20.6 11.5 n/a
Table 6: Distribution of Connotative Polarity from
AMT.
C-LP SENTIWN HUMAN JUDGES
?V ote 77.0 71.5 66.0
?Score 73.0 69.0 69.0
Table 7: Agreement (Accuracy) against AMT-
driven Gold Standard.
Turkers is not as good as that of C-LP lexicon. We
conjecture that this could be due to generally vary-
ing perception of different people on the connota-
tive polarity,19 while the corpus-driven induction
algorithms focus on the general connotative po-
larity corresponding to the most prevalent senses
of words in the corpus.
5.2 Extrinsic Evaluation
We conduct lexicon-based binary sentiment clas-
sification on the following two corpora.
SemEval From the SemEval task, we obtain a
set of news headlines with annotated scores (rang-
ing from -100 to 87). The positive/negative scores
indicate the degree of positive/negative polarity
orientation. We construct several sets of the posi-
tive and negative texts by setting thresholds on the
scores as shown in Table 8. ?? n? indicates that
the positive set consists of the texts with scores
? n and the negative set consists of the texts with
scores ? ?n.
Emoticon tweets The sentiment Twitter data20
consists of tweets containing either a smiley
emoticon (positive sentiment) or a frowny emoti-
con (negative sentiment). We filter out the tweets
with question marks or more than 30 words, and
keep the ones with at least two words in the union
of all polar words in the five lexicons in Table 8,
and then randomly select 10000 per class.
We denote the short text (e.g., content of tweets
or headline texts from SemEval) by t. w repre-
sents the word in t. W+/W? is the set of posi-
19Pearson correlation coefficient among turkers is 0.28,
which corresponds to a positive small to medium correlation.
Note that when the annotation of turkers is aggregated, we
observe agreement as high as 77% with respect to the learned
connotation lexicon.
20http://www.stanford.edu/?alecmgo/
cs224n/twitterdata.2009.05.25.c.zip
1781
DATA
LEXICON TWEET SEMEVAL
?20 ?40 ?60 ?80
C-LP 70.1 70.8 74.6 80.8 93.5
OVERLAY 68.5 70.0 72.9 76.8 89.6
PRED-ARG (CP) 60.5 64.2 69.3 70.3 79.2
SENTIWN 67.4 61.0 64.5 70.5 79.0
GI+MPQA 65.0 64.5 69.0 74.0 80.5
Table 8: Accuracy on Sentiment Classification
(%).
tive/negative words of the lexicon. We define the
weight of w as s(w). If w is adjective, s(w) = 2;
otherwise s(w) = 1. Then the polarity of each text
is determined as follows:
pol(t) =
?
????
????
positive if
W+?
w?t
s(w) ?
W??
w?t
s(w)
negative if
W+?
w?t
s(w) <
W??
w?t
s(w)
As shown in Table 8, C-LP generally performs
better than the other lexicons on both corpora.
Considering that only very simple classification
strategy is applied, the result by the connotation
lexicon is quite promising.
Finally, Table 1 highlights interesting exam-
ples of proper nouns with connotative polarity,
e.g., ?Mandela?, ?Google?, ?Hawaii? with pos-
itive connotation, and ?Monsanto?, ?Hallibur-
ton?, ?Enron? with negative connotation, sug-
gesting that our algorithms could potentially serve
as a proxy to track the general connotation of real
world entities. Table 2 shows example common
nouns with connotative polarity.
5.3 Practical Remarks on WSD and MWEs
In this work we aim to find the polarity of most
prevalent senses of each word, in part because it
is not easy to perform unsupervised word sense
disambiguation (WSD) on a large corpus in a reli-
able way, especially when the corpus consists pri-
marily of short n-grams. Although the resulting
lexicon loses on some of the polysemous words
with potentially opposite polarities, per-word con-
notation (rather than per-sense connotation) does
have a practical value: it provides a convenient
option for users who wish to avoid the burden of
WSD before utilizing the lexicon. Future work in-
cludes handling of WSD and multi-word expres-
sions (MWEs), e.g., ?Great Leader? (for Kim
Jong-Il), ?Inglourious Basterds? (a movie title).21
21These examples credit to an anonymous reviewer.
6 Related Work
A very interesting work of Mohammad and Tur-
ney (2010) uses Mechanical Turk in order to build
the lexicon of emotions evoked by words. In con-
trast, we present an automatic approach that in-
fers the general connotation of words. Velikovich
et al (2010) use graph propagation algorithms for
constructing a web-scale polarity lexicon for sen-
timent analysis. Although we employ the same
graph propagation algorithm, our graph construc-
tion is fundamentally different in that we integrate
stronger inductive biases into the graph topology
and the corresponding edge weights. As shown
in our experimental results, we find that judicious
construction of graph structure, exploiting multi-
ple complementing linguistic phenomena can en-
hance both the performance and the efficiency of
the algorithm substantially. Other interesting ap-
proaches include one based on min-cut (Dong et
al., 2012) or LDA (Xie and Li, 2012). Our pro-
posed approaches are more suitable for encoding
a much diverse set of linguistic phenomena how-
ever. But our work use a few seed predicates with
selectional preference instead of relying on word
similarity. Some recent work explored the use
of constraint optimization framework for inducing
domain-dependent sentiment lexicon (Choi and
Cardie (2009), Lu et al (2011)). Our work dif-
fers in that we provide comprehensive insights into
different formulations of ILP and LP, aiming to
learn the much different task of learning the gen-
eral connotation of words.
7 Conclusion
We presented a broad-coverage connotation lexi-
con that determines the subtle nuanced sentiment
of even those words that are objective on the sur-
face, including the general connotation of real-
world named entities. Via a comprehensive eval-
uation, we provided empirical insights into three
different types of induction algorithms, and pro-
posed one with good precision, coverage, and effi-
ciency.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. We thank reviewers for many insightful
comments and suggestions, and for providing us
with several very inspiring examples to work with.
1782
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexi-
cal resource for sentiment analysis and opinion min-
ing. In Proceedings of the Seventh conference on
International Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Lan-
guage Resources Association (ELRA).
J. Kathryn Bock. 1986. Syntactic persistence
in language production. Cognitive psychology,
18(3):355?387.
Thorsten Brants and Alex Franz. 2006. {Web 1T 5-
gram Version 1}.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 2 -
Volume 2, EMNLP ?09, pages 590?598, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicog-
raphy. Comput. Linguist., 16:22?29, March.
ILOG CPLEX. 2009. High-performance software for
mathematical programming and optimization. U RL
http://www.ilog.com/products/cplex.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the
Fourteenth Conference on Computational Natural
Language Learning, CoNLL ?10, pages 107?116,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Xishuang Dong, Qibo Zou, and Yi Guan. 2012. Set-
similarity joins based semi-supervised sentiment
analysis. In Neural Information Processing, pages
176?183. Springer.
Andrea Esuli and Fabrizio Sebastiani. 2006. Sen-
tiwordnet: A publicly available lexical resource
for opinion mining. In In Proceedings of the 5th
Conference on Language Resources and Evaluation
(LREC06), pages 417?422.
Song Feng, Ritwik Bose, and Yejin Choi. 2011. Learn-
ing general connotation of words using graph-based
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 1092?1103. Association for Computa-
tional Linguistics.
Stephan Greene and Philip Resnik. 2009. More than
words: Syntactic packaging and implicit sentiment.
In Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North American
Chapter of the Association for Computational Lin-
guistics, pages 503?511, Boulder, Colorado, June.
Association for Computational Linguistics.
Vasileios Hatzivassiloglou and Kathleen R McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Proceedings of the eighth conference on
European chapter of the Association for Computa-
tional Linguistics, pages 174?181. Association for
Computational Linguistics.
Bas Heerschop, Alexander Hogenboom, and Flavius
Frasincar. 2011. Sentiment lexicon creation from
lexical resources. In Business Information Systems,
pages 185?196. Springer.
Nobuhiro Kaji and Masaru Kitsuregawa. 2007. Build-
ing lexicon for sentiment analysis from massive col-
lection of html documents. In Proceedings of the
2007 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning (EMNLP-CoNLL).
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. JOURNAL OF THE ACM,
46(5):604?632.
Bill Louw. 1993. Irony in the text or insincerity in
the writer. Text and technology: In honour of John
Sinclair, pages 157?176.
Yue Lu, Malu Castellanos, Umeshwar Dayal, and
ChengXiang Zhai. 2011. Automatic construction
of a context-aware sentiment lexicon: an optimiza-
tion approach. In Proceedings of the 20th interna-
tional conference on World wide web, pages 347?
356. ACM.
Saif Mohammad and Peter Turney. 2010. Emotions
evoked by common words and phrases: Using me-
chanical turk to create an emotion lexicon. In Pro-
ceedings of the NAACL HLT 2010 Workshop on
Computational Approaches to Analysis and Genera-
tion of Emotion in Text, pages 26?34, Los Angeles,
CA, June. Association for Computational Linguis-
tics.
Arturo Montejo-Ra?ez, Eugenio Mart??nez-Ca?mara,
M. Teresa Mart??n-Valdivia, and L. Alfonso Uren?a
Lo?pez. 2012. Random walk weighting over sen-
tiwordnet for sentiment polarity detection on twit-
ter. In Proceedings of the 3rd Workshop in Com-
putational Approaches to Subjectivity and Sentiment
Analysis, pages 3?10, Jeju, Korea, July. Association
for Computational Linguistics.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Found. Trends Inf. Retr., 2(1-
2):1?135.
Martin J Pickering and Holly P Branigan. 1998. The
representation of verbs: Evidence from syntactic
priming in language production. Journal of Mem-
ory and Language, 39(4):633?651.
1783
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009. Expanding domain sentiment lexicon through
double propagation. In Proceedings of the 21st in-
ternational jont conference on Artifical intelligence,
IJCAI?09, pages 1199?1204, San Francisco, CA,
USA. Morgan Kaufmann Publishers Inc.
Dan Roth and Wen-tau Yih. 2004. A linear program-
ming formulation for global inference in natural lan-
guage tasks. Defense Technical Information Center.
John Sinclair. 1991. Corpus, concordance, colloca-
tion. Describing English language. Oxford Univer-
sity Press.
Anatol Stefanowitsch and Stefan Th Gries. 2003. Col-
lostructions: Investigating the interaction of words
and constructions. International journal of corpus
linguistics, 8(2):209?243.
Philip J. Stone and Earl B. Hunt. 1963. A computer
approach to content analysis: studies using the gen-
eral inquirer system. In Proceedings of the May 21-
23, 1963, spring joint computer conference, AFIPS
?63 (Spring), pages 241?256, New York, NY, USA.
ACM.
Michael Stubbs. 1995. Collocations and semantic pro-
files: on the cause of the trouble with quantitative
studies. Functions of language, 2(1):23?55.
Kristina Toutanova and Christopher D. Manning.
2000. Enriching the knowledge sources used in
a maximum entropy part-of-speech tagger. In In
EMNLP/VLC 2000, pages 63?70.
Peter Turney. 2001. Mining the web for synonyms:
Pmi-ir versus lsa on toefl.
Leonid Velikovich, Sasha Blair-Goldensohn, Kerry
Hannan, and Ryan McDonald. 2010. The via-
bility of web-derived polarity lexicons. In Human
Language Technologies: The 2010 Annual Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics. Association for
Computational Linguistics.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Eval-
uation (formerly Computers and the Humanities),
39(2/3):164?210.
Theresa Wilson, Paul Hoffmann, Swapna Somasun-
daran, Jason Kessler, Janyce Wiebe, Yejin Choi,
Claire Cardie, Ellen Riloff, and Siddharth Patward-
han. 2005a. Opinionfinder: a system for subjec-
tivity analysis. In Proceedings of HLT/EMNLP on
Interactive Demonstrations, pages 34?35, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In HLT ?05: Proceedings
of the conference on Human Language Technology
and Empirical Methods in Natural Language Pro-
cessing, pages 347?354, Morristown, NJ, USA. As-
sociation for Computational Linguistics.
Rui Xie and Chunping Li. 2012. Lexicon construc-
tion: A topic model approach. In Systems and Infor-
matics (ICSAI), 2012 International Conference on,
pages 2299?2303. IEEE.
Xiaojin Zhu and Zoubin Ghahramani. 2002. Learn-
ing from labeled and unlabeled data with label prop-
agation. In Technical Report CMU-CALD-02-107.
CarnegieMellon University.
1784
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 790?796,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Generalizing Image Captions for Image-Text Parallel Corpus
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg and Yejin Choi
Department of Computer Science
Stony Brook University
Stony Brook, NY 11794-4400
{pkuznetsova,vordonezroma,aberg,tlberg,ychoi}@cs.stonybrook.edu
Abstract
The ever growing amount of web images
and their associated texts offers new op-
portunities for integrative models bridging
natural language processing and computer
vision. However, the potential benefits of
such data are yet to be fully realized due
to the complexity and noise in the align-
ment between image content and text. We
address this challenge with contributions
in two folds: first, we introduce the new
task of image caption generalization, for-
mulated as visually-guided sentence com-
pression, and present an efficient algo-
rithm based on dynamic beam search with
dependency-based constraints. Second,
we release a new large-scale corpus with
1 million image-caption pairs achieving
tighter content alignment between images
and text. Evaluation results show the in-
trinsic quality of the generalized captions
and the extrinsic utility of the new image-
text parallel corpus with respect to a con-
crete application of image caption transfer.
1 Introduction
The vast number of online images with accom-
panying text raises hope for drawing synergistic
connections between human language technolo-
gies and computer vision. However, subtleties and
complexity in the relationship between image con-
tent and text make exploiting paired visual-textual
data an open and interesting problem.
Some recent work has approached the prob-
lem of composing natural language descriptions
for images by using computer vision to retrieve
images with similar content and then transferring
?A house being 
pulled by a boat.? 
?I saw her in the light 
of her reading lamp 
and sneaked back to 
her door with the 
camera.? 
?Sections of the 
bridge sitting in the 
Dyer Construction 
yard south of 
Cabelas Driver.? 
Circumstantial 
information that is not 
visually present 
Visually relevant, 
but with overly 
extraneous details 
Visually truthful, 
but for an uncommon 
situation 
Figure 1: Examples of captions that are not readily
applicable to other visually similar images.
text from the retrieved samples to the query im-
age (e.g. Farhadi et al (2010), Ordonez et al
(2011), Kuznetsova et al (2012)). Other work
(e.g. Feng and Lapata (2010a), Feng and Lapata
(2010b)) uses computer vision to bias summariza-
tion of text associated with images to produce de-
scriptions. All of these approaches rely on ex-
isting text that describes visual content, but many
times existing image descriptions contain signifi-
cant amounts of extraneous, non-visual, or other-
wise non-desirable content. The goal of this paper
is to develop techniques to automatically clean up
visually descriptive text to make it more directly
usable for applications exploiting the connection
between images and language.
As a concrete example, consider the first image
in Figure 1. This caption was written by the photo
owner and therefore contains information related
to the context of when and where the photo was
taken. Objects such as ?lamp?, ?door?, ?camera?
are not visually present in the photo. The second
image shows a similar but somewhat different is-
sue. Its caption describes visible objects such as
?bridge? and ?yard?, but ?Cabelas Driver? are
overly specific and not visually detectable. The
790
Dependency Constraints with Examples Additional Dependency ConstraintsConstraints Sentence Dependency
advcl*(?) Taken when it was running... taken?running acomp*(?), advmod(?), agent*(?), attr(?)
amod(?) A wooden chair in the living room chair? wooden auxpass(?), cc*(?),complm(?), cop*(?)
aux(?) This crazy dog was jumping... jumping?was csubj*/csubjpass*(?),expl(?), mark*(?)
ccomp*(?) I believe a bear was in the box... believe?was infmod*(?), mwe(?), nsubj*/nsubjpass*(?)
prep(?) A view from the balcony view?from npadvmod(?), nn(?), conj*(?), num*(?)
det(?) A cozy street cafe... cafe?A number(?), parataxis(?),?
dobj*(?) A curious cow surveys the road... surveys?road partmod*(?), pcomp*(?), purpcl*(?)
iobj*(?) ...rock gives the water the color gives?water possessive(?), preconj*(?), predet*(?)
neg(?) Not a cloud in the sky... cloud?Not prt(?), quantmod(?), rcmod(?), ref(?)
pobj*(?) This branch was on the ground... on?ground rel*(?), tmod*(?), xcomp*(?), xsubj(?)
Table 1: Dependency-based Constraints
text of the third image, ?A house being pulled by a
boat?, pertains directly to the visual content of the
image, but is unlikely to be useful for tasks such as
caption transfer because the depiction is unusual.1
This phenomenon of information gap between the
visual content of the images and their correspond-
ing narratives has been studied closely by Dodge
et al (2012).
The content misalignment between images and
text limits the extent to which visual detectors
can learn meaningful mappings between images
and text. To tackle this challenge, we introduce
the new task of image caption generalization that
rewrites captions to be more visually relevant and
more readily applicable to other visually similar
images. Our end goal is to convert noisy image-
text pairs in the wild (Ordonez et al, 2011) into
pairs with tighter content alignment, resulting in
new simplified captions over 1 million images.
Evaluation results show both the intrinsic quality
of the generalized captions and the extrinsic util-
ity of the new image-text parallel corpus. The new
parallel corpus will be made publicly available.2
2 Sentence Generalization as Constraint
Optimization
Casting the generalization task as visually-guided
sentence compression with lightweight revisions,
we formulate a constraint optimization problem
that aims to maximize content selection and lo-
cal linguistic fluency while satisfying constraints
driven from dependency parse trees. Dependency-
based constraints guide the generalized caption
1Open domain computer vision remains to be an open
problem, and it would be difficult to reliably distinguish pic-
tures of subtle visual differences, e.g., pictures of ?a water
front house with a docked boat? from those of ?a floating
house pulled by a boat?.
2Available at http://www.cs.stonybrook.edu/
?ychoi/imgcaption/
to be grammatically valid (e.g., keeping articles
in place, preventing dangling modifiers) while re-
maining semantically compatible with respect to a
given image-text pair (e.g., preserving predicate-
argument relations). More formally, we maximize
the following objective function:
F (y;x) = ?(y;x, v) + ?(y;x)
subject to C(y;x, v)
where x = {xi} is the input caption (a sentence),
v is the accompanying image, y = {yi} is the
output sentence, ?(y;x, v) is the content selection
score, ?(y;x) is the linguistic fluency score, and
C(y;x, v) is the set of hard constraints. Let l(yi)
be the index of the word in x that is selected as the
i?th word in the output y so that xl(yi) = yi. Then,
we factorize ?(?) and ?(?) as:
?(y;x, v) =
?
i
?(yi, x, v) =
?
i
?(xl(yi), v)
?(y;x) =
?
i
?(yi, ..., yi?K)
=
?
i
?(xl(yi), ..., xl(yi?K))
where K is the size of local context.
Content Selection ? Visual Estimates:
The computer vision system used consists of 7404
visual classifiers for recognizing leaf level Word-
Net synsets (Fellbaum, 1998). Each classifier is
trained using labeled images from the ImageNet
dataset (Deng et al, 2009) ? an image database
of over 14 million hand labeled images orga-
nized according to the WordNet hierarchy. Image
similarity is represented using a Spatial Pyramid
Match Kernel (SPM) (Lazebnik et al, 2006) with
Locality-constrained Linear Coding (Wang et al,
2010) on shape based SIFT features (Lowe, 2004).
791
  (a) (b)
0 1 2 3 4 5 6 7 80
200400
600800
# of s
enten
ces (
in tho
usan
ds)
0 1 2 3 40
400
800
1200
# of s
enten
ces (
in tho
usan
ds)
Figure 2: Number of sentences (y-axis) for each
average (x-axis in (a)) and maximum (x-axis in
(b)) number of words with future dependencies
Models are linear SVMs followed by a sigmoid to
produce probability for each node.3
Content Selection ? Salient Topics:
We consider Tf.Idf driven scores to favor salient
topics, as those are more likely to generalize
across many different images. Additionally, we
assign a very low content selection score (??) for
proper nouns and numbers and a very high score
(larger then maximum idf or visual score) for the
2k most frequent words in our corpus.
Local Linguistic Fluency:
We model linguistic fluency with 3-gram condi-
tional probabilities:
?(xl(yi), xl(yi?1), xl(yi?2)) (1)
= p(xl(yi)|xl(yi?2), xl(yi?1))
We experiment with two different ngram statis-
tics, one extracted from the Google Web 1T cor-
pus (Brants and Franz., 2006), and the other com-
puted from the 1M image-caption corpus (Or-
donez et al, 2011).
Dependency-driven Constraints:
Table 1 defines the list of dependencies used
as constraints driven from the typed dependen-
cies (de Marneffe and Manning, 2009; de Marn-
effe et al, 2006). The direction of arrows indi-
cate the direction of inclusion requirements. For
example, dep(X ?? Y ), denotes that ?X? must
be included whenever ?Y ? is included. Similarly,
dep(X ?? Y ) denotes that ?X? and ?Y ? must
either be included together or eliminated together.
We determine the uni- or bi-directionality of these
constraints by manually examining a few example
sentences corresponding to each of these typed de-
pendencies. Note that some dependencies such as
det(??) would hold regardless of the particular
3Code was provided by Deng et al (2012).
Method-1 (M1) v.s. Method-2 (M2) M1 winsover M2
SALIENCY ORIG 76.34%
VISUAL ORIG 81.75%
VISUAL SALIENCY 72.48%
VISUAL VISUAL W/O CONSTR 83.76%
VISUAL NGRAM-ONLY 90.20%
VISUAL HUMAN 19.00%
Table 2: Forced Choice Evaluation (LM Corpus =
Google)
lexical items, while others, e.g., dobj(??) may
or may not be necessary depending on the context.
Those dependencies that we determine as largely
context dependent are marked with * in Table 1.
One could consider enforcing all dependency
constraints in Table 1 as hard constraints so that
the compressed sentence must not violate any of
those directed dependency constraints. Doing so
would lead to overly conservative compression
with least compression ratio however. Therefore,
we relax those that are largely context dependent
as soft constraints (marked in Table 1 with *) by
introducing a constant penalty term in the objec-
tive function. Alternatively, the dependency based
constraints can be learned statistically from the
training corpus of paired original and compressed
sentences. Since we do not have such in-domain
training data at this time, we leave this exploration
as future research.
Dynamic Programming with Dynamic Beam:
The constraint optimization we formulated corre-
sponds to an NP-hard problem. In our work, hard
constraints are based only on typed dependencies,
and we find that long range dependencies occur in-
frequently in actual image descriptions, as plotted
in Figure 2. With this insight, we opt for decoding
based on dynamic programming with dynamically
adjusted beam.4 Alternatively, one can find an ap-
proximate solution using Integer Linear Program-
ming (e.g., Clarke and Lapata (2006), Clarke and
Lapata (2007), Martins and Smith (2009)).
3 Evaluation
Since there is no existing benchmark data for im-
age caption generalization, we crowdsource evalu-
ation using Amazon Mechanical Turk (AMT). We
empirically compare the following options:
4The required beam size at each step depends on how
many words have dependency constraints involving any word
following the current one ? beam size is at most 2p, where p
is the max number of words dependent on any future words.
792
Big elm tree over 
the house is no 
their anymore. 
? Tree over the house. 
Abandonned 
houses in the 
forest. 
? Houses in the 
     forest. 
A woman paints a tree in 
bloom near the duck pond 
in the Boston Public 
Garden, April 15, 2006. 
? A tree in bloom . 
Pillbox in field 
behind a pub 
car park. 
? Pub car. 
Flowering tree in 
mixed forest at 
Wakehurst. 
? Flowering tree  
    in forest. 
The insulbrick matches 
the yard. This is outside 
of medina ohio near the 
tonka truck house. 
? The yard. This is 
     outside the house. 
Query Image Retrieved Images 
Figure 3: Example Image Caption Transfer
Method LM strict matching semantic matchingCorpus BLEU P R F BLEU P R F
ORIG N/A 0.063 0.064 0.139 0.080 0.215 0.220 0.508 0.276
SALIENCY Image Corpus 0.060 0.074 0.077 0.068 0.302 0.411 0.399 0.356
VISUAL Image Corpus 0.060 0.075 0.075 0.068 0.305 0.422 0.397 0.360
SALIENCY Google Corpus 0.064 0.070 0.101 0.074 0.286 0.337 0.459 0.340
VISUAL Google Corpus 0.065 0.071 0.098 0.075 0.296 0.354 0.457 0.350
Table 3: Image Description Transfer: performance in BLEU and F1 with strict & semantic matching.
? ORIG: original uncompressed captions
? HUMAN: compressed by humans (See ? 3.2)
? SALIENCY: linguistic fluency + saliency-based
content selection + dependency constraints
? VISUAL: linguistic fluency + visually-guided
content selection + dependency constraints
? x W/O CONSTR: method xwithout dependency
constraints
? NGRAM-ONLY: linguistic fluency only
3.1 Intrinsic Evaluation: Forced Choice
Turkers are provided with an image and two cap-
tions (produced by different methods) and are
asked to select a better one, i.e., the most relevant
and plausible caption that contains the least extra-
neous information. Results are shown in Table 2.
We observe that VISUAL (full model with visually
guided content selection) performs the best, being
selected over SALIENCY (content-selection with-
out visual information) in 72.48% cases, and even
over the original image caption in 81.75% cases.
This forced-selection experiment between VI-
SUAL and ORIG demonstrates the degree of noise
prevalent in the image captions in the wild. Of
course, if compared against human-compressed
captions, the automatic captions are preferred
much less frequently ? in 19% of the cases. In
those 19% cases when automatic captions are pre-
ferred over human-compressed ones, it is some-
times that humans did not fully remove informa-
tion that is not visually present or verifiable, and
other times humans overly compressed. To ver-
ify the utility of dependency-based constraints,
we also compare two variations of VISUAL, with
and without dependency-based constraints. As ex-
pected, the algorithm with constraints is preferred
in the majority of cases.
3.2 Extrinsic Evaluation: Image-based
Caption Retrieval
We evaluate the usefulness of our new image-text
parallel corpus for automatic generation of image
descriptions. Here the task is to produce, for a
query image, a relevant description, i.e., a visu-
ally descriptive caption. Following Ordonez et al
(2011), we produce a caption for a query image
by finding top k most similar images within the
1M image-text corpus (Ordonez et al, 2011) and
then transferring their captions to the query im-
age. To compute evaluation measures, we take the
average scores of BLEU(1) and F-score (unigram-
based with respect to content-words) over k = 5
candidate captions.
Image similarity is computed using two global
(whole) image descriptors. The first is the GIST
feature (Oliva and Torralba, 2001), an image de-
scriptor related to perceptual characteristics of
scenes ? naturalness, roughness, openness, etc.
The second descriptor is also a global image de-
scriptor, computed by resizing the image into a
?tiny image? (Torralba et al, 2008), which is ef-
fective in matching the structure and overall color
of images. To find visually relevant images, we
compute the similarity of the query image to im-
793
Huge wall of glass 
at the Conference 
Centre in 
Yohohama  Japan.  
? Wall of glass  
My footprint in a 
sand box 
? A sand box  
James the cat is 
dreaming of running 
in a wide green 
valley 
? Running in 
a valley (not 
relevant) 
This little boy was so 
cute. He was flying his 
spiderman kite all by 
himself on top of Max 
Patch  
? This little boy was so 
cute. He was flying 
(semantically odd) 
A view of the post office 
building in Manila from 
the other side of the 
Pasig River  
? A view of the post 
office building from 
the side  
Cell phone shot of 
a hat stall in the 
Northeast Market, 
Baltimore, MD.  
? Cell phone shot.  
(visually not 
verifiable) 
Figure 4: Good (left three, in blue) and bad examples (right three, in red) of generalized captions
ages in the whole dataset using an unweighted sum
of gist similarity and tiny image similarity.
Gold standard (human compressed) captions are
obtained using AMT for 1K images. The results
are shown in Table 3. Strict matching gives credit
only to identical words between the gold-standard
caption and the automatically produced caption.
However, words in the original caption of the
query image (and its compressed caption) do not
overlap exactly with words in the retrieved cap-
tions, even when they are semantically very close,
which makes it hard to see improvements even
when the captions of the new corpus are more gen-
eral and transferable over other images. Therefore,
we also report scores based on semantic matching,
which gives partial credits to word pairs based on
their lexical similarity.5 The best performing ap-
proach with semantic matching is VISUAL (with
LM = Image corpus), improving BLEU, Precision,
F-score substantially over those of ORIG, demon-
strating the extrinsic utility of our newly gener-
ated image-text parallel corpus in comparison to
the original database. Figure 3 shows an example
of caption transfer.
4 Related Work
Several recent studies presented approaches to
automatic caption generation for images (e.g.,
Farhadi et al (2010), Feng and Lapata (2010a),
Feng and Lapata (2010b), Yang et al (2011),
Kulkarni et al (2011), Li et al (2011), Kuznetsova
et al (2012)). The end goal of our work differs in
that we aim to revise original image captions into
5We take Wu-Palmer Similarity as similarity mea-
sure (Wu and Palmer, 1994). When computing BLEU with
semantic matching, we look for the match with the highest
similarity score among words that have not been matched be-
fore. Any word matched once (even with a partial credit) will
be removed from consideration when matching next words.
descriptions that are more general and align more
closely to the visual image content.
In comparison to prior work on sentence com-
pression, our approach falls somewhere between
unsupervised to distant-supervised approach (e.g.,
Turner and Charniak (2005), Filippova and Strube
(2008)) in that there is not an in-domain train-
ing corpus to learn generalization patterns directly.
Future work includes exploring more direct su-
pervision from human edited sample generaliza-
tion (e.g., Knight and Marcu (2000), McDonald
(2006)) Galley and McKeown (2007), Zhu et al
(2010)), and the inclusion of edits beyond dele-
tion, e.g., substitutions, as has been explored by
e.g., Cohn and Lapata (2008), Cordeiro et al
(2009), Napoles et al (2011).
5 Conclusion
We have introduced the task of image caption gen-
eralization as a means to reduce noise in the paral-
lel corpus of images and text. Intrinsic and extrin-
sic evaluations confirm that the captions in the re-
sulting corpus align better with the image contents
(are often preferred over the original captions by
people), and can be practically more useful with
respect to a concrete application.
Acknowledgments
This research was supported in part by the Stony
Brook University Office of the Vice President for
Research. Additionally, Tamara Berg is supported
by NSF #1054133 and NSF #1161876. We thank
reviewers for many insightful comments and sug-
gestions.
794
References
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
version 1. In Linguistic Data Consortium.
James Clarke and Mirella Lapata. 2006. Constraint-
based sentence compression: An integer program-
ming approach. In Proceedings of the COL-
ING/ACL 2006 Main Conference Poster Sessions,
pages 144?151, Sydney, Australia, July. Association
for Computational Linguistics.
James Clarke and Mirella Lapata. 2007. Modelling
compression with discourse constraints. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 1?11, Prague, Czech Republic, June.
Association for Computational Linguistics.
Trevor Cohn and Mirella Lapata. 2008. Sentence
compression beyond word deletion. In Proceedings
of the 22nd International Conference on Compu-
tational Linguistics (Coling 2008), pages 137?144,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Joao Cordeiro, Gael Dias, and Pavel Brazdil. 2009.
Unsupervised induction of sentence compression
rules. In Proceedings of the 2009 Workshop
on Language Generation and Summarisation (UC-
NLG+Sum 2009), pages 15?22, Suntec, Singapore,
August. Association for Computational Linguistics.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2009. Stanford typed dependencies manual.
Marie-Catherine de Marneffe, Bill Maccartney, and
Christopher D. Manning. 2006. Generating typed
dependency parses from phrase structure parses.
In Language Resources and Evaluation Conference
2006.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hi-
erarchical Image Database. In Conference on Com-
puter Vision and Pattern Recognition.
Jia Deng, Jonathan Krause, Alexander C. Berg, and
L. Fei-Fei. 2012. Hedging your bets: Optimizing
accuracy-specificity trade-offs in large scale visual
recognition. In Conference on Computer Vision and
Pattern Recognition.
Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-
sch, Margaret Mitchell, Karl Stratos, Kota Yam-
aguchi, Yejin Choi, Hal Daume III, Alex Berg, and
Tamara Berg. 2012. Detecting visual text. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
762?772, Montre?al, Canada, June. Association for
Computational Linguistics.
Ali Farhadi, Mohsen Hejrati, Mohammad Amin
Sadeghi, Peter Young1, Cyrus Rashtchian, Julia
Hockenmaier, and David Forsyth. 2010. Every pic-
ture tells a story: generating sentences for images.
In European Conference on Computer Vision.
Christiane D. Fellbaum, editor. 1998. WordNet: an
electronic lexical database. MIT Press.
Yansong Feng and Mirella Lapata. 2010a. How many
words is a picture worth? automatic caption genera-
tion for news images. In Association for Computa-
tional Linguistics.
Yansong Feng and Mirella Lapata. 2010b. Topic mod-
els for image annotation and text illustration. In Hu-
man Language Technologies.
Katja Filippova and Michael Strube. 2008. Depen-
dency tree based sentence compression. In Proceed-
ings of the Fifth International Natural Language
Generation Conference, INLG ?08, pages 25?32,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Michel Galley and Kathleen McKeown. 2007. Lex-
icalized Markov grammars for sentence compres-
sion. In Human Language Technologies 2007:
The Conference of the North American Chapter of
the Association for Computational Linguistics; Pro-
ceedings of the Main Conference, pages 180?187,
Rochester, New York, April. Association for Com-
putational Linguistics.
Kevin Knight and Daniel Marcu. 2000. Statistics-
based summarization - step one: Sentence compres-
sion. In AAAI/IAAI, pages 703?710.
Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-
ing Li, Yejin Choi, Alexander C Berg, and Tamara L
Berg. 2011. Babytalk: Understanding and gener-
ating simple image descriptions. In Conference on
Computer Vision and Pattern Recognition.
Polina Kuznetsova, Vicente Ordonez, Alexander Berg,
Tamara Berg, and Yejin Choi. 2012. Collective gen-
eration of natural image descriptions. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 359?368, Jeju Island, Korea, July. As-
sociation for Computational Linguistics.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching. In Conference on Computer Vision and
Pattern Recognition, June.
Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-
der C. Berg, and Yejin Choi. 2011. Composing
simple image descriptions using web-scale n-grams.
In Proceedings of the Fifteenth Conference on Com-
putational Natural Language Learning, pages 220?
228, Portland, Oregon, USA, June. Association for
Computational Linguistics.
David G. Lowe. 2004. Distinctive image features from
scale-invariant keypoints. Int. J. Comput. Vision,
60:91?110, November.
795
Andre Martins and Noah A. Smith. 2009. Summariza-
tion with a joint model for sentence extraction and
compression. In Proceedings of the Workshop on
Integer Linear Programming for Natural Language
Processing, pages 1?9, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Ryan T. McDonald. 2006. Discriminative sentence
compression with soft syntactic evidence. In EACL
2006, 11st Conference of the European Chapter of
the Association for Computational Linguistics, Pro-
ceedings of the Conference, April 3-7, 2006, Trento,
Italy. The Association for Computer Linguistics.
Courtney Napoles, Chris Callison-Burch, Juri Ganitke-
vitch, and Benjamin Van Durme. 2011. Paraphras-
tic sentence compression with a character-based
metric: Tightening without deletion. In Proceed-
ings of the Workshop on Monolingual Text-To-Text
Generation, pages 84?90, Portland, Oregon, June.
Association for Computational Linguistics.
Aude Oliva and Antonio Torralba. 2001. Modeling the
shape of the scene: a holistic representation of the
spatial envelope. International Journal of Computer
Vision.
Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
2011. Im2text: Describing images using 1 million
captioned photographs. In Neural Information Pro-
cessing Systems (NIPS).
Antonio Torralba, Rob Fergus, and William T. Free-
man. 2008. 80 million tiny images: a large dataset
for non-parametric object and scene recognition.
Pattern Analysis and Machine Intelligence, 30.
Jenine Turner and Eugene Charniak. 2005. Super-
vised and unsupervised learning for sentence com-
pression. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguis-
tics (ACL?05), pages 290?297, Ann Arbor, Michi-
gan, June. Association for Computational Linguis-
tics.
Jinjun Wang, Jianchao Yang, Kai Yu, Fengjun Lv,
T. Huang, and Yihong Gong. 2010. Locality-
constrained linear coding for image classification.
In Conference on Computer Vision and Pattern
Recognition (CVPR).
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ?94, pages 133?138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Yezhou Yang, Ching Teo, Hal Daume III, and Yiannis
Aloimonos. 2011. Corpus-guided sentence genera-
tion of natural images. In Proceedings of the 2011
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 444?454, Edinburgh, Scot-
land, UK., July. Association for Computational Lin-
guistics.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353?1361, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
796
