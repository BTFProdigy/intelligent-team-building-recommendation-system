Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 57?60,
Sydney, July 2006. c?2006 Association for Computational Linguistics
The SAMMIE System: Multimodal In-Car Dialogue
Tilman Becker, Peter Poller,
Jan Schehl
DFKI
First.Last@dfki.de
Nate Blaylock, Ciprian Gerstenberger,
Ivana Kruijff-Korbayova?
Saarland University
talk-mit@coli.uni-sb.de
Abstract
The SAMMIE1 system is an in-car multi-
modal dialogue system for an MP3 ap-
plication. It is used as a testing environ-
ment for our research in natural, intuitive
mixed-initiative interaction, with particu-
lar emphasis on multimodal output plan-
ning and realization aimed to produce out-
put adapted to the context, including the
driver?s attention state w.r.t. the primary
driving task.
1 Introduction
The SAMMIE system, developed in the TALK
project in cooperation between several academic
and industrial partners, employs the Information
State Update paradigm, extended to model collab-
orative problem solving, multimodal context and
the driver?s attention state. We performed exten-
sive user studies in a WOZ setup to guide the sys-
tem design. A formal usability evaluation of the
system?s baseline version in a laboratory environ-
ment has been carried out with overall positive re-
sults. An enhanced version of the system will be
integrated and evaluated in a research car.
In the following sections, we describe the func-
tionality and architecture of the system, point out
its special features in comparison to existing work,
and give more details on the modules that are in
the focus of our research interests. Finally, we
summarize our experiments and evaluation results.
2 Functionality
The SAMMIE system provides a multi-modal inter-
face to an in-car MP3 player (see Fig. 1) through
speech and haptic input with a BMW iDrive input
device, a button which can be turned, pushed down
and sideways in four directions (see Fig. 2 left).
System output is provided by speech and a graphi-
cal display integrated into the car?s dashboard. An
example of the system display is shown in Fig. 2.
1SAMMIE stands for Saarbru?cken Multimodal MP3 Player
Interaction Experiment.
Figure 1: User environment in laboratory setup.
The MP3 player application offers a wide range
of functions: The user can control the currently
playing song, search and browse an MP3 database
by looking for any of the fields (song, artist, al-
bum, year, etc.), search and select playlists and
even construct and edit playlists.
The user of SAMMIE has complete freedom in
interacting with the system. Input can be through
any modality and is not restricted to answers to
system queries. On the contrary, the user can give
new tasks as well as any information relevant to
the current task at any time. This is achieved by
modeling the interaction as a collaborative prob-
lem solving process, and multi-modal interpreta-
tion that fits user input into the context of the
current task. The user is also free in their use
of multimodality: SAMMIE handles deictic refer-
ences (e.g., Play this title while pushing the iDrive
button) and also cross-modal references, e.g., Play
the third song (on the list). Table 1 shows a typ-
ical interaction with the SAMMIE system; the dis-
played song list is in Fig. 2. SAMMIE supports in-
teraction in German and English.
3 Architecture
Our system architecture follows the classical ap-
proach (Bunt et al, 2005) of a pipelined architec-
ture with multimodal interpretation (fusion) and
57
U: Show me the Beatles albums.
S: I have these four Beatles albums.
[shows a list of album names]
U: Which songs are on this one?
[selects the Red Album]
S: The Red Album contains these songs
[shows a list of the songs]
U: Play the third one.
S: [music plays]
Table 1: A typical interaction with SAMMIE.
fission modules encapsulating the dialogue man-
ager. Fig. 2 shows the modules and their inter-
action: Modality-specific recognizers and analyz-
ers provide semantically interpreted input to the
multimodal fusion module that interprets them in
the context of the other modalities and the cur-
rent dialogue context. The dialogue manager de-
cides on the next system move, based on its model
of the tasks as collaborative problem solving, the
current context and also the results from calls to
the MP3 database. The turn planning module then
determines an appropriate message to the user by
planning the content, distributing it over the avail-
able output modalities and finally co-ordinating
and synchronizing the output. Modality-specific
output modules generate spoken output and graph-
ical display update. All modules interact with the
extended information state which stores all context
information.
Figure 2: SAMMIE system architecture.
Many tasks in the SAMMIE system are mod-
eled by a plan-based approach. Discourse mod-
eling, interpretation management, dialogue man-
agement and linguistic planning, and turn plan-
ning are all based on the production rule system
PATE2 (Pfleger, 2004). It is based on some con-
cepts of the ACT-R 4.0 system, in particular the
goal-oriented application of production rules, the
2Short for (P)roduction rule system based on (A)ctivation
and (T)yped feature structure (E)lements.
activation of working memory elements, and the
weighting of production rules. In processing typed
feature structures, PATE provides two operations
that both integrate data and also are suitable for
condition matching in production rule systems,
namely a slightly extended version of the general
unification, but also the discourse-oriented opera-
tion overlay (Alexandersson and Becker, 2001).
4 Related Work and Novel Aspects
Many dialogue systems deployed today follow a
state-based approach that explicitly models the
full (finite) set of dialogue states and all possible
transitions between them. The VoiceXML3 stan-
dard is a prominent example of this approach. This
has two drawbacks: on the one hand, this approach
is not very flexible and typically allows only so-
called system controlled dialogues where the user
is restricted to choosing their input from provided
menu-like lists and answering specific questions.
The user never is in control of the dialogue. For
restricted tasks with a clear structure, such an ap-
proach is often sufficient and has been applied suc-
cessfully. On the other hand, building such appli-
cations requires a fully specified model of all pos-
sible states and transitions, making larger applica-
tions expensive to build and difficult to test.
In SAMMIE we adopt an approach that mod-
els the interaction on an abstract level as collab-
orative problem solving and adds application spe-
cific knowledge on the possible tasks, available re-
sources and known recipes for achieving the goals.
In addition, all relevant context information is
administered in an Extended Information State.
This is an extension of the Information State Up-
date approach (Traum and Larsson, 2003) to the
multi-modal setting.
Novel aspects in turn planning and realization
include the comprehensive modeling in a sin-
gle, OWL-based ontology and an extended range
of context-sensitive variation, including system
alignment to the user on multiple levels.
5 Flexible Multi-modal Interaction
5.1 Extended Information State
The information state of a multimodal system
needs to contain a representation of contextual in-
formation about discourse, but also a represen-
tation of modality-specific information and user-
specific information which can be used to plan
system output suited to a given context. The over-
3http://www.w3.org/TR/voicexml20
58
all information state (IS) of the SAMMIE system is
shown in Fig. 3.
The contextual information partition of the IS
represents the multimodal discourse context. It
contains a record of the latest user utterance and
preceding discourse history representing in a uni-
form way the salient discourse entities introduced
in the different modalities. We adopt the three-
tiered multimodal context representation used in
the SmartKom system (Pfleger et al, 2003). The
contents of the task partition are explained in the
next section.
5.2 Collaborative Problem Solving
Our dialogue manager is based on an
agent-based model which views dialogue
as collaborative problem-solving (CPS)
(Blaylock and Allen, 2005). The basic building
blocks of the formal CPS model are problem-
solving (PS) objects, which we represent as
typed feature structures. PS object types form a
single-inheritance hierarchy. In our CPS model,
we define types for the upper level of an ontology
of PS objects, which we term abstract PS objects.
There are six abstract PS objects in our model
from which all other domain-specific PS objects
inherit: objective, recipe, constraint, evaluation,
situation, and resource. These are used to model
problem-solving at a domain-independent level
and are taken as arguments by all update opera-
tors of the dialogue manager which implement
conversation acts (Blaylock and Allen, 2005).
The model is then specialized to a domain by
inheriting and instantiating domain-specific types
and instances of the PS objects.
5.3 Adaptive Turn Planning
The fission component comprises detailed con-
tent planning, media allocation and coordination
and synchronization. Turn planning takes a set
of CPS-specific conversational acts generated by
the dialogue manager and maps them to modality-
specific communicative acts.
Information on how content should be dis-
tributed over the available modalities (speech or
graphics) is obtained from Pastis, a module which
stores discourse-specific information. Pastis pro-
vides information about (i) the modality on which
the user is currently focused, derived by the cur-
rent discourse context; (ii) the user?s current cog-
nitive load when system interaction becomes a
secondary task (e.g., system interaction while
driving); (iii) the user?s expertise, which is rep-
resented as a state variable. Pastis also contains
information about factors that influence the prepa-
ration of output rendering for a modality, like the
currently used language (German or English) or
the display capabilities (e.g., maximum number of
displayable objects within a table). Together with
the dialogue manager?s embedded part of the in-
formation state, the information stored by Pastis
forms the Extended Information State of the SAM-
MIE system (Fig. 3).
Planning is then executed through a set of pro-
duction rules that determine which kind of infor-
mation should be presented through which of the
available modalities. The rule set is divided in two
subsets, domain-specific and domain-independent
rules which together form the system?s multi-
modal plan library.
contextual-info:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
last-user-utterance:
:
[
interp : set(grounding-acts)
modality-requested : modality
modalities-used : set(msInput)
]
discourse-history:
: list(discourse-objects)
modality-info:
:
[
speech : speechInfo
graphic : graphicInfo
]
user-info:
:
[
cognitive-load : cogLoadInfo
user-expertise : expertiseInfo
]
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
task-info:
[
cps-state : c-situation (see below for details)
pending-sys-utt : list(grounding-acts)
]
Figure 3: SAMMIE Information State structure.
5.4 Spoken Natural Language Output
Generation
Our goal is to produce output that varies in the sur-
face realization form and is adapted to the con-
text. A template-based module has been devel-
oped and is sufficient for classes of system output
that do not need fine-tuned context-driven varia-
tion. Our template-based generator can also de-
liver alternative realizations, e.g., alternative syn-
tactic constructions, referring expressions, or lexi-
cal items. It is implemented by a set of straightfor-
ward sentence planning rules in the PATE system
to build the templates, and a set of XSLT trans-
formations to yield the output strings. Output in
German and English is produced by accessing dif-
ferent dictionaries in a uniform way.
In order to facilitate incremental development
of the whole system, our template-based mod-
ule has a full coverage wrt. the classes of sys-
59
tem output that are needed. In parallel, we are
experimenting with a linguistically more power-
ful grammar-based generator using OpenCCG4,
an open-source natural language processing en-
vironment (Baldridge and Kruijff, 2003). This al-
lows for more fine-grained and controlled choices
between linguistic expressions in order to achieve
contextually appropriate output.
5.5 Modeling with an Ontology
We use a full model in OWL as the knowledge rep-
resentation format in the dialogue manager, turn
planner and sentence planner. This model in-
cludes the entities, properties and relations of the
MP3 domain?including the player, data base and
playlists. Also, all possible tasks that the user may
perform are modeled explicitly. This task model
is user centered and not simply a model of the
application?s API.The OWL-based model is trans-
formed automatically to the internal format used
in the PATE rule-interpreter.
We use multiple inheritance to model different
views of concepts and the corresponding presen-
tation possibilities; e.g., a song is a browsable-
object as well as a media-object and thus allows
for very different presentations, depending on con-
text. Thereby PATE provides an efficient and ele-
gant way to create more generic presentation plan-
ning rules.
6 Experiments and Evaluation
So far we conducted two WOZ data collection
experiments and one evaluation experiment with
a baseline version of the SAMMIE system. The
SAMMIE-1 WOZ experiment involved only spo-
ken interaction, SAMMIE-2 was multimodal, with
speech and haptic input, and the subjects had
to perform a primary driving task using a Lane
Change simulator (Mattes, 2003) in a half of their
experiment session. The wizard was simulating
an MP3 player application with access to a large
database of information (but not actual music) of
more than 150,000 music albums (almost 1 mil-
lion songs). In order to collect data with a variety
of interaction strategies, we used multiple wizards
and gave them freedom to decide about their re-
sponse and its realization. In the multimodal setup
in SAMMIE-2, the wizards could also freely de-
cide between mono-modal and multimodal output.
(See (Kruijff-Korbayova? et al, 2005) for details.)
We have just completed a user evaluation to
explore the user-acceptance, usability, and per-
formance of the baseline implementation of the
4http://openccg.sourceforge.net
SAMMIE multimodal dialogue system. The users
were asked to perform tasks which tested the sys-
tem functionality. The evaluation analyzed the
user?s interaction with the baseline system and
combined objective measurements like task com-
pletion (89%) and subjective ratings from the test
subjects (80% positive).
Acknowledgments This work has been carried
out in the TALK project, funded by the EU 6th
Framework Program, project No. IST-507802.
References
[Alexandersson and Becker2001] J. Alexandersson and
T. Becker. 2001. Overlay as the basic operation for
discourse processing in a multimodal dialogue system. In
Proceedings of the 2nd IJCAI Workshop on Knowledge
and Reasoning in Practical Dialogue Systems, Seattle,
Washington, August.
[Baldridge and Kruijff2003] J.M. Baldridge and G.J.M. Krui-
jff. 2003. Multi-Modal Combinatory Categorial Gram-
mar. In Proceedings of the 10th Annual Meeting of the
European Chapter of the Association for Computational
Linguistics (EACL?03), Budapest, Hungary, April.
[Blaylock and Allen2005] N. Blaylock and J. Allen. 2005. A
collaborative problem-solving model of dialogue. In Laila
Dybkj?r and Wolfgang Minker, editors, Proceedings of
the 6th SIGdial Workshop on Discourse and Dialogue,
pages 200?211, Lisbon, September 2?3.
[Bunt et al2005] H. Bunt, M. Kipp, M. Maybury, and
W. Wahlster. 2005. Fusion and coordination for multi-
modal interactive information presentation: Roadmap, ar-
chitecture, tools, semantics. In O. Stock and M. Zanca-
naro, editors, Multimodal Intelligent Information Presen-
tation, volume 27 of Text, Speech and Language Technol-
ogy, pages 325?340. Kluwer Academic.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG, pages
191?196.
[Mattes2003] S. Mattes. 2003. The lane-change-task as a tool
for driver distraction evaluation. In Proc. of IGfA.
[Pfleger et al2003] N. Pfleger, J. Alexandersson, and
T. Becker. 2003. A robust and generic discourse model
for multimodal dialogue. In Proceedings of the 3rd
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems, Acapulco.
[Pfleger2004] N. Pfleger. 2004. Context based multimodal
fusion. In ICMI ?04: Proceedings of the 6th interna-
tional conference on Multimodal interfaces, pages 265?
272, New York, NY, USA. ACM Press.
[Traum and Larsson2003] David R. Traum and Staffan Lars-
son. 2003. The information state approach to dialog man-
agement. In Current and New Directions in Discourse and
Dialog. Kluwer.
60
An Extended Architecture for Robust  Generation* 
T i lman Becker ,  Anne  K i lger ,  Pat r i ce  Lopez ,  Peter  Po l le r  
DFK I  GmbH 
Stuh lsatzenhausweg 3 
D-66123 Saarbr i i cken ,  Germany 
{becker, kilger, lopez, poller}@dfki, de 
Abst rac t  
Based on our experiences in VERBMOBIL, a large 
scale speech-to-speech translation system, we iden- 
tify two types of problems that a generation com- 
ponent must address in a realistic implementation 
and present relevant examples. As an extension to 
the architecture ofa translation system, we present a
module for robustness preprocessing on the interface 
between translation and generation. 
1 In t roduct ion  
Based on our experiences with VERBMOBIL, a large 
scale speech-to-speech translation system, we iden- 
tify two types of problems that a generation com- 
ponent must address in a comprehensive implemen- 
tation. Besides general task-inherent problems like, 
e.g., the processing of spontaneous speech input, the 
translation step itself, and real-time processing, we 
found that an implementation of such a large scale 
system additionally exhibits technical problems that 
are caused by various faults in the steps prior to 
generation. 
The task of VERBMOBIL is the multi-lingual (Ger- 
man, English, Japanese) speaker-independent trans- 
lation of spontaneous peech input that enables 
users to converse about the scheduling of a busi- 
ness appointment including travel, accommodation, 
and leisure time planning in a multi-lingual dia- 
logue. The system covers 10,000 words in each 
language with the corresponding knowledge bases 
(grammars, translation rules, etc.). In contrast to 
a text translation system, the processing of spon- 
taneous speech requires extended functionalities in 
almost ever:,- module because the system has to be 
able do deal with, e.g., ill-formed and disfluent (hes- 
Due to the high complexity of this task, the sys- 
tem is subdivided into 24 separate subtasks (imple- 
mented modules). 
For the translation step the system contains 
four different parallel translation "tracks" consist- 
ing Of three "shallow" (case based, statistical, and 
dialogue-act based (Reithinger, 1999)) and one 
"deep" translation track (see figure 1) for each lan- 
guage. The individual translation results are partly 
associated with confidence values reflecting their 
quality and then sent to a special selection compo- 
nent to choose the most appropriate one. Our prac- 
tical experience shows that there are cases in which 
the input to the generation component is impossible 
to process. Here the shallow translation paths serve 
as a fall-back in order to fulfill the strong necessity 
of a translation result as far as possible. 
Although some parts of the analysis task (e.g., re- 
solving scopal ambiguities) can actually be left un- 
solved when they are not necessary for the transla- 
tion task, in general, problems in some module result 
in an accumulated inadequacy of the final transla- 
tion. 
Since the translation task is distributed to a set 
of cooperating modules, there is a choice of solving 
the task inherent and technical problems either lo- 
cally inside the individual modules or handing them 
to problem specific correction modules. We found 
that robustness must be included in every module. 
For the architecture of the generation module, we 
have devised a submodule for robustness that pre- 
processes the input data. This proved an elegant 
and promising extension to achieve the required local 
module robustness without touching tile core gener- 
ation module directly. A similar module also ex- 
ists for analysis (see 'Robust Semantics' in figure 1), 
itations, repetitions, repairs) speech input. In a dia- (Worm, 1998). 
logue system, there is also.anapparently simp! ebut :. ? In.this paper~ we,foeus..on the generation eompo- 
very strong constraint on the system to achieve its 
task: For each user input the system has to produce 
a translation result. 
" The research within VERBMOBIL presented here is funded 
by the German Ministry of Research and q~mhnology under 
grant 011\.'101K/1. 
nent of our system. Besides the general robustness 
requirements, the mentioned inadequacy accunmla- 
tion reaches its maxinmm since generation is posi- 
tioned at the end of the translation process. In the 
following sections, we show how the strong robust- 
ness requirement influenced the architecture of our 
63 
User 1 
Language A 
User 2 
Language B 
Speech 
777 W ..... 
Figure 1: Simplified system architecture of the speech-to-speech translation system VERBMOBIL. 
generation module. We classify the above mentioned 
problems from the point of view of generation and 
present our solutions to these problems, mainly un- 
der the aspect of robust generation with problematic 
input. 
2 Task - inherent  and  Techn ica l  
Prob lems 
The problems for generation that arise in a speech- 
to-speech translation system fall into two main 
classes: as in any large-scale system, there will 
be software-engineering problems which we will call 
technical problems and there are task-inherent prob- 
lems that are particular to the translation task and 
the highly variable input in spontaneous peech. 
Since it is impossible to draw a clear line be- 
tween technical and task-inherent problems, we will 
present a short classification and then go into more 
detail without much discussion whether a certain 
problem should be viewed as technical or task- 
inherent. 
One would hope to be able to eliminate technical 
problems completely. However, in a large system, 
where development is distributed over many mod- 
ules (implemented at different sites), some robust- 
ness against certain technical problems can become 
a necessity, as our experiences have shown. This is 
even more important during the development phase- 
which a research system never leaves. Most technical 
problems have to do with violations of the interface 
definitions. Thisranges. from simple ~things uch as 
using unknown predicates in the semantic represen- 
tation to complex constructions that cannot be gen- 
erated (the generation gap). We actually regard the 
latter as a task-inherent problem. 
Secondly, tile task-inherent problems can be di- 
vided into problems that are caused by (i) spon- 
taneous speech input and (ii) insufficiencies in the 
analysis and translation steps. 
2.1 Robustness  in Ana lys i s  
The problems in (i) are quite varied and many cases 
are dealt with in analysis (and translation), some 
cases are dealt with in our robustness preprocess- 
ing submodule, a few in the classical submodules of 
generation. For example, there is a separate mod- 
ule on the level of speech recognition which deals 
with hesitations and self-corrections. Phenomena 
like ellipsis, phrasal and other incomplete utterances 
are handled by analysis, so generation must be able 
to deal with the corresponding semantic representa- 
tions too. Agreement errors are handled (i.e., cor- 
rected) in analysis. But some serious syntactic errors 
cannot be corrected. However, at least the maxi- 
mal analyzable segments are determined so that un- 
grammatical utterances are translated as sequences 
of several meaningful segments. 
2.2 Robustness  in Generat ion  
The problems in (ii) are caused by an accunmla- 
tion of problems which result in (semantic) input to 
the generator that cannot be processed. Robustness 
in our system concentrates on this type of problenl 
which is and should be handled as a separate step 
between analysis/transfer and generation. (See the 
discussion of the architecture in section 3.) 
The list below contains some examples that are 
picked up again in section 4. 
* Problems with the structure of the semantic rep- 
resentation: 
- unconnected subgraphs 
- multiple predicates referring to the same 
object 
64 
- omission of obligatory arguments 
? Problems with the content of the semantic rep- 
resentation: 
- contradicting information 
- missing information (e.g. agreement infor- 
mation) 
3 Arch i tecture  
As described in section t, the  deep processing in 
VERBMOBIL is based on a pipeline of modules which 
use a unique interface language (VIT 1) that incorpo- 
rates a semantic representation. Since this seman- 
tic representation is obviously grammar-independent 
and could reflect the effects of spoken, spontaneous 
language, we have no guarantee that the gram- 
mar covers the semantic representation given by the 
transfer module. Consequently we have chosen to 
extend the classical generation architecture with a 
new module dedicated to robust preprocessing. We 
first present our classical generator architecture (see 
also (Becker et al, 1998; Becker et al, 2000)) in 
terms of the RAGS architecture and then discuss its 
extension to the task-inherent problems. 
The RAGS architecture (Cahill et al, 1999) is a 
reference architecture for natural language genera- 
tion systems. Reflecting the common parts of natu- 
ral language generation systems, this proposal aims 
to provide a standard architecture allowing the iden- 
tification of some important generation subtasks and 
resources. By presenting our system in the light of 
the RAGS specifications, our goal is to propose gen- 
eral solutions that could be used by other researchers 
who need to extend their own generation architec- 
ture to similar tasks. 
While the macro-planning task is important and 
mandatory in text generation, it is limited in dia- 
logue translation. Most of the related problems, for 
instance the sentence segmentation a d the pronoun 
choices, have been solved by the user in the source 
language. Considering the RAGS architecture, con- 
ceptual and rhetorical evels of representation are 
also outside the scope of our system. Our architec- 
ture consists of four main modules (see figure 2). 
For an easy adaptation to other domains and lan- 
guages, we have emphasized an organization based 
on a general kernel system and the declarativity of 
knowledge sources (Becker et al, 1998). All but the 
first modules are captured by the RAGS architec- 
ture. However, the first module is dedicated solely 
to robustness in the specific speech-to-speech trans- 
lation task and will be presented and discussed last 
in this section. It can easily be added to a RAGS- 
like system whose whiteboard is perfectly suited for 
lVerbmobil Interface Term, (Bos et al, 1996; Dorna, 
1996) 
the transformations that the robustness preprocess- 
ing module performs. 
Robustness 
Preprocessing 
Module 
Standard 
Generation 
Module 
Repairing Strutural 
kx~.~ss ing  GHae~risfics for Generation J 
(%e:::::: 
Selecting Planning Ru les~ 
Checking Lexical Choice J 
C0nstraints . - ~ . .  
e Selecting LTAG Trees 
e Tree Combination 
? Inflection 
e Synthesis Annotation 
Figure 2: An extended generation system architec- 
ture 
M ic rop lann ing  Modu le  At the level of sentence 
generation, the quality of the planning process de- 
pends on the interdependencies between conceptual 
semantics, predicative semantics and syntax. A par- 
ticular lexical choice can imply constraints on other 
lexical items. The role of the microplanner is to re- 
alize lexical choices in a way that a syntactic realiza- 
tion is possible and costly backtracking is prevented. 
The microplanning task can be viewed as a con- 
straint solving problem and implemented using an 
adapted constraint solving mechanism in order to 
achieve efficiency, flexibility, and declarativity of 
knowledge. The microplanner produces a depen- 
dency tree representation i dicating for each node 
a set of syntactical constraints to be fulfilled by 
the corresponding lexical syntactic units (predicate, 
tense, aspect, mood, etc.). 
Syntact ic  Rea l izat ion Modu le  This module is 
in charge of the concrete syntax generation. The 
processing is .based ,on a fully lexicatized Tree- 
Adjoining Grammar derived from the HPSG gram- 
mar used in the deep-processing parser module 
(Kasper~et aL, 1995; Becker, 1998). 
S u r f a c e  Real izat ion  Modu le  The syntactic re- 
alization module produces a derived tree from which 
tile output string is extracted. The morphological 
features in preterminal nodes are used for inflection. 
The surface string is also annotated by syntactic in- 
formation (phrase boundary, aspect, sentence mood) 
65 
that are exploited by the speech synthesis module. 
Robustness Preprocess ing  Modu le  We have 
described three modules corresponding to classical 
tasks of generation systems and pointed out at the 
beginning of this section the necessity for robustness. 
Where can we integrate the required robustness in 
such a generation architecture? One approach could 
be the relaxation of constraints during the syntac- 
tic realization (relaxing word order or/and depen- 
dency relations). One can argue against this ap- 
proach that: 
clearly separated from the microplanning rules, jus- 
tifying our presentation of robustness as a separate 
module. 
4.2 Conforming  to the Interface Language 
Definition 
The definition of the interface language 2 comprises 
only its syntax and some semantic constraints. 
There is an implementation of expressions in the in- 
terface language as an abstract data type which can 
at least check syntactic conformity (Dorna, 1996). 
But we also have to deal with semantic faults. 
-*. There is no .straightf~r~ard~Way~t~Aimi.t~he.J~e~.:.~.~`-~,;~T~f~rs~e~amp~e~i~''minating>r`0bust~pre~r~ess- 
laxation of syntactic onstraints only to the ro- 
bustness problematic structures. 
? We must be sure that the microplanning module 
can deal with problematic semantic input. 
These points suggest to check and repair the 
inconsistencies of the semantic representation as 
early as possible, i.e., before sentence microplanning. 
Moreover we show in the next section that most of 
the problems presented in section 2 can be identified 
based on the microplanner input. 
We now present more concretely the robust pre- 
processing module. 
4 Robustness  
In this section we describe the types of problems 
defined in section 2 using examples from our system 
and discuss how our module is made robust enough 
to handle a lot of these problems. 
Before the semantic representation is handed to 
microplanning, the robustness preproeessing module 
of the generator checks the input, inspecting its parts 
for known problems. For each problem found, the 
preprocessor lowers a confidence value for the gen- 
eration output which measures the reliability of our 
result. In a number of cases, we use heuristics to fix 
problems, aiming at improved output. 
As discussed in section 2, problems in the input 
to the generator can be technical or task-inherent. 
Technical problems manifest themselves as faults 
wrt. the interface language definition, whereas the 
task-inherent problems concern mismatches between 
a specific semantic expression and the coverage of 
the natural language grammar used in the genera- 
tor. These mismatches are known as the generation 
gap (Meteer, 1990). 
4.1 Dec la ra t iv i ty  
In..our implementation; the  :robustness module is 
partly integrated into the constraint solving ap- 
proach of the microplanning module. Using the con- 
straint solving approach allows for a strict separa- 
tion of algorithms (i.e., some constraint solving al- 
gorithln) and declarative knowledge sources. On this 
level, the rules (constraints) for robustness can be 
ing is on the connectedness of the semantic input 
graph. Our interface language describes an interface 
term to contain a connected semantic graph plus an 
index pointing to the root of the graph. Two types 
of problems can occur according to this definition: 
Disconnectedness of the Graph: The robust- 
ness preprocessor checks whether the input 
graph is in fact connected. If there are several 
disconnected parts, a distinct generation call 
is made for each subgraph. In the end, all 
sub-results are connected to produce a global 
result. We are currently working on a better 
heuristic to order the sub-results, taking into 
account information about the word order in 
the source language. 
Wrong Index:  The robustness preprocessor tests 
whether the index points to the root of the 
graph or one of the subgraphs. For each sub- 
graph without an adequate index, we compute 
a local root pointer which is used for further 
processing. This turned out to be an easy and 
reliable heuristic, leading to good results. 
There are several types of technical problems 
which cannot be repaired well. Minimally, these 
cases are detected, warning messages are produced, 
and the confidence value is lowered. We apply 
heuristics where possible. Examples are unique- 
ness of labels (every semantic predicate must have 
a unique identifier), the use of undefined predicate 
names, and contradicting information (e.g., the use 
of  a DEFINITE and an INDEFINITE quantifier for the 
same object). In the case of incorrect predicate 
classes, i.e., where a predicate is used with an unde- 
fined-argument frame, only those parts of the input 
are handled which are analyzed as correct. 
4.3 Fal l ing into the Generat ion  Gap 
The robustness preprocessor even does more than 
checking for structural contradictions between in- 
put and interface language. Based on analyses of 
2A further complication in a research system like ours 
s tems from the fact that the interface language itself is de- 
veloped, i.e., changed over time. 
66 
a large amount of test-suites it is fed with some 
heuristics which help to bridge the generation gap 
that reflects the unpredictability, whether_a specific 
semantic structure can be mapped to an acceptable 
utterance in the target language. Some examples of 
heuristics used in our system are as follows: 
Conf l ic t ing In fo rmat ion :  Often it is inconsistent 
to allow several predicates to include the same 
depending structure in their argument frames, 
e.g., two predicates describing different prepo- 
sitions should not point to the same entity. We 
have to pick one-,possibitity~heuristically: ........ 
Gaps  in Generat ion  Knowledge:  There are in- 
put configurations that have no reflection 
within the generator's knowledge bases, e.g., 
the DISCOURSE predicate defining a sequence 
of otherwise unrelated parts of the input. The 
robustness preprocessor removes this predicate, 
thereby subdividing the connected graph into 
several unconnected ones and continuing as for 
disconnected graphs described above. 
Other examples for generation constraints that 
can conflict with the input are the occurrence 
of some specific cyclic subparts of graphs, self- 
referring predicates, and chains of predicates 
which are not realizable in generation. 
Robustness  inside the  Microp lanner  and the  
Syntact i c  Generator  additionally helps to get rid 
of some generation gap problems: 
Cont rad ic t ions  to Generat ion  Constra ints :  
The knowledge bases of the generator (mi- 
eroplanning rules, grammar and lexicon) 
describe constraints on the structure of the 
output utterance that might conflict with the 
input. A common problem occuring in our 
system is the occurrence of subordinating 
predicates with empty obligatory arguments. 
Here the microplanner relaxes the constraint 
for argument completeness and hands over a 
structure to the syntactic generator that does 
not fulfill all syntactic constraints or contains 
elliptical arguments. In these cases, the gram- 
mar constraints for obligatory arguments are 
relaxed in the syntactic generator and elliptical 
arguments are allowed.beyond the constraints 
of the grammar. The result is often output that 
reflects the spontaneous speech input which we 
accept for the sake of robustness. 
M iss ing  a t t r ibutes :  Often there are obligatory at- 
tributes for the semantic predicates missing in 
the input, e.g., statements about the direction- 
ality of prepositions, agreement information, 
etc. The generator uses heuristics to choose a 
value for its own. 
Cont rad ic t ions  on  the  Semant ic  Level: Some 
attributes may lead to conflicts during genera- 
tion,.e.g:, i f~ pronoun is given:as SORT~HUMAN 
and TYPE-~SPEAKER. The generator uses a 
heuristics to set the value of SORT in this case. 
So lv ing Part  o f  the  Ana lys i s  Task: Sometimes 
the input to the generator is underspecified in
a way that it can be improved easily by using 
simple heuristics to "continue analysis." A 
common example in. our system is an input 
expression like "on the third" which often is 
.... .. ~analyzed..as.. (ABSTR.,-NOM .A  OPal)(.3.).), e~-e,,..an 
elliptical noun with ordinal number 3. We 
add the sort TIME_DOFM 3 to the attributes of 
ABSTR_NOM SO that, e.g., a semantic relation 
TEMPORAL_OR_LOCAL is correctly mapped to 
the German preposition "an." 
4.4 How much robustness?  
There is a limit to the power of heuristics that we 
have determined using a large corpus of test data. 
Some examples for possible pitfalls: 
? When realizing "empty nominals" ABSTR_NOM 
as elliptical nouns, guessing the gender can 
cause problems: "Thursday is my free day ti " 
as FREE A DAY A ABSTR_NOM (with a reading 
as in "day job") might result in "*Donnerstag 
ist mein freies Tag ti." 
o Conflicts between sort and gender of a pronoun 
might be resolved incorrectly: "Es (English: 
'it') trifft sich ganz hervorragend" with PRON 
(GENDER:NTR, SORT~HUMAN) should not be 
translated as "#He is really great." 
Although the boundary beyond which deep trans- 
lation cannot be achieved even with heuristics is 
somewhat arbitrary, the big advantage of deep pro- 
cessing lies in the fact that the system 'knows' its 
boundaries and actually fails when a certain level of 
quality cannot be guaranteed. As discussed in sec- 
tion 1, in a dialogue system, a bad translation fight 
still be better than none at all, so one of the shallow 
modules can be selected when deep processing fails. 
5 Re la ted  Work  and  Conc lus ions  
VERB~;IOBIL also contains a component hat au- 
tomatically generates dialogue scripts and result 
summaries of the dialogues in all target languages 
(Alexandersson and Poller, 1998; Alexandersson and 
::Poller~ 2000 )~:: ~This component , uses the'generation 
modules of VERB1V\[OBIL for sentence generation as 
well as the translation system itself to achieve multi- 
linguality. To some extend, this task also benefits 
3DOFM: day of the month. Note that in this paper, the 
presentation of the semantic representation language is highly 
abstracted from tile actual interface language. 
67 
from the task inherent robustness features of the 
overall system and its modules which we described 
in this paper. 
Our problem classification also shows up in other 
generation systems. There is a multi-lingual gen- 
eration project (Uchida et al, 1999) that utilizes 
an interlingua-based semantic representation to gen- 
erate web-documents in different output languages 
from one common representation. Although techni- 
cal problems are less prominent, the task-inherent 
problems are almost he same. Again, the genera- 
R. Kasper, B. Kiefer, K. Netter, and K. Vijay- 
Shanker. 1995. Compilation of hpsg to tag. In 
. . . . .  Proceedings o f  .the:..33rd ~Aunual: Meeting :of the 
Association for Computational Linguistics, pages 
92-99, Cambridge, Mass. 
M.W. Meteer. t990. The "Generation Gap" - The 
Problem of Expressibility in Text Planning. Ph.D. 
thesis, Amherst, MA. BBN Report No. 7347. 
Norbert Reithinger. 1999. Robust information ex- 
traction in a speech translation system. In Pro- 
ceedings of EuroSpeech-99, pages 2427-2430. 
tot has to able to deal with, e.g., disconnected or Hiroshi Uchida, Meiying Zhu, and Tarcisio Della 
? contradicting inpu't graphs: . . . . . . . . . . . . . . . . . . . .  ~" ~ Sieiiit~2 "1999.-UNL::~I~S;" U/iitei:l ~Na~i6hg"Ufii-~~ ::: - 
sity, Tokyo, Japan, November. 
Re ferences  
Jan Alexandersson and Peter Poller. 1998. Toward 
multilingual protocol generation for spontaneous 
speech dialogues. In Proceedings of the Ninth In- 
ternational Workshop on Natural Language Gen- 
eration, Niagara-on-the-Lake, Ontario, Canada, 
August. 
Jan Alexandersson and Peter Poller. 2000. Multi- 
lingual summary generation i a speech-to-speech 
translation system for multilingual negotiation di- 
alogues. In Proceedings of INLG 2000, Mitzpe Ra- 
mon, Israel, June. 
T. Becker, W. Finkler, A. Kilger, and P. Poller. 
1998. An efficient kernel for multilingual genera- 
tion in speech-to-speech dialogue translation. In 
Proceedings of COLING/A CL-98, Montreal, Que- 
bec, Canada. 
Tilman Becker, Anne Kilger, Patrice Lopez, and 
Peter Poller. 2000. Multilingual generation for 
translation in speech-to-speech dialogues and its 
realization in verbmobil. In Proceedings of ECAI 
2000, Berlin, Germany, August. 
Tihnan Becker. 1998. Fully lexicalized head- 
driven syntactic generation. In Proceedings of the 
Ninth International Workshop on Natural Lan- 
guage Generation, Niagara-on-the-Lake, Ontario, 
Canada, August. 
Johan Bos, Bj6rn Gambfi.ck, Christian Lieske, 
Yoshiki Mori, Manfred Pinkal, and Karsten 
Worm. 1996. Compositional semantics in verb- 
mobil. In Proceedings of Coling '96, Copenhagen, 
Denmark. 
Lynne Cahill, Christy Doran, Roger Evans, Chris 
Mellish, Daniel Paiva, Mike Reape, Donia Scott, 
and Neil Tipper. 1999. Towards a Reference Ar- 
chitecture for Natural Language: Generation Sys- 
tems. Technical Report ITRI-99-14, Information 
Technology Research Institute (ITRI), University 
of Brighton. 
Michael Dorna. 1996. The adt package for the verb- 
mobil interface term. Verbmobil-lrleport 104, Uni- 
versity Stuttgart, April. 
Karsten Worm. 1998. A model for robust processing 
of spontaneous speech by integrating viable frag- 
ments. In Proceedings of COLING-ACL '98, Mon- 
treal, Canada. 
68 
Ends-based Dialogue Processing  
Jan Alexandersson, Tilman Becker, Ralf Engel, Markus Lo?ckelt,
Elsa Pecourt, Peter Poller, Norbert Pfleger and Norbert Reithinger
DFKI GmbH
Stuhlsatzenhausweg 3
66123 Saarbru?cken
 janal,becker,engel,loeckelt,pecourt,poller,pfleger,bert  @dfki.de
Abstract
We describe a reusable and scalable dialogue
toolbox and its application in multiple systems.
Our main claim is that ends-based representa-
tion and processing throughout the complete
dialogue backbone it essential to our approach.
1 Introduction
In the last couple of years our group at DFKI in
Saarbru?cken has been involved in a number of projects
aiming at interfacing different devices in an intelligent
way. The main goal of these projects has been to build
functioning robust systems with which it is natural to
communicate (not only for some few examples phrases).
During the projects we have developed a dialogue tool-
box consisting of a number of modules. By combining
these modules in different ways we are able to realize a
number of different types of dialogues, e. g., information
seeking/browsing, device control, multi/cross-application
and agent-mediated interactions for a number of (diverse)
applications and systems. The full-blown combination of
all modules form our dialogue backbone capable of en-
gaging in multimodal man?machine communication.
In this paper, we discuss some of the design decisions
taken along the road as well as lessons learned during the
projects. Based on our experiences, we argue that ends-
based processing is vital to the success of our approach.
We strive for a balance between complex theories and
pragmatic decisions. Of secondary interest is the imple-
mentation of theories capable of processing linguistically
exotic phenomena in favor of ends-based processing in
all modules of the toolbox. Hence it is more important to
reach the representation rather than how we get there.
An ontology is often ? as we understand it ? a good
ends-based representation but we can do without it. In
the MIAMM project (see section 2) we use no ontology

The research presented here is funded by the German Min-
istry of Research and Technology under grant 01 IL 905, the Eu-
ropean Union under the grants IST-2000-29487 and IST-2001-
32311 and IDS-Scheer AG.
but instead an event based representation. Whatever rep-
resentation we do choose, we would like to stress the im-
portance of a consequent principle-based design of the
representation and the fact that the complete backbone
uses it. Exactly this guarantees, e. g., the scalability of
our approach.
The paper is organized as follows: the next section pro-
vides an overview of projects and systems central to the
development of our toolbox. Section 3 describes most of
its modules. Before we conclude the paper, we provide a
list of claims and lessons learned in section 4.
2 A Number of Projects
In this paper, we describe a toolbox which we can cus-
tomize according to the projects needs. Using this tool-
box we have implemented a number of systems, all hav-
ing different requirements, needs and ends. They range
from (monomodal) typed input/output as in the NaRATo
project to multimodal agent-mediated communication as
in SmartKom. Below we describe the different projects
and systems showing that we are able to cover several
kinds of communication paradigm.
SmartKom
SMARTKOM is a mixed-initiative dialogue system that
provides full symmetric multimodality by combining
speech, gesture, and facial expressions for both user in-
put and system output (Wahlster, 2003). It provides
an anthropomorphic and affective user interface through
its personification of an embodied conversational agent,
called Smartakus. The interaction metaphor is based
on the so-called situated, delegation-oriented dialogue
paradigm: the user delegates a task to a virtual commu-
nication assistant which is visualized as a life-like char-
acter. The interface agent recognizes the user?s inten-
tions and goals, asks the user for feedback if necessary,
accesses the various services on behalf of the user, and
presents the results in an adequate manner. Non-verbal
reactions of the users are extracted from their facial ex-
pression or the prosodic features and affect subsequent
system presentations.
As it is depicted in Figure 1, SMARTKOM realizes a
flexible and adaptive shell for multimodal dialogues and
addresses three different application scenarios:
Figure 1: SMARTKOM?s dialogue backbone and applica-
tion scenarios
SMARTKOM PUBLIC realizes an advanced multimodal
information and communication kiosk for, e. g., shop-
ping malls. The user can get information about movies,
reserve seats in a theater, and communicate using
telephone, fax, or electronic mail. Before the sys-
tem grants access to personal data, e. g., an address
book, the user has to authenticate himself using either
hand contour recognition, signature or voice verification.
SMARTKOM HOME serves as a multimodal infotainment
companion for the home theater. A portable web-pad acts
as an advanced remote control where the user gets pro-
gramming information from an electronic program guide
service and easily controls consumer electronics devices
like a TV set or a VCR. Similar to the kiosk application,
the user may also use communication services at home.
SMARTKOM MOBILE realizes a mobile travel compan-
ion for navigation and location-based services. It uses a
PDA as a front end which can be added to a car navigation
system. This system offers services like integrated trip
planning and incremental route guidance. In the mobile
scenario speech input can be combined with pen-based
pointing.
All functionalities, modality combinations and techni-
cal realizations including a wide variety of hardware op-
tions for the periphery are addressed by the same core di-
alogue system with common shared knowledge sources.
The processing relies on a knowledge based, configurable
approach: we provide general solutions based on declar-
ative knowledge sources in favour for special solutions
and/or shortcuts or application specific procedural pro-
cessing steps within the dialogue core of the system.
The interaction processing is based on M3L (Multimodal
Markup Language), a complete XML language designed
in the context of SMARTKOM that covers all data in-
terfaces within the complex multimodal dialogue sys-
tem (Gurevych et al, 2003a). The technical realization
is based on the MULTIPLATFORM testbed (Herzog et
al., 2004), an integration platform that provides a dis-
tributed component architecture. MULTIPLATFORM is
implemented on the basis of the scalable and efficient
publish/subscribe approach that decouples data produc-
ers and data consumers. Software modules communicate
via so-called data pools that correspond to named mes-
sage queues. Every data pool can be linked to an individ-
ual data type specification in order to define admissible
message contents.
MIAMM
The main objective of the MIAMM project is to de-
velop new concepts and techniques in the field of multi-
modal interaction to allow fast and natural access to large
multimedia databases (Reithinger et al, 2003b). This
implies both the integration of available technologies in
the domain of speech interaction (Natural Language Un-
derstanding ? SPIN ? see section 3.1) and interaction
management (Action Planner ? AP ? see section 3.3)
and the design of novel technology for haptic designation
and manipulation coupled with an adequate visualization.
The envisioned end-user device is a hand-held PDA that
provides an interface to a music database. The device in-
cludes three force-feedback buttons on the left side and
one wheel on the upper right side (see figure 2). The
buttons allow navigation through the visualized data, and
performing of various actions on the presented objects
(e. g., select, play).
The MIAMM architecture follows the ?standard? ar-
chitecture of interactive systems, with the consecutive
steps mode analysis, mode coordination, interaction man-
agement, presentation planning, and mode design. To
cope with artefacts arising from processing time require-
ments and coordination of different processes, this archi-
tecture was modified, so that only events that are rele-
vant to other modules are sent, whereas the others remain
internal. Thus, haptic interaction is decoupled from the
more time-consuming speech processes, and only sends
feedback when it is needed for the resolution of under-
specified structures or when the interaction involves ex-
ternal actions, e. g., playing a selected track. The sys-
tem consists of two modules for natural language input
processing, namely recognition and interpretation. On
the output side an MP3 player is used to play the songs
and the pre-recorded speech prompts to provide acoustic
feedback. The visual-haptic-tactile module is responsi-
ble for the selection of the visualization, and for the as-
signment of haptic features to the force-feedback buttons.
The visualization module renders the graphic output and
interprets the force imposed by the user to the haptic but-
tons. The dialogue manager consists of two main blocks,
Figure 2: The force-feedback device developed in the MI-
AMM project. The display shows a view of a database
using a timeline.
the multimodal FUSION (see section 3.2) which is re-
sponsible for the resolution of multimodal references us-
ing the contextual information hold in the dialogue his-
tory, and the AP, that interprets the user intention and trig-
gers a suitable system response. The AP is connected via
a domain model to the multimedia database. The domain
model uses an inference engine that facilitates access to
the database.
The integration environment is based on the Simple
Object Access Protocol (SOAP) (see www.w3.org/
TR/SOAP). The communication between the modules
is based on the multimodal interface language (MMIL).
This specification accounts for the incremental integra-
tion of multimodal data to achieve a full understanding
of the multimodal acts within the system. It is flexible
enough to handle the various types of information pro-
cessed and generated by the different modules.
COMIC
COMIC is an European IST 5th framework project fo-
cusing on new methods of work and e-commerce (den
Os and Boves, 2003). Goal of this project is to develop
a user centric, multimodal interface for a bathroom de-
sign tool which was developed by the COMIC partner Vi-
Soft (see www.visoft.de). The implementation work
is accompanied by research in the cognitive aspects of
human-human and human-computer interaction.
Figure 3 shows a user interacting with the initial pro-
totype of the system. The system enables the users to
enter by speech and pen the blueprint of their bathroom
including handwriting and drawing dimensions of walls,
windows, and doors respectively. In a second step the
user can browse and choose decoration and sanitary ware
for the bathroom. Finally, the underlying application al-
lows real-time, three-dimensional exploring of the mod-
eled bathroom. System output includes the application
itself and a realistically animated, speaking head.
Figure 3: Interaction with the COMIC system.
The architecture of the COMIC system again resem-
bles the architecture of our core dialogue backbone.
However, only SPIN, FUSION and Generation1 are used
for this project all other modules are provided by other
partners. COMIC is also based on MULTIPLATFORM as
the integration middleware, allowing a reuse of the mod-
ule wrappers and engines. The representation of infor-
mation is similar to that of SMARTKOM although the ac-
tual ontology differs in significant parts (e. g., no upper
model). Hence the integration of SPIN and Generation
was limited to the revision and adaption of the language
and ontology dependent knowledge sources. FUSION,
however, needed a deeper adaption as outlined in section
3.2.
Yet another (kind of) system
For the system NaRATo we have used parts of our tool-
box ? language understanding, discourse modeling , ac-
tion planning, and generation ? for a dialogue system in-
terfacing the ARIS tool-set, a business process manage-
ment system (see www.ids-scheer.com). The sys-
tem uses typed input and output to provide access to a
given process model stored in a database.
3 A Number of Modules
Our toolbox deploys a number of modules which are con-
nected in a (nowadays) standard fashion (see figure 4).
The input channels are fused by the modality fusion. This
module is also responsible for resolving not just deictic
expressions using gesture and speech but also referen-
tial expressions involving the dialogue context. The dis-
course module is the central repository for modality de-
pendent and modality independent information. Here, the
1Generation in COMIC is actually only realization as the
Fission module takes care of content selection and (most of)
sentence planning.
user contribution is interpreted in context which involves
resolving, e. g., a wide range of elliptical contributions.
The action planner is the actual engine: using a regres-
sion planning approach the next system action is planned
possibly preceeded by access of some external device. Fi-
nally, the presentation manager renders the system action.
Here, the availability of different output modalities and
the situation are influencing the realization of the action.
Our architecture differs from that of (Blaylock et al,
2003) in that the responsibility of the next system ac-
tion is in our case purely decided by the action planner;
the approach has some similarities with the one taken in
(Larsson, 2002) in that most communicative actions rep-
resent request-response interactions along goals (akin to
QUDs), and there is a notion of information state, which
is however kept separated between the discourse mod-
eler (for information specific to dialogue content, roughly
equivalent to the SHARED information in IBiS) and the
action planner (for other information, such as the agenda
of the dialogue engine).
3.1 Natural Language Understanding
The task of the natural language understanding module is
to transform the output of the speech recognizer into a list
of possible user intentions which are already represented
in the system-wide high-level ontology (see section 4).
For this task a new template-based semantic parsing ap-
proach called SPIN (Engel, 2002) was developed at DFKI
and is used in all aforementioned projects.
As typical for a semantic parser, the approach does not
need a syntactic analysis, but the high level output struc-
ture is built up directly from word level. This is feasi-
ble since the input consists of spoken utterances intended
to interact with a computer system and therefore, they
are usually syntactically less complicated and limited in
length. Furthermore, the lack of a syntactical analysis in-
creases the robustness against speech recognition errors
(speaker independent recognizers still have a word error
rate of 10%-30%) and syntactically incorrect input by the
user.
SPIN differs from other existing semantic parsing ap-
proaches by providing a more powerful rule language
and a powerful built-in ontology formalism. The main
motivation for the powerful rule language is to simplify
the creation and maintenance of rules. As the amount
of required rules is quite large (e.g., in the SmartKom
project 435 templates are used), easy creation and main-
tenance of the rules is one of the most important issues for
parsers in dialogue systems. Additionally, high-level out-
put structures have to be generated and these output struc-
tures may be structurally quite different from the implied
structure of the input utterance. A powerful rule language
simplifies this task significantly.
Several off-line optimizations still provide fast pro-
cessing despite the increased rule power. The most im-
portant off-line optimization is the computation of a fixed
rule application order with the objective to avoid wasting
time by the generation of sub-optimal results.
The powerful built-in ontology formalism helps to in-
tegrate the module in dialogue systems by only creating
the knowledge bases and an interface layer but without
any changes in the code base. Due to the lack of a stan-
dard ontology formalism for dialogue systems, each di-
alogue system uses a slightly different formalism. The
powerful internal ontology formalism simplifies the task
of mapping the system-wide ontology formalism to the
internal one.
Current research will improve the approach in two ar-
eas. First, the time-consuming creation of the knowl-
edge bases which has to be done completely manually
up to now will be supported by machine learning tech-
niques. Second, the external linguistic preprocessing of
the speech recognizer output, like a syntactic analysis,
will be possible without incorporating linguistic informa-
tion into the knowledge bases. This would allow to pro-
cess syntactically more complicated user utterances and
still provides easy creation of the knowledge bases.
3.2 Modality Fusion
Multimodal dialogue systems like SmartKom or Comic
give users the opportunity to express their needs not only
by speech but also by different modalities, e. g., by ges-
turing or by using a pen. Furthermore, users can also
combine several modalities to express one multimodal ut-
terance (e. g., ?I want to start here? accompanied by a
pointing gesture towards a location on a map). As the rec-
ognizers and analyzers of the different modalities gener-
ate modality specific hypotheses, a component is needed
to synchronize and integrate those monomodal hypothe-
ses into multimodal ones. This module is called FUSION.
Based on human-human communication research,
e. g., (Oviatt, 1999), we can identify four basic interac-
tion patterns of how to use different modalities within a
single multimodal utterance:
redundant the information provided by two modalities
is basically the same,
concurrent two modalities are used one after another to
provide information,
complementary the information provided by two
modalities can be intertwined,
contradicting the information provided by one modality
is contradictory to the information provided by the other
modality.
All these interaction patterns can be resolved by ob-
taining access to information about the internal structure
of objects. Especially when having to integrate informa-
tion from one source into another, we need to know what
specific objects look like, e. g., which sub-objects they
Discourse
Modelling
Management
Application
Fusion
Modality Action
Planning
Presentation
Manager
Analyzer Generator
Context Information
Main Data Flow
Figure 4: The architecture of the full blown version of our dialogue toolbox. Modality Fusion combines the different
results from the analyzers; Discourse Modeling interprets in context; Action Planning determines the next system
action; Presentation Management splits and coordinates the output on the different output modalities.
comprise. This information is typically provided by an
ontology, e. g., via the type hierarchy and the slot defini-
tions of each object. So, what FUSION must accomplish
is to utilize processing strategies based on a type hierar-
chy and a given set of object definitions.
In SmartKom we applied a so called slot-filling ap-
proach for the integration of the two modalities speech
and gesture. Multimodal hypotheses are compiled by in-
serting the hypotheses of the gestural modality into the
hypotheses of the speech modality. The advantage of this
approach is that apart from an ontology no further knowl-
edge sources are required. This approach proved to be
very fast and robust. However, the drawback is that an
adaption to a different dialogue system or to new modal-
ities is quite expensive.
With respect to our overall goal of building a scal-
able and reusable core dialogue system, we uncoupled
the core FUSION system from the needs of the dialogue
system, the available modalities, and processing strate-
gies. Thus, we implemented a special purpose production
rule system. Key to this approach is that all processing
strategies are defined by production rules which can be
easily created and adapted to the new surroundings and
there are two powerful operations for accumulating in-
formation ? unification and overlay (Alexandersson and
Becker, 2003).
3.3 Action Planning
Task oriented cooperative dialogues, where participants
collaborate to achieve a common goal, can be viewed
as coherent sequences of utterances asking for actions to
be performed or introducing new information to the di-
alogue context. The task of the action planner is to rec-
ognize the user?s goal, to trigger required actions for its
achievement, and to devise appropriate sub-dialogues and
feedback. The actions can be internal, such as updating
the internal state of the system, or external, like database
queries, device operation or communication with the user.
Thus, the action planner controls both the task and the
interaction structure. Task and dialogue interactions are
viewed as joint communicative games played with dif-
ferent agents, including the user and all modules that di-
rectly communicate with the action planner.2 Participants
are abstractly represented by communication channels
transforming between the uniform internal representation
of communicative moves to the data structures used by
external participants. Each game is composed of a num-
ber of moves, defined by dialogue knowledge sources.
The game definitions are similar to STRIPS plan opera-
tors. They specify sets of preconditions and effects, and
additionally, for each move the channel through which
the data flows, and data structures containing the seman-
tic content of the move intention. The adoption of a dia-
logue goal triggers a planning process (non-linear regres-
sion planning, with hierarchical decomposition of sub-
goals) resulting in a series of communicative games to
be played to achieve the goal. Move execution is then
interleaved with checking their outcome, and possibly re-
planning if preconditions are violated. This strategy al-
lows the system to deal with unexpected user inputs like
misunderstandings or changing of goals.
The approach of planning with communicative games
has two benefits with respect to the scalability of the sys-
tem, one regarding communication channels, the other
stemming from the use of small dialogue game units.
It is possible to integrate support for any number of ad-
ditional devices to an already existing system by adding
new communication channels (one Java class each); di-
alogue moves that do not use these channels will not be
affected. Still, dialogue specifications for newly added
devices can make use of the already defined ones.
As described above, the dialogue behavior is coded
in terms of communicative games consisting of dialogue
moves. For predetermined sequences of moves (e. g., a
2We use the term ?communicative games? in addition to ?di-
alogue games,? since our dialogue model also includes com-
munication interaction with applications and devices, such as
database requests and answers, in terms of game moves.
fixed protocol for sending fax messages: (1) scan docu-
ment, (2) convert to fax format, (3) send it via fax ap-
plication), the dialogue game can resemble a fixed script,
like the pre-made plans used, e. g., by (Larsson, 2002)),
but in general, games specify atomic steps like single
request-response subdialogues. To devise the course of
action, a plan is then constructed dynamically as a game
sequence. This has the advantage that (1) the plan can be
flexibly adapted to changed circumstances, e. g., if a step
becomes obsolete or is addressed early, and (2) games
can be shared and reused as building blocks for other
applications. So, when new functionality is integrated,
the plan knowledge source will stay reasonably small?
growing linearly in the number of games, not exponen-
tially with the possible recipes.3
3.4 Discourse Modeling
The main objective of the discourse modeler (henceforth
DIM) is to incorporate information stemming from the
previous discourse context into the current intention hy-
potheses produced by the analysis modules. This objec-
tive decomposes into two main tasks which are on the
one hand enhancing a hypothesis with compatible back-
ground information and estimating how well it fits the
previous discourse context ? what we call enrichment and
validation ? and on the other hand the resolution of refer-
ring expressions.
Discourse processing in the framework of a multi-
modal dialogue system has to deal with an extended set
of input and output devices. Gestures, for example, ac-
companying speech not only support the resolution of re-
ferring expressions, in addition they change the discourse
context. In general, the resolution of referring expres-
sions within a multimodal approach requires access to a
visual context representation. One key aspect of DIM is
a unified context representation taking both the discourse
and the visual context into account.
Our approach consists of a three-tiered discourse rep-
resentation combined with a two layered focus handling,
see (Pfleger et al, 2003). The actual processing is
done by utilizing two operations: unification and over-
lay (Alexandersson and Becker, 2003). In combination
with a scoring function (Pfleger et al, 2002), the latter
is our main tool for enrichment and validation. Key to
this approach is that DIM can be easily adapted to other
dialogue systems with different tasks and demands. In
that sense, the actual context representation is indepen-
dent from the type of objects to be stored. Additionally,
DIM can be used not only within a multimodal dialogue
system but also within monomodal ones, as we showed
in the NaRATo project.
3The usual downside is, the planning space is of course ex-
ponential. But as we use goal-directed search, only a small frac-
tion of the possible plans is ever examined in practice.
3.5 Modality Fission
The modalities used in the SmartKom system are ges-
ture, mimics, speech and also graphical presentations on
devices of different sizes. The main task of multimodal
fission is partitioning, i. e., dividing the presentation tasks
into subtasks and generating an execution plan. A follow-
up task is then the coordination and synchronization of re-
lated tasks, e. g., presentation of a graphical element with
a pointing gesture and synchronization with speech.
The fission module is embedded in a presentation plan-
ner that also subsumes the graphical realization task. The
module generates a full plan for graphics, gesture and
mimics while the plan for speech is generated only on an
abstract subtask level that is handed as input to the Text
Generator (see next section).
The planning of a multimodal presentation consists of
two parts: static gesture-sensitive graphical elements and
a corresponding multimodal animation of the agent in-
cluding gestures referring to objects with aligned audio-
visual speech output. The first step performed on the in-
put is a transformation into the internal input format of
the core planning component PrePlan by applying an ap-
propriate XSLT-stylesheet.
Then, the presentation planner starts the planning pro-
cess by applying a set of presentation strategies which
define how the facts are presented in the given scenario.
Based on constraints, the strategies decompose the com-
plex presentation goal into primitive tasks and at the same
time they execute the media fission step depending on
available modalities, which means they decide which part
of the presentation should be instantiated as spoken out-
put, graphics, or gestures of our presentation agent.
After planning the graphical presentation, appropriate
speech and gesture presentations are generated. The ges-
ture and speech form is chosen depending on the graph-
ically shown information. I.e., if the graphically pre-
sented information is in the focus of a presentation, only
a comment is generated in speech output. The goal of the
gesture presentation is then to focus on the appropriate
graphical element. If there is no graphically presentable
information or it is insufficient, more speech is generated.
3.6 Natural Language Generator
The design of the Natural Language Generation (NLG)
module is guided by the need to (i) adapt only knowledge
sources when adding a new application and (ii) general-
izing the knowledge sources from the applications.
Thus the NLG module is divided into an engine and
declarative knowledge sources which are designed with
the goal of capturing generalizations. The input to the
NLG module are abstract presentation goals that are
based on the ends-based presentation; the output is (an-
notated) text that typically is sent to a speech synthesizer.
E.g., the NLG module in SmartKom uses syntactic struc-
ture and discourse information to supply richly annotated
text for the Concept-To-Speech (CTS) approach.
On the one hand, the NLG module is templated?based
(see also SPIN), skipping multiple layers of represen-
tation when mapping from the presentation goals. On
the other hand, the templates are ?fully specified? in
the sense that they include intermediate layers of rep-
resentation where possible to permit a later separation
of rules into a multi-stage generation module. E.g., in-
cluding syntax was also necessary for CTS, including se-
mantics allows for the extraction of a realization module
for COMIC. The template rules are based on the same
PrePlan planning component used in fission. At least
since (Reiter, 1995) the use of templates and ?deep rep-
resentations? is not seen as a contradiction. Picking up
on this idea, the generation component in SmartKom is
based on fully lexicalized generation (Becker, 1998), us-
ing partial derivation trees of a Tree-Adjoining Grammar
(TAG). Right from the beginning of development, deriva-
tion trees which are seen as reflecting syntactic depen-
dencies have been an explicitly represented layer in the
template rules. Thus the higher level planning rules de-
cide content selection, sentence plans and lexicalization,
leaving syntactic realization to a TAG-based second step.
During development, we have enriched the syntactic
trees with nested feature structures and have just finished
a transformation of the phrasal templates to a fully lexi-
calized TAG, where every lexical item has its unique tree.
4 Ends-Based Processing
One of the most important constraints when building a
functioning system has been the domain of the appli-
cation. Based on the domain we developed ends-based
representations which have so far mostly been ontolo-
gies or ontology-like structures, e. g., (Gurevych et al,
2003b) but which in fact could be event-based represen-
tations as well. How interpretation and presentation are
connected to the abstract representation is of secondary
interest; Our backbone uses this task-oriented represen-
tation for communication and processing and the way
there and back may exclude, for instance, traditional se-
mantics. We make two important observations: on the
one hand, that the complete backbone should use a sin-
gle representation, so that translations between different
representations are avoided. Important here is that each
module (ideally) separates its engine from its knowledge
base. On the other hand, the common representation has
to be ends-based and fulfil the needs of the application.
The latter point leads us to another lesson learned: The
application has to be examined and its needs have to be
mirrored in the representation. We also have to deter-
mine what interactions we are aiming for. Since, e. g., in
SmartKom, we pursue a situated delegation-oriented dia-
log paradigm ? meaning that the system is in itself not a
dialogue partner as in (Blaylock et al, 2003) but instead
the dialogue is mediated by an animated agent ? we en-
capsulate the details of the application APIs in an applica-
tion manager and hence provide a user-oriented view of
the application(s). Additionally, the dialogue plans are
represented separately from the ends-based representa-
tion in a different knowledge base, i. e., the plan speci-
fications for the action planner. However, the plans refer
to the application using the ends-based representation.
We have acquired our knowledge, e. g., ends-based
represenations or interpretation rules completely by hand.
While we avoid the potentially costly resources for the
collection and annotation of corpora for automated lear-
ning4, the question remains whether expanding knowl-
edge sources by hand is feasible. Our approach has in-
deed allowed for scaling up ? in SmartKom we have ex-
tended the system to more than 50 functionalities overall
(Reithinger et al, 2003a).
In the following, we list the most important lessons we
learned, which is by no means exhaustive:
Encapsulation Encapsulate the backbone from the ap-
plication(s). This was one of the main lessons from the
NaRATo and the SmartKom projects. We did not do it in
the NaRATo project and spent lots of time interfacing the
database. In SmartKom, such a module exists, and the
backbone developers could concentrate on more relevant
tasks in dialogue processing proper.
Representation Use one representation throughout the
backbone. It is a secondary question how exactly it is
done, but it is essential that you get there and avoid spend-
ing time on converting between different formalisms.
Representation (revisited) There is to be no presentation
(system output) without representation on the ends-based
representation level. This representation is part of the
global dialogue history residing in the discourse module
and can be accessed by any module, e. g., for reference
resolution at any time during the course of the dialogue.
Interface In the case of a multi-module approach, use
one well-defined representation for module communica-
tion. In most cases we have used XML and XML Schema
which is convenient because a wide variety of infrastruc-
ture and tools is available. For instance, most XML pro-
cessing tools allow for syntactic validation. However,
XML is not mandatory. A final remark here: using XML
in combination with stylesheets, we can in fact ? contrary
to the advice in Representation (above) ? translate or con-
vert messages to some internal representation easily.
Interface (revisited) Interfaces should be clean and well-
defined. One reason for the success of the SmartKom
project was the requirement to define every interface for-
mally by XML Schema. These XML Schemata were kept
in a project-wide repository and changed at this one place
4Supervised as well as unsupervised
after mutual agreement only. Due to the multi-blackboard
approach, there are not point-to-point connections, but   -
to-  connections, and an interface definition comprises
of a precise description of what is supposed to be an al-
lowed message for a specific blackboard.
Integration Our large projects have profited enormously
of a dedicated integration group providing infrastructure,
integration cycles and ? for, e. g., the SmartKom and
COMIC systems ? a testbed (Herzog et al, 2004).
Multimodality More modalities allow for more natural
communication, which normally employs multiple chan-
nels of expression, suited to the content to be communi-
cated. For natural language processing per se this raises
new and interesting challenges, e. g., cross-modal refer-
ential expressions. It is also the case that more modal-
ities constrain interpretation and hence enhance robust-
ness. The ends-based representation allow for modality-
independent processing in the backbone.
Standards Standards ease scalability. For, e. g., ends-
based representations and tools, we have previously de-
veloped custom-built software providing short-lived so-
lutions. In other situations we have chosen standards and
standard tools. We claim that the latter is beneficial in at
least two ways: It opens up the door for scalability since
we can re-use our as well as other?s resources. Secondly
it is easier to maintain our solution over time and projects.
5 Conclusion
DFKI?s dialogue toolbox was used in a number of fully
functional, differently sized systems with a variety of
interaction paradigms. Vital to its success in terms of
reusability and scalability was the choice of a modular de-
sign and ends-based representations throughout the com-
plete backbone. Starting from basic functionalities, it is
possible to extend the system coverage while incorporat-
ing new features. Future work includes reusing (parts of)
the backbone in EU and nationally funded large projects
like AMI, TALK, Inscape, VirtualHuman and SmartWeb.
References
Jan Alexandersson and Tilman Becker. 2003. The Formal
Foundations Underlying Overlay. In Proceedings of the
Fifth International Workshop on Computational Semantics
(IWCS-5), Tilburg, The Netherlands, February.
Tilman Becker. 1998. Fully lexicalized head-driven syntac-
tic generation. In Proceedings of the Ninth International
Workshop on Natural Language Generation, Niagara-on-the-
Lake, Ontario, Canada, August.
Nate Blaylock, James Allen, and George Ferguson. 2003.
Managing communicative intentions with collaborative
problem solving. In Ronnie W. Smith and Jan van Kup-
pevelt, editors, Current and New Directions in Discourse and
Dialogue. Kluwer.
Els den Os and Lou Boves. 2003. Towards ambient intelli-
gence: Multimodal computers that understand our intentions.
In eChallenges e-2003, pages 22?24.
Ralf Engel. 2002. SPIN: Language understanding for spoken
dialogue systems using a production system approach. In
Proceedings of 7th International Conference on Spoken Lan-
guage Processing (ICSLP-2002), pages 2717?2720, Denver,
Colorado, USA.
Iryna Gurevych, Robert Porzel, Elena Slinko, Norbert Pfleger,
Jan Alexandersson, and Stefan Merten. 2003a. Less is more:
Using a single knowledge representation in dialogue sys-
tems. In Proceedings of the HLT-NAACL Workshop on Text
Meaning, pages 14?21, Edmonton, Canada, May.
Iryna Gurevych, Robert Porzel, Hans-Peter Zorn, and Rainer
Malaka. 2003b. Semantic coherence scoring using an on-
tology. In Proceedings of the Human Language Technology
Conference - HLT-NAACL 2003, Edmonton, CA, May, 27?
June, 1.
Gerd Herzog, Heinz Kirchmann, Stefan Merten, Alassane Ndi-
aye, Peter Poller, and Tilman Becker. 2004. Large-scale
software integration for spoken language and multimodal di-
alog systems. Journal of Natural Language Engineering.
To appear in the special issue on ?Software Architecture for
Language Engineering?.
Staffan Larsson. 2002. Issue-based Dialogue Management.
Ph.D. thesis, Go?teborg University.
Sharon Oviatt. 1999. Ten myths of multimodal interaction.
Communications of the ACM, 42(11):74?81.
Norbert Pfleger, Jan Alexandersson, and Tilman Becker. 2002.
Scoring functions for overlay and their application in dis-
course processing. In KONVENS-02, Saarbru?cken, Septem-
ber ? October.
Norbert Pfleger, Ralf Engel, and Jan Alexandersson. 2003. Ro-
bust multimodal discourse processing. In Kruijff-Korbayova
and Kosny, editors, Proceedings of Diabruck: 7th Workshop
on the Semantics and Pragmatics of Dialogue, Wallerfangen,
Germany, September.
Ehud Reiter. 1995. NLG vs. templates. In 5th European Work-
shop in Natural Language Generation, pages 95?105, Lei-
den, May.
Norbert Reithinger, Jan Alexandersson, Tilman Becker, Anselm
Blocher, Ralf Engel, Markus Lo?eckelt, Jochen Mu?eller, Nor-
bert Pfleger, Peter Poller, Michael Streit, and Valentin Tsch-
ernomas. 2003a. Smartkom - adaptive and flexible mul-
timodal access to multiple applications. In Proceedings of
ICMI 2003, Vancouver, B.C.
Norbert Reithinger, Dirk Fedeler, Ashwani Kumar, Christoph
Lauer, Elsa Pecourt, and Laurent Romary. 2003b. MIAMM
- A Multimodal Dialogue System Using Haptics. In Jan van
Kuppevelt, Laila Dybkjaer, and Niels Ole Bersen, editors,
Natural, Intelligent and Effective Interaction in Multimodal
Dialogue Systems. Kluwer Academic Publishers.
Wolfgang Wahlster. 2003. Smartkom: Symmetric multimodal-
ity in an adaptive and reusable dialogue shell. In R. Krahl
and D. Gnther, editors, Proceedings of the Human Computer
Interaction Status Conference 2003, pages 47?62, Berlin:
DLR, June.
An Experiment Setup for Collecting Data for Adaptive Output Planning
in a Multimodal Dialogue System
Ivana Kruijff-Korbayova?, Nate Blaylock,
Ciprian Gerstenberger, Verena Rieser
Saarland University, Saarbru?cken, Germany
korbay@coli.uni-sb.de
Tilman Becker, Michael Kai?er,
Peter Poller, Jan Schehl
DFKI, Saarbru?cken, Germany
tilman.becker@dfki.de
Abstract
We describe a Wizard-of-Oz experiment setup for
the collection of multimodal interaction data for a
Music Player application. This setup was devel-
oped and used to collect experimental data as part
of a project aimed at building a flexible multimodal
dialogue system which provides an interface to an
MP3 player, combining speech and screen input
and output. Besides the usual goal of WOZ data
collection to get realistic examples of the behav-
ior and expectations of the users, an equally im-
portant goal for us was to observe natural behavior
of multiple wizards in order to guide our system
development. The wizards? responses were there-
fore not constrained by a script. One of the chal-
lenges we had to address was to allow the wizards
to produce varied screen output a in real time. Our
setup includes a preliminary screen output planning
module, which prepares several versions of possi-
ble screen output. The wizards were free to speak,
and/or to select a screen output.
1 Introduction
In the larger context of the TALK project1 we are develop-
ing a multimodal dialogue system for a Music Player appli-
cation for in-car and in-home use, which should support nat-
ural, flexible interaction and collaborative behavior. The sys-
tem functionalities include playback control, manipulation of
playlists, and searching a large MP3 database. We believe
that in order to achieve this goal, the system needs to provide
advanced adaptive multimodal output.
We are conducting Wizard-of-Oz experiments
[Bernsen et al, 1998] in order to guide the development
of our system. On the one hand, the experiments should
give us data on how the potential users interact with such
an application. But we also need data on the multimodal
interaction strategies that the system should employ to
achieve the desired naturalness, flexibility and collaboration.
We therefore need a setup where the wizard has freedom of
1TALK (Talk and Look: Tools for Ambient Linguistic Knowl-
edge; www.talk-project.org) is funded by the EU as project
No. IST-507802 within the 6th Framework program.
choice w.r.t. their response and its realization through single
or multiple modalities. This makes it different from previous
multimodal experiments, e.g., in the SmartKom project
[Tu?rk, 2001], where the wizard(s) followed a strict script.
But what we need is also different in several aspects from
taking recordings of straight human-human interactions: the
wizard does not hear the user?s input directly, but only gets a
transcription, parts of which are sometimes randomly deleted
(in order to approximate imperfect speech recognition);
the user does not hear the wizard?s spoken output directly
either, as the latter is transcribed and re-synthesized (to
produce system-like sounding output). The interactions
should thus more realistically approximate an interaction
with a system, and thereby contain similar phenomena (cf.
[Duran et al, 2001]).
The wizard should be able to present different screen out-
puts in different context, depending on the search results and
other aspects. However, the wizard cannot design screens on
the fly, because that would take too long. Therefore, we de-
veloped a setup which includes modules that support the wiz-
ard by providing automatically calculated screen output op-
tions the wizard can select from if s/he want to present some
screen output.
Outline In this paper we describe our experiment setup and
the first experiences with it. In Section 2 we overview the
research goals that our setup was designed to address. The
actual setup is presented in detail in Section 3. In Section 4
we describe the collected data, and we summarize the lessons
we learnt on the basis of interviewing the experiment partici-
pants. We briefly discuss possible improvements of the setup
and our future plans with the data in Section 5.
2 Goals of the Multimodal Experiment
Our aim was to gather interactions where the wizard can com-
bine spoken and visual feedback, namely, displaying (com-
plete or partial) results of a database search, and the user can
speak or select on the screen.
Multimodal Presentation Strategies The main aim was to
identify strategies for the screen output, and for the multi-
modal output presentation. In particular, we want to learn
Figure 1: Multimodal Wizard-of-Oz data collection setup for
an in-car music player application, using the Lane Change
driving simulator. Top right: User, Top left: Wizard, Bottom:
transcribers.
when and what content is presented (i) verbally, (ii) graphi-
cally or (iii) by some combination of both modes. We expect
that when both modalities are used, they do not convey the
same content or use the same level of granularity. These are
important questions for multimodal fission and for turn plan-
ning in each modality.
We also plan to investigate how the presentation strategies
influence the responses of the user, in particular w.r.t. what
further criteria the user specifies, and how she conveys them.
Multimodal Clarification Strategies The experiments
should also serve to identify potential strategies for multi-
modal clarification behavior and investigate individual strat-
egy performance. The wizards? behavior will give us an ini-
tial model how to react when faced with several sources of
interpretation uncertainty. In particular we are interested in
what medium the wizard chooses for the clarification request,
what kind of grounding level he addresses, and what ?sever-
ity? he indicates. 2 In order to invoke clarification behavior
we introduced uncertainties on several levels, for example,
multiple matches in the database, lexical ambiguities (e.g., ti-
tles that can be interpreted denoting a song or an album), and
errors on the acoustic level. To simulate non-understanding
on the acoustic level we corrupted some of the user utterances
by randomly deleting parts of them.
3 Experiment Setup
We describe here some of the details of the experiment. The
experimental setup is shown schematically in Figure 1. There
are five people involved in each session of the experiment: an
experiment leader, two transcribers, a user and a wizard.
The wizards play the role of an MP3 player application
and are given access to a database of information (but not
actual music) of more than 150,000 music albums (almost 1
2Severity describes the number of hypotheses indicated by the
wizard: having no interpretation, an uncertain interpretation, or sev-
eral ambiguous interpretations.
Figure 2: Screenshot from the FreeDB-based database appli-
cation, as seen by the wizard. First-level of choice what to
display.
million songs), extracted from the FreeDB database.3 Fig-
ure 2 shows an example screen shot of the music database
as it is presented to the wizard. Subjects are given a set of
predefined tasks and are told to accomplish them by using
an MP3 player with a multimodal interface. Tasks include
playing songs/albums and building playlists, where the sub-
ject is given varying amounts of information to help them
find/decide on which song to play or add to the playlist. In
a part of the session the users also get a primary driving task,
using a Lane Change driving simulator [Mattes, 2003]. This
enabled us to test the viability of combining primary and sec-
ondary task in our experiment setup. We also aimed to gain
initial insight regarding the difference in interaction flow un-
der such conditions, particularly with regard to multimodal-
ity.
The wizards can speak freely and display the search result
or the playlist on the screen. The users can also speak as well
as make selections on the screen.
The user?s utterances are immediately transcribed by a typ-
ist and also recorded. The transcription is then presented to
the wizard.4 We did this for two reasons: (1) To deprive
the wizards of information encoded in the intonation of utter-
ances, because our system will not have access to it either. (2)
To be able to corrupt the user input in a controlled way, sim-
ulating understanding problems at the acoustic level. Unlike
[Stuttle et al, 2004], who simulate automatic speech recogni-
tion errors using phone-confusion models, we used a tool that
?deletes? parts of the transcribed utterances, replacing them
by three dots. Word deletion was triggered by the experiment
leader. The word deletion rate varied: 20% of the utterances
got weakly and 20% strongly corrupted. In 60% of the cases
the wizard saw the transcribed speech uncorrupted.
The wizard?s utterances are also transcribed (and recorded)
3Freely available at http://www.freedb.org
4We were not able to use a real speech recognition system, be-
cause we do not have one trained for this domain. This is one of the
purposes the collected data will be used for.
Figure 3: Screenshot from the display presentation tool offer-
ing options for screen output to the wizard for second-level
of choice what to display an how.
and presented to the user via a speech synthesizer. There are
two reasons for doing this: One is to maintain the illusion for
the subjects that they are actually interacting with a system,
since it is known that there are differences between human-
human and human-computer dialogue [Duran et al, 2001],
and we want to elicit behavior in the latter condition; the
other has to do with the fact that synthesized speech is imper-
fect and sometimes difficult to understand, and we wanted to
reproduce this condition.
The transcription is also supported by a typing and spelling
correction module to minimize speech synthesis errors and
thus help maintain the illusion of a working system.
Since it would be impossible for the wizard to construct
layouts for screen output on the fly, he gets support for his
task from the WOZ system: When the wizard performs a
database query, a graphical interface presents him a first level
of output alternatives, as shown in Figure 2. The choices are
found (i) albums, (ii) songs, or (iii) artists. For a second level
of choice, the system automatically computes four possible
screens, as shown in Figure 3. The wizard can chose one of
the offered options to display to the user, or decide to clear
the user?s screen. Otherwise, the user?s screen remains un-
changed. It is therefore up to the wizard to decide whether
to use speech only, display only, or to combine speech and
display.
The types of screen output are (i) a simple text-message
conveying how many results were found, (ii) output of a list
of just the names (of albums, songs or artists) with the cor-
responding number of matches (for songs) or length (for al-
bums), (iii) a table of the complete search results, and (iv) a
table of the complete search results, but only displaying a sub-
set of columns. For each screen output type, the system uses
heuristics based on the search to decide, e.g., which columns
should be displayed. These four screens are presented to the
wizard in different quadrants on a monitor (cf. Figure 3),
allowing for selection with a simple mouse click. The heuris-
tics for the decision what to display implement preliminary
strategies we designed for our system. We are aware that due
to the use of these heuristics, the wizard?s output realization
may not be always ideal. We have collected feedback from
both the wizards and the users in order to evaluate whether
the output options were satisfactory (cf. Section 4 for more
details).
Technical Setup To keep our experimental system modu-
lar and flexible we implemented it on the basis of the Open
Agent Architecture (OAA) [Martin et al, 1999], which is a
framework for integrating a community of software agents in
a distributed environment. Each system module is encapsu-
lated by an OAA wrapper to form an OAA agent, which is
able to communicate with the OAA community. The exper-
imental system consists of 12 agents, all of them written in
Java. We made use of an OAA monitor agent which comes
with the current OAA distribution to trace all communication
events within the system for logging purposes.
The setup ran distributed over six PCs running different
versions of Windows and Linux.5
4 Collected Data and Experience
The SAMMIE-26 corpus collected in this experiment contains
data from 24 different subjects, who each participated in one
session with one of our six wizards. Each subject worked on
four tasks, first two without driving and then two with driving.
The duration was restricted to twice 15 minutes. Tasks were
of two types: searching for a title either in the database or in
an existing playlist, building a playlist satisfying a number of
constraints. Each of the two sets for each subject contained
one task of each type. The tasks again differed in how specific
information was provided. We aimed to keep the difficulty
level constant across users. The interactions were carried out
in German.7
The data for each session consists of a video and audio
recording and a logfile. Besides the transcriptions of the spo-
ken utterances, a number of other features have been anno-
tated automatically in the log files of the experiment, e.g.,
the wizard?s database query and the number of found results,
the type and form of the presentation screen chosen by the
wizard, etc. The gathered logging information for a single
experiment session consists of the communication events in
chronological order, each marked by a timestamp. Based on
this information, we can recapitulate the number of turns and
the specific times that were necessary to accomplish a user
task. We expect to use this data to analyze correlations be-
5We would like to thank our colleagues from CLT Sprachtech-
nologie http://www.clt-st.de/ for helping us to set up the
laboratory.
6SAMMIE stands for Saarbru?cken Multimodal MP3 Player In-
teraction Experiment. We have so far conducted two series of data-
collection experiments: SAMMIE-1 involved only spoken interaction
(cf. [Kruijff-Korbayova? et al, 2005] for more details), SAMMIE-2 is
the multimodal experiment described in this paper.
7However, most of the titles and artist names in the music
database are in English.
tween queries, numbers of results, and spoken and graphical
presentation strategies.
Whenever the wizard made a clarification request, the
experiment leader invoked a questionnaire window on the
screen, where the wizard had to classify his clarification re-
quest according to the primary source of the understanding
problem. At the end of each task, users were asked to what
extent they believed they accomplished their tasks and how
satisfied they were with the results. Similar to methods used
by [Skantze, 2003] and [Williams and Young, 2004], we plan
to include subjective measures of task completion and cor-
rectness of results in our evaluation matrix, as task descrip-
tions can be interpreted differently by different users.
Each subject was interviewed immediately after the ses-
sion. The wizards were interviewed once the whole experi-
ment was over. The interviews were carried out verbally, fol-
lowing a prepared list of questions. We present below some
of the points gathered through these interviews.
Wizard Interviews All 6 wizards rated the overall under-
standing as good, i.e., that communication completed suc-
cessfully. However, they reported difficulties due to delays in
utterance transmission in both directions, which caused un-
necessary repetitions due to unintended turn overlap.
There were differences in how different wizards rated and
used the different screen output options: The table containing
most of the information about the queried song(s) or album(s)
was rated best and shown most often by some wizards, while
others thought it contained too much information and would
not be clear at first glance for the users and hence they used
it less or never. The screen option containing the least infor-
mation in tabular form, namely only a list of songs/albums
with their length, received complementary judgments: some
of the wizards found it useless because it contained too little
information, and they thus did not use it, and others found it
very useful because it would not confuse the user by present-
ing too much information, and they thus used it frequently.
Finally, the screen containing a text message conveying only
the number of matches, if any, has been hardly used by the
wizards. The differences in the wizards? opinions about what
the users would find useful or not clearly indicate the need
for evaluation of the usefulness of the different screen output
options in particular contexts from the users? view point.
When showing screen output, the most common pattern
used by the wizards was to tell the user what was shown (e.g.,
I?ll show you the songs by Prince), and to display the screen.
Some wizards adapted to the user?s requests: if asked to show
something (e.g., Show me the songs by Prince), they would
show it without verbal comments; but if asked a question
(e.g., What songs by Prince are there? or What did you find?),
they would show the screen output and answer in speech.
Concerning the adaptation of multimodal presentation
strategies w.r.t. whether the user was driving or not, four
of the six wizards reported that they consciously used speech
instead of screen output if possible when the user was driving.
The remaining two wizards did not adapt their strategy.
On the whole, interviewing the wizards brought valuable
information on presentation strategies and the use of modal-
ities, but we expect to gain even more insight after the an-
notation and evaluation of the collected data. Besides ob-
servations about the interaction with the users, the wizards
also gave us various suggestions concerning the software used
in the experiment, e.g., the database interface (e.g., the pos-
sibility to decide between strict search and search for par-
tial matches, and fuzzy search looking for items with similar
spelling when no hits are found), the screen options presenter
(e.g., ordering of columns w.r.t. their order in the database in-
terface, the possibility to highlight some of the listed items),
and the speech synthesis system.
Subject Interviews In order to use the wizards? behavior as
a model for interaction design, we need to evaluate the wiz-
ards? strategies. We used user satisfaction, task experience,
and multi-modal feedback behavior as evaluation metrics.
The 24 experimental subjects were all native speakers of
German with good English skills. They were all students
(equally spread across subject areas), half of them male and
half female, and most of them were between 20 to 30 years
old.
In order to calculate user satisfaction, users were inter-
viewed to evaluate the system?s performance with a user sat-
isfaction survey. The survey probed different aspects of the
users? perception of their interaction with the system. We
asked the users to evaluate a set of five core metrics on a
5-point Likert scale. We followed [Walker et al, 2002] def-
inition of the overall user satisfaction as the sum of text-to-
speech synthesis performance, task ease, user expertise, over-
all difficulty and future use. The mean for user satisfaction
across all dialogues was 15.0 (with a standard derivation of
2.9). 8 A one-way ANOVA for user satisfaction between wiz-
ards (df=5, F=1.52 p=0.05) shows no significant difference
across wizards, meaning that the system performance was
judged to be about equally good for all wizards.
To measure task experience we elicited data on perceived
task success and satisfaction on a 5-point Likert scale after
each task was completed. For all the subjects the final per-
ceived task success was 4.4 and task satisfaction 3.9 across
the 4 tasks each subject had to complete. For task success
as well as for task satisfaction no significant variance across
wizards was detected.
Furthermore the subjects were asked about the employed
multi-modal presentation and clarification strategies.
The clarification strategies employed by the wizards
seemed to be successful: From the subjects? point of view,
mutual understanding was very good and the few misunder-
standings could be easily resolved. Nevertheless, in the case
of disambiguation requests and when grounding an utterance,
subjects ask for more display feedback. It is interesting to
note that subjects judged understanding difficulties on higher
levels of interpretation (especially reference resolution prob-
lems and problems with interpreting the intention) to be more
costly than problems on lower levels of understanding (like
the acoustic understanding). For the clarification strategy this
8[Walker et al, 2002] reported an average user satisfaction of
16.2 for 9 Communicator systems.
implies that the system should engage in clarification at the
lowest level a error was detected.9
Multi-modal presentation strategies were perceived to be
helpful in general, having a mean of 3.1 on a 5-point Lik-
ert scale. However, the subjects reported that too much in-
formation was being displayed especially for the tasks with
driving. 85.7% of the subjects reported that the screen out-
put was sometimes distracting them. 76.2% of the sub-
jects would prefer to more verbal feedback, especially while
driving. On a 3-point Likert scale subjects evaluated the
amount of the information presented verbally to be about
right (mean of 1.8), whereas they found the information pre-
sented on the screen to be too much (mean of 2.3). Stud-
ies by [Bernsen and Dybkjaer, 2001] on the appropriateness
of using verbal vs. graphical feedback for in-car dialogues
indicate that the need for text output is very limited. Some
subjects in that study, as well subjects in our study report that
they would prefer to not have to use the display at all while
driving. On the other hand subjects in our study perceived the
screen output to be very helpful in less stressful driving situa-
tions and when not driving (e.g. for memory assistance, clari-
fications etc.). Especially when they want to verify whether a
complex task was finally completed (e.g. building a playlist),
they ask for a displayed proof. For modality selection in in-
car dialogues the driver?s mental workload on primary and
secondary task has to be carefully evaluated with respect to a
situation model.
With respect to multi-modality subjects also asked for
more personalized data presentation. We therefore need to
develop intelligent ways to reduce the amount of data being
displayed. This could build on prior work on the generation
of ?tailored? responses in spoken dialogue according to a user
model [Moore et al, 2004].
The results for multi-modal feedback behavior showed no
significant variations across wizards except for the general
helpfulness of multi-modal strategies. An ANOVA Planned
Comparison of the wizard with the lowest mean against the
other wizards showed that his behavior was significantly
worse. It is interesting to note, that this wizard was using
the display less than the others. We might consider not to in-
clude the 4 sessions with this wizard in our output generation
model.
We also tried to analyze in more detail how the wizards?
presentation strategies influenced the results. The option
which was chosen most of the time was to present a table
with the search results (78.6%); to present a list was only cho-
sen in 17.5% of the cases and text only 0.04%. The wizards?
choices varied significantly only for presenting the table op-
tion. The wizard who was rated lowest for multimodality was
using the table option less, indicating that this option should
be used more often. This is also supported by the fact that the
show table option is the only presentation strategy which is
positively correlated to how the user evaluated multimodality
(Spearman?s r = 0.436*). We also could find a 2-tailed corre-
9Note that engaging at the lowest level just helps to save dialogue
?costs?. Other studies have shown that user satisfaction is higher
for strategies that would ?hide? the understanding error by asking
questions on higher levels [Skantze, 2003], [Raux et al, 2005]
lation between user satisfaction and multimodality judgment
(Spearman?s r = 0.658**). This indicates the importance of
good multimodal presentation strategies for user satisfaction.
Finally, the subjects were asked for own comments. They
liked to be able to provide vague information, e.g., ask for ?an
oldie?, and were expecting collaborative suggestions. They
also appreciated collaborative proposals based on inferences
made from previous conversations.
In sum, as the measures for user satisfaction, task experi-
ence, and multi-modal feedback strategies, the subjects? judg-
ments show a positive trend. The dialogue strategies em-
ployed by most of the wizards seem to be a good starting
point for building a baseline system. Furthermore, the results
indicate that intelligent multi-modal generation needs to be
adaptive to user and situation models.
5 Conclusions and Future Steps
We have presented an experiment setup that enables us to
gather multimodal interaction data aimed at studying not only
the behavior of the users of the simulated system, but also
that of the wizards. In order to simulate a dialogue system in-
teraction, the wizards were only shown transcriptions of the
user utterances, sometimes corrupted, to simulate automatic
speech recognition problems. The wizard?s utterances were
also transcribed and presented to the user through a speech
synthesizer. In order to make it possible for the wizards to
produce contextually varied screen output in real time, we
have included a screen output planning module which auto-
matically calculated several screen output versions every time
the wizard ran a database query. The wizards were free to
speak and/or display screen output. The users were free to
speak or select on the screen. In a part of each session, the
user was occupied by a primary driving task.
The main challenge for an experiment setup as described
here is the considerable delay between user input and wizard
response. This is due partly to the transcription and spelling
correction step and partly due to the time it takes the wizard to
decide on and enter a query to the database, then select a pre-
sentation and in parallel speak to the user. We have yet to ana-
lyze the exact distribution of time needed for these tasks. Sev-
eral ways can be chosen to speed up the process. Transcrip-
tion can be eliminated either by using speech recognition and
dealing with its errors, or instead applying signal processing
software, e.g., to filter out prosodic information from the user
utterance and/or to transform the wizard?s utterance into syn-
thetically sounding speech (e.g., using a vocoder). Database
search can be sped up in a number of ways too, ranging from
allowing selection directly from the transcribed text to auto-
matically preparing default searches by analyzing the user?s
utterance. Note, however, that the latter will most likely prej-
udice the wizard to stick to the proposed search.
We plan to annotate the corpus, most importantly w.r.t.
wizard presentation strategies and context features relevant
for the choice between them. We also plan to compare the
presentation strategies to the strategies in speech-only mode,
for which we collected data in an earlier experiment (cf.
[Kruijff-Korbayova? et al, 2005]).
For clarification strategies previous studies already showed
that the decision process needs to be highly dynamic by tak-
ing into account various features such as interpretation uncer-
tainties and local utility [Paek and Horvitz, 2000]. We plan
to use the wizard data to learn an initial multi-modal clarifi-
cation policy and later on apply reinforcement learning meth-
ods to the problem in order to account for long-term dialogue
goals, such as task success and user satisfaction.
The screen output options used in the experiment will also
be employed in the baseline system we are currently imple-
menting. The challenges involved there are to decide (i) when
to produce screen output, (ii) what (and how) to display and
(iii) what the corresponding speech output should be. We will
analyze the corpus in order to determine what the suitable
strategies are.
References
[Bernsen and Dybkjaer, 2001] Niels Ole Bernsen and Laila
Dybkjaer. Exploring natural interaction in the car. In
CLASS Workshop on Natural Interactivity and Intelligent
Interactive Information Representation, 2001.
[Bernsen et al, 1998] N. O. Bernsen, H. Dybkj?r, and
L. Dybkj?r. Designing Interactive Speech Systems ?
From First Ideas to User Testing. Springer, 1998.
[Duran et al, 2001] Christine Duran, John Aberdeen, Laurie
Damianos, and Lynette Hirschman. Comparing several as-
pects of human-computer and human-human dialogues. In
Proceedings of the 2nd SIGDIAL Workshop on Discourse
and Dialogue, Aalborg, 1-2 September 2001, pages 48?57,
2001.
[Kruijff-Korbayova? et al, 2005] Ivana Kruijff-Korbayova?,
Tilman Becker, Nate Blaylock, Ciprian Gerstenberger,
Michael Kai?er, Peter Poler, Jan Schehl, and Verena
Rieser. Presentation strategies for flexible multimodal
interaction with a music player. In Proceedings of
DIALOR?05 (The 9th workshop on the semantics and
pragmatics of dialogue (SEMDIAL), 2005.
[Martin et al, 1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. The open agent architecture: A framework for
building distributed software systems. Applied Artificial
Intelligence: An International Journal, 13(1?2):91?128,
Jan?Mar 1999.
[Mattes, 2003] Stefan Mattes. The lane-change-task as a tool
for driver distraction evaluation. In Proceedings of IGfA,
2003.
[Moore et al, 2004] Johanna D. Moore, Mary Ellen Foster,
Oliver Lemon, and Michael White. Generating tailored,
comparative descriptions in spoken dialogue. In Proceed-
ings of the Seventeenth International Florida Artificial In-
telligence Research Sociey Conference, AAAI Press, 2004.
[Paek and Horvitz, 2000] Tim Paek and Eric Horvitz. Con-
versation as action under uncertainty. In Proceedings of
the Sixteenth Conference on Uncertainty in Artificial In-
telligence, 2000.
[Raux et al, 2005] Antoine Raux, Brian Langner, Dan Bo-
hus, Allan W. Black, and Maxine Eskenazi. Let?s go pub-
lic! taking a spoken dialog system to the real world. 2005.
[Skantze, 2003] Gabriel Skantze. Exploring human error
handling strategies: Implications for spoken dialogue sys-
tems. In Proceedings of the ISCA Tutorial and Research
Workshop on Error Handling in Spoken Dialogue Systems,
2003.
[Stuttle et al, 2004] Matthew Stuttle, Jason Williams, and
Steve Young. A framework for dialogue data collection
with a simulated asr channel. In Proceedings of the IC-
SLP, 2004.
[Tu?rk, 2001] Ulrich Tu?rk. The technical processing in
smartkom data collection: a case study. In Proceedings
of Eurospeech2001, Aalborg, Denmark, 2001.
[Walker et al, 2002] Marylin Walker, R. Passonneau, J. Ab-
erdeen, J. Boland, E. Bratt, J. Garofolo, L. Hirschman,
A. Le, S. Lee, S. Narayanan, K. Papineni, B. Pellom,
J. Polifroni, A. Potamianos, P. Prabhu, A. Rudnicky,
G. Sandersa, S. Seneff, D. Stallard, and S. Whittaker.
Cross-site evaluation in darpa communicator: The june
2000 data collection. 2002.
[Williams and Young, 2004] Jason D. Williams and Steve
Young. Characterizing task-oriented dialog using a sim-
ulated asr channel. In Proceedings of the ICSLP, 2004.
The SAMMIE Multimodal Dialogue Corpus Meets the Nite XML Toolkit
Ivana Kruijff-Korbayova?, Verena Rieser,
Ciprian Gerstenberger
Saarland University, Saarbru?cken, Germany
vrieser@coli.uni-sb.de
Jan Schehl, Tilman Becker
DFKI, Saarbru?cken, Germany
jan.schehl@dfki.de
Abstract
We demonstrate work in progress1 us-
ing the Nite XML Toolkit on a cor-
pus of multimodal dialogues with an
MP3 player collected in a Wizard-of-Oz
(WOZ) experiments and annotated with
a rich feature set at several layers. We
designed an NXT data model, converted
experiment log file data and manual tran-
scriptions into NXT, and are building an-
notation tools using NXT libraries.
1 Introduction
In the TALK project2 we are developing a mul-
timodal dialogue system for an MP3 application
for in-car and in-home use. The system should
support natural, flexible interaction and collabo-
rative behavior. To achieve this, it needs to pro-
vide advanced adaptive multimodal output.
To determine the interaction strategies and
range of linguistic behavior naturally occurring
in this scenario, we conducted two WOZ exper-
iments: SAMMIE-1 involved only spoken inter-
action, SAMMIE-2 was multimodal, with speech
and screen input and output.3
We have been annotating the corpus on sev-
eral layers, representing linguistic, multimodal
and context information. The annotated corpus
will be used (i) to investigate various aspects of
1Our demonstration results from the efforts of a larger
team including also N. Blaylock, B. Fromkorth, M. Gra?c,
M. Kai?er, A. Moos, P. Poller and M. Wirth.
2TALK (Talk and Look: Tools for Ambient Linguis-
tic Knowledge; http://www.talk-project.org), funded by the
EU 6th Framework Program, project No. IST-507802.
3SAMMIE stands for Saarbru?cken Multimodal MP3
Player Interaction Experiment.
multimodal presentation and interaction strate-
gies both within and across the annotation lay-
ers; (ii) to design an initial policy for reinforce-
ment learning of multimodal clarifications.4 We
use the Nite XML Toolkit (NXT) (Carletta et al,
2003) to represent and browse the data and to de-
velop annotation tools.
Below we briefly describe our experiment
setup, the collected data and the annotation lay-
ers; we comment on methods and tools for data
representation and annotation, and then present
our NXT data model.
2 Experiment Setup
24 subjects in SAMMIE-1 and 35 in SAMMIE-2
performed several tasks with an MP3 player ap-
plication simulated by a wizard. For SAMMIE-
1 we had two, for SAMMIE-2 six wizards. The
tasks involved searching for titles and building
playlists satisfying various constraints. Each ses-
sion was 30 minutes long. Both users and wiz-
ards could speak freely. The interactions were
in German (although most of the titles and artist
names in the database were English).
SAMMIE-2 had a more complex setup. The
tasks the subjects had to fulfill were divided in
two classes: with vs. without operating a driv-
ing simulator. When presenting the search re-
sults, the wizards were free to produce mono-
or multimodal output as they saw fit; they could
speak freely and/or select one of four automati-
cally generated screen outputs, which contained
tables and lists of found songs/albums. The
users also had free choice between unconstrained
4See (Kruijff-Korbayova? et al, 2006) for more details
about the annotation goals and further usage of the corpus.
69
natural language and/or selecting items on the
screen. Both wizard and user utterances were im-
mediately transcribed. The wizard?s utterances
were presented to the user via a speech synthe-
sizer. To simulate acoustic understanding prob-
lems, the wizard sometimes received only part
of the transcribed user?s utterance, to elicit CRs.
(See (Kruijff-Korbayova? et al, 2005) for details.)
3 Collected Data
The SAMMIE-2 data for each session consists of
a video and audio recording and a log file.5 The
gathered logging information per session con-
sists of Open Agent Architecture (Martin et al,
1999) (OAA) messages in chronological order,
each marked by a timestamp. The log files con-
tain various information, e.g., the transcriptions
of the spoken utterances, the wizard?s database
query and the number of results, the screen op-
tion chosen by the wizard, classification of clari-
fication requests (CRs), etc.
4 Annotation Methods and Tools
The rich set of features we are interested in nat-
urally gives rise to a multi-layered view of the
corpus, where each layer is to be annotated inde-
pendently, but subsequent investigations involve
exploration and automatic processing of the inte-
grated data across layers.
There are two crucial technical requirements
that must be satisfied to make this possible: (i)
stand-off annotation at each layer and (ii) align-
ment of base data across layers. Without the for-
mer, we could not keep the layers separate, with-
out the latter we would not be able to align the
separate layers. An additional equally important
requirement is that elements at different layers
of annotation should be allowed to have overlap-
ping spans; this is crucial because, e.g., prosodic
units and syntactic phrases need not coincide.
Among the existing toolkits that support
multi-layer annotation, it was decided to use
NXT (Carletta et al, 2003)6 in the TALK
project. The NXT-based SAMMIE-2 corpus we
5For 19 sessions the full set of data files exists.
6http://www.ltg.ed.ac.uk/NITE/
are demonstrating has been created in several
steps: (1) The speech data was manually tran-
scribed using the Transcriber tool.7 (2) We auto-
matically extracted features at various annotation
layers by parsing the OAA messages in the log
files. (3) We automatically converted the tran-
scriptions and the information from the log files
into our NXT-based data representation format;
features annotated in the transcriptions and fea-
tures automatically extracted from the log files
were assigned to elements at the appropriate lay-
ers of representation in this step.
Manual annotation: We use tools specifi-
cally designed to support the particular annota-
tion tasks. We describe them below.
As already mentioned, we used Transcriber for
the manual transcriptions. We also performed
certain relatively simple annotations directly on
the transcriptions and coded them in-line by us-
ing special notation. This includes the identifica-
tion of self-speech, the identification of expres-
sions referring to domain objects (e.g., songs,
artists and albums) and the identification of utter-
ances that convey the results of database queries.
For other manual annotation tasks (the annota-
tion of CRs, task segmentation and completion,
referring expressions and the relations between
them) we have been building specialized tools
based on the NXT library of routines for build-
ing displays and interfaces based on Java Swing
(Carletta et al, 2003). Although NXT comes
with a number of example applications, these are
tightly coupled with the architecture of the cor-
pora they were built for. We therefore developed
a core basic tool for our own corpus; we mod-
ify this tool to suite each annotation task. To fa-
cilitate tool development, NXT provides GUI el-
ements linked directly to corpora elements and
support for handling complex multi-layer cor-
pora. This proved very helpful.
Figure 4 shows a screenshot of our CR anno-
tation tool. It allows one to select an utterance
in the left-hand side of the display by clicking
on it, and then choose the attribute values from
the pop-down lists on the right-hand side. Cre-
7http://trans.sourceforge.net/
70
ating relations between elements and creating el-
ements on top of other elements (e.g., words or
utterances) are extensions we are currently im-
plementing (and will complete by the time of the
workshop). First experiences using the tool to
identify CRs are promising.8 When demonstrat-
ing the system we will report the reliability of
other manual annotation tasks.
Automatic annotation using indexing: NXT
also provides a facility for automatic annotation
based on NiteQL query matches (Carletta et al,
2003). Some of our features, e.g., the dialogue
history ones, can be easily derived via queries.
5 The SAMMIE NXT Data Model
NXT uses a stand-off XML data format that con-
sist of several XML files that point to each other.
The NXT data model is a multi-rooted tree with
arbitrary graph structure. Each node has one set
of children, and can have multiple parents.
Our corpus consists of the following layers.
Two base layers: words and graphical output
events; both are time-aligned. On top of these,
structural layers correspond to one session per
subject, divided into task sections, which con-
sist of turns, and these consist of individual ut-
terances, containing words. Graphical output
events will be linked to turns at a featural layer.
Further structural layers are defined for CRs
and dialogue acts (units are utterances), domain
objects and discourse entities (units are expres-
sions consisting of words). We keep independent
layers of annotation separate, even when they can
in principle be merged into a single hierarchy.
Figure 2 shows a screenshot made with Ami-
gram (Lauer et al, 2005), a generic tool for
browsing and searching NXT data. On the left-
hand side one can see the dependencies between
the layers. The elements at the respective layers
are displayed on the right-hand side.
Below we indicate the features per layer:
? Words: Time-stamped words and other
sounds; we mark self-speech, pronuncia-
tion, deletion status, lemma and POS.
8Inter-annotator agreement of 0.788 (? corrected for
prevalence).
? Graphical output: The type and amount of
information displayed, the option selected
by the wizard, and the user?s choices.
? Utterances: Error rates due to word dele-
tion, and various features describing the
syntactic structure, e.g., mood, polarity,
diathesis, complexity and taxis, the pres-
ence of marked syntactic constructions such
as ellipsis, fronting, extraposition, cleft, etc.
? Turns: Time delay, dialogue duration so
far, and other dialogue history features, i.e.
values which accumulate over time.
? Domain objects and discourse entities:
Properties of referring expressions reflect-
ing the type and information status of dis-
course entities, and coreference/bridging
links between them.
? Dialogue acts: DAs based on an agent-
based approach to dialogue as collaborative
problem-solving (Blaylock et al, 2003),
e.g., determining joint objectives, find-
ing and instantiating recipes to accomplish
them, executing recipes and monitoring for
success. We also annotate propositional
content and the database queries.
? CRs: Additional features including the
source and degree of uncertainty, and char-
acteristics of the CRs strategy.
? Tasks: A set of features for estimating user
satisfaction online for reinforcement learn-
ing (Rieser et al, 2005).
? Session: Subject and wizard information,
user questionnaire aswers, and accumulat-
ing attribute values from other layers.
6 Summary
We described a multi-layered corpus of multi-
modal dialogues represented and annotated us-
ing NXT-based tools. Our data model relates lin-
guistic and graphical realization to a rich set of
context features and represents structural, hierar-
chical interactions between different annotation
layers. We combined different annotation meth-
ods to construct the corpus. Manual annotation
and annotation evaluation is on-going. The cor-
pus will be used (i) investigate multimodal pre-
sentation and interaction strategies with respect
71
Figure 1: NXT-based tool for annotating CRs
Figure 2: SAMMIE-2 corpus displayed in Amigram
to dialogue context and (ii) to design an initial
policy for reinforcement learning of multimodal
clarification strategies.
References
[Blaylock et al2003] N. Blaylock, J. Allen, and G. Fergu-
son. 2003. Managing communicative intentions with
collaborative problem solving. In Current and New
Directions in Discourse and Dialogue, pages 63?84.
Kluwer, Dordrecht.
[Carletta et al2003] J. Carletta, S. Evert, U. Heid, J. Kil-
gour, J. Robertson, and H. Voormann. 2003. The NITE
XML Toolkit: flexible annotation for multi-modal lan-
guage data. Behavior Research Methods, Instruments,
and Computers, special issue on Measuring Behavior.
Submitted.
[Kruijff-Korbayova? et al2005] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, J. Schehl, and V. Rieser. 2005. An experiment
setup for collecting data for adaptive output planning in
a multimodal dialogue system. In Proc. of ENLG.
[Kruijff-Korbayova? et al2006] I. Kruijff-Korbayova?,
T. Becker, N. Blaylock, C. Gerstenberger, M. Kai?er,
P. Poller, V. Rieser, and J. Schehl. 2006. The SAMMIE
corpus of multimodal dialogues with an mp3 player. In
Proc. of LREC (to appear).
[Lauer et al2005] C. Lauer, J. Frey, B. Lang, T. Becker,
T. Kleinbauer, and J. Alexandersson. 2005. Amigram
- a general-purpose tool for multimodal corpus annota-
tion. In Proc. of MLMI.
[Martin et al1999] D. L. Martin, A. J. Cheyer, and D. B.
Moran. 1999. The open agent architecture: A frame-
work for building distributed software systems. Applied
Artificial Intelligence: An International Journal, 13(1?
2):91?128, Jan?Mar.
[Rieser et al2005] V. Rieser, I. Kruijff-Korbayova?, and
O. Lemon. 2005. A corpus collection and annotation
framework for learning multimodal clarification strate-
gies. In Proc. of SIGdial.
72
Proceedings of the 8th International Conference on Computational Semantics, pages 320?325,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Well-formed Default Unification in
Non-deterministic Multiple Inheritance Hierarchies
Christian Schulz, Jan Alexandersson and Tilman Becker
DFKI, Saarbru?cken
1 Introduction
Default unification represents a fundamental extension of the common uni-
fication operation, where even in case of inconsistency between the infor-
mation to be merged, there will always be a result. As default unification
is no longer a commutative operation, a distinction is made between the
argument containing default information, the background, and the other ar-
gument consisting of non-default information, the cover.
An elegant point of departure to formalizing default unification has been
delivered by Carpenter?s credulous default unification (CDU) (Carpenter,
1993):
F
<
?
c
G = {F ? G
?
| G
?
? G is maximal such that F ? G
?
is defined}
Unlike previous strategies to replace conflicting parts by the non-default
information (Shieber, 1986; Kaplan, 1987; Bouma, 1990), Carpenter sug-
gests in case of clashes to relax the specificity in the defeasible part to
achieve consistency with the non-default structure. Since the detection of
the generalized default might not be deterministic, multiple results may
arise.
[
Entertainment
beginTime
...
]
[
Broadcast
Channel
...
] [
Performance
Location
...
]
(1) User: I?d like to go to the movies
tonight.
(2) SmartKom: Here (?) are the
films showing in Heidelberg.
(3) User: No, there is nothing inter-
esting there,
(4) User: what is showing on TV?
Figure 1: An excerpt from the SmartKom ontology and the multi-modal interaction
of a dialog between the user and the system. ? denotes the pointing gesture by the
presentation agent.
320
A CDU formalization in (Alexandersson & Becker, 2007) is used in the
SmartKom system (Wahlster, 2006) on the basis of typed feature structures
(TFS) and CDU as the knowledge representation and inference mechanism,
in order to monitor the discourse processing, where the user?s input, the
cover, updates the internal dialogue state, the background, see figure 1.
The second crucial point of departure is how Alexandersson and Becker
understand to exploit the information inherent to an inheritance hierarchy
and henceforward to transfer Carpenter?s notion to a very effective and
sound way to specify CDU, which stands in contrast to the formalization by
(Grover, Brew, Manandhar, & Moens, 1994)
1
that suffers from theoretical
and computational drawbacks.
This Work (Schulz, 2007) develops an extension to the CDU operation
presented in (Alexandersson & Becker, 2007) for which the CDU algorithm
is adapted to deal with multiple inheritance type hierarchies omitting the
requirement to be bounded complete.
2 Efficient Computation of Well-formed Default
Unification
The core result of our work is a refinement of the characterization of CDU
as described in (Carpenter, 1993) along with a revision of the algorithm
presented in (Alexandersson & Becker, 2007). By identifying basic scenar-
ios, see figure 2, we have gained insights into the limitations of Carpenter
as described below and identified the resulting deficits of the algorithm in
Alexandersson and Becker. Crucial facets of our work include efficient lat-
tice operations, for the calculation of maximal lower bound (mlb) and min-
imal upper bound (mub). During CDU we suggest that the well-formedness
condition on TFS formulated in (Copestake, 1992) is met. The two-stage
procedural approach of the CDU mechanism is as the following:
1 Assimilation is a preprocessing step that first computes the target types for the
cover and background; the Type Preprocessing. Next, the cover and the background
will be translated to their appropriate target types by the operations Specialization
and Generalization respectively.
2 Overlay? performs type assignment by combining the information from the two
TFSs returned by the assimilation and monitors the recursive invocation of Assim-
ilation.
2
1
In Grover et al priority union is aimed to resolve parallelism dependent anaphora,
in particular verb phrase ellipsis with a resolution mechanism based on the linguistic
discourse model of (Pru?st, Scha, & Berg, 1994).
2
For a detailed description of Overlay? see (Alexandersson & Becker, 2007).
321
t5
t
co
t
1
t
2
t
8
t
6
t
7
t
bg
t
3
t
4
t
3
t
1
t
2
t
co
t
bg
t
3
Figure 2: A CDU scenario that is not entirely covered by Carpenter?s definition (left).
Determination of the required search space of all relevant type configurations (right).
A convenient concept is the definition of the type configuration, which
enables to describe the Type Preprocessing effectively:
Definition 1 Type Configuration
Given t
bg
, the type of the background and t
co
, the type of the cover; a type
configuration t
conf
, ?t
?
bg
, t
?
co
? is a pair of type identifiers where, t
?
bg
is the abstract
background type of t
bg
with ?
t
(t
?
bg
, t
co
) 6= ? and t
?
bg
? t
bg
; t
?
co
is the abstract cover
type of t
co
with t
?
co
? ?
t
(t
?
bg
, t
co
)
At this point we would like to express subsumption ordering between two type
configurations t
?
conf
= ?t
?
bg
, t
?
co
? and t
??
conf
= ?t
??
bg
, t
??
co
? as follows. We say t
?
conf
?
t
??
conf
, if t
?
bg
? t
??
bg
and t
?
co
? t
??
co
. Given some type t and ?(q
co
) = t
co
we additionally
propose a function, that returns the potential abstract background types located
in the next level of the hierarchy T (t) = {t
?
| t
?
? t with t
?
is maximal and
?
t
(t
?
, t
co
) 6= ?}.
The scenario in figure 2 (left) bears the type configuration ?t
2
, t
3
? which is
covered by Carpenter?s definition, since t
2
corresponds to the relaxed struc-
ture of the background which according to Carpenter should contain max-
imal information. However, ?t
5
, t
4
? encloses the valid abstract background
type t
5
that is more general than t
2
, which is caused by the non-deterministic
inheritance behaviour among background?s supertypes; ?
t
(t
5
, t
co
) yields a
non-empty set {t
7
, t
4
}. This example reveals the limits of Carpenter?s defi-
nition, that can be adjusted by the omission of the restriction on specificity
imposed on the generalized background structure:
Definition 2 Credulous Default Unification Revised
F
<
?
c
G = {F ? G
?
| G
?
? G such that F ? G
?
is defined and maximal}
The definition says that the result of CDU between F and G are rep-
resented by unifications between F and G
?
, that are defined, most specific
322
and G
?
subsumes G. For this purpose we motivate the task of Type Prepro-
cessing to identify all type configurations that are relevant for subsequent
procedures in CDU. The hierarchy in figure 2 (left) comprises the type con-
figurations {?t
2
, t
3
?, ?t
5
, t
4
?, ?t
5
, t
7
?}, though ?t
2
, t
3
? makes ?t
5
, t
7
? dispens-
able, since ?t
5
, t
7
? results into a TFS that subsumes the outcome stemming
from ?t
2
, t
3
?. Type Preprocessing sorts out redundant type configurations by
the subsumption check and prevents them to be input for the Specialization
and the Generalization process respectively.
The algorithm of Alexandersson and Becker narrows down the search for
abstract background types to the candidates that are situated exclusively on
the path between the background type and the least upper bound of cover
and background. The hierarchy in figure 2 (right), however, contains an
abstract background type that is not located on any path towards elements
of mub, i. e., it is justified to state that in addition to t
2
also the type t
1
is
an abstract background type. In order to capture all potentially valid type
configurations the search may only terminate if the currently visited type
subsumes a mub element.
Algorithm 1 Assimilation
Let co and bg be two TFS such that co = ?Q
co
, q
co
, ?
co
, ?
co
? and bg = ?Q
bg
, q
bg
, ?
bg
,
?
bg
?. Further we have t
bg
:= ?
bg
(q?
bg
) and t
co
:= ?
co
(q?
co
). The assimilation of co
and bg, A(bg, co) = ?, where ? is a set of tuples of bg
?
and co
?
such that:
(1) if t
bg
? t
co
then
bg
?
= bg, co
?
= co
?bg
?
, co
?
? ? ?
(2) if t
co
? t
bg
if ?(co,MGsat (t
bg
)) 6= ?
then
CO
?
= ?(co,MGsat(t
bg
))
bg
?
= bg
{bg
?
} ? CO
?
? ?
else
go to (3
?
) with T (t
bg
),
t
mub
= t
co
(3) else
for each t
mub
? ?
t
(t
bg
, t
co
)
go to (3
?
) with T (t
bg
)
(3
?
)
for each t
bg
?
? T (t) go to (3.1)
(3.1)
if t
?
bg
? t
mub
then exit
else
for each t
mlb
? ?
t
(t
?
bg
, t
co
)
go to (3.2)
(3.2)
if ?t
?
bg
, t
mlb
? 6? t
?
conf
? t
?
conf
,
with t
?
conf
= ??(q
bg
?
), ?(q
co
?
)?
and ?bg
?
, co
?
? ? ?
then go to (3.3)
else go to (3
?
) with T (t
?
bg
)
(3.3.)
if ?(co,MGsat(t
mlb
)) 6= ?
then
CO
?
= ?(co,MGsat(t
mlb
))
bg
?
= G(bg, t
?
bg
)
{bg
?
} ? CO
?
? ?
go to (3
?
) with T (t
?
bg
)
else go to (3
?
) with T (t
?
bg
)
Figure 3: Along the CDU procedure Overlay? recursively assimilates the structures of
the arguments one level deeper and combines the previously assimilated parts stemming
from background and cover resulting in an intermediate TFS.
323
During CDU well-formedness of TFS is guaranteed if exclusively Special-
ization is well-formed, where its operational foundation refers to the ternary
unification operation in (Copestake, 1992). In the algorithm specification 1
we present the Assimilation algorithm concerning multiple inheritance hier-
archies featuring non-determinism.
3 Conclusion and Future Work
We have motivated and demonstrated a formalism of CDU dealing with mul-
tiple inheritance hierarchies featuring non-determinism. Thereby we have
provided a reformulation of Carpenter?s definition and a precisely formalized
characterization of the algorithm extending the work in (Alexandersson &
Becker, 2007). The non-deterministic behaviour increases considerably the
degree of difficulty regarding lattice operations in order to identify all pos-
sible outcomes in CDU. To this end, we were able to boil down the efficient
implementation of CDU to an efficient realization of mlb and mub compu-
tation based on the detailed discussion in (Schulz, 2007).
In this work we have gained insights considering coreferences, though
we postpone the theoretical and practical elaboration as a goal for future
research. A crucial contribution in (Alexandersson, Becker, & Pfleger, 2004)
is the usage of a scoring function that computes the best hypothesis among
the multiple outcome of CDU. The notion of informational distance appears
to be a reasonable device to restrain the production of multiple results in
the current context as well.
References
Alexandersson, J., & Becker, T. (2007). Efficient Computation of Over-
lay for Multiple Inheritance Hierarchies in Discourse Modeling. In
H. Bunt & R. Muskens (Eds.), (Vol. 3, pp. 423?455). Dordrecht: Dor-
drecht:Kluwer.
Alexandersson, J., Becker, T., & Pfleger, N. (2004). Scoring for overlay
based on informational distance. In Proceedings of Konvens 2004 (pp.
1?4). Vienna, Austria.
Bouma, G. (1990, 6?9 June). Defaults in Unification Grammar. In Pro-
ceedings of the 28th annual meeting on Association for Computational
Linguistics (pp. 165?172). Morristown, NJ, USA: Association for Com-
putational Linguistics.
324
Carpenter, B. (1993). Skeptical and Credulous Default Unification with
Applications to Templates and Inheritance. In T. Briscoe, V. de Paiva,
& A. Copestake (Eds.), Inheritance, Defaults, and the Lexicon (pp.
13?37). Cambridge, CA: Cambridge University Press.
Copestake, A. (1992). The Representation of Lexical Semantic Information.
Doctoral dissertation, University of Sussex.
Grover, C., Brew, C., Manandhar, S., & Moens, M. (1994). Priority Union
and Generalization in Discourse Grammars. In 32nd. Annual Meeting
of the Association for Computational Linguistics (pp. 17 ? 24). Las
Cruces, NM: Association for Computational Linguistics.
Kaplan, R. M. (1987). Three Seductions of Computational Psycholinguis-
tics. In P. Whitelock, H. Somers, P. Bennett, R. Johnson, & M. M.
Wood (Eds.), Linguistic Theory and Computer Applications (pp. 149?
188). London: Academic Press.
Pru?st, H., Scha, R., & Berg, M. van den. (1994). Discourse Grammar
and Verb Phrase Anaphora. In Linguistics and Philosophy 17 (pp.
261?327). Amsterdam, Netherlands: Springer.
Schulz, C. H. (2007). Well-formed Default Unification in Multiple Inheri-
tance Hierarchies. Diploma thesis, Saarland University, Saarbru?cken,
Germany.
Shieber, S. M. (1986). A Simple Reconstruction of GPSG. In Proc. of the
11th COLING (pp. 211?215). Morristown, NJ, USA: Association for
Computational Linguistics.
Wahlster, W. (2006). Dialogue Systems Go Multimodal: The Smartkom Ex-
perience. In Smartkom - Foundations of Multimodal Dialogue Systems
(pp. 3?27). Heidelberg, Germany: Springer.
325
