Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45?48,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Bayesian Learning of a Tree Substitution Grammar
Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
Tree substitution grammars (TSGs) of-
fer many advantages over context-free
grammars (CFGs), but are hard to learn.
Past approaches have resorted to heuris-
tics. In this paper, we learn a TSG us-
ing Gibbs sampling with a nonparamet-
ric prior to control subtree size. The
learned grammars perform significantly
better than heuristically extracted ones on
parsing accuracy.
1 Introduction
Tree substition grammars (TSGs) have potential
advantages over regular context-free grammars
(CFGs), but there is no obvious way to learn these
grammars. In particular, learning procedures are
not able to take direct advantage of manually an-
notated corpora like the Penn Treebank, which are
not marked for derivations and thus assume a stan-
dard CFG. Since different TSG derivations can
produce the same parse tree, learning procedures
must guess the derivations, the number of which is
exponential in the tree size. This compels heuristic
methods of subtree extraction, or maximum like-
lihood estimators which tend to extract large sub-
trees that overfit the training data.
These problems are common in natural lan-
guage processing tasks that search for a hid-
den segmentation. Recently, many groups have
had success using Gibbs sampling to address the
complexity issue and nonparametric priors to ad-
dress the overfitting problem (DeNero et al, 2008;
Goldwater et al, 2009). In this paper we apply
these techniques to learn a tree substitution gram-
mar, evaluate it on the Wall Street Journal parsing
task, and compare it to previous work.
2 Model
2.1 Tree substitution grammars
TSGs extend CFGs (and their probabilistic coun-
terparts, which concern us here) by allowing non-
terminals to be rewritten as subtrees of arbitrary
size. Although nonterminal rewrites are still
context-free, in practice TSGs can loosen the in-
dependence assumptions of CFGs because larger
rules capture more context. This is simpler than
the complex independence and backoff decisions
of Markovized grammars. Furthermore, subtrees
with terminal symbols can be viewed as learn-
ing dependencies among the words in the subtree,
obviating the need for the manual specification
(Magerman, 1995) or automatic inference (Chiang
and Bikel, 2002) of lexical dependencies.
Following standard notation for PCFGs, the
probability of a derivation d in the grammar is
given as
Pr(d) =
?
r?d
Pr(r)
where each r is a rule used in the derivation. Un-
der a regular CFG, each parse tree uniquely idenfi-
fies a derivation. In contrast, multiple derivations
in a TSG can produce the same parse; obtaining
the parse probability requires a summation over
all derivations that could have produced it. This
disconnect between parses and derivations com-
plicates both inference and learning. The infer-
ence (parsing) task for TSGs is NP-hard (Sima?an,
1996), and in practice the most probable parse is
approximated (1) by sampling from the derivation
forest or (2) from the top k derivations.
Grammar learning is more difficult as well.
CFGs are usually trained on treebanks, especially
the Wall Street Journal (WSJ) portion of the Penn
Treebank. Once the model is defined, relevant
45
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0  2  4  6  8  10  12  14
subtree height
Figure 1: Subtree count (thousands) across heights
for the ?all subtrees? grammar () and the supe-
rior ?minimal subset? () from Bod (2001).
events can simply be counted in the training data.
In contrast, there are no treebanks annotated with
TSG derivations, and a treebank parse tree of n
nodes is ambiguous among 2n possible deriva-
tions. One solution would be to manually annotate
a treebank with TSG derivations, but in addition
to being expensive, this task requires one to know
what the grammar actually is. Part of the thinking
motivating TSGs is to let the data determine the
best set of subtrees.
One approach to grammar-learning is Data-
Oriented Parsing (DOP), whose strategy is to sim-
ply take all subtrees in the training data as the
grammar (Bod, 1993). Bod (2001) did this, ap-
proximating ?all subtrees? by extracting from the
Treebank 400K random subtrees for each subtree
height ranging from two to fourteen, and com-
pared the performance of that grammar to that
of a heuristically pruned ?minimal subset? of it.
The latter?s performance was quite good, achiev-
ing 90.8% F
1
score1 on section 23 of the WSJ.
This approach is unsatisfying in some ways,
however. Instead of heuristic extraction we would
prefer a model that explained the subtrees found
in the grammar. Furthermore, it seems unlikely
that subtrees with ten or so lexical items will be
useful on average at test time (Bod did not report
how often larger trees are used, but did report that
including subtrees with up to twelve lexical items
improved parser performance). We expect there to
be fewer large subtrees than small ones. Repeat-
ing Bod?s grammar extraction experiment, this is
indeed what we find when comparing these two
grammars (Figure 1).
In summary, we would like a principled (model-
based) means of determining from the data which
1The harmonic mean of precision and recall: F
1
=
2PR
P+R
.
set of subtrees should be added to our grammar,
and we would like to do so in a manner that prefers
smaller subtrees but permits larger ones if the data
warrants it. This type of requirement is common in
NLP tasks that require searching for a hidden seg-
mentation, and in the following sections we apply
it to learning a TSG from the Penn Treebank.
2.2 Collapsed Gibbs sampling with a DP
prior2
For an excellent introduction to collapsed Gibbs
sampling with a DP prior, we refer the reader to
Appendix A of Goldwater et al (2009), which we
follow closely here. Our training data is a set of
parse trees T that we assume was produced by an
unknown TSG g with probability Pr(T |g). Using
Bayes? rule, we can compute the probability of a
particular hypothesized grammar as
Pr(g | T ) =
Pr(T | g) Pr(g)
Pr(T )
Pr(g) is a distribution over grammars that ex-
presses our a priori preference for g. We use a set
of Dirichlet Process (DP) priors (Ferguson, 1973),
one for each nonterminal X ? N , the set of non-
terminals in the grammar. A sample from a DP
is a distribution over events in an infinite sample
space (in our case, potential subtrees in a TSG)
which takes two parameters, a base measure and a
concentration parameter:
g
X
? DP (G
X
, ?)
G
X
(t) = Pr
$
(|t|; p
$
)
?
r?t
PrMLE(r)
The base measure G
X
defines the probability of a
subtree t as the product of the PCFG rules r ? t
that constitute it and a geometric distribution Pr
$
over the number of those rules, thus encoding a
preference for smaller subtrees.3 The parameter ?
contributes to the probability that previously un-
seen subtrees will be sampled. All DPs share pa-
rameters p
$
and ?. An entire grammar is then
given as g = {g
X
: X ? N}. We emphasize that
no head information is used by the sampler.
Rather than explicitly consider each segmen-
tation of the parse trees (which would define a
TSG and its associated parameters), we use a col-
lapsed Gibbs sampler to integrate over all possi-
2Cohn et al (2009) and O?Donnell et al (2009) indepen-
dently developed similar models.
3
G
X
(t) = 0 unless root(t) = X .
46
S1
NP
NN
ADVP
RB VBZ S2
NP
PRP
you
VP
VB
quit
Someone always makes
VP
Figure 2: Depiction of sub(S
2
) and sub(S
2
).
Highlighted subtrees correspond with our spinal
extraction heuristic (?3). Circles denote nodes
whose flag=1.
ble grammars and sample directly from the poste-
rior. This is based on the Chinese Restaurant Pro-
cess (CRP) representation of the DP. The Gibbs
sampler is an iterative procedure. At initialization,
each parse tree in the corpus is annotated with a
specific derivation by marking each node in the
tree with a binary flag. This flag indicates whether
the subtree rooted at that node (a height one CFG
rule, at minimum) is part of the subtree contain-
ing its parent. The Gibbs sampler considers ev-
ery non-terminal, non-root node c of each parse
tree in turn, freezing the rest of the training data
and randomly choosing whether to join the sub-
trees above c and rooted at c (outcome h
1
) or to
split them (outcome h
2
) according to the probabil-
ity ratio ?(h
1
)/(?(h
1
) + ?(h
2
)), where ? assigns
a probability to each of the outcomes (Figure 2).
Let sub(n) denote the subtree above and includ-
ing node n and sub(n) the subtree rooted at n; ? is
a binary operator that forms a single subtree from
two adjacent ones. The outcome probabilities are:
?(h
1
) = ?(t)
?(h
2
) = ?(sub(c)) ? ?(sub(c))
where t = sub(c) ? sub(c). Under the CRP, the
subtree probability ?(t) is a function of the current
state of the rest of the training corpus, the appro-
priate base measure G
root(t)
, and the concentra-
tion parameter ?:
?(t) =
count
z
t
(t) + ?G
root(t)
(t)
|z
t
| + ?
where z
t
is the multiset of subtrees in the frozen
portion of the training corpus sharing the same
root as t, and count
z
t
(t) is the count of subtree
t among them.
3 Experiments
3.1 Setup
We used the standard split for the Wall Street Jour-
nal portion of the Treebank, training on sections 2
to 21, and reporting results on sentences with no
more than forty words from section 23.
We compare with three other grammars.
? A standard Treebank PCFG.
? A ?spinal? TSG, produced by extracting n
lexicalized subtrees from each length n sen-
tence in the training data. Each subtree is de-
fined as the sequence of CFG rules from leaf
upward all sharing a head, according to the
Magerman head-selection rules. We detach
the top-level unary rule, and add in counts
from the Treebank CFG rules.
? An in-house version of the heuristic ?mini-
mal subset? grammar of Bod (2001).4
We note two differences in our work that ex-
plain the large difference in scores for the minimal
grammar from those reported by Bod: (1) we did
not implement the smoothed ?mismatch parsing?,
which permits lexical leaves of subtrees to act as
wildcards, and (2) we approximate the most prob-
able parse with the top single derivation instead of
the top 1,000.
Rule probabilities for all grammars were set
with relative frequency. The Gibbs sampler was
initialized with the spinal grammar derivations.
We construct sampled grammars in two ways: by
summing all subtree counts from the derivation
states of the first i sampling iterations together
with counts from the Treebank CFG rules (de-
noted (?, p
$
,?i)), and by taking the counts only
from iteration i (denoted (?, p
$
, i)).
Our standard CKY parser and Gibbs sampler
were both written in Perl. TSG subtrees were flat-
tened to CFG rules and reconstructed afterward,
with identical mappings favoring the most proba-
ble rule. For pruning, we binned nonterminals ac-
cording to input span and degree of binarization,
keeping the ten highest scoring items in each bin.
3.2 Results
Table 1 contains parser scores. The spinal TSG
outperforms a standard unlexicalized PCFG and
4All rules of height one, plus 400K subtrees sampled at
each height h, 2 ? h ? 14, minus unlexicalized subtrees of
h > 6 and lexicalized subtrees with more than twelve words.
47
grammar size LP LR F
1
PCFG 46K 75.37 70.05 72.61
spinal 190K 80.30 78.10 79.18
minimal subset 2.56M 76.40 78.29 77.33
(10, 0.7, 100) 62K 81.48 81.03 81.25
(10, 0.8, 100) 61K 81.23 80.79 81.00
(10, 0.9, 100) 61K 82.07 81.17 81.61
(100, 0.7, 100) 64K 81.23 80.98 81.10
(100, 0.8, 100) 63K 82.13 81.36 81.74
(100, 0.9, 100) 62K 82.11 81.20 81.65
(100, 0.7,?100) 798K 82.38 82.27 82.32
(100, 0.8,?100) 506K 82.27 81.95 82.10
(100, 0.9,?100) 290K 82.64 82.09 82.36
(100, 0.7, 500) 61K 81.95 81.76 81.85
(100, 0.8, 500) 60K 82.73 82.21 82.46
(100, 0.9, 500) 59K 82.57 81.53 82.04
(100, 0.7,?500) 2.05M 82.81 82.01 82.40
(100, 0.8,?500) 1.13M 83.06 82.10 82.57
(100, 0.9,?500) 528K 83.17 81.91 82.53
Table 1: Labeled precision, recall, and F
1
on
WSJ?23.
the significantly larger ?minimal subset? grammar.
The sampled grammars outperform all of them.
Nearly all of the rules of the best single iteration
sampled grammar (100, 0.8, 500) are lexicalized
(50,820 of 60,633), and almost half of them have
a height greater than one (27,328). Constructing
sampled grammars by summing across iterations
improved over this in all cases, but at the expense
of a much larger grammar.
Figure 3 shows a histogram of subtree size taken
from the counts of the subtrees (by token, not type)
actually used in parsing WSJ?23. Parsing with
the ?minimal subset? grammar uses highly lexi-
calized subtrees, but they do not improve accuracy.
We examined sentence-level F
1
scores and found
that the use of larger subtrees did correlate with
accuracy; however, the low overall accuracy (and
the fact that there are so many of these large sub-
trees available in the grammar) suggests that such
rules are overfit. In contrast, the histogram of sub-
tree sizes used in parsing with the sampled gram-
mar matches the shape of the histogram from the
grammar itself. Gibbs sampling with a DP prior
chooses smaller but more general rules.
4 Summary
Collapsed Gibbs sampling with a DP prior fits
nicely with the task of learning a TSG. The sam-
pled grammars are model-based, are simple to
specify and extract, and take the expected shape
100
101
102
103
104
105
106
 0  2  4  6  8  10  12
number of words in subtree?s frontier
(100,0.8,500), actual grammar
(100,0.8,500), used parsing WSJ23
minimal, actual grammar
minimal, used parsing WSJ23
Figure 3: Histogram of subtrees sizes used in pars-
ing WSJ?23 (filled points), as well as from the
grammars themselves (outlined points).
over subtree size. They substantially outperform
heuristically extracted grammars from previous
work as well as our novel spinal grammar, and can
do so with many fewer rules.
Acknowledgments This work was supported by
NSF grants IIS-0546554 and ITR-0428020.
References
Rens Bod. 1993. Using an annotated corpus as a
stochastic grammar. In Proc. ACL.
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy. In Proc. ACL.
David Chiang and Daniel M. Bikel. 2002. Recovering
latent information in treebanks. In COLING.
Trevor Cohn, Sharon Goldwater, and Phil Blun-
som. 2009. Inducing compact but accurate tree-
substitution grammars. In Proc. NAACL.
John DeNero, Alexandre Bouchard-Co?te?, and Dan
Klein. 2008. Sampling alignment structure under
a Bayesian translation model. In EMNLP.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathe-
matical Statistics, 1(2):209?230.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
T.J. O?Donnell, N.D. Goodman, J. Snedeker, and J.B.
Tenenbaum. 2009. Computation and reuse in lan-
guage. In Proc. Cognitive Science Society.
Khalil Sima?an. 1996. Computational complexity of
probabilistic disambiguation by means of tree gram-
mars. In COLING.
48
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89?98,
Paris, October 2009. c?2009 Association for Computational Linguistics
Weight pushing and binarization for fixed-grammar parsing
Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We apply the idea of weight pushing
(Mohri, 1997) to CKY parsing with fixed
context-free grammars. Applied after
rule binarization, weight pushing takes the
weight from the original grammar rule and
pushes it down across its binarized pieces,
allowing the parser to make better prun-
ing decisions earlier in the parsing pro-
cess. This process can be viewed as gen-
eralizing weight pushing from transduc-
ers to hypergraphs. We examine its ef-
fect on parsing efficiency with various bi-
narization schemes applied to tree sub-
stitution grammars from previous work.
We find that weight pushing produces dra-
matic improvements in efficiency, espe-
cially with small amounts of time and with
large grammars.
1 Introduction
Fixed grammar-parsing refers to parsing that em-
ploys grammars comprising a finite set of rules
that is fixed before inference time. This is in
contrast to markovized grammars (Collins, 1999;
Charniak, 2000), variants of tree-adjoining gram-
mars (Chiang, 2000), or grammars with wildcard
rules (Bod, 2001), all of which allow the con-
struction and use of rules not seen in the training
data. Fixed grammars must be binarized (either
explicitly or implicitly) in order to maintain the
O(n3|G|) (n the sentence length, |G| the grammar
size) complexity of algorithms such as the CKY
algorithm.
Recently, Song et al (2008) explored different
methods of binarization of a PCFG read directly
from the Penn Treebank (the Treebank PCFG),
showing that binarization has a significant effect
on both the number of rules and new nontermi-
nals introduced, and subsequently on parsing time.
This variation occurs because different binariza-
tion schemes produce different amounts of shared
rules, which are rules produced during the bina-
rization process from more than one rule in the
original grammar. Increasing sharing reduces the
amount of state that the parser must explore. Bina-
rization has also been investigated in the context of
parsing-based approaches to machine translation,
where it has been shown that paying careful atten-
tion to the binarization scheme can produce much
faster decoders (Zhang et al, 2006; Huang, 2007;
DeNero et al, 2009).
The choice of binarization scheme will not af-
fect parsing results if the parser is permitted to ex-
plore the whole search space. In practice, how-
ever, this space is too large, so parsers use prun-
ing to discard unlikely hypotheses. This presents
a problem for bottom-up parsing algorithms be-
cause of the way the probability of a rule is dis-
tributed among its binarized pieces: The standard
approach is to place all of that probability on the
top-level binarized rule, and to set the probabilities
of lower binarized pieces to 1.0. Because these
rules are reconstructed from the bottom up, prun-
ing procedures do not have a good estimate of the
complete cost of a rule until the entire original rule
has been reconstructed. It is preferable to have this
information earlier on, especially for larger rules.
In this paper we adapt the technique of weight
pushing for finite state transducers (Mohri, 1997)
to arbitrary binarizations of context-free grammar
rules. Weight pushing takes the probability (or,
more generally, the weight) of a rule in the origi-
nal grammar and pushes it down across the rule?s
binarized pieces. This helps the parser make bet-
89
ter pruning decisions, and to make them earlier in
the bottom-up parsing process. We investigate this
algorithm with different binarization schemes and
grammars, and find that it improves the time vs.
accuracy tradeoff for parsers roughly proportion-
ally to the size of the grammar being binarized.
This paper extends the work of Song et al
(2008) in three ways. First, weight pushing fur-
ther reduces the amount of time required for pars-
ing. Second, we apply these techniques to Tree
Substitution Grammars (TSGs) learned from the
Treebank, which are both larger and more accu-
rate than the context-free grammar read directly
from the Treebank.1 Third, we examine the inter-
action between binarization schemes and the in-
exact search heuristic of beam-based and k-best
pruning.
2 Weight pushing
2.1 Binarization
Not all binarization schemes are equivalent in
terms of efficiency of representation. Consider the
grammar in the lefthand column of Figure 1 (rules
1 and 2). If this grammar is right-binarized or
left-binarized, it will produce seven rules, whereas
the optimal binarization (depicted) produces only
5 rules due to the fact that two of them are shared.
Since the complexity of parsing with CKY is a
function of the grammar size as well as the input
sentence length, and since in practice parsing re-
quires significant pruning, having a smaller gram-
mar with maximal shared substructure among the
rules is desirable.
We investigate two kinds of binarization in this
paper. The first is right binarization, in which non-
terminal pairs are collapsed beginning from the
two rightmost children and moving leftward. The
second is a greedy binarization, similar to that of
Schmid (2004), in which the most frequently oc-
curring (grammar-wide) nonterminal pair is col-
lapsed in turn, according to the algorithm given in
Figure 2.
Binarization must ensure that the product of the
probabilities of the binarized pieces is the same as
that of the original rule. The easiest way to do
this is to assign each newly-created binarized rule
a probability of 1.0, and give the top-level rule the
complete probability of the original rule. In the
following subsection, we describe a better way.
1The mean rule rank in a Treebank PCFG is 2.14, while
the mean rank in our sampled TSG is 8.51. See Table 1.
NP
a JJ NN NN PP
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
?a:??JJ:NN?:NN??
a
PP
NPE
C
B
A
1Rule
NP
the JJ NN NN
Rule 2
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
NP
the
A
B
D
Figure 1: A two-rule grammar. The greedy
binarization algorithm produces the binarization
shown, with the shared structure highlighted. Bi-
narized rules A, B, and C are initially assigned
a probability of 1.0, while rules D and E are as-
signed the original probabilities of rules 2 and 1,
respectively.
2.2 Weight pushing
Spreading the weight of an original rule across
its binarized pieces is complicated by sharing,
because of the constraint that the probability of
shared binarized pieces must be set so that the
product of their probabilities is the same as the
original rule, for each rule the shared piece partici-
pates in. Mohri (1997) introduced weight pushing
as a step in the minimization of weighted finite-
state transducers (FSTs), which addressed a sim-
ilar problem for tasks employing finite-state ma-
chinery. At a high level, weight pushing moves
the weight of a path towards the initial state, sub-
ject to the constraint that the weight of each path
in the FST is unchanged. To do weight pushing,
one first computes for each state q in the trans-
ducer the shortest distance d(q) to any final state.
Let ?(q, a) be the state transition function, deter-
ministically transitioning on input a from state q to
state ?(q, a). Pushing adjusts the weight of each
edge w(e) according to the following formula:
w?(e) = d(q)?1 ? w(e)? d(?(q, a)) (1)
Mohri (1997, ?3.7) and Mohri and Riley (2001)
discuss how these operations can be applied us-
ing various semirings; in this paper we use the
(max,?) semiring. The important observation for
our purposes is that pushing can be thought of as a
sequence of local operations on individual nodes
90
1: function GREEDYBINARIZE(P )
2: while RANK(P ) > 2 do
3: ? := UPDATECOUNTS(P )
4: for each rule X ? x1x2 ? ? ?xr do
5: b := argmaxi?(2???r) ?[xi?1, xi]
6: l := ?xb?1 : xb?
7: add l ? xb?1xb to P
8: replace xb?1xb with l in rule
9: function UPDATECOUNTS(P )
10: ? := {} ? a dictionary
11: for each rule X ? x1x2 ? ? ?xr ? P do
12: for i ? (2 ? ? ? r) do
13: ?[xi?1, xi]++
return ?
Figure 2: A greedy binarization algorithm. The
rank of a grammar is the rank of its largest rule.
Our implementation updates the counts in ? more
efficiently, but we present it this way for clarity.
q, shifting a constant amount of weight d(q)?1
from q?s outgoing edges to its incoming edges.
Klein and Manning (2003) describe an encod-
ing of context-free grammar rule binarization that
permits weight pushing to be applied. Their ap-
proach, however, works only with left or right bi-
narizations whose rules can be encoded as an FST.
We propose a form of weight pushing that works
for arbitrary binarizations. Weight pushing across
a grammar can be viewed as generalizing push-
ing from weighted transducers to a certain kind of
weighted hypergraph. To begin, we use the fol-
lowing definition of a hypergraph:
Definition. A hypergraph H is a tuple
?V,E, F,R?, where V is a set of nodes, E is a
set of hyperedges, F ? V is a set of final nodes,
and R is a set of permissible weights on the hy-
peredges. Each hyperedge e ? E is a triple
?T (e), h(e), w(e)?, where h(e) ? V is its head
node, T (e) is a sequence of tail nodes, and w(e) is
its weight.
We can arrange the binarized rules of Figure 1
into a shared hypergraph forest (Figure 3), with
nodes as nonterminals and binarized rules as hy-
peredges. We distinguish between final and non-
final nodes and hyperedges. Nonfinal nodes are
those in V ?F . Nonfinal hyperdges ENF are those
in {e : h(e) ? V ? F}, that is, all hyperedges
whose head is a nonfinal node. Because all nodes
introduced by our binarization procedure expand
deterministically, each nonfinal node is the head
of no more than one such hyperedge. Initially, all
0.6/1.0
0.4/0.67?
1.0/0.6
1.0/1.0
1.0/1.0
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
NP
the
?a:??JJ:NN?:NN??
a
PP
Figure 3: The binarized rules of Figure 1 arranged
in a shared hypergraph forest. Each hyperedge is
labeled with its weight before/after pushing.
nonfinal hyperedges have a probability of 1, and fi-
nal hyperedges have a probability equal to the that
of the original unbinarized rule. Each path through
the forest exactly identifies a binarization of a rule
in the original grammar, and hyperpaths overlap
where binarized rules are shared.
Weight pushing in this hypergraph is similar to
weight pushing in a transducer. We consider each
nonfinal node v in the graph and execute a local
operation that moves weight in some way from the
set of edges {e : v ? T (e)} (v?s outgoing hyper-
edges) to the edge eh for which v = h(e) (v?s
incoming hyperedge).
A critical difference from pushing in trans-
ducers is that a node in a hyperpath may be
used more than once. Consider adding the rule
NP?JJ NN JJ NN to the binarized two-rule gram-
mar we have been considering. Greedy binariza-
tion could2 binarize it in the following manner
NP ? ?JJ:NN? ?JJ:NN?
?JJ:NN? ? JJ NN
which would yield the hypergraph in Figure 4. In
order to maintain hyperpath weights, a pushing
procedure at the ?JJ:NN? node must pay attention
to the number of times it appears in the set of tail
nodes of each outgoing hyperedge.
2Depending on the order in which the argmax variable i
of Line 5 from the algorithm in Figure 2 is considered. This
particular binarization would not have been produced if the
values 2 . . . r were tested sequentially.
91
0.6/1.0
0.3/0.5
1.0/0.6
1.0/1.0
1.0/1.0
0.1/0.27?
??JJ:NN?:NN?
?JJ:NN? NN
JJ NN
NP
the
?a:??JJ:NN?:NN??
a
PP
Figure 4: A hypergraph containing a hyperpath
representing a rule using the same binarized piece
twice. Hyperedge weights are again shown be-
fore/after pushing.
With these similarities and differences in mind,
we can define the local weight pushing procedure.
For each nonfinal node v in the hypergraph, we
define eh as the edge for which h(e) = v (as be-
fore), P = {e : v ? T (e)} (the set of outgo-
ing hyperedges), and c(v, T (e)) as the number of
times v appears in the sequence of tail nodes T (e).
The minimum amount of probability available for
pushing is then
max{ c(v,T (e))?w(e) : e ? P} (2)
This amount can then be multiplied into w(eh) and
divided out of each edge e ? P . This max is a
lower bound because we have to ensure that the
amount of probability we divide out of the weight
of each outgoing hyperedge is at least as large as
that of the maximum weight.
While finite state transducers each have a
unique equivalent transducer on which no further
pushing is possible, defined by Equation 1, this is
not the case when operating on hypergraphs. In
this generalized setting, the choice of which tail
nodes to push weight across can result in differ-
ent final solutions. We must define a strategy for
choosing among sequences of pushing operations,
and for this we now turn to a discussion of the
specifics of our algorithm.
2.3 Algorithm
We present two variants. Maximal pushing, analo-
gous to weight pushing in weighted FSTs, pushes
the original rule?s weight down as far as pos-
sible. Analysis of interactions between pruning
1: function DIFFUSEWEIGHTS(PBIN ,?)
2: R := bottom-up sort of PBIN
3: for each rule r ? R do
4: r.pr := max{ c(r,p)?p.pr : p ? ?(r)}
5: for each rule p ? ?(r) do
6: p.pr := p.pr/r.prc(r,p)
Figure 6: Maximal weight pushing algorithm ap-
plied to a binarized grammar, PBIN . ? is a dictio-
nary mapping from an internal binary rule to a list
of top-level binary rules that it appeared under.
and maximal pushing discovered situations where
maximal pushing resulted in search error (see
?4.2). To address this, we also discuss nthroot
pushing, which attempts to distribute the weight
more evenly across its pieces, by taking advantage
of the fact that Equation 2 is a lower bound on the
amount of probability available for pushing.
The algorithm for maximal pushing is listed
in Figure 6, and works in the following manner.
When binarizing we maintain, for each binarized
piece, a list of all the original rules that share
it. We then distribute that original rule?s weight
by considering each of these binarized pieces in
bottom-up topological order and setting the prob-
ability of the piece to the maximum (remaining)
probability of these parents. This amount is then
divided out of each of the parents, and the process
continues. See Figure 5 for a depiction of this pro-
cess. Note that, although we defined pushing as a
local operation between adjacent hyperedges, it is
safe to move probability mass from the top-level
directly to the bottom (as we do here). Intuitively,
we can imagine this as a series of local pushing
operations on all intervening nodes; the end result
is the same.
For nthroot pushing, we need to maintain a dic-
tionary ? which records, for each binary piece, the
rank (number of items on the rule?s righthand side)
of the original rule it came from. This is accom-
plished by replacing line 4 in Figure 6 with
r.pr := max{ (?(p)?1)?c(r,p)?p.pr : p ? ?(r)}
Applying weight pushing to a binarized PCFG
results in a grammar that is not a PCFG, be-
cause rule probabilities for each lefthand side
no longer sum to one. However, the tree dis-
tribution, as well as the conditional distribution
P(tree|string) (which are what matter for parsing)
are unchanged. To show this, we argue from
the algorithm in Figure 6, demonstrating that, for
92
step A B C D E
0 1.0 1.0 1.0 x y
1 max(x, y) ? ? xmax(x,y) ymax(x,y)
2 ? max(z1,D, z1,E) ? z1,Dmax(z1,D,z1,E)
z1,D
max(z1,D,z1,E)
3 ? ? max(z2,D, z2,E) z2,Dmax(z2,D,z2,E)
z2,E
max(z2,D,z2,E)
4 ? ? ? ? ?
Figure 5: Stepping through the maximal weight pushing algorithm for the binarized grammar in Figure 1.
Rule labels A through E were chosen so that the binarized pieces are sorted in topological order. A (?)
indicates a rule whose value has not changed from the previous step, and the value zr,c denotes the value
in row r column c.
each rule in the original grammar, its probability
is equal to the product of the probabilities of its
pieces in the binarized grammar. This invariant
holds at the start of the algorithm (because the
probability of each original rule was placed en-
tirely at the top-level rule, and all other pieces re-
ceived a probability of 1.0) and is also true at the
end of each iteration of the outer loop. Consider
this loop. Each iteration considers a single binary
piece (line 3), determines the amount of probabil-
ity to claim from the parents that share it (line 4),
and then removes this amount of weight from each
of its parents (lines 5 and 6). There are two impor-
tant considerations.
1. A binarized rule piece may be used more than
once in the reconstruction of an original rule;
this is important because we are assigning
probabilities to binarized rule types, but rule
reconstruction makes use of binarized rule to-
kens.
2. Multiplying together two probabilities results
in a lower number: when we shift weight p
from the parent rule to (n instances of) a bi-
narized piece beneath it, we are creating a
new set of probabilities pc and pp such that
pnc ? pp = p, where pc is the weight placed on
the binarized rule type, and pp is the weight
we leave at the parent. This means that we
must choose pc from the range [p, 1.0].3
In light of these considerations, the weight re-
moved from each parent rule in line 6 must be
greater than or equal to each parent sharing the
binarized rule piece. To ensure this, line 4 takes
3The upper bound of 1.0 is set to avoid assigning a nega-
tive weight to a rule.
the maximum of the c(r, p)th root of each parent?s
probability, where c(r, p) is the number of times
binarized rule token r appears in the binarization
of p.
Line 4 breaks the invariant, but line 6 restores it
for each parent rule the current piece takes part in.
From this it can be seen that weight pushing does
not change the product of the probabilities of the
binarized pieces for each rule in the grammar, and
hence the tree distribution is also unchanged.
We note that, although Figures 3 and 4 show
only one final node, any number of final nodes can
appear if binarized pieces are shared across differ-
ent top-level nonterminals (which our implemen-
tation permits and which does indeed occur).
3 Experimental setup
We present results from four different grammars:
1. The standard Treebank probabilistic context-
free grammar (PCFG).
2. A ?spinal? tree substitution grammar (TSG),
produced by extracting n lexicalized subtrees
from each length n sentence in the training
data. Each subtree is defined as the sequence
of CFG rules from leaf upward all sharing the
same lexical head, according to the Mager-
man head-selection rules (Collins, 1999). We
detach the top-level unary rule, and add in
counts from the Treebank CFG rules.
3. A ?minimal subset? TSG, extracted and then
refined according to the process defined in
Bod (2001). For each height h, 2 ? h ? 14,
400,000 subtrees are randomly sampled from
the trees in the training data, and the counts
93
rank
grammar # rules median mean max
PCFG 46K 1 2.14 51
spinal 190K 3 3.36 51
sampled 804K 8 8.51 70
minimal 2,566K 10 10.22 62
Table 1: Grammar statistics. A rule?s rank is the
number of symbols on its right-hand side.
grammar unbinarized right greedy
PCFG 46K 56K 51K
spinal 190K 309K 235K
sampled 804K 3,296K 1,894K
minimal 2,566K 15,282K 7,981K
Table 2: Number of rules in each of the complete
grammars before and after binarization.
are summed. From these counts we remove
(a) all unlexicalized subtrees of height greater
than six and (b) all lexicalized subtrees con-
taining more than twelve terminals on their
frontier, and we add all subtrees of height one
(i.e., the Treebank PCFG).
4. A sampled TSG produced by inducing
derivations on the training data using a
Dirichlet Process prior (described below).
The sampled TSG was produced by inducing a
TSG derivation on each of the trees in the train-
ing data, from which subtree counts were read di-
rectly. These derivations were induced using a
collapsed Gibbs sampler, which sampled from the
posterior of a Dirichlet process (DP) defined over
the subtree rewrites of each nonterminal. The DP
describes a generative process that prefers small
subtrees but occasionally produces larger ones;
when used for inference, it essentially discovers
TSG derivations that contain larger subtrees only
if they are frequent in the training data, which dis-
courages model overfitting. See Post and Gildea
(2009) for more detail. We ran the sampler for 100
iterations with a stop probability of 0.7 and the DP
parameter ? = 100, accumulating subtree counts
from the derivation state at the end of all the itera-
tions, which corresponds to the (100, 0.7,? 100)
grammar from that paper.
All four grammar were learned from all sen-
tences in sections 2 to 21 of the Wall Street Journal
portion of the Penn Treebank. All trees were pre-
processed to remove empty nodes and nontermi-
NP
NP
DT
a
JJ NN NN
PP
Figure 7: Rule 1 in Figure 1 was produced by
flattening this rule from the sampled grammar.
nal annotations. Punctuation was retained. Statis-
tics for these grammars can be found in Table 1.
We present results on sentences with no more than
forty words from section 23.
Our parser is a Perl implementation of the CKY
algorithm.4 For the larger grammars, memory lim-
itations require us to remove from consideration
all grammar rules that could not possibly take part
in a parse of the current sentence, which we do by
matching the rule?s frontier lexicalization pattern
against the words in the sentence. All unlexical-
ized rules are kept. This preprocessing time is not
included in the parsing times reported in the next
section.
For pruning, we group edges into equivalence
classes according to the following features:
? span (s, t) of the input
? level of binarization (0,1,2+)
The level of binarization refers to the height of a
nonterminal in the subtree created by binarizing a
CFG rule (with the exception that the root of this
tree has a binarization level of 0). The naming
scheme used to create new nonterminals in line 6
of Figure 2 means we can determine this level by
counting the number of left-angle brackets in the
nonterminal?s name. In Figure 1, binarized rules
D and E have level 0, C has level 3, B has level 2,
and A has level 1.
Within each bin, only the ? highest-weight
items are kept, where ? ? (1, 5, 10, 25, 50) is a pa-
rameter that we vary during our experiments. Ties
are broken arbitrarily. Additionally, we maintain a
beam within each bin, and an edge is pruned if its
score is not within a factor of 10?5 of the highest-
scoring edge in the bin. Pruning takes place when
the edge is added and then again at the end of each
4It is available from http://www.cs.rochester.
edu/
?
post/.
94
span in the CKY algorithm (but before applying
unary rules).
In order to binarize TSG subtrees, we follow
Bod (2001) in first flattening each subtree to a
depth-one PCFG rule that shares the subtree?s root
nonterminal and leaves, as depicted in Figure 7.
Afterward, this transformation is reversed to pro-
duce the parse tree for scoring. If multiple TSG
subtrees have identical mappings, we take only the
most probable one. Table 2 shows how grammar
size is affected by binarization scheme.
We note two differences in our work that ex-
plain the large difference between the scores re-
ported for the ?minimal subset? grammar in Bod
(2001) and here. First, we did not implement the
smoothed ?mismatch parsing?, which introduces
new subtrees into the grammar at parsing time by
allowing lexical leaves of subtrees to act as wild-
cards. This technique reportedly makes a large
difference in parsing scores (Bod, 2009). Second,
we approximate the most probable parse with the
single most probable derivation instead of the top
1,000 derivations, which Bod also reports as hav-
ing a large impact (Bod, 2003, ?4.2).
4 Results
Figure 8 displays search time vs. model score for
the PCFG and the sampled grammar. Weight
pushing has a significant impact on search effi-
ciency, particularly for the larger sampled gram-
mar. The spinal and minimal graphs are similar to
the PCFG and sampled graphs, respectively, which
suggests that the technique is more effective for
the larger grammars.
For parsing, we are ultimately interested in ac-
curacy as measured by F1 score.5 Figure 9 dis-
plays graphs of time vs. accuracy for parses with
each of the grammars, alongside the numerical
scores used to generate them. We begin by noting
that the improved search efficiency from Figure 8
carries over to the time vs. accuracy curves for
the PCFG and sampled grammars, as we expect.
Once again, we note that the difference is less pro-
nounced for the two smaller grammars than for the
two larger ones.
4.1 Model score vs. accuracy
The tables in Figure 9 show that parser accuracy
is not always a monotonic function of time; some
of the runs exhibited peak performance as early
5F1 = 2?P ?RP+R , where P is precision and R recall.
-340
-338
-336
-334
-332
-330
-328
-326
-324
-322
-320
1 5 10 25 50
m
o
de
l s
co
re
 (th
ou
sa
nd
s)
(greedy,max)
(greedy,nthroot)
(greedy,none)
(right,max)
(right,nthroot)
(right,none)
-370
-360
-350
-340
-330
-320
-310
-300
-290
1 5 10 25 50
m
o
de
l s
co
re
 (th
ou
sa
nd
s)
mean time per sentence (s)
(greedy,max)
(greedy,nthroot)
(greedy,none)
(right,max)
(right,nthroot)
(right,none)
Figure 8: Time vs. model score for the PCFG (top)
and the sampled grammar (bottom). Note that the
y-axis range differs between plots.
as at a bin size of ? = 10, and then saw drops
in scores when given more time. We examined
a number of instances where the F1 score for a
sentence was lower at a higher bin setting, and
found that they can be explained as modeling (as
opposed to search) errors. With the PCFG, these
errors were standard parser difficulties, such as PP
attachment, which require more context to resolve.
TSG subtrees, which have more context, are able
to correct some of these issues, but introduce a dif-
ferent set of problems. In many situations, larger
bin settings permitted erroneous analyses to re-
main in the chart, which later led to the parser?s
discovery of a large TSG fragment. Because these
fragments often explain a significant portion of the
sentence more cheaply than multiple smaller rules
multiplied together, the parser prefers them. More
often than not, they are useful, but sometimes they
are overfit to the training data, and result in an in-
correct analysis despite a higher model score.
Interestingly, these dips occur most frequently
for the heuristically extracted TSGs (four of six
95
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
PCFG
run 1 5 10 25 50
 (g,m) 66.44 72.45 72.54 72.54 72.51u (g,n) 65.44 72.21 72.47 72.45 72.47
N (g,-) 63.91 71.91 72.48 72.51 72.51
 (r,m) 67.30 72.45 72.61 72.47 72.49e (r,n) 64.09 71.78 72.33 72.45 72.47
? (r,-) 61.82 71.00 72.18 72.42 72.41
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
spinal
run 1 5 10 25 50
 (g,m) 68.33 78.35 79.21 79.25 79.24u (g,n) 64.67 78.46 79.04 79.07 79.09
N (g,-) 61.44 77.73 78.94 79.11 79.20
 (r,m) 69.92 79.07 79.18 79.25 79.05e (r,n) 67.76 78.46 79.07 79.04 79.04
? (r,-) 65.27 77.34 78.64 78.94 78.90
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
sampled
run 1 5 10 25 50
 (g,m) 63.75 80.65 81.86 82.40 82.41u (g,n) 61.87 79.88 81.35 82.10 82.17
N (g,-) 53.88 78.68 80.48 81.72 81.98
 (r,m) 72.98 81.66 82.37 82.49 82.40e (r,n) 65.53 79.01 80.81 81.91 82.13
? (r,-) 61.82 77.33 79.72 81.13 81.70
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,none)
(right,max)
(right,none)
minimal
run 1 5 10 25 50
 (g,m) 59.75 77.28 77.77 78.47 78.52u (g,n) 57.54 77.12 77.82 78.35 78.36
N (g,-) 51.00 75.52 77.21 78.30 78.13
 (r,m) 65.29 76.14 77.33 78.34 78.13e (r,n) 61.63 75.08 76.80 77.97 78.31
? (r,-) 59.10 73.42 76.34 77.88 77.91
Figure 9: Plots of parsing time vs. accuracy for each of the grammars. Each plot contains four sets of five
points (? ? (1, 5, 10, 25, 50)), varying the binarization strategy (right (r) or greedy (g)) and the weight
pushing technique (maximal (m) or none (-)). The tables also include data from nthroot (n) pushing.
96
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
(right,max)
(right,nthroot)
(right,none)
 50
 55
 60
 65
 70
 75
 80
 85
1 5 10 25 50
a
cc
u
ra
cy
mean time per sentence (s)
(greedy,max)
(greedy,nthroot)
(greedy,none)
Figure 10: Time vs. accuracy (F1) for the sampled
grammar, broken down by binarization (right on
top, greedy on bottom).
runs for the spinal grammar, and two for the min-
imal grammar) and for the PCFG (four), and least
often for the model-based sampled grammar (just
once). This may suggest that rules selected by our
sampling procedure are less prone to overfitting on
the training data.
4.2 Pushing
Figure 10 compares the nthroot and maximal
pushing techniques for both binarizations of the
sampled grammar. We can see from this figure
that there is little difference between the two tech-
niques for the greedy binarization and a large dif-
ference for the right binarization. Our original mo-
tivation in developing nthroot pushing came as a
result of analysis of certain sentences where max-
imal pushing and greedy binarization resulted in
the parser producing a lower model score than
with right binarization with no pushing. One such
example was binarized fragment A from Fig-
ure 1; when parsing a particular sentence in the
development set, the correct analysis required the
rule from Figure 7, but greedy binarization and
maximal pushing resulted in this piece getting
pruned early in the search procedure. This pruning
happened because maximal pushing allowed too
much weight to shift down for binarized pieces of
competing analyses relative to the correct analy-
sis. Using nthroot pushing solved the search prob-
lem in that instance, but in the aggregate it does
not appear to be helpful in improving parser effi-
ciency as much as maximal pushing. This demon-
strates some of the subtle interactions between bi-
narization and weight pushing when inexact prun-
ing heuristics are applied.
4.3 Binarization
Song et al (2008, Table 4) showed that CKY pars-
ing efficiency is not a monotonic function of the
number of constituents produced; that is, enumer-
ating fewer edges in the dynamic programming
chart does not always correspond with shorter run
times. We see here that efficiency does not al-
ways perfectly correlate with grammar size, ei-
ther. For all but the PCFG, right binarization
improves upon greedy binarization, regardless of
the pushing technique, despite the fact that the
right-binarized grammars are always larger than
the greedily-binarized ones.
Weight pushing and greedy binarization both in-
crease parsing efficiency, and the graphs in Fig-
ures 8 and 9 suggest that they are somewhat com-
plementary. We also investigated left binarization,
but discontinued that exploration because the re-
sults were nearly identical to that of right bina-
rization. Another popular binarization approach
is head-outward binarization. Based on the anal-
ysis above, we suspect that its performance will
fall somewhere among the binarizations presented
here, and that pushing will improve it as well. We
hope to investigate this in future work.
5 Summary
Weight pushing increases parser efficiency, espe-
cially for large grammars. Most notably, it im-
proves parser efficiency for the Gibbs-sampled
tree substitution grammar of Post and Gildea
(2009).
We believe this approach could alo bene-
fit syntax-based machine translation. Zhang et
al. (2006) introduced a synchronous binariza-
tion technique that improved decoding efficiency
and accuracy by ensuring that rule binarization
avoided gaps on both the source and target sides
97
(for rules where this was possible). Their binariza-
tion was designed to share binarized pieces among
rules, but their approach to distributing weight was
the default (nondiffused) case found in this paper
to be least efficient: The entire weight of the orig-
inal rule is placed at the top binarized rule and all
internal rules are assigned a probability of 1.0.
Finally, we note that the weight pushing algo-
rithm described in this paper began with a PCFG
and ensured that the tree distribution was not
changed. However, weight pushing need not be
limited to a probabilistic interpretation, but could
be used to spread weights for grammars with dis-
criminatively trained features as well, with neces-
sary adjustments to deal with positively and nega-
tively weighted rules.
Acknowledgments We thank the anonymous
reviewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and
ITR-0428020.
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Pro-
ceedings of the 39th Annual Conference of the As-
sociation for Computational Linguistics (ACL-01),
Toulouse, France.
Rens Bod. 2003. Do all fragments count? Natural
Language Engineering, 9(4):307?323.
Rens Bod. 2009. Personal communication.
Eugene Charniak. 2000. A maximum-entropy-
inspired parser. In Proceedings of the 2000 Meet-
ing of the North American chapter of the Association
for Computational Linguistics (NAACL-00), Seattle,
Washington.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proceedings of the 38th Annual Conference of the
Association for Computational Linguistics (ACL-
00), Hong Kong.
Michael John Collins. 1999. Head-driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
John DeNero, Mohit Bansal, Adam Pauls, and Dan
Klein. 2009. Efficient parsing for transducer gram-
mars. In Proceedings of the 2009 Meeting of the
North American chapter of the Association for Com-
putational Linguistics (NAACL-09), Boulder, Col-
orado.
Liang Huang. 2007. Binarization, synchronous bi-
narization, and target-side binarization. In North
American chapter of the Association for Computa-
tional Linguistics Workshop on Syntax and Struc-
ture in Statistical Translation (NAACL-SSST-07),
Rochester, NY.
Dan Klein and Christopher D. Manning. 2003. A*
parsing: Fast exact Viterbi parse selection. In Pro-
ceedings of the 2003 Meeting of the North American
chapter of the Association for Computational Lin-
guistics (NAACL-03), Edmonton, Alberta.
Mehryar Mohri and Michael Riley. 2001. A weight
pushing algorithm for large vocabulary speech
recognition. In European Conference on Speech
Communication and Technology, pages 1603?1606.
Mehryar Mohri. 1997. Finite-state transducers in lan-
guage and speech processing. Computational Lin-
guistics, 23(2):269?311.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proceedings of the
47th Annual Meeting of the Association for Compu-
tational Linguistics (ACL-09), Suntec, Singapore.
Helmut Schmid. 2004. Efficient parsing of highly am-
biguous context-free grammars with bit vectors. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING-04), Geneva,
Switzerland.
Xinying Song, Shilin Ding, and Chin-Yew Lin. 2008.
Better binarization for the CKY parsing. In 2008
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP-08), Honolulu, Hawaii.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of the 2006 Meet-
ing of the North American chapter of the Associ-
ation for Computational Linguistics (NAACL-06),
New York, NY.
98
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 327?337,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Stylometric Analysis of Scientific Articles
Shane Bergsma, Matt Post, David Yarowsky
Department of Computer Science and Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21218, USA
sbergsma@jhu.edu, post@cs.jhu.edu, yarowsky@cs.jhu.edu
Abstract
We present an approach to automatically re-
cover hidden attributes of scientific articles,
such as whether the author is a native English
speaker, whether the author is a male or a fe-
male, and whether the paper was published in
a conference or workshop proceedings. We
train classifiers to predict these attributes in
computational linguistics papers. The classi-
fiers perform well in this challenging domain,
identifying non-native writing with 95% accu-
racy (over a baseline of 67%). We show the
benefits of using syntactic features in stylom-
etry; syntax leads to significant improvements
over bag-of-words models on all three tasks,
achieving 10% to 25% relative error reduction.
We give a detailed analysis of which words
and syntax most predict a particular attribute,
and we show a strong correlation between our
predictions and a paper?s number of citations.
1 Introduction
Stylometry aims to recover useful attributes of doc-
uments from the style of the writing. In some do-
mains, statistical techniques have successfully de-
duced author identity (Mosteller and Wallace, 1984),
gender (Koppel et al, 2003), native language (Kop-
pel et al, 2005), and even whether an author has de-
mentia (Le et al, 2011). Stylometric analysis is im-
portant to marketers, analysts and social scientists
because it provides demographic data directly from
raw text. There has been growing interest in apply-
ing stylometry to the content generated by users of
Internet applications, e.g., detecting author ethnic-
ity in social media (Eisenstein et al, 2011; Rao et
Native/Non-native
Male/Female
Conference/Workshop
Stylometric Analysis of Scientific Articles
Abstract
We present an approach to automatically re-
cover hidden attributes of scientific articles,
such as whether the author is a native En-
glish speaker. We train classifiers to predict
these attributes in computational linguistics
papers. The classifiers perform well in this
challenging domain, identifying non-native
writing with 95% accuracy (over a baseline
of 67%), and outperforming reasonable base-
lines on two other difficult tasks. We show the
benefits of using syntactic features in stylom-
etry; syntax leads to significant improvements
over bag-of-words models on all three tasks,
achieving 10% to 25% relative error reduction.
We give an insightful analysis of which words
and syntax most predict a particular attribute,
and we show a strong correlation between our
predictions and a paper?s number of citations.
1 Introduction
Stylometry aims to recover useful attributes of doc-
uments from the style of their writing. In some
domains, statistical techniques have successfully
deduced author identities (Mosteller and Wallace,
1984), gender (Koppel et al, 2003), native language
(Koppel et al, 2005), and even whether an author
has dementia (Le et al, 2011). Stylometric analysis
is important to marketers, analysts and social scien-
tists because it provides demographic data directly
from raw text. There has been growing interest in
applying stylometry in Web 2.0 applications, e.g.,
detecting the ethnicity of Twitter users (Eisenstein
et al, 2011; Rao et al, 2011), or whether a person is
writing deceptive online reviews (Ott et al, 2011).
We evaluate stylometric techniques in the novel
domain of scientific writing. Science is a diffi-
cult domain; authors are compelled, often explic-
itly by reviewers/submission guidelines, to comply
!"#$%&'!(!)!"#$%&
*"+&',&*"+&
*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.
+1."(-*"
!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1
*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7
%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1
:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*'
'+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%
#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%
*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"
8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&"
)5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1
/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"
4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1
"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%
)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7
(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<
!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%
(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7
(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$
#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<
2 3,"(%45*")%,
K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1
,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"
3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//0
3"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7
G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"
AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$
+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%
.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1
'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/0
5$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&
(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<7
3"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&
"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%
8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<
!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/
3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561
*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1
.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
!"#$%&'!(!)!"#$%&
*"+&',&*"+&
*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.
+1."(-*"!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*''+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<73"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
!"#$%&'!(!)!"#$%&*"+&',&*"+&*"$!'-(./01(2
!"#$%&'"()* +,-$#.). %/ !*)',")0* +(")*$'.+1."(-*"!" #$"%"&' (& (##$)(*+ ') (,')-('.*(//0 $"1*)2"$ +.33"& (''$.4,'"% )5 %*."&'.6* ($'.*/"%7%,*+ (% 8+"'+"$ '+" (,'+)$ .% ( &('.2" 9&1:/.%+ %#"(;"$< !" '$(.& */(%%.6"$% ') #$"3.*''+"%" (''$.4,'"% .& *)-#,'('.)&(/ /.&:,.%'.*%#(#"$%< =+" */(%%.6"$% #"$5)$- 8"// .& '+.%*+(//"&:.&: 3)-(.&7 .3"&'.50.&: &)&1&('.2"8$.'.&: 8.'+ >?@ (**,$(*0 A)2"$ ( 4(%"/.&")5 BC@D7 (&3 ),'#"$5)$-.&: $"(%)&(4/" 4(%"1/.&"% )& '8) )'+"$ 3.56*,/' '(%;%< !" %+)8 '+"4"&"6'% )5 ,%.&: !"#$%&$'& ()%$*+)! .& %'0/)-1"'$0E %0&'(F /"(3% ') %.:&.6*(&' .-#$)2"-"&'%)2"$ 4(:1)518)$3% -)3"/% )& (// '+$"" '(%;%7(*+."2.&: GH@ ') I?@ $"/('.2" "$$)$ $"3,*'.)&<!" :.2" (& .&%.:+'5,/ (&(/0%.% )5 8+.*+ 8)$3%(&3 %0&'(F -)%' #$"3.*' ( #($'.*,/($ (''$.4,'"7(&3 8" %+)8 ( %'$)&: *)$$"/('.)& 4"'8""& ),$#$"3.*'.)&% (&3 ( #(#"$J% &,-4"$ )5 *.'('.)&%<2 3,"(%45*")%,K'0/)-"'$0 (.-% ') $"*)2"$ ,%"5,/ (''$.4,'"% )5 3)*1,-"&'% 5$)- '+" %'0/" )5 '+".$ 8$.'.&:< L& %)-"3)-(.&%7 %'('.%'.*(/ '"*+&.M,"% +(2" %,**"%%5,//03"3,*"3 (,'+)$ .3"&'.'."% AN)%'"//"$ (&3 !(//(*"7G>OPD7 :"&3"$ AQ)##"/ "' (/<7 IHHRD7 &('.2" /(&:,(:"AQ)##"/ "' (/<7 IHH?D7 (&3 "2"& 8+"'+"$ (& (,'+)$+(% 3"-"&'.( AS" "' (/<7 IHGGD< K'0/)-"'$.* (&(/0%.%.% .-#)$'(&' ') -($;"'"$%7 (&(/0%'% (&3 %)*.(/ %*."&1'.%'% 4"*(,%" .' #$)2.3"% 3"-):$(#+.* 3('( 3.$"*'/05$)- $(8 '"F'< =+"$" +(% 4""& :$)8.&: .&'"$"%' .&(##/0.&: %'0/)-"'$0 .& !"4 I<H (##/.*('.)&%7 "<:<73"'"*'.&: '+" "'+&.*.'0 )5 =8.''"$ ,%"$% A9.%"&%'".&"' (/<7 IHGGE T() "' (/<7 IHGGD7 )$ 8+"'+"$ ( #"$%)& .%8$.'.&: 3"*"#'.2" )&/.&" $"2."8% AU'' "' (/<7 IHGGD<!" "2(/,('" %'0/)-"'$.* '"*+&.M,"% .& '+" &)2"/3)-(.& )5 !&')#$',& -+'$'#.< K*."&*" .% ( 3.561*,/' 3)-(.&E (,'+)$% ($" *)-#"//"37 )5'"& "F#/.*1.'/0 40 $"2."8"$%V%,4-.%%.)& :,.3"/.&"%7 ') *)-#/0
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`$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.:!" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ (8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.:!" %+)8 '+" 2(/,")5!"#$%&$'& ()%$*+)!5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4"$+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$08),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.* *)--,&.'."%7 (&3 &"88(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,")5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$08),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(13"-.* *)--,&.'."%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,")5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%78" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%78+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'
W.:,$" GX Y$"3.*'.&: +.33"& (''$.4,'"% .& %*."&'.6* ($'.*/"%
8.'+ &)$-('.2" #$(*'.*"% .& %#"//.&: (&3 :$(--($<
N)$")2"$7 ')#.*(/ */,"% ($" /"%% %(/."&' '+(& .& 3)1
-(.&% /.;" %)*.(/ -"3.(< Z"' %*."&*" .% -)$" '+(&
[,%' ( :))3 *+(//"&:" 5)$ %'0/)-"'$0E .' .% (& .-#)$1
'(&' ($"( .& .'%"/5< K0%'"-% 5)$ %*."&'.6* %'0/)-"'$0
8),/3 :.2" %)*.)/):.%'% &"8 '))/% 5)$ (&(/0\.&: (*(1
3"-.* *)--,&.'."%7 (&3 &"8 8(0% ') $"%)/2" '+" &(1
',$" )5 *)//(4)$('.)& .& %#"*.6* ($'.*/"% A])+$. "' (/<7
IHGGD< ^,'+)$% -.:+' (/%) ,%" '+"%" '))/%7 "<:< ')
+"/# "&%,$" ( *)&%.%'"&' %'0/" .& -,/'.1(,'+)$"3 #(1
#"$% A_/)2"$ (&3 `.$%'7 G>>?D< U,$ 8)$; .&*/,3"%X
6'7 !"#$%&'"()* 8-.9.: !" #$"3.*' 8+"'+"$ (
#(#"$ .% 8$.''"&X AGD 40 ( &('.2" )$ &)&1&('.2" 9&1
:/.%+ %#"(;"$7 AID 40 ( -(/" )$ 5"-(/"7 (&3 ARD .& '+"
%'0/" )5 ( *)&5"$"&*" )$ ( 8)$;%+)# #(#"$< =+" /(''"$
.% ( &)2"/ %'0/)-"'$.* (&3 4.4/.)-"'$.* #$"3.*'.)&<
6'7 !"#$%&'"()* ;'-"5('.: !" %+)8 '+" 2(/,"
)5 !"#$%&$'& ()%$*+)! 5)$ %'0/)-"'$0< ^-)&: )'+"$%7
8" 3"%*$.4" $+)) !*/!$'$*$'0# .+%11%+ 5$(:-"&'%7
8+.*+ +(2" &)' #$"2.),%/0 4""& ,%"3 .& %'0/)-"'$0<
=K_ 5$(:-"&'% ($" .&'"$#$"'(4/"7 "56*."&'7 (&3 #($1
'.*,/($/0 "55"*'.2" 5)$ 3"'"*'.&: &)&1&('.2" 8$.'.&:<
!+./" $"*"&' %',3."% +(2" -)%'/0 "2(/,('"3 %.&1
:/" #$"3.*'.)& '(%;%7 8" *)-#($" 3.55"$"&' %'$('":."%
(*$)%% 3.55"$"&' '(%;% )& ( *)--)& 3('(%"' (&3 8.'+
( *)--)& .&5$(%'$,*',$"< L& (33.'.)& ') *)&'$(%'.&:
3.55"$"&' 5"(',$" '0#"%7 8" (/%) *)-#($" 3.55"$"&'
Figure 1: Predicting hidden attributes in scientific articles
with normative practices in spelling and grammar.
Moreover, topical clues are less salient than in do-
mains like social media. Yet science is more than
just a good challenge for stylometry; it is an impor-
tant area in itself. Systems for scientific stylometry
would give sociologists new tools for analyzing aca-
demic communities, and new ways to resolve the na-
ture of collaboration in specific articles (Johri et al,
2011). Authors might also use these tools, e.g. to
help ensure a consistent style in multi-authored pa-
pers (Glover and Hirst, 1995). Our work includes:
New Stylometric Tasks: We predict whether a
paper is written: (1) by a native or non-native En-
glish speaker, (2) by a male or female, and (3) in the
style of a conference or a workshop paper. The latter
is a novel stylometric and bibliometric prediction.
New Stylometric Features: We show the value
of syntactic features for stylometry. Among others,
we describe tree substitution grammar fragments,
which have not previously been used in stylometry.
TSG fragments are interpretable, efficient, and par-
ticularly effective for detecting non-native writing.
While recent studies have mostly evaluated sin-
gle prediction tasks, we compare different strategies
across different tasks on a common dataset and with
a common infrastructure. In addition to contrasting
different feature types, we also compare different
Figure 1: Predicting hidden attributes in scientific articles
al., 2011), or whether someone is writing deceptive
online reviews (Ott et al, 2011).
We evaluate stylometric techniques in the novel
domain of scientific writing. Science is a difficult
domain; authors are encouraged, often explicitly
by reviewers/submission-guidelines, to comply with
normative practices in style, spelling and grammar.
Moreover, topical clues are less salient than in do-
mains like social media. Success in this challenging
domain can bring us closer to correctly analyzing
the huge volumes of online text that are currently
unmarked for useful author attributes such as gender
and native-language.
Yet science is more than just a good stepping-
stone for stylometry; it is an important area in itself.
Systems for scientific stylometry would give sociol-
ogists new tools for analyzing academic communi-
ties, and new ways to resolve the nature of collab-
oration in specific articles (Johri et al, 2011). Au-
thors might also use these tools, e.g., to help ensure
a consistent style in multi-authored papers (Glover
and Hirst, 1995), or to determine sections of a paper
needing revision.
327
The contributions of our paper include:
New Stylometric Tasks: We predict whether
a paper is written: (1) by a native or non-native
speaker, (2) by a male or female, and (3) in the style
of a conference or workshop paper. The latter is a
fully novel stylometric and bibliometric prediction.
New Stylometric Features: We show the value
of syntactic features for stylometry. Among others,
we describe tree substitution grammar fragments,
which have not previously been used in stylometry.
TSG fragments are interpretable, efficient, and par-
ticularly effective for detecting non-native writing.
While recent studies have mostly evaluated sin-
gle prediction tasks, we compare different strategies
across different tasks on a common dataset and with
a common infrastructure. In addition to contrasting
different feature types, we compare different train-
ing strategies, exploring ways to make use of train-
ing instances with label uncertainty.
We also provide a detailed analysis that is inter-
esting from a sociolinguistic standpoint. Precisely
what words distinguish non-native writing? How
does the syntax of female authors differ from males?
What are the hallmarks of top-tier papers? Finally,
we identify some strong correlations between our
predictions and a paper?s citation count, even when
controlling for paper venue and origin.
2 Related Work
Bibliometrics is the empirical analysis of scholarly
literature; citation analysis is a well-known bib-
liometric approach for ranking authors and papers
(Borgman and Furner, 2001). Bibliometry and sty-
lometry can share goals but differ in techniques.
For example, in a work questioning the blindness
of double-blind reviewing, Hill and Provost (2003)
predict author identities. They ignore the article
body and instead consider (a) potential self-citations
and (b) similarity between the article?s citation list
and the citation lists of known papers. Radev et al
(2009a) perform a bibliometric analysis of compu-
tational linguistics. Teufel and Moens (2002) and
Qazvinian and Radev (2008) summarize scientific
articles, the latter by automatically finding and fil-
tering sentences in other papers that cite the target
article.
Our system does not consider citations; it is most
similar to work that uses raw article text. Hall et
al. (2008) build per-year topic models over scientific
literature to track the evolution of scientific ideas.
Gerrish and Blei (2010) assess the influence of indi-
vidual articles by modeling their impact on the con-
tent of future papers. Yogatama et al (2011) pre-
dict whether a paper will be cited based on both its
content and its meta-data such as author names and
publication venues. Johri et al (2011) use per-author
topic models to assess the nature of collaboration in
a particular article (e.g., apprenticeship or synergy).
One of the tasks in Sarawgi et al (2011) concerned
predicting gender in scientific writing, but they use a
corpus of only ten ?highly established? authors and
make the prediction using twenty papers for each.
Finally, Dale and Kilgarriff (2010) initiated a shared
task on automatic editing of scientific papers written
by non-native speakers, with the objective of devel-
oping ?tools which can help non-native speakers of
English (NNSs) (and maybe some native ones) write
academic English prose of the kind that helps a pa-
per get accepted.?
Lexical and pragmatic choices in academic writ-
ing have also been analyzed within the applied lin-
guistics community (Myers, 1989; Vassileva, 1998).
3 ACL Dataset and Preprocessing
We use papers from the ACL Anthology Network
(Radev et al, 2009b, Release 2011) and exploit its
manually-curated meta-data such as normalized au-
thor names, affiliations (including country, avail-
able up to 2009), and citation counts. We con-
vert each PDF to text1 but remove text before the
Abstract (to anonymize) and after the Acknowledg-
ments/References headings. We split the text into
sentences2 and filter any documents with fewer than
100 (this removes some short/demo papers, mal-
converted PDFs, etc. ? about 23% of the 13K pa-
pers with affiliation information). In case the text
was garbled, we then filtered the first 3 lines from
every file and any line with an ?@? symbol (which
might be part of an affiliation). We remove foot-
ers like Proceedings of ..., table/figure captions, and
any lines with non-ASCII characters (e.g. math
equations). Papers are then parsed via the Berke-
1Via the open-source utility pdftotext
2Splitter from cogcomp.cs.illinois.edu/page/tools
328
Task Training Set: Dev Test
Strict Lenient Set Set
NativeL 2127 3963 450 477
Venue 2484 3991 400 421
Gender 2125 3497 400 409
Table 1: Number of documents for each task
ley parser (Petrov et al, 2006), and part-of-speech
(PoS) tagged using CRFTagger (Phan, 2006).
Training sets always comprise papers from 2001-
2007, while test sets are created by randomly shuf-
fling the 2008-2009 portion and then dividing it into
development/test sets. We also use papers from
1990-2000 for experiments in ?7.3 and ?7.4.
4 Stylometric Tasks
Each task has both a Strict training set, using only
the data for which we are most confident in the la-
bels (as described below), and a Lenient set, which
forcibly assigns every paper in the training period
to some class (Table 1). All test papers are anno-
tated using a Strict rule. While our approaches for
automatically-assigning labels can be coarse, they
allow us to scale our analysis to a realistic cross-
section of academic papers, letting us discover some
interesting trends.
4.1 NativeL: Native vs. Non-Native English
We introduce the task of predicting whether a sci-
entific paper is written by a native English speaker
(NES) or non-native speaker (NNS). Prior work has
mostly made this prediction in learner corpora (Kop-
pel et al, 2005; Tsur and Rappoport, 2007; Wong
and Dras, 2011), although there have been attempts
in elicited speech transcripts (Tomokiyo and Jones,
2001) and e-mail (Estival et al, 2007). There has
also been a large body of work on correcting er-
rors in non-native writing, with a specific focus on
difficulties in preposition and article usage (Han et
al., 2006; Chodorow et al, 2007; Felice and Pul-
man, 2007; Tetreault and Chodorow, 2008; Gamon,
2010).
We annotate papers using two pieces of associated
meta-data: (1) author first names and (2) countries
of affiliation. We manually marked each country for
whether English is predominantly spoken there. We
then built a list of common first names of English
speakers via the top 150 male and female names
from the U.S. census.3 If the first author of a pa-
per has an English first name and English-speaking-
country affiliation, we mark as NES.4 If none of the
authors have an English first name nor an English-
speaking-country affiliation, we mark as NNS. We
use this rule to label our development and test data,
as well as our Strict training set. For Lenient train-
ing, we decide based solely on whether the first au-
thor is from an English-speaking country.
4.2 Venue: Top-Tier vs. Workshop
This novel task aims to distinguish top-tier papers
from those at workshops, based on style. We use
the annual meeting of the ACL as our canonical top-
tier venue. For evaluation and Strict training, we la-
bel all main-session ACL papers as top-tier, and all
workshop papers as workshop. For Lenient training,
we assign all conferences (LREC, Coling, EMNLP,
etc.) to be top-tier except for their non-main-session
papers, which we label as workshop.
4.3 Gender: Male vs. Female
Because we are classifying an international set of
authors, U.S. census names (the usual source of
gender ground-truth) provide incomplete informa-
tion. We therefore use the data of Bergsma and Lin
(2006).5 This data has been widely used in corefer-
ence resolution but never in stylometry. Each line
in the data lists how often a noun co-occurs with
male, female, neutral and plural pronouns; this is
commonly taken as an approximation of the true
gender distribution. E.g., ?bill clinton? is 98% male
(in 8344 instances) while ?elsie wayne? is 100% fe-
male (in 23). The data also has aggregate counts
over all nouns with the same first token, e.g., ?elsie
...? is 94% female (in 255 instances). For Strict
training/evaluation, we label papers with the fol-
lowing rule based on the first author?s first name:
3
www.census.gov/genealogy/names/names_files.
html We also manually added common nicknames for these,
e.g. Rob for Robert, Chris for Christopher, Dan for Daniel, etc.
4Of course, assuming the first author writes each paper is
imperfect. In fact, for some native/non-native collaborations,
our system ultimately predicts the 2nd (non-native) author to be
the main writer; in one case we confirmed the accuracy of this
prediction by personal communication with the authors.
5
www.clsp.jhu.edu/
?
sbergsma/Gender/
329
if the name has an aggregate count >30 and fe-
male probability >0.85, label as female; otherwise
if the aggregate count is >30 and male probabil-
ity >0.85, label male. This rule captures many of
ACL?s unambiguously-gendered names, both male
(Nathanael, Jens, Hiroyuki) and female (Widad,
Yael, Sunita). For Lenient training, we assign all
papers based only on whether the male or female
probability for the first author is higher. While po-
tentially noisy, there is precedent for assigning a sin-
gle gender to papers ?co-authored by researchers of
mixed gender? (Sarawgi et al, 2011).
5 Models and Training Strategies
Model: We take a discriminative approach to sty-
lometry, representing articles as feature vectors (?6)
and classifying them using a linear, L2-regularized
SVM, trained via LIBLINEAR (Fan et al, 2008).
SVMs are state-of-the-art and have been used pre-
viously in stylometry (Koppel et al, 2005).
Strategy: We test whether it?s better to train with
a smaller, more accurate Strict set, or a larger but
noisier Lenient set. We also explore a third strategy,
motivated by work in learning from noisy web im-
ages (Bergamo and Torresani, 2010), in which we
fix the Strict labels, but also include the remaining
examples as unlabeled instances. We then optimize
a Transductive SVM, solving an optimization prob-
lem where we not only choose the feature weights,
but also labels for unlabeled training points. Like
a regular SVM, the goal is to maximize the margin
between the positive and negative vectors, but now
the vectors have both fixed and imputed labels. We
optimize using Joachims (1999)?s software. While
the classifier is trained using a transductive strategy,
it is still tested inductively, i.e., on unseen data.
6 Stylometric Features
Koppel et al (2003) describes a range of features
that have been used in stylometry, ranging from
early manual selection of potentially discriminative
words, to approaches based on automated text cat-
egorization (Sebastiani, 2002). We use the follow-
ing three feature classes; the particular features were
chosen based on development experiments.
6.1 Bow Features
A variety of ?discouraging results? in the text cate-
gorization literature have shown that simple bag-of-
words (Bow) representations usually perform better
than ?more sophisticated? ones (e.g. using syntax)
(Sebastiani, 2002). This was also observed in sen-
timent classification (Pang et al, 2002). One key
aim of our research is to see whether this is true of
scientific stylometry. Our Bow representation uses
a feature for each unique lower-case word-type in
an article. We also preprocess papers by making all
digits ?0?. Normalizing digits and filtering capital-
ized words helps ensure citations and named-entities
are excluded from our features. The feature value is
the log-count of how often the corresponding word
occurs in the document.
6.2 Style Features
While text categorization relies on keywords, sty-
lometry focuses on topic-independent measures like
function word frequency (Mosteller and Wallace,
1984), sentence length (Yule, 1939), and PoS (Hirst
and Feiguina, 2007). We define a style-word to be:
(1) punctuation, (2) a stopword, or (3) a Latin abbre-
viation.6 We create Style features for all unigrams
and bigrams, replacing non-style-words separately
with both PoS-tags and spelling signatures.7 Each
feature is an N-gram, the value is its log-count in the
article. We also include stylistic meta-features such
as mean-words-per-sentence and mean-word-length.
6.3 Syntax Features
Unlike recent work using generative PCFGs (Ragha-
van et al, 2010; Sarawgi et al, 2011), we use syntax
directly as features in discriminative models, which
can easily incorporate arbitrary and overlapping syn-
tactic clues. For example, we will see that one indi-
cator of native text is the use of certain determin-
ers as stand-alone noun phrases (NPs), like this in
Figure 2. This contrasts with a proposed non-native
phrase, ?this/DT growing/VBG area/NN,? where this
instead modifies a noun. The Bow features are
clearly unhelpful: this occurs in both cases. The
6The stopword list is the standard set of 524 SMART-system
stopwords (following Tomokiyo and Jones (2001)). Latin ab-
breviations are i.e., e.g., etc., c.f., et or al.
7E.g., signature ?LC-ing? means lower-case, ending in ing.
These are created via a script included with the Berkeley parser.
330
we did this using . . .
PRP VBD DT VBG
NPNP
VP
. . .
Figure 2: Motivating deeper syntactic features: The
shaded TSG fragment indicates native English, but is not
directly encoded in Bow, Style, nor standard CFG-rules.
Style features are likewise unhelpful; this-VBG also
occurs in both cases. We need the deeper knowledge
that a specific determiner is used as a complete NP.
We evaluate three feature types that aim to cap-
ture such knowledge. In each case, we aggregate the
feature counts over all the parse trees constituting a
document. The feature value is the log-count of how
often each feature occurs. To remove content infor-
mation from the features, we preprocess the parse
tree terminals: all non-style-word terminals are re-
placed with their spelling signature (see ?6.2).
CFG Rules: We include a feature for every unique,
single-level context-free-grammar (CFG) rule appli-
cation in a paper (following Baayen et al (1996),
Gamon (2004), Hirst and Feiguina (2007), Wong
and Dras (2011)). The Figure 2 tree would have
features: NP?PRP, NP?DT, DT?this, etc. Such fea-
tures do capture that a determiner was used as an NP,
but they do not jointly encode which determiner was
used. This is an important omission; we?ll see that
other determiners acting as stand-alone NPs indicate
non-native writing (e.g., the word that, see ?7.2).
TSG Fragments: A tree-substitution grammar is a
generalization of CFGs that allow rewriting to tree
fragments rather than sequences of non-terminals
(Joshi and Schabes, 1997). Figure 2 gives the exam-
ple NP?(DT this). This fragment captures both the
identity of the determiner and its syntactic function
as an NP, as desired. Efficient Bayesian procedures
have recently been developed that enable the train-
ing of large-scale probabilistic TSG grammars (Post
and Gildea, 2009; Cohn et al, 2010).
While TSGs have not been used previously in sty-
lometry, Post (2011) uses them to predict sentence
grammaticality (i.e. detecting pseudo-sentences fol-
lowing Okanohara and Tsujii (2007) and Cherry and
Quirk (2008)). We use Post?s TSG training settings
and his public code.8 We parse with the TSG gram-
mar and extract the fragments as features. We also
follow Post by having features for aggregate TSG
statistics, e.g., how many fragments are of a given
size, tree-depth, etc. These syntactic meta-features
are somewhat similar to the manually-defined stylo-
metric features of Stamatatos et al (2001).
C&J Reranking Features: We also extracted the
reranking features of Charniak and Johnson (2005).
These features were hand-crafted for reranking the
output of a parser, but have recently been used for
other NLP tasks (Post, 2011; Wong and Dras, 2011).
They include lexicalized features for sub-trees and
head-to-head dependencies, and aggregate features
for conjunct parallelism and the degree of right-
branching. We get the features using another script
from Post.9 While TSG fragments tile a parse tree
into a few useful fragments, C&J features can pro-
duce thousands of features per sentence, and are thus
much more computationally-demanding.
7 Experiments and Results
We take the minority class as the positive class:
NES for NativeL, top-tier for Venue and female for
Gender, and calculate the precision/recall of these
classes. We tune three hyperparameters for F1-
score on development data: (1) the SVM regular-
ization parameter, (2) the threshold for classifying
an instance as positive (using the signed hyperplane-
distance as the score), and (3) for transductive train-
ing (?5), the fraction of unlabeled data to label as
positive. Statistical significance on held-out test data
is assessed with McNemar?s test, p<0.05. For F1-
score, we use the following reasonable Baseline: we
label all instances with the label of the minority class
(achieving 100% recall but low precision).
7.1 Selection of Syntax and Training Strategy
Development experiments showed that using all fea-
tures, Bow+Style+Syntax, works best on all tasks,
but there was no benefit in combining different
8http://github.com/mjpost/dptsg
9http://github.com/mjpost/extract-spfeatures.
331
Syntax Strategy NativeL Venue Gender
Baseline 50.5 45.0 28.7
CFG Strict 93.5 59.9 42.5
CFG Lenient 89.9 64.9 39.5
TSG Strict 93.6 60.7 40.0
TSG Lenient 90.9 64.4 39.1
C&J Strict 90.5 62.3 37.1
C&J Lenient 86.2 65.2 39.0
Table 2: F1 scores for Bow+Style+Syntax system on de-
velopment data: The best training strategy and the best
syntactic features depend on the task.
Syntax features. We also found no gain from trans-
ductive training, but greater cost, with more hyper-
parameter tuning and a slower SVM solver. The
best Syntax features depend on the task (Table 2).
Whether Strict or Lenient training: TSG was best
for NativeL, C&J was best for Venue, and CFG was
best for Gender. These trends continue on test data,
where TSG exceeds CFG (91.6% vs. 91.2%). For
the training strategy, Strict was best on NativeL and
Gender, while Lenient was best on Venue (Table 2).
This latter result is interesting: recall that for Venue,
Lenient training considers all conferences to be top-
tier, but evaluation is just on detecting ACL papers.
We suggest some reasons for this below, highlight-
ing some general features of conference papers that
extend beyond particular venues.
For the remainder of experiments on each task,
we fix the syntactic features and training strategy to
those that performed best on development data.
7.2 Test Results and Feature Analysis
Gender remains the most difficult task on test data,
but our F1 still substantially outperforms the base-
line (Table 3). Results on NativeL are particu-
larly impressive; in terms of accuracy, we classify
94.6% of test articles correctly (the majority-class
baseline is 66.9%). Regarding features, just using
Style+Syntax always works better than using Bow.
Combining all features always works better still.
The gains of Bow+Style+Syntax over vanilla Bow are
statistically significant in each case.
We also highlight important individual features:
NativeL: Table 4 gives Bow and Style features
for NativeL. Some reflect differences in common
Features NativeL Venue Gender
Baseline 49.8 45.5 33.1
Bow 88.8 60.7 42.5
Style 90.6 61.9 39.8
Syntax 88.7 64.6 41.2
Bow+Style 90.4 64.0 45.1
Bow+Syntax 90.3 65.8 42.9
Style+Syntax 89.4 65.5 43.3
Bow+Style+Syntax 91.6 66.7 48.2
Table 3: F1 scores with different features on held-out test
data: Including style and syntactic features is superior to
standard Bow features in all cases.
native/non-native topics; e.g., ?probabilities? pre-
dicts native while ?morphological? predicts non-
native. Several features, like ?obtained?, indicate L1
interference; i.e., many non-natives have a cognate
for obtain in their native language and thus adopt the
English word. As an example, the word obtained
occurs 3.7 times per paper from Spanish-speaking
areas (cognate obtenir) versus once per native paper
and 0.8 times per German-authored paper.
Natives also prefer certain abbreviations (e.g.
?e.g.?) while non-natives prefer others (?i.e.?, ?c.f.?,
?etc.?). Exotic punctuation also suggests native text:
the semi-colon, exclamation and question mark all
predict NES. Note this also varies by region; semi-
colons are most popular in NES countries but papers
from Israel and Italy are close behind.
Table 5 gives highly-weighted TSG features for
predicting NativeL. Note the determiner-as-NP us-
age described earlier (? 6.3): these, this and each
predict native when used as an NP; that-as-an-NP
predicts non-native. Furthermore, while not all na-
tive speakers use a comma before a conjunction in
a list, it?s nevertheless a good flag for native writ-
ing (?NP?NP, NP, (CC and) NP?). In terms of non-
native syntax, the passive voice is more common
(?VP?(VBZ is) VP? and ?VP?VBN (PP (IN as) NP)?).
We also looked for features involving determiners
since correct determiner usage is a common diffi-
culty for non-native speakers. We found cases where
determiners were missing where natives might have
used one (?NP?JJ JJ NN?), but also those where a de-
terminer might be optional and skipped by a native
speaker (?NP?(DT the) NN NNS?). Note that Table 5
332
Predicts native Predicts non-native
Bow feature Wt. Bow feature Wt.
initial 2.25 obtained -2.15
techniques 2.11 proposed -2.06
probabilities 1.38 method -2.06
additional 1.23 morphological -1.96
fewer 1.02 languages -1.23
Style feature Wt. Style feature Wt.
used to 1.92 , i.e. -2.60
JJR NN 1.90 have to -1.65
has VBN 1.90 the xxxx-ing -1.61
example , 1.75 thus -1.61
all of 1.73 usually -1.24
?s 1.69 mainly -1.21
allow 1.47 , because -1.12
has xxxx-ed 1.45 the VBN -1.12
may be 1.35 JJ for -1.11
; and 1.21 cf -0.97
e.g. 1.10 etc. -0.55
must VB 0.99 associated to -0.23
Table 4: NativeL: Examples of highly-weighted style and
content features in the Bow+Style+Syntax system.
examples are based on actual usage in ACL papers.
We also found that complex NPs were more asso-
ciated with native text. Features such as ?NP?DT JJ
NN NN NN?, and ?NP?DT NN NN NNS? predict native
writing.
Non-natives also rely more on boilerplate. For
example, the exact phrase ?The/This paper is orga-
nized as follows? occurs 3 times as often in non-
native compared to native text (in 7.5% of all non-
native papers). Sentence re-use is only indirectly
captured by our features; it would be interesting to
encode flags for it directly.
In general, we found very few highly-weighted
features that pinpoint ?ungrammatical? non-native
writing (the feature ?associated to? in Table 4 is a
rare example). Our classifiers largely detect non-
native writing on a stylistic rather than grammatical
basis.
Venue: Table 6 provides important Bow and Style
features for the Venue task (syntactic features omit-
ted due to space). While some features are topical
(e.g. ?biomedical?), the table gives a blueprint for
writing a solid main-conference paper. That is, good
papers often have an explicit probability model (or
algorithm), experimental baselines, error analysis,
TSG Fragment Example
Predicts native English author:
NP?NNP CD (Model) (1)
NP?(DT these) six of (these)
NP?(DT that) NN in (that) (language)
NP?(DT this) we did (this) using ...
VP?(VBN used) S (used) (to describe it)
NP?NP, NP, (CC and) NP (X), (Y), (and) (Z)
NP?(DT each) (each) consists of ...
Predicts non-native English author:
VP?(VBZ is) VP it (is) (shown below)
VP?VBN (PP (IN as) NP) (considered) (as) (a term)
NP?JJ JJ NN in (other) (large) (corpus)
NP?DT JJ (CD one) (a) (correct) (one)
NP?(DT the) NN NNS seen in (the) (test) (data)
NP?(DT that) larger than (that) of ...
QP?(IN about) CD (about) (200,000) words
Table 5: NativeL: Highly-weighted syntactic features
(descending order of absolute weight) and examples in
the Bow+Style+Syntax system.
and statistical significance checking. On the other
hand, there might be a bias at main conferences for
focused, incremental papers; features of workshop
papers highlight the exploration of ?interesting? new
ideas/domains. Here, the objective might only be to
show what is ?possible? or what one is ?able to? do.
Main conference papers prefer work that improves
?performance? by ?#%? on established tasks.
Gender: The CFG features for Gender are given
in Table 7. Several of the most highly-weighted
female features include pronouns (e.g. PRP$). A
higher frequency of pronouns in female writing has
been attested previously (Argamon et al, 2003), but
has not been traced to particular syntactic construc-
tions. Likewise, we observe a higher frequency of
not just negation (noted previously) but adverbs (RB)
in general (e.g. ?VP?MD RB VP?). In terms of Bow
features (not shown), the words contrast and com-
parison highly predict female, as do topical clues
like verb and resource. The top-three male Bow fea-
tures are (in order): simply, perform, parsing.
7.3 Author Rankings
While our objective is to predict attributes of pa-
pers, we also show how that we can identify author
attributes using a larger body of work. We make
NativeL and Gender predictions for all papers in the
333
Predicts ACL Predicts Workshop
Bow feature Wt. Bow feature Wt.
model 2.64 semantic -2.16
probability 1.66 analysis -1.65
performance 1.40 verb -1.35
baseline 1.36 lexical -1.33
= 1.26 study -0.92
algorithm 1.18 biomedical -0.87
large 1.16 preliminary -0.69
error 1.15 interesting -0.69
outperforms 1.02 aim -0.64
significant 0.96 manually -0.62
statistically 0.75 appears -0.54
Style feature Wt. Style feature Wt.
by VBG 1.04 able to -0.99
#% 0.82 xxxx-ed out -0.77
NN over 0.79 further NN -0.71
than the 0.79 NN should -0.69
improvement 0.75 will be -0.61
best 0.71 possible -0.57
xxxx-s by 0.70 have not -0.56
much JJR 0.67 currently -0.56
Table 6: Venue: Examples of highly-weighted style con-
tent features in the Bow+Style+Syntax system.
1990-2000 era using our Bow+Style+Syntax system.
For each author+affiliation with ?3 first-authored
papers, we take the average classifier score on these
papers.
Table 8 shows cases where our model strongly
predicts native, showing top authors with foreign af-
filiations and top authors in English-speaking coun-
tries.10 While not perfect, the predictions correctly
identify some native authors that would be difficult
to detect using only name and location data. For ex-
ample, Dekai Wu (Hong Kong) speaks English na-
tively; Christer Samuelsson lists near-native English
on his C.V.; etc. Likewise, we have also been able
to accurately identify a set of non-native speakers
with common American names that were working
at American universities.
Table 9 provides some of the extreme predictions
of our system on Gender. The extreme male and fe-
male predictions are based on both style and content;
females tend to work on summarization, discourse,
10Note again that this is based on the affiliation of these au-
thors during the 1990s; e.g. Gerald Penn published three papers
while at the University of Tu?bingen.
CFG Rule Example
Predicts female author:
NP?PRP$ NN NN (our) (upper) (bound)
QP?RB CD (roughly) (6000)
NP?NP, CC NP (a new NE tag), (or) (no NE tag)
NP?PRP$ JJ JJ NN (our) (first) (new) (approach)
VP?MD RB VP (may) (not) (be useful)
ADVP?RB RBR (significantly) (more)
Predicts male author:
ADVP?RB RB (only) (superficially)
NP?NP, SBAR we use (XYZ), (which is ...)
S?S: S. (Trust me): (I?m a doctor)
S?S, NP VP (To do so), (it) (needs help)
WHNP?WP NN depending on (what) (path) is ...
PP?IN PRN (in) ((Jelinek, 1976))
Table 7: Gender: Highly-weighted syntactic features
(descending order of weight) and examples in the
Bow+Style+Syntax system.
Highest NES Scores, non-English-country: Gerald
Penn,10 Ezra W. Black, Nigel Collier, Jean-Luc Gauvain,
Dan Cristea, Graham J. Russell, Kenneth R. Beesley,
Dekai Wu, Christer Samuelsson, Raquel Martinez
Highest NES Scores, English-country: Eric V. Siegel,
Lance A. Ramshaw, Stephanie Seneff, Victor W. Zue,
Joshua Goodman, Patti J. Price, Stuart M. Shieber, Jean
Carletta, Lynn Lambert, Gina-Anne Levow
Table 8: Authors scoring highest on NativeL, in descend-
ing order, based exclusively on article text.
etc., while many males focus on parsing. We also
tried making these lists without Bow features, but
the extreme examples still reflect topic to some ex-
tent. Topics themselves have their own style, which
the style features capture; it is difficult to fully sepa-
rate style from topic.
7.4 Correlation with Citations
We also test whether our systems? stylometric scores
correlate with the most common bibliometric mea-
sure: citation count. To reduce the impact of topic,
we only use Style+Syntax features. We plot re-
sults separately for ACL, Coling and Workshop pa-
pers (1990-2000 era). Papers at each venue are
sorted by their classifier scores and binned into five
score bins. Each point in the plot is the mean-
score/mean-number-of-citations for papers in a bin
(within-community citation data is via the AAN ?3
334
Highest Model Scores (Male): John Aberdeen,
Chao-Huang Chang, Giorgio Satta, Stanley F. Chen,
GuoDong Zhou, Carl Weir, Akira Ushioda, Hideki
Tanaka, Koichi Takeda, Douglas B. Paul, Hideo Watan-
abe, Adam L. Berger, Kevin Knight, Jason M. Eisner
Highest Model Scores (Female): Julia B. Hirschberg,
Johanna D. Moore, Judy L. Delin, Paola Merlo,
Rebecca J. Passonneau, Bonnie Lynn Webber, Beth
M. Sundheim, Jennifer Chu-Carroll, Ching-Long Yeh,
Mary Ellen Okurowski, Erik-Jan Van Der Linden
Table 9: Authors scoring highest (absolute values) on
Gender, in descending order, based exclusively on arti-
cle text.
and excludes self citations). We use a truncated
mean for citation counts, leaving off the top/bottom
five papers in each bin.
For NativeL, we only plot papers marked as na-
tive by our Strict rule (i.e. English name/country).
Papers with the lowest NativeL-scores receive many
fewer citations, but they soon level off (Figure 3(a)).
Many junior researchers at English universities are
non-native speakers; early-career non-natives might
receive fewer citations than well-known peers. The
correlation between citations and Venue-scores is
even stronger (Figure 3(b)); the top-ranked work-
shop papers receive five times as many citations
as the lowest ones, and are cited better than a
good portion of ACL papers. These figures sug-
gest that citation-predictors can get useful informa-
tion beyond typical Bow features (Yogatama et al,
2011). Although we focused on a past era, stylis-
tic/syntactic features should also be more robust to
the evolution of scientific topics; we plan to next test
whether we can better forecast future citations. It
would also be interesting to see whether these trends
transfer to other academic disciplines.
7.5 Further Experiments on NativeL
For NativeL, we also created a special test corpus of
273 papers written by first-time ACL authors (2008-
2009 era). This set closely aligns with the system?s
potential use as a tool to help new authors compose
papers. Two (native-speaking) annotators manually
annotated each paper for whether it was primarily
written by a native or non-native speaker (consid-
ering both content and author names/affiliations).
The annotators agreed on 90% of decisions, with an
 1
 10
 0.3  0.4  0.5  0.6  0.7  0.8
NativeL-Score
ACL
Coling
Workshop
(a)
 1
 10
 0.2  0.3  0.4  0.5  0.6
Venue-Score
ACL
Coling
Workshop
(b)
Figure 3: Correlation between predictions (x-axis) and
mean number of citations (y-axis, log-scale).
inter-annotator kappa of 66%. We divided the papers
into a test set and a development set. We applied our
Bow+Style+Syntax system exactly as trained above,
except we tuned its hyperparameters on the new de-
velopment data. The system performed quite well
on this set, reaching 68% F1 over a baseline of only
27%. Moreover, the system also reached 90% accu-
racy, matching the level of human agreement.
8 Conclusion
We have proposed, developed and successfully eval-
uated significant new tasks and methods in the sty-
lometric analysis of scientific articles, including the
novel resolution of publication venue based on pa-
per style, and novel syntactic features based on tree
substitution grammar fragments. In all cases, our
syntactic and stylistic features significantly improve
over a bag-of-words baseline, achieving 10% to 25%
relative error reduction in all three major tasks. We
have included a detailed and insightful analysis of
discriminative stylometric features, and we showed
a strong correlation between our predictions and a
paper?s number of citations. We observed evidence
for L1-interference in non-native writing, for dif-
ferences in topic between males and females, and
for distinctive language usage which can success-
fully identify papers published in top-tier confer-
ences versus wokrshop proceedings. We believe that
this work can stimulate new research at the intersec-
tion of computational linguistics and bibliometrics.
335
References
Shlomo Argamon, Moshe Koppel, Jonathan Fine, and
Anat Rachel Shimoni. 2003. Gender, genre, and writ-
ing style in formal written texts. Text, 23(3), August.
Harald Baayen, Fiona Tweedie, and Hans van Halteren.
1996. Outside the cave of shadows: Using syntactic
annotation to enhance authorship attribution. Literary
and Linguistic Computing, 11(3):121?132.
Alessandro Bergamo and Lorenzo Torresani. 2010. Ex-
ploiting weakly-labeled web images to improve object
classification: a domain adaptation approach. In Proc.
NIPS, pages 181?189.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proc. Coling-ACL,
pages 33?40.
Christine L. Borgman and Jonathan Furner. 2001. Schol-
arly communication and bibliometrics. Annual Review
of Information Science and Technology, 36:3?72.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, pages 173?180.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent SVMs. In
Proc. AMTA.
Martin Chodorow, Joel R. Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involving
prepositions. In Proc. ACL-SIGSEM Workshop on
Prepositions, pages 25?30.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. J. Mach.
Learn. Res., 11:3053?3096.
Robert Dale and Adam Kilgarriff. 2010. Helping our
own: Text massaging for computational linguistics as
a new shared task. In Proc. 6th International Natural
Language Generation Conference, pages 261?265.
Jacob Eisenstein, Noah A. Smith, and Eric P. Xing.
2011. Discovering sociolinguistic associations with
structured sparsity. In Proc. ACL, pages 1365?1374.
Dominique Estival, Tanja Gaustad, Son-Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proc. PACLING, pages 263?
272.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. J. Mach. Learn.
Res., 9:1871?1874.
Rachele De Felice and Stephen G. Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proc. ACL-SIGSEM Workshop on Prepositions, pages
45?50.
Michael Gamon. 2004. Linguistic correlates of style:
authorship classification with deep linguistic analysis
features. In Proc. Coling, pages 611?617.
Michael Gamon. 2010. Using mostly native data to cor-
rect errors in learners? writing: a meta-classifier ap-
proach. In Proc. HLT-NAACL, pages 163?171.
Sean Gerrish and David M. Blei. 2010. A language-
based approach to measuring scholarly impact. In
Proc. ICML, pages 375?382.
Angela Glover and Graeme Hirst. 1995. Detecting
stylistic inconsistencies in collaborative writing. In
Writers at work: Professional writing in the comput-
erized environment, pages 147?168.
David Hall, Daniel Jurafsky, and Christopher D. Man-
ning. 2008. Studying the history of ideas using topic
models. In Proc. EMNLP, pages 363?371.
Na-Rae Han, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by non-
native speakers. Nat. Lang. Eng., 12(2):115?129.
Shawndra Hill and Foster Provost. 2003. The myth of
the double-blind review?: Author identification using
only citations. SIGKDD Explor. Newsl., 5:179?184.
Graeme Hirst and Ol?ga Feiguina. 2007. Bigrams of
syntactic labels for authorship discrimination of short
texts. Literary and Linguistic Computing, 22(4):405?
417.
Thorsten Joachims. 1999. Transductive inference for
text classification using support vector machines. In
Proc. ICML, pages 200?209.
Nikhil Johri, Daniel Ramage, Daniel McFarland, and
Daniel Jurafsky. 2011. A study of academic collabo-
rations in computational linguistics using a latent mix-
ture of authors model. In Proc. 5th ACL-HLT Work-
shop on Language Technology for Cultural Heritage,
Social Sciences, and Humanities, pages 124?132.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Beyond
Words, volume 3, pages 71?122.
Moshe Koppel, Shlomo Argamon, and Anat Rachel Shi-
moni. 2003. Automatically categorizing written texts
by author gender. Literary and Linguistic Computing,
17(4):401?412.
Moshe Koppel, Jonathan Schler, and Kfir Zigdon. 2005.
Determining an author?s native language by mining a
text for errors. In Proc. KDD, pages 624?628.
Xuan Le, Ian Lancashire, Graeme Hirst, and Regina
Jokel. 2011. Longitudinal detection of dementia
through lexical and syntactic changes in writing: A
case study of three British novelists. Literary and Lin-
guistic Computing, 26(4):435?461.
Frederick Mosteller and David L. Wallace. 1984. Ap-
plied Bayesian and Classical Inference: The Case of
the Federalist Papers. Springer-Verlag.
Greg Myers. 1989. The pragmatics of politeness in sci-
entific articles. Applied Linguistics, 10(1):1?35.
336
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proc. ACL, pages 73?80.
Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T. Han-
cock. 2011. Finding deceptive opinion spam by any
stretch of the imagination. In Proc. ACL, pages 309?
319.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: Sentiment classification using
machine learning techniques. In Proc. EMNLP, pages
79?86.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. Coling-ACL, pages
433?440.
Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS
Tagger. crftagger.sourceforge.net.
Matt Post and Daniel Gildea. 2009. Bayesian learning
of a tree substitution grammar. In Proc. ACL-IJCNLP,
pages 45?48.
Matt Post. 2011. Judging grammaticality with tree sub-
stitution grammar derivations. In Proc. ACL, pages
217?222.
Vahed Qazvinian and Dragomir R. Radev. 2008. Scien-
tific paper summarization using citation summary net-
works. In Proc. Coling, pages 689?696.
Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-
son, and Pradeep Muthukrishnan. 2009a. A biblio-
metric and network analysis of the field of computa-
tional linguistics. Journal of the American Society for
Information Science and Technology.
Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed
Qazvinian. 2009b. The ACL anthology network cor-
pus. In Proc. ACL Workshop on Natural Language
Processing and Information Retrieval for Digital Li-
braries, pages 54?61.
Sindhu Raghavan, Adriana Kovashka, and Raymond
Mooney. 2010. Authorship attribution using proba-
bilistic context-free grammars. In Proc. ACL, pages
38?42.
Delip Rao, Michael Paul, Clay Fink, David Yarowsky,
Timothy Oates, and Glen Coppersmith. 2011. Hierar-
chical bayesian models for latent attribute detection in
social media. In Proc. ICWSM, pages 598?601.
Ruchita Sarawgi, Kailash Gajulapalli, and Yejin Choi.
2011. Gender attribution: tracing stylometric evidence
beyond topic and genre. In Proc. CoNLL, pages 78?
86.
Fabrizio Sebastiani. 2002. Machine learning in auto-
mated text categorization. ACM Comput. Surv., 34:1?
47.
Efstathios Stamatatos, Nikos Fakotakis, and George
Kokkinakis. 2001. Automatic text categorization in
terms of genre and author. Computational Linguistics,
26(4):471?495.
Joel R. Tetreault and Martin Chodorow. 2008. The ups
and downs of preposition error detection in ESL writ-
ing. In Proc. Coling, pages 865?872.
Simone Teufel and Marc Moens. 2002. Summariz-
ing scientific articles - experiments with relevance
and rhetorical status. Computational Linguistics,
28(4):409?445.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you? Naive Bayes
detection of non-native utterances. In Proc. NAACL.
Oren Tsur and Ari Rappoport. 2007. Using classi-
fier features for studying the effect of native language
on the choice of written second language words. In
Proc. Workshop on Cognitive Aspects of Computa-
tional Language Acquisition, pages 9?16.
Irena Vassileva. 1998. Who am I/who are we in aca-
demic writing? International Journal of Applied Lin-
guistics, 8(2):163?185.
Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting
parse structures for native language identification. In
Proc. EMNLP, pages 1600?1610.
Dani Yogatama, Michael Heilman, Brendan O?Connor,
Chris Dyer, Bryan R. Routledge, and Noah A. Smith.
2011. Predicting a scientific community?s response to
an article. In Proc. EMNLP, pages 594?604.
G. Udny Yule. 1939. On sentence-length as a statis-
tical characteristic of style in prose: With applica-
tion to two cases of disputed authorship. Biometrika,
30(3/4):363?390.
337
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 217?222,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Judging Grammaticality with Tree Substitution Grammar Derivations
Matt Post
Human Language Technology Center of Excellence
Johns Hopkins University
Baltimore, MD 21211
Abstract
In this paper, we show that local features com-
puted from the derivations of tree substitution
grammars ? such as the identify of particu-
lar fragments, and a count of large and small
fragments ? are useful in binary grammatical
classification tasks. Such features outperform
n-gram features and various model scores by
a wide margin. Although they fall short of
the performance of the hand-crafted feature
set of Charniak and Johnson (2005) developed
for parse tree reranking, they do so with an
order of magnitude fewer features. Further-
more, since the TSGs employed are learned
in a Bayesian setting, the use of their deriva-
tions can be viewed as the automatic discov-
ery of tree patterns useful for classification.
On the BLLIP dataset, we achieve an accuracy
of 89.9% in discriminating between grammat-
ical text and samples from an n-gram language
model.
1 Introduction
The task of a language model is to provide a measure
of the grammaticality of a sentence. Language mod-
els are useful in a variety of settings, for both human
and machine output; for example, in the automatic
grading of essays, or in guiding search in a machine
translation system. Language modeling has proved
to be quite difficult. The simplest models, n-grams,
are self-evidently poor models of language, unable
to (easily) capture or enforce long-distance linguis-
tic phenomena. However, they are easy to train, are
long-studied and well understood, and can be ef-
ficiently incorporated into search procedures, such
as for machine translation. As a result, the output
of such text generation systems is often very poor
grammatically, even if it is understandable.
Since grammaticality judgments are a matter of
the syntax of a language, the obvious approach for
modeling grammaticality is to start with the exten-
sive work produced over the past two decades in
the field of parsing. This paper demonstrates the
utility of local features derived from the fragments
of tree substitution grammar derivations. Follow-
ing Cherry and Quirk (2008), we conduct experi-
ments in a classification setting, where the task is to
distinguish between real text and ?pseudo-negative?
text obtained by sampling from a trigram language
model (Okanohara and Tsujii, 2007). Our primary
points of comparison are the latent SVM training
of Cherry and Quirk (2008), mentioned above, and
the extensive set of local and nonlocal feature tem-
plates developed by Charniak and Johnson (2005)
for parse tree reranking. In contrast to this latter set
of features, the feature sets from TSG derivations
require no engineering; instead, they are obtained
directly from the identity of the fragments used in
the derivation, plus simple statistics computed over
them. Since these fragments are in turn learned au-
tomatically from a Treebank with a Bayesian model,
their usefulness here suggests a greater potential for
adapting to other languages and datasets.
2 Tree substitution grammars
Tree substitution grammars (Joshi and Schabes,
1997) generalize context-free grammars by allow-
ing nonterminals to rewrite as tree fragments of ar-
bitrary size, instead of as only a sequence of one or
217
.S
.NP .VP
.VBD
.said
.NP .SBAR
..
Figure 1: A Tree Substitution Grammar fragment.
more children. Evaluated by parsing accuracy, these
grammars are well below state of the art. However,
they are appealing in a number of ways. Larger frag-
ments better match linguists? intuitions about what
the basic units of grammar should be, capturing, for
example, the predicate-argument structure of a verb
(Figure 1). The grammars are context-free and thus
retain cubic-time inference procedures, yet they re-
duce the independence assumptions in the model?s
generative story by virtue of using fewer fragments
(compared to a standard CFG) to generate a tree.
3 A spectrum of grammaticality
The use of large fragments in TSG grammar deriva-
tions provides reason to believe that such grammars
might do a better job at language modeling tasks.
Consider an extreme case, in which a grammar con-
sists entirely of complete parse trees. In this case,
ungrammaticality is synonymous with parser fail-
ure. Such a classifier would have perfect precision
but very low recall, since it could not generalize
at all. On the other extreme, a context-free gram-
mar containing only depth-one rules can basically
produce an analysis over any sequence of words.
However, such grammars are notoriously leaky, and
the existence of an analysis does not correlate with
grammaticality. Context-free grammars are too poor
models of language for the linguistic definition of
grammaticality (a sequence of words in the language
of the grammar) to apply.
TSGs permit us to posit a spectrum of grammati-
cality in between these two extremes. If we have a
grammar comprising small and large fragments, we
might consider that larger fragments should be less
likely to fit into ungrammatical situations, whereas
small fragments could be employed almost any-
where as a sort of ungrammatical glue. Thus, on
average, grammatical sentences will license deriva-
tions with larger fragments, whereas ungrammatical
sentences will be forced to resort to small fragments.
This is the central idea explored in this paper.
This raises the question of what exactly the larger
fragments are. A fundamental problem with TSGs is
that they are hard to learn, since there is no annotated
corpus of TSG derivations and the number of possi-
ble derivations is exponential in the size of a tree.
The most popular TSG approach has been Data-
Oriented Parsing (Scha, 1990; Bod, 1993), which
takes all fragments in the training data. The large
size of such grammars (exponential in the size of the
training data) forces either implicit representations
(Goodman, 1996; Bansal and Klein, 2010) ? which
do not permit arbitrary probability distributions over
the grammar fragments ? or explicit approxima-
tions to all fragments (Bod, 2001). A number of re-
searchers have presented ways to address the learn-
ing problems for explicitly represented TSGs (Zoll-
mann and Sima?an, 2005; Zuidema, 2007; Cohn et
al., 2009; Post and Gildea, 2009a). Of these ap-
proaches, work in Bayesian learning of TSGs pro-
duces intuitive grammars in a principled way, and
has demonstrated potential in language modeling
tasks (Post and Gildea, 2009b; Post, 2010). Our ex-
periments make use of Bayesian-learned TSGs.
4 Experiments
We experiment with a binary classification task, de-
fined as follows: given a sequence of words, deter-
mine whether it is grammatical or not. We use two
datasets: the Wall Street Journal portion of the Penn
Treebank (Marcus et al, 1993), and the BLLIP ?99
dataset,1 a collection of automatically-parsed sen-
tences from three years of articles from the Wall
Street Journal.
For both datasets, positive examples are obtained
from the leaves of the parse trees, retaining their to-
kenization. Negative examples were produced from
a trigram language model by randomly generating
sentences of length no more than 100 so as to match
the size of the positive data. The language model
was built with SRILM (Stolcke, 2002) using inter-
polated Kneser-Ney smoothing. The average sen-
tence lengths for the positive and negative data were
23.9 and 24.7, respectively, for the Treebank data
1LDC Catalog No. LDC2000T43.
218
dataset training devel. test
Treebank 3,836 2,690 3,398
91,954 65,474 79,998
BLLIP 100,000 6,000 6,000
2,596,508 155,247 156,353
Table 1: The number of sentences (first line) and words
(second line) using for training, development, and test-
ing of the classifier. Each set of sentences is evenly split
between positive and negative examples.
and 25.6 and 26.2 for the BLLIP data.
Each dataset is divided into training, develop-
ment, and test sets. For the Treebank, we trained
the n-gram language model on sections 2 - 21. The
classifier then used sections 0, 24, and 22 for train-
ing, development, and testing, respectively. For
the BLLIP dataset, we followed Cherry and Quirk
(2008): we randomly selected 450K sentences to
train the n-gram language model, and 50K, 3K, and
3K sentences for classifier training, development,
and testing, respectively. All sentences have 100
or fewer words. Table 1 contains statistics of the
datasets used in our experiments.
To build the classifier, we used liblinear (Fan
et al, 2008). A bias of 1 was added to each feature
vector. We varied a cost or regularization parame-
ter between 1e ? 5 and 100 in orders of magnitude;
at each step, we built a model, evaluating it on the
development set. The model with the highest score
was then used to produce the result on the test set.
4.1 Base models and features
Our experiments compare a number of different fea-
ture sets. Central to these feature sets are features
computed from the output of four language models.
1. Bigram and trigram language models (the same
ones used to generate the negative data)
2. A Treebank grammar (Charniak, 1996)
3. A Bayesian-learned tree substitution grammar
(Post and Gildea, 2009a)2
2The sampler was run with the default settings for 1,000
iterations, and a grammar of 192,667 fragments was then ex-
tracted from counts taken from every 10th iteration between
iterations 500 and 1,000, inclusive. Code was obtained from
http://github.com/mjpost/dptsg.
4. The Charniak parser (Charniak, 2000), run in
language modeling mode
The parsing models for both datasets were built from
sections 2 - 21 of the WSJ portion of the Treebank.
These models were used to score or parse the train-
ing, development, and test data for the classifier.
From the output, we extract the following feature
sets used in the classifier.
? Sentence length (l).
? Model scores (S). Model log probabilities.
? Rule features (R). These are counter features
based on the atomic unit of the analysis, i.e., in-
dividual n-grams for the n-gram models, PCFG
rules, and TSG fragments.
? Reranking features (C&J). From the Char-
niak parser output we extract the complete set
of reranking features of Charniak and Johnson
(2005), and just the local ones (C&J local).3
? Frontier size (Fn,F ln). Instances of this fea-
ture class count the number of TSG fragments
having frontier size n, 1 ? n ? 9.4 Instances
of F ln count only lexical items for 0 ? n ? 5.
4.2 Results
Table 2 contains the classification results. The first
block of models all perform at chance. We exper-
imented with SVM classifiers instead of maximum
entropy, and the only real change across all the mod-
els was for these first five models, which saw classi-
fication rise to 55 to 60%.
On the BLLIP dataset, the C&J feature sets per-
form the best, even when the set of features is re-
stricted to local ones. However, as shown in Table 3,
this performance comes at a cost of using ten times
as many features. The classifiers with TSG features
outperform all the other models.
The (near)-perfect performance of the TSG mod-
els on the Treebank is a result of the large number
of features relative to the size of the training data:
3Local features can be computed in a bottom-up manner.
See Huang (2008, ?3.2) for more detail.
4A fragment?s frontier is the number of terminals and non-
terminals among its leaves, also known its rank. For example,
the fragment in Figure 1 has a frontier size of 5.
219
feature set Treebank BLLIP
length (l) 50.0 46.4
3-gram score (S3) 50.0 50.1
PCFG score (SP ) 49.5 50.0
TSG score (ST ) 49.5 49.7
Charniak score (SC) 50.0 50.0
l + S3 61.0 64.3
l + SP 75.6 70.4
l + ST 82.4 76.2
l + SC 76.3 69.1
l + R2 62.4 70.6
l + R3 61.3 70.7
l + RP 60.4 85.0
l + RT 99.4 89.3
l + C&J (local) 89.1 92.5
l + C&J 88.6 93.0
l + RT + F? + F l? 100.0 89.9
Table 2: Classification accuracy.
feature set Treebank BLLIP
l + R3 18K 122K
l + RP 15K 11K
l + RT 14K 60K
l + C&J (local) 24K 607K
l + C&J 58K 959K
l + RT + F? 14K 60K
Table 3: Model size.
the positive and negative data really do evince dif-
ferent fragments, and there are enough such features
relative to the size of the training data that very high
weights can be placed on them. Manual examina-
tion of feature weights bears this out. Despite hav-
ing more features available, the Charniak & John-
son feature set has significantly lower accuracy on
the Treebank data, which suggests that the TSG fea-
tures are more strongly associated with a particular
(positive or negative) outcome.
For comparison, Cherry and Quirk (2008) report
a classification accuracy of 81.42 on BLLIP. We ex-
clude it from the table because a direct comparison is
not possible, since we did not have access to the split
on the BLLIP used in their experiments, but only re-
peated the process they described to generate it.
5 Analysis
Table 4 lists the highest-weighted TSG features as-
sociated with each outcome, taken from the BLLIP
model in the last row of Table 2. The learned
weights accord with the intuitions presented in Sec-
tion 3. Ungrammatical sentences use smaller, ab-
stract (unlexicalized) rules, whereas grammatical
sentences use higher rank rules and are more lexical-
ized. Looking at the fragments themselves, we see
that sensible patterns such as balanced parenthetical
expressions or verb predicate-argument structures
are associated with grammaticality, while many of
the ungrammatical fragments contain unbalanced
quotations and unlikely configurations.
Table 5 contains the most probable depth-one
rules for each outcome. The unary rules associated
with ungrammatical sentences show some interest-
ing patterns. For example, the rule NP? DT occurs
2,344 times in the training portion of the Treebank.
Most of these occurrences are in subject settings
over articles that aren?t required to modify a noun,
such as that, some, this, and all. However, in the
BLLIP n-gram data, this rule is used over the defi-
nite article the 465 times ? the second-most common
use. Yet this rule occurs only nine times in the Tree-
bank where the grammar was learned. The small
fragment size, together with the coarseness of the
nonterminal, permit the fragment to be used in dis-
tributional settings where it should not be licensed.
This suggests some complementarity between frag-
ment learning and work in using nonterminal refine-
ments (Johnson, 1998; Petrov et al, 2006).
6 Related work
Past approaches using parsers as language models
in discriminative settings have seen varying degrees
of success. Och et al (2004) found that the score
of a bilexicalized parser was not useful in distin-
guishing machine translation (MT) output from hu-
man reference translations. Cherry and Quirk (2008)
addressed this problem by using a latent SVM to
adjust the CFG rule weights such that the parser
score was a much more useful discriminator be-
tween grammatical text and n-gram samples. Mut-
ton et al (2007) also addressed this problem by com-
bining scores from different parsers using an SVM
and showed an improved metric of fluency.
220
grammatical ungrammatical
(VP VBD (NP CD)
PP)
F l0
(S (NP PRP) VP) (NP (NP CD) PP)
(S NP (VP TO VP)) (TOP (NP NP NP .))
F l2 F5
(NP NP (VP VBG
NP))
(S (NP (NNP UNK-
CAPS-NUM)))
(SBAR (S (NP PRP)
VP))
(TOP (S NP VP (. .)))
(SBAR (IN that) S) (TOP (PP IN NP .))
(TOP (S NP (VP (VBD
said) NP SBAR) .))
(TOP (S ? NP VP (. .)))
(NP (NP DT JJ NN)
PP)
(TOP (S PP NP VP .))
(NP (NP NNP NNP) ,
NP ,)
(TOP (NP NP PP .))
(TOP (S NP (ADVP
(RB also)) VP .))
F4
(VP (VB be) VP) (NP (DT that) NN)
(NP (NP NNS) PP) (TOP (S NP VP . ?))
(NP NP , (SBAR
WHNP (S VP)) ,)
(TOP (NP NP , NP .))
(TOP (S SBAR , NP
VP .))
(QP CD (CD million))
(ADJP (QP $ CD (CD
million)))
(NP NP (CC and) NP)
(SBAR (IN that) (S NP
VP))
(PP (IN In) NP)
F8 (QP $ CD (CD mil-
lion))
Table 4: Highest-weighted TSG features.
Outside of MT, Foster and Vogel (2004) argued
for parsers that do not assume the grammaticality of
their input. Sun et al (2007) used a set of templates
to extract labeled sequential part-of-speech patterns
together with some other linguistic features) which
were then used in an SVM setting to classify sen-
tences in Japanese and Chinese learners? English
corpora. Wagner et al (2009) and Foster and An-
dersen (2009) attempt finer-grained, more realistic
(and thus more difficult) classifications against un-
grammatical text modeled on the sorts of mistakes
made by language learners using parser probabili-
ties. More recently, some researchers have shown
that using features of parse trees (such as the rules
grammatical ungrammatical
(WHNP CD) (NN UNK-CAPS)
(NP JJ NNS) (S VP)
(PRT RP) (S NP)
(WHNP WP NN) (TOP FRAG)
(SBAR WHNP S) (NP DT JJ)
(WHNP WDT NN) (NP DT)
Table 5: Highest-weighted depth-one rules.
used) is fruitful (Wong and Dras, 2010; Post, 2010).
7 Summary
Parsers were designed to discriminate among struc-
tures, whereas language models discriminate among
strings. Small fragments, abstract rules, indepen-
dence assumptions, and errors or peculiarities in the
training corpus allow probable structures to be pro-
duced over ungrammatical text when using models
that were optimized for parser accuracy.
The experiments in this paper demonstrate the
utility of tree-substitution grammars in discriminat-
ing between grammatical and ungrammatical sen-
tences. Features are derived from the identities of
the fragments used in the derivations above a se-
quence of words; particular fragments are associated
with each outcome, and simple statistics computed
over those fragments are also useful. The most com-
plicated aspect of using TSGs is grammar learning,
for which there are publicly available tools.
Looking forward, we believe there is significant
potential for TSGs in more subtle discriminative
tasks, for example, in discriminating between finer
grained and more realistic grammatical errors (Fos-
ter and Vogel, 2004; Wagner et al, 2009), or in dis-
criminating among translation candidates in a ma-
chine translation framework. In another line of po-
tential work, it could prove useful to incorporate into
the grammar learning procedure some knowledge of
the sorts of fragments and features shown here to be
helpful for discriminating grammatical and ungram-
matical text.
References
Mohit Bansal and Dan Klein. 2010. Simple, accurate
parsing with an all-fragments grammar. In Proc. ACL,
Uppsala, Sweden, July.
221
Rens Bod. 1993. Using an annotated corpus as a stochas-
tic grammar. In Proc. ACL, Columbus, Ohio, USA.
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proc. ACL,
Toulouse, France, July.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and MaxEnt discriminative rerank-
ing. In Proc. ACL, Ann Arbor, Michigan, USA, June.
Eugene Charniak. 1996. Tree-bank grammars. In Proc.
of the National Conference on Artificial Intelligence.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. NAACL, Seattle, Washington, USA,
April?May.
Colin Cherry and Chris Quirk. 2008. Discriminative,
syntactic language modeling through latent svms. In
Proc. AMTA, Waikiki, Hawaii, USA, October.
Trevor Cohn, Sharon. Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proc. NAACL, Boulder, Colorado, USA,
June.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A li-
brary for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Jennifer Foster and ?istein E. Andersen. 2009. Gen-
errate: generating errors for use in grammatical error
detection. In Proceedings of the fourth workshop on
innovative use of nlp for building educational appli-
cations, pages 82?90. Association for Computational
Linguistics.
Jennifer Foster and Carl Vogel. 2004. Good reasons
for noting bad grammar: Constructing a corpus of un-
grammatical language. In Pre-Proceedings of the In-
ternational Conference on Linguistic Evidence: Em-
pirical, Theoretical and Computational Perspectives.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the DOP model. In Proc. EMNLP, Philadelphia,
Pennsylvania, USA, May.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of the
Annual Meeting of the Association for Computational
Linguistics (ACL), Columbus, Ohio, June.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors,Handbook of Formal Languages: Beyond
Words, volume 3, pages 71?122.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
linguistics, 19(2):330.
Andrew Mutton, Mark Dras, Stephen Wan, and Robert
Dale. 2007. Gleu: Automatic evaluation of sentence-
level fluency. In Proc. ACL, volume 45, page 344.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng, et al
2004. A smorgasbord of features for statistical ma-
chine translation. In Proc. NAACL.
Daisuke Okanohara and Jun?ichi Tsujii. 2007. A
discriminative language model with pseudo-negative
samples. In Proc. ACL, Prague, Czech Republic, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING/ACL, Syd-
ney, Australia, July.
Matt Post and Daniel Gildea. 2009a. Bayesian learning
of a tree substitution grammar. In Proc. ACL (short
paper track), Suntec, Singapore, August.
Matt Post and Daniel Gildea. 2009b. Language modeling
with tree substitution grammars. In NIPS workshop on
Grammar Induction, Representation of Language, and
Language Learning, Whistler, British Columbia.
Matt Post. 2010. Syntax-based Language Models for
Statistical Machine Translation. Ph.D. thesis, Univer-
sity of Rochester.
Remko Scha. 1990. Taaltheorie en taaltechnologie; com-
petence en performance. In R. de Kort and G.L.J.
Leerdam, editors, Computertoepassingen in de neer-
landistiek, pages 7?22, Almere, the Netherlands.
Andreas Stolcke. 2002. SRILM ? an extensible language
modeling toolkit. In Proc. International Conference
on Spoken Language Processing.
Ghihua Sun, Xiaohua Liu, Gao Cong, Ming Zhou,
Zhongyang Xiong, John Lee, and Chin-Yew Lin.
2007. Detecting erroneous sentences using automat-
ically mined sequential patterns. In Proc. ACL, vol-
ume 45.
JoachimWagner, Jennifer Foster, and Josef van Genabith.
2009. Judging grammaticality: Experiments in sen-
tence classification. CALICO Journal, 26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proc. Australasian Language Technology Association
Workshop, Melbourne, Australia, December.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for Data-Oriented Parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious Data-Oriented
Parsing. In Proc. EMNLP, Prague, Czech Republic,
June.
222
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866?872,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Explicit and Implicit Syntactic Features for Text Classification
Matt Post1 and Shane Bergsma1,2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Baltimore, MD
Abstract
Syntactic features are useful for many
text classification tasks. Among these,
tree kernels (Collins and Duffy, 2001)
have been perhaps the most robust and
effective syntactic tool, appealing for
their empirical success, but also be-
cause they do not require an answer
to the difficult question of which tree
features to use for a given task. We
compare tree kernels to different ex-
plicit sets of tree features on five diverse
tasks, and find that explicit features of-
ten perform as well as tree kernels on
accuracy and always in orders of mag-
nitude less time, and with smaller mod-
els. Since explicit features are easy to
generate and use (with publicly avail-
able tools), we suggest they should al-
ways be included as baseline compar-
isons in tree kernel method evaluations.
1 Introduction
Features computed over parse trees are use-
ful for a range of discriminative tasks, in-
cluding authorship attribution (Baayen et al,
1996), parse reranking (Collins and Duffy,
2002), language modeling (Cherry and Quirk,
2008), and native-language detection (Wong
and Dras, 2011). A major distinction among
these uses of syntax is how the features are rep-
resented. The implicit approach uses tree
kernels (Collins and Duffy, 2001), which make
predictions with inner products between tree
pairs. These products can be computed effi-
ciently with a dynamic program that produces
weighted counts of all the shared tree frag-
ments between a pair of trees, essentially in-
corporating all fragments without representing
any of them explicitly. Tree kernel approaches
have been applied successfully in many areas
of NLP (Collins and Duffy, 2002; Moschitti,
2004; Pighin and Moschitti, 2009).
Tree kernels were inspired in part by ideas
from Data-Oriented Parsing (Scha, 1990; Bod,
1993), which was in turn motivated by uncer-
tainty about which fragments to include in a
grammar. However, manual and automatic
approaches to inducing tree fragments have
recently been found to be useful in an ex-
plicit approach to text classification, which
employs specific tree fragments as features in
standard classifiers (Post, 2011; Wong and
Dras, 2011; Swanson and Charniak, 2012).
These feature sets necessarily represent only a
small subset of all possible tree patterns, leav-
ing open the question of what further gains
might be had from the unusued fragments.
Somewhat surprisingly, explicit and implicit
syntactic features have been explored largely
independently. Here, we compare them on a
range of classification tasks: (1,2) grammati-
cal classification (is a sentence written by a hu-
man?), (3) question classification (what type
of answer is sought by this question?), and
(4,5) native language prediction (what is the
native language of a text?s author?).
Our main contribution is to show that an ex-
plicit syntactic feature set performs as well or
better than tree kernels on each tested task,
and in orders of magnitude less time. Since
explicit features are simple to generate (with
publicly available tools) and flexible to use, we
recommend they be included as baseline com-
parisons in tree kernel method evaluations.
2 Experimental setup
We used the following feature sets:
N-grams All unigrams and bigrams.1
1Experiments with trigrams did not show any im-
866
CFG rules Counts of depth-one context-
free grammar (CFG) productions obtained
from the Berkeley parser (Petrov et al, 2006).
C&J features The parse-tree reranking
feature set of Charniak and Johnson (2005),
extracted from the Berkeley parse trees.
TSG features We also parsed with a
Bayesian tree substitution grammar (Post and
Gildea, 2009, TSG)2 and extracted fragment
counts from Viterbi derivations.
We build classifiers with Liblinear3 (Fan
et al, 2008). We divided each dataset into
training, dev, and test sets. We then trained
an L2-regularized L1-loss support vector ma-
chine (-s 3) with a bias parameter of 1 (-B 1),
optimizing the regularization parameter (-c)
on the dev set over the range {0.0001 . . . 100}
by multiples of 10. The best model was then
used to classify the test set. A sentence length
feature was included for every sentence.
For tree kernels, we used SVM-light-TK4
(Moschitti, 2004; Moschitti, 2006) with the
default settings (-t 5 -D 1 -L 0.4),5 which
also solves an L2-regularized L1-loss SVM op-
timization problem. We tuned the regulariza-
tion parameter (-c) on the dev set in the same
manner as described above, providing 4 GB of
memory to the kernel cache (-m 4000).6 We
used subset tree kernels, which compute the
similarity between two trees by implicitly enu-
merating all possible fragments of the trees (in
contrast with subtree kernels, where all frag-
ments fully extend to the leaves).
3 Tasks
Table 1 summarizes our datasets.
3.1 Coarse grammatical classification
Our first comparison is coarse grammatical
classification, where the goal is to distin-
guish between human-written sentences and
?pseudo-negative? sentences sampled from a
trigram language model constructed from in-
provement.
2github.com/mjpost/dptsg
3www.csie.ntu.edu.tw/~cjlin/liblinear/
4disi.unitn.it/moschitti/Tree-Kernel.htm
5Optimizing SVM-TK?s decay parameter (-L) did
not improve test-set accuracy, but did increase training
time (squaring the number of hyperparameter combi-
nations to evaluate), so we stuck with the default.
6Increased from the default of 40 MB, which halves
the running time.
train dev test
Coarse grammaticality (BLLIP)
sentences 100,000 6,000 6,000
Fine grammaticality (PTB)
sentences 79,664 3,978 3,840
Question classification (TREC-10)
sentences 4,907 545 500
Native language (ICLE; 7 languages)
documents 490 105 175
sentences 17,715 3,968 6,777
Native language (ACL; 5 languages)
documents 987 195 185
sentences 146,257 28,139 28,403
Table 1: Datasets.
system accuracy CPU time
Chance 50.0 -
N-gram 68.4 minutes
CFG 86.3 minutes
TSG 89.8 minutes
C&J 92.9 an hour
SVM-TK 91.0 a week
Table 2: Coarse grammaticality. CPU time is
for classifier setup, training, and testing.
domain data (Okanohara and Tsujii, 2007).
Cherry and Quirk (2008) first applied syn-
tax to this task, learning weighted parameters
for a CFG with a latent SVM. Post (2011)
found further improvements with fragment-
based representations (TSGs and C&J) with a
regular SVM. Here, we compare their results
to kernel methods. We repeat Post?s experi-
ments on the BLLIP dataset,7 using his exact
data splits (Table 2). To our knowledge, tree
kernels have not been applied to this task.
3.2 Fine grammatical classification
Real-world grammaticality judgments require
much finer-grained distinctions than the
coarse ones of the previous section (for exam-
ple, marking dropped determiners or wrong
verb inflections). For this task, we too pos-
itive examples from all sentences of sections
2?21 of the WSJ portion of the Penn Tree-
bank (Marcus et al, 1993). Negative exam-
ples were created by inserting one or two errors
7LDC Catalog No. LDC2000T43
867
system accuracy CPU time
Wong & Dras 60.6 -
Chance 50.0 -
N-gram 61.4 minutes
CFG 64.5 minutes
TSG 67.0 minutes
C&J 71.9 an hour
SVM-TK 67.8 weeks
Table 3: Fine-grained classification accuracy
(the Wong and Dras (2010) score is the highest
score from the last column of their Table 3).
system accuracy CPU time
Pighin & Moschitti 86.6 -
Bigram 73.2 seconds
CFG 90.0 seconds
TSG 85.6 seconds
C&J 89.6 minutes
SVM-TK 87.7 twenty min.
Table 4: Question classification (6 classes).
into the parse trees from the positive data us-
ing GenERRate (Foster and Andersen, 2009).
An example sentence pair is But the ballplay-
ers disagree[ing], where the negative exam-
ple incorrectly inflects the verb. Wong and
Dras (2010) reported good results with parsers
trained separately on the positive and negative
sides of the training data and classifiers built
from comparisons between the CFG produc-
tions of those parsers. We obtained their data
splits (described as NoisyWSJ in their paper)
and repeat their experiments here (Table 3).
3.3 Question Classification
We look next at question classification (QC).
Li and Roth (2002) introduced the TREC-10
dataset,8 a set of questions paired with labels
that categorize the question by the type of an-
swer it seeks. The labels are organized hi-
erarchically into six (coarse) top-level labels
and fifty (fine) refinements. An example ques-
tion from the ENTY/animal category is What
was the first domesticated bird?. Table 4 con-
tains results predicting just the coarse labels.
We compare to Pighin and Moschitti (2009),
and also repeat their experiments, finding a
slightly better result for them.
8cogcomp.cs.illinois.edu/Data/QA/QC/
system sent. voting whole
Wong & Dras - - 80.0
Style 42.0 75.3 86.8
CFG 39.5 73.2 83.7
TSG 38.7 72.1 83.2
C&J 42.9 76.3 86.3
SVM-TK 40.7 69.5 -
Style 42.5 65.3 83.7
CFG 39.2 52.6 86.3
TSG 40.4 56.8 84.7
C&J 49.2 66.3 81.1
SVM-TK 42.1 52.6 -
Table 5: Accuracy on ICLE (7 languages, top)
and ACL (five, bottom) datasets at the sen-
tence and document levels. All documents
were signature-stylized (?3.4).
We also experimented with the refined ver-
sion of the task, where we directly predict one
of the fifty refined categories, and found nearly
identical relative results, with the best explicit
feature set (CFG) returning an accuracy of
83.6% (in seconds), and the tree kernel system
69.8% (in an hour). For reference, Zhang and
Lee (2003) report 80.2% accuracy when train-
ing on the full training set (5,500 examples)
with an SVM and bag-of-words features.9
3.4 Native language identification
Native language identification (NLI) is the
task of determining a text?s author?s native
language. This is usually cast as a document-
level task, since there are often not enough
cues to identify native languages at smaller
granularities. As such, this task presents a
challenge to tree kernels, which are defined at
the level of a single parse tree and have no ob-
vious document-level extension. Table 5 there-
fore presents three evaluations: (a) sentence-
level accuracy, and document-level accuracy
from (b) sentence-level voting and (c) direct,
whole-document classification.
We perform these experiments on two
datasets. In order to mitigate topic bias10 and
other problems that have been reported with
9Pighin and Moschitti (2009) did not report results
on this version of the task.
10E.g., when we train with all words, the keyword
?Japanese? is a strong indicator for Japanese authors,
while ?Arabic? is a strong indicator for English ones.
868
the ICLE dataset (Tetreault et al, 2012),11 we
preprocessed each dataset into two signature-
stylized versions by replacing all words not in a
stopword list.12 The first version replaces non-
stopwords with word classes computed from
surface-form signatures,13 and the second with
POS tags.14 N-gram features are then taken
from both stylized versions of the corpus.
Restricting the feature representation to be
topic-independent is standard-practice in sty-
lometric tasks like authorship attribution, gen-
der identification, and native-language identi-
fication (Mosteller and Wallace, 1984; Koppel
et al, 2003; Tomokiyo and Jones, 2001).
3.4.1 ICLE v.2
The first dataset is a seven-language subset
of the International Corpus of Learner En-
glish, Version 2 (ICLE) (Granger et al, 2009),
which contains 3.7 million words of English
documents written by people with sixteen dif-
ferent native languages. Table 1 contains
scores, including one reported by Wong and
Dras (2011), who used the CFG and C&J fea-
tures, and whose data splits we mirror.15
3.4.2 ACL Anthology Network
We also experimented with native language
classification on scientific documents using
a version of the ACL Anthology Network
(Radev et al, 2009, AAN) annotated for ex-
periments in stylemetric tasks, including a
native/non-native author judgment (Bergsma
et al, 2012). For NLI, we further anno-
tated this dataset in a semi-automatic fash-
ion for the five most-common native languages
of ACL authors in our training era: English,
Japanese, German, Chinese, and French. The
annotation heuristics, designed to favor pre-
cision over recall, provided annotations for
1,959 of 8,483 papers (23%) in the 2001?2009
AAN.16
11Including prompts, characters, and special tokens
that correlate strongly with particular outcomes.
12The stopword list contains the set of 524 SMART-
system stopwords used by Tomokiyo and Jones (2001),
plus punctuation and Latin abbreviations.
13For example, suffix and capitalization.
14Via CRFTagger (Phan, 2006).
15Tetreault et al reported accuracies up to 90.1 in a
cross-validation setting that isn?t directly comparable.
16Details and data at old-site.clsp.jhu.edu/
~sbergsma/Stylo/.
60
70
80
90
100
0 0.01 0.1 1 10 100 1,000
a
c
c
u
r
a
c
y
training time (thousands of seconds)
size CFG CFG TSG TSG TSG+ TSG+ C&J C&J SVM-TK SVM-TK uSVM-TK USVM-TK
100 7 62.6 6 61.0 8 73.1 407 72.8 13 62.9 27 62.7
300 7 68.0 6 65.0 8 77.9 412 77.5 46 70.8 174 70.9
1000 7 73.3 6 70.9 9 78.4 433 82.2 227 77.1 1475 77.4
3000 9 75.8 7 77.5 12 82.3 465 87.1 1034 81.4 4394 81.2
10000 13 80.8 11 82.5 32 85.2 708 89.9 8984 85.5 6691 85.3
30000 37 83.5 29 85.8 108 87.7 1276 92.7 72859 88.8 7789 87.8
100000 133 86.3 85 89.1 406 89.8 3152 93.0 873969 91.0 8488 89.0
CFG
TSG
C&J
SVM-TK
uSVM-TK
uSVM-TK USVM-TK
1010.35 62.7
2628.84 70.9
7264.65 77.4
25447.47 81.2
29298.76 85.3
45938.05 87.8
48570.46 89.0
OLD VALUES
Figure 1: Train ng time (1000 econds) vs.
test accuracy for coarse grammaticality, plot-
ting test scores from models trained on 100,
300, 1k, 3k, 10k, 30k, and 100k instances.
4 Discussion
Syntactic features improve upon the n-gram
baseline for all tasks except whole-document
classification for ICLE. Tree kernels are often
among the best, but always trail (by orders
of magnitude) when runtime is considered.
Constructing the multi-class SVM-TK models
for the NLI tasks in particular was computa-
tionally burdensome, requiring cpu-months of
time. The C&J features are similarly often the
best, but incur a runtime cost due to the large
models. CFG and TSG features balance per-
formance, model size, and runtime. We now
compare these approaches in more depth.
4.1 Training time versus accuracy
Tree kernel training is quadratic in the size of
the training data, and its empirical slowness
is known. It is informative to examine learn-
ing curves to see how the time-accuracy trade-
offs extrapolate. We compared models trained
on the first 100, 300, 1k, 3k, 10k, 30k, and
100k data points of the coarse grammaticality
dataset, split evenly between positive and neg-
ative examples (Figure 1). SVM-TK improves
over the TSG and CFG models in the limit,
but at an extraordinary cost in training time:
100k training examples is already pushing the
bounds of practicality for tree kernel learning,
and generating curve?s next point would re-
quire several months of time. Kernel methods
also produce large models that result in slow
test-time performance, a problem dubbed the
?curse of kernelization? (Wang et al, 2010).
Approximate kernel methods designed to
scale to large datasets address this (Severyn
869
and Moschitti, 2010). We investigated the
uSVM-TK toolkit,17 which enables tuning the
tradeoff between training time and accuracy.
While faster than SVM-TK, its performance
was never better than explicit methods along
both dimensions (time and accuracy).
4.2 Overfitting
Overfitting is also a problem for kernel meth-
ods. The best models often had a huge number
of support vectors, achieving near-perfect ac-
curacy on the training set but making many
errors on the dev. and test sets. On the ICLE
task, close to 75% of all the training exam-
ples were used as support vectors. We found
only half as many support vectors used for the
explicit representations, implying less error
(Vapnik, 1998), and saw much lower variance
between training and testing performance.
4.3 Which fragments?
Our findings support the observations of
Cumby and Roth (2003), who point out that
kernels introduce a large number of irrelevant
features that may be especially harmful in
small-data settings, and that, when possible, it
is often better to have a set of explicit, relevant
features. In other words, it is better to have
the right features than all of them. Tree ker-
nels provide a robust, efficiently-computable
measure of comparison, but they also skirt the
difficult question, Which fragments?
So what are the ?right? features? Table 6)
presents an intuitive list from the coarse gram-
maticality task: phenomena such as balanced
parenthetical phrases and quotations are asso-
ciated with grammaticality, while small, flat,
abstract rules indicate samples from the n-
gram model. Similar intuitive results hold for
the other tasks. The immediate interpretabil-
ity of the explicit formalisms is another ad-
vantage, although recent work has shown that
weights on the implicit features can also be ob-
tained after a kind of linearization of the tree
kernel (Pighin and Moschitti, 2009).
Ultimately, which features matter is task-
dependent, and skirting the question is ad-
vantageous in many settings. But it is also
encouraging that methods for selecting frag-
ments and other tree features work so well,
17disi.unitn.it/~severyn/code.html
(TOP (S ? S , ? NP (VP (VBZ says) ADVP) .))
(FRAG (X SYM) VP .)
(PRN (-LRB- -LRB-) S (-RRB- -RRB-))
(PRN (-LRB- -LRB-) NP (-RRB- -RRB-))
(S NP VP .)
(NP (NP DT CD (NN %)) PP)
(NP DT)
(PP (IN of))
(TOP (NP NP PP PP .))
(NP DT JJ NNS)
Table 6: The highest- and lowest-weighted
TSG features (coarse grammaticality).
yielding quick, light-weight models that con-
trast with the heavy machinery of tree kernels.
5 Conclusion
Tree kernels provide a robust measure of com-
parison between trees, effectively making use
of all fragments. We have shown that for
some tasks, it is sufficient (and advantageous)
to instead use an explicitly-represented subset
of them. In addition to their flexibility and
interpetability, explicit syntactic features of-
ten outperformed tree kernels in accuracy, and
even where they did not, the cost was multiple
orders of magnitude increase in both training
and testing time. These results were consistent
across a range of task types, dataset sizes, and
classification arities (binary and multiclass).
There are a number of important caveats.
We explored a range of data settings, but
there are many others where tree kernels have
been proven useful, such as parse tree rerank-
ing (Collins and Duffy, 2002; Shen and Joshi,
2003), sentence subjectivity (Suzuki et al,
2004), pronoun resolution (Yang et al, 2006),
relation extraction (Culotta and Sorensen,
2004), machine translation evaluation (Liu
and Gildea, 2005), predicate-argument recog-
nition, and semantic role labeling (Pighin and
Moschitti, 2009). There are also tree ker-
nel variations such as dependency tree kernels
(Culotta and Sorensen, 2004) and shallow se-
mantic tree kernels (Moschitti et al, 2007).
These variables provide a rich environment for
future work; in the meantime, we take these re-
sults as compelling motivation for the contin-
ued development of explicit syntactic features
(both manual and automatically induced), and
suggest that such features should be part of
the baseline systems on applicable discrimina-
tive NLP tasks.
870
References
Harald Baayen, Hans Van Halteren, and Fiona
Tweedie. 1996. Outside the cave of shadows:
Using syntactic annotation to enhance author-
ship attribution. Literary and Linguistic Com-
puting, 11(3):121.
Shane Bergsma, Matt Post, and David Yarowsky.
2012. Stylometric analysis of scientific arti-
cles. In Proc. of NAACL-HLT, pages 327?337,
Montre?al, Canada, June. Association for Com-
putational Linguistics.
Rens Bod. 1993. Using an annotated corpus as a
stochastic grammar. In Proc. of ACL, Colum-
bus, Ohio, USA, June.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proc. of ACL, pages
173?180, Ann Arbor, Michigan, USA, June.
Colin Cherry and Chris Quirk. 2008. Discrimi-
native, syntactic language modeling through la-
tent SVMs. In Proc. of AMTA, Waikiki, Hawaii,
USA, October.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proc. of
NIPS.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: kernels
over discrete structures, and the voted percep-
tron. In Proc. of ACL, pages 173?180, Philadel-
phia, Pennsylvania, USA, July.
Aron Culotta and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In
Proc. of ACL, pages 423?429.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871?
1874.
Jennifer Foster and ?istein E. Andersen. 2009.
GenERRate: Generating errors for use in gram-
matical error detection. In Proceedings of the
fourth workshop on innovative use of NLP for
building educational applications, pages 82?90.
Sylviane Granger, Estelle Dagneaux, Fanny Me-
unier, and Magali Paquot. 2009. The Inter-
national Corpus of Learner English. Version 2.
Handbook and CD-Rom.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2003. Automatically categorizing
written texts by author gender. Literary and
Linguistic Computing, 17(4):401?412.
Xin Li and Dan Roth. 2002. Learning question
classifiers. In Proc. of COLING, pages 1?7.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 25?
32.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):330.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question answer classification. In Proc. of ACL,
pages 776?783, Prague, Czech Republic, June.
Alessandro Moschitti. 2004. A study on convo-
lution kernels for shallow semantic parsing. In
Proc. of ACL.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proc.
of EACL, volume 6, pages 113?120.
Frederick Mosteller and David L. Wallace. 1984.
Applied Bayesian and Classical Inference: The
Case of the Federalist Papers. Springer-Verlag.
Daisuke Okanohara and Jun?ichi Tsujii. 2007.
A discriminative language model with pseudo-
negative samples. In Proc. of ACL, Prague,
Czech Republic, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proc. of
ACL, Sydney, Australia, July.
Xuan-Hieu Phan. 2006. CRFTagger: CRF En-
glish POS Tagger. crftagger.sourceforge.net.
Daniele Pighin and Alessandro Moschitti. 2009.
Reverse engineering of tree kernel feature spaces.
In Proc. of EMNLP, pages 111?120, Singapore,
August.
Matt Post and Daniel Gildea. 2009. Bayesian
learning of a tree substitution grammar. In
Proc. of ACL (short paper track), Suntec, Sin-
gapore, August.
Matt Post. 2011. Judging grammaticality with
tree substitution grammar derivations. In Proc.
of ACL, Portland, Oregon, USA, June.
Dragomir R. Radev, Pradeep Muthukrishnan, and
Vahed Qazvinian. 2009. The ACL anthology
network corpus. In Proc. of ACL Workshop on
Natural Language Processing and Information
Retrieval for Digital Libraries, pages 54?61.
Remko Scha. 1990. Taaltheorie en taaltechnologie;
competence en performance. In R. de Kort and
G.L.J. Leerdam, editors, Computertoepassingen
in de neerlandistiek, pages 7?22, Almere, the
Netherlands. De Vereniging.
871
Aliaksei Severyn and Alessandro Moschitti. 2010.
Large-scale support vector learning with struc-
tural kernels. In Proc. of ECML/PKDD, pages
229?244.
Libin Shen and Aravind K. Joshi. 2003. An SVM-
based voting algorithm with application to parse
reranking. In Proc. of CoNLL, pages 9?16.
Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.
2004. Convolution kernels with feature selection
for natural language processing tasks. In Proc.
of ACL, pages 119?126.
Benjamin Swanson and Eugene Charniak. 2012.
Native language detection with tree substitu-
tion grammars. In Proc. of ACL (short papers),
pages 193?197, Jeju Island, Korea, July.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost
and found: Resources and empirical evaluations
in native language identification. In Proc. of
COLING, pages 2585?2602, Mumbai, India, De-
cember.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You?re not from ?round here, are you? Naive
Bayes detection of non-native utterances. In
Proc. of NAACL.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. John Wiley & Sons.
Zhuang Wang, Koby Crammer, and Slobodan
Vucetic. 2010. Multi-class pegasos on a bud-
get. In ICML, pages 1143?1150.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classifica-
tion. In Proceedings of the Australasian Lan-
guage Technology Association Workshop, Mel-
bourne, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Ex-
ploiting parse structures for native language
identification. In Proc. of EMNLP, pages 1600?
1610, Edinburgh, Scotland, UK., July.
Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2006. Kernel-based pronoun resolution with
structured syntactic knowledge. In Proc. of
Coling-ACL, pages 41?48.
Dell Zhang and Wee Sun Lee. 2003. Question
classification using support vector machines. In
Proceedings of the 26th annual international
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, SIGIR ?03,
pages 26?32, New York, NY, USA. ACM.
872
Transactions of the Association for Computational Linguistics, 1 (2013) 165?178. Action Editor: David Chiang.
Submitted 11/2012; Revised 3/2013; Published 5/2013. c?2013 Association for Computational Linguistics.
Learning to translate with products of novices: a suite of open-ended
challenge problems for teaching MT
Adam Lopez1, Matt Post1, Chris Callison-Burch1,2, Jonathan Weese, Juri Ganitkevitch,
Narges Ahmidi, Olivia Buzek, Leah Hanson, Beenish Jamil, Matthias Lee, Ya-Ting Lin,
Henry Pao, Fatima Rivera, Leili Shahriyari, Debu Sinha, Adam Teichert,
Stephen Wampler, Michael Weinberger, Daguang Xu, Lin Yang, and Shang Zhao?
Department of Computer Science, Johns Hopkins University
1Human Language Technology Center of Excellence, Johns Hopkins University
2Computer and Information Science Department, University of Pennsylvania
Abstract
Machine translation (MT) draws from several
different disciplines, making it a complex sub-
ject to teach. There are excellent pedagogical
texts, but problems in MT and current algo-
rithms for solving them are best learned by
doing. As a centerpiece of our MT course,
we devised a series of open-ended challenges
for students in which the goal was to im-
prove performance on carefully constrained
instances of four key MT tasks: alignment,
decoding, evaluation, and reranking. Students
brought a diverse set of techniques to the prob-
lems, including some novel solutions which
performed remarkably well. A surprising and
exciting outcome was that student solutions
or their combinations fared competitively on
some tasks, demonstrating that even newcom-
ers to the field can help improve the state-of-
the-art on hard NLP problems while simulta-
neously learning a great deal. The problems,
baseline code, and results are freely available.
1 Introduction
A decade ago, students interested in natural lan-
guage processing arrived at universities having been
exposed to the idea of machine translation (MT)
primarily through science fiction. Today, incoming
students have been exposed to services like Google
Translate since they were in secondary school or ear-
lier. For them, MT is science fact. So it makes sense
to teach statistical MT, either on its own or as a unit
? The first five authors were instructors and the remaining au-
thors were students in the worked described here. This research
was conducted while Chris Callison-Burch was at Johns Hop-
kins University.
in a class on natural language processing (NLP), ma-
chine learning (ML), or artificial intelligence (AI). A
course that promises to show students how Google
Translate works and teach them how to build some-
thing like it is especially appealing, and several uni-
versities and summer schools now offer such classes.
There are excellent introductory texts?depending
on the level of detail required, instructors can choose
from a comprehensive MT textbook (Koehn, 2010),
a chapter of a popular NLP textbook (Jurafsky and
Martin, 2009), a tutorial survey (Lopez, 2008), or
an intuitive tutorial on the IBM Models (Knight,
1999b), among many others.
But MT is not just an object of academic study.
It?s a real application that isn?t fully perfected, and
the best way to learn about it is to build an MT sys-
tem. This can be done with open-source toolkits
such as Moses (Koehn et al, 2007), cdec (Dyer et
al., 2010), or Joshua (Ganitkevitch et al, 2012), but
these systems are not designed for pedagogy. They
are mature codebases featuring tens of thousands of
source code lines, making it difficult to focus on
their core algorithms. Most tutorials present them
as black boxes. But our goal is for students to learn
the key techniques in MT, and ideally to learn by
doing. Black boxes are incompatible with this goal.
We solve this dilemma by presenting students
with concise, fully-functioning, self-contained com-
ponents of a statistical MT system: word alignment,
decoding, evaluation, and reranking. Each imple-
mentation consists of a na??ve baseline algorithm in
less than 150 lines of Python code. We assign them
to students as open-ended challenges in which the
goal is to improve performance on objective eval-
uation metrics as much as possible. This setting
mirrors evaluations conducted by the NLP research
165
community and by the engineering teams behind
high-profile NLP projects such as Google Translate
and IBM?s Watson. While we designate specific al-
gorithms as benchmarks for each task, we encour-
age creativity by awarding more points for the best
systems. As additional incentive, we provide a web-
based leaderboard to display standings in real time.
In our graduate class on MT, students took a va-
riety of different approaches to the tasks, in some
cases devising novel algorithms. A more exciting re-
sult is that some student systems or combinations of
systems rivaled the state of the art on some datasets.
2 Designing MT Challenge Problems
Our goal was for students to freely experiment with
different ways of solving MT problems on real data,
and our approach consisted of two separable com-
ponents. First, we provided a framework that strips
key MT problems down to their essence so students
could focus on understanding classic algorithms or
invent new ones. Second, we designed incentives
that motivated them to improve their solutions as
much as possible, encouraging experimentation with
approaches beyond what we taught in class.
2.1 Decoding, Reranking, Evaluation, and
Alignment for MT (DREAMT)
We designed four assignments, each corresponding
to a real subproblem in MT: alignment, decoding,
evaluation, and reranking.1 From the more general
perspective of AI, they emphasize the key problems
of unsupervised learning, search, evaluation design,
and supervised learning, respectively. In real MT
systems, these problems are highly interdependent,
a point we emphasized in class and at the end of each
assignment?for example, that alignment is an exer-
cise in parameter estimation for translation models,
that model choice is a tradeoff between expressivity
and efficient inference, and that optimal search does
not guarantee optimal accuracy. However, present-
ing each problem independently and holding all else
constant enables more focused exploration.
For each problem we provided data, a na??ve solu-
tion, and an evaluation program. Following Bird et
al. (2008) and Madnani and Dorr (2008), we imple-
mented the challenges in Python, a high-level pro-
1http://alopez.github.io/dreamt
gramming language that can be used to write very
concise programs resembling pseudocode.2,3 By de-
fault, each baseline system reads the test data and
generates output in the evaluation format, so setup
required zero configuration, and students could be-
gin experimenting immediately. For example, on re-
ceipt of the alignment code, aligning data and eval-
uating results required only typing:
> align | grade
Students could then run experiments within minutes
of beginning the assignment.
Three of the four challenges also included unla-
beled test data (except the decoding assignment, as
explained in ?4). We evaluated test results against a
hidden key when assignments were submitted.
2.2 Incentive Design
We wanted to balance several pedagogical goals: un-
derstanding of classic algorithms, free exploration
of alternatives, experience with typical experimental
design, and unhindered collaboration.
Machine translation is far from solved, so we ex-
pected more than reimplementation of prescribed al-
gorithms; we wanted students to really explore the
problems. To motivate exploration, we made the as-
signments competitive. Competition is a powerful
force, but must be applied with care in an educa-
tional setting.4 We did not want the consequences
of ambitious but failed experiments to be too dire,
and we did not want to discourage collaboration.
For each assignment, we guaranteed a passing
grade for matching the performance of a specific tar-
get alorithm. Typically, the target was important
but not state-of-the-art: we left substantial room for
improvement, and thus competition. We told stu-
dents the exact algorithm that produced the target ac-
curacy (though we expected them to derive it them-
selves based on lectures, notes, or literature). We
did not specifically require them to implement it, but
the guarantee of a passing grade provided a power-
ful incentive for this to be the first step of each as-
signment. Submissions that beat this target received
additional credit. The top five submissions received
full credit, while the top three received extra credit.
2http://python.org
3Some well-known MT systems have been implemented in
Python (Chiang, 2007; Huang and Chiang, 2007).
4Thanks to an anonymous reviewer for this turn of phrase.
166
This scheme provided strong incentive to continue
experimentation beyond the target alorithm.5
For each assignment, students could form teams
of any size, under three rules: each team had to pub-
licize its formation to the class, all team members
agreed to receive the same grade, and teams could
not drop members. Our hope was that these require-
ments would balance the perceived competitive ad-
vantage of collaboration against a reluctance to take
(and thus support) teammates who did not contribute
to the competitive effort.6 This strategy worked: out
of sixteen students, ten opted to work collaboratively
on at least one assignment, always in pairs.
We provided a web-based leaderboard that dis-
played standings on the test data in real time, iden-
tifying each submission by a pseudonymous han-
dle known only to the team and instructors. Teams
could upload solutions as often as they liked before
the assignment deadline. The leaderboard displayed
scores of the default and target alorithms. This in-
centivized an early start, since teams could verify
for themselves when they met the threshold for a
passing grade. Though effective, it also detracted
from realism in one important way: it enabled hill-
climbing on the evaluation metric. In early assign-
ments, we observed a few cases of this behavior,
so for the remaining assignments, we modified the
leaderboard so that changes in score would only be
reflected once every twelve hours. This strategy
trades some amount of scientific realism for some
measure of incentive, a strategy that has proven
effective in other pedagogical tools with real-time
feedback (Spacco et al, 2006).
To obtain a grade, teams were required to sub-
mit their results, share their code privately with the
instructors, and publicly describe their experimen-
tal process to the class so that everyone could learn
from their collective effort. Teams were free (but not
required) to share their code publicly at any time.
5Grades depend on institutional norms. In our case, high grades
in the rest of class combined with matching all assignment tar-
get alorithms would earn a B+; beating two target alorithms
would earn an A-; top five placement on any assignment would
earn an A; and top three placement compensated for weaker
grades in other course criteria. Everyone who completed all
four assignments placed in the top five at least once.
6The equilibrium point is a single team, though this team would
still need to decide on a division of labor. One student contem-
plated organizing this team, but decided against it.
Some did so after the assignment deadline.
3 The Alignment Challenge
The first challenge was word alignment: given a par-
allel text, students were challenged to produce word-
to-word alignments with low alignment error rate
(AER; Och and Ney, 2000). This is a variant of a
classic assignment not just in MT, but in NLP gen-
erally. Klein (2005) describes a version of it, and we
know several other instructors who use it.7 In most
of these, the object is to implement IBM Model 1
or 2, or a hidden Markov model. Our version makes
it open-ended by asking students to match or beat an
IBM Model 1 baseline.
3.1 Data
We provided 100,000 sentences of parallel data from
the Canadian Hansards, totaling around two million
words.8 This dataset is small enough to align in
a few minutes with our implementation?enabling
rapid experimentation?yet large enough to obtain
reasonable results. In fact, Liang et al (2006) report
alignment accuracy on data of this size that is within
a fraction of a point of their accuracy on the com-
plete Hansards data. To evaluate, we used manual
alignments of a small fraction of sentences, devel-
oped by Och and Ney (2000), which we obtained
from the shared task resources organized by Mihal-
cea and Pedersen (2003). The first 37 sentences
of the corpus were development data, with manual
alignments provided in a separate file. Test data con-
sisted of an additional 447 sentences, for which we
did not provide alignments.9
3.2 Implementation
We distributed three Python programs with the
data. The first, align, computes Dice?s coefficient
(1945) for every pair of French and English words,
then aligns every pair for which its value is above an
adjustable threshold. Our implementation (most of
7Among them, Jordan Boyd-Graber, John DeNero, Philipp
Koehn, and Slav Petrov (personal communication).
8http://www.isi.edu/natural-language/download/hansard/
9This invited the possibility of cheating, since alignments of the
test data are publicly available on the web. We did not adver-
tise this, but as an added safeguard we obfuscated the data by
distributing the test sentences randomly throughout the file.
167
Listing 1 The default aligner in DREAMT: thresh-
olding Dice?s coefficient.
for (f, e) in bitext:
for f_i in set(f):
f_count[f_i] += 1
for e_j in set(e):
fe_count[(f_i,e_j)] += 1
for e_j in set(e):
e_count[e_j] += 1
for (f_i, e_j) in fe_count.keys():
dice[(f_i,e_j)] = \
2.0 * fe_count[(f_i, e_j)] / \
(f_count[f_i] + e_count[e_j])
for (f, e) in bitext:
for (i, f_i) in enumerate(f):
for (j, e_j) in enumerate(e):
if dice[(f_i,e_j)] >= cutoff:
print "%i-%i " % (i,j)
which is shown in Listing 1) is quite close to pseu-
docode, making it easy to focus on the algorithm,
one of our pedagogical goals. The grade program
computes AER and optionally prints an alignment
grid for sentences in the development data, showing
both human and automatic alignments. Finally the
check program verifies that the results represent
a valid solution, reporting an error if not?enabling
students to diagnose bugs in their submissions.
The default implementation enabled immediate
experimentation. On receipt of the code, students
were instructed to align the first 1,000 sentences and
compute AER using a simple command.
> align -n 1000 | grade By varying the
number of input sentences and the threshold for an
alignment, students could immediately see the effect
of various parameters on alignment quality.
We privately implemented IBM Model 1 (Brown
et al, 1993) as the target alorithm for a passing
grade. We ran it for five iterations with English
as the target language and French as the source.
Our implementation did not use null alignment
or symmetrization?leaving out these common im-
provements offered students the possibility of dis-
covering them independently, and thereby rewarded.
A
E
R
?
10
0
20
30
40
50
60
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 1: Submission history for the alignment challenge.
Dashed lines represent the default and baseline system
performance. Each colored line represents a student, and
each dot represents a submission. For clarity, we show
only submissions that improved the student?s AER.
3.3 Challenge Results
We received 209 submissions from 11 teams over a
period of two weeks (Figure 1). Everyone eventually
matched or exceeded IBM Model 1 AER of 31.26.
Most students implemented IBM Model 1, but we
saw many other solutions, indicating that many truly
experimented with the problem:
? Implementing heuristic constraints to require
alignment of proper names and punctuation.
? Running the algorithm on stems rather than sur-
face words.
? Initializing the first iteration of Model 1 with
parameters estimated on the observed align-
ments in the development data.
? Running Model 1 for many iterations. Most re-
searchers typically run Model 1 for five itera-
tions or fewer, and there are few experiments
in the literature on its behavior over many iter-
ations, as there are for hidden Markov model
taggers (Johnson, 2007). Our students carried
out these experiments, reporting runs of 5, 20,
100, and even 2000 iterations. No improve-
ment was observed after 20 iterations.
168
? Implementing various alternative approaches
from the literature, including IBM Model 2
(Brown et al, 1993), competitive linking
(Melamed, 2000), and smoothing (Moore,
2004).
One of the best solutions was competitive linking
with Dice?s coefficient, modified to incorporate the
observation that alignments tend to be monotonic by
restricting possible alignment points to a window of
eight words around the diagonal. Although simple,
it acheived an AER of 18.41, an error reduction over
Model 1 of more than 40%.
The best score compares unfavorably against a
state-of-the-art AER of 3.6 (Liu et al, 2010). But
under a different view, it still represents a significant
amount of progress for an effort taking just over two
weeks: on the original challenge from which we ob-
tained the data (Mihalcea and Pedersen, 2003) the
best student system would have placed fifth out of
fifteen systems. Consider also the combined effort of
all the students: when we trained a perceptron clas-
sifier on the development data, taking each student?s
prediction as a feature, we obtained an AER of 15.4,
which would have placed fourth on the original chal-
lenge. This is notable since none of the systems
incorporated first-order dependencies on the align-
ments of adjacent words, long noted as an impor-
tant feature of the best alignment models (Och and
Ney, 2003). Yet a simple system combination of stu-
dent assignments is as effective as a hidden Markov
Model trained on a comparable amount of data (Och
and Ney, 2003).
It is important to note that AER does not neces-
sarily correlate with downstream performance, par-
ticularly on the Hansards dataset (Fraser and Marcu,
2007). We used the conclusion of the assignment as
an opportunity to emphasize this point.
4 The Decoding Challenge
The second challenge was decoding: given a fixed
translation model and a set of input sentences, stu-
dents were challenged to produce translations with
the highest model score. This challenge introduced
the difficulties of combinatorial optimization under
a deceptively simple setup: the model we provided
was a simple phrase-based translation model (Koehn
et al, 2003) consisting only of a phrase table and tri-
gram language model. Under this simple model, for
a French sentence f of length I , English sentence
e of length J , and alignment a where each element
consists of a span in both e and f such that every
word in both e and f is aligned exactly once, the
conditional probability of e and a given f is as fol-
lows.10
p(e, a|f) =
?
?i,i?,j,j???a
p(f i?i |ej
?
j )
J+1?
j=1
p(ej |ej?1, ej?2)
(1)
To evaluate output, we compute the conditional
probability of e as follows.
p(e|f) =
?
a
p(e, a|f) (2)
Note that this formulation is different from the typ-
ical Viterbi objective of standard beam search de-
coders, which do not sum over all alignments, but
approximate p(e|f) by maxa p(e, a|f). Though the
computation in Equation 2 is intractable (DeNero
and Klein, 2008), it can be computed in a few min-
utes via dynamic programming on reasonably short
sentences. We ensured that our data met this crite-
rion. The corpus-level probability is then the prod-
uct of all sentence-level probabilities in the data.
The model includes no distortion limit or distor-
tion model, for two reasons. First, leaving out the
distortion model slightly simplifies the implementa-
tion, since it is not necessary to keep track of the last
word translated in a beam decoder; we felt that this
detail was secondary to understanding the difficulty
of search over phrase permutations. Second, it actu-
ally makes the problem more difficult, since a simple
distance-based distortion model prefers translations
with fewer permutations; without it, the model may
easily prefer any permutation of the target phrases,
making even the Viterbi search problem exhibit its
true NP-hardness (Knight, 1999a; Zaslavskiy et al,
2009).
Since the goal was to find the translation with the
highest probability, we did not provide a held-out
test set; with access to both the input sentences and
10For simplicity, this formula assumes that e is padded with two
sentence-initial symbols and one sentence-final symbol, and
ignores the probability of sentence segmentation, which we
take to be uniform.
169
the model, students had enough information to com-
pute the evaluation score on any dataset themselves.
The difficulty of the challenge lies simply in finding
the translation that maximizes the evaluation. In-
deed, since the problem is intractable, even the in-
structors did not know the true solution.11
4.1 Data
We chose 48 French sentences totaling 716 words
from the Canadian Hansards to serve as test data.
To create a simple translation model, we used the
Berkeley aligner to align the parallel text from the
first assignment, and extracted a phrase table using
the method of Lopez (2007), as implemented in cdec
(Dyer et al, 2010). To create a simple language
model, we used SRILM (Stolcke, 2002).
4.2 Implementation
We distributed two Python programs. The first,
decode, decodes the test data monotonically?
using both the language model and translation
model, but without permuting phrases. The imple-
mentation is completely self-contained with no ex-
ternal dependencies: it implements both models and
a simple stack decoding algorithm for monotonic
translation. It contains only 122 lines of Python?
orders of magnitude fewer than most full-featured
decoders. To see its similarity to pseudocode, com-
pare the decoding algorithm (Listing 2) with the
pseudocode in Koehn?s (2010) popular textbook (re-
produced here as Algorithm 1). The second pro-
gram, grade, computes the log-probability of a set
of translations, as outline above.
We privately implemented a simple stack decoder
that searched over permutations of phrases, similar
to Koehn (2004). Our implementation increased the
codebase by 44 lines of code and included param-
eters for beam size, distortion limit, and the maxi-
mum number of translations considered for each in-
put phrase. We posted a baseline to the leaderboard
using values of 50, 3, and 20 for these, respectively.
11We implemented a version of the Lagrangian relaxation algo-
rithm of Chang and Collins (2011), but found it difficult to
obtain tight (optimal) solutions without iteratively reintroduc-
ing all of the original constraints. We suspect this is due to
the lack of a distortion penalty, which enforces a strong pref-
erence towards translations with little reordering. However,
the solution found by this algorithm is only approximates the
objective implied by Equation 2, which sums over alignments.
We also posted an oracle containing the most prob-
able output for each sentence, selected from among
all submissions received so far. The intent of this
oracle was to provide a lower bound on the best pos-
sible output, giving students additional incentive to
continue improving their systems.
4.3 Challenge Results
We received 71 submissions from 10 teams (Fig-
ure 2), again exhibiting variety of solutions.
? Implementation of greedy decoder which at
each step chooses the most probable translation
from among those reachable by a single swap
or retranslation (Germann et al, 2001; Langlais
et al, 2007).
? Inclusion of heuristic estimates of future cost.
? Implementation of a private oracle. Some stu-
dents observed that the ideal beam setting was
not uniform across the corpus. They ran their
decoder under different settings, and then se-
lected the most probable translation of each
sentence.
Many teams who implemented the standard stack
decoding algorithm experimented heavily with its
pruning parameters. The best submission used ex-
tremely wide beam settings in conjunction with a
reimplementation of the future cost estimate used in
Moses (Koehn et al, 2007). Five of the submissions
beat Moses using its standard beam settings after it
had been configured to decode with our model.
We used this assignment to emphasize the im-
portance of good models: the model score of the
submissions was generally inversely correlated with
BLEU, possibly because our simple model had no
distortion limits. We used this to illustrate the differ-
ence between model error and search error, includ-
ing fortuitous search error (Germann et al, 2001)
made by decoders with less accurate search.
5 The Evaluation Challenge
The third challenge was evaluation: given a test cor-
pus with reference translations and the output of sev-
eral MT systems, students were challenged to pro-
duce a ranking of the systems that closely correlated
with a human ranking.
170
Listing 2 The default decoder in DREAMT: a stack decoder for monotonic translation.
stacks = [{} for _ in f] + [{}]
stacks[0][lm.begin()] = initial_hypothesis
for i, stack in enumerate(stacks[:-1]):
for h in sorted(stack.itervalues(),key=lambda h: -h.logprob)[:alpha]:
for j in xrange(i+1,len(f)+1):
if f[i:j] in tm:
for phrase in tm[f[i:j]]:
logprob = h.logprob + phrase.logprob
lm_state = h.lm_state
for word in phrase.english.split():
(lm_state, word_logprob) = lm.score(lm_state, word)
logprob += word_logprob
logprob += lm.end(lm_state) if j == len(f) else 0.0
new_hypothesis = hypothesis(logprob, lm_state, h, phrase)
if lm_state not in stacks[j] or \
stacks[j][lm_state].logprob < logprob:
stacks[j][lm_state] = new_hypothesis
winner = max(stacks[-1].itervalues(), key=lambda h: h.logprob)
def extract_english(h):
return "" if h.predecessor is None else "%s%s " %
(extract_english(h.predecessor), h.phrase.english)
print extract_english(winner)
Algorithm 1 Basic stack decoding algorithm,
adapted from Koehn (2010), p. 165.
place empty hypothesis into stack 0
for all stacks 0...n? 1 do
for all hypotheses in stack do
for all translation options do
if applicable then
create new hypothesis
place in stack
recombine with existing hypothesis
prune stack if too big
5.1 Data
We chose the English-to-German translation sys-
tems from the 2009 and 2011 shared task at the an-
nual Workshop for Machine Translation (Callison-
Burch et al, 2009; Callison-Burch et al, 2011), pro-
viding the first as development data and the second
as test data. We chose these sets because BLEU
(Papineni et al, 2002), our baseline metric, per-
formed particularly poorly on them; this left room
for improvement in addition to highlighting some
lo
g 1
0
p(
e|f
)?
C
-1200
-1250
-1300
-1350
-1400
-20
days
-18
days
-16
days
-14
days
-12
days
-10
days
-8
days
-6
days
-4
days
-2
days
due
Figure 2: Submission history for the decoding challenge.
The dotted green line represents the oracle over submis-
sions.
deficiencies of BLEU. For each dataset we pro-
vided the source and reference sentences along with
anonymized system outputs. For the development
data we also provided the human ranking of the sys-
171
tems, computed from pairwise human judgements
according to a formula recommended by Bojar et al
(2011).12
5.2 Implementation
We provided three simple Python programs:
evaluate implements a simple ranking of the sys-
tems based on position-independent word error rate
(PER; Tillmann et al, 1997), which computes a bag-
of-words overlap between the system translations
and the reference. The grade program computes
Spearman?s ? between the human ranking and an
output ranking. The check program simply ensures
that a submission contains a valid ranking.
We were concerned about hill-climbing on the test
data, so we modified the leaderboard to report new
results only twice a day. This encouraged students to
experiment on the development data before posting
new submissions, while still providing intermittent
feedback.
We privately implemented a version of BLEU,
which obtained a correlation of 38.6 with the human
rankings, a modest improvement over the baseline
of 34.0. Our implementation underperforms the one
reported in Callison-Burch et al (2011) since it per-
forms no tokenization or normalization of the data.
This also left room for improvement.
5.3 Evaluation Challenge Results
We received 212 submissions from 12 teams (Fig-
ure 3), again demonstrating a wide range of tech-
niques.
? Experimentation with the maximum n-gram
length and weights in BLEU.
? Implementation of smoothed versions of BLEU
(Lin and Och, 2004).
? Implementation of weighted F-measure to bal-
ance both precision and recall.
? Careful normalization of the reference and ma-
chine translations, including lowercasing and
punctuation-stripping.
12This ranking has been disputed over a series of papers (Lopez,
2012; Callison-Burch et al, 2012; Koehn, 2012). The paper
which initiated the dispute, written by the first author, was di-
rectly inspired by the experience of designing this assignment.
Sp
ea
rm
an
?s
?
0.8
0.6
0.4
-7
days
-6
days
-5
days
-4
days
-3
days
-2
days
-1
days
due
Figure 3: Submission history for the evaluation chal-
lenge.
? Implementation of several techniques used in
AMBER (Chen and Kuhn, 2005).
The best submission, obtaining a correlation of
83.5, relied on the idea that the reference and ma-
chine translation should be good paraphrases of each
other (Owczarzak et al, 2006; Kauchak and Barzi-
lay, 2006). It employed a simple paraphrase sys-
tem trained on the alignment challenge data, us-
ing the pivot technique of Bannard and Callison-
Burch (2005), and computing the optimal alignment
between machine translation and reference under a
simple model in which words could align if they
were paraphrases. When compared with the 20
systems submitted to the original task from which
the data was obtained (Callison-Burch et al, 2011),
this system would have ranked fifth, quite near the
top-scoring competitors, whose correlations ranged
from 88 to 94.
6 The Reranking Challenge
The fourth challenge was reranking: given a test cor-
pus and a large N -best list of candidate translations
for each sentence, students were challenged to select
a candidate translation for each sentence to produce
a high corpus-level BLEU score. Due to an error
our data preparation, this assignment had a simple
solution that was very difficult to improve on. Nev-
ertheless, it featured several elements that may be
useful for future courses.
172
6.1 Data
We obtained 300-best lists from a Spanish-English
translation system built with the Joshua toolkit
(Ganitkevitch et al, 2012) using data and resources
from the 2011 Workshop on Machine Translation
(Callison-Burch et al, 2011). We provided 1989
training sentences, consisting of source and refer-
ence sentences along with the candidate translations.
We also included a test set of 250 sentences, for
which we provided only the source and candidate
translations. Each candidate translation included six
features from the underlying translation system, out
of an original 21; our hope was that students might
rediscover some features through experimentation.
6.2 Implementation
We conceived of the assignment as one in which stu-
dents could apply machine learning or feature engi-
neering to the task of reranking the systems, so we
provided several tools. The first of these, learn,
was a simple program that produced a vector of
feature weights using pairwise ranking optimization
(PRO; Hopkins and May, 2011), with a perceptron
as the underlying learning algorithm. A second,
rerank, takes a weight vector as input and reranks
the sentences; both programs were designed to work
with arbitrary numbers of features. The grade pro-
gram computed the BLEU score on development
data, while check ensured that a test submission
is valid. Finally, we provided an oracle program,
which computed a lower bound on the achievable
BLEU score on the development data using a greedy
approximation (Och et al, 2004). The leaderboard
likewise displayed an oracle on test data. We did
not assign a target alorithm, but left the assignment
fully open-ended.
6.3 Reranking Challenge Outcome
For each assignment, we made an effort to create
room for competition above the target alorithm.
However, we did not accomplish this in the rerank-
ing challenge: we had removed most of the features
from the candidate translations, in hopes that stu-
dents might reinvent some of them, but we left one
highly predictive implicit feature in the data: the
rank order of the underlying translation system. Stu-
dents discovered that simply returning the first can-
didate earned a very high score, and most of them
quickly converged to this solution. Unfortunately,
the high accuracy of this baseline left little room for
additional competition. Nevertheless, we were en-
couraged that most students discovered this by acci-
dent while attempting other strategies to rerank the
translations.
? Experimentation with parameters of the PRO
algorithm.
? Substitution of alternative learning algorithms.
? Implementation of a simplified minimum
Bayes risk reranker (Kumar and Byrne, 2004).
Over a baseline of 24.02, the latter approach ob-
tained a BLEU of 27.08, nearly matching the score
of 27.39 from the underlying system despite an im-
poverished feature set.
7 Pedagogical Outcomes
Could our students have obtained similar results by
running standard toolkits? Undoubtedly. However,
our goal was for students to learn by doing: they
obtained these results by implementing key MT al-
gorithms, observing their behavior on real data, and
improving them. This left them with much more in-
sight into how MT systems actually work, and in
this sense, DREAMT was a success. At the end of
class, we requested written feedback on the design
of the assignments. Many commented positively on
the motivation provided by the challenge problems:
? The immediate feedback of the automatic grad-
ing was really nice.
? Fast feedback on my submissions and my rela-
tive position on the leaderboard kept me both
motivated to start the assignments early and to
constantly improve them. Also knowing how
well others were doing was a good way to
gauge whether I was completely off track or not
when I got bad results.
? The homework assignments were very engag-
ing thanks to the clear yet open-ended setup
and their competitive aspects.
Students also commented that they learned a lot
about MT and even research in general:
173
Question 1 2 3 4 5 N/A
Feedback on my work for this course is useful - - - 4 9 3
This course enhanced my ability to work effectively in a team 1 - 5 8 2 -
Compared to other courses at this level, the workload for this course is high - 1 7 6 1 1
Table 1: Response to student survey questions on a Likert scale from 1 (strongly disagree) to 5 (strongly agree).
? I learned the most from the assignments.
? The assignments always pushed me one step
more towards thinking out loud how the par-
ticular task can be completed.
? I appreciated the setup of the homework prob-
lems. I think it has helped me learn how to
set up and attack research questions in an or-
ganized way. I have a much better sense for
what goes into an MT system and what prob-
lems aren?t solved.
We also received feedback through an anonymous
survey conducted at the end of the course before
posting final grades. Each student rated aspects
of the course on a five point Likert scale, from 1
(strongly disagree) to 5 (strongly agree). Several
questions pertained to assignments (Table 1), and al-
lay two possible concerns about competition: most
students felt that the assignments enhanced their col-
laborative skills, and that their open-endedness did
not result in an overload of work. For all survey
questions, student satisfaction was higher than av-
erage for courses in our department.
8 Discussion
DREAMT is inspired by several different ap-
proaches to teaching NLP, AI, and computer sci-
ence. Eisner and Smith (2008) teach NLP using
a competitive game in which students aim to write
fragments of English grammar. Charniak et al
(2000) improve the state-of-the-art in a reading com-
prehension task as part of a group project. Christo-
pher et al (1993) use NACHOS, a classic tool for
teaching operating systems by providing a rudimen-
tary system that students then augment. DeNero and
Klein (2010) devise a series of assignments based
on Pac-Man, for which students implement several
classic AI techniques. A crucial element in such ap-
proaches is a highly functional but simple scaffold-
ing. The DREAMT codebase, including grading and
validation scripts, consists of only 656 lines of code
(LOC) over four assignments: 141 LOC for align-
ment, 237 LOC for decoding, 86 LOC for evalua-
tion, and 192 LOC for reranking. To simplify imple-
mentation further, the optional leaderboard could be
delegated to Kaggle.com, a company that organizes
machine learning competitions using a model sim-
ilar to the Netflix Challenge (Bennet and Lanning,
2007), and offers pro bono use of its services for
educational challenge problems. A recent machine
learning class at Oxford hosted its assignments on
Kaggle (Phil Blunsom, personal communication).
We imagine other uses of DREAMT. It could be
used in an inverted classroom, where students view
lecture material outside of class and work on prac-
tical problems in class. It might also be useful in
massive open online courses (MOOCs). In this for-
mat, course material (primarily lectures and quizzes)
is distributed over the internet to an arbitrarily large
number of interested students through sites such as
coursera.org, udacity.com, and khanacademy.org. In
many cases, material and problem sets focus on spe-
cific techniques. Although this is important, there is
also a place for open-ended problems on which stu-
dents apply a full range of problem-solving skills.
Automatic grading enables them to scale easily to
large numbers of students.
On the scientific side, the scale of MOOCs might
make it possible to empirically measure the effec-
tiveness of hands-on or competitive assignments,
by comparing course performance of students who
work on them against that of those who do not.
Though there is some empirical work on competi-
tive assignments in the computer science education
literature (Lawrence, 2004; Garlick and Akl, 2006;
Regueras et al, 2008; Ribeiro et al, 2009), they
generally measure student satisfaction and retention
rather than the more difficult question of whether
such assignments actually improve student learning.
However, it might be feasible to answer such ques-
174
tions in large, data-rich virtual classrooms offered
by MOOCs. This is an interesting potential avenue
for future work.
Because our class came within reach of state-of-
the-art on each problem within a matter of weeks,
we wonder what might happen with a very large
body of competitors. Could real innovation oc-
cur? Could we solve large-scale problems? It may
be interesting to adopt a different incentive struc-
ture, such as one posed by Abernethy and Frongillo
(2011) for crowdsourcing machine learning prob-
lems: rather than competing, everyone works to-
gether to solve a shared task, with credit awarded
proportional to the contribution that each individual
makes. In this setting, everyone stands to gain: stu-
dents learn to solve problems as they are found in
the real world, instructors learn new insights into the
problems they pose, and, in the long run, users of
AI technology benefit from overall improvements.
Hence it is possible that posing open-ended, real-
world problems to students might be a small piece
of the puzzle of providing high-quality NLP tech-
nologies.
Acknowledgments
We are grateful to Colin Cherry and Chris Dyer
for testing the assignments in different settings and
providing valuable feedback, and to Jessie Young
for implementing a dual decomposition solution to
the decoding assignment. We thank Jason Eis-
ner, Frank Ferraro, Yoav Goldberg, Matt Gormley,
Ann Irvine, Rebecca Knowles, Ben Mitchell, Court-
ney Napoles, Michael Rushanan, Joanne Selinski,
Svitlana Volkova, and the anonymous reviewers for
lively discussion and helpful comments on previous
drafts of this paper. Any errors are our own.
References
J. Abernethy and R. M. Frongillo. 2011. A collaborative
mechanism for crowdsourcing prediction problems. In
Proc. of NIPS.
C. Bannard and C. Callison-Burch. 2005. Paraphrasing
with bilingual parallel corpora. In Proc. of ACL.
J. Bennet and S. Lanning. 2007. The netflix prize. In
Proc. of the KDD Cup and Workshop.
S. Bird, E. Klein, E. Loper, and J. Baldridge. 2008.
Multidisciplinary instruction with the natural language
toolkit. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
O. Bojar, M. Ercegovc?evic?, M. Popel, and O. Zaidan.
2011. A grain of salt for the WMT manual evaluation.
In Proc. of WMT.
P. E. Brown, S. A. D. Pietra, V. J. D. Pietra, and R. L.
Mercer. 1993. The mathematics of statistical machine
translation: Parameter estimation. Computational Lin-
guistics, 19(2).
C. Callison-Burch, P. Koehn, C. Monz, and J. Schroeder.
2009. Findings of the 2009 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, and O. Zaidan.
2011. Findings of the 2011 workshop on statistical
machine translation. In Proc. of WMT.
C. Callison-Burch, P. Koehn, C. Monz, M. Post, R. Sori-
cut, and L. Specia. 2012. Findings of the 2012 work-
shop on statistical machine translation. In Proc. of
WMT.
Y.-W. Chang and M. Collins. 2011. Exact decoding of
phrase-based translation models through Lagrangian
relaxation. In Proc. of EMNLP.
E. Charniak, Y. Altun, R. de Salvo Braz, B. Garrett,
M. Kosmala, T. Moscovich, L. Pang, C. Pyo, Y. Sun,
W. Wy, Z. Yang, S. Zeiler, and L. Zorn. 2000. Read-
ing comprehension programs in a statistical-language-
processing class. In Proc. of Workshop on Read-
ing Comprehension Tests as Evaluation for Computer-
Based Language Understanding Systems.
B. Chen and R. Kuhn. 2005. AMBER: A modified
BLEU, enhanced ranking metric. In Proc. of WMT.
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
W. A. Christopher, S. J. Procter, and T. E. Anderson.
1993. The nachos instructional operating system. In
Proc. of USENIX.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. of ACL.
J. DeNero and D. Klein. 2010. Teaching introductory
articial intelligence with Pac-Man. In Proc. of Sym-
posium on Educational Advances in Artificial Intelli-
gence.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297?302.
C. Dyer, A. Lopez, J. Ganitkevitch, J. Weese, F. Ture,
P. Blunsom, H. Setiawan, V. Eidelman, and P. Resnik.
2010. cdec: A decoder, alignment, and learning
framework for finite-state and context-free translation
models. In Proc. of ACL.
J. Eisner and N. A. Smith. 2008. Competitive grammar
writing. In Proc. of Workshop on Issues in Teaching
Computational Linguistics.
175
A. Fraser and D. Marcu. 2007. Measuring word align-
ment quality for statistical machine translation. Com-
putational Linguistics, 33(3).
J. Ganitkevitch, Y. Cao, J. Weese, M. Post, and
C. Callison-Burch. 2012. Joshua 4.0: Packing, PRO,
and paraphrases. In Proc. of WMT.
R. Garlick and R. Akl. 2006. Intra-class competitive
assignments in CS2: A one-year study. In Proc. of
International Conference on Engineering Education.
U. Germann, M. Jahr, K. Knight, D. Marcu, and K. Ya-
mada. 2001. Fast decoding and optimal decoding for
machine translation. In Proc. of ACL.
L. Huang and D. Chiang. 2007. Forest rescoring: Faster
decoding with integrated language models. In Proc. of
ACL.
M. Johnson. 2007. Why doesn?t EM find good HMM
POS-taggers? In Proc. of EMNLP.
D. Jurafsky and J. H. Martin. 2009. Speech and Lan-
guage Processing. Prentice Hall, 2nd edition.
D. Kauchak and R. Barzilay. 2006. Paraphrasing for
automatic evaluation. In Proc. of HLT-NAACL.
D. Klein. 2005. A core-tools statistical NLP course. In
Proc. of Workshop on Effective Tools and Methodolo-
gies for Teaching NLP and CL.
K. Knight. 1999a. Decoding complexity in word-
replacement translation models. Computational Lin-
guistics, 25(4).
K. Knight. 1999b. A statistical MT tutorial workbook.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of NAACL.
P. Koehn, H. Hoang, A. Birch, C. Callison-Burch,
M. Federico, N. Bertoldi, B. Cowan, W. Shen,
C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,
and E. Herbst. 2007. Moses: Open source toolkit for
statistical machine translation. In Proc. of ACL.
P. Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models.
In Proc. of AMTA.
P. Koehn. 2010. Statistical Machine Translation. Cam-
bridge University Press.
P. Koehn. 2012. Simulating human judgment in machine
translation evaluation campaigns. In Proc. of IWSLT.
S. Kumar and W. Byrne. 2004. Minimum bayes-risk
decoding for statistical machine translation. In Proc.
of HLT-NAACL.
P. Langlais, A. Patry, and F. Gotti. 2007. A greedy de-
coder for phrase-based statistical machine translation.
In Proc. of TMI.
R. Lawrence. 2004. Teaching data structures using
competitive games. IEEE Transactions on Education,
47(4).
P. Liang, B. Taskar, and D. Klein. 2006. Alignment by
agreement. In Proc. of NAACL.
C.-Y. Lin and F. J. Och. 2004. ORANGE: a method for
evaluating automatic evaluation metrics for machine
translation. In Proc. of COLING.
Y. Liu, Q. Liu, and S. Lin. 2010. Discriminative word
alignment by linear modeling. Computational Lin-
guistics, 36(3).
A. Lopez. 2007. Hierarchical phrase-based translation
with suffix arrays. In Proc. of EMNLP.
A. Lopez. 2008. Statistical machine translation. ACM
Computing Surveys, 40(3).
A. Lopez. 2012. Putting human assessments of machine
translation systems in order. In Proc. of WMT.
N. Madnani and B. Dorr. 2008. Combining open-source
with research to re-engineer a hands-on introductory
NLP course. In Proc. of Workshop on Issues in Teach-
ing Computational Linguistics.
I. D. Melamed. 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2).
R. Mihalcea and T. Pedersen. 2003. An evaluation ex-
ercise for word alignment. In Proc. on Workshop on
Building and Using Parallel Texts.
R. C. Moore. 2004. Improving IBM word alignment
model 1. In Proc. of ACL.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proc. of ACL.
F. J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29.
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of
features for statistical machine translation. In Proc. of
NAACL.
K. Owczarzak, D. Groves, J. V. Genabith, and A. Way.
2006. Contextual bitext-derived paraphrases in auto-
matic MT evaluation. In Proc. of WMT.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proc. of ACL.
L. Regueras, E. Verdu?, M. Verdu?, M. Pe?rez, J. de Castro,
and M. Mun?oz. 2008. Motivating students through
on-line competition: An analysis of satisfaction and
learning styles.
P. Ribeiro, M. Ferreira, and H. Simo?es. 2009. Teach-
ing artificial intelligence and logic programming in a
competitive environment. Informatics in Education,
(Vol 8 1):85.
J. Spacco, D. Hovemeyer, W. Pugh, J. Hollingsworth,
N. Padua-Perez, and F. Emad. 2006. Experiences with
marmoset: Designing and using an advanced submis-
sion and testing system for programming courses. In
Proc. of Innovation and technology in computer sci-
ence education.
176
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. of ICSLP.
C. Tillmann, S. Vogel, H. Ney, A. Zubiaga, and H. Sawaf.
1997. Accelerated DP based search for statistical
translation. In Proc. of European Conf. on Speech
Communication and Technology.
M. Zaslavskiy, M. Dymetman, and N. Cancedda. 2009.
Phrase-based statistical machine translation as a trav-
eling salesman problem. In Proc. of ACL.
177
178
The Language Demographics of Amazon Mechanical Turk
Ellie Pavlick1 Matt Post2 Ann Irvine2 Dmitry Kachaev2 Chris Callison-Burch1,2
1Computer and Information Science Department, University of Pennsylvania
2Human Language Technology Center of Excellence, Johns Hopkins University
Abstract
We present a large scale study of the languages
spoken by bilingual workers on Mechanical
Turk (MTurk). We establish a methodology
for determining the language skills of anony-
mous crowd workers that is more robust than
simple surveying. We validate workers? self-
reported language skill claims by measuring
their ability to correctly translate words, and
by geolocating workers to see if they reside in
countries where the languages are likely to be
spoken. Rather than posting a one-off survey,
we posted paid tasks consisting of 1,000 as-
signments to translate a total of 10,000 words
in each of 100 languages. Our study ran
for several months, and was highly visible on
the MTurk crowdsourcing platform, increas-
ing the chances that bilingual workers would
complete it. Our study was useful both to cre-
ate bilingual dictionaries and to act as cen-
sus of the bilingual speakers on MTurk. We
use this data to recommend languages with the
largest speaker populations as good candidates
for other researchers who want to develop
crowdsourced, multilingual technologies. To
further demonstrate the value of creating data
via crowdsourcing, we hire workers to create
bilingual parallel corpora in six Indian lan-
guages, and use them to train statistical ma-
chine translation systems.
1 Overview
Crowdsourcing is a promising new mechanism for
collecting data for natural language processing re-
search. Access to a fast, cheap, and flexible work-
force allows us to collect new types of data, poten-
tially enabling new language technologies. Because
crowdsourcing platforms like Amazon Mechanical
Turk (MTurk) give researchers access to a world-
wide workforce, one obvious application of crowd-
sourcing is the creation of multilingual technologies.
With an increasing number of active crowd workers
located outside of the United States, there is even the
potential to reach fluent speakers of lower resource
languages. In this paper, we investigate the feasi-
bility of hiring language informants on MTurk by
conducting the first large-scale demographic study
of the languages spoken by workers on the platform.
There are several complicating factors when try-
ing to take a census of workers on MTurk. The
workers? identities are anonymized, and Amazon
provides no information about their countries of ori-
gin or their language abilities. Posting a simple sur-
vey to have workers report this information may be
inadequate, since (a) many workers may never see
the survey, (b) many opt not to do one-off surveys
since potential payment is low, and (c) validating the
answers of respondents is not straightforward.
Our study establishes a methodology for deter-
mining the language demographics of anonymous
crowd workers that is more robust than simple sur-
veying. We ask workers what languages they speak
and what country they live in, and validate their
claims by measuring their ability to correctly trans-
late words and by recording their geolocation. To
increase the visibility and the desirability of our
tasks, we post 1,000 assignments in each of 100 lan-
guages. These tasks each consist of translating 10
foreign words into English. Two of the 10 words
have known translations, allowing us to validate that
the workers? translations are accurate. We construct
bilingual dictionaries with up to 10,000 entries, with
the majority of entries being new.
Surveying thousands of workers allows us to ana-
lyze current speaker populations for 100 languages.
79
Transactions of the Association for Computational Linguistics, 2 (2014) 79?92. Action Editor: Mirella Lapata.
Submitted 12/2013; Published 2/2014. c?2014 Association for Computational Linguistics.
11/26/13 turkermap.html
file:///Users/ellie/Documents/Research/turker-demographics/code/src/20130905/paper-rewrite/turkermap.html 1/1
1 1,998
Figure 1: The number of workers per country. This map was generated based on geolocating the IP address
of 4,983 workers in our study. Omitted are 60 workers who were located in more than one country during
the study, and 238 workers who could not be geolocated. The size of the circles represents the number
of workers from each country. The two largest are India (1,998 workers) and the United States (866). To
calibrate the sizes: the Philippines has 142 workers, Egypt has 25, Russia has 10, and Sri Lanka has 4.
The data also allows us to answer questions like:
How quickly is work completed in a given language?
Are crowdsourced translations reliably good? How
often do workers misrepresent their language abili-
ties to obtain financial rewards?
2 Background and Related Work
Amazon?s Mechanical Turk (MTurk) is an on-
line marketplace for work that gives employers
and researchers access to a large, low-cost work-
force. MTurk allows employers to provide micro-
payments in return for workers completing micro-
tasks. The basic units of work on MTurk are called
?Human Intelligence Tasks? (HITs). MTurk was de-
signed to accommodate tasks that are difficult for
computers, but simple for people. This facilitates
research into human computation, where people can
be treated as a function call (von Ahn, 2005; Little et
al., 2009; Quinn and Bederson, 2011). It has appli-
cation to research areas like human-computer inter-
action (Bigham et al., 2010; Bernstein et al., 2010),
computer vision (Sorokin and Forsyth, 2008; Deng
et al., 2010; Rashtchian et al., 2010), speech pro-
cessing (Marge et al., 2010; Lane et al., 2010; Parent
and Eskenazi, 2011; Eskenazi et al., 2013), and natu-
ral language processing (Snow et al., 2008; Callison-
Burch and Dredze, 2010; Laws et al., 2011).
On MTurk, researchers who need work completed
are called ?Requesters?, and workers are often re-
ferred to as ?Turkers?. MTurk is a true market, mean-
ing that Turkers are free to choose to complete the
HITs which interest them, and Requesters can price
their tasks competitively to try to attract workers and
have their tasks done quickly (Faridani et al., 2011;
Singer and Mittal, 2011). Turkers remain anony-
mous to Requesters, and all payment occurs through
Amazon. Requesters are able to accept submitted
work or reject work that does not meet their stan-
dards. Turkers are only paid if a Requester accepts
their work.
Several reports examine Mechanical Turk as an
economic market (Ipeirotis, 2010a; Lehdonvirta and
Ernkvist, 2011). When Amazon introduced MTurk,
it first offered payment only in Amazon credits, and
later offered direct payment in US dollars. More re-
cently, it has expanded to include one foreign cur-
rency, the Indian rupee. Despite its payments be-
ing limited to two currencies or Amazon credits,
MTurk claims over half a million workers from 190
countries (Amazon, 2013). This suggests that its
worker population should represent a diverse set of
languages.
80
A demographic study by Ipeirotis (2010b) fo-
cused on age, gender, martial status, income lev-
els, motivation for working on MTurk, and whether
workers used it as a primary or supplemental form
of income. The study contrasted Indian and US
workers. Ross et al. (2010) completed a longitudi-
nal follow-on study. A number of other studies have
informally investigated Turkers? language abilities.
Munro and Tily (2011) compiled survey responses
of 2,000 Turkers, revealing that four of the six most
represented languages come from India (the top six
being Hindi, Malayalam, Tamil, Spanish, French,
and Telugu). Irvine and Klementiev (2010) had
Turkers evaluate the accuracy of translations that
had been automatically inducted from monolingual
texts. They examined translations of 100 words in
42 low-resource languages, and reported geolocated
countries for their workers (India, the US, Romania,
Pakistan, Macedonia, Latvia, Bangladesh and the
Philippines). Irvine and Klementiev discussed the
difficulty of quality control and assessing the plausi-
bility of workers? language skills for rare languages,
which we address in this paper.
Several researchers have investigated using
MTurk to build bilingual parallel corpora for ma-
chine translation, a task which stands to benefit
low cost, high volume translation on demand (Ger-
mann, 2001). Ambati et al. (2010) conducted a pilot
study by posting 25 sentences to MTurk for Span-
ish, Chinese, Hindi, Telugu, Urdu, and Haitian Cre-
ole. In a study of 2000 Urdu sentences, Zaidan
and Callison-Burch (2011) presented methods for
achieving professional-level translation quality from
Turkers by soliciting multiple English translations
of each foreign sentence. Zbib et al. (2012) used
crowdsourcing to construct a 1.5 million word par-
allel corpus of dialect Arabic and English, train-
ing a statistical machine translation system that pro-
duced higher quality translations of dialect Arabic
than a system a trained on 100 times more Mod-
ern Standard Arabic-English parallel data. Zbib et
al. (2013) conducted a systematic study that showed
that training an MT system on crowdsourced trans-
lations resulted in the same performance as training
on professional translations, at 15 the cost. Hu etal. (2010; Hu et al. (2011) performed crowdsourced
translation by having monolingual speakers collab-
orate and iteratively improve MT output.
English 689 Tamil 253 Malayalam 219
Hindi 149 Spanish 131 Telugu 87
Chinese 86 Romanian 85 Portuguese 82
Arabic 74 Kannada 72 German 66
French 63 Polish 61 Urdu 56
Tagalog 54 Marathi 48 Russian 44
Italian 43 Bengali 41 Gujarati 39
Hebrew 38 Dutch 37 Turkish 35
Vietnamese 34 Macedonian 31 Cebuano 29
Swedish 26 Bulgarian 25 Swahili 23
Hungarian 23 Catalan 22 Thai 22
Lithuanian 21 Punjabi 21 Others ? 20
Table 1: Self-reported native language of 3,216
bilingual Turkers. Not shown are 49 languages with
?20 speakers. We omit 1,801 Turkers who did not
report their native language, 243 who reported 2 na-
tive languages, and 83 with ?3 native languages.
Several researchers have examined cost optimiza-
tion using active learning techniques to select the
most useful sentences or fragments to translate (Am-
bati and Vogel, 2010; Bloodgood and Callison-
Burch, 2010; Ambati, 2012).
To contrast our research with previous work, the
main contributions of this paper are: (1) a robust
methodology for assessing the bilingual skills of
anonymous workers, (2) the largest-scale census to
date of language skills of workers on MTurk, and (3)
a detailed analysis of the data gathered in our study.
3 Experimental Design
The central task in this study was to investigate Me-
chanical Turk?s bilingual population. We accom-
plished this through self-reported surveys combined
with a HIT to translate individual words for 100
languages. We evaluate the accuracy of the work-
ers? translations against known translations. In cases
where these were not exact matches, we used a sec-
ond pass monolingual HIT, which asked English
speakers to evaluate if a worker-provided translation
was a synonym of the known translation.
Demographic questionnaire At the start of each
HIT, Turkers were asked to complete a brief survey
about their language abilities. The survey asked the
following questions:
? Is [language] your native language?
? How many years have you spoken [language]?
81
? Is English your native language?
? How many years have you spoken English?
? What country do you live in?
We automatically collected each worker?s current lo-
cation by geolocating their IP address. A total of
5,281 unique workers completed our HITs. Of these,
3,625 provided answers to our survey questions, and
we were able to geolocate 5,043. Figure 1 plots
the location of workers across 106 countries. Table
1 gives the most common self-reported native lan-
guages.
Selection of languages We drew our data from the
different language versions of Wikipedia. We se-
lected the 100 languages with the largest number of
articles 1 (Table 2). For each language, we chose
the 1,000 most viewed articles over a 1 year period,2
and extracted the 10,000 most frequent words from
them. The resulting vocabularies served as the input
to our translation HIT.
Translation HIT For the translation task, we
asked Turkers to translate individual words. We
showed each word in the context of three sentences
that were drawn from Wikipedia. Turkers were al-
lowed to mark that they were unable to translate a
word. Each task contained 10 words, 8 of which
were words with unknown translations, and 2 of
which were quality control words with known trans-
lations. We gave special instruction for translat-
ing names of people and places, giving examples
of how to handle ?Barack Obama? and ?Australia?
using their interlanguage links. For languages with
non-Latin alphabets, names were transliterated.
The task paid $0.15 for the translation of 10
words. Each set of 10 words was independently
translated by three separate workers. 5,281 workers
completed 256,604 translation assignments, totaling
more than 3 million words, over a period of three
and a half months.
Gold standard translations A set of gold stan-
dard translations were automatically harvested from
1http://meta.wikimedia.org/wiki/List_of_
Wikipedias
2http://dumps.wikimedia.org/other/
pagecounts-raw/
500K+ ARTICLES: German (de), English (en), Spanish (es), French
(fr), Italian (it), Japanese (ja), Dutch (nl), Polish (pl), Portuguese
(pt), Russian (ru)
100K-500K ARTICLES: Arabic (ar), Bulgarian (bg), Catalan (ca),
Czech (cs), Danish (da), Esperanto (eo), Basque (eu), Persian (fa),
Finnish (fi), Hebrew (he), Hindi (hi), Croatian (hr), Hungarian (hu),
Indonesian (id), Korean (ko), Lithuanian (lt), Malay (ms), Norwe-
gian (Bokmal) (no), Romanian (ro), Slovak (sk), Slovenian (sl), Ser-
bian (sr), Swedish (sv), Turkish (tr), UKrainian (UK), Vietnamese
(vi), Waray-Waray (war), Chinese (zh)
10K-100K ARTICLES: Afrikaans (af) Amharic (am) Asturian (ast)
Azerbaijani (az) Belarusian (be) Bengali (bn) Bishnupriya Manipuri
(bpy) Breton (br) Bosnian (bs) Cebuano (ceb) Welsh (cy) Zazaki
(diq) Greek (el) West Frisian (fy) Irish (ga) Galician (gl) Gujarati
(gu) Haitian (ht) Armenian (hy) Icelandic (is) Javanese (jv) Geor-
gian (ka) Kannada (kn) Kurdish (ku) Luxembourgish (lb) Latvian
(lv) Malagasy (mg) Macedonian (mk) Malayalam (ml) Marathi
(mr) Neapolitan (nap) Low Saxon (nds) Nepali (ne) Newar / Nepal
Bhasa (new) Norwegian (Nynorsk) (nn) Piedmontese (pms) Sicil-
ian (scn) Serbo-Croatian (sh) Albanian (sq) Sundanese (su) Swahili
(sw) Tamil (ta) Telugu (te) Thai (th) Tagalog (tl) Urdu (ur) Yoruba
(yo)
<10K ARTICLES: Central Bicolano (bcl) Tibetan (bo) Ilokano (ilo)
Punjabi (pa) Kapampangan (pam) Pashto (ps) Sindhi (sd) Somali
(so) Uzbek (uz) Wolof (wo)
Table 2: A list of the languages that were used in our
study, grouped by the number of Wikipedia articles
in the language. Each language?s code is given in
parentheses. These language codes are used in other
figures throughout this paper.
Wikipedia for every language to use as embedded
controls. We used Wikipedia?s inter-language links
to pair titles of English articles with their corre-
sponding foreign article?s title. To get a more trans-
latable set of pairs, we excluded any pairs where: (1)
the English word was not present in the WordNet
ontology (Miller, 1995), (2) either article title was
longer than a single word, (3) the English Wikipedia
page was a subcategory of person or place, or (4)
the English and the foreign titles were identical or a
substring of the other.
Manual evaluation of non-identical translations
We counted all translations that exactly matched
the gold standard translation as correct. For non-
exact matches we created a second-pass quality as-
surance HIT. Turkers were shown a pair of En-
glish words, one of which was a Turker?s transla-
tion of the foreign word used for quality control,
and the other of which was the gold-standard trans-
lation of the foreign word. Evaluators were asked
whether the two words had the same meaning, and
chose between three answers: ?Yes?, ?No?, or ?Re-
82
Figure 2: Days to complete the translation HITs for
40 of the languages. Tick marks represent the com-
pletion of individual assignments.
lated but not synonymous.? Examples of mean-
ing equivalent pairs include: <petroglyphs, rock
paintings>, <demo, show> and <loam, loam: soil
rich in decaying matter>. Non-meaning equiva-
lents included: <assorted, minutes>, and <major,
URL of image>. Related items were things like
<sky, clouds>. Misspellings like <lactation, lac-
tiation > were judged to have same meaning, and
were marked as misspelled. Three separate Turkers
judged each pair, allowing majority votes for diffi-
cult cases.
We checked Turkers who were working on this
task by embedding pairs of words which were ei-
?? ?$ %??? ( ?? + ?$ ??? %?.? ?? ?? ???? 5 ?? ?????9 ???:? ?? ??<? 
In retribution pakistan also did six nuclear tests on 28 may 1998.
On 28 May Pakistan also conducted six nuclear tests as an act
of redressal.
Retaliating on this ?Pakistan? conducted Six(6) Nuclear Tests
on 28 May, 1998.
pakistan also did 6 nuclear test in retribution on 28 may, 1998
Figure 3: An example of the Turkers? translations of
a Hindi sentence. The translations are unedited and
contain fixable spelling, capitalization and grammat-
ical errors.
ther known to be synonyms (drawn from Word-
Net) or unrelated (randomly chosen from a corpus).
Automating approval/rejections for the second-pass
evaluation allowed the whole pipeline to be run au-
tomatically. Caching judgments meant that we ulti-
mately needed only 20,952 synonym tasks to judge
all of the submitted translations (a total of 74,572
non-matching word pairs). These were completed
by an additional 1,005 workers. Each of these as-
signments included 10 word pairs and paid $0.10.
Full sentence translations To demonstrate the
feasibility of using crowdsourcing to create multi-
lingual technologies, we hire Turkers to construct
bilingual parallel corpora from scratch for six In-
dian languages. Germann (2001) attempted to build
a Tamil-English translation system from scratch by
hiring professional translators, but found the cost
prohibitive. We created parallel corpora by trans-
lating the 100 most viewed Wikipedia pages in Ben-
gali, Malyalam, Hindi, Tamil, Telugu, and Urdu into
English. We collected four translations from differ-
ent Turkers for each source sentence.
Workers were paid $0.70 per HIT to translate
10 sentences. We accepted or rejected translations
based on a manual review of each worker?s submis-
sions, which included a comparison of the transla-
tions to a monotonic gloss (produced with a dic-
tionary), and metadata such as the amount of time
the worker took to complete the HIT and their geo-
graphic location.
Figure 3 shows an example of the translations we
obtained. The lack of a professionally translated
reference sentences prevented us from doing a sys-
tematic comparison between the quality of profes-
83
pt bs sh tl it sr ro es ms de af te hr id da nl tr gu sk fi he ml fr ja pa bg mk no gl ht ga sv cy lv hu kn az be lt ko ne eo ar pl mr ca cs sw ta hi bn nn ka so zh jv el ceb vi bcl is su uz lb bpy scn new ur sd br ps ru am wo bo
0.0
0.2
0.4
0.6
0.8
1.0
Figure 4: Translation quality for languages with at least 50 Turkers. The dark blue bars indicate the pro-
portion of translations which exactly matched gold standard translations, and light blue indicate translations
which were judged to be correct synonyms. Error bars show the 95% confidence intervals for each language.
sion and non-professional translations as Zaidan and
Callison-Burch (2011) did. Instead we evaluate the
quality of the data by using it to train SMT systems.
We present results in section 5.
4 Measuring Translation Quality
For single word translations, we calculate the qual-
ity of translations on the level of individual assign-
ments and aggregated over workers and languages.
We define an assignment?s quality as the proportion
of controls that are correct in a given assignment,
where correct means exactly correct or judged to be
synonymous.
Quality(ai) = 1ki
ki?
j=1
?(trij ? syns[gj]) (1)
where ai is the ith assignment, ki is the number of
controls in ai, trij is the Turker?s provided transla-
tion of control word j in assignment i, gj is the gold
standard translation of control word j, syns[gj] is
the set of words judged to be synonymous with gj
and includes gj , and ?(x) is Kronecker?s delta and
takes value 1 when x is true. Most assignments had
two known words embedded, so most assignments
had scores of either 0, 0.5, or 1.
Since computing overall quality for a language as
the average assignment quality score is biased to-
wards a small number of highly active Turkers, we
instead report language quality scores as the aver-
age per-Turker quality, where a Turker?s quality is
the average quality of all the assignments that she
completed:
Quality(ti) =
?
aj?assigns[i] Quality(aj)
| assigns[i] | (2)
where assigns[i] is the assignments completed
by Turker i, and Quality(a) is as above.
Quality for a language is then given by
Quality(li) =
?
tj?turkers[i] Quality(tj)
| turkers[i] | (3)
When a Turker completed assignments in more than
one language, their quality was computed separately
for each language. Figure 4 shows the transla-
tion quality for languages with contributions from
at least 50 workers.
Cheating using machine translation One obvi-
ous way for workers to cheat is to use available
online translation tools. Although we followed
best practices to deter copying-and-pasting into on-
line MT systems by rendering words and sentences
84
as images (Zaidan and Callison-Burch, 2011), this
strategy does not prevent workers from typing the
words into an MT system if they are able to type in
the language?s script.
To identify and remove workers who appeared to
be cheating by using Google Translate, we calcu-
lated each worker?s overlap with the Google transla-
tions. We used Google to translate all 10,000 words
for the 51 foreign languages that Google Trans-
late covered at the time of the study. We mea-
sured the percent of workers? translations that ex-
actly matched the translation returned from Google.
Figure 5a shows overlap between Turkers?s trans-
lations and Google Translate. When overlap is high,
it seems likely that those Turkers are cheating. It is
also reasonable to assume that honest workers will
overlap with Google some amount of the time as
Google?s translations are usually accurate. We di-
vide the workers into three groups: those with very
high overlap with Google (likely cheating by using
Google to translate words), those with reasonable
overlap, and those with no overlap (likely cheating
by other means, for instance, by submitting random
text).
Our gold-standard controls are designed to iden-
tify workers that fall into the third group (those who
are spamming or providing useless translations), but
they will not effectively flag workers who are cheat-
ing with Google Translate. We therefore remove the
500 Turkers with the highest overlap with Google.
This equates to removing all workers with greater
than 70% overlap. Figure 5b shows that removing
workers at or above the 70% threshold retains 90%
of the collected translations and over 90% of the
workers.
Quality scores reported throughout the paper re-
flect only translations from Turkers whose overlap
with Google falls below this 70% threshold.
5 Data Analysis
We performed an analysis of our data to address the
following questions:
? Do workers accurately represent their language
abilities? Should we constrain tasks by region?
? How quickly can we expect work to be com-
pleted in a particular language?
(a) Individual workers? overlap with Google Translate.
We removed the 500 workers with the highest overlap
(shaded region on the left) from our analyses, as it is rea-
sonable to assume these workers are cheating by submit-
ting translations from Google. Workers with no overlap
(shaded region on the right) are also likely to be cheating,
e.g. by submitting random text.
(b) Cumulative distribution of overlap with Google trans-
late for workers and translations. We see that eliminating
all workers with >70% overlap with google translate still
preserves 90% of translations and >90% of workers.
Figure 5
? Can Turkers? translations be used to train MT
systems?
? Do our dictionaries improve MT quality?
Language skills and location We measured the
average quality of workers who were in countries
that plausibly speak a language, versus workers from
countries that did not have large speaker populations
of that language. We used the Ethnologue (Lewis
85
Avg. Turker quality (# Ts) Primary locations Primary locations
In region Out of region of Turkers in region of Turkers out of region
Hindi 0.63 (296) 0.69 (7) India (284) UAE (5) UK (3) Saudi Arabia (2) Russia (1) Oman (1)
Tamil 0.65 (273) ** 0.25 (2) India (266) US (3) Canada (2) Tunisia (1) Egypt (1)
Malayalam 0.76 (234) 0.83 (2) India (223) UAE (6) US (3) Saudi Arabia (1) Maldives (1)
Spanish 0.81 (191) 0.84 (18) US (122) Mexico (16) Spain (14) India (15) New Zealand (1) Brazil (1)
French 0.75 (170) 0.82 (11) India (62) US (45) France (23) Greece (2) Netherlands (1) Japan (1)
Chinese 0.60 (116) 0.55 (21) US (75) Singapore (13) China (9) Hong Kong (6) Australia (3) Germany (2)
German 0.82 (91) 0.77 (41) Germany (48) US (25) Austria (7) India (34) Netherlands (1) Greece (1)
Italian 0.86 (90) * 0.80 (42) Italy (42) US (29) Romania (7) India (33) Ireland (2) Spain (2)
Amharic 0.14 (16) ** 0.01 (99) US (14) Ethiopia (2) India (70) Georgia (9) Macedonia (5)
Kannada 0.70 (105) NA (0) India (105)
Arabic 0.74 (60) ** 0.60 (45) Egypt (19) Jordan (16) Morocco (9) US (19) India (11) Canada (3)
Sindhi 0.19 (96) 0.06 (9) India (58) Pakistan (37) US (1) Macedonia (4) Georgia (2) Indonesia (2)
Portuguese 0.87 (101) 0.96 (3) Brazil (44) Portugal (31) US (15) Romania (1) Japan (1) Israel (1)
Turkish 0.76 (76) 0.80 (27) Turkey (38) US (18) Macedonia (8) India (19) Pakistan (4) Taiwan (1)
Telugu 0.80 (102) 0.50 (1) India (98) US (3) UAE (1) Saudi Arabia (1)
Irish 0.74 (54) 0.71 (47) US (39) Ireland (13) UK (2) India (36) Romania (5) Macedonia (2)
Swedish 0.73 (54) 0.71 (45) US (25) Sweden (22) Finland (3) India (23) Macedonia (6) Croatia (2)
Czech 0.71 (45) * 0.61 (50) US (17) Czech Republic (14) Serbia (5) Macedonia (22) India (10) UK (5)
Russian 0.15 (67) * 0.12 (27) US (36) Moldova (7) Russia (6) India (14) Macedonia (4) UK (3)
Breton 0.17 (3) 0.18 (89) US (3) India (83) Macedonia (2) China (1)
Table 3: Translation quality when partitioning the translations into two groups, one containing translations
submitted by Turkers whose location is within regions that plausibly speak the foreign language, and the
other containing translations from Turkers outside those regions. In general, in-region Turkers provide
higher quality translations. (**) indicates differences significant at p=0.05, (*) at p=0.10.
et al., 2013) to compile the list of countries where
each language is spoken. Table 3 compares the av-
erage translation quality of assignments completed
within the region of each language, and compares it
to the quality of assignments completed outside that
region.
Our workers reported speaking 95 languages na-
tively. US workers alone reported 61 native lan-
guages. Overall, 4,297 workers were located in a
region likely to speak the language from which they
were translating, and 2,778 workers were located
in countries considered out of region (meaning that
about a third of our 5,281 Turkers completed HITs
in multiple languages).
Table 3 shows the differences in translation qual-
ity when computed using in-region versus out-of-
region Turkers, for the languages with the greatest
number of workers. Within region workers typi-
cally produced higher quality translations. Given the
number of Indian workers on Mechanical Turk, it
is unsurprising that they represent majority of out-
of-region workers. For the languages that had more
than 75 out of region workers (Malay, Amharic, Ice-
landic, Sicilian, Wolof, and Breton), Indian workers
represented at least 70% of the out of region workers
in each language.
A few languages stand out for having suspiciously
strong performance by out of region workers, no-
tably Irish and Swedish, for which out of region
workers account for a near equivalent volume and
quality of translations to the in region workers. This
is admittedly implausible, considering the relatively
small number of Irish speakers worldwide, and the
very low number living in the countries in which our
Turkers were based (primarily India). Such results
highlight the fact that cheating using online transla-
tion resources is a real problem, and despite our best
efforts to remove workers using Google Translate,
some cheating is still evident. Restricting to within
region workers is an effective way to reduce the
prevalence of cheating. We discuss the languages
which are best supported by true native speakers in
section 6.
Speed of translation Figure 2 gives the comple-
tion times for 40 languages. The 10 languages to
finish in the shortest amount of time were: Tamil,
Malayalam, Telugu, Hindi, Macedonian, Spanish,
Serbian, Romanian, Gujarati, and Marathi. Seven of
the ten fastest languages are from India, which is un-
86
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 6: The total volume of translations (measured
in English words) as a function of elapsed days.
sentence English + dictionary
language pairs foreign words entries
Bengali 22k 732k 22k
Hindi 40k 1,488k 22k
Malayalam 32k 863k 23k
Tamil 38k 916k 25k
Telugu 46k 1,097k 21k
Urdu 35k 1,356k 20k
Table 4: Size of parallel corpora and bilingual dic-
tionaries collected for each language.
surprising given the geographic distribution of work-
ers. Some languages follow the pattern of having a
smattering of assignments completed early, with the
rate picking up later.
Figure 6 gives the throughput of the full-sentence
translation task for the six Indian languages. The
fastest language was Malayalam, for which we col-
lected half a million words of translations in just un-
der a week. Table 4 gives the size of the data set that
we created for each of these languages.
Training SMT systems We trained statistical
translation models from the parallel corpora that we
created for the six Indian languages using the Joshua
machine translation system (Post et al., 2012). Table
5 shows the translation performance when trained
on the bitexts alone, and when incorporating the
bilingual dictionaries created in our earlier HIT. The
scores reflect the performance when tested on held
out sentences from the training data. Adding the dic-
trained on bitext + BLEU
language bitexts alone dictionaries ?
Bengali 12.03 17.29 5.26
Hindi 16.19 18.10 1.91
Malayalam 6.65 9.72 3.07
Tamil 8.08 9.66 1.58
Telugu 11.94 13.70 1.76
Urdu 19.22 21.98 2.76
Table 5: BLEU scores for translating into English
using bilingual parallel corpora by themselves, and
with the addition of single-word dictionaries. Scores
are calculated using four reference translations and
represent the mean of three MERT runs.
tionaries to the training set produces consistent per-
formance gains, ranging from 1 to 5 BLEU points.
This represents a substantial improvement. It is
worth noting, however, that while the source doc-
uments for the full sentences used for testing were
kept disjoint from those used for training, there is
overlap between the source materials for the dictio-
naries and those from the test set, since both the dic-
tionaries and the bitext source sentences were drawn
from Wikipedia.
6 Discussion
Crowdsourcing platforms like Mechanical Turk give
researchers instant access to a diverse set of bilin-
gual workers. This opens up exciting new avenues
for researchers to develop new multilingual systems.
The demographics reported in this study are likely to
shift over time. Amazon may expand its payments to
new currencies. Posting long-running HITs in other
languages may recruit more speakers of those lan-
guages. New crowdsourcing platforms may emerge.
The data presented here provides a valuable snap-
shot of the current state of MTurk, and the methods
used can be applied generally in future research.
Based on our study, we can confidently recom-
mend 13 languages as good candidates for research
now: Dutch, French, German, Gujarati, Italian, Kan-
nada, Malayalam, Portuguese, Romanian, Serbian,
Spanish, Tagalog, and Telugu. These languages
have large Turker populations who complete tasks
quickly and accurately. Table 6 summarizes the
strengths and weaknesses of all 100 languages cov-
ered in our study. Several other languages are viable
87
workers quality speed
many high fast Dutch, French, German, Gu-
jarati, Italian, Kannada, Malay-
alam, Portuguese, Romanian,
Serbian, Spanish, Tagalog, Tel-
ugu
slow Arabic, Hebrew, Irish, Punjabi,
Swedish, Turkish
low fast Hindi, Marathi, Tamil, Urdu
or
medium
slow Bengali, Bishnupriya Ma-
nipuri, Cebuano, Chinese,
Nepali, Newar, Polish, Russian,
Sindhi, Tibetan
few high fast Bosnia, Croatian, Macedonian,
Malay, Serbo-Croatian
slow Afrikaans, Albanian,
Aragonese, Asturian, Basque,
Belarusian, Bulgarian, Central
Bicolano, Czech, Danish,
Finnish, Galacian, Greek,
Haitian, Hungarian, Icelandic,
Ilokano, Indonesian, Japanese,
Javanese, Kapampangan,
Kazakh, Korean, Lithuanian,
Low Saxon, Malagasy, Nor-
wegian (Bokmal), Sicilian,
Slovak, Slovenian, Thai, UKra-
nian, Uzbek, Waray-Waray,
West Frisian, Yoruba
low fast ?
or
medium
slow Amharic, Armenian, Azer-
baijani, Breton, Catalan,
Georgian, Latvian, Luxembour-
gish, Neapolitian, Norwegian
(Nynorsk), Pashto, Pied-
montese, Somali, Sudanese,
Swahili, Tatar, Vietnamese,
Walloon, Welsh
none low or
medium
slow Esperanto, Ido, Kurdish, Per-
sian, Quechua, Wolof, Zazaki
Table 6: The green box shows the best languages to
target on MTurk. These languages have many work-
ers who generate high quality results quickly. We
defined many workers as 50 or more active in-region
workers, high quality as?70% accuracy on the gold
standard controls, and fast if all of the 10,000 words
were completed within two weeks.
candidates provided adequate quality control mech-
anisms are used to select good workers.
Since Mechanical Turk provides financial incen-
tives for participation, many workers attempt to
complete tasks even if they do not have the lan-
guage skills necessary to do so. Since MTurk does
not provide any information about workers demo-
graphics, including their language competencies, it
can be hard to exclude such workers. As a result
naive data collection on MTurk may result in noisy
data. A variety of techniques should be incorporated
into crowdsourcing pipelines to ensure high quality
data. As a best practice, we suggest: (1) restricting
workers to countries that plausibly speak the foreign
language of interest, (2) embedding gold standard
controls or administering language pretests, rather
than relying solely on self-reported language skills,
and (3) excluding workers whose translations have
high overlap with online machine translation sys-
tems like Google translate. If cheating using exter-
nal resources is likely, then also consider (4) record-
ing information like time spent on a HIT (cumulative
and on individual items), patterns in keystroke logs,
tab/window focus, etc.
Although our study targeted bilingual workers on
Mechanical Turk, and neglected monolingual work-
ers, we believe our results reliably represent the cur-
rent speaker populations, since the vast majority of
the work available on the crowdsourced platform
is currently English-only. We therefore assume the
number of non-English speakers is small. In the fu-
ture, it may be desirable to recruit monolingual for-
eign workers. In such cases, we recommend other
tests to validate their language abilities in place of
our translation test. These could include perform-
ing narrative cloze, or listening to audio files con-
taining speech in different language and identifying
their language.
7 Data release
With the publication of this paper, we are releasing
all data and code used in this study. Our data release
includes the raw data, along with bilingual dictionar-
ies that are filtered to be high quality. It will include
256,604 translation assignments from 5,281 Turkers
and 20,952 synonym assignments from 1,005 Turk-
ers, along with meta information like geolocation
88
and time submitted, plus external dictionaries used
for validation. The dictionaries will contain 1.5M
total translated words in 100 languages, along with
code to filter the dictionaries based on different cri-
teria. The data also includes parallel corpora for six
Indian languages, ranging in size between 700,000
to 1.5 million words.
8 Acknowledgements
This material is based on research sponsored by
a DARPA Computer Science Study Panel phase 3
award entitled ?Crowdsourcing Translation? (con-
tract D12PC00368). The views and conclusions
contained in this publication are those of the authors
and should not be interpreted as representing offi-
cial policies or endorsements by DARPA or the U.S.
Government. This research was supported by the
Johns Hopkins University Human Language Tech-
nology Center of Excellence and through gifts from
Microsoft and Google.
The authors would like to thank the anonymous
reviewers for their thoughtful comments, which sub-
stantially improved this paper.
References
Amazon. 2013. Service summary tour for re-
questers on Amazon Mechanical Turk. https://
requester.mturk.com/tour.
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk. Association for Computational Lin-
guistics.
Vamshi Ambati, Stephan Vogel, and Jaime Carbonell.
2010. Active learning and crowd-sourcing for ma-
chine translation. In Proceedings of the 7th Interna-
tional Conference on Language Resources and Evalu-
ation (LREC).
Vamshi Ambati. 2012. Active Learning and Crowd-
sourcing for Machine Translation in Low Resource
Scenarios. Ph.D. thesis, Language Technologies In-
stitute, School of Computer Science, Carnegie Mellon
University, Pittsburgh, PA.
Michael S. Bernstein, Greg Little, Robert C. Miller,
Bjrn Hartmann, Mark S. Ackerman, David R. Karger,
David Crowell, and Katrina Panovich. 2010. Soylent:
a word processor with a crowd inside. In Proceed-
ings of the ACM Symposium on User Interface Soft-
ware and Technology (UIST).
Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Lit-
tle, Andrew Miller, Robert C. Miller, Robin Miller,
Aubrey Tatarowicz, Brandyn White, Samual White,
and Tom Yeh. 2010. VizWiz: nearly real-time an-
swers to visual questions. In Proceedings of the ACM
Symposium on User Interface Software and Technol-
ogy (UIST).
Michael Bloodgood and Chris Callison-Burch. 2010.
Large-scale cost-focused active learning for statisti-
cal machine translation. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics.
Chris Callison-Burch and Mark Dredze. 2010. Creating
speech and language data with Amazon?s Mechanical
Turk. In Proceedings of the NAACL HLT 2010 Work-
shop on Creating Speech and Language Data with
Amazon?s Mechanical Turk, pages 1?12, Los Angeles,
June. Association for Computational Linguistics.
Jia Deng, Alexander Berg, Kai Li, and Li Fei-Fei. 2010.
What does classifying more than 10,000 image cate-
gories tell us? In Proceedings of the 12th European
Conference of Computer Vision (ECCV, pages 71?84.
Maxine Eskenazi, Gina-Anne Levow, Helen Meng,
Gabriel Parent, and David Suendermann. 2013.
Crowdsourcing for Speech Processing, Applications to
Data Collection, Transcription and Assessment. Wi-
ley.
Siamak Faridani, Bjo?rn Hartmann, and Panagiotis G.
Ipeirotis. 2011. What?s the right price? pricing tasks
for finishing on time. In Third AAAI Human Compu-
tation Workshop (HCOMP?11).
Ulrich Germann. 2001. Building a statistical machine
translation system from scratch: How much bang for
the buck can we expect? In ACL 2001 Workshop on
Data-Driven Machine Translation, Toulouse, France.
Chang Hu, Benjamin B. Bederson, and Philip Resnik.
2010. Translation by iterative collaboration between
monolingual users. In Proceedings of ACM SIGKDD
Workshop on Human Computation (HCOMP).
Chang Hu, Philip Resnik, Yakov Kronrod, Vladimir Ei-
delman, Olivia Buzek, and Benjamin B. Bederson.
2011. The value of monolingual crowdsourcing in
a real-world translation scenario: Simulation using
haitian creole emergency sms messages. In Pro-
ceedings of the Sixth Workshop on Statistical Ma-
chine Translation, pages 399?404, Edinburgh, Scot-
land, July. Association for Computational Linguistics.
Panagiotis G. Ipeirotis. 2010a. Analyzing the mechani-
cal turk marketplace. In ACM XRDS, December.
Panagiotis G. Ipeirotis. 2010b. Demographics of
Mechanical Turk. Technical Report Working paper
89
CeDER-10-01, New York University, Stern School of
Business.
Ann Irvine and Alexandre Klementiev. 2010. Using Me-
chanical Turk to annotate lexicons for less commonly
used languages. In Workshop on Creating Speech and
Language Data with MTurk.
Ian Lane, Matthias Eck, Kay Rottmann, and Alex
Waibel. 2010. Tools for collecting speech corpora
via mechanical-turk. In Proceedings of the NAACL
HLT 2010 Workshop on Creating Speech and Lan-
guage Data with Amazon?s Mechanical Turk, Los An-
geles.
Florian Laws, Christian Scheible, and Hinrich Schu?tze.
2011. Active learning with amazon mechanical turk.
In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, Edinburgh,
Scotland.
Matthew Lease, Jessica Hullman, Jeffrey P. Bigham,
Juho Kim Michael S. Bernstein and, Walter Lasecki,
Saeideh Bakhshi, Tanushree Mitra, and Robert C.
Miller. 2013. Mechanical Turk is not anony-
mous. http://dx.doi.org/10.2139/ssrn.
2228728.
Vili Lehdonvirta and Mirko Ernkvist. 2011. Knowl-
edge map of the virtual economy: Converting
the virtual economy into development potential.
http://www.infodev.org/en/Document.
1056.pdf, April. An InfoDev Publication.
M. Paul Lewis, Gary F. Simons, and Charles D. Fennig
(eds.). 2013. Ethnologue: Languages of the world,
seventeenth edition. http://www.ethnologue.
com.
Greg Little, Lydia B. Chilton, Rob Miller, and Max Gold-
man. 2009. Turkit: Tools for iterative tasks on me-
chanical turk. In Proceedings of the Workshop on
Human Computation at the International Conference
on Knowledge Discovery and Data Mining (KDD-
HCOMP ?09), Paris.
Matthew Marge, Satanjeev Banerjee, and Alexander
Rudnicky. 2010. Using the Amazon Mechanical Turk
to transcribe and annotate meeting speech for extrac-
tive summarization. In Workshop on Creating Speech
and Language Data with MTurk.
George A. Miller. 1995. WordNet: a lexical database for
english. Communications of the ACM, 38(11):39?41.
Robert Munro and Hal Tily. 2011. The start of the
art: Introduction to the workshop on crowdsourcing
technologies for language and cognition studies. In
Crowdsourcing Technologies for Language and Cog-
nition Studies, Boulder.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Human Language
Technologies: The 2010 Annual Conference of the
North American Chapter of the Association for Com-
putational Linguistics, pages 207?215. Association for
Computational Linguistics.
Gabriel Parent and Maxine Eskenazi. 2011. Speaking
to the crowd: looking at past achievements in using
crowdsourcing for speech and predicting future chal-
lenges. In Proceedings Interspeech 2011, Special Ses-
sion on Crowdsourcing.
Matt Post, Chris Callison-Burch, and Miles Osborne.
2012. Constructing parallel corpora for six indian
languages via crowdsourcing. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
pages 401?409, Montre?al, Canada, June. Association
for Computational Linguistics.
Alexander J. Quinn and Benjamin B. Bederson. 2011.
Human computation: A survey and taxonomy of a
growing field. In Computer Human Interaction (CHI).
Cyrus Rashtchian, Peter Young, Micah Hodosh, and Ju-
lia Hockenmaier. 2010. Collecting image annotations
using Amazon?s Mechanical Turk. In Workshop on
Creating Speech and Language Data with MTurk.
Joel Ross, Lilly Irani, M. Six Silberman, Andrew Zal-
divar, and Bill Tomlinson. 2010. Who are the crowd-
workers?: Shifting demographics in Amazon Mechan-
ical Turk. In alt.CHI session of CHI 2010 extended
abstracts on human factors in computing systems, At-
lanta, Georgia.
Yaron Singer and Manas Mittal. 2011. Pricing mecha-
nisms for online labor markets. In Third AAAI Human
Computation Workshop (HCOMP?11).
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast - but is it
good? Evaluating non-expert annotations for natural
language tasks. In Proceedings of EMNLP.
Alexander Sorokin and David Forsyth. 2008. Utility
data annotation with amazon mechanical turk. In First
IEEE Workshop on Internet Vision at CVPR.
Luis von Ahn. 2005. Human Computation. Ph.D. thesis,
School of Computer Science, Carnegie Mellon Uni-
versity, Pittsburgh, PA.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: Professional quality from non-
professionals. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 1220?
1229. Association for Computational Linguistics.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of Arabic dialects. In The
2012 Conference of the North American Chapter of
the Association for Computational Linguistics. Asso-
ciation for Computational Linguistics.
90
Rabih Zbib, Gretchen Markiewicz, Spyros Matsoukas,
Richard Schwartz, and John Makhoul. 2013. Sys-
tematic comparison of professional and crowdsourced
reference translations for machine translation. In Pro-
ceedings of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Atlanta,
Georgia.
91
92
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 49?57,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Factors Affecting the Accuracy of Korean Parsing
Tagyoung Chung, Matt Post and Daniel Gildea
Department of Computer Science
University of Rochester
Rochester, NY 14627
Abstract
We investigate parsing accuracy on the Ko-
rean Treebank 2.0 with a number of different
grammars. Comparisons among these gram-
mars and to their English counterparts suggest
different aspects of Korean that contribute to
parsing difficulty. Our results indicate that the
coarseness of the Treebank?s nonterminal set
is a even greater problem than in the English
Treebank. We also find that Korean?s rela-
tively free word order does not impact parsing
results as much as one might expect, but in
fact the prevalence of zero pronouns accounts
for a large portion of the difference between
Korean and English parsing scores.
1 Introduction
Korean is a head-final, agglutinative, and mor-
phologically productive language. The language
presents multiple challenges for syntactic pars-
ing. Like some other head-final languages such
as German, Japanese, and Hindi, Korean exhibits
long-distance scrambling (Rambow and Lee, 1994;
Kallmeyer and Yoon, 2004). Compound nouns are
formed freely (Park et al, 2004), and verbs have
well over 400 paradigmatic endings (Martin, 1992).
Korean Treebank 2.0 (LDC2006T09) (Han and
Ryu, 2005) is a subset of a Korean newswire corpus
(LDC2000T45) annotated with morphological and
syntactic information. The corpus contains roughly
5K sentences, 132K words, and 14K unique mor-
phemes. The syntactic bracketing rules are mostly
the same as the previous version of the treebank
(Han et al, 2001) and the phrase structure annota-
tion schemes used are very similar to the ones used
in Penn English treebank. The Korean Treebank is
constructed over text that has been morphologically
analyzed; not only is the text tokenized into mor-
phemes, but all allomorphs are neutralized.
To our knowledge, there have been only a few pa-
pers focusing on syntactic parsing of Korean. Herm-
jakob (2000) implemented a shift-reduce parser for
Korean trained on very limited (1K sentences) data,
and Sarkar and Han (2002) used an earlier version
of the Treebank to train a lexicalized tree adjoining
grammar. In this paper, we conduct a range of ex-
periments using the Korean Treebank 2.0 (hereafter,
KTB) as our training data and provide analyses that
reveal insights into parsing morphologically rich lan-
guages like Korean. We try to provide comparisons
with English parsing using parsers trained on a simi-
lar amount of data wherever applicable.
2 Difficulties parsing Korean
There are several challenges in parsing Korean com-
pared to languages like English. At the root of many
of these challenges is the fact that it is highly in-
flected and morphologically productive. Effective
morphological segmentation is essential to learning
grammar rules that can generalize beyond the train-
ing data by limiting the number of out-of-vocabulary
words. Fortunately, there are good techniques for do-
ing so. The sentences in KTB have been segmented
into basic morphological units.
Second, Korean is a pro-drop language: subjects
and objects are dropped wherever they are pragmati-
cally inferable, which is often possible given its rich
morphology. Zero pronouns are a remarkably fre-
quent phenomenon in general (Han, 2006), occuring
49
an average of 1.8 times per sentence in the KTB.
The standard approach in parsing English is to ig-
nore NULL elements entirely by removing them (and
recursively removing unary parents of empty nodes
in a bottom-up fashion). This is less of a problem in
English because these empty nodes are mostly trace
elements that denote constituent movement. In the
KTB, these elements are removed altogether and a
crucial cue to grammatical inference is often lost.
Later we will show the profound effect this has on
parsing accuracy.
Third, word order in Korean is relatively free.
This is also partly due to the richer morphology,
since morphemes (rather than word order) are used
to denote semantic roles of phrases. Consider the
following example:
?? ???? ?? ??? .
John-NOM Mary-DAT book-ACC give-PAST .
In the example, any permutation of the first three
words produces a perfectly acceptable sentence.
This freedom of word order could potentially result
in a large number of rules, which could complicate
analysis with new ambiguities. However, formal
written Korean generally conforms to a canonical
word order (SOV).
3 Initial experiments
There has been some work on Korean morphologi-
cal analysis showing that common statistical meth-
ods such as maximum entropy modeling and condi-
tional random fields perform quite well (Lee et al,
2000; Sarkar and Han, 2002; Han and Palmer, 2004;
Lee and Rim, 2005). Most claim accuracy rate over
95%. In light of this, we focus on the parsing part of
the problem utilizing morphology analysis already
present in the data.
3.1 Setup
For our experiments we used all 5,010 sentences in
the Korean Treebank (KTB), which are already seg-
mented. Due to the small size of the corpus, we used
ten-fold cross validation for all of our experiments,
unless otherwise noted. Sentences were assigned to
folds in blocks of one (i.e., fold 1 contained sen-
tences 1, 11, 21, and so on.). Within each fold, 80%
of the data was assigned to training, 10% to devel-
opment, and 10% to testing. Each fold?s vocabulary
model F1 F1?40 types tokens
Korean 52.78 56.55 6.6K 194K
English (?02?03) 71.06 72.26 5.5K 96K
English (?02?04) 72.20 73.29 7.5K 147K
English (?02?21) 71.61 72.74 23K 950K
Table 1: Parser scores for Treebank PCFGs in Korean
and English. For English, we vary the size of the training
data to provide a better point of comparison against Ko-
rean. Types and tokens denote vocabulary sizes (which
for Korean is the mean over the folds).
was set to all words occurring more than once in its
training data, with a handful of count one tokens re-
placing unknown words based on properties of the
word?s surface form (all Korean words were placed
in a single bin, and English words were binned fol-
lowing the rules of Petrov et al (2006)). We report
scores on the development set.
We report parser accuracy scores using the stan-
dard F1 metric, which balances precision and recall
of the labeled constituents recovered by the parser:
2PR/(P + R). Throughout the paper, all evalua-
tion occurs against gold standard trees that contain
no NULL elements or nonterminal function tags or
annotations, which in some cases requires the re-
moval of those elements from parse trees output by
the parser.
3.2 Treebank grammars
We begin by presenting in Table 1 scores for the
standard Treebank grammar, obtained by reading a
standard context-free grammar from the trees in the
training data and setting rule probabilities to rela-
tive frequency (Charniak, 1996). For these initial
experiments, we follow standard practice in English
parsing and remove all (a) nonterminal function tags
and (b) NULL elements from the parse trees before
learning the grammar. For comparison purposes, we
present scores from parsing the Wall Street Journal
portion of the English Penn Treebank (PTB), using
both the standard training set and subsets of it cho-
sen to be similar in size to the KTB. All English
scores are tested on section 22.
There are two interesting results in this table.
First, Korean parsing accuracy is much lower than
English parsing accuracy, and second, the accuracy
difference does not appear to be due to a difference
in the size of the training data, since reducing the
50
size of the English training data did not affect accu-
racy scores very much.
Before attempting to explain this empirically, we
note that Rehbein and van Genabith (2007) demon-
strate that the F1 metric is biased towards parse trees
with a high ratio of nonterminals to terminals, be-
cause mistakes made by the parser have a smaller
effect on the overall evaluation score.1 They rec-
ommend that F1 not be used for comparing parsing
accuracy across different annotation schemes. The
nonterminal to terminal ratio in the KTB and PTB
are 0.40 and 0.45, respectively. It is a good idea to
keep this bias in mind, but we believe that this small
ratio difference is unlikely to account for the huge
gap in scores displayed in Table 1.
The gap in parsing accuracy is unsurprising in
light of the basic known difficulties parsing Korean,
summarized earlier in the paper. Here we observe a
number of features of the KTB that contribute to this
difficulty.
Sentence length On average, KTB sentences are
much longer than PTB sentences (23 words versus
48 words, respectively). Sentence-level F1 is in-
versely correlated with sentence length, and the rel-
atively larger drop in F1 score going from column 3
to 2 in Table 1 is partially accounted for by the fact
that column 3 represents 33% of the KTB sentences,
but 92% of the English sentences.
Flat annotation scheme The KTB makes rela-
tively frequent use of very flat and ambiguous rules.
For example, consider the extreme cases of rule am-
biguity in which the lefthand side nonterminal is
present three or more times on its righthand side.
There are only three instances of such ?triple+-
recursive? NPs among the?40K trees in the training
portion of the PTB, each occurring only once.
NP? NP NP NP , CC NP
NP? NP NP NP CC NP
NP? NP NP NP NP .
The KTB is an eighth of the size of this, but has
fifteen instances of such NPs (listed here with their
frequencies):
1We thank one of our anonymous reviewers for bringing this
to our attention.
NP? NP NP NP NP (6)
NP? NP NP NP NP NP (3)
NP? NP NP NP NP NP NP (2)
NP? NP NP NP NP NP NP NP (2)
NP? SLQ NP NP NP SRQ PAD (1)
NP? SLQ NP NP NP NP SRQ PAN (1)
Similar rules are common for other nonterminals as
well. Generally, flatter rules are easier to parse with
because they contribute to parse trees with fewer
nodes (and thus fewer independent decision points).
However, the presence of a single nonterminal on
both the left and righthand side of a rule means that
the annotation scheme is failing to capture distribu-
tional differences which must be present.
Nonterminal granularity This brings us to a final
point about the granularity of the nonterminals in the
KTB. After removing function tags, there are only
43 nonterminal symbols in the KTB (33 of them
preterminals), versus 72 English nonterminals (44
of them preterminals). Nonterminal granularity is
a well-studied problem in English parsing, and there
is a long, successful history of automatically refin-
ing English nonterminals to discover distributional
differences. In light of this success, we speculate
that the disparity in parsing performance might be
explained by this disparity in the number of nonter-
minals. In the next section, we provide evidence that
this is indeed the case.
4 Nonterminal granularity
There are many ways to refine the set of nontermi-
nals in a Treebank. A simple approach suggested
by Johnson (1998) is to simply annotate each node
with its parent?s label. The effect of this is to re-
fine the distribution of each nonterminal over se-
quences of children according to its position in the
sentence; for example, a VP beneath an SBAR node
will have a different distribution over children than a
VP beneath an S node. This simple technique alone
produces a large improvement in English Treebank
parsing. Klein and Manning (2003) expanded this
idea with a series of experiments wherein they manu-
ally refined nonterminals to different degrees, which
resulted in parsing accuracy rivaling that of bilexi-
calized parsing models of the time. More recently,
Petrov et al (2006) refined techniques originally
proposed by Matsuzaki et al (2005) and Prescher
51
SBJ subject with nominative case marker
OBJ complement with accusative case marker
COMP complement with adverbial postposition
ADV NP that function as adverbial phrase
VOC noun with vocative case maker
LV NP coupled with ?light? verb construction
Table 2: Function tags in the Korean treebank
model F1 F1?40
Korean
coarse 52.78 56.55
w/ function tags 56.18 60.21
English (small)
coarse 72.20 73.29
w/ function tags 70.50 71.78
English (standard)
coarse 71.61 72.74
w/ function tags 72.82 74.05
Table 3: Parser scores for Treebank PCFGs in Korean
and English with and without function tags. The small
English results were produced by training on ?02?04.
(2005) for automatically learning latent annotations,
resulting in state of the art parsing performance with
cubic-time parsing algorithms.
We begin this section by conducting some sim-
ple experiments with the existing function tags, and
then apply the latent annotation learning procedures
of Petrov et al (2006) to the KTB.
4.1 Function tags
The KTB has function tags that mark grammatical
functions of NP and S nodes (Han et al, 2001),
which we list all of them in Table 2. These function
tags are principally grammatical markers. As men-
tioned above, the parsing scores for both English
and Korean presented in Table 1 were produced with
grammars stripped of their function tags. This is
standard practice in English, where the existing tags
are known not to help very much. Table 3 presents
results of parsing with grammars with nonterminals
that retain these function tags (we include results
from Section 3 for comparison). Note that evalua-
tion is done against the unannotated gold standard
parse trees by removing the function tags after pars-
ing with them.
The results for Korean are quite pronounced:
we see a nearly seven-point improvement when re-
taining the existing tags. This very strongly sug-
gests that the KTB nonterminals are too coarse
when stripped of their function tags, and raises the
question of whether further improvement might be
gained from latent annotations.
The English scores allow us to make another point.
Retaining the provided function tags results in a
solid performance increase with the standard train-
ing corpus, but actually hurts performance when
training on the small dataset. Note clearly that this
does not suggest that parsing performance with the
grammar from the small English data could not be
improved with latent annotations (indeed, we will
show that they can), but only that the given annota-
tions do not help improve parsing accuracy. Taking
the Korean and English accuracy results from this ta-
ble together provides another piece of evidence that
the Korean nonterminal set is too coarse.
4.2 Latent annotations
We applied the latent annotation learning procedures
of Petrov et al2 to refine the nonterminals in the
KTB. The trainer learns refinements over the coarse
version of the KTB (with function tags removed). In
this experiment, rather than doing 10-fold cross vali-
dation, we split the KTB into training, development,
and test sets that roughly match the 80/10/10 splits
of the folds:
section file IDs
training 302000 to 316999
development 317000 to 317999
testing 320000 to 320999
This procedure results in grammars which can then
be used to parse new sentences. Table 4 displays the
parsing accuracy results for parsing with the gram-
mar (after smoothing) at the end of each split-merge-
smooth cycle.3 The scores in this table show that,
just as with the PTB, nonterminal refinement makes
a huge difference in parser performance.
Again with the caveat that direct comparison of
parsing scores across annotation schemes must be
taken loosely, we note that the KTB parsing accu-
racy is still about 10 points lower than the best ac-
2http://code.google.com/p/berkeleyparser/
3As described in Petrov et al (2006), to score a parse tree
produced with a refined grammar, we can either take the Viterbi
derivation or approximate a sum over derivations before project-
ing back to the coarse tree for scoring.
52
Viterbi max-sum
cycle F1 F1?40 F1 F1?40
1 56.93 61.11 61.04 64.23
2 63.82 67.94 66.31 68.90
3 69.86 72.83 72.85 75.63
4 74.36 77.15 77.18 78.18
5 78.07 80.09 79.93 82.04
6 78.91 81.55 80.85 82.75
Table 4: Parsing accuracy on Korean test data from the
grammars output by the Berkeley state-splitting grammar
trainer. For comparison, parsing all sentences of ?22 in
the PTB with the same trainer scored 89.58 (max-sum
parsing with five cycles) with the standard training corpus
and 85.21 when trained on ?2?4.
curacy scores produced in parsing the PTB which,
in our experiments, were 89.58 (using max-sum to
parse all sentences with the grammar obtained after
five cycles of training).
An obvious suspect for the difference in parsing
accuracy with latent grammars between English and
Korean is the difference in training set sizes. This
turns out not to be the case. We learned latent anno-
tations on sections 2?4 of the PTB and again tested
on section 22. The accuracy scores on the test set
peak at 85.21 (max-sum, all sentences, five cycles of
training). This is about five points lower than the En-
glish grammar trained on sections 2?21, but is still
over four points higher than the KTB results.
In the next section, we turn to one of the theoret-
ical difficulties with Korean parsing with which we
began the paper.
5 NULL elements
Both the PTB and KTB include many NULL ele-
ments. For English, these elements are traces de-
noting constituent movement. In the KTB, there
are many more kinds of NULL elements, in includ-
ing trace markers, zero pronouns, relative clause re-
duction, verb deletions, verb ellipsis, and other un-
known categories. Standard practice in English pars-
ing is to remove NULL elements in order to avoid
the complexity of parsing with ?-productions. How-
ever, another approach to parsing that avoids such
productions is to retain the NULL elements when
reading the grammar; at test time, the parser is given
sentences that contain markers denoting the empty
elements. To evaluate, we remove these elements
model F1 F1?40 tokens
English (standard training corpus)
coarse 71.61 72.74 950K
w/ function tags 72.82 74.05 950K
w/ NULLs 73.29 74.38 1,014K
Korean
w/ verb ellipses 52.85 56.52 3,200
w/ traces 55.88 59.42 3,868
w/ r.c. markers 56.74 59.87 3,794
w/ zero pronouns 57.56 61.17 4,101
latent (5) w/ NULLs 89.56 91.03 22,437
Table 5: Parser scores for Treebank PCFGs in English
and Korean with NULL elements. Tokens denotes the
number of words in the test data. The latent grammar
was trained for five iterations.
from the resulting parse trees output by the parser
and compare against the stripped-down gold stan-
dard used in previous sections, in order to provide
a fair point of comparison.
Parsing in this manner helps us to answer the ques-
tion of how much easier or more difficult parsing
would be if the NULL elements were present. In
this section, we present results from a variety of ex-
periments parsing will NULL tokens in this manner.
These results can be seen in Table 5. The first ob-
servation from this table is that in English, retaining
NULL elements makes a few points difference.
The first four rows of the KTB portion of Table 5
contains results with retaining different classes of
NULL elements, one at a time, according to the man-
ner described above. Restoring deleted pronouns
and relative clause markers has the largest effect,
suggesting that the absence of these optional ele-
ments removes key cues needed for parsing.
In order to provide a more complete picture of
the effect of empty elements, we train the Berkeley
latent annotation system on a version of the KTB
in which all empty elements are retained. The fi-
nal row of Table 5 contains the score obtained when
evaluating parse trees produced from parsing with
the grammar after the fifth iteration (after which per-
formance began to fall). With the empty elements,
we have achieved accuracy scores that are on par
with the best accuracy scores obtained parsing the
English Treebank.
53
6 Tree substitution grammars
We have shown that coarse labels and the prevalence
of NULL elements in Korean both contribute to pars-
ing difficulty. We now turn to grammar formalisms
that allow us to work with larger fragments of parse
trees than the height-one rules of standard context-
free grammars. Tree substitution grammars (TSGs)
have been shown to improve upon the standard En-
glish Treebank grammar (Bod, 2001) in parser ac-
curacy, and more recently, techniques for inferring
TSG subtrees in a Bayesian framework have enabled
learning more efficiently representable grammars,
permitting some interesting analysis (O?Donnell et
al., 2009; Cohn et al, 2009; Post and Gildea, 2009).
In this section, we try parsing the KTB with TSGs.
We experiment with different methods of learning
TSGs to see whether they can reveal any insights
into the difficulties parsing Korean.
6.1 Head rules
TSGs present some difficulties in learning and rep-
resentation, but a simple extraction heuristic called
a spinal grammar has been shown to be very use-
ful (Chiang, 2000; Sangati and Zuidema, 2009; Post
and Gildea, 2009). Spinal subtrees are extracted
from a parse tree by using a set of head rules to
maximally project each lexical item (a word or mor-
pheme). Each node in the parse tree having a differ-
ent head from its parent becomes the root of a new
subtree, which induces a spinal TSG derivation in
the parse tree (see Figure 1). A probabilistic gram-
mar is derived by taking counts from these trees,
smoothing them with counts of all depth-one rules
from the same training set, and setting rule probabil-
ities to relative frequency.
This heuristic requires a set of head rules, which
we present in Table 6. As an evaluation of our rules,
we list in Table 7 the accuracy results for parsing
with spinal grammars extracted using the head rules
we developed as well as with two head rule heuris-
tics (head-left and head-right). As a point of compar-
ison, we provide the same results for English, using
the standard Magerman/Collins head rules for En-
glish (Magerman, 1995; Collins, 1997). Function
tags were retained for Korean but not for English.
We observe a number of things from Table 7.
First, the relative performance of the head-left and
NT RC rule
S SFN second rightmost child
VV EFN rightmost XSV
VX EFN rightmost VJ or CO
ADJP EFN rightmost VJ
CV EFN rightmost VV
LV EFN rightmost VV
NP EFN rightmost CO
VJ EFN rightmost XSV or XSJ
VP EFN rightmost VX, XSV, or VV
? ? rightmost child
Table 6: Head rules for the Korean Treebank. NT is the
nonterminal whose head is being determined, RC identi-
fies the label of its rightmost child. The default is to take
the rightmost child as the head.
head-right spinal grammars between English and
Korean capture the linguistic fact that English is pre-
dominantly head-first and Korean is predominantly
head-final. In fact, head-finalness in Korean was so
strong that our head rules consist of only a handful
of exceptions to it. The default rule makes heads
of postpositions (case and information clitics) such
as dative case marker and topic marker. It is these
words that often have dependencies with words in
the rest of the sentence. The exceptions concern
predicates that occur in the sentence-final position.
As an example, predicates in Korean are composed
of several morphemes, the final one of which indi-
cates the mood of the sentence. However, this mor-
pheme often does not require any inflection to re-
flect long-distance agreement with the rest of the
sentence. Therefore, we choose the morpheme that
would be considered the root of the phrase, which
in Korean is the verbalization/adjectivization suf-
fix, verb, adjective, auxiliary predicate, and copula
(XSV, XSJ, VV, VJ, VX, CO). These items often in-
clude the information about valency of the predicate.
Second, in both languages, finer-grained specifi-
cation of head rules results in performance above
that of the heuristics (and in particular, the head-
left heuristic for English and head-right heuristic for
Korean). The relative improvements in the two lan-
guages are in line with each other: significant, but
not nearly as large as the difference between the
head-left and head-right heuristics.
Finally, we note that the test results together sug-
gest that parsing with spinal grammars may be a
54
(a) TOP
S
NP-SBJ
NPR
???
NNC
??
PAU
?
VP
NP-ADV
DAN
?
NNC
?
VP
VV
NNC
??
XSV
??
EPF
?
EFN
?
SFN
.
(b) S
NP-SBJ
NPR
???
NNC PAU
VP SFN
(c) S
NP-SBJ VP SFN
.
Figure 1: (a) A KTB parse tree; the bold nodes denote the top-level spinal subtree using our head selection rules. (b)
The top-level spinal subtree using the head-left and (c) head-right extraction heuristics. A gloss of the sentence is
Doctor Schwartz was fired afterward.
model F1 F1?40 size
Korean
spinal (head left) 59.49 63.33 49K
spinal (head right) 66.05 69.96 29K
spinal (head rules) 66.28 70.61 29K
English
spinal (head left) 77.92 78.94 158K
spinal (head right) 72.73 74.09 172K
spinal (head rules) 78.82 79.79 189K
Table 7: Spinal grammar scores on the KTB and on PTB
section 22.
good evaluation of a set of head selection rules.
6.2 Induced tree substitution grammars
Recent work in applying nonparametric machine
learning techniques to TSG induction has shown that
the resulting grammars improve upon standard En-
glish treebank grammars (O?Donnell et al, 2009;
Cohn et al, 2009; Post and Gildea, 2009). These
techniques use a Dirichlet Process prior over the sub-
tree rewrites of each nonterminal (Ferguson, 1973);
this defines a model of subtree generation that pro-
duces new subtrees in proportion to the number of
times they have previously been generated. Infer-
ence under this model takes a treebank and uses
Gibbs sampling to determine how to deconstruct a
parse tree into a single TSG derivation. In this sec-
tion, we apply these techniques to Korean.
This TSG induction requires one to specify a base
measure, which assigns probabilities to subtrees be-
ing generated for the first time in the model. One
base measure employed in previous work scored a
subtree by multiplying together the probabilities of
the height-one rules inside the subtree with a ge-
ometric distribution on the number of such rules.
Since Korean is considered to be a free word-order
language, we modified this base measure to treat the
children of a height-one rule as a multiset (instead of
a sequence). This has the effect of producing equiva-
lence classes among the sets of children of each non-
terminal, concentrating the mass on these classes in-
stead of spreading it across their different instantia-
tions.
To build the sampled grammars, we initialized the
samplers from the best spinal grammar derivations
and ran them for 100 iterations (once again, func-
tion tags were retained). We then took the state of
the training data at every tenth iteration, smoothed
together with the height-one rules from the standard
Treebank. The best score on the development data
for a sampled grammar was 68.93 (all sentences)
and 73.29 (sentences with forty or fewer words):
well above the standard Treebank scores from ear-
lier sections and above the spinal heuristics, but well
below the scores produced by the latent annotation
learning procedures (a result that is consistent with
English).
This performance increase reflects the results for
English demonstrated in the above works. We see a
large performance increase above the baseline Tree-
bank grammar, and a few points above the best
spinal grammar. One nice feature of these induced
TSGs is that the rules learned lend themselves to
analysis, which we turn to next.
6.3 Word order
In addition to the base measure mentioned above,
we also experimented with the standard base mea-
55
NP
NPR NNC
??
NNU NNX
?
Figure 2: Example of a long distance dependency learned
by TSG induction.
sure proposed by Cohn et al and Post & Gildea, that
treats the children of a nonterminal as a sequence.
The grammars produced sampling under a model
with this base measure were not substantively differ-
ent from those of the unordered base measure. A par-
tial explanation for this is that although Korean does
permit a significant amount of reordering relative to
English, the sentences in the KTB come from writ-
ten newswire text, where word order is more stan-
dardized. Korean sentences are characterized as hav-
ing a subject-object-verb (SOV) word order. There
is some flexibility; OSV, in particular, is common
in spoken Korean. In formal writing, though, SOV
word order is overwhelmingly preferred. We see this
reflected in the KTB, where SOV sentences are 63.5
times more numerous that OSV among sentences
that have explicitly marked both the subject and the
object. However, word order is not completely fixed
even in the formal writing. NP-ADV is most likely
to occur right before the VP it modifies, but can be
moved earlier. For example,
S? NP-SBJ NP-ADV VP
is 2.4 times more frequent than the alternative with
the order of the NPs reversed.
Furthermore, the notion of free(er) word order
does not apply to all constituents. An example is
nonterminals directly above preterminals. A Korean
verb may have up to seven affixes; however, they al-
ways agglutinate in a fixed order.
6.4 Long distance dependencies
The TSG inference procedure can be thought of
as discovering structural collocations in parse trees.
The model prefers subtrees that are common in the
data set and that comprise highly probable height-
one rules. The parsing accuracy of these grammars
is well below state of the art, but the grammars are
smaller, and the subtrees learned can help us analyze
the parse structure of the Treebank. One particular
class of subtree is one that includes multiple lexical
items with intervening nonterminals, which repre-
sent long distance dependencies that commonly co-
occur. In Korean, a certain class of nouns must ac-
company a particular class of measure word (a mor-
pheme) when counting the noun. In the example
shown in Figure 2, (NNC ??) (members of as-
sembly) is followed by NNU, which expands to in-
dicate ordinal, cardinal, and numeral nouns; NNU is
in turn followed by (NNX?), the politeness neutral
measure word for counting people.
7 Summary & future work
In this paper, we addressed several difficult aspects
of parsing Korean and showed that good parsing ac-
curacy for Korean can be achieved despite the small
size of the corpus.
Analysis of different parsing results from differ-
ent grammatical formalisms yielded a number of
useful observations. We found, for example, that the
set of nonterminals in the KTB is not differentiated
enough for accurate parsing; however, parsing accu-
racy improves substantially from latent annotations
and state-splitting techniques that have been devel-
oped with English as a testbed. We found that freer
word order may not be as important as might have
been thought from basic a priori linguistic knowl-
edge of Korean.
The prevalence of NULL elements in Korean is
perhaps the most interesting difficulty in develop-
ing good parsing approaches for Korean; this is
a key difference from English parsing that to our
knowledge is not addressed by any available tech-
niques. One potential approach is a special an-
notation of parents with deleted nodes in order to
avoid conflating rewrite distributions. For example,
S ? VP is the most common rule in the Korean
treebank after stripping away empty elements; how-
ever, this is a result of condensing the rule S? (NP-
SBJ *pro*) VP and S?VP, which presumably have
different distributions. Another approach would be
to attempt automatic recovery of empty elements as
a pre-processing step.
Acknowledgments We thank the anonymous re-
viewers for their helpful comments. This work
was supported by NSF grants IIS-0546554 and ITR-
0428020.
56
References
Rens Bod. 2001. What is the minimal set of fragments
that achieves maximal parse accuracy? In Proc. ACL,
Toulouse, France, July.
Eugene Charniak. 1996. Tree-bank grammars. In Proc.
of the National Conference on Artificial Intelligence,
pages 1031?1036.
David Chiang. 2000. Statistical parsing with an
automatically-extracted tree adjoining grammar. In
Proc. ACL, Hong Kong.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proc. NAACL.
Michael Collins. 1997. Three penerative, lexicalised
models for statistical parsing. In Proc. ACL/EACL.
Thomas S. Ferguson. 1973. A Bayesian analysis of
some nonparametric problems. Annals of Mathemat-
ical Statistics, 1(2):209?230.
Chung-Hye Han and Martha Palmer. 2004. A mor-
phological tagger for Korean: Statistical tagging com-
bined with corpus-based morphological rule applica-
tion. Machine Translation, 18(4):275?297.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank version 2.0. Technical report,
IRCS, University of Pennsylvania.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for Penn Korean Treebank.
Technical report, IRCS, University of Pennsylvania.
Na-Rae Han. 2006. Korean zero pronouns: analysis and
resolution. Ph.D. thesis, University of Pennsylvania,
Philadelphia, PA, USA.
Ulf Hermjakob. 2000. Rapid parser development: a ma-
chine learning approach for Korean. In Proc. NAACL,
pages 118?123, May.
Mark Johnson. 1998. PCFGmodels of linguistic tree rep-
resentations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and SinWon Yoon. 2004. Tree-local
MCTAG with shared nodes: Word order variation in
German and Korean. In Proc. TAG+7, Vancouver,
May.
Dan Klein and Chris Manning. 2003. Accurate unlexi-
calized parsing. In Proc. ACL.
Do-Gil Lee and Hae-Chang Rim. 2005. Probabilistic
models for Korean morphological analysis. In Com-
panion to the Proceedings of the International Joint
Conference on Natural Language Processing, pages
197?202.
Sang-zoo Lee, Jun-ichi Tsujii, and Hae-Chang Rim.
2000. Hidden markov model-based Korean part-of-
speech tagging considering high agglutinativity, word-
spacing, and lexical correlativity. In Proc. ACL.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proc. ACL.
Samuel E. Martin. 1992. Reference Grammar of Korean:
A Complete Guide to the Grammar and History of the
Korean Language. Tuttle Publishing.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic CFG with latent annotations. In
Proc. ACL, Ann Arbor, Michigan.
Timothy J. O?Donnell, Noah D. Goodman, and Joshua B.
Tenenbaum. 2009. Fragment grammar: Exploring
reuse in hierarchical generative processes. Technical
report, MIT.
Seong-Bae Park, Jeong-Ho Chang, and Byoung-Tak
Zhang. 2004. Korean compound noun decomposition
using syllabic information only. In Computational
Linguistics and Intelligent Text Processing (CICLing),
pages 146?157.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. COLING/ACL, Syd-
ney, Australia, July.
Matt Post and Daniel Gildea. 2009. Bayesian learning of
a tree substitution grammar. In Proc. ACL, Singapore,
Singapore, August.
Detlef Prescher. 2005. Inducing head-driven PCFGs
with latent heads: Refining a tree-bank grammar for
parsing. Machine Learning: ECML 2005, pages 292?
304.
Owen Rambow and Young-Suk Lee. 1994. Word order
variation and tree-adjoining grammar. Computational
Intelligence, 10:386?400.
Ines Rehbein and Josef van Genabith. 2007. Eval-
uating evaluation measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
(NODALIDA).
Federico Sangati and Willem Zuidema. 2009. Unsuper-
vised methods for head assignments. In Proc. EACL.
Anoop Sarkar and Chung-hye Han. 2002. Statistical
morphological tagging and parsing of Korean with an
LTAG grammar. In Proc. TAG+6.
57
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 478?484,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Joshua 3.0: Syntax-based Machine Translation
with the Thrax Grammar Extractor
Jonathan Weese1, Juri Ganitkevitch1, Chris Callison-Burch1, Matt Post2 and Adam Lopez1,2
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present progress on Joshua, an open-
source decoder for hierarchical and syntax-
based machine translation. The main fo-
cus is describing Thrax, a flexible, open
source synchronous context-free grammar ex-
tractor. Thrax extracts both hierarchical (Chi-
ang, 2007) and syntax-augmented machine
translation (Zollmann and Venugopal, 2006)
grammars. It is built on Apache Hadoop for
efficient distributed performance, and can eas-
ily be extended with support for new gram-
mars, feature functions, and output formats.
1 Introduction
Joshua is an open-source1 toolkit for hierarchical
machine translation of human languages. The origi-
nal version of Joshua (Li et al, 2009) was a reim-
plementation of the Python-based Hiero machine-
translation system (Chiang, 2007); it was later ex-
tended (Li et al, 2010) to support richer formalisms,
such as SAMT (Zollmann and Venugopal, 2006).
The main focus of this paper is to describe this
past year?s work in developing Thrax (Weese, 2011),
an open-source grammar extractor for Hiero and
SAMT grammars. Grammar extraction has shown
itself to be something of a black art, with decod-
ing performance depending crucially on a variety
of features and options that are not always clearly
described in papers. This hindered direct com-
parison both between and within grammatical for-
malisms. Thrax standardizes Joshua?s grammar ex-
1http://github.com/joshua-decoder/joshua
traction procedures by providing a flexible and con-
figurable means of specifying these settings. Sec-
tion 3 presents a systematic comparison of the two
grammars using identical feature sets.
In addition, Joshua now includes a single pa-
rameterized script that implements the entire MT
pipeline, from data preparation to evaluation. This
script is built on top of a module called CachePipe.
CachePipe is a simple wrapper around shell com-
mands that uses SHA-1 hashes and explicitly-
provided lists of dependencies to determine whether
a command needs to be run, saving time both in run-
ning and debugging machine translation pipelines.
2 Thrax: grammar extraction
In modern machine translation systems such as
Joshua (Li et al, 2009) and cdec (Dyer et al, 2010),
a translation model is represented as a synchronous
context-free grammar (SCFG). Formally, an SCFG
may be considered as a tuple
(N,S, T?, T? , G)
where N is a set of nonterminal symbols of the
grammar, S ? N is the goal symbol, T? and T?
are the source- and target-side terminal symbol vo-
cabularies, respectively, and G is a set of production
rules of the grammar.
Each rule in G is of the form
X ? ??, ?,??
where X ? N is a nonterminal symbol, ? is a se-
quence of symbols from N ? T?, ? is a sequence of
478
symbols from N ? T? , and ? is a one-to-one cor-
respondence between the nonterminal symbols of ?
and ?.
The language of an SCFG is a set of ordered pairs
of strings. During decoding, the set of candidate
translations of an input sentence f is the set of all
e such that the pair (f, e) is licensed by the transla-
tion model SCFG. Each candidate e is generated by
applying a sequence of production rules (r1 . . . rn).
The cost of applying each rule is:
w(X ? ??, ??) =
?
i
?i(X ? ??, ??)
?i (1)
where each ?i is a feature function and ?i is the
weight for ?i. The total translation model score of
a candidate e is the product of the rules used in its
derivation. This translation model score is then com-
bined with other features (such as a language model
score) to produce an overall score for each candidate
translation.
2.1 Hiero and SAMT
Throughout this work, we will reference two par-
ticular SCFG types known as Hiero and Syntax-
Augmented Machine Translation (SAMT).
A Hiero grammar (Chiang, 2007) is an SCFG
with only one type of nonterminal symbol, tradi-
tionally labeled X . A Hiero grammar can be ex-
tracted from a parallel corpus of word-aligned sen-
tence pairs as follows: If (f ji , e
l
k) is a sub-phrase
of the sentence pair, we say it is consistent with
the pair?s alignment if none of the words in f ji are
aligned to words outside of elk, and vice-versa. The
consistent sub-phrase may be extracted as an SCFG
rule. Furthermore, if a consistent phrase is contained
within another one, a hierarchical rule may be ex-
tracted by replacing the smaller piece with a nonter-
minal.
An SAMT grammar (Zollmann and Venugopal,
2006) is similar to a Hiero grammar, except that the
nonterminal symbol set is much larger, and its la-
bels are derived from a parse tree over either the
source or target side in the following manner. For
each rule, if the target side is spanned by one con-
stituent of the parse tree, we assign that constituent?s
label as the nonterminal symbol for the rule. Other-
wise, we assign an extended category of the form
C1 + C2, C1/C2, or C2 \C1 ? indicating that the
das begr??e ich sehr .
i
very
much
welcome this
.
PRP
NP
S
RB RB
ADVP
VP
VBP DT
NP
.
Figure 1: An aligned sentence pair.
target side spans two adjacent constituents, is a C1
missing a C2 to the right, or is a C1 missing a C2
on the left, respectively. Table 1 contains a list of
Hiero and SAMT rules extracted from the training
sentence pair in Figure 1.
2.2 System overview
The following were goals in the design of Thrax:
? the ability to extract different SCFGs (such as
Hiero and SAMT), and to adjust various extrac-
tion parameters for the grammars;
? the ability to easily change and extend the fea-
ture sets for each rule
? scalability to arbitrarily large training corpora.
Thrax treats the grammar extraction and scoring
as a series of dependent Hadoop jobs. Hadoop
(Venugopal and Zollmann, 2009) is an implementa-
tion of Google?s MapReduce (Dean and Ghemawat,
2004), a framework for distributed processing of
large data sets. Hadoop jobs have two parts. In the
map step, a set of key/value pairs is mapped to a set
of intermediate key/value pairs. In the reduce step,
all intermediate values associated with an interme-
diate key are merged.
The first step in the Thrax pipeline is to extract all
the grammar rules. The map step in this job takes as
input word-aligned sentence pairs and produces a set
of ordered pairs (r, c) where r is a rule and c is the
number of times it was extracted. During the reduce
step, these rule counts are summed, so the result is
a set of rules, along with the total number of times
each rule was extracted from the entire corpus.
479
Span Hiero SAMT
[1, 3] X ? ?sehr, very much? ADV P ? ?sehr, very much?
[0, 3] X ? ?X sehr, X very much? PRP +ADV P ? ?PRP sehr, PRP very much?
[3, 4] X ? ?begru??e,welcome? V BP ? ?begru??e,welcome?
[0, 6] X ? ?X ich sehr ., i very much X .? S ? ?V P ich sehr ., i very much V P .?
[0, 6] X ? ?X ., X .? S ? ?S/. ., S/. .?
Table 1: A subset of the Hiero and SAMT rules extracted from the sentence pair of Figure 1.
Given the rules and their counts, a separate
Hadoop job is run for each feature. These jobs can
all be submitted at once and run in parallel, avoid-
ing the linear sort-and-score workflow. The output
from each feature job is the same set of pairs (r, c)
as the input, except each rule r has been annotated
with some feature score f .
After the feature jobs have been completed, we
have several copies of the grammar, each of which
has been scored with one feature. A final Hadoop
job combines all these scores to produce the final
grammar.
Some users may not have access to a Hadoop
cluster. Thrax can be run in standalone or pseudo-
distributed mode on a single machine. It can also
be used with Amazon Elastic MapReduce,2 a web
service that provides computation time on a Hadoop
cluster on-demand.
2.3 Extraction
The first step in the Thrax workflow is the extraction
of grammar rules from an input corpus. As men-
tioned above, Hiero and SAMT grammars both re-
quire a parallel corpus with word-level alignments.
SAMT additionally requires that the target side of
the corpus be parsed.
There are several parameters that can make a sig-
nificant difference in a grammar?s overall translation
performance. Each of these parameters is easily ad-
justable in Thrax by changing its value in a configu-
ration file.
? maximum rule span
? maximum span of consistent phrase pairs
? maximum number of nonterminals
? minimum number of aligned terminals in rule
2http://aws.amazon.com/elasticmapreduce/
? whether to allow adjacent nonterminals on
source side
? whether to allow unaligned words at the edges
of consistent phrase pairs
Chiang (2007) gives reasonable heuristic choices
for these parameters when extracting a Hiero gram-
mar, and Lopez (2008) confirms some of them (max-
imum rule span of 10, maximum number of source-
side symbols at 5, and maximum number of non-
terminals at 2 per rule). ?) provided comparisons
among phrase-based, hierarchical, and syntax-based
models, but did not report extensive experimentation
with the model parameterizations.
When extracting Hiero- or SAMT-style gram-
mars, the first Hadoop job in the Thrax workflow
takes in a parallel corpus and produces a set of rules.
But in fact Thrax?s extraction mechanism is more
general than that; all it requires is a function that
maps a string to a set of rules. This makes it easy
to implement new grammars and extract them using
Thrax.
2.4 Feature functions
Thrax considers feature functions of two types: first,
there are features that can be calculated by looking
at each rule in isolation. Such features do not re-
quire a Hadoop job to calculate their scores, since
we may inspect the rules in any order. (In practice,
we calculate the scores at the very last moment be-
fore outputting the final grammar.) We call these
features simple features. Thrax implements the fol-
lowing simple features:
? a binary indicator functions denoting:
? whether the rule is purely abstract (i.e.,
has no terminal symbols)
480
? the rule is purely lexical (i.e., has no non-
terminals)
? the rule is monotonic or has reordering
? the rule has adjacent nonterminals on the
source side
? counters for
? the number of unaligned words in the rule
? the number of terminals on the target side
of the rule
? a constant phrase penalty
In addition to simple features, Thrax also imple-
ments map-reduce features. These are features that
require comparing rules in a certain order. Thrax
uses Hadoop to sort the rules efficiently and calcu-
late these feature functions. Thrax implements the
following map-reduce features:
? Phrasal translation probabilities p(?|?) and
p(?|?), calculated with relative frequency:
p(?|?) =
C(?, ?)
C(?)
(2)
(and vice versa), where C(?) is the number of
times a given event was extracted.
? Lexical weighting plex(?|?,A) and
plex(?|?,A). We calculate these weights
as given in (Koehn et al, 2003): let A be the
alignment between ? and ?, so (i, j) ? A if
and only if the ith word of ? is aligned to the
jth word of ?. Then we can define plex(?|?) as
n?
i=1
1
|{j : (i, j) ? A}|
?
(i,j)?A
w(?j |?i) (3)
where ?i is the ith word of ?, ?j is the jth word
of ?, and w(y|x) is the relative frequency of
seeing word y given x.
? Rarity penalty, given by
exp(1? C(X ? ??, ??)) (4)
where again C(?) is a count of the number of
times the rule was extracted.
The above features are all implemented and can
be turned on or off with a keyword in the Thrax con-
figuration file.
It is easy to extend Thrax with new feature func-
tions. For simple features, all that is needed is to im-
plement Thrax?s SIMPLEFEATURE interface defin-
ing a method that takes in a rule and calculates a
feature score. Map-reduce features are slightly more
complex: to subclass MAPREDUCEFEATURE, one
must define a mapper and reducer, but also a sort
comparator to determine in what order the rules are
compared during the reduce step.
2.5 Related work
Joshua includes a simple Hiero extractor (Schwartz
and Callison-Burch, 2010). The extractor runs as a
single Java process, which makes it difficult to ex-
tract larger grammars, since the host machine must
have enough memory to hold all of the rules at once.
Joshua?s extractor scores each rule with three feature
functions ? lexical probabilities in two directions,
and one phrasal probability score p(?|?).
The SAMT implementation of Zollmann and
Venugopal (2006) includes a several-thousand-line
Perl script to extract their rules. In addition to
phrasal and lexical probabilities, this extractor im-
plements several other features that are also de-
scribed in section 2.4.
Finally, the cdec decoder (Dyer et al, 2010) in-
cludes a grammar extractor that performs well only
when all rules can be held in memory.
Memory usage is a limitation of both the Joshua
and cdec extractors. Translation models can be very
large, and many feature scores require accumulation
of statistical data from the entire set of extracted
rules. Since it is impractical to keep the entire gram-
mar in memory, rules are usually sorted on disk and
then read sequentially. Different feature calcula-
tions may require different sort orders, leading to a
linear workflow that alternates between sorting the
grammar and calculating a feature score. To cal-
culate more feature scores, more sorts have to be
performed. This discourages the implementation of
new features. For example, Joshua?s built-in rule ex-
tractor calculates the phrasal probability p(?|?) for
each rule but, to save time, does not calculate its ob-
vious counterpart p(?|?), which would require an-
other sort.
481
Language pair sentences (K) words (M)
cs?en 332 4.7
de?en 279 5.5
en?cs 487 6.9
en?de 359 7.2
en?fr 682 12.5
fr?en 792 14.4
Table 2: Training data size after subsampling.
The SAMT extractor does not have a problem
with large data sets; SAMT can run on Hadoop, as
Thrax does.
The Joshua and cdec extractors only extract Hiero
grammars, and Zollmann and Venugopal?s extractor
can only extract SAMT-style grammars. They are
not designed to score arbitrary feature sets, either.
Since variation in translation models and feature sets
can have a significant effect on translation perfor-
mance, we have developed Thrax in order to make it
easy to build and test new models.
3 Experiments
We built systems for six language pairs for the WMT
2011 shared task: cz-en, en-cz, de-en, en-de, fr-en,
and en-fr.3 For each language pair, we built both
SAMT and hiero grammars.4 Table 3 contains the
results on the complete WMT 2011 test set.
To train the translation models, we used the pro-
vided Europarl and news commentary data. For cz-
en and en-cz, we also used sections of the CzEng
parallel corpus (Bojar and Z?abokrtsky?, 2009). The
parallel data was subsampled using Joshua?s built-
in subsampler to select sentences with n-grams rel-
evant to the tuning and test set. We used SRILM
to train a 5-gram language model with Kneser-Ney
smoothing using the appropriate side of the paral-
lel data. For the English LM, we also used English
Gigaword Fourth Edition.5
Before extracting an SCFG with Thrax, we used
the provided Perl scripts to tokenize and normalize
3fr=French, cz=Czech, de=German, en=English.
4Except for fr-en and en-fr. We were unable to decode with
SAMT grammars for these language pairs due to their large size.
We have since resolved this issue and will have scores for the
final version of the paper.
5LDC2009T13
pair hiero SAMT improvement
cz-en 21.1 21.7 +0.6
en-cz 16.8 16.9 +0.1
de-en 18.9 19.5 +0.6
en-de 14.3 14.9 +0.6
fr-en 28.0 - -
en-fr 30.4 - -
Table 3: Single-reference BLEU-4 scores.
the data. We also removed any sentences longer than
50 tokens (after tokenization). For SAMT grammar
extraction, we parsed the English training data us-
ing the Berkeley Parser (Petrov et al, 2006) with the
provided Treebank-trained grammar.
We tuned the model weights against the
WMT08 test set (news-test2008) using Z-
MERT (Zaidan, 2009), an implementation of mini-
mum error-rate training included with Joshua. We
decoded the test set to produce a 300-best list of
unique translations, then chose the best candidate for
each sentence using Minimum Bayes Risk reranking
(Kumar and Byrne, 2004). Figure 2 shows an exam-
ple derivation with an SAMT grammar. To re-case
the 1-best test set output, we trained a true-case 5-
gram language model using the same LM training
data as before, and used an SCFG translation model
to translate from the lowercased to true-case output.
The translation model used rules limited to five to-
kens in length, and contained no hierarchical rules.
4 CachePipe: Cached pipeline runs
Machine translation pipelines involve the specifica-
tion and execution of many different datasets, train-
ing procedures, and pre- and post-processing tech-
niques that can have large effects on translation out-
come, and which make direct comparisons between
systems difficult. The complexity of managing these
pipelines and experimental environments has led to a
number of different experimental management sys-
tems, such as Experiment.perl,6 Joshua 2.0?s Make-
file system (Li et al, 2010), and LoonyBin (Clark
and Lavie, 2010). In addition to managing the
pipeline, these scripts employ different techniques
to avoid expensive recomputation by caching steps.
6http://www.statmt.org/moses/?n=
FactoredTraining.EMS
482
the
reactor type will be operated with uranium
VBN
DT+NP
GLUE
VP
PP
der reaktortyp , das nicht
angereichert
wird zwar mit uran betrieben
, which is
not
enriched
ist .
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
.
S
VBN
DT+NP
GLUE
VP
PP
NP
GLUE
NN
COMMA+SBAR+.
ADJP
JJ
S
Figure 2: An SAMT derivation. The shaded terminal symbols are the lexicalized part of a rule with terminals
and non-terminals. The unshaded terminals are directly dominated by a nonterminal symbol.
However, these approaches are based on simple but
unreliable heuristics (such as timestamps or file ex-
istence) to make the caching determination.
Our solution to the caching dependency problem
is CachePipe. CachePipe is designed with the fol-
lowing goals: (1) robust content-based dependency
checking and (2) ease of use, including minimal
editing of existing scripts. CachePipe is essentially
a wrapper around command invocations. Presented
with a command to run and a list of file dependen-
cies, it computes SHA-1 hashes of the dependencies
and of the command invocation and stores them; the
command is executed only if any of those hashes are
different from previous runs. A basic invocation in-
volves specifying (1) a name or identifier associated
with the command or step, (2) the command to run,
and (3) a list of file dependencies. For example, to
copy file a to b from a shell prompt, the following
command could be used:
cachecmd copy "cp a b" a b
The first time the command is run, the file would be
copied; afterwards, the command would be skipped
after CachePipe verified that the contents of the de-
pendencies a and b had not changed.
CachePipe is open-source software, distributed
with Joshua or available separately.7 It currently
provides both a shell script interface and a program-
matic API for Perl. It accepts a number of other
arguments and dependency types. It also serves as
the foundation of a new script in Joshua 3.0 that im-
plements the complete Joshua pipeline, from data
preparation to evaluation.
5 Future work
Thrax is currently limited to SCFG-based translation
models. A natural development would be to extract
GHKM grammars (Galley et al, 2004) or more re-
cent tree-to-tree models (Zhang et al, 2008; Liu et
al., 2009; Chiang, 2010). We also hope that Thrax
will continue to be extended with more feature func-
tions as researchers develop and contribute them.
Acknowledgements
This research was supported by in part by the Eu-
roMatrixPlus project funded by the European Com-
mission (7th Framework Programme), and by the
NSF under grant IIS-0713448. Opinions, interpre-
tations, and conclusions are the authors? alone.
7https://github.com/joshua-decoder/
cachepipe
483
References
Ondr?ej Bojar and Zdene?k Z?abokrtsky?. 2009. CzEng0.9:
Large Parallel Treebank with Rich Annotation.
Prague Bulletin of Mathematical Linguistics, 92. in
print.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
David Chiang. 2010. Learning to translate with source
and target syntax. In Proc. ACL, Uppsala, Sweden,
July.
Jonathan H. Clark and Alon Lavie. 2010. Loony-
bin: Keeping language technologists sane through au-
tomated management of experimental (hyper) work-
flows. In Proc. LREC.
Jeffrey Dean and Sanjay Ghemawat. 2004. Mapreduce:
Simplified data processing on large clusters. In OSDI.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Jonathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,
Vladimir Eidelman, and Philip Resnik. 2010. cdec: A
decoder, alignment, and learning framework for finite-
state and context-free translation models. In Proc.
ACL 2010 System Demonstrations, pages 7?12.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Morristown, NJ, USA.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Proc.
ACL, Suntec, Singapore, August.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proc. COLING, Manchester, UK,
August.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. ACL, Sydney, Aus-
tralia, July.
Lane Schwartz and Chris Callison-Burch. 2010. Hier-
archical phrase-based grammar extraction in joshua:
Suffix arrays and prefix trees. The Prague Bulletin of
Mathematical Linguistics, 93:157?166, January.
Mark Steedman. 1999. Alternating quantifier scope in
ccg. In Proc. ACL, Stroudsburg, PA, USA.
Ashish Venugopal and Andreas Zollmann. 2009. Gram-
mar based statistical MT on Hadoop: An end-to-end
toolkit for large scale PSCFG based MT. The Prague
Bulletin of Mathematical Linguistics, 91:67?78.
Jonathan Weese. 2011. A systematic comparison of syn-
chronous context-free grammars for machine transla-
tion. Master?s thesis, Johns Hopkins University, May.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,
Chew Lim Tan, and Sheng Li. 2008. A tree se-
quence alignment-based tree-to-tree translation model.
In Proc. ACL, Columbus, Ohio, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proc. NAACL Workshop on Statistcal Machine Trans-
lation, New York, New York.
484
NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 23?30,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Toward Tree Substitution Grammars with Latent Annotations
Francis Ferraro and Benjamin Van Durme and Matt Post
Center for Language and Speech Processing, and
Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We provide a model that extends the split-
merge framework of Petrov et al (2006) to
jointly learn latent annotations and Tree Sub-
stitution Grammars (TSGs). We then conduct
a variety of experiments with this model, first
inducing grammars on a portion of the Penn
Treebank and the Korean Treebank 2.0, and
next experimenting with grammar refinement
from a single nonterminal and from the Uni-
versal Part of Speech tagset. We present quali-
tative analysis showing promising signs across
all experiments that our combined approach
successfully provides for greater flexibility
in grammar induction within the structured
guidance provided by the treebank, leveraging
the complementary natures of these two ap-
proaches.
1 Introduction
Context-free grammars (CFGs) are a useful tool for
describing the structure of language, modeling a va-
riety of linguistic phenomena while still permitting
efficient inference. However, it is widely acknowl-
edged that CFGs employed in practice make unre-
alistic independence and structural assumptions, re-
sulting in grammars that are overly permissive. One
successful approach has been to refine the nonter-
minals of grammars, first manually (Johnson, 1998;
Klein and Manning, 2003) and later automatically
(Matsuzaki et al, 2005; Dreyer and Eisner, 2006;
Petrov et al, 2006). In addition to improving pars-
ing accuracy, the automatically learned latent anno-
tations of these latter approaches yield results that
accord well with human intuitions, especially at the
lexical or preterminal level (for example, separating
demonstrative adjectives from definite articles under
the DT tag). It is more difficult, though, to extend
this analysis to higher-level nonterminals, where the
long-distance interactions among latent annotations
of internal nodes are subtle and difficult to trace.
In another line of work, many researchers have ex-
amined the use of formalisms with an extended do-
main of locality (Joshi and Schabes, 1997), where
the basic grammatical units are arbitrary tree frag-
ments instead of traditional depth-one context-free
grammar productions. In particular, Tree Substitu-
tion Grammars (TSGs) retain the context-free prop-
erties of CFGs (and thus the cubic-time inference)
while at the same time allowing for the modeling of
long distance dependencies. Fragments from such
grammars are intuitive, capturing exactly the sorts of
phrasal-level properties (such as predicate-argument
structure) that are not present in Treebank CFGs and
which are difficult to model with latent annotations.
This paper is motivated by the complementarity
of these approaches. We present our progress in
learning latent-variable TSGs in a joint approach that
extends the split-merge framework of Petrov et al
(2006). We present our current results on the Penn
and Korean treebanks (Marcus et al, 1993; Han et
al., 2001), demonstrating that we are able to learn
fragments that draw on the strengths of both ap-
proaches. Table 1 situates this work among other
contributions.
In addition to experimenting directly with the
Penn and Korean Treebanks, we also conducted two
experiments in this framework with the Universal
23
CFG TSG
none Charniak ?97 Cohn et al ?09
manual Klein & Manning ?03 Bansal & Klein ?10
automatic Matsuzaki et al ?05 This paper
Petrov et al ?06
Dreyer & Eisner ?06
Table 1: Representative prior work in learning refine-
ments for context-free and tree substitution grammars,
with zero, manual, or automatically induced latent anno-
tations.
POS tagset (Petrov et al, 2011). First, we investigate
whether the tagset can be automatically derived af-
ter mapping all nonterminals to a single, coarse non-
terminal. Second, we begin with the mapping de-
fined by the tagset, and investigate how closely the
learned annotations resemble the original treebank.
Together with our TSG efforts, this work is aimed at
increased flexibility in the grammar induction pro-
cess, while retaining the use of Treebanks for struc-
tural guidance.
2 Background
2.1 Latent variable grammars
Latent annotation learning is motivated by the ob-
served coarseness of the nonterminals in treebank
grammars, which often group together nodes with
different grammatical roles and distributions (such
as the role of NPs in subject and object position).
Johnson (1998) presented a simple parent-annotation
scheme that resulted in significant parsing improve-
ment. Klein and Manning (2003) built on these ob-
servations, introducing a series of manual refine-
ments that captured multiple linguistic phenomena,
leading to accurate and fast unlexicalized parsing.
Later, automated methods for nonterminal refine-
ment were introduced, first splitting all categories
equally (Matsuzaki et al, 2005), and later refin-
ing nonterminals to different degrees (Petrov et al,
2006) in a split-merge EM framework. This lat-
ter approach was able to recover many of the splits
manually determined by Klein and Manning (2003),
while also discovering interesting, novel clusterings,
especially at the lexical level. However, phrasal-
level analysis of latent-variable grammars is more
difficult. (2006) observed that these grammars could
learn long-distance dependencies through sequences
of substates that place all or most of their weight on
(a) A TSG fragment.
SBAR
IN
for
S
NP VP
TO
to
VP
(b) Equivalent CFG rules.
SBAR ? IN S
IN ? for
S ? NP VP
VP ? TO VP
TO ? to
Figure 1: Simple example of a TSG fragment and an
equivalent representation with a CFG.
particular productions, but such patterns must be dis-
covered manually via extensive analysis.
2.2 Tree substitution grammars
Tree substitution grammars (TSGs) allow for com-
plementary analysis. These grammars employ an ex-
tended domain of locality over traditional context-
free grammars by generalizing the atomic units of the
grammar from depth-one productions to fragments
of arbitrary size. An example TSG fragment along
with equivalent CFG rules are depicted in Figure 1.
The two formalisms areweakly equivalent, and com-
puting the most probable derivation of a sentence
with a TSG can be done in cubic time.
Unfortunately, learning TSGs is not straight-
forward, in large part because TSG-specific re-
sources (e.g., large scale TSG-annotated treebanks)
do not exist. One class of existing approaches,
known as Data-Oriented Parsing, simply uses all the
fragments (Bod, 1993, DOP). This does not scale
well to large treebanks, forcing the use of implicit
representations (Goodman, 1996) or heuristic sub-
sets (Bod, 2001). It has also been generally ob-
served that the use of all fragments results in poor,
overfit grammars, though this can be addressed with
held-out data (Zollmann and Sima?an, 2005) or sta-
tistical estimators to rule out fragments that are un-
likely to generalize (Zuidema, 2007). More recently,
a number of groups have found success employing
Bayesian non-parametric priors (Post and Gildea,
2009; Cohn et al, 2010), which put a downward
pressure on fragment size except where the data
warrant the inclusion of larger fragments. Unfortu-
nately, proper inference under these models is in-
tractable, and though Monte Carlo techniques can
24
provide an approximation, the samplers can be com-
plex, difficult to code, and slow to converge.
This history suggests two approaches to state-split
TSGs: (1) a Bayesian non-parametric sampling ap-
proach (incorporate state-splitting into existing TSG
work), or (2) EM (incorporate TSG induction into
existing state-splitting work). We choose the latter
path, and in the next section will describe our ap-
proach which combines the simplicity of DOP, the
intuitions motivating the Bayesian approach, and the
efficiency of EM-based state-splitting.
In related work, Bansal and Klein (2010) combine
(1996)?s implicit DOP representation with a num-
ber of the manual refinements described in Klein and
Manning (2003). They achieve some of the best re-
ported parsing scores for TSGwork and demonstrate
the complementarity of the tasks, but their approach
is not able to learn arbitrary distributions over frag-
ments, and the state splits are determined in a fixed
pre-processing step. Our approach addresses both of
these limitations.
3 State-Split TSG Induction
In this sectionwe describe howwe combine the ideas
of dop, Bayesian-induced TSGs and Petrov et al
(2006)?s state-splitting framework.1 We are able to
do so by adding a coupling step to each iteration.
That is, each iteration is of the form:
(1) split all symbols in two,
(2) merge 50% of the splits, and
(3) couple existing fragments.
Because every step results in a new grammar, pro-
duction probabilities are fit to observed data by run-
ning at most 50 rounds of EM after every step listed
above.2 We focus on our contribution ? the cou-
pling step? and direct those interested in details re-
garding splitting/merging to (Petrov et al, 2006).
Let T be a treebank and let F be the set of all
possible fragments in T . Define a tree T ? T
as a composition of fragments {Fi}ni=1 ? F , with
T = F1 ? ? ? ? ? Fn. We use X to refer to an arbi-
trary fragment, with rX being the root of X . Two
1Code available at cs.jhu.edu/~ferraro.
2We additionally apply Petrov et al (2006)?s smoothing step
between split and merge.
fragments X and Y may compose (couple), which
we denote byX ?Y .3 We assume thatX and Y may
couple only if X ? Y is an observed subtree.
3.1 Coupling Procedure
While Petrov et al (2006) posit all refinements sim-
ulatenously and then retract half, applying this strat-
egy to the coupling step would result in a combina-
torial explosion. We control this combinatorial in-
crease in three ways. First, we assume binary trees.
Second, we introduce a constraint set C ? F that dic-
tates what fragments are permitted to compose into
larger fragments. Third, we adopt the iterative ap-
proach of split-merge and incrementally make our
grammar more complex by forbidding a fragment
from participating in ?chained couplings:? X ?Y ?Z
is not allowed unless eitherX ?Y or Y ?Z is a valid
fragment in the previous grammar (and the chained
coupling is allowed by C). Note that setting C = ?
results in standard split/merge, while C = F results
in a latently-refined dop-1 model.
We say that ?XY? represents a valid coupling ofX
and Y only if X ? Y is allowed by C, whereas ?XY?
represents an invalid coupling ifX?Y is not allowed
by C. Valid couplings result in new fragments. (We
describe how to obtain C in ?3.3.)
Given a constraint set C and a current grammar G,
we construct a new grammar G?. For every fragment
F ? G, hypothesize a fragment F ? = F ? C, pro-
vided F ? C is allowed byC. In order to add F and
F ? to G?, we assign an initial probability to both frag-
ments (?3.2), and then use EM to determine appro-
priate weights. We do not explicitly remove smaller
fragments from the grammar, though it is possible
for weights to vanish throughout iterations of EM.
Note that a probabilistic TSG fragment may be
uniquely represented as its constituent CFG rules:
make the root of every internal depth-one subtree
unique (have unit probability) and place the entirety
of the TSG weight on the root depth-one rule. This
representation has multiple benefits: it not only al-
lows TSG induction within the split/merge frame-
work, but it also provides a straight-forward way to
use the inside-outside algorithm.
3Technically, the composition operator (?) is ambiguous if
there is more than one occurrence of rY in the frontier of X .
Although notation augmentations could resolve this, we rely on
context for disambiguation.
25
3.2 Fragment Probability Estimation
First, we define a count function c over fragments by
c(X) =
?
T?P(T )
?
??T
?X,? , (1)
where P(T ) is a parsed version of T , ? is a subtree
of T and ?X,? is 1 iff X matches ? .4 We may then
count fragment co-occurrence by
?
Y
c(X ? Y ) =
?
Y :?XY?
c(X ? Y ) +
?
Y :?XY?
c(X ? Y ).
Prior to running inside-outside, we must re-
allocate the probability mass from the previous frag-
ments to the hypothesized ones. As this is just
a temporary initialization, can we allocate mass
as done when splitting, where each rule?s mass is
uniformly distributed, modulo tie-breaking random-
ness, among its refinement offspring? Split/merge
only hypothesizes that a node should have a particu-
lar refinement, but by learning subtrees our coupling
method hypothesizes that deeper structure may bet-
ter explain data. This leads to the realization that a
symbol may both subsume, and be subsumed by, an-
other symbol in the same coupling step; it is not clear
how to apply the above redistribution technique to
our situation.
However, even if uniform-redistribution could
easily be applied, we would like to be able to indi-
cate how much we ?trust? newly hypothesized frag-
ments. We achieve this via a parameter ? ? [0, 1]:
as ? ? 1, we wish to move more of P [X | rX ]
to P [?XY? | rX ]. Note that we need to know which
fragmentsL couple below withX (?XL?), and which
fragments U couple above (?UX?).
For reallocation, we remove a fraction of the num-
ber of occurrences of top-couplings of X:
c? (X) = 1 ? ?
?
Y :?XY? c(X ? Y )
?
Y c(X ? Y )
, (2)
and some proportion of the number of occurrences
of bottom-couplings of X:
sub(X) =
?
U :?UX? c(U ?X)
?
U,L:?UL?
rX=rL
c(U ? L)
. (3)
4We use a parsed version because there are no labeled inter-
nal nodes in the original treebank.
To prevent division-by-zero (e.g., for pre-terminals),
(2) returns 1 and (3) returns 0 as necessary.
Given any fragmentX in an original grammar, let
? be its conditional probability: ? = P [X | rX ] .
For a new grammar, define the new conditional prob-
ability for X to be
P [X | rX ] ? ? ? |c?(X) ? sub(X)|, (4)
and
P [?XY? | rX ] ? ??
c(X ? Y )
?
Y c(X ? Y )
(5)
for applicable Y .
Taken together, equations (4) and (5) simply say
that X must yield some percentage of its current
mass to its hypothesized relatives ?XY?, the amount
of which is proportionately determined by c?. But we
may also hypothesize ?ZX?, which has the effect of
removing (partial) occurrences of X .5
Though we would prefer posterior counts of frag-
ments, it is not obvious how to efficiently obtain pos-
terior ?bigram? counts of arbitrarily large latent TSG
fragments (i.e., c(X ? Y )). We therefore obtain, in
linear time, Viterbi counts using the previous best
grammar. Although this could lead to count sparsity,
in practice our previous grammar provides sufficient
counts across fragments.
3.3 Coupling from Common Subtrees
We now turn to the question of how to acquire the
constraint set C. Drawing on the discussion in ?2.2,
the constraint set should, with little effort, enforce
sparsity. Similarly to our experiments in classifi-
cation with TSGs (Ferraro et al, 2012), we extract
a list of the K most common subtrees of size at
most R, which we refer to as F?R,K?. Note that if
F ? F?R,K?, then all subtreesF ? ofF must also be in
F?R,K?.6 Thus, we may incrementally build F?R,K?
in the following manner: given r, for 1 ? r ? R,
maintain a ranking S, by frequency, of all fragments
of size r; the key point is that S may be built from
F?r?1,K?. Once all fragments of size r have been
considered, retain only the top K fragments of the
ranked set F?r,K? = F?r?1,K? ? S.
5If c?(X) = sub(X), then define Eqn. (4) to be ?.
6Analogously, if an n-gram appears K times, then all con-
stituentm-grams,m < n, must also appear at leastK times.
26
This incremental approach is appealing for two
reasons: (1) practically, it helps temper the growth
of intermediate rankings F?r,K?; and (2) it provides
two tunable parametersR andK, which relate to the
base measure and concentration parameter of previ-
ous work (Post and Gildea, 2009; Cohn et al, 2010).
We enforce sparsity by thresholding at every itera-
tion.
4 Datasets
We perform a qualitative analysis of fragments
learned on datasets for two languages: the Ko-
rean Treebank v2.0 (Han and Ryu, 2005) and a
comparably-sized portion of the WSJ portion of the
Penn Treebank (Marcus et al, 1993). The Korean
Treebank (KTB) has predefined splits; to be compa-
rable for our analysis, from the PTB we used ?2-3
for training and ?22 for validation (we refer to this
as wsj2-3). As described in Chung et al (2010), al-
though Korean presents its own challenges to gram-
mar induction, the KTB yields additional difficulties
by including a high occurrence of very flat rules (in
5K sentences, there are 13 NP rules with at least four
righthand side NPs) and a coarser nonterminal set
than that of the Penn Treebank. On both sets, we
run for two iterations.
Recall that our algorithm is designed to induce a
state-split TSG on a binarized tree; as neither dataset
is binarized in native form we apply a left-branching
binarization across all trees in both collections as a
preprocessing step. Petrov et al (2006) found differ-
ent binarization methods to be inconsequential, and
we have yet to observe significant impact of this bi-
narization decision (this will be considered in more
detail in future work).
Recently Petrov et al (2011) provided a set of
coarse, ?universal? (as measured across 22 lan-
guages), part-of-speech tags. We explore here the
interaction of this tagset in our model on wsj2-3: call
thismodified version uwsj2-3, onwhichwe run three
iterations. By further coarsening the PTB tags, we
can ask questions such as: what is the refinement
pattern? Can we identify linguistic phenomena in a
different manner than we might without the univer-
sal tag set? Then, as an extreme, we replace all POS
tags with the same symbol ?X,? to investigate what
predicate/argument relationships can be derived: we
(a) Modal construction.
S2
S
NP0 VP0
VP
MD
will
VP0
(b) Modifiable NP.
NP2
NP
NN
president
PP0
(c) Nominal-modification.
NP0
NP
NP
NNP3 NNP1
NNP0
NNP0
(d) PP construction.
PP0
IN
at
NP
NP0 NNP0
(e) Initial Quotation.
SINV1
SINV
SINV
SINV0 ,0
?0
VP
VBZ0
Figure 2: Example fragments learned on wsj2-3.
call this set xwsj2-3 and run four times on it.7
5 Fragment Analysis
In this section we analyze hand-selected preliminary
fragments and lexical clusterings our system learns.
WSJ, ?2-3 As Figure 2 illustrates, after two iter-
ations we learn various types of descriptive lexical-
ized and unlexicalized fragments. For example, Fig-
ure 2a concisely creates a four-step modal construc-
tion (will), while 2b demonstrates how a potentially
useful nominal can be formed. Further, learned frag-
ments may generate phrases with multiple nominal
modifiers (2c), and lexicalized PPs (2d).
Note that phrases such as NP0 and VP0 are of-
ten lexicalized themselves (with determiners, com-
mon verbs and other constructions), though omitted
due to space constraints; these lexicalized phrases
could be very useful for 2a (given the incremental
7While the universal tag set has a Korean mapping, the sym-
bols do not coincide with the KTB symbols.
27
(a) Common noun refinements.
NNC
0 ?? ?? ??case this day at the moment
1 ?? ?? ??international economy world
2 ?? ?? ??related announcement report
(b) Verbal inflection.
VV0
NNC2 XSV
?
(c) Adjectival inflection.
VJ0
NNC1 XSJ
?
Figure 3: Clusters and fragments for the KTB.
coupling employed, 2a could not have been further
expanded in two iterations). Figure 2c demonstrates
how TSGs and latent annotations are naturally com-
plementary: the former provides structure while the
latter describes lexical distributions of nominals.
Figure 2e illustrates a final example of syntactic
structure, as we begin to learn how to properly an-
alyze a complex quotation. A full analysis requires
only five TSG rules while an equivalent CFG-only
construction requires eight.
KTB2 To illustrate emergent semantic and syntac-
tic patterns, we focus on common noun (NNC) re-
finements. As seen in Table 3a, top words from
NNC0 represent time expressions and planning-
related. As a comparison, two other refinements,
NNC1 and NNC2, are not temporally representative.
This distinction is important as NNC0 easily yields
adverbial phrases, while the resultant adverbial yield
for either NNC1 or NNC2 is much smaller.
Comparing NNC1 and NNC2, we see that the
highest-ranked members of the latter, which include
report and announcement, can be verbalized by ap-
pending an appropriate suffix. Nouns under NNC1,
such as economy and world, generally are subject
to adjectival, rather than verbal, inflection. Figures
3b and 3c capture these verbal and adjectival inflec-
tions, respectively, as lexicalized TSG fragments.
WSJ, ?2-3, Universal Tag Set In the preliminary
work done here, we find that after a small number of
iterations we can identify various cluster classifica-
tions for different POS tags. Figures 4a, 4b and 4c
provide examples for NOUN, VERB and PRON, re-
spectively. For NOUNs we found that refinements
correspond to agentive entities (refinements 0, 1,
e.g., corporations or governments), market or stock
concepts (2), and numerically-modifiable nouns (7).
Some refinements overlapped, or contained common
nouns usable in many different contexts (3).
Similarly for VERBs (4b), we find suggested dis-
tinctions among action (1) and belief/cognition (2)
verbs.8 Further, some verb clusters are formed of
eventive verbs, both general (3) and domain-specific
(0). Another cluster is primarily of copula/auxiliary
verbs (7). The remaining omitted categories appear
to overlap, and only once we examine the contexts
in which they occur do we see they are particularly
useful for parsing FRAGs.
Though NOUN and VERB clusters can be dis-
cerned, there tends to be overlap among refinements
that makes the analysis more difficult. On the other
hand, refinements for PRON (4c) tend to be fairly
clean and it is generally simple to describe each: pos-
sessives (1), personified wh-words (2) and general
wh-words (3). Moreover, both subject (5) and ob-
ject (6) are separately described.
Promisingly, we learn interactions among various
refinements in the form of TSG rules, as illustrated
by Figures 4d-4g. While all four examples involve
VERBs it is enlightening to analyze a VERB?s re-
finement and arguments. For example, the refine-
ments in 4d may lend a simple analysis of financial
actions, while 4e may describe different NP interac-
tions (note the different refinement symbols). Dif-
ferent VERB refinements may also coordinate, as in
4f, where participle or gerund may help modify a
main verb. Finally, note how in 4g, an object pro-
noun correctly occurs in object position. These ex-
amples suggest that even on coarsened POS tags, our
method is able to learn preliminary joint syntactic
and lexical relationships.
WSJ, ?2-3, Preterminals as X In this experiment,
we investigate whether the manual annotations of
Petrov et al (2011) can be re-derived through first
reducing one?s non-terminal tagset to the symbol
X and splitting until finding first the coarse grain
8The next highest-ranked verbs for refinement 1 include re-
ceived, doing and announced.
28
(a) Noun refinements.
NOUN
0 Corp Big Co.
1 Mr. U.S. New
2 Bush prices trading
3 Japan September Nissan
7 year % months
(b) Verb refinements.
VERB
0 says said sell buy rose
1 have had has been made
2 said says say added believe
3 sold based go trading filed
7 is are be was will
(c) Pronoun refinements.
PRON
1 its his your
2 who whom ?
3 what whose What
5 it he they
6 it them him
(d) VP structure.
VP0
VERB0 NP
ADJ3 NOUN3
(e) Declarative sentence.
S0
NP4 VP
VERB1 NP1
(f) Multiple VP interactions.
VP0
VP
VERB7 ADVP0
VP
VERB0 NP0
(g) Accusative use.
VP0
VERB0 NP
PRON6
Figure 4: Highest weighted representatives for lexical categories (4a-4c) and learned fragments (4d-4g), for uwsj2-3.
X Universal Tag
0 two market brain NOUN
1 ?s said says VERB
2 % company year NOUN
3 it he they PRON
5 also now even ADV
6 the a The DET
7 10 1 all NUM
9 . ? ... .
10 and or but CONJ
12 which that who PRON
13 is was are VERB
14 as of in ADP
15 up But billion ADP
Table 2: Top-three representatives for various refine-
ments of X, with reasonable analogues to Petrov et al
(2011)?s tags. Universal tag recovery is promising.
tags of the universal set, followed by finer-grain tags
from the original treebank. Due to the loss of lexi-
cal information, we run our system for four iterations
rather than three.
As observed in Table 2, there is strong overlap
observed between the induced refinements and the
original universal tags. Though there are 16 refine-
ments of X , due to lack of cluster coherence not all
are listed. Those tags and unlisted refinements seem
to be interwoven in a non-trivial way. We also see
complex refinements of both open- and closed-class
words occurring: refinements 0 and 2 correspond
with the open-class NOUN, while refinements 3 and
12, and 14 and 15 both correspond with the closed
classes PRON and ADP, respectively. Note that 1
and 13 are beginning to split verbs by auxiliaries.
6 Conclusion
We have shown that TSGs may be encoded and in-
duced within a framework of syntactic latent an-
notations. Results were provided for induction us-
ing the English Penn, and Korean Treebanks, with
further experiments based on the Universal Part of
Speech tagset. Examples shown suggest the promise
of our approach, with future work aimed at exploring
larger datasets using more extensive computational
resources.
Acknowledgements Thank you to the reviewers
for helpful feedback, and to JohnsHopkinsHLTCOE
for providing support. We would also like to thank
Byung Gyu Ahn for graciously helping us analyze
the Korean results. Any opinions expressed in this
work are those of the authors.
References
Mohit Bansal and Dan Klein. 2010. Simple, accurate
parsing with an all-fragments grammar. In Proceed-
ings of ACL, pages 1098?1107. Association for Com-
putational Linguistics.
Rens Bod. 1993. Using an annotated corpus as a stochas-
29
tic grammar. In Proceedings of EACL, pages 37?44.
Association for Computational Linguistics.
Rens Bod. 2001. What is the minimal set of fragments
that achievesmaximal parse accuracy? InProceedings
of ACL, pages 66?73. Association for Computational
Linguistics.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Proceed-
ings of AAAI, pages 598?603.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of korean parsing. In
Proceedings of the NAACL HLT Workshop on Sta-
tistical Parsing of Morphologically-Rich Languages
(SPMRL), pages 49?57, Los Angeles, California,
USA, June.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 548?556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal of
Machine Learning Research, 11:3053?3096, Decem-
ber.
Markus Dreyer and Jason Eisner. 2006. Better informed
training of latent syntactic features. In Proceedings of
EMNLP, pages 317?326, Sydney, Australia, July. As-
sociation for Computational Linguistics.
Francis Ferraro, Matt Post, and Benjamin Van Durme.
2012. Judging Grammaticality with Count-Induced
Tree Substitution Grammars. In Proceedings of the
Seventh Workshop in Innovated Use of NLP for Build-
ing Educational Applications.
Joshua Goodman. 1996. Efficient algorithms for pars-
ing the dop model. In Proceedings of EMNLP, pages
143?152.
Na-Rae Han and Shijong Ryu. 2005. Guidelines for
Penn Korean Treebank. Technical report, University
of Pennsylvania.
Chung-hye Han, Na-Rae Han, and Eon-Suk Ko. 2001.
Bracketing guidelines for penn korean treebank. Tech-
nical report, IRCS, University of Pennsylvania.
Mark Johnson. 1998. PCFG models of linguis-
tic tree representations. Computational Linguistics,
24(4):613?632.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
adjoining grammars. In G. Rozenberg and A. Salo-
maa, editors, Handbook of Formal Languages: Be-
yond Words, volume 3, pages 71?122. Springer.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st An-
nual Meeting on Association for Computational Lin-
guistics - Volume 1, pages 423?430, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-
rice Santorini. 1993. Building a large annotated cor-
pus of english: The Penn Treebank. Computational
linguistics, 19(2):313?330.
Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic cfg with latent annotations. InPro-
ceedings of ACL, pages 75?82, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. InProceedings of ACL-ICCL,
pages 433?440, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. In ArXiv, April.
Matt Post and Daniel Gildea. 2009. Bayesian learning of
a tree substitution grammar. In Proceedings of ACL-
IJCNLP (short papers), pages 45?48, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for data-oriented parsing.
Journal of Automata Languages and Combinatorics,
10(2/3):367.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of EMNLP-CoNLL, pages
551?560.
30
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 116?121,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Judging Grammaticality with Count-Induced Tree Substitution Grammars
Francis Ferraro, Matt Post and Benjamin Van Durme
Department of Computer Science, and HLTCOE
Johns Hopkins University
{ferraro,post,vandurme}@cs.jhu.edu
Abstract
Prior work has shown the utility of syntactic
tree fragments as features in judging the gram-
maticality of text. To date such fragments have
been extracted from derivations of Bayesian-
induced Tree Substitution Grammars (TSGs).
Evaluating on discriminative coarse and fine
grammaticality classification tasks, we show
that a simple, deterministic, count-based ap-
proach to fragment identification performs on
par with the more complicated grammars of
Post (2011). This represents a significant re-
duction in complexity for those interested in
the use of such fragments in the development
of systems for the educational domain.
1 Introduction
Automatically judging grammaticality is an im-
portant component in computer-assisted education,
with potential applications including large-scale es-
say grading and helping to interactively improve the
writing of both native and L2 speakers. While n-
gram models have been productive throughout nat-
ural language processing (NLP), they are obviously
insufficient as models of languages, since they do
not model language structure or correspondences
beyond the narrow Markov context.
Context-free grammars (CFGs) address many of
the problems inherent in n-grams, and are there-
fore intuitively much better suited for grammatical-
ity judgments. Unfortunately, CFGs used in practice
are permissive (Och et al, 2004) and make unreal-
istic independence and structural assumptions, re-
sulting in ?leaky? grammars that overgenerate and
thus serve poorly as models of language. How-
ever, approaches that make use of the CFG produc-
tions as discriminative features have performed bet-
ter. Cherry and Quirk (2008) improved upon an n-
gram baseline in grammatical classification by ad-
justing CFG production weights with a latent SVM,
while others have found it useful to use comparisons
between scores of different parsers (Wagner et al,
2009) or the use of CFG productions in linear clas-
sification settings (Wong and Dras, 2010) in classi-
fying sentences in different grammaticality settings.
Another successful approach in grammaticality
tasks has been the use of grammars with an extended
domain of locality. Post (2011) demonstrated that
larger syntactic patterns obtained from Tree Sub-
stitution Grammars (Joshi, 1985) outperformed the
Cherry and Quirk models. The intuitions underlying
their approach were that larger fragments are more
natural atomic units in modeling grammatical text,
and that larger fragments reduce the independence
assumptions of context-free generative models since
there are fewer substitution points in a derivation.
Their grammars were learned in a Bayesian setting
with Dirichlet Process priors, which have simple for-
mal specifications (c.f., Goldwater et al (2009, Ap-
pendix A)), but which can become quite complicated
in implementation.
In this paper, we observe that fragments used for
classification do not require an underlying proba-
bilistic model. Here, we present a simple extraction
method that elicits a classic formal non-probabilistic
grammar from training data by deterministically
counting fragments. Whereas Post parses with his
TSG and extracts the Viterbi derivation, we use an
116
SBAR
IN
for
S
NP VP
TO
to
VP
(a) A TSG fragment.
SBAR? IN S
IN? for
S? NP VP
VP? TO VP
TO? to
(b) Equivalent CFG rules.
Figure 1: Equivalent TSG fragment and CFG rules.
off-the-shelf parser and pattern match the fragments
in our grammar against the tree. With enough pos-
itive and negative training data (in the form of au-
tomatic parses of good and bad sentences), we can
construct classifiers that learn which fragments cor-
relate with grammaticality. The resulting model re-
sults in similar classification accuracy while doing
away with the complexity of Bayesian techniques.
2 Tree Substitution Grammars (TSGs)
Though CFGs and TSGs are weakly equivalent,
TSGs permit nonterminals to rewrite as tree frag-
ments of arbitrary size, whereas CFG rewrites are
limited to depth-one productions. Figure 1 de-
picts an example TSG fragment and equivalent CFG
rules; note that the entire internal structure of 1a is
described within a single rewrite.
Unfortunately, learning probabilistic TSGs is not
straight-forward, in large part because TSG-specific
resources (e.g., large scale TSG-annotated tree-
banks) do not exist. Approaches to this problem be-
gan by taking all fragments Fall in a treebank (Bod,
1993; Goodman, 1996), which resulted in very large
grammars composed mostly of fragments very un-
likely to generalize.1 A range of heuristic solutions
reduced these grammar sizes to a much smaller,
more compact subset of all fragments (Zollmann
and Sima?an, 2005; Zuidema, 2007). More recently,
more principled models have been proposed, taking
the form of inference in Bayesian non-parametric
models (Post and Gildea, 2009; Cohn et al, 2009).
In addition to providing a formal model for TSGs,
these techniques address the overfitting problem of
1The n-gram analog would be something like storing all 30-
grams seen in a corpus.
all fragments grammars with priors that discourage
large fragments unless there is enough evidence to
warrant their inclusion in the grammar. The problem
with such approaches, however, is that the sampling
procedures used to infer them can be complex, dif-
ficult to code, and slow to converge. Although more
general techniques have been proposed to better ex-
plore the search space (Cohn and Blunsom, 2010;
Cohn et al, 2010; Liang et al, 2010), the complex-
ity and non-determinism of these samplers remain,
and there are no publicly available implementations.
The underlying premise behind these grammar
learning approaches was the need for a probabilis-
tic grammar for parsing. Post (2011) showed that
the fragments extracted from derivations obtained
by parsing with probabilistic TSGs were useful as
features in two coarse-grained grammaticality tasks.
In such a setting, fragments are needed for classifica-
tion, but it is not clear that they need to be obtained
from derivations produced by parsing with proba-
bilistic TSGs. In the next section, we describe a sim-
ple, deterministic, count-based approach to learn-
ing an unweighted TSG. We will then demonstrate
(?4) the effectiveness of these grammars for gram-
maticality classification when fragments are pattern-
matched against parse trees obtained from a state-of-
the-art parser.
3 Counting Common Subtrees
Rather than derive probabilistic TSGs, we employ
a simple, iterative and deterministic (up to tie-
breaking) alternative to TSG extraction. Our method
extracts F?R,K?, the K most common subtrees of
size at most R. Though selecting the top K-most-
frequent fragments from all fragments is computa-
tionally challenging through brute force methods,
note that if F ? F?R,K?, then all subtrees F
? of F
must also be in F?R,K?.
2 Thus, we may incremen-
tally build F?R,K? in the following manner: given r,
for 1 ? r ? R, maintain a ranking S, by frequency,
of all fragments of size r; the key point is that S may
be built from F?r?1,K?. Once all fragments of size
r have been considered, retain only the top K frag-
ments of the ranked set F?r,K? = F?r?1,K? ? S.
3
2Analogously, if an n-gram appears K times, then all con-
stituent m-grams, m < n, must also appear at least K times.
3We found that, at the thresholding stage, ties may be arbi-
trarily broken with neglible-to-no effect on results.
117
Algorithm 1 EXTRACTFRAGMENTS (R,K)
Assume: Access to a treebank
1: S ? ?
2: F?1,K? ? top K CFG rules used
3: for r = 2 to R do
4: S ? S ? {observed 1-rule extensions of F ?
F?r?1,K?}
5: F?r,K? ? top K elements of F?r?1,K? ? S
6: end for
Pseudo-code is provided in Algorithm 1.4
This incremental approach is appealing for two
reasons. Firstly, our approach tempers the growth
of intermediate rankings F?r,K?. Secondly, we
have two tunable parameters R and K, which can
be thought of as weakly being related to the base
measure and concentration parameter of (Post and
Gildea, 2009; Cohn et al, 2010). Note that by
thresholding at every iteration, we enforce sparsity.
4 Experiments
We view grammaticality judgment as a binary clas-
sification task: is a sequence of words grammatical
or not? We evaluate on two tasks of differing granu-
larity: the first, a coarse-grain classification, follows
Cherry and Quirk (2008); the other, a fine-grain ana-
logue, is built upon Foster and Andersen (2009).
4.1 Datasets
For the coarse-grained task, we use the BLLIP5-
inspired dataset, as in Post (2011), which dis-
criminates between BLLIP sentences and Kneyser-
Ney trigram generated sentences (of equal length).
Grammatical and ungrammatical examples are given
in 1 and 2 below, respectively:
(1) The most troublesome report may be the
August merchandise trade deficit due out
tomorrow .
(2) To and , would come Hughey Co. may be
crash victims , three billion .
For the fine-grained task we use a version of the
BNC that has been automatically modified to be
4Code is available at: cs.jhu.edu/?ferraro.
5LDC2000T43
ungrammatical, via insertions, deletions or substi-
tutions of grammatically important words. As has
been argued in previous work, these automatically
generated errors, simulate more realistic errors (Fos-
ter and Andersen, 2009). Example 3 gives an origi-
nal sentence, with an italicized substitution error:
(3) The league ?s promoters hope retirees and
tourists will join die-hard fans like Mr. de
Castro and pack then stands to see the seniors .
Both sets contain train/dev/test splits with an
equal number of positive and negative examples, and
all instances have an available gold-standard parse6.
4.2 Models and Features
Algorithm 1 extracts common constructions, in the
form of count-extracted fragments. To test the ef-
ficacy of these fragments, we construct and experi-
ment with various discriminative models.
Given count-extracted fragments obtained from
EXTRACTFRAGMENTS(R,K), it is easy to define a
feature vector: for each query, there is a binary fea-
ture indicating whether a particular extracted frag-
ment occurs in its gold-standard parse. These count-
extracted features, along with the sentence length,
define the first model, called COUNT.
Although our extracted fragments may help
identify grammatical constructions, capturing un-
grammatical constructions may be difficult, since
we do not parse with our fragments. Thus,
we created two augmented models, COUNT+LEX
and COUNT+CFG, which built upon and extended
COUNT. COUNT+LEX included all preterminal and
lexical items. For COUNT+CFG, we included a bi-
nary feature for every rule that was used in the most
likely parse of a query sentence, according to a
PCFG7.
Following Post (2011), we train an `-2 regular-
ized SVM using liblinear8 (Fan et al, 2008)
per model. We optimized the models on dev data,
letting the smoothing parameter be 10m, for integral
m ? [?4, 2]: 0.1 was optimal for all models.
6We parsed all sentences with the Berkeley parser (Petrov et
al., 2006).
7We used the Berkeley grammar/parser (Petrov et al, 2006)
in accurate mode; all other options were their default values.
8csie.ntu.edu.tw/?cjlin/liblinear/
118
Task COUNT COUNT+LEX COUNT+CFG
coarse 86.3 86.8 88.3
fine 62.9 64.3 67.0
(a) Our count-based models, with R = 15, K = 50k.
Task 3 5 10 15
coarse 89.2 89.1 88.6 88.3
fine 67.9 67.2 67.2 67.0
(b) Performance of COUNT+CFG, with K =
50k and varying R.
Table 1: Development accuracy results.
Our three models all have the same two tunable
parameters, R and K. While we initially experi-
mented with R = 31,K ? {50k, 100k} ? in or-
der to be comparable to the size of Post (2011)?s ex-
tracted TSGs ? we noticed that very few, if any,
fragments of size greater than 15 are able to sur-
vive thresholding. Dev experimentation revealed
that K = 50k and 100k yielded nearly the same
results; for brevity, we report in Table 1a dev re-
sults for all three models, with R = 15,K =
50k. The differences across models was stark, with
COUNT+CFG yielding a two point improvement over
COUNT on coarse, but a four point improvement
on fine. While COUNT+LEX does improve upon
COUNT, on both tasks it falls short of COUNT+CFG.
These differences are not completely surprising:
one possible explanation is that the PCFG features
in COUNT+CFG yield useful negatively-biased fea-
tures, by providing a generative explanation. Due
to the supremacy of COUNT+CFG, we solely report
results on COUNT+CFG.
In Table 1b, we also examine the effect of ex-
tracted rule depth on dev classification accuracy,
where we fix K = 50k and vary R ? {3, 5, 10, 15},
where the best results are achieved with R = 3.
We evaluate two versions of COUNT+CFG: one with
R = 3 and the other with R = 15 (K = 50k for
both).
5 Results and Fragment Analysis
We build on Post (2011)?s results and compare
against bigram, CFG and TSG baselines. Each base-
line model is built from the same `-2 regularized
Method coarse fine
COUNT+CFG, R = 3 89.1 67.2
COUNT+CFG, R = 15 88.2 66.6
bigram 68.4 61.4
CFG 86.3 64.5
TSG 89.1 67.0
Table 2: Classification accuracy on test portions for
both coarse and fine, with K = 50k. Chance is 50%
for each task.
SVM as above, and each is optimized on dev data.
For the bigram baseline, the binary features corre-
spond with whether a particular bigram appears in
an instance, while the CFG baseline is simply the
augmentation feature set used for COUNT+CFG. For
the TSG baseline, the binary features correspond
with whether a particular fragment is used in the
most probable derivation of each input sentence (us-
ing Post?s Bayesian TSGs). All baselines use the
sentence length as a feature as well.
The results on the test portions of each dataset are
given in Table 2. When coupled with the best parse
output, our counting method was able to perform on
par with, and even surpass, Post?s TSGs. The sim-
pler model (R = 3) ties TSG performance on coarse
and exceeds it by two-tenths on fine; the more com-
plex model (R = 15) gets within a point on coarse
and four-tenths on fine. Note that both versions of
COUNT+CFG surpass the CFG baseline on both sets,
indicating that (1) encoding deeper structure, even
without an underlying probabilistic model, is use-
ful for grammaticality classifications, and (2) this
deeper structure can be achieved by a simple count-
ing scheme.
As PCFG output comprises a portion of our fea-
ture set, it is not surprising that a number of the
most discriminative positive and negative features,
such as flat NP and VP rules not frequent enough
to survive thresholding, were provided by the CFG
parse. While this points out a limitation of our
non-adaptive thresholding, note that even among
the highest weighted features, PCFG and count-
extracted features were interspersed. Further, con-
sidering that both versions of COUNT+CFG outper-
formed CFGs, it seems our method adds discrimina-
tive power to the CFG rules.
119
(a) Coarse (b) Fine
Grammatical Ungrammatical Grammatical Ungrammatical
1 (S NP VP (. .)) (S NP (VP (VBP are)
PP))
10 (SBAR (IN if) S) (SBAR (S VP))
2 (S (S (VP VBG NP))
VP)
(VP VBZ (S VP)) 11 (NP (DT these) NNS) (SBAR DT (S NP
VP))
3 (SBAR (IN while) S) (SBAR (S VP) ) 12 (VP (VBG being) VP) (S (VP VB NP))
4 (VP (VBD called) S) (VP VBN (S VP)) 13 (PP IN (S NP (VP
VBG NP)))
(S (VP VBZ NP))
5 (VP (VB give) NP NP) (NP (NP JJ NN)
SBAR)
14 (S (VP VBG VP)) (VP VB (S VP))
6 (NP NNP NNP NNP
(NNP Inc.))
(VP NN (PP IN NP)) 15 (PP IN (SBAR (IN
whether) S))
(S (VP VBP VP))
7 (PP (IN with) (S NP
VP))
(S (VP MD VP)) 16 (VP (VBD had) (VP
VBN S))
(S NP (VP (VBD
said)))
8 (SBAR (IN for) (S NP
(VP (TO to) VP)))
(SBAR (S (NP NNS)
VP))
17 (VP MD (VP VB NP
(PP IN NP) PP))*
(PP (PP IN NP) (CC
and) PP)*
9 (PRN (-LRB- -LRB-)
NP (-RRB- -RRB-))*
(S (ADJP JJ))* 18 (NP (DT no) NNS)* (PP (IN As) NP)*
Table 3: Most discriminative count-based features for COUNT+CFG on both tasks. For comparability to Post
(2011), R = 15,K = 50k, are shown. Asterisks (*) denote fragments hand-selected from the top 30.
Table 5 presents top weighted fragments from
COUNT+CFG on both coarse and fine, respectively.
Examining useful grammatical features across tasks,
we see a variety of fragments: though our fragments
heavily weight simple structure such as proper punc-
tuation (ex. 1) and parentheticals (ex. 9), they also
capture more complex phenomena such as lexical
argument descriptions (e.g., give, ex. 5). Our ex-
tracted fragments also describe common construc-
tions and transitions (e.g., 3, 8 and 15) and involved
verb phrases (e.g., gerunds in 2 and 14, passives in
16, and modals in 17).
Though for both tasks some ungrammatical frag-
ments easily indicate errors, such as sentence frag-
ments (e.g., example 6) or repeated words (ex. 11),
in general the analysis is more difficult. In part, this
is because, when isolated from errors, one may con-
struct grammatical sentences that use some of the
highest-weighted ungrammatical fragments. How-
ever, certain errors may force particular rules to be
inappropriately applied when acquiring the gold-
standard parse. For instance, example 10 typically
coordinates with larger VPs, via auxiliary verbs or
expletives (e.g., it). Affecting those crucial words
can significantly change the overall parse structure:
consider that in ?said it is too early. . . ,? it provides a
crucial sentential link; without it, ?is too early? may
be parsed as a sentence, and then glued on to the
former part.
6 Conclusion
In this work, we further examined TSGs as useful
judges of grammaticality for written English. Us-
ing an iterative, count-based approach, along with
the most likely PCFG parse, we were able to train a
discriminative classifier model ? COUNT+CFG ?
that surpassed the PCFG?s ability to judge gram-
maticality, and performed on par with Bayesian-
TSGs. Examining the highest weighted features, we
saw that complex structures and patterns encoded by
the count-based TSGs proved discriminatively use-
ful. This suggests new, simpler avenues for frag-
ment learning, especially for grammaticality judg-
ments and other downstream tasks.
Acknowledgements Thank you to the reviewers
for helpful feedback, and thanks to Johns Hopkins
HLTCOE for providing support. Any opinions ex-
pressed in this work are those of the authors.
120
References
R. Bod. 1993. Using an annotated corpus as a stochas-
tic grammar. In Proceedings of the sixth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 37?44. Association for Com-
putational Linguistics.
Colin Cherry and Chris Quirk. 2008. Discrimina-
tive, syntactic language modeling through latent svms.
Proceeding of Association for Machine Translation in
the America (AMTA-2008).
Trevor Cohn and Phil Blunsom. 2010. Blocked inference
in bayesian tree substitution grammars. In Proceed-
ings of ACL (short papers), pages 225?230, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Trevor Cohn, Sharon Goldwater, and Phil Blunsom.
2009. Inducing compact but accurate tree-substitution
grammars. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 548?556, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Trevor Cohn, Phil Blunsom, and Sharon Goldwater.
2010. Inducing tree-substitution grammars. Journal
of Machine Learning Research, 11:3053?3096, De-
cember.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874, June.
Jennifer Foster and Oistein E. Andersen. 2009. Gen-
ERRate: generating errors for use in grammatical error
detection. In Proceedings of the Fourth Workshop on
Innovative Use of NLP for Building Educational Ap-
plications, pages 82?90.
Sharon Goldwater, Thomas L. Griffiths, and Mark John-
son. 2009. A Bayesian framework for word segmen-
tation: Exploring the effects of context. Cognition,
112(1):21 ? 54.
Joshua Goodman. 1996. Efficient algorithms for parsing
the dop model. In Proceedings of EMNLP, pages 143?
152.
A.K. Joshi. 1985. Tree adjoining grammars: How much
context-sensitivity is required to provide reasonable
structural descriptions? Natural language parsing,
pages 206?250.
Percy Liang, Michael .I. Jordan, and Dan Klein. 2010.
Type-based MCMC. In Human Language Technolo-
gies: The 2010 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Lin-
guistics. Association for Computational Linguistics.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng, et al
2004. A smorgasbord of features for statistical ma-
chine translation. In Proceedings of NAACL.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL-
ICCL, pages 433?440, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Matt Post and Daniel Gildea. 2009. Bayesian learn-
ing of a tree substitution grammar. In Proceedings
of ACL-IJCNLP (short papers), pages 45?48, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Matt Post. 2011. Judging grammaticality with tree
substitution grammar derivations. In Proceedings of
ACL (short papers), pages 217?222, Stroudsburg, PA,
USA. Association for Computational Linguistics.
J. Wagner, J. Foster, and J. van Genabith. 2009. Judg-
ing grammaticality: Experiments in sentence classifi-
cation. CALICO Journal, 26(3):474?490.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classification. In
Proceedings of the Australasian Language Technology
Association Workshop.
Andreas Zollmann and Khalil Sima?an. 2005. A consis-
tent and efficient estimator for Data-Oriented Parsing.
Journal of Automata, Languages and Combinatorics,
10(2/3):367?388.
Willem Zuidema. 2007. Parsimonious data-oriented
parsing. In Proceedings of EMNLP-CoNLL, pages
551?560.
121
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 10?51,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Findings of the 2012 Workshop on Statistical Machine Translation
Chris Callison-Burch
Johns Hopkins University
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
SDL Language Weaver
Lucia Specia
University of Sheffield
Abstract
This paper presents the results of the WMT12
shared tasks, which included a translation
task, a task for machine translation evaluation
metrics, and a task for run-time estimation of
machine translation quality. We conducted a
large-scale manual evaluation of 103 machine
translation systems submitted by 34 teams.
We used the ranking of these systems to mea-
sure how strongly automatic metrics correlate
with human judgments of translation quality
for 12 evaluation metrics. We introduced a
new quality estimation task this year, and eval-
uated submissions from 11 teams.
1 Introduction
This paper presents the results of the shared tasks
of the Workshop on statistical Machine Translation
(WMT), which was held at NAACL 2012. This
workshop builds on six previous WMT workshops
(Koehn and Monz, 2006; Callison-Burch et al,
2007; Callison-Burch et al, 2008; Callison-Burch
et al, 2009; Callison-Burch et al, 2010; Callison-
Burch et al, 2011). In the past, the workshops have
featured a number of shared tasks: a translation task
between English and other languages, a task for au-
tomatic evaluation metrics to predict human judg-
ments of translation quality, and a system combina-
tion task to get better translation quality by combin-
ing the outputs of multiple translation systems. This
year we discontinued the system combination task,
and introduced a new task in its place:
? Quality estimation task ? Structured predic-
tion tasks like MT are difficult, but the dif-
ficulty is not uniform across all input types.
It would thus be useful to have some mea-
sure of confidence in the quality of the output,
which has potential usefulness in a range of set-
tings, such as deciding whether output needs
human post-editing or selecting the best trans-
lation from outputs from a number of systems.
This shared task focused on sentence-level es-
timation, and challenged participants to rate
the quality of sentences produced by a stan-
dard Moses translation system on an English-
Spanish news corpus in one of two tasks:
ranking and scoring. Predictions were scored
against a blind test set manually annotated with
relevant quality judgments.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dissem-
inate common test sets and public training data with
published performance numbers, and to refine eval-
uation methodologies for machine translation. As
with previous workshops, all of the data, transla-
tions, and collected human judgments are publicly
available.1 We hope these datasets form a valuable
resource for research into statistical machine transla-
tion, system combination, and automatic evaluation
or automatic prediction of translation quality.
2 Overview of the Shared Translation Task
The recurring task of the workshop examines trans-
lation between English and four other languages:
German, Spanish, French, and Czech. We created a
1http://statmt.org/wmt12/results.html
10
test set for each language pair by translating newspa-
per articles. We additionally provided training data
and two baseline systems.
2.1 Test data
The test data for this year?s task was created by hir-
ing people to translate news articles that were drawn
from a variety of sources from November 15, 2011.
A total of 99 articles were selected, in roughly equal
amounts from a variety of Czech, English, French,
German, and Spanish news sites:2
Czech: Blesk (1), CTK (1), E15 (1), den??k (4),
iDNES.cz (3), iHNed.cz (3), Ukacko (2),
Zheny (1)
French: Canoe (3), Croix (3), Le Devoir (3), Les
Echos (3), Equipe (2), Le Figaro (3), Libera-
tion (3)
Spanish: ABC.es (4), Milenio (4), Noroeste (4),
Nacion (3), El Pais (3), El Periodico (3), Prensa
Libre (3), El Universal (4)
English: CNN (3), Fox News (2), Los Angeles
Times (3), New York Times (3), Newsweek (1),
Time (3), Washington Post (3)
German: Berliner Kurier (1), FAZ (3), Giessener
Allgemeine (2), Morgenpost (3), Spiegel (3),
Welt (3)
The translations were created by the professional
translation agency CEET.3 All of the translations
were done directly, and not via an intermediate lan-
guage.
Although the translations were done profession-
ally, we observed a number of errors. These errors
ranged from minor typographical mistakes (I was
terrible. . . instead of It was terrible. . . ) to more
serious errors of incorrect verb choices and nonsen-
sical constructions. An example of the latter is the
French sentence (translated from German):
Il a gratte? une planche de be?ton, perdit des
pie`ces du ve?hicule.
(He scraped against a concrete crash bar-
rier and lost parts of the car.)
2For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
3http://www.ceet.eu/
Here, the French verb gratter is incorrect, and the
phrase planche de be?ton does not make any sense.
We did not quantify errors, but collected a number
of examples during the course of the manual evalua-
tion. These errors were present in the data available
to all the systems and therefore did not bias the re-
sults, but we suggest that next year a manual review
of the professionally-collected translations be taken
prior to releasing the data in order to correct mis-
takes and provide feedback to the translation agency.
2.2 Training data
As in past years we provided parallel corpora to train
translation models, monolingual corpora to train lan-
guage models, and development sets to tune system
parameters. Some statistics about the training mate-
rials are given in Figure 1.
2.3 Submitted systems
We received submissions from 34 groups across 18
institutions. The participants are listed in Table 1.
We also included two commercial off-the-shelf MT
systems, three online statistical MT systems, and
three online rule-based MT systems. Not all systems
supported all language pairs. We note that the eight
companies that developed these systems did not sub-
mit entries themselves, but were instead gathered by
translating the test data via their interfaces (web or
PC).4 They are therefore anonymized in this paper.
The data used to construct these systems is not sub-
ject to the same constraints as the shared task partic-
ipants. It is possible that part of the reference trans-
lations that were taken from online news sites could
have been included in the systems? models, for in-
stance. We therefore categorize all commercial sys-
tems as unconstrained when evaluating the results.
3 Human Evaluation
As with past workshops, we placed greater empha-
sis on the human evaluation than on the automatic
evaluation metric scores. It is our contention that
automatic measures are an imperfect substitute for
human assessment of translation quality. Therefore,
we define the manual evaluation to be primary, and
4We would like to thank Ondr?ej Bojar for harvesting the
commercial entries, Christian Federmann for the statistical MT
entries, and Herve? Saint-Amand for the rule-based MT entries.
11
Europarl Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Training Corpus
Spanish? English French? English German? English Czech? English
Sentences 157,302 137,097 158,840 136,151
Words 4,449,786 3,903,339 3,915,218 3,403,043 3,950,394 3,856,795 2,938,308 3,264,812
Distinct words 78,383 57,711 63,805 53,978 130,026 57,464 136,392 52,488
United Nations Training Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Training Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech
Sentence 51,827,706 8,627,438 16,708,622 30,663,107 18,931,106
Words 1,249,883,955 247,722,726 410,581,568 576,833,910 315,167,472
Distinct words 2,265,254 926,999 1,267,582 3,336,078 2,304,933
News Test Set
English Spanish French German Czech
Sentences 3003
Words 73,785 78,965 81,478 73,433 65,501
Distinct words 9,881 12,137 11,441 14,252 17,149
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of
distinct words (case-insensitive) is based on the provided tokenizer.
12
ID Participant
CMU Carnegie Mellon University (Denkowski et al, 2012)
CU-BOJAR Charles University - Bojar (Bojar et al, 2012)
CU-DEPFIX Charles University - DEPFIX (Rosa et al, 2012)
CU-POOR-COMB Charles University - Bojar (Bojar et al, 2012)
CU-TAMCH Charles University - Tamchyna (Tamchyna et al, 2012)
CU-TECTOMT Charles University - TectoMT (Dus?ek et al, 2012)
DFKI-BERLIN German Research Center for Artificial Intelligence (Vilar, 2012)
DFKI-HUNSICKER German Research Center for Artificial Intelligence - Hunsicker (Hunsicker et al, 2012)
GTH-UPM Technical University of Madrid (Lo?pez-Luden?a et al, 2012)
ITS-LATL Language Technology Laboratory @ University of Geneva (Wehrli et al, 2009)
JHU Johns Hopkins University (Ganitkevitch et al, 2012)
KIT Karlsruhe Institute of Technology (Niehues et al, 2012)
LIMSI LIMSI (Le et al, 2012)
LIUM University of Le Mans (Servan et al, 2012)
PROMT ProMT (Molchanov, 2012)
QCRI Qatar Computing Research Institute (Guzman et al, 2012)
QUAERO The QUAERO Project (Markus et al, 2012)
RWTH RWTH Aachen (Huck et al, 2012)
SFU Simon Fraser University (Razmara et al, 2012)
UEDIN-WILLIAMS University of Edinburgh - Williams (Williams and Koehn, 2012)
UEDIN University of Edinburgh (Koehn and Haddow, 2012)
UG University of Toronto (Germann, 2012)
UK Charles University - Zeman (Zeman, 2012)
UPC Technical University of Catalonia (Formiga et al, 2012)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C] Three online statistical machine translation systems
RBMT-[1,3,4] Three rule-based statistical machine translation systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations
from the commercial, online, and rule-based systems were crawled by us, not submitted by the respective companies,
and are therefore anonymized. Anonymized identifiers were chosen so as to correspond with the WMT11 systems.
13
Language Pair Num Label Labels per
Systems Count System
Czech-English 6 6,470 1,078.3
English-Czech 13 11,540 887.6
German-English 16 7,135 445.9
English-German 15 8,760 584.0
Spanish-English 12 5,705 475.4
English-Spanish 11 7,375 670.4
French-English 15 6,975 465.0
English-French 15 7,735 515.6
Overall 103 61,695 598
Table 2: A summary of the WMT12 ranking task, show-
ing the number of systems and number of labels (rank-
ings) collected for each of the language translation tasks.
use the human judgments to validate automatic met-
rics.
Manual evaluation is time consuming, and it re-
quires a large effort to conduct on the scale of our
workshop. We distributed the workload across a
number of people, beginning with shared-task par-
ticipants and interested volunteers. This year, we
also opened up the evaluation to non-expert anno-
tators hired on Amazon Mechanical Turk (Callison-
Burch, 2009). To ensure that the Turkers provided
high quality annotations, we used controls con-
structed from the machine translation ranking tasks
from prior years. Control items were selected such
that there was high agreement across the system de-
velopers who completed that item. In all, there were
229 people who participated in the manual evalua-
tion, with 91 workers putting in more than an hour?s
worth of effort, and 21 putting in more than four
hours. After filtering Turker rankings against the
controls to discard Turkers who fell below a thresh-
old level of agreement on the control questions,
there was a collective total of 336 hours of usable
labor. This is similar to the total of 361 hours of
labor collected for WMT11.
We asked annotators to evaluate system outputs
by ranking translated sentences relative to each
other. This was our official determinant of trans-
lation quality. The total number of judgments col-
lected for each of the language pairs is given in Ta-
ble 2.
3.1 Ranking translations of sentences
Ranking translations relative to each other is a rea-
sonably intuitive task. We therefore kept the instruc-
tions simple:
You are shown a source sentence followed
by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
Each screen for this task involved judging trans-
lations of three consecutive source segments. For
each source segment, the annotator was shown the
outputs of five submissions, and asked to rank them.
We refer to each of these as ranking tasks or some-
times blocks.
Every language task had more than five partici-
pating systems ? up to a maximum of 16 for the
German-English task. Rather than attempting to get
a complete ordering over the systems in each rank-
ing task, we instead relied on random selection and
a reasonably large sample size to make the compar-
isons fair.
We use the collected rank labels to assign each
system a score that reflects how highly that system
was usually ranked by the annotators. The score for
some systemA reflects how frequently it was judged
to be better than other systems. Specifically, each
block in whichA appears includes four implicit pair-
wise comparisons (against the other presented sys-
tems). A is rewarded once for each of the four com-
parisons in which A wins, and its score is the num-
ber of such winning pairwise comparisons, divided
by the total number of non-tying pairwise compar-
isons involving A.
This scoring metric is different from that used in
prior years in two ways. First, the score previously
included ties between system rankings. In that case,
the score for A reflected how often A was rated as
better than or equal to other systems, and was nor-
malized by all comparisons involving A. However,
this approach unfairly rewards systems that are sim-
ilar (and likely to be ranked as tied). This is prob-
lematic since many of the systems use variations of
the same underlying decoder (Bojar et al, 2011).
A second difference is that this year we no longer
include comparisons against reference translations.
In the past, reference translations were included
14
among the systems to be ranked as controls, and
the pairwise comparisons were used in determin-
ing the best system. However, workers have a very
clear preference for reference translations, so includ-
ing them unduly penalized systems that, through
(un)luck of the draw, were pitted against the ref-
erences more often. These changes are part of a
broader discussion of the best way to produce the
system ranking, which we discuss at length in Sec-
tion 4.
The system scores are reported in Section 3.3.
Appendix A provides detailed tables that contain
pairwise head-to-head comparisons between pairs of
systems.
3.2 Inter- and Intra-annotator agreement in
the ranking task
Each year we calculate the inter- and intra-annotator
agreement for the human evaluation, since a reason-
able degree of agreement must exist to support our
process as a valid evaluation setup. To ensure we
had enough data to measure agreement, we occa-
sionally showed annotators items that were repeated
from previously completed items. These repeated
items were drawn from ones completed by the same
annotator and from different annotators.
We measured pairwise agreement among anno-
tators using Cohen?s kappa coefficient (?) (Cohen,
1960), which is defined as
? =
P (A)? P (E)
1? P (E)
where P (A) is the proportion of times that the anno-
tators agree, and P (E) is the proportion of time that
they would agree by chance. Note that ? is basically
a normalized version of P (A), one which takes into
account how meaningful it is for annotators to agree
with each other, by incorporating P (E). Note also
that ? has a value of at most 1 (and could possibly
be negative), with higher rates of agreement result-
ing in higher ?.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate at
which annotators agree, in the context of pairwise
comparisons. P (A) is computed similarly for intra-
annotator agreement (i.e. self-consistency), but over
pairwise comparisons that were annotated more than
once by a single annotator.
As for P (E), it should capture the probability that
two annotators would agree randomly. Therefore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is com-
puted empirically, by observing how often annota-
tors actually rank two systems as being tied. We
note here that this empirical computation is a depar-
ture from previous years? analyses, where we had
assumed that the three categories are equally likely
(yielding P (E) = 19 +
1
9 +
1
9 =
1
3 ). We believe that
this is a more principled approach, which faithfully
reflects the motivation of accounting for P (E) in the
first place.
Table 3 gives ? values for inter-annotator and
intra-annotator agreement. These give an indica-
tion of how often different judges agree, and how
often single judges are consistent for repeated judg-
ments, respectively. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0 ? 0.2 is slight, 0.2 ? 0.4
is fair, 0.4 ? 0.6 is moderate, 0.6 ? 0.8 is sub-
stantial, and 0.8 ? 1.0 is almost perfect. Based on
these interpretations, the agreement for sentence-
level ranking is fair for inter-annotator and moder-
ate for intra-annotator agreement. Consistent with
previous years, intra-annotator agreement is higher
than inter-annotator agreement, except for English?
Czech.
An important difference from last year is that the
evaluations were not constrained only to workshop
participants, but were made available to all Turk-
ers. The workshop participants were trusted to com-
plete the tasks in good faith, and we have multiple
years of data establishing general levels of inter- and
intra-annotator agreement. Their HITs were unpaid,
and access was limited with the use of a qualifica-
tion. The Turkers completed paid tasks, and we used
controls to filter out fraudulent and unconscientious
workers.
15
INTER-ANNOTATOR AGREEMENT INTRA-ANNOTATOR AGREEMENT
LANGUAGE PAIRS P (A) P (E) ? P (A) P (E) ?
Czech-English 0.567 0.405 0.272 0.660 0.405 0.428
English-Czech 0.576 0.383 0.312 0.566 0.383 0.296
German-English 0.595 0.401 0.323 0.733 0.401 0.554
English-German 0.598 0.394 0.336 0.732 0.394 0.557
Spanish-English 0.540 0.408 0.222 0.792 0.408 0.648
English-Spanish 0.504 0.398 0.176 0.566 0.398 0.279
French-English 0.568 0.406 0.272 0.719 0.406 0.526
English-French 0.519 0.388 0.214 0.634 0.388 0.401
WMT12 0.568 0.396 0.284 0.671 0.396 0.455
WMT11 0.601 0.362 0.375 0.722 0.362 0.564
Table 3: Inter- and intra-annotator agreement rates for the WMT12 manual evaluation. For comparison, the WMT11
rows contain the results from the European languages individual systems task (Callison-Burch et al (2011), Table 7).
Agreement rates vary widely across languages.
For inter-annotator agreements, the range is 0.176 to
0.336, while intra-annotator agreement ranges from
0.279 to 0.648. We note in particular the low agree-
ment rates among judgments in the English-Spanish
task, which is reflected in the relative lack of statis-
tical significance Table 4. The agreement rates for
this year were somewhat lower than last year.
3.3 Results of the Translation Task
We used the results of the manual evaluation to an-
alyze the translation quality of the different systems
that were submitted to the workshop. In our analy-
sis, we aimed to address the following questions:
? Which systems produced the best translation
quality for each language pair?
? Which of the systems that used only the pro-
vided training materials produced the best
translation quality?
Table 4 shows the system ranking for each of the
translation tasks. For each language pair, we define
a system as ?winning? if no other system was found
statistically significantly better (using the Sign Test,
at p ? 0.10). In some cases, multiple systems are
listed as winners, either due to a large number of par-
ticipants or a low number of judgments per system
pair, both of which are factors that make it difficult
to achieve statistical significance.
As in prior years, unconstrained online systems
A and B are among the best for many tasks, with
a few notable exceptions. CU-DEPFIX, which post-
processes the output of ONLINE-B, was judged as
the best system for English-Czech. For the French-
English and English-French tasks, constrained sys-
tems came out on top, with LIMSI appearing both
times. Consistent with prior years, the rule-based
systems performed very well on the English-German
task. A rule-based system also had a good showing
for English-Spanish, but not really anywhere else.
Among the systems competing in all tasks, no sin-
gle system consistently appeared among the top en-
trants. Participants that competed in all tasks tended
to fair worse, with the exception of UEDIN. Addi-
tionally, KIT appeared in four tasks and was a con-
strained winner each time.
4 Methods for Overall Ranking
Last year one of the long papers published at WMT
criticized our method for compiling the overall rank-
ing for systems in the translation task (Bojar et
al., 2011). This year another paper shows some
additional potential inconsistencies in the rankings
(Lopez, 2012). In this section we delve into a de-
tailed analysis of a variety of methods that use the
human evaluation to create an overall ranking of sys-
tems.
In the human evaluation, we collect ranking judg-
ments for output from five systems at a time. We in-
terpret them as 10 ?
(
5?4
2
)
pairwise judgments over
systems and use these to analyze how each system
faired compared against each of the others. Not all
16
Czech-English
3,603?3,718 comparisons/system
System C? >others
ONLINE-B ? N 0.65
UEDIN ? Y 0.60
CU-BOJAR Y 0.53
ONLINE-A N 0.53
UK Y 0.37
JHU Y 0.32
Spanish-English
1,527?1,775 comparisons/system
System C? >others
ONLINE-A ? N 0.62
ONLINE-B ? N 0.61
QCRI ? Y 0.60
UEDIN ?? Y 0.58
UPC Y 0.57
GTH-UPM Y 0.52
RBMT-3 N 0.51
JHU Y 0.48
RBMT-4 N 0.46
RBMT-1 N 0.42
ONLINE-C N 0.42
UK Y 0.19
French-English
1,437?1,701 comparisons/system
System C? >others
LIMSI ?? Y 0.63
KIT ?? Y 0.61
ONLINE-A ? N 0.59
CMU ?? Y 0.57
ONLINE-B ? N 0.57
UEDIN Y 0.55
LIUM Y 0.52
RWTH Y 0.52
RBMT-1 N 0.46
RBMT-3 N 0.46
UK Y 0.44
SFU Y 0.44
RBMT-4 N 0.43
JHU Y 0.41
ONLINE-C N 0.32
English-Czech
2,652?3,146 comparisons/system
System C? >others
CU-DEPFIX ? N 0.66
ONLINE-B N 0.63
UEDIN ? Y 0.56
CU-TAMCH N 0.56
CU-BOJAR ? Y 0.54
CU-TECTOMT ? Y 0.53
ONLINE-A N 0.53
COMMERCIAL-1 N 0.48
COMMERCIAL-2 N 0.46
CU-POOR-COMB Y 0.44
UK Y 0.44
SFU Y 0.36
JHU Y 0.32
English-Spanish
2,013?2,294 comparisons/system
System C? >others
ONLINE-B ? N 0.65
RBMT-3 N 0.58
ONLINE-A ? N 0.56
PROMT N 0.55
UPC ? Y 0.52
UEDIN ? Y 0.52
RBMT-4 N 0.46
RBMT-1 N 0.45
ONLINE-C N 0.43
UK Y 0.41
JHU Y 0.36
English-French
1,410?1,697 comparisons/system
System C? >others
LIMSI ?? Y 0.66
RWTH Y 0.62
ONLINE-B N 0.60
KIT ?? Y 0.59
LIUM Y 0.55
UEDIN Y 0.53
RBMT-3 N 0.52
ONLINE-A N 0.51
PROMT N 0.51
RBMT-1 N 0.48
JHU Y 0.44
UK Y 0.40
RBMT-4 N 0.39
ONLINE-C N 0.39
ITS-LATL N 0.36
German-English
1,386?1,567 comparisons/system
System C? >others
ONLINE-A ? N 0.65
ONLINE-B ? N 0.65
QUAERO Y 0.61
RBMT-3 N 0.60
UEDIN ? Y 0.60
RWTH ? Y 0.56
KIT ? Y 0.55
LIMSI Y 0.54
QCRI Y 0.52
RBMT-1 N 0.51
RBMT-4 N 0.50
ONLINE-C N 0.43
DFKI-BERLIN Y 0.40
UK Y 0.37
JHU Y 0.34
UG Y 0.17
English-German
1,777?2,160 comparisons/system
System C? >others
ONLINE-B ? N 0.64
RBMT-3 N 0.63
RBMT-4 ? N 0.58
RBMT-1 N 0.56
LIMSI ? Y 0.55
ONLINE-A N 0.54
UEDIN-WILLIAMS ? Y 0.51
KIT ? Y 0.50
DFKI-HUNSICKER N 0.48
UEDIN ? Y 0.47
RWTH ? Y 0.47
ONLINE-C N 0.47
UK Y 0.45
JHU Y 0.43
DFKI-BERLIN Y 0.25
C? indicates whether system is constrained (unhighlighted rows): trained only using supplied training data, standard
monolingual linguistic tools, and, optionally, LDC?s English Gigaword.
? indicates a win: no other system is statistically significantly better at p-level ? 0.10 in pairwise comparison.
? indicates a constrained win: no other constrained system is statistically better.
Table 4: Official results for the WMT12 translation task. Systems are ordered by their > others score, reflecting how
often their translations won in pairwise comparisons. For detailed head-to-head comparisons, see Appendix A.
17
pairwise comparisons detect statistical significantly
superior quality of either system, and we note this
accordingly.
It is desirable to additionally produce an overall
ranking. In the past evaluation campaigns, we used
two different methods to obtain such a ranking, and
this year we use yet another one. In this section, we
discuss each of these overall ranking methods and a
few more.
4.1 Rank Ranges
In the first human evaluation, we use fluency and
adequacy judgments on a scale from 1 to 5 (Koehn
and Monz, 2006). We normalized the scores on a
per-sentence basis, thus converting them to a rela-
tive ranking in a 5-system comparison. We listed
systems by the average of these scores over all sen-
tences, in which they were judged.
We did not report ranks, but rank ranges. To
give an example: if a system scored neither sta-
tistically significantly better nor statistically signif-
icantly worse than 3 other systems, we assign it the
rank range 1?4. The given evidence is not sufficient
to rank it exactly, but it does rank somewhere in the
top 4.
In subsequent years, we did not continue the re-
porting of rank ranges (although they can be ob-
tained by examining the pairwise comparison ta-
bles), but we continued to report systems as win-
ners whenever there was not statistically signifi-
cantly outperformed by any other system.
4.2 Ratio of Wins and Ties
In the following years (Callison-Burch et al, 2007;
Callison-Burch et al, 2008; Callison-Burch et al,
2009; Callison-Burch et al, 2010; Callison-Burch et
al., 2011), we abandoned the idea of using fluency
and adequacy judgments, since they showed to be
less reliable than simple ranking of system transla-
tions. We also started to interpret the 5-system com-
parison as a set of pairwise comparisons.
Systems were then ranked by the ratio of how of-
ten they were ranked better or equal to any of the
other systems.
Given a set J of sentence-level judgments
(s1, s2, c) where s1 ? S and s2 ? S are two sys-
tems and
c =
?
??
??
win if s1 better than s2
tie if s1 equal to s2
loss if s1 worse than s2
(1)
then we can count the total number of wins and ties
of a system s as
win(s) = |{(s1, s2, c) ? J : s = s1, c = win}|+
|{(s1, s2, c) ? J : s = s2, c = loss}|
loss(s) = |{(s1, s2, c) ? J : s = s1, c = loss}|+
|{(s1, s2, c) ? J : s = s2, c = win}|
tie(s) = |{(s1, s2, c) ? J : s = s1, c = tie}|+
|{(s1, s2, c) ? J : s = s2, c = tie}|
(2)
and rank systems by the ratio
score(s) =
win(s) + tie(s)
win(s) + loss(s) + tie(s)
(3)
This ratio was used for the official rankings over
the last five years.
4.3 Ratio of Wins (Ignoring Ties)
Bojar et al (2011) present a persuasive argument
that our ranking scheme is biased towards systems
that are similar to many other systems. Given that
most of the systems are based on phrase-based mod-
els trained on the same training data, this is indeed a
valid concern.
They suggest ignoring ties, and using as ranking
score instead the following ratio:
score(s) =
win(s)
win(s) + loss(s)
(4)
This ratio is used for the official ranking this year.
4.4 Minimizing Pairwise Ranking Violations
Lopez (2012, in this volume) argues against using
aggregate statistics over a set of very diverse judg-
ments. Instead, a ranking that has the least number
of pairwise ranking violations is said to be preferred.
If we define the number of pairwise wins as
win(s1, s2) = |{(s1, s2, c) ? J : c = win}|+
|{(s2, s1, c) ? J : c = loss}|
(5)
then we define a count function for pairwise order
violations as
18
score(s1, s2) = max(0,win(s2, s1)? win(s1, s2))
(6)
Given a bijective ranking function R(s)? i with
the codomain of consecutive integers starting at 1,
the total number of pairwise ranking violations is de-
fined as
score(R) =
?
R(si)<R(sj)
score(si, sj) (7)
Finding the optimal rankingR that minimizes this
score is not trivial, but given the number of systems
involved in this evaluation campaign, it is quite man-
ageable.
4.5 Most Probable Ranking
We now introduce a variant to Lopez?s ranking
method. We motivate it first.
Consider the following scenario:
win(A,B) = 20 win(B,A) = 0
win(B,C) = 40 win(C,B) = 20
win(C,A) = 60 win(A,C) = 40
Since this constitutes a circle, there are three
rankings with the minimum number of 20 violation
(ABC, BCA, CAB).
However, we may want to take the ratio of wins
and losses for each pairwise ranking into account.
Using maximum likelihood estimation, we can de-
fine the probability that system s1 is better than sys-
tem s2 on a randomly drawn sentence as
p(s1 > s2) =
win(s1, s2)
win(s1, s2) + win(s2, s1)
(8)
We can then go on to define5 the probability of a
5Sketch of derivation:
p(s1 > s2 > s3) = p(s1 first)p(s2 second|s1 first)
(chain rule)
p(s1 first) = p(s1 > s2 and s1 > s3)
= p(s1 > s2)p(s1 > s3)
(independence assumption)
p(s2 sec.|s1 first) = p(s2 second)
(independence assumption)
= p(s2 > s3)
ranking of three systems as:
p(s1 > s2 > s3) = p(s1 > s2)p(s1 > s3)p(s2 > s3)
(9)
This function scores the three rankings in the ex-
ample above as follows:
p(A > B > C) = 2020
40
100
40
60 = 0.27
p(B > C > A) = 4060
0
20
60
100 = 0
p(C > A > B) = 60100
20
60
20
20 = 0.20
One disadvantage of this and the previous rank-
ing method is that they do not take advantage of all
available evidence. Consider the example:
win(A,B) = 100 win(B,A) = 0
win(A,C) = 60 win(C,A) = 40
win(B,C) = 50 win(C,B) = 50
Here, system A is clearly ahead, but how about B
and C? They are tied in their pairwise comparison.
So, both ABC and ACB have no pairwise ranking
violations and their most probable ranking score, as
defined above, is the same.
B is clearly worse than A, but C has a fighting
chance, and this should be reflected in the ranking.
The following two overall ranking methods over-
come this problem.
4.6 Monte Carlo Playoffs
The sports world is accustomed to the problem of
finding a ranking of sports teams, but being only able
to have pairwise competitions (think basketball or
football). One strategy is to stage playoffs.
Let?s say there are 4 systems: A,B, C, andD. As
in well-known play-off fashion, they are first seeded.
In our case, this happens randomly, say, 1:A, 2:B,
3:C, 4:D (for simplicity?s sake).
First round: A plays against D, B plays against
C. How do they play? We randomly select a sen-
tence on which they were compared (no ties). If A
is better according to human judgment than D, then
A wins.
Let?s say, A wins against D, and B loses against
C. This leads us to the final A against C and the
3rd place game D against B, in which, say, A and D
win. The resulting final ranking is ACDB.
We repeat this a million times with a different ran-
dom seeding every time, and compute the average
rank, which is then used for overall ranking.
19
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.641: ONLINE-B RBMT-4 RBMT-4 6.16: ONLINE-B 0.640 (1-2): ONLINE-B
2 0.627: RBMT-3 ONLINE-B ONLINE-B 6.39: RBMT-3 0.622 (1-2): RBMT-3
3 0.577: RBMT-4 RBMT-3 RBMT-3 6.98: RBMT-4 0.578 (3-5): RBMT-4
4 0.557: RBMT-1 RBMT-1 RBMT-1 7.32: RBMT-1 0.553 (3-6): RBMT-1
5 0.547: LIMSI ONLINE-A ONLINE-A 7.46: LIMSI 0.543 (3-7): LIMSI
6 0.537: ONLINE-A UEDIN-WILLIAMS LIMSI 7.57: ONLINE-A 0.534 (4-8): ONLINE-A
7 0.509: UEDIN-WILLIAMS LIMSI UEDIN-WILLIAMS 7.87: UEDIN-WILLIAMS 0.511 (5-9): UEDIN-WILLIAMS
8 0.503: KIT KIT KIT 7.98: KIT 0.503 (6-11): KIT
9 0.476: DFKI-HUNSICKER DFKI-HUNSICKER DFKI-HUNSICKER 8.32: UEDIN 0.477 (7-13): UEDIN
10 0.475: UEDIN ONLINE-C ONLINE-C 8.38: DFKI-HUNSICKER 0.472 (8-13): DFKI-HUNSICKER
11 0.470: RWTH UEDIN UEDIN 8.41: ONLINE-C 0.470 (8-13): ONLINE-C
12 0.470: ONLINE-C UK UK 8.44: RWTH 0.468 (8-13): RWTH
13 0.448: UK RWTH RWTH 8.72: UK 0.447 (10-14): UK
14 0.435: JHU JHU JHU 8.87: JHU 0.434 (12-14): JHU
15 0.249: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 11.15: DFKI-BERLIN 0.249 (15): DFKI-BERLIN
Table 5: Overall ranking with different methods (English?German)
4.7 Expected Wins
In European national football competitions, each
team plays against each other team, and at the end
the number of wins decides the rankings.6 We can
simulate this type of tournament as well with Monte
Carlo methods. However, in the limit, each team will
be on average ranked based on its expected number
of wins in the competition. We can compute the ex-
pected number of wins straightforward as
score(si) =
1
|S| ? 1
?
j,j 6=i
p(si > sj) (10)
Note that this is very similar to Bojar?s method of
ranking systems, with one additional and important
twist. We can rewrite Equation 4, the variant that
ignores ties, as:
score(si) =
win(si)
win(si)+loss(si)
(11)
=
?
j,j 6=i win(si,sj)?
j,j 6=i win(si,sj)+loss(si,sj)
(12)
This section?s Equation 10 can be rewritten as:
score(si) =
1
|S|
?
j,j 6=i
win(si, sj)
win(si, sj) + loss(si, sj)
(13)
The difference is that the new overall ranking
method normalizes the win ratios per pairwise rank-
ing. And this makes sense, since it overcomes one
6They actually play twice against each other, to balance out
home field advantage, which is not a concern here.
problem with our traditional and Bojar?s ranking
method.
Previously, some systems were put at an dis-
advantage, if they are compared more frequently
against good systems than against bad systems. This
could happen, if participants were not allowed to
rank their own systems (a constraint we enforced
in the past, but no longer). This was noticed by
judges a few years ago, when we had instant re-
porting of rankings during the evaluation period. If
you have one of the best systems and carry out a lot
of human judgments, then competitors? systems will
creep up higher, since they are not compared against
your own (very good) system anymore, but more fre-
quently against bad systems.
4.8 Comparison
Table 5 shows the different rankings for English?
German, a rather typical example. The table dis-
plays the ranking of the systems according to five
different methods, alongside with system scores ac-
cording to the ranking method: the win ratio (Bo-
jar), the average rank (MC Playoffs), and the ex-
pected win ratio (Expected Wins). For the latter, we
performed bootstrap resampling and computed rank
ranges that lie in a 95% confidence interval. You
can find the tables for the other language pairs in the
annex.
The win-based methods (Bojar, MC Playoffs, Ex-
pected Wins) give very similar rankings ? exhibit-
ing mostly just the occasional pairwise flip or for
20
many language pairs the ranking is identical. The
same is true for the two methods based on pairwise
rankings (Lopez, Most Probable). However, the two
types of ranking lead to significantly different out-
comes.
For instance, the win-based methods are pretty
sure that ONLINE-B and RBMT-3 are the two top
performers. Bootstrap resampling of rankings ac-
cording to Expected Wins ranking draws a clear
line between them and the rest. However, Lopez?s
method ranks RBMT-4 first. Why? In direct com-
parison of the three systems, RBMT-4 beats statis-
tically insignificantly ONLINE-B 45% wins against
42% wins and essentially ties with RBMT-3 41%
wins against 41% wins (ONLINE-B beats RBMT-3
49%?35%, p ? 0.01).
We use Bojar?s method as our official method for
ranking in Table 4 and as the human judgments that
we used when calculating how well automatic eval-
uation metrics correlate with human judgments.
4.9 Number of Judgments Needed
In general, there are not enough judgments to rank
systems unambiguously. How many judgments do
we need?
We may extrapolate this number from the num-
ber of judgments we have. Figure 2 provides some
hints. The outlier is Czech?English, for which only
6 systems were submitted and we can separate them
almost completely even at p-level 0.01. For all the
other language pairs, we can only draw for around
40% of the pairwise comparisons conclusions with
that level of statistical significance.
Since the plots also contains the ratio of signifi-
cant conclusions when sub-sampling the number of
judgments, we obtain curves with a clear upward
slope. For English?Czech, for which we were able
to collect much more judgments, we can draw over
60% significant conclusions. The curve for this lan-
guage pair does not look much different than the
other languages, suggesting that doubling the num-
ber of judgments should allow similar levels for
them as well.
5 Metrics Task
In addition to allowing us to analyze the translation
quality of different systems, the data gathered during
p-level 0.01
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.05
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
p-level 0.10
 0
 20
 40
 60
 80
 100
 0  5000  10000  15000  20000  25000
Czech-English (6 systems)French-English (15 systems)Spanish-English (12 systems)German-English (16 systems)English-Czech (13 systems)English-French (15 systems)English-Spanish (11 systems)English-German (15 systems)
Figure 2: Ratio of statistically significant pairwise com-
parisons at different p-levels, based on number of pair-
wise judgments collected.
21
Metric IDs Participant
AMBER National Research Council Canada (Chen et al, 2012)
METEOR CMU (Denkowski and Lavie, 2011)
SAGAN-STS FaMAF, UNC, Argentina (Castillo and Estrella, 2012)
SEMPOS Charles University (Macha?c?ek and Bojar, 2011)
SIMBLEU University of Sheffield (Song and Cohn, 2011)
SPEDE Stanford University (Wang and Manning, 2012)
TERRORCAT University of Zurich, DFKI, Charles U (Fishel et al, 2012)
BLOCKERRCATS, ENXERRCATS, WORD-
BLOCKERRCATS, XENERRCATS, POSF
DFKI (Popovic, 2012)
Table 6: Participants in the metrics task.
the manual evaluation is useful for validating auto-
matic evaluation metrics. Table 6 lists the partici-
pants in this task, along with their metrics.
A total of 12 metrics and their variants were sub-
mitted to the metrics task by 8 research groups. We
provided BLEU and TER scores as baselines. We
asked metrics developers to score the outputs of
the machine translation systems and system com-
binations at the system-level and at the segment-
level. The system-level metrics scores are given in
the Appendix in Tables 29?36. The main goal of
the metrics shared task is not to score the systems,
but instead to validate the use of automatic metrics
by measuring how strongly they correlate with hu-
man judgments. We used the human judgments col-
lected during the manual evaluation for the transla-
tion task and the system combination task to calcu-
late how well metrics correlate at system-level and
at the segment-level.
5.1 System-Level Metric Analysis
We measured the correlation of the automatic met-
rics with the human judgments of translation qual-
ity at the system-level using Spearman?s rank cor-
relation coefficient ?. We converted the raw scores
assigned to each system into ranks. We assigned a
human ranking to the systems based on the percent
of time that their translations were judged to be bet-
ter than the translations of any other system in the
manual evaluation (Equation 4).
When there are no ties, ? can be calculated using
the simplified equation:
? = 1?
6
?
d2i
n(n2 ? 1)
C
S
-E
N
-
6
S
Y
S
T
E
M
S
D
E
-E
N
-
16
S
Y
S
T
E
M
S
E
S
-E
N
-
12
S
Y
S
T
E
M
S
F
R
-E
N
-
15
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations into English
SEMPOS .94 .92 .94 .80 .90
AMBER .83 .79 .97 .85 .86
METEOR .66 .89 .95 .84 .83
TERRORCAT .71 .76 .97 .88 .83
SIMPBLEU .89 .70 .89 .82 .82
TER -.89 -.62 -.92 -.82 .81
BLEU .89 .67 .87 .81 .81
POSF .66 .66 .87 .83 .75
BLOCKERRCATS -.64 -.75 -.88 -.74 .75
WORDBLOCKEC -.66 -.67 -.85 -.77 .74
XENERRCATS -.66 -.64 -.87 -.77 .74
SAGAN-STS .66 n/a .91 n/a n/a
Table 7: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average absolute
value.
22
E
N
-C
Z
-
10
S
Y
S
T
E
M
S
E
N
-D
E
-
22
S
Y
S
T
E
M
S
E
N
-E
S
-
15
S
Y
S
T
E
M
S
E
N
-F
R
-
17
S
Y
S
T
E
M
S
A
V
E
R
A
G
E
System-level correlation for translations out of English
SIMPBLEU .83 .46 .42 .94 .66
BLOCKERRCATS -.65 -.53 -.47 -.93 .64
ENXERRCATS -.74 -.38 -.47 -.93 .63
POSF .80 .54 .37 .69 .60
WORDBLOCKEC -.71 -.37 -.47 -.81 .59
TERRORCAT .65 .48 .58 .53 .56
AMBER .71 .25 .50 .75 .55
TER -.69 -.41 -.45 -.66 .55
METEOR .73 .18 .45 .82 .54
BLEU .80 .22 .40 .71 .53
SEMPOS .52 n/a n/a n/a n/a
Table 8: System-level Spearman?s rho correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average abso-
lute value.
where di is the difference between the rank for
systemi and n is the number of systems. The pos-
sible values of ? range between 1 (where all systems
are ranked in the same order) and?1 (where the sys-
tems are ranked in the reverse order). Thus an auto-
matic evaluation metric with a higher absolute value
for ? is making predictions that are more similar to
the human judgments than an automatic evaluation
metric with a lower absolute ?.
The system-level correlations are shown in Ta-
ble 7 for translations into English, and Table 8 out
of English, sorted by average correlation across the
language pairs. The highest correlation for each
language pair and the highest overall average are
bolded. Once again this year, many of the metrics
had stronger correlation with human judgments than
BLEU. The metrics that had the strongest correlation
this year were SEMPOS for the into English direc-
tion and SIMPBLEU for the out of English direc-
tion.
5.2 Segment-Level Metric Analysis
We measured the metrics? segment-level scores with
the human rankings using Kendall?s tau rank corre-
F
R
-E
N
(1
15
94
PA
IR
S
)
D
E
-E
N
(1
19
34
PA
IR
S
)
E
S
-E
N
(9
79
6
PA
IR
S
)
C
S
-E
N
(1
10
21
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations into English
SPEDE07-PP .26 .28 .26 .21 .25
METEOR .25 .27 .25 .21 .25
AMBER .24 .25 .23 .19 .23
SIMPBLEU .19 .17 .19 .13 .17
TERRORCAT .18 .19 .18 .19 .19
XENERRCATS .17 .18 .18 .13 .17
POSF .16 .18 .15 .12 .15
WORDBLOCKEC .15 .16 .17 .13 .15
BLOCKERRCATS .07 .08 .08 .06 .07
SAGAN-STS n/a n/a .21 .20 n/a
Table 9: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation into English, ordered by average correla-
tion.
E
N
-F
R
(1
15
62
PA
IR
S
)
E
N
-D
E
(1
45
53
PA
IR
S
)
E
N
-E
S
(1
18
34
PA
IR
S
)
E
N
-C
S
(1
88
05
PA
IR
S
)
A
V
E
R
A
G
E
Segment-level correlation for translations out of English
METEOR .26 .18 .21 .16 .20
AMBER .23 .17 .22 .15 .19
TERRORCAT .18 .19 .18 .18 .18
SIMPBLEU .2 .13 .18 .10 .15
ENXERRCATS .20 .11 .17 .09 .14
POSF .15 .13 .15 .13 .14
WORDBLOCKEC .19 .1 .17 .1 .14
BLOCKERRCATS .13 .04 .12 .01 .08
Table 10: Segment-level Kendall?s tau correlation of the
automatic evaluation metrics with the human judgments
for translation out of English, ordered by average corre-
lation.
23
lation coefficient. We calculated Kendall?s tau as:
? =
num concordant pairs - num discordant pairs
total pairs
where a concordant pair is a pair of two translations
of the same segment in which the ranks calculated
from the same human ranking task and from the cor-
responding metric scores agree; in a discordant pair,
they disagree. In order to account for accuracy- vs.
error-based metrics correctly, counts of concordant
vs. discordant pairs were calculated specific to these
two metric types. The possible values of ? range
between 1 (where all pairs are concordant) and ?1
(where all pairs are discordant). Thus an automatic
evaluation metric with a higher value for ? is mak-
ing predictions that are more similar to the human
judgments than an automatic evaluation metric with
a lower ? .
We did not include cases where the human rank-
ing was tied for two systems. As the metrics produce
absolute scores, compared to five relative ranks in
the human assessment, it would be potentially un-
fair to the metric to count a slightly different met-
ric score as discordant with a tie in the relative hu-
man rankings. A tie in automatic metric rank for
two translations was counted as discordant with two
corresponding non-tied human judgments.
The correlations are shown in Table 9 for trans-
lations into English, and Table 10 out of English,
sorted by average correlation across the four lan-
guage pairs. The highest correlation for each lan-
guage pair and the highest overall average are
bolded. For the into English direction SPEDE and
METEOR tied for the highest segment-level correla-
tion. METEOR performed the best for the out of En-
glish direction, with AMBER doing admirably well
in both the into- and the out-of-English directions.
6 Quality Estimation task
Quality estimation aims to provide a quality indica-
tor for machine translated sentences at various gran-
ularity levels. It differs from MT evaluation, because
quality estimation techniques do not rely on refer-
ence translations. Instead, quality estimation is gen-
erally addressed using machine learning techniques
to predict quality scores. Potential applications of
quality estimation include:
? Deciding whether a given translation is good
enough for publishing as is
? Informing readers of the target language only
whether or not they can rely on a translation
? Filtering out sentences that are not good
enough even for post-editing by professional
translators
? Selecting the best translation among options
from multiple systems.
This shared-task provides a first common ground
for development and comparison of quality estima-
tion systems, focusing on sentence-level estimation.
It provides training and test datasets, along with
evaluation metrics and a baseline system. The goals
of this shared task are:
? To identify new and effective quality indicators
(features)
? To identify alternative machine learning tech-
niques for the problem
? To test the suitability of the proposed evalua-
tion metrics for quality estimation systems
? To establish the state of the art performance in
the field
? To contrast the performance of regression and
ranking techniques.
The task provides datasets for a single language
pair, text domain and MT system: English-Spanish
news texts produced by a phrase-based SMT sys-
tem (Moses) trained on Europarl and News Com-
mentaries corpora provided in the WMT10 transla-
tion task. As training data, translations were man-
ually annotated for quality in terms of post-editing
effort (1-5 scores) and were provided together with
their source sentences, reference translations, and
post-edited translations (Section 6.1). The shared-
task consisted on automatically producing quality-
estimations for a blind test-set, where English source
sentences and their MT-translations were used as in-
puts. Hidden (and subsequently publicly-released)
manual effort-annotations of those translations (ob-
tained in the same fashion as for the training data)
24
were used as reference labels to evaluate the per-
formance of the participating systems (Section 6.1).
Participants also had full access to the translation
engine-related resources (Section 6.1) and could use
any additional external resources. We have also pro-
vided a software package to extract baseline quality
estimation features (Section 6.3).
Participants could submit up to two systems for
two variations of the task: ranking, where par-
ticipants submit a ranking of translations (no ties
allowed), without necessarily giving any explicit
scores for translations, and scoring, where partici-
pants submit a score for each sentence (in the [1,5]
range). Each of these subtasks is evaluated using
specific metrics (Section 6.2).
6.1 Datasets and resources
Training data
The training data used was selected from data
available from previous WMT shared-tasks for
machine-translation: a subset of the WMT10
English-Spanish test set, and a subset of the WMT09
English-Spanish test set, for a total of 1832 sen-
tences.
The training data consists of the following re-
sources:
? English source sentences
? Spanish machine-translation outputs, created
using the SMT Moses engine
? Effort scores, created by using three profes-
sional post-editors using guidelines describ-
ing Post-Editing (PE) effort from highest effort
(score 1) to lowest effort (score 5)
? Post-Editing output, created by a pool of pro-
fessional post-editors starting from the source
sentences and the Moses translations; these PE
outputs were created before the effort scores
were elicited, and were shown to the PE-effort
judges to facilitate their effort estimates
? Spanish translation outputs, created as part of
the WMT machine-translation shared-task as
reference translations for the English source
sentences (independent of any MT output).
The guidelines used by the PE-effort judges to as-
sign scores 1-5 for each of the ?source, MT-output,
PE-output? triplets are the following:
[1] The MT output is incomprehensible, with lit-
tle or no information transferred accurately. It
cannot be edited, needs to be translated from
scratch.
[2] About 50-70% of the MT output needs to be
edited. It requires a significant editing effort in
order to reach publishable level.
[3] About 25-50% of the MT output needs to be
edited. It contains different errors and mis-
translations that need to be corrected.
[4] About 10-25% of the MT output needs to be
edited. It is generally clear and intelligible.
[5] The MT output is perfectly clear and intelligi-
ble. It is not necessarily a perfect translation,
but requires little or no editing.
Providing reliable effort estimates turned out to
be a difficult task for the PE-effort judges, even in
the current set-up (with post edited outputs available
for consultation). To eliminate some of the noise
from these judgments, we performed an intermedi-
ate cleaning step, in which we eliminated the sen-
tences for which the difference between the max-
imum score and the minimum score assigned be-
tween the three judges was > 1. We started the
data-creation process from a total of 2000 sentences
for the training set, and the final 1832 sentences we
selected as training data were the ones that passed
through this intermediate cleaning step.
Besides score disagreement, we noticed another
trend on the human judgements of PE-effort. Some
judges tend to give more moderate scores (in the
middle of available range), while others like to com-
mit also to scores that are more in the extremes of
the available range. Since the quality estimation task
would be negatively influenced by having most of
the scores in the middle of the range, we have chosen
to compute the final effort scores as an weighted av-
erage between the three PE-effort scores, with more
weight given to the judges with higher standard de-
viation from their own mean score. We have used
25
weights 3, 2, and 1 for the three PE-effort judges ac-
cording to this criterion. There is an additional ad-
vantage resulting from this weighted average score:
instead of obtaining average numbers only at val-
ues x.0, x.33, and x.66 (for unweighted average)7,
the weighted averages are spread more evenly in the
range [1, 5].
A few variations of the training data were pro-
vided, including version with cases restored and a
version detokenized. In addition, engine-internal
information from Moses such as phrase and word
alignments, detailed model scores, etc. (parameter
-trace), n-best lists and stack information from the
search graph as a word graph (parameter -output-
word-graph) as produced by the Moses engine were
provided.
The rationale behind releasing this engine-
internal data was to make it possible for this shared-
task to address quality estimation using a glass-box
approach, that is, making use of information from
the internal workings of the MT engine.
Test data
The test data was a subset of the WMT12 English-
Spanish test set, consisting of 442 sentences. The
test data consists of the following files:
? English source sentences
? Spanish machine-translation outputs, created
using the same SMT Moses engine used to cre-
ate the training data
? Effort scores, created by using three profes-
sional post-editors8 using guidelines describing
PE effort from highest effort (score 1) to lowest
effort (score 5)
The first two files were the input for the quality-
estimation shared-task participating systems. Since
the Moses engine used to create the MT outputs was
the same as the one used for generating the train-
ing data, the engine-internal resources are the same
7These three values are the only ones possible given the
cleaning step we perform prior to averaging the scores, which
ensures that the difference between the maximum score and the
minimum score is at most 1.
8The same post-editors that were used to create the training
data were used to create the test data.
as the ones we released as part of the training data
package.
The effort scores were released after the partic-
ipants submitted their shared-task submission, and
were solely used to evaluate the submissions accord-
ing to the established metrics. The guidelines used
by the PE-effort judges to assign 1-5 scores were the
same as the ones used for creating the training data.
We have used the same criteria to ensure the con-
sistency of the human judgments. The initial set of
candidates consisted of 604 sentences, of which only
442 met this criteria. The final scores used as gold-
values have been obtained using the same weighted-
average scheme as for the training data.
Resources
In addition to the training and test materials, we
made several additional resources that were used for
the baseline QE system and/or the SMT system that
produced the training and test datasets:
? The SMT training corpus: source and target
sides of the corpus used to train the Moses en-
gine. These are a concatenation of the Eu-
roparl and the news-commentary data sets from
WMT10 that were tokenized, cleaned (remov-
ing sentences longer than 80 tokens) and true-
cased.
? Two Language models: 5-gram LM generated
from the interpolation of the two target cor-
pora after tokenization and truecasing (used
by Moses) and a trigram LM generated from
the two source corpora and filtered to remove
singletons (used by the baseline QE system).
We also provided unigram, bigram and trigram
counts (used in the baseline QE system).
? An IBM Model 1 table that generated by
Giza++ using the SMT training corpora.
? A word-alignment file as produced by the
grow-diag-final heuristic in Moses for the SMT
training set.
? A phrase table with word alignment informa-
tion generated from the parallel corpora.
? The Moses configuration file used for decod-
ing.
26
6.2 Evaluation metrics
Ranking metrics
For the ranking task, we defined a novel met-
ric that provides some advantages over a more tra-
ditional ranking metrics like Spearman correlation.
Our metric, called DeltaAvg, assumes that the refer-
ence test set has a number associated with each en-
try that represents its extrinsic value. For instance,
using the effort scale we described in Section 6.1,
we associate a value between 1 and 5 with each
sentence, representing the quality of that sentence.
Given these values, our metric does not need an ex-
plicit reference ranking, the way the Spearman rank-
ing correlation does.9 The goal of the DeltaAvg met-
ric is to measure how valuable a proposed ranking
(which we call a hypothesis ranking) is according to
the extrinsic values associated with the test entries.
We first define a parameterized version of this
metric, called DeltaAvg[n]. The following notations
are used: for a given entry sentence s, V (s) repre-
sents the function that associates an extrinsic value
to that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.10 We also use the notation
Si,j =
?j
k=i Sk. Using these notations, we define:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1
? V (S) (14)
When the valuation function V is clear from the con-
text, we write DeltaAvg[n] for DeltaAvgV [n]. The
parameter n represents the number of quantiles we
want to split the set S into. For instance, n = 2
gives DeltaAvg[2] = V (S1) ? V (S), hence it mea-
sures the difference between the quality of the top
9A reference ranking can be implicitly induced according to
these values; if, as in our case, higher values mean better sen-
tences, then the reference ranking is defined such that higher-
scored sentences rank higher than lower-scored sentences.
10If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
quantile (top half) S1 and the overall quality (rep-
resented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2 ? V (S)))/2, hence it measures an aver-
age difference across two cases: between the quality
of the top quantile (top third) and the overall qual-
ity, and between the quality of the top two quan-
tiles (S1?S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average differ-
ence in quality across n ? 1 cases, with each case
measuring the impact in quality of adding an addi-
tional quantile, from top to bottom. Finally, we de-
fine:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
(15)
whereN = |S|/2. As before, we write DeltaAvg for
DeltaAvgV when the valuation function V is clear
from the context. The DeltaAvg metric is an aver-
age across all DeltaAvg[n] values, for those n values
for which the resulting quantiles have at least 2 en-
tries (no singleton quantiles). The DeltaAvg metric
has some important properties that are desired for a
ranking metric (see Section 6.4 for the results of the
shared-task that substantiate these claims):
? it is non-parametric (i.e., it does not depend on
setting particular parameters)
? it is automatic and deterministic (and therefore
consistent)
? it measures the quality of a hypothesis rank-
ing from an extrinsic perspective (as offered by
function V )
? its values are interpretable: for a given set of
ranked entries, a value DeltaAvg of 0.5 means
that, on average, the difference in quality be-
tween the top-ranked quantiles and the overall
quality is 0.5
? it has a high correlation with the Spearman rank
correlation coefficient, which makes it as use-
ful as the Spearman correlation, with the added
advantage of its values being extrinsically in-
terpretable.
27
In the rest of this paper, we present results for
DeltaAvg using as valuation function V the Post-
Editing effort scores, as defined in Section 6.1.
We also report the results of the ranking task using
the more-traditional Spearman correlation.
Scoring metrics
For the scoring task, we use two metrics that have
been traditionally used for measuring performance
for regression tasks: Mean Absolute Error (MAE) as
a primary metric, and Root of Mean Squared Error
(RMSE) as a secondary metric. For a given test set
S with entries si, 1 ? i ? |S|, we denote by H(si)
the proposed score for entry si (hypothesis), and by
V (si) the reference value for entry si (gold-standard
value). We formally define our metrics as follows:
MAE =
?N
i=1 |H(si)? V (si)|
N
(16)
RMSE =
?
?N
i=1(H(si)? V (si))
2
N
(17)
where N = |S|. Both these metrics are non-
parametric, automatic and deterministic (and there-
fore consistent), and extrinsically interpretable. For
instance, a MAE value of 0.5 means that, on aver-
age, the absolute difference between the hypothe-
sized score and the reference score value is 0.5. The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalizes larger errors more (via
the square function).
6.3 Participants
Eleven teams (listed in Table 11) submitted one or
more systems to the shared task, with most teams
submitting for both ranking and scoring subtasks.
Each team was allowed up to two submissions (for
each subtask). In the descriptions below participa-
tion in the ranking is denoted (R) and scoring is de-
noted (S).
Baseline system (R, S): the baseline system used
the feature extraction software (also provided
to all participants). It analyzed the source and
translation files and the SMT training corpus
to extract the following 17 system-independent
features that were found to be relevant in previ-
ous work (Specia et al, 2009):
? number of tokens in the source and target
sentences
? average source token length
? average number of occurrences of the tar-
get word within the target sentence
? number of punctuation marks in source
and target sentences
? LM probability of source and target sen-
tences using language models described in
Section 6.1
? average number of translations per source
word in the sentence: as given by IBM 1
model thresholded so that P (t|s) > 0.2,
and so that P (t|s) > 0.01 weighted by
the inverse frequency of each word in the
source side of the SMT training corpus
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower fre-
quency words) and 4 (higher frequency
words) in the source side of the SMT train-
ing corpus
? percentage of unigrams in the source sen-
tence seen in the source side of the SMT
training corpus
These features are used to train a Support Vec-
tor Machine (SVM) regression algorithm using
a radial basis function kernel with the LIBSVM
package (Chang and Lin, 2011). The ?,  and C
parameters were optimized using a grid-search
and 5-fold cross validation on the training set.
We note that although the system is referred to
as a ?baseline?, it is in fact a strong system.
Although it is simple it has proved to be ro-
bust across a range of language pairs, MT sys-
tems, and text domains. It is a simpler variant
of the system used in (Specia, 2011). The ratio-
nale behind having such a strong baseline was
to push systems to exploit alternative sources
of information and combination / learning ap-
proaches.
SDLLW (R, S): Both systems use 3 sets of fea-
tures: the 17 baseline features, 8 system-
dependent features from the decoder logs of
Moses, and 20 features developed internally.
Some of these features made use of additional
data and/or resources, such as a secondary
28
ID Participating team
PRHLT-UPV Universitat Politecnica de Valencia, Spain (Gonza?lez-Rubio et al, 2012)
UU Uppsala University, Sweden (Hardmeier et al, 2012)
SDLLW SDL Language Weaver, USA (Soricut et al, 2012)
Loria LORIA Institute, France (Langlois et al, 2012)
UPC Universitat Politecnica de Catalunya, Spain (Pighin et al, 2012)
DFKI DFKI, Germany (Avramidis, 2012)
WLV-SHEF University of Wolverhampton & University of Sheffield, UK (Felice and Specia, 2012)
SJTU Shanghai Jiao Tong University, China (Wu and Zhao, 2012)
DCU-SYMC Dublin City University, Ireland & Symantec, Ireland (Rubino et al, 2012)
UEdin University of Edinburgh, UK (Buck, 2012)
TCD Trinity College Dublin, Ireland (Moreau and Vogel, 2012)
Table 11: Participants in the WMT12 Quality Evaluation shared task.
MT system that was used as pseudo-reference
for the hypothesis, and POS taggers for both
languages. Feature-selection algorithms were
used to select subsets of features that directly
optimize the metrics used in the task. System
?SDLLW M5PbestAvgDelta? uses a resulting
15-feature set optimized towards the AvgDelta
metric. It employs an M5P model to learn a
decision-tree with only two linear equations.
System ?SDLLW SVM? uses a 20-feature set
and an SVM epsilon regression model with ra-
dial basis function kernel with parameters C,
gamma, and epsilon tuned on a development
set (305 training instances). The model was
trained with 10-fold cross validation and the
tuning process was restarted several times us-
ing different starting points and step sizes to
avoid overfitting. The final model was selected
based on its performance on the development
set and the number of support vectors.
UU (R, S): System ?UU best? uses the 17 base-
line features, plus 82 features from Hardmeier
(2011) (with some redundancy and some over-
lap with baseline features), and constituency
trees over input sentences generated by the
Stanford parser and dependency trees over both
input and output sentences generated by the
MaltParser. System ?UU bltk? uses only the
17 baseline features plus constituency and de-
pendency trees as above. The machine learn-
ing component in both cases is SVM regres-
sion (SVMlight software). For the ranking task,
the ranking induced by the regression output
is used. The system uses polynomial kernels
of degree 2 (UU best) and 3 (UU bltk) as well
as two different types of tree kernels for con-
stituency and dependency trees, respectively.
The SVM margin/error trade-off, the mixture
proportion between tree kernels and polyno-
mial kernels and the degree of the polynomial
kernels were optimised using grid search with
5-fold cross-validation over the training set.
TCD (R, S): ?TCD M5P-resources-only? uses
only the baseline features, while ?TCD M5P-
all? uses the baseline and additional features.
A number of metrics (used as features in
TCD M5P-all) were proposed which work in
the following way: given a sentence to eval-
uate (source sentence for complexity or target
sentence for fluency), it is compared against
some reference data using similarity mea-
sures (various metrics which compare distri-
butions of n-grams). The training data was
used as reference, along with the Google n-
grams dataset. Several learning methods were
tested using Weka on the training data (10-
fold cross-validation). The system submission
uses the M5P (regression with decision trees)
algorithm which performed best. Contrary to
what had been observed on the training data
using cross-validation, ?TCD M5P-resources-
only? performs better than ?TCD M5P-all? on
the test data.
29
PRHLT-UPV (R, S): The system addresses the
task using a regression algorithm with 475 fea-
tures, including the 17 the baseline features.
Most of the features are defined as word scores.
Among them, the features obtained form a
smoothed naive Bayes classifier have shown to
be particularly interesting. Different methods
to combine word-level scores into sentence-
level features were investigated. For model
building, SVM regression was used. Given
the large number of features, the training data
provided as part of the task was insufficient
yielding unstable systems with not so good per-
formance. Different feature selection methods
were implemented to determine a subset of rel-
evant features. The final submission used these
relevant features to train an SVM system whose
parameters were optimized with respect to the
final evaluation metrics.
UEDIN (R, S): The system uses the baseline fea-
tures along with some additional features: bi-
nary features for named entities in source using
Stanford NER Tagger; binary indicators for oc-
currence of quotes or parenthetical segments,
words in upper case and numbers; geometric
mean of target word probabilities and proba-
bility of worst scoring word under a Discrim-
inative Word Lexicon Model; Sparse Neural
Network directly mapping from source to tar-
get (using the vector space model) with source
and target side either filtered to relevant words
or hashed to reduce dimensionality; number of
times at least a 3-gram is seen normalized by
sentence length; and Levenshtein distance of
either source or translation to closest entry of
the SMT training corpus on word or character
level. An ensemble of neural networks opti-
mized for RMSE was used for prediction (scor-
ing) and ranking. The contribution of new fea-
tures was tested by adding them to the baseline
features using 5-fold cross-validation. Most
features did not result in any improvement over
the baseline. The final submission was a com-
bination of all feature sets that showed im-
provement.
SJTU (R, S): The task is treated as a regression
problem using the epsilon-SVM method. All
features are extracted from the official data, in-
volving no external NLP tools/resources. Most
of them come from the phrase table, decod-
ing data and SMT training data. The focus
is on special word relations and special phrase
patterns, thus several feature templates on this
topic are extracted. Since the training data is
not large enough to assign weights to all fea-
tures, methods for estimating common strings
or sequences of words are used. The training
data is divided in 3/4 for training and 1/4 for
development to filter ineffective features. Be-
sides the baseline features, the final submission
contains 18 feature templates and about 4 mil-
lion features in total.
WLV-SHEF (R, S): The systems integrates novel
linguistic features from the source and target
texts in an attempt to overcome the limitations
of existing shallow features for quality estima-
tion. These linguistically-informed features in-
clude part-of-speech information, phrase con-
stituency, subject-verb agreement and target
lexicon analysis, which are extracted using
parsers, corpora and auxiliary resources. Sys-
tems are built using epsilon-SVM regression
with parameters optimised using 5-fold cross-
validation on the training set and two differ-
ent feature sets: ?WLV-SHEF BL? uses the 17
baseline features plus 70 linguistically inspired
features, while ?WLV-SHEF FS? uses a larger
set of 70 linguistic plus 77 shallow features (in-
cluding the baseline). Although results indicate
that the models fall slightly below the baseline,
further analysis shows that linguistic informa-
tion is indeed informative and complementary
to shallow indicators.
DFKI (R, S): ?DFKI morphPOSibm1LM? (R) is
a simple linear interpolation of POS 6-gram
language model scores, morpheme 6-gram lan-
guage model scores, IBM 1 scores (both ?di-
rect? and ?inverse?) for POS 4-grams and for
morphemes. The parallel News corpora from
WMT10 is used as extra data to train the lan-
guage model and the IBM 1 model. ?DFKI cfs-
30
plsreg? and ?DFKI grcfs-mars? (S) use a col-
lection of 264 features generated containing
the baseline features and additional resources.
Numerous methods of feature selection were
tested using 10-fold cross validation on the
training data, reducing these to 23 feature sets.
Several regression and (discretized) classifica-
tion algorithms were employed to train predic-
tion models. The best-performing models in-
cluded features derived from PCFG parsing,
language quality checking and LM scoring, of
both source and target, besides features from
the SMT search graph and a few baseline fea-
tures. ?DFKI cfs-plsreg? uses a Best First
correlation-based feature selection technique,
trained with Partial Least Squares Regression,
while ?DFKI grcfs-mars? uses a Greedy Step-
wise correlation-based feature selection tech-
nique, trained with multivariate adaptive re-
gression splines.
DCU-SYMC (R, S): Systems are based on a clas-
sification approach using a set of features that
includes the baseline features. The manually
assigned quality scores provided for each MT
output in the training set were rounded in or-
der to apply classification algorithms on a lim-
ited set of classes (integer values from 1 to 5).
Three classifiers were combined by averaging
the predicted classes: SVM using sequential
minimal optimization and RBF kernel (parame-
ters optimized by grid search), Naive Bayes and
Random Forest. ?DCU-SYMC constrained? is
based on a set of 70 features derived only from
the data provided for the task. These include
a set of features which attempt to model trans-
lation adequacy using a bilingual topic model
built using Latent Dirichlet Allocation. ?DCU-
SYMC unconstrained? is based on 308 fea-
tures including the constrained ones and oth-
ers extracted using external tools: grammatical-
ity features extracted from the source segments
using the TreeTagger part-of-speech tagger, an
English precision grammar, the XLE parser and
the Brown re-ranking parser and features based
on part-of-speech tag counts extracted from the
MT output using a Spanish TreeTagger model.
Loria (S): Several numerical or boolean features
are computed from the source and target sen-
tences and used to train an SVM regression al-
gorithm with linear (?Loria SVMlinear?) and
radial basis function (?Loria SVMrbf?) as ker-
nel. For the radial basis function, a grid search
is performed to optimise the parameter ?. The
official submission use the baseline features
and a number of features proposed in previous
work (Raybaud et al, 2011), amounting to 66
features. A feature selection algorithm is used
in order to remove non-informative features.
No additional data other than that provided for
the shared task is considered. The training data
is split into a training part (1000 sentences) and
a development part (832 sentences) to learn the
regression model and optimise the parameters
of the regression and for feature selection.
UPC (R, S): The systems use several features on
top of the baseline features. These are mostly
based on different language models estimated
on reference and automatic Spanish transla-
tions of the news-v7 corpus. The automatic
translations are generated by the system used
for the shared task. N-gram LMs are esti-
mated on word forms, POS tags, stop words
interleaved by POS tags, stop-word patterns,
plus variants in which the POS tags are re-
placed with the stem or root of each target
word. The POS tags on the target side are ob-
tained by projecting source side annotations via
automatic alignments. The resulting features
are: the perplexity of each additional language
model, according to the two translations, and
the ratio between the two perplexities. Addi-
tionally, features that estimate the likelihood
of the projection of dependency parses on the
two translations are encoded. For learning, lin-
ear SVM regression is used. Optimization was
done via 5-fold cross-validation on a develop-
ment data. Features are encoded by means of
their z-scores, i.e. how many standard devia-
tions the observed value is above or below the
mean. A variant of the system, ?UPC-2? uses
an option of SVMLight that removes inconsis-
tent points from the training set and retrains the
model until convergence.
31
6.4 Results
Here we give the official results for the ranking and
scoring subtasks followed by a discussion that high-
lights the main findings of the task.
Ranking subtask
Table 12 gives the results for the ranking sub-
task. The table is sorted from best to worse using
the DeltaAvg metric scores (Equation 15) as pri-
mary key and the Spearman correlation scores as
secondary key.
The winning submissions for the ranking subtask
are SDLLW?s M5PbestDeltaAvg and SVM entries,
which have DeltaAvg scores of 0.63 and 0.61, re-
spectively. The difference with respect to all the
other submissions is statistically significant at p =
0.05, using pairwise bootstrap resampling (Koehn,
2004). The state-of-the-art baseline system has a
DeltaAvg score of 0.55 (Spearman rank correla-
tion of 0.58). Five other submissions have perfor-
mances that are not different from the baseline at a
statistically-significant level (p = 0.05), as shown
by the gray area in the middle of Table 12. Three
submissions scored higher than the baseline system
at p = 0.05 (systems above the middle gray area),
which indicates that this shared-task succeeded in
pushing the state-of-the-art performance to new lev-
els. The range of performance for the submissions
in the ranking task varies from a DeltaAvg of 0.65
down to a DeltaAvg of 0.15 (with Spearman values
varying from 0.64 down to 0.19).
In addition to the performance of the official sub-
mission, we report here results obtained by var-
ious oracle methods. The oracle methods make
use of various metrics that are associated in a or-
acle manner to the test input: the gold-label Ef-
fort metric for ?Oracle Effort?, the HTER metric
computed against the post-edited translations as ref-
erence for ?Oracle HTER?, and the BLEU metric
computed against the same post-edited translations
as reference for ?Oracle (H)BLEU?.11 The ?Oracle
Effort? DeltaAvg score of 0.95 gives an upperbound
in terms of DeltaAvg for the test set used in this
evaluation. It basically indicates that, for this set,
11We use the (H)BLEU notation to underscore the use of
Post-Edited translations as reference, as opposed to using ref-
erences that are not the product of a Post-Editing process, as for
the traditional BLEU metric.
the difference in PE effort between the top-quality
quantiles and the overall quality is 0.95 on average.
We would like to emphasize here that the DeltaAvg
metric does not have any a-priori range for its values.
The upperbound, for instance, is test-dependent, and
therefore an ?Oracle Effort? score is useful for un-
derstanding the performance level of real system-
submissions. The ?Oracle HTER? DeltaAvg score
of 0.77 is a more realistic upperbound for the cur-
rent set. Since the HTER metric is considered a
good approximation for the effort required in post-
editing, ranking the test set based on the HTER
scores (from lowest HTER to highest HTER) pro-
vides a good oracle comparison point. The oracle
based on (H)BLEU gives a lower DeltaAvg score,
which can be interpreted to mean that the BLEU
metric provides a lower correlation to post-editing
effort compared to HTER. We also note here that
there is room for improvement between the highest-
scoring submission (at DeltaAvg 0.63) and the ?Ora-
cle HTER? DeltaAvg score of 0.77. We are not sure
if this difference can be bridged completely, but hav-
ing measured a quantitative difference between the
current best-performance and a realistic upperbound
is an important achievement of this shared-task.
Scoring subtask
The results for the scoring task are presented in
Table 13, sorted from best to worse by using the
MAE metric scores (Equation 16) as primary key
and the RMSE metric scores (Equation 17) as sec-
ondary key.
The winning submission is SDLLW?s
M5PbestDeltaAvg, with an MAE of 0.61 and
an RMSE of 0.75 (the difference with respect to
all the other submissions is statistically significant
at p = 0.05, using pairwise bootstrap resam-
pling (Koehn, 2004)). The strong, state-of-the-art
quality-estimation baseline system is measured to
have an MAE of 0.69 and RMSE of 0.82, with six
other submissions having performances that are
not different from the baseline at a statistically-
significant level (p = 0.05), as shown by the gray
area in the middle of Table 13). Five submissions
scored higher than the baseline system at p = 0.05
(systems above the middle gray area), which
indicates that this shared-task also succeeded in
pushing the state-of-the-art performance to new
32
System ID DeltaAvg Spearman Corr
? SDLLW M5PbestDeltaAvg 0.63 0.64
? SDLLW SVM 0.61 0.60
UU bltk 0.58 0.61
UU best 0.56 0.62
TCD M5P-resources-only* 0.56 0.56
Baseline (17FFs SVM) 0.55 0.58
PRHLT-UPV 0.55 0.55
UEdin 0.54 0.58
SJTU 0.53 0.53
WLV-SHEF FS 0.51 0.52
WLV-SHEF BL 0.50 0.49
DFKI morphPOSibm1LM 0.46 0.46
DCU-SYMC unconstrained 0.44 0.41
DCU-SYMC constrained 0.43 0.41
TCD M5P-all* 0.42 0.41
UPC 1 0.22 0.26
UPC 2 0.15 0.19
Oracle Effort 0.95 1.00
Oracle HTER 0.77 0.70
Oracle (H)BLEU 0.71 0.62
Table 12: Official results for the ranking subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sions are indicated by a ? (the difference with respect to other systems is statistically significant with p = 0.05). The
systems in the gray area are not significantly different from the baseline system. Entries with * represent submissions
for which a bug-fix was applied after the submission deadline.
33
System ID MAE RMSE
? SDLLW M5PbestDeltaAvg 0.61 0.75
UU best 0.64 0.79
SDLLW SVM 0.64 0.78
UU bltk 0.64 0.79
Loria SVMlinear 0.68 0.82
UEdin 0.68 0.82
TCD M5P-resources-only* 0.68 0.82
Baseline (17FFs SVM) 0.69 0.82
Loria SVMrbf 0.69 0.83
SJTU 0.69 0.83
WLV-SHEF FS 0.69 0.85
PRHLT-UPV 0.70 0.85
WLV-SHEF BL 0.72 0.86
DCU-SYMC unconstrained 0.75 0.97
DFKI grcfs-mars 0.82 0.98
DFKI cfs-plsreg 0.82 0.99
UPC 1 0.84 1.01
DCU-SYMC constrained 0.86 1.12
UPC 2 0.87 1.04
TCD M5P-all 2.09 2.32
Oracle Effort 0.00 0.00
Oracle HTER (linear mapping into [1.5-5.0]) 0.56 0.73
Oracle (H)BLEU (linear mapping into [1.5-5.0]) 0.61 0.84
Table 13: Official results for the scoring subtask of the WMT12 Quality Evaluation shared-task. The winning submis-
sion is indicated by a ? (the difference with respect to the other submissions is statistically significant at p = 0.05).
The systems in the gray area are not different from the baseline system at a statistically significant level (p = 0.05).
Entries with * represent submissions for which a bug-fix was applied after the submission deadline.
34
levels in terms of absolute scoring. The range of
performance for the submissions in the scoring task
varies from an MAE of 0.61 up to an MAE of 0.87
(the outlier MAE of 2.09 is reportedly due to bugs).
We also calculate scoring Oracles using the meth-
ods used for the ranking Oracles. The difference is
that the HTER and (H)BLEU oracles need a way
of mapping their scores (which are usually in the
[0, 100] range) into the [1, 5] range. For the compar-
ison here, we did the mapping by excluding the 5%
top and bottom outlier scores, and then linearly map-
ping the remaining range into the [1.5, 5] range. The
?Oracle Effort? scores are not very indicative in this
case. However, the ?Oracle HTER? MAE score of
0.56 is a somewhat realistic lowerbound for the cur-
rent set (although the score could be decreased by a
smarter mapping from the HTER range to the Effort
range). We argue that since the HTER metric is con-
sidered a good approximation for the effort required
in post-editing, effort-like scores derived from the
HTER score provide a good way to compute oracle
scores in a deterministic manner. Note that again
the oracle based on (H)BLEU gives a worse MAE
score at 0.61, which support the interpretation that
the (H)BLEU metric provides a lower correlation
to post-editing effort compared to (H)TER. Over-
all, we consider the MAE values for these HTER
and (H)BLEU-based oracles to indicate high error
margins. Most notably the performance of the best
system gets the same MAE score as the (H)BLEU
oracle, at 0.61 MAE. We take this to mean that the
scoring task is more difficult compared to the rank-
ing task, since even oracle-based solutions get high
error scores.
6.5 Discussion
When looking back at the goals that we identified for
this shared-task, most of them have been success-
fully accomplished. In addition, we have achieved
additional ones that were not explicitly stated from
the beginning. In this section, we discuss the accom-
plishments of this shared-task in more detail, start-
ing from the defined goals and beyond.
Identify new and effective quality indicators
The vast majority of the participating systems use
external resources in addition to those provided for
the task, such as parsers, part-of-speech taggers,
named entity recognizers, etc. This has resulted in
a wide variety of features being used. Many of the
novel features have tried to exploit linguistically-
oriented features. While some systems did not
achieve improvements over the baseline while ex-
ploiting such features, others have (the ?UU? sub-
missions, for instance, exploiting both constituency
and dependency trees).
Another significant set of features that has been
previously overlooked is the feature set of the MT
decoder. Considering statistical engines, these fea-
tures are immediately available for quality predic-
tion from the internal trace of the MT decoder (in
a glass-box prediction scenario), and its contribu-
tion is significant. These features, which reflect the
?confidence? of the SMT system on the translations
it produces, have been shown to be complemen-
tary to other, system-independent (black-box) fea-
tures. For example, the ?SDLLW? submissions in-
corporate these features, and their feature selection
strategy consistently favored this feature set. The
power of this set of features alone is enough to yield
(when used with an M5P model) outputs that would
have been placed 4th in the ranking task and 5th
in the scoring task, a remarkable achievement. An-
other interesting feature used by the ?SDLLW? sub-
missions rely on pseudo-references, i.e., translations
produced by other MT systems for the same input
sentence.
Identify alternative machine learning techniques
Although SVM regression was used to compute the
baseline performance, the baseline ?system? pro-
vided for the task consisted solely of a software to
extract features, as opposed to a model built us-
ing the regression algorithm. The rationale behind
this decision was to encourage participants to exper-
iment with alternative methods for combining differ-
ent quality indicators. This was achieved to a large
extent.
The best-performing machine learning techniques
were found to be the M5P Regression Trees and the
SVM Regression (SVR) models. The merit of the
M5P Regression Trees is that it provides compact
models that are less prone to overfitting. In contrast,
the SVR models can easily overfit given the small
amount of training data available and the large num-
bers of features commonly used. Indeed, many of
35
the submissions that fell below the baseline perfor-
mance can blame overfitting for (part of) their sub-
optimal performance. However, SVR models can
achieve high performance through the use of tun-
ing and feature selection techniques to avoid overfit-
ting. Structured learning techniques were success-
fully used by the ?UU? submissions ? the second
best performing team ? to represent parse trees. This
seems an interesting direction to encode other sorts
of linguistic information about source and trans-
lation texts. Other interesting learning techniques
have been tried, such as Neural Networks, Par-
tial Least Squares Regression, or multivariate adap-
tive regression splines, but their performance does
not suggest they are strong candidates for learning
highly-performing quality-estimation models.
Test the suitability of evaluation metrics for qual-
ity estimation DeltaAvg, our proposed metric for
measuring ranking performance, proved suitable for
scoring the ranking subtask. Its high correlation with
the Spearman ranking metric, coupled with its ex-
trinsic interpretability, makes it a preferred choice
for future measurements. It is also versatile, in the
sense that the its valuation function V can change to
reflect different extrinsic measures of quality.
Establish the state of the art performance The
results on both the ranking and the scoring subtasks
established new state of the art levels on the test set
used in this shared task. In addition to these lev-
els, the oracle performance numbers also help under-
stand the current performance level, and how much
of a gap in performance there still exists. Addi-
tional data points regarding quality estimation per-
formance are needed to establish how stable this
measure of the performance gap is.
Contrast the performance of regression and
ranking techniques Most of the submissions in
the ranking task used the results provided by a re-
gression solution (submitted for the scoring task) to
infer the rankings. Also, optimizing for ranking per-
formance via a regression solution seems to result in
regression models that perform very well, as in the
case of the top-ranked submission.
6.6 Quality Estimation Conclusions
There appear to be significant differences between
considering the quality estimation task as a ranking
problem versus a scoring problem. The ranking-
based approach appears to be somewhat simpler
and more easily amenable to automatic solutions,
and at the same time provides immediate benefits
when integrated into larger applications (see, for in-
stance, the post-editing application described in Spe-
cia (2011)). The scoring-based approach is more dif-
ficult, as the high error rate even of oracle-based so-
lutions indicates. It is also well-known from human
evaluations of MT outputs that human judges also
have a difficult time agreeing on absolute-number
judgements to translations.
Our experience in creating the current datasets
confirms that, even with highly-trained profession-
als, it is difficult to arrive at consistent judge-
ments. We plan to have future investigations on
how to achieve more consistent ways of generating
absolute-number scores that reflect the quality of au-
tomated translations.
7 Summary
As in previous incarnations of this workshop we car-
ried out an extensive manual and automatic evalu-
ation of machine translation performance, and we
used the human judgements that we collected to val-
idate automatic metrics of translation quality. This
year was also the debut of a new quality estimation
task, which tries to predict the effort involved in hav-
ing post editors correct MT output. The quality es-
timation task differs from the metrics task in that it
does not involve reference translations.
As in previous years, all data sets generated by
this workshop, including the human judgments, sys-
tem translations and automatic scores, are publicly
available for other researchers to analyze.12
Acknowledgments
This work was supported in parts by the Euro-
MatrixPlus project funded by the European Com-
mission (7th Framework Programme), the GALE
program of the US Defense Advanced Research
Projects Agency, Contract No. HR0011-06-C-0022,
12http://statmt.org/wmt12/results.html
36
the US National Science Foundation under grant
IIS-0713448, and the CoSyne project FP7-ICT-4-
248531 funded by the European Commission. The
views and findings are the authors? alone. Thanks
for Adam Lopez for discussions about alternative
ways of ranking the overall system scores. The
Quality Estimation shared task organizers thank
Wilker Aziz for his help with the SMT models and
resources, and Mariano Felice for his help with the
system for the extraction of baseline features.
References
Eleftherios Avramidis. 2012. Quality estimation for
machine translation output using linguistic analysis
and decoding features. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ondr?ej Bojar, Milos? Ercegovc?evic?, Martin Popel, and
Omar Zaidan. 2011. A grain of salt for the wmt man-
ual evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 1?11, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Ondrej Bojar, Bushra Jawaid, and Amir Kamran. 2012.
Probes in a taxonomy of factored phrase-based mod-
els. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Christian Buck. 2012. Black box features for the WMT
2012 quality estimation shared task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (Meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion (WMT07), Prague, Czech Republic.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation (WMT08), Colmbus, Ohio.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009 work-
shop on statistical machine translation. In Proceedings
of the Fourth Workshop on Statistical Machine Trans-
lation (WMT09), Athens, Greece.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar F. Zaidan.
2010. Findings of the 2010 joint workshop on statisti-
cal machine translation and metrics for machine trans-
lation. In Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT10), Uppsala, Swe-
den.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011 work-
shop on statistical machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion, pages 22?64, Edinburgh, Scotland, July.
Chris Callison-Burch. 2009. Fast, cheap, and creative:
Evaluating translation quality using Amazon?s Me-
chanical Turk. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Process-
ing (EMNLP-2009), Singapore.
Julio Castillo and Paula Estrella. 2012. Semantic tex-
tual similarity for MT evaluation. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1?27:27. Software available at http://www.
csie.ntu.edu.tw/?cjlin/libsvm.
Boxing Chen, Roland Kuhn, and George Foster. 2012.
Improving amber, an MT evaluation metric. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Measur-
ment, 20(1):37?46.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3:
Automatic metric for reliable optimization and evalu-
ation of machine translation systems. In Proceedings
of the Sixth Workshop on Statistical Machine Transla-
tion.
Michael Denkowski, Greg Hanneman, and Alon Lavie.
2012. The CMU-avenue French-English translation
system. In Proceedings of the Seventh Workshop on
Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Ondr?ej Dus?ek, Zdene?k Z?abokrtsky?, Martin Popel, Mar-
tin Majlis?, Michal Nova?k, and David Marec?ek. 2012.
Formemes in English-Czech deep syntactic mt. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mariano Felice and Lucia Specia. 2012. Linguistic fea-
tures for quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
37
Mark Fishel, Rico Sennrich, Maja Popovic?, and Ondr?ej
Bojar. 2012. TerrorCat: a translation error
categorization-based MT quality metric. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Lluis Formiga, Carlos A. Henr??quez Q., Adolfo
Herna?ndez, Jose? B. Marin?o, Enric Monte, and Jose?
A. R. Fonollosa. 2012. The TALP-UPC phrase-based
translation systems for WMT12: Morphology simpli-
fication and domain adaptation. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post,
and Chris Callison-Burch. 2012. Joshua 4.0: Packing,
PRO, and paraphrases. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Ulrich Germann. 2012. Syntax-aware phrase-based sta-
tistical machine translation: System description. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Jesu?s Gonza?lez-Rubio, Alberto Sanch??s, and Francisco
Casacuberta. 2012. PRHLT submission to the
WMT12 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
Francisco Guzman, Preslav Nakov, Ahmed Thabet, and
Stephan Vogel. 2012. QCRI at WMT12: Exper-
iments in Spanish-English and German-English ma-
chine translation of news text. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Christian Hardmeier, Joakim Nivre, and Jo?rg Tiedemann.
2012. Tree kernels for machine translation quality
estimation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Christian Hardmeier. 2011. Improving machine transla-
tion quality prediction with syntactic tree kernels. In
Proceedings of the 15th conference of the European
Association for Machine Translation, pages 233?240,
Leuven, Belgium.
Matthias Huck, Stephan Peitz, Markus Freitag, Malte
Nuhn, and Hermann Ney. 2012. The RWTH aachen
machine translation system for WMT 2012. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Sabine Hunsicker, Chen Yu, and Christian Federmann.
2012. Machine learning for hybrid machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Barry Haddow. 2012. Towards effec-
tive use of training data in statistical machine transla-
tion. In Proceedings of the Seventh Workshop on Sta-
tistical Machine Translation, Montreal, Canada, June.
Association for Computational Linguistics.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
European languages. In Proceedings of NAACL 2006
Workshop on Statistical Machine Translation, New
York, New York.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the Empirical Methods in Natural Language Process-
ing Conference.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33:159?174.
David Langlois, Sylvain Raybaud, and Kamel Sma??li.
2012. LORIA system for the WMT12 quality esti-
mation shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Allauzen,
Marianna Apidianaki, Li Gong, Aure?lien Max, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois Yvon.
2012. LIMSI @ WMT12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Vero?nica Lo?pez-Luden?a, Rube?n San-Segundo, and
Juan M. Montero. 2012. UPM system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Adam Lopez. 2012. Putting human assessments of ma-
chine translation systems in order. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Matous? Macha?c?ek and Ondej Bojar. 2011. Approxi-
mating a deep-syntactic metric for mt evaluation and
tuning. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation, pages 373?379, Edin-
burgh, Scotland, July. Association for Computational
Linguistics.
Freitag Markus, Peitz Stephan, Huck Matthias, Ney Her-
mann, Niehues Jan, Herrmann Teresa, Waibel Alex,
38
Hai-son Le, Lavergne Thomas, Allauzen Alexandre,
Buschbeck Bianka, Crego Joseph Maria, and Senel-
lart Jean. 2012. Joint WMT 2012 submission of
the QUAERO project. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Alexander Molchanov. 2012. PROMT deephybrid sys-
tem for WMT12 shared translation task. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Erwan Moreau and Carl Vogel. 2012. Quality estima-
tion: an experimental study using unsupervised simi-
larity measures. In Proceedings of the Seventh Work-
shop on Statistical Machine Translation, Montreal,
Canada, June. Association for Computational Linguis-
tics.
Jan Niehues, Yuqi Zhang, Mohammed Mediani, Teresa
Herrmann, Eunah Cho, and Alex Waibel. 2012. The
karlsruhe institute of technology translation systems
for the WMT 2012. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
Daniele Pighin, Meritxell Gonza?lez, and Llu??s Ma`rquez.
2012. The upc submission to the WMT 2012 shared
task on quality estimation. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Maja Popovic. 2012. Class error rates for evaluation
of machine translation output. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Sylvain Raybaud, David Langlois, and Kamel Sma??li.
2011. ?This sentence is wrong.? Detecting errors in
machine-translated sentences. Machine Translation,
25(1):1?34.
Majid Razmara, Baskaran Sankaran, Ann Clifton, and
Anoop Sarkar. 2012. Kriya - the SFU system for
translation task at WMT-12. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Rudolf Rosa, David Marec?ek, and Ondr?ej Dus?ek. 2012.
DEPFIX: A system for automatic correction of czech
MT outputs. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Raphael Rubino, Jennifer Foster, Joachim Wagner, Jo-
hann Roturier, Rasul Samad Zadeh Kaljahi, and Fred
Hollowood. 2012. DCU-Symantec submission for the
WMT 2012 quality estimation task. In Proceedings of
the Seventh Workshop on Statistical Machine Transla-
tion, Montreal, Canada, June. Association for Compu-
tational Linguistics.
Christophe Servan, Patrik Lambert, Anthony Rousseau,
Holger Schwenk, and Lo??c Barrault. 2012. LIUM?s
smt machine translation systems for WMT 2012. In
Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
ranking based optimisation for sentence level MT eval-
uation. In Proceedings of the Sixth Workshop on Sta-
tistical Machine Translation.
Radu Soricut, Nguyen Bach, and Ziyuan Wang. 2012.
The SDL Language Weaver systems in the WMT12
quality estimation shared task. In Proceedings of the
Seventh Workshop on Statistical Machine Translation,
Montreal, Canada, June. Association for Computa-
tional Linguistics.
Lucia Specia, Marco Turchi, Nicola Cancedda, Marc
Dymetman, and Nello Cristianini. 2009. Estimat-
ing the Sentence-Level Quality of Machine Transla-
tion Systems. In Proceedings of the 13th Conference
of the European Association for Machine Translation,
pages 28?37, Barcelona.
Lucia Specia. 2011. Exploiting Objective Annotations
for Measuring Translation Post-editing Effort. In Pro-
ceedings of the 15th Conference of the European As-
sociation for Machine Translation, pages 73?80, Leu-
ven.
Ales? Tamchyna, Petra Galus?c?a?kova?, Amir Kamran, Milos?
Stanojevic?, and Ondr?ej Bojar. 2012. Selecting data for
English-to-Czech machine translation. In Proceedings
of the Seventh Workshop on Statistical Machine Trans-
lation, Montreal, Canada, June. Association for Com-
putational Linguistics.
David Vilar. 2012. DFKI?s smt system for WMT 2012.
In Proceedings of the Seventh Workshop on Statistical
Machine Translation, Montreal, Canada, June. Asso-
ciation for Computational Linguistics.
Mengqiu Wang and Christopher Manning. 2012.
SPEDE: Probabilistic edit distance metrics for MT
evaluation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, Montreal, Canada,
June. Association for Computational Linguistics.
Eric Wehrli, Luka Nerima, and Yves Scherrer. 2009.
Deep linguistic multilingual translation and bilingual
dictionaries. In Proceedings of the Fourth Workshop
on Statistical Machine Translation, pages 90?94.
Philip Williams and Philipp Koehn. 2012. GHKM rule
extraction and scope-3 parsing in Moses. In Proceed-
ings of the Seventh Workshop on Statistical Machine
39
Translation, Montreal, Canada, June. Association for
Computational Linguistics.
Chunyang Wu and Hai Zhao. 2012. Regression with
phrase indicators for estimating MT quality. In Pro-
ceedings of the Seventh Workshop on Statistical Ma-
chine Translation, Montreal, Canada, June. Associa-
tion for Computational Linguistics.
Daniel Zeman. 2012. Data issues of the multilingual
translation matrix. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, Mon-
treal, Canada, June. Association for Computational
Linguistics.
40
C
U
-B
O
JA
R
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
U
E
D
IN
U
K
CU-BOJAR ? .29? .43 .53? .47? .31?
JHU .59? ? .59? .67? .65? .44?
ONLINE-A .44 .28? ? .52? .46? .32?
ONLINE-B .36? .23? .34? ? .38? .25?
UEDIN .36? .23? .36? .48? ? .27?
UK .56? .33? .56? .63? .60? ?
> others 0.53 0.32 0.53 0.65 0.60 0.37
Table 14: Head to head comparison for Czech-English systems
A Pairwise System Comparisons by Human Judges
Tables 14?21 show pairwise comparisons between systems for each language pair. The numbers in each of
the tables? cells indicate the percentage of times that the system in that column was judged to be better than
the system in that row. Bolding indicates the winner of the two systems. The difference between 100 and
the sum of the complementary cells is the percent of time that the two systems were judged to be equal.
Because there were so many systems and data conditions the significance of each pairwise comparison
needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine differences
(rather than differences that are attributable to chance). In the following tables ? indicates statistical signif-
icance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical significance at
p ? 0.01, according to the Sign Test.
Each table contains a final row showing how often a system was ranked to be > than the others. As
suggested by Bojar et al (2011) present, this is calculated ignoring ties as:
score(s) =
win(s)
win(s) + loss(s)
(18)
B Automatic Scores
Tables 29?36 give the automatic scores for each of the systems.
41
C
O
M
M
E
R
C
IA
L
2
C
U
-B
O
JA
R
C
U
-D
E
P
F
IX
C
U
-P
O
O
R
-C
O
M
B
C
U
-T
A
M
C
H
C
U
-T
E
C
T
O
M
T
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
C
O
M
M
E
R
C
IA
L
1
S
F
U
U
E
D
IN
U
K
COMMERCIAL2 ? .48? .56? .43 .49? .50? .32? .49? .54? .36 .38? .50? .42
CU-BOJAR .33? ? .49? .29? .26 .39 .26? .40 .51? .37? .27? .43 .33?
CU-DEPFIX .28? .36? ? .26? .30? .32? .18? .31? .13? .33? .21? .31? .25?
CU-POOR-COMB .42 .40? .59? ? .41? .51? .34? .49? .57? .45 .33? .47? .42
CU-TAMCH .38? .24 .51? .27? ? .39 .22? .42 .47? .38? .28? .39 .28?
CU-TECTOMT .32? .42 .49? .33? .47 ? .24? .42 .46? .36? .33? .46 .40
JHU .54? .59? .69? .50? .62? .60? ? .59? .61? .52? .44 .62? .48?
ONLINE-A .36? .41 .51? .36? .43 .43 .24? ? .51? .40 .26? .45 .32?
ONLINE-B .32? .34? .24? .28? .35? .35? .22? .33? ? .31? .23? .33? .22?
COMMERCIAL1 .41 .48? .55? .41 .50? .49? .36? .46 .54? ? .30? .48 .41
SFU .47? .56? .64? .47? .55? .52? .36 .53? .64? .56? ? .58? .48?
UEDIN .36? .36 .50? .29? .38 .43 .24? .37 .48? .40 .25? ? .30?
UK .43 .47? .59? .43 .52? .44 .26? .50? .59? .47 .35? .52? ?
> others 0.46 0.54 0.66 0.44 0.56 0.53 0.32 0.53 0.63 0.48 0.36 0.56 0.44
Table 15: Head to head comparison for English-Czech systems
IT
S
-L
A
T
L
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
R
W
T
H
U
E
D
IN
U
K
ITS-LATL ? .49? .54? .55? .53? .59? .58? .38 .47? .32 .45? .47? .62? .53? .48
JHU .35? ? .47? .55? .42 .45 .55? .36 .49? .37 .46 .46? .47? .46? .29
KIT .25? .25? ? .37 .29? .28? .39 .27? .35 .30? .32? .33 .36 .24? .22?
LIMSI .23? .23? .34 ? .26 .21? .29? .25? .29? .19? .19? .32? .22? .29 .16?
LIUM .25? .36 .42? .34 ? .27? .46? .21? .40 .25? .37 .34 .35 .29 .30?
ONLINE-A .22? .33 .40? .45? .42? ? .44? .26? .43 .33? .38 .33 .47? .35 .30?
ONLINE-B .20? .22? .33 .43? .32? .29? ? .27? .36 .26? .33 .34 .39 .29? .24?
RBMT-4 .37 .47 .56? .60? .60? .55? .52? ? .41 .36 .39 .40 .58? .51? .42
RBMT-3 .30? .35? .43 .45? .40 .39 .37 .34 ? .27? .29 .23 .55? .42 .34?
ONLINE-C .36 .46 .46? .55? .49? .50? .58? .38 .48? ? .45? .43 .62? .45 .39
RBMT-1 .28? .36 .49? .58? .40 .42 .44 .35 .38 .31? ? .41 .45 .37 .30?
PROMT .20? .34? .41 .50? .46 .40 .40 .34 .22 .33 .32 ? .48? .41 .27?
RWTH .22? .28? .34 .37? .31 .28? .32 .27? .26? .22? .34 .31? ? .29 .17?
UEDIN .28? .29? .40? .39 .34 .35 .42? .31? .39 .34 .36 .34 .34 ? .27?
UK .37 .36 .53? .53? .44? .43? .48? .38 .52? .39 .44? .46? .52? .46? ?
> others 0.36 0.44 0.59 0.66 0.55 0.51 0.6 0.39 0.52 0.39 0.48 0.51 0.62 0.53 0.4
Table 16: Head to head comparison for English-French systems
42
D
F
K
I-
B
E
R
L
IN
D
F
K
I-
H
U
N
S
IC
K
E
R
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
U
E
D
IN
-W
IL
L
IA
M
S
U
E
D
IN
U
K
DFKI-BERLIN ? .62? .58? .64? .71? .68? .80? .68? .71? .58? .65? .62? .64? .61? .60?
DFKI-HUNSICKER .28? ? .42 .48 .51? .47 .52? .49? .57? .38 .53? .39 .39 .41 .39
JHU .24? .45 ? .43? .43 .47? .62? .56? .60? .46 .47? .46? .47? .39 .42
KIT .22? .41 .27? ? .39 .45 .60? .54? .58? .37 .47 .33 .43 .39 .26?
LIMSI .15? .37? .34 .36 ? .47 .49? .43 .43 .35 .48 .36 .37 .32 .31?
ONLINE-A .20? .37 .35? .41 .39 ? .45? .42 .51? .38 .49 .42 .40 .37 .36?
ONLINE-B .15? .35? .26? .27? .35? .30? ? .45 .35? .29? .41 .30? .34? .30? .18?
RBMT-4 .25? .22? .31? .31? .45 .45 .42 ? .41 .38 .40 .44 .35? .36? .36?
RBMT-3 .18? .27? .24? .28? .38 .36? .49? .41 ? .33? .26? .29? .28? .31? .34?
ONLINE-C .27? .47 .35 .49 .46 .44 .63? .48 .55? ? .49? .40 .43 .43 .46
RBMT-1 .19? .30? .33? .41 .41 .39 .45 .45 .50? .32? ? .34? .40 .39 .39
RWTH .20? .43 .30? .45 .45 .44 .58? .50 .58? .43 .53? ? .41 .40 .41
UEDIN-WILLIAMS .20? .46 .30? .36 .36 .45 .54? .52? .54? .41 .46 .38 ? .32 .30?
UEDIN .20? .45 .40 .38 .43 .48 .56? .56? .53? .47 .48 .29 .39 ? .35
UK .25? .49 .40 .45? .51? .49? .64? .51? .52? .44 .47 .34 .48? .40 ?
> others 0.25 0.48 0.43 0.50 0.55 0.54 0.64 0.58 0.63 0.47 0.56 0.47 0.51 0.47 0.45
Table 17: Head to head comparison for English-German systems
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
P
R
O
M
T
U
E
D
IN
U
K
U
P
C
JHU ? .52? .59? .50? .58? .48? .49? .56? .48? .44? .52?
ONLINE-A .27? ? .45 .34? .44 .31? .31? .44 .37 .28? .37
ONLINE-B .21? .37 ? .28? .35? .25? .28? .31? .30? .23? .31?
RBMT-4 .35? .52? .56? ? .49? .39 .40 .46? .45 .38? .45
RBMT-3 .26? .39 .46? .34? ? .32? .28? .24 .34? .32? .37
ONLINE-C .33? .54? .61? .40 .47? ? .43 .50? .50? .42 .48
RBMT-1 .39? .51? .61? .39 .49? .34 ? .47? .50? .39 .46
PROMT .28? .41 .51? .33? .29 .33? .34? ? .42 .32? .40
UEDIN .25? .41 .48? .38 .47? .30? .35? .43 ? .28? .39
UK .31? .52? .57? .48? .53? .42 .44 .52? .42? ? .50?
UPC .24? .40 .53? .40 .43 .39 .39 .46 .36 .28? ?
> others 0.36 0.56 0.65 0.46 0.58 0.43 0.45 0.55 0.52 0.41 0.52
Table 18: Head to head comparison for English-Spanish systems
43
C
M
U
JH
U
K
IT
L
IM
S
I
L
IU
M
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
R
W
T
H
S
F
U
U
E
D
IN
U
K
CMU ? .34? .32 .46 .35 .41 .39 .30? .36 .29? .35? .32 .28? .45 .33?
JHU .50? ? .63? .55? .53? .63? .57? .43 .42 .31? .46 .52? .43 .53? .43
KIT .40 .21? ? .36 .30 .35 .44 .33? .33? .23? .31? .25? .28? .23? .30?
LIMSI .35 .26? .37 ? .31? .35 .40 .29? .32? .23? .33? .29? .28? .29 .23?
LIUM .47 .25? .43 .53? ? .44 .42 .36 .43 .28? .38 .38 .32? .40 .42
ONLINE-A .45 .22? .41 .47 .40 ? .41 .30? .25? .28? .23? .40 .27? .40 .25?
ONLINE-B .45 .32? .38 .42 .41 .39 ? .34? .39 .30? .33? .30? .34? .44 .32?
RBMT-4 .56? .40 .54? .61? .48 .54? .54? ? .43 .31? .48? .45 .42 .52? .46
RBMT-3 .50 .46 .53? .53? .46 .54? .47 .33 ? .28? .40 .53? .52 .50 .48
ONLINE-C .59? .57? .72? .66? .59? .60? .61? .45? .54? ? .58? .65? .53? .66? .58?
RBMT-1 .54? .43 .58? .54? .48 .62? .55? .31? .44 .20? ? .47 .41 .56? .38
RWTH .39 .35? .50? .52? .43 .50 .55? .42 .37? .23? .40 ? .34? .36 .29?
SFU .57? .38 .55? .54? .48? .55? .51? .42 .38 .35? .45 .50? ? .41 .46
UEDIN .37 .32? .42? .42 .40 .43 .40 .34? .40 .24? .36? .39 .41 ? .29?
UK .50? .40 .48? .59? .44 .58? .50? .42 .41 .35? .49 .53? .35 .51? ?
> others 0.57 0.41 0.61 0.63 0.52 0.59 0.57 0.43 0.46 0.32 0.46 0.52 0.44 0.55 0.44
Table 19: Head to head comparison for French-English systems
D
F
K
I-
B
E
R
L
IN
JH
U
K
IT
L
IM
S
I
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
Q
U
A
E
R
O
R
W
T
H
U
E
D
IN
U
G
U
K
DFKI-BERLIN ? .38 .49 .52? .57? .65? .55? .62? .50 .49 .51? .66? .53? .61? .17? .37
JHU .45 ? .60? .66? .66? .69? .57? .60? .52 .62? .58? .67? .59? .62? .21? .37
KIT .36 .16? ? .47 .60? .50 .41 .50 .31? .39 .32 .36 .32 .39 .15? .26?
LIMSI .30? .14? .35 ? .49? .57? .49 .54 .34? .33? .43 .31 .44 .49? .14? .30?
ONLINE-A .32? .20? .22? .32? ? .39 .30? .44 .20? .30? .37 .35? .32? .31? .16? .29?
ONLINE-B .25? .21? .38 .29? .38 ? .27? .39 .31? .37 .30? .43 .34 .33? .12? .24?
RBMT-4 .33? .33? .49 .44 .57? .63? ? .46 .26? .40 .53? .51? .56? .48 .21? .32?
RBMT-3 .26? .30? .39 .40 .45 .45 .32 ? .35 .36 .34? .48 .33? .41 .13? .23?
ONLINE-C .36 .37 .58? .54? .70? .62? .57? .50 ? .53? .48 .57? .55? .58? .14? .45
RBMT-1 .41 .32? .48 .55? .64? .52 .42 .47 .34? ? .51 .49 .48 .45 .15? .25?
QCRI .31? .26? .43 .37 .48 .51? .36? .52? .43 .38 ? .48? .48? .45? .14? .23?
QUAERO .18? .19? .29 .33 .51? .43 .33? .42 .31? .37 .23? ? .34 .48? .09? .16?
RWTH .29? .25? .38 .34 .51? .48 .37? .58? .38? .40 .29? .39 ? .44 .20? .24?
UEDIN .24? .20? .38 .30? .55? .52? .42 .44 .35? .37 .29? .32? .38 ? .08? .22?
UG .68? .61? .72? .76? .76? .82? .72? .80? .70? .76? .73? .76? .73? .84? ? .57?
UK .43 .37 .48? .48? .54? .62? .57? .64? .44 .59? .49? .58? .51? .56? .20? ?
> others 0.40 0.34 0.55 0.54 0.65 0.65 0.50 0.60 0.43 0.51 0.52 0.61 0.56 0.6 0.17 0.37
Table 20: Head to head comparison for German-English systems
44
G
T
H
-U
P
M
JH
U
O
N
L
IN
E
-A
O
N
L
IN
E
-B
R
B
M
T
-4
R
B
M
T
-3
O
N
L
IN
E
-C
R
B
M
T
-1
Q
C
R
I
U
E
D
IN
U
K
U
P
C
GTH-UPM ? .41 .50? .52? .38 .46 .32? .35? .44? .46 .17? .41
JHU .37 ? .54? .56? .44 .48 .39 .39 .47? .50? .15? .47?
ONLINE-A .34? .31? ? .43 .28? .38? .29? .29? .40 .39 .16? .41
ONLINE-B .36? .30? .44 ? .34? .38 .30? .32? .37? .38 .18? .41
RBMT-4 .50 .45 .61? .57? ? .46 .41 .40 .53? .57? .21? .56?
RBMT-3 .42 .40 .53? .51 .36 ? .36? .31? .60? .54? .14? .54?
ONLINE-C .54? .48 .58? .62? .49 .50? ? .40 .58? .59? .23? .55?
RBMT-1 .56? .50 .59? .57? .40 .53? .41 ? .57? .59? .23? .58?
QCRI .28? .31? .45 .50? .38? .32? .29? .34? ? .31 .12? .33?
UEDIN .39 .27? .49 .49 .33? .38? .31? .31? .34 ? .15? .38
UK .74? .71? .81? .76? .73? .76? .69? .66? .76? .75? ? .77?
UPC .42 .32? .49 .49 .38? .36? .33? .35? .44? .36 .14? ?
> others 0.52 0.48 0.62 0.61 0.46 0.51 0.42 0.42 0.60 0.58 0.19 0.57
Table 21: Head to head comparison for Spanish-English systems
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.643: ONLINE-B ONLINE-B ONLINE-B 2.88: ONLINE-B 0.642 (1): ONLINE-B
2 0.606: UEDIN UEDIN UEDIN 3.07: UEDIN 0.603 (2): UEDIN
3 0.530: ONLINE-A CU-BOJAR CU-BOJAR 3.40: CU-BOJAR 0.528 (3-4): ONLINE-A
4 0.530: CU-BOJAR ONLINE-A ONLINE-A 3.40: ONLINE-A 0.528 (3-4): CU-BOJAR
5 0.375: UK UK UK 4.01: UK 0.379 (5): UK
6 0.318: JHU JHU JHU 4.24: JHU 0.320 (6): JHU
Table 22: Overall ranking with different methods (Czech?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.646: ONLINE-A ONLINE-B ONLINE-B 6.35: ONLINE-A 0.647 (1-3): ONLINE-A
2 0.645: ONLINE-B ONLINE-A ONLINE-A 6.44: ONLINE-B 0.642 (1-3): ONLINE-B
3 0.612: QUAERO UEDIN UEDIN 6.94: QUAERO 0.609 (2-5): QUAERO
4 0.599: RBMT-3 QUAERO QUAERO 7.04: RBMT-3 0.600 (2-6): RBMT-3
5 0.597: UEDIN RBMT-3 RBMT-3 7.16: UEDIN 0.593 (3-6): UEDIN
6 0.558: RWTH KIT KIT 7.76: RWTH 0.551 (5-9): RWTH
7 0.545: LIMSI RWTH RWTH 7.83: KIT 0.547 (5-10): KIT
8 0.544: KIT QCRI QCRI 7.85: LIMSI 0.545 (6-10): LIMSI
9 0.524: QCRI RBMT-4 RBMT-4 8.20: QCRI 0.521 (7-11): QCRI
10 0.505: RBMT-1 LIMSI LIMSI 8.40: RBMT-4 0.506 (8-11): RBMT-1
11 0.502: RBMT-4 RBMT-1 RBMT-1 8.42: RBMT-1 0.506 (8-11): RBMT-4
12 0.434: ONLINE-C ONLINE-C ONLINE-C 9.43: ONLINE-C 0.434 (12-13): ONLINE-C
13 0.402: DFKI-BERLIN DFKI-BERLIN DFKI-BERLIN 9.86: DFKI-BERLIN 0.405 (12-14): DFKI-BERLIN
14 0.374: UK UK UK 10.25: UK 0.377 (13-15): UK
15 0.337: JHU JHU JHU 10.81: JHU 0.338 (14-15): JHU
16 0.179: UG UG UG 13.26: UG 0.180 (16): UG
Table 23: Overall ranking with different methods (German?English)
45
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.630: LIMSI LIMSI LIMSI 6.33: LIMSI 0.626 (1-3): LIMSI
2 0.613: KIT CMU CMU 6.55: KIT 0.610 (1-4): KIT
3 0.593: ONLINE-A ONLINE-B ONLINE-B 6.80: ONLINE-A 0.592 (1-5): ONLINE-A
4 0.573: CMU KIT KIT 7.06: CMU 0.571 (2-6): CMU
5 0.569: ONLINE-B ONLINE-A ONLINE-A 7.12: ONLINE-B 0.567 (3-7): ONLINE-B
6 0.546: UEDIN LIUM LIUM 7.51: UEDIN 0.538 (5-8): UEDIN
7 0.523: LIUM RWTH RWTH 7.73: LIUM 0.522 (5-8): LIUM
8 0.515: RWTH UEDIN UEDIN 7.88: RWTH 0.510 (6-9): RWTH
9 0.459: RBMT-1 RBMT-1 RBMT-1 8.51: RBMT-1 0.463 (8-12): RBMT-1
10 0.457: RBMT-3 UK UK 8.56: RBMT-3 0.458 (9-13): RBMT-3
11 0.444: UK SFU SFU 8.75: SFU 0.444 (9-14): SFU
12 0.444: SFU RBMT-3 RBMT-3 8.78: UK 0.441 (9-14): UK
13 0.429: RBMT-4 RBMT-4 RBMT-4 8.92: RBMT-4 0.430 (10-14): RBMT-4
14 0.412: JHU JHU JHU 9.19: JHU 0.409 (12-14): JHU
15 0.321: ONLINE-C ONLINE-C ONLINE-C 10.31: ONLINE-C 0.319 (15): ONLINE-C
Table 24: Overall ranking with different methods (French?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.617: ONLINE-A ONLINE-A ONLINE-A 5.38: ONLINE-A 0.617 (1-4): ONLINE-A
2 0.612: ONLINE-B ONLINE-B ONLINE-B 5.43: ONLINE-B 0.611 (1-4): ONLINE-B
3 0.603: QCRI QCRI QCRI 5.56: QCRI 0.600 (1-4): QCRI
4 0.585: UEDIN UPC UPC 5.75: UEDIN 0.581 (2-5): UEDIN
5 0.565: UPC UEDIN UEDIN 5.89: UPC 0.567 (3-6): UPC
6 0.528: GTH-UPM RBMT-3 RBMT-3 6.29: GTH-UPM 0.526 (5-7): GTH-UPM
7 0.512: RBMT-3 JHU JHU 6.37: RBMT-3 0.518 (6-8): RBMT-3
8 0.477: JHU GTH-UPM GTH-UPM 6.73: JHU 0.480 (7-9): JHU
9 0.461: RBMT-4 RBMT-4 RBMT-4 6.92: RBMT-4 0.460 (8-10): RBMT-4
10 0.423: RBMT-1 ONLINE-C ONLINE-C 7.19: RBMT-1 0.429 (9-11): RBMT-1
11 0.420: ONLINE-C RBMT-1 RBMT-1 7.24: ONLINE-C 0.423 (9-11): ONLINE-C
12 0.189: UK UK UK 9.25: UK 0.188 (12): UK
Table 25: Overall ranking with different methods (Spanish?English)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.662: CU-DEPFIX CU-DEPFIX CU-DEPFIX 5.25: CU-DEPFIX 0.660 (1): CU-DEPFIX
2 0.628: ONLINE-B ONLINE-B ONLINE-B 5.78: ONLINE-B 0.616 (2): ONLINE-B
3 0.557: UEDIN UEDIN UEDIN 6.42: UEDIN 0.557 (3-6): UEDIN
4 0.555: CU-TAMCH CU-TAMCH CU-TAMCH 6.45: CU-TAMCH 0.555 (3-6): CU-TAMCH
5 0.543: CU-BOJAR CU-BOJAR CU-BOJAR 6.58: CU-BOJAR 0.541 (3-7): CU-BOJAR
6 0.531: CU-TECTOMT CU-TECTOMT CU-TECTOMT 6.69: CU-TECTOMT 0.532 (4-7): CU-TECTOMT
7 0.528: ONLINE-A ONLINE-A ONLINE-A 6.72: ONLINE-A 0.529 (4-7): ONLINE-A
8 0.478: COMMERCIAL1 COMMERCIAL2 COMMERCIAL2 7.27: COMMERCIAL1 0.477 (8-10): COMMERCIAL1
9 0.459: COMMERCIAL2 COMMERCIAL1 COMMERCIAL1 7.46: COMMERCIAL2 0.459 (8-11): COMMERCIAL2
10 0.442: CU-POOR-COMB CU-POOR-COMB CU-POOR-COMB 7.61: CU-POOR-COMB 0.443 (9-11): CU-POOR-COMB
11 0.437: UK UK UK 7.65: UK 0.440 (9-11): UK
12 0.360: SFU SFU SFU 8.40: SFU 0.362 (12): SFU
13 0.326: JHU JHU JHU 8.72: JHU 0.328 (13): JHU
Table 26: Overall ranking with different methods (English?Czech)
46
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.655: LIMSI LIMSI LIMSI 5.98: LIMSI 0.651 (1-2): LIMSI
2 0.615: RWTH RWTH RWTH 6.57: RWTH 0.609 (2-4): RWTH
3 0.595: ONLINE-B ONLINE-B ONLINE-B 6.84: ONLINE-B 0.589 (2-5): ONLINE-B
4 0.590: KIT KIT KIT 6.86: KIT 0.587 (2-5): KIT
5 0.554: LIUM LIUM LIUM 7.36: LIUM 0.550 (4-8): LIUM
6 0.534: UEDIN UEDIN UEDIN 7.67: UEDIN 0.526 (5-9): UEDIN
7 0.516: RBMT-3 RBMT-3 RBMT-3 7.85: RBMT-3 0.514 (5-10): RBMT-3
8 0.513: ONLINE-A ONLINE-A ONLINE-A 7.92: PROMT 0.507 (6-10): ONLINE-A
9 0.506: PROMT PROMT PROMT 7.92: ONLINE-A 0.507 (6-10): PROMT
10 0.483: RBMT-1 RBMT-1 RBMT-1 8.23: RBMT-1 0.483 (8-11): RBMT-1
11 0.436: JHU JHU JHU 8.85: JHU 0.436 (10-12): JHU
12 0.396: UK UK RBMT-4 9.34: RBMT-4 0.397 (11-15): RBMT-4
13 0.394: ONLINE-C RBMT-4 ITS-LATL 9.38: ONLINE-C 0.393 (12-15): ONLINE-C
14 0.394: RBMT-4 ITS-LATL ONLINE-C 9.41: UK 0.391 (12-15): UK
15 0.360: ITS-LATL ONLINE-C UK 9.81: ITS-LATL 0.360 (13-15): ITS-LATL
Table 27: Overall ranking with different methods (English?French)
Bojar Lopez Most Probable MC Playoffs Expected Wins
1 0.648: ONLINE-B ONLINE-B ONLINE-B 4.70: ONLINE-B 0.646 (1): ONLINE-B
2 0.579: RBMT-3 RBMT-3 RBMT-3 5.35: RBMT-3 0.577 (2-4): RBMT-3
3 0.561: ONLINE-A PROMT PROMT 5.49: ONLINE-A 0.561 (2-5): ONLINE-A
4 0.545: PROMT ONLINE-A ONLINE-A 5.66: PROMT 0.542 (3-6): PROMT
5 0.526: UEDIN UPC UPC 5.78: UEDIN 0.528 (4-6): UEDIN
6 0.524: UPC UEDIN UEDIN 5.81: UPC 0.525 (4-6): UPC
7 0.463: RBMT-4 RBMT-1 RBMT-1 6.33: RBMT-4 0.464 (7-9): RBMT-4
8 0.452: RBMT-1 RBMT-4 RBMT-4 6.42: RBMT-1 0.452 (7-9): RBMT-1
9 0.430: ONLINE-C UK ONLINE-C 6.57: ONLINE-C 0.434 (8-10): ONLINE-C
10 0.412: UK ONLINE-C UK 6.73: UK 0.415 (9-10): UK
11 0.357: JHU JHU JHU 7.17: JHU 0.357 (11): JHU
Table 28: Overall ranking with different methods (English?Spanish)
47
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Czech-English News Task
CU-BOJAR 0.17 0.2 39 0.31 44 0.66 0.50 0.21 0.65 0.2 50 639
JHU 0.16 0.18 41 0.28 41 0.63 0.47 0.19 0.65 0.10 53 692
ONLINE-A 0.18 0.21 40 0.31 43 0.68 0.51 0.21 0.62 0.22 50 648
ONLINE-B 0.18 0.23 40 0.30 42 0.67 0.53 0.23 0.59 0.20 52 660
UEDIN 0.18 0.22 39 0.32 45 0.69 0.53 0.23 0.60 0.25 49 627
UK 0.16 0.18 41 0.29 41 0.63 0.49 0.19 0.67 0.17 53 682
Table 29: Automatic evaluation metric scores for systems in the WMT12 Czech-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
German-English News Task
DFKI-BERLIN 0.17 0.21 40 0.3 43 0.46 0.18 0.61 0.25 50 653
JHU 0.17 0.2 41 0.29 42 0.42 0.21 0.61 0.20 52 672
KIT 0.18 0.23 39 0.31 45 0.46 0.23 0.58 0.28 49 630
LIMSI 0.18 0.23 39 0.31 45 0.48 0.23 0.6 0.30 49 628
ONLINE-A 0.18 0.21 40 0.32 44 0.50 0.22 0.6 0.27 50 645
ONLINE-B 0.19 0.24 39 0.31 44 0.53 0.24 0.59 0.29 50 636
RBMT-4 0.16 0.16 41 0.29 42 0.44 0.18 0.68 0.24 53 690
RBMT-3 0.16 0.17 40 0.3 42 0.47 0.19 0.66 0.29 52 677
ONLINE-C 0.15 0.14 42 0.28 40 0.43 0.17 0.70 0.26 54 711
RBMT-1 0.15 0.15 43 0.29 40 0.45 0.17 0.69 0.24 54 711
QCRI 0.18 0.23 40 0.31 44 0.46 0.23 0.59 0.26 50 639
QUAERO 0.19 0.24 38 0.32 46 0.49 0.24 0.57 0.3 48 613
RWTH 0.18 0.23 39 0.31 45 0.48 0.24 0.58 0.27 49 626
UEDIN 0.18 0.23 39 0.31 46 0.51 0.23 0.59 0.32 49 630
UG 0.11 0.11 45 0.24 35 0.38 0.14 0.77 0.10 59 768
UK 0.16 0.18 42 0.29 40 0.42 0.2 0.65 0.27 53 683
Table 30: Automatic evaluation metric scores for systems in the WMT12 German-English News Task
48
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
French-English News Task
CMU 0.20 0.29 36 0.34 51 0.54 0.29 0.52 0.25 44 561
JHU 0.19 0.26 37 0.33 47 0.50 0.26 0.54 0.20 46 596
KIT 0.21 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 551
LIMSI 0.21 0.30 35 0.34 52 0.55 0.3 0.51 0.25 43 546
LIUM 0.20 0.29 36 0.34 50 0.54 0.29 0.52 0.24 44 558
ONLINE-A 0.2 0.27 37 0.34 48 0.52 0.27 0.53 0.24 45 584
ONLINE-B 0.20 0.30 36 0.33 48 0.55 0.29 0.51 0.22 46 582
RBMT-4 0.18 0.20 38 0.32 45 0.49 0.21 0.64 0.15 48 622
RBMT-3 0.18 0.21 39 0.31 46 0.49 0.22 0.61 0.15 48 637
ONLINE-C 0.18 0.19 38 0.31 45 0.45 0.21 0.64 0.10 48 633
RBMT-1 0.18 0.21 39 0.32 47 0.5 0.22 0.62 0.15 48 626
RWTH 0.20 0.29 36 0.34 50 0.53 0.28 0.53 0.20 44 563
SFU 0.2 0.25 37 0.33 48 0.51 0.26 0.54 0.17 46 596
UEDIN 0.20 0.30 35 0.34 51 0.54 0.3 0.51 0.25 43 549
UK 0.19 0.25 38 0.33 47 0.52 0.25 0.57 0.17 47 602
Table 31: Automatic evaluation metric scores for systems in the WMT12 French-English News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
A
G
A
N
-S
T
S
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
X
E
N
E
R
R
C
A
T
S
Spanish-English News Task
GTH-UPM 0.21 0.29 35 0.35 51 0.7 0.55 0.29 0.51 0.31 43 565
JHU 0.21 0.29 35 0.35 51 0.7 0.56 0.29 0.51 0.31 43 560
ONLINE-A 0.22 0.31 34 0.36 52 0.72 0.58 0.31 0.49 0.36 42 535
ONLINE-B 0.22 0.38 33 0.36 53 0.70 0.60 0.35 0.45 0.35 41 523
RBMT-4 0.19 0.23 36 0.33 49 0.69 0.54 0.24 0.60 0.29 45 591
RBMT-3 0.19 0.23 36 0.33 49 0.69 0.54 0.23 0.60 0.29 45 590
ONLINE-C 0.19 0.22 37 0.33 47 0.68 0.5 0.23 0.61 0.24 46 598
RBMT-1 0.18 0.22 38 0.33 48 0.67 0.52 0.23 0.62 0.23 47 607
QCRI 0.22 0.33 33 0.36 54 0.71 0.6 0.32 0.49 0.32 40 523
UEDIN 0.22 0.33 33 0.36 54 0.71 0.59 0.32 0.48 0.32 40 519
UK 0.18 0.22 37 0.30 44 0.6 0.48 0.23 0.60 0.10 48 634
UPC 0.22 0.32 34 0.36 54 0.71 0.57 0.31 0.49 0.33 41 531
Table 32: Automatic evaluation metric scores for systems in the WMT12 Spanish-English News Task
49
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
E
M
P
O
S
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Czech News Task
COMMERCIAL-2 0.01 0.08 47 693 0.17 23 0.38 0.1 0.76 0.17 61
CU-BOJAR 0.17 0.13 45 644 0.21 28 0.4 0.13 0.69 0.26 57
CU-DEPFIX 0.19 0.16 44 623 0.22 28 0.45 0.15 0.66 0.30 55
CU-POOR-COMB 0.14 0.12 48 710 0.19 27 0.35 0.12 0.67 0.23 60
CU-TAMCH 0.17 0.13 45 647 0.21 28 0.38 0.13 0.69 0.29 57
CU-TECTOMT 0.16 0.12 48 690 0.19 26 0.36 0.12 0.68 0.22 60
JHU 0.16 0.1 47 691 0.2 23 0.39 0.11 0.69 0.10 60
ONLINE-A 0.17 0.13 n/a n/a 0.21 n/a 0.42 0.13 0.67 0.25 n/a
ONLINE-B 0.19 0.16 44 623 0.21 28 0.45 0.15 0.66 0.30 55
COMMERCIAL-1 0.11 0.09 48 692 0.18 22 0.38 0.10 0.74 0.21 61
SFU 0.15 0.11 47 674 0.19 23 0.39 0.11 0.71 0.21 60
UEDIN 0.18 0.15 45 639 0.21 27 0.41 0.14 0.66 0.40 56
UK 0.15 0.11 47 669 0.19 25 0.39 0.12 0.71 0.35 59
Table 33: Automatic evaluation metric scores for systems in the WMT12 English-Czech News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-German News Task
DFKI-BERLIN 0.18 0.14 46 628 0.35 41 0.13 0.69 0.10 57
DFKI-HUNSICKER 0.18 0.14 45 621 0.35 42 0.15 0.69 0.17 57
JHU 0.2 0.15 45 618 0.37 42 0.16 0.68 0.17 56
KIT 0.20 0.17 45 606 0.38 43 0.17 0.66 0.14 55
LIMSI 0.2 0.17 45 615 0.37 43 0.17 0.65 0.15 56
ONLINE-A 0.20 0.16 45 617 0.38 43 0.17 0.65 0.36 55
ONLINE-B 0.22 0.18 43 589 0.38 42 0.18 0.64 0.35 55
RBMT-4 0.18 0.14 45 623 0.35 42 0.15 0.69 0.35 57
RBMT-3 0.19 0.15 44 608 0.36 44 0.16 0.68 0.37 56
ONLINE-C 0.16 0.11 47 655 0.32 39 0.13 0.74 0.37 60
RBMT-1 0.17 0.13 47 643 0.34 42 0.15 0.70 0.36 58
RWTH 0.2 0.16 44 609 0.37 43 0.16 0.67 0.25 56
UEDIN-WILLIAMS 0.19 0.16 45 628 0.37 43 0.17 0.66 0.33 57
UEDIN 0.20 0.16 45 611 0.37 43 0.17 0.66 0.29 55
UK 0.18 0.14 46 632 0.36 40 0.15 0.71 0.27 58
Table 34: Automatic evaluation metric scores for systems in the WMT12 English-German News Task
50
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-French News Task
ITS-LATL 0.24 0.21 41 548 0.45 48 0.21 0.61 0.15 50
JHU 0.26 0.25 38 511 0.49 51 0.25 0.57 0.15 47
KIT 0.28 0.28 36 480 0.52 55 0.28 0.54 0.22 44
LIMSI 0.28 0.29 36 472 0.52 55 0.28 0.54 0.22 44
LIUM 0.28 0.28 37 480 0.51 54 0.28 0.55 0.20 45
ONLINE-A 0.26 0.25 39 512 0.5 52 0.26 0.57 0.17 47
ONLINE-B 0.24 0.21 36 473 0.48 45 0.26 0.77 0.10 49
RBMT-4 0.24 0.21 40 539 0.46 48 0.22 0.60 0.10 49
RBMT-3 0.26 0.24 39 511 0.48 52 0.24 0.58 0.14 47
ONLINE-C 0.23 0.2 41 550 0.45 50 0.21 0.62 0.10 50
RBMT-1 0.25 0.22 40 531 0.47 51 0.23 0.6 0.13 49
PROMT 0.26 0.24 38 502 0.49 52 0.25 0.58 0.18 46
RWTH 0.28 0.29 36 478 0.52 54 0.28 0.54 0.22 44
UEDIN 0.28 0.28 36 479 0.52 54 0.28 0.55 0.27 45
UK 0.25 0.23 39 523 0.48 51 0.24 0.6 0.17 48
Table 35: Automatic evaluation metric scores for systems in the WMT12 English-French News Task
A
M
B
E
R
B
L
E
U
-4
-C
L
O
S
E
S
T
-C
A
S
E
D
B
L
O
C
K
E
R
R
C
A
T
S
E
N
X
E
R
R
C
A
T
S
M
E
T
E
O
R
P
O
S
F
S
IM
P
B
L
E
U
T
E
R
T
E
R
R
O
R
C
A
T
W
O
R
D
B
L
O
C
K
E
R
R
C
A
T
S
English-Spanish News Task
JHU 0.29 0.29 37 494 0.54 52 0.29 0.51 0.14 45
ONLINE-A 0.31 0.31 36 475 0.56 54 0.31 0.48 0.2 43
ONLINE-B 0.33 0.36 34 431 0.57 54 0.34 0.48 0.25 42
RBMT-4 0.27 0.24 39 528 0.5 50 0.25 0.55 0.14 48
RBMT-3 0.28 0.26 39 510 0.51 51 0.26 0.54 0.13 46
ONLINE-C 0.26 0.24 40 532 0.5 49 0.25 0.55 0.10 48
RBMT-1 0.26 0.23 40 534 0.50 49 0.25 0.57 0.13 49
PROMT 0.29 0.27 38 497 0.52 52 0.28 0.53 0.18 45
UEDIN 0.31 0.32 35 466 0.56 55 0.32 0.49 0.19 42
UK 0.29 0.28 38 510 0.54 51 0.28 0.52 0.17 46
UPC 0.31 0.32 36 476 0.56 54 0.31 0.49 0.19 43
Table 36: Automatic evaluation metric scores for systems in the WMT12 English-Spanish News Task
51
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 283?291,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joshua 4.0: Packing, PRO, and Paraphrases
Juri Ganitkevitch1, Yuan Cao1, Jonathan Weese1, Matt Post2, and Chris Callison-Burch1
1Center for Language and Speech Processing
2Human Language Technology Center of Excellence
Johns Hopkins University
Abstract
We present Joshua 4.0, the newest version
of our open-source decoder for parsing-based
statistical machine translation. The main con-
tributions in this release are the introduction
of a compact grammar representation based
on packed tries, and the integration of our
implementation of pairwise ranking optimiza-
tion, J-PRO. We further present the exten-
sion of the Thrax SCFG grammar extractor
to pivot-based extraction of syntactically in-
formed sentential paraphrases.
1 Introduction
Joshua is an open-source toolkit1 for parsing-based
statistical machine translation of human languages.
The original version of Joshua (Li et al, 2009) was
a reimplementation of the Python-based Hiero ma-
chine translation system (Chiang, 2007). It was later
extended to support grammars with rich syntactic
labels (Li et al, 2010a). More recent efforts in-
troduced the Thrax module, an extensible Hadoop-
based extraction toolkit for synchronous context-
free grammars (Weese et al, 2011).
In this paper we describe a set of recent exten-
sions to the Joshua system. We present a new com-
pact grammar representation format that leverages
sparse features, quantization, and data redundancies
to store grammars in a dense binary format. This al-
lows for both near-instantaneous start-up times and
decoding with extremely large grammars. In Sec-
tion 2 we outline our packed grammar format and
1joshua-decoder.org
present experimental results regarding its impact on
decoding speed, memory use and translation quality.
Additionally, we present Joshua?s implementation
of the pairwise ranking optimization (Hopkins and
May, 2011) approach to translation model tuning.
J-PRO, like Z-MERT, makes it easy to implement
new metrics and comes with both a built-in percep-
tron classifier and out-of-the-box support for widely
used binary classifiers such as MegaM and Max-
Ent (Daume? III and Marcu, 2006; Manning and
Klein, 2003). We describe our implementation in
Section 3, presenting experimental results on perfor-
mance, classifier convergence, and tuning speed.
Finally, we introduce the inclusion of bilingual
pivoting-based paraphrase extraction into Thrax,
Joshua?s grammar extractor. Thrax?s paraphrase ex-
traction mode is simple to use, and yields state-of-
the-art syntactically informed sentential paraphrases
(Ganitkevitch et al, 2011). The full feature set of
Thrax (Weese et al, 2011) is supported for para-
phrase grammars. An easily configured feature-level
pruning mechanism allows to keep the paraphrase
grammar size manageable. Section 4 presents de-
tails on our paraphrase extraction module.
2 Compact Grammar Representation
Statistical machine translation systems tend to per-
form better when trained on larger amounts of bilin-
gual parallel data. Using tools such as Thrax, trans-
lation models and their parameters are extracted
and estimated from the data. In Joshua, translation
models are represented as synchronous context-free
grammars (SCFGs). An SCFG is a collection of
283
rules {ri} that take the form:
ri = Ci ? ??i, ?i,?i, ~?i?, (1)
where left-hand side Ci is a nonterminal symbol, the
source side ?i and the target side ?i are sequences
of both nonterminal and terminal symbols. Further,
?i is a one-to-one correspondence between the non-
terminal symbols of ?i and ?i, and ~?i is a vector of
features quantifying the probability of ?i translat-
ing to ?i, as well as other characteristics of the rule
(Weese et al, 2011). At decoding time, Joshua loads
the grammar rules into memory in their entirety, and
stores them in a trie data structure indexed by the
rules? source side. This allows the decoder to effi-
ciently look up rules that are applicable to a particu-
lar span of the (partially translated) input.
As the size of the training corpus grows, so does
the resulting translation grammar. Using more di-
verse sets of nonterminal labels ? which can signifi-
cantly improve translation performance ? further ag-
gravates this problem. As a consequence, the space
requirements for storing the grammar in memory
during decoding quickly grow impractical. In some
cases grammars may become too large to fit into the
memory on a single machine.
As an alternative to the commonly used trie struc-
tures based on hash maps, we propose a packed trie
representation for SCFGs. The approach we take is
similar to work on efficiently storing large phrase
tables by Zens and Ney (2007) and language mod-
els by Heafield (2011) and Pauls and Klein (2011) ?
both language model implementations are now inte-
grated with Joshua.
2.1 Packed Synchronous Tries
For our grammar representation, we break the SCFG
up into three distinct structures. As Figure 1 in-
dicates, we store the grammar rules? source sides
{?i}, target sides {?i}, and feature data {~?i} in sep-
arate formats of their own. Each of the structures
is packed into a flat array, and can thus be quickly
read into memory. All terminal and nonterminal
symbols in the grammar are mapped to integer sym-
bol id?s using a globally accessible vocabulary map.
We will now describe the implementation details for
each representation and their interactions in turn.
2.1.1 Source-Side Trie
The source-side trie (or source trie) is designed
to facilitate efficient lookup of grammar rules by
source side, and to allow us to completely specify a
matching set of rule with a single integer index into
the trie. We store the source sides {?i} of a grammar
in a downward-linking trie, i.e. each trie node main-
tains a record of its children. The trie is packed into
an array of 32-bit integers. Figure 1 illustrates the
composition of a node in the source-side trie. All
information regarding the node is stored in a con-
tiguous block of integers, and decomposes into two
parts: a linking block and a rule block.
The linking block stores the links to the child trie
nodes. It consists of an integer n, the number of chil-
dren, and n blocks of two integers each, containing
the symbol id aj leading to the child and the child
node?s address sj (as an index into the source-side
array). The children in the link block are sorted by
symbol id, allowing for a lookup via binary or inter-
polation search.
The rule block stores all information necessary to
reconstruct the rules that share the source side that
led to the current source trie node. It stores the num-
ber of rules, m, and then a tuple of three integers
for each of the m rules: we store the symbol id of
the left-hand side, an index into the target-side trie
and a data block id. The rules in the data block are
initially in an arbitrary order, but are sorted by ap-
plication cost upon loading.
2.1.2 Target-Side Trie
The target-side trie (or target trie) is designed to
enable us to uniquely identify a target side ?i with a
single pointer into the trie, as well as to exploit re-
dundancies in the target side string. Like the source
trie, it is stored as an array of integers. However,
the target trie is a reversed, or upward-linking trie:
a trie node retains a link to its parent, as well as the
symbol id labeling said link.
As illustrated in Figure 1, the target trie is ac-
cessed by reading an array index from the source
trie, pointing to a trie node at depth d. We then fol-
low the parent links to the trie root, accumulating
target side symbols gj into a target side string gd1 as
we go along. In order to match this traversal, the tar-
get strings are entered into the trie in reverse order,
i.e. last word first. In order to determine d from a
284
# children
# rules
child symbol
child address
rule left-hand side
target address
data block id
n ?
m ?
a
j
s
j
+
1
C
j
t
j
b
j
.
.
.
.
.
.
n
m
.
.
.
.
.
.
parent symbol
parent address
g
j
t
j-1
.
.
.
.
.
.
# features
feature id
feature value
n ?
f
j
v
j
.
.
.
n
.
.
.
.
.
Feature block 
index
Feature byte
buffer
Target trie
array
Source trie
array
f
j
.
.
.
.
.
.
Quantization
b
j
.
.
.
.
.
.
f
j
q
j
Figure 1: An illustration of our packed grammar data structures. The source sides of the grammar rules are
stored in a packed trie. Each node may contain n children and the symbols linking to them, and m entries
for rules that share the same source side. Each rule entry links to a node in the target-side trie, where the full
target string can be retrieved by walking up the trie until the root is reached. The rule entries also contain
a data block id, which identifies feature data attached to the rule. The features are encoded according to a
type/quantization specification and stored as variable-length blocks of data in a byte buffer.
pointer into the target trie, we maintain an offset ta-
ble in which we keep track of where each new trie
level begins in the array. By first searching the offset
table, we can determine d, and thus know how much
space to allocate for the complete target side string.
To further benefit from the overlap there may be
among the target sides in the grammar, we drop the
nonterminal labels from the target string prior to in-
serting them into the trie. For richly labeled gram-
mars, this collapses all lexically identical target sides
that share the same nonterminal reordering behavior,
but vary in nonterminal labels into a single path in
the trie. Since the nonterminal labels are retained in
the rules? source sides, we do not lose any informa-
tion by doing this.
2.1.3 Features and Other Data
We designed the data format for the grammar
rules? feature values to be easily extended to include
other information that we may want to attach to a
rule, such as word alignments, or locations of occur-
rences in the training data. In order to that, each rule
ri has a unique block id bi associated with it. This
block id identifies the information associated with
the rule in every attached data store. All data stores
are implemented as memory-mapped byte buffers
that are only loaded into memory when actually re-
quested by the decoder. The format for the feature
data is detailed in the following.
The rules? feature values are stored as sparse fea-
tures in contiguous blocks of variable length in a
byte buffer. As shown in Figure 1, a lookup table
is used to map the bi to the index of the block in the
buffer. Each block is structured as follows: a sin-
gle integer, n, for the number of features, followed
by n feature entries. Each feature entry is led by an
integer for the feature id fj , and followed by a field
of variable length for the feature value vj . The size
of the value is determined by the type of the feature.
Joshua maintains a quantization configuration which
maps each feature id to a type handler or quantizer.
After reading a feature id from the byte buffer, we
retrieve the responsible quantizer and use it to read
the value from the byte buffer.
Joshua?s packed grammar format supports Java?s
standard primitive types, as well as an 8-bit quan-
tizer. We chose 8 bit as a compromise between
compression, value decoding speed and transla-
285
Grammar Format Memory
Hiero (43M rules)
Baseline 13.6G
Packed 1.8G
Syntax (200M rules)
Baseline 99.5G
Packed 9.8G
Packed 8-bit 5.8G
Table 1: Decoding-time memory use for the packed
grammar versus the standard grammar format. Even
without lossy quantization the packed grammar rep-
resentation yields significant savings in memory
consumption. Adding 8-bit quantization for the real-
valued features in the grammar reduces even large
syntactic grammars to a manageable size.
tion performance (Federico and Bertoldi, 2006).
Our quantization approach follows Federico and
Bertoldi (2006) and Heafield (2011) in partitioning
the value histogram into 256 equal-sized buckets.
We quantize by mapping each feature value onto the
weighted average of its bucket. Joshua allows for an
easily per-feature specification of type. Quantizers
can be share statistics across multiple features with
similar value distributions.
2.2 Experiments
We assess the packed grammar representation?s
memory efficiency and impact on the decoding
speed on the WMT12 French-English task. Ta-
ble 1 shows a comparison of the memory needed
to store our WMT12 French-English grammars at
runtime. We can observe a substantial decrease in
memory consumption for both Hiero-style gram-
mars and the much larger syntactically annotated
grammars. Even without any feature value quantiza-
tion, the packed format achieves an 80% reduction
in space requirements. Adding 8-bit quantization
for the log-probability features yields even smaller
grammar sizes, in this case a reduction of over 94%.
In order to avoid costly repeated retrievals of indi-
vidual feature values of rules, we compute and cache
the stateless application cost for each grammar rule
at grammar loading time. This, alongside with a lazy
approach to rule lookup allows us to largely avoid
losses in decoding speed.
Figure shows a translation progress graph for the
WMT12 French-English development set. Both sys-
 0 500 1000 1500 2000 2500
 0  500  1000  1500  2000  2500Sentences Translated Seconds PassedStandardPacked
Figure 2: A visualization of the loading and decod-
ing speed on the WMT12 French-English develop-
ment set contrasting the packed grammar represen-
tation with the standard format. Grammar loading
for the packed grammar representation is substan-
tially faster than that for the baseline setup. Even
with a slightly slower decoding speed (note the dif-
ference in the slopes) the packed grammar finishes
in less than half the time, compared to the standard
format.
tems load a Hiero-style grammar with 43 million
rules, and use 16 threads for parallel decoding. The
initial loading time for the packed grammar repre-
sentation is dramatically shorter than that for the
baseline setup (a total of 176 seconds for loading and
sorting the grammar, versus 1897 for the standard
format). Even though decoding speed is slightly
slower with the packed grammars (an average of 5.3
seconds per sentence versus 4.2 for the baseline), the
effective translation speed is more than twice that of
the baseline (1004 seconds to complete decoding the
2489 sentences, versus 2551 seconds with the stan-
dard setup).
3 J-PRO: Pairwise Ranking Optimization
in Joshua
Pairwise ranking optimization (PRO) proposed by
(Hopkins and May, 2011) is a new method for dis-
criminative parameter tuning in statistical machine
translation. It is reported to be more stable than the
popular MERT algorithm (Och, 2003) and is more
scalable with regard to the number of features. PRO
treats parameter tuning as an n-best list reranking
problem, and the idea is similar to other pairwise
ranking techniques like ranking SVM and IR SVMs
286
(Li, 2011). The algorithm can be described thusly:
Let h(c) = ?w,?(c)? be the linear model score
of a candidate translation c, in which ?(c) is the
feature vector of c and w is the parameter vector.
Also let g(c) be the metric score of c (without loss
of generality, we assume a higher score indicates a
better translation). We aim to find a parameter vector
w such that for a pair of candidates {ci, cj} in an n-
best list,
(h(ci)? h(cj))(g(ci)? g(cj)) =
?w,?(ci)??(cj)?(g(ci)? g(cj)) > 0,
namely the order of the model score is consistent
with that of the metric score. This can be turned into
a binary classification problem, by adding instance
??ij = ?(ci)??(cj)
with class label sign(g(ci) ? g(cj)) to the training
data (and symmetrically add instance
??ji = ?(cj)??(ci)
with class label sign(g(cj) ? g(ci)) at the same
time), then using any binary classifier to find the w
which determines a hyperplane separating the two
classes (therefore the performance of PRO depends
on the choice of classifier to a large extent). Given
a training set with T sentences, there are O(Tn2)
pairs of candidates that can be added to the training
set, this number is usually much too large for effi-
cient training. To make the task more tractable, PRO
samples a subset of the candidate pairs so that only
those pairs whose metric score difference is large
enough are qualified as training instances. This fol-
lows the intuition that high score differential makes
it easier to separate good translations from bad ones.
3.1 Implementation
PRO is implemented in Joshua 4.0 named J-PRO.
In order to ensure compatibility with the decoder
and the parameter tuning module Z-MERT (Zaidan,
2009) included in all versions of Joshua, J-PRO is
built upon the architecture of Z-MERT with sim-
ilar usage and configuration files(with a few extra
lines specifying PRO-related parameters). J-PRO in-
herits Z-MERT?s ability to easily plug in new met-
rics. Since PRO allows using any off-the-shelf bi-
nary classifiers, J-PRO provides a Java interface that
enables easy plug-in of any classifier. Currently, J-
PRO supports three classifiers:
? Perceptron (Rosenblatt, 1958): the percep-
tron is self-contained in J-PRO, no external re-
sources required.
? MegaM (Daume? III and Marcu, 2006): the clas-
sifier used by Hopkins and May (2011).2
? Maximum entropy classifier (Manning and
Klein, 2003): the Stanford toolkit for maxi-
mum entropy classification.3
The user may specify which classifier he wants to
use and the classifier-specific parameters in the J-
PRO configuration file.
The PRO approach is capable of handling a large
number of features, allowing the use of sparse dis-
criminative features for machine translation. How-
ever, Hollingshead and Roark (2008) demonstrated
that naively tuning weights for a heterogeneous fea-
ture set composed of both dense and sparse features
can yield subpar results. Thus, to better handle the
relation between dense and sparse features and pro-
vide a flexible selection of training schemes, J-PRO
supports the following four training modes. We as-
sume M dense features and N sparse features are
used:
1. Tune the dense feature parameters only, just
like Z-MERT (M parameters to tune).
2. Tune the dense + sparse feature parameters to-
gether (M +N parameters to tune).
3. Tune the sparse feature parameters only with
the dense feature parameters fixed, and sparse
feature parameters scaled by a manually speci-
fied constant (N parameters to tune).
4. Tune the dense feature parameters and the scal-
ing factor for sparse features, with the sparse
feature parameters fixed (M+1 parameters to
tune).
J-PRO supports n-best list input with a sparse fea-
ture format which enumerates only the firing fea-
tures together with their values. This enables a more
compact feature representation when numerous fea-
tures are involved in training.
2hal3.name/megam
3nlp.stanford.edu/software
287
0 10 20 300
10
20
30
40
Iteration
BLE
U
Dev set MT03 (10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT04(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLE
U
Test set MT05(10 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Dev set MT03 (1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT04(1026 features)
 
 
PercepMegaMMax?Ent
0 10 20 300
10
20
30
40
Iteration
BLEU
Test set MT05(1026 features)
 
 
PercepMegaMMax?Ent
Figure 3: Experimental results on the development and test sets. The x-axis is the number of iterations (up to
30) and the y-axis is the BLEU score. The three curves in each figure correspond to three classifiers. Upper
row: results trained using only dense features (10 features); Lower row: results trained using dense+sparse
features (1026 features). Left column: development set (MT03); Middle column: test set (MT04); Right
column: test set (MT05).
Datasets Z-MERT
J-PRO
Percep MegaM Max-Ent
Dev (MT03) 32.2 31.9 32.0 32.0
Test (MT04) 32.6 32.7 32.7 32.6
Test (MT05) 30.7 30.9 31.0 30.9
Table 2: Comparison between the results given by Z-MERT and J-PRO (trained with 10 features).
3.2 Experiments
We did our experiments using J-PRO on the NIST
Chinese-English data, and BLEU score was used as
the quality metric for experiments reported in this
section.4 The experimental settings are as the fol-
lowing:
Datasets: MT03 dataset (998 sentences) as devel-
opment set for parameter tuning, MT04 (1788 sen-
tences) and MT05 (1082 sentences) as test sets.
Features: Dense feature set include the 10 regular
features used in the Hiero system; Sparse feature set
4We also experimented with other metrics including TER,
METEOR and TER-BLEU. Similar trends as reported in this
section were observed. These results are omitted here due to
limited space.
includes 1016 target-side rule POS bi-gram features
as used in (Li et al, 2010b).
Classifiers: Perceptron, MegaM and Maximum
entropy.
PRO parameters: ? = 8000 (number of candidate
pairs sampled uniformly from the n-best list), ? = 1
(sample acceptance probability), ? = 50 (number of
top candidates to be added to the training set).
Figure 3 shows the BLEU score curves on the
development and test sets as a function of itera-
tions. The upper and lower rows correspond to
the results trained with 10 dense features and 1026
dense+sparse features respectively. We intentionally
selected very bad initial parameter vectors to verify
the robustness of the algorithm. It can be seen that
288
with each iteration, the BLEU score increases mono-
tonically on both development and test sets, and be-
gins to converge after a few iterations. When only 10
features are involved, all classifiers give almost the
same performance. However, when scaled to over a
thousand features, the maximum entropy classifier
becomes unstable and the curve fluctuates signifi-
cantly. In this situation MegaM behaves well, but
the J-PRO built-in perceptron gives the most robust
performance.
Table 2 compares the results of running Z-MERT
and J-PRO. Since MERT is not able to handle nu-
merous sparse features, we only report results for
the 10-feature setup. The scores for both setups
are quite close to each other, with Z-MERT doing
slightly better on the development set but J-PRO
yielding slightly better performance on the test set.
4 Thrax: Grammar Extraction at Scale
4.1 Translation Grammars
In previous years, our grammar extraction methods
were limited by either memory-bounded extractors.
Moving towards a parallelized grammar extraction
process, we switched from Joshua?s formerly built-
in extraction module to Thrax for WMT11. How-
ever, we were limited to a simple pseudo-distributed
Hadoop setup. In a pseudo-distributed cluster, all
tasks run on separate cores on the same machine
and access the local file system simultaneously, in-
stead of being distributed over different physical ma-
chines and harddrives. This setup proved unreliable
for larger extractions, and we were forced to reduce
the amount of data that we used to train our transla-
tion models.
For this year, however, we had a permanent clus-
ter at our disposal, which made it easy to extract
grammars from all of the available WMT12 data.
We found that on a properly distributed Hadoop
setup Thrax was able to extract both Hiero gram-
mars and the much larger SAMT grammars on the
complete WMT12 training data for all tested lan-
guage pairs. The runtimes and resulting (unfiltered)
grammar sizes for each language pair are shown in
Table 3 (for Hiero) and Table 4 (for SAMT).
Language Pair Time Rules
Cs ? En 4h41m 133M
De ? En 5h20m 219M
Fr ? En 16h47m 374M
Es ? En 16h22m 413M
Table 3: Extraction times and grammar sizes for Hi-
ero grammars using the Europarl and News Com-
mentary training data for each listed language pair.
Language Pair Time Rules
Cs ? En 7h59m 223M
De ? En 9h18m 328M
Fr ? En 25h46m 654M
Es ? En 28h10m 716M
Table 4: Extraction times and grammar sizes for
the SAMT grammars using the Europarl and News
Commentary training data for each listed language
pair.
4.2 Paraphrase Extraction
Recently English-to-English text generation tasks
have seen renewed interest in the NLP commu-
nity. Paraphrases are a key component in large-
scale state-of-the-art text-to-text generation systems.
We present an extended version of Thrax that im-
plements distributed, Hadoop-based paraphrase ex-
traction via the pivoting approach (Bannard and
Callison-Burch, 2005). Our toolkit is capable of
extracting syntactically informed paraphrase gram-
mars at scale. The paraphrase grammars obtained
with Thrax have been shown to achieve state-of-the-
art results on text-to-text generation tasks (Ganitke-
vitch et al, 2011).
For every supported translation feature, Thrax im-
plements a corresponding pivoted feature for para-
phrases. The pivoted features are set up to be aware
of the prerequisite translation features they are de-
rived from. This allows Thrax to automatically de-
tect the needed translation features and spawn the
corresponding map-reduce passes before the pivot-
ing stage takes place. In addition to features use-
ful for translation, Thrax also offers a number of
features geared towards text-to-text generation tasks
such as sentence compression or text simplification.
Due to the long tail of translations in unpruned
289
Source Bitext Sentences Words Pruning Rules
Fr ? En 1.6M 45M p(e1|e2), p(e2|e1) > 0.001 49M
{Da + Sv + Cs + De + Es + Fr} ? En 9.5M 100M
p(e1|e2), p(e2|e1) > 0.02 31M
p(e1|e2), p(e2|e1) > 0.001 91M
Table 5: Large paraphrase grammars extracted from EuroParl data using Thrax. The sentence and word
counts refer to the English side of the bitexts used.
translation grammars and the combinatorial effect
of pivoting, paraphrase grammars can easily grow
very large. We implement a simple feature-level
pruning approach that allows the user to specify up-
per or lower bounds for any pivoted feature. If a
paraphrase rule is not within these bounds, it is dis-
carded. Additionally, pivoted features are aware of
the bounding relationship between their value and
the value of their prerequisite translation features
(i.e. whether the pivoted feature?s value can be guar-
anteed to never be larger than the value of the trans-
lation feature). Thrax uses this knowledge to dis-
card overly weak translation rules before the pivot-
ing stage, leading to a substantial speedup in the ex-
traction process.
Table 5 gives a few examples of large paraphrase
grammars extracted from WMT training data. With
appropriate pruning settings, we are able to obtain
paraphrase grammars estimated over bitexts with
more than 100 million words.
5 Additional New Features
? With the help of the respective original au-
thors, the language model implementations by
Heafield (2011) and Pauls and Klein (2011)
have been integrated with Joshua, dropping
support for the slower and more difficult to
compile SRILM toolkit (Stolcke, 2002).
? We modified Joshua so that it can be used as
a parser to analyze pairs of sentences using a
synchronous context-free grammar. We imple-
mented the two-pass parsing algorithm of Dyer
(2010).
6 Conclusion
We present a new iteration of the Joshua machine
translation toolkit. Our system has been extended to-
wards efficiently supporting large-scale experiments
in parsing-based machine translation and text-to-text
generation: Joshua 4.0 supports compactly repre-
sented large grammars with its packed grammars,
as well as large language models via KenLM and
BerkeleyLM.We include an implementation of PRO,
allowing for stable and fast tuning of large feature
sets, and extend our toolkit beyond pure translation
applications by extending Thrax with a large-scale
paraphrase extraction module.
Acknowledgements This research was supported
by in part by the EuroMatrixPlus project funded
by the European Commission (7th Framework Pro-
gramme), and by the NSF under grant IIS-0713448.
Opinions, interpretations, and conclusions are the
authors? alone.
References
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proceed-
ings of ACL.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Hal Daume? III and Daniel Marcu. 2006. Domain adap-
tation for statistical classifiers. Journal of Artificial
Intelligence Research, 26(1):101?126.
Chris Dyer. 2010. Two monolingual parses are bet-
ter than one (synchronous parse). In Proceedings of
HLT/NAACL, pages 263?266. Association for Compu-
tational Linguistics.
Marcello Federico and Nicola Bertoldi. 2006. How
many bits are needed to store probabilities for phrase-
based translation? In Proceedings of WMT06, pages
94?101. Association for Computational Linguistics.
Juri Ganitkevitch, Chris Callison-Burch, Courtney
Napoles, and Benjamin Van Durme. 2011. Learning
sentential paraphrases from bilingual parallel corpora
for text-to-text generation. In Proceedings of EMNLP.
Kenneth Heafield. 2011. Kenlm: Faster and smaller
language model queries. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguistics.
290
Kristy Hollingshead and Brian Roark. 2008. Rerank-
ing with baseline system scores and ranks as features.
Technical report, Center for Spoken Language Under-
standing, Oregon Health & Science University.
Mark Hopkins and Jonathan May. 2011. Tuning as rank-
ing. In Proceedings of EMNLP.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based ma-
chine translation. In Proc. WMT, Athens, Greece,
March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010a. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proc. WMT.
Zhifei Li, Ziyuan Wang, and Sanjeev Khudanpur. 2010b.
Unsupervised discriminative language model training
for machine translation using simulated confusion sets.
In Proceedings of COLING, Beijing, China, August.
Hang Li. 2011. Learning to Rank for Information Re-
trieval and Natural Language Processing. Morgan &
Claypool Publishers.
Chris Manning and Dan Klein. 2003. Optimization,
maxent models, and conditional estimation without
magic. In Proceedings of HLT/NAACL, pages 8?8. As-
sociation for Computational Linguistics.
Franz Och. 2003. Minimum error rate training in statis-
tical machine translation. In Proceedings of the 41rd
Annual Meeting of the Association for Computational
Linguistics (ACL-2003), Sapporo, Japan.
Adam Pauls and Dan Klein. 2011. Faster and smaller n-
gram language models. In Proceedings of ACL, pages
258?267, Portland, Oregon, USA, June. Association
for Computational Linguistics.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Seventh International Conference
on Spoken Language Processing.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the Thrax
grammar extractor. In Proceedings of WMT11.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Richard Zens and Hermann Ney. 2007. Efficient phrase-
table representation for machine translation with appli-
cations to online MT and speech translation. In Pro-
ceedings of HLT/NAACL, pages 492?499, Rochester,
New York, April. Association for Computational Lin-
guistics.
291
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 401?409,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Constructing Parallel Corpora for Six Indian Languages via Crowdsourcing
Matt Post? and Chris Callison-Burch?? and Miles Osborne?
?Human Langage Technology Center of Excellence, Johns Hopkins University
?Center for Language and Speech Processing, Johns Hopkins University
?School of Informatics, University of Edinburgh
Abstract
Recent work has established the efficacy of
Amazon?s Mechanical Turk for constructing
parallel corpora for machine translation re-
search. We apply this to building a collec-
tion of parallel corpora between English and
six languages from the Indian subcontinent:
Bengali, Hindi, Malayalam, Tamil, Telugu,
and Urdu. These languages are low-resource,
under-studied, and exhibit linguistic phenom-
ena that are difficult for machine translation.
We conduct a variety of baseline experiments
and analysis, and release the data to the com-
munity.
1 Introduction
The quality of statistical machine translation (MT)
systems is strongly related to the amount of paral-
lel text available for the language pairs. However,
most language pairs have little or no readily available
bilingual training data available. As a result, most
contemporary MT research tends to opportunisti-
cally focus on language pairs with large amounts of
parallel data.
A consequence of this bias is that language ex-
hibiting certain linguistic phenomena are underrep-
resented, including languages with complex mor-
phology and languages with divergent word order-
ings. In this paper, we describe our work gather-
ing and refining document-level parallel corpora be-
tween English and each of six verb-final languages
spoken on the Indian subcontinent: Bengali, Hindi,
Malayalam, Tamil, Telugu, and Urdu. This paper?s
contributions are as follows:
? We apply an established protocol for using
Amazon?s Mechanical Turk (MTurk) to collect
parallel data to train and evaluate translation
systems for six Indian languages.
? We investigate the relative performance of syn-
tactic translation models over hierarchical ones,
showing that syntax results in higher BLEU
scores in most cases.
? We explore the impact of training data quality
on the quality of the resulting model.
? We release the corpora to the research commu-
nity under the Creative Commons Attribution-
Sharealike 3.0 Unported License (CC BY-SA
3.0).1
2 Why Indian languages?
Indian languages are important objects of study for
a number of reasons. These languages are low-
resource languages in terms of the availability of
MT systems2 (and NLP tools in general) yet together
they represent nearly half a billion native speakers
(Table 1). Their speakers are well-educated, with
many of them speaking English either natively or as a
second language. Together with the degree of Inter-
net penetration in India, it is reasonably straightfor-
ward to find and hire non-expert translators through
crowdsourcing services like Amazon?s Mechanical
Turk.
1joshua-decoder.org/indian-parallel-corpora
2See sampark.iiit.ac.in/sampark/web/index.php/
content for a notable growing effort.
401
??????? ???? ???????????? ?????
senator her remarks prepared
Figure 1: An example of SOV word ordering in Tamil.
Translation: The senator prepared her remarks.
??? ?? ? ??
walk CONT PAST 1p
Figure 2: An example of the morphology of the Bengali
word ?????????, meaning [I] was walking. CONT denotes
the continuous aspect, while PAST denotes past tense.
In addition to a general desire to collect suitable
training corpora for low-resource languages, Indian
languages demonstrate a variety of linguistic phe-
nomena that are divergent from English and under-
studied. One example is head-finalness, exhibited
most obviously in a subject-object-verb (SOV) pat-
tern of sentence structure, in contrast to the gen-
eral SVO ordering of English sentences. One of
the motivations underlying linguistically-motivated
syntactic translation systems like GHKM (Galley et
al., 2004; Galley et al, 2006) or SAMT (Zollmann
and Venugopal, 2006) is to describe such transfor-
mations. This difference in word order has the po-
tential to serve as a better test bed for syntax-based
MT3 compared to translating between English and
European languages, most of which largely share its
word order. Figure 1 contains an example of SOV
reordering in Tamil.
A second important phenomenon present in these
languages is a high degree of morphological com-
plexity relative to English (Figure 2). Indian lan-
guages can be highly agglutinative, which means
that words are formed by concatenating morpholog-
ical affixes that convey information such as tense,
person, number, gender, mood, and voice. Mor-
phological complexity is a considerable hindrance at
all stages of the MT pipeline, but particularly align-
ment, where inflectional variations mask patterns
from alignment tools that treat words as atoms.
3Weuse hierarchical to denote translation grammars that use
only a single nonterminal (Chiang, 2007), in contrast to syntac-
tic systems, which make use of linguistic annotations (Zollmann
and Venugopal, 2006; Galley et al, 2006).
language script family L1
Bengali ????? Indo-Aryan 181M
Hindi ???? ?????? Indo-Aryan 180M
Malayalam ?????? Dravidian 35M
Tamil ????? Dravidian 65M
Telugu ?????? Dravidian 69M
Urdu ???? Indo-Aryan 60M
Table 1: Languages. L1 is the worldwide number of na-
tive speakers according to Lewis (2009).
3 Data collection
The source of the documents for our translation task
for each of the languages in Table 1 was the set of
the top-100 most-viewed documents from each lan-
guage?s Wikipedia. These lists were obtained us-
ing page view statistics compiled from dammit.lt/
wikistats over a one year period. We did not apply
any filtering for topic or content. Table 2 contains
a manually categorized list of documents for Hindi,
with some minimal annotations indicating how the
documents relate to those in the other languages.
These documents constitute a diverse set of topics,
including culture, the internet, and sex.
We collected the parallel corpora using a three-
step process designed to ensure the integrity of the
non-professional translations. The first step was to
build a bilingual dictionary (?3.1). These dictionar-
ies were used to bootstrap the experimental controls
in the collection of four translations of each source
sentence (?3.2). Finally, as a measure of data qual-
ity, we independently collect votes on the which of
the four redundant translations is the best (?3.3).
3.1 Dictionaries
A key component of managing MTurk workers is to
ensure that they are competently and conscientiously
undertaking the tasks. As non-speakers of all of the
Indian languages, we had no simple and scalable way
to judge the quality of the workers? translations. Our
solutionwas to bootstrap the process by first building
bilingual dictionaries for each of the datasets. The
dictionaries were then used to produce glosses of the
complete source sentences, which we compared to
the translations produced by the workers as a rough
means of manually gauging trust (?3.2).
The dictionaries were built in a separate MTurk
402
PLACES PEOPLE PEOPLE TECHNOLOGY LANGUAGE AND RELIGION
Agra A. P. J. Abdul Kalam Premchand Blog CULTURE Bhagavad Gita
Bihar Aishwarya Rai Rabindranath Tagore Google Ayurveda Diwali
China Akbar Rani Lakshmibai Hindi Web Resources Constitution of India Hanuman
Delhi Amitabh Bachchan Sachin Tendulkar Internet Cricket Hinduism
Himalayas Barack Obama Sarojini Naidu Mobile phone English language Hinduism
India Bhagat Singh Subhas Chandra Bose News aggregator Hindi Cable News Holi
Mumbai Dainik Jagran Surdas RSS Hindi literature Islam
Nepal Gautama Buddha Swami Vivekananda Wikipedia Hindi-Urdu grammar Mahabharata
Pakistan Harivansh Rai Bachchan Tulsidas YouTube Horoscope Puranas
Rajasthan Indira Gandhi Indian cuisine Quran
Red Fort Jaishankar Prasad THINGS SEX Sanskrit Ramayana
Taj Mahal Jawaharlal Nehru Air pollution Anal sex Standard Hindi Shiva
United States Kabir Earth Kama Sutra Shiva
Uttar Pradesh Kalpana Chawla Essay Masturbation EVENTS Taj Majal: Shiva Temple?
Mahadevi Varma Ganges Penis History of India Vedas
Meera General knowledge Sex positions World War II Vishnu
Mohammed Rafi Global warming Sexual intercourse
Mohandas Karamchand Gandhi Pollution Vagina
Mother Teresa Solar energy
Navbharat Times Terrorism
Table 2: The 100 most viewed Hindi Wikipedia articles (titles translated to English using inter-language links and
Google translate and manually categorized). Entries in bold were present in the top 100 lists of at least four of the
Indian top 100 lists. Earth, India,World War II, and Wikipedia were in the top 100 lists of all six languages.
language entries translations
Bengali 4,075 6,011
Hindi - -
Malayalam 41,502 144,505
Tamil 11,592 69,128
Telugu 12,193 38,532
Urdu 26,363 113,911
Table 3: Dictionary statistics. Entries is the number of
source-language types, while translations lists the num-
ber of words or phrases they translated to (i.e., the num-
ber of pairs in the dictionary). Controls for Hindi were
obtained using Google translate, the only one of these lan-
guages that were available at the outset of this project.
task, in which workers were asked to translate sin-
gle words and short phrases from the complete set of
Wikipedia documents. For each word, MTurk work-
ers were presented with three sentences containing
that word, which provided context. The control for
this task was obtained from the Wikipedia article ti-
tles which are linked across languages, and can thus
be assumed to be translations of each other. Workers
who performed too poorly on these known transla-
tions had their work rejected.
Table 3 lists the size of the dictionaries we con-
structed.
3.2 Translations
With the dictionaries in hand, we moved on to trans-
late the entireWikipedia documents. Each human in-
telligence task (HIT) posted onMTurk contained ten
sequential source-language sentences from a doc-
ument, and asked the worker to enter a free-form
translation for each. We collected four translations
from different translators for each source sentence.
To discourage cheating through cutting-and-pasting
into automatic translation systems, sentences were
presented as images. Workers were paid $0.70 per
HIT. We then manually determined whether to ac-
cept or reject a worker?s HITs based on a review of
each worker?s submissions, which included a com-
parison of the translations to a monotonic gloss (pro-
duced with the dictionary), the percentage of empty
translations, the amount of time the worker took to
complete the HIT, geographic location (self-reported
and geolocated by way of the worker?s IP address),
and by comparing different translations of the same
source segments against one another.
We obtained translations of the source-language
documents in a relatively short amount of time. Fig-
ure 3 depicts the number of translations collected as
a function of the amount of time from the posting of
the task. Malayalam provided the highest through-
put, generating half a million words in just under a
403
320 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30
800,000
0
100,000
200,000
300,000
400,000
500,000
600,000
700,000
Malay
alam
Tamil
Telugu
Hindi
Urdu
Bengali
Figure 3: The total volume of translations (measured in
English words) as a function of elapsed days. For Malay-
alam, we collected half a million words of translations in
just under a week.
week. For comparison, the Europarl corpus (Koehn,
2005) has about 50million words of English for each
of the Spanish and French parallel corpora.
As has been previously reported (Zbib et al,
2012), cost is another advantage of building train-
ing data on Mechanical Turk. Germann (2001) puts
the cost of professionally translated English at about
$0.30 perword for translation fromTamil. Our trans-
lations were obtained for less than $0.01 per word.
The rate of collection could likely be increased by
raising these payments, but it is unclear whether
quality would be affected by raising the base pay
(although it could be improved by paying for sub-
sequent quality control HITs, like editing).
The tradeoff for low-cost translations is increased
variance in translation quality when compared to the
more consistently-good professional translations.
Figure 4 contains some hand-picked examples of the
sorts of translations we obtained. Later, in the Exper-
iments section (?4), we will investigate the effects
this variance in translation quality has on the qual-
ity of the models that can be constructed. For now,
the variancemotivated the collection of an additional
dataset, described in the next section.
3.3 Votes
A prevailing issue with translations collected on
MTurk is the prevalence of low-quality translations.
Quality suffers for a variety of reasons: Turkers
lack formal training, often translate into a nonna-
tive tongue, may give insufficient attention to the
task, and likely desire to maximize their throughput
(and thus their wage). Unlike Zaidan and Callison-
Burch (2011), who embed controls containing source
language sentences with known professional trans-
lations, we had no professionally translated data.
Therefore, we could not measure the BLEU score of
the Turkers.
Motivated by desire to have some measure of the
relative quality and variance of the translations, we
designed another task in which we presented an in-
dependent set of Turkers with an original sentence
and its four translations, and asked them to vote on
which was best.4 Five independent workers voted
on the translations of each source sentence. Tallying
the resulting votes, we found that roughly 65% of
the sentences had five votes cast on just one or two
of the translations, and about 95% of the sentences
had all the votes cast on one, two, or three sentences.
This suggests both (1) that there was a difference in
the quality of the translations, and (2) the voters were
able to discern these differences, and took their task
seriously enough to report them.
3.4 Data sets
For each parallel corpus, we created a standardized
test set in the following manner. We first manu-
ally assigned each of the Wikipedia documents for
each language into one of the following nine cate-
gories: EVENTS, LANGUAGE AND CULTURE,
PEOPLE, PLACES, RELIGION, SEX, TECHNOL-
OGY, THINGS, or MISC. We then assigned doc-
uments to training, development, development test,
and test sets in round-robin fashion using a ratio of
roughly 7:1:1:1. For training data, each source sen-
tence was repeated four times in order to allow it
to be paired with each of its translations. For the
development and test sets, the multiple translations
served as alternate references. Table 4 lists sentence-
and word-level statistics for the datasets for each lan-
guage pair (these counts are prior to any tokeniza-
tion).
4We did not collect votes for Malayalam.
404
?????? 15,2007??? ???????????? ?????? ?????? ???? ?????? ???????????.
In March 15,2007 Wiki got a place in Oxford English dictionary.
On March 15, 2007 wiki was included in the Oxford English dictionary. (5)
ON MARCH 15, 2007, WIKI FOUND A PLACE IN THE OXFORD ENGLISH DICTIONARY
March 15, 2007 oxford english index of wiki?s place.
Figure 4: An example of the variance in translation quality for the human translations of a Tamil sentence; the format-
ting of the translations has been preserved exactly. The parenthesized number indicates the number of votes received
in the voting task (?3.3).
language dict train dev devtest test
Bengali 16k 539k 63k 61k 69k
6k 20k 914 907 1k
Hindi 0 1,249k 67k 98k 74k
0 37k 1k 993 1k
Malayalam 410k 664k 61k 68k 70k
144k 29k 1k 1k 1k
Tamil 189k 747k 62k 53k 54k
69k 35k 1k 1k 1k
Telugu 106k 951k 52k 45k 49k
38k 43k 1k 916 1k
Urdu 253k 1,198k 67k 49k 42k
113k 33k 736 777 605
Table 4: Data set sizes for each language pair: words in
the first row, parallel sentences in the second. (The dictio-
naries contains short phrases in addition to words, which
accounts for the difference in dictionary word and line
counts.)
4 Experiments
In this section, we present experiments on the col-
lected data sets in order to quantify their perfor-
mance. The experiments aim to address the follow-
ing questions:
1. How well can we translate the test sets?
2. Do linguistically motivated translation models
improve translation results?
3. What is the effect of data quality onmodel qual-
ity?
4.1 Setup
A principal point of comparison in this paper is be-
tween Hiero grammars (Chiang, 2007) and SAMT
grammars (Zollmann and Venugopal, 2006), the lat-
ter of which make use of linguistic annotations to
improve nonterminal reordering. These grammars
were trained with the Thrax grammar extractor us-
ing its default settings, and translated using Joshua
(Weese et al, 2011). We tuned with minimum error-
rate training (Och, 2003) using Z-MERT (Zaidan,
2009) and present the mean BLEU score on test
data over three separate runs (Clark et al, 2011).
MBR reranking (Kumar and Byrne, 2004) was ap-
plied to Joshua?s 300-best (unique) output, and eval-
uation was conducted with case-insensitive BLEU
with four references.
The training data was produced by pairing a
source sentence with each of its four translations.
We also added the dictionaries to the training data.
We built five-gram language models from the target
side of the training data using interpolated Kneser-
Ney smoothing. We also experimented with a larger-
scale language model built from English Gigaword,
but, notably, found a drop of over a point in BLEU
score. This points forward to some of the difficul-
ties encountered with the lack of text normalization,
discussed in ?5.
4.2 Baseline translations
We begin by presenting BLEU scores for Hiero and
SAMT translations of each of the six Indian language
test sets (Table 5). For comparison purposes, we
also present BLEU scores from Google translations
of these languages (where available).
We observe that systems built with SAMT gram-
mars improve measurably above the Hiero models,
with the exception of Tamil and Telugu. As an ex-
ternal reference point, the Google baseline transla-
tion scores far surpass the results of any of our sys-
tems, but were likely constructed from much larger
datasets.
Table 6 lists some manually-selected examples of
405
language Hiero SAMT diff Google
Bengali 12.72 13.53 +0.81 20.01
Hindi 15.53 17.29 +1.76 25.21
Malayalam 13.72 14.28 +0.56 -
Tamil 9.81 9.85 +0.04 13.51
Telugu 12.46 12.61 +0.15 16.03
Urdu 19.53 20.99 +1.46 23.09
Table 5: BLEU scores translating into English (four ref-
erences). BLEU scores are the mean of three MERT runs.
the sorts of translations we obtained from our sys-
tems. While anecdotal and not characteristic of over-
all quality, together with the generally good BLEU
scores, these examples provide a measure of the abil-
ity to obtain good translations from this dataset.
4.3 Voted training data
We noted above the high variance in the quality of
the translations obtained on MTurk. For data col-
lection efforts, there is a question of how much time
and effort to invest in quality control, since it comes
at the expense of simply collecting more data. We
can either collect additional redundant translations
(to increase quality) or translate more foreign sen-
tences (to increase coverage).
To test this, we constructed two smaller datasets,
each making use of only one of the four translations
of each source sentence:
? Selected randomly
? Selected by choosing the translation that re-
ceived a plurality of the votes (?3.3), breaking
ties randomly (best)
We again included the dictionaries in the training
data (where available). Table 7 contains results on
the same test sets as before. These results do not
clearly indicate that quality control through redun-
dant translations are worth the extra expense. Novot-
ney and Callison-Burch (2010) had a similar finding
for crowdsourced transcriptions.
5 Further Analysis
The previous section has shown that reasonable
BLEU scores can be obtained from baseline transla-
tion systems built from these corpora. While trans-
lation quality is an issue (for example, very lit-
?????????? ????? ?????
in srilanka solar government
chola rule in sri lanka
in srilanka chozhas ruled
chola reign in sri lanka
Figure 5: An example of inconsistent orthography. Words
in bold are translations of the second Tamil word.
eral translations, etc), the previous section?s voted
dataset experiments suggest this is not one of the
most important issues to address.
In this section, we undertake a manual analysis of
the collected datasets to inform future work. There
are a number of issues that arise due to non-Roman
scripts, high-variance translation quality, and the rel-
atively small amount of training data.
5.1 Orthographic issues
Manual analysis demonstrates that inconsistencies
with orthography are a serious problem. An exam-
ple of this can be found in Figure 5, which contains
a set of translations of a Tamil sentence. In particu-
lar, the spelling of the Tamil word ????? has three
different realizations among the sentence?s transla-
tions. The discrepancy between zha and la is due
to phonetic variants (phonetic similarity may also
account for the word solar). This discrepancy is
present throughout the training and test data, where
the -la variant is preferred to -zha by about 6:1 (the
counts are 848 and 142, respectively).
In addition to mistakes potentially caused by for-
eign scripts, there are many mistakes that are sim-
ply spelling errors. Table 8 contains examples of
misspellings (along with their counts) in the train-
ing portion of the Urdu-English dataset. As a point
of comparison, there are no misspellings of the word
in Europarl.
Such errors are present in many collections, of
course, but they are particularly harmful in small
datasets, and they appear to be especially prevalent
in datasets like these, translated as they were by non-
native speakers. Whether caused by Turker care-
lessness or difficulty in translation from non-Roman
scripts, these are common issues, solutions for which
could yield significant improvement in translation
performance.
406
Bengali ?? ????? ???? ???? ???? ?????????????? ??????? ??? ?
Hiero in this time dhaka university was established on the year 1921 .
SAMT in this time dhaka university was established in 1921 .
Malayalam ????????? ???????????? ?????????? ? ?????? 5 , 700 ?k ?????? ??????????????? .
Hiero the surface temperature of sun 5 , 700 degree k to down to .
SAMT temperature in the surface of the sun 5 , 700 degree k to down to .
Table 6: Some example translations.
Hiero SAMT
language random best random best
Bengali 9.43 9.29 9.65 9.50
Hindi 11.74 12.18 12.61 12.69
Tamil 7.73 7.48 7.88 7.76
Telugu 10.49 10.61 10.75 10.72
Urdu 13.51 14.26 14.63 16.03
Table 7: BLEU scores translating into English on a quar-
ter of the training data (plus dictionary), selected in two
ways: best (result of vote), and random. There is little
difference, suggesting quality control may not be terribly
important. We did not collect votes for Malayalam.
misspelling count
japenese 91
japans 40
japenes 9
japenies 3
japeneses 3
japeneese 1
japense 1
Table 8: Misspellings of japanese (947) in the training
portion of the Urdu-English data, along with their counts.
5.2 Alignments
Inconsistent orthography fragments the training
data, exacerbating problems already present due to
morpohological richness. One place this is mani-
fested is during alignment, where different spellings
mask patterns from the standard alignment tech-
niques. We observe a large number of poor align-
ments, due to interactions among these problems,
as well as the small size of the training data, well-
documented alignment mistakes (such as garbage
collecting), and the divergent sentence structures. In
particular, it seems that the defacto alignment heuris-
tics may be particularly ill-suited to these language
pairs and data conditions. Figure 6 (top) contains an
example of a particularly poor alignment produced
by the default alignment heuristic, the grow-diag-
and method described in Koehn et al (2003).
As a means of testing this, we varied the align-
ment combination heuristics using five alternatives
described in Koehn et al (2003) and available in the
symal program distributed with Moses (Koehn et
al., 2007). Experiments on Tamil produce a range
of BLEU scores between 7.45 and 10.19 (each result
is the average of three MERT runs). If we plot gram-
mar size versus BLEU score, we observe a general
trend that larger grammars seem to positively cor-
relate with BLEU score. We tested this more gen-
erally across languages using the Berkeley aligner5
(Liang et al, 2006) instead of GIZA alignments, and
found a consistent increase in BLEU score for the
Hiero grammars, often putting them on par with the
original SAMT results (Table 9). Manual analysis
suggests that the Berkeley aligner produces fewer,
more reasonable-looking alignments than the Moses
heuristics (Figure 6). This suggest a fruitful ap-
proaches in revisiting assumptions underlying align-
ment heuristics.
6 Related Work
Crowdsourcing datasets has been found to be helpful
for many tasks in natural language processing. Ger-
mann (2001) showed that humans could perform sur-
prisingly well with very poor translations obtained
from non-expert translators, in part likely because
coarse-level translational adequacy is sufficient for
the tasks they evaluated. That work was also pitched
as a rapid resource acquisition task, meant to test our
ability to quickly build systems in emergency set-
tings. This work further demonstrates the ability to
quickly acquire training data for MT systems with
5code.google.com/p/berkeleyaligner/
407
X?
X X
?
X X
?
X
?
X X
X
?
?
X
?
?"#
$??'(
)?+
??./0
??3
???
.
a
a
s
a
i
w
a
s
t
h
e
f
i
r
s
t
s
u
c
c
e
s
s
f
u
l
l
m
o
v
i
e
f
o
r
a
j
i
t
h
k
u
m
a
r
.
?
X
?
?
?
?
?
X X
?
a
a
s
a
i
w
a
s
t
h
e
f
i
r
s
t
s
u
c
c
e
s
s
f
u
l
l
m
o
v
i
e
f
o
r
a
j
i
t
h
k
u
m
a
r
.
?"#
$??'(
)?+
??./0
??3
???
.
Figure 6: A bad Tamil alignment produced with the
grow-diag-and alignment combination heuristic (top); the
Berkeley aligner is better (bottom). A ? is a correct
guess, an X marks a false positive, and a ? denotes a false
negative. Hiero?s extraction heuristics yield 4 rules for
the top alignment and 16 for the bottom.
reasonable translation accuracy.
Closely related to our work here is that of Novot-
ney and Callison-Burch (2010), who showed that
transcriptions for training speech recognition sys-
tems could be obtained from Mechanical Turk with
near baseline recognition performance and at a sig-
nificantly lower cost. They also showed that redun-
dant annotation was not worthwhile, and suggested
that money was better spent obtaining more data.
Separately, Ambati and Vogel (2010) probed the
MTurk worker pool for workers capable of translat-
ing a number of low-resource languages, including
Hindi, Telugu, and Urdu, demonstrating that such
workers could be found and quantifying acceptable
grammar size
pair GIZA++ Berkeley BLEU gain
Bengali 15m 27m 13.54 +0.82
Hindi 34m 60m 16.47 +0.94
Malayalam 12m 27m 12.70 -1.02
Tamil 19m 30m 10.10 +0.29
Telugu 28m 46m 13.36 +0.90
Urdu 38m 58m 20.41 +0.88
Table 9: Hiero translation results using Berkeley align-
ments instead of GIZA++ heuristics. The gain columns
denotes improvements relative to the Hiero systems in Ta-
ble 5. In many cases (bold gains), the BLEU scores are
at or above even the SAMT models from that table.
wages and collection rates.
The techniques described here are similar to those
described in Zaidan and Callison-Burch (2011), who
showed that crowdsourcing with appropriate quality
controls could be used to produce professional-level
translations for Urdu-English translation. This pa-
per extends that work by applying their techniques
to a larger set of Indian languages and scaling it to
training-data-set sizes.
7 Summary
We have described the collection of six parallel cor-
pora containing four-way redundant translations of
the source-language text. The Indian languages of
these corpora are low-resource and understudied,
and exhibit markedly different linguistic properties
compared to English. We performed baseline exper-
iments quantifying the translation performance of a
number of systems, investigated the effect of data
quality on model quality, and suggested a number of
approaches that could improve the quality of models
constructed from the datasets. The parallel corpora
provide a suite of SOV languages for translation re-
search and experiments.
Acknowledgments We thank Lexi Birch for dis-
cussions about strategies for selecting and assem-
bling the data sets. This research was supported in
part by gifts from Google and Microsoft, the Euro-
MatrixPlus project funded by the EuropeanCommis-
sion (7th Framework Programme), and a DARPA
grant entitled ?Crowdsourcing Translation?. The
views in this paper are the authors? alone.
408
References
Vamshi Ambati and Stephan Vogel. 2010. Can crowds
build parallel corpora for machine translation systems?
In Proceedings of the NAACL HLT 2010 Workshop on
Creating Speech and Language Data with Amazon?s
Mechanical Turk, Los Angeles, California.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.
Smith. 2011. Better hypothesis testing for statistical
machine translation: Controlling for optimizer insta-
bility. In ACL, pages 176?181. Association for Com-
putational Linguistics.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In Proc.
NAACL, Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL, Sydney, Australia, July.
Ulrich Germann. 2001. Building a statistical ma-
chine translation system from scratch: how much
bang for the buck can we expect? In ACL work-
shop on Data-driven methods in machine translation,
Toulouse, France, July. Association for Computational
Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
NAACL, Edmonton, Alberta, Canada, May?June.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ond?ej Bojar, Alexandra Constantin,
and Evan Herbst. 2007. Moses: Open source toolkit
for statistical machine translation. In Proceedings of
the Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, Prague, Czech Republic,
June.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. InMachine translation
summit.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine translation.
In Proc. NAACL, Boston, Massachusetts, USA, May.
M. Paul Lewis, editor. 2009. Ethnologue: Languages of
the World. SIL International, Dallas, TX, USA, six-
teenth edition.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by agreement. In HLT-NAACL, pages 104?111.
Association for Computational Linguistics.
Scott Novotney and Chris Callison-Burch. 2010. Cheap,
fast and good enough: Automatic speech recognition
with non-expert transcription. In Proc. NAACL, Los
Angeles, California, June.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL, Sapporo,
Japan, July.
JonathanWeese, Juri Ganitkevitch, Chris Callison-Burch,
Matt Post, and Adam Lopez. 2011. Joshua 3.0:
Syntax-based machine translation with the thrax gram-
mar extractor. InProceedings of the SixthWorkshop on
Statistical Machine Translation.
Omar F. Zaidan and Chris Callison-Burch. 2011. Crowd-
sourcing translation: professional quality from non-
professionals. In Proc. ACL, Portland, Oregon, USA,
June.
Omar F. Zaidan. 2009. Z-MERT: A fully configurable
open source tool for minimum error rate training of
machine translation systems. The Prague Bulletin of
Mathematical Linguistics, 91:79?88.
Rabih Zbib, Erika Malchiodi, Jacob Devlin, David
Stallard, Spyros Matsoukas, Richard Schwartz, John
Makhoul, Omar F. Zaidan, and Chris Callison-Burch.
2012. Machine translation of arabic dialects. In Proc.
NAACL, Montreal, June.
Andreas Zollmann and Ashish Venugopal. 2006. Syntax
augmented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, New York, New York, USA, June.
409
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Findings of the 2013 Workshop on Statistical Machine Translation
Ondr?ej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Chris Callison-Burch
University of Pennsylvania
Christian Federmann
Saarland University
Barry Haddow
University of Edinburgh
Philipp Koehn
University of Edinburgh
Christof Monz
University of Amsterdam
Matt Post
Johns Hopkins University
Radu Soricut
Google
Lucia Specia
University of Sheffield
Abstract
We present the results of the WMT13
shared tasks, which included a translation
task, a task for run-time estimation of ma-
chine translation quality, and an unoffi-
cial metrics task. This year, 143 machine
translation systems were submitted to the
ten translation tasks from 23 institutions.
An additional 6 anonymized systems were
included, and were then evaluated both au-
tomatically and manually, in our largest
manual evaluation to date. The quality es-
timation task had four subtasks, with a to-
tal of 14 teams, submitting 55 entries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2013. This workshop builds
on seven previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al, 2007, 2008,
2009, 2010, 2011, 2012).
This year we conducted three official tasks: a
translation task, a human evaluation of transla-
tion results, and a quality estimation task.1 In
the translation task (?2), participants were asked
to translate a shared test set, optionally restrict-
ing themselves to the provided training data. We
held ten translation tasks this year, between En-
glish and each of Czech, French, German, Span-
ish, and Russian. The Russian translation tasks
were new this year, and were also the most popu-
lar. The system outputs for each task were evalu-
ated both automatically and manually.
The human evaluation task (?3) involves ask-
ing human judges to rank sentences output by
anonymized systems. We obtained large numbers
of rankings from two groups: researchers (who
1The traditional metrics task is evaluated in a separate pa-
per (Macha?c?ek and Bojar, 2013).
contributed evaluations proportional to the number
of tasks they entered) and workers on Amazon?s
Mechanical Turk (who were paid). This year?s ef-
fort was our largest yet by a wide margin; we man-
aged to collect an order of magnitude more judg-
ments than in the past, allowing us to achieve sta-
tistical significance on the majority of the pairwise
system rankings. This year, we are also clustering
the systems according to these significance results,
instead of presenting a total ordering over systems.
The focus of the quality estimation task (?6)
is to produce real-time estimates of sentence- or
word-level machine translation quality. This task
has potential usefulness in a range of settings, such
as prioritizing output for human post-editing, or
selecting the best translations from a number of
systems. This year the following subtasks were
proposed: prediction of percentage of word edits
necessary to fix a sentence, ranking of up to five al-
ternative translations for a given source sentence,
prediction of post-editing time for a sentence, and
prediction of word-level scores for a given trans-
lation (correct/incorrect and types of edits). The
datasets included English-Spanish and German-
English news translations produced by a number
of machine translation systems. This marks the
second year we have conducted this task.
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation methodologies for machine trans-
lation. As before, all of the data, translations,
and collected human judgments are publicly avail-
able.2 We hope these datasets serve as a valu-
able resource for research into statistical machine
translation, system combination, and automatic
evaluation or prediction of translation quality.
2http://statmt.org/wmt13/results.html
1
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and five other lan-
guages: German, Spanish, French, Czech, and ?
new this year ? Russian. We created a test set for
each language pair by translating newspaper arti-
cles and provided training data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources. A total of 52
articles were selected, in roughly equal amounts
from a variety of Czech, English, French, German,
Spanish, and Russian news sites:3
Czech: aktua?lne?.cz (1), CTK (1), den??k (1),
iDNES.cz (3), lidovky.cz (1), Novinky.cz (2)
French: Cyber Presse (3), Le Devoir (1), Le
Monde (3), Liberation (2)
Spanish: ABC.es (2), BBC Spanish (1), El Peri-
odico (1), Milenio (3), Noroeste (1), Primera
Hora (3)
English: BBC (2), CNN (2), Economist (1),
Guardian (1), New York Times (2), The Tele-
graph (1)
German: Der Standard (1), Deutsche Welle (1),
FAZ (1), Frankfurter Rundschau (2), Welt (2)
Russian: AIF (2), BBC Russian (2), Izvestiya (1),
Rosbalt (1), Vesti (1)
The stories were translated by the professional
translation agency Capita, funded by the EU
Framework Programme 7 project MosesCore, and
by Yandex, a Russian search engine.4 All of the
translations were done directly, and not via an in-
termediate language.
2.2 Training data
As in past years we provided parallel corpora to
train translation models, monolingual corpora to
train language models, and development sets to
tune system parameters. Some training corpora
were identical from last year (Europarl5, United
Nations, French-English 109 corpus, CzEng),
some were updated (News Commentary, mono-
lingual data), and new corpora were added (Com-
mon Crawl (Smith et al, 2013), Russian-English
3For more details see the XML test files. The docid tag
gives the source and the date for each document in the test set,
and the origlang tag indicates the original source language.
4http://www.yandex.com/
5As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
parallel data provided by Yandex, Russian-English
Wikipedia Headlines provided by CMU).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their en-
try names are listed in Table 1; each system did
not necessarily appear in all translation tasks. We
also included three commercial off-the-shelf MT
systems and three online statistical MT systems,6
which we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
We run the evaluation campaign using an up-
dated version of Appraise (Federmann, 2012); the
tool has been extended to support collecting judg-
ments using Amazon?s Mechanical Turk, replac-
ing the annotation system used in previous WMTs.
The software, including all changes made for this
year?s workshop, is available from GitHub.7
This year differs from prior years in a few im-
portant ways:
? We collected about ten times more judgments
that we have in the past, using judgments
from both participants in the shared task and
non-experts hired on Amazon?s Mechanical
Turk.
? Instead of presenting a total ordering of sys-
tems for each pair, we cluster them and report
a ranking over the clusters.
6Thanks to Herve? Saint-Amand and Martin Popel for har-
vesting these entries.
7https://github.com/cfedermann/Appraise
2
Europarl Parallel Corpus
Spanish? English French? English German? English Czech? English
Sentences 1,965,734 2,007,723 1,920,209 646,605
Words 56,895,229 54,420,026 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 176,258 117,481 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 174,441 157,168 178,221 140,324 150,217
Words 5,116,388 4,520,796 4,928,135 4,066,721 4,597,904 4,541,058 3,206,423 3,507,249 3,841,950 4,008,949
Distinct words 84,273 61,693 69,028 58,295 142,461 61,761 138,991 54,270 145,997 57,991
Common Crawl Parallel Corpus
Spanish? English French? English German? English Czech? English Russian? English
Sentences 1,845,286 3,244,152 2,399,123 161,838 878,386
Words 49,561,060 46,861,758 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 710,755 640,778 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
Spanish? English French? English
Sentences 11,196,913 12,886,831
Words 318,788,686 365,127,098 411,916,781 360,341,450
Distinct words 593,567 581,339 565,553 666,077
109 Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English
Sentences 514,859
Words 1,191,474 1,230,644
Distinct words 282,989 251,328
Europarl Language Model Data
English Spanish French German Czech
Sentence 2,218,201 2,123,835 2,190,579 2,176,537 668,595
Words 59,848,044 60,476,282 63,439,791 53,534,167 14,946,399
Distinct words 123,059 181,837 145,496 394,781 172,461
News Language Model Data
English Spanish French German Czech Russian
Sentence 68,521,621 13,384,314 21,195,476 54,619,789 27,540,749 19,912,911
Words 1,613,778,461 386,014,234 524,541,570 983,818,841 456,271,247 351,595,790
Distinct words 3,392,137 1,163,825 1,590,187 6,814,953 2,655,813 2,195,112
News Test Set
English Spanish French German Czech Russian
Sentences 3000
Words 64,810 73,659 73,659 63,412 57,050 58,327
Distinct words 8,935 10,601 11,441 12,189 15,324 15,736
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
3
ID Institution
BALAGUR Yandex School of Data Analysis (Borisov et al, 2013)
CMU
CMU-TREE-TO-TREE
Carnegie Mellon University (Ammar et al, 2013)
CU-BOJAR,
CU-DEPFIX,
CU-TAMCHYNA
Charles University in Prague (Bojar et al, 2013)
CU-KAREL, CU-ZEMAN Charles University in Prague (B??lek and Zeman, 2013)
CU-PHRASEFIX,
CU-TECTOMT
Charles University in Prague (Galus?c?a?kova? et al, 2013)
DCU Dublin City University (Rubino et al, 2013a)
DCU-FDA Dublin City University (Bicici, 2013a)
DCU-OKITA Dublin City University (Okita et al, 2013)
DESRT Universita` di Pisa (Miceli Barone and Attardi, 2013)
ITS-LATL University of Geneva
JHU Johns Hopkins University (Post et al, 2013)
KIT Karlsruhe Institute of Technology (Cho et al, 2013)
LIA Universite? d?Avignon (Huet et al, 2013)
LIMSI LIMSI (Allauzen et al, 2013)
MES-* Munich / Edinburgh / Stuttgart (Durrani et al, 2013a; Weller et al, 2013)
OMNIFLUENT SAIC (Matusov and Leusch, 2013)
PROMT PROMT Automated Translations Solutions
QCRI-MES Qatar / Munich / Edinburgh / Stuttgart (Sajjad et al, 2013)
QUAERO QUAERO (Peitz et al, 2013a)
RWTH RWTH Aachen (Peitz et al, 2013b)
SHEF University of Sheffield
STANFORD Stanford University (Green et al, 2013)
TALP-UPC TALP Research Centre (Formiga et al, 2013a)
TUBITAK TU?BI?TAK-BI?LGEM (Durgar El-Kahlout and Mermer, 2013)
UCAM University of Cambridge (Pino et al, 2013)
UEDIN,
UEDIN-HEAFIELD
University of Edinburgh (Durrani et al, 2013b)
UEDIN-SYNTAX University of Edinburgh (Nadejde et al, 2013)
UMD University of Maryland (Eidelman et al, 2013)
UU Uppsala University (Stymne et al, 2013)
COMMERCIAL-1,2,3 Anonymized commercial systems
ONLINE-A,B,G Anonymized online systems
Table 1: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
4
3.1 Ranking translations of sentences
The ranking among systems is produced by col-
lecting a large number of rankings between the
systems? translations. Every language task had
many participating systems (the largest was 19,
for the Russian-English task). Rather than asking
judges to provide a complete ordering over all the
translations of a source segment, we instead ran-
domly select five systems and ask the judge to rank
just those. We call each of these a ranking task.
A screenshot of the ranking interface is shown in
Figure 2.
For each ranking task, the judge is presented
with a source segment, a reference translation,
and the outputs of five systems (anonymized and
randomly-ordered). The following simple instruc-
tions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
The rankings of the systems are numbered from 1
to 5, with 1 being the best translation and 5 be-
ing the worst. Each ranking task has the potential
to provide 10 pairwise rankings, and fewer if the
judge chooses any ties. For example, the ranking
{A:1, B:2, C:4, D:3, E:5}
provides 10 pairwise rankings, while the ranking
{A:3, B:3, C:4, D:3, E:1}
provides just 7. The absolute value of the ranking
or the degree of difference is not considered.
We use the collected pairwise rankings to assign
each system a score that reflects how highly that
system was usually ranked by the annotators. The
score for some system A reflects how frequently it
was judged to be better than other systems when
compared on the same segment; its score is the
number of pairwise rankings where it was judged
to be better, divided by the total number of non-
tying pairwise comparisons. These scores were
used to compute clusters of systems and rankings
between them (?3.4).
3.2 Collecting the data
A goal this year was to collect enough data to
achieve statistical significance in the rankings. We
distributed the workload among two groups of
judges: researchers and Turkers. The researcher
group comprised partipants in the shared task, who
were asked to contribute judgments on 300 sen-
tences for each system they contributed. The re-
searcher evaluation was held over three weeks
from May 17?June 7, and yielded about 280k pair-
wise rankings.
The Turker group was composed of non-expert
annotators hired on Amazon?s Mechanical Turk
(MTurk). A basic unit of work on MTurk is called
a Human Intelligence Task (HIT) and included
three ranking tasks, for which we paid $0.25. To
ensure that the Turkers provided high quality an-
notations, this portion of the evaluation was be-
gun after the researcher portion had completed,
enabling us to embed controls in the form of high-
consensus pairwise rankings in the Turker HITs.
To build these controls, we collected ranking tasks
containing pairwise rankings with a high degree of
researcher consensus. An example task is here:
SENTENCE 504
SOURCE Vor den heiligen Sta?tten verbeugen
REFERENCE Let?s worship the holy places
SYSTEM A Before the holy sites curtain
SYSTEM B Before we bow to the Holy Places
SYSTEM C To the holy sites bow
SYSTEM D Bow down to the holy sites
SYSTEM E Before the holy sites pay
MATRIX
A B C D E
A - 0 0 0 3
B 5 - 0 1 5
C 6 6 - 0 6
D 6 8 5 - 6
E 0 0 0 0 -
Matrix entry Mi,j records the number of re-
searchers who judged System i to be better than
System j. We use as controls pairwise judgments
for which |Mi,j?Mj,i| > 5, i.e., judgments where
the researcher consensus ran strongly in one direc-
tion. We rejected HITs from Turkers who encoun-
tered at least 10 of these controls and failed more
than 50% of them.
There were 463 people who participated in the
Turker portion of the manual evaluation, contribut-
ing 664k pairwise rankings from Turkers who
passed the controls. Together with the researcher
judgments, we collected close to a million pair-
wise rankings, compared to 101k collected last
year: a ten-fold increase. Table 2 contains more
detail.
5
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly-ordered) and has to rank
these according to their translation quality, ties are allowed. For technical reasons, annotators on Amazon?s Mechanical Turk
received all three ranking tasks for a single HIT on a single page, one upon the other.
3.3 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960), which is de-
fined as
? = P (A)? P (E)1? P (E)
where P (A) is the proportion of times that the an-
notators agree, and P (E) is the proportion of time
that they would agree by chance. Note that ? is ba-
sically a normalized version of P (A), one which
takes into account how meaningful it is for anno-
tators to agree with each other, by incorporating
P (E). The values for ? range from 0 to 1, with
zero indicating no agreement and 1 perfect agree-
ment.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A > B, A = B, or A < B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it should capture the probability
that two annotators would agree randomly. There-
fore:
P (E) = P (A>B)2 + P (A=B)2 + P (A<B)2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 3 gives ? values for inter-annotator agree-
ment for WMT11?WMT13 while Table 4 de-
tails intra-annotator agreement scores. Due to the
change of annotation software, we used a slightly
different way of computing annotator agreement
scores. Therefore, we chose to re-compute values
for previous WMTs to allow for a fair comparison.
The exact interpretation of the kappa coefficient is
difficult, but according to Landis and Koch (1977),
0?0.2 is slight, 0.2?0.4 is fair, 0.4?0.6 is moderate,
6
LANGUAGE PAIR Systems Rankings Average
Czech-English 11 85,469 7,769.91
English-Czech 12 102,842 8,570.17
German-English 17 128,668 7,568.71
English-German 15 77,286 5,152.40
Spanish-English 12 67,832 5,652.67
English-Spanish 13 60,464 4,651.08
French-English 13 80,741 6,210.85
English-French 17 100,783 5,928.41
Russian-English 19 151,422 7,969.58
English-Russian 14 87,323 6,237.36
Total 148 942,840 6,370.54
WMT12 103 101,969 999.69
WMT11 133 63,045 474.02
Table 2: Amount of data collected in the WMT13 manual evaluation. The final two rows report summary information from the
previous two workshops.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.400 0.311 0.244 0.342 0.279
English-Czech 0.460 0.359 0.168 0.408 0.075
German-English 0.324 0.385 0.299 0.443 0.324
English-German 0.378 0.356 0.267 0.457 0.239
Spanish-English 0.494 0.298 0.277 0.415 0.295
English-Spanish 0.367 0.254 0.206 0.333 0.249
French-English 0.402 0.272 0.275 0.405 0.321
English-French 0.406 0.296 0.231 0.434 0.237
Russian-English ? ? 0.278 0.315 0.324
English-Russian ? ? 0.243 0.416 0.207
Table 3: ? scores measuring inter-annotator agreement. The WMT13r and WMT13m columns provide breakdowns for re-
searcher annotations and MTurk annotations, respectively. See Table 4 for corresponding intra-annotator agreement scores.
0.6?0.8 is substantial, and 0.8?1.0 is almost per-
fect. We find that the agreement rates are more or
less the same as in prior years.
The WMT13 column contains both researcher
and Turker annotations at a roughly 1:2 ratio. The
final two columns break out agreement numbers
between these two groups. The researcher agree-
ment rates are similar to agreement rates from past
years, while the Turker agreement are well below
researcher agreement rates, varying widely, but of-
ten comparable to WMT11 and WMT12. Clearly,
researchers are providing us with more consistent
opinions, but whether these differences are ex-
plained by Turkers racing through jobs, the partic-
ularities that inform researchers judging systems
they know well, or something else, is hard to tell.
Intra-annotator agreement scores are also on par
from last year?s level, and are often much better.
We observe better intra-annotator agreement for
researchers compared to Turkers.
As a small test, we varied the threshold of ac-
ceptance against the controls for the Turker data
alone and computed inter-annotator agreement
scores on the datasets for the Russian?English task
(the only language pair where we had enough data
at high thresholds). Table 5 shows that higher
thresholds do indeed give us better agreements,
but not monotonically. The increasing ?s sug-
gests that we can find a segment of Turkers who
do a better job and that perhaps a slightly higher
threshold of 0.6 would serve us better, while the
remaining difference against the researchers sug-
gests there may be different mindsets informing
the decisions. In any case, getting the best perfor-
mance out of the Turkers remains difficult.
3.4 System Score
Given the multitude of pairwise comparisons, we
would like to rank the systems according to a
single score computed for each system. In re-
7
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13r WMT13m
Czech-English 0.597 0.454 0.479 0.483 0.478
English-Czech 0.601 0.390 0.290 0.547 0.242
German-English 0.576 0.392 0.535 0.643 0.515
English-German 0.528 0.433 0.498 0.649 0.452
Spanish-English 0.574 1.000 0.575 0.605 0.537
English-Spanish 0.426 0.329 0.492 0.468 0.492
French-English 0.673 0.360 0.578 0.585 0.565
English-French 0.524 0.414 0.495 0.630 0.486
Russian-English ? ? 0.450 0.363 0.477
English-Russian ? ? 0.513 0.582 0.500
Table 4: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation. The WMT13r and WMT13m columns provide breakdowns for researcher annotations and MTurk annota-
tions, respectively. The perfect inter-annotator agreement for Spanish-English is a result of there being very little data for that
language pair.
thresh. rankings ?
0.5 16,605 0.234
0.6 9,999 0.337
0.7 3,219 0.360
0.8 1,851 0.395
0.9 849 0.336
Table 5: Agreement as a function of threshold for Turkers on
the Russian?English task. The threshold is the percentage of
controls a Turker must pass for her rankings to be accepted.
cent evaluation campaigns, we tweaked the metric
and now arrived at a intuitive score that has been
demonstrated to be accurate in ranking systems ac-
cording to their true quality (Koehn, 2012).
The score, which we call EXPECTED WINS, has
an intuitive explanation. If the system is compared
against a randomly picked opposing system, on a
randomly picked sentence, by a randomly picked
judge, what is the probability that its translation is
ranked higher?
Formally, the score for a system Si among a set
of systems {Sj} given a pool of pairwise rankings
summarized as win(A,B) ? the number of times
system A is ranked higher than system B ? is
defined as follows:
score(Si) = 1|{Sj}|
?
j,j 6=i
win(Si, Sj)
win(Si, Sj) + win(Sj , Si)
Note that this score ignores ties.
3.5 Rank Ranges and Clusters
Given the scores, we would like to rank the sys-
tems, which is straightforward. But we would also
like to know, if the obtained system ranking is
statistically significant. Typically, given the large
number of systems that participate, and the simi-
larity of the systems given a common training data
condition and often common toolsets, there will be
some systems that will be very close in quality.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute the expected wins score for each system
based on this sample, and rank each system. By
repeating this procedure a 1,000 times, we can de-
termine a range of ranks, into which system falls
at least 95% of the time (i.e., at least 950 times) ?
corresponding to a p-level of p ? 0.05.
Furthermore, given the rank ranges for each sys-
tem, we can cluster systems with overlapping rank
ranges.8
For all language pairs and all systems, Table 6
reports all system scores, rank ranges, and clus-
ters. The official interpretation of these results
is that systems in the same cluster are considered
tied. Given the large number of judgements that
we collected, it was possible to group on average
about two systems in a cluster, even though the
systems in the middle are typically in larger clus-
ters.
8Formally, given ranges defined by start(Si) and end(Si),
we seek the largest set of clusters {Cc} that satisfies:
?S ?C : S ? C
S ? Ca, S ? Cb ? Ca = Cb
Ca 6= Cb ? ?Si ? Ca, Sj ? Cb :
start(Si) > end(Sj) or start(Sj) > end(Si)
8
Czech-English
# score range system
1 0.607 1 UEDIN-HEAFIELD
2 0.582 2-3 ONLINE-B
0.573 2-4 MES
0.562 3-5 UEDIN
0.547 4-7 ONLINE-A
0.542 5-7 UEDIN-SYNTAX
0.534 6-7 CU-ZEMAN
8 0.482 8 CU-TAMCHYNA
9 0.458 9 DCU-FDA
10 0.321 10 JHU
11 0.297 11 SHEF-WPROA
English-Czech
# score range system
1 0.580 1-2 CU-BOJAR
0.578 1-2 CU-DEPFIX
3 0.562 3 ONLINE-B
4 0.525 4 UEDIN
5 0.505 5-7 CU-ZEMAN
0.502 5-7 MES
0.499 5-8 ONLINE-A
0.484 7-9 CU-PHRASEFIX
0.476 8-9 CU-TECTOMT
10 0.457 10-11 COMMERCIAL-1
0.450 10-11 COMMERCIAL-2
12 0.389 12 SHEF-WPROA
Spanish-English
# score range system
1 0.624 1 UEDIN-HEAFIELD
2 0.595 2 ONLINE-B
3 0.570 3-5 UEDIN
0.570 3-5 ONLINE-A
0.567 3-5 MES
6 0.537 6 LIMSI-SOUL
7 0.514 7 DCU
8 0.488 8-9 DCU-OKITA
0.484 8-9 DCU-FDA
10 0.462 10 CU-ZEMAN
11 0.425 11 JHU
12 0.169 12 SHEF-WPROA
English-Spanish
# rank range system
1 0.637 1 ONLINE-B
2 0.582 2-4 ONLINE-A
0.578 2-4 UEDIN
0.567 3-4 PROMT
5 0.535 5-6 MES
0.528 5-6 TALP-UPC
7 0.491 7-8 LIMSI
0.474 7-9 DCU
0.472 8-10 DCU-FDA
0.455 9-11 DCU-OKITA
0.446 10-11 CU-ZEMAN
12 0.417 12 JHU
13 0.324 13 SHEF-WPROA
German-English
# rank range system
1 0.660 1 ONLINE-B
2 0.620 2-3 ONLINE-A
0.608 2-3 UEDIN-SYNTAX
4 0.586 4-5 UEDIN
0.584 4-5 QUAERO
0.571 5-7 KIT
0.562 6-7 MES
8 0.543 8-9 RWTH-JANE
0.533 8-10 MES-REORDER
0.526 9-10 LIMSI-SOUL
11 0.480 11 TUBITAK
12 0.462 12-13 UMD
0.462 12-13 DCU
14 0.396 14 CU-ZEMAN
15 0.367 15 JHU
16 0.311 16 SHEF-WPROA
17 0.238 17 DESRT
English-German
# rank range system
1 0.637 1-2 ONLINE-B
0.636 1-2 PROMT
3 0.614 3 UEDIN-SYNTAX
0.587 3-5 ONLINE-A
0.571 4-6 UEDIN
0.554 5-6 KIT
7 0.523 7 STANFORD
8 0.507 8 LIMSI-SOUL
9 0.477 9-11 MES-REORDER
0.476 9-11 JHU
0.460 10-12 CU-ZEMAN
0.453 11-12 TUBITAK
13 0.361 13 UU
14 0.329 14-15 SHEF-WPROA
0.323 14-15 RWTH-JANE
English-Russian
# rank range system
1 0.641 1 PROMT
2 0.623 2 ONLINE-B
3 0.556 3-4 CMU
0.542 3-6 ONLINE-G
0.538 3-7 ONLINE-A
0.531 4-7 UEDIN
0.520 5-7 QCRI-MES
8 0.498 8 CU-KAREL
9 0.478 9-10 MES-QCRI
0.469 9-10 JHU
11 0.434 11-12 COMMERCIAL-3
0.426 11-13 LIA
0.419 12-13 BALAGUR
14 0.331 14 CU-ZEMAN
French-English
# rank range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.573 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
English-French
# rank range system
1 0.607 1-2 UEDIN
0.600 1-3 ONLINE-B
0.588 2-4 LIMSI-SOUL
0.584 3-4 KIT
5 0.553 5-7 PROMT
0.551 5-8 STANFORD
0.547 5-8 MES
0.537 6-9 MES-INFLECTION
0.533 7-10 RWTH-PB
0.516 9-11 ONLINE-A
0.499 10-11 DCU
12 0.427 12 CU-ZEMAN
13 0.408 13 JHU
14 0.382 14 OMNIFLUENT
15 0.350 15 ITS-LATL
16 0.326 16 ITS-LATL-PE
Russian-English
# rank range system
1 0.657 1 ONLINE-B
2 0.604 2-3 CMU
0.588 2-3 ONLINE-A
4 0.562 4-6 ONLINE-G
0.561 4-6 PROMT
0.550 5-7 QCRI-MES
0.546 5-7 UCAM
8 0.527 8-9 BALAGUR
0.519 8-10 MES-QCRI
0.507 9-11 UEDIN
0.497 10-12 OMNIFLUENT
0.492 11-14 LIA
0.483 12-15 OMNIFLUENT-C
0.481 12-15 UMD
0.476 13-15 CU-KAREL
16 0.432 16 COMMERCIAL-3
17 0.417 17 UEDIN-SYNTAX
18 0.396 18 JHU
19 0.215 19 CU-ZEMAN
Table 6: Official results for the WMT13 translation task. Systems are ordered by the expected win score. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05. This method is also used to determine the
range of ranks into which system falls. Systems with grey background indicate use of resources that fall outside the constraints
provided for the shared task.
9
4 Understandability of English?Czech
For the English-to-Czech translation, we con-
ducted a variation of the ?understandability? test
as introduced in WMT09 (Callison-Burch et al,
2009) and used in WMT10. In order to obtain
additional reference translations, we conflated this
test with post-editing. The procedure was as fol-
lows:
1. Monolingual editing (also called blind edit-
ing). The first annotator is given just the MT
output and requested to correct it. Given er-
rors in MT outputs, some guessing of the
original meaning is often inevitable and the
annotators are welcome to try. If unable, they
can mark the sentences as incomprehensible.
2. Review. A second annotator is asked to
validate the monolingual edit given both the
source and reference translations. Our in-
structions specify three options:
(a) If the monolingual edit is an adequate
translation and acceptably fluent Czech,
confirm it without changes.
(b) If the monolingual edit is adequate but
needs polishing, modify the sentence
and prefix it with the label ?OK:?.
(c) If the monolingual edit is wrong, cor-
rect it. You may start from the origi-
nal unedited MT output, if that is eas-
ier. Avoid using the reference directly,
prefer words from MT output whenever
possible.
The motivation behind this procedure is that we
want to save the time necessary for reading the
sentence. If the reviewer has already considered
whether the sentence is an acceptable translation,
they do not need to read the MT output again in
order to post-edit it. Our approach is thus some-
what the converse of Aziz et al (2013) who ana-
lyze post-editing effort to obtain rankings of MT
systems. We want to measure the understandabil-
ity of MT outputs and obtain post-edits at the same
time.
Both annotation steps were carried out in
the CASMACAT/Matecat post-editing user inter-
face.9, modified to provide the relevant variants of
the sentence next to the main edit box. Screen-
shots of the two annotation phases are given in
Figure 3 and Figure 4.
9http://www.casmacat.eu/index.php?n=Workbench
Occurrence GOOD ALMOST BAD EMPTY Total
First 34.7 0.1 42.3 11.0 4082
Repeated 41.1 0.1 41.0 6.1 805
Overall 35.8 0.1 42.1 10.2 4887
Table 7: Distribution of review statuses.
Similarly to the traditional ranking task, we pro-
vided three consecutive sentences from the origi-
nal text, each translated with a different MT sys-
tem. The annotators are free to use this contex-
tual information when guessing the meaning or re-
viewing the monolingual edits. Each ?annotation
HIT? consists of 24 sentences, i.e. 8 snippets of 3
consecutive sentences.
4.1 Basic Statistics on Editing
In total, 21 annotators took part in the exercise, 20
of them contributed to monolingual editing and 19
contributed to the reviews.
Connecting each review with the monolingual
edit (some edits received multiple reviews), we ob-
tain one data row. We collected 4887 data rows
(i.e. sentence revisions) for 3538 monolingual ed-
its, covering 1468 source sentences as translated
by 12 MT systems (including the reference).
Not all MT systems were considered for each
sentence, we preferred to obtain judgments for
more source sentences.
Based on the annotation instructions, each data
row has one of the four possible statuses: GOOD,
ALMOST, BAD, and EMPTY. GOOD rows are
those where the reviewer accepted the monolin-
gual edit without changes, ALMOST edits were
modified by the reviewer but they were marked as
?OK?. BAD edits were changed by the reviewer
and no ?OK? mark was given. Finally, the sta-
tus EMPTY is assigned to rows where the mono-
lingual editor refused to edit the sentence. The
EMPTY rows nevertheless contain the (?regular?)
post-edit of the reviewer, so they still provide a
new reference translation for the sentence.
Table 7 summarizes the distribution of row sta-
tuses depending on one more significant distinc-
tion: whether the monolingual editor has seen the
sentence before or not. We see that EMPTY and
BAD monolingual edits together drop by about
6% absolute when the sentence is not new to the
monolingual editor. The occurrence is counted as
?repeated? regardless whether the annotator has
previously seen the sentence in an editing or re-
viewing task. Unless stated otherwise, we exclude
repeated edits from our calculations.
10
Figure 3: In this screen, the annotator is expected to correct the MT output given only the context of at most two neighbouring
machine-translated sentences.
ALMOST Pairwise
treated Comparisons Agreement ?
inter
separate 2690 56.0 0.270
as BAD 2690 67.9 0.351
as GOOD 2690 65.2 0.289
intra
separate 170 65.3 0.410
as BAD 170 69.4 0.386
as GOOD 170 71.8 0.422
Table 8: Annotator agreement when reviewing monolingual
edits.
4.2 Agreement on Understandability
Before looking at individual system results, we
consider annotator agreement in the review step.
Details are given in Table 8. Given a (non-
EMPTY) string from a monolingual edit, we
would like to know how often two acceptability
judgments by two different reviewers (inter-) or
the same reviewer (intra-) agree. The repeated ed-
its remain in this analysis because we are not in-
terested in the origin of the string.
Our annotation setup leads to three possible la-
bels: GOOD, ALMOST, and BAD. The agree-
ment on one of three classes is bound to be lower
than the agreement on two classes, so we also re-
interpret ALMOST as either GOOD or BAD. Gen-
erally speaking, ALMOST is a positive judgment,
so it would be natural to treat it as GOOD. How-
ever, in our particular setup, when the reviewer
modified the sentence and forgot to add the label
?OK:?, the item ended up in the BAD class. We
conclude that this is indeed the case: the inter-
annotator agreement appears higher if ALMOST
is treated as BAD. Future versions of the review-
ing interface should perhaps first ask for the yes/no
judgment and only then allow to post-edit.
The ? values in Table 8 are the Fleiss?
kappa (Fleiss, 1971), accounting for agreement by
chance given the observed label distributions.
In WMT09, the agreements for this task were
higher: 77.4 for inter-AA and 86.6 for intra-AA.
(In 2010, the agreements for this task were not re-
ported.) It is difficult to say whether the differ-
ence lies in the particular language pair, the dif-
ferent set of annotators, or the different user in-
terface for our reviewing task. In 2009 and 2010,
the reviewers were shown 5 monolingual edits at
once and they were asked to judge each as accept-
able or not acceptable. We show just one segment
and they have probably set their minds on the post-
editing rather than acceptability judgment. We be-
lieve that higher agreements can be reached if the
reviewers first validate one or more of the edits and
only then are allowed to post-edit it.
4.3 Understandability of English?Czech
Table 9 brings about the first main result of our
post-editing effort. For each system (including
the reference translation), we check how often a
monolingual edit was marked OK or ALMOST
by the subsequent reviewer. The average under-
standability across all MT systems into Czech is
44.2?1.6%. This is a considerable improvement
compared to 2009 where the best systems pro-
duced about 32% understandable sentences. In
11
Figure 4: In this screen, the annotator is expected to validate the monolingual edit, correcting it if necessary. The annotator is
expected to add the prefix ?OK:? if the correction was more or less cosmetic.
Rank System Total Observations % Understandable
Overall incl. ref. 4082 46.7?1.6
Overall without ref. 3808 44.2?1.6
1 Reference 274?31 80.3?4.8
2-6 CU-ZEMAN 348?34 51.7?5.1
2-6 UEDIN 332?33 51.5?5.4
2-6 ONLINE-B 337?34 50.7?5.3
2-6 CU-BOJAR 341?35 50.7?5.2
2-7 CU-DEPFIX 350?34 48.0?5.3
6-10 COMMERCIAL-2 358?36 43.6?5.2
6-11 COMMERCIAL-1 316?34 41.5?5.5
7-12 CU-TECTOMT 338?34 39.4?5.2
8-12 MES 346?36 38.4?5.2
8-12 CU-PHRASEFIX 394?40 38.1?4.8
10-12 SHEF-WPROA 348?32 34.2?5.1
2009 Reference 91
2009 Best System 32
2010 Reference 97
2010 Best System 58
Table 9: Understandability of English?Czech systems. The
? values indicate empirical confidence bounds at 95%. Rank
ranges were also obtained in the same resampling: in 95% of
observations, the system was ranked in the given range.
2010, the best systems or system combinations
reached 55%?58%. The test set across years and
the quality of references and judgments also play a
role. In our annotation setup, the references appear
to be correctly understandable only to 80.3?4.8%.
To estimate the variance of these results due
to the particular sentences chosen, we draw 1000
random samples from the dataset, preserving the
dataset size and repeating some. The exact num-
ber of judgments per system can thus vary. We
report the 95% empirical confidence interval after
the ??? signs in Table 9 (the systems range from
?4.8 to?5.5). When we drop individual blind ed-
itors or reviewers, the understandability judgments
differ by about ?2 to ?4. In other words, the de-
pendence on the test set appears higher than the
dependence on the annotators.
The limited size of our dataset alows us only
to separate two main groups of systems: those
ranking 2?6 and those ranking worse. This rough
grouping vaguely matches with WMT13 ranking
results as given in Table 6. A somewhat surpris-
ing observation is that two automatic corrections
ranked better in WMT13 ranking but score worse
in understandability: CU-DEPFIX fixes some lost
negation and some agreement errors of CU-BOJAR
and CU-PHRASEFIX is a standard statistical post-
editing of a transfer-based system CU-TECTOMT.
A detailed inspection of the data is necessary to
explain this.
5 More Reference Translations for Czech
Our annotation procedure described in Section 4
allowed us to obtain a considerable number of ad-
ditional reference translations on top of official
single reference.
12
Refs 1 2 3 4 5 6 7 8 9 10-16
Sents 233 709 174 123 60 48 40 27 25 29
Table 10: Number of source sentences with the given number
of distinct reference translations.
In total, our edits cover 1468 source sentences,
i.e. about a half of the official test set size, and pro-
vide 4311 unique references. On average, one sen-
tence in our set has 2.94?2.17 unique reference
translations. Table 10 provides a histogram.
It is well known that automatic MT evalua-
tion methods perform better with more references,
because a single one may not confirm a correct
part of MT output. This issue is more severe
for morphologically rich languages like Czech
where about 1/3 of MT output was correct but not
confirmed by the reference (Bojar et al, 2010).
Advanced evaluation methods apply paraphras-
ing to smooth out some of the lexical divergence
(Kauchak and Barzilay, 2006; Snover et al, 2009;
Denkowski and Lavie, 2010). Simpler techniques
such as lemmatizing are effective for morphologi-
cally rich languages (Tantug et al, 2008; Kos and
Bojar, 2009) but they will lose resolution once the
systems start performing generally well.
WMTs have taken the stance that a big enough
test set with just a single reference should compen-
sate for the lack of other references. We use our
post-edited reference translations to check this as-
sumption for BLEU and NIST as implemented in
mteval-13a (international tokenization switched
on, which is not the default setting).
We run many probes, randomly picking the test
set size (number of distinct sentences) and the
number of distinct references per sentence. Note
that such test sets are somewhat artificially more
diverse; in narrow domains, source sentences can
repeat and even appear verbatim in the training
data, and in natural test sets with multiple refer-
ences, short sentences can receive several identical
translations.
For each probe, we measure the Spearman?s
rank correlation coefficient ? of the ranks pro-
posed by BLEU or NIST and the manual ranks.
We use the same implementation as applied in the
WMT13 Shared Metrics Task (Macha?c?ek and Bo-
jar, 2013). Note that the WMT13 metrics task still
uses the WMT12 evaluation method ignoring ties,
not the expected wins. As Koehn (2012) shows,
the two methods do not differ much.
Overall, the correlation is strongly impacted by
Figure 5: Correlation of BLEU and WMT13 manual ranks
for English?Czech translation
Figure 6: Correlation of NIST and WMT13 manual ranks
for English?Czech translation
the particular choice of test sentences and refer-
ence translations. By picking sentences randomly,
similarly or equally sized test sets can reach dif-
ferent correlations. Indeed, e.g. for a test set of
about 1500 distinct sentences selected from the
3000-sentence official test set (1 reference trans-
lation), we obtain correlations for BLEU between
0.86 and 0.94.
Figure 5 plots the correlations of BLEU and the
system rankings, Figure 6 provides the same pic-
ture for NIST. The upper triangular part of the plot
contains samples from our post-edited reference
translations, the lower rectangular part contains
probes from the official test set of 3000 sentences
with 1 reference translation.
To interpret the observations, we also calculate
the average and standard deviation of correlations
for each cell in Figures 5 and 6. Figures 7 and
8 plot the values for 1, 6, 7 and 8 references for
13
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
B
L
E
U
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 7: Projections from Figure 5 of BLEU and WMT13
manual ranks for English?Czech translation
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 10  100  1000C
o
r
r
e
l
a
t
i
o
n
 
o
f
 
N
I
S
T
 
a
n
d
 
m
a
n
u
a
l
 
r
a
n
k
i
n
g
Test set size
Refs: official 1Refs: postedited 1Refs: postedited 6Refs: postedited 7Refs: postedited 8
Figure 8: Projections from Figure 6 of NIST and WMT13
manual ranks for English?Czech translation
BLEU and NIST, resp. The projections confirm
that the average correlations grow with test set
size, the growth is however sub-logarithmic.
Starting from as few as a dozen of sentences, we
see that using more references is better than using
a larger test set. For BLEU, we however already
seem to reach false positives at 7 references for
one or two hundred sentences: larger sets with just
one reference may correlate slightly better.
Using one reference obtained by post-editing
seems better than using the official (independent)
reference translations. BLEU is more affected
than NIST by this difference even at relatively
large test set size. Note that our post-edits are in-
spired by all MT systems, the good as well as the
bad ones. This probably provides our set with a
certain balance.
Overall, the best balance between the test set
size and the number of references seems to lie
somewhere around 7 references and 100 or 200
sentences. Creating such a test set could be even
cheaper than the standard 3000 sentences with just
one reference. However, the wide error bars re-
mind us that even this setting can lead to correla-
tions anywhere between 0.86 and 0.96. For other
languages, data sets types or other MT evaluation
methods, the best setting can be quite different and
has to be sought for.
6 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
second edition of the WMT shared task on qual-
ity estimation builds on the previous edition of the
task (Callison-Burch et al, 2012), with variants to
this previous task, including both sentence-level
and word-level estimation, with new training and
test datasets, along with evaluation metrics and
baseline systems.
The motivation to include both sentence- and
word-level estimation come from the different po-
tential applications of these variants. Some inter-
esting uses of sentence-level quality estimation are
the following:
? Decide whether a given translation is good
enough for publishing as is.
? Inform readers of the target language only
whether or not they can rely on a translation.
? Filter out sentences that are not good enough
for post-editing by professional translators.
? Select the best translation among options
from multiple MT and/or translation memory
systems.
Some interesting uses of word-level quality es-
timation are the following:
? Highlight words that need editing in post-
editing tasks.
? Inform readers of portions of the sentence
which are not reliable.
? Select the best segments among options from
multiple translation systems for MT system
combination.
The goals of this year?s shared task were:
14
? To explore various granularity levels for the
task (sentence-level and word-level).
? To explore the prediction of more objective
scores such as edit distance and post-editing
time.
? To explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics in the task of ranking alternative
translations generated by different MT sys-
tems.
? To identify new and effective quality indica-
tors (features) for all variants of the quality
estimation task.
? To identify effective machine learning tech-
niques for all variants of the quality estima-
tion task.
? To establish the state of the art performance
in the field.
Four subtasks were proposed, as we discuss in
Sections 6.1 and 6.2. Each subtask provides spe-
cific datasets, annotated for quality according to
the subtask (Section 6.3), and evaluates the system
submissions using specific metrics (Section 6.6).
When available, external resources (e.g. SMT
training corpus) and translation engine-related re-
sources were given to participants (Section 6.4),
who could also use any additional external re-
sources (no distinction between open and close
tracks is made). Participants were also provided
with a software package to extract quality esti-
mation features and perform model learning (Sec-
tion 6.5), with a suggested list of baseline features
and learning method (Section 6.7). Participants
could submit up to two systems for each subtask.
6.1 Sentence-level Quality Estimation
Task 1.1 Predicting Post-editing Distance This
task is similar to the quality estimation task in
WMT12, but with one important difference in the
scoring variant: instead of using the post-editing
effort scores in the [1-5] range, we use HTER
(Snover et al, 2006) as quality score. This score
is to be interpreted as the minimum edit distance
between the machine translation and its manually
post-edited version, and its range is [0, 1] (0 when
no edit needs to be made, and 1 when all words
need to be edited). Two variants of the results
could be submitted in the shared task:
? Scoring: A quality score for each sentence
translation in [0,1], to be interpreted as an
HTER score; lower scores mean better trans-
lations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning). The reference ranking is de-
fined based on the true HTER scores.
Task 1.2 Selecting Best Translation This task
consists in ranking up to five alternative transla-
tions for the same source sentence produced by
multiple MT systems. We use essentially the same
data provided to participants of previous years
WMT?s evaluation metrics task ? where MT eval-
uation metrics are assessed according to how well
they correlate with human rankings. However, ref-
erence translations produced by humans are not be
used in this task.
Task 1.3 Predicting Post-editing Time For this
task systems are required to produce, for each
translation, the expected time (in seconds) it
would take a translator to post-edit such an MT
output. The main application for predictions of
this type is in computer-aided translation where
the predicted time can be used to select among dif-
ferent hypotheses or even to omit any MT output
in cases where no good suggestion is available.
6.2 Word-level Quality Estimation
Based on the data of Task 1.3, we define Task 2, a
word-level annotation task for which participants
are asked to produce a label for each token that
indicates whether the word should be changed by
a post-editor or kept in the final translation. We
consider the following two sets of labels for pre-
diction:
? Binary classification: a keep/change label,
the latter meaning that the token should be
corrected in the post-editing process.
? Multi-class classification: a label specifying
the edit action that should be performed on
the token (keep as is, delete, or substitute).
6.3 Datasets
Task 1.1 Predicting post-editing distance For
the training of models, we provided the WMT12
15
quality estimation dataset: 2,254 English-
Spanish news sentences extracted from previous
WMT translation task English-Spanish test sets
(WMT09, WMT10, and WMT12). These were
translated by a phrase-based SMT Moses system
trained on Europarl and News Commentaries cor-
pora as provided by WMT, along with their source
sentences, reference translations, post-edited
translations, and HTER scores. We used TERp
(default settings: tokenised, case insensitive,
etc., but capped to 1)10 to compute the HTER
scores. Likert scores in [1,5] were also provided,
as participants may choose to use them for the
ranking variant.
As test data, we use a subset of the WMT13
English-Spanish news test set with 500 sentences,
whose translations were produced by the same
SMT system used for the training set. To com-
pute the true HTER labels, the translations were
post-edited under the same conditions as those on
the training set. As in any blind shared task, the
HTER scores were solely used to evaluate the sub-
missions, and were only released to participants
after they submitted their systems.
A few variations of the training and test data
were provided, including a version with cases re-
stored and a version detokenized. In addition,
we provided a number of engine-internal informa-
tion from Moses for glass-box feature extraction,
such as phrase and word alignments, model scores,
word graph, n-best lists and information from the
decoder?s search graph.
Task 1.2 Selecting best translation As training
data, we provided a large set of up to five alter-
native machine translations produced by different
MT systems for each source sentence and ranked
for quality by humans. This was the outcome of
the manual evaluation of the translation task from
WMT09-WMT12. It includes two language pairs:
German-English and English-Spanish, with 7,098
and 4,592 source sentences and up to five ranked
translations, totalling 32,922 and 22,447 transla-
tions, respectively.
As test data, a set of up to five alternative ma-
chine translations per source sentence from the
WMT08 test sets was provided, with 365 (1,810)
and 264 (1,315) source sentences (translations)
for German-English and English-Spanish, respec-
tively. We note that there was some overlap be-
tween the MT systems used in the training data
10http://www.umiacs.umd.edu/?snover/terp/
and test datasets, but not all systems were the
same, as different systems participate in WMT
over the years.
Task 1.3 and Task 2 Predicting post-editing
time and word-level edits For Tasks 1.3 and 2
we provides a new dataset consisting of 22 English
news articles which were translated into Span-
ish using Moses and post-edited during a CAS-
MACAT11 field trial. Of these, 15 documents have
been processed repeatedly by at least 2 out of 5
translators, resulting in a total of 1,087 segments.
For each segment we provided:
? English source and Spanish translation.
? Spanish MT output which was used as basis
for post-editing.
? Document and translator ID.
? Position of the segment within the document.
The metadata about translator and document was
made available as we expect that translator perfor-
mance and normalisation over document complex-
ity can be helpful when predicting the time spend
on a given segment.
For the training portion of the data we also pro-
vided:
? Time to post-edit in seconds (Task 1.3).
? Binary (Keep, Change) and multiclass (Keep,
Substitute, Delete) labels on word level along
with explicit tokenization (Task 2).
The labels in Task 2 are derived by comput-
ing WER between the original machine translation
and its post-edited version.
6.4 Resources
For all tasks, we provided resources to extract
quality estimation features when these were avail-
able:
? The SMT training corpus (WMT News and
Europarl): source and target sides of the cor-
pus used to train the SMT engines for Tasks
1.1, 1.3, and 2, and truecase models gener-
ated from these. These corpora can also be
used for Task 1.2, but we note that some of
the MT systems used in the datasets of this
task were not statistical or did not use (only)
the training corpus provided by WMT.
11http://casmacat.eu/
16
? Language models: n-gram language models
of source and target languages generated us-
ing the SMT training corpora and standard
toolkits such as SRILM Stolcke (2002), and
a language model of POS tags for the target
language. We also provided unigram, bigram
and trigram counts.
? IBM Model 1 lexical tables generated by
GIZA++ using the SMT training corpora.
? Phrase tables with word alignment informa-
tion generated by scripts provided by Moses
from the parallel corpora.
? For Tasks 1.1, 1.3 and 2, the Moses config-
uration file used for decoding or the code to
re-run the entire Moses system.
? For Task 1.1, both English and Spanish re-
sources for a number of advanced features
such as pre-generated PCFG parsing models,
topic models, global lexicon models and mu-
tual information trigger models.
We refer the reader to the QUEST website12 for
a detailed list of resources provided for each task.
6.5 QUEST Framework
QUEST (Specia et al, 2013) is an open source
framework for quality estimation which provides a
wide variety of feature extractors from source and
translation texts and external resources and tools.
These range from simple, language-independent
features, to advanced, linguistically motivated fea-
tures. They include features that rely on informa-
tion from the MT system that generated the trans-
lations (glass-box features), and features that are
oblivious to the way translations were produced
(black-box features).
QUEST also integrates a well-known machine
learning toolkit, scikit-learn,13 and other algo-
rithms that are known to perform well on this task
(e.g. Gaussian Processes), providing a simple and
effective way of experimenting with techniques
for feature selection and model building, as well
as parameter optimisation through grid search.
From QUEST, a subset of 17 features and an
SVM regression implementation were used as
baseline for Tasks 1.1, 1.2 and 1.3. The software
was made available to all participants.
12http://www.quest.dcs.shef.ac.uk/
13http://scikit-learn.org/
6.6 Evaluation Metrics
Task 1.1 Predicting post-editing distance
Evaluation is performed against the HTER and/or
ranking of translations using the same metrics as
in WMT12. For the scoring variant of the task,
we use two standard metrics for regression tasks:
Mean Absolute Error (MAE) as a primary metric,
and Root of Mean Squared Error (RMSE) as a
secondary metric. To improve readability, we
report these error numbers by first mapping the
HTER values to the [0, 100] interval, to be read
as percentage-points of the HTER metric. For a
given test set S with entries si, 1 ? i ? |S|, we
denote by H(si) the proposed score for entry si
(hypothesis), and by V (si) the reference value for
entry si (gold-standard value):
MAE =
?N
i=1 |H(si)? V (si)|
|S|
RMSE =
??N
i=1(H(si)? V (si))2
|S|
Both these metrics are non-parametric, auto-
matic and deterministic (and therefore consistent),
and extrinsically interpretable. For instance, a
MAE value of 10 means that, on average, the ab-
solute difference between the hypothesized score
and the reference score value is 10 percentage
points (i.e., 0.10 difference in HTER scores). The
interpretation of RMSE is similar, with the differ-
ence that RMSE penalises larger errors more (via
the square function).
For the ranking variant of the task, we use the
DeltaAvg metric proposed in the 2012 edition of
the task (Callison-Burch et al, 2012) as our main
metric. This metric assumes that each reference
test instance has an extrinsic number associated
with it that represents its ranking with respect to
the other test instances. For completeness, we
present here again the definition of DeltaAvg.
The goal of the DeltaAvg metric is to measure
how valuable a proposed ranking (which we call a
hypothesis ranking) is, according to the true rank-
ing values associated with the test instances. We
first define a parametrised version of this metric,
called DeltaAvg[n]. The following notations are
used: for a given entry sentence s, V (s) represents
the function that associates an extrinsic value to
that entry; we extend this notation to a set S, with
V (S) representing the average of all V (s), s ? S.
17
Intuitively, V (S) is a quantitative measure of the
?quality? of the set S, as induced by the extrinsic
values associated with the entries in S. For a set
of ranked entries S and a parameter n, we denote
by S1 the first quantile of set S (the highest-ranked
entries), S2 the second quantile, and so on, for n
quantiles of equal sizes.14 We also use the nota-
tion Si,j = ?jk=i Sk. Using these notations, wedefine:
DeltaAvgV [n] =
?n?1
k=1 V (S1,k)
n? 1 ? V (S)
When the valuation function V is clear from the
context, we write DeltaAvg[n] for DeltaAvgV [n].
The parameter n represents the number of quan-
tiles we want to split the set S into. For instance,
n = 2 gives DeltaAvg[2] = V (S1)?V (S), hence it
measures the difference between the quality of the
top quantile (top half) S1 and the overall quality
(represented by V (S)). For n = 3, DeltaAvg[3] =
(V (S1)+V (S1,2)/2?V (S) = ((V (S1)?V (S))+
(V (S1,2?V (S)))/2, hence it measures an average
difference across two cases: between the quality of
the top quantile (top third) and the overall quality,
and between the quality of the top two quantiles
(S1 ? S2, top two-thirds) and the overall quality.
In general, DeltaAvg[n] measures an average dif-
ference in quality across n ? 1 cases, with each
case measuring the impact in quality of adding an
additional quantile, from top to bottom. Finally,
we define:
DeltaAvgV =
?N
n=2 DeltaAvgV [n]
N ? 1
where N = |S|/2. As before, we write DeltaAvg
for DeltaAvgV when the valuation function V is
clear from the context. The DeltaAvg metric is an
average across all DeltaAvg[n] values, for those
n values for which the resulting quantiles have at
least 2 entries (no singleton quantiles).
We present results for DeltaAvg using as valu-
ation function V the HTER scores, as defined in
Section 6.3. We also use Spearman?s rank correla-
tion coefficient ? as a secondary metric.
Task 1.2 Selecting best translation The perfor-
mance on the task of selecting the best transla-
tion from a pool of translation candidates is mea-
14If the size |S| is not divisible by n, then the last quantile
Sn is assumed to contain the rest of the entries.
sured by comparing proposed (hypothesis) rank-
ings against human-produced rankings. The met-
ric used is Kendall?s ? rank correlation coefficient,
computed as follows:
? = |concordant pairs| ? |discordant pairs||total pairs|
where a concordant pair is a pair of two transla-
tions for the same source segment in which the
ranking order proposed by a human annotator and
the ranking order of the hypothesis agree; in a dis-
cordant pair, they disagree. The possible values of
? range between 1 (where all pairs are concordant)
and ?1 (where all pairs are discordant). Thus a
system with ranking predictions having a higher
? value makes predictions that are more similar
to human judgements than a system with ranking
predictions having a lower ? . Note that, in general,
being able to predict rankings with an accuracy
of ? = ?1 is as difficult as predicting rankings
with an accuracy of ? = 1, whereas a completely
random ranking would have an expected value of
? = 0. The range is therefore said to be symmet-
ric.
However, there are two distinct ways of mea-
suring rank correlation using Kendall?s ? , related
to the way ties are treated. They greatly affect how
Kendall?s ? numbers are to be interpreted, and es-
pecially the symmetry property. We explain the
difference in detail in what follows.
Kendall?s ? with ties penalised If the goal is
to measure to what extent the difference in qual-
ity visible to a human annotator has been captured
by an automatically produced hypothesis (recall-
oriented view), then proposing a tie between t1
and t2 (t1-equal-to-t2) when the pair was judged
(in the reference) as t1-better-than-t2 is treated as
a failure-to-recall. In other words, it is as bad as
proposing t1-worse-than-t2. Henceforth, we call
this recall-oriented measure ?Kendall?s ? with ties
penalised?. This metric has the following proper-
ties:
? it is completely fair when comparing differ-
ent methods to produce ranking hypotheses,
because the denominator (number of total
pairs) is the same (it is the number of non-
tied pairs under the human judgements).
? it is non-symmetric, in the sense that a value
of ? = ?1 is not as difficult to obtain as ? =
18
1 (simply proposing only ties gets a ? = ?1);
hence, the sign of the ? value matters.
? the expected value of a completely random
ranking is not necessarily ? = 0, but rather
depends on the number of ties in the refer-
ence rankings (i.e., it is test set dependent).
Kendall?s ? with ties ignored If the goal
is to measure to what extent the difference in
quality signalled by an automatically produced
hypothesis is reflected in the human annota-
tion (precision-oriented view), then proposing t1-
equal-to-t2 when the pair was judged differently
in the reference does no harm the metric.
Henceforth, we call this precision-oriented
measure ?Kendall?s ? with ties ignored?. This
metric has the following properties:
? it is not completely fair when comparing dif-
ferent methods to produce ranking hypothe-
ses, because the denominator (number of to-
tal pairs) may not be the same (it is the num-
ber of non-tied pairs under each system?s pro-
posal).
? it is symmetric, in the sense that a value of
? = ?1 is as difficult to obtain as ? = 1;
hence, the sign of the ? value may not mat-
ter. 15
? the expected value of a completely random
ranking is ? = 0 (test-set independent).
The first property is the most worrisome from
the perspective of reporting the results of a shared
task, because a system may fare very well on this
metric simply because it choses not to commit
(proposes ties) most of the time. Therefore, to
give a better understanding of the systems? perfor-
mance, for Kendall?s ? with ties ignored we also
provide the number of non-ties proposed by each
system.
Task 1.3 Predicting post-editing time Submis-
sions are evaluated in terms of Mean Average Er-
ror (MAE) against the actual time spent by post-
editors (in seconds). By using a linear error mea-
sure we limit the influence of outliers: sentences
that took very long to edit or where the measure-
ment taken is questionable.
15In real life applications this distinction matters. Even
if, from a computational perspective, it is as hard to get ?
close to?1 as it is to get it close to 1, knowing the sign is the
difference between selecting the best or the worse translation.
To further analyse the influence of extreme val-
ues, we also compute Spearman?s rank correlation
? coefficient which does not depend on the abso-
lute values of the predictions.
We also give RMSE and Pearson?s correlation
coefficient r for reference.
Task 2 Predicting word-level scores The word-
level task is primarily evaluated by macro-
averaged F-measure. Because the class distribu-
tion is skewed ? in the test data about one third
of the tokens are marked as correct ? we compute
precision and recall and F1 for each class individ-
ually. Consider the following confusion matrix for
the two classes Keep and Change:
predicted
(K)eep (C)hange
expected (K)eep 10 20(C)hange 30 40
For the given example we derive true-positive
(tp), true-negative (tn), false-positive (fp), and
false-negative (fn) counts:
tpK = 10 fpK = 30 fnK = 20
tpC = 40 fpC = 20 fnC = 30
precisionK =
tpK
tpK + fpK
= 10/40
recallK =
tpK
tpK + fnK
= 10/30
F1,K =
2 ? precisionK ? recallK
precisionK +recallK
A single cumulative statistic can be computed
by averaging the resulting F-measures (macro av-
eraging) or by micro averaging in which case pre-
cision and recall are first computed by accumulat-
ing the relevant values for all classes (O?zgu?r et al,
2005), e.g.
precision = tpK + tpC(tpK + fpK) + (tpC + fpC)
The latter gives equal weight to each exam-
ple and is therefore dominated by performance on
the largest class while macro-averaged F-measure
gives equal weight to each class.
The same setup is used to evaluate the perfor-
mance in the multiclass setting. Please note that
here the test data only contains 4% examples for
class (D)elete.
19
ID Participating team
CMU Carnegie Mellon University, USA (Hildebrand and Vogel, 2013)
CNGL Centre for Next Generation Localization, Ireland (Bicici, 2013b)
DCU Dublin City University, Ireland (Almaghout and Specia, 2013)
DCU-SYMC Dublin City University & Symantec, Ireland (Rubino et al, 2013b)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis and
Popovic, 2013)
FBK-UEdin Fondazione Bruno Kessler, Italy & University of Edinburgh, UK (Camargo de
Souza et al, 2013)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al, 2013)
LIMSI Laboratoire d?Informatique pour la Me?canique et les Sciences de l?Inge?nieur,
France (Singh et al, 2013)
LORIA Lorraine Laboratory of Research in Computer Science and its Applications,
France (Langlois and Smaili, 2013)
SHEF University of Sheffield, UK (Beck et al, 2013)
TCD-CNGL Trinity College Dublin & CNGL, Ireland (Moreau and Rubino, 2013)
TCD-DCU-CNGL Trinity College Dublin, Dublin City University & CNGL, Ireland (Moreau and
Rubino, 2013)
UMAC University of Macau, China (Han et al, 2013)
UPC Universitat Politecnica de Catalunya, Spain (Formiga et al, 2013b)
Table 11: Participants in the WMT13 Quality Estimation shared task.
6.7 Participants
Table 11 lists all participating teams submitting
systems to any subtask in this shared task. Each
team was allowed up to two submissions for each
subtask. In the descriptions below participation in
specific tasks is denoted by a task identifier: T1.1,
T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.3):
QUEST was used to extract 17 system-
independent features from the source and
translation files and the SMT training cor-
pus that were found to be relevant in previous
work (same features as in the WMT12 shared
task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? Language model probability of source
and target sentences using language
models provided by the task.
? average number of translations per
source word in the sentence: as given
by IBM 1 model thresholded so that
P (t|s) > 0.2, and so that P (t|s) > 0.01
weighted by the inverse frequency of
each word in the source side of the SMT
training corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source side of the
SMT training corpus
? percentage of unigrams in the source
sentence seen in the source side of the
SMT training corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within the
SCIKIT-LEARN toolkit. The ?,  and C pa-
rameters were optimized using a grid-search
and 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as a ?baseline?, it is in fact a strong
system. For tasks of the same type as 1.1
and 1.3, it has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting post-editing effort, as it
has also been shown in the previous edition
of the task (Callison-Burch et al, 2012).
The same features could be useful for a base-
line system for Task 1.2. In our official re-
20
sults, however, the baseline for Task 1.2 is
simpler than that: it proposes random ranks
for each pair of alternative translations for a
given source sentence, as we will discuss in
Section 6.8.
CMU (T1.1, T1.2, T1.3): The CMU quality
estimation system was trained on features
based on language models, the MT sys-
tem?s distortion model and phrase table fea-
tures, statistical word lexica, several sentence
length statistics, source language word and
bi-gram frequency statistics, n-best list agree-
ment and diversity, source language parse,
source-target word alignment and a depen-
dency parse based cohesion penalty. These
features were extracted using GIZA++, a
forced alignment algorithm and the Stanford
parser (de Marneffe et al, 2006). The pre-
diction models were trained using four clas-
sifiers in the Weka toolkit (Hall et al, 2009):
linear regression, M5P trees, multi layer per-
ceptron and SVM regression. In addition to
main system submission, a classic n-best list
re-ranking approach was used for Task 1.2.
CNGL (T1.1, T1.2, T1.3, T2): CNGL systems
are based on referential translation machines
(RTM) (Bic?ici and van Genabith, 2013), par-
allel feature decay algorithms (FDA) (Bicici,
2013a), and machine translation performance
predictor (MTPP) (Bic?ici et al, 2013), all
of which allow to obtain language and MT
system-independent predictions. For each
task, RTM models were developed using the
parallel corpora and the language model cor-
pora distributed by the WMT13 translation
task and the language model corpora pro-
vided by LDC for English and Spanish.
The sentence-level features are described in
MTPP (Bic?ici et al, 2013); they include
monolingual or bilingual features using n-
grams defined over text or common cover
link (CCL) (Seginer, 2007) structures as the
basic units of information over which sim-
ilarity calculations are made. RTMs use
308 features about coverage and diversity,
IBM1, and sentence translation performance,
retrieval closeness and minimum Bayes re-
trieval risk, distributional similarity and en-
tropy, IBM2 alignment, character n-grams,
and sentence readability. The learning mod-
els are Support Vector Machines (SVR) and
SVR with partial least squares (SVRPLS).
The word-level features include CCL links,
word length, location, prefix, suffix, form,
context, and alignment, totalling 511K fea-
tures for binary classification, and 637K for
multiclass classification. Generalised lin-
ear models (GLM) (Collins, 2002) and GLM
with dynamic learning (GLMd) were used.
DCU (T1.2): The main German-English submis-
sion uses six Combinatory Categorial Gram-
mar (CCG) features: CCG supertag lan-
guage model perplexity and log probability,
the number of maximal CCG constituents in
the translation output which are the highest-
probability minimum number of CCG con-
stituents that span the translation output, the
percentage of CCG argument mismatches be-
tween each subsequent CCG supertags, the
percentage of CCG argument mismatches be-
tween each subsequent CCG maximal cate-
gories and the minimum number of phrases
detected in the translation output. A second
submission uses the aforementioned CCG
features combined with 80 features from
QUEST as described in (Specia, 2011). For
the CCG features, the C&C parser was used
to parse the translation output. Moses was
used to build the phrase table from the SMT
training corpus with maximum phrase length
set to 7. The language model of supertags
was built using the SRILM toolkit. As learn-
ing algorithm, Logistic Regression as pro-
vided by the SCIKIT-LEARN toolkit was used.
The training data was prepared by converting
each ranking of translation outputs to a set
of pairwise comparisons according to the ap-
proach proposed by Avramidis et al (2011).
The rankings were generated back from pair-
wise comparisons predicted by the model.
DCU-SYMC (T1.1): The DCU-Symantec team
employed a wide set of features which in-
cluded language model, n-gram counts and
word-alignment features as well as syntac-
tic features, topic model features and pseudo-
reference features. The main learning algo-
rithm was SVR, but regression tree learning
was used to perform feature selection, re-
ducing the initial set of 442 features to 96
features (DCU-Symantec alltypes) and 134
21
(DCU-Symantec combine). Two methods
for feature selection were used: a best-first
search in the feature space using regression
trees to evaluate the subsets, and reading bi-
narised features directly from the nodes of
pruned regression trees.
The following NLP tools were used in feature
extraction: the Brown English Wall-Street-
Journal-trained statistical parser (Charniak
and Johnson, 2005), a Lexical Functional
Grammar parser (XLE), together with a
hand-crafted Lexical Functional Grammar,
the English ParGram grammar (Kaplan et al,
2004), and the TreeTagger part-of-speech
tagger (Schmidt, 1994) with off-the-shelf
publicly available pre-trained tagging mod-
els for English and Spanish. For pseudo-
reference features, the Bing, Moses and Sys-
tran translation systems were used. The Mal-
let toolkit (McCallum, 2002) was used to
build the topic models and features based on
a grammar checker were extracted with Lan-
guageTool.16
DFKI (T1.2, T1.3): DFKI?s submission for Task
1.2 was based on decomposing rankings into
pairs (Avramidis, 2012), where the best sys-
tem for each pair was predicted with Lo-
gistic Regression (LogReg). For German-
English, LogReg was trained with Stepwise
Feature Selection (Hosmer, 1989) on two
feature sets: Feature Set 24 includes ba-
sic counts augmented with PCFG parsing
features (number of VPs, alternative parses,
parse probability) on both source and tar-
get sentences (Avramidis et al, 2011), and
pseudo-reference METEOR score; the most
successful set, Feature Set 33 combines those
24 features with the 17 baseline features. For
English-Spanish, LogReg was used with L2
Regularisation (Lin et al, 2007) and two fea-
ture sets were devised after scoring features
with ReliefF (Kononenko, 1994) and Infor-
mation Gain (Hunt et al, 1966). Feature Set
431 combines 30 features with highest abso-
lute Relief-F and Information Gain (15 from
each). features with the highest
Task 1.3 was modelled using feature sets
selected after Relief-F scoring of external
black-box and glass-box features extracted
16http://www.languagetool.org/
from the SMT decoding process. The most
successful submission (linear6) was trained
with Linear Regression including the 17 fea-
tures with highest positive Relief-F. Most
prominent features include the alternative
possible parses of the source and target sen-
tence, the positions of the phrases with the
lowest and highest probability and future
cost estimate in the translation, the counts of
phrases in the decoding graph whose prob-
ability or whether the future cost estimate
is higher/lower than their standard deviation,
counts of verbs and determiners, etc. The
second submission (pls8) was trained with
Partial Least Squares regression (Stone and
Brooks, 1990) including more glass-box fea-
tures.
FBK-Uedin (T1.1, T1.3):
The submissions explored features built on
MT engine resources including automatic
word alignment, n-best candidate translation
lists, back-translations and word posterior
probabilities. Information about word align-
ments is used to extract quantitative (amount
and distribution of the alignments) and qual-
itative (importance of the aligned terms) fea-
tures under the assumption that alignment
information can help tasks where sentence-
level semantic relations need to be identified
(Souza et al, 2013). Three similar English-
Spanish systems are built and used to provide
pseudo-references (Soricut et al, 2012) and
back-translations, from which automatic MT
evaluation metrics could be computed and
used as features.
All features were computed over a concatena-
tion of several publicly available parallel cor-
pora for the English-Spanish language pair
such as Europarl, News Commentary, and
MultiUN. The models were developed using
supervised learning algorithms: SVMs (with
feature selection step prior to model learning)
and extremely randomized trees.
LIG (T2): The LIG systems are designed to
deal with both binary and multiclass variants
of the word level task. They integrate sev-
eral features including: system-based (graph
topology, language model, alignment con-
text, etc.), lexical (Part-of-Speech tags), syn-
tactic (constituent label, distance to the con-
22
stituent tree root) and semantic (target and
source polysemy count). Besides the exist-
ing components of the SMT system, feature
extraction requires further external tools and
resources, such as: TreeTagger (for POS tag-
ging), Bekerley Parser trained with AnCora
treebank (for generating constituent trees in
Spanish), WordNet and BabelNet (for pol-
ysemy count), Google Translate. The fea-
ture set is then combined and trained using
a Conditional Random Fields (CRF) learn-
ing method. During the labelling phase, the
optimal threshold is tuned using a small de-
velopment set split from the original training
set. In order to retain the most informative
features and eliminate the redundant ones, a
Sequential Backward Selection algorithm is
employed over the all-feature systems. With
the binary classifier, the Boosting technique
is applied to allow a number of sub feature
sets to complement each other, resulting in
the ?stronger? combined system.
LIMSI (T1.1, T1.3): The two tasks were treated
as regression problems using a simple elas-
tic regression, a linear model trained with L1
and L2 regularisers. Regarding features, the
submissions mainly aimed at evaluating the
usefulness for quality estimation of n-gram
posterior probabilities (Gispert et al, 2013)
that quantify the probability for a given n-
gram to be part of the system output. Their
computation relies on all the hypotheses con-
sidered by a SMT system during decoding:
intuitively, the more hypotheses a n-gram ap-
pears in, the more confident the system is
that this n-gram is part of the correct trans-
lation, and the higher its posterior probabil-
ity is. The feature set contains 395 other fea-
tures that differs, in two ways, from the tra-
ditional features used in quality estimation.
First, it includes several features based on
large span continuous space language mod-
els (Le et al, 2011) that have already proved
their efficiency both for the translation task
and the quality estimation task. Second, each
feature was expanded into two ?normalized
forms? in which their value was divided ei-
ther by the source length or the target length
and, when relevant, into a ?ratio form? in
which the feature value computed on the tar-
get sentence is divided by its value computed
in the source sentence.
LORIA (T1.1): The system uses the 17 baseline
features, plus several numerical and boolean
features computed from the source and target
sentences (Langlois et al, 2012). These are
based on language model information (per-
plexity, level of back-off, intra-lingual trig-
gers), translation table (IBM1 table, inter-
lingual triggers). For language models, for-
ward and backward models are built. Each
feature gives a score to each word in the sen-
tence, and the score of the sentence is the av-
erage of word scores. For several features,
the score of a word depends on the score of its
neighbours. This leads to 66 features. Sup-
port Vector Machines are used to learn a re-
gression model. In training is done in a multi-
stage procedure aimed at increasing the size
of the training corpus. Initially, the train-
ing corpus with machine translated sentences
provided by the task is used to train an SVM
model. Then this model is applied to the post-
edited and reference sentences (also provided
as part of the task). These are added to the
quality estimation training corpus using as la-
bels the SVM predictions. An algorithm to
tune the predicted scores on a development
corpus is used.
SHEF (T1.1, T1.3): These submissions use
Gaussian Processes, a non-parametric prob-
abilistic learning framework for regression,
along with two techniques to improve predic-
tion performance and minimise the amount
of resources needed for the problem: feature
selection based on optimised hyperparame-
ters and active learning to reduce the training
set size (and therefore the annotation effort).
The initial set features contains all black box
and glass box features available within the
QUEST framework (Specia et al, 2013) for
the dataset at hand (160 in total for Task 1.1,
and 80 for Task 1.3). The query selection
strategy for active learning is based on the
informativeness of the instances using Infor-
mation Density, a measure that leverages be-
tween the variance among instances and how
dense the region (in the feature space) where
the instance is located is. To perform fea-
ture selection, following (Shah et al, 2013)
features are ranked by the Gaussian Process
23
algorithm according to their learned length
scales, which can be interpreted as the rel-
evance of such feature for the model. This
information was used for feature selection
by discarding the lowest ranked (least use-
ful) ones. based on empirical results found
in (Shah et al, 2013), the top 25 features for
both models were selected and used to retrain
the same regression algorithm.
UPC (T1.2): The methodology used a broad set
of features, mainly available through the last
version of the Asiya toolkit for MT evalua-
tion (Gonza`lez et al, 2012)17. Concretely,
86 features were derived for the German-to-
English and 97 features for the English-to-
Spanish tasks. These features cover differ-
ent approaches and include standard qual-
ity estimation features, as provided by the
above mentioned Asiya and QUEST toolk-
its, but also a variety of features based on
pseudo-references, explicit semantic analy-
sis and specialised language models trained
on the parallel and monolingual corpora pro-
vided by the WMT Translation Task.
The system selection task is approached by
means of pairwise ranking decisions. It uses
Random Forest classifiers with ties, expand-
ing the work of 402013cFormiga et al), from
which a full ranking can be derived and the
best system per sentence is identified. Once
the classes are given by the Random Forest,
one can build a graph by means of the adja-
cency matrix of the pairwise decision. The fi-
nal ranking is assigned through a dominance
scheme similar to Pighin et al (2012).
An important remark of the methodology is
the feature selection process, since it was no-
ticed that the learner was sensitive to the fea-
tures used. Selecting the appropriate set of
features was crucial to achieve a good per-
formance. The best feature combination was
composed of: i) a baseline quality estimation
feature set (Asiya or Quest) but not both of
them, ii) Length Model, iii) Pseudo-reference
aligned based features, and iv) adapted lan-
guage models. However, within the de-en
task, substituting Length Model and Aligned
Pseudo-references by the features based on
17http://asiya.lsi.upc.edu/
Semantic Roles could bring marginally bet-
ter accuracy.
TCD-CNGL (T1.1) and TCD-DCU-CNGL
(T1.3): The system is based on features
which are commonly used for style classifi-
cation (e.g. author identification). The as-
sumption is that low/high quality translations
can be characterised by some patterns which
are frequent and/or differ significantly from
the opposite category. Such features are in-
tended to focus on striking patterns rather
than to capture the global quality in a sen-
tence, but they are used in conjunction with
classical features for quality estimation (lan-
guage modelling, etc.). This requires two
steps in the training process: first the refer-
ence categories against which sentences will
be compared are built, then the standard qual-
ity estimation model training stage is per-
formed. Both datasets (Tasks 1.1 and 1.3)
were used for both tasks. Since the number
of features can be very high (up to 65,000),
a combination of various heuristics for se-
lecting features was used before the training
stage (the submitted systems were trained us-
ing SVM with RBF kernels).
UMAC (T1.1, T1.2, T2): For Task 1.1, the fea-
ture set consists in POS sequences of the
source and target languages, using 12 uni-
versal tags that are common in both lan-
guages. The algorithm is an enhanced ver-
sion of the BLEU metric (EBLEU) designed
with a modified length penalty and added re-
call factor, and having the precision and re-
call components grouped using the harmonic
mean. For Task 1.2, in addition to the uni-
versal POS sequences of the source and tar-
get languages, features include the scores of
length penalty, precision, recall and rank.
Variants of EBLEU with different strategies
for alignment are used, as well as a Na??ve
Bayes classification algorithm. For Task 2,
the features used are unigrams (from previous
4th to following 3rd tokens), bigrams (from
previous 2nd to following 2nd tokens), skip
bigrams (previous and next token), trigrams
(from previous 2nd to following 2nd tokens).
The learning algorithms are Conditional Ran-
dom Fields and Na??ve Bayes.
24
6.8 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing distance
Table 12 summarises the results for the ranking
variant of the task. They are sorted from best to
worse using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are CNGL SVRPLS, with a
DeltaAvg score of 11.09, and DCU-SYMC all-
types, with a DeltaAvg score of 10.13. While the
former holds the higher score, the difference is not
significant at the p ? 0.05 level as estimated by a
bootstrap resampling test.
Both submissions are better than the baseline
system by a very wide margin, a larger relative im-
provement than that obtained in the corresponding
WMT12 task. In addition, five submissions (out
of 12 systems) scored significantly higher than the
baseline system (systems above the middle gray
area), which is a larger proportion than that in last
year?s task (only 3 out of 16 systems), indicat-
ing that this shared task succeeded in pushing the
state-of-the-art performance to new levels.
In addition to the performance of the official
submission, we report results obtained by two or-
acle methods: the gold-label HTER metric com-
puted against the post-edited translations as ref-
erence (Oracle HTER), and the BLEU metric (1-
BLEU to obtain the same range as HTER) com-
puted against the same post-edited translations as
reference (Oracle HBLEU). The ?Oracle HTER?
DeltaAvg score of 16.38 gives an upperbound in
terms of DeltaAvg for the test set used in this eval-
uation. It indicates that, for this set, the differ-
ence in post-editing effort between the top quality
quantiles and the overall quality is 16.38 on aver-
age. The oracle based on HBLEU gives a lower
DeltaAvg score, which is expected since HTER
was our actual gold label. However, it is still
significantly higher than the score of the winning
submission, which shows that there is significant
room for improvement even by the highest scor-
ing submissions.
The results for the scoring variant of the task
are presented in Table 13, sorted from best to
worse by using the MAE metric scores as primary
key and the RMSE metric scores as secondary key.
According to MAE scores, the winning submis-
sion is SHEF FS (MAE = 12.42), which uses fea-
ture selection and a novel learning algorithm for
the task, Gaussian Processes. The baseline sys-
tem is measured to have an MAE of 14.81, with
six other submissions having performances that
are not different from the baseline at a statisti-
cally significant level, as shown by the gray area
in the middle of Table 13). Nine submissions (out
of 16) scored significantly higher than the base-
line system (systems above the middle gray area),
a considerably higher proportion of submissions
as compared to last year (5 out of 19), which indi-
cates that this shared task also succeeded in push-
ing the state-of-the-art performance to new levels
in terms of absolute scoring. Only one (6%) sys-
tem scored significantly lower than the baseline,
as opposed to 8 (42%) in last year?s task.
For the sake of completeness, we also show or-
acles figures using the same methods as for the
ranking variant of the task. Here the lowerbound
in error (Oracle HTER) will clearly be zero, as
both MAE and RMSE are measured against the
same gold label used for the oracle computation.
?Oracle HBLEU? is also not indicative in this
case, as the although the values for the two metrics
(HTER and HBLEU) are within the same ranges,
they are not directly comparable. This explains the
larger MAE/RMSE figures for ?Oracle HBLEU?
than those for most submissions.
Task 1.2 Selecting the best translation
Below we present the results for this task for each
of the two Kendall?s ? flavours presented in Sec-
tion 6.6, for the German-English test set (Tables 14
and 16) and the English-Spanish test set (Tables 15
and 17). The results are sorted from best to worse
using each of the Kendall?s ? metric flavours.
For German-English, the winning submission is
DFKI?s logRegFss33 entry, for both Kendall?s ?
with ties penalised and ties ignored, with ? = 0.31
(since this submission has no ties, the two met-
rics give the same ? value). A trivial baseline that
proposes random ranks (with ties allowed) has a
Kendall?s ? with ties penalised of -0.12 (as this
metric penalises the system?s ties that were non-
ties in the reference), and a Kendall?s ? with ties
ignored of 0.08. Most of the submissions per-
formed better than this simple baseline. More in-
terestingly perhaps is the comparison between the
best submission and the performance by an ora-
25
System ID DeltaAvg Spearman ?
? CNGL SVRPLS 11.09 0.55
? DCU-SYMC alltypes 10.13 0.59
SHEF FS 9.76 0.57
CNGL SVR 9.88 0.51
DCU-SYMC combine 9.84 0.59
CMU noB 8.98 0.57
SHEF FS-AL 8.85 0.50
Baseline bb17 SVR 8.52 0.46
CMU full 8.23 0.54
LIMSI 8.15 0.44
TCD-CNGL open 6.03 0.33
TCD-CNGL restricted 5.85 0.31
UMAC 2.74 0.11
Oracle HTER 16.38 1.00
Oracle HBLEU 15.74 0.93
Table 12: Official results for the ranking variant of the WMT13 Quality Estimation Task 1.1. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test. Oracle results that use human-references are also shown for comparison purposes.
System ID MAE RMSE
? SHEF FS 12.42 15.74
SHEF FS-AL 13.02 17.03
CNGL SVRPLS 13.26 16.82
LIMSI 13.32 17.22
DCU-SYMC combine 13.45 16.64
DCU-SYMC alltypes 13.51 17.14
CMU noB 13.84 17.46
CNGL SVR 13.85 17.28
FBK-UEdin extra 14.38 17.68
FBK-UEdin rand-svr 14.50 17.73
LORIA inctrain 14.79 18.34
Baseline bb17 SVR 14.81 18.22
TCD-CNGL open 14.81 19.00
LORIA inctraincont 14.83 18.17
TCD-CNGL restricted 15.20 19.59
CMU full 15.25 18.97
UMAC 16.97 21.94
Oracle HTER 0.00 0.00
Oracle HBLEU (1-HBLEU) 16.85 19.72
Table 13: Official results for the scoring variant of the WMT13 Quality Estimation Task 1.1. The winning submission is
indicated by a ? (it is significantly better than the other submissions according to bootstrap resampling (10k times) with 95%
confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant level
according to the same test. Oracle results that use human-references are also shown for comparison purposes.
26
German-English System ID Kendall?s ? with ties penalised
? DFKI logRegFss33 0.31
DFKI logRegFss24 0.28
CNGL SVRPLSF1 0.17
CNGL SVRF1 0.17
DCU CCG 0.15
UPC AQE+SEM+LM 0.11
UPC AQE+LeM+ALGPR+LM 0.10
DCU baseline+CCG 0.00
Baseline Random-ranks-with-ties -0.12
UMAC EBLEU-I -0.39
UMAC NB-LPR -0.49
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.19
Oracle BLEU (margin 0.01) 0.05
Oracle METEOR-ex (margin 0.00) 0.23
Oracle METEOR-ex (margin 0.01) 0.06
Table 14: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties penalised
? CNGL SVRPLSF1 0.15
CNGL SVRF1 0.13
DFKI logRegL2-411 0.09
DFKI logRegL2-431 0.04
UPC QQE+LeM+ALGPR+LM -0.03
UPC AQE+LeM+ALGPR+LM -0.06
CMU BLEUopt -0.11
Baseline Random-ranks-with-ties -0.23
UMAC EBLEU-A -0.27
UMAC EBLEU-I -0.35
CMU cls -0.63
Oracle Human 1.00
Oracle BLEU (margin 0.00) 0.17
Oracle BLEU (margin 0.02) -0.06
Oracle METEOR-ex (margin 0.00) 0.19
Oracle METEOR-ex (margin 0.02) 0.05
Table 15: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties penalised. The winning submissions are indicated by a ?. Oracle results that use human-references are
also shown for comparison purposes.
27
German-English System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? DFKI logRegFss33 0.31 882/882
DFKI logRegFss24 0.28 882/882
UPC AQE+SEM+LM 0.27 768/882
UPC AQE+LeM+ALGPR+LM 0.24 788/882
DCU CCG 0.18 862/882
CNGL SVRPLSF1 0.17 882/882
CNGL SVRF1 0.17 881/882
Baseline Random-ranks-with-ties 0.08 718/882
DCU baseline+CCG 0.01 874/882
UMAC NB-LPR 0.01 447/882
UMAC EBLEU-I -0.03 558/882
Oracle Human 1.00 882/882
Oracle BLEU (margin 0.00) 0.22 859/882
Oracle BLEU (margin 0.01) 0.27 728/882
Oracle METEOR-ex (margin 0.00) 0.20 869/882
Oracle METEOR-ex (margin 0.01) 0.24 757/882
Table 16: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for German-English, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
English-Spanish System ID Kendall?s ? with ties ignored Nr. of non-ties / Nr. of decisions
? CMU cls 0.23 192/633
CNGL SVRPLSF1 0.16 632/633
CNGL SVRF1 0.13 631/633
DFKI logRegL2-411 0.13 610/633
UPC QQE+LeM+ALGPR+LM 0.11 554/633
UPC AQE+LeM+ALGPR+LM 0.08 554/633
UMAC EBLEU-A 0.07 430/633
DFKI logRegL2-431 0.04 633/633
Baseline Random-ranks-with-ties 0.03 507/633
UMAC EBLEU-I 0.02 407/633
CMU BLEUopt -0.11 633/633
Oracle Human 1.00 633/633
Oracle BLEU (margin 0.00) 0.19 621/633
Oracle BLEU (margin 0.02) 0.26 474/633
Oracle METEOR-ex (margin 0.00) 0.25 623/633
Oracle METEOR-ex (margin 0.02) 0.28 517/633
Table 17: Official results for the Task 1.2 of the WMT13 Quality Estimation shared task for English-Spanish, using as metric
Kendall?s ? with ties ignored. The winning submissions are indicated by a ?. Oracle results that use human-references are also
shown for comparison purposes.
28
cle method that has access to human-created refer-
ences. This oracle uses human references to com-
pute BLEU and METEOR scores for each trans-
lation segment, and consequently computes rank-
ings for the competing translations based on these
scores. To reflect the impact of ties on the two
versions of Kendall?s ? metric we use, we allow
these ranks to be tied if the difference between the
oracle BLEU or METEOR scores is smaller than
a margin (see lower section of Tables 14 and 16,
with margins of 0 and 0.01 for the scores). For ex-
ample, under a regime of BLEU with margin 0.01,
a translation with BLEU score of 0.172 would get
the same rank as a translation with BLEU score of
0.164 (difference of 0.008), but a higher rank than
a translation with BLEU score of 0.158 (difference
of 0.014). Not surprisingly, under the Kendall?s
? with ties penalised the best Oracle BLEU or
METEOR performance happens for a 0.0 mar-
gin (which makes ties possible only for exactly-
matching scores), for a value of ? = 0.19 and
? = 0.23, respectively. Under the Kendall?s ? with
ties ignored, the Oracle BLEU performance for a
0.01 margin (i.e, translations under 1 BLEU point
should be considered as having the same rank)
achieves ? = 0.27, while Oracle METEOR for a
0.01 margin achieves ? = 0.24. These values are
lower than the ? = 0.31 of the winning submis-
sion without access to reference translations, sug-
gesting that quality estimation models are capable
of better modelling translation differences com-
pared to traditional, human reference-based MT
evaluation metrics.
For English-Spanish, under Kendall?s ? with
ties penalised the winning submission is CNGL?s
SVRPLSF1, with ? = 0.15. Under Kendall?s ?
with ties ignored, the best scoring submission is
CMU?s cls with ? = 0.23, but this is achieved
by offering non-tie judgements only for 192 of the
633 total judgements (30% of them). As we dis-
cussed in Section 6.6, the ?Kendall?s ? with ties
ignored? metric is weak with respect to compar-
ing different submissions, since it favours systems
that are do not commit to a given rank and rather
produce a large number of ties. This becomes even
clearer when we look at the performance of the or-
acle methods (Tables 15 and 17). Under Kendall?s
? with ties penalised, ?Oracle BLEU? (margin
0.00) achieves ? = 0.17, while under Kendall?s
? with ties ignored, ?Oracle BLEU? (margin 0.02)
has a ? = 0.26. This results in 474 non-tie deci-
sions (75% of them), and a better ? value com-
pared to ?Oracle BLEU? (margin 0.00), with a
? = 0.19 under the same metric. The oracle values
for both BLEU and METEOR are close to the ?
values of the winning submissions, supporting the
conclusion that quality estimation techniques can
successfully replace traditional, human reference-
based MT evaluation metrics.
Task 1.3 Predicting post-editing time
Results for this task are presented in Table 18.
A third of the submissions was able to beat the
baseline. Among these FBK-UEDIN?s submission
ranked best in terms of MAE, our main metric for
this task, and also achieved the lowest RMSE.
Only three systems were able to beat our base-
line in terms of MAE. Please note that while all
features were available to the participants, our
baseline is actually a competitive system.
The second-best entry, CNGL SVR, reached
the highest Spearman?s rank correlation, our sec-
ondary metric. Furthermore, in terms of this met-
ric all four top-ranking entries, two by CNGL and
FBK-UEDIN respectively, are significantly better
than the baseline (10k bootstrap resampling test
with 95% confidence intervals). As high ranking
submissions also yield strong rank correlation to
the observed post-editing time, we can be confi-
dent that improvements in MAE are not only due
to better handling of extreme cases.
Many participants submitted two variants of
their systems with different numbers of features
and/or machine learning approaches. In Table 18
we can see these are grouped closely together giv-
ing rise to the assumption that the general pool of
available features and thereby the used resources
and strongest features are most relevant for a sys-
tem?s performance. Another hint in that direction
is the observation the top-ranked systems rely on
additional data and resources to generate their fea-
tures.
Task 2 Predicting word-level scores
Results for this task are presented in Table 19 and
20, sorted by macro average F1. Since this is a
new task, we have yet to establish a strong base-
line. For reference we provide a trivial baseline
that predicts the dominant class ? (K)eep ? for ev-
ery token.
The first observation in Table 19 is that this triv-
ial baseline is difficult to beat in terms of accuracy.
However, considering our main metric ? macro-
29
System ID MAE RMSE Pearson?s r Spearman?s ?
? FBK-UEDIN Extra 47.5 82.6 0.65 0.75
? FBK-UEDIN Rand-SVR 47.9 86.7 0.66 0.74
CNGL SVR 49.2 90.4 0.67 0.76
CNGL SVRPLS 49.6 86.6 0.68 0.74
CMU slim 51.6 84.7 0.63 0.68
Baseline bb17 SVR 51.9 93.4 0.61 0.70
DFKI linear6 52.4 84.3 0.64 0.68
CMU full 53.6 92.2 0.58 0.60
DFKI pls8 53.6 88.3 0.59 0.67
TCD-DCU-CNGL SVM2 55.8 98.9 0.47 0.60
TCD-DCU-CNGL SVM1 55.9 99.4 0.48 0.60
SHEF FS 55.9 103.1 0.42 0.61
SHEF FS-AL 64.6 99.1 0.57 0.60
LIMSI elastic 70.6 114.4 0.58 0.64
Table 18: Official results for the Task 1.3 of the WMT13 Quality Estimation shared-task. The winning submissions are
indicated by a ? (they are significantly better than all other submissions according to bootstrap resampling (10k times) with
95% confidence intervals). The systems in the gray area are not different from the baseline system at a statistically significant
level according to the same test.
Keep Change
System ID Accuracy Prec. Recall F1 Prec. Recall F1 Macro F1
? LIG FS BIN 0.74 0.79 0.86 0.82 0.56 0.43 0.48 0.65
? LIG BOOST BIN 0.74 0.78 0.88 0.83 0.57 0.37 0.45 0.64
CNGL GLM 0.70 0.76 0.86 0.80 0.47 0.31 0.38 0.59
UMAC NB 0.56 0.82 0.49 0.62 0.37 0.73 0.49 0.55
CNGL GLMd 0.71 0.74 0.93 0.82 0.51 0.19 0.28 0.55
UMAC CRF 0.71 0.72 0.98 0.83 0.49 0.04 0.07 0.45
Baseline (one class) 0.71 0.71 1.00 0.83 0.00 0.00 0.00 0.42
Table 19: Official results for Task 2: binary classification on word level of the WMT13 Quality Estimation shared-task. The
winning submissions are indicated by a ?.
System ID F1 Keep F1 Substitute F1 Delete Micro-F1 Macro-F1
? LIG FS MULT 0.83 0.44 0.072 0.72 0.45
? LIG ALL MULT 0.83 0.45 0.064 0.72 0.45
UMAC NB 0.62 0.43 0.042 0.52 0.36
CNGL GLM 0.83 0.18 0.028 0.71 0.35
CNGL GLMd 0.83 0.14 0.034 0.72 0.34
UMAC CRF 0.83 0.04 0.012 0.71 0.29
Baseline (one class) 0.83 0.00 0.000 0.71 0.28
Table 20: Official results for Task 2: multiclass classification on word level of the WMT13 Quality Estimation shared-task.
The winning submissions are indicated by a ?.
30
average F1 ? it is clear that all systems outperform
the baseline. The winning systems by LIG for the
binary task are also the top ranking systems on the
multiclass task.
While promising results are found for the bi-
nary variant of the task where systems are able to
achieve an F1 of almost 0.5 for the relevant class
? Change, the multiclass prediction variant of the
task seem to suffer from its severe class imbalance.
In fact, none of the systems shows good perfor-
mance when predicting deletions.
6.9 Discussion
In what follows, we discuss the main accomplish-
ments of this shared task starting from the goals
we had previously identified for it.
Explore various granularity levels for the
quality-prediction task The decision on which
level of granularity quality estimation is applied
depends strongly on the intended application. In
Task 2 we tested binary word-level classification
in a post-editing setting. If such annotation is pre-
sented through a user interface we imagine that
words marked as incorrect would be hidden from
the editor, highlighted as possibly wrong or that a
list of alternatives would we generated.
With respect to the poor improvements over
trivial baselines, we consider that the results for
word-level prediction could be mostly connected
to limitations of the datasets provided, which are
very small for word-level prediction, as compared
to successful previous work such as (Bach et al,
2011). Despite the limited amount of training
data, several systems were able to predict dubious
words (binary variant of the task), showing that
this can be a promising task. Extending the granu-
larity even further by predicting the actual editing
action necessary for a word yielded less positive
results than the binary setting.
We cannot directly compare sentence- and
word-level results. However, since sentence-level
predictions can benefit from more information
available and therefore more signal on which the
prediction is based, the natural conclusion is that,
if there is a choice in the prediction granularity,
to opt for the coarser one possible (i.e., sentence-
level over word-level). But certain applications
may require finer granularity levels, and therefore
word-level predictions can still be very valuable.
Explore the prediction of more objective scores
Given the multitude of possible applications for
quality estimation we must decide which predicted
values are both useful and accurate. In this year?s
task we have attempted to address the useful-
ness criterion by moving from the subjective, hu-
man judgement-based scores, to the prediction of
scores that can be more easily interpreted for prac-
tical applications: post-editing distance or types of
edits (word-level), post-editing time, and ranking
of alternative translations.
The general promise of using objective scores is
that predicting a value that is related to the use case
will make quality estimation more applicable and
yield lower deviance compared to the use of proxy
metrics. The magnitude of this benefit should be
sufficient to account for the possible additional ef-
fort related to collecting such scores.
While a direct comparison between the differ-
ent types of scores used for this year?s tasks is not
possible as they are based on different datasets, if
we compare last year?s task on predicting 1-5 lik-
ert scores (and generating an overall ranking of all
translations in the test set) with this year?s Task
1.1, which is virtually the same, but using post-
editing distance as gold-label, we see that the num-
ber of systems that outperform the baseline 18 is
proportionally larger this year. We can also notice
a higher relative improvement of these submis-
sions over the baseline system. While this could
simply be a consequence of progress in the field, it
may also provide an indication that objective met-
rics are more suitable for the problem.
Particularly with respect to post-editing time,
given that this label has a long tailed distribution
and is not trivial to measure even in a controlled
environment, the results of Task 1.3 are encour-
aging. Comparison with the better results seen
on Tasks 1.1 and 1.2, however, suggests that, for
Task 1.3, additional data processing, filtering, and
modelling (including modelling translator-specific
traits such as their variance in time) is required, as
evidenced in (Cohn and Specia, 2013).
Explore the use of quality estimation tech-
niques to replace reference-based MT evalua-
tion metrics When it comes to the task of au-
tomatically ranking alternative translations gener-
ated by different MT systems, the traditional use
of reference-based MT evaluation metrics is chal-
lenged by the findings of this task.
The top ranking quality estimation submissions
18The two baselines are exactly the same, and therefore the
comparison is meaningful.
31
to Task 1.2 have performances that outperform or
are at least at the same level with the ones that
involve the use of human references. The most in-
teresting property of these techniques is that, be-
ing reference-free, they can be used for any source
sentences, and therefore are ready to be deployed
for arbitrary texts.
An immediate application for this capability is
a procedure by which MT system-selection is per-
formed, based on the output of such quality esti-
mators. Additional measurements are needed to
determine the level of improvement in translation
quality that the current performance of these tech-
niques can achieve in a system-selection scenario.
Identify new and effective quality indicators
Quality indicators, or features, are core to the
problem of quality estimation. One significant dif-
ference this year with respect to previous year was
the availability of QUEST, a framework for the ex-
traction of a large number of features. A few sub-
missions used these larger sets ? as opposed to the
17 baseline features used in the 2012 edition ? as
their starting point, to which they added other fea-
tures. Most features available in this framework,
however, had already been used in previous work.
Novel families of features used this year which
seems to have played an important role are those
proposed by CNGL. They include a number of
language and MT-system independent monolin-
gual and bilingual similarity metrics between the
sentences for prediction and corpora of the lan-
guage pair under consideration. Based on standard
regression algorithm (the same used by the base-
line system), the submissions from CNGL using
such feature families topped many of the tasks.
Another interesting family of features is that
used by TCD-CNGL and TCD-DCU-CNGL for
Tasks 1.1 and 1.3. These were borrowed from
work on style or authorship identification. The as-
sumption is that low/high quality translations can
be characterised by some patterns which are fre-
quent and/or differ significantly from patterns be-
longing to the opposite category.
Like in last year?s task, the vast majority of
the participating systems used external resources
in addition to those provided for the task, par-
ticularly for linguistically-oriented features, such
as parsers, part-of-speech taggers, named entity
recognizers, etc. A novel set of syntactic fea-
tures based on Combinatory Categorial Grammar
(CCG) performed reasonably well in Task 1.2:
with six CCG-based features and no additional
features, the system outperformed the baseline
system and also a second submission where the
17 baseline features were added. This highlights
the potential of linguistically-motivated features
for the problem.
As expected, different feature sets were used
for different tasks. This is essential for Task 2,
where word-level features are certainly necessary.
For example, LIG used a number of lexical fea-
tures such as part-of-speech tag, word-posterior
probabilities, syntactic (constituent label, distance
to the constituent tree root, and target and source
polysemy count). For submissions where a se-
quence labelling algorithm such as a Conditional
Random Fields was used for prediction, the inter-
dependencies between adjacent words and labels
was also modelled though features.
Pseudo-references, i.e., scores from standard
evaluation metrics such as BLEU based on trans-
lations generated by an alternative MT system as
?reference?, featured in more than half of the sub-
missions for sentence-level tasks. This is not sur-
prising given their performance in previous work
on quality estimation.
Identify effective machine learning techniques
for all variants of the quality estimation task
For the sentence-level tasks, standard regression
methods such as SVR performed well as in the
previous edition of the shared task, topping the
results for the ranking variant of Task 1.1, both
first and second place. In fact this algorithm was
used by most submissions that outperformed the
baseline. An alternative algorithm to SVR with
very promising results and which was introduced
for the problem this year is that of Gaussian Pro-
cesses. It was used by SHEF, the winning submis-
sion in the scoring variant of Task 1.1, which also
performed well in the ranking variant, despite its
hyperparameters having been optimised for scor-
ing only. Algorithms behave similarly for Task
1.3, with SVR performing particularly well.
For Task 1.2, logistic regression performed the
best or among the best, along with SVR. One of
the most effective approach for this task, however,
appears to be one that is better tailored for the
task, namely pair-wise decomposition for ranking.
This approach benefits from transforming a k-way
ranking problem into a series of simpler, 2-way
ranking problems, which can be more accurately
solved. Another approach that shows promise is
32
that of ensemble of regressors, in which the output
is the results combining the predictions of differ-
ent regression models.
Linear-chain Conditional Random Fields are a
popular model of choice for sequence labelling
tasks and have been successfully used by several
participants in Task 2, along with discriminatively
trained Hidden Markov Models and Na??ve Bayes.
As in the previous edition, feature engineer-
ing and feature selection prior to model learning
were important components in many submissions.
However, the role of individual features is hard
to judge separately from the role of the machine
learning techniques employed.
Establish the state of the art performance All
four tasks addressed in this shared task have
achieved a dual role that is important for the re-
search community: (i) to make publicly available
new data sets that can serve to compare different
approaches and contributions; and (ii) to estab-
lish the present state-of-the-art performance in the
field, so that progress can be easily measured and
tracked. In addition, the public availability of the
scoring scripts makes evaluation and direct com-
parison straightforward.
Many participants submitted predictions for
several tasks. Comparison of the results shows
that there is little overlap between the best sys-
tems when the predicted value is varied. While
we did not formally require the participants to use
similar systems across tasks, these results indicate
that specialised systems with features selected de-
pending on the predicted variable can in fact be
beneficial.
As we mentioned before, compared to the pre-
vious edition of the task, we noticed (for Task
1.1) a larger relative improvement of scores over
the baseline system, as well as a larger propor-
tion of systems outperforming the baseline sys-
tems, which are a good indication that the field is
progressing over the years. For example, in the
scoring variant of Task 1.1, last year only 5 out of
20 systems (i.e. 25% of the systems) were able to
significantly outperform the baseline. This year, 9
out 16 systems (i.e. 56%) outperformed the same
baseline. Last year, the relative improvement of
the winning submission with respect to the base-
line system was 13%, while this year the relative
improvement is of 19%.
Overall, the tables of results presented in Sec-
tion 6.8 give a comprehensive view of the current
state-of-the-art on the data sets used for this shared
task, as well as indications on how much room
there still is for improvement via figures from ora-
cle methods. As a result, people interested in con-
tributing to research in these machine translation
quality estimation tasks will be able to do so in a
principled way, with clearly established state-of-
the-art levels and straightforward means of com-
parison.
7 Summary
As in previous incarnations of this workshop we
carried out an extensive manual and automatic
evaluation of machine translation performance,
and we used the human judgements that we col-
lected to validate automatic metrics of translation
quality. We also refined last year?s quality estima-
tion task, asking for methods that predict sentence-
level post-editing effort and time, rank translations
from alternative systems, and pinpoint words in
the output that are more likely to be wrong.
As in previous years, all data sets generated by
this workshop, including the human judgments,
system translations and automatic scores, are pub-
licly available for other researchers to analyze.19
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Google, Microsoft and Yandex.
We would also like to thank our colleagues Ma-
tous? Macha?c?ek and Martin Popel for detailed dis-
cussions.
References
Allauzen, A., Pe?cheux, N., Do, Q. K., Dinarelli,
M., Lavergne, T., Max, A., Le, H.-S., and Yvon,
F. (2013). LIMSI @ WMT13. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 60?67, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Almaghout, H. and Specia, L. (2013). A CCG-
based Quality Estimation Metric for Statistical
Machine Translation. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Ammar, W., Chahuneau, V., Denkowski, M., Han-
neman, G., Ling, W., Matthews, A., Murray,
19http://statmt.org/wmt13/results.html
33
K., Segall, N., Lavie, A., and Dyer, C. (2013).
The CMU machine translation systems at WMT
2013: Syntax, synthetic translation options, and
pseudo-references. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 68?75, Sofia, Bulgaria. Association for
Computational Linguistics.
Avramidis, E. (2012). Comparative Quality Es-
timation: Automatic Sentence-Level Ranking
of Multiple Machine Translation Outputs. In
Proceedings of 24th International Conference
on Computational Linguistics, pages 115?132,
Mumbai, India.
Avramidis, E. and Popovic, M. (2013). Selecting
feature sets for comparative and time-oriented
quality estimation of machine translation out-
put. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 327?
334, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Avramidis, E., Popovic?, M., Vilar, D., and Bur-
chardt, A. (2011). Evaluate with confidence es-
timation: Machine ranking of translation out-
puts using grammatical features. In Proceed-
ings of the Sixth Workshop on Statistical Ma-
chine Translation.
Aziz, W., Mitkov, R., and Specia, L. (2013).
Ranking Machine Translation Systems via Post-
Editing. In Proc. of Text, Speech and Dialogue
(TSD), Lecture Notes in Artificial Intelligence,
Berlin / Heidelberg. Za?padoc?eska? univerzita v
Plzni, Springer Verlag.
Bach, N., Huang, F., and Al-Onaizan, Y. (2011).
Goodness: A method for measuring machine
translation confidence. In Proceedings of the
49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 211?219, Portland, Ore-
gon, USA.
Beck, D., Shah, K., Cohn, T., and Specia, L.
(2013). SHEF-Lite: When less is more for
translation quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 335?340, Sofia, Bulgaria.
Association for Computational Linguistics.
Bic?ici, E., Groves, D., and van Genabith, J. (2013).
Predicting sentence translation quality using ex-
trinsic and language independent features. Ma-
chine Translation.
Bic?ici, E. and van Genabith, J. (2013). CNGL-
CORE: Referential translation machines for
measuring semantic similarity. In *SEM 2013:
The Second Joint Conference on Lexical and
Computational Semantics, Atlanta, Georgia,
USA. Association for Computational Linguis-
tics.
Bicici, E. (2013a). Feature decay algorithms
for fast deployment of accurate statistical ma-
chine translation systems. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 76?82, Sofia, Bulgaria. Associa-
tion for Computational Linguistics.
Bicici, E. (2013b). Referential translation ma-
chines for quality estimation. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 341?349, Sofia, Bulgaria.
Association for Computational Linguistics.
B??lek, K. and Zeman, D. (2013). CUni multilin-
gual matrix in the WMT 2013 shared task. In
Proceedings of the Eighth Workshop on Statis-
tical Machine Translation, pages 83?89, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Bojar, O., Kos, K., and Marec?ek, D. (2010). Tack-
ling Sparse Data Issue in Machine Translation
Evaluation. In Proceedings of the ACL 2010
Conference Short Papers, pages 86?91, Upp-
sala, Sweden. Association for Computational
Linguistics.
Bojar, O., Rosa, R., and Tamchyna, A. (2013).
Chimera ? three heads for English-to-Czech
translation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
90?96, Sofia, Bulgaria. Association for Compu-
tational Linguistics.
Borisov, A., Dlougach, J., and Galinskaya, I.
(2013). Yandex school of data analysis ma-
chine translation systems for WMT13. In Pro-
ceedings of the Eighth Workshop on Statistical
Machine Translation, pages 97?101, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
34
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montre?al, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Buck, C., Turchi, M.,
and Negri, M. (2013). FBK-UEdin participa-
tion to the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 350?
356, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Charniak, E. and Johnson, M. (2005). Coarse-to-
fine n-best parsing and maxent discriminative
reranking. In Proceedings of the 43rd Annual
Meeting on Association for Computational Lin-
guistics, pages 173?180. Association for Com-
putational Linguistics.
Cho, E., Ha, T.-L., Mediani, M., Niehues, J., Her-
rmann, T., Slawik, I., and Waibel, A. (2013).
The Karlsruhe Institute of Technology transla-
tion systems for the WMT 2013. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 102?106, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling Anno-
tator Bias with Multi-task Gaussian Processes:
An Application to Machine Translation Quality
Estimation. In Proceedings of the 51st Annual
Meeting of the Association for Computational
Linguistics (to appear).
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
de Marneffe, M.-C., MacCartney, B., and Man-
ning, C. D. (2006). Generating typed depen-
dency parses from phrase structure parses. In
Proceedings of LREC-06, pages 449?454.
Denkowski, M. and Lavie, A. (2010). Meteor-next
and the meteor paraphrase tables: improved
evaluation support for five target languages. In
Proceedings of the Joint Fifth Workshop on Sta-
tistical Machine Translation and MetricsMATR,
WMT ?10, pages 339?342, Stroudsburg, PA,
USA. Association for Computational Linguis-
tics.
Durgar El-Kahlout, I. and Mermer, C. (2013).
TU?btak-blgem german-english machine trans-
lation systems for w13. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 107?111, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Durrani, N., Fraser, A., Schmid, H., Sajjad, H.,
and Farkas, R. (2013a). Munich-Edinburgh-
Stuttgart submissions of OSM systems at
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
120?125, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Durrani, N., Haddow, B., Heafield, K., and Koehn,
P. (2013b). Edinburgh?s machine translation
systems for European language pairs. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 112?119, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
35
Eidelman, V., Wu, K., Ture, F., Resnik, P., and Lin,
J. (2013). Towards efficient large-scale feature-
rich statistical machine translation. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 126?131, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Fleiss, J. L. (1971). Measuring nominal scale
agreement among many raters. Psychological
Bulletin, 76(5):378?382.
Formiga, L., Costa-jussa`, M. R., Marin?o, J. B.,
Fonollosa, J. A. R., Barro?n-Ceden?o, A., and
Marquez, L. (2013a). The TALP-UPC phrase-
based translation systems for WMT13: System
combination with morphology generation, do-
main adaptation and corpus filtering. In Pro-
ceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 132?138, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Formiga, L., Gonza`lez, M., Barro?n-Ceden?o, A.,
Fonollosa, J. A. R., and Marquez, L. (2013b).
The TALP-UPC approach to system selection:
Asiya features and pairwise classification using
random forests. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 357?362, Sofia, Bulgaria. Association for
Computational Linguistics.
Formiga, L., Ma`rquez, L., and Pujantell, J.
(2013c). Real-life translation quality estimation
for mt system selection. In Proceedings of MT
Summit XIV (to appear), Nice, France.
Galus?c?a?kova?, P., Popel, M., and Bojar, O. (2013).
PhraseFix: Statistical post-editing of TectoMT.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 139?145,
Sofia, Bulgaria. Association for Computational
Linguistics.
Gispert, A., Blackwood, G., Iglesias, G., and
Byrne, W. (2013). N-gram posterior probabil-
ity confidence measures for statistical machine
translation: an empirical study. Machine Trans-
lation, 27:85?114.
Gonza`lez, M., Gime?nez, J., and Ma`rquez, L.
(2012). A graphical interface for mt evaluation
and error analysis. In Proceedings of the ACL
2012 System Demonstrations, pages 139?144,
Jeju Island, Korea.
Green, S., Cer, D., Reschke, K., Voigt, R., Bauer,
J., Wang, S., Silveira, N., Neidert, J., and Man-
ning, C. D. (2013). Feature-rich phrase-based
translation: Stanford University?s submission to
the WMT 2013 translation task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 146?151, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hall, M., Frank, E., Holmes, G., Pfahringer,
B., Reutemann, P., and Witten, I. H. (2009).
The weka data mining software: an update.
SIGKDD Explor. Newsl., 11(1):10?18.
Han, A. L.-F., Wong, D. F., Chao, L. S., Lu, Y., He,
L., Wang, Y., and Zhou, J. (2013). A descrip-
tion of tunable machine translation evaluation
systems in WMT13 metrics task. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 412?419, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Hildebrand, S. and Vogel, S. (2013). MT quality
estimation: The CMU system for WMT?13. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 371?377, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Hosmer, D. (1989). Applied logistic regression.
Wiley, New York, 8th edition.
Huet, S., Manishina, E., and Lefe`vre, F.
(2013). Factored machine translation systems
for Russian-English. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 152?155, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Hunt, E., Martin, J., and Stone, P. (1966). Experi-
ments in Induction. Academic Press, New York.
Kaplan, R., Riezler, S., King, T., Maxwell, J.,
Vasserman, A., and Crouch, R. (2004). Speed
and accuracy in shallow and deep stochastic
parsing. In Proceedings of the Human Lan-
guage Technology Conference and the 4th An-
nual Meeting of the North American Chapter of
the Association for Computational Linguistics
(HLT/NAACL 04).
36
Kauchak, D. and Barzilay, R. (2006). Paraphras-
ing for automatic evaluation. In Proceedings
of the main conference on Human Language
Technology Conference of the North American
Chapter of the Association of Computational
Linguistics, HLT-NAACL ?06, pages 455?462,
Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Koehn, P. (2012). Simulating human judgment in
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Kononenko, I. (1994). Estimating attributes: anal-
ysis and extensions of RELIEF. In Proceedings
of the European conference on machine learn-
ing on Machine Learning, pages 171?182, Se-
caucus, NJ, USA. Springer-Verlag New York,
Inc.
Kos, K. and Bojar, O. (2009). Evaluation of Ma-
chine Translation Metrics for Czech as the Tar-
get Language. Prague Bulletin of Mathematical
Linguistics, 92:135?147.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Langlois, D., Raybaud, S., and Sma??li, K. (2012).
Loria system for the wmt12 quality estimation
shared task. In Proceedings of the Seventh
Workshop on Statistical Machine Translation,
pages 114?119, Montre?al, Canada.
Langlois, D. and Smaili, K. (2013). LORIA sys-
tem for the WMT13 quality estimation shared
task. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 378?
383, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Le, H. S., Oparin, I., Allauzen, A., Gauvain, J.-L.,
and Yvon, F. (2011). Structured output layer
neural network language model. In ICASSP,
pages 5524?5527.
Lin, C.-J., Weng, R. C., and Keerthi, S. S. (2007).
Trust region Newton methods for large-scale lo-
gistic regression. In Proceedings of the 24th
international conference on Machine learning
- ICML ?07, pages 561?568, New York, New
York, USA. ACM Press.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Macha?c?ek, M. and Bojar, O. (2013). Results of the
WMT13 metrics shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 43?49, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Matusov, E. and Leusch, G. (2013). Omnifluent
English-to-French and Russian-to-English sys-
tems for the 2013 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 156?161, Sofia, Bulgaria. Association for
Computational Linguistics.
McCallum, A. K. (2002). MALLET: a machine
learning for language toolkit.
Miceli Barone, A. V. and Attardi, G. (2013).
Pre-reordering for machine translation using
transition-based walks on dependency parse
trees. In Proceedings of the Eighth Workshop
on Statistical Machine Translation, pages 162?
167, Sofia, Bulgaria. Association for Computa-
tional Linguistics.
Moreau, E. and Rubino, R. (2013). An approach
using style classification features for quality es-
timation. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
427?432, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Nadejde, M., Williams, P., and Koehn, P. (2013).
Edinburgh?s syntax-based machine translation
systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
168?174, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Okita, T., Liu, Q., and van Genabith, J. (2013).
Shallow semantically-informed PBSMT and
HPBSMT. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
175?182, Sofia, Bulgaria. Association for Com-
putational Linguistics.
O?zgu?r, A., O?zgu?r, L., and Gu?ngo?r, T. (2005). Text
37
categorization with class-based and corpus-
based keyword selection. In Proceedings of
the 20th International Conference on Computer
and Information Sciences, ISCIS?05, pages
606?615, Berlin, Heidelberg. Springer.
Peitz, S., Mansour, S., Huck, M., Freitag, M.,
Ney, H., Cho, E., Herrmann, T., Mediani,
M., Niehues, J., Waibel, A., Allauzen, A.,
Khanh Do, Q., Buschbeck, B., and Wand-
macher, T. (2013a). Joint WMT 2013 submis-
sion of the QUAERO project. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 183?190, Sofia, Bulgaria.
Association for Computational Linguistics.
Peitz, S., Mansour, S., Peter, J.-T., Schmidt, C.,
Wuebker, J., Huck, M., Freitag, M., and Ney,
H. (2013b). The RWTH aachen machine trans-
lation system for WMT 2013. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 191?197, Sofia, Bulgaria.
Association for Computational Linguistics.
Pighin, D., Formiga, L., and Ma`rquez, L.
(2012). A graph-based strategy to streamline
translation quality assessments. In Proceed-
ings of the Tenth Conference of the Associa-
tion for Machine Translation in the Americas
(AMTA?2012), San Diego, USA.
Pino, J., Waite, A., Xiao, T., de Gispert, A., Flego,
F., and Byrne, W. (2013). The University of
Cambridge Russian-English system at WMT13.
In Proceedings of the Eighth Workshop on Sta-
tistical Machine Translation, pages 198?203,
Sofia, Bulgaria. Association for Computational
Linguistics.
Post, M., Ganitkevitch, J., Orland, L., Weese, J.,
Cao, Y., and Callison-Burch, C. (2013). Joshua
5.0: Sparser, better, faster, server. In Proceed-
ings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 204?210, Sofia, Bul-
garia. Association for Computational Linguis-
tics.
Rubino, R., Toral, A., Corte?s Va??llo, S., Xie, J.,
Wu, X., Doherty, S., and Liu, Q. (2013a). The
CNGL-DCU-Prompsit translation systems for
WMT13. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages
211?216, Sofia, Bulgaria. Association for Com-
putational Linguistics.
Rubino, R., Wagner, J., Foster, J., Roturier, J.,
Samad Zadeh Kaljahi, R., and Hollowood, F.
(2013b). DCU-Symantec at the WMT 2013
quality estimation shared task. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 390?395, Sofia, Bulgaria.
Association for Computational Linguistics.
Sajjad, H., Smekalova, S., Durrani, N., Fraser, A.,
and Schmid, H. (2013). QCRI-MES submis-
sion at WMT13: Using transliteration mining
to improve statistical machine translation. In
Proceedings of the Eighth Workshop on Statisti-
cal Machine Translation, pages 217?222, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Schmidt, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
the International Conference on New Methods
in Natural Language Processing.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An In-
vestigation on the Effectiveness of Features for
Translation Quality Estimation. In Proceedings
of MT Summit XIV (to appear), Nice, France.
Singh, A. K., Wisniewski, G., and Yvon, F.
(2013). LIMSI submission for the WMT?13
quality estimation task: an experiment with n-
gram posteriors. In Proceedings of the Eighth
Workshop on Statistical Machine Translation,
pages 396?402, Sofia, Bulgaria. Association for
Computational Linguistics.
Smith, J., Saint-Amand, H., Plamada, M., Koehn,
P., Callison-Burch, C., and Lopez, A. (2013).
Dirt cheap web-scale parallel text from the
Common Crawl. In Proceedings of the 2013
Conference of the Association for Computa-
tional Linguistics (ACL 2013), Sofia, Bulgaria.
Association for Computational Linguistics.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Snover, M., Madnani, N., Dorr, B. J., and
Schwartz, R. (2009). Fluency, adequacy, or
hter?: exploring different human judgments
with a tunable mt metric. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
38
lation, StatMT ?09, pages 259?268, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Soricut, R., Bach, N., and Wang, Z. (2012). The
SDL Language Weaver Systems in the WMT12
Quality Estimation Shared Task. In Proceed-
ings of the 7th Workshop on Statistical Machine
Translation, pages 145?151.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L. (2011). Exploiting Objective Annota-
tions for Measuring Translation Post-editing Ef-
fort. In Proceedings of the 15th Conference of
the European Association for Machine Transla-
tion, pages 73?80, Leuven.
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria. Association for Computational Lin-
guistics.
Stolcke, A. (2002). SRILM ? An extensible lan-
guage modeling toolkit. In Proceedings of the
7th International Conference on Spoken Lan-
guage Processing (ICSLP 2002), pages 901?
904.
Stone, M. and Brooks, R. J. (1990). Contin-
uum regression: cross-validated sequentially
constructed prediction embracing ordinary least
squares, partial least squares and principal com-
ponents regression. Journal of the Royal
Statistical Society Series B Methodological,
52(2):237?269.
Stymne, S., Hardmeier, C., Tiedemann, J., and
Nivre, J. (2013). Tunable distortion limits and
corpus cleaning for SMT. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 223?229, Sofia, Bulgaria. Associ-
ation for Computational Linguistics.
Tantug, A. C., Oflazer, K., and El-Kahlout, I. D.
(2008). BLEU+: a Tool for Fine-Grained BLEU
Computation. In (ELRA), E. L. R. A., edi-
tor, Proceedings of the Sixth International Lan-
guage Resources and Evaluation (LREC?08),
Marrakech, Morocco.
Weller, M., Kisselew, M., Smekalova, S., Fraser,
A., Schmid, H., Durrani, N., Sajjad, H., and
Farkas, R. (2013). Munich-Edinburgh-Stuttgart
submissions at WMT13: Morphological and
syntactic processing for SMT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 230?237, Sofia, Bulgaria.
Association for Computational Linguistics.
39
A Pairwise System Comparisons by Human Judges
Tables 21?30 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according bootstrap resampling (p ?
0.05). Gray lines separate clusters based on non-overlapping rank ranges.
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
M
ES
UE
DI
N
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
CU
-ZE
M
AN
CU
-TA
M
CH
YN
A
DC
U-
FD
A
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .50 .48? .43? .47? .43? .44? .38? .32? .25? .26?
ONLINE-B .50 ? .46? .48? .47? .49 .44? .40? .39? .29? .27?
MES .52? .54? ? .49 .47? .44? .45? .42? .41? .27? .25?
UEDIN .57? .52? .51 ? .51 .48? .47? .42? .39? .28? .25?
ONLINE-A .53? .53? .53? .49 ? .48 .51 .44? .42? .31? .30?
UEDIN-SYNTAX .57? .51 .56? .52? .52 ? .51 .43? .41? .29? .26?
CU-ZEMAN .56? .56? .55? .53? .49 .49 ? .45? .42? .32? .29?
CU-TAMCHYNA .62? .60? .58? .58? .56? .57? .55? ? .46? .35? .32?
DCU-FDA .68? .61? .59? .61? .58? .59? .58? .54? ? .32? .32?
JHU .75? .71? .73? .72? .69? .71? .68? .65? .68? ? .46?
SHEF-WPROA .74? .73? .75? .75? .70? .74? .71? .68? .68? .54? ?
score .60 .58 .57 .56 .54 .54 .53 .48 .45 .32 .29
rank 1 2-3 2-4 3-5 4-7 5-7 6-7 8 9 10 11
Table 21: Head to head comparison, ignoring ties, for Czech-English systems
CU
-B
OJ
AR
CU
-D
EP
FI
X
ON
LI
NE
-B
UE
DI
N
CU
-ZE
M
AN
M
ES
ON
LI
NE
-A
CU
-PH
RA
SE
FI
X
CU
-TE
CT
OM
T
CO
M
M
ER
CI
AL
-1
CO
M
M
ER
CI
AL
-2
SH
EF
-W
PR
OA
CU-BOJAR ? .51 .47? .44? .42? .43? .48 .41? .37? .39? .38? .33?
CU-DEPFIX .49 ? .48? .42? .43? .41? .47? .42? .40? .40? .39? .34?
ONLINE-B .53? .52? ? .47? .44? .44? .44? .44? .44? .41? .36? .34?
UEDIN .56? .58? .53? ? .47? .47? .48 .45? .44? .42? .43? .38?
CU-ZEMAN .58? .57? .56? .53? ? .49 .49 .48? .46? .47? .47? .35?
MES .57? .59? .56? .53? .51 ? .50 .47? .46? .43? .44? .42?
ONLINE-A .52 .53? .56? .52 .51 .50 ? .52 .47? .47? .47? .46?
CU-PHRASEFIX .59? .58? .56? .55? .52? .53? .48 ? .49 .48? .49 .42?
CU-TECTOMT .63? .60? .56? .56? .54? .54? .53? .51 ? .46? .46? .40?
COMMERCIAL-1 .61? .60? .59? .58? .53? .57? .53? .52? .54? ? .49 .42?
COMMERCIAL-2 .62? .61? .64? .57? .53? .56? .53? .51 .54? .51 ? .43?
SHEF-WPROA .67? .66? .66? .62? .65? .58? .54? .58? .60? .58? .57? ?
score .58 .57 .56 .52 .50 .50 .49 .48 .47 .45 .45 .38
rank 1-2 1-2 3 4 5-7 5-7 5-8 7-9 8-9 10-11 10-11 12
Table 22: Head to head comparison, ignoring ties, for English-Czech systems
40
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N-
SY
NT
AX
UE
DI
N
QU
AE
RO
KI
T
M
ES
RW
TH
-JA
NE
M
ES
-SZ
EG
ED
-R
EO
RD
ER
-SP
LI
T
LI
M
SI
-N
CO
DE
-SO
UL
TU
BI
TA
K
UM
D
DC
U
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
DE
SR
T
ONLINE-B ? .48 .44? .37? .44? .41? .42? .40? .35? .37? .32? .31? .31? .27? .23? .18? .16?
ONLINE-A .52 ? .47 .45? .47 .43? .42? .41? .44? .40? .35? .36? .34? .31? .27? .25? .21?
UEDIN-SYNTAX .56? .53 ? .48 .46? .48? .46? .46? .45? .45? .35? .35? .34? .28? .25? .20? .19?
UEDIN .63? .55? .52 ? .51 .46? .47? .49 .44? .43? .39? .34? .35? .32? .28? .24? .22?
QUAERO .56? .53 .54? .49 ? .49 .52 .44? .46? .44? .39? .38? .37? .30? .31? .25? .21?
KIT .59? .57? .52? .54? .51 ? .45? .51 .43? .46? .37? .38? .41? .35? .31? .25? .21?
MES .58? .58? .54? .53? .48 .55? ? .49 .49 .46? .44? .37? .40? .34? .30? .26? .20?
RWTH-JANE .60? .59? .54? .51 .56? .49 .51 ? .46? .50 .45? .46? .47? .38? .33? .28? .20?
MES-SZEGED-REORDER-SPLIT .65? .56? .55? .56? .54? .57? .51 .54? ? .53? .44? .41? .41? .36? .34? .31? .21?
LIMSI-NCODE-SOUL .63? .60? .55? .57? .56? .54? .54? .50 .47? ? .51 .45? .43? .37? .34? .30? .22?
TUBITAK .68? .65? .65? .61? .61? .63? .56? .55? .56? .49 ? .48? .49 .39? .41? .30? .25?
UMD .69? .64? .65? .66? .62? .62? .63? .54? .59? .55? .52? ? .48? .41? .40? .33? .27?
DCU .69? .66? .66? .65? .63? .59? .60? .53? .59? .57? .51 .52? ? .41? .38? .37? .25?
CU-ZEMAN .73? .69? .72? .68? .70? .65? .66? .62? .64? .63? .61? .59? .59? ? .44? .43? .29?
JHU .77? .73? .75? .72? .69? .69? .70? .67? .66? .66? .59? .60? .62? .56? ? .43? .30?
SHEF-WPROA .82? .75? .80? .76? .75? .75? .74? .72? .69? .70? .70? .67? .63? .57? .57? ? .41?
DESRT .84? .79? .81? .78? .79? .79? .80? .80? .79? .78? .75? .73? .75? .71? .70? .59? ?
score .66 .62 .60 .58 .58 .57 .56 .54 .53 .52 .48 .46 .46 .39 .36 .31 .23
rank 1 2-3 2-3 4-5 4-5 5-7 6-7 8-9 8-10 9-10 11 12-13 12-13 14 15 16 17
Table 23: Head to head comparison, ignoring ties, for German-English systems
ON
LI
NE
-B
PR
OM
T
UE
DI
N-
SY
NT
AX
ON
LI
NE
-A
UE
DI
N
KI
T
ST
AN
FO
RD
LI
M
SI
-N
CO
DE
-SO
UL
M
ES
-R
EO
RD
ER
JH
U
CU
-ZE
M
AN
TU
BI
TA
K
UU SH
EF
-W
PR
OA
RW
TH
-JA
NE
ONLINE-B ? .55? .50 .45? .45? .34? .37? .37? .37? .32? .32? .33? .24? .21? .26?
PROMT .45? ? .48? .50 .43? .40? .39? .36? .37? .31? .31? .32? .27? .24? .27?
UEDIN-SYNTAX .50 .52? ? .57? .45? .43? .38? .41? .39? .38? .33? .33? .26? .25? .22?
ONLINE-A .55? .50 .43? ? .51 .42? .48 .41? .36? .44? .44? .38? .32? .27? .29?
UEDIN .55? .57? .55? .49 ? .52 .45? .45? .42? .43? .37? .34? .29? .27? .31?
KIT .66? .60? .57? .58? .48 ? .48 .45? .42? .36? .39? .40? .30? .29? .26?
STANFORD .63? .61? .62? .52 .55? .52 ? .50 .44? .48 .44? .43? .34? .29? .32?
LIMSI-NCODE-SOUL .63? .64? .59? .59? .55? .55? .50 ? .44? .44? .44? .47? .40? .34? .33?
MES-REORDER .63? .63? .61? .64? .58? .58? .56? .56? ? .50 .46? .49 .38? .37? .34?
JHU .68? .69? .62? .56? .57? .64? .52 .56? .50 ? .48? .45? .36? .37? .34?
CU-ZEMAN .68? .69? .67? .56? .63? .61? .56? .56? .54? .52? ? .48 .40? .33? .34?
TUBITAK .67? .68? .67? .62? .66? .60? .57? .53? .51 .55? .52 ? .38? .40? .32?
UU .76? .73? .74? .68? .71? .70? .66? .60? .62? .64? .60? .62? ? .44? .46?
SHEF-WPROA .79? .76? .75? .73? .73? .71? .71? .66? .63? .63? .67? .60? .56? ? .47?
RWTH-JANE .74? .73? .78? .71? .69? .74? .68? .67? .66? .66? .66? .68? .54? .53? ?
score .63 .63 .61 .58 .57 .55 .52 .50 .47 .47 .46 .45 .36 .32 .32
rank 1-2 1-2 3 3-5 4-6 5-6 7 8 9-11 9-11 10-12 11-12 13 14-15 14-15
Table 24: Head to head comparison, ignoring ties, for English-German systems
41
UE
DI
N-
HE
AF
IE
LD
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
ON
LI
NE
-A
M
ES
-SI
M
PL
IF
IE
DF
RE
NC
H
DC
U
RW
TH
CM
U-
TR
EE
-TO
-TR
EE
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .45? .46? .46? .42? .42? .34? .34? .29? .33? .31? .28? .24?
UEDIN .55? ? .52? .43? .45? .46? .40? .38? .33? .36? .33? .32? .23?
ONLINE-B .54? .48? ? .49 .46? .44? .45? .40? .38? .34? .36? .31? .26?
LIMSI-NCODE-SOUL .54? .57? .51 ? .52? .47 .45? .42? .38? .36? .34? .31? .28?
KIT .58? .55? .54? .48? ? .47 .46? .44? .39? .38? .37? .33? .28?
ONLINE-A .58? .54? .56? .53 .53 ? .47 .45? .40? .40? .39? .34? .32?
MES-SIMPLIFIEDFRENCH .66? .60? .55? .55? .54? .53 ? .48? .44? .40? .39? .39? .32?
DCU .66? .62? .60? .58? .56? .55? .52? ? .45? .45? .42? .41? .36?
RWTH .71? .67? .62? .62? .61? .60? .56? .55? ? .48? .47? .47? .38?
CMU-TREE-TO-TREE .67? .64? .66? .64? .62? .60? .60? .55? .52? ? .50 .48 .37?
CU-ZEMAN .69? .67? .64? .66? .63? .61? .61? .58? .53? .50 ? .47? .39?
JHU .72? .68? .69? .69? .67? .66? .61? .59? .53? .52 .53? ? .45?
SHEF-WPROA .76? .77? .74? .72? .72? .68? .68? .64? .62? .63? .61? .55? ?
score .63 .60 .59 .57 .56 .54 .51 .48 .43 .42 .42 .38 .32
rank 1 2-3 2-3 4-5 4-5 5-6 7 8 9-10 9-11 10-11 12 13
Table 25: Head to head comparison, ignoring ties, for French-English systems
UE
DI
N
ON
LI
NE
-B
LI
M
SI
-N
CO
DE
-SO
UL
KI
T
PR
OM
T
ST
AN
FO
RD
M
ES
M
ES
-IN
FL
EC
TI
ON
RW
TH
-PH
RA
SE
-B
AS
ED
-JA
NE
ON
LI
NE
-A
DC
U
CU
-ZE
M
AN
JH
U
OM
NI
FL
UE
NT
IT
S-L
AT
L
IT
S-L
AT
L-P
E
UEDIN ? .49 .47? .48 .50 .44? .41? .40? .47? .39? .41? .35? .29? .30? .27? .24?
ONLINE-B .51 ? .46? .47? .47? .44? .49 .43? .43? .43? .38? .35? .36? .28? .25? .25?
LIMSI-NCODE-SOUL .53? .54? ? .45? .48 .48 .45? .43? .44? .45? .41? .32? .34? .30? .27? .27?
KIT .52 .53? .55? ? .48 .46? .45? .43? .45? .46? .38? .30? .33? .31? .29? .29?
PROMT .50 .53? .52 .52 ? .50 .48 .52? .45? .47 .48? .38? .36? .36? .34? .31?
STANFORD .56? .56? .52 .54? .50 ? .52 .48 .44? .49 .44? .39? .34? .36? .30? .29?
MES .59? .51 .55? .55? .52 .48 ? .52 .51 .45? .45? .36? .37? .34? .29? .29?
MES-INFLECTION .60? .57? .57? .57? .48? .52 .48 ? .54? .51 .46? .37? .35? .31? .33? .31?
RWTH-PHRASE-BASED-JANE .53? .57? .56? .55? .55? .56? .49 .46? ? .53 .49 .38? .36? .34? .35? .31?
ONLINE-A .61? .57? .55? .54? .53 .51 .55? .49 .47 ? .50 .45? .38? .38? .39? .35?
DCU .59? .62? .59? .62? .52? .56? .55? .54? .51 .50 ? .42? .40? .40? .36? .35?
CU-ZEMAN .65? .65? .68? .70? .62? .61? .64? .63? .62? .55? .58? ? .50 .42? .41? .37?
JHU .71? .64? .66? .67? .64? .66? .63? .65? .64? .62? .60? .50 ? .47? .42? .38?
OMNIFLUENT .70? .72? .70? .69? .64? .64? .66? .69? .66? .62? .60? .58? .53? ? .43? .42?
ITS-LATL .73? .75? .72? .71? .66? .70? .71? .67? .65? .61? .64? .59? .58? .57? ? .45?
ITS-LATL-PE .76? .75? .73? .71? .69? .71? .71? .69? .69? .65? .65? .63? .62? .58? .55? ?
score .60 .60 .58 .58 .55 .55 .54 .53 .53 .51 .49 .42 .40 .38 .35 .32
rank 1-2 1-3 2-4 3-4 5-7 5-8 5-8 6-9 7-10 9-11 10-11 12 13 14 15 16
Table 26: Head to head comparison, ignoring ties, for English-French systems
42
UE
DI
N-
HE
AF
IE
LD
ON
LI
NE
-B
UE
DI
N
ON
LI
NE
-A
M
ES
LI
M
SI
-N
CO
DE
-SO
UL
DC
U
DC
U-
OK
IT
A
DC
U-
FD
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
UEDIN-HEAFIELD ? .49 .42? .45? .43? .40? .34? .43? .37? .34? .31? .15?
ONLINE-B .51 ? .49 .44? .46? .47? .42? .39? .40? .37? .37? .16?
UEDIN .58? .51 ? .55? .50 .47? .43? .42? .39? .39? .35? .14?
ONLINE-A .55? .56? .45? ? .50 .44? .45? .42? .42? .41? .37? .18?
MES .57? .54? .50 .50 ? .47? .45? .41? .41? .40? .38? .15?
LIMSI-NCODE-SOUL .60? .53? .53? .56? .53? ? .46? .45? .44? .43? .38? .18?
DCU .66? .58? .57? .55? .55? .54? ? .44? .47? .42? .41? .16?
DCU-OKITA .57? .61? .58? .58? .59? .55? .56? ? .49 .46? .46? .18?
DCU-FDA .63? .60? .61? .58? .59? .56? .53? .51 ? .48? .43? .18?
CU-ZEMAN .66? .63? .61? .59? .60? .57? .58? .54? .52? ? .43? .18?
JHU .69? .63? .65? .63? .62? .62? .59? .54? .57? .57? ? .22?
SHEF-WPROA .85? .84? .86? .82? .85? .82? .84? .82? .82? .82? .78? ?
score .62 .59 .57 .57 .56 .53 .51 .48 .48 .46 .42 .16
rank 1 2 3-5 3-5 3-5 6 7 8-9 8-9 10 11 12
Table 27: Head to head comparison, ignoring ties, for Spanish-English systems
ON
LI
NE
-B
ON
LI
NE
-A
UE
DI
N
PR
OM
T
M
ES
TA
LP
-U
PC
LI
M
SI
-N
CO
DE
DC
U
DC
U-
FD
A
DC
U-
OK
IT
A
CU
-ZE
M
AN
JH
U
SH
EF
-W
PR
OA
ONLINE-B ? .49 .45? .43? .38? .35? .34? .35? .37? .34? .33? .32? .23?
ONLINE-A .51 ? .49 .48 .38? .46? .42? .41? .43? .38? .38? .37? .31?
UEDIN .55? .51 ? .49 .46? .45? .43? .42? .36? .38? .38? .38? .26?
PROMT .57? .52 .51 ? .46? .48 .43? .43? .40? .37? .39? .34? .29?
MES .62? .62? .54? .54? ? .46? .44? .44? .41? .40? .43? .36? .32?
TALP-UPC .65? .54? .55? .52 .54? ? .50 .45? .44? .40? .40? .37? .32?
LIMSI-NCODE .66? .58? .57? .57? .56? .50 ? .46? .51 .48 .44? .45? .35?
DCU .65? .59? .58? .57? .56? .55? .54? ? .50 .48 .48 .45? .36?
DCU-FDA .63? .57? .64? .60? .59? .56? .49 .50 ? .53? .49 .42? .32?
DCU-OKITA .66? .62? .62? .63? .60? .60? .52 .52 .47? ? .50 .47? .36?
CU-ZEMAN .67? .62? .62? .61? .57? .60? .56? .52 .51 .50 ? .46? .40?
JHU .68? .63? .62? .66? .64? .63? .55? .55? .58? .53? .54? ? .37?
SHEF-WPROA .77? .69? .74? .71? .68? .68? .65? .64? .68? .64? .60? .63? ?
score .63 .58 .57 .56 .53 .52 .49 .47 .47 .45 .44 .41 .32
rank 1 2-4 2-4 3-4 5-6 5-6 7-8 7-9 8-10 9-11 10-11 12 13
Table 28: Head to head comparison, ignoring ties, for English-Spanish systems
43
ON
LI
NE
-B
CM
U
ON
LI
NE
-A
ON
LI
NE
-G
PR
OM
T
QC
RI
-M
ES
UC
AM
-M
UL
TI
FR
ON
TE
ND
BA
LA
GU
R
M
ES
-Q
CR
I
UE
DI
N
OM
NI
FL
UE
NT
-U
NC
NS
TR
LI
A
OM
NI
FL
UE
NT
-C
NS
TR
UM
D
CU
-K
AR
EL
CO
M
M
ER
CI
AL
-3
UE
DI
N-
SY
NT
AX
JH
U
CU
-ZE
M
AN
ONLINE-B ? .40? .42? .41? .37? .37? .41? .33? .33? .37? .33? .33? .35? .38? .34? .33? .29? .28? .14?
CMU .60? ? .50 .46? .43? .47? .42? .42? .39? .43? .41? .41? .40? .38? .36? .30? .30? .29? .17?
ONLINE-A .58? .50 ? .50 .51 .43? .47? .44? .40? .41? .43? .38? .40? .38? .38? .39? .34? .30? .19?
ONLINE-G .59? .54? .50 ? .55? .50 .51 .48 .42? .41? .44? .43? .46? .40? .44? .36? .34? .33? .19?
PROMT .63? .57? .49 .45? ? .43? .47? .43? .47? .47? .43? .39? .44? .43? .37? .41? .40? .38? .25?
QCRI-MES .63? .53? .57? .50 .57? ? .48 .46? .47? .45? .43? .45? .45? .38? .42? .37? .33? .40? .19?
UCAM-MULTIFRONTEND .59? .58? .53? .49 .53? .52 ? .47? .48 .46? .46? .42? .45? .46? .45? .40? .39? .33? .17?
BALAGUR .67? .58? .56? .52 .57? .54? .53? ? .47? .49 .45? .53? .40? .44? .44? .41? .36? .33? .23?
MES-QCRI .67? .61? .60? .58? .53? .53? .52 .53? ? .49 .47? .47? .43? .43? .44? .38? .42? .39? .17?
UEDIN .63? .57? .59? .59? .53? .55? .54? .51 .51 ? .48 .52 .44? .52 .49 .42? .43? .35? .21?
OMNIFLUENT-UNCNSTR .67? .59? .57? .56? .57? .57? .54? .55? .53? .52 ? .51 .46? .48 .48 .44? .40? .39? .25?
LIA .67? .59? .62? .57? .61? .55? .58? .47? .53? .48 .49 ? .51 .49 .48 .50 .41? .39? .20?
OMNIFLUENT-CNSTR .65? .60? .60? .54? .56? .55? .55? .60? .57? .56? .54? .49 ? .51 .48 .47? .40? .40? .25?
UMD .62? .62? .62? .60? .57? .62? .54? .56? .57? .48 .52 .51 .49 ? .53? .42? .46? .42? .19?
CU-KAREL .66? .64? .62? .56? .63? .58? .55? .56? .56? .51 .52 .52 .52 .47? ? .44? .40? .47? .24?
COMMERCIAL-3 .67? .70? .61? .64? .59? .63? .60? .59? .62? .58? .56? .50 .53? .58? .56? ? .51 .44? .32?
UEDIN-SYNTAX .71? .70? .66? .66? .60? .67? .61? .64? .58? .57? .60? .59? .60? .54? .60? .49 ? .45? .25?
JHU .72? .71? .70? .67? .62? .60? .67? .67? .61? .65? .61? .61? .60? .58? .53? .56? .55? ? .24?
CU-ZEMAN .86? .83? .81? .81? .75? .81? .83? .77? .83? .79? .75? .80? .75? .81? .76? .68? .75? .76? ?
score .65 .60 .58 .56 .56 .55 .54 .52 .51 .50 .49 .49 .48 .48 .47 .43 .41 .39 .21
rank 1 2-3 2-3 4-6 4-6 5-7 5-7 8-9 8-10 9-11 10-12 11-14 12-15 12-15 13-15 16 17 18 19
Table 29: Head to head comparison, ignoring ties, for Russian-English systems
PR
OM
T
ON
LI
NE
-B
CM
U
ON
LI
NE
-G
ON
LI
NE
-A
UE
DI
N
QC
RI
-M
ES
CU
-K
AR
EL
M
ES
-Q
CR
I
JH
U
CO
M
M
ER
CI
AL
-3
LI
A
BA
LA
GU
R
CU
-ZE
M
AN
PROMT ? .44? .39? .47 .46? .36? .37? .37? .32? .35? .28? .30? .32? .24?
ONLINE-B .56? ? .44? .41? .44? .38? .37? .35? .33? .39? .33? .31? .35? .24?
CMU .61? .56? ? .52 .49 .47? .43? .41? .39? .44? .44? .40? .35? .28?
ONLINE-G .53 .59? .48 ? .48 .50 .48 .46 .46? .42? .38? .43? .38? .36?
ONLINE-A .54? .56? .51 .52 ? .47 .49 .49 .48 .44? .38? .40? .40? .34?
UEDIN .64? .62? .53? .50 .53 ? .49 .46? .42? .39? .44? .41? .38? .29?
QCRI-MES .63? .63? .57? .52 .51 .51 ? .48 .45? .44? .42? .39? .40? .29?
CU-KAREL .63? .65? .59? .54 .51 .54? .52 ? .50 .46? .43? .40? .42? .34?
MES-QCRI .68? .67? .61? .54? .52 .58? .55? .50 ? .48? .47? .43? .45? .34?
JHU .65? .61? .56? .58? .56? .61? .56? .54? .52? ? .51 .44? .44? .33?
COMMERCIAL-3 .72? .67? .56? .62? .62? .56? .58? .57? .53? .49 ? .52 .48 .44?
LIA .70? .69? .60? .57? .60? .59? .61? .60? .57? .56? .48 ? .47? .41?
BALAGUR .68? .65? .65? .62? .60? .62? .60? .58? .55? .56? .52 .53? ? .41?
CU-ZEMAN .76? .76? .72? .64? .66? .71? .71? .66? .66? .67? .56? .59? .59? ?
score .64 .62 .55 .54 .53 .53 .52 .49 .47 .46 .43 .42 .41 .33
rank 1 2 3-4 3-6 3-7 4-7 5-7 8 9-10 9-10 11-12 11-13 12-13 14
Table 30: Head to head comparison, ignoring ties, for English-Russian systems
44
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 206?212,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
Joshua 5.0: Sparser, better, faster, server
Matt Post1 and Juri Ganitkevitch2 and Luke Orland1 and Jonathan Weese2 and Yuan Cao2
1Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
Chris Callison-Burch
Computer and Information Sciences Department
University of Pennsylvania
Abstract
We describe improvements made over the
past year to Joshua, an open-source trans-
lation system for parsing-based machine
translation. The main contributions this
past year are significant improvements in
both speed and usability of the grammar
extraction and decoding steps. We have
also rewritten the decoder to use a sparse
feature representation, enabling training of
large numbers of features with discrimina-
tive training methods.
1 Introduction
Joshua is an open-source toolkit1 for hierarchical
and syntax-based statistical machine translation
of human languages with synchronous context-
free grammars (SCFGs). The original version of
Joshua (Li et al, 2009) was a port (from Python to
Java) of the Hiero machine translation system in-
troduced by Chiang (2007). It was later extended
to support grammars with rich syntactic labels (Li
et al, 2010). Subsequent efforts produced Thrax,
the extensible Hadoop-based extraction tool for
synchronous context-free grammars (Weese et al,
2011), later extended to support pivoting-based
paraphrase extraction (Ganitkevitch et al, 2012).
Joshua 5.0 continues our yearly update cycle.
The major components of Joshua 5.0 are:
?3.1 Sparse features. Joshua now supports an
easily-extensible sparse feature implementa-
tion, along with tuning methods (PRO and
kbMIRA) for efficiently setting the weights
on large feature vectors.
1joshua-decoder.org
?3.2 Significant speed increases. Joshua 5.0 is up
to six times faster than Joshua 4.0, and also
does well against hierarchical Moses, where
end-to-end decoding (including model load-
ing) of WMT test sets is as much as three
times faster.
?3.3 Thrax 2.0. Our reengineered Hadoop-based
grammar extractor, Thrax, is up to 300%
faster while using significantly less interme-
diate disk space.
?3.4 Many other features. Joshua now includes a
server mode with fair round-robin scheduling
among and within requests, a bundler for dis-
tributing trained models, improvements to the
Joshua pipeline (for managing end-to-end ex-
periments), and better documentation.
2 Overview
Joshua is an end-to-end statistical machine trans-
lation toolkit. In addition to the decoder com-
ponent (which performs the actual translation), it
includes the infrastructure needed to prepare and
align training data, build translation and language
models, and tune and evaluate them.
This section provides a brief overview of the
contents and abilities of this toolkit. More infor-
mation can be found in the online documentation
(joshua-decoder.org/5.0/).
2.1 The Pipeline: Gluing it all together
The Joshua pipeline ties together all the infrastruc-
ture needed to train and evaluate machine transla-
tion systems for research or industrial purposes.
Once data has been segmented into parallel train-
ing, development, and test sets, a single invocation
of the pipeline script is enough to invoke this entire
infrastructure from beginning to end. Each step is
206
broken down into smaller steps (e.g., tokenizing a
file) whose dependencies are cached with SHA1
sums. This allows a reinvoked pipeline to reliably
skip earlier steps that do not need to be recom-
puted, solving a common headache in the research
and development cycle.
The Joshua pipeline is similar to other ?ex-
periment management systems? such as Moses?
Experiment Management System (EMS), a much
more general, highly-customizable tool that al-
lows the specification and parallel execution of
steps in arbitrary acyclic dependency graphs
(much like the UNIX make tool, but written with
machine translation in mind). Joshua?s pipeline
is more limited in that the basic pipeline skeleton
is hard-coded, but reduced versatility covers many
standard use cases and is arguably easier to use.
The pipeline is parameterized in many ways,
and all the options below are selectable with
command-line switches. Pipeline documentation
is available online.
2.2 Data preparation, alignment, and model
building
Data preparation involves data normalization (e.g.,
collapsing certain punctuation symbols) and tok-
enization (with the Penn treebank or user-specified
tokenizer). Alignment with GIZA++ (Och and
Ney, 2000) and the Berkeley aligner (Liang et al,
2006b) are supported.
Joshua?s builtin grammar extractor, Thrax, is
a Hadoop-based extraction implementation that
scales easily to large datasets (Ganitkevitch et al,
2013). It supports extraction of both Hiero (Chi-
ang, 2005) and SAMT grammars (Zollmann and
Venugopal, 2006) with extraction heuristics eas-
ily specified via a flexible configuration file. The
pipeline also supports GHKM grammar extraction
(Galley et al, 2006) using the extractors available
from Michel Galley2 or Moses.
SAMT and GHKM grammar extraction require
a parse tree, which are produced using the Berke-
ley parser (Petrov et al, 2006), or can be done out-
side the pipeline and supplied as an argument.
2.3 Decoding
The Joshua decoder is an implementation of the
CKY+ algorithm (Chappelier et al, 1998), which
generalizes CKY by removing the requirement
2nlp.stanford.edu/?mgalley/software/
stanford-ghkm-latest.tar.gz
that the grammar first be converted to Chom-
sky Normal Form, thereby avoiding the complex-
ities of explicit binarization schemes (Zhang et
al., 2006; DeNero et al, 2009). CKY+ main-
tains cubic-time parsing complexity (in the sen-
tence length) with Earley-style implicit binariza-
tion of rules. Joshua permits arbitrary SCFGs, im-
posing no limitation on the rank or form of gram-
mar rules.
Parsing complexity is still exponential in the
scope of the grammar,3 so grammar filtering re-
mains important. The default Thrax settings ex-
tract only grammars with rank 2, and the pipeline
implements scope-3 filtering (Hopkins and Lang-
mead, 2010) when filtering grammars to test sets
(for GHKM).
Joshua uses cube pruning (Chiang, 2007) with
a default pop limit of 100 to efficiently explore the
search space. Other decoder options are too nu-
merous to mention here, but are documented on-
line.
2.4 Tuning and testing
The pipeline allows the specification (and optional
linear interpolation) of an arbitrary number of lan-
guage models. In addition, it builds an interpo-
lated Kneser-Ney language model on the target
side of the training data using KenLM (Heafield,
2011; Heafield et al, 2013), BerkeleyLM (Pauls
and Klein, 2011) or SRILM (Stolcke, 2002).
Joshua ships with MERT (Och, 2003) and PRO
implementations. Tuning with k-best batch MIRA
(Cherry and Foster, 2012) is also supported via
callouts to Moses.
3 What?s New in Joshua 5.0
3.1 Sparse features
Until a few years ago, machine translation systems
were for the most part limited in the number of fea-
tures they could employ, since the line-based op-
timization method, MERT (Och, 2003), was not
able to efficiently search over more than tens of
feature weights. The introduction of discrimina-
tive tuning methods for machine translation (Liang
et al, 2006a; Tillmann and Zhang, 2006; Chiang
et al, 2008; Hopkins and May, 2011) has made
it possible to tune large numbers of features in
statistical machine translation systems, and open-
3Roughly, the number of consecutive nonterminals in a
rule (Hopkins and Langmead, 2010).
207
source implementations such as Cherry and Foster
(2012) have made it easy.
Joshua 5.0 has moved to a sparse feature rep-
resentation internally. First, to clarify terminol-
ogy, a feature as implemented in the decoder is
actually a template that can introduce any number
of actual features (in the standard machine learn-
ing sense). We will use the term feature function
for these templates and feature for the individual,
traditional features that are induced by these tem-
plates. For example, the (typically dense) features
stored with the grammar on disk are each separate
features contributed by the PHRASEMODEL fea-
ture function template. The LANGUAGEMODEL
template contributes a single feature value for each
language model that was loaded.
For efficiency, Joshua does not store the en-
tire feature vector during decoding. Instead, hy-
pergraph nodes maintain only the best cumulative
score of each incoming hyperedge, and the edges
themselves retain only the hyperedge delta (the in-
ner product of the weight vector and features in-
curred by that edge). After decoding, the feature
vector for each edge can be recomputed and ex-
plicitly represented if that information is required
by the decoder (for example, during tuning).
This functionality is implemented via the fol-
lowing feature function interface, presented here
in simplified pseudocode:
interface FeatureFunction:
apply(context, accumulator)
The context comprises fixed pieces of the input
sentence and hypergraph:
? the hypergraph edge (which represents the
SCFG rule and sequence of tail nodes)
? the complete source sentence
? the input span
The accumulator object?s job is to accumulate
feature (name,value) pairs fired by a feature func-
tion during the application of a rule, via another
interface:
interface Accumulator:
add(feature_name, value)
The accumulator generalization4 permits the use
of a single feature-gathering function for two ac-
cumulator objects: the first, used during decoding,
maintains only a weighted sum, and the second,
4Due to Kenneth Heafield.
used (if needed) during k-best extraction, holds
onto the entire sparse feature vector.
For tuning large sets of features, Joshua sup-
ports both PRO (Hopkins and May, 2011), an in-
house version introduced with Joshua 4.0, and k-
best batch MIRA (Cherry and Foster, 2012), im-
plemented via calls to code provided by Moses.
3.2 Performance improvements
We introduced many performance improvements,
replacing code designed to get the job done under
research timeline constraints with more efficient
alternatives, including smarter handling of locking
among threads, more efficient (non string-based)
computation of dynamic programming state, and
replacement of fixed class-based array structures
with fixed-size literals.
We used the following experimental setup to
compare Joshua 4.0 and 5.0: We extracted a large
German-English grammar from all sentences with
no more than 50 words per side from Europarl v.7
(Koehn, 2005), News Commentary, and the Com-
mon Crawl corpora using Thrax default settings.
After filtering against our test set (newstest2012),
this grammar contained 70 million rules. We then
trained three language models on (1) the target
side of our grammar training data, (2) English
Gigaword, and (3) the monolingual English data
released for WMT13. We tuned a system using
kbMIRA and decoded using KenLM (Heafield,
2011). Decoding was performed on 64-core 2.1
GHz AMD Opteron processors with 256 GB of
available memory.
Figure 1 plots the end-to-end runtime5 as a
function of the number of threads. Each point in
the graph is the minimum of at least fifteen runs
computed at different times over a period of a few
days. The main point of comparison, between
Joshua 4.0 and 5.0, shows that the current version
is up to 500% faster than it was last year, espe-
cially in multithreaded situations.
For further comparison, we took these models,
converted them to hierarchical Moses format, and
then decoded with the latest version.6 We com-
piled Moses with the recommended optimization
settings7 and used the in-memory (SCFG) gram-
5i.e., including model loading time and grammar sorting
6The latest version available on Github as of June 7, 2013
7With tcmalloc and the following compile flags:
--max-factors=1 --kenlm-max-order=5
debug-symbols=off
208
 500 1000 2000 3000 4000
 5000 10000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 4.0 (in-memory)Moses (in-memory)Joshua 4.0 (packed)Joshua 5.0 (packed)
Figure 1: End-to-end runtime as a function of the
number of threads. Each data point is the mini-
mum of at least fifteen different runs.
 200 300 400 500 1000 2000
 3000 4000 5000
 2 4  8  16  32  48decoding time (seconds) thread count
Joshua 5.0Moses
Figure 2: Decoding time alone.
mar format. BLEU scores were similar.8 In this
end-to-end setting, Joshua is about 200% faster
than Moses at high thread counts (Figure 1).
Figure 2 furthers the Moses and Joshua com-
parison by plotting only decoding time (subtract-
ing out model loading and sorting times). Moses?
decoding speed is 2?3 times faster than Joshua?s,
suggesting that the end-to-end gains in Figure 1
are due to more efficient grammar loading.
3.3 Thrax 2.0
The Thrax module of our toolkit has undergone
a similar overhaul. The rule extraction code was
822.88 (Moses), 22.99 (Joshua 4), and 23.23 (Joshua 5).
long-term investment holding on to
det
amod
the
JJ NN VBG IN TO DT
NP
PP
VP
? ?
the long-term
?
=~sig
?
dep-det-R-investment
pos-L-TO 
pos-R-NN  
lex-R-investment 
lex-L-to 
dep-amod-R-investment
syn-gov-NP syn-miss-L-NN 
lex-L-on-to 
pos-L-IN-TO  
dep-det-R-NN dep-amod-R-NN
Figure 3: Here, position-aware lexical and part-of-
speech n-gram features, labeled dependency links,
and features reflecting the phrase?s CCG-style la-
bel NP/NN are included in the context vector.
rewritten to be easier to understand and extend, al-
lowing, for instance, for easy inclusion of alterna-
tive nonterminal labeling strategies.
We optimized the data representation used for
the underlying map-reduce framework towards
greater compactness and speed, resulting in a
300% increase in extraction speed and an equiv-
alent reduction in disk I/O (Table 1). These
gains enable us to extract a syntactically labeled
German-English SAMT-style translation grammar
from a bitext of over 4 million sentence pairs in
just over three hours. Furthermore, Thrax 2.0 is
capable of scaling to very large data sets, like
the composite bitext used in the extraction of the
paraphrase collection PPDB (Ganitkevitch et al,
2013), which counted 100 million sentence pairs
and over 2 billion words on the English side.
Furthermore, Thrax 2.0 contains a module fo-
cused on the extraction of compact distributional
signatures over large datasets. This distribu-
tional mode collects contextual features for n-
gram phrases, such as words occurring in a win-
dow around the phrase, as well as dependency-
based and syntactic features. Figure 3 illustrates
the feature space. We then compute a bit signature
from the resulting feature vector via a randomized
locality-sensitive hashing projection. This yields a
compact representation of a phrase?s typical con-
text. To perform this projection Thrax relies on
the Jerboa toolkit (Van Durme, 2012). As part of
the PPDB effort, Thrax has been used to extract
rich distributional signatures for 175 million 1-
to-4-gram phrases from the Annotated Gigaword
corpus (Napoles et al, 2012), a parsed and pro-
209
Cs-En Fr-En De-En Es-En
Rules 112M 357M 202M 380M
Space Time Space Time Space Time Space Time
Joshua 4.0 120GB 112 min 364GB 369 min 211GB 203 min 413GB 397 min
Joshua 5.0 31GB 25 min 101GB 81 min 56GB 44 min 108GB 84 min
Difference -74.1% -77.7% -72.3% -78.0% -73.5% -78.3% -73.8% -78.8%
Table 1: Comparing Hadoop?s intermediate disk space use and extraction time on a selection of Europarl
v.7 Hiero grammar extractions. Disk space was measured at its maximum, at the input of Thrax?s final
grammar aggregation stage. Runtime was measured on our Hadoop cluster with a capacity of 52 mappers
and 26 reducers. On average Thrax 2.0, bundled with Joshua 5.0, is up to 300% faster and more compact.
cessed version of the English Gigaword (Graff et
al., 2003).
Thrax is distributed with Joshua and is also
available as a separate download.9
3.4 Other features
Joshua 5.0 also includes many features designed
to increase its usability. These include:
? A TCP/IP server architecture, designed to
handle multiple sets of translation requests
while ensuring fairness in thread assignment
both across and within these connections.
? Intelligent selection of translation and lan-
guage model training data using cross-
entropy difference to rank training candidates
(Moore and Lewis, 2010; Axelrod et al,
2011) (described in detail in Orland (2013)).
? A bundler for easy packaging of trained mod-
els with all of its dependencies.
? A year?s worth of improvements to the
Joshua pipeline, including many new features
and supported options, and increased robust-
ness to error.
? Extended documentation.
4 WMT Submissions
We submitted a constrained entry for all tracks ex-
cept English-Czech (nine in total). Our systems
were constructed in a straightforward fashion and
without any language-specific adaptations using
the Joshua pipeline. For each language pair, we
trained a Hiero system on all sentences with no
more than fifty words per side in the Europarl,
News Commentary, and Common Crawl corpora.
9github.com/joshua-decoder/thrax
We built two interpolated Kneser-Ney language
models: one from the monolingual News Crawl
corpora (2007?2012), and another from the tar-
get side of the training data. For systems translat-
ing into English, we added a third language model
built on Gigaword. Language models were com-
bined linearly into a single language model using
interpolation weights from the tuning data (new-
stest2011). We tuned our systems with kbMIRA.
For truecasing, we used a monolingual translation
system built on the training data, and finally deto-
kenized with simple heuristics.
5 Summary
The 5.0 release of Joshua is the result of a signif-
icant year-long research, engineering, and usabil-
ity effort that we hope will be of service to the
research community. User-friendly packages of
Joshua are available from joshua-decoder.
org, while developers are encouraged to partic-
ipate via github.com/joshua-decoder/
joshua. Mailing lists, linked from the main
Joshua page, are available for both.
Acknowledgments Joshua?s sparse feature rep-
resentation owes much to discussions with Colin
Cherry, Barry Haddow, Chris Dyer, and Kenneth
Heafield at MT Marathon 2012 in Edinburgh.
This material is based on research sponsored
by the NSF under grant IIS-1249516 and DARPA
under agreement number FA8750-13-2-0017 (the
DEFT program). The U.S. Government is autho-
rized to reproduce and distribute reprints for Gov-
ernmental purposes. The views and conclusions
contained in this publication are those of the au-
thors and should not be interpreted as representing
official policies or endorsements of DARPA or the
U.S. Government.
210
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In Proceedings of EMNLP, pages 355?
362, Edinburgh, Scotland, UK., July.
J.C. Chappelier, M. Rajman, et al 1998. A generalized
CYK algorithm for parsing stochastic CFG. In First
Workshop on Tabulation in Parsing and Deduction
(TAPD98), pages 133?137.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of NAACL-HLT, pages 427?436, Montre?al,
Canada, June.
David Chiang, Yuval Marton, and Philip Resnik.
2008. Online large-margin training of syntactic and
structural translation features. In Proceedings of
EMNLP, Waikiki, Hawaii, USA, October.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL, Ann Arbor, Michigan.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
John DeNero, Adam Pauls, and Dan Klein. 2009.
Asynchronous binarization for synchronous gram-
mars. In Proceedings of ACL, Suntec, Singapore,
August.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of ACL/COLING, Sydney, Australia, July.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, PRO, and paraphrases. In Proceedings of
the Workshop on Statistical Machine Translation.
Juri Ganitkevitch, Chris Callison-Burch, and Benjamin
Van Durme. 2013. Ppdb: The paraphrase database.
In Proceedings of HLT/NAACL.
D. Graff, J. Kong, K. Chen, and K. Maeda. 2003.
English gigaword. Linguistic Data Consortium,
Philadelphia.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.
Clark, and Philipp Koehn. 2013. Scalable modi-
fied Kneser-Ney language model estimation. In Pro-
ceedings of ACL, Sofia, Bulgaria, August.
Kenneth Heafield. 2011. KenLM: Faster and smaller
language model queries. In Proceedings of the
Workshop on Statistical Machine Translation, pages
187?197. Association for Computational Linguis-
tics.
Mark Hopkins and Greg Langmead. 2010. SCFG
decoding without binarization. In Proceedings of
EMNLP, pages 646?655.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of EMNLP.
Philipp Koehn. 2005. Europarl: A parallel corpus for
statistical machine translation. In MT summit, vol-
ume 5.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Sanjeev Khudanpur, Lane Schwartz, Wren
Thornton, Jonathan Weese, and Omar Zaidan. 2009.
Joshua: An open source toolkit for parsing-based
machine translation. In Proceedings of the Work-
shop on Statistical Machine Translation, Athens,
Greece, March.
Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-
itkevitch, Ann Irvine, Sanjeev Khudanpur, Lane
Schwartz, Wren N.G. Thornton, Ziyuan Wang,
Jonathan Weese, and Omar F. Zaidan. 2010. Joshua
2.0: a toolkit for parsing-based machine translation
with syntax, semirings, discriminative training and
other goodies. In Proceedings of the Workshop on
Statistical Machine Translation.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,
and Ben Taskar. 2006a. An end-to-end discrimi-
native approach to machine translation. In Proceed-
ings of ACL/COLING.
Percy Liang, Ben Taskar, and Dan Klein. 2006b.
Alignment by agreement. In Proceedings of
NAACL, pages 104?111, New York City, USA, June.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of ACL (short papers), pages 220?224.
Courtney Napoles, Matt Gormley, and Benjamin Van
Durme. 2012. Annotated gigaword. In Proceedings
of AKBC-WEKEX 2012.
F. J. Och and H. Ney. 2000. Improved statistical align-
ment models. In Proceedings of ACL, pages 440?
447, Hong Kong, China, October.
Franz Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL,
Sapporo, Japan.
Luke Orland. 2013. Intelligent selection of trans-
lation model training data for machine translation
with TAUS domain data: A summary. Master?s the-
sis, Johns Hopkins University, Baltimore, Maryland,
June.
Adam Pauls and Dan Klein. 2011. Faster and smaller
n-gram language models. In Proceedings of ACL,
pages 258?267, Portland, Oregon, USA, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of ACL,
Sydney, Australia, July.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Seventh International
Conference on Spoken Language Processing.
211
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proceedings of ACL/COLING, pages 721?728, Syd-
ney, Australia, July.
Benjamin Van Durme. 2012. Jerboa: A toolkit for ran-
domized and streaming algorithms. Technical Re-
port 7, Human Language Technology Center of Ex-
cellence, Johns Hopkins University.
Jonathan Weese, Juri Ganitkevitch, Chris Callison-
Burch, Matt Post, and Adam Lopez. 2011. Joshua
3.0: Syntax-based machine translation with the
Thrax grammar extractor. In Proceedings of the
Workshop on Statistical Machine Translation.
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin
Knight. 2006. Synchronous binarization for ma-
chine translation. In Proceedings of HLT/NAACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart pars-
ing. In Proceedings of the Workshop on Statistical
Machine Translation, New York, New York.
212
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1?11,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Efficient Elicitation of Annotations for Human Evaluation of Machine
Translation
Keisuke Sakaguchi
?
, Matt Post
?
, Benjamin Van Durme
?
?
Center for Language and Speech Processing
?
Human Language Technology Center of Excellence
Johns Hopkins University, Baltimore, Maryland
{keisuke,post,vandurme}@cs.jhu.edu
Abstract
A main output of the annual Workshop
on Statistical Machine Translation (WMT)
is a ranking of the systems that partici-
pated in its shared translation tasks, pro-
duced by aggregating pairwise sentence-
level comparisons collected from human
judges. Over the past few years, there
have been a number of tweaks to the ag-
gregation formula in attempts to address
issues arising from the inherent ambigu-
ity and subjectivity of the task, as well as
weaknesses in the proposed models and
the manner of model selection.
We continue this line of work by adapt-
ing the TrueSkill
TM
algorithm ? an online
approach for modeling the relative skills
of players in ongoing competitions, such
as Microsoft?s Xbox Live ? to the hu-
man evaluation of machine translation out-
put. Our experimental results show that
TrueSkill outperforms other recently pro-
posed models on accuracy, and also can
significantly reduce the number of pair-
wise annotations that need to be collected
by sampling non-uniformly from the space
of system competitions.
1 Introduction
The Workshop on Statistical Machine Translation
(WMT) has long been a central event in the ma-
chine translation (MT) community for the evalua-
tion of MT output. It hosts an annual set of shared
translation tasks focused mostly on the translation
of western European languages. One of its main
functions is to publish a ranking of the systems
for each task, which are produced by aggregating
a large number of human judgments of sentence-
level pairwise rankings of system outputs. While
the performance on many automatic metrics is also
# score range system
1 0.638 1 UEDIN-HEAFIELD
2 0.604 2-3 UEDIN
0.591 2-3 ONLINE-B
4 0.571 4-5 LIMSI-SOUL
0.562 4-5 KIT
0.541 5-6 ONLINE-A
7 0.512 7 MES-SIMPLIFIED
8 0.486 8 DCU
9 0.439 9-10 RWTH
0.429 9-11 CMU-T2T
0.420 10-11 CU-ZEMAN
12 0.389 12 JHU
13 0.322 13 SHEF-WPROA
Table 1: System rankings presented as clusters
(WMT13 French-English competition). The score
column is the percentage of time each system was
judged better across its comparisons (?2.1).
reported (e.g., BLEU (Papineni et al., 2002)), the
human evaluation is considered primary, and is in
fact used as the gold standard for its metrics task,
where evaluation metrics are evaluated.
In machine translation, the longstanding dis-
agreements about evaluation measures do not go
away when moving from automatic metrics to hu-
man judges. This is due in no small part to the in-
herent ambiguity and subjectivity of the task, but
also arises from the particular way that the WMT
organizers produce the rankings. The system-
level rankings are produced by collecting pairwise
sentence-level comparisons between system out-
puts. These are then aggregated to produce a com-
plete ordering of all systems, or, more recently, a
partial ordering (Koehn, 2012), with systems clus-
tered where they cannot be distinguished in a sta-
tistically significant way (Table 1, taken from Bo-
jar et al. (2013)).
A number of problems have been noted with
this approach. The first has to do with the na-
ture of ranking itself. Over the past few years, the
WMT organizers have introduced a number of mi-
nor tweaks to the ranking algorithm (?2) in reac-
tion to largely intuitive arguments that have been
1
raised about how the evaluation is conducted (Bo-
jar et al., 2011; Lopez, 2012). While these tweaks
have been sensible (and later corroborated), Hop-
kins and May (2013) point out that this is essen-
tially a model selection task, and should prop-
erly be driven by empirical performance on held-
out data according to some metric. Instead of in-
tuition, they suggest perplexity, and show that a
novel graphical model outperforms existing ap-
proaches on that metric, with less amount of data.
A second problem is the deficiency of the mod-
els used to produce the ranking, which work by
computing simple ratios of wins (and, option-
ally, ties) to losses. Such approaches do not con-
sider the relative difficulty of system matchups,
and thus leave open the possibility that a system
is ranked highly from the luck of comparisons
against poorer opponents.
Third, a large number of judgments need to be
collected in order to separate the systems into clus-
ters to produce a partial ranking. The sheer size of
the space of possible comparisons (all pairs of sys-
tems times the number of segments in the test set)
requires sampling from this space and distributing
the annotations across a number of judges. Even
still, the number of judgments needed to produce
statistically significant rankings like those in Ta-
ble 1 grows quadratically in the number of par-
ticipating systems (Koehn, 2012), often forcing
the use of paid, lower-quality annotators hired on
Amazon?s Mechanical Turk. Part of the prob-
lem is that the sampling strategy collects data uni-
formly across system pairings. Intuitively, we
should need many fewer annotations between sys-
tems with divergent base performance levels, in-
stead focusing the collection effort on system pairs
whose performance is more matched, in order to
tease out the gaps between similarly-performing
systems. Why spend precious human time on re-
dundantly affirming predictable outcomes?
To address these issues, we developed a varia-
tion of the TrueSkill model (Herbrich et al., 2006),
an adaptative model of competitions originally de-
veloped for the Xbox Live online gaming commu-
nity. It assumes that each player?s skill level fol-
lows a Gaussian distribution N (?, ?
2
), in which
? represents a player?s mean performance, and ?
2
the system?s uncertainty about its current estimate
of this mean. These values are updated after each
?game? (in our case, the value of a ternary judg-
ment) in proportion to how surprising the outcome
is. TrueSkill has been adapted to a number of
areas, including chess, advertising, and academic
conference management.
The rest of this paper provides an empirical
comparison of a number of models of human eval-
uation (?2). We evaluate on perplexity and also
on accuracy, showing that the two are not always
correlated, and arguing for the primacy of the lat-
ter (?3). We find that TrueSkill outperforms other
models (?4). Moreover, TrueSkill also allows us to
drastically reduce the amount of data that needs to
be collected by sampling non-uniformly from the
space of all competitions (?5), which also allows
for greater separation of the systems into ranked
clusters (?6).
2 Models
Before introducing our adaptation of the TrueSkill
model for ranking translation systems with human
judgments (?2.3), we describe two comparisons:
the ?Expected Wins? model used in recent evalu-
ations, and the Bayesian model proposed by Hop-
kins and May (?2.2).
As we described briefly in the introduction,
WMT produces system rankings by aggregating
sentence-level ternary judgments of the form:
(i, S
1
, S
2
, pi)
where i is the source segment (id), S
1
and S
2
are the system pair drawn from a set of systems
{S}, and pi ? {<,>,=} denotes whether the
first system was judged to be better than, worse
than, or equivalent to the second. These ternary
judgments are obtained by presenting judges with
a randomly-selected input sentence and the refer-
ence, followed by five randomly-selected transla-
tions of that sentence. Annotators are asked to
rank these systems from best (rank 1) to worst
(rank 5), ties permitted, and with no meaning as-
cribed to the absolute values or differences be-
tween ranks. This is done to accelerate data collec-
tion, since it yields ten pairwise comparisons per
ranking. Tens of thousands of judgments of this
form constitute the raw data used to compute the
system-level rankings. All the work described in
this section is computed over these pairwise com-
parisons, which are treated as if they were col-
lected independently.
2.1 Expected Wins
The ?Expected Wins? model computes the per-
centage of times that each system wins in its
2
pairwise comparisons. Let A be the complete
set of annotations or judgments of the form
{i, S
1
, S
2
, pi
R
}. We assume these judgments have
been converted into a normal form where S
1
is ei-
ther the winner or is tied with S
2
, and therefore
pi
R
? {<,=}. Let ?(x, y) be the Kronecker delta
function.
1
We then define the function:
wins(S
i
, S
j
) =
|A|
?
n=1
?(S
i
, S
(n)
1
)?(S
j
, S
(n)
2
)?(pi
(n)
R
, <)
which counts the number of annotations for which
system S
i
was ranked better than system S
j
. We
define a single-variable version that marginalizes
over all annotations:
wins(S
i
) =
?
S
j
6=S
i
wins(S
i
, S
j
)
We also define analogous functions for loses and
ties. Until the WMT11 evaluation (Callison-Burch
et al., 2011), the score for each system S
i
was
computed as follows:
score(S
i
) =
wins(S
i
) + ties(S
i
)
wins(S
i
) + ties(S
i
) + loses(S
i
)
Bojar et al. (2011) suggested that the inclusion of
ties biased the results, due to their large numbers,
the underlying similarity of many of the models,
and the fact that they are counted for both systems
in the tie, and proposed the following modified
scoring function:
score(S
i
) =
1
|{S}|
?
S
j
6=S
i
wins(S
i
, S
j
)
wins(S
i
, S
j
) + wins(S
j
, S
i
)
This metric computes an average relative fre-
quency of wins, excluding ties, and was used
in WMT12 and WMT13 (Callison-Burch et al.,
2012; Bojar et al., 2013).
The decision to exclude ties isn?t without
its problems; for example, an evaluation where
two systems are nearly always judged equivalent
should be relevant in producing the final ranking
of systems. Furthermore, as Hopkins and May
(2013) point out, throwing out data to avoid bi-
asing a model suggests a problem with the model.
We now turn to a description of their model, which
addresses these problems.
1
?(x, y) =
{
1 if x = y
0 o.w.
2.2 The Hopkins and May (2013) model
Recent papers (Koehn, 2012; Hopkins and May,
2013) have proposed models focused on the rel-
ative ability of the competition systems. These
approaches assume that each system has a mean
quality represented by a Gaussian distribution with
a fixed variance shared across all systems. In the
graphical model formulation of Hopkins and May
(2013), the pairwise judgments (i, S
1
, S
2
, pi) are
imagined to have been generated according to the
following process:
? Select a source sentence i
? Select two systems S
1
and S
2
. A system
S
j
is associated with a Gaussian distribution
N (?
S
j
, ?
2
a
), samples from which represent
the quality of translations
? Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
? Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
2
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
The task is to then infer the posterior parameters,
given the data: the system means ?
S
j
and, by ne-
cessity, the latent values {q
i
} for each of the pair-
wise comparison training instances. Hopkins and
May do not publish code or describe details of this
algorithm beyond mentioning Gibbs sampling, so
we used our own implementation,
3
and describe it
here for completeness.
After initialization, we have training instances
of the form (i, S
1
, S
2
, pi
R
, q
1
, q
2
), where all but the
q
i
are observed. At a high level, the sampler iter-
ates over the training data, inferring values of q
1
and q
2
for each annotation together in a single step
of the sampler from the current values of the sys-
tems means, {?
j
}.
4
At the end of each iteration,
2
Note that better systems have higher relative abilities
{?
S
j
}. Better translations subsequently have on-average
higher values {q
i
}, which translate into a lower ranking pi.
3
github.com/keisks/wmt-trueskill
4
This worked better than a version of the sampler that
changed one at a time.
3
these means are then recomputed by re-averaging
all values of {q
i
} associated with that system. Af-
ter the burn-in period, the ?s are stored as samples,
which are averaged when the sampling concludes.
During each iteration, q
1
and q
2
are resampled
from their corresponding system means:
q
1
? N (?
S
1
, ?
2
a
)
q
2
? N (?
S
2
, ?
2
a
)
We then update these values to respect the annota-
tion pi as follows. Let t = q
1
?q
2
(S
1
is the winner
by human judgments), and ensure that the values
are outside the decision radius, d:
q
?
1
=
{
q
1
t ? d
q
1
+
1
2
(d? t) otherwise
q
?
2
=
{
q
2
t ? d
q
2
?
1
2
(d? t) otherwise
In the case of a tie:
q
?
1
=
?
?
?
?
?
?
?
?
?
q
1
+
1
2
(d? t) t ? d
q
1
t < d
q
1
+
1
2
(?d? t) t ? ?d
q
?
2
=
?
?
?
?
?
?
?
?
?
q
2
?
1
2
(d? t) t ? d
q
2
t < d
q
2
?
1
2
(?d? t) t ? ?d
These values are stored for the current iteration
and averaged at its end to produce new estimates
of the system means. The quantity d? t can be in-
terpreted as a loss function, returning a high value
when the observed outcome is unexpected and a
low value otherwise (Figure 1).
2.3 TrueSkill
Prior to 2012, the WMT organizers included refer-
ence translations among the system comparisons.
These were used as a control against which the
evaluators could be measured for consistency, on
the assumption that the reference was almost al-
ways best. They were also included as data points
in computing the system ranking. Another of
Bojar et al. (2011)?s suggestions was to exclude
this data, because systems compared more of-
ten against the references suffered unfairly. This
can be further generalized to the observation that
not all competitions are equal, and a good model
should incorporate some notion of ?match diffi-
culty? when evaluating system?s abilities. The
inference procedure above incorporates this no-
tion implicitly in the inference procedure, but the
model itself does not include a notion of match
difficulty or outcome surprisal.
A model that does is TrueSkill
5
(Herbrich et al.,
2006). TrueSkill is an adaptive, online system that
also assumes that each system?s skill level follows
a Gaussian distribution, maintaining a mean ?
S
j
for each system S
j
representing its current esti-
mate of that system?s native ability. However, it
also maintains a per-system variance, ?
2
S
j
, which
represents TrueSkill?s uncertainty about its esti-
mate of each mean. After an outcome is observed
(a game in which the result is a win, loss, or draw),
the size of the updates is proportional to how sur-
prising the outcome was, which is computed from
the current system means and variances. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates.
Before defining the update equations, we need
to be more concrete about how this notion of sur-
prisal is incorporated. Let t = ?
S
1
? ?
S
2
, the dif-
ference in system relative abilities, and let  be a
fixed hyper-parameter corresponding to the earlier
decision radius. We then define two loss functions
of this difference for wins and for ties:
v
win
(t, ) =
N (?+ t)
?(?+ t)
v
tie
(t, ) =
N (?? t)?N (? t)
?(? t)? ?(?? t)
where ?(x) is the cumulative distribution function
and theN s are Gaussians. Figures 1 and 2 display
plots of these two functions compared to the Hop-
kins and May model. Note how v
win
(Figure 1) in-
creases exponentially as ?
S
2
becomes greater than
the (purportedly) better system, ?
S
1
.
As noted above, TrueSkill maintains not only
estimates {?
S
j
} of system abilities, but also
system-specific confidences about those estimates
5
The goal of this section is to provide an intuitive descrip-
tion of TrueSkill as adapted for WMT manual evaluations,
with enough detail to carry the main ideas. For more details,
please see Herbrich et al. (2006).
4
?1.0 ?0.5 0.0 0.5 1.0t = ?S
1
? ?S
2
0.0
0.5
1.0
1.5
v(t,?
)
TrueSkillHM
Figure 1: TrueSkill?s v
win
and the corresponding
loss function in the Hopkins and May model as
a function of the difference t of system means
( = 0.5, c = 0.8 for TrueSkill, and d = 0.5 for
Hopkins and May model).
?1.5 ?1.0 ?0.5 0.0 0.5 1.0 1.5t = ?S
1
? ?S
2
?1.0
?0.5
0.0
0.5
1.0
v(t,?
)
TrueSkillHM
Figure 2: TrueSkills v
tie
and the corresponding
loss function in the Hopkins and May model as
a function of the difference t of system means
( = 0.5, c = 0.3, and d = 0.5).
{?
S
j
}. These confidences also factor into the up-
dates: while surprising outcomes result in larger
updates to system means, higher confidences (rep-
resented by smaller variances) result in smaller
updates. TrueSkill defines the following value:
c
2
= 2?
2
+ ?
2
S
1
+ ?
2
S
2
which accumulates the variances along ?, another
free parameter. We can now define the update
equations for the system means:
?
S
1
= ?
S
1
+
?
2
S
1
c
? v
(
t
c
,

c
)
?
S
2
= ?
S
2
?
?
2
S
2
c
? v
(
t
c
,

c
)
The second term in these equations captures the
idea about balancing surprisal with confidence,
described above.
In order to update the system-level confidences,
TrueSkill defines another set of functions, w, for
the cases of wins and ties. These functions are
multiplicative factors that affect the amount of
change in ?
2
:
w
win
(t, ) = v
win
? (v
win
+ t? )
w
tie
(t, ) = v
tie
+
(? t) ? N (? t) + (+ t) ? N (+ t)
?(? t)? ?(?? t)
The underlying idea is that these functions cap-
ture the outcome surprisal via v. This update al-
ways decreases the size of the variances ?
2
, which
means uncertainty of ? decreases as comparisons
go on. With these defined, we can conclude by
defining the updates for ?
2
S
1
and ?
2
S
2
:
?
2
S
1
= ?
2
S
1
?
[
1?
?
2
S
1
c
2
? w
(
t
c
,

c
)
]
?
2
S
2
= ?
2
S
2
?
[
1?
?
2
S
2
c
2
? w
(
t
c
,

c
)
]
One final complication not presented here but rel-
evant to adapting TrueSkill to the WMT setting:
the parameter ? and another parameter (not dis-
cussed) ? are incorporated into the update equa-
tions to give more weight to recent matches. This
?latest-oriented? property is useful in the gaming
setting for which TrueSkill was built, where play-
ers improve over time, but is not applicable in the
WMT competition setting. To cancel this property
in TrueSkill, we set ? = 0 and ? = 0.025 ? |A| ??
2
in order to lessen the impact of the order in which
annotations are presented to the system.
2.4 Data selection with TrueSkill
A drawback of the standard WMT data collection
method is that it samples uniformly from the space
of pairwise system combinations. This is undesir-
able: systems with vastly divergent relative abil-
ity need not be compared as often as systems that
are more evenly matched. Unfortunately, one can-
not sample non-uniformly without knowing ahead
of time which systems are better. TrueSkill pro-
vides a solution to this dilemma with its match-
selection ability: systems with similar means and
low variances can be confidently considered to be
close matches. This presents a strong possibility
of reducing the amount of data that needs to be
5
collected in the WMT competitions. In fact, the
TrueSkill formulation provides a way to compute
the probability of a draw between two systems,
which can be used to compute for a system S
i
a
conditional distribution over matches with other
systems {S
j 6=i
}.
Formally, in the TrueSkill model, the match-
selection (chance to draw) between two players
(systems in WMT) is computed as follows:
p
draw
=
?
2?
2
c
2
? exp(?
(?
a
? ?
b
)
2
2c
2
)
However, our setting for canceling the ?latest-
oriented? property affects this matching quality
equation, where most systems are almost equally
competitive (? 1). Therefore, we modify the equa-
tion in the following manner which simply de-
pends on the difference of ?.
p?
draw
=
1
exp(|?
a
? ?
b
|)
TrueSkill selects the matches it would like to
create, according to this selection criteria. We do
this according to the following process:
1. Select a system S
1
(e.g., the one with the
highest variance)
2. Compute a normalized distribution over
matches with other systems pairs p?
draw
3. Draw a system S
2
from this distribution
4. Draw a source sentence, and present to the
judge for annotation
3 Experimental setup
3.1 Datasets
We used the evaluation data released by WMT13.
6
The data contains (1) five-way system rankings
made by either researchers or Turkers and (2)
translation data consisting of source sentences, hu-
man reference translations, and submitted transla-
tions. Data exists for 10 language pairs. More de-
tails about the dataset can be found in the WMT
2013 overview paper (Bojar et al., 2013).
Each five-way system ranking was converted
into ten pairwise judgments (?2). We trained the
models using randomly selected sets of 400, 800,
1,600, 3,200, and 6,400 pairwise comparisons,
6
statmt.org/wmt13/results.html
each produced in two ways: selecting from all re-
searchers, or split between researchers and Turk-
ers. An important note is that the training data
differs according to the model. For the Expected
Wins and Hopkins and May model, we sim-
ply sample uniformly at random. The TrueSkill
model, however, selects its own training data (with
replacement) according to the description in Sec-
tion 2.4.
7
For tuning hyperparameters and reporting test
results, we used development and test sets of 2,000
comparisons drawn entirely from the researcher
judgments, and fixed across all experiments.
3.2 Perplexity
We first compare the Hopkins and May model and
TrueSkill using perplexity on the test data T , com-
puted as follows:
ppl(p|T ) = 2
?
?
(i,S
1
,S
2
,pi)?T
log
2
p(pi|S
1
,S
2
)
where p is the model under consideration. The
probability of each observed outcome pi between
two systems S
1
and S
2
is computed by taking a
difference of the Gaussian distributions associated
with those systems:
N (?
?
, ?
2
?
) = N (?
S
1
, ?
2
S
1
)?N (?
S
2
, ?
2
S
2
)
= N (?
S
1
? ?
S
2
, ?
2
S
1
+ ?
2
S
2
)
This Gaussian can then be carved into three pieces:
the area where S
1
loses, the middle area represent-
ing ties (defined by a decision radius, r, whose
value is fit using development data), and a third
area representing where S
1
wins. By integrating
over each of these regions, we have a probability
distribution over these outcomes:
p(pi | S
1
, S
2
) =
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
0
??
N (?
?
, ?
2
?
) if pi is >
?
r
0
N (?
?
, ?
2
?
) if pi is =
?
?
r
N (?
?
, ?
2
?
) if pi is <
We do not compute perplexity for the Expected
Wins model, which does not put any probability
mass on ties.
7
We use a Python implementation of TrueSkill
(github.com/sublee/trueskill).
6
3.3 Accuracy
Perplexity is often viewed as a neutral metric, but
without access to unbounded training data or the
true model parameters, it can only be approxi-
mated. Furthermore, it does not always corre-
late perfectly with evaluation metrics. As such,
we also present accuracy results, measuring each
model?s ability to predict the values of the ternary
pairwise judgments made by the annotators. These
are computed using the above equation, picking
the highest value of p(pi) for all annotations be-
tween each system pair (S
i
, S
j
). As with perplex-
ity, we emphasize that these predictions are func-
tions of the system pair only, and not the individual
sentences under consideration, so the same out-
come is always predicted for all sentences between
a system pair.
3.4 Parameter Tuning
We follow the settings described in Hopkins and
May (2013) for their model: ?
a
= 0.5, ?
obs
= 1.0,
and d = 0.5. In TrueSkill, in accordance with the
Hopkins and May model, we set the initial ? and
? values for each system to 0 and 0.5 respectively,
and  to 0.25.
For test data, we tuned the ?decision ra-
dius? parameter r by doing grid search over
{0.001, 0.01, 0.1, 0.3, 0.5}, searching for the
value which minimized perplexity and maximized
accuracy on the development set. We do this for
each model and language pair. When tuned by
perplexity, r is typically either 0.3 or 0.5 for both
models and language pairs, whereas, for accuracy,
the best r is either 0.001, 0.01, or 0.1.
4 Results
4.1 Model Comparison
Figure 3 shows the perplexity of the two mod-
els with regard to the number of training compar-
isons. The perplexities in the figure are averaged
over all ten language pairs in the WMT13 dataset.
Overall, perplexities decrease according to the in-
crease of training size. The Hopkins and May
and TrueSkill models trained on both researcher
and Turker judgments are comparable, whereas
the Hopkins and May model trained on researcher
judgments alone shows lower perplexity than the
corresponding TrueSkill model.
In terms of accuracy, we see that the TrueSkill
model has the highest accuracies, saturating at just
over 3,000 training instances (Figure 4). TrueSkill
1000 2000 3000 4000 5000 6000Training Data Size2.80
2.85
2.90
2.95
3.00
Perp
lexit
y
HM-allHM-resTS-allTS-res
Figure 3: Model Perplexities for WMT13 dataset.
?all? indicates that models are trained on both re-
searcher and Turker judgements, and ?res? means
that models are trained on only researcher judge-
ments.
outperforms Expected Win and the Hopkins and
May, especially when the training size is small
(Table 2). We also note that training on researcher
judgments alone (dashed lines) results in better
performance than training on both researchers and
Turker judgments. This likely reflects both a bet-
ter match between training and test data (recall the
test data consists of researcher judgments only),
as well as the higher consistency of this data, as
evidenced by the annotator agreement scores pub-
lished in the WMT overview paper (Bojar et al.,
2013). Recall that the models only have access
to the system pair (and not the sentences them-
selves), and thus make the same prediction for pi
for a particular system pair, regardless of which
source sentence was selected. As an upper bound
for performance on this metric, Table 2 contains
an oracle score, which is computed by selecting,
for each pair of systems, the highest-probability
ranking.
8
Comparing the plots, we see there is not a per-
fect relationship between perplexity and accuracy
among the models; the low perplexity does not
mean the high accuracy, and in fact the order of
the systems is different.
4.2 Free-for-all matches
TrueSkill need not deal with judgments in pairs
only, but was in fact designed to be used in a vari-
ety of settings, including N-way free-for-all games
8
Note that this might not represent a consistent ranking
among systems, but is itself an upper bound on the highest-
scoring consistent ranking.
7
1000 2000 3000 4000 5000 6000Training Data Size0.460
0.465
0.470
0.475
0.480
0.485
0.490
0.495
0.500
Accu
racy
ExpWin-allExpWin-resHM-allHM-resTS-allTS-res
Figure 4: Model accuracies with different training
domain for WMT13 dataset.
Train Size Exp-Win HM TrueSkill
400 0.465 0.471 0.479
800 0.471 0.475 0.483
all 1600 0.479 0.477 0.493
3200 0.486 0.489 0.493
6400 0.487 0.490 0.495
400 0.460 0.463 0.484
800 0.475 0.473 0.488
res 1600 0.481 0.482 0.493
3200 0.492 0.494 0.497
6400 0.495 0.496 0.497
Upper Bound 0.525
Table 2: Model accuracies: models are tuned by
accuracy instead of perplexity. Upper bound is
computed by selecting the most frequent choice
(<,>,=) for each system pair.
with many players all competing for first place.
This adapts nicely to WMT?s actual collection set-
ting. Recall that annotators are presented with five
translations which are then ranked; we can treat
this setting as a 5-way free-for-all match. While
the details of these updates are beyond the scope of
this paper, they are presented in the original model
and are implemented in the toolkit we used. We
thus also conducted experiments varying the value
of N from 2 to 5.
The results are shown in Tables 3 and 4, which
hold constant the number of matches and pairwise
judgments, respectively. When fixing the num-
ber of matches, the 5-way setting is at an advan-
tage, since there is much more information in each
match; in contrast, when fixing the number of pair-
wise comparisons, the 5-way setting is at a dis-
advantage, since many fewer competitions consti-
# N=2 N=3 N=4 N=5
400 0.479 0.482 0.491 0.492
800 0.483 0.493 0.495 0.495
1600 0.493 0.492 0.497 0.495
3200 0.493 0.494 0.498 0.497
6400 0.495 0.498 0.498 0.498
Table 3: Accuracies when training with N-way
free-for-all models, fixing the number of matches.
# N=2 N=3 N=4 N=5
400 0.479 0.475 0.470 0.459
800 0.483 0.488 0.476 0.466
1600 0.493 0.488 0.481 0.481
3200 0.493 0.492 0.487 0.489
6400 0.495 0.496 0.494 0.495
Table 4: Accuracies when training with N-way
free-for-all models, fixing the number of pairwise
comparisons.
tute these comparisons. The results bear this out,
but also suggest that the standard WMT setting
? which extracts ten pairwise comparisons from
each 5-way match and treats them independently
? works well. We will not speculate further here,
but provide this experiment purely to motivate po-
tential future work. Here we will focus our con-
clusions to the pair-wise ranking scenario.
5 Reduced Data Collection with
Non-uniform Match Selection
As mentioned earlier, a drawback of the selection
of training data for annotation is that it is sampled
uniformly from the space of system pair compe-
titions, and an advantage of TrueSkill is its abil-
ity to instead compute a distribution over pairings
and thereby focus annotation efforts on competi-
tive matches. In this section, we report results in
the form of heat maps indicating the percentage of
pairwise judgments requested by TrueSkill across
the full cross-product of system pairs, using the
WMT13 French-English translation task.
Figure 5 depicts a system-versus-system heat
map for all judgments in the dataset. Across this
figure and the next two, systems are sorted along
each axis by the final values of ? inferred by
TrueSkill during training, and the heat of each
square is proportional to the percentage of judg-
ments obtained between those two systems. The
diagonal reflects the fact that systems do not com-
pete against themselves, and the stripe at row and
column 5 reflects a system that was entered late
8
1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 5: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems in
the WMT13 French-English translation task.
1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 6: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems
used in the first 20% of TrueSkill model.
1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
2.0
Figure 7: Heat map for the ratio of pairwise judg-
ments across the full cross-product of systems
used in the last 20% of TrueSkill model.
into the WMT13 competition and thus had many
fewer judgments. It is clear that these values are
roughly uniformly distributed. This figure serves
as a sort of baseline, demonstrating the lack of pat-
terns in the data-selection process.
The next two figures focus on the data that
TrueSkill itself selected for its use from among all
of the available data. Figure 6 is a second heat
map presenting the set of system pairs selected by
TrueSkill for the first 20% of its matches chosen
during training, while Figure 7 presents a heat map
of the last 20%. The contrast is striking: whereas
the judgments are roughly uniformly distributed at
the beginning, the bulk of the judgments obtained
for the last set are clustered along the diagonal,
where the most competitive matches lie.
Together with the higher accuracy of TrueSkill,
this suggests that it could be used to decrease the
amount of data that needs to be collected in future
WMT human evaluations by focusing the annota-
tion effort on more closely-matched systems.
6 Clustering
As pointed out by Koehn (2012), a ranking pre-
sented as a total ordering among systems con-
ceals the closeness of comparable systems. In the
WMT13 competition, systems are grouped into
clusters, which is equivalent to presenting only
a partial ordering among the systems. Clusters
are constructed using bootstrap resampling to in-
fer many system rankings. From these rankings,
rank ranges are then collected, which can be used
to construct 95% confidence intervals, and, in turn,
to cluster systems whose ranges overlap. We use
a similar approach for clustering in the TrueSkill
model. We obtain rank ranges for each system by
running the TrueSkill model 100 times,
9
throw-
ing out the top and bottom 2 rankings for each
system, and clustering where rank ranges overlap.
For comparison, we also do this for the other two
models, altering the amount of training data from
1k to 25k in increments of 1,000, and plotting the
number of clusters that can be obtained from each
technique on each amount of training data.
Figure 8 show the number of clusters according
to the increase of training data for three models.
TrueSkill efficiently split the systems into clusters
compared to other two methods. Figure 9 and 10
present the result of clustering two different size of
9
We also tried the sampling 1,000 times and the clustering
granularities were the same.
9
5000 10000 15000 20000 25000Pairwise Comparisons
0
1
2
3
4
5
6
7
Num
. of 
Clu
ster
s
ExpWin
HM
TS
Figure 8: The number of clusters according to
the increase of training data for WMT13 French-
English (13 systems in total).
training data (1K and 25K pairwise comparisons)
on the TrueSkill model, which indicates that the
rank ranges become narrow and generate clusters
reasonably as the number of training samples in-
creases. The ranking and clusters are slightly dif-
ferent from the official result (Table 1) mainly be-
cause the official result is based on Expected Wins.
One noteworthy observation is that the ranking
of systems between Figure 9 and Figure 10 is the
same, further corroborating the stability and ac-
curacy of the TrueSkill model even with a small
amount of data. Furthermore, while the need
to cluster systems forces the collection of sig-
nificantly more data than if we wanted only to
report a total ordering, TrueSkill here produces
nicely-sized clusters with only 25K pairwise com-
parisons, which is nearly one-third large of that
used in the WMT13 campaign (80K for French-
English, yielding 8 clusters).
7 Conclusion
Models of ?relative ability? (Koehn, 2012; Hop-
kins and May, 2013) are a welcome addition to
methods for inferring system rankings from hu-
man judgments. The TrueSkill variant presented
in this paper is a promising further development,
both in its ability to achieve higher accuracy levels
than alternatives, and in its ability to sample non-
uniformly from the space of system pair match-
ings. It?s possible that future WMT evaluations
could significantly reduce the amount of data they
need to collect, also potentially allowing them to
draw from expert annotators alone (the developers
uedin-h on.Buedin-w LIMSI KIT on.A MES-S DCU CMURWTH cu-z JHU Shef
1
2
3
4
5
6
7
8
9
10
11
12
13
Figure 9: The result of clustering by TrueSkill
model with 1K training data from WMT13
French-English. The boxes range from the lower
to upper quartile values, with means in the middle.
The whiskers show the full range of each system?s
rank after the bootstrap resampling.
uedin-h on.Buedin-w LIMSI KIT on.A MES-S DCU CMURWTH cu-z JHU Shef
1
2
3
4
5
6
7
8
9
10
11
12
13
Figure 10: The result of clustering by TrueSkill
model with 25K training data. Dashed lines sep-
arate systems with non-overlapping rank ranges,
splitting the data into clusters.
of the participating systems), without the need to
hire non-experts on Mechanical Turk.
One piece missing from the methods explored
and proposed in this paper is models of the actual
translations being compared by judges. Clearly,
it is properties of the sentences themselves that
judges use to make their judgments, a fact which
is captured only indirectly by modeling transla-
tion qualities sampled from system abilities. This
observation has been used in the development
of automatic evaluation metrics (Song and Cohn,
2011), and is something we hope to explore in fu-
ture work for system ranking.
10
References
Ond?rej Bojar, Milo?s Ercegov?cevi?c, Martin Popel, and
Omar Zaidan. 2011. A Grain of Salt for the WMT
Manual Evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation, pages
1?11, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Omar Zaidan. 2011. Findings of the 2011
Workshop on Statistical Machine Translation. In
Proceedings of the Sixth Workshop on Statisti-
cal Machine Translation, pages 22?64, Edinburgh,
Scotland, July. Association for Computational Lin-
guistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the Seventh
Workshop on Statistical Machine Translation, pages
10?51, Montr?eal, Canada, June. Association for
Computational Linguistics.
Ralf Herbrich, Tom Minka, and Thore Graepel. 2006.
TrueSkill
TM
: A Bayesian Skill Rating System. In
Proceedings of the Twentieth Annual Conference on
Neural Information Processing Systems, pages 569?
576, Vancouver, British Columbia, Canada, Decem-
ber. MIT Press.
Mark Hopkins and Jonathan May. 2013. Models of
translation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Computa-
tional Linguistics (Volume 1: Long Papers), pages
1416?1424, Sofia, Bulgaria, August. Association for
Computational Linguistics.
Philipp Koehn. 2012. Simulating Human Judgment
in Machine Translation Evaluation Campaigns. In
Proceedings of the 9th International Workshop on
Spoken Language Translation (IWSLT), pages 179?
184, Hong Kong, China, December. International
Speech Communication Association.
Adam Lopez. 2012. Putting Human Assessments of
Machine Translation Systems in Order. In Proceed-
ings of the Seventh Workshop on Statistical Machine
Translation, pages 1?9, Montr?eal, Canada, June. As-
sociation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania, July. Association for Computational
Linguistics.
Xingyi Song and Trevor Cohn. 2011. Regression and
Ranking based Optimisation for Sentence Level MT
Evaluation. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, pages 123?129,
Edinburgh, Scotland, July. Association for Compu-
tational Linguistics.
11
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12?58,
Baltimore, Maryland USA, June 26?27, 2014.
c
?2014 Association for Computational Linguistics
Findings of the 2014 Workshop on Statistical Machine Translation
Ond
?
rej Bojar
Charles University in Prague
Christian Buck
University of Edinburgh
Christian Federmann
Microsoft Research
Barry Haddow
University of Edinburgh
Philipp Koehn
JHU / Edinburgh
Johannes Leveling
Dublin City University
Christof Monz
University of Amsterdam
Pavel Pecina
Charles University in Prague
Matt Post
Johns Hopkins University
Herve Saint-Amand
University of Edinburgh
Radu Soricut
Google
Lucia Specia
University of Sheffield
Ale
?
s Tamchyna
Charles University in Prague
Abstract
This paper presents the results of the
WMT14 shared tasks, which included a
standard news translation task, a sepa-
rate medical translation task, a task for
run-time estimation of machine translation
quality, and a metrics task. This year, 143
machine translation systems from 23 insti-
tutions were submitted to the ten transla-
tion directions in the standard translation
task. An additional 6 anonymized sys-
tems were included, and were then evalu-
ated both automatically and manually. The
quality estimation task had four subtasks,
with a total of 10 teams, submitting 57 en-
tries.
1 Introduction
We present the results of the shared tasks of
the Workshop on Statistical Machine Translation
(WMT) held at ACL 2014. This workshop builds
on eight previous WMT workshops (Koehn and
Monz, 2006; Callison-Burch et al., 2007, 2008,
2009, 2010, 2011, 2012; Bojar et al., 2013).
This year we conducted four official tasks: a
translation task, a quality estimation task, a met-
rics task
1
and a medical translation task. In the
translation task (?2), participants were asked to
translate a shared test set, optionally restricting
themselves to the provided training data. We held
ten translation tasks this year, between English and
each of Czech, French, German, Hindi, and Rus-
sian. The Hindi translation tasks were new this
year, providing a lesser resourced data condition
on a challenging language pair. The system out-
puts for each task were evaluated both automati-
cally and manually.
1
The metrics task is reported in a separate paper
(Mach?a?cek and Bojar, 2014).
The human evaluation (?3) involves asking
human judges to rank sentences output by
anonymized systems. We obtained large num-
bers of rankings from researchers who contributed
evaluations proportional to the number of tasks
they entered. Last year, we dramatically increased
the number of judgments, achieving much more
meaningful rankings. This year, we developed a
new ranking method that allows us to achieve the
same with fewer judgments.
The quality estimation task (?4) this year
included sentence- and word-level subtasks:
sentence-level prediction of 1-3 likert scores,
sentence-level prediction of percentage of word
edits necessary to fix a sentence, sentence-level
prediction of post-editing time, and word-level
prediction of scores at different levels of granular-
ity (correct/incorrect, accuracy/fluency errors, and
specific types of errors). Datasets were released
with English-Spanish, English-German, Spanish-
English and German-English news translations
produced by 2-3 machine translation systems and,
for some subtasks, a human translation.
The medical translation task (?5) was intro-
duced this year. Unlike the ?standard? translation
task, the test sets come from the very specialized
domain of medical texts. The aim of this task was
not only domain adaptation but also the utilization
of translation systems in a larger scenario, namely
cross-lingual information retrieval (IR). Extrinsic
evaluation in an IR setting was a part of this task
(on the other hand, manual evaluation of transla-
tion quality was not carried out).
The primary objectives of WMT are to evaluate
the state of the art in machine translation, to dis-
seminate common test sets and public training data
with published performance numbers, and to re-
fine evaluation and estimation methodologies for
machine translation. As before, all of the data,
12
translations, and collected human judgments are
publicly available.
2
We hope these datasets serve
as a valuable resource for research into statistical
machine translation and automatic evaluation or
prediction of translation quality.
2 Overview of the Translation Task
The recurring task of the workshop examines
translation between English and other languages.
As in the previous years, the other languages in-
clude German, French, Czech and Russian.
We dropped Spanish and added Hindi this year.
From a linguistic point of view, Spanish poses
similar problems as French, making its prior in-
clusion less valuable. Hindi is not only interest-
ing since it is a more distant language than the
European languages we include, but also because
we have much less training data, thus forcing re-
searchers to deal with low resource conditions, but
also providing them with a language pair that does
not suffer from the computational complexities of
having to deal with massive amounts of training
data.
We created a test set for each language pair by
translating newspaper articles and provided train-
ing data.
2.1 Test data
The test data for this year?s task was selected from
news stories from online sources, as before. How-
ever, we changed our method to create the test sets.
In previous years, we took equal amounts of
source sentences from all six languages involved
(around 500 sentences each), and translated them
into all other languages. While this produced a
multi-parallel test corpus that could be also used
for language pairs (such as Czech-Russian) that
we did not include in the evaluation, it did suf-
fer from artifacts from the larger distance between
source and target sentences. Most test sentences
involved the translation a source sentence that
was translated from a their language into a tar-
get sentence (which was compared against a trans-
lation from that third language as well). Ques-
tions have been raised, if the evaluation of, say,
French-English translation is best served when
testing on sentences that have been originally writ-
ten in, say, Czech. For discussions about trans-
lationese please for instance refer to Koppel and
Ordan (2011).
2
http://statmt.org/wmt14/results.html
This year, we took about 1500 English sen-
tences and translated them into the other 5 lan-
guages, and then additional 1500 sentences from
each of the other languages and translated them
into English. This gave us test sets of about 3000
sentences for our English-X language pairs, which
have been either written originally written in En-
glish and translated into X, or vice versa.
The composition of the test documents is shown
in Table 1. The stories were translated by the pro-
fessional translation agency Capita, funded by the
EU Framework Programme 7 project MosesCore,
and by Yandex, a Russian search engine com-
pany.
3
All of the translations were done directly,
and not via an intermediate language.
2.2 Training data
As in past years we provided parallel corpora
to train translation models, monolingual cor-
pora to train language models, and development
sets to tune system parameters. Some train-
ing corpora were identical from last year (Eu-
roparl
4
, United Nations, French-English 10
9
cor-
pus, CzEng, Common Crawl, Russian-English
Wikipedia Headlines provided by CMU), some
were updated (Russian-English parallel data pro-
vided by Yandex, News Commentary, monolin-
gual data), and a new corpus was added (Hindi-
English corpus, Bojar et al. (2014)), Hindi-English
Wikipedia Headline corpus).
Some statistics about the training materials are
given in Figure 1.
2.3 Submitted systems
We received 143 submissions from 23 institu-
tions. The participating institutions and their entry
names are listed in Table 2; each system did not
necessarily appear in all translation tasks. We also
included four commercial off-the-shelf MT sys-
tems and four online statistical MT systems, which
we anonymized.
For presentation of the results, systems are
treated as either constrained or unconstrained, de-
pending on whether their models were trained only
on the provided data. Since we do not know how
they were built, these online and commercial sys-
tems are treated as unconstrained during the auto-
matic and human evaluations.
3
http://www.yandex.com/
4
As of Fall 2011, the proceedings of the European Parlia-
ment are no longer translated into all official languages.
13
Europarl Parallel Corpus
French? English German? English Czech? English
Sentences 2,007,723 1,920,209 646,605
Words 60,125,563 55,642,101 50,486,398 53,008,851 14,946,399 17,376,433
Distinct words 140,915 118,404 381,583 115,966 172,461 63,039
News Commentary Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 183,251 201,288 146,549 165,602
Words 5,688,656 4,659,619 5,105,101 5,046,157 3,288,645 3,590,287 4,153,847 4,339,974
Distinct words 72,863 62,673 150,760 65,520 139,477 55,547 151,101 60,801
Common Crawl Parallel Corpus
French? English German? English Czech? English Russian? English
Sentences 3,244,152 2,399,123 161,838 878,386
Words 91,328,790 81,096,306 54,575,405 58,870,638 3,529,783 3,927,378 21,018,793 21,535,122
Distinct words 889,291 859,017 1,640,835 823,480 210,170 128,212 764,203 432,062
United Nations Parallel Corpus
French? English
Sentences 12,886,831
Words 411,916,781 360,341,450
Distinct words 565,553 666,077
10
9
Word Parallel Corpus
French? English
Sentences 22,520,400
Words 811,203,407 668,412,817
Distinct words 2,738,882 2,861,836
CzEng Parallel Corpus
Czech? English
Sentences 14,833,358
Words 200,658,857 228,040,794
Distinct words 1,389,803 920,824
Hindi-English Parallel Corpus
Hindi? English
Sentences 287,202
Words 6,002,418 3,953,851
Distinct words 121,236 105,330
Yandex 1M Parallel Corpus
Russian? English
Sentences 1,000,000
Words 24,121,459 26,107,293
Distinct words 701,809 387,646
Wiki Headlines Parallel Corpus
Russian? English Hindi? English
Sentences 514,859 32,863
Words 1,191,474 1,230,644 141,042 70,075
Distinct words 282,989 251,328 25,678 26,989
Europarl Language Model Data
English French German Czech
Sentence 2,218,201 2,190,579 2,176,537 668,595
Words 59,848,044 63,439,791 53,534,167 14,946,399
Distinct words 123,059 145,496 394,781 172,461
News Language Model Data
English French German Czech Russian Hindi
Sentence 90,209,983 30,451,749 89,634,193 36,426,900 32,245,651 1,275,921
Words 2,109,603,244 748,852,739 1,606,506,785 602,950,410 575,423,682 36,297,394
Distinct words 4,089,792 1,906,470 10,248,707 3,101,846 2,860,837 258,759
News Test Set
French? English German? English Czech? English Russian? English Hindi? English
Sentences 3003 3003 3003 3003 2507
Words 81,194 71,147 63,078 67,624 60,240 68,866 62,107 69,329 86,974 55,822
Distinct words 11,715 10,610 13,930 10,458 16,774 9,893 17,009 9,938 8,292 9,217
Figure 1: Statistics for the training and test sets used in the translation task. The number of words and the number of distinct
words (case-insensitive) is based on the provided tokenizer.
14
Language Sources (Number of Documents)
Czech aktu?aln?e.cz (2), blesk.cz (3), blisty.cz (1), den??k.cz (9), e15.cz (1), iDNES.cz (17), ihned.cz (14), lidovky.cz (8), medi-
afax.cz (2), metro.cz (1), Novinky.cz (5), pravo.novinky.cz (6), reflex.cz (2), tyden.cz (1), zdn.cz (1).
French BBC French Africa (1), Canoe (9), Croix (4), Cyber Presse (12), Dernieres Nouvelles (1), dhnet.be (5), Equipe (1),
Euronews (6), Journal Metro.com (1), La Libre.be (2), La Meuse.be (2), Le Devoir (3), Le Figaro (8), Le Monde (3),
Les Echos (15), Lexpress.fr (3), Liberation (1), L?independant (2), Metro France (1), Nice-Matin (6), Le Nouvel Ob-
servateur (3), Radio Canada (6), Reuters (7).
English ABC News (5), BBC (5), CBS News (5), CNN (5), Daily Mail (5), Financial Times (5), Fox News (2), Globe and
Mail (1), Independent (1), Los Angeles Times (1), New Yorker (1), News.com Australia (16), Reuters (3), Scotsman (2),
smh.com.au (2), stv.tv (1), Telegraph (6), UPI (2).
German Abendzeitung N?urnberg (1), all-in.de (2), Augsburger Allgemeine (1), AZ Online (1), B?orsenzeitung (1), come-
on.de (1), Der Westen (2), DZ Online (1), Reutlinger General-Anzeiger (1), Generalanzeiger Bonn (1), Giessener
Anzeiger (1), Goslarsche Zeitung (1), Hersfelder Zeitung (1), J?udische Allgemeine (1), Kreisanzeiger (2),
Kreiszeitung (2), Krone (1), Lampertheimer Zeitung (2), Lausitzer Rundschau (1), Mittelbayerische (1), Morgen-
post (1), nachrichten.at (1), Neue Presse (1), OP Online (1), Potsdamer Neueste Nachrichten (1), Passauer Neue
Presse (1), Recklingh?auser Zeitung (1), Rhein Zeitung (1), salzburg.com (1), Schwarzw?alder Bote (29), Segeberger
Zeitung (1), Soester Anzeiger (1), S?udkurier (17), svz.de (1), Tagesspiegel (1), Usinger Anzeiger (3), Volksblatt.li (1),
Westf?alischen Anzeiger (3), Wiener Zeitung (1), Wiesbadener Kurier (1), Westdeutsche Zeitung (1), Wilhelmshavener
Zeitung (1), Yahoo Deutschland (1).
Hindi Bhaskar (24), Jagran (61), Navbharat Times / India Times (4), ndtv (2).
Russian 168.ru (1), aif (3), altapress.ru (2), argumenti.ru (2), BBC Russian (3), belta.by (2), communa.ru (1), dp.ru (1), eg-
online.ru (1), Euronews (2), fakty.ua (2), gazeta.ru (1), inotv.rt.com (1), interfax (1), Izvestiya (1), Kommersant (7),
kp (2), lenta.ru (4), lgng (1), litrossia.ru (1), mirnov.ru (5), mk (8), mn.ru (2), newizv (2), nov-pravda.ru (1), no-
vayagazeta (1), nr2.ru (8), pnp.ru (1), rbc.ru (3), ria.ru (4), rosbalt.ru (1), sovsport.ru (6), Sport Express (10), trud.ru (4),
tumentoday.ru (1), vesti.ru (10), zr.ru (1).
Table 1: Composition of the test set. For more details see the XML test files. The docid tag gives the source and the date for
each document in the test set, and the origlang tag indicates the original source language.
3 Human Evaluation
As with past workshops, we contend that auto-
matic measures of machine translation quality are
an imperfect substitute for human assessments.
We therefore conduct a manual evaluation of the
system outputs and define its results to be the prin-
cipal ranking of the workshop. In this section, we
describe how we collected this data and compute
the results, and then present the official results of
the ranking.
This year?s evaluation was conducted a bit dif-
ferently. The main differences are:
? In contrast to the past two years, we collected
judgments entirely from researchers partici-
pating in the shared tasks and trusted friends
of the community. Last year, about two thirds
of the data were solicited from random volun-
teers on the Amazon Mechanical Turk. For
some language pairs, the Turkers data had
much lower inter-annotator agreement com-
pared to the researchers.
? As a result, we collected about seventy-five
percent less data, but were able to obtain
good confidence intervals on the clusters with
the use of new approaches to ranking.
? We compared three different ranking method-
ologies, selecting the one with the highest ac-
curacy on held-out data.
We also maintain many of our customs from
prior years, including the presentation of the re-
sults in terms of a partial ordering (clustering) of
the systems. Systems in the same cluster could not
be meaningfully distinguished and should be con-
sidered ties.
3.1 Data collection
The system ranking is produced from a large set of
pairwise annotations between system pairs. These
pairwise annotations are collected in an evaluation
campaign that enlists participants in the shared
task to contribute one hundred ?Human Intelli-
gence Tasks? (HITs) per system submitted. Each
HIT consists of three ranking tasks. In a rank-
ing task, an annotator is presented with a source
segment, a human reference translation, and the
outputs of five anonymized systems, randomly se-
lected from the set of participating systems, and
randomly ordered.
To run the evaluation, we use Appraise
5
(Fe-
dermann, 2012), an open-source tool built on
Python?s Django framework. At the top of each
HIT, the following instructions are provided:
You are shown a source sentence fol-
lowed by several candidate translations.
Your task is to rank the translations from
best to worst (ties are allowed).
5
https://github.com/cfedermann/Appraise
15
ID Institution
AFRL, AFRL-PE Air Force Research Lab (Schwartz et al., 2014)
CIMS University of Stuttgart / University of Munich (Cap et al., 2014)
CMU Carnegie Mellon University (Matthews et al., 2014)
CU-* Charles University, Prague (Tamchyna et al., 2014)
DCU-FDA Dublin City University (Bicici et al., 2014)
DCU-ICTCAS Dublin City University (Li et al., 2014b)
DCU-LINGO24 Dublin City University / Lingo24 (wu et al., 2014)
EU-BRIDGE EU-BRIDGE Project (Freitag et al., 2014)
KIT Karlsruhe Institute of Technology (Herrmann et al., 2014)
IIT-BOMBAY IIT Bombay (Dungarwal et al., 2014)
IIIT-HYDERABAD IIIT Hyderabad
IMS-TTT University of Stuttgart / University of Munich (Quernheim and Cap, 2014)
IPN-UPV-* IPN-UPV (Costa-juss`a et al., 2014)
KAZNU Amandyk Kartbayev, FBK
LIMSI-KIT LIMSI / Karlsruhe Instutute of Technology (Do et al., 2014)
MANAWI-* Universit?at des Saarlandes (Tan and Pal, 2014)
MATRAN Abu-MaTran Project: Promsit / DCU / UA (Rubino et al., 2014)
PROMT-RULE,
PROMT-HYBRID
PROMT
RWTH RWTH Aachen (Peitz et al., 2014)
STANFORD Stanford University (Neidert et al., 2014; Green et al., 2014)
UA-* University of Alicante (S?anchez-Cartagena et al., 2014)
UEDIN-PHRASE,
UEDIN-UNCNSTR
University of Edinburgh (Durrani et al., 2014b)
UEDIN-SYNTAX University of Edinburgh (Williams et al., 2014)
UU, UU-DOCENT Uppsala University (Hardmeier et al., 2014)
YANDEX Yandex School of Data Analysis (Borisov and Galinskaya, 2014)
COMMERCIAL-[1,2] Two commercial machine translation systems
ONLINE-[A,B,C,G] Four online statistical machine translation systems
RBMT-[1,4] Two rule-based statistical machine translation systems
Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the
commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore
anonymized in a fashion consistent with previous years of the workshop.
16
Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a
source segment, a reference translation, and the outputs of five systems (anonymized and randomly ordered), and is asked to
rank these according to their translation quality, with ties allowed.
A screenshot of the ranking interface is shown in
Figure 2. Annotators are asked to rank the sys-
tems from 1 (best) to 5 (worst), with ties permit-
ted. Note that a lower rank is better. The rankings
provided by a ranking task are then reduced to a
set of ten pairwise rankings produced by consider-
ing all
(
5
2
)
combinations of systems in the ranking
task. For example, consider the following annota-
tion provided among systems A,B, F,H , and J :
1 2 3 4 5
F ?
A ?
B ?
J ?
H ?
This is reduced to the following set of pairwise
judgments:
A > B,A = F,A > H,A < J
B < F,B < H,B < J
F > H,F < J
H < J
Here,A > B should be read is ?A is ranked higher
than (worse than) B?. Note that by this procedure,
the absolute value of ranks and the magnitude of
their differences are discarded.
For WMT13, nearly a million pairwise anno-
tations were collected from both researchers and
paid workers on Amazon?s Mechanical Turk, in
a roughly 1:2 ratio. This year, we collected data
from researchers only, an ability that was enabled
by the use of a new technique for producing the
partial ranking for each task (?3.3.3). Table 3 con-
tains more detail.
3.2 Annotator agreement
Each year we calculate annotator agreement
scores for the human evaluation as a measure of
the reliability of the rankings. We measured pair-
wise agreement among annotators using Cohen?s
kappa coefficient (?) (Cohen, 1960). If P (A) be
the proportion of times that the annotators agree,
and P (E) is the proportion of time that they would
17
LANGUAGE PAIR Systems Rankings Average
Czech?English 5 21,130 4,226.0
English?Czech 10 55,900 5,590.0
German?English 13 25,260 1,943.0
English?German 18 54,660 3,036.6
French?English 8 26,090 3,261.2
English?French 13 33,350 2,565.3
Russian?English 13 34,460 2,650.7
English?Russian 9 28,960 3,217.7
Hindi?English 9 20,900 2,322.2
English?Hindi 12 28,120 2,343.3
TOTAL WMT 14 110 328,830 2,989.3
WMT13 148 942,840 6,370.5
WMT12 103 101,969 999.6
WMT11 133 63,045 474.0
Table 3: Amount of data collected in the WMT14 manual evaluation. The final three rows report summary information from
the previous two workshops.
agree by chance, then Cohen?s kappa is:
? =
P (A)? P (E)
1? P (E)
Note that ? is basically a normalized version of
P (A), one which takes into account how mean-
ingful it is for annotators to agree with each other
by incorporating P (E). The values for ? range
from 0 to 1, with zero indicating no agreement and
1 perfect agreement.
We calculate P (A) by examining all pairs of
systems which had been judged by two or more
judges, and calculating the proportion of time that
they agreed that A < B, A = B, or A > B. In
other words, P (A) is the empirical, observed rate
at which annotators agree, in the context of pair-
wise comparisons.
As for P (E), it captures the probability that two
annotators would agree randomly. Therefore:
P (E) = P (A<B)
2
+ P (A=B)
2
+ P (A>B)
2
Note that each of the three probabilities in P (E)?s
definition are squared to reflect the fact that we are
considering the chance that two annotators would
agree by chance. Each of these probabilities is
computed empirically, by observing how often an-
notators actually rank two systems as being tied.
Table 4 gives ? values for inter-annotator agree-
ment for WMT11?WMT14 while Table 5 de-
tails intra-annotator agreement scores, including
the division of researchers (WMT13
r
) and MTurk
(WMT13
m
) data. The exact interpretation of the
kappa coefficient is difficult, but according to Lan-
dis and Koch (1977), 0?0.2 is slight, 0.2?0.4 is
fair, 0.4?0.6 is moderate, 0.6?0.8 is substantial,
and 0.8?1.0 is almost perfect. The agreement rates
are more or less in line with prior years: worse for
some tasks, better for others, and on average, the
best since WMT11 (where agreement scores were
likely inflated due to inclusion of reference trans-
lations in the comparisons).
3.3 Models of System Rankings
The collected pairwise rankings are used to pro-
duce a ranking of the systems. Machine transla-
tion evaluation has always been a subject of con-
tention, and no exception to this rule exists for the
WMT manual evaluation. While the precise met-
ric has varied over the years, it has always shared
a common idea of computing the average num-
ber of times each system was judged better than
other systems, and ranking from highest to low-
est. For example, in WMT11 Callison-Burch et al.
(2011), the metric computed the percentage of the
time each system was ranked better than or equal
to other systems, and included comparisons to hu-
man references. In WMT12 Callison-Burch et al.
(2012), comparisons to references were dropped.
In WMT13, rankings were produced over 1,000
bootstrap-resampled sets of the training data. A
rank range was collected for each system across
these folds; the average value was used to order
the systems, and a 95% confidence interval across
these ranks was used to organize the systems into
equivalence classes containing systems with over-
18
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.400 0.311 0.244 0.342 0.279 0.305
English?Czech 0.460 0.359 0.168 0.408 0.075 0.360
German?English 0.324 0.385 0.299 0.443 0.324 0.368
English?German 0.378 0.356 0.267 0.457 0.239 0.427
French?English 0.402 0.272 0.275 0.405 0.321 0.357
English?French 0.406 0.296 0.231 0.434 0.237 0.302
Hindi?English ? ? ? ? ? 0.400
English?Hindi ? ? ? ? ? 0.413
Russian?English ? ? 0.278 0.315 0.324 0.324
English?Russian ? ? 0.243 0.416 0.207 0.418
MEAN 0.395 0.330 0.260 0.367
Table 4: ? scores measuring inter-annotator agreement. See Table 5 for corresponding intra-annotator agreement scores.
LANGUAGE PAIR WMT11 WMT12 WMT13 WMT13
r
WMT13
m
WMT14
Czech?English 0.597 0.454 0.479 0.483 0.478 0.382
English?Czech 0.601 0.390 0.290 0.547 0.242 0.448
German?English 0.576 0.392 0.535 0.643 0.515 0.344
English?German 0.528 0.433 0.498 0.649 0.452 0.576
French?English 0.673 0.360 0.578 0.585 0.565 0.629
English?French 0.524 0.414 0.495 0.630 0.486 0.507
Hindi?English ? ? ? ? ? 0.605
English?Hindi ? ? ? ? ? 0.535
Russian?English ? ? 0.450 0.363 0.477 0.629
English?Russian ? ? 0.513 0.582 0.500 0.570
MEAN 0.583 0.407 0.479 0.522
Table 5: ? scores measuring intra-annotator agreement, i.e., self-consistency of judges, across for the past few years of the
human evaluation.
lapping ranges.
This year, we introduce two new changes. First,
we pit the WMT13 method against two new ap-
proaches: that of Hopkins and May (2013, ?3.3.2),
and another based on TrueSkill (Sakaguchi et al.,
2014, ?3.3.3). Second, we compare these two
methods against WMT13?s ?Expected Wins? ap-
proach, and then select among them by determin-
ing which of them has the highest accuracy in
terms of predicting annotations on a held-out set
of pairwise judgments.
3.3.1 Method 1: Expected Wins (EW)
Introduced for WMT13, the EXPECTED WINS has
an intuitive score demonstrated to be accurate in
ranking systems according to an underlying model
of ?relative ability? (Koehn, 2012a). The idea is
to gauge the probability that a system S
i
will be
ranked better than another system randomly cho-
sen from a pool of opponents {S
j
: j 6= i}. If
we define the function win(A,B) as the number
of times system A is ranked better than system B,
then we can define this as follows:
score
EW
(S
i
) =
1
|{S
j
}|
?
j,j 6=i
win(S
i
, S
j
)
win(S
i
, S
j
) + win(S
j
, S
i
)
Note that this score ignores ties.
3.3.2 Method 2: Hopkins and May (HM)
Hopkins and May (2013) introduced a graphical
model formulation of the task, which makes the
notion of underlying system ability even more ex-
plicit. Each system S
J
in the pool {S
j
} is repre-
sented by an associated relative ability ?
j
and a
variance ?
2
a
(fixed across all systems) which serve
as the parameters of a Gaussian distribution. Sam-
ples from this distribution represent the quality
of sentence translations, with higher quality sam-
ples having higher values. Pairwise annotations
(S
1
, S
2
, pi) are generated according to the follow-
ing process:
19
1. Select two systems S
1
and S
2
from the pool
of systems {S
j
}
2. Draw two ?translations?, adding random
Gaussian noise with variance ?
2
obs
to simulate
the subjectivity of the task and the differences
among annotators:
q
1
? N (?
S
1
, ?
2
a
) +N (0, ?
2
obs
)
q
2
? N (?
S
2
, ?
2
a
) +N (0, ?
2
obs
)
3. Let d be a nonzero real number that defines
a fixed decision radius. Produce a rating pi
according to:
pi =
?
?
?
< q
1
? q
2
> d
> q
2
? q
1
> d
= otherwise
Hopkins and May use Gibbs sampling to infer
the set of system means from an annotated dataset.
Details of this inference procedure can be found in
Sakaguchi et al. (2014). The score used to produce
the rankings is simply the system mean associated
with each system:
score
HM
(S
i
) = ?
S
i
3.3.3 Method 3: TrueSkill (TS)
TrueSkill is an adaptive, online system that em-
ploys a similar model of relative ability Herbrich
et al. (2006). It was initially developed for Xbox
Live?s online player community, where it is used
to model player ability, assign levels, and select
competitive matches. Each player S
j
is modeled
by two parameters: TrueSkill?s current estimate
of each system?s relative ability, ?
S
j
, and a per-
system measure of TrueSkill?s uncertainty of those
estimates, ?
2
S
j
. When the outcome of a match is
observed, TrueSkill uses the relative status of the
two systems to update these estimates. If a trans-
lation from a system with a high mean is judged
better than a system with a greatly lower mean, the
result is not surprising, and the update size for the
corresponding system means will be small. On the
other hand, when an upset occurs in a competition,
the means will receive larger updates. Sakaguchi
et al. (2014) provide an adaptation of this approach
to the WMT manual evaluation, and showed that
it performed well on WMT13 data.
Similar to the Hopkins and May model,
TrueSkill scores systems by their inferred means:
score
TS
(S
i
) = ?
S
i
This score is then used to sort the systems and pro-
duce the ranking.
3.4 Method Selection
We have three methods which, provided with the
collected data, produce different rankings of the
systems. Which of them is correct? More imme-
diately, which one of them should we publish as
the official ranking for the WMT14 manual eval-
uation? As discussed, the method used to com-
pute the ranking has been tweaked a bit each year
over the past few years in response to criticisms
(e.g., Lopez (2012); Bojar et al. (2011)). While the
changes were reasonable (and later corroborated),
Hopkins and May (2013) pointed out that this task
of model selection should be driven by empirical
evaluation on held-out data, and suggested per-
plexity as the metric of choice.
We choose instead a more direct gold-standard
evaluation metric: the accuracy of the rankings
produced by each method in predicting pairwise
judgments. We use each method to produce a par-
tial ordering of the systems, grouping them into
equivalence classes. This partial ordering unam-
biguously assigns a prediction pi
P
between any
pair of systems (S
i
, S
j
). By comparing the pre-
dicted relationship pi
P
to the actual annotation for
each pairwise judgment in the test data (by token),
we can compute an accuracy score for each model.
We predict accuracy in this manner using 100-
fold cross-validation. For each task, we split the
data into a fixed set of 100 randomly-selected
folds. Each fold serves as a test set, with the
remaining ninety-nine folds available as training
data for each method. Note that the total order-
ing over systems provided by the score
?
functions
defined do not predict ties. In order to do enable
the models to predict ties, we produce equivalence
classes using the following procedure:
? Assign S
1
to a cluster
? For each system S
i
, assign it to the current
cluster if score(S
i?1
) ? score(S
i
) ? r; oth-
erwise, assign it to a new cluster
The value of r (the decision radius for ties)
is tuned using accuracy on the entire training
data using grid search over the values r ?
0, 0.01, 0.02, . . . , .25 (26 values in total). This
value is tuned separately for each method on each
fold. Table 6 contains an example partial ordering.
20
System Score Rank
B 0.60 1
D 0.44 2
E 0.39 2
A 0.25 2
F -0.09 3
C -0.22 3
Table 6: The partial ordering computed with the provided
scores when r = 0.15.
Task EW HM TS Oracle
Czech?English 40.4 41.1 41.1 41.2
English?Czech 45.3 45.6 45.9 46.8
French?English 49.0 49.4 49.3 50.3
English?French 44.6 44.4 44.7 46.0
German?English 43.5 43.7 43.7 45.2
English?German 47.3 47.4 47.2 48.2
Hindi?English 62.5 62.2 62.5 62.6
English?Hindi 53.3 53.7 53.5 55.7
Russian?English 47.6 47.7 47.7 50.6
English?Russian 46.5 46.1 46.4 48.2
MEAN 48.0 48.1 48.2 49.2
Table 7: Accuracies for each method across 100 folds, for
each translation task. The oracle uses the most frequent out-
come between each pair of systems, and therefore might not
constitute a feasible ranking.
After training, each model has defined a partial
ordering over systems.
6
This is then used to com-
pute accuracy on all the pairwise judgments in the
test fold. This process yields 100 accuracies for
each method; the average accuracy across all the
folds can then be used to compute the best method.
Table 7 contains accuracy results for the three
methods on the WMT14 tasks. On average, there
is a small improvement in accuracy moving from
Expected Wins to the H&M model, and then again
to the TrueSkill model; however, there is no pat-
tern to the best model for each class. The Oracle
column is computed by selecting the most prob-
able outcome (pi ? {<,=, >}) for each system
pair, and provides an upper bound on accuracy
when predicting outcomes using only system-level
information. Furthermore, this method of oracle
computation might not represent a feasible rank-
ing or clustering,
7
.
The TrueSkill approach was best overall, so we
used it to produce the official rankings for all lan-
6
It is a total ordering when r = 0, or when all the system
scores are outside the decision radius.
7
For example, if there were a cycle of ?better than? judg-
ments among a set of systems.
guage pairs.
3.5 Rank Ranges and Clusters
Above we saw how to produce system scores for
each method, which provides a total ordering of
the systems. But we would also like to know if the
obtained system ranking is statistically significant.
Given the large number of systems that participate,
and the similarity of the underlying systems result-
ing from the common training data condition and
(often) toolsets, there will be some systems that
will be very close in quality. These systems should
be grouped together in equivalence classes.
To establish the reliability of the obtained sys-
tem ranking, we use bootstrap resampling. We
sample from the set of pairwise rankings an equal
sized set of pairwise rankings (allowing for multi-
ple drawings of the same pairwise ranking), com-
pute a TrueSkill model score for each system
based on this sample, and then rank the systems
from 1..|{S
j
}|. By repeating this procedure 1,000
times, we can determine a range of ranks, into
which system falls at least 95% of the time (i.e.,
at least 950 times) ? corresponding to a p-level
of p ? 0.05. Furthermore, given the rank ranges
for each system, we can cluster systems with over-
lapping rank ranges.
8
Table 8 reports all system scores, rank ranges,
and clusters for all language pairs and all systems.
The official interpretation of these results is that
systems in the same cluster are considered tied.
Given the large number of judgments that we col-
lected, it was possible to group on average about
two systems in a cluster, even though the systems
in the middle are typically in larger clusters.
3.6 Cluster analysis
The official ranking results for English-German
produced clusters compute at the 90% confidence
level due to the presence of a very large cluster
(of nine systems). While there is always the pos-
sibility that this cluster reflects a true ambiguity, it
is more likely due to the fact that we didn?t have
enough data: English?German had the most sys-
8
Formally, given ranges defined by start(S
i
) and end(S
i
),
we seek the largest set of clusters {C
c
} that satisfies:
?S ?C : S ? C
S ? C
a
, S ? C
b
? C
a
= C
b
C
a
6= C
b
? ?S
i
? C
a
, S
j
? C
b
:
start(S
i
) > end(S
j
) or start(S
j
) > end(S
i
)
21
Czech?English
# score range system
1 0.591 1 ONLINE-B
2 0.290 2 UEDIN-PHRASE
3 -0.171 3-4 UEDIN-SYNTAX
-0.243 3-4 ONLINE-A
4 -0.468 5 CU-MOSES
English?Czech
# score range system
1 0.371 1-3 CU-DEPFIX
0.356 1-3 UEDIN-UNCNSTR
0.333 1-4 CU-BOJAR
0.287 3-4 CU-FUNKY
2 0.169 5-6 ONLINE-B
0.113 5-6 UEDIN-PHRASE
3 0.030 7 ONLINE-A
4 -0.175 8 CU-TECTO
5 -0.534 9 COMMERCIAL1
6 -0.950 10 COMMERCIAL2
Russian?English
# score range system
1 0.583 1 AFRL-PE
2 0.299 2 ONLINE-B
3 0.190 3-5 ONLINE-A
0.178 3-5 PROMT-HYBRID
0.123 4-7 PROMT-RULE
0.104 5-8 UEDIN-PHRASE
0.069 5-8 YANDEX
0.066 5-8 ONLINE-G
4 -0.017 9 AFRL
5 -0.159 10 UEDIN-SYNTAX
6 -0.306 11 KAZNU
7 -0.487 12 RBMT1
8 -0.642 13 RBMT4
English?Russian
# score range system
1 0.575 1-2 PROMT-RULE
0.547 1-2 ONLINE-B
2 0.426 3 PROMT-HYBRID
3 0.305 4-5 UEDIN-UNCNSTR
0.231 4-5 ONLINE-G
4 0.089 6-7 ONLINE-A
0.031 6-7 UEDIN-PHRASE
5 -0.920 8 RBMT4
6 -1.284 9 RBMT1
German?English
# score range system
1 0.451 1 ONLINE-B
2 0.267 2-3 UEDIN-SYNTAX
0.258 2-3 ONLINE-A
3 0.147 4-6 LIMSI-KIT
0.146 4-6 UEDIN-PHRASE
0.138 4-6 EU-BRIDGE
4 0.026 7-8 KIT
-0.049 7-8 RWTH
5 -0.125 9-11 DCU-ICTCAS
-0.157 9-11 CMU
-0.192 9-11 RBMT4
6 -0.306 12 RBMT1
7 -0.604 13 ONLINE-C
French?English
# score range system
1 0.608 1 UEDIN-PHRASE
2 0.479 2-4 KIT
0.475 2-4 ONLINE-B
0.428 2-4 STANFORD
3 0.331 5 ONLINE-A
4 -0.389 6 RBMT1
5 -0.648 7 RBMT4
6 -1.284 8 ONLINE-C
English?French
# score range system
1 0.327 1 ONLINE-B
2 0.232 2-4 UEDIN-PHRASE
0.194 2-5 KIT
0.185 2-5 MATRAN
0.142 4-6 MATRAN-RULES
0.120 4-6 ONLINE-A
3 0.003 7-9 UU-DOCENT
-0.019 7-10 PROMT-HYBRID
-0.033 7-10 UA
-0.069 8-10 PROMT-RULE
4 -0.215 11 RBMT1
5 -0.328 12 RBMT4
6 -0.540 13 ONLINE-C
English?German
# score range system
1 0.264 1-2 UEDIN-SYNTAX
0.242 1-2 ONLINE-B
2 0.167 3-6 ONLINE-A
0.156 3-6 PROMT-HYBRID
0.155 3-6 PROMT-RULE
0.155 3-6 UEDIN-STANFORD
3 0.094 7 EU-BRIDGE
4 0.033 8-10 RBMT4
0.031 8-10 UEDIN-PHRASE
0.012 8-10 RBMT1
5 -0.032 11-12 KIT
-0.069 11-13 STANFORD-UNC
-0.100 12-14 CIMS
-0.126 13-15 STANFORD
-0.158 14-16 UU
-0.191 15-16 ONLINE-C
6 -0.307 17-18 IMS-TTT
-0.325 17-18 UU-DOCENT
Hindi?English
# score range system
1 1.326 1 ONLINE-B
2 0.559 2-3 ONLINE-A
0.476 2-4 UEDIN-SYNTAX
0.434 3-4 CMU
3 0.323 5 UEDIN-PHRASE
4 -0.198 6-7 AFRL
-0.280 6-7 IIT-BOMBAY
5 -0.549 8 DCU-LINGO24
6 -2.092 9 IIIT-HYDERABAD
English?Hindi
# score range system
1 1.008 1 ONLINE-B
2 0.915 2 ONLINE-A
3 0.214 3 UEDIN-UNCNSTR
4 0.120 4-5 UEDIN-PHRASE
0.054 4-5 CU-MOSES
5 -0.111 6-7 IIT-BOMBAY
-0.142 6-7 IPN-UPV-CNTXT
6 -0.233 8-9 DCU-LINGO24
-0.261 8-9 IPN-UPV-NODEV
7 -0.449 10-11 MANAWI-H1
-0.494 10-11 MANAWI
8 -0.622 12 MANAWI-RMOOV
Table 8: Official results for the WMT14 translation task. Systems are ordered by their inferred system means. Lines between
systems indicate clusters according to bootstrap resampling at p-level p ? .05, except for English?German, where p ? 0.1.
This method is also used to determine the range of ranks into which system falls. Systems with grey background indicate use
of resources that fall outside the constraints provided for the shared task.
22
tems (18, compared to 13 for the next languages),
yet only an average amount of per-system data.
Here, we look at this language pair in more detail,
in order to justify this decision, and to shed light
on the differences between the ranking methods.
Table 9 presents the 95% confidence-level clus-
terings for English?German computed with each
of the three methods, along with lines that show
the reorderings of the systems between them. Re-
orderings of this type have been used to argue
against the reliability of the official WMT rank-
ing (Lopez, 2012; Hopkins and May, 2013). This
table shows that these reorderings are captured en-
tirely by the clustering approach we used. This rel-
ative consensus of these independently-computed
and somewhat different models suggests that the
published ranking is approaching the true ambigu-
ity underlying systems within the same cluster.
Looking across all language pairs, we find that
the total ordering predicted by EW and TS is ex-
actly the same for eight of the ten language pair
tasks, and is constrained to reorderings within
the official cluster for the other two (German?
English ? just one adjacent swap ? and English?
German, depicted in Table 9).
3.7 Conclusions
The official ranking method employed by WMT
over the past few years has changed a few times as
a result of error analysis and introspection. Until
this year, these results were largely based on the
intuitions of the community and organizers about
deficiencies in the models. In addition to their in-
tuitive appeal, many of these changes (such as the
decision to throw out comparisons against refer-
ences) have been empirically validated Hopkins
and May (2013). The actual effect of the refine-
ments in the ranking metric has been minor pertur-
bations in the permutation of systems. The cluster-
ing method of Koehn (2012b), in which the official
rankings are presented as a partial (instead of to-
tal) ordering, alleviated many of the problems ob-
served by Lopez (2012), and also capture all the
variance across the new systems introduced this
year. In addition, presenting systems as clusters
appeals to intuition. As such, we disagree with
claims that there is a problem with irreproducibil-
ity of the results of the workshop evaluation task,
and especially disagree that there is anything ap-
proaching a ?crisis of confidence? (Hopkins and
May, 2013). These claims seem to us to be over-
stated.
Conducting proper model selection by compar-
ison on held-out data, however, is a welcome sug-
gestion, and our inclusion of this process supports
improved confidence in the ranking results. That
said, it is notable that the different methods com-
pute very similar orderings. This avoids hallu-
cinating distinctions among systems that are not
really there, and captures the intuition that some
systems are basically equivalent. The chief ben-
efit of the TrueSkill model is not in outputting a
better complete ranking of the systems, but lies in
its reduced variance, which allow us to cluster the
systems with less data. There is also the unex-
plored avenue of using TrueSkill to drive the data
collection, steering the annotations of judges to-
wards evenly matched systems during the collec-
tion phase, potentially allowing confident results
to be presented while collecting even less data.
There is, of course, more work to be done.
We have produced this year statistically significant
clusters with a third of the data required last year,
which is an improvement. Models of relative abil-
ity are a natural fit for the manual evaluation, and
the introduction of an online Bayesian approach
to data collection present further opportunities to
reduce the amount of data needed. These methods
also provide a framework for extending the models
in a variety of potentially useful ways, including
modeling annotator bias, incorporating sentence
metadata (such as length, difficulty, or subtopic),
and adding features of the sentence pairs.
4 Quality Estimation Task
Machine translation quality estimation is the task
of predicting a quality score for a machine trans-
lated text without access to reference translations.
The most common approach is to treat the problem
as a supervised machine learning task, using stan-
dard regression or classification algorithms. The
third edition of the WMT shared task on qual-
ity estimation builds on the previous editions of
the task (Callison-Burch et al., 2012; Bojar et al.,
2013), with tasks including both sentence-level
and word-level estimation, with new training and
test datasets.
The goals of this year?s shared task were:
? To investigate the effectiveness of different
quality labels.
? To explore word-level quality prediction at
23
Expected Wins Hopkins & May TrueSkill
UEDIN-SYNTAX UEDIN-SYNTAX UEDIN-SYNTAX
ONLINE-B ONLINE-B ONLINE-B
ONLINE-A UEDIN-STANFORD ONLINE-A
UEDIN-STANFORD PROMT-HYBRID PROMT-HYBRID
PROMT-RULE ONLINE-A PROMT-RULE
PROMT-HYBRID PROMT-RULE UEDIN-STANFORD
EU-BRIDGE EU-BRIDGE EU-BRIDGE
RBMT4 UEDIN-PHRASE RBMT4
UEDIN-PHRASE RBMT4 UEDIN-PHRASE
RBMT1 RBMT1 RBMT1
KIT KIT KIT
STANFORD-UNC STANFORD-UNC STANFORD-UNC
CIMS CIMS CIMS
STANFORD STANFORD STANFORD
UU UU UU
ONLINE-C ONLINE-C ONLINE-C
IMS-TTT UU-DOCENT IMS-TTT
UU-DOCENT IMS-TTT UU-DOCENT
Table 9: A comparison of the rankings produced by Expected Wins, Hopkins & May, and TrueSkill for English?German (the
task with the most systems and the largest cluster). The lines extending all the way across mark the official English?German
clustering (computed from TrueSkill with 90% confidence intervals), while bold entries mark the start of new clusters within
each method or column (computed at the 95% confidence level). The TrueSkill clusterings contain all the system reorderings
across the other two ranking methods.
different levels of granularity.
? To study the effects of training and test
datasets with mixed domains, language pairs
and MT systems.
? To examine the effectiveness of quality pre-
diction methods on human translations.
Four tasks were proposed: Tasks 1.1, 1.2, 1.3
are defined at the sentence-level (Sections 4.1),
while Task 2, at the word-level (Section 4.2). Each
task provides one or more datasets with up to four
language pairs each: English-Spanish, English-
German, German-English, Spanish-English, and
up to four alternative translations generated by:
a statistical MT system (SMT), a rule-based MT
system (RBMT), a hybrid MT system, and a hu-
man. These datasets were annotated with differ-
ent labels for quality by professional translators as
part of the QTLaunchPad
9
project. External re-
sources (e.g. parallel corpora) were provided to
participants. Any additional resources, including
additional quality estimation training data, could
9
http://www.qt21.eu/launchpad/
be used by participants (no distinction between
open and close tracks is made). Participants were
also provided with a software package to extract
quality estimation features and perform model
learning, with a suggested list of baseline features
and learning method for sentence-level prediction.
Participants, described in Section 4.3, could sub-
mit up to two systems for each task.
Data used for building specific MT systems or
internal system information (such as n-best lists)
were not made available this year as multiple MT
systems were used to produced the datasets, in-
cluding rule-based systems. In addition, part of
the translations were produced by humans. Infor-
mation on the sources of translations was not pro-
vided either. Therefore, as a general rule, partici-
pants were only allowed to use black-box features.
4.1 Sentence-level Quality Estimation
For the sentence-level tasks, two variants of the
results could be submitted for each task and lan-
guage pair:
? Scoring: An absolute quality score for each
sentence translation according to the type of
24
prediction, to be interpreted as an error met-
ric: lower scores mean better translations.
? Ranking: A ranking of sentence translations
for all source test sentences from best to
worst. For this variant, it does not matter how
the ranking is produced (from HTER predic-
tions, likert predictions, or even without ma-
chine learning).
Evaluation was performed against the true label
and/or HTER ranking using the same metrics as in
previous years:
? Scoring: Mean Average Error (MAE) (pri-
mary metric), Root Mean Squared Error
(RMSE).
? Ranking: DeltaAvg (primary metric) (Bojar
et al., 2013) and Spearman?s rank correlation.
For all sentence-level these tasks, the same 17
features as in WMT12-13 were used to build base-
line systems. The SVM regression algorithm
within QUEST (Specia et al., 2013)
10
was applied
for that with RBF kernel and grid search for pa-
rameter optimisation.
Task 1.1 Predicting post-editing effort
Data in this task is labelled with discrete and
absolute scores for perceived post-editing effort,
where:
? 1 = Perfect translation, no post-editing
needed at all.
? 2 = Near miss translation: translation con-
tains maximum of 2-3 errors, and possibly
additional errors that can be easily fixed (cap-
italisation, punctuation, etc.).
? 3 = Very low quality translation, cannot be
easily fixed.
The datasets were annotated in a ?triage? phase
aimed at selecting translations of type ?2? (near
miss) that could be annotated for errors at the
word-level using the MQM metric (see Task 2, be-
low) for a more fine-grained and systematic trans-
lation quality analysis. Word-level errors in trans-
lations of type ?3? are too difficult if not impos-
sible to annotate and classify, particularly as they
often contain inter-related errors in contiguous or
overlapping word spans.
10
http://www.quest.dcs.shef.ac.uk/
For the training of prediction models, we pro-
vide a new dataset consisting of source sen-
tences and their human translations, as well as
two-three versions of machine translations (by an
SMT system, an RBMT system and, for English-
Spanish/German only, a hybrid system), all in the
news domain, extracted from tests sets of various
WMT years and MT systems that participated in
the translation shared task:
# Source sentences # Target sentences
954 English 3,816 Spanish
350 English 1,400 German
350 German 1,050 English
350 Spanish 1,050 English
As test data, for each language pair and MT sys-
tem (or human translation) we provide a new set
of translations produced by the same MT systems
(and humans) as those used for the training data:
# Source sentences # Target sentences
150 English 600 Spanish
150 English 600 German
150 German 450 English
150 Spanish 450 English
The distribution of true scores in both training
and test sets for each language pair is given in Fig-
ures 3.
0%#
10%#
20%#
30%#
40%#
50%#
60%#
{en-
de-1
}#
{en-
de-2
}#
{en-
de-3
}#
{de-
en-1
}#
{de-
en-2
}#
{de-
en-3
}#
{en-
es-1
}#
{en-
es-2
}##
{en-
es-3
}##
{es-
en-1
}#
{es-
en-2
}#
{es-
en-3
}#
#Training##### #Test####
Figure 3: Distribution of true 1-3 scores by langauge pair.
Additionally, we provide some out of domain
test data. These translations were annotated in
the same way as above, each dataset by one Lan-
guage Service Provider (LSP), i.e, one profes-
sional translator, with two LPSs producing data in-
dependently for English-Spanish. They were gen-
erated using the LSPs? own source data (a different
domain from news), and own MT system (differ-
ent from the three used for the official datasets).
The results on these datasets were not considered
25
for the official ranking of the participating sys-
tems:
# Source sentences # Target sentences
971 English 971 Spanish
297 English 297 German
388 Spanish 388 English
Task 1.2 Predicting percentage of edits
In this task we use HTER (Snover et al., 2006) as
quality score. This score is to be interpreted as
the minimum edit distance between the machine
translation and its manually post-edited version,
and its range is [0, 1] (0 when no edit needs to
be made, and 1 when all words need to be edited).
We used TERp (default settings: tokenised, case
insensitive, etc., but capped to 1)
11
to compute the
HTER scores.
For practical reasons, the data is a subset of
Task 1.1?s dataset: only translations produced
by the SMT system English-Spanish. As train-
ing data, we provide 896 English-Spanish trans-
lation suggestions and their post-editions. As
test data, we provide a new set of 208 English-
Spanish translations produced by the same SMT
system. Each of the training and test translations
was post-edited by a professional translator using
the CASMACAT
12
web-based tool, which also col-
lects post-editing time on a sentence-basis.
Task 1.3 Predicting post-editing time
For this task systems are required to produce, for
each translation, a real valued estimate of the time
(in milliseconds) it takes a translator to post-edit
the translation. The training and test sets are a sub-
set of that uses in Task 1.2 (subject to filtering of
outliers). The difference is that the labels are now
the number of milliseconds that were necessary to
post-edit each translation.
As training data, we provide 650 English-
Spanish translation suggestions and their post-
editions. As test data, we provide a new set of 208
English-Spanish translations (same test data as for
Task 1.2).
4.2 Word-level Quality Estimation
The data for this task is based on a subset of the
datasets used for Task 1.1, for all language pairs,
11
http://www.umiacs.umd.edu/
?
snover/terp/
12
http://casmacat.eu/
human and machine translations: those transla-
tions labelled ?2? (near misses), plus additional
data provided by industry (either on the news do-
main or on other domains, such as technical doc-
umentation, produced using their own MT sys-
tems, and also pre-labelled as ?2?). All seg-
ments were annotated with word-level labels by
professional translators using the core categories
in MQM (Multidimensional Quality Metrics)
13
as
error typology (see Figure 4). Each word or se-
quence of words was annotated with a single error.
For (supposedly rare) cases where a decision be-
tween multiple fine-grained error types could not
be made, annotators were requested to choose a
coarser error category in the hierarchy.
Participants are asked to produce a label for
each token that indicates quality at different lev-
els of granularity:
? Binary classification: an OK / bad label,
where bad indicates the need for editing the
token.
? Level 1 classification: an OK / accuracy /
fluency label, specifying coarser level cate-
gories of errors for each token, or ?OK? for
tokens with no error.
? Multi-class classification: one of the labels
specifying the error type for the token (termi-
nology, mistranslation, missing word, etc.) in
Figure 4, or ?OK? for tokens with no error.
As training data, we provide tokenised transla-
tion output for all language pairs, human and ma-
chine translations, with tokens annotated with all
issue types listed above, or ?OK?. The annotation
was performed manually by professional transla-
tors as part of the QTLaunchPad project. For
the coarser variants, fine-grained errors are gen-
eralised to Accuracy or Fluency, or ?bad? for the
binary variant. The amount of available training
data varies by language pair:
# Source sentences # Target sentences
1,957 English 1,957 Spanish
715 English 715 German
350 German 350 English
900 Spanish 900 English
13
http://www.qt21.eu/launchpad/content/
training
26
Figure 4: MQM metric as error typology.
As test data, we provide additional data points
for all language pairs, human and machine trans-
lations:
# Source sentences # Target sentences
382 English 382 Spanish
150 English 150 German
100 German 100 English
150 Spanish 150 English
In contrast to Tasks 1.1?1.3, no baseline feature
set is provided to the participants.
Similar to last year (Bojar et al., 2013), the
word-level task is primarily evaluated by macro-
averaged F-measure (in %). Because the class dis-
tribution is skewed ? in the test data about 78% of
the tokens are marked as ?OK? ? we compute pre-
cision, recall, and F
1
for each class individually,
weighting F
1
scores by the frequency of the class
in the test data. This avoids giving undue impor-
tance to less frequent classes. Consider the follow-
ing confusion matrix for Level 1 annotation, i.e.
the three classes (O)K, (F)luency, and (A)ccuracy:
reference
O F A
predicted
O 4172 1482 193
F 1819 1333 214
A 198 133 69
For each of the three classes we assume a binary
setting (one-vs-all) and derive true-positive (tp),
false-positive (fp), and false-negative (fn) counts
from the rows and columns of the confusion ma-
trix as follows:
tp
O
= 4172
fp
O
= 1482 + 193 = 1675
fn
O
= 1819 + 198 = 2017
tp
F
= 1333
fp
F
= 1819 + 214 = 2033
fn
F
= 1482 + 133 = 1615
tp
A
= 69
fp
A
= 198 + 133 = 331
fn
A
= 193 + 214 = 407
We continue to compute F
1
scores for each
class c ? {O,F,A}:
precision
c
= tp
c
/(tp
c
+ fp
c
)
recall
c
= tp
c
/(tp
c
+ fn
c
)
F
1,c
=
2 ? precision
c
? recall
c
precision
c
+recall
c
yielding:
precision
O
= 4172/(4172 + 1675) = 0.7135
recall
O
= 4172/(4172 + 2017) = 0.6741
F
1,O
=
2 ? 0.7135 ? 0.6741
0.7135 + 0.6741
= 0.6932
? ? ?
F
1,F
= 0.4222
F
1,A
= 0.1575
Finally, we compute the average of F
1,c
scores
weighted by the occurrence count N(c) of c:
weightedF
1,ALL
=
1
?
c
N(c)
?
c
N
c
? F
1,c
weightedF
1,ERR
=
1
?
c:c 6=O
N(c)
?
c:c 6=O
N
c
? F
1,c
27
which for the above example gives:
weightedF
1,ALL
=
1
6189 + 2948 + 476
?
(6189 ? 0.6932 + 2948 ? 0.4222
+476 ? 0.1575) = 0.5836
weightedF
1,ERR
=
1
2948 + 476
?
(2948 ? 0.4222 + 476 ? 0.1575)
= 0.3854
We choose F
1,ERR
as our primary evaluation mea-
sure because it most closely mimics the common
application of F
1
scores in binary classification:
one is interested in the performance in detecting a
positive class, which in this case would be erro-
neous words. This does, however, ignore the num-
ber of correctly classified words of the OK class,
which is why we also report F
1,ALL
. In addition,
we follow Powers (2011) and report Matthews
Correlation Coefficient (MCC), averaged in the
same way as F
1
, as our secondary metric. Finally,
for contrast we also report Accuracy (ACC).
4.3 Participants
Table 10 lists all participating teams. Each team
was allowed up to two submissions for each task
and language pair. In the descriptions below, par-
ticipation in specific tasks is denoted by a task
identifier: T1.1, T1.2, T1.3, and T2.
Sentence-level baseline system (T1.1, T1.2,
T1.3): QUEST is used to extract 17 system-
independent features from source and trans-
lation sentences and parallel corpora (same
features as in the WMT12 shared task):
? number of tokens in the source and tar-
get sentences.
? average source token length.
? average number of occurrences of the
target word within the target sentence.
? number of punctuation marks in source
and target sentences.
? language model (LM) probability of
source and target sentences based on
models for the WMT News Commen-
tary corpus.
? average number of translations per
source word in the sentence as given by
IBM Model 1 extracted from the WMT
News Commentary parallel corpus, and
thresholded so that P (t|s) > 0.2, or
so that P (t|s) > 0.01 weighted by the
inverse frequency of each word in the
source side of the parallel corpus.
? percentage of unigrams, bigrams and tri-
grams in frequency quartiles 1 (lower
frequency words) and 4 (higher fre-
quency words) in the source language
extracted from the WMT News Com-
mentary corpus.
? percentage of unigrams in the source
sentence seen in the source side of the
WMT News Commentary corpus.
These features are used to train a Support
Vector Machine (SVM) regression algorithm
using a radial basis function kernel within
the SCIKIT-LEARN toolkit. The ?,  and C
parameters were optimised via grid search
with 5-fold cross validation on the training
set. We note that although the system is re-
ferred to as ?baseline?, it is in fact a strong
system. It has proved robust across a range
of language pairs, MT systems, and text do-
mains for predicting various forms of post-
editing effort (Callison-Burch et al., 2012;
Bojar et al., 2013).
DCU (T1.1): DCU-MIXED and DCU-SVR use
a selection of features available in QUEST,
such as punctuation statistics, LM perplex-
ity, n-gram frequency quartile statistics and
coarse-grained POS frequency ratios, and
four additional feature types: combined POS
and stop word LM features, source-side
pseudo-reference features, inverse glass-box
features for translating the translation and er-
ror grammar parsing features. For machine
learning, the QUEST framework is expanded
to combine logistic regression and support
vector regression and to handle cross- valida-
tion and randomisation in a way that training
items with the same source side are kept to-
gether. External resources are monolingual
corpora taken from the WMT 2014 transla-
tion task for LMs, the MT system used for the
inverse glass-box features (Li et al., 2014b)
and, for error grammar parsing, the Penn-
Treebank and an error grammar derived from
it (Foster, 2007).
28
ID Participating team
DCU Dublin City University Team 1, Ireland (Hokamp et al., 2014)
DFKI German Research Centre for Artificial Intelligence, Germany (Avramidis,
2014)
FBK-UPV-UEDIN Fondazione Bruno Kessler, Italy, UPV Universitat Polit`ecnica de Val`encia,
Spain & University of Edinburgh, UK (Camargo de Souza et al., 2014)
LIG Laboratoire d?Informatique Grenoble, France (Luong et al., 2014)
LIMSI Laboratoire d?Informatique pour la M?ecanique et les Sciences de l?Ing?enieur,
France (Wisniewski et al., 2014)
MULTILIZER Multilizer, Finland
RTM-DCU Dublin City University Team 2, Ireland (Bicici and Way, 2014)
SHEF-lite University of Sheffield Team 1, UK (Beck et al., 2014)
USHEFF University of Sheffield Team 2, UK (Scarton and Specia, 2014)
YANDEX Yandex, Russia
Table 10: Participants in the WMT14 Quality Estimation shared task.
DFKI (T1.2): DFKI/SVR builds upon the base-
line system (above) by adding non-redundant
data from the WMT13 task for predicting
the same label (HTER) and additional fea-
tures such as (a) rule-based language cor-
rections (language tool) (b), PCFG parsing
statistics and counts of tree labels, (c) po-
sition statistics of parsing labels, (d) posi-
tion statistics of trigrams with low probabil-
ity. DFKI/SVRxdata uses a similar setting,
with the addition of more training data from
non-minimally post-edited translation out-
puts (references), filtered based on a thresh-
old on the edit distance between the MT out-
put and the freely-translated reference.
FBK-UPV-UEDIN (T1.2, T1.3, T2): The sub-
missions for the word-level task (T2) use fea-
tures extracted from word posterior probabil-
ities and confusion network descriptors com-
puted over the 100k-best hypothesis transla-
tions generated by a phrase-based SMT sys-
tem. They also use features from word lexi-
cons, and POS tags of each word for source
and translation sentences. The predictions of
the Binary model are used as a feature for the
Level 1 and Multi-class settings. Both condi-
tional random fields (CRF) and bidirectional
long short-term memory recurrent neural net-
works (BLSTM-RNNs) are used for the Bi-
nary setting, and BLSTM-RNNs only for the
Level 1 and Multi-class settings.
The sentence-level QE submissions (T1.2
and T1.3) are trained on black-box features
extracted using QUEST in addition to fea-
tures based on word alignments, word poste-
rior probabilities and diversity scores (Souza
et al., 2013). These features are computed
over 100k-best hypothesis translations also
used for task 2. In addition, a set of ratios
computed from the word-level predictions of
the model trained on the binary setting of
task 2 is used. A total of 221 features and
the extremely randomised trees (Geurts et al.,
2006) learning algorithm are used to train re-
gression models.
LIG (T2): Conditional Random Fields classi-
fiers are trained with features used in LIG?s
WMT13 systems (Luong et al., 2013): tar-
get and source words, alignment informa-
tion, source and target alignment context,
LM scores, target and source POS tags,
lexical categorisations (stopword, punctua-
tion, proper name, numerical), constituent
label, depth in the constituent tree, target
polysemy count, pseudo reference. These
are combined with novel features: word
occurrence in multiple translation systems
and POS tag-based LM scores (longest tar-
get/source n-gram length and backoff score
for POS tag). These features require external
NLP tools and resources such as: TreeTag-
ger, GIZA++, Bekerley parser, Link Gram-
mar parser, WordNet and BabelNet, Google
Translate (pseudo-reference). For the binary
task, the optimal classification threshold is
tuned based on a development set split from
the original training set. Feature selection is
employed over the all features (for the binary
29
task only), with the Sequential Backward Se-
lection algorithm. The best performing fea-
ture set is then also used for the Level 1 and
Multi-class variants.
LIMSI (T2): The submission relies on a ran-
dom forest classifier and considers only 16
dense and continuous features. To prevent
sparsity issues, lexicalised information such
as the word or the previous word identities
is not included. The features considered are
mostly classic MT features and can be cat-
egorised into two classes: association fea-
tures, which describe the quality of the as-
sociation between the source sentence and
each target word, and fluency features, which
describe the ?quality? of the translation hy-
potheses. The latter rely on different lan-
guage models (either on POS or on words)
and the former on IBM Model 1 translation
probabilities and on pseudo- references, i.e.
translation produced by an independent MT
system. Random forests are known to per-
form well in tasks like this one, in which
only a few dense and continuous features are
available, possibly because of their ability to
take into account complex interactions be-
tween features and to automatically partition
the continuous feature values into a discrete
set of intervals that achieves the best classifi-
cation performance. Since they predict the
class probabilities, it is possible to directly
optimize the F
1
score during training by find-
ing, with a grid search method, the decision
threshold that achieved the best F
1
score on
the training set.
MULTILIZER (T1.2, T1.3): The 80 black-box
features from QUEST are used in addition to
new features based on using other MT en-
gines for forward and backward translations.
In forward translations, the idea is that dif-
ferent MT engines make different mistakes.
Therefore, when several forward translations
are similar to each other, these translations
are more likely to be correct. This is con-
firmed by the Pearson correlation of similar-
ities between the forward translations against
the true scores (above 0.5). A backward
translation is very error-prone and therefore
it has to be used in combination with for-
ward translations. A single back-translation
similar to original source segment does not
bring much information. Instead, when sev-
eral MT engines give back-translations simi-
lar to this source segment, one can conclude
that the translation is reliable. Those transla-
tions where similarities both in forward trans-
lation and backward translation are high are
intuitively more likely to be good. A simple
feature selection method that omits all fea-
tures with Pearson correlation against the true
scores below 0.2 is used. The systems sub-
mitted are obtained using linear regression
models.
RTM-DCU (T1.1, T1.2, T1.3, T2): RTM-DCU
systems are based on referential translation
machines (RTM) (Bic?ici, 2013) and parallel
feature decay algorithms (ParFDA5) (Bic?ici
et al., 2014), which allow language and MT
system-independent predictions. For each
task, individual RTM models are developed
using the parallel corpora and the language
model corpora distributed by the WMT14
translation task and the language model cor-
pora provided by LDC for English and Span-
ish. RTMs use 337 to 437 sentence-level fea-
tures for coverage and diversity, IBM1 and
sentence translation performance, retrieval
closeness and minimum Bayes retrieval risk,
distributional similarity and entropy, IBM2
alignment, character n-grams, sentence read-
ability, and parse output tree structures. The
features use ngrams defined over text or com-
mon cover link (CCL) (Seginer, 2007) struc-
tures as the basic units of information over
which similarity calculations are performed.
Learning models include ridge regression
(RR), support vector machines (SVR), and
regression trees (TREE), which are applied
after partial least squares (PLS) or feature
selection (FS). For word-level prediction,
generalised linear models (GLM) (Collins,
2002) and GLM with dynamic learning
(GLMd) (Bic?ici, 2013) are used with word-
level features including CCL links, word
length, location, prefix, suffix, form, context,
and alignment, totalling up to a couple of mil-
lion features.
SHEF-lite (T1.1, T1.2, T1.3): These submis-
sions use the framework of Multi-task Gaus-
sian Processes, where multiple datasets are
30
combined in a multi-task setting similar to
the one used by Cohn and Specia (2013).
For T1.1, data for all language pairs is put
together, and each language is considered a
task. For T1.2 and T1.3, additional datasets
from previous shared task years are used,
each encoded as a different task. For all tasks,
the QUEST framework is used to extract a set
of 80 black-box features (a superset of the 17
baseline features). To cope with the large size
of the datasets, the SHEF-lite-sparse submis-
sion uses Sparse Gaussian Processes, which
provide sensible sparse approximations using
only a subset of instances (inducing inputs)
to speed up training and prediction. For this
?sparse? submission, feature selection is per-
formed following the approach of Shah et al.
(2013) by ranking features according to their
learned length-scales and selecting the top 40
features.
USHEFF (T1.1, T1.2, T1.3): USHEFF submis-
sions exploit the use of consensus among
MT systems by comparing the MT sys-
tem output to several alternative translations
generated by other MT systems (pseudo-
references). The comparison is done using
standard evaluation metrics (BLEU, TER,
METEOR, ROUGE for all tasks, and two
metrics based on syntactic similarities from
shallow and dependency parser information
for T1.2 and T1.3). Figures extracted from
such metrics are used as features to com-
plement prediction models trained on the 17
baseline features. Different from the standard
use of pseudo-reference features, these fea-
tures do not assume that the alternative MT
systems are better than the system of inter-
est. A more realistic scenario is considered
where the quality of the pseudo-references is
not known. For T1, no external systems in
addition to those provided for the shared task
are used: for a given translation, all alter-
native translations for the same source seg-
ment (two or three, depending on the lan-
guage pair) are used as pseudo-references.
For T1.2 and T1.3, for each source sentence,
all alternative translations produced by MT
systems on the same data (WMT12/13) are
used as pseudo-references. The hypothesis
is that by using translations from several MT
systems one can find consensual information
and this can smooth out the effect of ?coinci-
dences? in the similarities between systems?
translations. SVM regression with radial ba-
sis function kernel and hyper-parameters op-
timised via grid search is used to build the
models.
YANDEX (T1.1): Both submissions are based
on the the 80 black-box features, plus an
LM score from a larger language model,
a pseudo-reference, and several additional
features based on POS tags and syntactic
parsers. The first attempt uses an extract
of the top 5 features selected with a greedy
search from the set of all features. SVM re-
gression is used as machine learning algo-
rithm. The second attempt uses the same
features processed with Yandex? implemen-
tation of the gradient tree boosting (Ma-
trixNet).
4.4 Results
In what follows we give the official results for all
tasks followed by a discussion that highlights the
main findings for each of the tasks.
Task 1.1 Predicting post-editing effort
Table 11 summarises the results for the ranking
variant of Task 1.1. They are sorted from best to
worst using the DeltaAvg metric scores as primary
key and the Spearman?s rank correlation scores as
secondary key.
The winning submissions for the ranking vari-
ant of Task 1.1 are as follows: for English-Spanish
it is RTM-DCU/RTM-TREE, with a DeltaAvg
score of 0.26; for Spanish-English it is USH-
EFF, with a DeltaAvg score of 0.23; for English-
German it is again RTM-DCU/RTM-TREE, with a
DeltaAvg score of 0.39; and for German-English it
is RTM-DCU/RTM-RR, with a DeltaAvg score of
0.38. These winning submissions are better than
the baseline system by a large margin, which indi-
cates that current best performance in MT quality
estimation has reached levels that are clearly be-
yond what the baseline system can produce. As for
the other systems, according to DeltaAvg, com-
pared to the previous year results a smaller per-
centage of systems is able to beat the baseline.
This might be a consequence of the use of the met-
ric for the prediction of only three discrete labels.
The results for the scoring task are presented in
Table 12, sorted from best to worst using the MAE
31
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.26 0.38
? RTM-DCU/RTM-TREE 0.26 0.41
? YANDEX/SHAD BOOSTEDTREES2 0.23 0.35
USHEFF 0.21 0.33
SHEFF-lite 0.21 0.33
YANDEX/SHAD SVR1 0.18 0.29
SHEFF-lite-sparse 0.17 0.27
Baseline SVM 0.14 0.22
Spanish-English
? USHEFF 0.23 0.30
? RTM-DCU/RTM-PLS-RR 0.20 0.35
? RTM-DCU/RTM-FS-RR 0.19 0.36
Baseline SVM 0.12 0.21
SHEFF-lite-sparse 0.12 0.17
SHEFF-lite 0.11 0.15
English-German
? RTM-DCU/RTM-TREE 0.39 0.54
RTM-DCU/RTM-PLS-TREE 0.33 0.42
USHEFF 0.26 0.41
SHEFF-lite 0.26 0.36
Baseline SVM 0.23 0.34
SHEFF-lite-sparse 0.23 0.33
German-English
? RTM-DCU/RTM-RR 0.38 0.51
? RTM-DCU/RTM-PLS-RR 0.35 0.45
USHEFF 0.28 0.30
SHEFF-lite 0.24 0.27
Baseline SVM 0.21 0.25
SHEFF-lite-sparse 0.14 0.17
Table 11: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
32
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-PLS-TREE 0.49 0.61
? SHEFF-lite 0.49 0.63
? USHEFF 0.49 0.63
? SHEFF-lite/sparse 0.49 0.69
? RTM-DCU/RTM-TREE 0.49 0.61
Baseline SVM 0.52 0.66
YANDEX/SHAD BOOSTEDTREES2 0.56 0.68
YANDEX/SHAD SVR1 0.64 0.81
DCU-Chris/SVR 0.66 0.88
DCU-Chris/MIXED 0.94 1.14
Spanish-English
? RTM-DCU/RTM-FS-RR 0.53 0.64
? SHEFF-lite/sparse 0.54 0.69
? RTM-DCU/RTM-PLS-RR 0.55 0.71
USHEFF 0.57 0.67
Baseline SVM 0.57 0.68
SHEFF-lite 0.62 0.77
DCU-Chris/MIXED 0.65 0.91
English-German
? RTM-DCU/RTM-TREE 0.58 0.68
RTM-DCU/RTM-PLS-TREE 0.60 0.71
SHEFF-lite 0.63 0.74
USHEFF 0.64 0.75
SHEFF-lite/sparse 0.64 0.75
Baseline SVM 0.64 0.76
DCU-Chris/MIXED 0.69 0.98
German-English
? RTM-DCU/RTM-RR 0.55 0.67
? RTM-DCU/RTM-PLS-RR 0.57 0.74
USHEFF 0.63 0.76
SHEFF-lite 0.65 0.77
Baseline SVM 0.65 0.78
Table 12: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.1. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (1M times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
33
metric scores as primary key and the RMSE metric
scores as secondary key.
The winning submissions for the scoring variant
of Task 1.1 are as follows: for English-Spanish it
is RTM-DCU/RTM-TREE with a MAE of 0.49;
for Spanish-English it is RTM-DCU/RTM-FS-
RR with a MAE of 0.53; for English-German
it is again RTM-DCU/RTM-TREE, with a MAE
of 0.58; and for German-English it is RTM-
DCU/RTM-RR with a MAE of 0.55. These sub-
missions are again much better than the baseline
system, which under the scoring variant seems
to perform at a middle-of-the-pack level or lower
compared to the overall pool of submissions.
Overall, more systems are able to outperform the
baseline according to the scoring metric.
The top system for most language pairs are
essentially based on the same core techniques
(RTM-DCU) according to both the DeltaAvg and
MAE metrics. The ranking of other systems, how-
ever, can be substantially different according to the
two metrics.
Task 1.2 Predicting percentage of edits
Table 13 summarises the results for the ranking
variant of Task 1.2. For readability purposes we
have used a multiplication-factor of 100 in the
scoring script, which makes the HTER numbers
(both predicted and gold) to be in the [0, 100]
range. They are sorted from best to worst using
the DeltaAvg metric scores as primary key and the
Spearman?s rank correlation scores as secondary
key.
The winning submission for the ranking vari-
ant of Task 1.2 is RTM-DCU/RTM-SVR, with a
DeltaAvg score of 9.31. There is a large mar-
gin between this score and the baseline score of
DeltaAvg 5.08, which indicates again that current
best performance has reached levels that are much
beyond what this baseline system can produce.
The vast majority of the submissions perform bet-
ter than the baseline (the only exception is the sub-
mission from SHEFF-lite, for which the authors
report a major issue with the learning algorithm).
The results for the scoring variant are presented
in Table 14, sorted from best to worst by using the
MAE metric scores as primary key and the RMSE
metric scores as secondary key.
The winning submission for the scoring variant
of Task 1.2 is FBK-UPV-UEDIN/WP with a MAE
of 12.89, while the baseline system has a MAE
of 15.23. Most of the submissions perform better
than the baseline.
Task 1.3 Predicting post-editing time
Table 15 summarises the results for the ranking
variant of Task 1.3. For readability purposes, we
have used a multiplication-factor of 0.001 in the
scoring script, which makes the time (both pre-
dicted and gold) to be measured in seconds. They
are sorted from best to worst using the DeltaAvg
metric scores as primary key and the Spearman?s
rank correlation scores as secondary key.
The winning submission for the ranking vari-
ant of Task 1.3 is RTM-DCU/RTM-RR, with a
DeltaAvg score of 17.02 (when predicting sec-
onds). The interesting aspect of these results is
that the DeltaAvg numbers have a direct real-
world interpretation, in terms of time spent (or
saved, depending on one?s view-point) for post-
editing machine-produced translations. A more
elaborate discussion on this point can be found in
Section 4.5.
The winning submission for the scoring variant
of Task 1.3 is RTM-DCU/RTM-SVR, with a MAE
of 16.77. Note that all of the submissions perform
significantly better than the baseline, which has a
MAE of 21.49, and that the majority is not signif-
icantly worse than the top scoring submission.
Task 2 Predicting word-level edits
The results for Task 2 are summarised in Tables
17?19. The results are ordered by F
1
score for
the Error (BAD) class. For comparison, two triv-
ial baselines are included, one that marks every
word as correct and that marks every word with
the most common error class found in the training
data. Both baselines are clearly useless for any ap-
plication, but help put the results in perspective.
Most teams submitted systems for a single lan-
guage pair: English-Spanish; only a single team
produced predictions for all four pairs.
Table 17 gives the results of the binary (OK vs.
BAD) classification variant of Task 2. The win-
ning submissions for this variant are as follows:
for English-Spanish it is FBK-UPV-UEDIN/RNN
with a weighted F
1
of 48.73; for Spanish-
English it is RTM-DCU/RTM-GLMd with a
weighted F
1
of 29.14; for English-German it is
RTM-DCU/RTM-GLM with a weighted F
1
of
45.30; and for German-English it is again RTM-
DCU/RTM-GLM with a weighted F
1
of 26.13.
Remarkably, for three out of four language
pairs, the systems fail to beat our trivial baseline of
34
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-SVR 9.31 0.53
? RTM-DCU/RTM-TREE 8.57 0.48
? USHEFF 7.93 0.45
SHEFF-lite/sparse 7.69 0.43
Baseline 5.08 0.31
SHEFF-lite 0.72 0.09
Table 13: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. These are the top-scoring submission and those that are not significantly worse according to bootstrap
resampling (100k times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system
at a statistically significant level according to the same test.
System ID MAE RMSE
English-Spanish
? FBK-UPV-UEDIN/WP 12.89 16.74
? RTM-DCU/RTM-SVR 13.40 16.69
? USHEFF 13.61 17.84
RTM-DCU/RTM-TREE 14.03 17.48
DFKI/SVR 14.32 17.74
FBK-UPV-UEDIN/NOWP 14.38 18.10
SHEFF-lite/sparse 15.04 18.38
MULTILIZER 15.04 20.86
Baseline 15.23 19.48
DFKI/SVRxdata 16.01 19.52
SHEFF-lite 18.15 23.41
Table 14: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.2. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with 95% confidence intervals. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
System ID DeltaAvg Spearman Corr
English-Spanish
? RTM-DCU/RTM-RR 17.02 0.68
? RTM-DCU/RTM-SVR 16.60 0.67
SHEFF-lite/sparse 16.33 0.63
SHEFF-lite 16.08 0.64
USHEFF 14.98 0.59
Baseline 14.71 0.57
Table 15: Official results for the ranking variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
35
System ID MAE RMSE
English-Spanish
? RTM-DCU/RTM-SVR 16.77 26.17
?MULTILIZER/MLZ2 17.07 25.83
? SHEFF-lite 17.13 27.33
?MULTILIZER/MLZ1 17.31 25.51
? SHEFF-lite/sparse 17.42 27.35
? FBK-UPV-UEDIN/WP 17.48 25.31
RTM-DCU/RTM-RR 17.50 25.97
FBK-UPV-UEDIN/NOWP 18.69 26.58
USHEFF 21.48 34.28
Baseline 21.49 34.28
Table 16: Official results for the scoring variant of the WMT14 Quality Evaluation Task 1.3. The winning submissions
are indicated by a ?. They are statistically indistinguishable from the top submission according to bootstrap resampling (1M
times) with a 95% confidence interval. The systems in the gray area are not different from the baseline system at a statistically
significant level according to the same test.
weighted F
1
F
1
System ID All Bad ? MCC ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 64.38
Baseline (always Bad) 18.71 52.53 0.00 35.62
? FBK-UPV-UEDIN/RNN 62.00 48.73 18.23 61.62
LIMSI/RF 60.55 47.32 15.44 60.09
LIG/FS 63.55 44.47 19.41 64.67
LIG/BL ALL 63.77 44.11 19.91 65.12
FBK-UPV-UEDIN/RNN+tandem+crf 62.17 42.63 16.32 63.26
RTM-DCU/RTM-GLM 60.68 35.08 13.45 63.74
RTM-DCU/RTM-GLMd 60.24 32.89 12.98 63.97
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 82.37
Baseline (always Bad) 5.28 29.98 0.00 17.63
? RTM-DCU/RTM-GLMd 79.54 29.14 25.47 82.98
RTM-DCU/RTM-GLM 79.42 26.91 25.93 83.43
English-German
Baseline (always OK) 59.39 0.00 0.00 71.33
Baseline (always Bad) 12.78 44.57 0.00 28.67
? RTM-DCU/RTM-GLM 71.51 45.30 28.61 72.97
RTM-DCU/RTM-GLMd 68.73 36.91 21.32 71.41
German-English
Baseline (always OK) 67.82 0.00 0.00 77.60
Baseline (always Bad) 8.20 36.60 0.00 22.40
? RTM-DCU/RTM-GLM 72.41 26.13 16.08 76.14
RTM-DCU/RTM-GLMd 71.42 22.97 12.63 75.46
Table 17: Official results for the binary part of the WMT14 Quality Evaluation Task 2. The winning submissions are indicated
by a ?. All values are given as percentages.
36
marking all the words as wrong. This may either
indicate that the predictions themselves are of low
quality or the chosen evaluation approach is mis-
leading. On the other hand F
1
scores are a com-
mon measure of binary classification performance
and no averaging is performed here.
Table 18 gives the results of the Level 1
classification (OK, Fluency, Accuracy) variant
of Task 2. Here the second baseline is to
always predict Fluency errors, as this is the
most common error category in the training
data. The winning submissions of this vari-
ant are as follows: for English-Spanish it
is FBK-UPV-UEDIN/RNN+tandem+crf with a
weighted F
1
of 23.94 and for Spanish-English,
English-German, and German-English it is RTM-
DCU/RTM-GLMd with weighted F
1
scores of
23.94, 21.94, and 8.57 respectively.
As before, all systems fail to outperform the
single-class baseline for the Spanish-English lan-
guage pair according to our primary metric. How-
ever, for Spanish-English and English-German
both submissions are able to beat the baseline by
large margin. We also observe that the absolute
numbers vary greatly between language pairs.
Table 19 gives the results of the Multi-class
classification variant of Task 2. Again, the sec-
ond baseline is to always predict the most common
error category in the training data, which varies
depending on language pair and produces and in-
creasingly weak baseline as the number of classes
rises.
The winning submissions of this variant are
as follows: for English-Spanish, Spanish-English,
and English-German it is RTM-DCU/RTM-GLM
with weighted F
1
scores of 26.84, 8.75, and 15.02
respectively and and for German-English it is
RTM-DCU/RTM-GLMd with a weighted F
1
of
3.08. Not only do these systems perform above
our baselines for all but the German-English lan-
guage pair, they also outperform all other sub-
missions for English-Spanish. Remarkably, RTM-
DCU/RTM-GLM wins English-Spanish for all of
the proposed metrics by a sizeable margin.
4.5 Discussion
In what follows, we discuss the main accomplish-
ments of this year?s shared task starting from the
goals we had previously identified for it.
Investigating the effectiveness of different
quality labels
For the sentence-level tasks, the results of this
year?s shared task allow us to investigate the ef-
fectiveness of predicting translation quality using
three very different quality labels: perceived post-
editing effort on a scale of [1-3] (Task 1.1); HTER
scores (Task 1.2); and the time that a translator
takes to post-edit the translation (Task 1.3). One of
the ways one can compare the effectiveness across
all these different labels is to look at how well
the models can produce predictions that correlate
with the gold label that we have at our disposal.
A measure of correlation that does not depend
on the value of the labels is Spearman?s ranking
correlation. From this perspective, the label that
seems the most effective appears to be post-editing
time (Task 1.3), with the best system (RTM-
DCU/RTM-RR) producing a Spearman?s ? of 0.68
(English-Spanish translations, see Table 15). In
comparison, when perceived post-editing effort la-
bels are used (Task 1.1), the best systems achieve
a Spearman?s ? of 0.38 and 0.30 for English-
Spanish and Spanish-English translations, respec-
tively, and ? of 0.54 and 0.51 for English-German
and German-English, respectively (Table 11); for
HTER scores (Task 1.2) the best systems achieve
a Spearman?s ? of 0.53 for English-Spanish trans-
lations (Table 13).
This comparison across tasks seems to indicate
that, among the three labels we have proposed,
post-editing time seems to be the most learnable,
in the sense that automatic predictions can vest
match the gold labels (in this case, with respect
to the rankings they induce). A possible reason
for this is that post-editing time correlates with the
length of the source sentence whereas HTER is a
normalised measure.
Compared to the results regarding time predic-
tion in the Quality Evaluation shared task from
2013 (Bojar et al., 2013), we note that this time
all submissions were able to beat the baseline sys-
tem (compared to only 1/3 of the submissions in
2013). In addition, better handling of the data
acquisition reduced the number of outliers in this
year?s dataset allowing for numbers that are more
reliably interpretable. As an example of its in-
terpretability, consider the following: the winning
submission for the ranking variant of Task 1.3 is
RTM-DCU/RTM-RR, with a a Spearman?s ? of
0.68 and a DeltaAvg score of 17.02 (when predict-
37
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always fluency) 14.39 40.41 0.00 0.00 30.67
? FBK-UPV-UEDIN/RNN+tandem+crf 58.36 38.54 16.63 13.89 57.98
FBK-UPV-UEDIN/RNN 60.32 37.25 18.22 15.51 61.75
LIG/BL ALL 58.97 31.79 14.95 11.48 61.13
LIG/FS 58.95 31.78 14.92 11.46 61.10
RTM-DCU/RTM-GLMd 58.23 26.62 12.60 12.76 62.94
RTM-DCU/RTM-GLM 56.47 29.91 8.11 7.96 58.56
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always fluency) 2.67 15.13 0.00 0.00 12.24
? RTM-DCU/RTM-GLMd 78.89 23.94 25.41 25.45 83.17
RTM-DCU/RTM-GLM 78.78 21.96 26.31 26.99 83.69
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always fluency) 3.83 13.35 0.00 0.00 14.82
? RTM-DCU/RTM-GLMd 64.58 21.94 17.69 15.92 69.26
RTM-DCU/RTM-GLM 64.43 21.10 16.99 14.93 69.34
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always fluency) 3.34 14.92 0.00 0.00 13.79
? RTM-DCU/RTM-GLMd 69.17 8.57 10.61 5.76 75.91
RTM-DCU/RTM-GLM 69.09 8.26 9.95 5.76 75.97
Table 18: Official results for the Level 1 classification part of the WMT14 Quality Evaluation Task 2. The winning submissions
are indicated by a ?. All values are given as percentages.
38
weighted F
1
weighted MCC
System ID All Errors ? All Errors ACC
English-Spanish
Baseline (always OK) 50.43 0.00 0.00 0.00 64.38
Baseline (always unintelligible) 7.93 22.26 0.00 0.00 21.99
? RTM-DCU/RTM-GLM 60.52 26.84 23.77 21.45 66.83
FBK-UPV-UEDIN/RNN+tandem+crf 52.96 23.07 15.17 10.74 52.13
LIG/BL ALL 56.66 20.50 18.56 13.39 60.39
LIG/FS 56.66 20.50 18.56 13.39 60.39
FBK-UPV-UEDIN/RNN 52.84 17.09 7.66 4.24 57.18
RTM-DCU/RTM-GLMd 51.87 3.22 10.16 4.04 64.42
Spanish-English
Baseline (always OK) 74.41 0.00 0.00 0.00 82.37
Baseline (always word order) 0.34 1.96 0.00 0.00 4.24
? RTM-DCU/RTM-GLM 76.34 8.75 19.82 13.43 83.27
RTM-DCU/RTM-GLMd 76.21 8.19 19.35 15.32 83.17
English-German
Baseline (always OK) 59.39 0.00 0.00 0.00 71.33
Baseline (always mistranslation) 2.48 8.66 0.00 0.00 11.78
? RTM-DCU/RTM-GLM 63.57 15.02 17.57 15.08 70.82
RTM-DCU/RTM-GLMd 63.33 12.48 18.70 13.20 71.45
German-English
Baseline (always OK) 67.82 0.00 0.00 0.00 77.60
Baseline (always word order) 1.56 6.96 0.00 0.00 9.23
? RTM-DCU/RTM-GLMd 67.62 3.08 7.19 1.48 74.73
RTM-DCU/RTM-GLM 67.86 2.36 7.55 1.79 75.75
Table 19: Official results for the Multi-class classification part of the WMT14 Quality Evaluation Task 2. The winning
submissions are indicated by a ?. All values are given as percentages.
39
ing seconds). This number has a direct real-world
interpretation: using the order proposed by this
system, a human translator would spend, on av-
erage, about 17 seconds less on a sentence taken
from the top of the ranking compared to a sen-
tence picked randomly from the set.
14
To put this
number into perspective, for this dataset the av-
erage time to complete a sentence post-editing is
39 seconds. As such, one has an immediate inter-
pretation for the usefulness of using such a rank-
ing: translating around 100 sentences taken from
the top of the rankings would take around 36min
(at about 22 seconds/sentence), while translating
the same number of sentences extracted randomly
from the same dataset would take around 1h5min
(at about 39 seconds/sentence). It is in this sense
that we consider post-editing time an interpretable
label.
Another desirable property of label predictions
is usefulness; this property, however, it highly
task-dependent and therefore cannot be judged in
the absence of a specific task. For instance, an in-
terpretable label like post-editing time may not be
that useful in a task the requires one to place the
machine translations into ?ready to publish? and
?not ready to publish? bins. For such an appli-
cation, labels such as the ones used by Task 1.1
are clearly more useful, and also very much inter-
pretable within the scope of the task. Our attempt
at presenting the Quality Prediction task with a va-
riety of prediction labels illustrates a good range
of properties for the proposed labels and enables
one to draw certain conclusions depending on the
needs of the specific task at hand.
For the word-level tasks, different quality labels
equate with using different levels of granularity for
the predictions, which we discuss next.
Exploring word-level quality prediction at
different levels of granularity
Previous work on word-level predictions, e.g. (Bo-
jar et al., 2013) has focused on prediction of auto-
matically derived labels, generally due to practical
considerations as the manual annotation is labour
intensive. While easily applicable, automatic an-
notations, using for example TER alignment be-
tween the machine translation and reference (or
post-edition), face the same problems as automatic
14
Note that the 17.02 seconds figure is a difference in real-
time, not predicted time; what is considered in this variant of
Task 1.3 is only the predicted ranking of data points, not the
absolute values of the predictions.
MT evaluation metrics as they fail to account for
different word choices and lack the ability to re-
liably distinguish meaning preserving reorderings
from those that change the semantics of the out-
put. Furthermore, previous automatic annotation
for word-level quality estimation has focused on
binary labels: correct / incorrect, or at most, the
main edit operations that can be captured by align-
ment metrics like TER: correct, insertion, dele-
tion, substitution.
In this year?s task we were able to provide
manual fine-grained annotations at the word-level
produced by humans irrespective of references or
post-editions. Error categories range from fre-
quent ones, such as unintelligible, mistranslation,
and terminology, to rare ones such as additions or
omissions. For example, only 10 out of more than
3,400 errors in the English-Spanish test set fall
into the latter categories, while over 2,000 words
are marked as unintelligible. By hierarchically
grouping errors into coarser categories we aimed
to find a compromise between data sparsity and
the expressiveness of the labels. What marks a
good compromise depends on the use case, which
we do not specify here, and the quality of the finer
grained predictions: if a system is able to predict
even rare errors these may be grouped later if nec-
essary.
Overall, word-level error prediction seems to re-
main a challenging task as evidenced by the fact
that many submissions were unable to beat a triv-
ial baseline. We hypothesise that this is at least
partially due to a mismatch in loss-functions used
in training and testing. We know from the sys-
tem descriptions that some systems were tuned to
optimise squared error or accuracy, while evalua-
tion was performed using weighted F
1
scores. On
the other hand, even a comparison of just accuracy
shows that systems struggle to obtain a lower error
rates than the ?all-OK? baseline.
Such performance problems are consistent over
the three levels of granularity, contrary to the in-
tuition that binary classification would be easier.
A notable exception is the RTM-DCU/RTM-GLM
system, which is able to beat both the baseline and
all other systems on the Multi-Class variant of the
English-Spanish task ? cf. Table 19 ? with regard
to all metrics. For this and most other submis-
sions we observe that labels are not consistent for
different granularities, i.e. at token marked with a
specific error in the multi-class variant may still
40
carry an ?OK? label in binary annotation. Thus,
additional coarse grained annotations may be de-
rived by automatic means. For example, mapping
the multi-class predictions of the above system to
coarser categories improves the F
1,ERR
score in
Table 17 from 35.08 to 37.02 but does not change
the rank with respect to the other entries.
The fact that coarse grained predictions seem
not to be derived from the fine-grained ones leads
us to believe that most participants treated the
different granularities as independent classifica-
tion tasks. The FBK-UPV-UEDIN team trans-
fers information in the opposite direction by using
their binary predictions as features for Level-1 and
multi-class.
Given the current quality of word-level predic-
tion it remains unclear if these systems can already
be employed in a practical setting, e.g. to focus the
attention of post-editors.
Studying the effects of training and test
datasets with mixed domains, language pairs
and MT systems
This year?s shared task made available datasets for
more than one language pair with the same or dif-
ferent types of annotation, 2-3 multiple MT sys-
tems (plus a human translation) per language pair,
and out-of-domain test data (Tasks 1.1 and 2). In-
stances for each language pair were kept in sep-
arate datasets and thus the ?language pair? vari-
able can be analysed independently. However, for
a given language pair, datasets mix translation sys-
tems (and humans) in Task 1.1, and also text do-
mains in Task 2.
Directly comparing the performance across lan-
guage pairs is not possible, given that their
datasets have different numbers of instances (pro-
duced by 3 or 4 systems) and/or different true
score distributions (see Figure 3). For a relative
comparison (although not all systems submitted
results for all language pairs, which is especially
true in Task 2), we observe in Task 1.1 that for all
language pairs generally at least half of the sys-
tems did better than the baseline. To our surprise,
only one submission combined data for multiple
languages together for Task 1.1: SHEF-lite, treat-
ing each language pair data as a different task in
a multi-task learning setting. However, only for
the ?sparse? variant of the submission significant
gains were reported over modelling each task in-
dependently (with the tasks still sharing the same
data kernel and the same hyperparameters).
The interpretation of the results for Task 2 is
very dependent on the evaluation metric used,
but generally speaking a large variation in per-
formance was found between different languages,
with English-Spanish performing the best, possi-
bly given the much larger number of training in-
stances. Data for Task 2 also presented varied true
score distributions (as shown by the performance
of the baseline (e.g. always ?OK?) in Tables 17-
19.
One of the main goals with Task 1.1 (and Task 2
to some extent) was to test the robustness of mod-
els in a blind setting where multiple MT systems
(and human translations) are put together and their
identifiers are now known. All submissions for
these tasks were therefore translation system ag-
nostic, with no submission attempting to perform
meta-identification of the origins of the transla-
tions. For Task 1.1, data from multiple MT sys-
tems was explicitly used by USHEFF though the
idea of consensus translations. Translations from
all but the system of interest for the same source
segment were used as pseudo-references. The
submission significantly outperformed the base-
line for all language pairs and did particularly well
for Spanish-English and English-Spanish.
An in depth analysis of Task 1.1?s datasets on
the difference in prediction performance between
models built and applied for individual transla-
tion systems and models built and tested for all
translations pooled together is presented in (Shah
and Specia, 2014). Not surprisingly, the former
models perform significantly better, with MAE
scores ranging between 0.35 and 0.5 for differ-
ent language pairs and MT systems, and signifi-
cantly lower scores for models trained and tested
on human translations only (MAE scores between
0.2 and 0.35 for different language pairs), against
MAE scores ranging between 0.5 and 0.65 for
models with pooled data.
For Tasks 1.2 and 1.3, two submissions included
English-Spanish data which had been produced by
yet different MT systems (SHEF-lite and DFKI).
While using these additional instances seemed at-
tractive given the small number of instances avail-
able for these tasks, it is not clear what their contri-
bution was. For example, with a reduced set of in-
stances (only 400) from the combined sets, SHEF-
lite/sparse performed significantly better than its
variant SHEF-lite.
Finally, with respect to out-of-domain (different
41
text domain and MT system) test data, for Task
1.1, none of the papers submitted included experi-
ments. (Shah and Specia, 2014) applied the mod-
els trained on pooled datasets (as explained above)
for each language pair to the out-of-domain test
sets. The results were surprisingly positive, with
average MAE score of 0.5, compared to the 0.5-
0.65 range for in-domain data (see above). Further
analysis is necessary to understand the reasons for
that.
In Task 2, the official training and test sets al-
ready include out-of-domain data because of the
very small amount of in-domain data available,
and thus is is hard to isolate the effect of this data
on the results.
Examining the effectiveness of quality
prediction methods on human translations
Datasets for Tasks 1.1 and 2 contain human trans-
lations, in addition to the automatic translations
from various MT systems. Predicting human
translation quality is an area that has been largely
unexplored. Previous work has looked into dis-
tinguishing human from machine translations (e.g.
(Gamon et al., 2005)), but this problem setting is
somehow artificial, and moreover arguably harder
to solve nowadays given the higher general qual-
ity of current MT systems (Shah and Specia,
2014). Although human translations are obviously
of higher quality in general, many segments are
translated by MT systems with the same or similar
levels of quality as human translation. This is par-
ticularly true for Task 2, since data had been pre-
viously categorised and only ?near misses? were
selected for the word-level annotation, i.e., human
and machine translations that were both nearly
perfect in this case.
While no distinction was made between human
and machine translations in our tasks, we believe
the mix of these two types of translations has had
a negative impact in prediction performance. Intu-
itively, one can expect errors in human translation
to be more subtle, and hence more difficult to cap-
ture via standard quality estimation features. For
example, an incorrect lexical choice (due to, e.g.,
ambiguity) which still fits the context and does not
make the translation ungrammatical is unlikely to
be captured. We hoped that participants would de-
sign features for this particular type of translation,
but although linguistically motivated features have
been exploited, they did not seem appropriate for
human translations.
It is interesting to mention the indirect use of
human translations by USHEFF for Tasks 1.1-1.3:
given a translation for a source segment, all other
translations for the same segment were used as
pseudo-references. Apart from when this transla-
tion was actually the human translation, the hu-
man translation was effectively used as a refer-
ence. While this reference was mixed with 2-
3 other pseudo-references (other machine transla-
tions) for the feature computations, these features
led to significant gains in performance over the
baseline features Scarton and Specia (2014).
We believe that more investigation is needed for
human translation quality prediction. Tasks ded-
icated to this type of data at both sentence- and
word-level in the next editions of this shared task
would be a possible starting point. The acquisi-
tion of such data is however much more costly, as
it is arguably hard to find examples of low quality
human translation, unless specific settings, such as
translation learner corpora, are considered.
5 Medical Translation Task
The Medical Translation Task addresses the prob-
lem of domain-specific and genre-specific ma-
chine translation. The task is split into two sub-
tasks: summary translation, focused on transla-
tion of sentences from summaries of medical ar-
ticles, and query translation, focused on transla-
tion of queries entered by users into medical infor-
mation search engines.
In general, texts of specific domains and gen-
res are characterized by the occurrence of special
vocabulary and syntactic constructions which are
rare or even absent in traditional (general-domain)
training data and therefore difficult for MT. Spe-
cific training data (containing such vocabulary and
syntactic constructions) is usually scarce or not
available at all. Medicine, however, is an exam-
ple of a domain for which in-domain training data
(both parallel and monolingual) is publicly avail-
able in amounts which allow to train a complete
SMT system or to adapt an existing one.
5.1 Task Description
In the Medical Translation Task, we provided links
to various medical-domain training resources and
asked participants to use the data to train or adapt
their systems to translate unseen test sets for both
subtasks between English and Czech (CS), Ger-
man (DE), and French (FR), in both directions.
42
The summary translation test data is domain-
specific, but otherwise can be considered as ordi-
nary sentences. On the other hand, the query trans-
lation test data is also specific for its genre (gen-
eral style) ? it contains short sequences of (more
or less) of independent terms rather than complete
and grammatical sentences, the usual target of cur-
rent MT systems.
Similarly to the standard Translation Task, the
participants of the Medical Translation Task were
allowed to use only the provided resources in the
constrained task (in addition to data allowed in
the constrained standard Translation Task), but
could exploit any additional resources in the un-
constrained task. The submissions were expected
with true letter casing and detokenized. The trans-
lation quality was measured using automatic eval-
uation metrics, manual evaluation was not per-
formed.
5.2 Test and Development Data
The test and development data sets for this task
were provided by the EU FP7 project Khres-
moi.
15
This projects develops a multi-lingual
multi-modal search and access system for biomed-
ical information and documents and its MT com-
ponent allows users to use non-English queries to
search in English documents and see summaries
of retrieved documents in their preferred language
(Czech, German, or French). The statistics of the
data sets are presented in Tables 20 and 21.
For the summary translation subtask, 1,000
and 500 sentences were provided for test devel-
opment purposes, respectively. The sentences
were randomly sampled from automatically gen-
erated summaries (extracts) of English documents
(web pages) containing medical information rel-
evant to 50 topics provided for the CLEF 2013
eHealth Task 3.
16
Out-of-domain and ungram-
matical sentences were manually removed. The
sentences were then translated by medical experts
into Czech, German and French, and the transla-
tions were reviewed. Each sentence was provided
with the corresponding document ID and topic ID.
The set also included a description for each of the
50 topics. The data package (Khresmoi Summary
Translation Test Data 1.1) is now available from
the LINDAT/CLARIN repository
17
and more de-
15
http://khresmoi.eu/
16
https://sites.google.com/site/
shareclefehealth/
17
http://hdl.handle.net/11858/
tails can be found in Zde?nka Ure?sov?a and Pecina
(2014).
For the query translation subtask, the main
test set contains 1,000 queries for test and 508
queries for development purposes. The original
English queries were extracted at random from
real user query logs provided by the Health on the
Net foundation
18
(queries by general public) and
the Trip database
19
(queries by medical experts).
Each query was translated into Czech, German,
and French by medical experts and the transla-
tions were reviewed. The data package (Khresmoi
Query Translation Test Data 1.0) is available from
the LINDAT/CLARIN repository.
20
An additional test set for the query translation
subtask was adopted from the CLEF 2013 eHealth
Task 3 (Pecina et al., 2014). It contains 50 queries
constructed from titles of the test topics (originally
in English) translated into Czech, German, and
French by medical experts. The participants were
asked to translate the queries back to English and
the resulting translations were used in an informa-
tion retrieval (IR) experiment for extrinsic evalua-
tion.
5.3 Training Data
This section reviews the in-domain resources
which were allowed for the constrained Medical
Translation Task in addition to resources for the
constrained standard Translation Task (see Section
2). Most of the corpora are available for direct
download, others can be obtained upon registra-
tion. The corpora usually employ their own, more
or less complex data format. To lower the entry
barrier, we provided a set of easy-to-use scripts to
convert the data to a plain text format suitable for
MT training.
5.3.1 Parallel Training Data
The medical-domain parallel data includes the fol-
lowing corpora (see Table 22 for statistics): The
EMEA corpus (Tiedemann, 2009) contains doc-
uments from the European Medicines Agency,
automatically processed and aligned on sentence
level. It is available for many language pairs, in-
cluding those relevant to this task. UMLS is a
multilingual metathesaurus of health and biomed-
00-097C-0000-0023-866E-1
18
http://www.hon.ch/
19
http://www.tripdatabase.com/
20
http://hdl.handle.net/11858/
00-097C-0000-0022-D9BF-5
43
sents tokens
total Czech German French English
dev 500 9,209 9,924 12,369 10,350
test 1,000 19,191 20,831 26,183 21,423
Table 20: Statistics of summary test data.
queries tokens
total general expert Czech German French English
dev 508 249 259 1,128 1,041 1,335 1,084
test 1,000 500 500 2,121 1,951 2,490 2,067
Table 21: Statistics of query test data.
L1?L2 Czech?English DE?EN FR?EN
data set sents L1 tokens L2 tokens sents L1 tokens L2 tokens sents L1 tokens L2 tokens
EMEA 1,053 13,872 14,378 1,108 13,946 14,953 1,092 17,605 14,786
UMLS 1,441 4,248 5,579 2,001 6,613 8,153 2,171 8,505 8,524
Wiki 3 5 6 10 19 22 8 19 17
MuchMore 29 688 740
PatTr 1,848 102,418 106,727 2,201 127,098 108,665
COPPA 664 49,016 39,933
Table 22: Statistics of the in-domain parallel training data allowed for the constrained task (in thousands).
data set English Czech German French
PatTR 121,592 53,242 54,608
UMLS 7,991 63 24 37
Wiki 26,945 1,784 10,232 8,376
AACT 13,341
DrugBank 953
FMA 884
GENIA 557
GREC 62
PIL 662
Table 23: Sizes of monolingual training data allowed for the
constrained tasks (in thousands of tokens).
ical vocabularies and standards (U.S. National Li-
brary of Medicine, 2009). The UMLS dataset
was constructed by selecting the concepts which
have translations in the respective languages. The
Wiki dataset contains bilingual pairs of titles of
Wikipedia articles belonging to the categories
identified to be medical-domain within the Khres-
moi project. It is available for all three lan-
guage pairs. The MuchMore Springer Corpus
is a German?English parallel corpus of medical
journals abstracts published by Springer (Buitelaar
et al., 2003). PatTR is a parallel corpus extracted
from the MAREC patent collection (W?aschle and
Riezler, 2012). It is available for German?English
and French?English. For the medical domain,
we only consider text from patents indicated to
be from the medicine-related categories (A61,
C12N, C12P). COPPA (Corpus of Parallel Patent
Applications (Pouliquen and Mazenc, 2011) is a
French?English parallel corpus extracted from the
MAREC patent collection (W?aschle and Riezler,
2012). The medical-domain subset is identified by
the same categories as in PatTR.
5.3.2 Monolingual Training Data
The medical-domain monolingual data consists of
the following corpora (statistics are presented in
Table 23): The monolingual UMLS dataset con-
tains concept descriptions in CS, DE, and FR ex-
tracted from the UMLS Metathesaurus (see Sec-
tion 5.3.1). The monolingual Wiki dataset con-
sists of articles belonging to the categories iden-
tified to be medical-domain within the Khresmoi
project. The PatTR dataset contains non-parallel
data extracted from the medical patents included
in the PatTR corpus (see Section 5.3.1). AACT is a
collection of restructured and reformatted English
texts publicly available and downloadable from
ClinicalTrials.gov, containing clinical studies con-
ducted around the world. DrugBank is a bioin-
formatics and cheminformatics resource contain-
ing drug descriptions (Knox et al., 2011). GENIA
is a corpus of biomedical literature compiled and
annotated within the GENIA project (Kim et al.,
2003). FMA stands for the Foundational Model
of Anatomy Ontology, a knowledge source for
biomedical informatics concerned with symbolic
representation of the phenotypic structure of the
human body (Rosse and Mejino Jr., 2008). GREC
(Gene Regulation Event Corpus) is a semantically
annotated English corpus of abstracts of biomedi-
cal papers (Thompson et al., 2009). The PIL cor-
pus is a collection of documents giving instruc-
tions to patients about their medication (Bouayad-
Agha et al., 2000).
5.4 Participants
A total of eight teams participated in the Medical
Translation Task by submitting their systems to at
least one subtask for one or more translation direc-
tions. A list of the participants is given in Table 24;
we provide short descriptions of their systems in
the following.
CUNI was involved in the organization of the task,
and their primary goal was to set up a baseline for
both the subtasks and for all translation directions.
44
ID Participating team
CUNI Charles University in Prague (Du?sek et al., 2014)
DCU-Q Dublin City University (Okita et al., 2014)
DCU-S Dublin City University (Zhang et al., 2014)
LIMSI Laboratoire dInformatique pour la Mecanique et les Sciences de lIng?enieur (P?echeux et al., 2014)
POSTECH Pohang University of Science and Technology (Li et al., 2014a)
UEDIN University of Edinburgh (Durrani et al., 2014a)
UM-DA University of Macau (Wang et al., 2014)
UM-WDA University of Macau (Lu et al., 2014)
Table 24: Participants in the WMT14 Medical Translation Task.
Their systems are based on the Moses phrase-
based toolkit and linear interpolation of in-domain
and out-of-domain language models and phrase ta-
bles. The constrained/unconstrained systems dif-
fer in the training data only. The constrained
ones are built using all allowed training data; the
unconstrained ones take advantage of additional
web-crawled monolingual data used for training of
the language models, and additional parallel non-
medical data from the PatTr and COPPA patent
collections.
DCU-Q submitted a system designed specifically
for terminology translation in the query translation
task for EN?FR and FR?EN. This system supports
six terminology extraction methods and is able to
detect rare word pairs including zero-appearance
word pairs. It uses monotonic decoding with lat-
tice inputs, avoiding unnecessary hypothesis ex-
pansions by the reordering model.
DCU-S submitted a system to the FR?EN sum-
mary translation subtask only. The system is
similar to DCU?s system for patent translation
(phrased-based using Moses) but adapted to trans-
late medical summaries and reports.
LIMSI took part in the summary translation sub-
task for English to French.Their primary submis-
sion uses a combination of two translation sys-
tems: NCODE, based on bilingual n-gram trans-
lation models; and an on-the-fly estimation of
the parameters of Moses along with a vector
space model to perform domain adaptation. A
continuous-space language model is also used in
a post-processing step for each system.
POSTECH submitted a phrase-based SMT sys-
tem and query translation system for the DE?EN
language pair in both subtasks. They analysed
three types of query formation, generated query
translation candidates using term-to-term dictio-
naries and a phrase-based system, and then scored
them using a co-occurrence word frequency mea-
sure to select the best candidate.
UEDIN applied the Moses phrase-based system to
all language pairs and both subtasks. They used
the hierarchical reordering model and the OSM
feature, same as in UEDIN?s news translation sys-
tem, and applied compound splitting to German
input. They used separate language models built
on in-domain and out-of-domain data with linear
interpolation. For all language pairs except CS-
EN and DE-EN, they selected data for the transla-
tion model using modified Moore-Lewis filtering.
For DE-EN and CS-EN, they concatenated all the
supplied parallel training data.
UM-DA submitted systems for all language pairs
in the summary translation subtask based on a
combination of different adaptation steps, namely
domain-specific pre-processing, language model
adaptation, translation model adaptation, numeric
adaptation, and hyphenated word adaptation. Data
for the domain-adapted language and translation
models were selected using various data selection
techniques.
UM-WDA submitted systems for all language
pairs in the summary translation subtask. Their
systems are domain-adapted using web-crawled
in-domain resources: bilingual dictionaries and
monolingual data. The translation model and lan-
guage model trained on the crawled data were in-
terpolated with the best-performing language and
translation model employed in the UM-DA sys-
tems.
5.5 Results
MT quality in the Medical Translation Task
is evaluated using automatic evaluation metrics:
BLEU (Papineni et al., 2002), TER (Snover et al.,
2006), PER (Tillmann et al., 1997), and CDER
(Leusch et al., 2006). BLEU scores are reported as
percentage and all error rates are reported as one
minus the original value, also as percentage, so
that all metrics are in the 0-100 range, and higher
scores indicate better translations.
The main reason for not conducting human
evaluation, as it happens in the standard Trans-
45
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 29.64 29.79
?
1.07 47.45
?
1.15 61.64
?
1.06 52.18
?
0.98 31.68
?
1.14 49.84
?
1.10 64.38
?
1.06 54.10
?
0.96
CUNI 22.44 22.57
?
0.95 41.43
?
1.16 55.46
?
1.09 46.42
?
0.96 32.34
?
1.12 50.24
?
1.20 65.07
?
1.10 54.42
?
0.96
UEDIN 36.65 36.87
?
1.23 54.35
?
1.19 67.16
?
1.00 57.61
?
1.01 38.02
?
1.24 56.14
?
1.17 69.24
?
1.01 58.96
?
0.96
UM-DA 37.62 37.79
?
1.26 54.55
?
1.20 68.29
?
0.88 57.28
?
1.03 38.81
?
1.28 56.04
?
1.20 70.06
?
0.82 58.45
?
1.05
CUNI 22.92 23.06
?
0.97 42.49
?
1.10 56.10
?
1.12 47.13
?
0.95 33.18
?
1.15 51.48
?
1.15 66.00
?
1.03 55.30
?
0.96
CUNI 22.69 22.84
?
0.98 42.21
?
1.14 56.01
?
1.11 46.79
?
0.94 32.84
?
1.13 51.10
?
1.11 65.79
?
1.07 54.81
?
0.96
UM-WDA 37.35 37.53
?
1.26 54.39
?
1.19 68.21
?
0.83 57.16
?
1.07 38.61
?
1.27 55.92
?
1.17 70.02
?
0.81 58.36
?
1.07
ONLINE 39.57
?
1.21 58.24
?
1.14 70.16
?
0.78 60.04
?
1.02 40.62
?
1.23 59.72
?
1.11 71.94
?
0.74 61.26
?
1.01
German?English
CUNI 28.20 28.34
?
1.12 46.66
?
1.13 61.53
?
1.03 50.57
?
0.93 30.69
?
1.19 48.91
?
1.16 64.12
?
1.04 52.52
?
0.95
CUNI 28.85 28.99
?
1.15 47.12
?
1.15 61.98
?
1.07 50.72
?
0.98 31.37
?
1.21 49.29
?
1.13 64.53
?
1.05 52.64
?
0.98
POSTECH 25.92 25.99
?
1.06 43.66
?
1.14 59.62
?
0.92 47.13
?
0.90 26.97
?
1.06 45.13
?
1.12 61.53
?
0.89 48.37
?
0.88
UEDIN 37.31 37.53
?
1.19 55.72
?
1.14 68.82
?
0.99 58.35
?
0.95 38.60
?
1.25 57.18
?
1.12 70.46
?
0.98 59.53
?
0.94
UM-DA 35.71 35.81
?
1.23 53.08
?
1.16 66.82
?
0.98 55.91
?
0.96 36.55
?
1.27 54.01
?
1.13 68.05
?
0.97 56.78
?
0.95
CUNI 30.58 30.71
?
1.10 48.68
?
1.09 63.19
?
1.08 52.72
?
0.94 33.14
?
1.19 50.98
?
1.06 65.88
?
1.04 54.74
?
0.94
CUNI 30.22 30.32
?
1.12 47.71
?
1.18 62.20
?
1.10 52.17
?
0.91 32.75
?
1.20 50.00
?
1.14 64.87
?
1.06 54.19
?
0.92
UM-WDA 32.70 32.88
?
1.19 49.60
?
1.18 63.74
?
1.01 53.50
?
0.96 33.95
?
1.23 51.05
?
1.19 65.54
?
0.98 54.73
?
0.96
ONLINE 41.18
?
1.24 59.33
?
1.09 70.95
?
0.92 61.92
?
1.01 42.29
?
1.23 60.76
?
1.08 72.51
?
0.88 63.06
?
0.96
French?English
CUNI 34.42 34.55
?
1.20 52.24
?
1.17 64.52
?
1.03 56.48
?
0.91 36.52
?
1.23 54.35
?
1.12 67.07
?
1.00 58.34
?
0.91
CUNI 33.67 33.59
?
1.16 50.39
?
1.23 61.75
?
1.16 56.74
?
0.97 35.55
?
1.21 52.55
?
1.26 64.45
?
1.13 58.63
?
0.91
DCU-B 44.85 45.01
?
1.24 62.57
?
1.12 74.11
?
0.78 64.33
?
0.99 46.12
?
1.26 64.04
?
1.06 75.84
?
0.74 65.55
?
0.94
UEDIN 46.44 46.68
?
1.26 64.12
?
1.16 74.47
?
0.87 66.40
?
0.96 48.01
?
1.29 65.70
?
1.15 76.30
?
0.86 67.76
?
0.91
UM-DA 47.08 47.22
?
1.33 64.08
?
1.16 75.41
?
0.88 66.15
?
0.96 48.23
?
1.31 65.36
?
1.10 76.95
?
0.89 67.18
?
0.93
CUNI 34.74 34.89
?
1.12 52.39
?
1.16 63.76
?
1.09 57.29
?
0.94 36.84
?
1.17 54.56
?
1.13 66.43
?
1.07 59.14
?
0.90
CUNI 35.04 34.99
?
1.18 52.11
?
1.24 63.24
?
1.09 57.51
?
0.97 37.04
?
1.18 54.38
?
1.17 66.02
?
1.05 59.55
?
0.93
UM-WDA 43.84 44.06
?
1.32 61.14
?
1.18 73.13
?
0.87 63.09
?
1.00 45.17
?
1.36 62.63
?
1.15 74.94
?
0.84 64.37
?
0.99
ONLINE 46.99
?
1.35 64.31
?
1.12 76.07
?
0.78 66.09
?
1.00 47.99
?
1.33 65.65
?
1.07 77.65
?
0.75 67.20
?
0.96
English?Czech
CUNI 17.36 17.65
?
0.96 37.17
?
1.02 49.13
?
0.98 40.31
?
0.95 18.75
?
0.96 38.32
?
1.02 50.82
?
0.91 41.39
?
0.94
CUNI 16.64 16.89
?
0.93 36.57
?
1.05 48.79
?
0.98 39.46
?
0.90 17.94
?
0.96 37.74
?
1.03 50.50
?
0.97 40.59
?
0.91
UEDIN 23.45 23.74
?
1.00 44.20
?
1.10 55.38
?
0.88 46.23
?
0.99 24.20
?
1.00 44.92
?
1.08 56.38
?
0.90 46.78
?
1.00
UM-DA 22.61 22.72
?
0.98 42.73
?
1.16 54.12
?
0.93 44.73
?
1.01 23.12
?
1.01 43.41
?
1.14 55.11
?
0.93 45.32
?
1.02
CUNI 20.56 20.84
?
1.01 39.98
?
1.09 51.98
?
0.99 42.86
?
1.00 22.03
?
1.05 41.19
?
1.08 53.66
?
0.97 43.93
?
1.01
CUNI 19.50 19.72
?
0.97 38.09
?
1.10 50.12
?
1.06 41.50
?
0.96 20.91
?
1.02 39.26
?
1.12 51.79
?
1.04 42.59
?
0.96
UM-WDA 22.14 22.33
?
0.96 42.30
?
1.11 53.89
?
0.92 44.48
?
1.01 22.72
?
0.97 43.02
?
1.09 54.89
?
0.95 45.08
?
0.99
ONLINE 33.45
?
1.28 51.64
?
1.28 61.82
?
1.10 53.97
?
1.18 34.02
?
1.31 52.35
?
1.22 62.84
?
1.08 54.52
?
1.18
English?German
CUNI 12.52 12.64
?
0.77 29.84
?
0.99 45.38
?
1.14 34.69
?
0.81 16.63
?
0.91 33.63
?
1.07 50.03
?
1.24 38.43
?
0.87
CUNI 12.42 12.53
?
0.77 29.02
?
1.05 44.27
?
1.16 34.62
?
0.78 16.41
?
0.91 32.87
?
1.08 48.99
?
1.21 38.37
?
0.86
POSTECH 15.46 15.59
?
0.91 34.41
?
1.01 49.00
?
0.83 37.11
?
0.90 15.98
?
0.92 34.98
?
1.00 49.94
?
0.81 37.60
?
0.87
UEDIN 20.88 21.01
?
1.03 40.03
?
1.08 55.54
?
0.91 42.95
?
0.90 21.40
?
1.03 40.55
?
1.08 56.33
?
0.92 43.41
?
0.90
UM-DA 20.89 21.09
?
1.07 40.76
?
1.03 55.45
?
0.89 43.02
?
0.93 21.52
?
1.08 41.31
?
1.01 56.38
?
0.90 43.58
?
0.91
CUNI 14.29 14.42
?
0.81 31.82
?
1.03 47.01
?
1.13 36.81
?
0.79 18.87
?
0.90 35.76
?
1.11 51.76
?
1.17 40.65
?
0.87
CUNI 13.44 13.58
?
0.75 30.37
?
1.03 45.80
?
1.14 35.80
?
0.76 17.84
?
0.89 34.41
?
1.13 50.75
?
1.18 39.85
?
0.78
UM-WDA 18.77 18.91
?
1.00 37.92
?
1.02 53.59
?
0.85 40.90
?
0.86 19.30
?
1.02 38.42
?
1.01 54.40
?
0.85 41.34
?
0.86
ONLINE 23.92
?
1.06 44.33
?
0.97 57.47
?
0.80 46.35
?
0.91 24.29
?
1.07 44.83
?
0.98 58.20
?
0.80 46.71
?
0.92
English?French
CUNI 30.30 30.67
?
1.11 46.59
?
1.09 59.83
?
1.04 50.51
?
0.93 32.06
?
1.12 48.01
?
1.09 61.66
?
1.00 51.83
?
0.94
CUNI 29.35 29.71
?
1.10 45.84
?
1.07 58.81
?
1.04 50.00
?
0.96 31.02
?
1.10 47.24
?
1.09 60.57
?
1.02 51.31
?
0.94
LIMSI 40.14 43.54
?
1.22 59.70
?
1.04 69.45
?
0.86 61.35
?
0.96 44.04
?
1.22 60.32
?
1.03 70.20
?
0.85 61.90
?
0.94
LIMSI 38.83 42.21
?
1.13 58.88
?
1.01 68.70
?
0.81 60.59
?
0.93 42.69
?
1.12 59.53
?
0.98 69.50
?
0.80 61.17
?
0.91
UEDIN 40.74 44.24
?
1.16 60.66
?
1.07 70.35
?
0.82 62.28
?
0.95 44.85
?
1.17 61.43
?
1.05 71.27
?
0.81 62.94
?
0.91
UM-DA 41.24 41.68
?
1.12 58.72
?
1.06 69.37
?
0.78 60.12
?
0.95 42.16
?
1.11 59.39
?
1.05 70.21
?
0.77 60.71
?
0.92
CUNI 32.23 32.61
?
1.09 48.48
?
1.08 61.13
?
1.01 52.24
?
0.93 34.08
?
1.10 49.93
?
1.11 62.92
?
0.99 53.65
?
0.92
CUNI 32.45 32.84
?
1.06 48.68
?
1.06 61.32
?
0.98 52.35
?
0.94 34.22
?
1.07 50.09
?
1.04 63.04
?
0.96 53.67
?
0.91
UM-WDA 40.78 41.16
?
1.13 58.20
?
0.99 68.93
?
0.84 59.64
?
0.94 41.79
?
1.12 59.10
?
0.96 70.01
?
0.84 60.39
?
0.91
ONLINE 58.63
?
1.26 70.70
?
1.12 78.22
?
0.81 71.89
?
0.96 59.27
?
1.26 71.50
?
1.10 79.16
?
0.81 72.63
?
0.94
Table 25: Official results of translation quality evaluation in the medical summary translation subtask.
46
original normalized truecased normalized lowercased
ID BLEU BLEU 1-TER 1-PER 1-CDER BLEU 1-TER 1-PER 1-CDER
Czech?English
CUNI 10.71 10.57
?
3.42 15.72
?
2.77 23.37
?
3.03 18.68
?
2.42 30.13
?
4.85 53.38
?
3.01 62.53
?
2.84 55.44
?
2.87
CUNI 9.92 9.78
?
3.04 16.84
?
2.84 23.80
?
3.08 19.85
?
2.40 28.21
?
4.56 54.15
?
3.04 62.56
?
2.99 55.91
?
2.79
UEDIN 24.66 24.68
?
4.52 39.88
?
3.05 49.97
?
3.29 41.81
?
2.80 28.25
?
4.94 45.31
?
3.14 55.66
?
3.06 46.67
?
2.77
CUNI 12.00 11.86
?
3.42 18.49
?
2.74 24.67
?
2.85 21.08
?
2.29 31.91
?
4.81 57.61
?
3.13 65.02
?
2.99 59.24
?
2.69
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
ONLINE 28.88
?
4.96 47.31
?
3.35 55.19
?
3.21 49.88
?
2.89 35.33
?
5.20 55.80
?
3.20 64.05
?
2.97 57.94
?
2.85
German?English
CUNI 10.90 10.74
?
3.41 18.89
?
2.39 26.09
?
2.00 20.29
?
2.07 32.15
?
5.23 55.56
?
2.90 63.68
?
2.34 56.45
?
2.62
CUNI 10.71 10.55
?
3.47 18.40
?
2.35 25.45
?
2.04 19.84
?
2.07 32.06
?
5.19 54.85
?
2.91 62.87
?
2.39 55.52
?
2.61
POSTECH 18.06 17.97
?
4.38 28.57
?
3.30 40.38
?
2.77 31.79
?
2.80 21.99
?
4.65 35.76
?
3.35 47.84
?
2.82 38.84
?
2.92
POSTECH 17.99 17.88
?
4.72 29.79
?
3.04 41.15
?
2.48 32.49
?
2.63 24.41
?
4.83 41.72
?
3.19 53.33
?
2.55 44.06
?
2.88
UEDIN 23.33 23.39
?
4.37 38.55
?
3.65 48.21
?
3.43 40.75
?
3.05 27.17
?
4.63 43.87
?
3.52 53.76
?
3.48 45.72
?
3.03
CUNI 10.54 10.39
?
3.48 18.86
?
2.48 26.65
?
2.05 20.53
?
2.08 32.39
?
5.45 56.79
?
3.02 65.52
?
2.26 57.96
?
2.56
CUNI 8.75 8.49
?
3.60 19.10
?
2.27 24.98
?
1.95 19.95
?
2.02 30.00
?
5.59 56.07
?
2.92 62.92
?
2.32 56.27
?
2.56
ONLINE 19.97
?
4.46 37.03
?
3.26 43.91
?
3.22 40.95
?
2.93 33.86
?
4.87 53.28
?
3.28 60.86
?
3.22 56.33
?
2.98
French?English
CUNI 13.90 13.79
?
3.61 18.49
?
2.55 28.35
?
2.81 20.36
?
2.20 34.97
?
5.34 59.54
?
2.94 72.30
?
2.63 58.86
?
2.76
CUNI 12.10 11.95
?
3.41 17.23
?
2.57 27.12
?
2.88 19.15
?
2.28 33.74
?
5.01 58.95
?
2.96 71.25
?
2.76 58.20
?
2.81
DCU-Q 30.85 31.24
?
5.08 58.88
?
2.97 67.94
?
2.62 59.19
?
2.62 36.88
?
5.07 66.38
?
2.85 75.86
?
2.37 66.29
?
2.55
DCU-Q 26.51 26.16
?
4.40 48.02
?
3.72 57.34
?
3.24 53.56
?
2.79 28.61
?
4.52 53.65
?
3.73 63.51
?
3.21 59.07
?
2.79
UEDIN 27.20 27.60
?
3.98 38.54
?
3.22 48.81
?
3.26 39.77
?
2.95 32.23
?
4.27 43.66
?
3.20 54.31
?
3.17 44.53
?
2.79
CUNI 14.03 14.00
?
3.30 20.11
?
2.38 29.00
?
2.71 21.62
?
2.22 38.98
?
5.08 62.90
?
2.87 74.49
?
2.45 62.12
?
2.64
CUNI 13.38 13.16
?
3.52 17.79
?
2.56 28.84
?
2.81 19.17
?
2.23 35.00
?
5.20 59.52
?
2.98 73.08
?
2.57 58.41
?
2.68
ONLINE 32.96
?
5.04 53.68
?
3.21 64.27
?
2.80 54.40
?
2.66 38.09
?
5.52 61.44
?
3.08 72.59
?
2.61 61.60
?
2.78
English?Czech
CUNI 8.37 8.00
?
3.65 17.74
?
2.23 26.46
?
1.96 19.48
?
2.10 19.49
?
4.60 41.53
?
2.94 51.34
?
2.51 42.54
?
2.74
CUNI 9.04 8.75
?
3.64 18.25
?
2.27 26.97
?
1.92 19.69
?
2.11 21.46
?
5.05 42.36
?
3.09 51.99
?
2.40 43.18
?
2.68
UEDIN 12.57 12.40
?
3.61 21.15
?
2.96 33.56
?
2.80 22.30
?
2.67 14.06
?
3.80 24.92
?
2.90 37.85
?
2.72 25.58
?
2.70
UEDIN 6.64 6.21
?
4.73 -2.35
?
3.06 5.95
?
3.48 -0.97
?
3.12 14.35
?
3.52 14.51
?
3.19 24.96
?
3.50 15.11
?
3.10
CUNI 9.06 8.64
?
3.82 19.92
?
2.24 26.97
?
1.94 20.82
?
2.06 22.42
?
5.24 44.89
?
2.94 52.89
?
2.40 45.36
?
2.78
CUNI 8.49 8.01
?
6.05 18.13
?
2.28 25.19
?
1.86 19.19
?
2.01 21.04
?
4.80 42.66
?
2.87 50.34
?
2.47 43.30
?
2.74
ONLINE 21.09
?
4.60 48.56
?
2.82 54.72
?
2.51 48.30
?
2.83 24.37
?
4.80 51.93
?
2.74 58.10
?
2.50 51.62
?
2.80
English?German
CUNI 10.17 10.01
?
3.92 26.48
?
3.24 36.71
?
3.37 29.26
?
2.96 13.02
?
4.17 31.96
?
3.41 42.39
?
3.21 34.61
?
2.95
CUNI 9.98 9.69
?
3.94 26.16
?
3.19 35.50
?
3.23 28.86
?
2.94 12.90
?
4.28 31.75
?
3.33 41.24
?
3.21 34.38
?
3.05
POSTECH 13.43 13.01
?
5.91 26.38
?
3.09 35.75
?
3.16 27.86
?
2.82 15.05
?
5.71 30.45
?
3.10 39.89
?
3.14 31.79
?
3.00
POSTECH 13.41 13.15
?
5.21 22.18
?
3.09 30.89
?
3.31 24.17
?
3.06 14.96
?
5.15 26.13
?
3.19 34.92
?
3.40 27.98
?
3.12
UEDIN 10.45 10.14
?
3.86 23.44
?
3.43 34.55
?
3.34 25.46
?
3.17 11.91
?
4.42 27.91
?
3.45 39.08
?
3.42 29.63
?
3.31
CUNI 8.91 7.72
?
6.48 30.05
?
3.22 40.65
?
2.71 31.91
?
2.88 13.66
?
5.37 35.51
?
3.28 46.12
?
2.74 37.27
?
3.01
CUNI 9.14 8.69
?
6.44 27.66
?
3.31 37.95
?
3.45 31.00
?
2.82 14.03
?
5.92 33.53
?
3.45 44.03
?
3.53 36.73
?
3.00
ONLINE 20.07
?
6.06 41.07
?
3.23 47.41
?
2.86 41.61
?
3.02 21.67
?
6.23 43.78
?
3.23 50.18
?
2.95 44.26
?
3.06
English?French
CUNI 13.12 12.92
?
2.84 21.95
?
2.41 33.19
?
2.09 23.70
?
2.24 28.42
?
3.98 51.43
?
2.90 63.74
?
2.35 52.64
?
2.58
CUNI 12.80 12.65
?
2.81 19.16
?
2.61 31.61
?
2.21 21.91
?
2.32 27.52
?
4.05 47.47
?
3.08 61.43
?
2.37 49.82
?
2.72
DCU-Q 27.69 27.84
?
4.11 48.97
?
3.06 60.90
?
2.55 51.84
?
2.83 28.98
?
4.16 51.73
?
3.10 63.84
?
2.47 54.43
?
2.76
UEDIN 20.16 21.76
?
3.42 31.66
?
4.23 44.37
?
4.13 44.29
?
2.73 23.25
?
3.49 35.38
?
4.19 48.52
?
4.07 47.94
?
2.75
CUNI 13.78 13.57
?
3.00 21.92
?
2.51 33.47
?
2.03 24.16
?
2.32 30.07
?
4.10 51.12
?
3.08 63.61
?
2.45 52.96
?
2.67
CUNI 15.27 15.24
?
3.12 23.58
?
2.54 34.39
?
2.54 25.79
?
2.32 31.40
?
4.15 53.60
?
2.96 65.39
?
2.57 55.47
?
2.69
ONLINE 28.93
?
3.66 49.20
?
3.08 60.85
?
2.69 51.68
?
2.78 30.88
?
3.66 52.25
?
3.08 64.06
?
2.62 54.59
?
2.68
Table 26: Official results of translation quality evaluation in the medical query translation subtask.
source lang. ID P@5 P@10 NDCG@5 NDCG@10 MAP Rprec bpref rel
Czech?English CUNI 0.3280 0.3340 0.2873 0.2936 0.2217 0.2362 0.3473 1461
German?English CUNI 0.2800 0.3000 0.2467 0.2630 0.2057 0.2077 0.3310 1426
French?English CUNI 0.3280 0.3380 0.2811 0.2882 0.2206 0.2284 0.3504 1481
DCU-Q 0.3480 0.3460 0.3060 0.3072 0.2252 0.2358 0.3659 1524
UEDIN 0.4440 0.4300 0.3793 0.3826 0.2843 0.2935 0.3936 1544
English (monolingual) 0.4600 0.4700 0.4091 0.4205 0.3035 0.3198 0.3858 1638
Table 27: Official results of retrieval evaluation in the query translation subtask.
47
lation Task, was the lack of domain expertise of
prospective raters. While in the standard task, the
only requirement for the raters was to be a na-
tive speaker of the target language, in the Med-
ical Translation Task, a very good knowledge of
the domain would be necessary to provide reli-
able judgements and the raters with such an ex-
pertise (medical doctors and native speakers) were
not available.
The complete results of the task are presented
in Table 25 (for summary translation) and Ta-
bles 26 and 27 (for query translation). Partici-
pant IDs given in bold indicate primary submis-
sions, IDs in normal font refer to contrastive sub-
missions. The first section for each translation di-
rection (white background) refers to constrained
submissions and the second one (light-gray back-
ground) to unconstrained submissions. The col-
umn denoted as ?original? contains BLEU scores
as reported by the Matrix submission system ob-
tained on the original submitted translations. Due
to punctuation inconsistency in the original refer-
ence translations, we decided to perform punctu-
ation normalization before calculating the official
scores. The columns denoted as ?normalized true-
cased? contain scores obtained on the submitted
translations after punctuation normalization and
the columns denoted as ?normalized lowercased?
contain scores obtained after punctuation normal-
ization and lowercasing. The normalization script
is available in the package with summary transla-
tion test data. The confidence intervals were ob-
tained by bootstrap resampling with a confidence
level of 95%. Figures in bold denote the best con-
strained system and, if its score is higher, the best
unconstrained system for each translation direc-
tion and each metric. For comparison, we also
present results of a major on-line translation sys-
tem (denoted as ONLINE).
The results of the extrinsic evaluation of query
translation submissions are given in 27. We used
the CLEF 2013 eHealth Task 3 test collection con-
taining about 1 million web pages (in English),
50 test queries (originally in English and trans-
lated to Czech, German, and French), and their
relevance assessments. Some of the participants
of the WMT Medical Task (three teams with five
submissions in total) submitted translations of the
queries (from Czech, German, and French) into
English and these translations were used to query
the CLEF 2013 eHealth Task 3 test collection us-
ing a state-of-the-art system based on a BM25
model, described in Pecina et al. (2014). Origi-
nally, we asked for 10 best translations for each
query, but only the best one were used for the
evaluation. The results are provided in terms of
standard IR evaluation measures: precision at a
cut-off of 5 and 10 documents (P@5, P@10),
normalized discounted cumulative gain (J?arvelin
and Kek?al?ainen, 2002) at 5 and 10 documents
(NDCG@5, NDCG@10), mean average precision
(MAP) (Voorhees and Harman, 2005), precision
reached after R documents retrieved, where R in-
dicates the number of the relevant documents for
each query in the entire collection (Rprec), binary
preference (bpref) (Buckley and Voorhees, 2004),
and number or relevant documents retrieved (rel).
The cross-lingual results are also compared with
the monolingual one (obtained by using the refer-
ence (English) translations of the test topics) to see
how the system would perform if the queries were
translated perfectly.
5.6 Discussion and Conclusion
Both the subtasks turned out to be quite challeng-
ing not only because of the specific domain ? in
summary sentences, we can observe much higher
density of terminology than in ordinary sentences;
the queries, which are also rich in terminology, do
not form sentences at all.
Most submissions were based on systems par-
ticipating in the standard Translation Task and
trained on the provided data or its subsets CUNI
provided baseline systems for all language pairs in
both subtasks, which turned to be relatively strong
for the query translation task, especially in trans-
lation to English, but only in terms of scores ob-
tained on normalized and lowercased translations
since their truecasing component did not perform
well.
In the summary translation subtask, the best
overall results were achieved by the UEDIN team
which won for DE?EN, EN?CS, and EN?FR, fol-
lowed by the UM-DA team, which performed on
par with UEDIN in all other translation.
The unconstrained submissions in almost all
cases did not outperform the results of the con-
strained submissions. Some improvements were
observed in the query translations subtasks by the
CUNI?s unconstrained system with language mod-
els trained on larger in-domain data.
The ONLINE system outperforms all other sub-
48
missions with only two exceptions ? the UM-DA?s
and UEDIN?s systems for the summary translation
in the FR?EN direction, though the score differ-
ences are within the 95% confidence interval.
In the query translation subtask, DCU-Q built
a system designed specifically for terminology
translation between French and English and out-
performed all other participants in translation into
English; however, the confidence intervals in the
query translation task are much wider and most of
the differences in scores of the automatic metrics
are not statistically significant.
The extrinsic evaluation in the cross-lingual in-
formation retrieval was conducted for translations
into English only. CUNI provided the baselines
for all directions, but other submissions were done
for FR?EN only. Here, the winner is UEDIN, who
outperformed both CUNI and DCU-Q, and their
scores are very close to those obtained using the
reference English translations.
Acknowledgments
This work was supported in parts by the
MosesCore, Casmacat, Khresmoi, Matecat and
QTLaunchPad projects funded by the European
Commission (7th Framework Programme), and by
gifts from Yandex.
We would also like to thank our colleagues Ma-
tou?s Mach?a?cek and Martin Popel for detailed dis-
cussions.
References
Avramidis, E. (2014). Efforts on machine learning
over human-mediated translation edit rate. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Beck, D., Shah, K., and Specia, L. (2014). Shef-
lite 2.0: Sparse multi-task gaussian processes
for translation quality estimation. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Bic?ici, E. (2013). Referential translation machines
for quality estimation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, Sofia, Bulgaria.
Bic?ici, E., Liu, Q., and Way, A. (2014). Parallel
FDA5 for fast deployment of accurate statisti-
cal machine translation systems. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, USA. Association
for Computational Linguistics.
Bicici, E., Liu, Q., and Way, A. (2014). Parallel
fda5 for fast deployment of accurate statistical
machine translation systems. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bicici, E. and Way, A. (2014). Referential transla-
tion machines for predicting translation quality.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Bojar, O., Buck, C., Callison-Burch, C., Feder-
mann, C., Haddow, B., Koehn, P., Monz, C.,
Post, M., Soricut, R., and Specia, L. (2013).
Findings of the 2013 Workshop on Statistical
Machine Translation. In Proceedings of the
Eighth Workshop on Statistical Machine Trans-
lation, pages 1?42, Sofia, Bulgaria. Association
for Computational Linguistics.
Bojar, O., Diatka, V., Rychl?y, P., Stra?n?ak, P.,
Tamchyna, A., and Zeman, D. (2014). Hindi-
English and Hindi-only Corpus for Machine
Translation. In Proceedings of the Ninth Inter-
national Language Resources and Evaluation
Conference, Reykjavik, Iceland. ELRA.
Bojar, O., Ercegov?cevi?c, M., Popel, M., and
Zaidan, O. (2011). A grain of salt for the WMT
manual evaluation. In Proceedings of the Sixth
Workshop on Statistical Machine Translation,
pages 1?11, Edinburgh, Scotland. Association
for Computational Linguistics.
Borisov, A. and Galinskaya, I. (2014). Yandex
school of data analysis russian-english machine
translation system for wmt14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Bouayad-Agha, N., Scott, D. R., and Power, R.
(2000). Integrating content and style in doc-
uments: A case study of patient information
leaflets. Information Design Journal, 9(2?
3):161?176.
Buckley, C. and Voorhees, E. M. (2004). Re-
trieval evaluation with incomplete information.
49
In Proceedings of the 27th Annual International
ACM SIGIR Conference on Research and De-
velopment in Information Retrieval, pages 25?
32, Sheffield, United Kingdom.
Buitelaar, P., Sacaleanu, B.,
?
Spela Vintar, Stef-
fen, D., Volk, M., Dejean, H., Gaussier, E.,
Widdows, D., Weiser, O., and Frederking, R.
(2003). Multilingual concept hierarchies for
medical information organization and retrieval.
Public deliverable, MuchMore project.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2007). (Meta-) evaluation
of machine translation. In Proceedings of the
Second Workshop on Statistical Machine Trans-
lation (WMT07), Prague, Czech Republic.
Callison-Burch, C., Fordyce, C., Koehn, P., Monz,
C., and Schroeder, J. (2008). Further meta-
evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Ma-
chine Translation (WMT08), Colmbus, Ohio.
Callison-Burch, C., Koehn, P., Monz, C., Pe-
terson, K., Przybocki, M., and Zaidan, O. F.
(2010). Findings of the 2010 joint workshop
on statistical machine translation and metrics
for machine translation. In Proceedings of the
Fourth Workshop on Statistical Machine Trans-
lation (WMT10), Uppsala, Sweden.
Callison-Burch, C., Koehn, P., Monz, C., Post, M.,
Soricut, R., and Specia, L. (2012). Findings of
the 2012 workshop on statistical machine trans-
lation. In Proceedings of the Seventh Workshop
on Statistical Machine Translation, pages 10?
51, Montr?eal, Canada. Association for Compu-
tational Linguistics.
Callison-Burch, C., Koehn, P., Monz, C., and
Schroeder, J. (2009). Findings of the 2009
workshop on statistical machine translation. In
Proceedings of the Fourth Workshop on Sta-
tistical Machine Translation (WMT09), Athens,
Greece.
Callison-Burch, C., Koehn, P., Monz, C., and
Zaidan, O. (2011). Findings of the 2011 work-
shop on statistical machine translation. In Pro-
ceedings of the Sixth Workshop on Statistical
Machine Translation, pages 22?64, Edinburgh,
Scotland.
Camargo de Souza, J. G., Gonz?alez-Rubio, J.,
Buck, C., Turchi, M., and Negri, M. (2014).
Fbk-upv-uedin participation in the wmt14 qual-
ity estimation shared-task. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Cap, F., Weller, M., Ramm, A., and Fraser, A.
(2014). Cims ? the cis and ims joint submis-
sion to wmt 2014 translating from english into
german. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Cohen, J. (1960). A coefficient of agreement for
nominal scales. Educational and Psychological
Measurment, 20(1):37?46.
Cohn, T. and Specia, L. (2013). Modelling an-
notator bias with multi-task gaussian processes:
An application to machine translation quality
estimation. In Proceedings of the 51st An-
nual Meeting of the Association for Compu-
tational Linguistics, ACL-2013, pages 32?42,
Sofia, Bulgaria.
Collins, M. (2002). Discriminative training meth-
ods for hidden markov models: theory and ex-
periments with perceptron algorithms. In Pro-
ceedings of the ACL-02 conference on Empir-
ical methods in natural language processing -
Volume 10, EMNLP ?02, pages 1?8, Strouds-
burg, PA, USA. Association for Computational
Linguistics.
Costa-juss`a, M. R., Gupta, P., Rosso, P., and
Banchs, R. E. (2014). English-to-hindi sys-
tem description for wmt 2014: Deep source-
context features for moses. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Do, Q. K., Herrmann, T., Niehues, J., Allauzen,
A., Yvon, F., and Waibel, A. (2014). The
kit-limsi translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Dungarwal, P., Chatterjee, R., Mishra, A.,
Kunchukuttan, A., Shah, R., and Bhattacharyya,
P. (2014). The iit bombay hindi-english transla-
tion system at wmt 2014. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
50
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014a). Edinburgh?s phrase-based machine
translation systems for wmt-14. In Proceedings
of the ACL 2014 Ninth Workshop of Statistical
Machine Translation, Baltimore, USA.
Durrani, N., Haddow, B., Koehn, P., and Heafield,
K. (2014b). Edinburghs phrase-based machine
translation systems for wmt-14. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Du?sek, O., Haji?c, J., Hlav?a?cov?a, J., Nov?ak, M.,
Pecina, P., Rosa, R., Tamchyna, A., Ure?sov?a,
Z., and Zeman, D. (2014). Machine transla-
tion of medical texts in the khresmoi project. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
Federmann, C. (2012). Appraise: An Open-
Source Toolkit for Manual Evaluation of Ma-
chine Translation Output. The Prague Bulletin
of Mathematical Linguistics (PBML), 98:25?
35.
Foster, J. (2007). Treebanks gone bad: Parser eval-
uation and retraining using a treebank of un-
grammatical sentences. International Journal
on Document Analysis and Recognition, 10(3-
4):129?145.
Freitag, M., Peitz, S., Wuebker, J., Ney, H., Huck,
M., Sennrich, R., Durrani, N., Nadejde, M.,
Williams, P., Koehn, P., Herrmann, T., Cho,
E., and Waibel, A. (2014). Eu-bridge mt:
Combined machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Gamon, M., Aue, A., and Smets, M. (2005).
Sentence-level MT evaluation without reference
translations: beyond language modeling. In
Proceedings of the Annual Conference of the
European Association for Machine Translation,
Budapest.
Geurts, P., Ernst, D., and Wehenkel, L. (2006). Ex-
tremely randomized trees. Machine Learning,
63(1):3?42.
Green, S., Cer, D., and Manning, C. (2014).
Phrasal: A toolkit for new directions in statis-
tical machine translation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hardmeier, C., Stymne, S., Tiedemann, J., Smith,
A., and Nivre, J. (2014). Anaphora models and
reordering for phrase-based smt. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Herbrich, R., Minka, T., and Graepel, T. (2006).
TrueSkill
TM
: A Bayesian Skill Rating Sys-
tem. In Proceedings of the Twentieth Annual
Conference on Neural Information Processing
Systems, pages 569?576, Vancouver, British
Columbia, Canada. MIT Press.
Herrmann, T., Mediani, M., Cho, E., Ha, T.-L.,
Niehues, J., Slawik, I., Zhang, Y., and Waibel,
A. (2014). The karlsruhe institute of technol-
ogy translation systems for the wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Hokamp, C., Calixto, I., Wagner, J., and Zhang,
J. (2014). Target-centric features for transla-
tion quality estimation. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Hopkins, M. and May, J. (2013). Models of trans-
lation competitions. In Proceedings of the 51st
Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers),
pages 1416?1424, Sofia, Bulgaria.
J?arvelin, K. and Kek?al?ainen, J. (2002). Cumu-
lated gain-based evaluation of ir techniques.
ACM Transactions on Information Systems,
20(4):422?446.
Kim, J.-D., Ohta, T., Tateisi, Y., and Tsujii, J.
(2003). GENIA corpus ? a semantically anno-
tated corpus for bio-textmining. Bioinformatics,
19(suppl 1):i180?i182.
Knox, C., Law, V., Jewison, T., Liu, P., Ly,
S., Frolkis, A., Pon, A., Banco, K., Mak, C.,
Neveu, V., Djoumbou, Y., Eisner, R., Guo,
A. C., and Wishart, D. S. (2011). DrugBank 3.0:
a comprehensive resource for Omics research
on drugs. Nucleic acids research, 39(suppl
1):D1035?D1041.
Koehn, P. (2012a). Simulating human judgment in
51
machine translation evaluation campaigns. In
International Workshop on Spoken Language
Translation (IWSLT).
Koehn, P. (2012b). Simulating Human Judgment
in Machine Translation Evaluation Campaigns.
In Proceedings of the Ninth International Work-
shop on Spoken Language Translation, pages
179?184, Hong Kong, China.
Koehn, P. and Monz, C. (2006). Manual and au-
tomatic evaluation of machine translation be-
tween European languages. In Proceedings of
NAACL 2006 Workshop on Statistical Machine
Translation, New York, New York.
Koppel, M. and Ordan, N. (2011). Translationese
and its dialects. In Proceedings of the 49th An-
nual Meeting of the Association for Computa-
tional Linguistics: Human Language Techolo-
gies, pages 1318?1326, Portland, Oregon.
Landis, J. R. and Koch, G. G. (1977). The mea-
surement of observer agreement for categorical
data. Biometrics, 33:159?174.
Leusch, G., Ueffing, N., and Ney, H. (2006). Cder:
Efficient mt evaluation using block movements.
In Proceedings of the 11th Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 241?248, Trento,
Italy.
Li, J., Kim, S.-J., Na, H., and Lee, J.-H. (2014a).
Postech?s system description for medical text
translation task. In Proceedings of the ACL
2014 Ninth Workshop of Statistical Machine
Translation, Baltimore, USA.
Li, L., Wu, X., Vaillo, S. C., Xie, J., Way, A., and
Liu, Q. (2014b). The dcu-ictcas mt system at
wmt 2014 on german-english translation task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Lopez, A. (2012). Putting Human Assessments of
Machine Translation Systems in Order. In Pro-
ceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 1?9, Montr?eal,
Canada. Association for Computational Lin-
guistics.
Lu, Y., Wang, L., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira, F. (2014). Domain adapta-
tion for medical text translation using web re-
sources. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
Luong, N. Q., Besacier, L., and Lecouteux, B.
(2014). Lig system for word level qe task at
wmt14. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Luong, N. Q., Lecouteux, B., and Besacier, L.
(2013). LIG system for WMT13 QE task: In-
vestigating the usefulness of features in word
confidence estimation for MT. In Proceedings
of the Eighth Workshop on Statistical Machine
Translation, pages 384?389, Sofia, Bulgaria.
Association for Computational Linguistics.
Mach?a?cek, M. and Bojar, O. (2014). Results of
the wmt14 metrics shared task. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Matthews, A., Ammar, W., Bhatia, A., Feely, W.,
Hanneman, G., Schlinger, E., Swayamdipta, S.,
Tsvetkov, Y., Lavie, A., and Dyer, C. (2014).
The cmu machine translation systems at wmt
2014. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Neidert, J., Schuster, S., Green, S., Heafield, K.,
and Manning, C. (2014). Stanford universitys
submissions to the wmt 2014 translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Okita, T., Vahid, A. H., Way, A., and Liu, Q.
(2014). Dcu terminology translation system for
medical query subtask at wmt14. In Proceed-
ings of the ACL 2014 Ninth Workshop of Statis-
tical Machine Translation, Baltimore, USA.
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
(2002). BLEU: a method for automatic eval-
uation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics, pages 311?318,
Philadelphia, PA, USA. Association for Com-
putational Linguistics.
P?echeux, N., Gong, L., Do, Q. K., Marie, B.,
Ivanishcheva, Y., Allauzen, A., Lavergne, T.,
52
Niehues, J., Max, A., and Yvon, Y. (2014).
LIMSI @ WMT?14 Medical Translation Task.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, USA.
Pecina, P., Du?sek, O., Goeuriot, L., Haji?c, J.,
Hlav?a?cov?a, J., Jones, G., Kelly, L., Leveling, J.,
Mare?cek, D., Nov?ak, M., Popel, M., Rosa, R.,
Tamchyna, A., and Ure?sov?a, Z. (2014). Adapta-
tion of machine translation for multilingual in-
formation retrieval in the medical domain. Arti-
ficial Intelligence in Medicine, (0):?.
Peitz, S., Wuebker, J., Freitag, M., and Ney, H.
(2014). The rwth aachen german-english ma-
chine translation system for wmt 2014. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Pouliquen, B. and Mazenc, C. (2011). COPPA,
CLIR and TAPTA: three tools to assist in over-
coming the patent barrier at WIPO. In Pro-
ceedings of the Thirteenth Machine Translation
Summit, pages 24?30, Xiamen, China. Asia-
Pacific Association for Machine Translation.
Powers, D. M. W. (2011). Evaluation: from preci-
sion, recall and f-measure to roc, informedness,
markedness & correlation. Journal of Machine
Learning Technologies.
Quernheim, D. and Cap, F. (2014). Large-scale ex-
act decoding: The ims-ttt submission to wmt14.
In Proceedings of the Ninth Workshop on Sta-
tistical Machine Translation, Baltimore, Mary-
land, USA. Association for Computational Lin-
guistics.
Rosse, C. and Mejino Jr., J. L. V. (2008). The
foundational model of anatomy ontology. In
Burger, A., Davidson, D., and Baldock, R., ed-
itors, Anatomy Ontologies for Bioinformatics,
volume 6 of Computational Biology, pages 59?
117. Springer London.
Rubino, R., Toral, A., S?anchez-Cartagena, V. M.,
Ferr?andez-Tordera, J., Ortiz Rojas, S., Ram??rez-
S?anchez, G., S?anchez-Mart??nez, F., and Way,
A. (2014). Abu-matran at wmt 2014 transla-
tion task: Two-step data selection and rbmt-
style synthetic rules. In Proceedings of the
Ninth Workshop on Statistical Machine Trans-
lation, Baltimore, Maryland, USA. Association
for Computational Linguistics.
Sakaguchi, K., Post, M., and Van Durme, B.
(2014). Efficient elicitation of annotations for
human evaluation of machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland.
S?anchez-Cartagena, V. M., P?erez-Ortiz, J. A., and
S?anchez-Mart??nez, F. (2014). The ua-prompsit
hybrid machine translation system for the 2014
workshop on statistical machine translation. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Scarton, C. and Specia, L. (2014). Exploring con-
sensus in machine translation for quality esti-
mation. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Schwartz, L., Anderson, T., Gwinnup, J., and
Young, K. (2014). Machine translation and
monolingual postediting: The afrl wmt-14 sys-
tem. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
Seginer, Y. (2007). Learning Syntactic Structure.
PhD thesis, University of Amsterdam.
Shah, K., Cohn, T., and Specia, L. (2013). An
investigation on the effectiveness of features for
translation quality estimation. In Proceedings
of the Machine Translation Summit XIV, pages
167?174, Nice, France.
Shah, K. and Specia, L. (2014). Quality estimation
for translation selection. In Proceedings of the
17th Annual Conference of the European As-
sociation for Machine Translation, Dubrovnik,
Croatia.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L.,
and Makhoul, J. (2006). A study of transla-
tion edit rate with targeted human annotation.
In Proceedings of the 7th Biennial Conference
of the Association for Machine Translation in
the Americas (AMTA-2006), Cambridge, Mas-
sachusetts.
Souza, J. G. C. d., Espl-Gomis, M., Turchi, M.,
and Negri, M. (2013). Exploiting qualitative in-
formation from automatic word alignment for
cross-lingual nlp tasks. In The 51st Annual
53
Meeting of the Association for Computational
Linguistics - Short Papers (ACL Short Papers
2013).
Specia, L., Shah, K., de Souza, J. G. C., and Cohn,
T. (2013). QuEst - A Translation Quality Esti-
mation Framework. In Proceedings of the 51th
Conference of the Association for Computa-
tional Linguistics (ACL), Demo Session, Sofia,
Bulgaria.
Tamchyna, A., Popel, M., Rosa, R., and Bojar, O.
(2014). Cuni in wmt14: Chimera still awaits
bellerophon. In Proceedings of the Ninth Work-
shop on Statistical Machine Translation, Balti-
more, Maryland, USA. Association for Compu-
tational Linguistics.
Tan, L. and Pal, S. (2014). Manawi: Using
multi-word expressions and named entities to
improve machine translation. In Proceedings
of the Ninth Workshop on Statistical Machine
Translation, Baltimore, Maryland, USA. Asso-
ciation for Computational Linguistics.
Thompson, P., Iqbal, S., McNaught, J., and Ana-
niadou, S. (2009). Construction of an annotated
corpus to support biomedical information ex-
traction. BMC bioinformatics, 10(1):349.
Tiedemann, J. (2009). News from OPUS ? a
collection of multilingual parallel corpora with
tools and interfaces. In Recent Advances in
Natural Language Processing, volume 5, pages
237?248, Borovets, Bulgaria. John Benjamins.
Tillmann, C., Vogel, S., Ney, H., Zubiaga, A.,
and Sawaf, H. (1997). Accelerated DP based
search for statistical translation. In Kokki-
nakis, G., Fakotakis, N., and Dermatas, E., edi-
tors, Proceedings of the Fifth European Confer-
ence on Speech Communication and Technol-
ogy, pages 2667?2670, Rhodes, Greece. Inter-
national Speech Communication Association.
U.S. National Library of Medicine (2009). UMLS
reference manual. Metathesaurus. Bethesda,
MD, USA.
Voorhees, E. M. and Harman, D. K., editors
(2005). TREC: Experiment and evaluation in
information retrieval, volume 63 of Digital li-
braries and electronic publishing series. MIT
press Cambridge, Cambridge, MA, USA.
Wang, L., Lu, Y., Wong, D. F., Chao, L. S., Wang,
Y., and Oliveira., F. (2014). Combining domain
adaptation approaches for medical text transla-
tion. In Proceedings of the ACL 2014 Ninth
Workshop of Statistical Machine Translation,
Baltimore, USA.
W?aschle, K. and Riezler, S. (2012). Analyz-
ing parallelism and domain similarities in the
MAREC patent corpus. In Salampasis, M. and
Larsen, B., editors, Multidisciplinary Informa-
tion Retrieval, volume 7356 of Lecture Notes
in Computer Science, pages 12?27. Springer
Berlin Heidelberg.
Williams, P., Sennrich, R., Nadejde, M., Huck, M.,
Hasler, E., and Koehn, P. (2014). Edinburghs
syntax-based systems at wmt 2014. In Proceed-
ings of the Ninth Workshop on Statistical Ma-
chine Translation, Baltimore, Maryland, USA.
Association for Computational Linguistics.
Wisniewski, G., P?echeux, N., Allauzen, A., and
Yvon, F. (2014). Limsi submission for wmt?14
qe task. In Proceedings of the Ninth Workshop
on Statistical Machine Translation, Baltimore,
Maryland, USA. Association for Computational
Linguistics.
wu, x., Haque, R., Okita, T., Arora, P., Way, A.,
and Liu, Q. (2014). Dcu-lingo24 participation
in wmt 2014 hindi-english translation task. In
Proceedings of the Ninth Workshop on Statisti-
cal Machine Translation, Baltimore, Maryland,
USA. Association for Computational Linguis-
tics.
Zde?nka Ure?sov?a, Ond?rej Du?sek, J. H. and Pecina,
P. (2014). Multilingual test sets for machine
translation of search queries for cross-lingual
information retrieval in the medical domain. In
To appear in Proceedings of the Ninth Interna-
tional Conference on Language Resources and
Evaluation, Reykjavik, Iceland.
Zhang, J., Wu, X., Calixto, I., Vahid, A. H., Zhang,
X., Way, A., and Liu, Q. (2014). Experiments in
medical translation shared task at wmt 2014. In
Proceedings of the ACL 2014 Ninth Workshop
of Statistical Machine Translation, Baltimore,
USA.
54
A Pairwise System Comparisons by Human Judges
Tables 28?37 show pairwise comparisons between systems for each language pair. The numbers in each
of the tables? cells indicate the percentage of times that the system in that column was judged to be better
than the system in that row, ignoring ties. Bolding indicates the winner of the two systems.
Because there were so many systems and data conditions the significance of each pairwise compar-
ison needs to be quantified. We applied the Sign Test to measure which comparisons indicate genuine
differences (rather than differences that are attributable to chance). In the following tables ? indicates sta-
tistical significance at p ? 0.10, ? indicates statistical significance at p ? 0.05, and ? indicates statistical
significance at p ? 0.01, according to the Sign Test.
Each table contains final rows showing how likely a system would win when paired against a randomly
selected system (the expected win ratio score) and the rank range according the official method used in
Table 8. Gray lines separate clusters based on non-overlapping rank ranges.
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
C
U
-
M
O
S
E
S
ONLINE-B ? .47? .43? .42? .39?
UEDIN-PHRASE .53? ? .44? .44? .41?
UEDIN-SYNTAX .57? .56? ? .49 .48?
ONLINE-A .58? .56? .51 ? .48?
CU-MOSES .61? .59? .52? .52? ?
score .57 .54 .47 .46 .44
rank 1 2 3-4 3-4 5
Table 28: Head to head comparison, ignoring ties, for Czech-English systems
C
U
-
D
E
P
F
I
X
U
E
D
I
N
-
U
N
C
N
S
T
R
C
U
-
B
O
J
A
R
C
U
-
F
U
N
K
Y
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
O
N
L
I
N
E
-
A
C
U
-
T
E
C
T
O
C
O
M
M
E
R
C
I
A
L
1
C
O
M
M
E
R
C
I
A
L
2
CU-DEPFIX ? .50 .42? .48 .44? .43? .41? .35? .30? .24?
UEDIN-UNCNSTR .50 ? .51 .48 .42? .37? .42? .39? .31? .26?
CU-BOJAR .58? .49 ? .49 .45? .44? .40? .36? .32? .24?
CU-FUNKY .52 .52 .51 ? .48 .47? .44? .34? .33? .26?
ONLINE-B .56? .58? .55? .52 ? .48 .47? .41? .31? .26?
UEDIN-PHRASE .57? .63? .56? .53? .52 ? .48 .44? .32? .27?
ONLINE-A .59? .58? .60? .56? .53? .52 ? .45? .37? .30?
CU-TECTO .65? .61? .64? .66? .59? .56? .55? ? .42? .30?
COMMERCIAL1 .70? .69? .68? .67? .69? .68? .63? .58? ? .40?
COMMERCIAL2 .76? .74? .76? .74? .74? .73? .70? .70? .60? ?
score .60 .59 .58 .57 .54 .52 .50 .44 .36 .28
rank 1-3 1-3 1-4 3-4 5-6 5-6 7 8 9 10
Table 29: Head to head comparison, ignoring ties, for English-Czech systems
55
O
N
L
I
N
E
-
B
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
A
L
I
M
S
I
-
K
I
T
E
U
-
B
R
I
D
G
E
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
R
W
T
H
D
C
U
-
I
C
T
C
A
S
C
M
U
R
B
M
T
4
R
B
M
T
1
O
N
L
I
N
E
-
C
ONLINE-B ? .46 .40? .41? .35? .42? .38? .35? .40? .31? .33? .32? .22?
UEDIN-SYNTAX .54 ? .51 .47 .47 .45 .45? .39? .36? .38? .35? .34? .27?
ONLINE-A .60? .49 ? .42? .44? .51 .41? .38? .44? .42? .38? .31? .20?
LIMSI-KIT .59? .53 .58? ? .55 .53 .31? .45? .39? .41? .37? .35? .29?
EU-BRIDGE .65? .53 .56? .45 ? .45 .44? .48 .40? .37? .39? .37? .30?
UEDIN-PHRASE .58? .55 .49 .47 .55 ? .48 .39? .34? .45? .40? .40? .34?
KIT .62? .55? .59? .69? .56? .52 ? .45? .41? .45? .47 .40? .31?
RWTH .65? .61? .62? .55? .52 .61? .55? ? .54 .44? .44? .38? .37?
DCU-ICTCAS .60? .64? .56? .61? .60? .66? .59? .46 ? .51 .49 .46? .40?
CMU .69? .62? .58? .59? .63? .55? .55? .56? .49 ? .53 .42? .43?
RBMT4 .67? .65? .62? .63? .61? .60? .53 .56? .51 .47 ? .51 .37?
RBMT1 .68? .66? .69? .65? .63? .60? .60? .62? .54? .58? .49 ? .38?
ONLINE-C .78? .73? .80? .71? .70? .66? .69? .63? .60? .57? .63? .62? ?
score .63 .58 .58 .55 .55 .54 .49 .47 .45 .44 .44 .40 .32
rank 1 2-3 2-3 4-6 4-6 4-6 7-8 7-8 9-11 9-11 9-11 12 13
Table 30: Head to head comparison, ignoring ties, for German-English systems
U
E
D
I
N
-
S
Y
N
T
A
X
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
S
T
A
N
F
O
R
D
E
U
-
B
R
I
D
G
E
R
B
M
T
4
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
1
K
I
T
S
T
A
N
F
O
R
D
-
U
N
C
C
I
M
S
S
T
A
N
F
O
R
D
U
U
O
N
L
I
N
E
-
C
I
M
S
-
T
T
T
U
U
-
D
O
C
E
N
T
UEDIN-SYNTAX ? .55? .46? .45? .46? .44? .41? .45? .43? .41? .38? .38? .36? .33? .38? .30? .30? .25?
ONLINE-B .45? ? .50 .48 .50 .47 .43? .46? .41? .45? .39? .39? .37? .32? .35? .34? .30? .29?
ONLINE-A .54? .50 ? .44? .52 .50 .45? .43? .43? .42? .39? .41? .42? .42? .37? .44? .38? .33?
PROMT-HYBRID .55? .52 .56? ? .45? .47 .47 .46? .50 .44? .42? .40? .41? .38? .39? .39? .33? .34?
PROMT-RULE .54? .50 .48 .55? ? .51 .47 .47 .45? .38? .42? .40? .43? .41? .43? .38? .35? .29?
UEDIN-STANFORD .56? .53 .50 .53 .49 ? .48 .50 .47 .44? .46 .36? .36? .36? .36? .35? .30? .32?
EU-BRIDGE .59? .57? .55? .53 .53 .52 ? .46? .43? .52 .42? .42? .45? .35? .36? .41? .38? .30?
RBMT4 .55? .54? .57? .54? .53 .50 .54? ? .53 .49 .44? .49 .50 .47 .40? .42? .38? .40?
UEDIN-PHRASE .57? .59? .57? .50 .55? .53 .57? .47 ? .50 .55? .47 .45? .44? .43? .42? .37? .34?
RBMT1 .59? .55? .58? .56? .62? .56? .48 .51 .50 ? .47 .47 .45? .47 .43? .42? .38? .41?
KIT .62? .61? .61? .58? .58? .54 .58? .56? .45? .53 ? .47 .49 .46 .43? .48 .34? .37?
STANFORD-UNC .62? .61? .59? .60? .60? .64? .58? .51 .53 .53 .53 ? .48 .47 .45? .45? .39? .41?
CIMS .64? .63? .58? .59? .57? .64? .55? .50 .55? .55? .51 .52 ? .53 .42? .52 .47 .42?
STANFORD .67? .68? .58? .62? .59? .64? .65? .53 .56? .53 .54 .53 .47 ? .53 .42? .39? .48
UU .62? .65? .62? .61? .57? .64? .64? .60? .57? .57? .57? .55? .58? .47 ? .46? .45? .38?
ONLINE-C .70? .66? .56? .61? .62? .65? .59? .58? .58? .58? .52 .55? .48 .58? .54? ? .48 .47
IMS-TTT .70? .70? .62? .67? .65? .70? .62? .62? .63? .62? .66? .61? .53 .61? .55? .52 ? .49
UU-DOCENT .75? .71? .67? .66? .71? .68? .70? .60? .66? .59? .63? .59? .58? .52 .62? .53 .51 ?
score .60 .59 .56 .56 .56 .56 .54 .51 .51 .50 .48 .47 .46 .44 .43 .42 .38 .37
rank 1-2 1-2 3-6 3-6 3-6 3-6 7 8-10 8-10 8-10 11-12 11-13 12-14 13-15 14-16 15-16 17-18 17-18
Table 31: Head to head comparison, ignoring ties, for English-German systems
56
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
O
N
L
I
N
E
-
B
S
T
A
N
F
O
R
D
O
N
L
I
N
E
-
A
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
UEDIN-PHRASE ? .48 .48 .45? .43? .28? .28? .19?
KIT .52 ? .54? .48 .44? .31? .29? .21?
ONLINE-B .52 .46? ? .51 .47 .31? .30? .24?
STANFORD .55? .52 .49 ? .46? .34? .30? .23?
ONLINE-A .57? .56? .53 .54? ? .32? .29? .21?
RBMT1 .72? .69? .69? .66? .68? ? .42? .33?
RBMT4 .72? .71? .70? .70? .71? .58? ? .39?
ONLINE-C .81? .79? .76? .77? .79? .67? .61? ?
score .63 .60 .59 .58 .57 .40 .35 .25
rank 1 2-4 2-4 2-4 5 6 7 8
Table 32: Head to head comparison, ignoring ties, for French-English systems
O
N
L
I
N
E
-
B
U
E
D
I
N
-
P
H
R
A
S
E
K
I
T
M
A
T
R
A
N
M
A
T
R
A
N
-
R
U
L
E
S
O
N
L
I
N
E
-
A
U
U
-
D
O
C
E
N
T
P
R
O
M
T
-
H
Y
B
R
I
D
U
A
P
R
O
M
T
-
R
U
L
E
R
B
M
T
1
R
B
M
T
4
O
N
L
I
N
E
-
C
ONLINE-B ? .46? .48 .46? .50 .41? .39? .39? .37? .38? .37? .35? .27?
UEDIN-PHRASE .54? ? .50 .47 .46 .46? .42? .41? .46? .42? .35? .34? .33?
KIT .52 .50 ? .53 .51 .50 .43? .49 .41? .42? .35? .37? .29?
MATRAN .54? .53 .47 ? .49 .50 .43? .43? .38? .48 .40? .34? .32?
MATRAN-RULES .50 .54 .49 .51 ? .53 .40? .45? .46? .42? .44? .40? .34?
ONLINE-A .59? .54? .50 .50 .47 ? .44? .49 .47 .45? .42? .37? .34?
UU-DOCENT .61? .58? .57? .57? .60? .56? ? .43? .52 .46? .39? .44? .33?
PROMT-HYBRID .61? .59? .51 .57? .55? .51 .57? ? .50 .41? .46? .44? .35?
UA .63? .54? .59? .62? .54? .53 .48 .50 ? .49 .46? .43? .34?
PROMT-RULE .62? .58? .58? .52 .58? .55? .54? .59? .51 ? .47 .39? .37?
RBMT1 .63? .65? .65? .60? .56? .58? .61? .54? .54? .53 ? .46? .45?
RBMT4 .65? .66? .63? .66? .60? .63? .56? .56? .57? .61? .54? ? .45?
ONLINE-C .73? .67? .71? .67? .66? .66? .67? .65? .66? .63? .55? .55? ?
score .59 .57 .55 .55 .54 .53 .49 .49 .48 .47 .43 .40 .34
rank 1 2-4 2-5 2-5 4-6 4-6 7-9 7-10 7-10 8-10 11 12 13
Table 33: Head to head comparison, ignoring ties, for English-French systems
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
S
Y
N
T
A
X
C
M
U
U
E
D
I
N
-
P
H
R
A
S
E
A
F
R
L
I
I
T
-
B
O
M
B
A
Y
D
C
U
-
L
I
N
G
O
2
4
I
I
I
T
-
H
Y
D
E
R
A
B
A
D
ONLINE-B ? .36? .33? .37? .31? .21? .20? .14? .00
ONLINE-A .64? ? .48 .47? .44? .31? .30? .24? .12?
UEDIN-SYNTAX .67? .52 ? .47 .46? .33? .29? .24? .12?
CMU .63? .53? .53 ? .47 .37? .31? .26? .11?
UEDIN-PHRASE .69? .56? .54? .53 ? .40? .33? .25? .11?
AFRL .79? .69? .67? .63? .60? ? .53 .40? .16?
IIT-BOMBAY .80? .70? .71? .69? .67? .47 ? .44? .19?
DCU-LINGO24 .86? .76? .76? .74? .75? .60? .56? ? .19?
IIIT-HYDERABAD .94? .88? .88? .89? .89? .84? .81? .81? ?
score .75 .62 .61 .60 .57 .44 .41 .34 .13
rank 1 2-3 2-4 3-4 5 6-7 6-7 8 9
Table 34: Head to head comparison, ignoring ties, for Hindi-English systems
57
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
U
E
D
I
N
-
U
N
C
N
S
T
R
U
E
D
I
N
-
P
H
R
A
S
E
C
U
-
M
O
S
E
S
I
I
T
-
B
O
M
B
A
Y
I
P
N
-
U
P
V
-
C
N
T
X
T
D
C
U
-
L
I
N
G
O
2
4
I
P
N
-
U
P
V
-
N
O
D
E
V
M
A
N
A
W
I
-
H
1
M
A
N
A
W
I
M
A
N
A
W
I
-
R
M
O
O
V
ONLINE-B ? .49 .28? .29? .27? .23? .22? .20? .17? .12? .13? .13?
ONLINE-A .51 ? .31? .29? .27? .25? .20? .20? .21? .19? .16? .15?
UEDIN-UNCNSTR .72? .69? ? .44? .49 .39? .40? .34? .39? .29? .30? .27?
UEDIN-PHRASE .71? .71? .56? ? .48 .45? .44? .39? .37? .31? .31? .32?
CU-MOSES .73? .73? .51 .52 ? .47 .42? .40? .45? .36? .35? .33?
IIT-BOMBAY .77? .75? .61? .55? .53 ? .50 .47 .45? .41? .40? .36?
IPN-UPV-CNTXT .78? .80? .60? .56? .58? .50 ? .51 .41? .40? .40? .37?
DCU-LINGO24 .80? .80? .66? .61? .60? .53 .49 ? .52 .41? .41? .39?
IPN-UPV-NODEV .83? .79? .61? .63? .55? .55? .59? .48 ? .46? .44? .38?
MANAWI-H1 .88? .81? .71? .69? .64? .59? .60? .59? .54? ? .35? .34?
MANAWI .87? .84? .70? .69? .65? .60? .60? .59? .56? .65? ? .39?
MANAWI-RMOOV .87? .85? .73? .68? .67? .64? .63? .61? .62? .66? .61? ?
score .77 .75 .57 .54 .52 .47 .46 .43 .42 .38 .35 .31
rank 1 2 3 4-5 4-5 6-7 6-7 8-9 8-9 10-11 10-11 12
Table 35: Head to head comparison, ignoring ties, for English-Hindi systems
A
F
R
L
-
P
E
O
N
L
I
N
E
-
B
O
N
L
I
N
E
-
A
P
R
O
M
T
-
H
Y
B
R
I
D
P
R
O
M
T
-
R
U
L
E
U
E
D
I
N
-
P
H
R
A
S
E
Y
A
N
D
E
X
O
N
L
I
N
E
-
G
A
F
R
L
U
E
D
I
N
-
S
Y
N
T
A
X
K
A
Z
N
U
R
B
M
T
1
R
B
M
T
4
AFRL-PE ? .42? .40? .39? .39? .41? .35? .39? .28? .26? .26? .29? .21?
ONLINE-B .58? ? .42? .43? .45? .45? .42? .43? .46? .37? .33? .29? .31?
ONLINE-A .60? .58? ? .50 .45? .51 .47 .45? .42? .40? .33? .32? .30?
PROMT-HYBRID .61? .57? .50 ? .47 .45? .49 .44? .43? .44? .39? .31? .27?
PROMT-RULE .61? .55? .55? .53 ? .46? .47 .49 .48 .42? .36? .34? .30?
UEDIN-PHRASE .59? .55? .49 .55? .54? ? .49 .50 .47 .44? .32? .37? .29?
YANDEX .65? .58? .53 .51 .53 .51 ? .48 .50 .43? .34? .36? .34?
ONLINE-G .61? .57? .55? .56? .51 .50 .52 ? .48 .43? .39? .35? .30?
AFRL .72? .54? .58? .57? .52 .53 .50 .52 ? .44? .41? .41? .37?
UEDIN-SYNTAX .74? .63? .60? .56? .58? .56? .57? .57? .56? ? .51 .36? .37?
KAZNU .74? .67? .67? .61? .64? .68? .66? .61? .59? .49 ? .44? .38?
RBMT1 .71? .71? .68? .69? .66? .63? .64? .65? .59? .64? .56? ? .47
RBMT4 .79? .69? .70? .73? .70? .71? .66? .70? .63? .63? .62? .53 ?
score .66 .58 .55 .55 .53 .53 .52 .51 .49 .45 .40 .36 .32
rank 1 2 3-5 3-5 4-7 5-8 5-8 5-8 9 10 11 12 13
Table 36: Head to head comparison, ignoring ties, for Russian-English systems
P
R
O
M
T
-
R
U
L
E
O
N
L
I
N
E
-
B
P
R
O
M
T
-
H
Y
B
R
I
D
U
E
D
I
N
-
U
N
C
N
S
T
R
O
N
L
I
N
E
-
G
O
N
L
I
N
E
-
A
U
E
D
I
N
-
P
H
R
A
S
E
R
B
M
T
4
R
B
M
T
1
PROMT-RULE ? .51 .45? .43? .43? .39? .38? .15? .00
ONLINE-B .49 ? .50 .47? .38? .36? .38? .16? .13?
PROMT-HYBRID .55? .50 ? .49 .47 .39? .40? .18? .15?
UEDIN-UNCNSTR .57? .53? .51 ? .50 .44? .36? .25? .18?
ONLINE-G .57? .62? .53 .50 ? .46? .44? .23? .18?
ONLINE-A .61? .64? .61? .56? .54? ? .49 .24? .18?
UEDIN-PHRASE .62? .62? .60? .64? .56? .51 ? .30? .21?
RBMT4 .85? .84? .82? .75? .77? .76? .70? ? .42?
RBMT1 .91? .87? .85? .82? .82? .82? .79? .58? ?
score .64 .64 .61 .58 .55 .51 .49 .26 .19
rank 1-2 1-2 3 4-5 4-5 6-7 6-7 8 9
Table 37: Head to head comparison, ignoring ties, for English-Russian systems
58
