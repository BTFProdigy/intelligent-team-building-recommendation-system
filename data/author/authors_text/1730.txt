Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 169?176
Manchester, August 2008
A classifier-based approach to preposition and determiner error
correction in L2 English
Rachele De Felice and Stephen G. Pulman
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford OX1 3QD, UK
{rachele.defelice|stephen.pulman}@comlab.ox.ac.uk
Abstract
In this paper, we present an approach to the
automatic identification and correction of
preposition and determiner errors in non-
native (L2) English writing. We show that
models of use for these parts of speech
can be learned with an accuracy of 70.06%
and 92.15% respectively on L1 text, and
present first results in an error detection
task for L2 writing.
1 Introduction
The field of research in natural language process-
ing (NLP) applications for L2 language is con-
stantly growing. This is largely driven by the ex-
panding population of L2 English speakers, whose
varying levels of ability may require different types
of NLP tools from those designed primarily for
native speakers of the language. These include
applications for use by the individual and within
instructional contexts. Among the key tools are
error-checking applications, focusing particularly
on areas which learners find the most challenging.
Prepositions and determiners are known to be one
of the most frequent sources of error for L2 En-
glish speakers, a finding supported by our analysis
of a small error-tagged corpus we created (deter-
miners 17% of errors, prepositions 12%). There-
fore, in developing a system for automatic error
detection in L2 writing, it seems desirable to focus
on these problematic, and very common, parts of
speech (POS).
This paper gives a brief overview of the prob-
lems posed by these POS and of related work. We
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
then present our proposed approach on both L1 and
L2 data and discuss the results obtained so far.
2 The problem
2.1 Prepositions
Prepositions are challenging for learners because
they can appear to have an idiosyncratic behaviour
which does not follow any predictable pattern even
across nearly identical contexts. For example, we
say I study in Boston but I study at MIT; or He is
independent of his parents, but dependent on his
son. As it is hard even for L1 speakers to articulate
the reasons for these differences, it is not surpris-
ing that learners find it difficult to master preposi-
tions.
2.2 Determiners
Determiners pose a somewhat different problem
from prepositions as, unlike them, their choice is
more dependent on the wider discourse context
than on individual lexical items. The relation be-
tween a noun and a determiner is less strict than
that between a verb or noun and a preposition, the
main factor in determiner choice being the specific
properties of the noun?s context. For example, we
can say boys like sport or the boys like sport, de-
pending on whether we are making a general state-
ment about all boys or referring to a specific group.
Equally, both she ate an apple and she ate the ap-
ple are grammatically well-formed sentences, but
only one may be appropriate in a given context, de-
pending on whether the apple has been mentioned
previously. Therefore, here, too, it is very hard to
come up with clear-cut rules predicting every pos-
sible kind of occurrence.
169
3 Related work
Although in the past there has been some research
on determiner choice in L1 for applications such as
generation and machine translation output, work to
date on automatic error detection in L2 writing has
been fairly limited. Izumi et al (2004) train a max-
imum entropy classifier to recognise various er-
rors using contextual features. They report results
for different error types (e.g. omission - precision
75.7%, recall 45.67%; replacement - P 31.17%,
R 8%), but there is no break-down of results by
individual POS. Han et al (2006) use a maxi-
mum entropy classifier to detect determiner errors,
achieving 83% accuracy. Chodorow et al (2007)
present an approach to preposition error detection
which also uses a model based on a maximum en-
tropy classifier trained on a set of contextual fea-
tures, together with a rule-based filter. They report
80% precision and 30% recall. Finally, Gamon et
al. (2008) use a complex system including a deci-
sion tree and a language model for both preposi-
tion and determiner errors, while Yi et al (2008)
propose a web count-based system to correct de-
terminer errors (P 62%, R 41%).
The work presented here displays some similar-
ities to the papers mentioned above in its use of a
maximum entropy classifier and a set of features.
However, our feature set is more linguistically so-
phisticated in that it relies on a full syntactic anal-
ysis of the data. It includes some semantic compo-
nents which we believe play a role in correct class
assignment.
4 Contextual models for prepositions and
determiners
4.1 Feature set
The approach proposed in this paper is based on
the belief that although it is difficult to formulate
hard and fast rules for correct preposition and de-
terminer usage, there is enough underlying regu-
larity of characteristic syntactic and semantic con-
texts to be able to predict usage to an acceptable
degree of accuracy. We use a corpus of grammat-
ically correct English to train a maximum entropy
classifier on examples of correct usage. The classi-
fier can therefore learn to associate a given prepo-
sition or determiner to particular contexts, and re-
liably predict a class when presented with a novel
instance of a context for one or the other.
The L1 source we use is the British National
Head noun ?apple?
Number singular
Noun type count
Named entity? no
WordNet category food, plant
Prep modification? yes, ?on?
Object of Prep? no
Adj modification? yes, ?juicy?
Adj grade superlative
POS ?3 VV, DT, JJS, IN, DT, NN
Table 1: Determiner feature set for Pick the juiciest
apple on the tree.
POS modified verb
Lexical item modified ?drive?
WordNet Category motion
Subcat frame pp to
POS of object noun
Object lexical item ?London?
Named entity? yes, type = location
POS ?3 NNP, VBD, NNP
Grammatical relation iobj
Table 2: Preposition feature set for John drove to
London.
Corpus (BNC) as we believe this offers a represen-
tative sample of different text types. We represent
training and testing items as vectors of values for
linguistically motivated contextual features. Our
feature vectors include 18 feature categories for
determiners and 13 for prepositions; the main ones
are illustrated in Table 1 and Table 2 respectively.
Further determiner features note whether the noun
is modified by a predeterminer, possessive, nu-
meral, and/or a relative clause, and whether it is
part of a ?there is. . . ? phrase. Additional preposi-
tion features refer to the grade of any adjectives or
adverbs modified (base, comparative, superlative)
and to whether the items modified are modified by
more than one PP
1
.
In De Felice and Pulman (2007), we described
some of the preprocessing required and offered
some motivation for this approach. As for our
choice of features, we aim to capture all the ele-
ments of a sentence which we believe to have an
effect on preposition and determiner choice, and
which can be easily extracted automatically - this
is a key consideration as all the features derived
rely on automatic processing of the text. Grammat-
ical relations refer to RASP-style grammatical re-
lations between heads and complements in which
the preposition occurs (see e.g. (Briscoe et al,
1
A full discussion of each feature, including motivation
for its inclusion and an assessment of its contribution to the
model, is found in De Felice (forthcoming).
170
Author Accuracy
Baseline 26.94%
Gamon et al 08 64.93%
Chodorow et al 07 69.00%
Our model 70.06%
Table 3: Classifier performance on L1 prepositions
2006)). Semantic word type information is taken
from WordNet lexicographer classes, 40 broad se-
mantic categories which all nouns and verbs in
WordNet belong to
2
(e.g. ?verb of motion?, ?noun
denoting food?), while the POStags are from the
Penn Treebank tagset - we note the POS of three
words either side of the target word
3
. For each
occurrence of a preposition or determiner in the
corpus, we obtain a feature vector consisting of
the preposition or determiner and its context, de-
scribed in terms of the features noted above.
5 Acquiring the models
5.1 Prepositions
At the moment, we restrict our analysis to the nine
most frequent prepositions in the data: at, by, for,
from, in, of, on, to, and with, to ensure a sufficient
amount of data for training. This gives a training
dataset comprising 8,898,359 instances. We use
a standard maximum entropy classifier
4
and do
not omit any features, although we plan to experi-
ment with different feature combinations to deter-
mine if, and how, this would impact the classifier?s
performance. Before testing our model on learner
data, it is important to ascertain that it can correctly
associate prepositions to a given context in gram-
matical, well-edited data. We therefore tested the
model on a section of the BNC not used in train-
ing, section J. Our best result to date is 70.06%
accuracy (test set size: 536,193). Table 3 relates
our results to others reported in the literature on
comparable tasks. The baseline refers to always
choosing the most frequent option, namely of.
We can see that our model?s performance com-
pares favourably to the best results in the literature,
although direct comparisons are hard to draw since
different groups train and test on different preposi-
tion sets and on different types of data (British vs.
American English, BNC vs. news reports, and so
2
No word sense disambiguation was performed at this
stage.
3
In NPs with a null determiner, the target is the head noun.
4
Developed by James Curran.
Proportion of training data Precision Recall
of 27.83% (2,501,327) 74.28% 90.47%
to 20.64% (1,855,304) 85.99% 81.73%
in 17.68% (1,589,718) 60.15% 67.60%
for 8.01% (720,369) 55.47% 43.78%
on 6.54% (587,871) 58.52% 45.81%
with 6.03% (541,696) 58.13% 46.33%
at 4.72% (424,539) 57.44% 52.12%
by 4.69% (421,430) 63.83% 56.51%
from 3.86% (347,105) 59.20% 32.07%
Table 4: L1 results - individual prepositions
on). Furthermore, it should be noted that Gamon
et al report more than one figure in their results,
as there are two components to their model: one
determining whether a preposition is needed, and
the other deciding what the preposition should be.
The figure reported here refers to the latter task,
as it is the most similar to the one we are evalu-
ating. Additionally, Chodorow et al also discuss
some modifications to their model which can in-
crease accuracy; the result noted here is the one
more directly comparable to our own approach.
5.1.1 Further discussion
To fully assess the model?s performance on the L1
data, it is important to consider factors such as per-
formance on individual prepositions, the relation-
ship between training dataset size and accuracy,
and the kinds of errors made by the model.
Table 4 shows the classifier?s performance on in-
dividual prepositions together with the size of their
training datasets. At first glance, a clear correlation
appears between the amount of data seen in train-
ing and precision and recall, as evidenced for ex-
ample by of or to, for which the classifier achieves
a very high score. In other cases, however, the cor-
relation is not so clear-cut. For example by has
one of the smallest data sets in training but higher
scores than many of the other prepositions, while
for is notable for the opposite reason, namely hav-
ing a large dataset but some of the lowest scores.
The absence of a definite relation between
dataset size and performance suggests that there
might be a cline of ?learnability? for these prepo-
sitions: different prepositions? contexts may be
more or less uniquely identifiable, or they may
have more or fewer senses, leading to less confu-
sion for the classifier. One simple way of verify-
ing the latter case is by looking at the number of
senses assigned to the prepositions by a resource
171
Target prep Confused with
at by for from in of on to with
at xx 4.65% 10.82% 2.95% 36.83% 19.46% 9.17% 10.28% 5.85%
by 6.54% xx 8.50% 2.58% 41.38% 19.44% 5.41% 10.04% 6.10%
for 8.19% 3.93% xx 1.91% 25.67% 36.12% 5.60% 11.29% 7.28%
from 6.19% 4.14% 6.72% xx 26.98% 26.74% 7.70% 16.45% 5.07%
in 7.16% 9.28% 10.68% 3.01% xx 43.40% 10.92% 8.96% 6.59%
of 3.95% 2.00% 18.81% 3.36% 40.21% xx 9.46% 14.77% 7.43%
on 5.49% 3.85% 8.66% 2.29% 32.88% 27.92% xx 12.20% 6.71%
to 9.77% 3.82% 11.49% 3.71% 24.86% 27.95% 9.43% xx 8.95%
with 3.66% 4.43% 12.06% 2.24% 28.08% 26.63% 6.81% 16.10% xx
Table 5: Confusion matrix for L1 data - prepositions
such as the Oxford English Dictionary. However,
we find no good correlation between the two as the
preposition with the most senses is of (16), and
that with the fewest is from (1), thus negating the
idea that fewer senses make a preposition easier
to learn. The reason may therefore be found else-
where, e.g. in the lexical properties of the contexts.
A good picture of the model?s errors can be
had by looking at the confusion matrix in Table 5,
which reports, for each preposition, what the clas-
sifier?s incorrect decision was. Analysis of these
errors may establish whether they are related to the
dataset size issue noted above, or have a more lin-
guistically grounded explanation.
From the table, the frequency effect appears evi-
dent: in almost every case, the three most frequent
wrong choices are the three most frequent prepo-
sitions, to, of, and in, although interestingly not in
that order, in usually being the first choice. Con-
versely, the less frequent prepositions are less of-
ten suggested as the classifier?s choice. This effect
precludes the possibility at the moment of draw-
ing any linguistic conclusions. These may only be
gleaned by looking at the errors for the three more
frequent prepositions. We see for example that
there seems to be a strong relation between of and
for, the cause of which is not immediately clear:
perhaps they both often occur within noun phrases
(e.g. book of recipes, book for recipes). More pre-
dictable is the confusion between to and from, and
between locative prepositions such as to and at, al-
though the effect is less strong for other potentially
confusable pairs such as in and at or on.
Table 6 gives some examples of instances where
the classifier?s chosen preposition differs from that
found in the original text. In most cases, the clas-
sifier?s suggestion is also grammatically correct,
Classifier choice Correct phrase
demands of the sector demands for. . .
condition for development condition of. . .
travel to speed travel at. . .
look at the USA look to. . .
Table 6: Examples of classifier errors on preposi-
tion L1 task
Author Accuracy
Baseline 59.83%
Han et al 06 83.00%
Gamon et al 08 86.07%
Turner and Charniak 07 86.74%
Our model 92.15%
Table 7: Classifier performance - L1 determiners
but the overall meaning of the phrases changes
somewhat. For example, while the demands of
the sector are usually made by the sector itself,
the demands for the sector suggest that someone
else may be making them. These are subtle dif-
ferences which it may be impossible to capture
without a more sophisticated understanding of the
wider context.
The example with travel, on the other hand,
yields an ungrammatical result. We assume that
the classifier has acquired a very strong link be-
tween the lexical item travel and the preposition to
that directs it towards this choice (cf. also the ex-
ample of look at/to). This suggests that individual
lexical items play an important role in preposition
choice along with other more general syntactic and
semantic properties of the context.
172
%of training data Prec. Recall
a 9.61% (388,476) 70.52% 53.50%
the 29.19% (1,180,435) 85.17% 91.51%
null 61.20% (2,475,014) 98.63% 98.79%
Table 8: L1 results - individual determiners
5.2 Determiners
For the determiner task, we also consider only the
three most frequent cases (a, the, null), which
gives us a training dataset consisting of 4,043,925
instances. We achieve accuracy of 92.15% on the
L1 data (test set size: 305,264), as shown in Ta-
ble 7. Again, the baseline refers to the most fre-
quent class, null.
The best reported results to date on determiner
selection are those in Turner and Charniak (2007).
Our model outperforms their n-gram language
model approach by over 5%. Since the two ap-
proaches are not tested on the same data this com-
parison is not conclusive, but we are optimistic that
there is a real difference in accuracy since the type
of texts used are not dissimilar. As in the case of
the prepositions, it is interesting to see whether this
high performance is equally distributed across the
three classes; this information is reported in Ta-
ble 8. Here we can see that there is a very strong
correlation between amount of data seen in train-
ing and precision and recall. The indefinite arti-
cle?s lower ?learnability?, and its lower frequency
appears not to be peculiar to our data, as it is also
found by Gamon et al among others.
The disparity in training is a reflection of the dis-
tribution of determiners in the English language.
Perhaps if this imbalance were addressed, the
model would more confidently learn contexts of
use for a, too, which would be desirable in view of
using this information for error correction. On the
other hand, this would create a distorted represen-
tation of the composition of English, which may
not be what we want in a statistical model of lan-
guage. We plan to experiment with smaller scale,
more similar datasets to ascertain whether the issue
is one of training size or of inherent difficulty in
learning about the indefinite article?s occurrence.
In looking at the confusion matrix for determin-
ers (Table 9), it is interesting to note that for the
classifier?s mistakes involving a or the, the erro-
neous choice is in the almost always the other de-
terminer rather than the null case. This suggests
that the frequency effect is not so strong as to over-
Target det Confused with
a the null
a xx 92.92% 7.08%
the 80.66% xx 19.34%
null 14.51% 85.49% xx
Table 9: Confusion matrix for L1 determiners
ride any true linguistic information the model has
acquired, otherwise the predominant choice would
always be the null case. On the contrary, these re-
sults show that the model is indeed capable of dis-
tinguishing between contexts which require a de-
terminer and those which do not, but requires fur-
ther fine tuning to perform better in knowing which
of the two determiner options to choose. Perhaps
the introduction of a discourse dimension might
assist in this respect. We plan to experiment with
some simple heuristics: for example, given a se-
quence ?Determiner Noun?, has the noun appeared
in the preceding few sentences? If so, we might
expect the to be the correct choice rather than a.
6 Testing the model
6.1 Working with L2 text
To evaluate the model?s performance on learner
data, we use a subsection of the Cambridge
Learner Corpus (CLC)
5
. We envisage our model to
eventually be of assistance to learners in analysing
their writing and identifying instances of preposi-
tion or determiner usage which do not correspond
to what it has been trained to expect; the more
probable instance would be suggested as a more
appropriate alternative. In using NLP tools and
techniques which have been developed with and
for L1 language, a loss of performance on L2 data
is to be expected. These methods usually expect
grammatically well-formed input; learner text is
often ungrammatical, misspelled, and different in
content and structure from typical L1 resources
such as the WSJ and the BNC.
6.2 Prepositions
For the preposition task, we extract 2523 instances
of preposition use from the CLC (1282 correct,
1241 incorrect) and ask the classifier to mark them
5
The CLC is a computerised database of contemporary
written learner English (currently over 25m words). It was
developed jointly by Cambridge ESOL and Cambridge Uni-
versity Press. The Cambridge Error Coding System has been
developed and applied manually to the data by Cambridge
University Press.
173
Instance type Accuracy
Correct 66.7%
Incorrect 70%
Table 10: Accuracy on L2 data - prepositions. Ac-
curacy on incorrect instances refers to the classifier
successfully identifying the preposition in the text
as not appropriate for that context.
as correct or incorrect. The results from this task
are presented in Table 10. These first results sug-
gest that the model is fairly robust: the accuracy
rate on the correct data, for example, is not much
lower than that on the L1 data. In an application
designed to assist learners, it is important to aim
to reduce the rate of false alarms - cases where the
original is correct, but the model flags an error - to
a minimum, so it is positive that this result is com-
paratively high. Accuracy on error identification is
at first glance even more encouraging. However, if
we look at the suggestions the model makes to re-
place the erroneous preposition, we find that these
are correct only 51.5% of the time, greatly reduc-
ing its usefulness.
6.2.1 Further discussion
A first analysis of the classifier?s decisions and its
errors points to various factors which could be im-
pairing its performance. Spelling mistakes in the
input are one of the most immediate ones. For ex-
ample, in the sentence I?m Franch, responsable on
the computer services, the classifier is not able to
suggest a correct alternative to the erroneous on:
since it does not recognise the adjective as a mis-
spelling of responsible, it loses the information as-
sociated with this lexical feature, which could po-
tentially determine the preposition choice.
A more complex problem arises when poor
grammar in the input misleads the parser so that
the information it gives for a sentence is incor-
rect, especially as regards PP attachment. In this
example, I wold like following equipment to my
speech: computer, modem socket and microphone,
the missing the leads the parser to treat following
as a verb, and believes it to be the verb to which the
preposition is attached. It therefore suggests from
as a correction, which is a reasonable choice given
the frequency of phrases such as to follow from.
However, this was not what the PP was meant
to modify: impaired performance from the parser
could be a significant negative factor in the model?s
performance. It would be interesting to test the
model on texts written by students of different lev-
els of proficiency, as their grammar may be more
error-free and more likely to be parsed correctly.
Alternatively, we could modify the parser so as to
skip cases where it requires several attempts before
producing a parse, as these more challenging cases
could be indicative of very poorly structured sen-
tences in which misused prepositions are depen-
dent on more complex errors.
A different kind of problem impacting our accu-
racy scores derives from those instances where the
classifier selects a preposition which can be cor-
rect in the given context, but is not the correct one
in that particular case. In the example I received
a beautiful present at my birthday, the classifier
identifies the presence of the error, and suggests
the grammatically and pragmatically appropriate
correction for. The corpus annotators, however,
indicate on as the correct choice. Since we use
their annotations as the benchmark against which
to evaluate the model, this instance is counted as
the classifier being wrong because it disagrees with
the annotators. A better indication of the model?s
performance may be to independently judge its de-
cisions, to avoid being subject to the annotators?
bias. Finally, we are beginning to look at the rela-
tions between preposition errors and other types of
error such as verb choice, and how these are anno-
tated in the data.
An overview of the classifier?s error patterns for
the data in this task shows that they are largely sim-
ilar to those observed in the L1 data. This sug-
gests that the gap in performance between L1 and
L2 is due more to the challenges posed by learner
text than by inherent shortcomings in the model,
and therefore that the key to better performance
is likely to lie in overcoming these problems. In
future work we plan to use L2 data where some
of the spelling errors and non-preposition or deter-
miner errors have been corrected so that we can
see which of the other errors are worth focussing
on first.
6.3 Determiners
Our work on determiner error correction is still in
the early stages. We follow a similar procedure to
the prepositions task, selecting a number of both
correct and incorrect instances. On the former (set
size 2000) accuracy is comparable to that on L1
data: 92.2%. The danger of false alarms, then, ap-
pears not to be as significant as for the prepositions
174
task. On the incorrect instances (set size ca. 1200),
however, accuracy is less than 10%.
Preliminary error analysis shows that the model
is successful at identifying cases of misused deter-
miner, e.g. a for the or vice versa, doing so in over
two-thirds of cases. However, by far the most fre-
quent error type for determiners is not confusion
between indefinite and definite article, but omitting
an article where one is needed. At the moment, the
model detects very few of these errors, no doubt in-
fluenced by the preponderance of null cases seen
in training. Furthermore, some of the issues raised
earlier in discussing the application of NLP tools
to L2 language hold for this task, too.
In addition to those, though, in this task more
than for prepositions we believe that differences in
text type between the training texts - the BNC -
and the testing material - learner essays - has a sig-
nificant negative effect on the model. In this task,
the lexical items play a crucial role in class assign-
ment. If the noun in question has not been seen in
training, the classifier may be unable to make an
informed choice. Although the BNC comprises a
wide variety of texts, there may not be a sufficient
number covering topics typical of learner essays,
such as ?business letters? or ?postcards to penpals?.
Also, the BNC was created with material from al-
most 20 years ago, and learners writing in contem-
porary English may use lexical items which are not
very frequently seen in the BNC. A clear exam-
ple of this discrepancy is the noun internet, which
requires the definite article in English, but not in
several other languages, leading to countless sen-
tences such as I saw it in internet, I booked it on
internet, and so on. This is one of the errors the
model never detects: a fact which is not surpris-
ing when we consider that this noun occurs only
four times in the whole of the training data. It may
be therefore necessary to consider using alternative
sources of training data to overcome this problem
and improve the classifier?s performance.
7 Comparison to human learners
In developing this model, our first aim was not to
create something which learns like a human, but
something that works in the best and most effi-
cient possible way. However, it is interesting to
see whether human learners and classifiers display
similar patterns of errors in preposition choice.
This information has twofold value: as well as be-
ing of pedagogical assistance to instructors of En-
glish L2, were the classifier to display student-like
error patterns, insights into ?error triggers? could
be derived from the L2 pedagogical literature to
improve the classifier. The analysis of the types
of errors made by human learners yields some in-
sights which might be worthy of further investi-
gation. A clear one is the confusion between the
three locative and temporal prepositions at, in, and
on (typical sentence: The training programme will
start at the 1st August). This type of error is made
often by both learners and the model on both types
of data, suggesting that perhaps further attention
to features might be necessary to improve discrim-
ination between these three prepositions.
There are also interesting divergences. For ex-
ample, a common source of confusion in learners
is between by and from, as in I like it because
it?s from my favourite band. However, this confu-
sion is not very frequent in the model, a difference
which could be explained either by the fact that,
as noted above, performance on from is very low
and so the classifier is unlikely to suggest it, or that
in training the contexts seen for by are sufficiently
distinctive that the classifier is not misled like the
learners.
Finally, a surprising difference comes from
looking at what to is confused with. The model
often suggests at where to would be correct. This
is perhaps not entirely unusual as both can occur
with locative complements (one can go to a place
or be at a place) and this similarity could be con-
fusing the classifier. Learners, however, although
they do make this kind of mistake, are much more
hampered by the confusion between for and to, as
in She was helpful for me or This is interesting
for you. In other words, for learners it seems that
the abstract use of this preposition, its benefactive
sense, is much more problematic than the spatial
sense. We can hypothesise that the classifier is less
distracted by these cases because the effect of the
lexical features is stronger.
A more detailed discussion of the issues arising
from the comparison of confusion pairs cannot be
had here. However, in noting both divergences and
similarities between the two learners, human and
machine, we may be able to derive useful insights
into the way the learning processes operate, and
what factors could be more or less important for
them.
175
8 Conclusions and future directions
This paper discussed a contextual feature based
approach to the automatic acquisition of models
of use for prepositions and determiners, which
achieve an accuracy of 70.06% and 92.15% re-
spectively, and showed how it can be applied to an
error correction task for L2 writing, with promis-
ing early results. There are several directions that
can be pursued to improve accuracy on both types
of data. The classifier can be further fine-tuned to
acquire more reliable models of use for the two
POS. We can also experiment with its confidence
thresholds, for example allowing it to make an-
other suggestion when its confidence in its first
choice is low. Furthermore, issues relating to the
use of NLP tools with L2 data must be addressed,
such as factoring out spelling or other errors in the
data, and perhaps training on text types which are
more similar to the CLC. In the longer term, we
also envisage mining the information implicit in
our training data to create a lexical resource de-
scribing the statistical tendencies observed.
Acknowledgements
We wish to thank Stephen Clark and Laura Rimell for stim-
ulating discussions and the anonymous reviewers for their
helpful comments. We acknowledge Cambridge University
Press?s assistance in accessing the Cambridge Learner Corpus
data. Rachele De Felice was supported by an AHRC scholar-
ship for the duration of her studies.
References
Briscoe, Ted, John Carroll, and Rebecca Watson.
2006. The second release of the RASP system. In
COLING-ACL 06 Demo Session.
Chodorow, Martin, Joel Tetreault, and Na-Rae Han.
2007. Detection of grammatical errors involv-
ing prepositions. In Proceedings of the 4th ACL-
SIGSEM Workshop on Prepositions.
De Felice, Rachele and Stephen Pulman. 2007. Au-
tomatically acquiring models of preposition use. In
Proceedings of the 4th ACL-SIGSEM Workshop on
Prepositions.
De Felice, Rachele. forthcoming. Recognising prepo-
sition and determiner errors in learner English.
Ph.D. thesis, Oxford University Computing Labora-
tory.
Gamon, M., J. Gao, C. Brockett, A. Klementiev,
W. Dolan, D. Belenko, and L. Vanderwende. 2008.
Using contextual speller techniques and language
modeling for ESL error correction. In Proceedings
of IJCNLP.
Han, Na-Rae, Martin Chodorow, and Claudia Leacock.
2006. Detecting errors in English article usage by
non-native speakers. Natural Language Engineer-
ing, 12(1):115?129.
Izumi, Emi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004. SST speech corpus of Japanese learners?
English and automatic detection of learners? errors.
ICAME, 28:31?48.
Turner, Jenine and Eugen Charniak. 2007. Language
modeling for determiner selection. In NAACL-HLT
Companion volume.
Yi, Xing, Jianfeng Gao, and William Dolan. 2008. A
web-based English proofing system for ESL users.
In Proceedings of IJCNLP.
176
Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 45?50,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Automatically acquiring models of preposition use
Rachele De Felice and Stephen G. Pulman
Oxford University Computing Laboratory
Wolfson Building, Parks Road, Oxford OX1 3QD, UK
{rachele.defelice|stephen.pulman}@comlab.ox.ac.uk
Abstract
This paper proposes a machine-learning
based approach to predict accurately, given
a syntactic and semantic context, which
preposition is most likely to occur in that
context. Each occurrence of a preposition in
an English corpus has its context represented
by a vector containing 307 features. The
vectors are processed by a voted perceptron
algorithm to learn associations between con-
texts and prepositions. In preliminary tests,
we can associate contexts and prepositions
with a success rate of up to 84.5%.
1 Introduction
Prepositions have recently become the focus of
much attention in the natural language processing
community, as evidenced for example by the ACL
workshops, a dedicated Sem-Eval task, and The
Preposition Project (TPP, Litkowski and Hargraves
2005). This is because prepositions play a key role
in determining the meaning of a phrase or sentence,
and their correct interpretation is crucial for many
NLP applications: AI entities which require spatial
awareness, natural language generation (e.g. for au-
tomatic summarisation, QA, MT, to avoid generat-
ing sentences such as *I study at England), auto-
matic error detection, especially for non-native En-
glish speakers. We present here an approach to
learning which preposition is most appropriate in a
given context by representing the context as a vector
populated by features referring to its syntactic and
semantic characteristics. Preliminary tests on five
prepositions - in, of, on, to, with - yield a success
rate of between 71% and 84.5%. In Section 2, we il-
lustrate our motivations for using a vector-based ap-
proach. Section 3 describes the vector creation, and
Section 4 the learning procedure. Section 5 presents
a discussion of some preliminary results, and Sec-
tion 6 offers an assessment of our method.
2 Contextual features
Modelling preposition use is challenging because it
is often difficult to explain why in two similar con-
texts a given preposition is correct in one but not the
other. For example, we say A is similar to B, but dif-
ferent from C, or we study in England, but at King?s
College. Nor can we rely on co-occurrence with par-
ticular parts of speech (POS), as most prepositions
have a reasonably wide distribution. Despite this
apparently idiosyncratic behaviour, we believe that
prepositional choice is governed by a combination
of several syntactic and semantic features. Contexts
of occurrence can be represented by vectors; a ma-
chine learning algorithm trained on them can predict
with some confidence, given a new occurrence of a
context vector, whether a certain preposition is ap-
propriate in that context or not.
We consider the following macro-categories of
features to be relevant: POS being modified; POS of
the preposition?s complement; given a RASP-style
grammatical relation output (GR; see e.g. Briscoe
et al 2006), what GRs the preposition occurs in;
named entity (NE) information - whether the mod-
ified or complement items are NEs; WordNet in-
formation - to which of the WordNet lexicographer
45
classes1 the modified and complement nouns and
verbs belong; immediate context - POS tags of ?2
word window around the preposition. For example,
given a sentence such as John drove to Cambridge,
we would note that this occurrence of the preposi-
tion to modifies a verb, its complement is a location
NE noun, the verb it modifies is a ?verb of motion?,
the tags surrounding it are NNP, VBD, NNP2, and it
occurs in the relation ?iobj? with the verb, and ?dobj?
with the complement noun.
Our 307-feature set aims to capture all the salient
elements of a sentence which we believe could be in-
volved in governing preposition choice, and which
can be accurately recognised automatically. Our
choice of features is provisional but based on a study
of errors frequently made by learners of English:
however, when we spot a misused preposition, it of-
ten takes some reflection to understand which ele-
ments of the sentence are making that preposition
choice sound awkward, and thus we have erred on
the side of generosity. In some cases it is easier: we
observe that in the earlier example England is a loca-
tion NE while King?s College is an organisation NE:
this distinction may be the trigger for the difference
in preposition choice.
3 Vector construction
The features are acquired from a version of the
British National Corpus (BNC) processed by the
C&C tools pipeline (Clark and Curran, to appear).
The output of the C&C tools pipeline, which in-
cludes stemmed words, POS tags, NER, GRs and
Combinatory Categorial Grammar (CCG) deriva-
tions of each sentence, is processed by a Python
script which, for each occurrence of a preposition in
a sentence, creates a vector for that occurrence and
populates it with 0s and 1s according to the absence
or presence of each feature in its context. Each vec-
tor therefore represents a corpus-seen occurrence of
a preposition and its context. For each preposition
we then construct a dataset to be processed by a ma-
chine learning algorithm, containing all the vectors
which do describe that preposition?s contexts, and
an equal number of those which do not: our hypoth-
1These are 41 broad semantic categories (e.g. ?noun denot-
ing a shape?, ?verb denoting a cognitive process?) to which all
nouns and verbs in WordNet are assigned.
2Penn Treebank tagset.
esis is that these will be sufficiently different from
the ?positive? contexts that a machine learning algo-
rithm will be able to associate the positive vectors
more strongly to that preposition.
4 Testing the approach
To test our approach, we first experimented with
a small subset of the BNC, about 230,000 words
(9993 sentences, of which 8997 contained at least
one preposition). After processing we were left with
over 33,000 vectors associated with a wide range of
prepositions. Of course there is a certain amount of
noise: since the vectors describe what the parser has
tagged as prepositions, if something has been mis-
tagged as one, then there will be a vector for it. Thus
we find in our data vectors for things such as if and
whether, which are not generally considered prepo-
sitions, and occasionally even punctuation items are
misanalysed as prepositions; however, these repre-
sent only a small fraction of the total and so do not
constitute a problem.
Even with a relatively large number of vectors,
data sparseness is still an issue and for many prepo-
sitions we did not find a large number of occurrences
in our dataset. Because of this, and because this
is only a preliminary, small-scale exploration of the
feasibility of this approach, we decided to initially
focus on only 5 common prepositions3 : in (4278 oc-
currences), of (7485), on (1483), to (48414), with
(1520). To learn associations between context vec-
tors and prepositions, we use the Voted Perceptron
algorithm (Freund and Schapire 1999). At this stage
we are only interested in establishing whether a
preposition is correctly associated with a given con-
text or not, so a binary classifier such as the Voted
Perceptron is well-suited for our task. At a later
stage we aim to expand this approach so that a noti-
fication of error or inappropriateness is paired with
suggestions for other, more likely prepositions. A
possible implementation of this is the output of a
3These prepositions often occur in compound prepositions
such as in front of ; their inclusion in the data could yield mis-
leading results. However out of 33,339 vectors, there were only
463 instances of compound prepositions, so we do not find their
presence skews the results.
4Here to includes occurrences as an infinitival marker. This
is because the tagset does not distinguish between the two oc-
currences; also, with a view to learner errors, its misuse as both
a preposition and an infinitival marker is very common.
46
ranked list of the probability of each preposition oc-
curring in the context under examination, especially
as of course there are many cases in which more
than one preposition is possible (cf. the folder on
the briefcase vs. the folder in the briefcase).
We use the Weka machine learning package to run
the Voted Perceptron. Various parameters can be
modified to obtain optimal performance: the num-
ber of epochs the perceptron should go through, the
maximum number of perceptrons allowed, and the
exponent of the polynomial kernel function (which
allows a linear function such as the perceptron to
deal with non-linearly separable data), as well as,
of course, different combinations of vector features.
We are experimenting with several permutations of
these factors to ascertain which combination gives
the best performance. Preliminary results obtained
so far show an average accuracy of 75.6%.
5 Results and Discussion
We present here results from two of the experiments,
which consider two possible dimensions of varia-
tion: the polynomial function exponent, d, and the
presence of differing subsets of features: WordNet
or NE information and the ?2 POS tag window.
Tests were run 10 times in 10-fold cross-validation.
5.1 The effect of the d value
The value of d is widely acknowledged in the litera-
ture to play a key role in improving the performance
of the learning algorithm; the original experiment
described in Freund and Schapire (1999) e.g. reports
results using values of d from 1 to 6, with d=2 as
the optimal value. Therefore our first investigation
compared performance with values for d set to d=1
and d=2, with the other parameters set to 10 epochs
and 10,000 as the maximum number of perceptrons
allowed (Table 1).
We can see that the results, as a first attempt at
this approach, are encouraging, achieving a success
rate of above 80% in two cases. Performance on on
is somewhat disappointing, prompting the question
whether this is because less data was available for it
(although with, with roughly the same sized dataset,
performs better), or if there is something intrinsic to
the syntactic and semantic properties of this prepo-
sition that makes its use harder to pinpoint. The
average performance of 75.6 - 77% is a promising
starting point, and offers a solid base on which to
proceed with a finer tuning of the various parame-
ters, including the feature set, which could lead to
better results. The precision and recall support our
confidence in this approach, as there are no great dif-
ferences between the two in any dataset: this means
that the good results we are achieving are not com-
ing at the expense of one or the other measure.
If we compare results for the two values of d, we
note that, contrary to expectations, there is no dra-
matic improvement. In most cases it is between less
than 1% and just over that; only on shows a marked
improvement of 4%. However, a positive trend is
evident, and we will continue experimenting with
variations on this parameter?s value to determine its
optimal setting.
5.2 The effect of various feature categories
As well as variations on the learning algorithm it-
self, we also investigate how different types of fea-
tures affect performance. This is interesting not only
from a processing perspective - if some features are
not adding any useful information then they may be
disregarded, thus speeding up processing time - but
also from a linguistic one. If we wish to use insights
from our work to assist in the description of preposi-
tion use, an awareness of the extent to which differ-
ent elements of language contribute to preposition
choice is clearly of great importance.
Here we present some results using datasets in
which we have excluded various combinations of the
NE, WordNet and POS tag features. The WordNet
and POS macrocategories of features are the largest
sets - when both are removed, the vector is left with
only 31 features - so it is interesting to note how this
affects performance. Furthermore, the WordNet in-
formation is in a sense the core ?lexical semantics?
component, so its absence allows for a direct com-
parison between a model ?with semantics? and one
without. However, the WordNet data is also quite
noisy. Many lexical items are assigned to several
categories, because we are not doing any sense res-
olution on our data. The POS tag features represent
?context? in its most basic sense, detached from strict
syntactic and semantic considerations; it is useful to
examine the contribution this type of less sophisti-
cated information can make.
47
d=1 d=2
Preposition %correct Precision Recall F-score %correct Precision Recall F-score
in 76.30% 0.75 0.78 0.77 76.61% 0.77 0.77 0.77
of 83.64% 0.88 0.78 0.83 84.47% 0.87 0.81 0.84
on 65.66% 0.66 0.65 0.65 69.09% 0.69 0.69 0.69
to 81.42% 0.78 0.87 0.82 82.43% 0.81 0.85 0.83
with 71.25% 0.73 0.69 0.70 72.88% 0.73 0.72 0.73
av. 75.65% 0.76 0.75 0.75 77.10% 0.77 0.77 0.77
Table 1: The effect of the d value
All features No W.Net No POS No NER No WN + POS GRs only
% correct 83.64% 83.47% 81.46% 83.33% 81.00% 81.46%
Precision 0.88 0.89 0.76 0.88 0.74 0.93
Recall 0.78 0.76 0.91 0.77 0.94 0.68
F-score 0.83 0.82 0.83 0.82 0.83 0.78
Table 2: OF: the effect of various feature categories (d=1)
Full results cannot be presented due to space re-
strictions: we present those for ?of?, which are rep-
resentative. In almost case, the dataset with all fea-
tures included is the one with the highest percentage
of correct classifications, so all features do indeed
play a role in achieving the final result. However,
among the various sets variation is of just 1 or 2%,
nor do f-scores vary much. There are some interest-
ing alternations in the precision and recall scores and
a closer investigation of these might provide some
insight into the part played by each set of features:
clearly there are some complex interactions between
them rather than a simple monotonic combination.
Such small variations allow us to conclude that
these sets of features are not hampering peformance
(because their absence does not in general lead to
better results), but also that they may not be a major
discriminating factor in preposition choice: gram-
matical relations seem to be the strongest feature -
only 18 components of the vector! This does not
imply that semantics, or the immediate context of a
word, play no role: it may just be that the way this
data is captured is not the most informative for our
purposes. However, we must also consider if some-
thing else in the feature set is impeding better perfor-
mance, or if this is the best we can achieve with these
parameters, and need to identify more informative
features. We are currently working on expanding
the feature set, considering e.g. subcategorisation
information for verbs, as well as experimenting with
the removal of other types of features, and using the
WordNet data differently. On the other hand, we also
observe that each macrocategory of features does
contribute something to the final result. This could
suggest that there is no one magic bullet-like feature
which definitely and faultlessly identifies a preposi-
tion but rather, as indeed we know by the difficulties
encountered in finding straightforward identification
criteria for prepositions, this depends on a complex
interrelation of features each of which contributes
something to the whole.
6 Evaluation and related work
6.1 Error detection evaluation
One of our motivations in this work was to inves-
tigate the practical utility of our context models in
an error detection task. The eventual aim is to be
able, given a preposition context, to predict the most
likely preposition to occur in it: if that differs from
the one actually present, we have an error. Using
real learner English as testing material at our current
stage of development is too complex, however. This
kind of text presents several challenges for NLP and
for our task more specifically, such as spelling mis-
takes - misspelled words would not be recognised
by WordNet or any other lexical item-based com-
ponent. Furthermore, often a learner?s error cannot
simply be described in terms of one word needing
to be replaced by another, but has a more complex
structure. Although it is our intention to be able to
process these kinds of texts eventually, as an interim
evaluation we felt that it was best to focus just on
texts where the only feature susceptible to error was
a preposition. We therefore devised a simple artifi-
cial error detection task using a corpus in which er-
48
rors are artificially inserted in otherwise correct text,
for which we present interim results (the dataset is
currently quite small) and we compare it against a
?brute force? baseline, namely using the recently re-
leased Google n-gram data to predict the most likely
preposition.
We set up a task aimed at detecting errors in the
use of of and to, for which we had obtained the best
results in the basic classification tests reported ear-
lier, and we created for this purpose a small corpus
using BBC news articles, as we assume the presence
of errors there, spelling or otherwise, is extremely
unlikely. Errors were created by replacing correct
occurrences of one of the prepositions with another,
incorrect, one, or inserting of or to in place of other
prepositions. All sentences contained at least one
preposition. Together with a set of sentences where
the prepositions were all correct, we obtained a set
of 423 sentences for testing, consisting of 492 prepo-
sition instances. The aim was to replicate both kinds
of errors one can make in using prepositions5 .
We present here some results from this small
scale task; the data was classified by a model of the
algorithm trained on the BNC data with all features
included, 10 epochs, and d=2. If we run the task on
the vectors representing all occurrences of each of
the prepositions, and ask the classifier to distinguish
between correct and incorrect usages, we find the
percentage of correct classifications as follows:
Prep Accuracy Precision Recall
of 75.8 0.72 0.68
to 81.35 0.76 0.74
Average: 78.58 0.74 0.71
These results show both high precision and high
recall, as do those for the dataset consisting of cor-
rect occurrences of the preposition and use of an-
other preposition instead of the right one: (of - 75%,
to - 67% - these are accuracy figures only, as preci-
sion and recall make no sense here.) This small task
shows that it is possible to use our model to reliably
check a text for preposition errors.
However, these results need some kind of base-
line for comparison. The most obvious baseline
would be a random choice between positive and neg-
ative (i.e. the context matches or does not match the
5A third, omitting it altogether, will be accounted for in fu-
ture work.
preposition) which we would expect to be success-
ful 50% of the time. Compared to that the observed
accuracies of 75% or more on all of these various
classification tasks is clearly significant, represent-
ing a 50% or more reduction in the error rate.
However, we are also working on a more chal-
lenging baseline consisting of a simple 3-gram
lookup in the Google n-gram corpus (ca. 980 million
3-grams). For example, given the phrase y Paris,
we could decide to use to rather than at because we
find 10,000 occurrences of y to Paris and hardly
any of y at Paris. In a quick experiment, we ex-
tracted 106 three-word sequences, consisting of one
word each side of the preposition, from a random
sample of the BBC dataset, ensuring each type of er-
ror was equally represented. For each sequence, we
queried the Google corpus for possible prepositions
in that sequence, selecting the most frequent one as
the answer. Despite the very general nature of some
of the 3-grams (e.g. one of the), this method per-
forms very well: the n-gram method scores 87.5%
for of (vs. our 75.8%) and 72.5% for to (vs. our
81.35%). This is only a suggestive comparison, be-
cause the datasets were not of the same size: by the
time of the workshop we hope to have a more rig-
orous baseline to report. Clearly, unless afflicted by
data sparseness, the raw word n-gram method will
be very hard to beat, since it will be based on fre-
quently encountered examples of correct usage. It is
therefore encouraging that our method appears to be
of roughly comparable accuracy even though we are
using no actual word features at all, but only more
abstract ones as described earlier. An obvious next
step, if this result holds up to further scrutiny, is to
experiment with combinations of both types of in-
formation.
6.2 Related work
Although, as noted above, there is much research be-
ing carried out on prepositions at the moment, to the
best of our knowledge there is no work which takes
an approach similar to ours in the task of preposi-
tion choice and error correction, i.e. one that aims to
automate the process of context construction rather
than relying on manually constructed grammars or
other resources such as dictionaries (cf. TPP). Fur-
thermore, much current research seems to have as
its primary aim a semantic and functional descrip-
49
tion of prepositions. While we agree this is a key
aspect of preposition use, and indeed hope at a later
stage of our research to derive some insights into this
behaviour from our data, at present we are focusing
on the more general task of predicting a preposition
given a context, regardless of semantic function.
With regard to related work, as already men-
tioned, there is no direct comparison we can make
in terms of learning preposition use by a similar
method. One useful benchmark could be results ob-
tained by others on a task similar to ours, i.e. error
detection, especially in the language of non-native
speakers. In this case the challenge is finding work
which is roughly comparable: there are a myriad of
variables in this field, from the characteristics of the
learner (age, L1, education...) to the approach used
to the types of errors considered. With this in mind,
all we can do is mention some work which we feel
is closest in spirit to our approach, but stress that the
figures are for reference only, and cannot be com-
pared directly to ours.
Chodorow and Leacock (2000) try to identify er-
rors on the basis of context, as we do here, and
more specifically a ?2 word window around the
word of interest, from which they consider func-
tion words and POS tags. Mutual information is
used to determine more or less likely sequences of
words, so that less likely sequences suggest the pres-
ence of an error. Unlike ours, their work focuses on
content words rather than function words; they re-
port a precision of 78% and a recall of 20%. Our
precision is comparable to this, and our recall is
much higher, which is an important factor in error
detection: a user is likely to lose trust in a sys-
tem which cannot spot his/her errors very often6.
Izumi et al (2004) work with a corpus of En-
glish spoken by Japanese students; they attempt to
identify errors using various contextual features and
maximum entropy based-methods. They report re-
sults for omission errors (precision 75.7%, recall
45.67%) and for replacement errors (P 31.17%, R
8%). With the caveat that we are not working with
spoken language, which presents several other chal-
lenges, we note that in our task the errors, akin to re-
placement errors, are detected with much more suc-
6Although of course precision is a key measure: it is not
helpful for the user to be exposed to false alarms.
cess. Finally we can note the work done by Eeg-
Olofsson and Knutsson (2003) on preposition errors
in L2 Swedish. Their system uses manually crafted
rules, unlike ours, and its performance is reported as
achieving a recall of 25%. On the basis of this brief
and by no means exhaustive overview of the field,
we claim that our results in the error detection task
are competitive, and we are working on fine-tuning
various parameters to improve them further.
7 Conclusion
We have presented an automated approach to learn-
ing associations between sentence contexts and
prepositions which does not depend on manually
crafted grammars and achieves a success rate of up
to 84.5%. This model was tested on a small set
of texts with artificially created preposition errors,
and was found to be successful at detecting between
76% and 81% of errors. Ongoing work is focusing
on how to further improve performance taking into
consideration both the parameters of the voted per-
ceptron algorithm and the feature set of the vectors.
Acknowledgements
We wish to thank Stephen Clark for stimulating discussions and
the anonymous reviewers for their helpful comments. Rachele
De Felice is supported by an AHRC scholarship.
References
Ted Briscoe, John Carroll, and Rebecca Watson. 2006. The
second release of the RASP system. In COLING/ACL-06
Demo Session, Sydney, Australia.
Martin Chodorow and Claudia Leacock. 2000. An unsuper-
vised method for detecting grammatical errors. In NAACL-
00, Seattle, Washington.
Stephen Clark and James Curran. To appear. Wide-coverage
Efficient Statistical Parsing with CCG and Log-linear Mod-
els.
Jens Eeg-Olofsson and Ola Knutsson. 2003. Automatic gram-
mar checking for second language learners - the use of
prepositions. In Nodalida-03, Reykjavik, Iceland.
Yoav Freund and Robert E. Schapire. 1999 Large margin clas-
sification using the perceptron algorithm. Machine Learning
37:277-296
Emi Izumi, Kiyotaka Uchimoto, and Hitoshi Isahara. 2004
SST speech corpus of Japanese learners? English and auto-
matic detection of learners? errors. ICAME 28:31-48
Ken Litkowski and Orin Hargraves. 2005. The Preposition
Project. In Second ACL-SIGSEM Prepositions Workshop,
Colchester, UK.
Guido Minnen, John Carroll, and Darren Pearce. 2001 Ap-
plied Morphological Processing of English. Natural Lan-
guage Engineering 7(3):207-223
50
