XML-based Phrase Alignment in Parallel Treebanks
Martin Volk, Sofia Gustafson-Capkova?, Joakim Lundborg,
Torsten Marek, Yvonne Samuelsson, Frida Tidstro?m
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
volk@ling.su.se
Abstract
This paper describes the usage of XML for
representing cross-language phrase align-
ments in parallel treebanks. We have de-
veloped a TreeAligner as a tool for interac-
tively inserting and correcting such align-
ments as an independent level of treebank
annotation.
1 Introduction
The combined research on treebanks and paral-
lel corpora has recently led to parallel treebanks.
A parallel treebank consists of syntactically anno-
tated sentences in two or more languages, taken
from translated (i.e. parallel) documents. In ad-
dition, the syntax trees of two corresponding sen-
tences are aligned on a sub-sentential level. This
means word level, phrase level and clause level,
but we will refer to it as phrase alignment since
it best represents the idea. Parallel treebanks can
be used as training or evaluation corpora for word
and phrase alignment, as input for example-based
machine translation (EBMT), as training corpora
for transfer rules, or for translation studies.
We are developing an English-German-Swedish
parallel treebank. In this paper we will focus on
the representation of the treebank and the align-
ment. We will briefly explain the steps for building
the parallel treebank and describe our new align-
ment tool. This paper is a follow-up and revision
of (Samuelsson and Volk, 2005) based on fresh in-
sights from this tool.
2 Building the treebanks
Our parallel treebank contains the first two chap-
ters of Jostein Gaarder?s novel ?Sofie?s World?
with about 500 sentences.1 In addition it contains
500 sentences from economy texts (a quarterly re-
port by a multinational company as well as part of
a bank?s annual report).
In creating the parallel treebank, we have
first annotated the monolingual treebanks with
the ANNOTATE treebank editor.2 It includes
Thorsten Brants? statistical Part-of-Speech Tagger
and Chunker. The chunker follows the TIGER
annotation guidelines for German (Brants and
Hansen, 2002), which gives a flat phrase structure
tree. This means, for instance, no unary nodes,
no ?unnecessary? NPs (noun phrases) within PPs
(prepositional phrases) and no finite VPs (verb
phrases).
Using a flat tree structure for manual treebank
annotation has two advantages for the human an-
notator: fewer annotation decisions, and a better
overview of the trees. This comes at the prize
of the trees not being complete from a linguistic
point of view. Moreover, flat syntax trees are also
problematic for node alignment in a parallel tree-
bank. We prefer to have ?deep trees? to be able to
draw the alignment on as many levels as possible;
in fact, the more detailed the sentence structure is,
the more expressive our alignment can become.
As an example, let us look at the work
flow for the German-Swedish parallel treebank.
We first annotated the German sentences semi-
automatically in the flat manner, and we then auto-
matically deepened the flat syntax trees (Samuels-
son and Volk, 2004).
1A prototype of the parallel treebank was developed by
Yvonne Samuelsson and contains the first chapter of the
novel in German and Swedish. Later, a French version was
added and aligned to the Swedish treebank by (Tidstro?m,
2005). We would like to thank Eckhard Bick, Declan Groves
and Jo?rg Tiedemann for their help.
2www.coli.uni-sb.de/sfb378/negra-corpus/annotate.html
93
We annotated the Swedish sentences by first
tagging them with a Part-of-Speech tagger trained
on SUC (the Stockholm-Umea? Corpus). Since we
did not have a Swedish treebank to train a Swedish
chunker, we used a trick to apply the German
chunker for Swedish sentences. We mapped the
Swedish Part-of-Speech tags in the Swedish sen-
tences to the corresponding German tags. Since
the German chunker works on these tags, it then
suggested constituents for the Swedish sentences,
assuming they were German sentences. These
experiments and the resulting time gain were re-
ported in (Volk and Samuelsson, 2004). Upon
completion of the Swedish treebank with flat syn-
tax trees, we applied the same deepening method
as for German, and we then converted the Part-of-
Speech labels back to the Swedish labels.
Finally, we annotated the English sentences ac-
cording to the Penn Treebank guidelines. We
trained the PoS tagger and the chunker on the Penn
Treebank and integrated them into ANNOTATE.
The English guidelines lead to complete trees so
that the deepening step is not needed.
3 XML Representation of the Trees
After finishing the monolingual treebanks with
ANNOTATE, the trees were exported from the
accompanying SQL database and converted into
TIGER-XML. TIGER-XML is a line-based (i.e.
not nested and thus database-friendly) representa-
tion for graph structures, which includes syntax
trees with node labels, edge labels, multiple fea-
tures on the word level and even crossing edges.3
In a TIGER-XML graph each leaf (= token) and
each node (= linguistic constituent) has a unique
identifier which is prefixed with the sentence num-
ber. Leaves are numbered from 1 to 499 and nodes
starting from 500 (under the plausible assumption
that no sentence will ever have more than 499 to-
kens). As can be seen in the following exam-
ple, node 500 in sentence 12 is of the category
PP (prepositional phrase). The phrase consists
of word number 4, which is the preposition in,
plus node 502 which in turn is marked as an NP
(noun phrase), consisting of the words 5 and 6. It
should be noted that the id attribute in the token
lines serves a dual purpose of identifier and order
marker. This makes it possible to represent cross-
ing branches.
<s id="s12">
3See www.ims.uni-stuttgart.de/projekte/TIGER
<graph root="s12_501">
<terminals>
<t id="s12_1" word="Jetzt" pos="ADV" />
<t id="s12_2" word="bog" pos="VVFIN" />
<t id="s12_3" word="sie" pos="PPER" />
<t id="s12_4" word="in" pos="APPR" />
<t id="s12_5" word="den" pos="ART" />
<t id="s12_6" word="Kl?verveien" pos="NE"/>
<t id="s12_7" word="ein" pos="PTKVZ" />
<t id="s12_8" word="." pos="$." />
</terminals>
<nonterminals>
<nt id="s12_500" cat="PP">
<edge label="HD" idref="s12_4" />
<edge label="NK" idref="s12_502" />
</nt>
<nt id="s12_502" cat="NP">
<edge label="NK" idref="s12_5" />
<edge label="HD" idref="s12_6" />
</nt>
[...]
</nonterminals>
</graph>
</s>
This means that the token identifiers and con-
stituent identifiers are used as pointers to represent
the nested tree structure. This example thus repre-
sents the upper tree in figure 1.
One might wonder why tree nesting is not di-
rectly mapped into XML nesting. But the require-
ment that the representation format must support
crossing edges rules out this option. TIGER-XML
is a powerful representation format and is typically
used with constituent symbols on the nodes and
functional information on the edge labels. This
constitutes a combination of constituent structure
and dependency structure information.
4 XML Representation of the Alignment
Phrase alignment can be regarded as an additional
layer of information on top of the syntax struc-
ture. We use the unique node identifiers for the
phrase alignment across parallel trees. We also
use an XML representation for storing the align-
ment. The alignment file first stores the names of
the treebank files and assigns identifiers to them.
Every single phrase alignment is then stored with
the tag align. Thus the entry in the following
example represents the alignment of node 505 in
sentence 13 of language one (German) to the node
506 in sentence 14 of language two (Swedish).
<treebanks>
<tbank file="Sofie_DE.xml" id="De"/>
<tbank file="Sofie_SV.xml" id="Sv"/>
</treebanks>
<align type="exact">
<node node_id="s13_505" tbank_id="De"/>
<node node_id="s14_506" tbank_id="Sv"/>
</align>
94
This representation allows phrase alignments
within m:n sentence alignments, which we have
used in our project. The XML also allows m:n
phrase alignments, which we however have not
used for reasons of simplicity and clarity. Two
nodes are aligned if the words which they span
convey the same meaning and could serve as trans-
lation units.
The alignment format allows alignments to be
specified between an arbitrary number of nodes,
for example nodes from three languages. And
it includes an attribute type which we currently
use to distinguish between exact and approximate
alignments.
5 Our Tree Alignment Tool
After finishing the monolingual trees we want to
align them on the phrase level. For this purpose
we have developed a ?TreeAligner?. This program
is a graphical user interface to insert (or correct)
alignments between pairs of syntax trees.4 The
TreeAligner can be seen in the line of tools such
as I*Link (Ahrenberg et al, 2002) or Cairo (Smith
and Jahr, 2000) but it is especially tailored to visu-
alize and align full syntax trees.
The TreeAligner requires three input files. One
TIGER-XML file with the trees from language
one, another TIGER-XML file with the trees from
language two, plus the alignment file as described
above. The alignment file might initially be empty
when we want to start manual alignment from
scratch, or it might contain automatically com-
puted alignments for correction. The TreeAligner
displays tree pairs with the trees in mirror orien-
tation (one top-up and one top-down). See fig-
ure 1 for an example. This has the advantage that
the alignment lines cross fewer parts of the lower
tree. The trees are displayed with node labels and
greyed-out edge labels. The PoS labels are omit-
ted in the display since they are not relevant for the
task.
Each alignment is displayed as a dotted line be-
tween one node (or word) from each tree. Clicking
on a node (or a word) in one tree and dragging the
mouse pointer to a node (or a word) in the other
tree inserts an alignment line. Figure 2 shows an
example of a tree pair with alignment lines. Cur-
rently the TreeAligner supports two types of align-
4The TreeAligner has been implemented in Python by
Joakim Lundborg and is freely available at www.ling.su.se/
DaLi/downloads/treealigner/index.htm
Figure 1: Tree pair German-Swedish in the
TreeAligner.
ment lines (displayed in different colors) which
are used to indicate exact translation correspon-
dence vs. approximate translation correspondence.
However, our experiments indicate that eventually
more alignment types will be needed to precisely
represent different translation deviations.
Often one tree needs to be aligned to two trees
in the other language. We therefore provide the
option to scroll the trees independently. For in-
stance, if we have aligned only a part of tree 20
from language one to tree 18 of language two, we
may scroll to tree 19 of language two in order to
align the remaining parts of tree 20.5
The TreeAligner is designed as a stand-alone
tool (i.e. it is not prepared for collaborative anno-
tation). It stores every alignment in an XML file
(in the format described above) as soon as the user
moves to a new tree pair. It has been tested on
parallel treebanks with several hundred trees each.
6 Conclusion
We have shown a straightforward way to tie in
XML-based phrase alignment information with
syntax trees represented in TIGER-XML. The
alignment information is stored independently
from the treebank files. This independence allows
for a modularization and separation of the anno-
tation but it entails that the synchronization of the
5The final result of an m:n tree alignment can be visual-
ized with an SVG-based display which we have described in
(Samuelsson and Volk, 2005). SVG (Scalable Vector Graph-
ics) describes vector graphics in XML.
95
Figure 2: Tree pair German-Swedish with alignment in the TreeAligner.
treebanks with the alignment needs to be guarded
separately. If any of the treebanks is modified, the
modification of the alignment needs to follow.
We have argued for the use of a graphical
TreeAligner to display and interactively modify
the alignment between parallel syntax trees. The
TreeAligner allows for m:n sentence alignment,
word alignment and node alignment. And it sup-
ports the distinction between exact and approxi-
mate alignments.
As a next step we plan to integrate a com-
ponent for automatic phrase alignment into the
TreeAligner. The user can then select a tree pair
and will get automatic phrase alignment predic-
tions. We have already experimented with the
projection of automatically computed word align-
ments to predict phrase alignment. Of course, the
automatic phrase alignment has to be manually
checked if we want to ensure high quality align-
ment data.
Another avenue of further research is the inclu-
sion of yet more levels of annotation. For exam-
ple, we are currently experimenting with the anno-
tation of semantic frames on top of the treebanks.
We use the SALSA tool developed at Saarbru?cken
University (Erk and Pado, 2004) which also as-
sumes TIGER-XML input. So, TIGER-XML has
become the lingua franca of treebank annotation
which allows for the addition of arbitrary layers.
References
Lars Ahrenberg, Magnus Merkel, and Mikael Anders-
son. 2002. A system for incremental and interactive
word linking. In Proc. of LREC-2002, pages 485?
490, Las Palmas.
Sabine Brants and Silvia Hansen. 2002. Developments
in the TIGER annotation scheme and their realiza-
tion in the corpus. In Proc. of LREC-2002, pages
1643?1649, Las Palmas.
Katrin Erk and Sebastian Pado. 2004. A powerful and
versatile XML format for representing role-semantic
annotation. In Proc. of LREC-2004, Lisbon.
Yvonne Samuelsson and Martin Volk. 2004. Au-
tomatic node insertion for treebank deepening. In
Proc. of 3rd Workshop on Treebanks and Linguistic
Theories, Tu?bingen, December.
Yvonne Samuelsson and Martin Volk. 2005. Presen-
tation and representation of parallel treebanks. In
Proc. of the Treebank-Workshop at Nodalida, Joen-
suu, May.
Noah A. Smith and Michael E. Jahr. 2000. Cairo:
An alignment visualization tool. In Proc. of LREC-
2000, Athens.
Frida Tidstro?m. 2005. Extending a parallel treebank
with data in French. C-uppsats, Department of Lin-
guistics, Stockholm University, April.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
96
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 51?57
Manchester, August 2008
Human Judgements in Parallel Treebank Alignment
Martin Volk and Torsten Marek
University of Zurich
Institute of Computational Linguistics
8050 Zurich, Switzerland
volk@cl.uzh.ch
Yvonne Samuelsson
Stockholm University
Department of Linguistics
106 91 Stockholm, Sweden
yvonne.samuelsson@ling.su.se
Abstract
We have built a parallel treebank that
includes word and phrase alignment.
The alignment information was manually
checked using a graphical tool that al-
lows the annotator to view a pair of trees
from parallel sentences. We found the
compilation of clear alignment guidelines
to be a difficult task. However, experi-
ments with a group of students have shown
that we are on the right track with up to
89% overlap between the student annota-
tion and our own. At the same time these
experiments have helped us to pin-point
the weaknesses in the guidelines, many of
which concerned unclear rules related to
differences in grammatical forms between
the languages.
1 Introduction
Establishing translation correspondences is a dif-
ficult task. This task is traditionally called align-
ment and is usually performed on the paragraph
level, sentence level and word level. Alignment
answers the question: Which part of a text in lan-
guage L1 corresponds in meaning to which part of
a text in language L2 (under the assumption that
the two texts represent the same meaning in differ-
ent languages). This may mean that one text is the
translation of the other or that both are translations
derived from a third text.
There is considerable interest in automating the
alignment process. Automatic sentence alignment
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
of legacy translations helps to fill translation mem-
ories. Automatic word alignment is a crucial step
in training statistical machine translation systems.
Both sentence and word alignment have to deal
with 1:many alignments, i.e. sometimes a sentence
in one language is translated as two or three sen-
tences in the other language.
In other respects sentence alignment and word
alignment are fundamentally different. It is rela-
tively safe to assume the same sentence order in
both languages when computing sentence align-
ment. But such a monotonicity assumption is not
possible for word alignment which needs to allow
for word order differences and thus for crossing
alignments. And while algorithms for sentence
alignment usually focus on length comparisons (in
terms of numbers of characters), word alignment
algorithms use cross-language cooccurrence fre-
quencies as a key feature.
Our work focuses on word alignment and on an
intermediate alignment level which we call phrase
alignment. Phrase alignment encompasses the
alignment from simple noun phrases and preposi-
tional phrases all the way to complex clauses. For
example, on the word alignment level we want to
establish the correspondence of the German ?verb
form plus separated prefix? fing an with the Eng-
lish verb form began. While in phrase alignment
we mark the correspondence of the verb phrases
ihn in den Briefkasten gesteckt and dropped it in
the mail box.
We regard phrase alignment as alignment be-
tween linguistically motivated phrases, in con-
trast to some work in statistical machine trans-
lation where phrase alignment is defined as the
alignment between arbitrary word sequences. Our
phrase alignment is alignment between nodes in
constituent structure trees. See figure 1 for an ex-
51
ample of a tree pair with word and phrase align-
ment.
We believe that such linguistically motivated
phrase alignment provides useful phrase pairs for
example-based machine translation, and provides
interesting insights for translation science and
cross-language comparisons. Phrase alignments
are particularly useful for annotating correspon-
dences of idiomatic or metaphoric language use.
2 The Parallel Treebank
We have built a trilingual parallel treebank in Eng-
lish, German and Swedish. The treebank consists
of around 500 trees from the novel Sophie?s World
and 500 trees from economy texts (an annual re-
port from a bank, a quarterly report from an inter-
national engineering company, and the banana cer-
tification program of the Rainforest Alliance). The
sentences in Sophie?s World are relatively short
(14.8 tokens on average in the English version),
while the sentences in the economy texts are much
longer (24.3 tokens on average; 5 sentences in the
English version have more than 100 tokens).
The treebanks in English and German consist of
constituent structure trees that follow the guide-
lines of existing treebanks, the NEGRA/TIGER
guidelines for German and the Penn treebank
guidelines for English. There were no guidelines
for Swedish constituent structure trees. We have
therefore adapted the German treebank guidelines
for Swedish. Both German trees and Swedish trees
are annotated with flat structures but subsequently
automatically deepened to result in richer and lin-
guistically more plausible tree structures.
When the monolingual treebanks were finished,
we started with the word and phrase alignment.
For this purpose we have developed a special tool
called the Stockholm TreeAligner (Lundborg et
al., 2007) which displays two trees and allows the
user to draw alignment lines by clicking on nodes
and words. This tool is similar to word alignment
tools like ILink (Ahrenberg et al, 2003) or Cairo
(Smith and Jahr, 2000). As far as we know our tool
is unique in that it allows the alignments of lin-
guistically motivated phrases via node alignments
in parallel constituent structure trees (cf. (Samuels-
son and Volk, 2007)).
After having solved the technical issues, the
challenge was to compile precise and comprehen-
sive guidelines to ensure smooth and consistent
alignment decisions. In (Samuelsson and Volk,
2006) we have reported on a first experiment to
evaluate inter-annotator agreement from our align-
ment tasks.
In this paper we report on another recently con-
ducted experiment in which we tried to identify
the weaknesses in our alignment guidelines. We
asked 12 students to alignment 20 tree pairs (Eng-
lish and German) taken from our parallel treebank.
By comparing their alignments to our Gold Stan-
dard and to each other we gained valuable insights
into the difficulty of the alignment task and the
quality of our guidelines.
3 Related Research
Our research on word and phrase alignment is re-
lated to previous work on word alignment as e.g.
in the Blinker project (Melamed, 1998) or in the
UPLUG project (Ahrenberg et al, 2003). Align-
ment work on parallel treebanks is rare. Most
notably there is the Prague Czech-English tree-
bank (Kruijff-Korbayova? et al, 2006) and the
Linko?ping Swedish-English treebank (Ahrenberg,
2007). There has not been much work on the align-
ment of linguistically motivated phrases. Tinsley
et al (2007) and Groves et al (2004) report on
semi-automatic phrase alignment as part of their
research on example-based machine translation.
Considering the fact that the alignment task is
essentially a semantic annotation task, we may
also compare our results to other tasks in seman-
tic corpus annotation. For example, we may con-
sider the methods for resolving annotation con-
flicts and the figures for inter-annotator agreement
in frame-semantic annotation as found in the Ger-
man SALSA project (cf. (Burchardt et al, 2006)).
4 Our Alignment Guidelines
We have compiled alignment guidelines for word
and phrase alignment between annotated syntax
trees. The guidelines consist of general principles,
concrete rules and guiding principles.
The most important general principles are:
1. Align items that can be re-used as units in a
machine translation system.
2. Align as many items (i.e. words and phrases)
as possible.
3. Align as close as possible to the tokens.
The first principle is central to our work. It
defines the general perspective for our alignment.
52
Figure 1: Tree pair German-English with word and phrase alignments.
We do not want to know which part of a sentence
has possibly given rise to which part of the cor-
respondence sentence. Instead our perspective is
on whether a phrase pair is general enough to be
re-used as translation unit in a machine translation
system. For example, we do not want to align die
Verwunderung u?ber das Leben with their astonish-
ment at the world although these two phrases were
certainly triggered by the same phrase in the orig-
inal and both have a similar function in the two
corresponding sentences. These two phrases seen
in isolation are too far apart in meaning to license
their re-use. We are looking for correspondences
like was fu?r eine seltsame Welt and what an ex-
traordinary world which would make for a good
translation in many other contexts.
Some special rules follow from this principle.
For example, we have decided that a pronoun in
one language shall never be aligned with a full
noun in the other, since such a pair is not directly
useful in a machine translation system.
Principles 2 and 3 are more technical. Princi-
ple 2 tells our annotators that alignment should be
exhaustive. We want to re-use as much as pos-
sible from the treebank, so we have to look for
as many alignments as possible. And principle 3
says that in case of doubt the alignment should go
to the node that is closest to the terminals. For
example, our German treebank guidelines require
a multi-word proper noun to first be grouped in
a PN phrase which is a daughter node of a noun
phrase [[Sofie Amundsen]PN ]NP whereas
the English guidelines only require the NP node
[Sophie Amundsen]NP. When we align the
two names, principle 3 tells us to draw the align-
ment line between the German PN node and the
English NP node since the PN node is closer to the
tokens than the German NP node.
Often we are confronted with phrases that are
not exact translation correspondences but approx-
imate translation correspondences. Consider the
phrases mehr als eine Maschine and more than a
piece of hardware. This pair does not represent the
closest possible translation but it represents a pos-
sible translation in many contexts. In a way we
could classify this pair as the ?second-best? trans-
lation. To allow for such distinctions we provide
our annotators with a choice between exact transla-
tion correspondences and approximate correspon-
dences. We also use the term fuzzy correspon-
dence to refer to and give an intuitive picture of
these approximate correspondences. The option to
53
distinguish between different alignment strengths
sounded very attractive at the start but it turned out
to be the source for some headaches later. Where
and how can we draw the line between exact and
fuzzy translation correspondences?
We have formulated some clear-cut rules:
1. If an acronym is to be aligned with a spelled-
out term, it is always an approximate align-
ment. For example, in our economy reports
the English acronym PT stands for Power
Technology and is aligned to the German En-
ergietechnik as a fuzzy correspondence.
2. Proper names shall be aligned as exact align-
ments (even if they are spelled differently
across languages; e.g. Sofie vs. Sophie).
But many open questions persist. Is einer der
ersten Tage im Mai an exact or rather a fuzzy trans-
lation correspondence of early May? We decided
that it is not an exact correspondence. How shall
we handle zu dieser Jahreszeit vs. at this time of
the year where a literal translation would be in this
season? We decided that the former is still an exact
correspondence. These examples illustrate the dif-
ficulties that make us wonder how useful the dis-
tinction between exact and approximate translation
correspondence really is.
Automatically ensuring the overall consistency
of the alignment decisions is a difficult task.
But we have used a tool to ensure the consis-
tency within the exact and approximate alignment
classes. The tool computes the token span for each
alignment and checks if the same tokens pairs have
always received the same alignment type. For ex-
ample, if the phrase pair mit einer blitzschnellen
Bewegung and with a lightning movement is once
annotated as exact alignment, then it should always
be annotated as exact alignment. Figure 1 shows
approximate alignments between the PPs in der
Hand and in her hand. It was classified as approxi-
mate rather than exact alignment since the German
PP lacks the possessive determiner.
Currently our alignment guidelines are 6 pages
long with examples for English-German and
English-Swedish alignments.
5 Experiments with Student Annotators
In order to check the inter-annotator agreement for
the alignment task we performed the following ex-
periment. We gave 20 tree pairs in German and
English to 12 advanced undergraduate students in
a class on ?Machine Translation and Parallel Cor-
pora?. Half of the tree pairs were taken from our
Sophie?s World treebank and the other half from
our Economy treebank. We made sure that there
was one 1:2 sentence alignment in the sample. The
students did not have access to the Gold Standard
alignment.
In class we demonstrated the alignment tool to
the students and we introduced the general align-
ment principles to them. Then the students were
given a copy of the alignment guidelines. We
asked them to do the alignments independently of
each other and to the best of their knowledge ac-
cording to the guidelines.
In our own annotation of the 20 tree pairs (= the
Gold Standard alignment) we have the following
numbers of alignments:
type exact fuzzy total
Sophie part word 75 3 78
phrase 46 12 58
Economy part word 159 19 178
phrase 62 9 71
In the Sophie part of the experiment treebank we
have 78 word-to-word alignments and 58 phrase-
to-phrase alignments. Note that some phrases con-
sist only of one word and thus the same alignment
information is represented twice. We have deliber-
ately kept this redundancy.
The alignments in the Sophie part consist of
125 times 1:1 alignments, 4 times 1:2 alignments
and one 1:3 alignment (wa?re vs. would have been)
when viewed from the German side. There are 3
times 1:2 alignments (e.g. introducing vs. stellte
vor) and no other 1:many alignment when viewed
from the English side.
In the Economy part the picture is similar. The
vast majority are 1:1 alignments. There are 207
times 1:1 alignments and 21 times 1:2 alignments
(many of which are German compound nouns)
when viewed from German. And there are 235
times 1:1 alignments, plus 4 times 1:2 alignments,
plus 2 times 1:3 alignments when viewed from
English (e.g. the Americas was aligned to the three
tokens Nord- und Su?damerika).
The student alignments showed a huge vari-
ety in terms of numbers of alignments. In the
Sophie part they ranged from 125 alignments to
bare 47 alignments (exact alignments and fuzzy
alignments taken together). In the Economy part
the variation was between 259 and 62 alignments.
54
On closer inspection we found that the student
with the lowest numbers works as a translator
and chose to use a very strict criterion of transla-
tion equivalence rather than translation correspon-
dence. Three other students at the end of the list are
not native speakers of either German and English.
We therefore decided to exclude these 4 students
from the following comparison.
The student alignments allow for the investiga-
tion of a number of interesting questions:
1. How did the students? alignments differ from
the Gold Standard?
2. Which were the alignments done by all stu-
dents?
3. Which were the alignments done by single
students only?
4. Which alignments varied most between exact
and fuzzy alignment?
When we compared each student?s alignments
to the Gold Standard alignments, we computed
three figures:
1. How often did the student alignment and the
Gold Standard alignment overlap?
2. How many Gold Standard alignments did the
student miss?
3. How many student alignments were not in the
Gold Standard?
The remaining 8 students reached between 81%
and 48% overlap with the Gold Standard on the
Sophie part, and between 89% and 66% overlap
with the Gold Standard on the Economy texts. This
can be regarded as their recall values if we assume
that the Gold Standard represents the correct align-
ments. These same 8 students additionally had
between 2 and 22 own alignments in the Sophie
part and between 12 and 55 own alignments in the
Economy part.
So the interesting question is: What kind of
alignments have they missed, and which were
the additional own alignments that they suggested
(alignments that are not in the gold standard)? We
first checked the students with the highest numbers
of own alignments. We found that some of these
alignments were due to the fact that students had
ignored the rule to align as close to the tokens as
possible (principle 3 above).
Another reason was that students sometimes
aligned a word (or some words) with a node.
For example, one student had aligned the word
natu?rlich to the phrase of course instead of to the
word sequence of course. Our alignment tool al-
lows that, but the alignment guidelines discour-
age such alignments. There might be exceptional
cases where a word-to-phrase alignment is neces-
sary in order to keep valuable information, but in
general we try to stick to word-to-word and phrase-
to-phrase alignments.
Another discrepancy occurred when the stu-
dents aligned a German verb group with a single
verb form in English (e.g. ist zuru?ckzufu?hren vs.
reflecting). We have decided to only align the full
verb to the full verb (independent of the inflection).
This means that we align only zuru?ckzufu?hren to
reflecting in this example.
The uncertainties on how to deal with different
grammatical forms led to the most discrepancies.
Shall we align the definite NP die Umsa?tze with
the indefinite NP revenues since it is much more
common to drop the article in an English plural NP
than in German? Shall we align a German genitive
NP with an of-PP in English (der beiden Divisio-
nen vs. of the two divisions)? We have decided to
give priority to form over function and thus to align
the NP der beiden Divisionen with the NP the two
divisions. But of course this choice is debatable.
When we compute the intersection of the align-
ments done by all students (ignoring the difference
between exact and fuzzy alignments), we find that
about 50% of the alignments done by the student
with the smallest number of alignments is shared
by all other students. All of the alignments in the
intersection are in our Gold Standard file. This in-
dicates that there is a core of alignments that are
obvious and uncontroversial. Most of them are
word alignments.
When we compute the union of the alignments
done by all students (again ignoring the difference
between exact and fuzzy alignments), we find that
the number of alignments in the union is 40% to
50% higher than the number of alignments done by
the student with the highest number of alignments.
It is also about 40% to 50% higher than the number
of alignments in the Gold Standard. This means
that there is considerable deviation from the Gold
Standard.
Comparing the union of the students? align-
ments to the Gold Standard points to some weak-
55
nesses of the guidelines. For example, one align-
ment in the Gold Standard that was missed by all
students concerns the alignment of a German pro-
noun (wenn sie die Hand ausstreckte) to an empty
token in English (herself shaking hands). Our
guidelines recommend to align such cases as fuzzy
alignments, but of course it is difficult to determine
that the empty token really corresponds to the Ger-
man word.
Other discrepancies concern cases of differing
grammatical forms, e.g. a German definite singu-
lar noun phrase (die Hand) that was aligned to an
English plural noun phrase (Hands) in the Gold
Standard but missed by all students. Finally there
are a few cases where obvious noun phrase cor-
respondences were simply overlooked by all stu-
dents (sich - herself ) although the tokens them-
selves were aligned. Such cases should be handled
by an automated process in the alignment tool that
projects from aligned tokens to their mother nodes
(in particular in cases of single token phrases).
We also investigated how many exact align-
ments and how many fuzzy alignments the stu-
dents had used. The following table gives the fig-
ures.
exact fuzzy overlap total
Sophie part 152 106 69 189
Economy part 296 188 119 366
The alignments done by all students resulted in a
union set of 189 alignments for the Sophie part and
366 alignments for the Economy part. The align-
ments in the Sophie part consisted of 152 exact
alignments and 106 fuzzy alignments. This means
that 69 alignments were marked as both exact and
fuzzy. In other words, in 69 cases at least one stu-
dent has marked an alignment as fuzzy while at
least one other student has marked the same align-
ment as good. So there is still considerable con-
fusion amongst the annotators on how to decide
between exact and fuzzy alignments. And in case
of doubt many students have decided in favor of
fuzzy alignments.
6 Conclusions
We have shown the difficulties in creating cross-
language word and phrase alignments. Experi-
ments with a group of students have helped to iden-
tify the weaknesses in our alignment guidelines
and in our Gold Standard alignment. We have re-
alized that the guidelines need to contain a host
of fine-grained alignment rules and examples that
will clarify critical cases.
In order to evaluate a set of alignment experi-
ments with groups of annotators it is important to
have good visualization tools to present the results.
We have worked with Perl scripts for the compar-
ison and with our own TreeAligner tool for the vi-
sualization. For example we have used two colors
to visualize a student?s alignment overlap with the
Gold Standard in one color and his own alignments
(that are not in the Gold Standard) in another color.
In order to visualize the agreements of the whole
group it would be desirable to have the option to in-
crease the alignment line width in proportion to the
number of annotators that have chosen a particular
alignment link. This would give an intuitive im-
pression of strong alignment links and weak align-
ment links.
Another option for future extension of this work
is an even more elaborate classification of the
alignment links. (Hansen-Schirra et al, 2006) have
demonstrated how a fine-grained distinction be-
tween different alignment types could look like.
Annotating such a corpus will be labor-intensive
but provide for a wealth of cross-language obser-
vations.
References
Ahrenberg, Lars, Magnus Merkel, and Michael Petter-
stedt. 2003. Interactive word alignment for language
engineering. In Proc. of EACL-2003, Budapest.
Ahrenberg, Lars. 2007. LinES: An English-Swedish
parallel treebank. In Proc. of Nodalida, Tartu.
Burchardt, A., K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. The SALSA corpus: A Ger-
man corpus resource for lexical semantics. In Pro-
ceedings of LREC 2006, pages 969?974, Genoa.
Groves, Declan, Mary Hearne, and Andy Way. 2004.
Robust sub-sentential alignment of phrase-structure
trees. In Proceedings of Coling 2004, pages 1072?
1078, Geneva, Switzerland, Aug 23?Aug 27. COL-
ING.
Hansen-Schirra, Silvia, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and
alignment in an English-German translation corpus.
In Proceedings of the EACL Workshop on Multidi-
mensional Markup in Natural Language Processing
(NLPXML-2006), pages 35? 42, Trento.
Kruijff-Korbayova?, Ivana, Kla?ra Chva?talova?, and Oana
Postolache. 2006. Annotation guidelines for the
Czech-English word alignment. In Proceedings of
LREC, Genova.
56
Lundborg, Joakim, Torsten Marek, Mae?l Mettler,
and Martin Volk. 2007. Using the Stockholm
TreeAligner. In Proc. of The 6th Workshop on Tree-
banks and Linguistic Theories, Bergen, December.
Melamed, Dan. 1998. Manual annotation of transla-
tional equivalence: The blinker project. Technical
Report 98-06, IRCS, Philadelphia PA.
Samuelsson, Yvonne and Martin Volk. 2006. Phrase
alignment in parallel treebanks. In Hajic, Jan and
Joakim Nivre, editors, Proc. of the Fifth Workshop on
Treebanks and Linguistic Theories, pages 91?102,
Prague, December.
Samuelsson, Yvonne and Martin Volk. 2007. Align-
ment tools for parallel treebanks. In Proceedings of
GLDV Fru?hjahrstagung 2007.
Smith, Noah A. and Michael E. Jahr. 2000. Cairo:
An alignment visualization tool. In Proc. of LREC-
2000, Athens.
Tinsley, John, Ventsislav Zhechev, Mary Hearne, and
Andy Way. 2007. Robust language pair-independent
sub-tree alignment. In Machine Translation Summit
XI Proceedings, Copenhagen.
57
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 192?196,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Combining Parallel Treebanks and Geo-Tagging
Martin Volk, Anne Go?hring, Torsten Marek
University of Zurich, Institute of Computational Linguistics
volk@cl.uzh.ch
Abstract
This paper describes a new kind of seman-
tic annotation in parallel treebanks. We
build French-German parallel treebanks of
mountaineering reports, a text genre that
abounds with geographical names which
we classify and ground with reference to
a large gazetteer of Swiss toponyms. We
discuss the challenges in obtaining a high
recall and precision in automatic ground-
ing, and sketch how we represent the
grounding information in our treebank.
1 Introduction
Treebanks have become valuable resources in nat-
ural language processing as training corpora for
natural language parsers, as repositories for lin-
guistic research, or as evaluation corpora for dif-
ferent NLP systems. We define a treebank as
a collection of syntactically annotated sentences.
The annotation can vary from constituent to de-
pendency or tecto-grammatical structures. The
term treebank is mostly used to denote manually
checked collections, but recently it has been ex-
tended to also refer to automatically parsed cor-
pora.
We have built manually checked treebanks for
various text genres (see section 3): economy texts,
a popular science philosophy novel, and technical
user manuals. We are now entering a new genre,
mountaineering reports, with the goal to link tex-
tual to spatial information. We build French and
German treebanks of translated texts from the
Swiss Alpine Club. This genre contains a multi-
tude of geographical names (e.g. mountains and
valleys, glaciers and rivers). Therefore we need to
include the identification and grounding of these
toponyms as part of the annotation process.
In this paper we first describe our corpus of
alpine texts, then our work on creating paral-
lel treebanks which includes aligning the parallel
trees on word and phrase level. We sketch the dif-
ficulties in disambiguating the toponyms and de-
scribe our integration of the toponym identifiers as
a special kind of semantic annotation in the tree-
bank.
2 Our Text+Berg Corpus
In our project Text+Berg1 we digitize alpine her-
itage literature from various European countries.
Currently our group digitizes all yearbooks of the
Swiss Alpine Club (SAC) from 1864 until today.
Each yearbook consists of 300 to 600 pages and
contains reports on mountain expeditions, culture
of mountain peoples, as well as the flora, fauna
and geology of the mountains.
The corpus preparation presented interesting
challenges in automatic OCR correction, language
identification, and text structure recognition which
we have described in (Volk et al, 2010).
As of March 2010 we have scanned and OCR-
converted 142 books from 1864 to 1982, corre-
sponding to nearly 70,000 pages. This resulted in
a multilingual corpus of 6101 articles in German,
2659 in French, 155 in Italian, 13 in Romansch,
and 3 in Swiss-German. The parallel part of our
corpus currently contains 701 translated articles
amounting to 2.6 million tokens in French and 2.3
million tokens in German.
3 Parallel Treebanks
In recent years the combined research on tree-
banks and parallel corpora has led to parallel tree-
banks. We have built a parallel treebank (En-
glish, German, Swedish) which contains 1500 sen-
tences in three languages: 500 sentences each
from Jostein Gaarder?s novel ?Sophie?s World?,
from economy texts (e.g. business reports from
mechanical engineering company ABB and from
the bank SEB), and from a technical manual with
1See www.textberg.ch.
192
usage instructions for a DVD player (Go?hring,
2009).
We have annotated the English sentences
according to the well-established Penn Tree-
bank guidelines. For German we followed the
TIGER annotation guidelines, and we adapted
these guidelines also for Swedish (see (Volk
and Samuelsson, 2004)). For French treebank-
ing we are looking for inspiration from the Le
Monde treebank (Abeille? et al, 2003) and from
L?Arboratoire (Bick, 2010). The Le Monde tree-
bank is a constituent structure treebank partially
annotated with functional labels. L?Arboratoire is
based on constraint grammar analysis but can also
output constituent trees.
3.1 Our Tree Alignment Tool
After finishing the monolingual trees we aligned
them on the word level and phrase level. For
this purpose we have developed the TreeAligner
(Lundborg et al, 2007). This program comes with
a graphical user interface to insert or modify align-
ments between pairs of syntax trees.2
The TreeAligner displays tree pairs with the
trees in mirror orientation (one top-up and one top-
down). This has the advantage that the alignment
lines cross fewer parts of the lower tree. Figure 1
shows an example of a tree pair with alignment
lines. The lines denote translation equivalence.
Both trees are constituent structure trees, but the
edge labels contain function labels (like subject,
object, attribute) which can be used to easily con-
vert the trees to dependency structures (cf. (Marek
et al, 2009)).
Recently we have extended the TreeAligner?s
functionality from being solely an alignment tool
to also being a powerful search tool over parallel
treebanks (Volk et al, 2007; Marek et al, 2008).
This enables our annotators to improve the align-
ment quality by cross-checking previous align-
ments. This functionality makes the TreeAligner
also attractive to a wider user base (e.g. linguists,
translation scientists) who are interested in search-
ing rather than building parallel treebanks.
3.2 Similar Treebanking Projects
Parallel treebanks have evolved into an active re-
search field in the last decade. Cmejrek et al
2The TreeAligner has been implemented in Python by
Joakim Lundborg and Torsten Marek and is freely available
at http://kitt.cl.uzh.ch/kitt/treealigner.
(2003) have built a parallel treebank for the spe-
cific purpose of machine translation, the Czech-
English Penn Treebank with tecto-grammatical
dependency trees. Other parallel treebank projects
include Croco (Hansen-Schirra et al, 2006) which
is aimed at building a English-German tree-
bank for translation studies, LinES an English-
Swedish parallel treebank (Ahrenberg, 2007), and
the English-French HomeCentre treebank (Hearne
and Way, 2006), a hand-crafted parallel treebank
consisting of 810 sentence pairs from a Xerox
printer manual.
Some researchers have tried to exploit parallel
treebanks for example-based or statistical machine
translation (Tinsley et al, 2009). Since manually
created treebanks are too small for this purpose,
various researchers have worked on automatically
parsing and aligning parallel treebanks. Zhechev
(2009) and Tiedemann and Kotze? (2009) have
presented methods for automatic cross-language
phrase alignment.
There have been various attempts to enrich
treebanks with semantic information. For exam-
ple, the Propbank project has assigned semantic
roles to Penn treebank sentences (Kingsbury et al,
2002). Likewise the SALSA project has added
frame-semantic annotations on top of syntax trees
from the German TIGER treebank (Burchardt et
al., 2006). Frame-semantics was extended to par-
allel treebanks by (Pado?, 2007) and (Volk and
Samuelsson, 2007). To our knowledge a treebank
with grounded toponym information has not been
created yet.
4 Geo-Tagging
Named entity recognition is an important aspect of
information extraction. But it has also been recog-
nized as important for the access to heritage data.
In a previous project we have investigated meth-
ods for named entity recognition in newspaper
texts (Volk and Clematide, 2001). In that work
we had only distinguished two types of geograph-
ical names: city names and country names. This
was sufficient for texts that dealt mostly with facts
like a company being located in a certain coun-
try or having started business in a certain city.
In contrast to that, our alpine text corpus deals
with much more fine-grained location informa-
tion: mountains and valleys, glaciers and climb-
ing routes, cabins and hotels, rivers and lakes. In
fact the description of movements (e.g. in moun-
193
MonteNE RosaNEderART RundtourNN umAPPR denARTDasART klassischeADJA Endst?ckNN desART Schwarzberg-WeisstorsNEderART GletscherpassNNausAPZR istVAFINvonAPPR ZermattNN .$.
.PCT_SestV leD_def colN_C glaciaireA_qual deP Schwarzberg-WeisstorN_PLaD_def derni?reA_qual ?tapeN_C ?P partirV deP ZermattN_PduP tourN_C duP MontN_P RoseN_P
HDNPPNC PNCPN NK HDNP
NK HD AGNPHD HDNKPPNK HDNP
HD NKPP
NK HD MNRMNRNP
NK NK HD AGNP
HD SPSP S
HD
HD
HD
HDNP
PPMONP
SP
HD
PHD
HDNP
PP
MO
HD
HD
HD
PNC PNCNP
PPMONP
PPMO NPSP S
Figure 1: German-French tree pair with alignments in the TreeAligner.
tains) requires all kinds of intricate references to
positions and directions in three dimensions.
In order to recognize the geographical names in
our corpus we have acquired a large list of Swiss
toponyms.
4.1 The SwissTopo Name List
The Swiss Federal Office of Topography (www.
swisstopo.ch) maintains a database of all
names that appear on its topographical maps. We
have obtained a copy of this database which con-
tains 156,755 names in 61 categories. Categories
include settlements (10 categories ranging from
large cities to single houses), bodies of water (13
categories from major rivers to ponds and wells),
mountains (7 categories from mountain ranges to
small hills), valleys, mountain passes, streets and
man-made facilities (e.g. bridges and tunnels), and
single objects like hotels, mountain cabins, monu-
ments etc. Some objects are subclassified accord-
ing to size. For example, cities are subdivided into
main, large, middle and small cities according to
their number of inhabitants.
Every name is listed in the SwissTopo database
with its coordinates, its altitude (if applicable and
available), the administrative unit to which it be-
longs (usually the name of a nearby town), and the
canton.
4.2 A First Experiment: Finding Mountain
Names
We selected an article from the SAC yearbook
of 1900 to check the precision and recall of au-
tomatically identifying mountain names based on
the SwissTopo name list. The article is titled
?Bergfahrten im Clubgebiet (von Dr. A. Walker)?.
It is an article in German with a wealth of French
mountain names since the author reports about his
hikes in the French speaking part of Switzerland.
We took the article after OCR without any further
manual correction. After our tokenization (incl.
the splitting of punctuation symbols) it consisted
of 9380 tokens.
We used the SwissTopo mountain names classi-
fied as ?Massiv, HGipfel, GGipfel, and KGipfel?
i.e. the 4 highest mountain classes. They consist of
5588 mountain names. This leads to a recall of 54
mountain names (20 different mountain names) at
194
the expense of erroneously marking 6 nouns Gen-
darm, Haupt, Kamm, Stand, Stein, Turm as moun-
tain names.
How many mountain names have we missed
to identify? A manual inspection showed that
there are another 92 mountain names (35 differ-
ent mountain names) missing. So recall of the
naive exact matching is below 40% despite the
large gazetteer. We have reported on a number of
reasons for missed names in (Volk et al, 2010).
We found that spelling variations and partial co-
references account for the majority of recall prob-
lems. In addition we need to disambiguate be-
tween name-noun and name-name homographs.
This leaves the issue on how to represent the geo-
tagging information in our treebank.
5 Geonames in Treebanks
Named entity classification can be divided into
name recognition, disambiguation and grounding.
The first two steps are applicable to all kinds of
names. The final step of grounding the names is
different depending on the name types. A per-
son name may be grounded by refering to the per-
son?s Wikipedia page. The same could be done
for a geographical name. The obvious disadvan-
tage are changing URLs and missing Wikipedia
pages. The goal of grounding must be to link
the name to the most stable and most reliable
?ground?. Therefore toponyms are often linked to
their geographical coordinates. We have chosen to
link the toponyms from our alpine texts to unique
identifiers in the SwissTopo database. This works
well for Swiss names and particularly well for par-
allel French-German sentence pairs. The cross-
language alignment assures that the names are rec-
ognized in either language and the classification
information can then automatically be transfered
to the other language.
In our example in figure 1, the mountain name
?Monte Rosa? is listed in SwissTopo with its al-
titude (4633 m) and its location close to Zermatt.
Since ?Zermatt? itself occurs in the sentence, this
is strong evidence that we have identified the cor-
rect mountain, and we will attach its SwissTopo
identification number in our treebank. Technically
this means we add a reference to the gazetteer and
to the identifier within the gazetteer into the XML
representation of the linguistic object.
In our German example sentence ?Monte Rosa?
is annotated as a proper name (PN). This occur-
rence is phrase 502 in sentence 311 of our tree-
bank. The grounding id (g id) is taken from Swis-
sTopo which then allows us to access the geo-
graphical coordinates, the altitude and neighbor-
hood information.
<nt id="s311_502"
cat="PN"
g_source="SwissTopo"
g_id="7355873" >
Instead of integrating the grounding pointers di-
rectly in the XML file of the treebank, it is possible
to use stand-off annotation by connecting the iden-
tifier of the geo-name with the identifier from the
gazetteer in a separate file.
The alignments in our parallel treebank lead
to the advantage that the grounding information
needs to be saved only once. In our example, the
corresponding mountain name ?Mont Rose? in the
French translation is listed in SwissTopo only as a
building in the municipality of Genthod in the can-
ton Geneva. Since we have strong evidence from
the German sentence, we can rule out this option.
Zermatt itself occurs in both the French and
German sentences in our example. It is listed in
SwissTopo with its altitude (1616 m) and classi-
fied as mid-sized municipality (2000 to 10,000 in-
habitants). Zermatt is a unique name in SwissTopo
and therefore is grounded via its SwissTopo identi-
fier. Likewise we ground ?Schwarzberg Weisstor?
(spelled without hyphen in SwissTopo) which is
listed as foot pass in the municipality of Saas-
Almagell. In case of doubt we could verify
that Saas-Almagell and Zermatt are neighboring
towns, which indeed they are.
6 Conclusions
Grounding toponyms in parallel treebanks repre-
sents a new kind of semantic annotation. We have
sketched the issues in automatic toponym classi-
fication and disambiguation. We are working on
a French-German parallel treebank of alpine texts
which contain a multitude of toponyms that de-
scribe way-points on climbing or hiking routes but
also panorama views. We are interested in iden-
tifying all toponyms in order to enable treebank
access via geographical maps. In the future we
want to automatically compute and display climb-
ing routes from the textual descriptions. The an-
notated treebank will then serve as a gold standard
for the evaluation of the automatic geo-tagging.
195
References
Anne Abeille?, Lionel Cle?ment, and Francois Toussenel.
2003. Building a Treebank for French. In Anne
Abeille?, editor, Building and Using Parsed Corpora,
volume 20 of Text, Speech and Language Technol-
ogy, chapter 10, pages 165?187. Kluwer, Dordrecht.
Lars Ahrenberg. 2007. LinES: An English-Swedish
parallel treebank. In Proc. of Nodalida, Tartu.
Eckhard Bick. 2010. FrAG, a hybrid constraint gram-
mar parser for French. In Proceedings of LREC,
Malta.
A. Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado?,
and M. Pinkal. 2006. The SALSA corpus: A Ger-
man corpus resource for lexical semantics. In Pro-
ceedings of LREC 2006, pages 969?974, Genoa.
Martin Cmejrek, Jan Curin, and Jiri Havelka. 2003.
Treebanks in machine translation. In Proc. Of the
2nd Workshop on Treebanks and Linguistic Theo-
ries, pages 209?212, Va?xjo?.
Anne Go?hring. 2009. Spanish expansion of a parallel
treebank. Lizentiatsarbeit, University of Zurich.
Silvia Hansen-Schirra, Stella Neumann, and Mihaela
Vela. 2006. Multi-dimensional annotation and
alignment in an English-German translation corpus.
In Proceedings of the EACL Workshop on Multidi-
mensional Markup in Natural Language Processing
(NLPXML-2006), pages 35? 42, Trento.
Mary Hearne and Andy Way. 2006. Disambiguation
strategies for data-oriented translation. In Proceed-
ings of the 11th Conference of the European Asso-
ciation for Machine Translation (EAMT), pages 59?
68, Oslo.
P. Kingsbury, M. Palmer, and M. Marcus. 2002.
Adding semantic annotation to the Penn TreeBank.
In Proceedings of the Human Language Technology
Conference (HLT?02), San Diego.
Joakim Lundborg, Torsten Marek, Mae?l Mettler, and
Martin Volk. 2007. Using the Stockholm
TreeAligner. In Proc. of The 6th Workshop on Tree-
banks and Linguistic Theories, Bergen, December.
Torsten Marek, Joakim Lundborg, and Martin Volk.
2008. Extending the TIGER query language with
universal quantification. In Proceeding of KON-
VENS, pages 3?14, Berlin.
Torsten Marek, Gerold Schneider, and Martin Volk.
2009. A framework for constituent-dependency
conversion. In Proceedings of the 8th Workshop on
Treebanks and Linguistic Theories, Milano, Decem-
ber.
Sebastian Pado?. 2007. Cross-Lingual Annotation
Projection Models for Role-Semantic Information.
Ph.D. thesis, Saarland University, Saarbru?cken.
Jo?rg Tiedemann and Gideon Kotze?. 2009. Building
a large machine-aligned parallel treebank. In Pro-
ceedings of the 8th International Workshop on Tree-
banks and Linguistic Theories, pages 197?208, Mi-
lano.
John Tinsley, Mary Hearne, and Andy Way. 2009.
Exploiting parallel treebanks to improve phrase-
based statistical machine translation. In Computa-
tional Linguistics and Intelligent Text Processing.
Springer.
Martin Volk and Simon Clematide. 2001. Learn-
filter-apply-forget. Mixed approaches to named en-
tity recognition. In Ana M. Moreno and Reind P.
van de Riet, editors, Applications of Natural Lan-
guage for Information Systems. Proc. of 6th Inter-
national Workshop NLDB?01, volume P-3 of Lec-
ture Notes in Informatics (LNI) - Proceedings, pages
153?163, Madrid.
Martin Volk and Yvonne Samuelsson. 2004. Boot-
strapping parallel treebanks. In Proc. of Work-
shop on Linguistically Interpreted Corpora (LINC)
at COLING, Geneva.
Martin Volk and Yvonne Samuelsson. 2007. Frame-
semantic annotation on a parallel treebank. In Proc.
of Nodalida Workshop on Building Frame Semantics
Resources for Scandinavian and Baltic Languages,
Tartu.
Martin Volk, Joakim Lundborg, and Mae?l Mettler.
2007. A search tool for parallel treebanks. In Proc.
of Workshop on Linguistic Annotation at ACL, pages
85?92, Prague.
Martin Volk, Noah Bubenhofer, Adrian Althaus, Maya
Bangerter, Lenz Furrer, and Beni Ruef. 2010. Chal-
lenges in building a multilingual alpine heritage cor-
pus. In Proceedings of LREC, Malta.
Ventsislav Zhechev. 2009. Automatic Generation of
Parallel Treebanks. An Efficient Unsupervised Sys-
tem. Ph.D. thesis, School of Computing at Dublin
City University.
196
