Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1362?1370,
Beijing, August 2010
A Minimum Error Weighting Combination Strategy for Chinese
Semantic Role Labeling
Tao Zhuang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{tzhuang, cqzong}@nlpr.ia.ac.cn
Abstract
Many Semantic Role Labeling (SRL)
combination strategies have been pro-
posed and tested on English SRL task.
But little is known about how much Chi-
nese SRL can benefit from system combi-
nation. And existing combination strate-
gies trust each individual system?s output
with the same confidence when merging
them into a pool of candidates. In our ap-
proach, we assign different weights to dif-
ferent system outputs, and add a weighted
merging stage to the conventional SRL
combination architecture. We also pro-
pose a method to obtain an appropriate
weight for each system?s output by min-
imizing some error function on the devel-
opment set. We have evaluated our strat-
egy on Chinese Proposition Bank data set.
With our minimum error weighting strat-
egy, the F1 score of the combined result
achieves 80.45%, which is 1.12% higher
than baseline combination method?s re-
sult, and 4.90% higher than the best in-
dividual system?s result.
1 Introduction
In recent years, Chinese Semantic Role Labeling
has received much research effort (Sun and Juraf-
sky, 2004; Xue, 2008; Che et al, 2008; Ding and
Chang, 2008; Sun et al, 2009; Li et al, 2009).
And Chinese SRL is also included in CoNLL-
2009 shared task (Hajic? et al, 2009). On the data
set used in (Xue, 2008), the F1 score of the SRL
results using automatic syntactic analysis is still
in low 70s (Xue, 2008; Che et al, 2008; Sun et
al., 2009). As pointed out by Xue (Xue, 2008),
the SRL errors are mainly caused by the errors
in automatic syntactic analysis. In fact, Chinese
SRL suffers from parsing errors even more than
English SRL, because the state-of-the-art parser
for Chinese is still not as good as that for En-
glish. And previous research on English SRL
shows that combination is a robust and effective
method to alleviate SRL?s dependency on pars-
ing results (Ma`rquez et al, 2005; Koomen et
al., 2005; Pradhan et al, 2005; Surdeanu et al,
2007; Toutanova et al, 2008). However, the ef-
fect of combination for Chinese SRL task is still
unknown. This raises two questions at least: (1)
How much can Chinese SRL benefit from combi-
nation? (2) Can existing combination strategies
be improved? All existing combination strate-
gies trust each individual system?s output with the
same confidence when putting them into a pool
of candidates. But according to our intuition, dif-
ferent systems have different performance. And
the system that have better performance should
be trusted with more confidence. We can use our
prior knowledge about the combined systems to
do a better combination.
The observations above motivated the work in
this paper. Instead of directly merging outputs
with equal weights, different outputs are assigned
different weights in our approach. An output?s
weight stands for the confidence we have in that
output. We acquire these weights by minimizing
an error function on the development set. And
we use these weights to merge the outputs. In
this paper, outputs are generated by a full parsing
based Chinese SRL system and a shallow parsing
based SRL system. The full parsing based system
1362
use multiple parse trees to generate multiple SRL
outputs. Whereas the shallow parsing based sys-
tem only produce one SRL output. After merging
all SRL outputs, we use greedy and integer lin-
ear programming combination methods to com-
bine the merged outputs.
We have evaluated our combination strategy on
Chinese Propbank data set used in (Xue, 2008)
and get encouraging results. With our minimum
error weighting (MEW) strategy, the F1 score
of the combined result achieves 80.45%. This
is a significant improvement over the best re-
ported SRL performance on this data set, which
is 74.12% in the literature (Sun et al, 2009).
2 Related work
A lot of research has been done on SRL combina-
tion. Most of them focused on English SRL task.
But the combination methods are general. And
they are closely related to the work in this paper.
Punyakanok et al (2004) formulated an Integer
Linear Programming (ILP) model for SRL. Based
on that work, Koomen et al (2005) combined sev-
eral SRL outputs using ILP method. Ma`rquez et
al. (2005) proposed a combination strategy that
does not require the individual system to give a
score for each argument. They used a binary clas-
sifier to filter different systems? outputs. Then
they used a greedy method to combine the can-
didates that pass the filtering process. Pradhan
et al (2005) combined systems that are based on
phrase-structure parsing, dependency parsing, and
shallow parsing. They also used greedy method
when combining different outputs. Surdeanu et
al. (2007) did a complete research on a variety of
combination strategies. All these research shows
that combination can improve English SRL per-
formance by 2?5 points on F1 score. However,
little is known about how much Chinese SRL can
benefit from combination. And, as we will show,
existing combination strategies can still be im-
proved.
3 Individual SRL Systems
3.1 Full Parsing Based System
The full parsing based system utilize full syn-
tactic analysis to perform semantic role labeling.
We implemented a Chinese semantic role label-
ing system similar to the one described in (Xue,
2008). Our system consists of an argument identi-
fication stage and an argument classification stage.
In the argument identification stage, a number of
argument locations are identified in a sentence.
In the argument classification stage, each location
identified in the first stage is assigned a semantic
role label. The features used in this paper are the
same with those used in (Xue, 2008).
Maximum entropy classifier is employed for
both the argument identification and classification
tasks. And Zhang Le?s MaxEnt toolkit1 is used for
implementation.
3.2 Shallow Parsing Based System
The shallow parsing based system utilize shal-
low syntactic information at the level of phrase
chunks to perform semantic role labeling. Sun
et al (2009) proposed such a system on Chinese
SRL and reported encouraging results. The sys-
tem used in this paper is based on their approach.
For Chinese chunking, we adopted the method
used in (Chen et al, 2006), in which chunking is
regarded as a sequence labeling task with IBO2
representation. The features used for chunking
are the uni-gram and bi-gram word/POS tags with
a window of size 2. The SRL task is also re-
garded as a sequence labeling problem. For an
argument with label ARG*, we assign the label
B-ARG* to its first chunk, and the label I-ARG*
to its rest chunks. The chunks outside of any argu-
ment are assigned the label O. The features used
for SRL are the same with those used in the one-
stage method in (Sun et al, 2009).
In this paper, we employ Tiny SVM along with
Yamcha (Kudo and Matsumoto, 2001) for Chi-
nese chunking, and CRF++2 for SRL.
3.3 Individual systems? outputs
The maximum entropy classifier used in full pars-
ing based system and the CRF model used in shal-
low paring based system can both output classi-
fication probabilities. For the full parsing based
system, the classification probability of the ar-
1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit
.html
2http://crfpp.sourceforge.net/
1363
gument classification stage is used as the argu-
ment?s probability. Whereas for the shallow pars-
ing based system, an argument is usually com-
prised of multiple chunks. For example, an argu-
ment with label ARG0 may contain three chunks
labeled as: B-ARG0, I-ARG0, I-ARG0. And each
chunk has a label probability. Thus we have three
probabilities p1, p2, p3 for one argument. In this
case, we use the geometric mean of individual
chunks? probabilities (p1 ? p2 ? p3)1/3 as the ar-
gument?s probability.
As illustrated in Figure 1, in an individual sys-
tem?s output, each argument has three attributes:
its location in sentence loc, represented by the
number of its first word and last word; its semantic
role label l; and its probability p.
Sent: ?????????????????
Args: [ ARG0 ] [pred] [ ARG1 ]
loc: (0, 2) (4, 7)
l: ARG0 ARG1
p: 0.94 0.92
Figure 1: Three attributes of an output argument:
location loc, label l, and probability p.
So each argument outputted by a system is a
triple (loc, l, p). For example, the ARG0 in Fig-
ure 1 is ((0, 2),ARG0, 0.94). Because the outputs
of baseline systems are to be combined, we call
such triple a candidate for combination.
4 Approach Overview
As illustrated in Figure 2, the architecture of our
system consists of a candidates generation stage, a
weighted merging stage, and a combination stage.
In the candidates generation stage, the baseline
systems are run individually and their outputs are
collected. We use 2-best parse trees of Berkeley
parser (Petrov and Klein, 2007) and 1-best parse
tree of Bikel parser (Bikel, 2004) and Stanford
parser (Klein and Manning, 2003) as inputs to the
full parsing based system. The second best parse
tree of Berkeley parser is used here for its good
quality. So together we have four different out-
puts from the full parsing based system. From the
shallow parsing based system, we have only one
output.
Sentence
Weighted
merging
Full parsing based SRL system Shallow parsing based SRL system
Berkeley
parser
Bikel
parser
Stanford
parser Chunker
Output1 Output4Output3Output2 Output5
Candidates pool
Combination
Final results
Candidates
Generation
Stage
Weigthed
Merging
Stage
Combination
Stage
Figure 2: The overall architecture of our system.
In the weighted merging stage, each system
output is assigned a weight according to our prior
knowledge obtained on the development set. De-
tails about how to obtain appropriate weights will
be explained in Section 6. Then all candidates
with the same loc and l are merged to one by
weighted summing their probabilities. Specifi-
cally, suppose that there are n system outputs to
be combined, with the i-th output?s weight to be
wi. And the candidate in the i-th output with loc
and l is (loc, l, pi) (If there is no candidate with loc
and l in the i-th output, pi is 0.). Then the merged
candidate is (loc, l, p), where p = ?ni=1 wipi.
After the merging stage, a pool of merged can-
didates is obtained. In the combination stage,
candidates in the pool are combined to form a
consistent SRL result. Greedy and integer lin-
ear programming combination methods are exper-
imented in this paper.
1364
5 Combination Methods
5.1 Global constraints
When combining the outputs, two global con-
straints are enforced to resolve the conflict be-
tween outputs. These two constraints are:
1. No duplication: There is no duplication for
key arguments: ARG0 ? ARG5.
2. No overlapping: Arguments cannot overlap
with each other.
We say two argument candidates conflict with
each other if they do not satisfy the two constraints
above.
5.2 Two combination methods
Under these constraints, two methods are explored
to combine the outputs. The first one is a greedy
method. In this method, candidates with probabil-
ity below a threshold are deleted at first. Then the
remaining candidates are inspected in descending
order according to their probabilities. And each
candidate will be put into a solution set if it does
not conflict with candidates already in the set.
This greedy combination method is very simple
and has been adopted in previous research (Prad-
han et al, 2005; Ma`rquez et al, 2005).
The second combination method is integer lin-
ear programming (ILP) method. ILP method was
first applied to SRL in (Punyakanok et al, 2004).
Here we formulate an ILP model whose form is
different from the model in (Punyakanok et al,
2004; Koomen et al, 2005). For convenience, we
denote the whole label set as {l1, l2, . . . , ln}. And
let l1 ? l6 stand for the key argument labels ARG0
? ARG5 respectively. Suppose there are m differ-
ent locations, denoted as loc1, . . . , locm, among
all candidates in the pool. And the probability of
assigning lj to loci is pij . A binary variable xij is
defined as:
xij =
{
1 if loci is assigned label lj ,
0 otherwise.
The objective of the ILP model is to maximize the
sum of arguments? probabilities:
max
m?
i=1
n?
j=1
(pij ? T )xij (1)
where T is a threshold to prevent including too
many candidates in solution. T is similar to the
threshold in greedy combination method. In this
paper, both thresholds are empirically tuned on
development data, and both are set to be 0.2.
The inequalities in equation (2) make sure that
each loc is assigned at most one label.
?1 ? i ? m :
n?
j=1
xij ? 1 (2)
The inequalities in equation (3) satisfy the No
duplication constraint.
?1 ? j ? 6 :
m?
i=1
xij ? 1 (3)
For any location loci, let Ci denote the index
set of the locations that overlap with it. Then
the No overlapping constraint means that if loci
is assigned a label, i.e., ?nj=1 xij = 1, then for
any k ? Ci, lock cannot be assigned any label,
i.e., ?nj=1 xkj = 0. A common technique in ILP
modeling to form such a constraint is to use a suf-
ficiently large auxiliary constant M . And the con-
straint is formulated as:
?1 ? i ? m :
?
k?Ci
n?
j=1
xkj ? (1?
n?
j=1
xij)M
(4)
In this case, M only needs to be larger than the
number of candidates to be combined. In this pa-
per, M = 500 is large enough. And we employ
lpsolve3 to solve the ILP model.
Note that the form of the ILP model in this
paper is different from that in (Punyakanok et
al., 2004; Koomen et al, 2005) in three as-
pects: (1) A special label class null, which means
no label is assigned, was added to the label set
in (Punyakanok et al, 2004; Koomen et al, 2005).
Whereas no such special class is needed in our
model, because if no label is assigned to loci,?n
j=1 xij = 0 would simply indicate this case.
This makes our model contain fewer variables.
(2) Without null class in our model, we need to
use a different technique to formulate the No-
overlapping constraint. (3) In order to compare
3http://lpsolve.sourceforge.net/
1365
with the greedy combination method, the ILP
model in this paper conforms to exactly the same
constraints as the greedy method. Whereas many
more global constraints were taken into account
in (Punyakanok et al, 2004; Koomen et al, 2005).
6 Train Minimum Error Weights
The idea of minimum error weighting is straight-
forward. Individual outputs O1, O2, . . . , On
are assigned weights w1, w2, . . . , wn respectively.
These weights are normalized, i.e., ?ni=1 wi = 1.
An output?s weight can be seen as the confidence
we have in that output. It is a kind of prior knowl-
edge we have about that output. We can gain this
prior knowledge on the development set. As long
as the data of the development set and the test set
are similar, this prior knowledge should be able
to help to guide SRL combination on test set. In
this section, we discuss how to obtain appropriate
weights.
6.1 Training model
Suppose the golden answer and SRL result on de-
velopment set are d and r respectively. An error
function Er(r, d) is a function that measures the
error contained in r in reference to d. An error
function can be defined as the number of wrong
arguments in r. It can also be defined using preci-
sion, recall, or F1 score. For example, Er(r, d) =
1? Precision(r, d), or Er(r, d) = 1? F1(r, d).
Smaller value of error function means less error in
r.
The combination process can also be seen as
a function, which maps the outputs and weights
to the combined result r: r = Comb(On1 , wn1 ).
Therefore, the error function of our system on de-
velopment set is:
Er(r, d) = Er(Comb(On1 , wn1 ), d) (5)
From equation (5), it can be seen that: Given de-
velopment set d, if the outputs to be combined On1
and the combination method Comb are fixed, the
error function is just a function of the weights. So
we can obtain appropriate weights by minimizing
the error function:
w?n1 = argminwn1
Er(Comb(On1 , wn1 ), d) (6)
6.2 Training algorithm
Algorithm 1 Powell Training Algorithm.
1: Input : Error function Er(w).
2: Initialize n directions d1, . . . ,dn, and
a start point w in Rn.
3: Set termination threshold ?.
4: do:
5: w1 ? w
6: for i ? 1, . . . , n:
7: ?i ? argmin? f(wi + ?di)
8: wi+1 ? wi + ?idi
9: dn+1 ? wn+1 ?w
10: ?? ? argmin
?
f(w + ?dn+1)
11: w? ? w + ??dn+1
12: ?Er ? Er(w)? Er(w?)
13: i ? arg max
1?j?n
Er(wj)? Er(wj+1)
14: if (??)2 ? ?ErEr(wi)? Er(wi+1) :
15: for j ? i, . . . , n:
16: dj ? dj+1
17: w ? w?
18: while ?Er > ?
19: Output: The minimum error weights w.
There are two difficulties to solve the optimiza-
tion problem in equation 6. The first one is that
the error function cannot be written to an analyt-
ical form. This is because the Comb function,
which stands for the combination process, cannot
be written as an analytical formula. So the prob-
lem cannot be solved using canonical gradient-
based optimization algorithms, because the gradi-
ent function cannot be derived. The second diffi-
culty is that, according to our experience, the er-
ror function has many local optima, which makes
it difficult to find a global optima.
To resolve the first difficulty, Modified Powell?s
method (Yuan, 1993) is employed to solve the op-
timization problem. Powell?s method is a heuris-
tic search method that does not require the objec-
tive function to have an explicit analytical form.
The training algorithm is presented in Algorithm
1. In Algorithm 1, the line search problem in steps
7 and 10 is solved using Brent?s method (Yuan,
1993). And the temination threshold ? is empiri-
cally set to be 0.001 in this paper.
1366
To resolve the second difficulty, we perform
multiple searches using different start points, and
then choose the best solution found.
7 Experiments
7.1 Experimental setup
We use Chinese Proposition Bank (CPB) 1.0 and
Chinese Tree Bank (CTB) 5.0 of Linguistic Data
Consortium corpus in our experiments. The train-
ing set is comprised of 648 files(chtb 081.fid to
chtb 885.fid). The development set is comprised
of 40 files(chtb 041.fid to chtb 080.fid). The
test set is comprised of 72 files(chtb 001.fid to
chtb 040.fid and chtb 900.fid to chtb 931.fid).
The same data setting has been used in (Xue,
2008; Ding and Chang, 2008; Sun et al, 2009).
Sun et al (2009) used sentences with golden seg-
mentation and POS tags as input to their SRL
system. However, we use sentences with only
golden segmentation as input. Then we perform
automatic POS tagging using Stanford POS tag-
ger (Toutanova et al, 2003). In (Xue, 2008), the
parser used by the SRL system is trained on the
training and development set plus 275K words of
broadcast news. In this paper, all parsers used
by the full parsing based system are trained on
the training set plus the broadcast news portion
of CTB6.0. And the chunker used in the shallow
parsing based system is trained just on the training
set.
7.2 Individual outputs? performance
In this paper the four outputs of the full parsing
based system are represented by FO1 ? FO4 re-
spectively. Among them, FO1 and FO2 are the
outputs using the first and second best parse trees
of Berkeley parser, FO3 and FO4 are the outputs
using the best parse trees of Stanford parser and
Bikel parser respectively. The output of the shal-
low parsing based system is represented by SO.
The individual outputs? performance on develop-
ment and test set are listed in Table 1.
From Table 1 we can see that the performance
of individual outputs are similar on development
set and test set. On both sets, the F1 scores of
individual outputs are in the same order: FO1 >
FO2 > SO > FO3 > FO4.
Data set Outputs P (%) R(%) F1
FO1 79.17 72.09 75.47
FO2 77.89 70.56 74.04
development FO3 72.57 67.02 69.68
FO4 75.60 63.45 69.00
SO 73.72 67.35 70.39
FO1 80.75 70.98 75.55
FO2 79.44 69.37 74.06
test FO3 73.95 66.37 70.00
FO4 75.89 63.26 69.00
SO 75.69 67.90 71.59
Table 1: The results of individual systems on de-
velopment and test set.
7.3 Combining outputs of full parsing based
system
In order to investigate the benefit that the full
parsing based system can get from using multi-
ple parsers, we combine the four outputs FO1 ?
FO4. The combination results are listed in Ta-
ble 2. In tables of this paper, ?Grd? and ?ILP?
stand for greedy and ILP combination methods re-
spectively, and ?+MEW? means the combination
is performed with MEW strategy.
P (%) R(%) F1
Grd 82.68 73.36 77.74
ILP 82.21 73.93 77.85
Grd+MEW 81.30 75.38 78.23
ILP+MEW 81.27 75.74 78.41
Table 2: The results of combining outputs of full
parsing based system on test set.
Er FO1 FO2 FO3 FO4
Grd 1? F1 0.31 0.16 0.30 0.23
ILP 1? F1 0.33 0.10 0.27 0.30
Table 3: The minimum error weights for the re-
sults in Table 2.
From Table 2 and Table 1, we can see that, with-
out MEW strategy, the F1 score of combination
result is about 2.3% higher than the best individ-
ual output. With MEW strategy, the F1 score is
improved about 0.5% further. That is to say, with
MEW strategy, the benefit of combination is im-
proved by about 20%. Therefore, the effect of
MEW is very encouraging.
Here the error function for MEW training is
chosen to be 1 ? F1. And the trained weights
for greedy and ILP methods are listed in Table 3
1367
separately. In tables of this paper, the column Er
corresponds to the error function used for MEW
strategy.
7.4 Combining all outputs
We have also combined all five outputs. The re-
sults are listed in Table 4. Compared with the re-
sults in Table 2, we can see that the combination
results is largely improved, especially the recall.
P (%) R(%) F1
Grd 83.64 75.32 79.26
ILP 83.31 75.71 79.33
Grd+MEW 83.34 77.47 80.30
ILP+MEW 83.02 78.03 80.45
Table 4: The results of combining all outputs on
test set.
From Table 4 and Table 1 we can see that with-
out MEW strategy, the F1 score of combination
result is about 3.8% higher than the best individ-
ual output. With MEW, the F1 score is improved
further by more than 1%. That means the bene-
fit of combination is improved by over 25% with
MEW strategy.
Here the error function for MEW training is still
1 ? F1, and the trained weights are listed in Ta-
ble 5.
Er FO1 FO2 FO3 FO4 SO
Grd 1? F1 0.23 0.12 0.23 0.20 0.22
ILP 1? F1 0.24 0.08 0.22 0.21 0.25
Table 5: The minimum error weights for the re-
sults in Table 4.
7.5 Using alternative error functions for
minimum error weights training
In previous experiments, we use 1 ? F1 as error
function. As pointed out in Section 6, the def-
inition of error function is very general. So we
have experimented with two other error functions,
which are 1 ? Precision, and 1 ? Recall. Ob-
viously, these two error functions favor precision
and recall separately. The results of combining
all five outputs using these two error functions are
listed in Table 6, and the trained weights are listed
in Table 7.
From Table 6 and Table 4, we can see that when
1 ? Precison is used as error function, the pre-
Er P (%) R(%) F1
Grd+MEW 1? P 85.31 73.42 78.92
ILP+MEW 1? P 85.62 72.76 78.67
Grd+MEW 1?R 81.94 77.55 79.68
ILP+MEW 1?R 79.74 78.34 79.03
Table 6: The results of combining all outputs with
alternative error functions.
Er FO1 FO2 FO3 FO4 SO
Grd 1? P 0.25 0.24 0.22 0.22 0.07
ILP 1? P 0.30 0.26 0.20 0.15 0.09
Grd 1?R 0.21 0.10 0.17 0.15 0.37
ILP 1?R 0.24 0.04 0.10 0.22 0.39
Table 7: The minimum error weights for the re-
sults in Table 6.
cision of combination result is largely improved.
But the recall decreases a lot. Similar effect of the
error function 1?Recall is also observed.
The results of this subsection reflect the flex-
ibility of MEW strategy. This flexibility comes
from the generality of the definition of error func-
tion. The choice of error function gives us some
control over the results we want to get. We can
define different error functions to favor precision,
or recall, or some error counts such as the number
of misclassified arguments.
7.6 Discussion
In this paper, the greedy and ILP combination
methods conform to the same simple constraints
specified in Section 5. From the experiment
results, we can see that ILP method generates
slightly better results than greedy method.
In Subsection 7.4, we see that combining all
outputs using ILP method with MEW strategy
yields 4.90% improvement on F1 score over the
best individual output FO1. In order to under-
stand each output?s contribution to the improve-
ment over FO1. We compare the differences be-
tween outputs.
Let CO denote the set of correct arguments in
an output O. Then we get the following statistics
when comparing two outputs A and B: (1) the
number of common correct arguments in A and
B, i.e., |CA ? CB| ; (2) the number of correct ar-
guments in A and not in B, i.e., |CA \CB|; (3) the
number of correct arguments in B and not in A,
i.e., |CB \ CA|. The comparison results between
1368
some outputs on test set are listed in Table 8. In
this table, UF stands for the union of the 4 outputs
FO1 ? FO4.
A B |CA ? CB | |CA \ CB | |CB \ CA|
FO2 5498 508 372
FO3 5044 962 552
FO4 4815 1191 512FO1
SO 4826 1180 920
UF SO 5311 1550 435
Table 8: Comparison between outputs on test set.
From Table 8 we can see that the output SO
has 4826 common correct arguments with FO1,
which is relatively small. And, more importantly,
SO contains 920 correct arguments not in FO1,
which is much more than any other output con-
tains. Therefore, SO is more complementary to
FO1 than other outputs. On the contrary, FO2 is
least complementary to FO1. Even compared with
the union of FO1 ? FO4, SO still contains 435
correct arguments not in the union. This shows
that the output of shallow parsing based system is
a good complement to the outputs of full parsing
based system. This explains why recall is largely
improved when SO is combined in Subsection 7.4.
From the analysis above we can also see that the
weights in Table 5 are quite reasonable. In Ta-
ble 5, SO is assigned the largest weight and FO2
is assigned the smallest weight.
In Subsection 7.3, the MEW strategy improves
the benefit of combination by about 20%. And in
Subsection 7.4, the MEW strategy improves the
benefit of combination by over 25%. This shows
that the MEW strategy is very effective for Chi-
nese SRL combination.
To our best knowledge, no results on Chinese
SRL combination has been reported in the litera-
ture. Therefore, to compare with previous results,
the top two results of single SRL system in the
literature and the result of our combination sys-
tem on this data set are listed in Table 9. For the
results in Table 9, the system of Sun et al uses
sentences with golden POS tags as input. Xue?s
system and our system both use sentences with
automatic POS tags as input. The result of Sun
et al (2009) is the best reported result on this data
set in the literature.
POS P (%) R(%) F1
(Xue, 2008) auto 76.8 62.5 68.9
(Sun et al, 2009) gold 79.25 69.61 74.12
Ours auto 83.02 78.03 80.45
Table 9: Previous best single system?s results and
our combination system?s result on this data set.
8 Conclusions
In this paper, we propose a minimum error
weighting strategy for SRL combination and in-
vestigate the benefit that Chinese SRL can get
from combination. We assign different weights to
different system outputs and add a weighted merg-
ing stage to conventional SRL combination sys-
tem architecture. And we also propose a method
to train these weights on development set. We
evaluate the MEW strategy on Chinese Propbank
data set with greedy and ILP combination meth-
ods.
Our experiments have shown that the MEW
strategy is very effective for Chinese SRL combi-
nation, and the benefit of combination can be im-
proved over 25% with this strategy. And also, the
MEW strategy is very flexible. With different def-
initions of error function, this strategy can favor
precision, or recall, or F1 score. The experiments
have also shown that Chinese SRL can benefit a
lot from combination, especially when systems
based on different syntactic views are combined.
The SRL result with the highest F1 score in this
paper is generated by ILP combination together
with MEW strategy. In fact, the MEW strategy is
easy to incorporate with other combination meth-
ods, just like incorporating with the greedy and
ILP combination methods in this paper.
Acknowledgment
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053, 90820303 and 60736014,
the National Key Technology R&D Program un-
der Grant No. 2006BAH03B02, the Hi-Tech Re-
search and Development Program (?863? Pro-
gram) of China under Grant No. 2006AA010108-
4, and also supported by the China-Singapore In-
stitute of Digital Media (CSIDM) project under
grant No. CSIDM-200804.
1369
References
Daniel Bikel. 2004. Intricacies of Collins Parsing
Model. Computational Linguistics, 30(4):480-511.
Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a Hybrid Con-
volution Tree Kernel for Semantic Role Labeling.
ACM Transactions on Asian Language Information
Processing, 2008, 7(4).
Wenliang Chen, Yujie Zhang, and Hitoshi Isahara.
2006. An empirical study of Chinese chunking. In
Proceedings of COLING/ACL-2006.
Weiwei Ding and Baobao Chang. 2008. Improving
Chinese Semantic Role Classification with Hierar-
chical Feature Selection Strategy. In Proceedings of
EMNLP-2008.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-
2009 Shared Task: Syntactic and Semantic Depen-
dencies in Multiple Languages. In Proceedings of
CoNLL-2009.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of ACL-
2003.
Peter Koomen, Vasin Punyakanok, Dan Roth, and
Wen-tau Yih. 2005. Generalized Inference with
Multiple Semantic Role Labeling Systems. In Pro-
ceedings of CoNLL-2005 shared task.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proceedings of
NAACL-2001.
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving Nominal SRL
in Chinese Language with Verbal SRL Information
and Automatic Predicate Recognition. In Proceed-
ings of EMNLP-2009.
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and
Jordi Turmo. 2005. A Robust Combination Strat-
egy for Semantic Role Labeling. In Proceedings of
EMNLP-2005.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized parsing. In Proceedings of ACL-
2007.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2005. Se-
mantic Role Labeling Using Different Syntactic
Views. In Proceedings of ACL-2005.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING-2004.
Honglin Sun and Daniel Jurafsky. 2004. Shallow
semantic parsing of Chinese. In Proceedings of
NAACL-2004.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese Semantic Role Labeling with Shal-
low Parsing. In Proceedings of EMNLP-2009.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artificial Intel-
ligence Research (JAIR), 29:105-151.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Se-
mantic Role Labeling. Computational Linguistics,
34(2): 145-159.
Kristina Toutanova, Dan Klein, Christopher Manning,
and Yoram Singer. 2003. Feature-Rich Part-of-
Speech Tagging with a Cyclic Dependency Net-
work. In Proceedings of HLT-NAACL-2003.
Nianwen Xue. 2008. Labeling Chinese Predicates
with Semantic Roles. Computational Linguistics,
34(2): 225-255.
Yaxiang Yuan. 1993. Numerical Methods for Nonlin-
ear Programming. Shanghai Scientific and Techni-
cal Pulishers, Shanghai.
1370
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 304?314,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Joint Inference for Bilingual Semantic Role Labeling
Tao Zhuang and Chengqing Zong
National Laboratory of Pattern Recognition
Institute of Automation, Chinese Academy of Sciences
{tzhuang, cqzong}@nlpr.ia.ac.cn
Abstract
We show that jointly performing semantic role
labeling (SRL) on bitext can improve SRL
results on both sides. In our approach, we
use monolingual SRL systems to produce ar-
gument candidates for predicates in bitext at
first. Then, we simultaneously generate SRL
results for two sides of bitext using our joint
inference model. Our model prefers the bilin-
gual SRL result that is not only reasonable on
each side of bitext, but also has more consis-
tent argument structures between two sides.
To evaluate the consistency between two argu-
ment structures, we also formulate a log-linear
model to compute the probability of aligning
two arguments. We have experimented with
our model on Chinese-English parallel Prop-
Bank data. Using our joint inference model,
F1 scores of SRL results on Chinese and En-
glish text achieve 79.53% and 77.87% respec-
tively, which are 1.52 and 1.74 points higher
than the results of baseline monolingual SRL
combination systems respectively.
1 Introduction
In recent years, there has been an increasing inter-
est in SRL on several languages. However, little
research has been done on how to effectively per-
form SRL on bitext, which has important applica-
tions including machine translation (Wu and Fung,
2009). A conventional way to perform SRL on bi-
text is performing SRL on each side of bitext sep-
arately, as has been done by Fung et al (2007) on
Chinese-English bitext. However, it is very difficult
to obtain good SRL results on both sides of bitext
in this way. The reason is that even the state-of-
the-art SRL systems do not have very high accuracy
on both English text (Ma`rquez et al, 2008; Pradhan
et al, 2008; Punyakanok et al, 2008; Toutanova et
al., 2008), and Chinese text (Che et al, 2008; Xue,
2008; Li et al, 2009; Sun et al, 2009).
On the other hand, the semantic equivalence be-
tween two sides of bitext means that they should
have consistent predicate-argument structures. This
bilingual argument structure consistency can guide
us to find better SRL results. For example, in Fig-
ure 1(a), the argument structure consistency can
guide us to choose a correct SRL result on Chinese
side. Consistency between two argument structures
is reflected by sound argument alignments between
them, as shown in Figure 1(b). Previous research has
shown that bilingual constraints can be very help-
ful for parsing (Burkett and Klein, 2008; Huang et
al., 2008). In this paper, we show that the bilingual
argument structure consistency can be leveraged to
substantially improve SRL results on both sides of
bitext.
Formally, we present a joint inference model to
preform bilingual SRL. Using automatic word align-
ment on bitext, we first identify a pair of predicates
that align with each other. And we use monolin-
gual SRL systems to produce argument candidates
for each predicate. Then, our model jointly generate
SRL results for both predicates from their argument
candidates, using integer linear programming (ILP)
technique. An overview of our approach is shown in
Figure 2.
Our joint inference model consists of three com-
ponents: the source side, the target side, and the ar-
304
In recent years the pace of opening up to the outside of China `s construction market    has   further    accelerated
[    AM-TMP    ]  [                                                        A1                                                 ]          [   A2  ]   [   Pred   ]
R1:    [                A1                  ]    [ AM-TMP ]    [            C-A1           ]     [ AM-ADV ]    [Pred]
R2:    [                                                     A1                                           ]     [ AM-ADV ]    [Pred]
?? ?? ?? ?? ? ? ? ?? ?? ??? ??
      zhongguo jianzhu shichang      jinnian lai        dui wai kaifang bufa         jinyibu         jiakuai
[    AM-TMP   ] [                                                        A1                                                  ]         [   A2   ]  [    Pred    ]
In recent years the pace of opening up to the outside of China `s construction market    has   further    accelerated
?? ?? ?? ?? ? ? ? ?? ?? ??? ??
[ A1 ] [ AM-TMP ] [ C-A1          ]    [AM-ADV]    [Pred]
(a) Word alignment and SRL results for a Chinese-English predicate pair.
(b) Argument alignments for a Chinese-English predicate pair.
Figure 1: An example from Chinese-English parallel PropBank. In (a), the SRL results are generated by the state-
of-the-art monolingual SRL systems. The English SRL result is correct. But it is to more difficult to get correct
SRL result on Chinese side, because the AM-TMP argument embeds into a discontinuous A1 argument. The Chinese
SRL result in the row marked by ?R1? is correct and consistent with the result on English side. Whereas the result in
the row marked by ?R2? is incorrect and inconsistent with the result on English side, with the circles showing their
inconsistency. The argument structure consistency can guide us to choose the correct Chinese SRL result.
Monolingual
SRL System
Monolingual
SRL System
Our Joint
Inference
Model
Source-side
 Predidate
Target side
Predicate
Source-side
SRL 
Candidates
Target-side
SRL 
Candidates BilingualSRL Result
Figure 2: Overview of our approach.
gument alignment between two sides. These three
components correspond to three interrelated factors:
the quality of the SRL result on source side, the qual-
ity of the SRL result on target side, and the argu-
ment structure consistency between the SRL results
on both sides. To evaluate the consistency between
the two argument structures in our joint inference
model, we formulate a log-linear model to compute
the probability of aligning two arguments. Experi-
ments on Chinese-English parallel PropBank shows
that our model significantly outperforms monolin-
gual SRL combination systems on both Chinese and
English sides.
The rest of this paper is organized as follows: Sec-
tion 2 introduces related work. Section 3 describes
how we generate SRL candidates on each side of bi-
text. Section 4 presents our joint inference model.
Section 5 presents our experiments. And Section 6
concludes our work.
2 Related Work
Some existing work on monolingual SRL combina-
tion is related to our work. Punyakanok et al (2004;
2008) formulated an ILP model for SRL. Koomen
et al (2005) combined several SRL outputs using
ILP method. Ma`rquez et al (2005) and Pradhan et
al. (2005) proposed combination strategies that are
not based on ILP method. Surdeanu et al (2007)
did a complete research on a variety of combination
strategies. Zhuang and Zong (2010) proposed a min-
imum error weighting combination strategy for Chi-
nese SRL combination.
Research on SRL utilizing parallel corpus is also
related to our work. Pado? and Lapata (2009) did
research on cross-lingual annotation projection on
English-German parallel corpus. They performed
SRL only on the English side, and then mapped
the English SRL result to German side. Fung et
al. (2007) did pioneering work on studying argu-
ment alignment on Chinese-English parallel Prop-
Bank. They performed SRL on Chinese and En-
glish sides separately. Then, given the SRL result
on both sides, they automatically induced the argu-
ment alignment between two sides.
The major difference between our work and all
existing research is that our model performs SRL in-
ference on two sides of bitext simultaneously. In our
305
model, we jointly consider three interrelated factors:
SRL result on the source side, SRL result on the tar-
get side, and the argument alignment between them.
3 Generating Candidates for Inference
3.1 Monolingual SRL System
As shown in Figure 2, we need to use a monolin-
gual SRL system to generate candidates for our joint
inference model. We have implemented a monolin-
gual SRL system which utilize full phrase-structure
parse trees to perform SRL. In this system, the whole
SRL process is comprised of three stages: pruning,
argument identification, and argument classification.
In the pruning stage, the heuristic pruning method
in (Xue, 2008) is employed. In the argument iden-
tification stage, a number of argument locations are
identified in a sentence. In the argument classifica-
tion stage, each location identified in the previous
stage is assigned a semantic role label. Maximum
entropy classifier is employed for both the argument
identification and classification tasks. And Zhang
Le?s MaxEnt toolkit1 is used for implementation.
We use the monolingual SRL system described
above for both Chinese and English SRL tasks. For
the Chinese SRL task, the features used in this paper
are the same with those used in (Xue, 2008). For
the English SRL task, the features used are the same
with those used in (Pradhan et al, 2008).
3.2 Output of the Monolingual SRL System
The maximum entropy classifier in our monolingual
SRL system can output classification probabilities.
We use the classification probability of the argument
classification stage as an argument?s probability. As
illustrated in Figure 3, in an individual system?s out-
put, each argument has three attributes: its location
in sentence loc, represented by the number of its first
word and last word; its semantic role label l; and its
probability p.
So each argument outputted by a system is a triple
(loc, l, p). For example, the A0 argument in Figure 3
is ((0, 2),A0, 0.94). Because these outputs are to be
combined, we call such triple a candidate.
1http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit
.html
Sent: 	??]?????I	n??O:
Args: [ A0 ] [Pred] [ A1 ]
loc: (0, 2) (4, 7)
l: A0 A1
p: 0.94 0.92
Figure 3: Three attributes of an output argument: location
loc, label l, and probability p.
3.3 Generating and Merging Candidates
To generate candidates for joint inference, we need
to have multiple SRL results on each side of bi-
text. Therefore, for both Chinese and English SRL
systems, we use the 3-best parse trees of Berkeley
parser (Petrov and Klein, 2007) and 1-best parse
trees of Bikel parser (Bikel, 2004) and Stanford
parser (Klein and Manning, 2003) as inputs. All the
three parsers are multilingual parsers. The second
and third best parse trees of Berkeley parser are used
for their good quality. Therefore, each monolingual
SRL system produces 5 different outputs.
Candidates from different outputs may have the
same loc and l but different p. So we merge all
candidates with the same loc and l into one by av-
eraging their probabilities. For a merged candidate
(loc, l, p), we say that p is the probability of assign-
ing l to loc.
4 Joint Inference Model
Our model can be conceptually decomposed to three
components: the source side, the target side, and the
argument alignment. The objective function of our
joint inference model is the weighted sum of three
sub-objectives:
max Os + ?1Ot + ?2Oa (1)
where Os and Ot represent the quality of the SRL
results on source and target sides, and Oa represents
the soundness of the argument alignment between
the SRL results on two sides, ?1, ?2 are positive
weights corresponding to the importance of Ot and
Oa respectively.
4.1 Components of Source and Target Sides
4.1.1 Source Side Component
The source side component aims to improve the
SRL result on source side. This is equivalent to a
306
monolingual SRL combination problem.
For convenience, we denote the whole semantic
role label set for source language as {ls1, l
s
2, . . . , l
s
Ls},
in which ls1 ? l
s
6 stand for the key argument labels
A0 ? A5 respectively. Suppose there are Ns differ-
ent locations, denoted as locs1, . . . , loc
s
Ns , among all
candidates on the source side. The probability of as-
signing lsj to loc
s
i is p
s
ij . An indicator variable xij is
defined as:
xij = [loc
s
i is assigned label l
s
j ].
Then the source side sub-objective Os in equation
(1) is the sum of arguments? probabilities on source
side:
Os =
Ns?
i=1
Ls?
j=1
(psij ? Ts)xij (2)
where Ts is a bias to prevent including too many can-
didates in solution (Surdeanu et al, 2007).
We consider the following two linguistically mo-
tivated constraints:
1. No duplication: There is no duplication for key
arguments: A0 ? A5.
2. No overlapping: Arguments cannot overlap
with each other.
In (Punyakanok et al, 2004), several more con-
straints are considered. According to (Surdeanu
et al, 2007), however, no significant performance
improvement can be obtained by considering more
constraints than the two above. So we do not con-
sider other constraints.
The inequalities in (3) make sure that each locsi is
assigned at most one label.
?1 ? i ? Ns :
Ls?
j=1
xij ? 1 (3)
The inequalities in (4) satisfy the No duplication
constraint.
?1 ? j ? 6 :
Ns?
i=1
xij ? 1 (4)
For any source side location locsi , let Ci denote
the index set of the locations that overlap with it.
Then the No overlapping constraint means that if
locsi is assigned a label, i.e.,
?Ns
j=1 xij = 1, then
for any u ? Ci, locsu cannot be assigned any label,
i.e.,
?Ns
j=1 xuj = 0. A common technique in ILP
modeling to form such a constraint is to use a suf-
ficiently large auxiliary constant M . And the con-
straint is formulated as:
?1 ? i ? Ns :
?
u?Ci
Ls?
j=1
xuj ? (1?
Ls?
j=1
xij)M (5)
In this case,M only needs to be larger than the num-
ber of candidates to be combined. In this paper,
M = 500 is large enough.
4.1.2 Target Side Component
In principle, the target side component of our joint
inference model is the same with the source side
component.
The whole semantic role label set for target lan-
guage is denoted by {lt1, l
t
2, . . . , l
t
Lt}. There are
Nt different locations, denoted as loct1, . . . , loc
t
Nt ,
among all candidates in the target side. And lt1 ? l
t
6
stand for the key argument labels A0 ? A5 respec-
tively. The probability of assigning ltj to loc
t
k is p
t
kj .
An indicator variable ykj is defined as:
ykj = [loc
t
k is assigned label l
t
j ].
Then the target side sub-objective Ot in equation (1)
is:
Ot =
Nt?
k=1
Lt?
j=1
(ptkj ? Tt)ykj (6)
The constraints on target side are as follows:
Each loctk is assigned at most one label:
?1 ? k ? Nt :
Lt?
j=1
ykj ? 1 (7)
The No duplication constraint:
?1 ? j ? 6 :
Nt?
k=1
ykj ? 1 (8)
The No overlapping constraint:
?1 ? k ? Nt :
?
v?Ck
Lt?
j=1
yvj ? (1?
Lt?
j=1
ykj)M (9)
In (9), Ck denotes the index set of the locations that
overlap with loctk, and the constant M is set to 500
in this paper.
307
4.2 Argument Alignment
The argument alignment component is the core of
our joint inference model. It gives preference to the
bilingual SRL results that have more consistent ar-
gument structures.
For a source side argument argsi = (loc
s
i , l
s) and
a target side argument argtk = (loc
t
k, l
t), let zik be
the following indicator variable:
zik = [arg
s
i aligns with arg
t
k].
We use paik to represent the probability that arg
s
i and
argtk align with each other, i.e., p
a
ik = P (zik = 1).
We call paik the argument alignment probability
between argsi and arg
t
k.
4.2.1 Argument Alignment Probability Model
We use a log-linear model to compute the argu-
ment alignment probability paik between arg
s
i and
argtk. Let (s, t) denote a bilingual sentence pair and
wa denote the word alignment on (s, t). Our log-
linear model defines a distribution on zik given the
tuple tup = (argsi , arg
t
k, wa, s, t):
P (zik|tup) ? exp(w
T?(tup))
where ?(tup) is the feature vector. With this model,
paik can be computed as p
a
ik = P (zik = 1|tup).
In order to study the argument alignment in cor-
pus and to provide training data for our log-linear
model, we have manually aligned the arguments in
60 files (chtb 0121.fid to chtb 0180.fid) of Chinese-
English parallel PropBank. On this data set, we get
the argument alignment matrix in Table 1.
Ch\En A0 A1 A2 A3 A4 AM* NUL
A0 492 30 4 0 0 0 46
A1 98 853 43 2 0 0 8
A2 9 57 51 1 0 47 0
A3 1 0 2 6 0 0 0
A4 0 0 2 0 3 0 0
AM* 0 2 39 0 0 895 221
NUL 53 14 27 0 0 45 0
Table 1: The argument alignment matrix on manually
aligned corpus.
Each entry in Table 1 is the number of times for
which one type of Chinese argument aligns with one
type of English argument. AM* stands for all ad-
juncts types like AM-TMP, AM-LOC, etc., and NUL
means that the argument on the other side cannot be
aligned with any argument on this side. For exam-
ple, the number 46 in the A0 row and NUL column
means that Chinese A0 argument cannot be aligned
with any argument on English side for 46 times in
our manually aligned corpus.
We use the following features in our model.
Word alignment feature: If there are many word-
to-word alignments between argsi and arg
t
k, then
it is very probable that argsi and arg
t
k would align
with each other. We adopt the method used in (Pado?
and Lapata, 2009) to measure the word-to-word
alignments between argsi and arg
t
k. And the word
alignment feature is defined as same as the word
alignment-based word overlap in (Pado? and Lapata,
2009). Note that this is a real-valued feature.
Head word alignment feature: The head word
of an argument is usually more representative than
other words. So we use whether the head words of
argsi and arg
t
k align with each other as a binary fea-
ture. The use of this feature is inspired by the work
in (Burkett and Klein, 2008).
Semantic role labels of two arguments: From Ta-
ble 1, we can see that semantic role labels of two ar-
guments are a good indicator of whether they should
align with each other. For example, a Chinese A0
argument aligns with an English A0 argument most
of the times, and never aligns with an English AM*
argument in Table 1. Therefore, the semantic role
labels of argsi and arg
t
k are used as a feature.
Predicate verb pair: Different predicate pairs have
different argument alignment patterns. Let?s take the
Chinese predicate O/zengzhang and the English
predicate grow as an example. The argument align-
ment matrix for all instances of the Chinese-English
predicate pair (zengzhang, grow) in our manually
aligned corpus is shown in Table 2.
CH \EN A0 A1 A2 AM* NUL
A0 0 16 0 0 0
A1 0 0 12 0 0
AM* 0 0 4 7 10
NUL 0 0 0 2 0
Table 2: The argument alignment matrix for the predicate
pair (zengzhang, grow).
From Table 2 we can see that all A0 arguments of
zengzhang align with A1 arguments of grow. This
308
is very different from the results in Table 1, where a
Chinese A0 argument tends to align with an English
A0 argument. This phenomenon shows that a pred-
icate pair can determine which types of arguments
should align with each other. Therefore, we use the
predicate pair as a feature.
4.2.2 Argument Alignment Component
The argument alignment sub-objective Oa in
equation (1) is the sum of argument alignment prob-
abilities:
Oa =
Ns?
i=1
Nt?
k=1
(paik ? Ta)zik (10)
where Ta is a bias to prevent including too many
alignments in final solution, and paik is computed
using the log-linear model described in subsec-
tion 4.2.1.
Oa reflects the consistency between argument
structures on two sides of bitext. Larger Oa means
better argument alignment between two sides, thus
indicates more consistency between argument struc-
tures on two sides.
The following constraints are considered:
1. Conformity with bilingual SRL result. For
all candidates on both source and target sides, only
those that are chosen to be arguments on each side
can be aligned.
2. One-to-many alignment limit. Each argument
can not be aligned with more than 3 arguments.
3. Complete argument alignment. Each argument
on source side must be aligned with at least one ar-
gument on target side, and vice versa.
The Conformity with bilingual SRL result con-
straint is necessary to validly integrate the bilingual
SRL result with the argument alignment. This con-
straint means that if argsi and arg
t
k align with each
other, i.e., zik = 1, then locsi must be assigned
a label on source side, i.e.,
?Ls
j=1 xij = 1, and
loctk must be assigned a label on target side, i.e.,?Lt
j=1 ykj = 1. So this constraint can be represented
as:
?1 ? i ? Ns, 1 ? k ? Nt :
Ls?
j=1
xij ? zik (11)
?1 ? k ? Nt, 1 ? i ? Ns :
Lt?
j=1
ykj ? zik (12)
The One-to-many alignment limit constraint
comes from our observation on manually aligned
corpus. We have found that no argument aligns with
more than 3 arguments in our manually aligned cor-
pus. This constraint can be represented as:
?1 ? i ? Ns :
Nt?
k=1
zik ? 3 (13)
?1 ? k ? Nt :
Ns?
i=1
zik ? 3 (14)
The Complete argument alignment constraint
comes from the semantic equivalence between two
sides of bitext. For each source side location locsi ,
if it is assigned a label, i.e.,
?Ls
j=1 xij = 1, then it
must be aligned with some arguments on target side,
i.e.,
?Nt
k=1 zik ? 1. This can be represented as:
?1 ? i ? Ns :
Nt?
k=1
zik ?
Ls?
j=1
xij (15)
Similarly, each target side argument must be aligned
to at least one source side argument. This can be
represented as:
?1 ? k ? Nt :
Ns?
i=1
zik ?
Lt?
j=1
ykj (16)
4.3 Complete Argument Alignment as a Soft
Constraint
Although the hard Complement argument alignment
constraint is ideally reasonable, in real situations this
constraint does not always hold. The manual argu-
ment alignment result shown in Table 1 indicates
that in some cases an argument cannot be aligned
with any argument on the other side (see the NUL
row and column in Table 1). Therefore, it would
be reasonable to change the hard Complement argu-
ment alignment constraint to a soft one. To do so,
we need to remove the hard Complement argument
alignment constraint and add penalty for violation of
this constraint.
If an argument does not align with any argument
on the other side, we say it aligns with NUL. And we
define the following indicator variables:
zi,NUL = [argsi aligns with NUL], 1 ? i ? Ns.
309
zNUL,k = [argtk aligns with NUL], 1 ? k ? Nt.
Then
?Ns
i=1 zi,NUL is the number of source side ar-
guments that align with NUL. And
?Nt
k=1 zNUL,k is
the number of target side arguments that align with
NUL. For each argument that aligns with NUL, we
add a penalty ?3 to the argument alignment sub-
objective Oa. Therefore, the sub-objective Oa in
equation (10) is changed to:
Oa =
Ns?
i=1
Nt?
k=1
(paik ? Ta)zik
??3(
Ns?
i=1
zi,NUL +
Nt?
k=1
zNUL,k) (17)
From the definition of zi,NUL, it is obvious that,
for any 1 ? i ? Ns, zi,NUL and zik(1 ? k ? Nt)
have the following relationship: If
?Nt
k=1 zik ? 1,
i.e., argsi aligns with some arguments on target side,
then zi,NUL = 0; Otherwise, zi,NUL = 1. These
relationships can be captured by the following con-
straints:
?1 ? i ? Ns, 1 ? k ? Nt : zi,NUL ? 1?zik (18)
?1 ? i ? Ns :
Nt?
k=1
zik + zi,NUL ? 1 (19)
Similarly, for any 1 ? k ? Nt, zNUL,k and
zik(1 ? i ? Ns) observe the following constraints:
?1 ? k ? Nt, 1 ? i ? Ns : zNUL,k ? 1? zik
(20)
?1 ? k ? Nt :
Ns?
i=1
zik + zNUL,k ? 1 (21)
4.4 Models Summary
So far, we have presented two versions of our joint
inference model. The first version treats Comple-
ment argument alignment as a hard constraint. We
will refer to this version as Joint1. The objective
function of Joint1 is defined by equations (1, 2, 6,
10). And the constraints of Joint1 are defined by
equations (3-5, 7-9, 11-16).
The sencond version treats Complement argument
alignment as a soft constraint. We will refer to this
version as Joint2. The objective function of Joint2
is defined by equations (1, 2, 6, 17). And the con-
straints of Joint2 are defined by equations (3-5, 7-9,
11-14, 18-21).
Our baseline models are monolingual SRL com-
bination models. We will refer to the source side
combination model as SrcCmb. The objective of Sr-
cCmb is to maximize Os, which is defined in equa-
tion (2). And the constraints of SrcCmb are defined
by equations (3-5). Similarly, we will refer to the tar-
get side combination model as TrgCmb. The objec-
tive of TrgCmb is to maximize Ot defined in equa-
tion (6). And the constraints of TrgCmb are defined
by equations (7-9). In this paper, we employ lp-
solve2 to solve all ILP models.
5 Experiments
5.1 Experimental Setup
In our experiments, we use the Xinhua News por-
tion of Chinese and English data in LDC OntoNotes
Release 3.0. This data is a Chinese-English parallel
proposition bank described in (Palmer et al, 2005).
It contains parallel proposition annotations for 325
files (chtb 0001.fid to chtb 0325.fid) from Chinese-
English parallel Treebank. The English part of this
data contains proposition annotations only for ver-
bal predicates. Therefore, we only consider verbal
predicates in this paper.
We employ the GIZA++ toolkit (Och and Ney,
2003) to perform automatic word alignment. Be-
sides the parallel PropBank data, we use additional
4,500K Chinese-English sentence pairs3 to induce
word alignments for both directions, with the default
GIZA++ settings. The alignments are symmetrized
using the intersection heuristic (Och and Ney, 2003),
which is known to produce high-precision align-
ments.
We use 80 files (chtb 0001.fid to chtb 0080.fid)
as test data, and 40 files (chtb 0081.fid to
chtb 0120.fid) as development data. Although our
joint inference model needs no training, we still
need to train a log-linear argument alignment prob-
ability model, which is used in the joint inference
model. As specified in subsection 4.2.1, the train-
2http://lpsolve.sourceforge.net/
3These data includes the following LDC corpus:
LDC2002E18, LDC2003E07, LDC2003E14, LDC2005T06,
LDC2004T07, LDC2000T50.
310
ing set for the argument alignment probability model
consists of 60 files (chtb 0121.fid to chtb 0180.fid)
with manual argument alignment. Unfortunately,
the quality of automatic word alignment on one-
to-many Chinese-English sentence pairs is usually
very poor. So we only include one-to-one Chinese-
English sentence pairs in all data. And not all predi-
cates in a sentence pair can be included. Only bilin-
gual predicate pairs are included. A bilingual pred-
icate pair is defined to be a pair of predicates in bi-
text which align with each other in automatic word
alignment. Table 3 shows how many sentences and
predicates are included in each data set.
Test Dev Train
Articles 1-80 81-120 121-180
Chinese Sentences 1067 578 778
English Sentences 1182 620 828
Bilingual pairs 821 448 614
Chinese Predicates 3792 2042 2572
English Predicates 2864 1647 1860
Bilingual pairs 1476 790 982
Table 3: Sentence and predicate counts.
Our monolingual SRL systems are trained sep-
arately. Our Chinese SRL system is trained on
640 files (chtb 0121.fid to chtb 0931.fid) in Chinese
Propbank 1.0. Because Xinhua News is a quite dif-
ferent domain from WSJ, the training set for our En-
glish SRL system includes not only Sections 02?21
of WSJ data in English Propbank, but also 205 files
(chtb 0121.fid to chtb 0325.fid) in the English part
of parallel PropBank. For Chinese, the syntactic
parsers are trained on 640 files (chtb 0121.fid to
chtb 0931.fid) plus the broadcast news portion of
Chinese Treebank 6.0. For English, the syntactic
parsers are trained on the following data: Sections
02?21 of WSJ data in English Treebank, 205 files
(chtb 0121.fid to chtb 0325.fid) of Xinhua News
data in OntoNotes 3.0, and the Sinorama data in
OntoNotes 3.0. We treat discontinuous and corefer-
ential arguments in accordance to the CoNLL-2005
shared task (Carreras and Ma`rquez, 2005). The first
part of a discontinuous argument is labeled as it is,
and the second part is labeled with a prefix ?C-?.
All coreferential arguments are labeled with a prefix
?R-?.
5.2 Tuning Parameters in Models
The models Joint1, Joint2, SrcCmb, and TrgCmb
have different parameters. For each model, we have
automatically tuned its parameters on development
set using Powell?s Mothod (Brent, 1973). Powell?s
Method is a heuristic optimization algorithm that
does not require the objective function to have an ex-
plicit analytical formula. For a monolingual model
like SrcCmb or TrgCmb, our objective is to maxi-
mize the F1 score of the model?s result on develop-
ment set. But a joint model, like Joint1 or Joint2,
generates SRL results on both sides of bitext. So
our objective is to maximize the sum of the two F1
scores of the model?s results for both Chinese and
English on development set. For all models, we re-
gard the parameters to be tuned as variables. Then
we optimize our objective using Powell?s Method.
The solution of this optimization is the values of pa-
rameters. To avoid finding poor local optimum, we
perform the optimization 30 times with different ini-
tial parameter values, and choose the best solution
found. The final parameter values are listed in Ta-
ble 4.
Model Ts Tt Ta ?1 ?2 ?3
SrcCmb 0.21 - - - - -
TrgCmb - 0.32 - - - -
Joint1 0.17 0.22 0.36 0.96 1.04 -
Joint2 0.15 0.26 0.42 1.02 1.21 0.15
Table 4: Parameter values in models.
5.3 Individual SRL Outputs? Performance
As specified in subsection 3.3, the monoligual SRL
system uses different parse trees to generate multi-
ple SRL outputs. The performance of these outputs
on test set is shown in Table 5. In Table 5, O1?O3
are the outputs using 3-best parse trees of Berkeley
parser respectively, O4 and O5 are the outputs us-
ing the best parse trees of Stanford parser and Bikel
parser respectively.
As specified in subsection 5.1, only a small part
of English SRL training data is in the same domain
with test data. Therefore, the English SRL result in
Table 5 is not very impressive. But the Chinese SRL
result is pretty good.
311
Side Outputs P (%) R(%) F1
O1 79.84 71.95 75.69
O2 78.53 70.32 74.20
Chinese O3 78.41 69.99 73.96
O4 73.21 67.13 70.04
O5 75.32 63.78 69.07
O1 77.13 70.42 73.62
O2 75.88 69.06 72.31
English O3 75.74 68.65 72.02
O4 71.57 66.11 68.73
O5 73.12 68.04 70.49
Table 5: The results of individual monolingual SRL out-
puts on test set.
5.4 Effects of Different Constraints
The One-to-many limit and Complete argument
alignment constraints in subsection 4.2.2 comes
from our empirical knowledge. To investigate the
effect of these two constraits, we remove them from
our joint inference models one by one, and observe
the performance variations on test set. The results
are shown in Table 6. In Table 6, ?c2? refers to the
One-to-many limit constraint, ?c3? refers to the Com-
plete argument alignment constraint, and ?-? means
removing. For example, ?Joint1 - c2? means remov-
ing the constraint ?c2? from the model Joint1. Recall
that the only difference between Joint1 and Joint2 is
that ?c3? is a hard constraint in Joint1, but a soft con-
straint in Joint2. Therefore, ?Joint2 - c3? and ?Joint2
- c2 - c3? do not appear in Table 6, because they are
the same with ?Joint1 - c3? and ?Joint1 - c2 - c3?
respectively.
Model Side P (%) R(%) F1
Joint1
Chinese
82.95 75.21 78.89
Joint1 - c2 81.46 75.97 78.62
Joint1 - c3 82.36 74.68 78.33
Joint1 - c2 - c3 82.04 74.67 78.18
Joint2 83.35 76.04 79.53
Joint2 - c2 82.41 76.03 79.09
Joint1
English
79.38 75.16 77.21
Joint1 - c2 78.51 75.22 76.83
Joint1 - c3 78.66 74.55 76.55
Joint1 - c2 - c3 78.37 74.37 76.32
Joint2 79.64 76.18 77.87
Joint2 - c2 78.41 75.89 77.13
Table 6: Results of different joint models on test set.
From Table 6, we can see that the constraints ?c2?
and ?c3? both have positive effect in our joint in-
ference model, because removing any one of them
causes performance degradation. And removing
?c3? from Joint1 causes more performance degrada-
tion than removing ?c2?. This means that ?c3? plays
a more important role than ?c2? in our joint inference
model. Indeed, by treating ?c3? as a soft constraint,
the model Joint2 has the best performance on both
sides of bitext.
5.5 Final Results
We use Joint2 as our final joint inference model.
And as specified in subsection 4.4, our baselines are
monolingual SRL combination models: SrcCmb for
Chinese, and TrgCmb for English. Note that SrcCmb
and TrgCmb are basically the same as the state-of-
the-art combination model in (Surdeanu et al, 2007)
with No overlapping and No duplication constraints.
The final results on test set are shown in Table 7.
Side Model P (%) R(%) F1
Chinese
SrcCmb 82.58 73.92 78.01
Joint2 83.35 76.04 79.53
English
TrgCmb 79.02 73.44 76.13
Joint2 79.64 76.18 77.87
Table 7: Comparison between monolingual combination
model and our joint inference model on test set.
From Table 5 and Table 7, we can see that SrcCmb
and TrgCmb improve F1 scores over the best indi-
vidual SRL outputs by 2.32 points and 2.51 points
on Chinese and English seperately. Thus they form
strong baselines for our joint inference model. Even
so, our joint inference model still improves F1 score
over SrcCmb by 1.52 points, and over TrgCmb by
1.74 points.
From Table 7, we can see that, despite only part of
training data for English SRL system is in-domain,
our joint inference model still produces good En-
glish SRL result. And the F1 score of Chinese SRL
result reaches 79.53%, which represents the state-
of-the-art Chinese SRL performance to date.
6 Conclusions
In this paper, we propose a joint inference model
to perform bilingual SRL. Our joint inference
model incorporates not only linguistic constraints on
312
source and target sides of bitext, but also the bilin-
gual argument structure consistency requirement on
bitext. Experiments on Chinese-English parallel
PropBank show that our joint inference model is
very effective for bilingual SRL. Compared to state-
of-the-art monolingual SRL combination baselines,
our joint inference model substantially improves
SRL results on both sides of bitext. In fact, the so-
lution of our joint inference model contains not only
the SRL results on bitext, but also the optimal argu-
ment alignment between two sides of bitext. This
makes our model especially suitable for application
in machine translation, which needs to obtain the ar-
gument alignment.
Acknowledgments
The research work has been partially funded by
the Natural Science Foundation of China under
Grant No. 60975053 and 60736014, the National
Key Technology R&D Program under Grant No.
2006BAH03B02. We would like to thank Jiajun
Zhang for helpful discussions and the anonymous
reviewers for their valuable comments.
References
Daniel Bikel. 2004. Intricacies of Collins Parsing Model.
Computational Linguistics, 30(4):480-511.
Richard P. Brent. 1973. Algorithms for Minimization
without Derivatives. Prentice-Hall, Englewood Cliffs,
NJ.
David Burkett, and Dan Klein. 2008. Two Languages
are Better than One (for Syntactic Parsing). In Pro-
ceedings of EMNLP-2008, pages 877-886.
Xavier Carreras, and Llu??s Ma`rquez. 2005. Introduction
to the CoNLL-2005 shared task: semantic role label-
ing. In Proceedings of CoNLL-2005, pages 152-164.
Wanxiang Che, Min Zhang, Ai Ti Aw, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. Using a Hybrid Convo-
lution Tree Kernel for Semantic Role Labeling. ACM
Transactions on Asian Language Information Process-
ing, 2008, 7(4): 1-23.
Pascale Fung, Zhaojun Wu, Yongsheng Yang and Dekai
Wu. 2007. Learning Bilingual Semantic Frames:
Shallow Semantic Parsing vs. Semantic Role Projec-
tion. In Proceedings of the 11th Conference on The-
oretical and Methodological Issues in Machine Trans-
lation, pages 75-84.
Liang Huang, Wenbin Jiang, Qun Liu. 2009.
Bilingually-Constrained (Monolingual) Shift-Reduce
Parsing. In Proceedings of EMNLP-2009, pages 1222-
1231.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL-2003,
pages 423-430.
Peter Koomen, Vasin Punyakanok, Dan Roth, and Wen-
tau Yih. 2005. Generalized Inference with Multiple
Semantic Role Labeling Systems. In Proceedings of
CoNLL-2005 shared task, pages 181-184.
Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,
and Peide Qian. 2009. Improving Nominal SRL in
Chinese Language with Verbal SRL Information and
Automatic Predicate Recognition. In Proceedings of
EMNLP-2009, pages 1280-1288.
Llu??s Ma`rquez, Xavier Carreras, Kenneth C. Litkowski,
Suzanne Stevenson. 2008. Semantic Role Labeling:
An Introduction to the Special Issue. Computational
Linguistics, 34(2):145-159.
Llu??s Ma`rquez, Mihai Surdeanu, Pere Comas, and Jordi
Turmo. 2005. A Robust Combination Strategy for
Semantic Role Labeling. In Proceedings of EMNLP-
2005, pages 644-651.
Frans J. Och, and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29:19-51.
Sebastian Pado?, and Mirella Lapata. 2009. Cross-lingual
Annotation Projection of Semantic Roles. Journal of
Artificial Intelligence Research (JAIR), 36:307-340.
Martha Palmer, Nianwen Xue, Olga Babko-Malaya, Jiny-
ing Chen, Benjamin Snyder. 2005. A Parallel Propo-
sition Bank II for Chinese and English. In Frontiers
in Corpus Annotation, Workshop in conjunction with
ACL-05, pages 61-67.
Slav Petrov and Dan Klein. 2007. Improved Inference
for Unlexicalized parsing. In Proceedings of ACL-
2007, pages 46-54.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James
H. Martin, and Daniel Jurafsky. 2005. Semantic Role
Labeling Using Different Syntactic Views. In Pro-
ceedings of ACL-2005, pages 581-588.
Sameer S. Pradhan, Wayne Ward, James H. Martin.
2008. Towards Robust Semantic Role Labeling. Com-
putational Linguistics, 34(2):289-310.
Vasin Punyakanok, Dan Roth, Wen-tauYih. 2008. The
Importance of Syntactic Parsing and Inference in Se-
mantic Role Labeling. Computational Linguistics,
34(2):257-287.
Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav
Zimak. 2004. Semantic Role Labeling via Integer
Linear Programming Inference. In Proceedings of
COLING-2004, pages 1346-1352.
Weiwei Sun, Zhifang Sui, Meng Wang, and Xin Wang.
2009. Chinese Semantic Role Labeling with Shallow
313
Parsing. In Proceedings of EMNLP-2009, pages 1475-
1483.
Mihai Surdeanu, Llu??s Ma`rquez, Xavier Carreras, and
Pere R. Comas. 2007. Combination Strategies for
Semantic Role Labeling. Journal of Artificial Intel-
ligence Research (JAIR), 29:105-151.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A Global Joint Model for Seman-
tic Role Labeling. Computational Linguistics, 34(2):
145-159.
Dekai Wu, and Pascale Fung. 2009. Semantic Roles for
SMT: A Hybrid Two-Pass Model. In Proceedings of
NAACL-2009, pages 13-16.
Nianwen Xue. 2008. Labeling Chinese Predicates with
Semantic Roles. Computational Linguistics, 34(2):
225-255.
Tao Zhuang, and Chengqing Zong. 2010. A Minimum
Error Weighting Combination Strategy for Chinese Se-
mantic Role Labeling. In Proceedings of COLING-
2010, pages 1362-1370.
314
