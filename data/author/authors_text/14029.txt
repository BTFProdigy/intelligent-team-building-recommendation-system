Coling 2010: Poster Volume, pages 400?408,
Beijing, August 2010
Learning to Model Domain-Specific Utterance Sequences for Extractive
Summarization of Contact Center Dialogues
Ryuichiro Higashinaka?, Yasuhiro Minami?, Hitoshi Nishikawa?,
Kohji Dohsaka?, Toyomi Meguro?, Satoshi Takahashi?, Genichiro Kikui?
? NTT Cyber Space Laboratories, NTT Corporation
? NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp, minami@cslab.kecl.ntt.co.jp
nishikawa.hitoshi@lab.ntt.co.jp, {dohsaka,meguro}@cslab.kecl.ntt.co.jp
{takahashi.satoshi,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a novel extractive
summarization method for contact cen-
ter dialogues. We use a particular
type of hidden Markov model (HMM)
called Class Speaker HMM (CSHMM),
which processes operator/caller utterance
sequences of multiple domains simulta-
neously to model domain-specific utter-
ance sequences and common (domain-
wide) sequences at the same time. We
applied the CSHMM to call summariza-
tion of transcripts in six different con-
tact center domains and found that our
method significantly outperforms compet-
itive baselines based on the maximum
coverage of important words using integer
linear programming.
1 Introduction
In modern business, contact centers are becom-
ing more and more important for improving cus-
tomer satisfaction. Such contact centers typically
have quality analysts who mine calls to gain in-
sight into how to improve business productivity
(Takeuchi et al, 2007; Subramaniam et al, 2009).
To enable them to handle the massive number of
calls, automatic summarization has been utilized
and shown to successfully reduce costs (Byrd et
al., 2008). However, one of the problems in cur-
rent call summarization is that a domain ontology
is required for understanding operator/caller utter-
ances, which makes it difficult to port one summa-
rization system from domain to domain.
This paper describes a novel automatic sum-
marization method for contact center dialogues
without the costly process of creating domain on-
tologies. More specifically, given contact center
dialogues categorized into multiple domains, we
create a particular type of hidden Markov model
(HMM) called Class Speaker HMM (CSHMM)
to model operator/caller utterance sequences. The
CSHMM learns to distinguish sequences of indi-
vidual domains and common sequences in all do-
mains at the same time. This approach makes it
possible to accurately distinguish utterances spe-
cific to a certain domain and thereby has the po-
tential to generate accurate extractive summaries.
In Section 2, we review recent work on auto-
matic summarization, including its application to
contact center dialogues. In Section 3, we de-
scribe the CSHMM. In Section 4, we describe
our automatic summarization method in detail. In
Section 5, we describe the experiment we per-
formed to verify our method and present the re-
sults. In Section 6, we summarize and mention
future work.
2 Related Work
There is an abundance of research in automatic
summarization. It has been successfully applied to
single documents (Mani, 2001) as well as to mul-
tiple documents (Radev et al, 2004), and various
summarization methods, such as the conventional
LEAD method, machine-learning based sentence
selection (Kupiec et al, 1995; Osborne, 2002),
and integer linear programming (ILP) based sen-
tence extraction (Gillick and Favre, 2009), have
been proposed. Recent years have seen work on
summarizing broadcast news speech (Hori and
Furui, 2003), multi-party meetings (Murray et al,
2005), and contact center dialogues (Byrd et al,
2008). However, despite the large amount of pre-
vious work, little work has tackled the automatic
summarization of multi-domain data.
400
In the past few decades, contact center dia-
logues have been an active research focus (Gorin
et al, 1997; Chu-Carroll and Carpenter, 1999).
Initially, the primary aim of such research was
to transfer calls from answering agents to oper-
ators as quickly as possible in the case of prob-
lematic situations. However, real-time processing
of calls requires a tremendous engineering effort,
especially when customer satisfaction is at stake,
which led to recent work on the offline process-
ing of calls, such as call mining (Takeuchi et al,
2007) and call summarization (Byrd et al, 2008).
The work most related to ours is (Byrd et al,
2008), which maps operator/caller utterances to
an ontology in the automotive domain by using
support vector machines (SVMs) and creates a
structured summary by heuristic rules that assign
the mapped utterances to appropriate summary
sections. Our work shares the same motivation
as theirs in that we want to make it easier for
quality analysts to analyze the massive number of
calls. However, we tackle the problem differently
in that we propose a newmodeling of utterance se-
quences for extractive summarization that makes
it unnecessary to create heuristics rules by hand
and facilitates the porting of a summarization sys-
tem.
HMMs have been successfully applied to au-
tomatic summarization (Barzilay and Lee, 2004).
In their work, an HMM was used to model the
transition of content topics. The Viterbi decod-
ing (Rabiner, 1990) was performed to find con-
tent topics that should be incorporated into a sum-
mary. Their approach is similar to ours in that
HMMs are utilized to model topic sequences, but
they did not use data of multiple domains in creat-
ing their model. In addition, their method requires
training data (original articles with their reference
summaries) in order to find which content top-
ics should be included in a summary, whereas our
method requires only the raw sequences with their
domain labels.
3 Class Speaker HMM
A Class Speaker HMM (CSHMM) is an exten-
sion of Speaker HMM (SHMM), which has been
utilized to model two-party conversations (Me-
guro et al, 2009). In an SHMM, there are two
states, and each state emits utterances of one of
the two conversational participants. The states are
1:speaker1 2:speaker2
Speaker HMM for Class 1
3:speaker1 4:speaker2
Speaker HMM for Class 2
Figure 1: Topology of an ergodic CSHMM. Num-
bers before ?speaker1? and ?speaker2? denote state
IDs.
connected ergodically and the emission/transition
probabilities are learned from training data by
using the EM-algorithm. Although Meguro et
al., (2009) used SHMMs to analyze the flow of
listening-oriented dialogue, we extend their idea
to make it applicable to classification tasks, such
as dialogue segmentation.
A CSHMM is simply a concatenation of
SHMMs, each of which is trained by using ut-
terance sequences of a particular dialogue class.
After such SHMMs are concatenated, the Viterbi
algorithm is used to decode an input utterance
sequence into class labels by estimating from
which class each utterance has most likely to have
been generated. Figure 1 illustrates the basic
topology of a CSHMM where two SHMMs are
concatenated ergodically. When the most likely
state sequence for an input utterance sequence is
<1,3,4,2>, we can convert these state IDs into
their corresponding classes; that is, <1,2,2,1>,
which becomes the result of utterance classifica-
tion.
We have conceived three variations of CSHMM
as we describe below. They differ in how we treat
utterance sequences that appear commonly in all
classes and how we train the transition probabili-
ties between independently trained SHMMs.
3.1 Ergodic CSHMM
The most basic CSHMM is the ergodic CSHMM,
which is a simple concatenation of SHMMs in
an ergodic manner as shown in Fig. 1. For K
classes, K SHMMs are combined with the initial
and transition probabilities all set to equal. In this
CSHMM, the assignment of class labels solely de-
pends on the output distributions of each class.
3.2 Ergodic CSHMM with Common States
This type of CSHMM is the same as the ergodic
CSHMM except that it additionally has a SHMM
trained from all dialogues of all classes. There-
401
3:speaker1 4:speaker2
1:speaker1 2:speaker2
5:speaker1 6:speaker2
Speaker HMM for Class 1
Speaker HMM for Class 2
Speaker HMM for All Classes (Class 0)
Figure 2: CSHMM with common states.
Copy
Class
1
M1
M1M0
Retrain
Train
Class
k
M0
Mk
Mk
Retrain
Train
Class
K
M0
MK
MK
Retrain
Train
All 
Classes
M0
Train
+
M0
M1 Mk MK
AVG
Concatenate
M1+0 Mk+0 MK+0
M1M0 M0 Mk
M0 MK
M1+0 Mk+0 MK+0
Step 1
Step 2
Step 3
Step 2?
END
Mconcat
If the fitting has 
converged for 
all Mk+0
Split Mconcat into 
pairs again and 
retrain Mk+0
M1?MK become 
less likely to
output common 
sequences
Transition probabilities
of M0 are redistributed
between M0 and Mk
Figure 3: Three steps to create a CSHMM using
concatenated training.
fore, for K classes, this CSHMM has K + 1
SHMMs. Figure 2 shows the model topology.
This newly added SHMM works in a manner sim-
ilar to the background model (Reynolds et al,
2000) representing sequences that are common
to all classes. By having these common states,
common utterance sequences can be classified as
?common?, making it possible to avoid forcefully
classifying common utterance sequences into one
of the given classes.
Detecting common sequences is especially
helpfulwhen several classes overlap in nature. For
example, most dialogues commonly start and end
with greetings, and many calls at contact centers
commonly contain exchanges in which the opera-
tor requests personal information about the caller
for confirmation. Regarding the model topology
in Fig. 2, if the most likely state sequence by
the Viterbi decoding is <1,4,5,6,3,2>, we obtain
a class label sequence <1,2,0,0,2,1> where the
third and fourth utterances are classified as ?zero?,
meaning that they do not belong to any class.
3.3 CSHMM using Concatenated Training
The CSHMMs presented so far have two prob-
lems: one is that the order of utterances of differ-
ent classes cannot be taken into account because
of the equal transition probabilities. As a result,
the very merit of HMMs, their ability to model
time series data, is lost. The other is that the out-
put distributions of common states may be overly
broad because they are the averaged distributions
over all classes; that is, the best path determined
by the Viterbi decoding may not go through the
common states at all.
Our solution to these problems is to apply con-
catenated training (Lee, 1989), which has been
successfully used in speech recognition to model
phoneme sequences in an unsupervised manner.
The procedure for concatenated training is illus-
trated in Fig. 3 and has three steps.
step 1 Let Mk (Mk ? M, 1 ? k ? K) be the
SHMM trained using dialogues Dk where
Dk = {?dj|c(dj) = k}, and M0 be the
SHMM trained using all dialogues; i.e., D.
Here, K means the total number of classes
and c(dj) the class assigned to a dialogue dj .
step 2 Connect each Mk ? M with a copy of
M0 using equal initial and transition proba-
bilities (we call this connected model Mk+0)
and retrain Mk+0 with ?dj ? Dk where
c(dj) = k.
step 3 Merge all models Mk+0 (1 ? k ? K) to
produce one concatenated HMM (Mconcat).
Here, the output probabilities of the copies
of M0 are averaged over K when all models
are merged to create a combined model. If
the fitting of all Mk+0 models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M0 and Mk for all k. Here, the transi-
tion probabilities from M0 to Ml(l 6= k) are
summed and equally distributed between the
copied M0?s self-loop and transitions to the
states in Mk.
In concatenated training, the transition and output
probabilities can be optimized between M0 and
402
Contact
Center
Dialogues
Domain 1
Domain K
?
HMM for Domain 1 HMM for Domain K
HMM for All Domains
Model topic label 
sequences
INPUT: A dialogue in Domain k
Topic Model
Topic label sequence
L
S
A
/
L
D
A
A
s
s
i
g
n
 
t
o
p
i
c
l
a
b
e
l
s
Domain label sequence OUTPUT: summary
Viterbi decoding
Assign
topic labels
Select utterances labeled with Domain k
Class Speaker
HMM
?..
Utterance sequence
Feature sequence
Extract content words
as utterance features
Figure 4: Overview of our summarization
method.
Mk, meaning that the output probabilities of utter-
ance sequences that are common and also found
in Mk can be moved from Mk to M0. This makes
the distribution of Mk sharp (not broad/uniform),
making it likely to output only the utterances rep-
resentative of a class k. As regards M0, its distri-
bution of output probabilities can also be sharp-
ened for utterances that occur commonly in all
classes. This sharpening of distributions is likely
to be helpful for class discrimination.
4 Summarization Method
We apply CSHMMs to extractive summarization
of contact center dialogues because such dia-
logues are two-party, can be categorized into mul-
tiple classes by their call domains (e.g., inquiry
types), and are likely contain many overlapping
exchanges between an operator and a caller across
domains, such as greetings, the confirmation of
personal information, and other cliches in busi-
ness (e.g., name exchanges, thanking/apologizing
phrases, etc.), making them the ideal target for
CSHMMs.
In our method, summarization is performed by
decoding a sequence of utterances of a domain
DMk into domain labels and selecting those ut-
terances that have domain labels DMk. This
makes it possible to extract utterances that are
characteristic of DMk in relation to other do-
mains. Our assumption is that extracting charac-
teristic sequences of a given domain provides a
good summary for that domain because such se-
quences should contain important information ne-
cessitated by the domain.
Figure 4 outlines our extractive summarization
process. The process consists of a training phase
and a decoding phase as described below.
Training phase: Let D (d1 . . . dN ) be the entire
set of contact center dialogues, DMk (DMk ?
DM, 1 ? k ? K) the domain assigned to do-
main k, and Udi,1 . . .Udi,H the utterances in di.
Here, H is the number of utterances in di. From
D, we create two models: a topic model (TM )
and a CSHMM.
The topic model is used to assign a single topic
to each utterance so as to facilitate the training
of the CSHMM by reducing the dimensions of
the feature space. The same approach has been
taken in (Barzilay and Lee, 2004). The topic
model can be created by such techniques as prob-
abilistic latent semantic analysis (PLSA) (S?ingliar
and Hauskrecht, 2006) and latent Dirichlet alo-
cation (LDA) (Tam and Schultz, 2005). PLSA
models the latent topics of the documents and its
Baysian extension is LDA, which also models the
co-occurrence of topics using the Dirichlet prior.
We first derive features Fd1 . . . FdN for the dia-logues. Here, we assume a bag-of-words repre-
sentation for the features; therefore, Fdi is repre-sented as {< w1, c1 > . . . < wV , cV >}, where
V means the total number of content words in the
vocabulary and < wi, ci > denotes that a content
word wi appears ci times in a dialogue. Note that
we derive the features for dialogues, not for utter-
ances, because utterances in dialogue can be very
short, often consisting of only one or two words
and thus making it hard to calculate the word co-
occurrence required for creating a topic model.
From the features, we build a topic model that in-
cludes P(z|w), where w is a word and z is a topic.
Using the topic model, we can assign a single
topic label to every utterance in D by finding its
likely topic; i.e., argmax
z
?
w?words(Udi) P(z|w).
After labeling all utterances in D with topic la-
bels, we train a CSHMM that learns characteristic
topic label sequences in each domain as well as
common topic label sequences across domains.
Decoding phase: Let dj be the input dialogue,
DM(dj) (? DM ) the table for obtaining the do-
main label of dj , and Udj ,1 . . .Udj ,Hdj the utter-ances in dj, where Hdj is the number of the utter-ances. We use TM to map the utterances to topic
403
Domain # Tasks Sentences Characters
FIN 15 8.93 289.93
ISP 15 7.20 259.53
LGU 20 9.85 328.55
MO 15 10.07 326.20
PC 15 9.40 354.07
TEL 18 8.44 322.22
ALL 98 9.01 314.46
Table 1: Scenario statistics: the number of tasks
and averaged number of sentences/characters in a
task scenario in the six domains.
labels Tdj ,1 . . .Tdj ,Hdj and convert them into do-main label sequences DMdj ,1 . . .DMdj ,Hdj us-ing the trained CSHMM by the Viterbi decoding.
Then, we select Udj ,h (1 ? h ? Hdj ) whose cor-responding domain labelDMdj ,h equalsDM(dj)and output the selected utterances in the order of
appearance in the original dialogue as a summary.
5 Experiment
We performed an experiment to verify our sum-
marization method. We first collected simulated
contact center dialogues using human subjects.
Then, we compared our method with baseline sys-
tems. Finally, we analyzed the created summaries
to investigate what had been learned by our CSH-
MMs.
5.1 Dialogue Data
Since we do not have access to actual contact cen-
ter data, we recruited human subjects to collect
simulated contact center dialogues. A total of 90
participants (49 males and 41 females) took the
roles of operator or a caller and talked over tele-
phones in separate rooms. The callers were given
realistic scenarios that included their motivation
for a call as well as detailed instructions about
what to ask. The operators, who had experience
of working at contact centers, were given manuals
containing the knowledge of the domain and ex-
plaining how to answer questions in specific sce-
narios.
The dialogues took place in six different do-
mains: Finance (FIN), Internet Service Provider
(ISP), Local Government Unit (LGU), Mail Or-
der (MO), PC support (PC), and Telecommuni-
cation (TEL). In each domain, there were 15?20
tasks. Table 1 shows the statistics of the task sce-
narios used by the callers. We cannot describe the
details of each domain for lack of space, but ex-
MO task No. 3: It is becoming a good season for the
Japanese Nabe (pan) cuisine. You own a Nabe restau-
rant and it is going well. When you were searching on
the Internet, thinking of creating a new dish, you saw
that drop-shipped Shimonoseki puffer fish was on sale.
Since you thought the puffer fish cuisine would become
hot in the coming season, you decided to order it as a
trial. . . . You ordered a puffer fish set on the Internet,
but you have not received the confirmation email that
you were supposed to receive. . . . You decided to call
the contact center to make an inquiry, ask them whether
the order has been successful, and request them to send
you the confirmation email.
Figure 5: Task scenario in the MO domain. The
scenario was originally in Japanese and was trans-
lated by the authors.
amples of the tasks for FIN are inquiries about in-
surance, notifications of the loss of credit cards,
and applications for finance loans, and those for
ISP are inquiries about fees for Internet access, re-
quests to forward emails, and reissuance of pass-
words. Figure 5 shows one of the task scenarios
in the MO domain.
We collected data on two separate occasions us-
ing identical scenarios but different participants,
which gave us two sets of dialogue data. We used
the former for training our summarization sys-
tem and the latter for testing. We only use the
transcriptions in this paper so as to avoid partic-
ular problems of speech. All dialogues were in
Japanese. Tables 2 and 3 show the statistics of the
training data and the test data, respectively. As
can be seen from the tables, each dialogue is quite
long, which attests to the complexity of the tasks.
5.2 Training our Summarization System
For training our system, we first created a topic
model using LDA.We performed a morphological
analysis using ChaSen1 to extract content words
from each dialogue and made its bag-of-words
features. We defined content words as nouns,
verbs, adjectives, unknown words, and interjec-
tions (e.g., ?yes?, ?no?, ?thank you?, and ?sorry?).
We included interjections because they occur very
frequently in dialogues and often possess impor-
tant content, such as agreement and refusal, in
transactional communication. We use this defini-
tion of content words throughout the paper.
Then, using an LDA software package2, we
built a topic model. We tentatively set the number
1http://chasen-legacy.sourceforge.jp/
2http://chasen.org/?daiti-m/dist/lda/
404
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 59 75.73 72.69 148.42 17.44 7.54 12.59
ISP 64 55.09 53.17 108.27 20.11 8.03 14.18
LGU 76 58.28 50.55 108.83 12.83 8.55 10.84
MO 70 66.39 58.74 125.13 15.09 7.43 11.49
PC 56 89.34 77.80 167.14 15.48 6.53 11.31
TEL 66 75.58 63.97 139.55 12.74 8.24 10.67
ALL 391 69.21 61.96 131.17 15.40 7.69 11.76
Table 2: Training data statistics: Averaged num-
ber of utterances per dialogue and characters per
utterance for each domain. OPE and CAL denote
operator and caller, respectively. See Section 5.1
for the full domain names.
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 60 73.97 61.05 135.02 14.53 7.50 11.35
ISP 59 76.08 61.24 137.32 15.43 6.94 11.65
LGU 56 66.55 51.59 118.14 14.54 7.53 11.48
MO 47 75.53 64.87 140.40 10.53 6.79 8.80
PC 44 124.02 94.16 218.18 14.23 7.79 11.45
TEL 41 93.71 68.54 162.24 13.94 7.85 11.37
ALL 307 83.07 65.69 148.76 13.98 7.41 11.08
Table 3: Test data statistics.
of topics to 100. Using this topic model, we la-
beled all utterances in the training data using these
100 topic labels.
We trained seven different CSHMMs in all: one
ergodic CSHMM (ergodic0), three variants of er-
godic CSHMMs with common states (ergodic1,
ergodic2, ergodic3), and three variants of CSH-
MMs with concatenated training (concat1, con-
cat2, concat3). The difference within the variants
is in the number of common states. The numbers
0?3 after ?ergodic? and ?concat? indicate the num-
ber of SHMMs containing common states. For
example, ergodic3 has nine SHMMs (six SHMMs
for the six domains plus three SHMMs contain-
ing common states). Since more states would
enable more minute modeling of sequences, we
made such variants in the hope that common se-
quences could be more accurately modeled. We
also wanted to examine the possibility of creat-
ing sharp output distributions in common states
without the concatenated training by such minute
modeling. These seven CSHMMs make seven dif-
ferent summarization systems.
5.3 Baselines
Baseline-1: BL-TF We prepared two baseline
systems for comparison. One is a simple sum-
marizer based on the maximum coverage of high
term frequency (TF) content words. We call
this baseline BL-TF. This baseline summarizes a
dialogue by maximizing the following objective
function:
max
?
zi?Z
weight(wi) ? zi
where ?weight? returns the importance of a con-
tent word wi and zi is a binary value indicating
whether to include wi in the summary. Here,
?weight? returns the count of wi in a given dia-
logue. The maximization is done using ILP (we
used an off-the-shelf solver lp solve3) with the
following three constraints:
xi, zi ? {0, 1}
?
xi?X
lixi ? K
?
i
mijxi ? zj (?zj ? Z)
where xi is a binary value that indicates whether
to include the i-th utterance in the summary, li is
the length of the i-th utterance,K is the maximum
number of characters to include in a summary, and
mij is a binary value that indicates whether wi is
included in the j-th utterance. The last constraint
means that if a certain utterance is included in the
summary, all words in that utterance have to be
included in the summary.
Baseline-2: BL-DD Although BL-TF should be
a very competitive baseline because it uses the
state-of-the-art formulation as noted in (Gillick
and Favre, 2009), having only this baseline is
rather unfair because it does not make use of the
training data, whereas our proposed method uses
them. Therefore, we made another baseline that
learns domain-specific dictionaries (DDs) from
the training data and incorporates them into the
weights of content words of the objective function
of BL-TF. We call this baseline BL-DD. In this
baseline, the weight of a content word wi in a do-
main DMk is
weight(wi,DMk) =
log(P(wi|DMk))
log(P(wi|DM\DMk))
3http://lpsolve.sourceforge.net/5.5/
405
Metric ergodic0 ergodic1 ergodic2 ergodic3 concat1 concat2 concat3
PROPOSED
F 0.177 0.177 0.177 0.177 0.187?e0e1e2e3 0.198?+e0e1e2e3c1 0.199?+e0e1e2e3c1precision 0.145 0.145 0.145 0.145 0.161? 0.191?+ 0.195?+
recall 0.294 0.294 0.294 0.294 0.280? 0.259?+ 0.259?+
(Same-length) BL-TF
F 0.171 0.171 0.171 0.171 0.168 0.164 0.163
precision 0.132 0.132 0.132 0.132 0.135 0.140 0.140
recall 0.294 0.294 0.294 0.294 0.270 0.241 0.240
(Same-length) BL-DD
F 0.189 0.189 0.189 0.189 0.189 0.187 0.187
precision 0.155 0.155 0.155 0.155 0.162 0.170 0.172
recall 0.287 0.287 0.287 0.287 0.273 0.250 0.248
Compression Rate 0.42 0.42 0.42 0.42 0.37 0.30 0.30
Table 4: F-measure, precision, and recall averaged over all 307 dialogues (cf. Table 3) in the test
set for the proposed methods and baselines BL-TF and BL-DD configured to output the same-length
summaries as the proposed systems. The averaged compression rate for each proposed system is shown
at the bottom. The columns (ergodic0?concat3) indicate our methods as well as the character lengths
used by the baselines. Asterisks, ?+?, e0?e3, and c1?c3 indicate our systems? statistical significance by
the Wilcoxon signed-rank test (p<0.01) over BL-TF, BL-DD, ergodic0?3, and concat1?3, respectively.
Statistical tests for the precision and recall were only performed between the proposed systems and
their same-length baseline counterparts. Bold font indicates the best score in each row.
where P(wi|DMk) denotes the occurrence prob-
ability of wi in the dialogues of DMk , and
P(wi|DM\DMk) the occurrence probability of
wi in all domains except for DMk. This log like-
lihood ratio estimates how much a word is char-
acteristic of a given domain. Incorporating such
weights would make a very competitive baseline.
5.4 Evaluation Procedure
We made our seven proposed systems and two
baselines (BL-TF and BL-DD) output extractive
summaries for the test data. Since one of the
shortcomings of our proposedmethod is its inabil-
ity to set the compression rate, we made our sys-
tems output summaries first and made the baseline
systems output their summaries within the charac-
ter lengths of our systems? summaries.
We used scenario texts (See Fig. 5) as reference
data; that is, a dialogue dealing with a certain task
is evaluated using the scenario text for that task.
As an evaluation criterion, we used the F-measure
(F1) to evaluate the retrieval accuracy on the ba-
sis of the recall and precision of retrieved content
words. We used the scenarios as references be-
cause they contain the basic content exchanged
between an operator and a caller, the retrieval ac-
curacy of which should be important for quality
analysts.
We could have used ROUGE (Lin and Hovy,
2003), but we did not because ROUGE does not
correlate well with human judgments in conversa-
tional data (Liu and Liu, 2008). Another benefit of
using the F-measure is that summaries of varying
lengths can be compared.
5.5 Results
Table 4 shows the evaluation results for the pro-
posed systems and the baselines. It can be seen
that concat3 shows the best performance in F-
measure among all systems, having a statistically
better performance over all systems except for
concat2. The CSHMMs with concatenated train-
ing were all better than ergodic0?3. Here, the per-
formance (and output) of ergodic0?3 was exactly
the same. This happened because of the broad dis-
tributions in their common states; no paths went
through the common states and all paths went
through the SHMMs of the six domains instead.
The evaluation results in Table 4 may be rather
in favor of our systems because the summarization
lengths were set by the proposed systems. There-
fore, we performed another experiment to inves-
tigate the performance of the baselines with vary-
ing compression rates and compared their perfor-
mance with the proposed systems in F-measure.
We found that the best performance was achieved
by BL-DD when the compression rate was 0.4
with the F-measure of 0.191, which concat3 sig-
nificantly outperformed by the Wilcoxon signed-
rank test (p<0.01). Note that the performance
shown in Table 4 may seem low. However, we
found that the maximum recall is 0.355 (cal-
406
CAL1 When I order a product from you, I get a confir-
mation email
CAL2 Puffer fish
CAL3 Sets I have ordered, but I haven?t received
the confirmation email
OPE1 Order
OPE2 I will make a confirmation whether you have
ordered
OPE3 Ten sets of Shimonoseki puffer fish by drop-
ship
OPE4 ?Yoriai? (name of the product)
OPE5 Two kilos of bony parts of tiger puffer fish
OPE6 Baked fins for fin sake
OPE7 600 milliliter of puffer fish soy sauce
OPE8 And, grated radish and red pepper
OPE9 Your desired delivery date is the 13th of Febru-
ary
CAL4 Yes, all in small cases
CAL5 This is q in alphabet right?
CAL6 Hyphen g
CAL7 You mean that the order was successful
OPE10 Yes, it was Nomura at JDS call center
Figure 6: Example output of concat3 for MO task
No. 3 (cf Fig. 5). The utterances were translated
by the authors. The compression rate for this dia-
logue was 0.24.
culated by using summaries with no compres-
sion). This means that the maximum F-measure
we could attain is lower than 0.524 (when the pre-
cision is ideal with 1). This is because of the dif-
ferences between the scenarios and the actual di-
alogues. We want to pursue ways to improve our
evaluation methodology in the future.
Despite such issues in evaluation, from the re-
sults, we conclude that our extractive summa-
rization method is effective and that having the
common states and training CSHMMs with con-
catenated training are useful in modeling domain-
specific sequences of contact center dialogues.
5.6 Example of System Output
Figure 6 shows an example output of concat3 for
the scenario MO task No. 3 (cf. Fig. 5). Bold font
indicates utterances that were NOT included in the
summary of concat3?s same-length-BF-DD coun-
terpart. It is clear that sequences related to the
MO domain were successfully extracted. When
we look at the summary of BF-DD, we see such
utterances as ?Can I have your address from the
postcode? and ?Finally, can I have your email ad-
dress?, which are obvious cliches in contact center
dialogues. This indicates the usefulness of com-
mon states for ignoring such common exchanges.
6 Summary and Future Work
This paper proposed a novel extractive sum-
marization method for contact center dialogues.
We devised a particular type of HMM called
CSHMM, which processes operator/caller utter-
ance sequences of multiple domains simulta-
neously to model domain-specific utterance se-
quences and common sequences at the same time.
We trained a CSHMM using the transcripts of
simulated contact center dialogues and verified its
effectiveness for the summarization of calls.
There still remain several limitations in our ap-
proach. One is its inability to change the com-
pression rate, which we aim to solve in the next
step using the forward-backward algorithm (Ra-
biner and Juang, 1986). This algorithm can cal-
culate the posterior probability of each state at
each time frame given an input dialogue sequence,
enabling us to extract top-N domain-specific se-
quences. We also need to find the appropriate
topic number for the topic model. In our imple-
mentation, we used a tentative value of 100, which
may not be appropriate. In addition, we believe
the topic model and the CSHMM can be unified
because these models are fundamentally similar,
especially when LDA is employed. Model topolo-
gies may also have to be reconsidered. In our
CSHMM with concatenated training, the states in
domain-specific SHMMs are only connected to
the common states, which may be inappropriate
because there could be a case where a domain
changes from one to another without having a
common sequence. ApplyingCSHMMs to speech
and other NLP tasks is another challenge. As a
near-term goal, we aim to apply our method to the
summarization of meetings, where we will need to
extend our CSHMMs to deal with more than two
participants. Finally, we also want to build a con-
tact center dialogue agent by extending the CSH-
MMs to partially observableMarkov decision pro-
cesses (POMDPs) (Williams and Young, 2007) by
following the recent work on building POMDPs
from dialogue data in the dynamic Bayesian net-
work (DBN) framework (Minami et al, 2009).
Acknowledgments
We thank the members of the Spoken Dialog
System Group, especially Noboru Miyazaki and
Satoshi Kobashikawa, for their effort in dialogue
data collection.
407
References
Barzilay, Regina and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to gener-
ation and summarization. In Proceedings of the Human
Language Technology Conference of the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 113?120.
Byrd, Roy J., Mary S. Neff, Wilfried Teiken, Youngja
Park, Keh-Shin F. Cheng, Stephen C. Gates, and Karthik
Visweswariah. 2008. Semi-automated logging of contact
center telephone calls. In Proceeding of the 17th ACM
conference on Information and knowledge management
(CIKM), pages 133?142.
Chu-Carroll, Jennifer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational Lin-
guistics, 25(3):361?388.
Gillick, Dan and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18.
Gorin, Allen L., Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113?127.
Hori, Chiori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions on
Multimedia, 5(3):368?378.
Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of the
18th annual international ACM SIGIR conference on Re-
search and development in information retrieval (SIGIR),
pages 68?73.
Lee, Kai-Fu. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics on Human Language Technology (NAACL-HLT),
pages 71?78.
Liu, Feifan and Yang Liu. 2008. Correlation between
ROUGE and human evaluation of extractive meeting sum-
maries. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on Human
Language Technologies (HLT), pages 201?204.
Mani, Inderjeet. 2001. Automatic summarization. John
Benjamins Publishing Company.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji Dohsaka,
Yasuhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proceedings of the SIGDIAL 2009 conference, pages
124?127.
Minami, Yasuhiro, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proceedings of the 1st international workshop on spoken
dialogue systems technology (IWSDS), pages 254?263.
Murray, Gabriel, Steve Renals, and Jean Carletta. 2005. Ex-
tractive summarization of meeting recordings. In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology (EUROSPEECH), pages
593?596.
Osborne, Miles. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Workshop
on Automatic Summarization, pages 1?8.
Rabiner, Lawrence R. and Biing-Hwang Juang. 1986. An
introduction to hiddenMarkov models. IEEE ASSP Mag-
azine, 3(1):4?16.
Rabiner, Lawrence R. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267?296.
Radev, Dragomir R., Hongyan Jing, Ma?gorzata Stys?, and
Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Information Processing & Manage-
ment, 40(6):919?938.
Reynolds, Douglas A., Thomas F. Quatieri, and Robert B.
Dunn. 2000. Speaker verification using adaptedGaussian
mixture models. Digital Signal Processing, 10(1-3):19 ?
41.
Subramaniam, L. Venkata, Tanveer A. Faruquie, Shajith Ik-
bal, Shantanu Godbole, and Mukesh K. Mohania. 2009.
Business intelligence from voice of customer. In Pro-
ceedings of the 2009 IEEE International Conference on
Data Engineering (ICDE), pages 1391?1402.
Takeuchi, Hironori, L Venkata Subramaniam, Tetsuya Na-
sukawa, Shourya Roy, and Sreeram Balakrishnan. 2007.
A conversation-mining system for gathering insights to
improve agent productivity. In Proceedings of the IEEE
International Conference on E-Commerce Technology
and the IEEE International Conference on Enterprise
Computing, E-Commerce, and E-Services, pages 465?
468.
Tam, Yik-Cheung and Tanja Schultz. 2005. Dynamic
language model adaptation using variational Bayes in-
ference. In Proceedings of the 9th European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), pages 5?8.
S?ingliar, Tomas and Milos Hauskrecht. 2006. Noisy-OR
component analysis and its application to link analy-
sis. The Journal of Machine Learning Research, 7:2189?
2213.
Williams, JasonD. and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393?422.
408
Coling 2010: Poster Volume, pages 910?918,
Beijing, August 2010
Opinion Summarization with Integer Linear Programming Formulation
for Sentence Extraction and Ordering
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation{ nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
In this paper we propose a novel algorithm
for opinion summarization that takes ac-
count of content and coherence, simulta-
neously. We consider a summary as a se-
quence of sentences and directly acquire
the optimum sequence from multiple re-
view documents by extracting and order-
ing the sentences. We achieve this with a
novel Integer Linear Programming (ILP)
formulation. Our proposed formulation is
a powerful mixture of the Maximum Cov-
erage Problem and the Traveling Sales-
man Problem, and is widely applicable to
text generation and summarization tasks.
We score each candidate sequence accord-
ing to its content and coherence. Since
our research goal is to summarize reviews,
the content score is defined by opinions
and the coherence score is developed in
training against the review document cor-
pus. We evaluate our method using the
reviews of commodities and restaurants.
Our method outperforms existing opinion
summarizers as indicated by its ROUGE
score. We also report the results of human
readability experiments.
1 Introduction
The Web now holds a massive number of reviews
describing the opinions of customers about prod-
ucts and services. These reviews can help the cus-
tomer to reach purchasing decisions and guide the
business activities of companies such as product
improvement. It is, however, almost impossible to
read all reviews given their sheer number.
Automatic text summarization, particularly
opinion summarization, is expected to allow all
possible reviews to be efficiently utilized. Given
multiple review documents, our summarizer out-
puts text consisting of ordered sentences. A typ-
This restaurant offers customers a delicious menu and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Table 1: A typical summary.
ical summary is shown in Table 1. This task is
considered as multidocument summarization.
Existing summarizers focus on organizing sen-
tences so as to include important information in
the given document into a summary under some
size limitation. A serious problem is that most of
these summarizers completely ignore coherence
of the summary, which improves reader?s compre-
hension as reported by Barzilay et al (2002).
To make summaries coherent, the extracted
sentences must be appropriately ordered. How-
ever, most summarization systems delink sentence
extraction from sentence ordering, so a sentence
can be extracted that can never be ordered natu-
rally with the other extracted sentences. More-
over, due to recent advances in decoding tech-
niques for text summarization, the summarizers
tend to select shorter sentences to optimize sum-
mary content. It aggravates this problem.
Although a preceding work tackles this prob-
lem by performing sentence extraction and order-
ing simultaneously (Nishikawa et al, 2010), they
adopt beam search and dynamic programming to
search for the optimal solution, so their proposed
method may fail to locate it.
To overcome this weakness, this paper proposes
a novel Integer Linear Programming (ILP) formu-
lation for searching for the optimal solution effi-
ciently. We formulate the multidocument sum-
marization task as an ILP problem that tries to
optimize the content and coherence of the sum-
mary by extracting and ordering sentences simul-
taneously. We apply our method to opinion sum-
marization and show that it outperforms state-of-
the-art opinion summarizers in terms of ROUGE
evaluations. Although in this paper we challenge
910
our method with opinion summarization, it can be
widely applied to other text generation and sum-
marization tasks.
This paper is organized as follows: Section 2
describes related work. Section 3 describes our
proposal. Section 4 reports our evaluation experi-
ments. We conclude this paper in Section 5.
2 Related Work
2.1 Sentence Extraction
Although a lot of summarization algorithms have
been proposed, most of them solely extract sen-
tences from a set of sentences in the source docu-
ment set. These methods perform extractive sum-
marization and can be formalized as follows:
S? = argmax
S?T
L(S) (1)
s.t. length(S) ? K
T stands for all sentences in the source docu-
ment set and S is an arbitrary subset of T . L(S)
is a function indicating the score of S as deter-
mined by one or more criteria. length(S) indi-
cates the length of S, K is the maximum size of
the summary. That is, most summarization algo-
rithms search for, or decode, the set of sentences S?
that maximizes function L under the given maxi-
mum size of the summary K. Thus most stud-
ies focus on the design of function L and efficient
search algorithms (i.e. argmax operation in Eq.1).
Objective Function
Many useful L functions have been proposed
including the cosine similarity of given sentences
(Carbonell and Goldstein, 1998) and centroid
(Radev et al, 2004); some approaches directly
learn function L from references (Kupiec et al,
1995; Hirao et al, 2002).
There are two approaches to defining the score
of the summary. One defines the weight on each
sentence forming the summary. The other defines
a weight for a sub-sentence, concept, that the sum-
mary contains.
McDonald (2007) and Martins and Smith
(2009) directly weight sentences and use MMR
to avoid redundancy (Carbonell and Goldstein,
1998). In contrast to their approaches, we set
weights on concepts, not sentences. Gillick
and Favre (2009) reported that the concept-based
model achieves better performance and scalability
than the sentence-based model when it is formu-
lated as ILP.
There is a wide range of choice with regard
to the unit of the concept. Concepts include
words and the relationship between named en-
tities (Filatova and Hatzivassiloglou, 2004), bi-
grams (Gillick and Favre, 2009), and word stems
(Takamura and Okumura, 2009).
Some summarization systems that target re-
views, opinion summarizers, extract particular
information, opinion, from the input sentences
and leverage them to select important sentences
(Carenini et al, 2006; Lerman et al, 2009). In
this paper, since we aim to summarize reviews,
the objective function is defined through opinion
as the concept that the reviews contain. We ex-
plain our detailed objective function in Section 3.
We describe features of above existing summariz-
ers in Section 4 and compare our method to them
as baselines.
Decoding Method
The algorithms proposed for argmax operation
include the greedy method (Filatova and Hatzivas-
siloglou, 2004), stack decoding (Yih et al, 2007;
Takamura and Okumura, 2009) and Integer Linear
Programming (Clarke and Lapata, 2007; McDon-
ald, 2007; Gillick and Favre, 2009; Martins and
Smith, 2009). Gillick and Favre (2009) and Taka-
mura and Okumura (2009) formulate summariza-
tion as a Maximum Coverage Problem. We also
use this formulation. While these methods focus
on extracting a set of sentences from the source
document set, our method performs extraction and
ordering simultaneously.
Some studies attempt to generate a single sen-
tence (i.e. headline) from the source document
(Banko et al, 2000; Deshpande et al, 2007).
While they extract and order words from the
source document as a unit, our model uses the unit
of sentences. This problem can be formulated as
the Traveling Salesman Problem and its variants.
Banko et al (2000) uses beam search to identify
approximate solutions. Deshpande et al (2007)
uses ILP and a randomized algorithm to find the
optimal solution.
2.2 Sentence Ordering
It is known that the readability of a collection of
sentences, a summary, can be greatly improved
by appropriately ordering them (Barzilay et al,
2002). Features proposed to create the appropri-
ate order include publication date of document
(Barzilay et al, 2002), content words (Lapata,
2003; Althaus et al, 2004), and syntactic role of
911
 
  
    
 
  
             	      	

     
     
Figure 1: Graph representation of summarization.
words (Barzilay and Lapata, 2005). Some ap-
proaches use machine learning to integrate these
features (Soricut and Marcu, 2006; Elsner et al,
2007). Generally speaking, these methods score
the discourse coherence of a fixed set of sentences.
These methods are separated from the extraction
step so they may fail if the set includes sentences
that are impossible to order naturally.
As mentioned above, there is a preceding work
that attempted to perform sentence extraction and
ordering simultaneously (Nishikawa et al, 2010).
Differences between this paper and that work are
as follows:
? This work adopts ILP solver as a decoder.
ILP solver allows the summarizer to search
for the optimal solution much more rapidly
than beam search (Deshpande et al, 2007),
which was adopted by the prior work. To
permit ILP solver incorporation, we propose
in this paper a totally new ILP formulation.
The formulation can be widely used for text
summarization and generation.
? Moreover, to learn better discourse coher-
ence, we adopt the Passive-Aggressive al-
gorithm (Crammer et al, 2006) and use
Kendall?s tau (Lapata, 2006) as the loss func-
tion. In contrast, the above work adopts Av-
eraged Perceptron (Collins, 2002) and has no
explicit loss function.
These advances make this work very different
from that work.
3 Our Method
3.1 The Model
We consider a summary as a sequence of sen-
tences. As an example, document set D =
{d1, d2, d3} is given to a summarizer. We de-
fine d as a single document. Document d1,
which consists of four sentences, is describe
by d1 = {s11, s12, s13, s14}. Documents d2
and d3 consist of five sentences and three sen-
tences (i.e. d2 = {s21, s22, s23, s24, s25}, d3 =
e1 e2 e3 . . . e6 e7 e8
s11 1 0 0 1 0 0
s12 0 1 0 0 0 0
s13 0 0 0 0 0 1
.
.
.
.
.
.
s31 0 0 0 0 0 0
s32 0 0 1 0 1 0
s33 0 0 0 0 0 1
Table 2: Sentence-Concept Matrix.
{s31, s32, s33}). If the summary consists of four
sentences s11, s23, s32, s33 and they are ordered as
s11 ? s23 ? s32 ? s33, we add symbols indicat-
ing the beginning of the summary s0 and the end
of the summary s4, and describe the summary as
S = ?s0, s11, s23, s32, s33, s4?. Summary S can
be represented as a directed path that starts at s0
and ends at s4 as shown in Fig. 1.
We describe a directed arc between si and sj as
ai,j ? A. The directed path shown in Fig. 1 is de-
composed into nodes, s0, s11, s23, s32, s33, s4, and
arcs, a0,11, a11,23, a23,32, a32,33, a33,4.
To represent the discourse coherence of two ad-
jacent sentences, we define weight ci,j ? C as
the coherence score on the directed arc ai,j . We
assume that better summaries have higher coher-
ence scores, i.e. if the sum of the scores of the arcs?
ai,j?S ci,jai,j is high, the summary is coherent.
We also assume that the source document set
D includes set of concepts e ? E. Each concept
e is covered by one or more of the sentences in
the document set. We show this schema in Ta-
ble 2. According to Table 2, document set D has
eight concepts e1, e2, . . . , e7, e8 and sentence s11
includes concepts e1 and e6 while sentence s12 in-
cludes e2.
We consider each concept ei has a weight wi.
We assume that concept ei will have high weight
wi if it is important. This paper improves sum-
mary quality by maximizing the sum of these
weights.
We define, based on the above assumption, the
following objective function:
L(S) = ?ei?S wiei +
?
ai,j?S ci,jai,j (2)
s.t. length(S) ? K
Summarization is, in this paper, realized by
maximizing the sum of weights of concepts in-
cluded in the summary and the coherence score of
all adjacent sentences in the summary under the
912
limit of maximum summary size. Note that while
S and T represents the set of sentences in Eq.1,
they represent the sequence of sentences in Eq.2.
Maximizing Eq.2 is NP-hard. If each sen-
tence in the source document set has one concept
(i.e. Table 2 is a diagonal matrix), Eq.2 becomes
the Prize Collecting Traveling Salesman Problem
(Balas, 1989). Therefore, a highly efficient decod-
ing method is essential.
3.2 Parameter Estimation
Our method requires two parameters: weights
w ? W of concepts and coherence c ? C of two
adjacent sentences. We describe them here.
Content Score
In this paper, as mentioned above, since we at-
tempt to summarize reviews, we adopt opinion
as a concept. We define opinion e = ?t, a, p?
as the tuple of target t, aspect a and its polarity
p ? {?1, 0, 1}. We define target t as the tar-
get of an opinion. For example, the target t of
the sentence ?This digital camera has good im-
age quality.? is digital camera. We define aspect
a as a word that represents a standpoint appro-
priate for evaluating products and services. With
regard to digital cameras, aspects include image
quality, design and battery life. In the above ex-
ample sentence, the aspect is image quality. Po-
larity p represents whether the opinion is positive
or negative. In this paper, we define p = ?1 as
negative, p = 0 as neutral and p = 1 as posi-
tive. Thus the example sentence contains opinion
e = ?digital camera, image quality, 1?.
Opinions are extracted using a sentiment ex-
pression dictionary and pattern matching from de-
pendency trees of sentences. This opinion extrac-
tor is the same as that used in Nishikawa et al
(2010).
As the weight wi of concept ei, we use only
the frequency of each opinion in the input docu-
ment set, i.e. we assume that an opinion that ap-
pears frequently in the input is important. While
this weighting is relatively naive compared to Ler-
man et al (2009)?s method, our ROUGE evalua-
tion shows that this approach is effective.
Coherence Score
In this section, we define coherence score c.
Since it is not easy to model the global coherence
of a set of sentences, we approximate the global
coherence by the sum of local coherence i.e. the
sum of coherence scores of sentence pairs. We
define local coherence score ci,j of two sentences
x = {si, sj} and their order y = ?si, sj? repre-
senting si ? sj as follows:
ci,j = w ? ?(x, y) (3)
w??(x, y) is the inner product ofw and ?(x, y),
w is a parameter vector and ?(x, y) is a feature
vector of the two sentences si and sj .
Since coherence consists of many different el-
ements and it is difficult to model all of them,
we approximate the features of coherence as the
Cartesian product of the following features: con-
tent words, POS tags of content words, named en-
tity tags (e.g. LOC, ORG) and conjunctions. Lap-
ata (2003) proposed most of these features.
We also define feature vector ?(x,y) of the bag
of sentences x = {s0, s1, . . . , sn, sn+1} and its
entire order y = ?s0, s1, . . . , sn, sn+1? as follows:
?(x,y) =
?
x,y
?(x, y) (4)
Therefore, the score of order y is w ? ?(x,y).
Given a training set, if trained parameter vector w
assigns score w ? ?(x,yt) to correct order yt that
is higher than score w ??(x, y?) assigned to incor-
rect order y?, it is expected that the trained parame-
ter vector will give a higher score to coherently or-
dered sentences than to incoherently ordered sen-
tences.
We use the Passive-Aggressive algorithm
(Crammer et al, 2006) to find w. The Passive-
Aggressive algorithm is an online learning algo-
rithm that updates the parameter vector by taking
up one example from the training examples and
outputting the solution that has the highest score
under the current parameter vector. If the output
differs from the training example, the parameter
vector is updated as follows;
min ||wi+1 ?wi|| (5)
s.t. s(x,yt;wi+1)? s(x, y?;wi+1) ? `(y?;yt)
s(x,y;w) = w ? ?(x,y)
wi is the current parameter vector and wi+1 is
the updated parameter vector. That is, Eq.5 means
that the score of the correct order must exceed the
score of an incorrect order by more than loss func-
tion `(y?;yt) while minimizing the change in pa-
rameters.
When updating the parameter vector, this al-
gorithm requires the solution that has the highest
score under the current parameter vector, so we
have to run an argmax operation. Since we are
913
attempting to order a set of sentences, the opera-
tion is regarded as solving the Traveling Salesman
Problem (Althaus et al, 2004); that is, we locate
the path that offers the maximum score through
all n sentences where s0 and sn+1 are starting and
ending points, respectively. This operation is NP-
hard and it is difficult to find the global optimal
solution. To overcome this, we find an approxi-
mate solution by beam search.1
We define loss function `(y?;yt) as follows:
`(y?;yt) = 1? ? (6)
? = 1 ? 4 S(y?,yt)N(N ? 1) (7)
? indicates Kendall?s tau. S(y?,yt) is the mini-
mum number of operations that swap adjacent ele-
ments (i.e. sentences) needed to bring y? to yt (La-
pata, 2006). N indicates the number of elements.
Since Lapata (2006) reported that Kendall?s tau
reliably reproduces human ratings with regard to
sentence ordering, using it to minimize the loss
function is expected to yield more reliable param-
eters.
We omit detailed derivations due to space limi-
tations. Parameters are updated as per the follow-
ing equation.
wi+1 = wi + ?i(?(x,yt)? ?(x, y?)) (8)
?i = `(y?;yt) ? s(x,yt;w
i) + s(x, y?;wi)
||?(x,yt)? ?(x, y?)||2 + 12C
(9)
C in Eq.9 is the aggressiveness parameter that
controls the degree of parameter change.
Note that our method learns w from documents
automatically annotated by a POS tagger and a
named entity tagger. That is, manual annotation
isn?t required.
3.3 Decoding with Integer Linear
Programming Formulation
This section describes an ILP formulation of the
above model. We use the same notation con-
vention as introduced in Section 3.1. We use
s ? S, a ? A, e ? E as the decision variable.
Variable si ? S indicates the inclusion of the i
th sentence. If the i th sentence is part of the
summary, then si is 1. If it is not part of the
1Obviously, ILP can be used to search for the path that
maximizes the score. While beam search tends to fail to find
out the optimal solution, it is tractable and the learning al-
gorithm can estimate the parameter from approximate solu-
tions. For these reasons we use beam search.
summary, then si is 0. Variable ai,j ? A indi-
cates the adjacency of the i th and j th sentences.
If these two sentences are ordered as si ? sj ,
then ai,j is 1. Variable ei ? E indicates the in-
clusion of the i th concept ei. Taking Fig.1 as
an example, variables s0, s11, s23, s32, s33, s4 and
a0,11, a11,23, a23,32, a32,33, a33,4 are 1. ei, which
correspond to the concepts in the above extracted
sentences, are also 1.
We represent the above objective function
(Eq.2) as follows:
max
?
?
??
?
ei?E
wiei + (1 ? ?)
?
ai,j?A
ci,jai,j
?
?
? (10)
Eq.10 attempts to cover as much of the concepts
included in input document set as possible accord-
ing to their weights w ? W and orders sentences
according to discourse coherence c ? C. ? is a
scaling factor to balance w and c.
We then impose some constraints on Eq.10 to
acquire the optimum solution.
First, we range the above three variables s ?
S, a ? A, e ? E.
si, ai,j , ei ? {0, 1} ?i, j
In our model, a summary can?t include the same
sentence, arc, or concept twice. Taking Table 2
for example, if s13 and s33 are included in a sum-
mary, the summary has two e8, but e8 is 1. This
constraint avoids summary redundancy.
The summary must meet the condition of maxi-
mum summary size. The following inequality rep-
resents the size constraint:
?
si?S
lisi ? K
li ? L indicates the length of sentence si. K is
the maximum size of the summary.
The following inequality represents the rela-
tionship between sentences and concepts in the
sentences.
?
i
mijsi ? ej ?j
The above constraint represents Table 2. mi,j is
an element of Table 2. If si is not included in the
summary, the concepts in si are not included.
Symbols indicating the beginning and end of
the summary must be part of the summary.
914
s0 = 1
sn+1 = 1
n is the number of sentences in the input docu-
ment set.
Next, we describe the constraints placed on
arcs.
The beginning symbol must be followed by a
sentence or a symbol and must not have any pre-
ceding sentences/symbols. The end symbol must
be preceded by a sentence or a symbol and must
not have any following sentences/symbols. The
following equations represent these constraints:
?
i
a0,i = 1
?
i
ai,0 = 0
?
i
an+1,i = 0
?
i
ai,n+1 = 1
Each sentence in the summary must be pre-
ceded and followed by a sentence/symbol.
?
i
ai,j +
?
i
aj,i = 2sj ?j
?
i
ai,j =
?
i
aj,i ?j
The above constraints fail to prevent cycles. To
rectify this, we set the following constraints.
?
i
f0,i = n
?
i
fi,0 ? 1
?
i
fi,j ?
?
i
fj,i = sj ?j
fi,j ? nai,j ?i, j
The above constraints indicate that flows f are
sent from s0 as a source to sn+1 as a sink. n unit
flows are sent from the source and each node ex-
pends one unit of flows. More than one flow has
to arrive at the sink. By setting these constraints,
the nodes consisting of a cycle have no flow. Thus
solutions that contain a cycle are prevented. These
constraints have also been used to avoid cycles in
headline generation (Deshpande et al, 2007).
4 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We tested our
method and two baselines in two domains: re-
views of commodities and restaurants. We col-
lected 4,475 reviews of 100 commodities and
2,940 reviews of 100 restaurants from websites.
The commodities included items such as digital
cameras, printers, video games, and wines. The
average document size was 10,173 bytes in the
commodity domain and 5,343 bytes in the restau-
rant domain. We attempted to generate 300 byte
summaries, so the summarization rates were about
3% and 6%, respectively.
We prepared 4 references for each review, thus
there were 400 references in each domain. The au-
thors were not those who made up the references.
These references were used for ROUGE and read-
ability evaluation.
Since our method requires the parameter vec-
tor w for determining the coherence scores. We
trained the parameter vector for each domain.
Each parameter vector was trained using 10-fold
cross validation. We used 8 samples to train, 1
to develop, and 1 to test. In the restaurant do-
main, we added 4,390 reviews to each training set
to alleviate data sparseness. In the commodity do-
main, we add 47,570 reviews.2
As the solver, we used glpk.3 According to the
development set, ? in Eq.10 was set as 0.1.
4.1 Baselines
We compare our method to the references (which
also provide the upper bound) and the opinion
summarizers proposed by Carenini et al (2006)
and Lerman et al (2009) as the baselines.
In the ROUGE evaluations, Human indicates
ROUGE scores between references. To compare
our summarizer to human summarization, we cal-
culated ROUGE scores between each reference
and the other three references, and averaged them.
In the readability evaluations, we randomly se-
lected one reference for each commodity and each
restaurant and compared them to the results of the
three summarizers.
Carenini et al (2006)
Carenini et al (2006) proposed two opinion
2The commodities domain suffers from stronger review
variation than the restaurant domain so more training data
was needed.
3http://www.gnu.org/software/glpk/
915
summarizers. One uses a natural language genera-
tion module, and other is based on MEAD (Radev
et al, 2004). Since it is difficult to mimic the natu-
ral language generation module, we implemented
the latter one. The objective function Carenini et
al. (2006) proposed is as follows:
L1(S) =
?
a?S
?
s?D
|polaritys(a)| (11)
polaritys(a) indicates the polarity of aspect a
in sentence s present in source document set D.
That is, this function gives a high score to a sum-
mary that covers aspects frequently mentioned in
the input, and whose polarities tend to be either
positive or negative.
The solution is identified using the greedy
method. If there is more than one sentence that
has the same score, the sentence that has the
higher centroid score (Radev et al, 2004) is ex-
tracted.
Lerman et al (2009)
Lerman et al (2009) proposed three objective
functions for opinion summarization, and we im-
plemented one of them. The function is as fol-
lows:
L2(S) = ?(KL(pS(a), pD(a)) (12)
+
?
a?A
KL(N (x|?aS , ?2aS ),N (x|?aD , ?
2
aD)))
KL(p, q) means the Kullback-Leibler diver-
gence between probability distribution p and q.
pS(a) and pD(a) are probability distributions in-
dicating how often aspect a ? A occurs in sum-
mary S and source document set D respectively.
N (x|?, ?2) is a Gaussian distribution indicating
distribution of polarity of an aspect whose mean
is ? and variance is ?2. ?aS , ?aD and ?2aS , ?2aD
are the means and the variances of aspect a in
summary S and source document set D, respec-
tively. These parameters are determined using
maximum-likelihood estimation.
That is, the above objective function gives high
score to a summary whose distributions of aspects
and polarities mirror those of the source document
set.
To identify the optimal solution, Lerman et al
(2009) use a randomized algorithm. First, the
summarizer randomly extracts sentences from the
source document set, then iteratively performs in-
sert/delete/swap operations on the summary to in-
crease Eq.12 until summary improvement satu-
rates. While this method is prone to lock onto
Commodity R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.158 0.202 0.186
(Lerman et al, 2009) 0.205 0.247 0.227
Our Method 0.231 0.251 0.230
Human 0.384 0.392 0.358
Restaurant R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.251 0.281 0.258
(Lerman et al, 2009) 0.260 0.296 0.273
Our Method 0.285 0.303 0.273
Human 0.358 0.370 0.335
Table 3: Automatic ROUGE evaluation.
# of Sentences
(Carenini et al, 2006) 3.79
(Lerman et al, 2009) 6.28
Our Method 7.88
Human 5.83
Table 4: Average number of sentences in the sum-
mary.
local solutions, the summarizer can reach the op-
timal solution by changing the starting sentences
and repeating the process. In this experiment, we
used 100 randomly selected starting points.
4.2 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
The results of these experiments are shown in
Table 3. ROUGE scores increase in the order of
(Carenini et al, 2006), (Lerman et al, 2009) and
our method, but no method could match the per-
formance of Human. Our method significantly
outperformed Lerman et al (2009)?s method over
ROUGE-2 according to the Wilcoxon signed-rank
test, while it shows no advantage over ROUGE-
SU4 and ROUGE-SU9.
Although our weighting of the set of sentences
is relatively naive compared to the weighting pro-
posed by Lerman et al (2009), our method out-
performs their method. There are two reasons
for this; one is that we adopt ILP for decoding,
so we can acquire preferable solutions efficiently.
While the score of Lerman et al (2009)?s method
may be improved by adopting ILP, it is difficult
to do so because their objective function is ex-
tremely complex. The other reason is the coher-
ence score. Since our coherence score is based on
916
Commodity (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 27/45 18/29 8/46
(Lerman et al, 2009) 18/45 - 29/48 11/47
Our Method 11/29 19/48 - 5/46
Human 38/46 36/47 41/46 -
Restaurant (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 31/45 17/31 8/48
(Lerman et al, 2009) 14/45 - 25/47 7/46
Our Method 14/31 22/47 - 8/50
Human 40/48 39/46 42/50 -
Table 5: Readability evaluation.
content words, it may impact the content of the
summary.
4.3 Readability
Readability was evaluated by human judges.
Since it is difficult to perform absolute evalua-
tion to judge the readability of summaries, we
performed a paired comparison test. The judges
were shown two summaries of the same input and
decided which was more readable. The judges
weren?t informed which method generated which
summary. We randomly chose 50 sets of reviews
from each domain, so there were 600 paired sum-
maries.4 However, as shown in Table 4, the aver-
age numbers of sentences in the summary differed
widely from the methods and this might affect the
readability evaluation. It was not fair to include
the pairs that were too different in terms of the
number of sentences. Therefore, we removed the
pairs that differed by more than five sentences.
In the experiment, 523 pairs were used, and 21
judges evaluated about 25 summaries each. We
drew on DUC 2007 quality questions5 for read-
ability assessment.
Table 5 shows the results of the experiment.
Each element in the table indicates the number
of times the corresponding method won against
other method. For example, in the commodity do-
main, the summaries that Lerman et al (2009)?s
method generated were compared with the sum-
maries that Carenini et al (2006)?s method gener-
ated 45 times, and Lerman et al (2009)?s method
won 18 times. The judges significantly preferred
the references in both domains. There were no
significant differences between our method and
the other two methods. In the restaurant do-
4
4C2 ? 100 = 600
5http://www-nlpir.nist.gov/projects/
duc/duc2007/quality-questions.txt
main, there was a significant difference between
(Carenini et al, 2006) and (Lerman et al, 2009).
Since we adopt ILP, our method tends to pack
shorter sentences into the summary. However,
our coherence score prevents this from degrading
summary readability.
5 Conclusion
This paper proposed a novel algorithm for opinion
summarization that takes account of content and
coherence, simultaneously. Our method directly
searches for the optimum sentence sequence by
extracting and ordering sentences present in the
input document set. We proposed a novel ILP
formulation against selection-and-ordering prob-
lems; it is a powerful mixture of the Maximum
Coverage Problem and the Traveling Salesman
Problem. Experiments revealed that the algo-
rithm creates summaries that have higher ROUGE
scores than existing opinion summarizers. We
also performed readability experiments. While
our summarizer tends to extract shorter sentences
to optimize summary content, our proposed co-
herence score prevented this from degrading the
readability of the summary.
One future work includes enriching the features
used to determine the coherence score. We expect
that features such as entity grid (Barzilay and La-
pata, 2005) will improve overall algorithm perfor-
mance. We also plan to apply our model to tasks
other than opinion summarization.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the anonymous reviewers for their
comments.
917
References
Althaus, Ernst, Nikiforos Karamanis and Alexander Koller.
2004. Computing Locally Coherent Discourses. In Proc.
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics.
Balas, Egon. 1989. The prize collecting traveling salesman
problem. Networks, 19(6):621?636.
Banko, Michele, Vibhu O. Mittal and Michael J. Witbrock.
2000. Headline Generation Based on Statistical Transla-
tion. In Proc. of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics.
Barzilay, Regina, Noemie Elhadad and Kathleen McKeown.
2002. Inferring Strategies for Sentence Ordering in Mul-
tidocument Summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Barzilay, Regina and Mirella Lapata. 2005. Modeling Lo-
cal Coherence: An Entity-based Approach. In Proc. of
the 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
Carbonell, Jaime and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of the 21st An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Carenini, Giuseppe, Raymond Ng and Adam Pauls. 2006.
Multi-Document Summarization of Evaluative Text. In
Proc. of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Clarke, James and Mirella Lapata. 2007. Modelling Com-
pression with Discourse Constraints. In Proc. of the 2007
Joint Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Language
Learning.
Collins, Michael. 2002. Discriminative Training Methods for
Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning Re-
search, 7:551?585.
Deshpande, Pawan, Regina Barzilay and David R. Karger.
2007. Randomized Decoding for Selection-and-Ordering
Problems. In Proc. of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics.
Elsner, Micha, Joseph Austerweil and Eugene Charniak.
2007. A unified local and global model for discourse co-
herence. In Proc. of Human Language Technologies 2007:
The Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004. A For-
mal Model for Information Selection in Multi-Sentence
Text Extraction. In Proc. of the 20th International Con-
ference on Computational Linguistics.
Gillick, Dan and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proc. of Human Language
Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics Workshop on Integer Linear Programming for
NLP.
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and Yuji
Matsumoto. 2002. Extracting important sentences with
support vector machines. In Proc. of the 19th Interna-
tional Conference on Computational Linguistics.
Kupiec, Julian, Jan Pedersen and Francine Chen. 1995. A
Trainable Document Summarizer. In Proc. of the 18th An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Lapata, Mirella. 2003. Probabilistic Text Structuring: Exper-
iments with Sentence Ordering. In Proc. of the 41st An-
nual Meeting of the Association for Computational Lin-
guistics.
Lapata, Mirella. 2006. Automatic Evaluation of Informa-
tion Ordering: Kendall?s Tau. Computational Linguistics,
32(4):471?484.
Lerman, Kevin, Sasha Blair-Goldensohn and Ryan McDon-
ald. 2009. Sentiment Summarization: Evaluating and
Learning User Preferences. In Proc. of the 12th Confer-
ence of the European Chapter of the Association for Com-
putational Linguistics.
Lin, Chin-Yew. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Text Summarization
Branches Out.
Martins, Andre F. T., and Noah A. Smith. 2009. Summariza-
tion with a Joint Model for Sentence Extraction and Com-
pression. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics Work-
shop on Integer Linear Programming for NLP.
McDonald, Ryan. 2007. A Study of Global Inference Algo-
rithms in Multi-document Summarization. In Proc. of the
29th European Conference on Information Retrieval.
Nishikawa, Hitoshi, Takaaki Hasegawa, Yoshihiro Matsuo
and Genichiro Kikui. 2010. Optimizing Informativeness
and Readability for Sentiment Summarization. In Proc. of
the 48th Annual Meeting of the Association for Computa-
tional Linguistics.
Radev, Dragomir R., Hongyan Jing, Magorzata Sty and
Daniel Tam. 2004. Centroid-based summarization of mul-
tiple documents. Information Processing and Manage-
ment, 40(6):919?938.
Soricut, Radu and Daniel Marcu. 2006. Discourse Genera-
tion Using Utility-Trained Coherence Models. In Proc. of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics.
Takamura, Hiroya and Manabu Okumura. 2009. Text Sum-
marization Model based on Maximum Coverage Problem
and its Variant. In Proc. of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics.
Yih, Wen-tau, Joshua Goodman, Lucy Vanderwende and
Hisami Suzuki. 2007. Multi-Document Summarization by
Maximizing Informative Content-Words. In Proc. of the
20th International Joint Conference on Artificial Intelli-
gence.
918
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1648?1659, Dublin, Ireland, August 23-29 2014.
Learning to Generate Coherent Summary
with Discriminative Hidden Semi-Markov Model
Hitoshi Nishikawa
1
, Kazuho Arita
1
, Katsumi Tanaka
1
,
Tsutomu Hirao
2
, Toshiro Makino
1
and Yoshihiro Matsuo
1
Nippon Telegraph and Telephone Corporation
1
1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan
2
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237 Japan
{
nishikawa.hitoshi, arita.kazuho, tanaka.katsumi
hirao.tsutomu, makino.toshiro, matsuo.yoshihiro
}
@lab.ntt.co.jp
Abstract
In this paper we introduce a novel single-document summarization method based on a hidden
semi-Markov model. This model can naturally model single-document summarization as the
optimization problem of selecting the best sequence from among the sentences in the input doc-
ument under the given objective function and knapsack constraint. This advantage makes it
possible for sentence selection to take the coherence of the summary into account. In addition
our model can also incorporate sentence compression into the summarization process. To demon-
strate the effectiveness of our method, we conduct an experimental evaluation with a large-scale
corpus consisting of 12,748 pairs of a document and its reference. The results show that our
method significantly outperforms the competitive baselines in terms of ROUGE evaluation, and
the linguistic quality of summaries is also improved. Our method successfully mimicked the
reference summaries, about 20 percent of the summaries generated by our method were com-
pletely identical to their references. Moreover, we show that large-scale training samples are
quite effective for training a summarizer.
1 Introduction
Single-document summarization is attracting much more attention as a key technology in providing
better information access in a commercial context. The Financial Times and CNN have been providing
summaries of articles in their websites to attract users, and Summly, which has been acquired by Yahoo!,
provided the service of automatically summarizing articles on the Internet. Given the cost of manual
summarization, we can greatly improve the information access of Internet users by creating an automatic
summarizer that can approach the summarization quality of humans.
To mimic manually-written summaries, one important aspect is coherence (Nenkova and McKeown,
2011). Although coherence has been studied widely in a field of multi-document summarization (Kara-
manis et al., 2004; Barzilay and Lapata, 2005; Nishikawa et al., 2010; Christensen et al., 2013), it has not
been studied enough in the context of single-document summarization. In this paper, we revisit the prob-
lem of coherence and employ it to produce both informative and linguistically high-quality summaries.
To obtain such summaries, we introduce a novel summarization method based on a hidden semi-
Markov model. The method has the properties of both the popular single-document summarization
model, the knapsack problem, which packs the sentences into the given length and the hidden Markov
model, which takes summary coherence into account by determining sentence context when selecting
sentences. By leveraging this, we can build a summarizer that naturally achieves coherence.
We state the novelty and contributions of this paper as follows:
? We regard single-document summarization as a combinatorial optimization problem modeled by a
hidden semi-Markov model and propose an efficient decoding algorithm for the problem.
? We introduce various features related to coherence in a combinatorial formulation. We extend a
hidden semi-Markov model to achieve discrimination, so our method can take advantage of many
features for predicting coherence.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1648
? We show that our large-scale corpus greatly improves the performance of summarization.
This paper is organized as follows. In Section 2, we describe related work. In Section 3, we detail
our proposed model. We also explain how the parameters in our model are optimized and how sentences
are compressed. In Section 4, we explain how variants of the original sentences are generated. In
Section 5, we explain the decoding algorithm for our method. In Section 6, we explain the settings of
our experiments, our corpus, and compared methods. In Section 7, we show results of the experiments
conducted to evaluate our method. In Section 8, we conclude this paper.
2 Related Work
2.1 Single-Document Summarization
Basically, single-document summarization can be done through sentence selection (Nenkova and McK-
eown, 2011) . The document to be summarized is decomposed into a set of sentences and then the
summarizer selects a subset of the sentences as a summary.
McDonald (2007) pointed out that single-document summarization can be formulated as a well-known
combinatorial optimization problem, the knapsack problem. Given a set of sentences together with their
lengths and values, the summarizer packs them into a summary so that the total value is as large as possi-
ble but the total length is less than or equal to a given maximum summary length. Interestingly, a hidden
semi-Markov model (Yu, 2010) can be regarded as a natural extension of the knapsack problem, we take
advantage of this property for single-document summarization. We elaborate the relation between the
knapsack problem and the hidden semi-Markov model in Section 3.
To generate coherent summaries in single-document summarization, there are two types of ap-
proaches
1
: tree-based approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) and
sequence-based approaches (Barzilay and Lee, 2004; Shen et al., 2007). The former rely on the tree
representation of a document based on the Rhetorical Structure Theory (RST) (Mann and Thompson,
1988). Basically, the former approaches (Marcu, 1997; Daume and Marcu, 2002; Hirao et al., 2013) trim
the tree representation of a document by making use of nucleus-satellite relations among sentences. The
advantage of RST-based approaches is that they can take advantage of global information about the doc-
uments. However, a drawback is that they depend heavily on the RST parser that is used. Performance
is remarkably sensitive to the accuracy of RST parsing, and hence we have to build a good RST parser.
Instead of making use of the global structure of the document, the sequence-based methods rely on and
take advantage of the local coherence of sentences. As one advantage over the tree-based approaches,
the sequence-based approaches do not require tools as RST parsers, and hence they are more robust. For
this reason, this paper focuses on sequence-based approaches.
The previous works most closely related to our method are those proposed by Barzilay and Lee (2004)
and Shen et al. (2007). Barzilay and Lee built a hidden Markov model to capture the content structure of
documents and used it to identify the important sentences. Shen et al. (2007) extended the HMM-based
approach to make it discriminative by making use of conditional random fields (Lafferty et al., 2001).
Conditional random fields can incorporate various features to identify the importance of a sentence and
they showed its effectiveness. A shortcoming of these approaches is that their model only classifies sen-
tences into two classes, it cannot take account of output length directly. This deficiency is problematic
because in practical usage the maximum length of a summary is specified by the user; hence, the sum-
marizer should be able to control output length. In contrast to their method, our approach naturally takes
the maximum summary length into account when summarizing a document.
2.2 Coherence
In the context of multi-document summarization, coherence has been studied widely. In multi-document
summarization, sentences are selected from different documents, and hence some way of ordering the
sentences is required. Sentence ordering (Barzilay et al., 2002; Althaus et al., 2004; Karamanis et al.,
1
As an interesting related work, Clarke and Lapata (2007) compresses documents by making use of Centering Theory
(Grosz et al., 1995). However, in their approach, the desired length of an output summary could not be specified and hence they
said their method was compression rather than summarization.
1649
Figure 1: An example of the hidden semi-Markov model. The system observes a sequence consisting
of 10 symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic hidden
Markov model, states can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for
non-unit lengths. Hence, the system traverses only 6 states despite observing 10 symbols.
2004; Okazaki et al., 2004) is a task to order extracted sentences and is closely related to coherence
(Lapata, 2003; Barzilay and Lapata, 2005; Nenkova et al., 2010; Pitler et al., 2010; Louis and Nenkova,
2012). Many effective features have been found out to capture coherence and we utilize these features.
Some work proposed a model that could jointly taking the content of the summary and its coherence
into account (Nishikawa et al., 2010; Christensen et al., 2013). Since extracted sentences in multi-
document summarization must be ordered, a task that is NP-hard, they relied on integer linear program-
ming (Nishikawa et al., 2010) or a local search strategy (Christensen et al., 2013). The former can locate
the optimal solution at a heavy computation cost, while the latter runs quickly but there is no guarantee
of locating the optimal solution. In contrast to their trade-off, our proposed algorithm, based on dynamic
programming, can locate the optimal solution quickly because the single-document summarization can
skip the ordering operation by reproducing the original order of the input sentences.
In this paper, we show that coherence also takes an important role in single-document summarization.
We model the coherence between adjacent sentences in the summary by leveraging the hidden semi-
Markov model, which can naturally capture the coherence between sentences.
3 Summarization with Hidden Semi-Markov Model
We first introduce the knapsack problem, which can naturally model single-document summarization.
Next, we explain the hidden semi-Markov model and show its relationship to the knapsack problem.
Then, we elaborate our summarization method.
3.1 Knapsack Problem
The knapsack problem is a type of combinatorial optimization problem (Korte and Vygen, 2008). Given
a set of elements, each of which has a score and size, the problem is formulated as the task of finding
the best subset in terms of maximizing the sum of their scores under the size limitation. As mentioned
above, single-document summarization can be regarded as an instance of the knapsack problem. The
best combination of input sentences can be found by calculating the value of each sentence and packing
them into a summary through the dynamic programming knapsack algorithm.
3.2 Hidden Semi-Markov Model
The hidden semi-Markov model (HSMM) is an extension of the hidden Markov model (HMM) (Yu,
2010). In the popular hidden Markov model, each state persists for only one unit length. For example,
if a system observes 10 discrete symbols, it outputs 10 hidden states. In the HSMM, each state can
persist for some unit lengths through the concept of duration. For example, if a system observes 10
discrete symbols and each state persists for two unit lengths, i.e., their duration is 2, the system outputs
5 hidden states. We show an example in Figure 1. The system observes a sequence consisting of 10
symbols o
1
...o
10
over time t
1
...t
10
and transitions between states s
1
...s
3
. Unlike the basic HMM, states
can persist for a non-unit length. In this figure, state s
2
and state s
3
persist for a non-unit length. Hence,
the system traverses 6 states even though it observes 10 symbols. This property has been utilized for
1650
sequential tagging, such as named entity recognition (Sarawagi and Cohen, 2004), scene text recognition
(Weinman et al., 2008) and phonetic recognition (Kim et al., 2011).
The hidden semi-Markov model is closely related to the knapsack problem. The length, K, of the
observed symbols can be regarded as a knapsack constraint. We can consider that the system tries to pack
the states of the model into the observed sequence of symbols by transitioning over the states under the
knapsack constraint so as to maximize the likelihood. Therefore, the hidden semi-Markov can naturally
be used for single-document summarization. Suppose that the document to be summarized consists of
10 sentences and the length of each of them is measured by the number of words. In this case, the system
transitions over 10 states corresponding to the 10 sentences until it cannot select any further sentence due
to the given length requirement. Since each state persists for the length of the corresponding sentence,
the remaining length decreases every time the system transitions to a new state.
While an HMM is basically a generative model, Collins (2002) extended it to create a discriminative
model. An HSMM can also be extended to become discriminative model (Sarawagi and Cohen, 2004).
Our discriminative HSMM learns through the application of max-margin training.
3.3 Formulation
We consider there are n input sentences s
1
, s
2
, ..., s
n
. These sentences have lengths ?
1
, ?
2
, ..., ?
n
and
weights w
1
, w
2
, ..., w
n
. We assume that a sentence that has a high weight should be present in the output
summary. We also consider each sentence, s
i
, has m
i
variants s
i,1
, s
i,2
, ..., s
i,m
, each produced by some
sort of sentence compression or paraphrase module. These variants also have lengths ?
i,1
, ?
i,2
, ..., ?
i,m
i
and weights w
i,1
, w
i,2
, ..., w
i,m
i
. For simplicity, we hereinafter note the original sentences s
1
, s
2
, ..., s
n
as s
1,0
, s
2,0
, ..., s
n,0
. Hence we have original sentence s
i,0
and variants s
i,1
, s
i,2
, ..., s
i,m
. Let s
0,0
and
s
n+1,0
be special symbols indicating the beginning of a document and the end of a document, respec-
tively. We define coherence c
g,h,i,j
as the coherence between sentence s
g,h
and sentence s
i,j
. An output
summary is described as a sequence of input sentences, g. LetG be the entire set of sequences that can be
constructed from the input sentences, i.e., g ? G. Finally, let K be the maximum length of the summary
desired. With these notations, our proposed method can be formulated as the following optimization
problem:
g
?
= argmax
g?G
?
s
i,j
?sent(g)
w
i,j
+
?
(s
g,h
,s
i,j
)?adj(g)
c
g,h,i,j
(1)
s.t.
?
s
i,j
?sent(g)
?
i,j
? K, (2)
where sent(g) and adj(g) indicate a set of sentences in g and a set of adjacent sentences in g, respec-
tively. That is, our model tries to find the best sequence of sentences under the knapsack constraint so as
to maximize the sum of weights and sentence coherence. In contrast to the common knapsack problem
which cannot take the variants and sentence coherence into account, our method, based on the hidden
semi-Markov model, does so naturally.
3.4 Parameter Optimization
Here we elaborate how parameters in the model are optimized to achieve the desired summaries. The
goal is to determine the value of w
i,j
for all i, j and c
g,h,i,j
for all g, h, i, j. We define w
i,j
and c
g,h,i,j
as
follows:
w
i,j
= w
w
? f
w
(s
i,j
) (3)
c
g,h,i,j
= w
c
? f
c
(s
g,h
, s
i,j
), (4)
where f
w
and f
c
are d
w
-dimensional and d
c
-dimensional feature vectors for sentences and sentence pairs,
respectively, andw
w
andw
c
are d
w
-dimensional and d
c
-dimensional parameter vectors for sentences and
sentence pairs, respectively. The goal of optimization is to determine the values of both vector w
w
and
1651
wc
, given feature function f
w
and f
c
. For simplicity, let s be a summary, let f = ?f
w
, f
c
? be a (d
w
+ d
c
)-
dimensional feature function for the whole summary and let w = ?w
w
,w
c
? be a (d
w
+ d
c
)-dimensional
weight vector. The value that the objective function outputs for summary s is w ? f(s).
To optimize the parameter, we employ the Passive-Aggressive algorithm (Crammer, 2006), a widely-
used structured learning method. Since the algorithm offers online learning, it can learn the parameter
quickly and is easy to implement. To learn the parameter so that the output summary is optimized to
the evaluation criteria popular in document summarization research, ROUGE (Lin, 2004), we introduce
ROUGE as the loss function. The parameter is estimated by solving the following formula iteratively
2
:
w
new
= argmin
w
1
2
||w ? w
old
||
2
(5)
s.t. w ? f(r) ? w ? f(s) ? loss(s; r),
where w
new
is the parameter vector after update, w
old
is the parameter vector before update, r is a
reference summary, and loss is the loss function. We define loss as 1 ? ROUGE(s; r). Among the
variants of ROUGE, we used ROUGE-1 for the loss function.
3.4.1 Sentence Feature
The features introduced in this section are used to calculate the weights of sentences, w
i,j
.
Term Frequency: Term frequency is a classic feature in document summarization (Luhn, 1958). We
calculate the total number of times each content word occurs in the document and then, for each sentence,
sum the totals of the content words that appear in the sentence as the value of this feature.
Word: We also use the words and parts-of-speech as features.
Named Entity: Named entities such as a name of person or organization are important. We use named
entities and classes as features.
Length: The length of a sentence may indicate the information value of its content. We use the length of
a sentence, measured by character number, as a feature.
Position: The position of a sentence is a classically important feature. We use the position of a sentence,
the relative position of a sentence, whether the sentence is the first in the document and whether the
sentence is the first in a paragraph, the position of the paragraph in which the sentence is, as features.
3.4.2 Coherence Feature
The features introduced in this section are used to calculate sentence coherence, c
g,h,i,h
.
Lexical Transition: Lapata (2003) showed that the structure of the document can be captured by word-
pairs consisting of words of two adjacent sentences. We use this feature for capturing the links between
two sentences
3
. We build a set of word pairs where one occurs in a precedent sentence and the other
occurs in a succeeding one, and use the elements of the set as a feature.
Lexical Cohesion: Pitler et al. (2010) showed that the similarity of two sentences is one of the strongest
features for predicting coherence. We reproduce this feature for generating coherent summaries. We
calculate cosine similarity between two sentences and use its value as a feature.
Entity Grid: Previous studies showed that Entity Grid (Barzilay and Lapata, 2005) is a strong feature
for predicting coherence (Pitler et al., 2010). We also employ this feature for summarization. While the
entity vector made from the entity grid was originally defined for whole documents, we build the entity
vector for each pair of two sentences because our model is based on the Markovian assumption, and
hence the coherence score is defined between two sentences.
2
As we explain later in Section 5, computation complexity of our algorithm is pseudo-polynomial, and hence the best
solution of our model can be located quickly. This is also advantageous in the learning phase because to learn parameters using
structured learning, the learner has to generate a summary to calculate the loss. Since our algorithm can quickly find the best
solution and generate a summary, it can also contribute to shortening the time required for learning.
3
It is expected that this feature will also contribute to sentence selection. Barzilay and Elhadad (1997) showed that a closely
related word-pair was a good indicator for sentence selection. This feature captures this property by learning.
1652
 0
 2000
 4000
 6000
 8000
 10000
 12000
 14000
 16000
 18000
 0  5  10  15  20  25  30
Th
e n
um
be
r o
f s
en
ten
ces
Levenshtein distance
Figure 2: Distribution of Levenshtein distance in the
aligned sentences. Among the 36,413 sentences in
the references, 16,643 were identical (Levenshtein
distance is 0) to the aligned sentences in the input
documents.
 0.6
 0.61
 0.62
 0.63
 0.64
 0.65
 0.66
 0.67
 0.68
 0.69
 0.7
 0  2000  4000  6000  8000  10000
RO
UG
E-
2
The number of training samples
Figure 3: Learning curve of HSMM.
4 Generating Sentence Variants
Since our model can take the variants of an original sentence in the input document as in the multi-
candidate reduction framework (Zajic et al., 2007), we incorporate sentence compression.
We generate a few variants of each original sentence by trimming the dependency tree of the sentence;
this simple operation is sufficient for reproducing reference summaries. By aligning sentences in a refer-
ence summary with those in the corresponding input document
4
, we found that human summaries were
quite conservative. Among the 36,413 sentences in the references, 16,643 were identical to the aligned
sentences in the input documents. Furthermore, most remaining sentences were virtually identical to the
original sentences; revisions were minor, and can be reproduced by simple operations. Few sentences
exhibited paraphrasing or more sophisticated operations. We plot the distribution of Levenshtein distance
in the aligned sentences in Figure 2. According to this observation, we produce the following types of
variants by sentence compression:
1. Removing information in parentheses. Some sentences contain parentheses containing additional
information for readers. The first type of variant deletes text in parentheses.
2. Shortening sentences by trimming their dependency trees. Basically this method follows the sen-
tence trimmer proposed by Nomoto (2008). While using his method, we keep the predicate and its
obligatory arguments in the sentences to keep the sentences grammatical. If a predicate is trimmed,
its obligatory arguments are also trimmed and vice versa. Since there are an exponential number
of subtrees in one tree, we use only n-best subtrees by ranking them according to n-gram language
likelihood and dependency-based language likelihood. We used the dependency parser proposed by
Imamura et al (Imamura et al., 2007) to acquire the dependency tree.
5 Decoding with Dynamic Programming
To solve Equation 1 under the constraints of Equation 2, we use dynamic programming. Algorithm
1 shows the pseudo code of the decoding algorithm. Line 1 to Line 7 initializes the variables used in
the algorithm. Vector x = ?x
0
, ..., x
n+1
? stores which sentence and which variants are included in the
output summary. If x
3
= 2, s
3,2
is included in the summary. V , P and S are two-dimensional arrays,
each of which is used as a dynamic programming table. They store the process of dynamic programming.
4
Alignment proceeds in two steps: first, we calculate the Levenshtein distance between sentences in the document and its
reference, and then we align sentences so as to minimize the distance between them.
1653
Algorithm 1 Decoding Algorithm: Filling Table
1: x = ?x
0
, ..., x
n+1
?
2: for i = 0 to n + 1 do
3: x
i
= ?1
4: V [0][i]? ?1
5: P [0][i]? ?1
6: S[0][i]? 0
7: V [0][0] = 0
8: for k = 1 to K do
9: for i = 1 to n do
10: V [k][i]? V [k ? 1][i]
11: P [k][i]? P [k ? 1][i]
12: S[k][i]? S[k ? 1][i]
13: for v = 0 to m
i
do
14: if ?
i,v
? k then
15: for h = 0 to i? 1 do
16: u = V [k ? ?
i,v
][h]
17: if u ?= ?1 ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
? S[k][i] then
18: V [k][i]? v
19: P [k][i]? h
20: S[k][i]? S[k ? ?
i,v
][h] + w
i,v
+ c
h,u,i,v
21: V [K + 1][n + 1]? 0
22: P [K + 1][n + 1]? 0
23: S[K + 1][n + 1]? 0
24: for h = 1 to n do
25: u = V [K][h]
26: if S[K][h] + c
h,u,n+1,0
? S[K + 1][n + 1] then
27: P [K + 1][n + 1]? h
28: S[K + 1][n + 1]? S[K][h] + c
h,u,n+1,0
Document Reference
Avg. # of characters 476.2 142.0
Avg. # of words 298.6 88.3
Avg. # of sentences 9.7 2.9
Table 1: The statistics of our corpus.
V [k][i] stores which variants are used at time k, i. If V [k][i] = 0, original sentence s
i,0
is selected at
time k, i. If V [k][i] = ?1, no sentence is selected at time k, i. P [k][i] stores a pointer to the sentence
connected to the front of the current sentence. S[k][i] stores the value of the objective function at time
k, i. Line 8 to Line 36 locates the best sequence of sentences based on the following recurrence formula:
S[k][i] =
{
max
h=0...i?1,v=0...m
S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
(A)
S[k ? 1][i] (B),
(6)
where case A is: ?
i,v
? k ? S[k ? 1][i] ? S[k ? ?
i,v
][h] + w
i,v
+ c
h,V [k??
i,v
][h],i,v
and case B is:
otherwise. This recurrence formula means that at time k, i the best variant to be selected as can be
determined at time k ? ?
i,v
, h. Hence, for all k ? 1...K and i ? 1...n, the algorithm finds the best
sequence of sentences at time k, i. After Algorithm 1 locates the best sequence of sentences by filling
the tables, the best sequence can be restored by backtracing along the pointers stored in P . Finally, the
algorithm outputs x, which stores which sentences and variants are used in the best sequence. Since
this algorithm is based on a dynamic programming knapsack algorithm (Korte and Vygen, 2008), it runs
in pseudo-polynomial time. This is a significant advantage over the methods that rely on integer linear
programming solvers due to their substantial computation cost.
6 Experiments
6.1 Data
We prepared 12,748 pairs of Japanese newspaper articles and their manually-written reference sum-
maries. This is one of the largest corpus available for single-document summarization research. The
length of all references is within 150 characters. All references in the corpus were written by a specialist
staff in a Japanese newspaper company and the company sold these summaries for commercial purposes.
1654
We list the statistics of our corpus in Table 1. As shown, the task is to summarize the document in about
a third of its original length in terms of the number of words.
6.2 Evaluation Criteria
ROUGE; ROUGE is an automatic evaluation method for automatic summarization proposed by Lin
(2004). We used ROUGE-1 and ROUGE-2 to evaluate the summaries. Since our document-reference
pairs are written in Japanese, we segmented the sentences into words using the Japanese morphological
analyzer developed by Fuchi and Takagi (1998). When calculating the ROUGE score, we used only
content words (i.e. nouns, verbs and adjectives) and so excluded function words as stop words.
Linguistic Quality: To evaluate the linguistic quality of the summaries generated by our method, we
performed a manual evaluation according to quality questions proposed by the National Institute of
Standards and Technology (NIST) (2007)
5
. We randomly sampled 100 summaries from the outputs of
each method described below and asked 7 subjects to evaluate the summaries according to the questions.
All subjects were Japanese native and none were among the authors. Since the quality questions by
NIST (2007) were designed for multi-document summarization, we used 3 of the 5 NIST questions for
single-document summarization: grammaticality, referential clarity, and structure/coherence. We also
asked the subjects to evaluate overall summary quality.
6.3 Compared Methods
We compared the following 8 methods.
Random: Random method selects sentences in the input document randomly.
Lead: Lead method is a classic baseline in single-document summarization. It only extracts the words
from the beginning of the document until the extracted words reach the given length. We simply extracted
150 characters from the beginning of each document.
Knapsack: The knapsack problem can be used as a single-document summarization model (McDonald,
2007). In this baseline, the weight of each sentence was calculated based on the average probabilities
of the words in the sentence (Nenkova and Vanderwende, 2005). Then, a summary was generated by
solving the knapsack problem.
Knapsack with Supervision: Instead of the average word probabilities used in the above baseline, we
used only sentence features f
w
to weigh a sentence.
Conditional Random Fields: Conditional random fields can be used to weigh sentences (Shen et al.,
2007). Since CRFs required binary labels in learning, we aligned sentences in an input document with
the sentences in its reference as explained in Section 4. We used the probabilities of sentences from
CRFs as the weights of the knapsack problem.
Hidden Semi-Markov Model: This is our proposed method without variants of the original sentences.
It selected sentences only from the set of original sentences.
Hidden Semi-Markov Model with Compression: This is our proposed method with variants of the
original sentences. It selected from among the variants and the original ones.
Human: In the linguistic quality evaluation, we added references to the summaries generated by the
above methods to show the upper bound.
When learning, we did 10-fold cross validation. In the experiments, statistical significance was
checked by Wilcoxon signed-rank test (Wilcoxon, 1945). To counteract the problem of multiple com-
parisons, we used the Holm-Bonferroni method (Holm, 1979) to adjust the significance level, ?.
7 Results and Discussion
We show the results of our experiment in Table 2 and Table 3. In this section, first we discuss the results
of the ROUGE evaluation, and then we discuss the results of the linguistic quality evaluation.
In the ROUGE evaluation, all the compared methods except for RANDOM showed good performance.
This is because, as shown in Section 4, many references consisted of sentences identical to the original
5
Some recent studies have tried to predict the readability of the text automatically (Pitler et al., 2010).
1655
Method R-1 R-2 Idt.
RANDOM 0.417 0.291 1.2%
LEAD 0.779
C,S,U,R
0.727
C,S,U,R
4.4%
KP 0.704
R
0.611
R
9.3%
KP(S) 0.729
U,R
0.647
U,R
10.4%
CRFs 0.741
U,R
0.675
S,U,R
11.3%
HSMM 0.769
C,S,U,R
0.703
C,S,U,R
15.2%
HSMM(C) 0.785
C,S,U,R
0.722
C,S,U,R
20.4%
Table 2: Results of the ROUGE evaluation.
?R-1? and ?R-2? correspond to ROUGE-1 and
ROUGE-2, respectively. The values in the col-
umn of ?Idt.? are the percentage of summaries
completely-identical to the corresponding refer-
ences. In the table,
C,S,U,L,R
indicate statisti-
cal significance against CRFs, KP(S), KP, LEAD,
RANDOM, respectively.
Method Gram. Ref. S./C. Overall
LEAD 1.9 3.9 2.5 2.1
KP 4.1
L
3.7 3.4 3.5
KP(S) 4.2
L
3.6 3.5 3.6
L
CRFs 4.1
L
3.9 3.7
L
3.6
L
HSMM 4.3
L
4.0 4.1
L
4.0
L
HSMM(C) 4.0
L
3.9 4.0
L
3.9
L
HUMAN 4.7
L
4.5 4.7
L
4.8
L
Table 3: Results of the linguistic quality evalua-
tion. The values ranged from 1 (very poor) to 5
(very good) (National Institute of Standards and
Technology, 2007). We show statistical signifi-
cance with the same notations as Table 2.
ones, and hence the references can be reproduced if important sentences are identified. Since the com-
pression rate in our corpus was relatively light, it made important information easy to identify. Among
the compared methods, both LEAD and our proposed method, HSMM(C), achieved the best result. There
was no significant difference between LEAD and HSMM(C). This surprising performance of LEAD was
due to the ROUGE evaluation. The words in the document leads were likely to be important, and LEAD
drew on this property. However, as we mentioned later, it sacrificed the linguistic quality to achieve the
high ROUGE score. Furthermore, it failed to yield summaries identical to the reference. In contrast to
LEAD, almost 20% of the summaries generated by HSMM(C) were identical to the references. This
shows that our method successfully mimicked human assessments. HSMM followed the best models.
There was a statistically significant difference between HSMM(C) and HSMM. Since some sentences,
especially the first sentence in the document, were long and the first sentence was particularly impor-
tant to summarize the document, sentence compression yielded a significant improvement. As shown
in Table 2, employing compression greatly improved the percentage of identical summaries. HSMM
significantly outperformed all of the baseline extractive methods except LEAD. While CRFs can take
advantage of all features used in HSMM, CRFs cannot take the evaluation measure such as ROUGE and
the knapsack constraint into account in learning. HSMM also significantly outperformed KP(S). This
difference is particularly important, and shows the usefulness of features related to coherence. While
KP(S) used only features about sentences, HSMM successfully mimicked the references as it drew on
the features related to coherence.
We show the learning curve of HSMM in Figure 3. We fixed 2,748 pairs for testing, and learned
parameters from 100, 250, 500, 1,000, 2,500, 5,000, 7,500 and 10,000 pairs. The curve in the figure
clearly shows the effectiveness of our large-scale corpus in learning. It seems that the curve does not
saturate and hence HSMM performance can be improved by more training samples. As in the results
recently shown by Filippova (2013), this result implies that large-scale data is important in the field
of document summarization as in other fields of computational linguistics. Past studies in document
summarization relied on relatively small datasets consisting of a few dozen or at most a few hundred
pairs of a document and its reference in learning. In contrast to the past studies, there are over 10,000
pairs in our dataset and the results show its effectiveness.
Second, we discuss the result of the linguistic quality evaluation. Unlike the ROUGE evaluation,
HSMM achieved the best result. As previous studies have pointed out (Nenkova and McKeown, 2011),
sentence compression commonly tends to degrade the linguistic quality of a summary while improving
its content. As shown in Table 3, the grammaticality of HSMM(C) is lower than that of HSMM, but the
1656
difference is not significant. Although we could not observe any significant difference between HSMM
and other extractive baselines, our proposals, HSMM and HSMM(C), yielded the best result in terms
of structure/coherence. By making use of the features related to coherence, we successfully improved
summary quality. In contrast to the surprising performance of LEAD in the ROUGE evaluation, in the
linguistic quality evaluation, LEAD yielded the worst performance. Since LEAD had to cut the sentences
when it reached the given length, it create ungrammatical fragments.
Finally, we touch on the balance between the quality of content and linguistic quality. Comparing
Table 2 to 3, we can see the correlation between the quality of content and linguistic quality. This re-
sult is reasonable because we can extract much more information from grammatical and well-organized
sentences. Although we optimized the parameter to maximize the ROUGE score, it also yielded im-
provements in linguistic quality. This is because the manually-generated reference summaries are ba-
sically grammatical and well-organized and the parameter is learnt to mimic them. However, there is
an inherent trade-off between the quality of content and linguistic quality. For example, under stricter
length limitations, instead of cohesive devices such as conjunctions, which can improve the coherence of
sentences, content words would be preferred for summary inclusion to augment information. Balancing
them to maximize reader satisfaction is an interesting problem.
8 Conclusions
In this paper we presented a novel single-document summarization method based on the hidden semi-
Markov model, which is a natural extension of the knapsack problem. Our model naturally takes account
of sentence context when identifying important sentences. This property is particularly important to
ensure the coherence of output summaries and to produce informative and linguistically high-quality
summaries. We also proposed an algorithm based on dynamic programming so the best solution can be
located quickly. Experiments on a very large-scale single-document summarization corpus showed that
our proposed method significantly outperforms competitive baselines.
As future work, we plan to tackle on the summarization task where higher compression is demanded.
To generate shorter summaries, we plan to employ more sophisticated approaches, such as paraphrasing.
Acknowledgement
The corpus used in this paper is owned by The Mainichi Newspapers Co., Ltd. and is leased to Nippon
Telegraph and Telephone Corporation. We sincerely appreciate their consideration. We also appreciate
the insightful comments from reviewers. Their comments greatly improved the quality of this paper.
References
Ernst Althaus, Nikiforos Karamanis, and Alexander Koller. 2004. Computing locally coherent discourses. In
Proceedings of the 42nd Meeting of the Association for Computational Linguistics (ACL), pages 399?406.
Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the
Intelligent Scalable Text Summarization Workshop (ISTS), pages 10?17.
Regina Barzilay and Mirella Lapata. 2005. Modeling local coherence: an entity-based approach. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics (ACL), pages 141?148.
Regina Barzilay and Lillian Lee. 2004. Catching the drift: Probabilistic content models, with applications to
generation and summarization. In HLT-NAACL 2004: Main Proceedings, pages 113?120.
Regina Barzilay, Noemie Elhadad, and Kathleen R. McKeown. 2002. Inferring strategies for sentence ordering in
multidocument news summarization. Journal of Artificial Intelligence Research, 17:35?55.
Janara Christensen, Mausam, Stephen Soderland, and Oren Etzioni. 2013. Towards coherent multi-document
summarization. In Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 1163?1173.
James Clarke and Mirella Lapata. 2007. Modelling compression with discourse constraints. In Proceedings of
the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 1?11.
1657
Michael Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pages 1?8.
Koby Crammer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research,
7(Mar):551?585.
Hal Daume, III and Daniel Marcu. 2002. A noisy-channel model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 449?456.
Katja Filippova. 2013. Overcoming the lack of parallel data in sentence compression. In Proceedings of the 2013
Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1481?1491.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese morphological analyzer using word co-occurrence: Jtag.
In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th
International Conference on Computational Linguistics (ACL-COLING), pages 409?413.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi. 1995. Centering: a framework for modeling the local
coherence of discourse. Computational Linguistics, 21(2):203?225.
Tsutomu Hirao, Yasuhisa Yoshida, Masaaki Nishino, Norihito Yasuda, and Masaaki Nagata. 2013. Single-
document summarization as a tree knapsack problem. In Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing (EMNLP), pages 1515?1520.
Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics,
6(2):65?70.
Kenji Imamura, Genichiro Kikui, and Norihito Yasuda. 2007. Japanese dependency parsing using sequential label-
ing for semi-spoken language. In Proceedings of the 45th Annual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 225?228.
Nikiforos Karamanis, Massimo Poesio, Chris Mellish, and Jon Oberlander. 2004. Evaluating centering-based
metrics of coherence. In Proceedings of the 42nd Meeting of the Association for Computational Linguistics
(ACL), pages 391?398.
Sungwoong Kim, Sungrack Yun, and Chang D. Yoo. 2011. Large margin discriminative semi-markov model
for phonetic recognition. IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING,
7(19):1999?2012.
Bernhard Korte and Jens Vygen. 2008. Combinatorial Optimization. Springer-Verlag, third edition.
John Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on
Machine Learning (ICML), pages 282?289.
Mirella Lapata. 2003. Probabilistic text structuring: Experiments with sentence ordering. In Proceedings of the
41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 545?552.
Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Proceedings of ACL Workshop
Text Summarization Branches Out, pages 74?81.
Annie Louis and Ani Nenkova. 2012. A coherence model based on syntactic patterns. In Proceedings of the 2012
Conference on Empirical Methods on Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL).
Hans P. Luhn. 1958. The automatic creation of literature abstracts. IBM Journal of Research and Development,
22(2):159?165.
William C. Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Daniel Marcu. 1997. From discourse structure to text summaries. In Proceedings of ACL/EACL 1997 Summariza-
tion Workshop, pages 82?88.
Ryan McDonald. 2007. A study of global inference algorithms in multi-document summarization. In Proceedings
of the 29th European Conference on Information Retrieval (ECIR), pages 557?564.
1658
National Institute of Standards and Technology. 2007. The linguistic quality questions. http://www-nlpir.
nist.gov/projects/duc/duc2007/quality-questions.txt.
Ani Nenkova and Kathleen McKeown. 2011. Automatic Summarization. Now Publishers.
Ani Nenkova and Lucy Vanderwende. 2005. The impact of frequency on summarization. Technical report,
MSR-TR-2005-101.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily Pitler. 2010. Structural features for predicting the linguistic
quality of text: Applications to machine translation, automatic summarization and human-authored text. In
Emiel Krahmer and Theunem Mariet, editors, Empirical Methods in Natural Language Generation: Data-
oriented Methods and Empirical Evaluation, pages 222?241. Springer.
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo, and Genichiro Kikui. 2010. Opinion summarization
with integer linear programming formulation for sentence extraction and ordering. In Coling 2010: Posters,
pages 910?918.
Tadashi Nomoto. 2008. A generic sentence trimmer with crfs. In Proceedings of the 46th Annual Conference of
the Association for Computational Linguistics: Human Language Technologies (ACL-HLT), pages 299?307.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka. 2004. Improving chronological sentence ordering by
precedence relation. In Proceedings of the 20th International Conference on Computational Linguistics (Col-
ing), pages 750?756.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010. Automatic evaluation of linguistic quality in multi-document
summarization. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics
(ACL), pages 544?554.
Sunita Sarawagi and William W. Cohen. 2004. Semi-markov conditional random fields for information extraction.
In Advances in Neural Information Processing Systems 17, pages 1185?1192.
Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and Zheng Chen. 2007. Document summarization using conditional
random fields. In Proceedings of the 20th international joint conference on Artifical intelligence (IJCAI), pages
2862?2867.
Jerod J. Weinman, Erik Learned-Miller, and Allen Hanson. 2008. A discriminative semi-markov model for robust
scene text recognition. In Proceedings of the 19th International Conference on Pattern Recognition (ICPR),
pages 1?5.
Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):80?83.
Shun-Zheng Yu. 2010. Hidden semi-markov models. Artificial Intelligence, 174(2):215?243.
David M. Zajic, Bonnie J. Dorr, Jimmy Lin, and Schwartz Richard. 2007. Multi-candidate reduction: Sentence
compression as a tool for document summarization tasks. Information Processing and Management, 43:1549?
1570.
1659
Proceedings of the ACL 2010 Conference Short Papers, pages 325?330,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimizing Informativeness and Readability
for Sentiment Summarization
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan
{
nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
We propose a novel algorithm for senti-
ment summarization that takes account of
informativeness and readability, simulta-
neously. Our algorithm generates a sum-
mary by selecting and ordering sentences
taken from multiple review texts according
to two scores that represent the informa-
tiveness and readability of the sentence or-
der. The informativeness score is defined
by the number of sentiment expressions
and the readability score is learned from
the target corpus. We evaluate our method
by summarizing reviews on restaurants.
Our method outperforms an existing al-
gorithm as indicated by its ROUGE score
and human readability experiments.
1 Introduction
The Web holds a massive number of reviews de-
scribing the sentiments of customers about prod-
ucts and services. These reviews can help the user
reach purchasing decisions and guide companies?
business activities such as product improvements.
It is, however, almost impossible to read all re-
views given their sheer number.
These reviews are best utilized by the devel-
opment of automatic text summarization, partic-
ularly sentiment summarization. It enables us to
efficiently grasp the key bits of information. Senti-
ment summarizers are divided into two categories
in terms of output style. One outputs lists of
sentences (Hu and Liu, 2004; Blair-Goldensohn
et al, 2008; Titov and McDonald, 2008), the
other outputs texts consisting of ordered sentences
(Carenini et al, 2006; Carenini and Cheung, 2008;
Lerman et al, 2009; Lerman and McDonald,
2009). Our work lies in the latter category, and
a typical summary is shown in Figure 1. Although
visual representations such as bar or rader charts
This restaurant offers customers delicious foods and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Figure 1: A typical summary.
are helpful, such representations necessitate some
simplifications of information to presentation. In
contrast, text can present complex information that
can?t readily be visualized, so in this paper we fo-
cus on producing textual summaries.
One crucial weakness of existing text-oriented
summarizers is the poor readability of their results.
Good readability is essential because readability
strongly affects text comprehension (Barzilay et
al., 2002).
To achieve readable summaries, the extracted
sentences must be appropriately ordered (Barzilay
et al, 2002; Lapata, 2003; Barzilay and Lee, 2004;
Barzilay and Lapata, 2005). Barzilay et al (2002)
proposed an algorithm for ordering sentences ac-
cording to the dates of the publications from which
the sentences were extracted. Lapata (2003) pro-
posed an algorithm that computes the probability
of two sentences being adjacent for ordering sen-
tences. Both methods delink sentence extraction
from sentence ordering, so a sentence can be ex-
tracted that cannot be ordered naturally with the
other extracted sentences.
To solve this problem, we propose an algorithm
that chooses sentences and orders them simulta-
neously in such a way that the ordered sentences
maximize the scores of informativeness and read-
ability. Our algorithm efficiently searches for the
best sequence of sentences by using dynamic pro-
gramming and beam search. We verify that our
method generates summaries that are significantly
better than the baseline results in terms of ROUGE
score (Lin, 2004) and subjective readability mea-
sures. As far as we know, this is the first work to
325
simultaneously achieve both informativeness and
readability in the area of multi-document summa-
rization.
This paper is organized as follows: Section 2
describes our summarization method. Section 3
reports our evaluation experiments. We conclude
this paper in Section 4.
2 Optimizing Sentence Sequence
Formally, we define a summary S? =
?s0, s1, . . . , sn, sn+1? as a sequence consist-
ing of n sentences where s0 and sn+1 are symbols
indicating the beginning and ending of the se-
quence, respectively. Summary S? is also defined
as follows:
S? = argmax
S?T
[Info(S) + ?Read(S)] (1)
s.t. length(S) ? K
where Info(S) indicates the informativeness
score of S, Read(S) indicates the readability
score of S, T indicates possible sequences com-
posed of sentences in the target documents, ?
is a weight parameter balancing informativeness
against readability, length(S) is the length of S,
and K is the maximum size of the summary.
We introduce the informativeness score and the
readability score, then describe how to optimize a
sequence.
2.1 Informativeness Score
Since we attempt to summarize reviews, we as-
sume that a good summary must involve as many
sentiments as possible. Therefore, we define the
informativeness score as follows:
Info(S) =
?
e?E(S)
f(e) (2)
where e indicates sentiment e = ?a, p? as the tu-
ple of aspect a and polarity p = {?1, 0, 1}, E(S)
is the set of sentiments contained S, and f(e) is the
score of sentiment e. Aspect a represents a stand-
point for evaluating products and services. With
regard to restaurants, aspects include food, atmo-
sphere and staff. Polarity represents whether the
sentiment is positive or negative. In this paper, we
define p = ?1 as negative, p = 0 as neutral and
p = 1 as positive sentiment.
Notice that Equation 2 defines the informative-
ness score of a summary as the sum of the score
of the sentiments contained in S. To avoid du-
plicative sentences, each sentiment is counted only
once for scoring. In addition, the aspects are clus-
tered and similar aspects (e.g. air, ambience) are
treated as the same aspect (e.g. atmosphere). In
this paper we define f(e) as the frequency of e in
the target documents.
Sentiments are extracted using a sentiment lex-
icon and pattern matched from dependency trees
of sentences. The sentiment lexicon1 consists of
pairs of sentiment expressions and their polarities,
for example, delicious, friendly and good are pos-
itive sentiment expressions, bad and expensive are
negative sentiment expressions.
To extract sentiments from given sentences,
first, we identify sentiment expressions among
words consisting of parsed sentences. For ex-
ample, in the case of the sentence ?This restau-
rant offers customers delicious foods and a relax-
ing atmosphere.? in Figure 1, delicious and re-
laxing are identified as sentiment expressions. If
the sentiment expressions are identified, the ex-
pressions and its aspects are extracted as aspect-
sentiment expression pairs from dependency tree
using some rules. In the case of the example sen-
tence, foods and delicious, atmosphere and relax-
ing are extracted as aspect-sentiment expression
pairs. Finally extracted sentiment expressions are
converted to polarities, we acquire the set of sen-
timents from sentences, for example, ? foods, 1?
and ? atmosphere, 1?.
Note that since our method relies on only senti-
ment lexicon, extractable aspects are unlimited.
2.2 Readability Score
Readability consists of various elements such as
conciseness, coherence, and grammar. Since it
is difficult to model all of them, we approximate
readability as the natural order of sentences.
To order sentences, Barzilay et al (2002)
used the publication dates of documents to catch
temporally-ordered events, but this approach is not
really suitable for our goal because reviews focus
on entities rather than events. Lapata (2003) em-
ployed the probability of two sentences being ad-
jacent as determined from a corpus. If the cor-
pus consists of reviews, it is expected that this ap-
proach would be effective for sentiment summa-
rization. Therefore, we adopt and improve Lap-
ata?s approach to order sentences. We define the
1Since we aim to summarize Japanese reviews, we utilize
Japanese sentiment lexicon (Asano et al, 2008). However,
our method is, except for sentiment extraction, language in-
dependent.
326
readability score as follows:
Read(S) =
n
?
i=0
w>?(si, si+1) (3)
where, given two adjacent sentences si and
si+1, w>?(si, si+1), which measures the connec-
tivity of the two sentences, is the inner product of
w and ?(si, si+1), w is a parameter vector and
?(si, si+1) is a feature vector of the two sentences.
That is, the readability score of sentence sequence
S is the sum of the connectivity of all adjacent sen-
tences in the sequence.
As the features, Lapata (2003) proposed the
Cartesian product of content words in adjacent
sentences. To this, we add named entity tags (e.g.
LOC, ORG) and connectives. We observe that the
first sentence of a review of a restaurant frequently
contains named entities indicating location. We
aim to reproduce this characteristic in the order-
ing.
We also define feature vector ?(S) of the entire
sequence S = ?s0, s1, . . . , sn, sn+1? as follows:
?(S) =
n
?
i=0
?(si, si+1) (4)
Therefore, the score of sequence S is w>?(S).
Given a training set, if a trained parameter w as-
signs a score w>?(S+) to an correct order S+
that is higher than a score w>?(S?) to an incor-
rect order S?, it is expected that the trained pa-
rameter will give higher score to naturally ordered
sentences than to unnaturally ordered sentences.
We use Averaged Perceptron (Collins, 2002) to
find w. Averaged Perceptron requires an argmax
operation for parameter estimation. Since we at-
tempt to order a set of sentences, the operation is
regarded as solving the Traveling Salesman Prob-
lem; that is, we locate the path that offers maxi-
mum score through all n sentences as s0 and sn+1
are starting and ending points, respectively. Thus
the operation is NP-hard and it is difficult to find
the global optimal solution. To alleviate this, we
find an approximate solution by adopting the dy-
namic programming technique of the Held and
Karp Algorithm (Held and Karp, 1962) and beam
search.
We show the search procedure in Figure 2. S
indicates intended sentences and M is a distance
matrix of the readability scores of adjacent sen-
tence pairs. Hi(C, j) indicates the score of the
hypothesis that has covered the set of i sentences
C and has the sentence j at the end of the path,
Sentences: S = {s1, . . . , sn}
Distance matrix: M = [ai,j ]i=0...n+1,j=0...n+1
1: H0({s0}, s0) = 0
2: for i : 0 . . . n ? 1
3: for j : 1 . . . n
4: foreach Hi(C\{j}, k) ? b
5: Hi+1(C, j) = maxHi(C\{j},k)?bHi(C\{j}, k)
6: +Mk,j
7: H? = maxHn(C,k) H
n(C, k) +Mk,n+1
Figure 2: Held and Karp Algorithm.
i.e. the last sentence of the summary being gener-
ated. For example, H2({s0, s2, s5}, s2) indicates
a hypothesis that covers s0, s2, s5 and the last sen-
tence is s2. Initially, H0({s0}, s0) is assigned the
score of 0, and new sentences are then added one
by one. In the search procedure, our dynamic pro-
gramming based algorithm retains just the hypoth-
esis with maximum score among the hypotheses
that have the same sentences and the same last sen-
tence. Since this procedure is still computationally
hard, only the top b hypotheses are expanded.
Note that our method learns w from texts auto-
matically annotated by a POS tagger and a named
entity tagger. Thus manual annotation isn?t re-
quired.
2.3 Optimization
The argmax operation in Equation 1 also involves
search, which is NP-hard as described in Section
2.2. Therefore, we adopt the Held and Karp Algo-
rithm and beam search to find approximate solu-
tions. The search algorithm is basically the same
as parameter estimation, except for its calculation
of the informativeness score and size limitation.
Therefore, when a new sentence is added to a hy-
pothesis, both the informativeness and the read-
ability scores are calculated. The size of the hy-
pothesis is also calculated and if the size exceeds
the limit, the sentence can?t be added. A hypoth-
esis that can?t accept any more sentences is re-
moved from the search procedure and preserved
in memory. After all hypotheses are removed,
the best hypothesis is chosen from among the pre-
served hypotheses as the solution.
3 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We collected 2,940
reviews of 100 restaurants from a website. The
327
R-2 R-SU4 R-SU9
Baseline 0.089 0.068 0.062
Method1 0.157 0.096 0.089
Method2 0.172 0.107 0.098
Method3 0.180 0.110 0.101
Human 0.258 0.143 0.131
Table 1: Automatic ROUGE evaluation.
average size of each document set (corresponds to
one restaurant) was 5,343 bytes. We attempted
to generate 300 byte summaries, so the summa-
rization rate was about 6%. We used CRFs-
based Japanese dependency parser (Imamura et
al., 2007) and named entity recognizer (Suzuki et
al., 2006) for sentiment extraction and construct-
ing feature vectors for readability score, respec-
tively.
3.1 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
To evaluate the effects of the informativeness
score, the readability score and the optimization,
we compared the following five methods.
Baseline: employs MMR (Carbonell and Gold-
stein, 1998). We designed the score of a sentence
as term frequencies of the content words in a doc-
ument set.
Method1: uses optimization without the infor-
mativeness score or readability score. It also used
term frequencies to score sentences.
Method2: uses the informativeness score and
optimization without the readability score.
Method3: the proposed method. Following
Equation 1, the summarizer searches for a se-
quence with high informativeness and readability
score. The parameter vector w was trained on the
same 2,940 reviews in 5-fold cross validation fash-
ion. ? was set to 6,000 using a development set.
Human is the reference summaries. To com-
pare our summarizer to human summarization, we
calculated ROUGE scores between each reference
and the other references, and averaged them.
The results of these experiments are shown in
Table 1. ROUGE scores increase in the order of
Method1, Method2 and Method3 but no method
could match the performance of Human. The
methods significantly outperformed Baseline ac-
Numbers
Baseline 1.76
Method1 4.32
Method2 10.41
Method3 10.18
Human 4.75
Table 2: Unique sentiment numbers.
cording to the Wilcoxon signed-rank test.
We discuss the contribution of readability to
ROUGE scores. Comparing Method2 to Method3,
ROUGE scores of the latter were higher for all cri-
teria. It is interesting that the readability criterion
also improved ROUGE scores.
We also evaluated our method in terms of sen-
timents. We extracted sentiments from the sum-
maries using the above sentiment extractor, and
averaged the unique sentiment numbers. Table 2
shows the results.
The references (Human) have fewer sentiments
than the summaries generated by our method. In
other words, the references included almost as
many other sentences (e.g. reasons for the senti-
ments) as those expressing sentiments. Carenini
et al (2006) pointed out that readers wanted ?de-
tailed information? in summaries, and the reasons
are one of such piece of information. Including
them in summaries would greatly improve sum-
marizer appeal.
3.2 Readability
Readability was evaluated by human judges.
Three different summarizers generated summaries
for each document set. Ten judges evaluated the
thirty summaries for each. Before the evalua-
tion the judges read evaluation criteria and gave
points to summaries using a five-point scale. The
judges weren?t informed of which method gener-
ated which summary.
We compared three methods; Ordering sen-
tences according to publication dates and posi-
tions in which sentences appear after sentence
extraction (Method2), Ordering sentences us-
ing the readability score after sentence extrac-
tion (Method2+) and searching a document set
to discover the sequence with the highest score
(Method3).
Table 3 shows the results of the experiment.
Readability increased in the order of Method2,
Method2+ and Method3. According to the
328
Readability point
Method2 3.45
Method2+ 3.54
Method3 3.74
Table 3: Readability evaluation.
Wilcoxon signed-rank test, there was no signifi-
cance difference between Method2 and Method2+
but the difference between Method2 and Method3
was significant, p < 0.10.
One important factor behind the higher read-
ability of Method3 is that it yields longer sen-
tences on average (6.52). Method2 and Method2+
yielded averages of 7.23 sentences. The difference
is significant as indicated by p < 0.01. That is,
Method2 and Method2+ tended to select short sen-
tences, which made their summaries less readable.
4 Conclusion
This paper proposed a novel algorithm for senti-
ment summarization that takes account of infor-
mativeness and readability, simultaneously. To
summarize reviews, the informativeness score is
based on sentiments and the readability score is
learned from a corpus of reviews. The preferred
sequence is determined by using dynamic pro-
gramming and beam search. Experiments showed
that our method generated better summaries than
the baseline in terms of ROUGE score and read-
ability.
One future work is to include important infor-
mation other than sentiments in the summaries.
We also plan to model the order of sentences glob-
ally. Although the ordering model in this paper is
local since it looks at only adjacent sentences, a
model that can evaluate global order is important
for better summaries.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the reviewers for their comments.
References
Hisako Asano, Toru Hirano, Nozomi Kobayashi and
Yoshihiro Matsuo. 2008. Subjective Information In-
dexing Technology Analyzing Word-of-mouth Con-
tent on the Web. NTT Technical Review, Vol.6, No.9.
Regina Barzilay, Noemie Elhadad and Kathleen McK-
eown. 2002. Inferring Strategies for Sentence Or-
dering in Multidocument Summarization. Journal of
Artificial Intelligence Research (JAIR), Vol.17, pp.
35?55.
Regina Barzilay and Lillian Lee. 2004. Catching the
Drift: Probabilistic Content Models, with Applica-
tions to Generation and Summarization. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pp. 113?120.
Regina Barzilay and Mirella Lapata. 2005. Modeling
Local Coherence: An Entity-based Approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pp.
141?148.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A. Reis and Jeff Rey-
nar. 2008. Building a Sentiment Summarizer for Lo-
cal Service Reviews. WWW Workshop NLP Chal-
lenges in the Information Explosion Era (NLPIX).
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval (SIGIR), pp. 335?356.
Giuseppe Carenini, Raymond Ng and Adam Pauls.
2006. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the 11th European
Chapter of the Association for Computational Lin-
guistics (EACL), pp. 305?312.
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
Extractive vs. NLG-based Abstractive Summariza-
tion of Evaluative Text: The Effect of Corpus Con-
troversiality. In Proceedings of the 5th International
Natural Language Generation Conference (INLG),
pp. 33?41.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 2002 Conference on Empirical Methods on
Natural Language Processing (EMNLP), pp. 1?8.
Michael Held and Richard M. Karp. 1962. A dy-
namic programming approach to sequencing prob-
lems. Journal of the Society for Industrial and Ap-
plied Mathematics (SIAM), Vol.10, No.1, pp. 196?
210.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD), pp. 168?
177.
329
Kenji Imamura, Genichiro Kikui and Norihito Yasuda.
2007. Japanese Dependency Parsing Using Sequen-
tial Labeling for Semi-spoken Language. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pp. 225?228.
Mirella Lapata. 2003. Probabilistic Text Structuring:
Experiments with Sentence Ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 545?552.
Kevin Lerman, Sasha Blair-Goldensohn and Ryan Mc-
Donald. 2009. Sentiment Summarization: Evalu-
ating and Learning User Preferences. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pp. 514?522.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
Summarization: An Experiment with Consumer Re-
views. In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), Companion Vol-
ume: Short Papers, pp. 113?116.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pp. 74?81.
Jun Suzuki, Erik McDermott and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multi-
variate Evaluation Measures. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL
(COLING-ACL), pp. 217?224.
Ivan Titov and Ryan McDonald. 2008. A Joint Model
of Text and Aspect Ratings for Sentiment Summa-
rization. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT),
pp. 308?316.
330
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 388?392,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Grammar Error Correction
Using Pseudo-Error Sentences and Domain Adaptation
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi Nishikawa
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, 239-0847, Japan
{
imamura.kenji, saito.kuniko
sadamitsu.kugatsu, nishikawa.hitoshi
}
@lab.ntt.co.jp
Abstract
This paper presents grammar error correction
for Japanese particles that uses discrimina-
tive sequence conversion, which corrects erro-
neous particles by substitution, insertion, and
deletion. The error correction task is hindered
by the difficulty of collecting large error cor-
pora. We tackle this problem by using pseudo-
error sentences generated automatically. Fur-
thermore, we apply domain adaptation, the
pseudo-error sentences are from the source
domain, and the real-error sentences are from
the target domain. Experiments show that sta-
ble improvement is achieved by using domain
adaptation.
1 Introduction
Case marks of a sentence are represented by postpo-
sitional particles in Japanese. Incorrect usage of the
particles causes serious communication errors be-
cause the cases become unclear. For example, in
the following sentence, it is unclear what must be
deleted.
mail o todoi tara sakujo onegai-shi-masu
mail ACC. arrive when delete please
?When ? has arrived an e-mail, please delete it.?
If the accusative particle o is replaced by a nomi-
native one ga, it becomes clear that the writer wants
to delete the e-mail (?When the e-mail has arrived,
please delete it.?). Such particle errors frequently
occur in sentences written by non-native Japanese
speakers.
This paper presents a method that can automat-
ically correct Japanese particle errors. This task
corresponds to preposition/article error correction in
English. For English error correction, many stud-
ies employ classifiers, which select the appropriate
prepositions/articles, by restricting the error types
to articles and frequent prepositions (Gamon, 2010;
Han et al, 2010; Rozovskaya and Roth, 2011).
On the contrary, Mizumoto et al (2011) proposed
translator-based error correction. This approach can
handle all error types by converting the learner?s
sentences into the correct ones. Although the target
of this paper is particle error, we employ a similar
approach based on sequence conversion (Imamura
et al, 2011) since this offers excellent scalability.
The conversion approach requires pairs of the
learner?s and the correct sentences. However, col-
lecting a sufficient number of pairs is expensive. To
avoid this problem, we use additional corpus con-
sisting of pseudo-error sentences automatically gen-
erated from correct sentences that mimic the real-
errors (Rozovskaya and Roth, 2010b). Furthermore,
we apply a domain adaptation technique that re-
gards the pseudo-errors and the real-errors as the
source and the target domain, respectively, so that
the pseudo-errors better match the real-errors.
2 Error Correction by Discriminative
Sequence Conversion
We start by describing discriminative sequence con-
version. Our error correction method converts the
learner?s word sequences into the correct sequences.
Our method is similar to phrase-based statistical ma-
chine translation (PBSMT), but there are three dif-
ferences; 1) it adopts the conditional random fields,
2) it allows insertion and deletion, and 3) binary and
real features are combined. Unlike the classification
388
Incorrect Particle Correct Particle Note
? no/POSS. INS
? o/ACC. INS
ga/NOM. o/ACC. SUB
o/ACC. ni/DAT. SUB
o/ACC. ga/NOM. SUB
wa/TOP. o/ACC. SUB
no/POSS. ? DEL
: :
Table 1: Example of Phrase Table (partial)
approach, the conversion approach can correct mul-
tiple errors of all types in a sentence.
2.1 Basic Procedure
We apply the morpheme conversion approach that
converts the results of a speech recognizer into word
sequences for language analyzer processing (Ima-
mura et al, 2011). It corrects particle errors in the
input sentences as follows.
? First, all modification candidates are obtained by
referring to a phrase table. This table, called the
confusion set (Rozovskaya and Roth, 2010a) in
the error correction task, stores pairs of incorrect
and correct particles (Table 1). The candidates are
packed into a lattice structure, called the phrase
lattice (Figure 1). To deal with unchanged words,
it also copies the input words and inserts them into
the phrase lattice.
? Next, the best phrase sequence in the phrase lat-
tice is identified based on the conditional random
fields (CRFs (Lafferty et al, 2001)). The Viterbi
algorithm is applied to the decoding because error
correction does not change the word order.
? While training, word alignment is carried out by
dynamic programming matching. From the align-
ment results, the phrase table is constructed by ac-
quiring particle errors, and the CRF models are
trained using the alignment results as supervised
data.
2.2 Insertion / Deletion
Since an insertion can be regarded as replacing an
empty word with an actual word, and deletion is the
replacement of an actual word with an empty one,
we treat these operations as substitution without dis-
tinction while learning/applying the CRF models.
mailnounInput Words oACC. todoiverb taraPART ?
Phrase Lattice mail o todoi tara
copy INS copySUB copy copy
<s>
Incorrect Particle
noun
noPOSS.
ACC.
gaNOM.
niDAT.
verb PART
oACC.
Figure 1: Example of Phrase Lattice
However, insertion is a high cost operation be-
cause it may occur at any location and can cause
lattice size to explode. To avoid this problem, we
permit insertion only immediately after nouns.
2.3 Features
In this paper, we use mapping features and link fea-
tures. The former measure the correspondence be-
tween input and output words (similar to the trans-
lation models of PBSMT). The latter measure the
fluency of the output word sequence (similar to lan-
guage models).
The mapping features are all binary. The focusing
phrase and its two surrounding words of the input
are regarded as the window. The mapping features
are defined as the pairs of the output phrase and 1-,
2-, and 3-grams in the window.
The link features are important for the error cor-
rection task because the system has to judge output
correctness. Fortunately, CRF, which is a kind of
discriminative model, can handle features that de-
pend on each other; we mix two types of features
as follows and optimize their weights in the CRF
framework.
? N -gram features: N -grams of the output words,
from 1 to 3, are used as binary features. These
are obtained from a training corpus (paired sen-
tences). Since the feature weights are optimized
considering the entire feature space, fine-tuning
can be achieved. The accuracy becomes almost
perfect on the training corpus.
? Language model probability: This is a logarith-
mic value (real value) of the n-gram probability
of the output word sequence. One feature weight
is assigned. The n-gram language model can be
389
constructed from a large sentence set because it
does not need the learner?s sentences.
Incorporating binary and real features yields a
rough approximation of generative models in semi-
supervised CRFs (Suzuki and Isozaki, 2008). It can
appropriately correct new sentences while maintain-
ing high accuracy on the training corpus.
3 Pseudo-error Sentences and Domain
Adaptation
The error corrector described in Section 2 requires
paired sentences. However, it is expensive to col-
lect them. We resolve this problem by using pseudo-
error sentences and domain adaptation.
3.1 Pseudo-Error Generation
Correct sentences, which are halves of the paired
sentences, can be easily acquired from corpora such
as newspaper articles. Pseudo-errors are generated
from them by the substitution, insertion, and dele-
tion functions according to the desired error pat-
terns.
We utilize the method of Rozovskaya and Roth
(2010b). Namely, when particles appear in the cor-
rect sentence, they are replaced by incorrect ones in
a probabilistic manner by applying the phrase table
(which stores the error patterns) in the opposite di-
rection. The error generation probabilities are rel-
ative frequencies on the training corpus. The mod-
els are learnt using both the training corpus and the
pseudo-error sentences.
3.2 Adaptation by Feature Augmentation
Although the error generation probabilities are com-
puted from the real-error corpus, the error distribu-
tion that results may be inappropriate. To better fit
the pseudo-errors to the real-errors, we apply a do-
main adaptation technique. Namely, we regard the
pseudo-error corpus as the source domain and the
real-error corpus as the target domain, and models
are learnt that fit the target domain.
In this paper, we use Daume (2007)?s feature aug-
mentation method for the domain adaptation, which
eliminates the need to change the learning algo-
rithm. This method regards the models for the
source domain as the prior distribution and learns
the models for the target domain.
Common Source TargetFeature Space
Ds Ds 0Source Data
Dt 0 DtTarget Data
Figure 2: Feature Augmentation
We briefly review feature augmentation. The fea-
ture space is segmented into three parts: common,
source, and target. The features extracted from the
source domain data are deployed to the common
and the source spaces, and those from the target do-
main data are deployed to the common and the target
spaces. Namely, the feature space is tripled (Figure
2).
The parameter estimation is carried out in the
usual way on the above feature space. Consequently,
the weights of the common features are emphasized
if the features are consistent between the source and
the target. With regard to domain dependent fea-
tures, the weights in the source or the target space
are emphasized.
Error correction uses only the features in the com-
mon and target spaces. The error distribution ap-
proaches that of the real-errors because the weights
of features are optimized to the target domain. In ad-
dition, it becomes robust against new sentences be-
cause the common features acquired from the source
domain can be used even when they do not appear in
the target domain.
4 Experiments
4.1 Experimental Settings
Real-error Corpus: We collected learner?s sen-
tences written by Chinese native speakers. The sen-
tences were created from English Linux manuals
and figures, and Japanese native speakers revised
them. From these sentences, only particle errors
were retained; the other errors were corrected. As
a result, we obtained 2,770 paired sentences. The
number of incorrect particles was 1,087 (8.0%) of
13,534. Note that most particles did not need to be
revised. The number of pair types of incorrect parti-
cles and their correct ones was 132.
Language Model: It was constructed from
Japanese Wikipedia articles about computers and
390
0.5
0.6
0.7
0.8
0.9
1
Precision Rate
TRGSRCALLAUG
0.3
0.4
0 0.05 0.1 0.15 0.2 0.25
Precision Rate
Recall Rate
Figure 3: Recall/Precision Curve (Error Generation Mag-
nification is 1.0)
Japanese Linux manuals, 527,151 sentences in total.
SRILM (Stolcke et al, 2011) was used to train a
trigram model.
Pseudo-error Corpus: The pseudo-errors were
generated using 10,000 sentences randomly selected
from the corpus for the language model. The mag-
nification of the error generation probabilities was
changed from 0.0 (i.e., no errors) to 2.0 (the relative
frequency in the real-error corpus was taken as 1.0).
Evaluation Metrics: Five-fold cross-validation
on the real-error corpus was used. We used two met-
rics: 1) Precision and recall rates of the error correc-
tion by the systems, and 2) Relative improvement,
the number of differences between improved and de-
graded particles in the output sentences (no changes
were ignored). This is a practical metric because it
denotes the number of particles that human rewriters
do not need to revise after the system correction.
4.2 Results
Figure 3 plots the precision/recall curves for the fol-
lowing four combinations of training corpora and
method.
? TRG: The models were trained using only the
real-error corpus (baseline).
? SRC: Trained using only the pseudo-error corpus.
? ALL: Trained using the real-error and pseudo-
error corpora by simply adding them.
? AUG:
The proposed method. The feature augmentation
was realized by regarding the pseudo-errors as the
-50
0
+50
+100
0.0 0.5 1.0 1.5 2.0 
Relative Improvement
-150
-100
Relative Improvement
Error Generation Probability(Magnification)
TRGSRCALLAUG
Figure 4: Relative Improvement among Error Generation
Probabilities
source domain and the real-errors as the target do-
main.
The SRC case, which uses only the pseudo-error
sentences, did not match the precision of TRG. The
ALL case matched the precision of TRG at high
recall rates. AUG, the proposed method, achieved
higher precision than TRG at high recall rates. At
the recall rate of 18%, the precision rate of AUGwas
55.4%; in contrast, that of TRG was 50.5%. Fea-
ture augmentation effectively leverages the pseudo-
errors for error correction.
Figure 4 shows the relative improvement of each
method according to the error generation probabil-
ities. In this experiment, ALL achieved higher im-
provement than TRG at error generation probabili-
ties ranging from 0.0 to 0.6. Although the improve-
ments were high, we have to control the error gen-
eration probability because the improvements in the
SRC case fell as the magnification was raised. On
the other hand, AUG achieved stable improvement
regardless of the error generation probability. We
can conclude that domain adaptation to the pseudo-
error sentences is the preferred approach.
5 Conclusions
This paper presented an error correction method of
Japanese particles that uses pseudo-error generation.
We applied domain adaptation in which the pseudo-
errors are regarded as the source domain and the
real-errors as the target domain. In our experiments,
domain adaptation achieved stable improvement in
system performance regardless of the error genera-
tion probability.
391
References
Hal Daume, III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics (ACL 2007),
pages 256?263, Prague, Czech Republic.
Michael Gamon. 2010. Using mostly native data to
correct errors in learners? writing. In Human Lan-
guage Technologies: The 2010 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL-HLT 2010), pages
163?171, Los Angeles, California.
Na-Rae Han, Joel Tetreault, Soo-Hwa Lee, and Jin-
Young Ha. 2010. Using an error-annotated learner
corpus to develop an ESL/EFL error correction sys-
tem. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta.
Kenji Imamura, Tomoko Izumi, Kugatsu Sadamitsu, Ku-
niko Saito, Satoshi Kobashikawa, and Hirokazu Masa-
taki. 2011. Morpheme conversion for connecting
speech recognizer and language analyzers in unseg-
mented languages. In Proceedings of Interspeech
2011, pages 1405?1408, Florence, Italy.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In
Proceedings of the 18th International Conference
on Machine Learning (ICML-2001), pages 282?289,
Williamstown, Massachusetts.
Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata,
and Yuji Matsumoto. 2011. Mining revision log of
language learning SNS for automated Japanese error
correction of second language learners. In Proceed-
ings of 5th International Joint Conference on Natural
Language Processing (IJCNLP 2011), pages 147?155,
Chiang Mai, Thailand.
Alla Rozovskaya and Dan Roth. 2010a. Generating
confusion sets for context-sensitive error correction.
In Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing (EMNLP
2010), pages 961?970, Cambridge, Massachusetts.
Alla Rozovskaya and Dan Roth. 2010b. Training
paradigms for correcting errors in grammar and usage.
In Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the As-
sociation for Computational Linguistics (NAACL-HLT
2010), pages 154?162, Los Angeles, California.
Alla Rozovskaya and Dan Roth. 2011. Algorithm se-
lection and model adaptation for ESL correction tasks.
In Proceedings of the 49th Annual Meeting of the As-
sociation for Computational Linguistics: Human Lan-
guage Techologies (ACL-HLT 2011), pages 924?933,
Portland, Oregon.
Andreas Stolcke, Jing Zheng, Wen Wang, and Victor
Abrash. 2011. SRILM at sixteen: Update and
outlook. In Proceedings of IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU
2011), Waikoloa, Hawaii.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-word
scale unlabeled data. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-08:
HLT), pages 665?673, Columbus, Ohio.
392
Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 78?84,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Pilot Study on Readability Prediction with Reading Time
Hitoshi Nishikawa, Toshiro Makino and Yoshihiro Matsuo
NTT Media Intelligence Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan
{
nishikawa.hitoshi
makino.toshiro, matsuo.yoshihiro
}
@lab.ntt.co.jp
Abstract
In this paper we report the results of a pi-
lot study of basing readability prediction
on training data annotated with reading
time. Although reading time is known to
be a good metric for predicting readabil-
ity, previous work has mainly focused on
annotating the training data with subjec-
tive readability scores usually on a 1 to
5 scale. Instead of the subjective assess-
ments of complexity, we use the more ob-
jective measure of reading time. We create
and evaluate a predictor using the binary
classification problem; the predictor iden-
tifies the better of two documents correctly
with 68.55% accuracy. We also report a
comparison of predictors based on reading
time and on readability scores.
1 Introduction
Several recent studies have attempted to predict
the readability of documents (Pitler and Nenkova,
2008; Burstein et al, 2010; Nenkova et al, 2010;
Pitler et al, 2010; Tanaka-Ishii et al, 2010). Pre-
dicting readability has a very important role in the
field of computational linguistics and natural lan-
guage processing:
? Readability prediction can help users retrieve
information from the Internet. If the read-
ability of documents can be predicted, search
engines can rank the documents according to
readability, allowing users to access the infor-
mation they need more easily (Tanaka-Ishii et
al., 2010).
? The predicted readability of a document can
be used as an objective function in natural
language applications such as machine trans-
lation, automatic summarization, and docu-
ment simplification. Machine translation can
use a readability predictor as a part of the ob-
jective function to make more fluent transla-
tions (Nenkova et al, 2010). The readabil-
ity predictor can also be used as a part of a
summarizer to generate readable summaries
(Pitler et al, 2010). Document simplification
can help readers understand documents more
easily by automatically rewriting documents
that are not easy to read (Zhu et al, 2010;
Woodsend and Lapata, 2011). This is pos-
sible by paraphrasing the sentences so as to
maximize document readability.
? Readability prediction can be used for educa-
tional purposes (Burstein et al, 2010). It can
assess human-generated documents automat-
ically.
Most studies build a predictor that outputs a
readability score (generally 1-5 scale) or a clas-
sifier or ranker that identifies which of two doc-
uments has the better readability. Using textual
complexity to rank documents may be adequate
for several applications in the fields of information
retrieval, machine translation, document simplifi-
cation, and the assessment of human-written doc-
uments. Approaches based on complexity, how-
ever, do not well support document summariza-
tion.
In the context of automatic summarization,
users want concise summaries to understand the
important information present in the documents as
rapidly as possible? to create summaries that can
be read as quickly as possible, we need a func-
tion that can evaluate the quality of the summary
in terms of reading time.
78
To achieve this goal, in this paper, we show the
results of our pilot study on predicting the reading
time of documents. Our predictor has two features
as follows:
1. Our predictor is trained by documents di-
rectly annotated with reading time. While
previous work employs subjective assess-
ments of complexity, we directly use the
reading time to build a predictor. As a pre-
dictor, we adopt Ranking SVM (Joachims,
2002).
2. The predictor predicts the reading time with-
out recourse to features related to document
length since our immediate goal is text sum-
marization. A preliminary experiment con-
firms that document length is effective for
readability prediction confirming the work
by (Pitler and Nenkova, 2008; Pitler et al,
2010). Summarization demands that the pre-
dictor work well regardless of text length.
This is the first report to show that the result of
training a predictor with data annotated by read-
ing time is to improve the quality of automatic
readability prediction. Furthermore, we report
the result of the comparison between our read-
ing time predictor and a conventional complexity-
based predictor.
This paper is organized as follows: Section 2
describes related work. Section 3 describes the
data used in the experiments. Section 4 describes
our model. Section 5 elaborates the features for
predicting document readability based on read-
ing time. Section 6 reports our evaluation exper-
iments. We conclude this paper and show future
directions in Section 7.
2 Related Work
Recent work formulates readability prediction as
an instance of a classification, regression, or rank-
ing problem. A document is regarded as a mix-
ture of complex features and its readability is pre-
dicted by the use of machine learning (Pitler and
Nenkova, 2008; Pitler et al, 2010; Tanaka-Ishii et
al., 2010). Pitler and Nenkova (2008) built a clas-
sifier that employs various features extracted from
a document and newswire documents annotated
with a readability score on a 1 to 5 scale. They in-
tegrated complex features by using SVM and iden-
tified the better document correctly with 88.88%
accuracy. They reported that the log likelihood of
a document based on its discourse relations, the
log likelihood of a document based on n-gram, the
average number of verb phrases in sentences, the
number of words in the document were good in-
dicators on which to base readability prediction.
Pitler et al, (2010) used the same framework to
predict the linguistic quality of a summary. In the
field of automatic summarization, linguistic qual-
ity has been assessed manually and hence to auto-
mate the assessment is an important research prob-
lem (Pitler et al, 2010). A ranker based on Rank-
ing SVM has been constructed (Joachims, 2002)
and identified the better of two summaries cor-
rectly with an accuracy of around 90%. Tanaka-
Ishii et al, (2010) also built a ranker to predict the
rank of documents according to readability. While
Tanaka-Ishii et al used word-level features for the
prediction, Pitler and Nenkova (2008) and Pitler
et al, (2010) also leveraged sentence-level fea-
tures and document-level features. In this paper,
we extend their findings to predict readability. We
elaborate our feature set in Section 5. While all
of them either classify or rank the documents by
assigning a readability score on a 1-5 scale, our
research goal is to build a predictor that can also
estimate the reading time.
In the context of multi-document summariza-
tion, the linguistic quality of a summary is pre-
dicted to order the sentences extracted from the
original documents (Barzilay and Lapata, 2005;
Lapata, 2006; Barzilay and Lapata, 2008). In
multi-document summarization, since sentences
are extracted from the original documents without
regard for context, they must be ordered in some
way to make the summary coherent. One of the
most important features for ordering sentences is
the entity grid suggested by Barzilay and Lapata
(2005; 2008). It captures transitions in the seman-
tic roles of the noun phrases in a document, and
can predict the quality of an order of the sentences
with high accuracy. It was also used as an im-
portant feature in the work by Pitler and Nenkova
(2008) and Piter et al, (2010) to predict the read-
ability of a document. Burstein et al, (2010) used
it for an educational purpose, and used it to predict
79
the readability of essays. Lapata (Lapata, 2006)
suggested the use of Kendall?s Tau as an indicator
of the quality of a set of sentences in particular or-
der; she also reported that self-paced reading time
is a good indicator of quality. While Lapata fo-
cuses on sentence ordering, our research goal is to
predict the overall quality of a document in terms
of reading time.
3 Data
To build a predictor that can estimate the read-
ing time of a document, we made a collection
of documents and annotated each with its read-
ing time and readability score. We randomly se-
lected 400 articles from Kyoto Text Corpus 4.0 1.
The corpus consists of newswire articles written
in Japanese and annotated with word boundaries,
part-of-speech tags and syntactic structures. We
developed an experimental system that showed ar-
ticles for each subject and gathered reading times.
Each article was read by 4 subjects. All subjects
are native speakers of Japanese.
Basically, we designed our experiment follow-
ing Pitler and Nenkova (2008). The subjects were
asked to use the system to read the articles. They
could read each document without a time limit, the
only requirement being that they were to under-
stand the content of the document. While the sub-
jects were reading the article, the reading time was
recorded by the system. We didn?t tell the subjects
that the time was being recorded.
To prevent the subjects from only partially read-
ing the document and raise the reliability of the re-
sults, we made a multiple-choice question for each
document; the answer was to be found in the doc-
ument. This was used to weed out unreliable re-
sults.
After the subjects read the document, they were
asked to answer the question.
Finally, the subjects were asked questions re-
lated to readability as follows:
1. How well-written is this article?
2. How easy was it to understand?
3. How interesting is this article?
Following the work by Pitler and Nenkova
(2008), the subjects answered by selecting a value
1http://nlp.ist.i.kyoto-u.ac.jp/EN/
between 1 and 5, with 5 being the best and 1 be-
ing the worst and we used only the answer to the
first question (How well-written is this article?) as
the readability score. We dropped the results in
which the subjects gave the wrong answer to the
multiple-choice question. Finally, we had 683 tu-
ples of documents, reading times, and readability
scores.
4 Model
To predict the readability of a document according
to reading time, we use Ranking SVM (Joachims,
2002). A target document is converted to a feature
vector as explained in Section 5, then the predictor
ranks two documents. The predictor assigns a real
number to a document as its score; ranking is done
according to score. In this paper, a higher score
means better readability, i.e., shorter reading time.
5 Features
In this section we elaborate the features used to
predict the reading time. While most of them were
introduced in previous work, see Section 3, the
word level features are introduced here.
5.1 Word-level Features
Character Type (CT)
Japanese sentences consist of several types of
characters: kanji, hiragana, katakana, and Roman
letters. We use the ratio of the number of kanji to
the number of hiragana as a feature of the docu-
ment.
Word Familiarity (WF)
Amano and Kondo (2007) developed a list of
words annotated with word familiarity; it indicates
how familiar a word is to Japanese native speakers.
The list is the result of a psycholinguistic experi-
ment and the familiarity ranges from 1 to 7, with
7 being the most familiar and 1 being the least fa-
miliar. We used the average familiarity of words
in the document as a feature.
5.2 Sentence-level Features
Language Likelihood (LL)
Language likelihood based on an n-gram language
model is widely used to generate natural sen-
tences. Intuitively, a sentence whose language
likelihood is high will have good readability. We
80
made a trigram language model from 17 years
(1991-2007) of Mainichi Shinbun Newspapers by
using SRILM Toolkit. Since the language model
assigns high probability to shorter documents, we
normalized the probability by the number of words
in a document.
Syntactic Complexity (TH/NB/NC/NP)
Schwarm and Ostendorf (2005) suggested that
syntactic complexity of a sentence can be used as
a feature for reading level assessment. We use the
following features as indicators of syntactic com-
plexity:
? The height of the syntax tree (TH): we use
the height of the syntax tree as an indicator of
the syntactic complexity of a sentence. Com-
plex syntactic structures demand that readers
make an effort to interpret them. We use the
average, maximum and minimum heights of
syntax trees in a document as a feature.
? The number of bunsetsu (NB): in Japanese
dependency parsing, syntactic relations are
defined between bunsetsu; they are almost
the same as Base-NP (Veenstra, 1998) with
postpositions. If a sentence has a lot of bun-
setsu, it can have a complex syntactic struc-
ture. We use the average, maximum and min-
imum number of them as a feature.
? The number of commas (NC): a comma sug-
gests a complex syntax structure such as sub-
ordinate and coordinate clauses. We use the
average, maximum and minimum number of
them as a feature.
? The number of predicates (NP): intuitively,
a sentence can be syntactically complex if it
has a lot of predicates. We use the average,
maximum and minimum number of them as
a feature.
5.3 Document-level Features
Discourse Relations (DR)
Pitler and Nenkova (2008) used discourse rela-
tions of the Penn Discourse Treebank (Prasad
et al, 2008) as a feature. Since our corpus
doesn?t have human-annotated discourse relations
between the sentences, we use the average num-
ber of connectives per sentence as a feature. In-
tuitively, the explicit discourse relations indicated
by the connectives will yield better readability.
Entity Grid (EG)
Along with the previous work (Pitler and
Nenkova, 2008; Pitler et al, 2010), we use entity
grid (Barzilay and Lapata, 2005; Barzilay and La-
pata, 2008) as a feature. We make a vector whose
element is the transition probability between syn-
tactic roles (i.e. subject, object and other) of the
noun phrases in a document. Since our corpus
consists of Japanese documents, we use postpo-
sitions to recognize the syntactic role of a noun
phrase. Noun phrases with postpositions ?Ha? and
?Ga? are recognized as subjects. Noun phrases
with postpositions ?Wo? and ?Ni? are recognized
as objects. Other noun phrases are marked as
other. We combine the entity grid vector to form a
final feature vector for predicting reading time.
Lexical Cohesion (LC)
Lexical cohesion is one of the strongest features
for predicting the linguistic quality of a summary
(Pitler et al, 2010). Following their work, we
leverage the cosine similarity of adjacent sen-
tences as a feature. To calculate it, we make
a word vector by extracting the content words
(nouns, verbs and adjectives) from a sentence. The
frequency of each word in the sentence is used as
the value of the sentence vector. We use the aver-
age, maximum and minimum cosine similarity of
the sentences as a feature.
6 Experiments
This section explains the setting of our experi-
ment. As mentioned above, we adopted Ranking
SVM as a predictor. Since we had 683 tuples (doc-
uments, reading time and readability scores), we
made 683C2 = 232, 903 pairs of documents for
Ranking SVM. Each pair consists of two docu-
ments where one has a shorter reading time than
the other. The predictor learned which parameters
were better at predicting which document would
have the shorter reading time, i.e. higher score.
We performed a 10-fold cross validation on the
pairs consisting of the reading time explained in
Section 3 and the features explained in Section 5.
In order to analyze the contribution of each feature
81
Features Accuracy
ALL 68.45
TH + EG + LC 68.55
Character Type (CT) 52.14
Word Familiarity (WF) 51.30
Language Likelihood (LL) 50.40
Height of Syntax Tree (TH) 61.86
Number of Bunsetsu (NB) 51.54
Number of Commas (NC) 47.07
Number of Predicates (NP) 52.82
Discourse Relations (DR) 48.04
Entity Grid (EG) 67.74
Lexical Cohesion (LC) 61.63
Document Length 69.40
Baseline 50.00
Table 1: Results of proposed reading time predic-
tor.
to prediction accuracy, we adopted a linear kernel.
The range of the value of each feature was normal-
ized to lie between -1 and 1.
6.1 Classification based on reading time
Table 1 shows the results yielded by the read-
ing time predictor. ALL indicates the accuracy
achieved by the classifier with all features ex-
plained in Section 5. At the bottom of Table 1,
Baseline shows the accuracy of random classifica-
tion. As shown in Table 1, since the height of syn-
tax tree, entity grid and lexical cohesion are good
indicators for the prediction, we combined these
features. TH + EG + LC indicates that this combi-
nation achieves the best performance.
As to individual features, most of them couldn?t
distinguish a better document from a worse one.
CT, WF and LL show similar performance to
Baseline. The reason why these features failed to
clearer identify the better of the pair could be be-
cause the documents are newswire articles. The
ratio between kanji and hiragana, CT, is similar in
most of the articles and hence it couldn?t identify
the better document. Similarly, there isn?t so much
of a difference among the documents in terms of
word familiarity, WF. The language model used,
LL, was not effective against the documents tested
but it is expected that it would useful if the target
documents came from different fields.
Among the syntactic complexity features, TH
offers the best performance. Since its learned
feature weight is negative, the result shows that
a higher syntax tree causes longer reading time.
While TH has shows good performance, NB, NC
and NP fail to offer any significant advantage. As
with the word-level features, there isn?t so much
of a difference among the documents in terms of
the values of these features. This is likely because
most of the newswire articles are written by ex-
perts for a restricted field.
Among the document-level features, EG and
LC show good performance. While Pitler and
Nenkova (2008) have shown that the discourse re-
lation feature is strongest at predicting the linguis-
tic quality of a document, DR shows poor perfor-
mance. Whereas they modeled the discourse rela-
tions by a multinomial distribution using human-
annotated labels, DR was simply the number of
connectives in the document. A more sophisti-
cated approach will be needed to model discourse.
EG and LC show the best prediction perfor-
mance of the single features, which agrees with
previous work (Pitler and Nenkova, 2008; Pitler
et al, 2010). While, as shown above, most of the
sentence-level features don?t have good discrimi-
native performance, EG and LC work well. Since
these features can work well in homogeneous doc-
uments like newswire articles, it is reasonable to
expect that they will also work well in heteroge-
neous documents from various domains.
We also show the classification result achieved
with document length. Piter and Nenkova (2008)
have shown that document length is a strong indi-
cator for readability prediction. We measure docu-
ment length by three criteria: the number of char-
acters, the number of words and the number of
sentences in the document. We used these values
as features and built a predictor. While the docu-
ment length has the strongest classification perfor-
mance, the predictor with TH + EG + LC shows
equivalent performance.
6.2 Classification based on readability score
We also report that the result of the classification
based on the readability score in Table 2. Along
with the result of the reading time, we tested
ALL and TH + EG + LC, and the single features.
While DR shows poor classification performance
in terms of reading time, it shows the best classi-
82
Features Accuracy
ALL 57.25
TH + DR + EG + LC 56.51
TH + EG + LC 56.50
Character Type (CT) 51.96
Word Familiarity (WF) 51.50
Language Likelihood (LL) 50.68
Height of Syntax Tree (TH) 55.77
Number of Bunsetsu (NB) 52.99
Number of Commas (NC) 51.50
Number of Predicates (NP) 52.56
Discourse Relations (DR) 58.14
Entity Grid (EG) 56.14
Lexical Cohesion (LC) 55.77
Document Length 56.83
Baseline 50.00
Table 2: A result of classification based on read-
ability score.
Cor. coef.
Reading Time 0.822
Readability Score 0.445
Table 3: Correlation coefficients of the reading
time and readability score between the subjects.
We calculated the coefficient for each pair of sub-
jects and then averaged them.
fication performance as regards readability score.
Hence we add the result of TH + DR + EG + LC.
It agrees with the findings showed by Pitler and
Nenkova (2008) in which they have shown dis-
course relation is the best feature for predicting the
readability score.
In general, the same features used for classifica-
tion based on the reading time work well for pre-
dicting the readability score. TH and EG, LC have
good prediction performance.
6.3 Variation in reading time vs. variation in
readability score
We show the correlation between the subjects in
terms of the variation in reading time and read-
ability score in Table 3. As shown, the reading
time shows much higher correlation (less varia-
tion) than the readability score. This agrees with
the findings shown by Lapata (2006) in which
the reading time is a better indicator for read-
ability prediction. Since the readability score
varies widely among the subjects, training be-
comes problematic with lowers predictor perfor-
mance.
The biggest difference between the prediction
of the reading time and readability score is the
effect of feature DR. One hypothesis that could
explain the difference is that the use of connec-
tives works as a strong sign that the document has
a good readability score?it doesn?t necessarily
imply that the document has good readability?
for the subjects. That is, the subjects perceived
the documents with more connectives as readable,
however, those connectives contribute to the read-
ing time. Of course, our feature about discourse
relations is just based on their usage frequency and
hence more precise modeling could improve per-
formance.
7 Conclusion and Future Work
This paper has described our pilot study of read-
ability prediction based on reading time. With au-
tomatic summarization in mind, we built a predic-
tor that can predict the reading time, and read-
ability, of a document. Our predictor identified
the better of two documents with 68.55% accuracy
without using features related to document length.
The following findings can be extracted from
the results described above:
? The time taken to read documents can be
predicted through existing machine learning
technique and the features extracted from
training data annotated with reading time
(Pitler and Nenkova, 2008; Pitler et al,
2010).
? As Lapata (2006) has shown, reading time is
a highly effective indicator of readability. In
our experiment, reading time showed good
agreement among the subjects and hence
more coherent prediction results can be ex-
pected.
Future work must proceed in many directions:
1. Measuring more precise reading time is one
important problem. One solution is to use an
eye tracker; it can measure the reading time
more accurately because it can capture when
83
the subject finishes reading a document. In
order to prepare the data used in this paper,
we set questions so as to identify and drop
unreliable data. The eye tracker could allevi-
ate this effort.
2. Testing the predictor in another domain is
necessary for creating practical applications.
We tested the predictor only in the domain
of newswire articles, as described earlier, and
different results might be recorded in do-
mains other than newswire articles.
3. Improving the accuracy of the predictor is
also important. There could be other fea-
tures associated with readability prediction.
We plan to explore other features.
4. Applying the predictor to natural language
generation tasks is particularly important. We
plan to integrate our predictor into a summa-
rizer and evaluate its performance.
References
Shigeaki Amano and Tadahisa Kondo. 2007. Reliabil-
ity of familiarity rating of ordinary japanese words
for different years and places. Behavior Research
Methods, 39(4):1008?1011.
Regina Barzilay and Mirella Lapata. 2005. Model-
ing local coherence: an entity-based approach. In
Proceedings of the 43rd Annual Meeting on Asso-
ciation for Computational Linguistics (ACL), pages
141?148.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Jill Burstein, Joel Tetreault, and Slava Andreyev. 2010.
Using entity-based features to model coherence in
student essays. In Proceedings of Human Lan-
guage Technologies: The 2010 Annual Conference
of the North American Chapter of the Association
for Computational Linguistics (NAACL-HLT), pages
681?684.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proceedings of the ACM
Conference on Knowledge Discovery and Data Min-
ing (KDD), pages 133?142.
Mirella Lapata. 2006. Automatic evaluation of infor-
mation ordering: Kendall?s tau. Computational Lin-
guistics, 32(4):471?484.
Ani Nenkova, Jieun Chae, Annie Louis, and Emily
Pitler. 2010. Structural features for predicting the
linguistic quality of text: Applications to machine
translation, automatic summarization and human-
authored text. In Emiel Krahmer and Theunem
Mariet, editors, Empirical Methods in Natural Lan-
guage Generation: Data-oriented Methods and Em-
pirical Evaluation, pages 222?241. Springer.
Emily Pitler and Ani Nenkova. 2008. Revisiting
readability: A unified framework for predicting text
quality. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 186?195.
Emily Pitler, Annie Louis, and Ani Nenkova. 2010.
Automatic evaluation of linguistic quality in multi-
document summarization. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 544?554.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-
sakaki, Livio Robaldo, Aravind Joshi, and Bonnie
Webber. 2008. The penn discourse treebank 2.0. In
Proceedings of the 6th International Conference on
Language Resources and Evaluation (LREC).
Sarah Schwarm and Mari Ostendorf. 2005. Reading
level assessment using support vector machines and
statistical language models. In Proceedings of the
43rd Annual Meeting of the Association for Compu-
tational Linguistics (ACL), pages 523?530.
Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-
ada. 2010. Sorting by readability. Computational
Linguistics, 36(2):203?227.
Jorn Veenstra. 1998. Fast np chunking using memory-
based learning techniques. In Proceedings of the
8th Belgian-Dutch Conference on Machine Learn-
ing (Benelearn), pages 71?78.
Kristian Woodsend and Mirella Lapata. 2011. Learn-
ing to simplify sentences with quasi-synchronous
grammar and integer programming. In Proceed-
ings of the 2011 Conference on Empirical Methods
in Natural Language Processing (EMNLP), pages
409?420.
Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.
2010. A monolingual tree-based translation model
for sentence simplification. In Proceedings of the
23rd International Conference on Computational
Linguistics (Coling 2010), pages 1353?1361.
84
