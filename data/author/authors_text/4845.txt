Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 321?328
Manchester, August 2008
Improving Statistical Machine Translation using
Lexicalized Rule Selection
Zhongjun He1,2 and Qun Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{zjhe,liuqun,sxlin}@ict.ac.cn
Abstract
This paper proposes a novel lexicalized ap-
proach for rule selection for syntax-based
statistical machine translation (SMT). We
build maximum entropy (MaxEnt) mod-
els which combine rich context informa-
tion for selecting translation rules dur-
ing decoding. We successfully integrate
the MaxEnt-based rule selection models
into the state-of-the-art syntax-based SMT
model. Experiments show that our lexical-
ized approach for rule selection achieves
statistically significant improvements over
the state-of-the-art SMT system.
1 Introduction
The syntax-based statistical machine translation
(SMT) models (Chiang, 2005; Liu et al, 2006;
Galley et al, 2006; Huang et al, 2006) use rules
with hierarchical structures as translation knowl-
edge, which can capture long-distance reorderings.
Generally, a translation rule consists of a left-hand-
side (LHS) 1and a right-hand-side (RHS). The
LHS and RHS can be words, phrases, or even syn-
tactic trees, depending on SMT models. Transla-
tion rules can be learned automatically from par-
allel corpus. Usually, an LHS may correspond to
multiple RHS?s in multiple rules. Therefore, in sta-
tistical machine translation, the rule selection task
is to select the correct RHS for an LHS during de-
coding.
The conventional approach for rule selection is
to use precomputed translation probabilities which
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1In this paper, we use LHS and source-side interchange-
ably (so are RHS and target-side).
are estimated from the training corpus, as well as a
n-gram language model which is trained on the tar-
get language. The limitation of this method is that
it ignores context information (especially on the
source-side) during decoding. Take the hierarchi-
cal model (Chiang, 2005) as an example. Consider
the following rules for Chinese-to-English transla-
tion 2:
(1) X ? ?? X
1
? X
2
, X
2
in X
1
?
(2) X ? ?? X
1
? X
2
, at X
1
?s X
2
?
(3) X ? ?? X
1
? X
2
, with X
2
of X
1
?
These rules have the same source-side, and all of
them can pattern-match all the following source
phrases:
(a) ?
in
[??
economic
??]
1
field
?
?s
[??]
2
cooperation
[cooperation]
2
in [the economic field]
1
(b) ?
at
[??]
1
today
?
?s
[??
meeting
?]
2
on
at [today]
1
?s [meeting]
2
(c) ?
with
[??]
1
people
?
?s
[??
support
?]
2
under
with [the support]
2
of [the people]
1
Given a source phrase, how does the decoder
know which rule is suitable? In fact, rule (1) and
rule (2) have different syntactic structures (the left
two trees of Figure 1). Thus rule (1) can be used
for translating noun phrase (a), and rule (2) can be
applied to prepositional phrase (b). The weakness
2In this paper, we use Chinese and English as the source
and target language, respectively.
321
NP
DNP
PP
?
X
1
?
X
2
X
2
of
X
1
PP
LCP
NP
?
X
1
?
X
2
at
X
1
?s
X
2
PP
LCP
NP
?
X
1
?
X
2
with
X
2
of
X
1
Figure 1: Syntactic structures of the same source-side in different rules.
of Chiang?s hierarchical model is that it cannot
distinguish different structures on the source-side.
The linguistically syntax-based models (Liu et al,
2006; Huang et al, 2006) can distinguish syntactic
structures by parsing source sentence. However,
as an LHS tree may correspond to different RHS
strings in different rules (the right two rules of Fig-
ure 1), these models also face the rule selection
problem during decoding.
In this paper, we propose a lexicalized approach
for rule selection for syntax-based statistical ma-
chine translation. We use the maximum entropy
approach to combine various context features, e.g.,
context words of rules, boundary words of phrases,
parts-of-speech (POS) information. Therefore, the
decoder can use rich context information to per-
form context-dependent rule selection. We build
a maximum entropy based rule selection (MaxEnt
RS) model for each ambiguous hierarchical LHS,
the LHS which contains nonterminals and corre-
sponds to multiple RHS?s in multiple rules. We
integrate the MaxEnt RS models into the state-of-
the-art hierarchical SMT system (Chiang, 2005).
Experiments show that the lexicalized rule se-
lection approach improves translation quality of
the state-of-the-art SMT system, and the improve-
ments are statistically significant.
2 Previous Work
2.1 The Selection Problem in SMT
Statistical machine translation systems usually
face the selection problem because of the one-to-
many correspondence between the source and tar-
get language. Recent researches showed that rich
context information can help SMT systems per-
form selection and improves translation quality.
The discriminative phrasal reordering models
(Xiong et al, 2006; Zens and Ney, 2006) pro-
vided a lexicalized method for phrase reordering.
In these models, LHS and RHS can be consid-
ered as phrases and reordering types, respectively.
Therefore the selection task is to select a reorder-
ing type for phrases. They use a MaxEnt model
to combine context features and distinguished two
kinds of reorderings between two adjacent phrases:
monotone or swap. However, our method is more
generic, we perform lexicalized rule selection for
syntax-based SMT models. In these models, the
rules with hierarchical structures can handle re-
orderings of non-adjacent phrases. Furthermore,
the rule selection can be considered as a multi-
class classification task, while the phrase reorder-
ing between two adjacent phrases is a two-class
classification task.
Recently, word sense disambiguation (WSD)
techniques improved the performance of SMT sys-
tems by helping the decoder perform lexical selec-
tion. Carpuat and Wu (2007b) integrated a WSD
system into a phrase-based SMT system, Pharaoh
(Koehn, 2004a). Furthermore, they extendedWSD
to phrase sense disambiguation (PSD) (Carpuat
and Wu, 2007a). Either the WSD or PSD system
combines rich context information to solve the am-
biguity problem for words or phrases. Their exper-
iments showed stable improvements of translation
quality. These are different from our work. On
one hand, they focus on solving the lexical am-
biguity problem, and use a WSD or PSD system
to predict translations for phrases which only con-
sist of words. However, we put emphasis on rule
selection, and predict translations for hierarchical
LHS?s which consist of both words and nontermi-
nals. On the other hand, they incorporated a WSD
or PSD system into a phrase-based SMT system
with a weak distortion model for phrase reorder-
ing. While we incorporate MaxEnt RS models
into the state-of-the-art syntax-based SMT system,
which captures phrase reordering by using a hier-
archical model.
322
Chan et al (2007) incorporated a WSD system
into the hierarchical SMT system, Hiero (Chi-
ang, 2005), and reported statistically significant
improvement. But they only focused on solving
ambiguity for terminals of translation rules, and
limited the length of terminals up to 2. Different
from their work, we consider a translation rule as a
whole, which contains both terminals and nonter-
minals. Moreover, they explored features for the
WSD system only on the source-side. While we
define context features for the MaxEnt RS models
on both the source-side and target-side.
2.2 The Hierarchical Model
The hierarchical model (Chiang, 2005; Chiang,
2007) is built on a weighted synchronous context-
free grammar (SCFG) . A SCFG rule has the fol-
lowing form:
X ? ??, ?,??(4)
where X is a nonterminal, ? is an LHS string con-
sists of terminals and nonterminals, ? is the trans-
lation of ?, ? defines a one-one correspondence
between nonterminals in ? and ?. For example,
(5) X ? ?????, economic development?
(6) X ? ? X
1
? X
2
? the X
2
of X
1
?
Rule (5) contains only terminals, which is simi-
lar to phrase-to-phrase translation in phrase-based
SMT models. Rule (6) contains both terminals
and nonterminals, which causes a reordering of
phrases. The hierarchical model uses the max-
imum likelihood method to estimate translation
probabilities for a phrase pair ??, ??, independent
of any other context information.
To perform translation, Chiang uses a log-linear
model (Och and Ney, 2002) to combine various
features. The weight of a derivation D is computed
by:
w(D) =
?
i
?
i
(D)
?
i(7)
where ?
i
(D) is a feature function and ?
i
is the fea-
ture weight of ?
i
(D). During decoding, the de-
coder searches the best derivation with the lowest
cost by applying SCFG rules. However, the rule
selections are independent of context information,
except the left neighboring n ? 1 target words for
computing n-gram language model.
3 Lexicalized Rule Selection
The rule selection task can be considered as a
multi-class classification task. For a source-side,
each corresponding target-side is a label. The max-
imum entropy approach (Berger et al, 1996) is
known to be well suited to solve the classification
problem. Therefore, we build a maximum entropy
based rule selection (MaxEnt RS) model for each
ambiguous hierarchical LHS. In this section, we
will describe how to build the MaxEnt RS mod-
els and how to integrate them into the hierarchical
SMT model.
3.1 The MaxEnt RS Model
Following (Chiang, 2005), we use ??, ?? to repre-
sent a SCFG rule extracted from the training cor-
pus, where ? and ? are source and target strings,
respectively. The nonterminals in ? and ? are rep-
resented by X
k
, where k is an index indicating
one-one correspondence between nonterminals in
source and target sides. Let us use f(X
k
) to rep-
resent the source text covered by X
k
, and e(X
k
)
to represent the translation of f(X
k
). Let C(?) be
the context information of source text matched by
?, and C(?) be the context information of target
text matched by ?. Under the MaxEnt model, we
have:
P
rs
(?|?, f(X
k
), e(X
k
)) =(8)
exp[
?
i
?
i
h
i
(C(?), C(?), f(X
k
), e(X
k
))]
?
?
?
exp[
?
i
?
i
h
i
(C(?
?
), C(?), f(X
k
), e(X
k
))]
where h
i
is a binary feature function, ?
i
is the fea-
ture weight of h
i
. The MaxEnt RS model com-
bines rich context information of grammar rules,
as well as information of the subphrases which
will be reduced to nonterminal X during decoding.
However, these information is ignored by Chiang?s
hierarchical model.
We design three kinds of features for a rule
??, ??:
? Lexical features, which are the words imme-
diately to the left and right of ?, and boundary
words of subphrase f(X
k
) and e(X
k
);
? Parts-of-speech (POS) features, which are
POS tags of the source words defined in lexi-
cal features.
? Length features, which are the length of sub-
phrases f(X
k
) and e(X
k
).
323
Side Type Name Description
W?
?1
The source word immediately to the left of ?
W?
+1
The source word immediately to the right of ?
WL
f(X
k
)
The first word of f(X
k
)
Lexical Features
WR
f(X
k
)
The last word of f(X
k
)
P?
?1
POS of W?
?1
P?
+1
POS of W?
+1
PL
f(X
k
)
POS of WL
f(X
k
)
POS Features
PR
f(X
k
)
POS of WR
f(X
k
)
Source-side
Length Feature LEN
f(X
k
)
Length of source subphrase f(X
k
)
WL
e(X
k
)
The first word of e(X
k
)Lexical Features
WR
e(X
k
)
The last word of e(X
k
)Target-side
Length Feature LEN
e(X
k
)
Length of target subphrase e(X
k
)
Table 1: Feature categories of the MaxEnt RS model.
Type Feature
W?
?1
=?? W?
+1
=b
Lexical Features WL
f(X
1
)
=?? WR
f(X
1
)
=?? WL
f(X
2
)
=?? WR
f(X
1
)
=??
WL
e(X
1
)
=economic WR
e(X
1
)
=field WL
e(X
2
)
=cooperation WR
f(X
1
)
=cooperation
P?
?1
=v W?
+1
=wjPOS Features
PL
f(X
1
)
=n PR
f(X
1
)
=n PL
f(X
2
)
=vn PR
f(X
2
)
=vn
Length Feature LEN
f(X
1
)
=2 LEN
f(X
2
)
=1 LEN
e(X
1
)
=2 LEN
e(X
2
)
=1
Table 2: Features of rule X ? ?? X
1
? X
2
, X
2
in the X
1
?.
??/v ?/p ??/n ??/n ?/ude ??/vn b/wj
strengthen
the
cooperation
in
the
economic
field
.
Figure 2: An training example for rule extraction.
Table 1 shows these features in detail.
These features can be easily gathered accord-
ing to Chinag?s rule extraction method (Chiang,
2005). We use an example for illustration. Fig-
ure 2 is a word-aligned training example with POS
tags on the source side. We can obtain a SCFG
rule:
(9) X ? ?? X
1
? X
2
, X
2
in the X
1
?
Where the source phrases covered by X
1
and X
2
are ??? ??? and ????, respectively. Table
2 shows features of this rule. Note that following
(Chiang, 2005), we limit the number of nontermi-
nals of a rule up to 2. Thus a rule may have 20
features at most.
After extracting features from the training cor-
pus, we use the toolkit implemented by Zhang
(2004) to train a MaxEnt RS model for each am-
biguous hierarchical LHS. We set iteration number
to 100 and Gaussian prior to 1.
3.2 Integrating the MaxEnt RS Models into
the SMT Model
We integrate the MaxEnt RS models into the SMT
model during the translation of each source sen-
tence. Thus the MaxEnt RS models can help the
decoder perform context-dependent rule selection
during decoding.
In (Chiang, 2005), the log-linear model com-
bines 8 features: the translation probabilities
P (?|?) and P (?|?), the lexical weights P
w
(?|?)
and P
w
(?|?), the language model, the word
penalty, the phrase penalty, and the glue rule
penalty. For integration, we add two new features:
? P
rs
(?|?, f(X
k
), e(X
k
)). This feature is
computed by the MaxEnt RS model, which
gives a probability that the model selecting a
target-side ? given an ambiguous source-side
?, considering context information.
? P
rsn
= exp(1). This feature is similar to
phrase penalty feature. In our experiments,
324
we find that some source-sides are not am-
biguous, and correspond to only one target-
side. However, if a source-side ?? is not am-
biguous, the first feature P
rs
will be set to 1.0.
In fact, these rules are not reliable since they
usually occur only once in the training corpus.
Therefore, we use this feature to reward the
ambiguous source-side. During decoding, if
an LHS has multiple translations, this feature
is set to exp(1), otherwise it is set to exp(0).
The advantage of our integration is that we need
not change the main decoding algorithm of a SMT
system. Furthermore, the weights of the new fea-
tures can be trained together with other features of
the translation model.
Chiang (2007) uses the CKY algorithm with a
cube pruning method for decoding. This method
can significantly reduce the search space by effi-
ciently computing the top-n items rather than all
possible items at a node, using the k-best Algo-
rithms of Huang and Chiang (2005) to speed up
the computation. In cube pruning, the translation
model is treated as the monotonic backbone of
the search space, while the language model score
is a non-monotonic cost that distorts the search
space (see (Huang and Chiang, 2005) for defini-
tion of monotonicity). Similarly, in the MaxEnt
RS model, source-side features form a monotonic
score while target-side features constitute a non-
monotonic cost that can be seen as part of the lan-
guage model.
For translating a source sentence F J
I
, the de-
coder adopts a bottom-up strategy. All derivations
are stored in a chart structure. Each cell c[i, j] of
the chart contains all partial derivations which cor-
respond to the source phrase f j
i
. For translating
a source-side span [i, j], we first select all possi-
ble rules from the rule table. Meanwhile, we can
obtain features of the MaxEnt RS models which
are defined on the source-side since they are fixed
before decoding. During decoding, for a source
phrase f j
i
, suppose the rule
X ? ?fk
i
X
1
f
j
t
, e
k
?
i
?
X
1
e
j
?
t
?
?(10)
is selected by the decoder, where i ? k < t ? j
and k + 1 < t, then we can gather features which
are defined on the target-side of the subphrase X
1
from the ancestor chart cell c[k + 1, t ? 1] since
the span [k + 1, t ? 1] has already been covered.
Then the new feature scores P
rs
and P
rsn
can be
computed. Therefore, the cost of the derivation can
be obtained. Finally, the decoding is completed
when the whole sentence is covered, and the best
derivation of the source sentence F J
I
is the item
with the lowest cost in cell c[I, J ].
4 Experiments
4.1 Corpus
We carry out experiments on two translation tasks
with different sizes and domains of the training
corpus.
? IWSLT-05: We use about 40,000 sentence
pairs from the BTEC corpus with 354k Chi-
nese words and 378k English words as our
training data. The English part is used to train
a trigram language model. We use IWSLT-04
test set as the development set and IWSLT-05
test set as the test set.
? NIST-03: We use the FBIS corpus as the
training corpus, which contains 239k sen-
tence pairs with 6.9M Chinese words and
8.9M English words. For this task, we train
two trigram language models on the English
part of the training corpus and the Xinhua
portion of the Gigaword corpus, respectively.
NIST-02 test set is used as the development
set and NIST-03 test set is used as the test set.
4.2 Training
To train the translation model, we first run
GIZA++ (Och and Ney, 2000) to obtain word
alignment in both translation directions. Then the
word alignment is refined by performing ?grow-
diag-final? method (Koehn et al, 2003). We use
the same method suggested in (Chiang, 2005) to
extract SCFG grammar rules. Meanwhile, we
gather context features for training the MaxEnt RS
models. The maximum initial phrase length is set
to 10 and the maximum rule length of the source-
side is set to 5.
We use SRI Language Modeling Toolkit (Stol-
cke, 2002) to train language models for both tasks.
We use minimum error rate training (Och, 2003) to
tune the feature weights for the log-linear model.
The translation quality is evaluated by BLEU
metric (Papineni et al, 2002), as calculated by
mteval-v11b.pl with case-insensitive matching of
n-grams, where n = 4.
4.3 Baseline
We reimplement the decoder of Hiero (Chiang,
2007) in C++, which is the state-of-the-art SMT
325
System IWSLT-05 NIST-03
Baseline 56.20 28.05
+ MaxEnt RS
SLex 56.51 28.26
PF 56.95 28.78
SLex+PF 56.99 28.89
SLex+PF+SLen 57.10 28.96
SLex+PF +SLen+TF 57.20 29.02
Table 3: BLEU-4 scores (case-insensitive) on IWSLT-05 task and NIST MT-03 task. SLex = Source-side
Lexical Features, PF = POS Features, SLen = Source-side Length Feature, TF = Target-side features.
system. During decoding, we set b = 100 to prune
grammar rules, ? = 10, b = 30 to prune X cells,
and ? = 10, b = 15 to prune S cells. For cube
pruning, we set the threshold ? = 1.0. See (Chi-
ang, 2007) for meanings of these pruning parame-
ters.
The baseline system uses precomputed phrase
translation probabilities and two trigram language
models to perform rule selection, independent of
any other context information. The results are
shown in the row Baseline of Table 3. For IWSLT-
05 task, the baseline system achieves a BLEU-4
score of 56.20. For NIST MT-03 task, the BLEU-
4 score is 28.05 .
4.4 Baseline + MaxEnt RS
As described in Section 3.2, we add two new fea-
tures to integrate the MaxEnt RS models into the
hierarchical model. To run the decoder, we share
the same pruning settings with the baseline system.
Table 3 shows the results.
Using all features defined in Section 3.1 to train
the MaxEnt RS models, for IWSLT-05 task, the
BLEU-4 score is 57.20, which achieves an abso-
lute improvement of 1.0 over the baseline. For
NIST-03 task, our system obtains a BLEU-4 score
of 29.02, with an absolute improvement of 0.97
over the baseline. Using Zhang?s significance
tester (Zhang et al, 2004) to perform paired boot-
strap sampling (Koehn, 2004b), both improve-
ments on the two tasks are statistically significant
at p < 0.05.
In order to explore the utility of the context fea-
tures, we train the MaxEnt RS models on different
feature sets. We find that POS features are the most
useful features since they can generalize over all
training examples. Moreover, length feature also
yields improvement. However, these features are
never used in the baseline.
NO. of NO. of NO. of
LHS H-LHS AH-LHS
NIST MT-03 163,097 148,671 95,424
Baseline 12,069 7,164 5,745
+MaxEnt RS
(All features) 12,655 10,306 9,259
Table 4: Number of possible source-sides of SCFG
rules for NIST-03 task and number of source-sides
of the best translation. H-LHS = Hierarchical
LHS, AH-LHS = Ambiguous hierarchical LHS.
5 Analysis
Table 4 shows the number of source-sides of
the SCFG rules for NIST-03 task. After extract-
ing grammar rules from the training corpus, there
are 163,097 source-sides match the test corpus,
91.15% are hierarchical LHS?s (H-LHS, the LHS
which contains nonterminals). For the hierarchi-
cal LHS?s, 64.18% are ambiguous (AH-LHS, the
H-LHS which has multiple translations). This in-
dicates that the decoder will face serious rule se-
lection problem during decoding. We also note the
number of the source-sides of the best translation
for the test corpus. For the baseline system, the
number of H-LHS only account for 59.36% of to-
tal LHS?s. However, by incorporating MaxEnt RS
models, that proportion increases to 81.44%, since
the number of AH-LHS increases. The reason is
that, we use the feature P
rsn
to reward ambiguous
hierarchical LHS?s. This has some advantages. On
one hand, H-LHS can capture phrase reorderings.
On the other hand, AH-LHS is more reliable than
non-ambiguous LHS, since most non-ambiguous
LHS?s occur only once in the training corpus.
In order to know how the MaxEnt RS models
improve the performance of the SMT system, we
326
study the best translation of Baseline and Base-
line+MaxEnt RS. We find that the MaxEnt RS
models improve translation quality in 2 ways.
5.1 Better Phrase reordering
Since the SCFG rules which contain nonterminals
can capture reordering of phrases, better rule se-
lection will produce better phrase reordering. For
example, the source sentence ?... [??????
??]
1
? [???? ???]
2
... ? is translated
as follows:
? Reference: ... the five permanent members of
the UN Security Council ...
? Baseline: ... the [United Nations Security
Council]
1
[five permanent members]
2
...
? +MaxEnt RS: ... [the five permanent
members]
2
of [the UN Security Council]
1
...
The source sentence is translated incorrectly by the
baseline system, which selects the rule
(11) X ? ? X
1
? X
2
, the X
1
X
2
?
and produces a monotone translation. In contrast,
by considering information of the subphrases X
1
and X
2
, the MaxEnt RS model chooses the rule
(12) X ? ? X
1
? X
2
, X
2
of X
1
?
and obtains a correct translation by swapping X
1
and X
2
on the target-side.
5.2 Better Lexical Translation
The MaxEnt RS models can also help the decoder
perform better lexical translation than the baseline.
This is because the SCFG rules contain terminals.
When the decoder selects a rule for a source-side,
it also determines the translations of the source ter-
minals. For example, the translations of the source
sentence ??????????? ?b? are
as follows:
? Reference I?m afraid this flight is full.
? Baseline: I?m afraid already booked for this
flight.
? +MaxEnt RS: I?m afraid this flight is full.
Here, the baseline translates the Chinese phrase
???? into ?booked? by using the rule:
(13) X ? ? X
1
??, X
1
booked?
The meaning is not fully expressed since the Chi-
nese word ??? is not translated. However, the
MaxEnt RS model obtains a correct translation by
using the rule:
(14) X ? ? X
1
??, X
1
full ?
However, we also find that some results pro-
duced by the MaxEnt RS models seem to decrease
the BLEU score. An interesting example is the
translation of the source sentence ??????
???:
? Reference1: What is the name of this street?
? Reference2: What is this street called?
? Baseline: What is the name of this street?
? +MaxEnt RS: What?s this street called?
In fact, both translations are correct. But the trans-
lation of the baseline fully matches Reference1.
Although the translation produced by the MaxEnt
RS model is almost the same as Reference2, as
the BLEU metric is based on n-gram matching,
the translation ?What?s? cannot match ?What is?
in Reference2. Therefore, the MaxEnt RS model
achieves a lower BLEU score.
6 Conclusion
In this paper, we propose a generic lexicalized ap-
proach for rule selection. We build maximum en-
tropy based rule selection models for each ambigu-
ous hierarchical source-side of translation rules.
The MaxEnt RS models combine rich context in-
formation, which can help the decoder perform
context-dependent rule selection during decod-
ing. We integrate the MaxEnt RS models into
the hierarchical SMT model by adding two new
features. Experiments show that the lexicalized
approach for rule selection achieves statistically
significant improvements over the state-of-the-art
syntax-based SMT system.
Furthermore, our approach not only can be used
for the formally syntax-based SMT systems, but
also can be applied to the linguistically syntax-
based SMT systems. For future work, we will ex-
plore more sophisticated features for the MaxEnt
RS models and integrate the models into the lin-
guistically syntax-based SMT systems.
327
Acknowledgements
We would like to show our special thanks to Hwee
Tou Ng, Liang Huang, Yajuan Lv and Yang Liu
for their valuable suggestions. We also appreciate
the anonymous reviewers for their detailed com-
ments and recommendations. This work was sup-
ported by the National Natural Science Foundation
of China (NO. 60573188 and 60736014), and the
High Technology Research and Development Pro-
gram of China (NO. 2006AA010108).
References
Berger, A. L., S. A. Della Pietra, and V. J. Della.
1996. A maximum entropy approach to natural lan-
guage processing. Computational Linguistics, page
22(1):39?72.
Carpuat, Marine and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense dis-
ambiguation for statistical machine translation. In
11th Conference on Theoretical and Methodological
Issues in Machine Translation, pages 43?52.
Carpuat, Marine and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of EMNLP-CoNLL 2007,
pages 61?72.
Chan, Yee Seng, Hwee Tou Ng, and David Chiang.
2007. Word sense disambiguation improves sta-
tistical machine translation. In Proceedings of the
45th Annual Meeting of the Association for Compu-
tational Linguistics, pages 33?40.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 263?270.
Chiang, David. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, pages 33(2):201?
228.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Pro-
ceedings of COLING-ACL 2006, pages 961?968.
Huang, Liang and David Chiang. 2005. Better k-
best parsing. In Proceedings of the 9th International
Workshop on Parsing Technologies.
Huang, Liang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Bi-
ennial Conference of the Association for Machine
Translation in the Americas.
Koehn, Philipp, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127?133.
Koehn, Philipp. 2004a. Pharaoh: a beam search de-
coder for phrase-based statistical machine translation
models. In Proceedings of the Sixth Conference of
the Association for Machine Translation in the Amer-
icas, pages 115?124.
Koehn, Philipp. 2004b. Statistical significance tests for
machine translation evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), pages 388?395.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 609?616.
Och, Franz Josef and Hermann Ney. 2000. Improved
statistical alignment models. In Proceedings of the
38th Annual Meeting of the Association for Compu-
tational Linguistics, pages 440?447.
Och, Franz Josef and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for sta-
tistical machine translation. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics, pages 295?302.
Och, Franz Josef. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Process-
ing, volume 2, pages 901?904.
Xiong, Deyi, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of the
44th Annual Meeting of the Association for Compu-
tational Linguistics, pages 521?528.
Zens, Richard and Hermann Ney. 2006. Discrimina-
tive reordering models for statistical machine trans-
lation. In Proceedings of the Workshop on Statistical
Machine Translation, pages 55?63.
Zhang, Ying, Stephan Vogel, and Alex Waibel. 2004.
Interpreting bleu/nist scores: How much improve-
ment do we need to have a better system? In Pro-
ceedings of the Fourth International Conference on
Language Resources and Evaluation, pages 2051?
2054.
Zhang, Le. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
328
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89?97,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Maximum Entropy based Rule Selection Model for
Syntax-based Statistical Machine Translation
Qun Liu1 and Zhongjun He1,2 and Yang Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{liuqun,zjhe,yliu,sxlin}@ict.ac.cn
Abstract
This paper proposes a novel maximum en-
tropy based rule selection (MERS) model
for syntax-based statistical machine transla-
tion (SMT). The MERS model combines lo-
cal contextual information around rules and
information of sub-trees covered by variables
in rules. Therefore, our model allows the de-
coder to perform context-dependent rule se-
lection during decoding. We incorporate the
MERS model into a state-of-the-art linguis-
tically syntax-based SMT model, the tree-
to-string alignment template model. Experi-
ments show that our approach achieves signif-
icant improvements over the baseline system.
1 Introduction
Syntax-based statistical machine translation (SMT)
models (Liu et al, 2006; Galley et al, 2006; Huang
et al, 2006) capture long distance reorderings by us-
ing rules with structural and linguistical information
as translation knowledge. Typically, a translation
rule consists of a source-side and a target-side. How-
ever, the source-side of a rule usually corresponds
to multiple target-sides in multiple rules. Therefore,
during decoding, the decoder should select a correct
target-side for a source-side. We call this rule selec-
tion.
Rule selection is of great importance to syntax-
based SMT systems. Comparing with word selec-
tion in word-based SMT and phrase selection in
phrase-based SMT, rule selection is more generic
and important. This is because that a rule not only
contains terminals (words or phrases), but also con-
NP
DNP
NP
X 1
DEG
NPB
NN
X 2
NN
NP
DNP
NP
X 1
DEG
NPB
NN
X 2
NN
X 1 X 2 levels X 2 standard of X 1
Figure 1: Example of translation rules
tains nonterminals and structural information. Ter-
minals indicate lexical translations, while nontermi-
nals and structural information can capture short or
long distance reorderings. See rules in Figure 1 for
illustration. These two rules share the same syntactic
tree on the source side. However, on the target side,
either the translations for terminals or the phrase re-
orderings for nonterminals are quite different. Dur-
ing decoding, when a rule is selected and applied to a
source text, both lexical translations (for terminals)
and reorderings (for nonterminals) are determined.
Therefore, rule selection affects both lexical transla-
tion and phrase reordering.
However, most of the current syntax-based sys-
tems ignore contextual information when they se-
lecting rules during decoding, especially the infor-
mation of sub-trees covered by nonterminals. For
example, the information of X 1 and X 2 is not
recorded when the rules in Figure 1 extracted from
the training examples in Figure 2. This makes the
decoder hardly distinguish the two rules. Intuitively,
information of sub-trees covered by nonterminals as
well as contextual information of rules are believed
89
NP
DNP
X 1 :NP DEG
NPB
X 2 :NN NN
NP
DNP
X 1 :NP DEG
NPB
X 2 :NN NN
industrial products manufacturing levels overall standard of the match
Figure 2: Training examples for rules in Figure 1
to be helpful for rule selection.
Recent research showed that contextual infor-
mation can help perform word or phrase selec-
tion. Carpuat and Wu (2007b) and Chan et
al. (2007) showed improvents by integrating word-
sense-disambiguation (WSD) system into a phrase-
based (Koehn, 2004) and a hierarchical phrase-
based (Chiang, 2005) SMT system, respectively.
Similar to WSD, Carpuat and Wu (2007a) used con-
textual information to solve the ambiguity prob-
lem for phrases. They integrated a phrase-sense-
disambiguation (PSD) model into a phrase-based
SMT system and achieved improvements.
In this paper, we propose a novel solution for
rule selection for syntax-based SMT. We use the
maximum entropy approach to combine rich con-
textual information around a rule and the informa-
tion of sub-trees covered by nonterminals in a rule.
For each ambiguous source-side of translation rules,
a maximum entropy based rule selection (MERS)
model is built. Thus the MERS models can help the
decoder to perform a context-dependent rule selec-
tion.
Comparing with WSD (or PSD), there are some
advantages of our approach:
? Our approach resolves ambiguity for rules with
multi-level syntactic structure, while WSD re-
solves ambiguity for strings that have no struc-
tures;
? Our approach can help the decoder perform
both lexical selection and phrase reorderings,
while WSD can help the decoder only perform
lexical selection;
? Our method takes WSD as a special case, since
a rule may only consists of terminals.
In our previous work (He et al, 2008), we re-
ported improvements by integrating a MERS model
into a formally syntax-based SMT model, the hier-
archical phrase-based model (Chiang, 2005). In this
paper, we incorporate the MERS model into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment template (TAT) model
(Liu et al, 2006). The basic differences are:
? The MERS model here combines rich informa-
tion of source syntactic tree as features since
the translation model is linguistically syntax-
based. He et al (2008) did not use this in-
formation.
? In this paper, we build MERS models for all
ambiguous source-sides, including lexicalized
(source-side which only contains terminals),
partially lexicalized (source-side which con-
tains both terminals and nonterminals), and un-
lexicalized (source-side which only contains
nonterminals). He et al (2008) only built
MERS models for partially lexicalized source-
sides.
In the TAT model, a TAT can be considered as a
translation rule which describes correspondence be-
tween source syntactic tree and target string. TAT
can capture linguistically motivated reorderings at
short or long distance. Experiments show that by
incorporating MERS model, the baseline system
achieves statistically significant improvement.
This paper is organized as follows: Section 2
reviews the TAT model; Section 3 introduces the
MERS model and describes feature definitions; Sec-
tion 4 demonstrates a method to incorporate the
MERS model into the translation model; Section 5
reports and analyzes experimental results; Section 6
gives conclusions.
2 Baseline System
Our baseline system is Lynx (Liu et al, 2006),
which is a linguistically syntax-based SMT system.
For translating a source sentence fJ1 = f1...fj ...fJ ,
Lynx firstly employs a parser to produce a source
syntactic tree T (fJ1 ), and then uses the source
syntactic tree as the input to search translations:
90
e?I1 = argmaxe?I1Pr(e
I
1|f
J
1 )(1)
= argmaxe?I1Pr(T (f
J
1 )|f
J
1 )Pr(e
I
1|T (f
J
1 ))
In doing this, Lynx uses tree-to-string alignment
template to build relationship between source syn-
tactic tree and target string. A TAT is actually a
translation rule: the source-side is a parser tree with
leaves consisting of words and nonterminals, the
target-side is a target string consisting of words and
nonterminals.
TAT can be learned from word-aligned, source-
parsed parallel corpus. Figure 4 shows three types
of TATs extracted from the training example in Fig-
ure 3: lexicalized (the left), partially lexicalized
(the middle), unlexicalized (the right). Lexicalized
TAT contains only terminals, which is similar to
phrase-to-phrase translation in phrase-based model
except that it is constrained by a syntactic tree on the
source-side. Partially lexicalized TAT contains both
terminals and non-terminals, which can be used for
both lexical translation and phrase reordering. Un-
lexicalized TAT contains only nonterminals and can
only be used for phrase reordering.
Lynx builds translation model in a log-linear
framework (Och and Ney, 2002):
P (eI1|T (f
J
1 )) =(2)
exp[
?
m ?mhm(e
I
1, T (f
J
1 ))]?
e? exp[
?
m ?mhm(e
I
1, T (f
J
1 ))]
Following features are used:
? Translation probabilities: P (e?|T? ) and P (T? |e?);
? Lexical weights: Pw(e?|T? ) and Pw(T? |e?);
? TAT penalty: exp(1), which is analogous to
phrase penalty in phrase-based model;
? Language model Plm(eI1);
? Word penalty I .
In Lynx, rule selection mainly depends on trans-
lation probabilities and lexical weights. These four
scores describe how well a source tree links to a tar-
get string, which are estimated on the training cor-
pus according to occurrence times of e? and T? . There
IP
NPB
NN NN NN
VP
VV VPB
VV
The incomes of city and village resident continued to grow
Figure 3: Word-aligned, source-parsed training example.
NN NPB
NN
X 1
NN NN
NPB
NN
X 1
NN
X 2
NN
X 3
city and village incomes of X 1 resident X 3 X 1 X 2
Figure 4: TATs learned from the training example in Fig-
ure 3.
are no features in Lynx that can capture contextual
information during decoding, except for the n-gram
language model which considers the left and right
neighboring n-1 target words. But this information
it very limited.
3 The Maximum Entropy based Rule
Selection Model
3.1 The model
In this paper, we focus on using contextual infor-
mation to help the TAT model perform context-
dependent rule selection. We consider the rule se-
lection task as a multi-class classification task: for
a source syntactic tree T? , each corresponding target
string e? is a label. Thus during decoding, when a
TAT ?T? , e??? is selected, T? is classified into label e??,
actually.
A good way to solve the classification problem is
the maximum entropy approach:
Prs(e?|T? , T (Xk)) =(3)
exp[
?
i ?ihi(e?, C(T? ), T (Xk))]
?
e??
exp[
?
i ?ihi(e??, C(T? ), T (Xk))]
91
where T? and e? are the source tree and target string of
a TAT, respectively. hi is a binary feature functions
and ?i is the feature weight of hi. C(T? ) defines local
contextual information of T? . Xk is a nonterminal in
the source tree T? , where k is an index. T (Xk) is the
source sub-tree covered by Xk.
The advantage of the MERS model is that it uses
rich contextual information to compute posterior
probability for e? given T? . However, the transla-
tion probabilities and lexical weights in Lynx ignore
these information.
Note that for each ambiguous source tree, we
build a MERS model. That means, if there are
N source trees extracted from the training corpus
are ambiguous (the source tree which corresponds
to multiple translations), thus for each ambiguous
source tree Ti (i = 1, ..., N ), a MERS model Mi
(i = 1, ..., N ) is built. Since a source tree may cor-
respond to several hundreds of target translations at
most, the feature space of a MERS model is not pro-
hibitively large. Thus the complexity for training a
MERS model is low.
3.2 Feature Definition
Let ?T? , e?? be a translation rule in the TAT model.
We use f(T? ) to represent the source phrase covered
by T? . To build a MERS model for the source tree T? ,
we explore various features listed below.
1. Lexical Features (LF)
These features are defined on source words.
Specifically, there are two kinds of lexical fea-
tures: external features f?1 and f+1, which
are the source words immediately to the left
and right of f(T? ), respectively; internal fea-
tures fL(T (Xk)) and fR(T (Xk)), which are
the left most and right most boundary words of
the source phrase covered by T (Xk), respec-
tively.
See Figure 5 (a) for illustration. In
this example, f?1=t??ga?o, f+1=zh?`za`o,
fL(T (X1))=go?ngye`, fR(T (X1))=cha?np??n.
2. Parts-of-speech (POS) Features (POSF)
These features are the POS tags of the source
words defined in the lexical features: P?1,
P+1, PL(T (Xk)), PR(T (Xk)) are the POS
tags of f?1, f+1, fL(T (Xk)), fR(T (Xk)), re-
VP
VV
t??ga?o
DNP
X 1 :NP
NN
go?ngye`
NN
cha?np??n
DEG
de
NPB
NN
zh?`za`o
(a) Lexical Features
VP
VV
t??ga?o
DNP
X 1 :NP
NN
go?ngye`
NN
cha?np??n
DEG
de
NPB
NN
zh?`za`o
(b) POS Features
DNP
X 1 :NP
2 words
DEG
de
NP
DNP
X 1 :NP DEG
de
(c) Span Feature (d) Parent Feature
NP
DNP
X 1 :NP DEG
de
NPB
(e) Sibling Feature
Figure 5: Illustration of features of theMERSmodel. The
source tree of the TAT is ? DNP(NP X 1 ) (DEG de)?.
Gray nodes denote information included in the feature.
92
spectively. POS tags can generalize over all
training examples.
Figure 5 (b) shows POS features. P?1=VV,
P+1=NN, PL(T (X1))=NN, PR(T (X1))=NN.
3. Span Features (SPF)
These features are the length of the source
phrase f(T (Xk)) covered by T (Xk). In Liu?s
TATmodel, the knowledge learned from a short
span can be used for a larger span. This is not
reliable. Thus we use span features to allow the
MERS model to learn a preference for short or
large span.
In Figure 5 (c), the span of X 1 is 2.
4. Parent Feature (PF)
The parent node of T? in the parser tree of the
source sentence. The same source sub-tree may
have different parent nodes in different training
examples. Therefore, this feature may provide
information for distinguishing source sub-trees.
Figure 5 (d) shows that the parent is a NP node.
5. Sibling Features (SBF)
The siblings of the root of T? . This feature con-
siders neighboring nodes which share the same
parent node.
In Figure 5 (e), the source tree has one sibling
node NPB.
Those features make use of rich information
around a rule, including the contextual information
of a rule and the information of sub-trees covered
by nonterminals. They are never used in Liu?s TAT
model.
Figure 5 shows features for a partially lexicalized
source tree. Furthermore, we also build MERS mod-
els for lexicalized and unlexicalized source trees.
Note that for lexicalized tree, features do not include
the information of sub-trees since there is no nonter-
minals.
The features can be easily obtained by modify-
ing the TAT extraction algorithm described in (Liu
et al, 2006). When a TAT is extracted from a
word-aligned, source-parsed parallel sentence, we
just record the contextual features and the features of
the sub-trees. Then we use the toolkit implemented
by Zhang (2004) to train MERS models for the am-
biguous source syntactic trees separately. We set the
iteration number to 100 and Gaussian prior to 1.
4 Integrating the MERS Models into the
Translation Model
We integrate the MERS models into the TAT model
during the translation of each source sentence. Thus
the MERS models can help the decoder perform
context-dependent rule selection during decoding.
For integration, we add two new features into the
log-linear translation model:
? Prs(e?|T? , T (Xk)). This feature is computed by
the MERS model according to equation (3),
which gives a probability that the model select-
ing a target-side e? given an ambiguous source-
side T? , considering rich contextual informa-
tion.
? Pap = exp(1). During decoding, if a source
tree has multiple translations, this feature is set
to exp(1), otherwise it is set to exp(0). Since
the MERS models are only built for ambiguous
source trees, the first feature Prs(e?|T? , T (Xk))
for non-ambiguous source tree will be set to
1.0. Therefore, the decoder will prefer to
use non-ambiguous TATs. However, non-
ambiguous TATs usually occur only once in the
training corpus, which are not reliable. Thus
we use this feature to reward ambiguous TATs.
The advantage of our integration is that we need
not change the main decoding algorithm of Lynx.
Furthermore, the weights of the new features can be
trained together with other features of the translation
model.
5 Experiments
5.1 Corpus
We carry out experiments on Chinese-to-English
translation. The training corpus is the FBIS cor-
pus, which contains 239k sentence pairs with 6.9M
Chinese words and 8.9M English words. For the
language model, we use SRI Language Modeling
Toolkit (Stolcke, 2002) with modified Kneser-Ney
smoothing (Chen and Goodman, 1998) to train two
tri-gram language models on the English portion of
93
No. of No. of No. of ambiguous
Type
TATs source trees source trees
% ambiguous
Lexicalized 333,077 16,367 14,380 87.86
Partially Lexicalized 342,767 38,497 28,397 73.76
Unlexicalized 83,024 7,384 5,991 81.13
Total 758,868 62,248 48,768 78.34
Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.
System
Features
P (e?|T? ) P (T? |e?) Pw(e?|T? ) Pw(T? |e?) lm1 lm2 TP WP Prs AP
Lynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - -
+MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.207
Table 2: Feature weights obtained by minimum error rate training on the development set. The first 8 features are used
by Lynx. TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty. Note that in fact, the positive weight for
WP and AP indicate a reward.
the training corpus and the Xinhua portion of the Gi-
gaword corpus, respectively. NIST MT 2002 test set
is used as the development set. NIST MT 2003 and
NIST MT 2005 test sets are used as the test sets.
The translation quality is evaluated by BLEU met-
ric (Papineni et al, 2002), as calculated by mteval-
v11b.pl with case-insensitive matching of n-grams,
where n = 4.
5.2 Training
To train the translation model, we first run GIZA++
(Och and Ney, 2000) to obtain word alignment in
both translation directions. Then the word alignment
is refined by performing ?grow-diag-final? method
(Koehn et al, 2003). We use a Chinese parser de-
veloped by Deyi Xiong (Xiong et al, 2005) to parse
the Chinese sentences of the training corpus.
Our TAT extraction algorithm is similar to Liu et
al. (2006), except that we make some tiny modifica-
tions to extract contextual features for MERS mod-
els. To extract TAT, we set the maximum height of
the source sub-tree to h = 3, the maximum number
of direct descendants of a node of sub-tree to c = 5.
See (Liu et al, 2006) for specific definitions of these
parameters.
Table 1 shows statistical information of TATs
which are filtered by the two test sets. For each type
(lexicalized, partially lexicalized, unlexicalized) of
TATs, a great portion of the source trees are am-
biguous. The number of ambiguous source trees ac-
counts for 78.34% of the total source trees. This in-
dicates that the TAT model faces serious rule selec-
tion problem during decoding.
5.3 Results
We use Lynx as the baseline system. Then the
MERS models are incorporated into Lynx, and
the system is called Lynx+MERS. To run the
decoder, Lynx and Lynx+MERS share the same
settings: tatTable-limit=30, tatTable-threshold=0,
stack-limit=100, stack-threshold=0.00001. The
meanings of the pruning parameters are the same to
Liu et al (2006).
We perform minimum error rate training (Och,
2003) to tune the feature weights for the log-linear
model to maximize the systems?s BLEU score on the
development set. The weights are shown in Table 2.
These weights are then used to run Lynx and
Lynx+MERS on the test sets. Table 3 shows the
results. Lynx obtains BLEU scores of 26.15 on
NIST03 and 26.09 on NIST05. Using all features
described in Section 3.2, Lynx+MERS finally ob-
tains BLEU scores of 27.05 on NIST03 and 27.28
on NIST05. The absolute improvements is 0.90
and 1.19, respectively. Using the sign-test described
by Collins et al (2005), both improvements are
statistically significant at p < 0.01. Moreover,
Lynx+MERS also achieves higher n-gram preci-
sions than Lynx.
94
Test Set System BLEU-4
Individual n-gram precisions
1 2 3 4
NIST03
Lynx 26.15 71.62 35.64 18.64 9.82
+MERS 27.05 72.00 36.72 19.51 10.37
NIST05
Lynx 26.09 70.39 35.12 18.53 10.11
+MERS 27.28 71.16 36.19 19.62 10.95
Table 3: BLEU-4 scores (case-insensitive) on the test sets.
5.4 Analysis
The baseline system only uses four features for
rule selection: the translation probabilities P (e?|T? )
and P (T? |e?); and the lexical weights Pw(e?|T? ) and
Pw(T? |e?). These features are estimated on the train-
ing corpus by the maximum likelihood approach,
which does not allow the decoder to perform a con-
text dependent rule selection. Although Lynx uses
language model as feature, the n-gram language
model only considers the left and right n-1 neigh-
boring target words.
The MERS models combines rich contextual in-
formation as features to help the decoder perform
rule selection. Table 4 shows the effect of different
feature sets. We test two classes of feature sets: the
single feature (the top four rows of Table 4) and the
combination of features (the bottom five rows of Ta-
ble 4). For the single feature set, the POS tags are
the most useful and stable features. Using this fea-
ture, Lynx+MERS achieves improvements on both
the test sets. The reason is that POS tags can be gen-
eralized over all training examples, which can alle-
viate the data sparseness problem.
Although we find that some single features may
hurt the BLEU score, they are useful in combina-
tion of features. This is because one of the strengths
of the maximum entropy model is that it can in-
corporate various features to perform classification.
Therefore, using all features defined in Section 3.2,
we obtain statistically significant improvements (the
last row of Table 4). In order to know how the
MERS models improve translation quality, we in-
spect the 1-best outputs of Lynx and Lynx+MERS.
We find that the first way that theMERSmodels help
the decoder is that they can perform better selection
for words or phrases, similar to the effect of WSD
or PSD. This is because that lexicalized and partially
lexicalized TAT contains terminals. Considering the
Feature Sets NIST03 NIST05
LF 26.12 26.32
POSF 26.36 26.21
PF 26.17 25.90
SBF 26.47 26.08
LF+POSF 26.61 26.59
LF+POSF+SPF 26.70 26.44
LF+POSF+PF 26.81 26.56
LF+POSF+SBF 26.68 26.89
LF+POSF+SPF+PF+SBF 27.05 27.28
Table 4: BLEU-4 scores on different feature sets.
following examples:
? Source:
? Reference: Malta is located in southern Eu-
rope
? Lynx: Malta in southern Europe
? Lynx+MERS: Malta is located in southern Eu-
rope
Here the Chinese word ? ? is incor-
rectly translated into ?in? by the baseline system.
Lynx+MERS produces the correct translation ?is lo-
cated in?. That is because, the MERS model consid-
ers more contextual information for rule selection.
In the MERS model, Prs(in| ) = 0.09, which is
smaller than Prs(is located in| ) = 0.14. There-
fore, the MERS model prefers the translation ?is lo-
cated in?. Note that here the source tree (VV )
is lexicalized, and the role of the MERS model is
actually the same as WSD.
The second way that the MERS models help the
decoder is that they can perform better phrase re-
orderings. Considering the following examples:
95
? Source: [ ]1 [ ]2
...
? Reference: According to its [development
strategy]2 [in the Chinese market]1 ...
? Lynx: Accordance with [the Chinese market]1
[development strategy]2 ...
? Lynx+MERS: According to the [development
strategy]2 [in the Chinese market]1
The syntactic tree of the Chinese phrase ?
? is shown in Figure 6. How-
ever, there are two TATs which can be applied to the
source tree, as shown in Figure 7. The baseline sys-
tem selects the left TAT and produces a monotone
translation of the subtrees ?X 1 :PP? and ?X 2 :NPB?.
However, Lynx+MERS uses the right TAT and per-
forms correct phrase reordering by swapping the two
source phrases. Here the source tree is partially lex-
icalized, and both the contextual information and
the information of sub-trees covered by nontermi-
nals are considered by the MERS model.
6 Conclusion
In this paper, we propose a maximum entropy based
rule selection model for syntax-based SMT. We
use two kinds information as features: the local-
contextual information of a rule, the information of
sub-trees matched by nonterminals in a rule. During
decoding, these features allow the decoder to per-
form a context-dependent rule selection. However,
this information is never used in most of the current
syntax-based SMT models.
The advantage of the MERS model is that it can
help the decoder not only perform lexical selection,
but also phrase reorderings. We demonstrate one
way to incorporate the MERS models into a state-
of-the-art linguistically syntax-based SMT model,
the tree-to-string alignment model. Experiments
show that by incorporating the MERS models, the
baseline system achieves statistically significant im-
provements.
We find that rich contextual information can im-
prove translation quality for a syntax-based SMT
system. In future, we will explore more sophisti-
cated features for the MERS model. Moreover, we
will test the performance of the MERS model on
large scale corpus.
NP
DNP
PP DEG
NPB
in Chinese market of
development strategy
Figure 6: Syntactic tree of the source phrase ?
?.
NP
DNP
PP
X 1
DEG
NPB
X 2
NP
DNP
PP
X 1
DEG
NPB
X 2
X 1 X 2 X 2 X 1
Figure 7: TATs which can be used for the source phrase
? ?.
Acknowledgements
We would like to thank Yajuan Lv for her valuable
suggestions. This work was supported by the Na-
tional Natural Science Foundation of China (NO.
60573188 and 60736014), and the High Technology
Research and Development Program of China (NO.
2006AA010108).
References
Marine Carpuat and Dekai Wu. 2007a. How phrase
sense disambiguation outperforms word sense disam-
biguation for statistical machine translation. In 11th
Conference on Theoretical and Methodological Issues
in Machine Translation, pages 43?52.
Marine Carpuat and Dekai Wu. 2007b. Improving sta-
tistical machine translation using word sense disam-
biguation. In Proceedings of EMNLP-CoNLL 2007,
pages 61?72.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proceedings of the 45th Annual
96
Meeting of the Association for Computational Linguis-
tics, pages 33?40.
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical Report TR-10-98, Harvard University
Center for Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting of the Association for Com-
putational Linguistics, pages 263?270.
M. Collins, P. Koehn, and I. Kucerova. 2005. Clause re-
structuring for statistical machine translation. In Proc.
of ACL05, pages 531?540.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING-ACL 2006, pages 961?968.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics
(Coling 2008), pages 321?328.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
Statistical syntax-directed translation with extended
domain of locality. In Proceedings of the 7th Biennial
Conference of the Association for Machine Translation
in the Americas.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings of
HLT-NAACL 2003, pages 127?133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 44th Annual Meeting of
the Association for Computational Linguistics, pages
609?616.
Franz Josef Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics, pages 440?447.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Computa-
tional Linguistics, pages 160?167.
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. 2002.
Bleu: a method for automatic evaluation of machine
translation. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguistics,
pages 311?318.
Andreas Stolcke. 2002. Srilm ? an extensible lan-
guage modeling toolkit. In Proceedings of the Inter-
national Conference on Spoken language Processing,
volume 2, pages 901?904.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and
Yueliang Qian. 2005. Parsing the penn chinese tree-
bank with semantic knowledge. In Proceedings of
IJCNLP 2005, pages 70?81.
Le Zhang. 2004. Maximum entropy model-
ing toolkit for python and c++. available at
http://homepages.inf.ed.ac.uk/s0450736/maxent too-
lkit.html.
97
Parsing the Penn Chinese Treebank
with Semantic Knowledge
Deyi Xiong1,2, Shuanglong Li1,3,
Qun Liu1, Shouxun Lin1, and Yueliang Qian1
1 Institute of Computing Technology, Chinese Academy of Sciences,
P.O. Box 2704, Beijing 100080, China
{dyxiong, liuqun, sxlin}@ict.ac.cn
2 Graduate School of Chinese Academy of Sciences
3 University of Science and Technology Beijing
Abstract. We build a class-based selection preference sub-model to in-
corporate external semantic knowledge from two Chinese electronic se-
mantic dictionaries. This sub-model is combined with modifier-head gen-
eration sub-model. After being optimized on the held out data by the
EM algorithm, our improved parser achieves 79.4% (F1 measure), as well
as a 4.4% relative decrease in error rate on the Penn Chinese Treebank
(CTB). Further analysis of performance improvement indicates that se-
mantic knowledge is helpful for nominal compounds, coordination, and
NV tagging disambiguation, as well as alleviating the sparseness of in-
formation available in treebank.
1 Introduction
In the recent development of full parsing technology, semantic knowledge is sel-
dom used, though it is known to be useful for resolving syntactic ambiguities.
The reasons for this may be twofold. The first one is that it can be very difficult
to add additional features which are not available in treebanks to generative
models like Collins (see [1]), which are very popular for full parsing. For smaller
tasks, like prepositional phrase attachment disambiguation, semantic knowledge
can be incorporated flexibly using different learning algorithms (see [2,3,4,5]).
For full parsing with generative models, however, incorporating semantic knowl-
edge may involve great changes of model structures. The second reason is that
semantic knowledge from external dictionaries seems to be noisy, ambiguous and
not available in explicit forms, compared with the information from treebanks.
Given these two reasons, it seems to be difficult to combine the two different
information sources?treebank and semantic knowledge?into one integrated sta-
tistical parsing model.
One feasible way to solve this problem is to keep the original parsing model
unchanged and build an additional sub-model to incorporate semantic knowledge
from external dictionaries. The modularity afforded by this approach makes
it easier to expand or update semantic knowledge sources with the treebank
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 70?81, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Parsing the Penn Chinese Treebank with Semantic Knowledge 71
unchanged or vice versa. Further, the combination of the semantic sub-model
and the original parsing model can be optimized automatically.
In this paper, we build a class-based selection preference sub-model, which
is embedded in our lexicalized parsing model, to incorporate external seman-
tic knowledge. We use two Chinese electronic dictionaries and their combi-
nation as our semantic information sources. Several experiments are carried
out on the Penn Chinese Treebank to test our hypotheses. The results indi-
cate that a significant improvement in performance is achieved when seman-
tic knowledge is incorporated into parsing model. Further improvement analy-
sis is made. We confirm that semantic knowledge is indeed useful for nominal
compounds and coordination ambiguity resolution. And surprisingly, semantic
knowledge is also helpful to correct Chinese NV mistagging errors mentioned
by Levy and Manning (see [12]). Yet another great benefit to incorporating
semantic knowledge is to alleviate the sparseness of information available in
treebank.
2 The Baseline Parser
Our baseline parsing model is similar to the history-based, generative and lexical-
ized Model 1 of Collins (see [1]). In this model, the right hand side of lexicalized
rules is decomposed into smaller linguistic objects as follows:
P (h) ? #Ln(ln)...L1(l1)H(h)R1(r1)...Rm(rm)# .
The uppercase letters are delexicalized nonterminals, while the lowercase letters
are lexical items, e.g. head word and head tag (part-of-speech tag of the head
word), corresponding to delexicalized nonterminals. H(h) is the head constituent
of the rule from which the head lexical item h is derived according to some head
percolation rules.1 The special termination symbol # indicates that there is no
more symbols to the left/right. Accordingly, the rule probability is factored into
three distributions. The first distribution is the probability of generating the
syntactic label of the head constituent of a parent node with label P , head word
Hhw and head tag Hht:
PrH(H |P, Hht, Hhw) .
Then each left/right modifier of head constituent is generated in two steps: first
its syntactic label Mi and corresponding head tag Miht are chosen given context
features from the parent (P ), head constituent (H, Hht, Hhw), previously gen-
erated modifier (Mi?1, Mi?1ht) and other context information like the direction
(dir) and distance2 (dis) to the head constituent:
1 Here we use the modified head percolation table for Chinese from Xia (see [6]).
2 Our distance definitions are different for termination symbol and non-termination
symbol, which are similar to Klein and Manning (see [7]).
72 D. Xiong et al
PrM (Mi, Miht|HCM ) .
where the history context HCM is defined as the joint event of
P, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis .
Then the new modifier?s head word Mihw is also generated with the probability:
PrMw(Mihw|HCMw ) .
where the history context HCMw is defined as the joint event of
P, H, Hht, Hhw, Mi?1, Mi?1ht, dir, dis, Mi, Miht .
All the three distributions are smoothed through Witten-Bell interpolation
just like Collins (see [1]). For the distribution PrM , we build back-off struc-
tures with six levels, which are different from Collins? since we find our back-off
structures work better than the three-level back-off structures of Collins. For
the distribution PrMw , the parsing model backs off to the history context with
head word Hhw removed, then to the modifier head tag Miht, just like Collins.
Gildea (see [9]) and Bikel (see [10]) both observed that the effect of bilexical de-
pendencies is greatly impaired due to the sparseness of bilexical statistics. Bikel
even found that the parser only received an estimate that made use of bilexi-
cal statistics a mere 1.49% of the time. However, according to the wisdom of
the parsing community, lexical bigrams, the word pairs (Mihw, Hhw) are very
informative with semantic constraints. Along this line, in this paper, we build
an additional class-based selectional preference sub-model, which is described
in section 3, to make good use of this semantic information through selectional
restrictions between head and modifier words.
Our parser takes segmented but untagged sentences as input. The probability
of unknown words, Pr(uword|tag), is estimated based on the first character of
the word and if the first characters are unseen, the probability is estimated by
absolute discounting.
We do some linguistically motivated re-annotations for the baseline parser.
The first one is marking non-recursive noun phrases from other common noun
phrases without introducing any extra unary levels (see [1,8]). We find this basic
NP re-annotation very helpful for the performance. We think it is because of the
annotation style of the Upenn Chinese Treebank (CTB). According to Xue et al
(see [11]), noun-noun compounds formed by an uninterrupted sequence of words
POS-tagged as NNs are always left flat because of difficulties in determining
which modifies which. The second re-annotation is marking basic VPs, which we
think is beneficial for reducing multilevel VP adjunction ambiguities (see [12]).
To speed up parsing, we use the beam thresholding techniques in Xiong et
al. (see [13]). In all cases, the thresholding for completed edges is set at ct = 9
and incomplete edges at it = 7. The performance of the baseline parser is 78.5%
in terms of F1 measure of labeled parse constituents on the same CTB training
and test sets with Bikel et al (see [14])
Parsing the Penn Chinese Treebank with Semantic Knowledge 73
3 Incorporating Semantic Knowledge
In this section, we describe how to incorporate semantic knowledge from external
semantic dictionaries into parsing model to improve the performance. Firstly, we
extract semantic categories through two Chinese electronic semantic dictionaries
and some heuristic rules. Then we build a selection preference sub-model based
on extracted semantic categories. In section 3.3, we present our experiments
and results in detail. And finally, we compare parses from baseline parser with
those from the new parser incorporated with semantic knowledge. We empirically
confirm that semantic knowledge is helpful for nominal compound, coordination
and POS tagging ambiguity resolution. Additionally, we also find that semantic
knowledge can greatly alleviate problems caused by data sparseness.
3.1 Extracting Semantic Categories
Semantic knowledge is not presented in treebanks and therefore has to be ex-
tracted from external knowledge sources. We have two Chinese electronic se-
mantic dictionaries, both are good knowledge sources for us to extract semantic
categories. One is the HowNet dictionary3, which covers 67,440 words defined
by 2112 different sememes. The other is the ?TongYiCi CiLin? expanded version
(henceforth CiLin)4, which represents 77,343 words in a dendrogram.
HowNet (HN): Each sememe defined by the HowNet is regarded as a semantic
category. And through the hypernym-hyponym relation between different cat-
egories, we can extract semantic categories at various granularity levels. Since
words may have different senses, and therefore different definitions in HowNet,
we just use the first definition of words in HowNet. At the first level HN1, we ex-
tract the first definitions and use them as semantic categories of words. Through
the hypernym ladders, we can get HN2, HN3, by replacing categories at lower
level with their hypernyms at higher level. Table 1 shows information about
words and extracted categories at different levels.
CiLin (CL): CL is a branching diagram, where each node represents a semantic
category. There are three levels in total, and from the top down, 12 categories in
the first level (CL1), 97 categories in the second level (CL2), 1400 categories in
the third level (CL3). We extract semantic categories at level CL1, CL2 and CL3.
HowNet+CiLin: Since the two dictionaries have different ontologies and rep-
resentations of semantic categories, we establish a strategy to combine them:
HowNet is used as a primary dictionary, and CiLin as a secondary dictionary.
If a word is not found in HowNet but found in Cilin, we will look up other
words from its synset defined by CiLin in HowNet. If HowNet query succeeds,
the corresponding semantic category in HowNet will be assigned to this word.
3 http://www.keenage.com/.
4 The dictionary is recorded and expanded by Information Retrieval Laboratory,
Harbin Institute of Technology.
74 D. Xiong et al
Table 1. Sizes and coverage of words and semantic categories from different semantic
knowledge sources
Data HN1 HN2 HN3 CL1 CL2 CL3
words in train 9522 6040 6469
words in test 1824 1538 1581
words in both 1412 1293 1310
classes in train - 1054 381 118 12 92 1033
classes in test - 520 251 93 12 79 569
classes in both - 504 248 93 12 79 552
According to our experimental results, we choose HN2 as the primary semantic
category set and combine it with CL1, CL2 and CL3.
Heuristic Rules (HR): Numbers and time expressions are recognized using
simple heuristic rules. For a better recognition, one can define accurate regular
expressions. However, we just collect suffixes and feature characters to match
strings. For example, Chinese numbers are strings whose characters all come
from a predefined set. These two classes are merged into HowNet and labelled
by semantic categories from HowNet.
In our experiments, we combine HN2, CL1/2/3, and HR as our external
sources. In these combinations {HN2+CL1/2/3+HR}, all semantic classes come
from the primary semantic category set HN2, therefore we get the same class
coverage that we obtain from the single source HN2 but a bigger word coverage.
The number of covered words of these combinations in {train, test, both} is
{7911, 1672, 1372} respectively.
3.2 Building Class-Based Selection Preference Sub-model
There are several ways to incorporate semantic knowledge into parsing model.
Bikel (see [15]) suggested a way to capture semantic preferences by employing
bilexical-class statistics, in other words, dependencies among head-modifier word
classes. Bikel did not carry it out and therefore greater details are not available.
However, the key point, we think, is to use classes extracted from semantic
dictionary, instead of words, to model semantic dependencies between head and
modifier. Accordingly, we build a similar bilexical-class sub-model as follows:
Prclass(CMihw|CHhw, Hht, Miht, dir) .
where CMihw and CHhw represent semantic categories of words Mihw and Hhw,
respectively. This model is combined with sub-model PrMw to form a mixture
model Pmix:
Prmix = ?PrMw + (1 ? ?)Prclass . (1)
? is hand-optimized, and an improvement of about 0.5% in terms of F1 measure is
gained. However, even a very slight change in the value of ?, e.g. 0.001, will have
a great effect on the performance. Besides, it seems that the connection between
Parsing the Penn Chinese Treebank with Semantic Knowledge 75
entropy, i.e. the total negative logarithm of the inside probability of trees, and F1
measure, is lost, while this relation is observed in many experiments. Therefore,
automatic optimization algorithms, like EM, can not work in this mixture model.
The reason, we guess, is that biclass dependencies among head-modifier word
classes seem too coarse-grained to capture semantic preferences between head
and modifier. In most cases, a head word has a strong semantic constraints on
the concept ? of mw, one of its modifier words, but that doesn?t mean other
words in the same class with the head word has the same semantic preferences
on the concept ?. For example, the verb eat impose a selection restriction on
its object modifier5: it has to be solid food. On the other hand, the verb drink
specifies its object modifier to be liquid beverage. At the level HN2, verb eat
and drink have the same semantic category metabolize. However, they impose
different selection preferences on their PATIENT roles.
To sum up, bilexical dependencies are too fine-grained when being used to
capture semantic preferences and therefore lead to serious data sparseness. Bi-
class dependencies, which result in an unstable performance improvement, on the
other hand, seem to be too coarse-grained for semantic preferences. We build a
class-based selection preference model:
Prsel(CMihw|Hhw, P ) .
This model is similar to Resnik (see [2]). We use the parent node label P to
represent the grammatical relation between head and modifier. Besides, in this
model, only modifier word is replaced with its semantic category. The depen-
dencies between head word and modifier word class seem to be just right for
capturing these semantic preferences.
The final mixture model is the combination of the class-based selection pref-
erence sub-model Prsel and modifier-head generation sub-model PrMw :
Prmix = ?PrMw + (1 ? ?)Prsel . (2)
Since the connection between entropy and F1 measure is observed again, EM
algorithm is used to optimize ?. Just like Levy (see [12]), we set aside articles 1-
25 in CTB as held out data for EM algorithm and use articles 26-270 as training
data during ? optimization.
3.3 Experimental Results
We have designed several experiments to check the power of our class-based se-
lection preference model with different semantic data sources. In all experiments,
we first use the EM algorithm to optimize the parameter ?. As mentioned above,
during parameter optimization, articles 1-25 are used as held out data and ar-
ticles 26-270 are used as training data. Then we test our mixture model with
optimized parameter ? using the training data of articles 1-270 and test data of
articles 271-300 of length at most 40 words.
5 According to Thematic Role theory, this modifier has a PATIENT role.
76 D. Xiong et al
Table 2. Results for incorporating different semantic knowledge sources. The baseline
parser is described in Sect. 2. in detail.
Baseline HN1 HN2 HN3 CL1 CL2 CL3
F1(%) 78.5 78.6 79.1 78.9 77.5 78.7 78.8
Table 3. Results for combinations of different semantic knowledge sources
Baseline HN2+CL1+HR HN2+CL2+HR HN2+CL3+HR
F1(%) 78.5 79.2 79.4 79.3
Firstly, we carry out experiments on HowNet and CiLin, separately. Exper-
imental results are presented in Table 2. As can be seen, CiLin has a greater
coverage of words than that of HowNet, however, it works worse than HowNet.
And at the level CL1, coarse-grained classes even yield degraded results. It?s dif-
ficult to explain this, but the main reason may be that HowNet has a fine-grained
and substantial ontology while CiLin is designed only as a synset container.
Since HowNet has a better semantic representation and CiLin better cov-
erage, we want to combine them. The combination is described in Sect. 3.1,
where HN2 is used as the primary semantic category set. Words found by CiLin
and heuristic rules are labelled by semantic categories from HN2. Results are
shown in Table 3. Although external sources HN2+CL1/2/3+HR have the iden-
tical word coverage and yield exactly the same number of classes, the different
word-class distributions in them lead to the different results.
Due to the combination of HN2, CL2 and HR, we see that our new parser
with external semantic knowledge outperforms the baseline parser by 0.9% in
F1 measure. Given we are already at the 78% level of accuracy, an improve-
ment of 0.9% is well worth obtaining and confirms the importance of semantic
dependencies on parsing. Further, we do the significance test using Bikel?s sig-
nificance tester6 which is modified to output p-value for F1. The significance
level for F-score is at most (43376+1)/(1048576+1) = 0.041. A second 1048576
iteration produces the similar result. Therefore the improvement is statistically
significant.
3.4 Performance Improvement Analysis
We manually analyze parsing errors of the baseline parser (BP ) as well as per-
formance improvement of the new parser (IP ) with semantic knowledge from
the combination of HN2, CL2 and HR. Improvement analysis can provide an
additional valuable perspective: how semantic knowledge helps to resolve some
ambiguities. We compare BP and IP on the test data parse by parse. There are
299 sentences of length at most 40 words among the total 348 test sentences. The
two parsers BP and IP found different parses for 102 sentences, among which
6 See http://www.cis.upenn.edu/ dbikel/software.html
Parsing the Penn Chinese Treebank with Semantic Knowledge 77
Table 4. Frequency of parsing improvement types. AR represents ambiguity resolution.
Type Count Percent(%)
Nominal Compound AR 19 38
Coordination AR 9 18
NV AR in NV+noun 6 12
Other AR 16 32
IP yields better parse trees for 47 sentences according to the gold standard trees.
We have concentrated on these 47 sentences and compared parse trees found by
IP with those found by BP . Frequencies of major types of parsing improvement
is presented in Table 4. Levy and Manning (see [12])(henceforth L&M) observed
the top three parsing error types: NP-NP modification, Coordination and NV
mistagging, which are also common in our baseline parser. As can be seen, our
improved parser can address these types of ambiguities to some extent through
semantic knowledge.
Nominal Compounds (NCs) Disambiguation: Nominal compounds are no-
torious ?every way ambiguous? constructions.7 The different semantic interpre-
tations have different dependency structures. According to L&M, this ambiguity
will be addressed by the dependency model when word frequencies are large
enough to be reliable. However, even for the treebank central to a certain topic,
many very plausible dependencies occur only once.8 A good technique for re-
solving this conflict is to generalize the dependencies from word pairs to word-
class pairs. Such generalized dependencies, as noted in section 3.2, can capture
semantic preferences, as well as alleviate the data sparseness associated with
standard bilexical statistics. In our class-based selection preference model, if the
frequency of pair [CMhw, Hhw]9 is large enough, the parser can interpret nominal
compounds correctly, that is, it can tell which modify which.
NCs are always parsed as flatter structures by our baseline parser, just like
the tree a. in Figure 1. This is partly because of the annotation style of CTB,
where there is no NP-internal structure. For these NCs without internal analysis,
we re-annotated them as basic NPs with label NPB, as mentioned in section 2.
This re-annotation really helps. Another reason is that the baseline parser, or
the modifier word generating sub-model PMw , can not capture hierarchical se-
mantic dependencies of internal structures of NCs due to the sparseness of bilex-
ical dependencies. In our new parser, however, the selection preference model is
able to build semantically preferable structures through word-class dependency
statistics. For NCs like (n1, n2, n3), where ni is a noun, dependency structures
7 ?Every way ambiguous? constructions are those for which the number of analy-
ses is the number of binary trees over the terminal elements. Prepositional phrase
attachment, coordination, and nominal compounds are all ?every way ambiguous?
constructions.
8 Just as Klein et al (see [8]) said, one million words of training data just isn?t enough.
9 Henceforth, [s1, s2] denotes a dependency structure, where s1 is a modifier word or
its semantic class (C), and s2 is the head word.
78 D. Xiong et al
a. NPB




NR
??
NN
??
NN
??
b. NP




NP

NPB
NR
??
NPB
NN
??
NPB
NN
??
Fig. 1. Nominal Compounds: The North Korean government?s special envoy. a. is the
incorrect flat parse, b. is the right one in corpus
{[Cn1 , n2], [Cn1 , n3], [Cn2 , n3]} will be checked in terms of semantic acceptability
and semantically preferable structures will be built finally. For more complicated
NCs, similar analysis follows.
In our example (see Fig. 1.), the counts of word dependencies [??/North
Korea, ??/government] and [??/North Korea,??/special envoy] in the
training data both are 0. Therefore, it is impossible for the baseline parser to
have a preference between these two dependency structures. On the other hand,
the counts of word-class dependencies [???,??/government], where ???
is the semantic category of?? in HN2, is much larger than the counts of [??
?,??/special envoy] and [??,??/special envoy], where?? is the semantic
category of ?? in the training data. Therefore, the dependency structure of
[??/North Korea, ??/government] will be built.
Coordination Disambiguation: Coordination is another kind of ?every way
ambiguous? construction. For coordination structures, the head word is meaning-
less. But that doesn?t matter, since semantic dependency between the spurious
head and modifier will be used to measure the meaning similarity of coordinated
structures. Therefore, our selection preference model still works in coordination
constructions. We have also found VP coordination ambiguity, which is similar
to that observed by L&M. The latter VP in coordinated VPs is often parsed as
an IP due to pro-drop by the baseline parser. That is, the coordinated structure
VP is parsed as: V P 0 ? V P 1IP 2. This parse will be penalized by the selection
preference model because the hypothesis that the head word of IP 2 has a sim-
ilar meaning to the head word of V P 1 under the grammatical relation V P 0 is
infrequent.
NV-ambiguous Tagging Disambiguation: The lack of overt morphological
marking for transforming verbal words to nominal words in Chinese results in
ambiguity between these two categories. L&M argued that the way to resolve
this ambiguity is to look at more external context, like some function words,
e.g. adverbial or prenominal modifiers, co-occurring with NV-ambiguous words.
However, in some cases, NV-ambiguous words can be tagged correctly without
external context. Chen et al (see [16]) studied the pattern of NV+noun, which
will be analyzed as a predicate-object structure if NV is a verb and a modifier-
noun structure if NV is a noun. They found that in most cases, this pattern can
Parsing the Penn Chinese Treebank with Semantic Knowledge 79
a. VP

VPB
VV
??
NPB
NN
??
b. NPB

NN
??
NN
??
Fig. 2. NV-ambiguity: a. implement plans (incorrect parse) versus b. implementation
plans (corpus)
Table 5. Previous Results on CTB parsing for sentences of length at most 40 words
LP LR F1
Bikel and Chiang 2000 77.2 76.2 76.7
Levy and Manning 2003 78.4 79.2 78.8
Present work 80.1 78.7 79.4
Bikel Thesis 2004 81.2 78.0 79.6
Chiang and Bikel 2002 81.1 78.8 79.9
be parsed correctly without any external context. Furthermore, they argued that
semantic preferences are helpful for the resolution of ambiguity between these
two different structures. In our selection preference model, semantic preferences
interweave with grammatical relations. These semantic dependencies impose con-
straints on the structure of the pattern NV+noun and therefore on the POS
tag of NV. Figure 2 shows our new parser can correct NV mistagging errors
occurring in the pattern of NV+noun.
Smoothing:Besides the three ambiguity resolution noted above, semantic knowl-
edge indeed helps alleviate the fundamental sparseness of the lexical dependency
information available in the CTB. For many word pairs [mod,head], whose count
information is not available in the training data, the dependency statistics of head
and modifier can still work through the semantic category of mod. During our man-
ual analysis of performance improvement, many other structural ambiguities are
addressed due to the smoothing function of semantic knowledge.
4 Related Work on CTB Parsing
Previous work on CTB parsing and their results are shown in table 5. Bikel and
Chiang (see [14]) used two different models on CTB, one based on the modi-
fied BBN model which is very similar to our baseline model, the other on Tree
Insertion Grammar (TIG). While our baseline model used the same unknown
word threshold with Bikel and Chiang but smaller beam width, our result out-
performs theirs due to other features like distance, basic NP re-annotation used
by our baseline model. Levy and Manning (see [12]) used a factored model with
rich re-annotations guided by error analysis. In the baseline model, we also used
several re-annotations but find most re-annotations they suggested do not fit
80 D. Xiong et al
our model. The three parsing error types expounded above are also found by
L&M. However, we used more efficient measures to keep our improved model
from these errors.
The work of Bikel thesis (see [10]) emulated Collins? model and created a
language package to Chinese parsing. He used subcat frames and an additional
POS tagger for unseen words. Chiang and Bikel (see [17]) used the EM algorithm
on the same TIG-parser to improve the head percolation table for Chinese pars-
ing. Both these two parsers used fine-tuned features recovered from the treebank
that our model does not use. This leads to better results and indicates that there
is still room of improvement for our model.
5 Conclusions
We have shown that how semantic knowledge may be incorporated into a gener-
ative model for full parsing, which reaches 79.4% in CTB. Experimental results
are quite consistent with our intuition. After the manual analysis of performance
improvement, the working mechanism of semantic knowledge in the selection
preference model is quite clear:
1. Using semantic categories extracted from external dictionaries, the class-
based selection preference model first generalizes standard bilexical depen-
dencies, some of which are not available in training data, to word-class de-
pendencies. These dependencies are neither too fine-grained nor too coarse-
grained compared with bilexical and biclass dependencies, and really help to
alleviate fundamental information sparseness in treebank.
2. Based on the generalized word-class pairs, semantic dependencies are cap-
tured and used to address different kinds of ambiguities, like nominal com-
pounds, coordination construction, even NV-ambiguous words tagging.
Our experiments show that generative models have room for improvement
by employing semantic knowledge. And that may be also true for discrimina-
tive models, since these models can easily incorporate richer features in a well-
founded fashion. This is the subject of our future work.
Acknowledgements
This work was supported in part byNational HighTechnologyResearch andDevel-
opment Program under grant #2001AA114010 and #2003AA111010. We would
like to acknowledge anonymous reviewers who provided helpful suggestions.
References
1. Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Pars-
ing. PhD thesis, University of Pennsylvania.
2. Philip Stuart Resnik. 1993. Selection and Information: A Class-Based Approach to
Lexical Relationships. PhD thesis, University of Pennsylvania, Philadelphia, PA,
USA.
Parsing the Penn Chinese Treebank with Semantic Knowledge 81
3. Sanda Harabagiu. 1996. An Application of WordNet to Prepositional Attachement.
In Proceedings of ACL-96, June 1996, Santa Cruz CA, pages 360-363.
4. Yuval Krymolowski and Dan Roth. 1998. Incorporating Knowledge in Natural Lan-
guage Learning: A Case Study. In COLING-ACL?98 Workshop on Usage of Word-
Net in Natural Language Processing Systems,Montreal, Canada.
5. Mark McLauchlan. 2004. Thesauruses for Prepositional Phrase Attachment. In
Proceedings of CoNLL-2004,Boston, MA, USA, 2004, pp. 73-80.
6. Fei Xia. 1999. Automatic Grammar Generation from Two Different Perspectives.
PhD thesis, University of Pennsylvania.
7. Dan Klein and Christopher D. Manning. 2002. Fast Exact Natural Language Pars-
ing with a Factored Model. In Advances in Neural Information Processing Systems
15 (NIPS-2002).
8. Dan Klein and Christopher D. Manning. 2003. Accurate Unlexicalized Parsing. In
Proceedings of ACL-03.
9. Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of
EMNLP-01, Pittsburgh, Pennsylvania.
10. Daniel M. Bikel. 2004a. On the Parameter Space of Generative Lexicalized Statis-
tical Parsing Models. PhD thesis, University of Pennsylvania.
11. Nianwen Xue and Fei Xia. 2000. The Bracketing Guidelines for Chinese Treebank
Project. Technical Report IRCS 00-08, University of Pennsylvania.
12. Roger Levy and Christopher Manning. 2003. Is it harder to parse Chinese, or the
Chinese Treebank? In Proceedings of ACL-03.
13. Deyi Xiong, Qun Liu and Shouxun Lin. 2005. Lexicalized Beam Thresholding Pars-
ing with Prior and Boundary Estimates. In Proceedings of the 6th Conference on
Intelligent Text Processing and Computational Linguistics (CICLing), Mexico City,
Mexico, 2005.
14. Daniel M. Bikel and David Chiang. 2000. Two statistical parsing models applied
to the chinese treebank. In Proceedings of the Second Chinese Language Processing
Workshop, pages 1-6.
15. Daniel M. Bikel. 2004b. Intricacies of Collins? Parsing Model. to appear in Com-
putational Linguistics.
16. Kejian Chen and Weimei Hong. 1996. Resolving Ambiguities of Predicate-object
and Modifier-noun Structures for Chinese V-N Patterns. in Chinese. In Communi-
cation of COLIPS, Vol.6, #2, pages 73-79.
17. David Chiang and Daniel M. Bikel. 2002. Recovering Latent Information in Tree-
banks. In proceedings of COLING,2002.
Refinements in BTG-based Statistical Machine Translation
Deyi Xiong, Min Zhang, Aiti Aw
Human Language Technology
Institute for Infocomm Research
21 Heng Mui Keng Terrace
Singapore 119613
{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sg
Haitao Mi, Qun Liu and Shouxun Lin
Key Lab of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing China, 100080
{htmi, liuqun, sxlin}@ict.ac.cn
Abstract
Bracketing Transduction Grammar (BTG)
has been well studied and used in statistical
machine translation (SMT) with promising
results. However, there are two major issues
for BTG-based SMT. First, there is no effec-
tive mechanism available for predicting or-
ders between neighboring blocks in the orig-
inal BTG. Second, the computational cost is
high. In this paper, we introduce two re-
finements for BTG-based SMT to achieve
better reordering and higher-speed decod-
ing, which include (1) reordering heuristics
to prevent incorrect swapping and reduce
search space, and (2) special phrases with
tags to indicate sentence beginning and end-
ing. The two refinements are integrated into
a well-established BTG-based Chinese-to-
English SMT system that is trained on large-
scale parallel data. Experimental results on
the NIST MT-05 task show that the proposed
refinements contribute significant improve-
ment of 2% in BLEU score over the baseline
system.
1 Introduction
Bracket transduction grammar was proposed by Wu
(1995) and firstly employed in statistical machine
translation in (Wu, 1996). Because of its good trade-
off between efficiency and expressiveness, BTG re-
striction is widely used for reordering in SMT (Zens
et al, 2004). However, BTG restriction does not
provide a mechanism to predict final orders between
two neighboring blocks.
To solve this problem, Xiong et al (2006)
proposed an enhanced BTG with a maximum en-
tropy (MaxEnt) based reordering model (MEBTG).
MEBTG uses boundary words of bilingual phrases
as features to predict their orders. Xiong et
al. (2006) reported significant performance im-
provement on Chinese-English translation tasks in
two different domains when compared with both
Pharaoh (Koehn, 2004) and the original BTG us-
ing flat reordering. However, error analysis of the
translation output of Xiong et al (2006) reveals
that boundary words predict wrong swapping, espe-
cially for long phrases although the MaxEnt-based
reordering model shows better performance than
baseline reordering models.
Another big problem with BTG-based SMT is the
high computational cost. Huang et al (2005) re-
ported that the time complexity of BTG decoding
with m-gram language model is O(n3+4(m?1)). If a
4-gram language model is used (common in many
current SMT systems), the time complexity is as
high as O(n15). Therefore with this time complexity
translating long sentences is time-consuming even
with highly stringent pruning strategy.
To speed up BTG decoding, Huang et al (2005)
adapted the hook trick which changes the time
complexity from O(n3+4(m?1)) to O(n3+3(m?1)).
However, the implementation of the hook trick with
pruning is quite complicated. Another method to in-
crease decoding speed is cube pruning proposed by
Chiang (2007) which reduces search space signifi-
cantly.
In this paper, we propose two refinements to ad-
dress the two issues, including (1) reordering heuris-
505
tics to prevent incorrect swapping and reduce search
space using swapping window and punctuation re-
striction, and (2) phrases with special tags to indicate
beginning and ending of sentence. Experimental re-
sults show that both refinements improve the BLEU
score significantly on large-scale data.
The above refinements can be easily implemented
and integrated into a baseline BTG-based SMT sys-
tem. However, they are not specially designed for
BTG-based SMT and can also be easily integrated
into other systems with different underlying trans-
lation strategies, such as the state-of-the-art phrase-
based system (Koehn et al, 2007), syntax-based sys-
tems (Chiang et al, 2005; Marcu et al, 2006; Liu et
al., 2006).
The rest of the paper is organized as follows. In
section 2, we review briefly the core elements of
the baseline system. In section 3 we describe our
proposed refinements in detail. Section 4 presents
the evaluation results on Chinese-to-English trans-
lation based on these refinements as well as results
obtained in the NIST MT-06 evaluation exercise. Fi-
nally, we conclude our work in section 5.
2 The Baseline System
In this paper, we use Xiong et al (2006)?s sys-
tem Bruin as our baseline system. Their system has
three essential elements which are (1) a stochastic
BTG, whose rules are weighted using different fea-
tures in log-linear form, (2) a MaxEnt-based reorder-
ing model with features automatically learned from
bilingual training data, (3) a CKY-style decoder us-
ing beam search similar to that of Wu (1996). We
describe the first two components briefly below.
2.1 Model
The translation process is modeled using BTG rules
which are listed as follows
A ? [A1, A2] (1)
A ? ?A1, A2? (2)
A ? x/y (3)
The lexical rule (3) is used to translate source phrase
x into target phrase y and generate a block A. The
two rules (1) and (2) are used to merge two consec-
utive blocks into a single larger block in a straight or
inverted order.
To construct a stochastic BTG, we calculate rule
probabilities using the log-linear model (Och and
Ney, 2002). For the two merging rules (1) and (2),
the assigned probability Prm(A) is defined as fol-
lows
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where ?, the reordering score of block A1 and
A2, is calculated using the MaxEnt-based reordering
model (Xiong et al, 2006) described in the next sec-
tion, ?? is the weight of ?, and 4pLM (A1,A2) is the
increment of language model score of the two blocks
according to their final order, ?LM is its weight.
For the lexical rule (3), it is applied with a proba-
bility Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|y|)?6
?p?LMLM (y) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical translation
probabilities in both directions, exp(1) and exp(|y|)
are the phrase penalty and word penalty, respec-
tively and ?s are weights of features. These features
are commonly used in the state-of-the-art systems
(Koehn et al, 2005; Chiang et al, 2005).
2.2 MaxEnt-based Reordering Model
The MaxEnt-based reordering model is defined on
two consecutive blocks A1 and A2 together with
their order o ? {straight, inverted} according to
the maximum entropy framework.
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(6)
where the functions hi ? {0, 1} are model features
and ?i are weights of the model features trained au-
tomatically (Malouf, 2002).
There are three steps to train a MaxEnt-based re-
ordering model. First, we need to extract reordering
examples from unannotated bilingual data, then gen-
erate features from these examples and finally esti-
mate feature weights.
506
For extracting reordering examples, there are two
points worth mentioning:
1. In the extraction of useful reordering examples,
there is no length limitation over blocks com-
pared with extracting bilingual phrases.
2. When enumerating all combinations of neigh-
boring blocks, a good way to keep the number
of reordering examples acceptable is to extract
smallest blocks with the straight order while
largest blocks with the inverted order .
3 Refinements
In this section we describe two refinements men-
tioned above in detail. First, we present fine-
grained reordering heuristics using swapping win-
dow and punctuation restriction. Secondly, we inte-
grate special bilingual phrases with sentence begin-
ning/ending tags.
3.1 Reordering Heuristics
We conduct error analysis of the translation out-
put of the baseline system and observe that Bruin
sometimes incorrectly swaps two large neighboring
blocks on the target side. This happens frequently
when inverted order successfully challenges straight
order by the incorrect but strong support from the
language model and the MaxEnt-based reordering
model. The reason is that only boundary words
are used as evidences by both language model and
MaxEnt-based reordering model when the decoder
selects which merging rule (straight or inverted) to
be used 1. However, statistics show that bound-
ary words are not reliable for predicting the right
order between two larger neighboring blocks. Al-
Onaizan and Papineni (2006) also proved that lan-
guage model is insufficient to address long-distance
word reordering. If a wrong inverted order is se-
lected for two large consecutive blocks, incorrect
long-distance swapping happens.
Yet another finding is that many incorrect swap-
pings are related to punctuation marks. First, the
source sequence within a pair of balanced punctua-
tion marks (quotes and parentheses) should be kept
1In (Xiong et al, 2006), the language model uses the left-
most/rightmost words on the target side as evidences while the
MaxEnt-based reordering model uses the boundary words on
both sides.
Chinese: ?? : ??????????
?????????????????
????
Bruin: urgent action , he said : ?This is a very
serious situation , we can only hope that there
will be a possibility .?
Bruin+RH: he said : ?This is a very serious sit-
uation , we can only hope that there will be the
possibility to expedite action .?
Ref: He said: ?This is a very serious situa-
tion. We can only hope that it is possible to
speed up the operation.?
Figure 1: An example of incorrect long-distance
swap. The underlined Chinese words are incorrectly
swapped to the beginning of the sentence by the
original Bruin. RH means reordering heuristics.
within the punctuation after translation. However,
it is not always true when reordering is involved.
Sometime the punctuation marks are distorted with
the enclosed words sequences being moved out.
Secondly, it is found that a series of words is fre-
quently reordered from one side of a structural mark,
such as commas, semi-colons and colons, to the
other side of the mark for long sentences contain-
ing such marks. Generally speaking, on Chinese-
to-English translation, source words are translated
monotonously relative to their adjacent punctuation
marks, which means their order relative to punctua-
tion marks will not be changed. In summary, punctu-
ation marks place a strong constraint on word order
around them.
For example, in Figure 1, Chinese words ???
??? are reordered to sentence beginning. That is
an incorrect long-distance swapping, which makes
the reordered words moved out from the balanced
punctuation marks ??? and ???, and incorrectly
precede their previous mark ???.
These incorrect swappings definitely jeopardize
the quality of translation. Here we propose two
straightforward but effective heuristics to control
and adjust the reordering, namely swapping window
and punctuation restriction.
Swapping Window (SW): It constrains block
swapping in the following way
ACTIVATE A ? ?A1, A2? IF |A1s|+ |A2s| < sws
507
where |Ais| denotes the number of words on the
source side Ais of block Ai, sws is a pre-defined
swapping window size. Any inverted reordering be-
yond the pre-defined swapping window size is pro-
hibited.
Punctuation Restriction (PR): If two neighbor-
ing blocks include any of the punctuation marks p ?
{? ? ? ? ? ? ? ? ? ? ? ?}, the two
blocks will be merged with straight order.
Punctuation marks were already used in pars-
ing (Christine Doran, 2000) and statistical machine
translation (Och et al, 2003). In (Och et al,
2003), three kinds of features are defined, all re-
lated to punctuation marks like quotes, parentheses
and commas. Unfortunately, no statistically signifi-
cant improvement on the BLEU score was reported
in (Och et al, 2003). In this paper, we consider
this problem from a different perspective. We em-
phasize that words around punctuation marks are
reordered ungrammatically and therefore we posi-
tively use punctuation marks as a hard decision to
restrict such reordering around punctuations. This
is straightforward but yet results in significant im-
provement on translation quality.
The two heuristics described above can be used
together. If the following conditions are satisfied,
we can activate the inverted rule:
|A1s|+ |A2s| < sws && P
?
(A1s
?
A2s) = ?
where P is the set of punctuation marks mentioned
above.
The two heuristics can also speed up decoding be-
cause decoding will be monotone within those spans
which are not in accordance with both heuristics.
For a sentence with n words, the total number of
spans is O(n2). If we set sws = m (m < n),
then the number of spans with monotone search is
O((n?m)2). With punctuation restriction, the non-
monotone search space will reduce further.
3.2 Phrases with Sentence Beginning/Ending
Tags
We observe that in a sentence some phrases are more
likely to be located at the beginning, while other
phrases are more likely to be at the end. This kind of
location information with regard to the phrase posi-
tion could be used for reordering. A straightforward
way to use this information is to mark the begin-
ning and ending of word-aligned sentences with ?s?
and ?/s? respectively. This idea is borrowed from
language modeling (Stolcke, 2002). The corre-
sponding tags at the source and target sentences are
aligned to each other, i.e, the beginning tag of source
sentences is aligned to the beginning tag of target
sentences, similarly for the ending tag. Figure 2
shows a word-aligned sentence pair annotated with
the sentence beginning and ending tag.
During training, the sentence beginning and end-
ing tags (?s? and ?/s?) are treated as words. There-
fore the phrase extraction and MaxEnt-based re-
ordering training algorithm need not to be modified.
Phrases with the sentence beginning/ending tag will
be extracted and MaxEnt-based reordering features
with such tags will also be generated. For example,
from the word-aligned sentence pair in Figure 2, we
can extract tagged phrases like
?s??? ||| ?s? Tibet ?s
?? ?/s? ||| achievements ?/s?
and generate MaxEnt-based reordering features with
tags like
hi(o, b1, b2) =
{ 1, b2.t1 = ?/s?, o = s
0, otherwise
where b1, b2 are blocks, t1 denotes the last source
word, o = s means the order between two blocks
is straight. To avoid wrong alignments, we remove
tagged phrases where only the beginning/ending tag
is extracted on either side of the phrases, such as
?s? ||| ?s? Those?
?/s? ||| ?/s?
During decoding, we first annotate source sen-
tences with the beiginning/ending tags, then trans-
late them as what Bruin does. Note that phrases
with sentence beginning/ending tags will be used in
the same way as ordinary phrases without such tags
during decoding. With the additional support of lan-
guage model and MaxEnt-based reordering model,
we observe that phrases with such tags are always
moved to the beginning or ending of sentences cor-
rectly.
508
?s? ?? ?? ?? ?? ?? ?? ?/s?
?s? Tibet ?s financial work has gained remarkable achievements ?/s?
Figure 2: A word-aligned sentence pair annotated with the sentence beginning and ending tag.
4 Evaluation
In this section, we report the performance of the en-
hanced Bruin on the NIST MT-05 and NIST MT-06
Chinese-to-English translation tasks. We describe
the corpus, model training, and experiments related
to the refinements described above.
4.1 Corpus
The bilingual training data is derived from the fol-
lowing various sources: the FBIS (LDC2003E14),
Hong Kong Parallel Text (Hong Kong News and
Hong Kong Hansards, LDC2004T08), Xinhua News
(LDC2002E18), Chinese News Translation Text
Part1 (LDC2005T06), Translations from the Chi-
nese Treebank (LDC2003E07), Chinese English
News Magazine (LDC2005E47). It contains 2.4M
sentence pairs in total (68.1M Chinese words and
73.8M English words).
For the efficiency of minimum-error-rate training,
we built our development set using sentences not ex-
ceeding 50 characters from the NIST MT-02 evalu-
ation test data (580 sentences).
4.2 Training
We use exactly the same way and configuration de-
scribed in (He et al, 2006) to preprocess the training
data, align words and extract phrases.
We built two four-gram language models using
Xinhua section of the English Gigaword corpus
(181.1M words) and the English side of the bilin-
gual training data described above respectively. We
applied modified Kneser-Ney smoothing as imple-
mented in the SRILM toolkit (Stolcke, 2002).
The MaxEnt-based reordering model is trained
using the way of (Xiong et al, 2006). The difference
is that we only use lexical features generated by tail
words of blocks, instead of head words, removing
features generated by the combination of two bound-
ary words.
Bleu(%) Secs/sent
Bruin 29.96 54.3
sws RH1 RH12 RH1 RH12
5 29.65 29.95 42.6 41.2
10 30.55 31.27 46.2 41.8
15 30.26 31.40 48.0 42.2
20 30.19 31.42 49.1 43.2
Table 1: Effect of reordering heuristics. RH1 de-
notes swapping window while RH12 denotes swap-
ping window with the addition of punctuation re-
striction.
4.3 Translation Results
Table 1 compares the BLEU scores 2 and the speed
in seconds/sentence of the baseline system Bruin
and the enhanced system with reordering heuristics
applied. The second row gives the BLEU score and
the average decoding time of Bruin. The rows be-
low row 3 show the BLEU scores and speed of the
enhanced Bruin with different combinations of re-
ordering heuristics. We can clearly see that the re-
ordering heuristics proposed by us have a two-fold
effect on the performance: improving the BLEU
score and decreasing the average decoding time.
The example in Figure 1 shows how reordering
heuristics prevent incorrect long-distance swapping
which is not in accordance with the punctuation re-
striction.
Table 1 also shows that a 15-word swapping win-
dow is an inflexion point with the best tradeoff be-
tween the decoding time and the BLEU score. We
speculate that in our corpus most reorderings hap-
pen within a 15-word window. We use the FBIS
corpus to testify this hypothesis. In this corpus, we
extract all reordering examples using the algorithm
of Xiong et al (2006). Figure 3 shows the reorder-
ing length distribution curve in this corpus. Accord-
2In this paper, all BLEU scores are case-sensitive and evalu-
ated on the NIST MT-05 Chinese-to-English translation task if
there is no special note.
509
0 10 20 30 40 50 60 70 80
0
5
10
15
20
25
Pe
rce
nt 
(%
)
Reordering Length
Figure 3: Reordering length distribution. The hor-
izontal axis (reordering length) indicates the num-
ber of words on the source side of two neighboring
blocks which are to be swapped. The vertical axis
represents what proportion of reorderings with a cer-
tain length is likely to be in all reordering examples
with an inverted order.
Bleu(%)
Without Special Phrases 31.40
With Special Phrases 32.01
Table 2: Effect of integrating special phrases with
the sentence beginning/ending tag.
ing to our statistics, reorderings within a window
not exceeding 15 words have a very high proportion,
97.29%. Therefore we set sws = 15 for later exper-
iments.
Table 2 shows the effect of integrating special
phrases with sentence beginning/ending tags into
Bruin. As special phrases accounts for only 1.95%
of the total phrases used, an improvement of 0.6%
in BLEU score is well worthwhile. Further, the im-
provement is statistically significant at the 99% con-
fidence level according to Zhang?s significant tester
(Zhang et al, 2004). Figure 4 shows several exam-
ples translated with special phrases integrated. We
can see that phrases with sentence beginning/ending
tags are correctly selected and located at the right
place.
Table 3 shows the performance of two systems on
the NIST MT-05 Chinese test data, which are (1)
System Refine MT-05 MT-06
Bruin - 29.96 -
EBruin RH 31.40 30.22
EBruin RH+SP 32.01 -
Table 3: Results of different systems. The refine-
ments RH, SP represent reordering heuristics and
special phrases with the sentence beginning/ending
tag, respectively.
Bruin, trained on the large data described above; and
(2) enhanced Bruin (EBruin) with different refine-
ments trained on the same data set. This table also
shows the evaluation result of the enhanced Bruin
with reordering heuristics, obtained in the NIST MT-
06 evaluation exercise. 3
5 Conclusions
We have described in detail two refinements for
BTG-based SMT which include reordering heuris-
tics and special phrases with tags. The refinements
were integrated into a well-established BTG-based
system Bruin introduced by Xiong et al (2006). Re-
ordering heuristics proposed here achieve a twofold
improvement: better reordering and higher-speed
decoding. To our best knowledge, we are the first
to integrate special phrases with the sentence be-
ginning/ending tag into SMT. Experimental results
show that the above refinements improve the base-
line system significantly.
For further improvements, we will investigate
possible extensions to the BTG grammars, e.g.
learning useful nonterminals using unsupervised
learning algorithm.
Acknowledgements
We would like to thank the anonymous review-
ers for useful comments on the earlier version of
this paper. The first author was partially sup-
ported by the National Science Foundations of
China (No. 60573188) and the High Technology
Research and Development Program of China (No.
2006AA010108) while he studied in the Institute of
Computing Technology, Chinese Academy of Sci-
ences.
3Full results are available at http://www.nist.gov/
speech/tests/mt/doc/mt06eval official results.html.
510
With Special Phrases Without Special Phrases
?s? Japan had already pledged to provide 30 mil-
lion US dollars of aid due to the tsunami victims of
the country . ?/s?
originally has pledged to provide 30 million US
dollars of aid from Japan tsunami victimized coun-
tries .
?s? the results of the survey is based on the re-
sults of the chiefs of the Ukrainian National 50.96%
cast by chiefs . ?/s?
is based on the survey findings Ukraine 50.96% cast
by the chiefs of the chiefs of the country .
?s? and at the same time , the focus of the world have
been transferred to other areas . ?/s?
and at the same time , the global focus has shifted
he.
Figure 4: Examples translated with special phrases integrated. The bold underlined words are special phrases
with the sentence beginning/ending tag.
References
Yaser Al-Onaizan, Kishore Papineni. 2006. Distortion
Models for Statistical Machine Translation. In Pro-
ceedings of ACL-COLING 2006.
David Chiang, Adam Lopez, Nitin Madnani, Christof
Monz, Philip Resnik, Michael Subotin. 2005. The
Hiero Machine Translation System: Extensions, Eval-
uation, and Analysis. In Proceedings of HLT/EMNLP,
pages 779?786, Vancouver, October 2005.
David Chiang. 2007. Hierarchical Phrase-based Transla-
tion. In computational linguistics, 33(2).
Christine Doran. 2000. Punctuation in a Lexicalized
Grammar. In Proceedings of Workshop TAG+5, Paris.
Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, Qun
Liu. 2006. ICT System Description for the 2006
TC-STAR Run #2 SLT Evaluation. In Proceedings of
TC-STAR Workshop on Speech-to-Speech Translation,
Barcelona, Spain.
Liang Huang, Hao Zhang and Daniel Gildea. 2005. Ma-
chine Translation as Lexicalized Parsing with Hooks.
In Proceedings of the 9th International Workshop
on Parsing Technologies (IWPT-05), Vancouver, BC,
Canada, October 2005.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proceedings of the Sixth Conference of the
Association for Machine Translation in the Americas,
pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch
Mayne, Chris Callison-Burch, Miles Osborne and
David Talbot. 2005. Edinburgh System Description
for the 2005 IWSLT Speech Translation Evaluation. In
International Workshop on Spoken Language Transla-
tion.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. ACL
2007, demonstration session, Prague, Czech Republic,
June 2007.
Yang Liu, Qun Liu, Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Transla-
tion. In Proceedings of ACL-COLING 2006.
Robert Malouf. 2002. A comparison of algorithms for
maximum entropy parameter estimation. In Proceed-
ings of CoNLL-2002.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. SPMT: Statistical Ma-
chine Translation with Syntactified Target Language
Phraases. In Proceedings of EMNLP.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statisti-
cal machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, Dragomir Radev. 2003. Final
Report of Johns Hopkins 2003 Summer Workshop on
Syntax for Statistical Machine Translation.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Dekai Wu. 1995. Stochastic inversion transduction
grammars, with application to segmentation, bracket-
ing, and alignment of parallel corpora. In Proceedings
of IJCAL 1995, pages 1328-1334, Montreal, August.
511
Dekai Wu. 1996. A Polynomial-Time Algorithm for Sta-
tistical Machine Translation. In Proceedings of ACL
1996.
Deyi Xiong, Qun Liu and Shouxun Lin. 2006. Maxi-
mum Entropy Based Phrase Reordering Model for Sta-
tistical Machine Translation. In Proceedings of ACL-
COLING 2006, pages 521?528.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Ma-
chine Translation. In Proceedings of CoLing 2004,
Geneva, Switzerland, pp. 205-211.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting BLEU/NIST scores: How much improve-
ment do we need to have a better system? In Proceed-
ings of LREC 2004, pages 2051? 2054.
512
Proceedings of the 43rd Annual Meeting of the ACL, pages 459?466,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Log-linear Models for Word Alignment
Yang Liu , Qun Liu and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
No. 6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
{yliu, liuqun, sxlin}@ict.ac.cn
Abstract
We present a framework for word align-
ment based on log-linear models. All
knowledge sources are treated as feature
functions, which depend on the source
langauge sentence, the target language
sentence and possible additional vari-
ables. Log-linear models allow statis-
tical alignment models to be easily ex-
tended by incorporating syntactic infor-
mation. In this paper, we use IBM Model
3 alignment probabilities, POS correspon-
dence, and bilingual dictionary cover-
age as features. Our experiments show
that log-linear models significantly out-
perform IBM translation models.
1 Introduction
Word alignment, which can be defined as an object
for indicating the corresponding words in a parallel
text, was first introduced as an intermediate result of
statistical translation models (Brown et al, 1993). In
statistical machine translation, word alignment plays
a crucial role as word-aligned corpora have been
found to be an excellent source of translation-related
knowledge.
Various methods have been proposed for finding
word alignments between parallel texts. There are
generally two categories of alignment approaches:
statistical approaches and heuristic approaches.
Statistical approaches, which depend on a set of
unknown parameters that are learned from training
data, try to describe the relationship between a bilin-
gual sentence pair (Brown et al, 1993; Vogel and
Ney, 1996). Heuristic approaches obtain word align-
ments by using various similarity functions between
the types of the two languages (Smadja et al, 1996;
Ker and Chang, 1997; Melamed, 2000). The cen-
tral distinction between statistical and heuristic ap-
proaches is that statistical approaches are based on
well-founded probabilistic models while heuristic
ones are not. Studies reveal that statistical alignment
models outperform the simple Dice coefficient (Och
and Ney, 2003).
Finding word alignments between parallel texts,
however, is still far from a trivial work due to the di-
versity of natural languages. For example, the align-
ment of words within idiomatic expressions, free
translations, and missing content or function words
is problematic. When two languages widely differ
in word order, finding word alignments is especially
hard. Therefore, it is necessary to incorporate all
useful linguistic information to alleviate these prob-
lems.
Tiedemann (2003) introduced a word alignment
approach based on combination of association clues.
Clues combination is done by disjunction of single
clues, which are defined as probabilities of associa-
tions. The crucial assumption of clue combination
that clues are independent of each other, however,
is not always true. Och and Ney (2003) proposed
Model 6, a log-linear combination of IBM transla-
tion models and HMM model. Although Model 6
yields better results than naive IBM models, it fails
to include dependencies other than IBM models and
HMM model. Cherry and Lin (2003) developed a
459
statistical model to find word alignments, which al-
low easy integration of context-specific features.
Log-linear models, which are very suitable to in-
corporate additional dependencies, have been suc-
cessfully applied to statistical machine translation
(Och and Ney, 2002). In this paper, we present a
framework for word alignment based on log-linear
models, allowing statistical models to be easily ex-
tended by incorporating additional syntactic depen-
dencies. We use IBM Model 3 alignment proba-
bilities, POS correspondence, and bilingual dictio-
nary coverage as features. Our experiments show
that log-linear models significantly outperform IBM
translation models.
We begin by describing log-linear models for
word alignment. The design of feature functions
is discussed then. Next, we present the training
method and the search algorithm for log-linear mod-
els. We will follow with our experimental results
and conclusion and close with a discussion of possi-
ble future directions.
2 Log-linear Models
Formally, we use following definition for alignment.
Given a source (?English?) sentence e = eI1 = e1,
. . . , ei, . . . , eI and a target language (?French?) sen-
tence f = fJ1 = f1, . . . , fj , . . . , fJ . We define a link
l = (i, j) to exist if ei and fj are translation (or part
of a translation) of one another. We define the null
link l = (i, 0) to exist if ei does not correspond to a
translation for any French word in f . The null link
l = (0, j) is defined similarly. An alignment a is
defined as a subset of the Cartesian product of the
word positions:
a ? {(i, j) : i = 0, . . . , I; j = 0, . . . , J} (1)
We define the alignment problem as finding the
alignment a that maximizes Pr(a | e, f ) given e and
f .
We directly model the probability Pr(a | e, f ).
An especially well-founded framework is maximum
entropy (Berger et al, 1996). In this framework, we
have a set of M feature functions hm(a, e, f), m =
1, . . . , M . For each feature function, there exists
a model parameter ?m, m = 1, . . . , M . The direct
alignment probability is given by:
Pr(a|e, f) = exp[
?M
m=1 ?mhm(a, e, f)]?
a? exp[
?M
m=1 ?mhm(a?, e, f)](2)
This approach has been suggested by (Papineni et
al., 1997) for a natural language understanding task
and successfully applied to statistical machine trans-
lation by (Och and Ney, 2002).
We obtain the following decision rule:
a? = argmax
a
{ M?
m=1
?mhm(a, e, f)
}
(3)
Typically, the source language sentence e and the
target sentence f are the fundamental knowledge
sources for the task of finding word alignments. Lin-
guistic data, which can be used to identify associ-
ations between lexical items are often ignored by
traditional word alignment approaches. Linguistic
tools such as part-of-speech taggers, parsers, named-
entity recognizers have become more and more ro-
bust and available for many languages by now. It
is important to make use of linguistic information
to improve alignment strategies. Treated as feature
functions, syntactic dependencies can be easily in-
corporated into log-linear models.
In order to incorporate a new dependency which
contains extra information other than the bilingual
sentence pair, we modify Eq.2 by adding a new vari-
able v:
Pr(a|e, f ,v) = exp[
?M
m=1 ?mhm(a, e, f ,v)]?
a? exp[
?M
m=1 ?mhm(a?, e, f ,v)](4)
Accordingly, we get a new decision rule:
a? = argmax
a
{ M?
m=1
?mhm(a, e, f ,v)
}
(5)
Note that our log-linear models are different from
Model 6 proposed by Och and Ney (2003), which
defines the alignment problem as finding the align-
ment a that maximizes Pr(f , a | e) given e.
3 Feature Functions
In this paper, we use IBM translation Model 3 as the
base feature of our log-linear models. In addition,
we also make use of syntactic information such as
part-of-speech tags and bilingual dictionaries.
460
3.1 IBM Translation Models
Brown et al (1993) proposed a series of statisti-
cal models of the translation process. IBM trans-
lation models try to model the translation probabil-
ity Pr(fJ1 |eI1), which describes the relationship be-
tween a source language sentence eI1 and a target
language sentence fJ1 . In statistical alignment mod-
els Pr(fJ1 , aJ1 |eI1), a ?hidden? alignment a = aJ1 is
introduced, which describes a mapping from a tar-
get position j to a source position i = aj . The
relationship between the translation model and the
alignment model is given by:
Pr(fJ1 |eI1) =
?
aJ1
Pr(fJ1 , aJ1 |eI1) (6)
Although IBM models are considered more co-
herent than heuristic models, they have two draw-
backs. First, IBM models are restricted in a way
such that each target word fj is assigned to exactly
one source word eaj . A more general way is to
model alignment as an arbitrary relation between
source and target language positions. Second, IBM
models are typically language-independent and may
fail to tackle problems occurred due to specific lan-
guages.
In this paper, we use Model 3 as our base feature
function, which is given by 1:
h(a, e, f) = Pr(fJ1 , aJ1 |eI1)
=
(
m? ?0
?0
)
p0m?2?0p1?0
l?
i=1
?i!n(?i|ei)?
m?
j=1
t(fj |eaj )d(j|aj , l,m) (7)
We distinguish between two translation directions
to use Model 3 as feature functions: treating English
as source language and French as target language or
vice versa.
3.2 POS Tags Transition Model
The first linguistic information we adopt other than
the source language sentence e and the target lan-
guage sentence f is part-of-speech tags. The use
of POS information for improving statistical align-
ment quality of the HMM-based model is described
1If there is a target word which is assigned to more than one
source words, h(a, e, f) = 0.
in (Toutanova et al, 2002). They introduce addi-
tional lexicon probability for POS tags in both lan-
guages.
In IBM models as well as HMM models, when
one needs the model to take new information into
account, one must create an extended model which
can base its parameters on the previous model. In
log-linear models, however, new information can be
easily incorporated.
We use a POS Tags Transition Model as a fea-
ture function. This feature learns POS Tags tran-
sition probabilities from held-out data (via simple
counting) and then applies the learned distributions
to the ranking of various word alignments. We
define eT = eT I1 = eT1, . . . , eTi, . . . , eTI and
fT = fT J1 = fT1, . . . , fTj , . . . , fTJ as POS tag
sequences of the sentence pair e and f . POS Tags
Transition Model is formally described as:
Pr(fT|a, eT) =
?
a
t(fTa(j)|eTa(i)) (8)
where a is an element of a, a(i) is the corresponding
source position of a and a(j) is the target position.
Hence, the feature function is:
h(a, e, f , eT, fT) =
?
a
t(fTa(j)|eTa(i)) (9)
We still distinguish between two translation direc-
tions to use POS tags Transition Model as feature
functions: treating English as source language and
French as target language or vice versa.
3.3 Bilingual Dictionary
A conventional bilingual dictionary can be consid-
ered an additional knowledge source. We could use
a feature that counts how many entries of a conven-
tional lexicon co-occur in a given alignment between
the source sentence and the target sentence. There-
fore, the weight for the provided conventional dic-
tionary can be learned. The intuition is that the con-
ventional dictionary is expected to be more reliable
than the automatically trained lexicon and therefore
should get a larger weight.
We define a bilingual dictionary as a set of entries:
D = {(e, f, conf)}. e is a source language word,
f is a target langauge word, and conf is a positive
real-valued number (usually, conf = 1.0) assigned
461
by lexicographers to evaluate the validity of the en-
try. Therefore, the feature function using a bilingual
dictionary is:
h(a, e, f ,D) =
?
a
occur(ea(i), fa(j), D) (10)
where
occur(e, f,D) =
{
conf if (e, f) occurs in D
0 else
(11)
4 Training
We use the GIS (Generalized Iterative Scaling) al-
gorithm (Darroch and Ratcliff, 1972) to train the
model parameters ?M1 of the log-linear models ac-
cording to Eq. 4. By applying suitable transforma-
tions, the GIS algorithm is able to handle any type of
real-valued features. In practice, We use YASMET
2 written by Franz J. Och for performing training.
The renormalization needed in Eq. 4 requires a
sum over a large number of possible alignments. If
e has length l and f has length m, there are pos-
sible 2lm alignments between e and f (Brown et
al., 1993). It is unrealistic to enumerate all possi-
ble alignments when lm is very large. Hence, we
approximate this sum by sampling the space of all
possible alignments by a large set of highly proba-
ble alignments. The set of considered alignments are
also called n-best list of alignments.
We train model parameters on a development cor-
pus, which consists of hundreds of manually-aligned
bilingual sentence pairs. Using an n-best approx-
imation may result in the problem that the param-
eters trained with the GIS algorithm yield worse
alignments even on the development corpus. This
can happen because with the modified model scaling
factors the n-best list can change significantly and
can include alignments that have not been taken into
account in training. To avoid this problem, we iter-
atively combine n-best lists to train model parame-
ters until the resulting n-best list does not change,
as suggested by Och (2002). However, as this train-
ing procedure is based on maximum likelihood cri-
terion, there is only a loose relation to the final align-
ment quality on unseen bilingual texts. In practice,
2Available at http://www.fjoch.com/YASMET.html
having a series of model parameters when the itera-
tion ends, we select the model parameters that yield
best alignments on the development corpus.
After the bilingual sentences in the develop-
ment corpus are tokenized (or segmented) and POS
tagged, they can be used to train POS tags transition
probabilities by counting relative frequencies:
p(fT |eT ) = NA(fT, eT )N(eT )
Here, NA(fT, eT ) is the frequency that the POS tag
fT is aligned to POS tag eT and N(eT ) is the fre-
quency of eT in the development corpus.
5 Search
We use a greedy search algorithm to search the
alignment with highest probability in the space of all
possible alignments. A state in this space is a partial
alignment. A transition is defined as the addition of
a single link to the current state. Our start state is
the empty alignment, where all words in e and f are
assigned to null. A terminal state is a state in which
no more links can be added to increase the probabil-
ity of the current alignment. Our task is to find the
terminal state with the highest probability.
We can compute gain, which is a heuristic func-
tion, instead of probability for efficiency. A gain is
defined as follows:
gain(a, l) = exp[
?M
m=1 ?mhm(a ? l, e, f)]
exp[?Mm=1 ?mhm(a, e, f)]
(12)
where l = (i, j) is a link added to a.
The greedy search algorithm for general log-
linear models is formally described as follows:
Input: e, f , eT, fT, and D
Output: a
1. Start with a = ?.
2. Do for each l = (i, j) and l /? a:
Compute gain(a, l)
3. Terminate if ?l, gain(a, l) ? 1.
4. Add the link l? with the maximal gain(a, l)
to a.
5. Goto 2.
462
The above search algorithm, however, is not effi-
cient for our log-linear models. It is time-consuming
for each feature to figure out a probability when
adding a new link, especially when the sentences
are very long. For our models, gain(a, l) can be
obtained in a more efficient way 3:
gain(a, l) =
M?
m=1
?mlog
(hm(a ? l, e, f)
hm(a, e, f)
)
(13)
Note that we restrict that h(a, e, f) ? 0 for all fea-
ture functions.
The original terminational condition for greedy
search algorithm is:
gain(a, l) = exp[
?M
m=1 ?mhm(a ? l, e, f)]
exp[?Mm=1 ?mhm(a, e, f)]
? 1.0
That is:
M?
m=1
?m[hm(a ? l, e, f)? hm(a, e, f)] ? 0.0
By introducing gain threshold t, we obtain a new
terminational condition:
M?
m=1
?mlog
(hm(a ? l, e, f)
hm(a, e, f)
)
? t
where
t =
M?
m=1
?m
{
log
(hm(a ? l, e, f)
hm(a, e, f)
)
?[hm(a ? l, e, f)? hm(a, e, f)]
}
Note that we restrict h(a, e, f) ? 0 for all feature
functions. Gain threshold t is a real-valued number,
which can be optimized on the development corpus.
Therefore, we have a new search algorithm:
Input: e, f , eT, fT, D and t
Output: a
1. Start with a = ?.
2. Do for each l = (i, j) and l /? a:
Compute gain(a, l)
3We still call the new heuristic function gain to reduce no-
tational overhead, although the gain in Eq. 13 is not equivalent
to the one in Eq. 12.
3. Terminate if ?l, gain(a, l) ? t.
4. Add the link l? with the maximal gain(a, l)
to a.
5. Goto 2.
The gain threshold t depends on the added link
l. We remove this dependency for simplicity when
using it in search algorithm by treating it as a fixed
real-valued number.
6 Experimental Results
We present in this section results of experiments on
a parallel corpus of Chinese-English texts. Statis-
tics for the corpus are shown in Table 1. We use a
training corpus, which is used to train IBM transla-
tion models, a bilingual dictionary, a development
corpus, and a test corpus.
Chinese English
Train Sentences 108 925
Words 3 784 106 3 862 637
Vocabulary 49 962 55 698
Dict Entries 415 753
Vocabulary 206 616 203 497
Dev Sentences 435
Words 11 462 14 252
Ave. SentLen 26.35 32.76
Test Sentences 500
Words 13 891 15 291
Ave. SentLen 27.78 30.58
Table 1. Statistics of training corpus (Train), bilin-
gual dictionary (Dict), development corpus (Dev),
and test corpus (Test).
The Chinese sentences in both the development
and test corpus are segmented and POS tagged by
ICTCLAS (Zhang et al, 2003). The English sen-
tences are tokenized by a simple tokenizer of ours
and POS tagged by a rule-based tagger written by
Eric Brill (Brill, 1995). We manually aligned 935
sentences, in which we selected 500 sentences as
test corpus. The remaining 435 sentences are used
as development corpus to train POS tags transition
probabilities and to optimize the model parameters
and gain threshold.
Provided with human-annotated word-level align-
ment, we use precision, recall and AER (Och and
463
Size of Training Corpus
1K 5K 9K 39K 109K
Model 3 E ? C 0.4497 0.4081 0.4009 0.3791 0.3745
Model 3 C ? E 0.4688 0.4261 0.4221 0.3856 0.3469
Intersection 0.4588 0.4106 0.4044 0.3823 0.3687
Union 0.4596 0.4210 0.4157 0.3824 0.3703
Refined Method 0.4154 0.3586 0.3499 0.3153 0.3068
Model 3 E ? C 0.4490 0.3987 0.3834 0.3639 0.3533
+ Model 3 C ? E 0.3970 0.3317 0.3217 0.2949 0.2850
+ POS E ? C 0.3828 0.3182 0.3082 0.2838 0.2739
+ POS C ? E 0.3795 0.3160 0.3032 0.2821 0.2726
+ Dict 0.3650 0.3092 0.2982 0.2738 0.2685
Table 2. Comparison of AER for results of using IBM Model 3 (GIZA++) and log-linear models.
Ney, 2003) for scoring the viterbi alignments of each
model against gold-standard annotated alignments:
precision = |A ? P ||A|
recall = |A ? S||S|
AER = 1? |A ? S|+ |A ? P ||A|+ |S|
where A is the set of word pairs aligned by word
alignment systems, S is the set marked in the gold
standard as ?sure? and P is the set marked as ?pos-
sible? (including the ?sure? pairs). In our Chinese-
English corpus, only one type of alignment was
marked, meaning that S = P .
In the following, we present the results of log-
linear models for word alignment. We used GIZA++
package (Och and Ney, 2003) to train IBM transla-
tion models. The training scheme is 15H535, which
means that Model 1 are trained for five iterations,
HMM model for five iterations and finally Model
3 for five iterations. Except for changing the iter-
ations for each model, we use default configuration
of GIZA++. After that, we used three types of meth-
ods for performing a symmetrization of IBM mod-
els: intersection, union, and refined methods (Och
and Ney , 2003).
The base feature of our log-linear models, IBM
Model 3, takes the parameters generated by GIZA++
as parameters for itself. In other words, our log-
linear models share GIZA++ with the same parame-
ters apart from POS transition probability table and
bilingual dictionary.
Table 2 compares the results of our log-linear
models with IBM Model 3. From row 3 to row 7
are results obtained by IBM Model 3. From row 8
to row 12 are results obtained by log-linear models.
As shown in Table 2, our log-linear models
achieve better results than IBM Model 3 in all train-
ing corpus sizes. Considering Model 3 E ? C of
GIZA++ and ours alone, greedy search algorithm
described in Section 5 yields surprisingly better
alignments than hillclimbing algorithm in GIZA++.
Table 3 compares the results of log-linear mod-
els with IBM Model 5. The training scheme is
15H5354555. Our log-linear models still make use
of the parameters generated by GIZA++.
Comparing Table 3 with Table 2, we notice that
our log-linear models yield slightly better align-
ments by employing parameters generated by the
training scheme 15H5354555 rather than 15H535,
which can be attributed to improvement of param-
eters after further Model 4 and Model 5 training.
For log-linear models, POS information and an
additional dictionary are used, which is not the case
for GIZA++/IBM models. However, treated as a
method for performing symmetrization, log-linear
combination alone yields better results than intersec-
tion, union, and refined methods.
Figure 1 shows how gain threshold has an effect
on precision, recall and AER with fixed model scal-
ing factors.
Figure 2 shows the effect of number of features
464
Size of Training Corpus
1K 5K 9K 39K 109K
Model 5 E ? C 0.4384 0.3934 0.3853 0.3573 0.3429
Model 5 C ? E 0.4564 0.4067 0.3900 0.3423 0.3239
Intersection 0.4432 0.3916 0.3798 0.3466 0.3267
Union 0.4499 0.4051 0.3923 0.3516 0.3375
Refined Method 0.4106 0.3446 0.3262 0.2878 0.2748
Model 3 E ? C 0.4372 0.3873 0.3724 0.3456 0.3334
+ Model 3 C ? E 0.3920 0.3269 0.3167 0.2842 0.2727
+ POS E ? C 0.3807 0.3122 0.3039 0.2732 0.2667
+ POS C ? E 0.3731 0.3091 0.3017 0.2722 0.2657
+ Dict 0.3612 0.3046 0.2943 0.2658 0.2625
Table 3. Comparison of AER for results of using IBM Model 5 (GIZA++) and log-linear models.
-12 -10 -8 -6 -4 -2 0 2 4 6 8 10
0.0
0.2
0.4
0.6
0.8
1.0
gain threshold
 Precision
 Recall
 AER
Figure 1. Precision, recall and AER over different
gain thresholds with the same model scaling factors.
and size of training corpus on search efficiency for
log-linear models.
Table 4 shows the resulting normalized model
scaling factors. We see that adding new features also
has an effect on the other model scaling factors.
7 Conclusion
We have presented a framework for word alignment
based on log-linear models between parallel texts. It
allows statistical models easily extended by incor-
porating syntactic information. We take IBM Model
3 as base feature and use syntactic information such
as POS tags and bilingual dictionary. Experimental
1k 5k 9k 39k 109k
200
400
600
800
1000
1200
t
i
m
e
 
c
o
n
s
u
m
e
d
 
f
o
r
 
s
e
a
r
c
h
i
n
g
 
(
s
e
c
o
n
d
)
size of training corpus
 M3EC
 M3EC + M3CE
 M3EC + M3CE + POSEC
 M3EC + M3CE + POSEC + POSCE
 M3EC + M3CE + POSEC + POSCE + Dict
Figure 2. Effect of number of features and size of
training corpus on search efficiency.
MEC +MCE +PEC +PCE +Dict
?1 1.000 0.466 0.291 0.202 0.151
?2 - 0.534 0.312 0.212 0.167
?3 - - 0.397 0.270 0.257
?4 - - - 0.316 0.306
?5 - - - - 0.119
Table 4. Resulting model scaling factors: ?1: Model
3 E ? C (MEC); ?2: Model 3 C ? E (MCE); ?3:
POS E ? C (PEC); ?4: POS C ? E (PCE); ?5: Dict
(normalized such that ?5m=1 ?m = 1).
results show that log-linear models for word align-
ment significantly outperform IBM translation mod-
els. However, the search algorithm we proposed is
465
supervised, relying on a hand-aligned bilingual cor-
pus, while the baseline approach of IBM alignments
is unsupervised.
Currently, we only employ three types of knowl-
edge sources as feature functions. Syntax-based
translation models, such as tree-to-string model (Ya-
mada and Knight, 2001) and tree-to-tree model
(Gildea, 2003), may be very suitable to be added into
log-linear models.
It is promising to optimize the model parameters
directly with respect to AER as suggested in statisti-
cal machine translation (Och, 2003).
Acknowledgement
This work is supported by National High Technol-
ogy Research and Development Program contract
?Generally Technical Research and Basic Database
Establishment of Chinese Platform? (Subject No.
2004AA114010).
References
Adam L. Berger, Stephen A. Della Pietra, and Vincent J.
DellaPietra. 1996. A maximum entropy approach to
natural language processing. Computational Linguis-
tics, 22(1):39-72, March.
Eric Brill. 1995. Transformation-based-error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4), December.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert. L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263-311.
Colin Cherry and Dekang Lin. 2003. A probability
model to improve word alignment. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), Sapporo, Japan.
J. N. Darroch and D. Ratcliff. 1972. Generalized itera-
tive scaling for log-linear models. Annals of Mathe-
matical Statistics, 43:1470-1480.
Daniel Gildea. 2003. Loosely tree-based alignment for
machine translation. In Proceedings of the 41st An-
nual Meeting of the Association for Computational
Linguistics (ACL), Sapporo, Japan.
Sue J. Ker and Jason S. Chang. 1997. A class-based ap-
proach to word alignment. Computational Linguistics,
23(2):313-343, June.
I. Dan Melamed 2000. Models of translational equiv-
alence among words. Computational Linguistics,
26(2):221-249, June.
Franz J. Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proceedings of the 40th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 295-302, Philadelphia, PA,
July.
Franz J. Och. 2002. Statistical Machine Translation:
From Single-Word Models to Alignment Templates.
Ph.D. thesis, Computer Science Department, RWTH
Aachen, Germany, October.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics (ACL), pages: 160-167, Sapporo, Japan.
Franz J. Och and Hermann Ney. 2003. A systematic
comparison of various statistical alignment models.
Computational Linguistics, 29(1):19-51, March.
Kishore A. Papineni, Salim Roukos, and Todd Ward.
1997. Feature-based language understanding. In Eu-
ropean Conf. on Speech Communication and Technol-
ogy, pages 1435-1438, Rhodes, Greece, September.
Frank Smadja, Vasileios Hatzivassiloglou, and Kathleen
R. McKeown 1996. Translating collocations for bilin-
gual lexicons: A statistical approach. Computational
Linguistics, 22(1):1-38, March.
Jo?rg Tiedemann. 2003. Combining clues for word align-
ment. In Proceedings of the 10th Conference of Euro-
pean Chapter of the ACL (EACL), Budapest, Hungary,
April.
Kristina Toutanova, H. Tolga Ilhan, and Christopher D.
Manning. 2003. Extensions to HMM-based statistical
word alignment models. In Proceedings of Empirical
Methods in Natural Langauge Processing, Philadel-
phia, PA.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. HMM-based word alignment in statistical trans-
lation. In Proceedings of the 16th Int. Conf. on Com-
putational Linguistics, pages 836-841, Copenhagen,
Denmark, August.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical machine translation model. In Pro-
ceedings of the 39th Annual Meeting of the Association
for Computational Linguistics (ACL), pages: 523-530,
Toulouse, France, July.
Huaping Zhang, Hongkui Yu, Deyi Xiong, and Qun Liu.
2003. HHMM-based Chinese lexical analyzer ICT-
CLAS. In Proceedings of the second SigHan Work-
shop affiliated with 41th ACL, pages: 184-187, Sap-
poro, Japan.
466
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521?528,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Maximum Entropy Based Phrase Reordering
Model for Statistical Machine Translation
Deyi Xiong
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
Graduate School of Chinese Academy of Sciences
dyxiong@ict.ac.cn
Qun Liu and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{liuqun, sxlin}@ict.ac.cn
Abstract
We propose a novel reordering model for
phrase-based statistical machine transla-
tion (SMT) that uses a maximum entropy
(MaxEnt) model to predicate reorderings
of neighbor blocks (phrase pairs). The
model provides content-dependent, hier-
archical phrasal reordering with general-
ization based on features automatically
learned from a real-world bitext. We
present an algorithm to extract all reorder-
ing events of neighbor blocks from bilin-
gual data. In our experiments on Chinese-
to-English translation, this MaxEnt-based
reordering model obtains significant im-
provements in BLEU score on the NIST
MT-05 and IWSLT-04 tasks.
1 Introduction
Phrase reordering is of great importance for
phrase-based SMT systems and becoming an ac-
tive area of research recently. Compared with
word-based SMT systems, phrase-based systems
can easily address reorderings of words within
phrases. However, at the phrase level, reordering
is still a computationally expensive problem just
like reordering at the word level (Knight, 1999).
Many systems use very simple models to re-
order phrases 1. One is distortion model (Och
and Ney, 2004; Koehn et al, 2003) which penal-
izes translations according to their jump distance
instead of their content. For example, if N words
are skipped, a penalty of N will be paid regard-
less of which words are reordered. This model
takes the risk of penalizing long distance jumps
1In this paper, we focus our discussions on phrases that
are not necessarily aligned to syntactic constituent boundary.
which are common between two languages with
very different orders. Another simple model is flat
reordering model (Wu, 1996; Zens et al, 2004;
Kumar et al, 2005) which is not content depen-
dent either. Flat model assigns constant probabili-
ties for monotone order and non-monotone order.
The two probabilities can be set to prefer mono-
tone or non-monotone orientations depending on
the language pairs.
In view of content-independency of the dis-
tortion and flat reordering models, several re-
searchers (Och et al, 2004; Tillmann, 2004; Ku-
mar et al, 2005; Koehn et al, 2005) proposed a
more powerful model called lexicalized reorder-
ing model that is phrase dependent. Lexicalized
reordering model learns local orientations (mono-
tone or non-monotone) with probabilities for each
bilingual phrase from training data. During de-
coding, the model attempts to finding a Viterbi lo-
cal orientation sequence. Performance gains have
been reported for systems with lexicalized reorder-
ing model. However, since reorderings are re-
lated to concrete phrases, researchers have to de-
sign their systems carefully in order not to cause
other problems, e.g. the data sparseness problem.
Another smart reordering model was proposed
by Chiang (2005). In his approach, phrases are re-
organized into hierarchical ones by reducing sub-
phrases to variables. This template-based scheme
not only captures the reorderings of phrases, but
also integrates some phrasal generalizations into
the global model.
In this paper, we propose a novel solution for
phrasal reordering. Here, under the ITG constraint
(Wu, 1997; Zens et al, 2004), we need to con-
sider just two kinds of reorderings, straight and
inverted between two consecutive blocks. There-
fore reordering can be modelled as a problem of
521
classification with only two labels, straight and
inverted. In this paper, we build a maximum en-
tropy based classification model as the reordering
model. Different from lexicalized reordering, we
do not use the whole block as reordering evidence,
but only features extracted from blocks. This is
more flexible. It makes our model reorder any
blocks, observed in training or not. The whole
maximum entropy based reordering model is em-
bedded inside a log-linear phrase-based model of
translation. Following the Bracketing Transduc-
tion Grammar (BTG) (Wu, 1996), we built a
CKY-style decoder for our system, which makes
it possible to reorder phrases hierarchically.
To create a maximum entropy based reordering
model, the first step is learning reordering exam-
ples from training data, similar to the lexicalized
reordering model. But in our way, any evidences
of reorderings will be extracted, not limited to re-
orderings of bilingual phrases of length less than a
predefined number of words. Secondly, features
will be extracted from reordering examples ac-
cording to feature templates. Finally, a maximum
entropy classifier will be trained on the features.
In this paper we describe our system and the
MaxEnt-based reordering model with the associ-
ated algorithm. We also present experiments that
indicate that the MaxEnt-based reordering model
improves translation significantly compared with
other reordering approaches and a state-of-the-art
distortion-based system (Koehn, 2004).
2 System Overview
2.1 Model
Under the BTG scheme, translation is more
like monolingual parsing through derivations.
Throughout the translation procedure, three rules
are used to derive the translation
A [ ]? (A1, A2) (1)
A ? ?? (A1, A2) (2)
A ? (x, y) (3)
During decoding, the source sentence is seg-
mented into a sequence of phrases as in a standard
phrase-based model. Then the lexical rule (3) 2 is
2Currently, we restrict phrases x and y not to be null.
Therefore neither deletion nor insertion is carried out during
decoding. However, these operations are to be considered in
our future version of model.
used to translate source phrase y into target phrase
x and generate a block A. Later, the straight rule
(1) merges two consecutive blocks into a single
larger block in the straight order; while the in-
verted rule (2) merges them in the inverted order.
These two merging rules will be used continuously
until the whole source sentence is covered. When
the translation is finished, a tree indicating the hi-
erarchical segmentation of the source sentence is
also produced.
In the following, we will define the model in
a straight way, not in the dynamic programming
recursion way used by (Wu, 1996; Zens et al,
2004). We focus on defining the probabilities of
different rules by separating different features (in-
cluding the language model) out from the rule
probabilities and organizing them in a log-linear
form. This straight way makes it clear how rules
are used and what they depend on.
For the two merging rules straight and inverted,
applying them on two consecutive blocks A1 and
A2 is assigned a probability Prm(A)
Prm(A) = ??? ? 4?LMpLM (A1,A2) (4)
where the ? is the reordering score of block A1
and A2, ?? is its weight, and 4pLM (A1,A2) is the
increment of the language model score of the two
blocks according to their final order, ?LM is its
weight.
For the lexical rule, applying it is assigned a
probability Prl(A)
Prl(A) = p(x|y)?1 ? p(y|x)?2 ? plex(x|y)?3
?plex(y|x)?4 ? exp(1)?5 ? exp(|x|)?6
?p?LMLM (x) (5)
where p(?) are the phrase translation probabilities
in both directions, plex(?) are the lexical transla-
tion probabilities in both directions, and exp(1)
and exp(|x|) are the phrase penalty and word
penalty, respectively. These features are very com-
mon in state-of-the-art systems (Koehn et al,
2005; Chiang, 2005) and ?s are weights of fea-
tures.
For the reordering model ?, we define it on the
two consecutive blocks A1 and A2 and their order
o ? {straight, inverted}
? = f(o,A1, A2) (6)
Under this framework, different reordering mod-
els can be designed. In fact, we defined four re-
ordering models in our experiments. The first one
522
is NONE, meaning no explicit reordering features
at all. We set ? to 1 for all different pairs of
blocks and their orders. So the phrasal reorder-
ing is totally dependent on the language model.
This model is obviously different from the mono-
tone search, which does not use the inverted rule at
all. The second one is a distortion style reordering
model, which is formulated as
? =
{
exp(0), o = straight
exp(|A1|) + (|A2|), o = inverted
where |Ai| denotes the number of words on the
source side of blocks. When ?? < 0, this de-
sign will penalize those non-monotone transla-
tions. The third one is a flat reordering model,
which assigns probabilities for the straight and in-
verted order. It is formulated as
? =
{
pm, o = straight
1? pm, o = inverted
In our experiments on Chinese-English tasks, the
probability for the straight order is set at pm =
0.95. This is because word order in Chinese and
English is usually similar. The last one is the maxi-
mum entropy based reordering model proposed by
us, which will be described in the next section.
We define a derivation D as a sequence of appli-
cations of rules (1) ? (3), and let c(D) and e(D)
be the Chinese and English yields of D. The prob-
ability of a derivation D is
Pr(D) =
?
i
Pr(i) (7)
where Pr(i) is the probability of the ith applica-
tion of rules. Given an input sentence c, the final
translation e? is derived from the best derivation
D?
D? = argmax
c(D)=c
Pr(D)
e? = e(D?) (8)
2.2 Decoder
We developed a CKY style decoder that employs a
beam search algorithm, similar to the one by Chi-
ang (2005). The decoder finds the best derivation
that generates the input sentence and its transla-
tion. From the best derivation, the best English e?
is produced.
Given a source sentence c, firstly we initiate the
chart with phrases from phrase translation table
by applying the lexical rule. Then for each cell
that spans from i to j on the source side, all pos-
sible derivations spanning from i to j are gener-
ated. Our algorithm guarantees that any sub-cells
within (i, j) have been expanded before cell (i, j)
is expanded. Therefore the way to generate deriva-
tions in cell (i, j) is to merge derivations from
any two neighbor sub-cells. This combination is
done by applying the straight and inverted rules.
Each application of these two rules will generate
a new derivation covering cell (i, j). The score of
the new generated derivation is derived from the
scores of its two sub-derivations, reordering model
score and the increment of the language model
score according to the Equation (4). When the
whole input sentence is covered, the decoding is
over.
Pruning of the search space is very important for
the decoder. We use three pruning ways. The first
one is recombination. When two derivations in
the same cell have the same w leftmost/rightmost
words on the English yields, where w depends on
the order of the language model, they will be re-
combined by discarding the derivation with lower
score. The second one is the threshold pruning
which discards derivations that have a score worse
than ? times the best score in the same cell. The
last one is the histogram pruning which only keeps
the top n best derivations for each cell. In all our
experiments, we set n = 40, ? = 0.5 to get a
tradeoff between speed and performance in the de-
velopment set.
Another feature of our decoder is the k-best list
generation. The k-best list is very important for
the minimum error rate training (Och, 2003a)
which is used for tuning the weights ? for our
model. We use a very lazy algorithm for the k-best
list generation, which runs two phases similarly to
the one by Huang et al (2005). In the first phase,
the decoder runs as usual except that it keeps some
information of weaker derivations which are to be
discarded during recombination. This will gener-
ate not only the first-best of final derivation but
also a shared forest. In the second phase, the
lazy algorithm runs recursively on the shared for-
est. It finds the second-best of the final deriva-
tion, which makes its children to find their second-
best, and children?s children?s second-best, until
the leaf node?s second-best. Then it finds the third-
best, forth-best, and so on. In all our experiments,
we set k = 200.
523
The decoder is implemented in C++. Using the
pruning settings described above, without the k-
best list generation, it takes about 6 seconds to
translate a sentence of average length 28.3 words
on a 2GHz Linux system with 4G RAM memory.
3 Maximum Entropy Based Reordering
Model
In this section, we discuss how to create a max-
imum entropy based reordering model. As de-
scribed above, we defined the reordering model ?
on the three factors: order o, block A1 and block
A2. The central problem is, given two neighbor
blocks A1 and A2, how to predicate their order
o ? {straight, inverted}. This is a typical prob-
lem of two-class classification. To be consistent
with the whole model, the conditional probabil-
ity p(o|A1, A2) is calculated. A simple way to
compute this probability is to take counts from the
training data and then to use the maximum likeli-
hood estimate (MLE)
p(o|A1, A2) = Count(o,A
1, A2)
Count(A1, A2) (9)
The similar way is used by lexicalized reordering
model. However, in our model this way can?t work
because blocks become larger and larger due to us-
ing the merging rules, and finally unseen in the
training data. This means we can not use blocks
as direct reordering evidences.
A good way to this problem is to use features of
blocks as reordering evidences. Good features can
not only capture reorderings, avoid sparseness, but
also integrate generalizations. It is very straight
to use maximum entropy model to integrate fea-
tures to predicate reorderings of blocks. Under the
MaxEnt model, we have
? = p?(o|A1, A2) = exp(
?
i ?ihi(o,A1, A2))?
o exp(
?
i ?ihi(o,A1, A2))(10)
where the functions hi ? {0, 1} are model features
and the ?i are weights of the model features which
can be trained by different algorithms (Malouf,
2002).
3.1 Reordering Example Extraction
Algorithm
The input for the algorithm is a bilingual corpus
with high-precision word alignments. We obtain
the word alignments using the way of Koehn et al
(2005). After running GIZA++ (Och and Ney,
target
source
b1
b2
b3
b4
c1
c2
Figure 1: The bold dots are corners. The ar-
rows from the corners are their links. Corner c1 is
shared by block b1 and b2, which in turn are linked
by the STRAIGHT links, bottomleft and topright
of c1. Similarly, block b3 and b4 are linked by the
INVERTED links, topleft and bottomright of c2.
2000) in both directions, we apply the ?grow-
diag-final? refinement rule on the intersection
alignments for each sentence pair.
Before we introduce this algorithm, we intro-
duce some formal definitions. The first one is
block which is a pair of source and target contigu-
ous sequences of words
b = (si2i1 , t
j2
j1)
b must be consistent with the word alignment M
?(i, j) ? M, i1 ? i ? i2 ? j1 ? j ? j2
This definition is similar to that of bilingual phrase
except that there is no length limitation over block.
A reordering example is a triple of (o, b1, b2)
where b1 and b2 are two neighbor blocks and o
is the order between them. We define each vertex
of block as corner. Each corner has four links in
four directions: topright, topleft, bottomright, bot-
tomleft, and each link links a set of blocks which
have the corner as their vertex. The topright and
bottomleft link blocks with the straight order, so
we call them STRAIGHT links. Similarly, we call
the topleft and bottomright INVERTED links since
they link blocks with the inverted order. For con-
venience, we use b ?? L to denote that block b
is linked by the link L. Note that the STRAIGHT
links can not coexist with the INVERTED links.
These definitions are illustrated in Figure 1.
The reordering example extraction algorithm is
shown in Figure 2. The basic idea behind this al-
gorithm is to register all neighbor blocks to the
associated links of corners which are shared by
them. To do this, we keep an array to record link
524
1: Input: sentence pair (s, t) and their alignment M
2: < := ?
3: for each span (i1, i2) ? s do
4: find block b = (si2i1 , t
j2
j1) that is consistent with M
5: Extend block b on the target boundary with one possi-
ble non-aligned word to get blocks E(b)
6: for each block b? ? b?E(b) do
7: Register b? to the links of four corners of it
8: end for
9: end for
10: for each corner C in the matrix M do
11: if STRAIGHT links exist then
12: < := <?{(straight, b1, b2)},
b1 ?? C.bottomleft, b2 ?? C.topright
13: else if INVERTED links exist then
14: < := <?{(inverted, b1, b2)},
b1 ?? C.topleft, b2 ?? C.bottomright
15: end if
16: end for
17: Output: reordering examples <
Figure 2: Reordering Example Extraction Algo-
rithm.
information of corners when extracting blocks.
Line 4 and 5 are similar to the phrase extraction
algorithm by Och (2003b). Different from Och,
we just extend one word which is aligned to null
on the boundary of target side. If we put some
length limitation over the extracted blocks and out-
put them, we get bilingual phrases used in standard
phrase-based SMT systems and also in our sys-
tem. Line 7 updates all links associated with the
current block. You can attach the current block
to each of these links. However this will increase
reordering examples greatly, especially those with
the straight order. In our Experiments, we just at-
tach the smallest blocks to the STRAIGHT links,
and the largest blocks to the INVERTED links.
This will keep the number of reordering examples
acceptable but without performance degradation.
Line 12 and 14 extract reordering examples.
3.2 Features
With the extracted reordering examples, we can
obtain features for our MaxEnt-based reordering
model. We design two kinds of features, lexi-
cal features and collocation features. For a block
b = (s, t), we use s1 to denote the first word of the
source s, t1 to denote the first word of the target t.
Lexical features are defined on the single word
s1 or t1. Collocation features are defined on the
combination s1 or t1 between two blocks b1 and
b2. Three kinds of combinations are used. The first
one is source collocation, b1.s1&b2.s1. The sec-
ond is target collocation, b1.t1&b2.t1. The last one
hi(o, b1, b2) =
{ 1, b1.t1 = E1, o = O
0, otherwise
hj(o, b1, b2) =
{ 1, b1.t1 = E1, b2.t1 = E2, o = O
0, otherwise
Figure 3: MaxEnt-based reordering feature tem-
plates. The first one is a lexical feature, and the
second one is a target collocation feature, where
Ei are English words, O ? {straight, inverted}.
is block collocation, b1.s1&b1.t1 and b2.s1&b2.t1.
The templates for the lexical feature and the collo-
cation feature are shown in Figure 3.
Why do we use the first words as features?
These words are nicely at the boundary of blocks.
One of assumptions of phrase-based SMT is that
phrase cohere across two languages (Fox, 2002),
which means phrases in one language tend to be
moved together during translation. This indicates
that boundary words of blocks may keep informa-
tion for their movements/reorderings. To test this
hypothesis, we calculate the information gain ra-
tio (IGR) for boundary words as well as the whole
blocks against the order on the reordering exam-
ples extracted by the algorithm described above.
The IGR is the measure used in the decision tree
learning to select features (Quinlan, 1993). It
represents how precisely the feature predicate the
class. For feature f and class c, the IGR(f, c)
IGR(f, c) = En(c)? En(c|f)En(f) (11)
where En(?) is the entropy and En(?|?)
is the conditional entropy. To our sur-
prise, the IGR for the four boundary words
(IGR(?b1.s1, b2.s1, b1.t1, b2.t1?, order) =
0.2637) is very close to that for the two blocks
together (IGR(?b1, b2?, order) = 0.2655).
Although our reordering examples do not cover
all reordering events in the training data, this
result shows that boundary words do provide
some clues for predicating reorderings.
4 Experiments
We carried out experiments to compare against
various reordering models and systems to demon-
strate the competitiveness of MaxEnt-based re-
ordering:
1. Monotone search: the inverted rule is not
used.
525
2. Reordering variants: the NONE, distortion
and flat reordering models described in Sec-
tion 2.1.
3. Pharaoh: A state-of-the-art distortion-based
decoder (Koehn, 2004).
4.1 Corpus
Our experiments were made on two Chinese-to-
English translation tasks: NIST MT-05 (news do-
main) and IWSLT-04 (travel dialogue domain).
NIST MT-05. In this task, the bilingual train-
ing data comes from the FBIS corpus with 7.06M
Chinese words and 9.15M English words. The tri-
gram language model training data consists of En-
glish texts mostly derived from the English side
of the UN corpus (catalog number LDC2004E12),
which totally contains 81M English words. For the
efficiency of minimum error rate training, we built
our development set using sentences of length at
most 50 characters from the NIST MT-02 evalua-
tion test data.
IWSLT-04. For this task, our experiments were
carried out on the small data track. Both the
bilingual training data and the trigram language
model training data are restricted to the supplied
corpus, which contains 20k sentences, 179k Chi-
nese words and 157k English words. We used the
CSTAR 2003 test set consisting of 506 sentence
pairs as development set.
4.2 Training
We obtained high-precision word alignments us-
ing the way described in Section 3.1. Then we
ran our reordering example extraction algorithm to
output blocks of length at most 7 words on the Chi-
nese side together with their internal alignments.
We also limited the length ratio between the target
and source language (max(|s|, |t|)/min(|s|, |t|))
to 3. After extracting phrases, we calculated the
phrase translation probabilities and lexical transla-
tion probabilities in both directions for each bilin-
gual phrase.
For the minimum-error-rate training, we re-
implemented Venugopal?s trainer 3 (Venugopal
et al, 2005) in C++. For all experiments, we ran
this trainer with the decoder iteratively to tune the
weights ?s to maximize the BLEU score on the
development set.
3See http://www.cs.cmu.edu/ ashishv/mer.html. This is a
Matlab implementation.
Pharaoh
We shared the same phrase translation tables
between Pharaoh and our system since the two
systems use the same features of phrases. In fact,
we extracted more phrases than Pharaoh?s trainer
with its default settings. And we also used our re-
implemented trainer to tune lambdas of Pharaoh
to maximize its BLEU score. During decoding,
we pruned the phrase table with b = 100 (default
20), pruned the chart with n = 100, ? = 10?5
(default setting), and limited distortions to 4
(default 0).
MaxEnt-based Reordering Model
We firstly ran our reordering example extraction
algorithm on the bilingual training data without
any length limitations to obtain reordering ex-
amples and then extracted features from these
examples. In the task of NIST MT-05, we
obtained about 2.7M reordering examples with
the straight order, and 367K with the inverted
order, from which 112K lexical features and
1.7M collocation features after deleting those
with one occurrence were extracted. In the task
of IWSLT-04, we obtained 79.5k reordering
examples with the straight order, 9.3k with the
inverted order, from which 16.9K lexical features
and 89.6K collocation features after deleting those
with one occurrence were extracted. Finally, we
ran the MaxEnt toolkit by Zhang 4 to tune the
feature weights. We set iteration number to 100
and Gaussian prior to 1 for avoiding overfitting.
4.3 Results
We dropped unknown words (Koehn et al, 2005)
of translations for both tasks before evaluating
their BLEU scores. To be consistent with the
official evaluation criterions of both tasks, case-
sensitive BLEU-4 scores were computed For the
NIST MT-05 task and case-insensitive BLEU-4
scores were computed for the IWSLT-04 task 5.
Experimental results on both tasks are shown in
Table 1. Italic numbers refer to results for which
the difference to the best result (indicated in bold)
is not statistically significant. For all scores, we
also show the 95% confidence intervals computed
using Zhang?s significant tester (Zhang et al,
2004) which was modified to conform to NIST?s
4See http://homepages.inf.ed.ac.uk/s0450736
/maxent toolkit.html.
5Note that the evaluation criterion of IWSLT-04 is not to-
tally matched since we didn?t remove punctuation marks.
526
definition of the BLEU brevity penalty.
We observe that if phrasal reordering is totally
dependent on the language model (NONE) we
get the worst performance, even worse than the
monotone search. This indicates that our language
models were not strong to discriminate between
straight orders and inverted orders. The flat and
distortion reordering models (Row 3 and 4) show
similar performance with Pharaoh. Although they
are not dependent on phrases, they really reorder
phrases with penalties to wrong orders supported
by the language model and therefore outperform
the monotone search. In row 6, only lexical fea-
tures are used for the MaxEnt-based reordering
model; while row 7 uses lexical features and col-
location features. On both tasks, we observe that
various reordering approaches show similar and
stable performance ranks in different domains and
the MaxEnt-based reordering models achieve the
best performance among them. Using all features
for the MaxEnt model (lex + col) is marginally
better than using only lex features (lex).
4.4 Scaling to Large Bitexts
In the experiments described above, collocation
features do not make great contributions to the per-
formance improvement but make the total num-
ber of features increase greatly. This is a prob-
lem for MaxEnt parameter estimation if it is scaled
to large bitexts. Therefore, for the integration of
MaxEnt-based phrase reordering model in the sys-
tem trained on large bitexts, we remove colloca-
tion features and only use lexical features from
the last words of blocks (similar to those from the
first words of blocks with similar performance).
This time the bilingual training data contain 2.4M
sentence pairs (68.1M Chinese words and 73.8M
English words) and two trigram language models
are used. One is trained on the English side of
the bilingual training data. The other is trained on
the Xinhua portion of the Gigaword corpus with
181.1M words. We also use some rules to trans-
late numbers, time expressions and Chinese per-
son names. The new Bleu score on NIST MT-05
is 0.291 which is very promising.
5 Discussion and Future Work
In this paper we presented a MaxEnt-based phrase
reordering model for SMT. We used lexical fea-
tures and collocation features from boundary
words of blocks to predicate reorderings of neigh-
Systems NIST MT-05 IWSLT-04
monotone 20.1 ? 0.8 37.8 ? 3.2
NONE 19.6 ? 0.8 36.3 ? 2.9
Distortion 20.9 ? 0.8 38.8 ? 3.0
Flat 20.5 ? 0.8 38.7 ? 2.8
Pharaoh 20.8 ? 0.8 38.9 ? 3.3
MaxEnt (lex) 22.0 ? 0.8 42.4 ? 3.3
MaxEnt (lex + col) 22.2 ? 0.8 42.8 ? 3.3
Table 1: BLEU-4 scores (%) with the 95% confi-
dence intervals. Italic numbers refer to results for
which the difference to the best result (indicated in
bold) is not statistically significant.
bor blocks. Experiments on standard Chinese-
English translation tasks from two different do-
mains showed that our method achieves a signif-
icant improvement over the distortion/flat reorder-
ing models.
Traditional distortion/flat-based SMT transla-
tion systems are good for learning phrase transla-
tion pairs, but learn nothing for phrasal reorder-
ings from real-world data. This is our original
motivation for designing a new reordering model,
which can learn reorderings from training data just
like learning phrasal translations. Lexicalized re-
ordering model learns reorderings from training
data, but it binds reorderings to individual concrete
phrases, which restricts the model to reorderings
of phrases seen in training data. On the contrary,
the MaxEnt-based reordering model is not limited
by this constraint since it is based on features of
phrase, not phrase itself. It can be easily general-
ized to reorder unseen phrases provided that some
features are fired on these phrases.
Another advantage of the MaxEnt-based re-
ordering model is that it can take more fea-
tures into reordering, even though they are non-
independent. Tillmann et. al (2005) also use a
MaxEnt model to integrate various features. The
difference is that they use the MaxEnt model to
predict not only orders but also blocks. To do that,
it is necessary for the MaxEnt model to incorpo-
rate real-valued features such as the block trans-
lation probability and the language model proba-
bility. Due to the expensive computation, a local
model is built. However, our MaxEnt model is just
a module of the whole log-linear model of transla-
tion which uses its score as a real-valued feature.
The modularity afforded by this design does not
incur any computation problems, and make it eas-
527
ier to update one sub-model with other modules
unchanged.
Beyond the MaxEnt-based reordering model,
another feature deserving attention in our system
is the CKY style decoder which observes the ITG.
This is different from the work of Zens et. al.
(2004). In their approach, translation is generated
linearly, word by word and phrase by phrase in a
traditional way with respect to the incorporation
of the language model. It can be said that their de-
coder did not violate the ITG constraints but not
that it observed the ITG. The ITG not only de-
creases reorderings greatly but also makes reorder-
ing hierarchical. Hierarchical reordering is more
meaningful for languages which are organized hi-
erarchically. From this point, our decoder is simi-
lar to the work by Chiang (2005).
The future work is to investigate other valuable
features, e.g. binary features that explain blocks
from the syntactical view. We think that there is
still room for improvement if more contributing
features are used.
Acknowledgements
This work was supported in part by National High
Technology Research and Development Program
under grant #2005AA114140 and National Nat-
ural Science Foundation of China under grant
#60573188. Special thanks to Yajuan Lu? for
discussions of the manuscript of this paper and
three anonymous reviewers who provided valuable
comments.
References
Ashish Venugopal, Stephan Vogel. 2005. Considerations in
Maximum Mutual Information and Minimum Classifica-
tion Error training for Statistical Machine Translation. In
the Proceedings of EAMT-05, Budapest, Hungary May 30-
31.
Christoph Tillmann. 2004. A block orientation model for
statistical machine translation. In HLT-NAACL, Boston,
MA, USA.
Christoph Tillmann and Tong Zhang. 2005. A Localized
Prediction Model for statistical machine translation. In
Proceedings of ACL 2005, pages 557?564.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of ACL
2005, pages 263?270.
Dekai Wu. 1996. A Polynomial-Time Algorithm for Statis-
tical Machine Translation. In Proceedings of ACL 1996.
Dekai Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Computa-
tional Linguistics, 23:377?404.
Franz Josef Och and Hermann Ney. 2000. Improved statisti-
cal alignment models. In Proceedings of ACL 2000, pages
440?447.
Franz Josef Och. 2003a. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL 2003,
pages 160?167.
Franz Josef Och. 2003b. Statistical Machine Translation:
From Single-Word Models to Alignment Templates The-
sis.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation. Com-
putational Linguistics, 30:417?449.
Franz Josef Och, Ignacio Thayer, Daniel Marcu, Kevin
Knight, Dragos Stefan Munteanu, Quamrul Tipu, Michel
Galley, and Mark Hopkins. 2004. Arabic and Chinese MT
at USC/ISI. Presentation given at NIST Machine Transla-
tion Evaluation Workshop.
Heidi J. Fox. 2002. Phrasal cohesion and statistical machine
translation. In Proceedings of EMNLP 2002.
J. R. Quinlan. 1993. C4.5: progarms for machine learning.
Morgan Kaufmann Publishers.
Kevin Knight. 1999. Decoding complexity in wordreplace-
ment translation models. Computational Linguistics,
Squibs & Discussion, 25(4).
Liang Huang and David Chiang. 2005. Better k-best parsing.
In Proceedings of the Ninth International Workshop on
Parsing Technology, Vancouver, October, pages 53?64.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003.
Statistical Phrase-Based Translation. In Proceedings of
HLT/NAACL.
Philipp Koehn. 2004. Pharaoh: a beam search decoder for
phrase-based statistical machine translation models. In
Proceedings of the Sixth Conference of the Association for
Machine Translation in the Americas, pages 115?124.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,
Chris Callison-Burch, Miles Osborne and David Talbot.
2005. Edinburgh System Description for the 2005 IWSLT
Speech Translation Evaluation. In International Work-
shop on Spoken Language Translation.
R. Zens, H. Ney, T. Watanabe, and E. Sumita. 2004. Re-
ordering Constraints for Phrase-Based Statistical Machine
Translation. In Proceedings of CoLing 2004, Geneva,
Switzerland, pp. 205-211.
Robert Malouf. 2002. A comparison of algorithms for maxi-
mum entropy parameter estimation. In Proceedings of the
Sixth Conference on Natural Language Learning (CoNLL-
2002).
Shankar Kumar and William Byrne. 2005. Local phrase
reordering models for statistical machine translation. In
Proceedings of HLT-EMNLP.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do
we need to have a better system? In Proceedings of LREC
2004, pages 2051? 2054.
528
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 609?616,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Tree-to-String Alignment Template for Statistical Machine Translation
Yang Liu , Qun Liu , and Shouxun Lin
Institute of Computing Technology
Chinese Academy of Sciences
No.6 Kexueyuan South Road, Haidian District
P. O. Box 2704, Beijing, 100080, China
{yliu,liuqun,sxlin}@ict.ac.cn
Abstract
We present a novel translation model
based on tree-to-string alignment template
(TAT) which describes the alignment be-
tween a source parse tree and a target
string. A TAT is capable of generating
both terminals and non-terminals and per-
forming reordering at both low and high
levels. The model is linguistically syntax-
based because TATs are extracted auto-
matically from word-aligned, source side
parsed parallel texts. To translate a source
sentence, we first employ a parser to pro-
duce a source parse tree and then ap-
ply TATs to transform the tree into a tar-
get string. Our experiments show that
the TAT-based model significantly outper-
forms Pharaoh, a state-of-the-art decoder
for phrase-based models.
1 Introduction
Phrase-based translation models (Marcu and
Wong, 2002; Koehn et al, 2003; Och and Ney,
2004), which go beyond the original IBM trans-
lation models (Brown et al, 1993) 1 by model-
ing translations of phrases rather than individual
words, have been suggested to be the state-of-the-
art in statistical machine translation by empirical
evaluations.
In phrase-based models, phrases are usually
strings of adjacent words instead of syntactic con-
stituents, excelling at capturing local reordering
and performing translations that are localized to
1The mathematical notation we use in this paper is taken
from that paper: a source string fJ1 = f1, . . . , fj , . . . , fJ is
to be translated into a target string eI1 = e1, . . . , ei, . . . , eI .
Here, I is the length of the target string, and J is the length
of the source string.
substrings that are common enough to be observed
on training data. However, a key limitation of
phrase-based models is that they fail to model re-
ordering at the phrase level robustly. Typically,
phrase reordering is modeled in terms of offset po-
sitions at the word level (Koehn, 2004; Och and
Ney, 2004), making little or no direct use of syn-
tactic information.
Recent research on statistical machine transla-
tion has lead to the development of syntax-based
models. Wu (1997) proposes Inversion Trans-
duction Grammars, treating translation as a pro-
cess of parallel parsing of the source and tar-
get language via a synchronized grammar. Al-
shawi et al (2000) represent each production in
parallel dependency tree as a finite transducer.
Melamed (2004) formalizes machine translation
problem as synchronous parsing based on multi-
text grammars. Graehl and Knight (2004) describe
training and decoding algorithms for both gen-
eralized tree-to-tree and tree-to-string transduc-
ers. Chiang (2005) presents a hierarchical phrase-
based model that uses hierarchical phrase pairs,
which are formally productions of a synchronous
context-free grammar. Ding and Palmer (2005)
propose a syntax-based translation model based
on a probabilistic synchronous dependency in-
sert grammar, a version of synchronous gram-
mars defined on dependency trees. All these ap-
proaches, though different in formalism, make use
of synchronous grammars or tree-based transduc-
tion rules to model both source and target lan-
guages.
Another class of approaches make use of syn-
tactic information in the target language alone,
treating the translation problem as a parsing prob-
lem. Yamada and Knight (2001) use a parser in
the target language to train probabilities on a set of
609
operations that transform a target parse tree into a
source string.
Paying more attention to source language anal-
ysis, Quirk et al (2005) employ a source language
dependency parser, a target language word seg-
mentation component, and an unsupervised word
alignment component to learn treelet translations
from parallel corpus.
In this paper, we propose a statistical translation
model based on tree-to-string alignment template
which describes the alignment between a source
parse tree and a target string. A TAT is capa-
ble of generating both terminals and non-terminals
and performing reordering at both low and high
levels. The model is linguistically syntax-based
because TATs are extracted automatically from
word-aligned, source side parsed parallel texts.
To translate a source sentence, we first employ a
parser to produce a source parse tree and then ap-
ply TATs to transform the tree into a target string.
One advantage of our model is that TATs can
be automatically acquired to capture linguistically
motivated reordering at both low (word) and high
(phrase, clause) levels. In addition, the training of
TAT-based model is less computationally expen-
sive than tree-to-tree models. Similarly to (Galley
et al, 2004), the tree-to-string alignment templates
discussed in this paper are actually transformation
rules. The major difference is that we model the
syntax of the source language instead of the target
side. As a result, the task of our decoder is to find
the best target string while Galley?s is to seek the
most likely target tree.
2 Tree-to-String Alignment Template
A tree-to-string alignment template z is a triple
?T? , S?, A??, which describes the alignment A? be-
tween a source parse tree T? = T (F J ?1 ) 2 and
a target string S? = EI?1 . A source string F J
?
1 ,
which is the sequence of leaf nodes of T (F J ?1 ),
consists of both terminals (source words) and non-
terminals (phrasal categories). A target string EI?1
is also composed of both terminals (target words)
and non-terminals (placeholders). An alignment
A? is defined as a subset of the Cartesian product
of source and target symbol positions:
A? ? {(j, i) : j = 1, . . . , J ?; i = 1, . . . , I ?} (1)
2We use T (?) to denote a parse tree. To reduce notational
overhead, we use T (z) to represent the parse tree in z. Simi-
larly, S(z) denotes the string in z.
Figure 1 shows three TATs automatically
learned from training data. Note that when
demonstrating a TAT graphically, we represent
non-terminals in the target strings by blanks.
NP
NR
??
NN
??
LCP
NP
NR
??
CC
?
NR
LC
?
NP
DNP
NP DEG
NP
President Bush
between United States and
Figure 1: Examples of tree-to-string alignment
templates obtained in training
In the following, we formally describe how to
introduce tree-to-string alignment templates into
probabilistic dependencies to model Pr(eI1|fJ1 ) 3.
In a first step, we introduce the hidden variable
T (fJ1 ) that denotes a parse tree of the source sen-
tence fJ1 :
Pr(eI1|fJ1 ) =
?
T (fJ1 )
Pr(eI1, T (fJ1 )|fJ1 ) (2)
=
?
T (fJ1 )
Pr(T (fJ1 )|fJ1 )Pr(eI1|T (fJ1 ), fJ1 ) (3)
Next, another hidden variable D is introduced
to detach the source parse tree T (fJ1 ) into a se-
quence of K subtrees T?K1 with a preorder transver-
sal. We assume that each subtree T?k produces
a target string S?k. As a result, the sequence
of subtrees T?K1 produces a sequence of target
strings S?K1 , which can be combined serially to
generate the target sentence eI1. We assume that
Pr(eI1|D,T (fJ1 ), fJ1 ) ? Pr(S?K1 |T?K1 ) because eI1
is actually generated by the derivation of S?K1 .
Note that we omit an explicit dependence on the
detachment D to avoid notational overhead.
Pr(eI1|T (fJ1 ), fJ1 ) =
?
D
Pr(eI1, D|T (fJ1 ), fJ1 ) (4)
=
?
D
Pr(D|T (fJ1 ), fJ1 )Pr(eI1|D,T (fJ1 ), fJ1 ) (5)
=
?
D
Pr(D|T (fJ1 ), fJ1 )Pr(S?K1 |T?K1 ) (6)
=
?
D
Pr(D|T (fJ1 ), fJ1 )
K?
k=1
Pr(S?k|T?k) (7)
3The notational convention will be as follows. We use
the symbol Pr(?) to denote general probability distribution
with no specific assumptions. In contrast, for model-based
probability distributions, we use generic symbol p(?).
610
NP
DNP
NP
NR
??
DEG
?
NP
NN
??
NN
??
NP
DNP
NP DEG
?
NP
NP
NR
??
NP
NN NN
NN
??
NN
??
?? ? ?? ??
parsing
detachment production
of
China
economic development
combination
economic development of China
Figure 2: Graphic illustration for translation pro-
cess
To further decompose Pr(S?|T? ), the tree-to-
string alignment template, denoted by the variable
z, is introduced as a hidden variable.
Pr(S?|T? ) =
?
z
Pr(S?, z|T? ) (8)
=
?
z
Pr(z|T? )Pr(S?|z, T? ) (9)
Therefore, the TAT-based translation model can
be decomposed into four sub-models:
1. parse model: Pr(T (fJ1 )|fJ1 )
2. detachment model: Pr(D|T (fJ1 ), fJ1 )
3. TAT selection model: Pr(z|T? )
4. TAT application model: Pr(S?|z, T? )
Figure 2 shows how TATs work to perform
translation. First, the input source sentence is
parsed. Next, the parse tree is detached into five
subtrees with a preorder transversal. For each sub-
tree, a TAT is selected and applied to produce a
string. Finally, these strings are combined serially
to generate the translation (we use X to denote the
non-terminal):
X1 ? X2 of X3
? X2 of China
? X3 X4 of China
? economic X4 of China
? economic development of China
Following Och and Ney (2002), we base our
model on log-linear framework. Hence, all knowl-
edge sources are described as feature functions
that include the given source string fJ1 , the target
string eI1, and hidden variables. The hidden vari-
able T (fJ1 ) is omitted because we usually make
use of only single best output of a parser. As we
assume that all detachment have the same proba-
bility, the hidden variable D is also omitted. As
a result, the model we actually adopt for exper-
iments is limited because the parse, detachment,
and TAT application sub-models are simplified.
Pr(eI1, zK1 |fJ1 )
= exp[
?M
m=1 ?mhm(eI1, fJ1 , zK1 )]?
e?I1,z?K1 exp[
?M
m=1 ?mhm(e?I1, fJ1 , z?K1 )]
e?I1 = argmax
eI1,zK1
{ M?
m=1
?mhm(eI1, fJ1 , zK1 )
}
For our experiments we use the following seven
feature functions 4 that are analogous to default
feature set of Pharaoh (Koehn, 2004). To simplify
the notation, we omit the dependence on the hid-
den variables of the model.
h1(eI1, fJ1 ) = log
K?
k=1
N(z) ? ?(T (z), T?k)
N(T (z))
h2(eI1, fJ1 ) = log
K?
k=1
N(z) ? ?(T (z), T?k)
N(S(z))
h3(eI1, fJ1 ) = log
K?
k=1
lex(T (z)|S(z)) ? ?(T (z), T?k)
h4(eI1, fJ1 ) = log
K?
k=1
lex(S(z)|T (z)) ? ?(T (z), T?k)
h5(eI1, fJ1 ) = K
h6(eI1, fJ1 ) = log
I?
i=1
p(ei|ei?2, ei?1)
h7(eI1, fJ1 ) = I
4When computing lexical weighting features (Koehn et
al., 2003), we take only terminals into account. If there are
no terminals, we set the feature value to 1. We use lex(?)
to denote lexical weighting. We denote the number of TATs
used for decoding by K and the length of target string by I .
611
Tree String Alignment
( NR?? ) Bush 1:1
( NN?? ) President 1:1
( VV?? ) made 1:1
( NN?? ) speech 1:1
( NP ( NR ) ( NN ) ) X1 | X2 1:2 2:1
( NP ( NR?? ) ( NN ) ) X | Bush 1:2 2:1
( NP ( NR ) ( NN?? ) ) President | X 1:2 2:1
( NP ( NR?? ) ( NN?? ) ) President | Bush 1:2 2:1
( VP ( VV ) ( NN ) ) X1 | a | X2 1:1 2:3
( VP ( VV?? ) ( NN ) ) made | a | X 1:1 2:3
( VP ( VV ) ( NN?? ) ) X | a | speech 1:1 2:3
( VP ( VV?? ) ( NN?? ) ) made | a | speech 1:1 2:3
( IP ( NP ) ( VP ) ) X1 | X2 1:1 2:2
Table 1: Examples of TATs extracted from the TSA in Figure 3 with h = 2 and c = 2
3 Training
To extract tree-to-string alignment templates from
a word-aligned, source side parsed sentence pair
?T (fJ1 ), eI1, A?, we need first identify TSAs (Tree-
String-Alignment) using similar criterion as sug-
gested in (Och and Ney, 2004). A TSA is a triple
?T (f j2j1 ), ei2i1 , A?)? that is in accordance with the
following constraints:
1. ?(i, j) ? A : i1 ? i ? i2 ? j1 ? j ? j2
2. T (f j2j1 ) is a subtree of T (fJ1 )
Given a TSA ?T (f j2j1 ), ei2i1 , A??, a triple
?T (f j4j3 ), ei4i3 , A?? is its sub TSA if and only
if:
1. T (f j4j3 ), ei4i3 , A?? is a TSA
2. T (f j4j3 ) is rooted at the direct descendant of
the root node of T (f j1j2 )
3. i1 ? i3 ? i4 ? i2
4. ?(i, j) ? A? : i3 ? i ? i4 ? j3 ? j ? j4
Basically, we extract TATs from a TSA
?T (f j2j1 ), ei2i1 , A?? using the following two rules:
1. If T (f j2j1 ) contains only one node,
then ?T (f j2j1 ), ei2i1 , A?? is a TAT
2. If the height of T (f j2j1 ) is greater than one,
then build TATs using those extracted from
sub TSAs of ?T (f j2j1 ), ei2i1 , A??.
IP
NP
NR
??
NN
??
VP
VV
??
NN
??
President Bush made a speech
Figure 3: An example of TSA
Usually, we can extract a very large amount of
TATs from training data using the above rules,
making both training and decoding very slow.
Therefore, we impose three restrictions to reduce
the magnitude of extracted TATs:
1. A third constraint is added to the definition of
TSA:
?j?, j?? : j1 ? j? ? j2 and j1 ? j?? ? j2
and (i1, j?) ? A? and (i2, j??) ? A?
This constraint requires that both the first
and last symbols in the target string must be
aligned to some source symbols.
2. The height of T (z) is limited to no greater
than h.
3. The number of direct descendants of a node
of T (z) is limited to no greater than c.
Table 1 shows the TATs extracted from the TSA
in Figure 3 with h = 2 and c = 2.
As we restrict that T (f j2j1 ) must be a subtree of
T (fJ1 ), TATs may be treated as syntactic hierar-
612
chical phrase pairs (Chiang, 2005) with tree struc-
ture on the source side. At the same time, we face
the risk of losing some useful non-syntactic phrase
pairs. For example, the phrase pair
???????? President Bush made
can never be obtained in form of TAT from the
TSA in Figure 3 because there is no subtree for
that source string.
4 Decoding
We approach the decoding problem as a bottom-up
beam search.
To translate a source sentence, we employ a
parser to produce a parse tree. Moving bottom-
up through the source parse tree, we compute a
list of candidate translations for the input subtree
rooted at each node with a postorder transversal.
Candidate translations of subtrees are placed in
stacks. Figure 4 shows the organization of can-
didate translation stacks.
NP
DNP
NP
NR
??
DEG
?
NP
NN
??
NN
??
8
4 7
2 3 5 6
1
...
1
...
2
...
3
...
4
...
5
...
6
...
7
...
8
Figure 4: Candidate translations of subtrees are
placed in stacks according to the root index set by
postorder transversal
A candidate translation contains the following
information:
1. the partial translation
2. the accumulated feature values
3. the accumulated probability
A TAT z is usable to a parse tree T if and only
if T (z) is rooted at the root of T and covers part
of nodes of T . Given a parse tree T , we find all
usable TATs. Given a usable TAT z, if T (z) is
equal to T , then S(z) is a candidate translation of
T . If T (z) covers only a portion of T , we have
to compute a list of candidate translations for T
by replacing the non-terminals of S(z) with can-
didate translations of the corresponding uncovered
subtrees.
NP
DNP
NP DEG
?
NP
8
4 7
2 3
of
...
1
...
2
...
3
...
4
...
5
...
6
...
7
...
8
Figure 5: Candidate translation construction
For example, when computing the candidate
translations for the tree rooted at node 8, the TAT
used in Figure 5 covers only a portion of the parse
tree in Figure 4. There are two uncovered sub-
trees that are rooted at node 2 and node 7 respec-
tively. Hence, we replace the third symbol with
the candidate translations in stack 2 and the first
symbol with the candidate translations in stack 7.
At the same time, the feature values and probabil-
ities are also accumulated for the new candidate
translations.
To speed up the decoder, we limit the search
space by reducing the number of TATs used for
each input node. There are two ways to limit the
TAT table size: by a fixed limit (tatTable-limit) of
how many TATs are retrieved for each input node,
and by a probability threshold (tatTable-threshold)
that specify that the TAT probability has to be
above some value. On the other hand, instead of
keeping the full list of candidates for a given node,
we keep a top-scoring subset of the candidates.
This can also be done by a fixed limit (stack-limit)
or a threshold (stack-threshold). To perform re-
combination, we combine candidate translations
that share the same leading and trailing bigrams
in each stack.
5 Experiments
Our experiments were on Chinese-to-English
translation. The training corpus consists of 31, 149
sentence pairs with 843, 256 Chinese words and
613
System Features BLEU4
d + ?(e|f) 0.0573 ? 0.0033
Pharaoh d + lm + ?(e|f) + wp 0.2019 ? 0.0083
d + lm + ?(f |e) + lex(f |e) + ?(e|f) + lex(e|f) + pp + wp 0.2089 ? 0.0089
h1 0.1639 ? 0.0077
Lynx h1 + h6 + h7 0.2100 ? 0.0089
h1 + h2 + h3 + h4 + h5 + h6 + h7 0.2178 ? 0.0080
Table 2: Comparison of Pharaoh and Lynx with different feature settings on the test corpus
949, 583 English words. For the language model,
we used SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a trigram model with modi-
fied Kneser-Ney smoothing (Chen and Goodman,
1998) on the 31, 149 English sentences. We se-
lected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development cor-
pus, and used the 2005 NIST MT Evaluation test
set as our test corpus. We evaluated the transla-
tion quality using the BLEU metric (Papineni et
al., 2002), as calculated by mteval-v11b.pl with its
default setting except that we used case-sensitive
matching of n-grams.
5.1 Pharaoh
The baseline system we used for comparison was
Pharaoh (Koehn et al, 2003; Koehn, 2004), a
freely available decoder for phrase-based transla-
tion models:
p(e|f) = p?(f |e)?? ? pLM(e)?LM ?
pD(e, f)?D ? ?
length(e)?W(e) (10)
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule ?diag-
and? described in (Koehn et al, 2003) to obtain
a single many-to-many word alignment for each
sentence pair. After that, we used some heuristics,
which including rule-based translation of num-
bers, dates, and person names, to further improve
the alignment accuracy.
Given the word-aligned bilingual corpus, we
obtained 1, 231, 959 bilingual phrases (221, 453
used on test corpus) using the training toolkits
publicly released by Philipp Koehn with its default
setting.
To perform minimum error rate training (Och,
2003) to tune the feature weights to maximize the
system?s BLEU score on development set, we used
optimizeV5IBMBLEU.m (Venugopal and Vogel,
2005). We used default pruning settings for
Pharaoh except that we set the distortion limit to
4.
5.2 Lynx
On the same word-aligned training data, it took
us about one month to parse all the 31, 149 Chi-
nese sentences using a Chinese parser written by
Deyi Xiong (Xiong et al, 2005). The parser was
trained on articles 1 ? 270 of Penn Chinese Tree-
bank version 1.0 and achieved 79.4% (F1 mea-
sure) as well as a 4.4% relative decrease in er-
ror rate. Then, we performed TAT extraction de-
scribed in section 3 with h = 3 and c = 5
and obtained 350, 575 TATs (88, 066 used on test
corpus). To run our decoder Lynx on develop-
ment and test corpus, we set tatTable-limit = 20,
tatTable-threshold = 0, stack-limit = 100, and
stack-threshold = 0.00001.
5.3 Results
Table 2 shows the results on test set using Pharaoh
and Lynx with different feature settings. The 95%
confidence intervals were computed using Zhang?s
significance tester (Zhang et al, 2004). We mod-
ified it to conform to NIST?s current definition
of the BLEU brevity penalty. For Pharaoh, eight
features were used: distortion model d, a trigram
language model lm, phrase translation probabili-
ties ?(f |e) and ?(e|f), lexical weightings lex(f |e)
and lex(e|f), phrase penalty pp, and word penalty
wp. For Lynx, seven features described in sec-
tion 2 were used. We find that Lynx outperforms
Pharaoh with all feature settings. With full fea-
tures, Lynx achieves an absolute improvement of
0.006 over Pharaoh (3.1% relative). This differ-
ence is statistically significant (p < 0.01). Note
that Lynx made use of only 88, 066 TATs on test
corpus while 221, 453 bilingual phrases were used
for Pharaoh.
The feature weights obtained by minimum er-
614
FeaturesSystem d lm ?(f |e) lex(f |e) ?(e|f) lex(e|f) pp wp
Pharaoh 0.0476 0.1386 0.0611 0.0459 0.1723 0.0223 0.3122 -0.2000
Lynx - 0.3735 0.0061 0.1081 0.1656 0.0022 0.0824 0.2620
Table 3: Feature weights obtained by minimum error rate training on the development corpus
BLEU4
tat 0.2178 ? 0.0080
tat + bp 0.2240 ? 0.0083
Table 4: Effect of using bilingual phrases for Lynx
ror rate training for both Pharaoh and Lynx are
shown in Table 3. We find that ?(f |e) (i.e. h2) is
not a helpful feature for Lynx. The reason is that
we use only a single non-terminal symbol instead
of assigning phrasal categories to the target string.
In addition, we allow the target string consists of
only non-terminals, making translation decisions
not always based on lexical evidence.
5.4 Using bilingual phrases
It is interesting to use bilingual phrases to
strengthen the TAT-based model. As we men-
tioned before, some useful non-syntactic phrase
pairs can never be obtained in form of TAT be-
cause we restrict that there must be a correspond-
ing parse tree for the source phrase. Moreover,
it takes more time to obtain TATs than bilingual
phrases on the same training data because parsing
is usually very time-consuming.
Given an input subtree T (F j2j1 ), if F
j2
j1 is a string
of terminals, we find all bilingual phrases that the
source phrase is equal to F j2j1 . Then we build a
TAT for each bilingual phrase ?fJ ?1 , eI
?
1 , A??: the
tree of the TAT is T (F j2j1 ), the string is eI
?
1 , and
the alignment is A?. If a TAT built from a bilingual
phrase is the same with a TAT in the TAT table, we
prefer to the greater translation probabilities.
Table 4 shows the effect of using bilingual
phrases for Lynx. Note that these bilingual phrases
are the same with those used for Pharaoh.
5.5 Results on large data
We also conducted an experiment on large data to
further examine our design philosophy. The train-
ing corpus contains 2.6 million sentence pairs. We
used all the data to extract bilingual phrases and
a portion of 800K pairs to obtain TATs. Two tri-
gram language models were used for Lynx. One
was trained on the 2.6 million English sentences
and another was trained on the first 1/3 of the Xin-
hua portion of Gigaword corpus. We also included
rule-based translations of named entities, dates,
and numbers. By making use of these data, Lynx
achieves a BLEU score of 0.2830 on the 2005
NIST Chinese-to-English MT evaluation test set,
which is a very promising result for linguistically
syntax-based models.
6 Conclusion
In this paper, we introduce tree-to-string align-
ment templates, which can be automatically
learned from syntactically-annotated training data.
The TAT-based translation model improves trans-
lation quality significantly compared with a state-
of-the-art phrase-based decoder. Treated as spe-
cial TATs without tree on the source side, bilingual
phrases can be utilized for the TAT-based model to
get further improvement.
It should be emphasized that the restrictions
we impose on TAT extraction limit the expressive
power of TAT. Preliminary experiments reveal that
removing these restrictions does improve transla-
tion quality, but leads to large memory require-
ments. We feel that both parsing and word align-
ment qualities have important effects on the TAT-
based model. We will retrain the Chinese parser
on Penn Chinese Treebank version 5.0 and try to
improve word alignment quality using log-linear
models as suggested in (Liu et al, 2005).
Acknowledgement
This work is supported by National High Tech-
nology Research and Development Program con-
tract ?Generally Technical Research and Ba-
sic Database Establishment of Chinese Plat-
form?(Subject No. 2004AA114010). We are
grateful to Deyi Xiong for providing the parser and
Haitao Mi for making the parser more efficient and
robust. Thanks to Dr. Yajuan Lv for many helpful
comments on an earlier draft of this paper.
615
References
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 2000. Learning dependency translation mod-
els as collections of finite-state head transducers.
Computational Linguistics, 26(1):45-60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263-311.
Stanley F. Chen and Joshua Goodman. 1998. Am
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Har-
vard University Center for Research in Computing
Technology.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of 43rd Annual Meeting of the ACL, pages
263-270.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insert grammars. In Proceedings of 43rd Annual
Meeting of the ACL, pages 541-548.
Michel Galley, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL-HLT 2004, pages 273-
280.
Jonathan Graehl and Kevin Knight. 2004. Training
tree transducers. In Proceedings of NAACL-HLT
2004, pages 105-112.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of HLT-NAACL 2003, pages 127-133.
Philipp Koehn. 2004. Pharaoh: a beam search de-
coder for phrase-based statistical machine trnasla-
tion models. In Proceedings of the Sixth Confer-
ence of the Association for Machine Translation in
the Americas, pages 115-124.
Yang Liu, Qun Liu, and Shouxun Lin. 2005. Log-
linear models for word alignment. In Proceedings
of 43rd Annual Meeting of the ACL, pages 459-466.
Daniel Marcu and William Wong. 2002. A phrase-
based, joint probability model for statistical machine
translation. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 133-139.
Dan Melamed. 2004. Statistical machine translation
by parsing. In Proceedings of 42nd Annual Meeting
of the ACL, pages 653-660.
Franz J. Och and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of 38th
Annual Meeting of the ACL, pages 440-447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of 40th Annual
Meeting of the ACL, pages 295-302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417-449.
Franz J. Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of
41st Annual Meeting of the ACL, pages 160-167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the ACL, pages 311-318.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: Syntactically in-
formed phrasal SMT. In Proceedings of 43rd An-
nual Meeting of the ACL, pages 271-279.
Andreas Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 2, pages 901-904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and min-
imum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Con-
ference of the European Association for Machine
Translation (EAMT-05).
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377-403.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin,
and Yueliang Qian. 2005. Parsing the Penn Chinese
treebank with semantic knowledge. In Proceedings
of IJCNLP 2005, pages 70-81.
Kenji Yamada and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of 39th Annual Meeting of the ACL, pages 523-530.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004.
Interpreting BLEU/NIST scores: How much im-
provement do we need to have a better system? In
Proceedings of the Fourth International Conference
on Language Resources and Evaluation (LREC),
pages 2051-2054.
616
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 704?711,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Forest-to-String Statistical Translation Rules
Yang Liu , Yun Huang , Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100080, China
{yliu,huangyun,liuqun,sxlin}@ict.ac.cn
Abstract
In this paper, we propose forest-to-string
rules to enhance the expressive power of
tree-to-string translation models. A forest-
to-string rule is capable of capturing non-
syntactic phrase pairs by describing the cor-
respondence between multiple parse trees
and one string. To integrate these rules
into tree-to-string translation models, auxil-
iary rules are introduced to provide a gen-
eralization level. Experimental results show
that, on the NIST 2005 Chinese-English test
set, the tree-to-string model augmented with
forest-to-string rules achieves a relative im-
provement of 4.3% in terms of BLEU score
over the original model which allows tree-
to-string rules only.
1 Introduction
The past two years have witnessed the rapid de-
velopment of linguistically syntax-based translation
models (Quirk et al, 2005; Galley et al, 2006;
Marcu et al, 2006; Liu et al, 2006), which induce
tree-to-string translation rules from parallel texts
with linguistic annotations. They demonstrated very
promising results when compared with the state of
the art phrase-based system (Och and Ney, 2004)
in the NIST 2006 machine translation evaluation 1.
While Galley et al (2006) and Marcu et al (2006)
put emphasis on target language analysis, Quirk et
al. (2005) and Liu et al (2006) show benefits from
modeling the syntax of source language.
1See http://www.nist.gov/speech/tests/mt/
One major problem with linguistically syntax-
based models, however, is that tree-to-string rules
fail to syntactify non-syntactic phrase pairs because
they require a syntax tree fragment over the phrase
to be syntactified. Here, we distinguish between syn-
tactic and non-syntactic phrase pairs. By ?syntactic?
we mean that the phrase pair is subsumed by some
syntax tree fragment. The phrase pairs without trees
over them are non-syntactic. Marcu et al (2006)
report that approximately 28% of bilingual phrases
are non-syntactic on their English-Chinese corpus.
We believe that it is important to make available
to syntax-based models all the bilingual phrases that
are typically available to phrase-based models. On
one hand, phrases have been proven to be a simple
and powerful mechanism for machine translation.
They excel at capturing translations of short idioms,
providing local re-ordering decisions, and incorpo-
rating context information straightforwardly. Chi-
ang (2005) shows significant improvement by keep-
ing the strengths of phrases while incorporating syn-
tax into statistical translation. On the other hand,
the performance of linguistically syntax-based mod-
els can be hindered by making use of only syntac-
tic phrase pairs. Studies reveal that linguistically
syntax-based models are sensitive to syntactic anal-
ysis (Quirk and Corston-Oliver, 2006), which is still
not reliable enough to handle real-world texts due to
limited size and domain of training data.
Various solutions are proposed to tackle the prob-
lem. Galley et al (2004) handle non-constituent
phrasal translation by traversing the tree upwards
until reaches a node that subsumes the phrase.
Marcu et al (2006) argue that this choice is inap-
704
propriate because large applicability contexts are re-
quired.
For a non-syntactic phrase pair, Marcu et al
(2006) create a xRS rule headed by a pseudo, non-
syntactic nonterminal symbol that subsumes the
phrase and corresponding multi-headed syntactic
structure; and one sibling xRS rule that explains how
the non-syntactic nonterminal symbol can be com-
bined with other genuine nonterminals so as to ob-
tain genuine parse trees. The name of the pseudo
nonterminal is designed to reflect how the corre-
sponding rule can be fully realized. However, they
neglect alignment consistency when creating sibling
rules. In addition, it is hard for the naming mecha-
nism to deal with more complex phenomena.
Liu et al (2006) treat bilingual phrases as lexi-
calized TATs (Tree-to-string Alignment Template).
A bilingual phrase can be used in decoding if the
source phrase is subsumed by the input parse tree.
Although this solution does help, only syntactic
bilingual phrases are available to the TAT-based
model. Moreover, it is problematic to combine
the translation probabilities of bilingual phrases and
TATs, which are estimated independently.
In this paper, we propose forest-to-string rules
which describe the correspondence between multi-
ple parse trees and a string. They can not only cap-
ture non-syntactic phrase pairs but also have the ca-
pability of generalization. To integrate these rules
into tree-to-string translation models, auxiliary rules
are introduced to provide a generalization level. As
there is no pseudo node or naming mechanism, the
integration of forest-to-string rules is flexible, rely-
ing only on their root nodes. The forest-to-string and
auxiliary rules enable tree-to-string models to derive
in a more general way, while the strengths of con-
ventional tree-to-string rules still remain.
2 Forest-to-String Translation Rules
We define a tree-to-string rule r as a triple ?T? , S?, A??,
which describes the alignment A? between a source
parse tree T? = T (fJ ?
1
) and a target string S? = eI?
1
.
A source string fJ ?
1
, which is the sequence of leaf
nodes of T (fJ ?
1
), consists of both terminals (source
words) and nonterminals (phrasal categories). A tar-
get string eI?
1
is also composed of both terminals
(target words) and nonterminals (placeholders). An
IP
NP
NN
  
VP
SB
 
VP
NP
NN
  
VV
 
PU
 
The gunman was killed by police .
Figure 1: An English sentence aligned with a Chi-
nese parse tree.
alignment A? is defined as a subset of the Cartesian
product of source and target symbol positions:
A? ? {(j, i) : j = 1, . . . , J ?; i = 1, . . . , I ?}
A derivation ? = r
1
? r
2
? . . . ? rn is a left-
most composition of translation rules that explains
how a source parse tree T = T (fJ
1
), a target sen-
tence S = eI
1
, and the word alignment A are syn-
chronously generated. For example, Table 1 demon-
strates a derivation composed of only tree-to-string
rules for the ?T, S,A? tuple in Figure 1 2.
As we mentioned before, tree-to-string rules can
not syntactify phrase pairs that are not subsumed
by any syntax tree fragments. For example, for the
phrase pair ??   ?, ?The gunman was?? in Fig-
ure 1, it is impossible to extract an equivalent tree-
to-string rule that subsumes the same phrase pair
because valid tree-to-string rules can not be multi-
headed.
To address this problem, we propose forest-to-
string rules3 to subsume the non-syntactic phrase
pairs. A forest-to-string rule r 4 is a triple ?F? , S?, A??,
which describes the alignment A? between K source
parse trees F? = T?K
1
and a target string S?. The
source string fJ ?
1
is therefore the sequence of leaf
nodes of F? .
Auxiliary rules are introduced to integrate forest-
to-string rules into tree-to-string translation models.
An auxiliary rule is a special unlexicalized tree-to-
string rule that allows multiple source nonterminals
2We use ?X? to denote a nonterminal in the target string. If
there are more than one nonterminals, they are indexed.
3The term ?forest? refers to an ordered and finite set of trees.
4We still use ?r? to represent a forest-to-string rule to reduce
notational overhead.
705
No. Rule
(1) ( IP ( NP ) ( VP ) ( PU ) ) X
1
X
2
X
3
1:1 2:2 3:3
(2) ( NP ( NN   ) ) The gunman 1:1 1:2
(3) ( VP ( SB  ) ( VP ( NP ( NN ) ) ( VV  ) ) ) was killed by X 1:1 2:4 3:2
(4) ( NN   ) police 1:1
(5) ( PU  ) . 1:1
Table 1: A derivation composed of only tree-to-string rules for Figure 1.
No. Rule
(1) ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
1:1 2:1 3:2 4:2
(2) ( NP ( NN   ) ) ( SB  ) The gunman was 1:1 1:2 2:3
(3) ( VP ( NP ) ( VV  ) ) ( PU  ) killed by X . 1:3 2:1 3:4
(4) ( NP ( NN   ) ) police 1:1
Table 2: A derivation composed of tree-to-string, forest-to-string, and auxiliary rules for Figure 1.
to correspond to one target nonterminal, suggesting
that the forest-to-string rules that are rooted at such
source nonterminals can be integrated.
For example, Table 2 shows a derivation com-
posed of tree-to-string, forest-to-string, and auxil-
iary rules for the ?T, S,A? tuple in Figure 1. r
1
is
an auxiliary rule, r
2
and r
3
are forest-to-string rules,
and r
4
is a conventional tree-to-string rule.
Following Marcu et al (2006), we define the
probability of a tuple ?T, S,A? as the sum over all
derivations ?i ? ? that are consistent with the tuple,
c(?) = ?T, S,A?. The probability of each deriva-
tion ?i is given by the product of the probabilities of
all the rules p(rj) in the derivation.
Pr(T, S,A) =
?
?
i
??,c(?)=?T,S,A?
?
r
j
??
i
p(rj) (1)
3 Training
We obtain tree-to-string and forest-to-string rules
from word-aligned, source side parsed bilingual cor-
pus. The extraction algorithm is shown in Figure 2.
Note that T ? denotes either a tree or a forest.
For each span, the ?tree/forest, string, alignment?
triples are identified first. If a triple is consistent with
the alignment, the skeleton of the triple is computed
then. A skeleton s is a rule satisfying the following:
1. s ? R(t), s is induced from t.
2. node(T (s)) ? 2, the tree/forest of s contains
two or more nodes.
3. ?r ? R(t) ? node(T (r)) ? 2, T (s) ? T (r),
the tree/forest of s is the subgraph of that of any
r containing two or more nodes.
1: Input: a source tree T = T (fJ
1
), a target string
S = eI
1
, and word alignment A between them
2: R := ?
3: for u := 0 to J ? 1 do
4: for v := 1 to J ? u do
5: identify the triple set T corresponding to
span (v, v + u)
6: for each triple t = ?T ?, S?, A?? ? T do
7: if ?T ?, S?? is not consistent with A then
8: continue
9: end if
10: if u = 0 ? node(T ?) = 1 then
11: add t to R
12: add ?root(T ?), ?X?, 1:1? to R
13: else
14: compute the skeleton s of the triple t
15: register rules that are built on s using rules
extracted from the sub-triples of t:
R := R? build(s,R)
16: end if
17: end for
18: end for
19: end for
20: Output: rule set R
Figure 2: Rule extraction algorithm.
Given the skeleton and rules extracted from the
sub-triples, the rules for the triple can be acquired.
For example, the algorithm identifies the follow-
ing triple for span (1, 2) in Figure 1:
?( NP ( NN   ) ) ( SB  ),?The gunman was?, 1:1 1:2 2:3?
The skeleton of the triple is:
?( NP ) ( SB ),?X
1
X
2
?, 1:1 2:2?
As the algorithm proceeds bottom-up, five rules
have already been extracted from the sub-triples,
rooted at ?NP? and ?SB? respectively:
?( NP ),?X?, 1:1?
?( NP ( NN ) ),?X?, 1:1?
?( NP ( NN   ) ),?The gunman?, 1:1 1:2?
706
?( SB ),?X?, 1:1?
?( SB  ),?was?, 1:1?
Hence, we can obtain new rules by replacing the
source and target symbols of the skeleton with corre-
sponding rules and also by modifying the alignment
information. For the above triple, the combination
of the five rules produces 2 ? 3 = 6 new rules:
?( NP ) ( SB ),?X
1
X
2
?, 1:1 2:2?
?( NP ) ( SB  ),?X was?, 1:1 2:2?
?( NP ( NN ) ) ( SB ),?X
1
X
2
?, 1:1 2:2?
?( NP ( NN ) ) ( SB  ),?X was?, 1:1 2:2?
?( NP ( NN   ) ) ( SB ),?The gunman X?, 1:1 1:2?
?( NP ( NN   ) ) ( SB  ),?The gunman was?, 1:1 1:2 2:3?
Since we need only to check the alignment con-
sistency, in principle all phrase pairs can be captured
by tree-to-string and forest-to-string rules. To lower
the complexity for both training and decoding, we
impose four restrictions:
1. Both the first and the last symbols in the target
string must be aligned to some source symbols.
2. The height of a tree or forest is no greater than
h.
3. The number of direct descendants of a node is
no greater than c.
4. The number of leaf nodes is no greater than l.
Although possible, it is infeasible to learn aux-
iliary rules from training data. To extract an auxil-
iary rule which integrates at least one forest-to-string
rule, one need traverse the parse tree upwards until
one reaches a node that subsumes the entire forest
without violating the alignment consistency. This
usually results in very complex auxiliary rules, es-
pecially on real-world training data, making both
training and decoding very slow. As a result, we
construct auxiliary rules in decoding instead.
4 Decoding
Given a source parse tree T (fJ
1
), our decoder finds
the target yield of the single best derivation that has
source yield of T (fJ
1
):
S? = argmax
S,A
Pr(T, S,A)
= argmax
S,A
?
?
i
??,c(?)=?T,S,A?
?
r
j
??
i
p(rj)
1: Input: a source parse tree T = T (fJ
1
)
2: for u := 0 to J ? 1 do
3: for v := 1 to J ? u do
4: for each T ? spanning from v to v + u do
5: if T ? is a tree then
6: for each usable tree-to-string rule r do
7: for each derivation ? inferred from r
and derivations in matrix do
8: add ? to matrix[v, v + u, root(T ?)]
9: end for
10: end for
11: search subcell divisions D[v, v + u]
12: for each subcell division d ? D[v, v + u] do
13: if d contains at least one forest cell then
14: construct auxiliary rule r
a
15: for each derivation ? inferred from r
a
and derivations in matrix do
16: add ? to matrix[v, v + u, root(T ?)]
17: end for
18: end if
19: end for
20: else
21: for each usable forest-to-string rule r do
22: for each derivation ? inferred from r
and derivations in matrix do
23: add ? to matrix[v, v + u, ??]
24: end for
25: end for
26: search subcell divisions D[v, v + u]
27: end if
28: end for
29: end for
30: end for
31: find the best derivation ?? in matrix[1, J, root(T )] and
get the best translation ?S = e(??)
32: Output: a target string ?S
Figure 3: Decoding algorithm.
? argmax
S,A,?
?
r
j
??,c(?)=?T,S,A?
p(rj) (2)
Figure 3 demonstrates the decoding algorithm.
It organizes the derivations into an array matrix
whose cells matrix[j
1
, j
2
,X] are sets of derivations.
[j
1
, j
2
,X] represents a tree/forest rooted at X span-
ning from j
1
to j
2
. We use the empty string ?? to
denote the pseudo root of a forest.
Next, we will explain how to infer derivations for
a tree/forest provided a usable rule. If T (r) = T?,
there is only one derivation which contains only the
rule r. This usually happens for leaf nodes. If
T (r) ? T ?, the rule r resorts to derivations from
subcells to infer new derivations. Suppose that the
decoder is to translate the source tree in Figure 1
and finds a usable rule for [1, 5, ?IP?]:
?( IP ( NP ) ( VP ) ( PU ) ),?X
1
X
2
X
3
?, 1:1 2:2 3:3?
707
Subcell Division Auxiliary Rule
[1, 1][2, 2][3, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
X
3
1:1 2:2 3:3 4:3
[1, 2][3, 4][5, 5] ( IP ( NP ) ( VP ( SB ) ( VP ) ) ( PU ) ) X
1
X
2
X
3
1:1 2:1 3:2 4:3
[1, 3][4, 5] ( IP ( NP ) ( VP ( SB ) ( VP ( NP ) ( VV ) ) ) ( PU ) ) X
1
X
2
1:1 2:1 3:1 4:2 5:2
[1, 1][2, 5] ( IP ( NP ) ( VP ) ( PU ) ) X
1
X
2
1:1 2:2 3:2
Table 3: Subcell divisions and corresponding auxiliary rules for the source tree in Figure 1
Since the decoding algorithm proceeds in a
bottom-up fashion, the uncovered portions have al-
ready been translated.
For [1, 1, ?NP?], suppose that we can find a
derivation in matrix:
?( NP ( NN   ) ),?The gunman?, 1:1 1:2?
For [2, 4, ?VP?], we find a derivation in matrix:
?( VP ( SB  ) ( VP ( NP ( NN )) (VV ) ) ),
?was killed by X?, 1:1 2:4 3:2?
?( NN   ),?police?, 1:1?
For [5, 5, ?PU?], we find a derivation in matrix:
?( PU  ),?.?, 1:1?
Henceforth, we get a derivation for [1, 5, ?IP?],
shown in Table 1.
A translation rule r is said to be usable to an input
tree/forest T ? if and only if:
1. T (r) ? T ?, the tree/forest of r is the subgraph
of T ?.
2. root(T (r)) = root(T ?), the root sequence of
T (r) is identical to that of T ?.
For example, the following rules are usable to the
tree ?( NP ( NR   ) ( NN   ) )?:
?( NP ( NR ) ( NN ) ),?X
1
X
2
?, 1:2 2:1?
?( NP ( NR   ) ( NN ) ),?China X?, 1:1 2:2?
?( NP ( NR   ) ( NN  ) ),?China economy?, 1:1 2:2?
Similarly, the forest-to-string rule
?( ( NP ( NR ) ( NN ) ) ( VP ) ),?X
1
X
2
X
3
?, 1:2 2:1 3:3?
is usable to the forest
( NP ( NR ) ( NN   ) ) ( VP (VV )( NN  ) )
As we mentioned before, auxiliary rules are spe-
cial unlexicalized tree-to-string rules that are built in
decoding rather than learnt from real-world data. To
get an auxiliary rule for a cell, we need first identify
its subcell division.
A cell sequence c
1
, c
2
, . . . , cn is referred to as a
subcell division of a cell c if and only if:
1. c
1
.begin = c.begin
1: Input: a cell [j
1
, j
2
], the derivation array matrix,
the subcell division array D
2: if j
1
= j
2
then
3: p? := 0
4: for each derivation ? in matrix[j
1
, j
2
, ?] do
5: p? := max(p(?), p?)
6: end for
7: add {[j
1
, j
2
]} : p? to D[j
1
, j
2
]
8: else
9: if [j
1
, j
2
] is a forest cell then
10: p? := 0
11: for each derivation ? in matrix[j
1
, j
2
, ?] do
12: p? := max(p(?), p?)
13: end for
14: add {[j
1
, j
2
]} : p? to D[j
1
, j
2
]
15: end if
16: for j := j
1
to j
2
? 1 do
17: for each division d
1
? D[j
1
, j] do
18: for each division d
2
? D[j + 1, j
2
] do
19: create a new division: d := d
1
? d
2
20: add d to D[j
1
, j
2
]
21: end for
22: end for
23: end for
24: end if
25: Output: subcell divisions D[j
1
, j
2
]
Figure 4: Subcell division search algorithm.
2. cn.end = c.end
3. cj .end + 1 = cj+1.begin, 1 ? j < n
Given a subcell division, it is easy to construct the
auxiliary rule for a cell. For each subcell, one need
transverse the parse tree upwards until one reaches
nodes that subsume it. All descendants of these
nodes are dropped. The target string consists of only
nonterminals, the number of which is identical to
that of subcells. To limit the search space, we as-
sume that the alignment between the source tree and
the target string is monotone.
Table 3 shows some subcell divisions and corre-
sponding auxiliary rules constructed for the source
tree in Figure 1. For simplicity, we ignore the root
node label.
There are 2n?1 subcell divisions for a cell which
has a length of n. We need only consider the sub-
708
cell divisions which contain at least one forest cell
because tree-to-string rules have already explored
those contain only tree cells.
The actual search algorithm for subcell divisions
is shown in Figure 4. We use matrix[j
1
, j
2
, ?] to de-
note all trees or forests spanning from j
1
to j
2
. The
subcell divisions and their associated probabilities
are stored in an array D. We define an operator ?
between two divisions: their cell sequences are con-
catenated and the probabilities are accumulated.
As sometimes there are no usable rules available,
we introduce default rules to ensure that we can al-
ways get a translation for any input parse tree. A de-
fault rule is a tree-to-string rule 5, built in two ways:
1. If the input tree contains only one node, the
target string of the default rule is equal to the
source string.
2. If the height of the input tree is greater than
one, the tree of the default rule contains only
the root node and its direct descendants of the
input tree, the string contains only nontermi-
nals, and the alignment is monotone.
To speed up the decoder, we limit the search space
by reducing the number of rules used for each cell.
There are two ways to limit the rule table size: by
a fixed limit a of how many rules are retrieved for
each cell, and by a probability threshold ? that spec-
ify that the rule probability has to be above some
value. Also, instead of keeping the full list of deriva-
tions for a cell, we store a top-scoring subset of the
derivations. This can also be done by a fixed limit
b or a threshold ?. The subcell division array D, in
which divisions containing forest cells have priority
over those composed of only tree cells, is pruned by
keeping only a-best divisions.
Following Och and Ney (2002), we base our
model on log-linear framework and adopt the seven
feature functions described in (Liu et al, 2006). It
is very important to balance the preference between
conventional tree-to-string rules and the newly-
introduced forest-to-string and auxiliary rules. As
the probabilities of auxiliary rules are not learnt
from training data, we add a feature that sums up the
5There are no default rules for forests because only tree-to-
string rules are essential to tree-to-string translation models.
node count of auxiliary rules of a derivation to pe-
nalize the use of forest-to-string and auxiliary rules.
5 Experiments
In this section, we report on experiments with
Chinese-to-English translation. The training corpus
consists of 31, 149 sentence pairs with 843, 256 Chi-
nese words and 949, 583 English words. For the
language model, we used SRI Language Modeling
Toolkit (Stolcke, 2002) to train a trigram model with
modified Kneser-Ney smoothing (Chen and Good-
man, 1998) on the 31, 149 English sentences. We
selected 571 short sentences from the 2002 NIST
MT Evaluation test set as our development corpus,
and used the 2005 NIST MT Evaluation test set as
our test corpus. Our evaluation metric is BLEU-4
(Papineni et al, 2002), as calculated by the script
mteval-v11b.pl with its default setting except that
we used case-sensitive matching of n-grams. To
perform minimum error rate training (Och, 2003)
to tune the feature weights to maximize the sys-
tem?s BLEU score on development set, we used the
script optimizeV5IBMBLEU.m (Venugopal and Vo-
gel, 2005).
We ran GIZA++ (Och and Ney, 2000) on the
training corpus in both directions using its default
setting, and then applied the refinement rule ?diag-
and? described in (Koehn et al, 2003) to obtain a
single many-to-many word alignment for each sen-
tence pair. Next, we employed a Chinese parser
written by Deyi Xiong (Xiong et al, 2005) to parse
all the 31, 149 Chinese sentences. The parser was
trained on articles 1-270 of Penn Chinese Treebank
version 1.0 and achieved 79.4% in terms of F1 mea-
sure.
Given the word-aligned, source side parsed bilin-
gual corpus, we obtained bilingual phrases using the
training toolkits publicly released by Philipp Koehn
with its default setting. Then, we applied extrac-
tion algorithm described in Figure 2 to extract both
tree-to-string and forest-to-string rules by restricting
h = 3, c = 5, and l = 7. All the rules, including
bilingual phrases, tree-to-string rules, and forest-to-
string rules, are filtered for the development and test
sets.
According to different levels of lexicalization, we
divide translation rules into three categories:
709
Rule L P U Total
BP 251, 173 0 0 251, 173
TR 56, 983 41, 027 3, 529 101, 539
FR 16, 609 254, 346 25, 051 296, 006
Table 4: Number of rules used in experiments (BP:
bilingual phrase, TR: tree-to-string rule, FR: forest-
to-string rule; L: lexicalized, P: partial lexicalized,
U: unlexicalized).
System Rule Set BLEU4
Pharaoh BP 0.2182 ? 0.0089
BP 0.2059 ? 0.0083
TR 0.2302 ? 0.0089Lynx TR + BP 0.2346 ? 0.0088
TR + FR + AR 0.2402 ? 0.0087
Table 5: Comparison of Pharaoh and Lynx with dif-
ferent rule sets.
1. lexicalized: all symbols in both the source and
target strings are terminals
2. unlexicalized: all symbols in both the source
and target strings are nonterminals
3. partial lexicalized: otherwise
Table 4 shows the statistics of rules used in our ex-
periments. We find that even though forest-to-string
rules are introduced the total number (i.e. 73, 592)
of lexicalized tree-to-string and forest-to-string rules
is still far less than that (i.e. 251, 173) of bilingual
phrases. This difference results from the restriction
we impose in training that both the first and last sym-
bols in the target string must be aligned to some
source symbols. For the forest-to-string rules, par-
tial lexicalized ones are in the majority.
We compared our system Lynx against a freely
available phrase-based decoder Pharaoh (Koehn et
al., 2003). For Pharaoh, we set a = 20, ? = 0,
b = 100, ? = 10?5, and distortion limit dl = 4. For
Lynx, we set a = 20, ? = 0, b = 100, and ? = 0.
Two postprocessing procedures ran to improve the
outputs of both systems: OOVs removal and recapi-
talization.
Table 5 shows results on test set using Pharaoh
and Lynx with different rule sets. Note that Lynx
is capable of using only bilingual phrases plus de-
Forest-to-String Rule Set BLEU4
None 0.2225 ? 0.0085
L 0.2297 ? 0.0081
P 0.2279 ? 0.0083
U 0.2270 ? 0.0087
L + P + U 0.2312 ? 0.0082
Table 6: Effect of lexicalized, partial lexicalized,
and unlexicalized forest-to-string rules.
fault rules to perform monotone search. The 95%
confidence intervals were computed using Zhang?s
significance tester (Zhang et al, 2004). We mod-
ified it to conform to NIST?s current definition of
the BLEU brevity penalty. We find that Lynx out-
performs Pharaoh significantly. The integration of
forest-to-string rules achieves an absolute improve-
ment of 1.0% (4.3% relative) over using tree-to-
string rules only. This difference is statistically sig-
nificant (p < 0.01). It also achieves better result
than treating bilingual phrases as lexicalized tree-to-
string rules. To produce the best result of 0.2402,
Lynx made use of 26, 082 tree-to-string rules, 9, 219
default rules, 5, 432 forest-to-string rules, and 2, 919
auxiliary rules. This suggests that tree-to-string
rules still play a central role, although the integra-
tion of forest-to-string and auxiliary rules is really
beneficial.
Table 6 demonstrates the effect of forest-to-string
rules with different lexicalization levels. We set
a = 3, ? = 0, b = 10, and ? = 0. The second row
?None? shows the result of using only tree-to-string
rules. ?L? denotes using tree-to-string rules and lex-
icalized forest-to-string rules. Similarly, ?L+P+U?
denotes using tree-to-string rules and all forest-to-
string rules. We find that lexicalized forest-to-string
rules are more useful.
6 Conclusion
In this paper, we introduce forest-to-string rules to
capture non-syntactic phrase pairs that are usually
unaccessible to traditional tree-to-string translation
models. With the help of auxiliary rules, forest-to-
string rules can be integrated into tree-to-string mod-
els to offer more general derivations. Experiment re-
sults show that the tree-to-string model augmented
with forest-to-string rules significantly outperforms
710
the original model which allows tree-to-string rules
only.
Our current rule extraction algorithm attaches the
unaligned target words to the nearest ascendants that
subsume them. This constraint hampers the expres-
sive power of our model. We will try a more general
way as suggested in (Galley et al, 2006), making
no a priori assumption about assignment and using
EM training to learn the probability distribution. We
will also conduct experiments on large scale training
data to further examine our design philosophy.
Acknowledgement
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
Stanley F. Chen and Joshua Goodman. 1998. An empir-
ical study of smoothing techniques for language mod-
eling. Technical report, Harvard University Center for
Research in Computing Technology.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings
of ACL 2005, pages 263?270, Ann Arbor, Michigan,
June.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule? In
Proceedings of HLT/NAACL 2004, pages 273?280,
Boston, Massachusetts, USA, May.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of COLING/ACL 2006, pages 961?968, Sydney,
Australia, July.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. InProceed-
ings of HLT/NAACL 2003, pages 127?133, Edmonton,
Canada, May.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of COLING/ACL 2006, pages
609?616, Sydney, Australia, July.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and
Kevin Knight. 2006. Spmt: Statistical machine trans-
lation with syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44?52, Sydney,
Australia, July.
Franz J. Och and Hermann Ney. 2000. Improved statis-
tical alignment models. In Proceedings of ACL 2000,
pages 440?447.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL 2002,
pages 295?302.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz J. Och. 2003. Minimum error rate training in sta-
tistical machine translation. In Proceedings of ACL
2003, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of ACL
2002, pages 311?318, Philadephia, USA, July.
Chris Quirk and Simon Corston-Oliver. 2006. The im-
pact of parse quality on syntactically-informed statis-
tical machine translation. In Proceedings of EMNLP
2006, pages 62?69, Sydney, Australia, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005, pages
271?279, Ann Arbor, Michigan, June.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
volume 30, pages 901?904.
Ashish Venugopal and Stephan Vogel. 2005. Consid-
erations in maximum mutual information and mini-
mum classification error training for statistical ma-
chine translation. In Proceedings of the Tenth Confer-
ence of the European Association for Machine Trans-
lation, pages 271?279.
Deyi Xiong, Shuanglong Li, Qun Liu, and Shouxun Lin.
2005. Parsing the penn chinese treebank with seman-
tic knowledge. In Proceedings of IJCNLP 2005, pages
70?81.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. In-
terpreting bleu/nist scores how much improvement do
we need to have a better system? In Proceedings
of Fourth International Conference on Language Re-
sources and Evaluation, pages 2051?2054.
711
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 161?164,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Partial Matching Strategy for Phrase-based Statistical Machine Translation
Zhongjun He1,2 and Qun Liu1 and Shouxun Lin1
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, China
2Graduate University of Chinese Academy of Sciences
Beijing, 100049, China
{zjhe,liuqun,sxlin}@ict.ac.cn
Abstract
This paper presents a partial matching strat-
egy for phrase-based statistical machine trans-
lation (PBSMT). Source phrases which do not
appear in the training corpus can be trans-
lated by word substitution according to par-
tially matched phrases. The advantage of this
method is that it can alleviate the data sparse-
ness problem if the amount of bilingual corpus
is limited. We incorporate our approach into
the state-of-the-art PBSMT system Moses and
achieve statistically significant improvements
on both small and large corpora.
1 Introduction
Currently, most of the phrase-based statistical ma-
chine translation (PBSMT) models (Marcu and
Wong, 2002; Koehn et al, 2003) adopt full matching
strategy for phrase translation, which means that a
phrase pair (f? , e?) can be used for translating a source
phrase f? , only if f? = f? . Due to lack of generaliza-
tion ability, the full matching strategy has some lim-
itations. On one hand, the data sparseness problem
is serious, especially when the amount of the bilin-
gual data is limited. On the other hand, for a certain
source text, the phrase table is redundant since most
of the bilingual phrases cannot be fully matched.
In this paper, we address the problem of trans-
lation of unseen phrases, the source phrases that
are not observed in the training corpus. The
alignment template model (Och and Ney, 2004)
enhanced phrasal generalizations by using words
classes rather than the words themselves. But the
phrases are overly generalized. The hierarchical
phrase-based model (Chiang, 2005) used hierar-
chical phrase pairs to strengthen the generalization
ability of phrases and allow long distance reorder-
ings. However, the huge grammar table greatly in-
creases computational complexity. Callison-Burch
et al (2006) used paraphrases of the trainig corpus
for translating unseen phrases. But they only found
and used the semantically similar phrases. Another
method is to use multi-parallel corpora (Cohn and
Lapata, 2007; Utiyama and Isahara, 2007) to im-
prove phrase coverage and translation quality.
This paper presents a partial matching strategy for
translating unseen phrases. When encountering un-
seen phrases in a source sentence, we search par-
tially matched phrase pairs from the phrase table.
Then we keep the translations of the matched part
and translate the unmatched part by word substitu-
tion. The advantage of our approach is that we alle-
viate the data sparseness problem without increasing
the amount of bilingual corpus. Moreover, the par-
tially matched phrases are not necessarily synony-
mous. We incorporate the partial matching method
into the state-of-the-art PBSMT system, Moses. Ex-
periments show that, our approach achieves statis-
tically significant improvements not only on small
corpus, but also on large corpus.
2 Partial Matching for PBSMT
2.1 Partial Matching
We use matching similarity to measure how well the
source phrases match each other. Given two source
phrases f?J1 and f? ?
J
1 , the matching similarity is com-
puted as:
161
?/P {I/N <?/N u?/V ??/N
issued warning to the American people
?/P /N <?/N ?5/V ?/N
bring advantage to the Taiwan people
Figure 1: An example of partially matched phrases with
the same POS sequence and word alignment.
SIM(f?J1 , f? ?
J
1 ) =
?J
j=1 ?(fj , f ?j)
J (1)
where,
?(f, f ?) =
{
1 if f = f ?
0 otherwise (2)
Therefore, partial matching takes full matching
(SIM(f? , f?) = 1.0) as a special case. Note that in
order to improve search efficiency, we only consider
the partially matched phrases with the same length.
In our experiments, we use a matching threshold
? to tune the precision of partial matching. Low
threshold indicates high coverage of unseen phrases,
but will suffer from much noise. In order to alleviate
this problem, we search partially matched phrases
under the constraint that they must have the same
parts-of-speech (POS) sequence. See Figure 1 for
illustration. Although the matching similarity of the
two phrases is only 0.2, as they have the same POS
sequence, the word alignments are the same. There-
fore, the lower source phrase can be translated ac-
cording to the upper phrase pair with correct word
reordering. Furthermore, this constraint can sharply
decrease the computational complexity since there
is no need to search the whole phrase table.
2.2 Translating Unseen Phrases
We translate an unseen phrase fJ1 according to the
partially matched phrase pair (f ?J1 , e?I1, a?) as follows:
1. Compare each word between fJ1 and f ?J1 to get
the position set of the different words: P =
{j|fj 6= f ?j , j = 1, 2, . . . , J};
2. Remove f ?j from f ?J1 and e?aj from e?I1, where
j ? P ;
3. Find the translation e for fj(j ? P ) from the
phrase table and put it into the position aj in
e?I1 according to the word alignment a?.
u
?U
-?
I
u
?
-?
?.?
arrived in Prague last evening
u
-?
arrived in
arrived in Thailand yesterday
Figure 2: An example of phrase translation.
Figure 2 shows an example. In fact, we create a
translation template dynamically in step 2:
?u X1 -? X2, arrived in X2 X1? (3)
Here, on the source side, each of the non-terminal
X corresponds to a single source word. In addition,
the removed sub-phrase pairs should be consistent
with the word alignment matrix.
Following conventional PBSMT models, we use
4 features to measure phrase translation quality: the
translation weights p(f? |e?) and p(e?|f?), the lexical
weights pw(f? |e?) and pw(e?|f?). The new constructed
phrase pairs keep the translation weights of their
?parent? phrase pair. The lexical weights are com-
puted by word substitution. Suppose S{(f ?, e?)} is
the pair set in (f? ?,e??,a?) which replaced by S{(f, e)}
to create the new phrase pair (f? ,e?,a?), the lexical
weight is computed as:
pw(f? |e?, a?)
=
pw(f? ?|e??, a?) ?
?
(f,e)?S{(f,e)} pw(f |e)?
(f ?,e?)?S{(f ?,e?)} pw(f ?|e?)
(4)
Therefore, the newly constructed phrase pairs can be
used for decoding as they have already existed in the
phrase table.
2.3 Incorporating Partial Matching into the
PBSMT Model
In this paper, we incorporate the partial matching
strategy into the state-of-the-art PBSMT system,
Moses1. Given a source sentence, Moses firstly
uses the full matching strategy to search all possi-
ble translation options from the phrase table, and
then uses a beam-search algorithm for decoding.
1http://www.statmt.org/moses/
162
Therefore, we do incorporation by performing par-
tial matching for phrase translation before decod-
ing. The advantage is that the main search algorithm
need not be changed.
For a source phrase f? , we search partially
matched phrase pair (f? ?, e??, a?) from the phrase table.
If SIM(f? , f? ?)=1.0, which means f? is observed in
the training corpus, thus e?? can be directly stored as a
translation option. However, if ? ? SIM(f? , f? ?) <
1.0, we construct translations for f? according to Sec-
tion 2.2. Then the newly constructed translations are
stored as translation options.
Moses uses translation weights and lexical
weights to measure the quality of a phrase transla-
tion pair. For partial matching, besides these fea-
tures, we add matching similarity SIM(f? , f? ?) as a
new feature. For a source phrase, we select top N
translations for decoding. In Moses, N is set by the
pruning parameter ttable-limit.
3 Experiments
We carry out experiments on Chinese-to-English
translation on two tasks: Small-scale task, the train-
ing corpus consists of 30k sentence pairs (840K +
950K words); Large-scale task, the training cor-
pus consists of 2.54M sentence pairs (68M + 74M
words). The 2002 NIST MT evaluation test data is
used as the development set and the 2005 NIST MT
test data is the test set. The baseline system we used
for comparison is the state-of-the-art PBSMT sys-
tem, Moses.
We use the ICTCLAS toolkit2 to perform Chinese
word segmentation and POS tagging. The training
script of Moses is used to train the bilingual corpus.
We set the maximum length of the source phrase
to 7, and record word alignment information in the
phrase table. For the language model, we use the
SRI Language Modeling Toolkit (Stolcke, 2002) to
train a 4-gram model on the Xinhua portion of the
Gigaword corpus.
To run the decoder, we set ttable-limit=20,
distortion-limit=6, stack=100. The translation qual-
ity is evaluated by BLEU-4 (case-sensitive). We per-
form minimum-error-rate training (Och, 2003) to
tune the feature weights of the translation model to
maximize the BLEU score on development set.
2http://www.nlp.org.cn/project/project.php?proj id=6
? 1.0 0.7 0.5 0.3 0.1
BLEU 24.44 24.43 24.86 25.31 25.13
Table 1: Effect of matching threshold on BLEU score.
3.1 Small-scale Task
Table 1 shows the effect of matching threshold on
translation quality. The baseline uses full matching
(?=1.0) for phrase translation and achieves a BLEU
score of 24.44. With the decrease of the matching
threshold, the BLEU scores increase. when ?=0.3,
the system obtains the highest BLEU score of 25.31,
which achieves an absolute improvement of 0.87
over the baseline. However, if the threshold con-
tinue decreasing, the BLEU score decreases. The
reason is that low threshold increases noise for par-
tial matching.
The effect of matching threshold on the coverage
of n-gram phrases is shown in Figure 3. When us-
ing full matching (?=1.0), long phrases (length?3)
face a serious data sparseness problem. With the de-
crease of the threshold, the coverage increases.
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
 100
 1  2  3  4  5  6  7
co
ve
ra
ge
 ra
tio
 on
 th
e t
es
t s
et
phrase length
?=1.0
?=0.7
?=0.5
?=0.3
?=0.1
Figure 3: Effect of matching threshold on the coverage of
n-gram phrases.
Table 2 shows the phrase number of 1-best out-
put under ?=1.0 and ?=0.3. When ?=1.0, the long
phrases (length?3) only account for 2.9% of the to-
tal phrases. When ?=0.3, the number increases to
10.7%. Moreover, the total phrase of ?=0.3 is less
than that of ?=1.0, since source text is segmented
into more long phrases under partial matching, and
most of the long phrases are translated from partially
matched phrases (the row 0.3? SIM <1.0).
3.2 Large-scale Task
For this task, the BLEU score of the baseline is
30.45. However, for partial matching method with
163
Phrase Length 1 2 3 4 5 6 7 total
?=1.0 19485 4416 615 87 12 2 1 24618
SIM=1.0 14750 2977 387 48 10 1 0?=0.3 0.3? SIM <1.0 0 1196 1398 306 93 17 12 21195
Table 2: Phrase number of 1-best output. ?=1.0 means full matching. For ?=0.3, SIM=1.0 means full matching,
0.3 ? SIM < 1.0 means partial matching.
?=0.53, the BLEU score is 30.96, achieving an ab-
solute improvement of 0.51. Using Zhang?s signif-
icant tester (Zhang et al, 2004), both the improve-
ments on the two tasks are statistically significant at
p < 0.05.
The improvement on large-scale task is less than
that on small-scale task since larger corpus relieves
data sparseness. However, the partial matching ap-
proach can also improve translation quality by using
long phrases. For example, the segmentation and
translation for the Chinese sentence ???L
?????? are as follows:
Full matching:
? | ?L? |? | |?? |?
long term | economic output | , but | the | trend | will
Partial matching:
? | ?L???? |?
but | the long-term trend of economic output | will
Here the source phrase ??L ?  ? ?
?? cannot be fully matched. Thus the decoder
breaks it into 4 short phrases, but performs an in-
correct reordering. Using partial matching, the long
phrase is translated correctly since it can partially
matched the phrase pair ??Lu7,???
the inevitable trend of economic development?.
3.3 Conclusion
This paper presents a partial matching strategy for
phrase-based statistical machine translation. Phrases
which are not observed in the training corpus can
be translated according to partially matched phrases
by word substitution. Our method can relieve data
sparseness problem without increasing the amount
of the corpus. Experiments show that our approach
achieves statistically significant improvements over
the state-of-the-art PBSMT system Moses.
In future, we will study sophisticated partial
matching methods, since current constraints are ex-
cessively strict. Moreover, we will study the effect
3Due to time limit, we do not tune the threshold for large-
scale task.
of word alignment on partial matching, which may
affect word substitution and reordering.
Acknowledgments
We would like to thank Yajuan Lv and Yang Liu
for their valuable suggestions. This work was sup-
ported by the National Natural Science Foundation
of China (NO. 60573188 and 60736014), and the
High Technology Research and Development Pro-
gram of China (NO. 2006AA010108).
References
C. Callison-Burch, P. Koehn, and M. Osborne. 2006.
Improved statistical machine translation using para-
phrases. In Proc. of NAACL06, pages 17?24.
D. Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of ACL05,
pages 263?270.
T. Cohn and M. Lapata. 2007. Machine translation by
triangulation: Making effective use of multi-parallel
corpora. In Proc. of ACL07, pages 728?735.
P. Koehn, F. J. Och, and D. Marcu. 2003. Statistical
phrase-based translation. In Proc. of HLT-NAACL03,
pages 127?133.
D. Marcu and W. Wong. 2002. A phrasebased joint
probabilitymodel for statistical machine translation. In
Proc. of EMNLP02, pages 133?139.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417?449.
F. J. Och. 2003. Minimum error rate training in statistical
machine translation. In Proc. of ACL03, pages 160?
167.
A. Stolcke. 2002. Srilm ? an extensible language model-
ing toolkit. In Proc. of ICSLP02, pages 901?904.
M. Utiyama and H. Isahara. 2007. A comparison of pivot
methods for phrase-based statistical machine transla-
tion. In Proc. of NAACL-HLT07, pages 484?491.
Y. Zhang, S. Vogel, and A. Waibel. 2004. Interpreting
bleu/nist scores: How much improvement do we need
to have a better system? In Proc. of LREC04, pages
2051?2054.
164
Proceedings of the Second Workshop on Statistical Machine Translation, pages 40?47,
Prague, June 2007. c?2007 Association for Computational Linguistics
A Dependency Treelet String Correspondence
Model for Statistical Machine Translation
Deyi Xiong, Qun Liu and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, China, 100080
{dyxiong, liuqun, sxlin}@ict.ac.cn
Abstract
This paper describes a novel model using
dependency structures on the source side
for syntax-based statistical machine transla-
tion: Dependency Treelet String Correspon-
dence Model (DTSC). The DTSC model
maps source dependency structures to tar-
get strings. In this model translation pairs of
source treelets and target strings with their
word alignments are learned automatically
from the parsed and aligned corpus. The
DTSC model allows source treelets and tar-
get strings with variables so that the model
can generalize to handle dependency struc-
tures with the same head word but with dif-
ferent modifiers and arguments. Addition-
ally, target strings can be also discontinuous
by using gaps which are corresponding to
the uncovered nodes which are not included
in the source treelets. A chart-style decod-
ing algorithm with two basic operations?
substituting and attaching?is designed for
the DTSC model. We argue that the DTSC
model proposed here is capable of lexical-
ization, generalization, and handling discon-
tinuous phrases which are very desirable for
machine translation. We finally evaluate our
current implementation of a simplified ver-
sion of DTSC for statistical machine trans-
lation.
1 Introduction
Over the last several years, various statistical syntax-
based models were proposed to extend traditional
word/phrase based models in statistical machine
translation (SMT) (Lin, 2004; Chiang, 2005; Ding
et al, 2005; Quirk et al, 2005; Marcu et al, 2006;
Liu et al, 2006). It is believed that these models
can improve the quality of SMT significantly. Com-
pared with phrase-based models, syntax-based mod-
els lead to better reordering and higher flexibility
by introducing hierarchical structures and variables
which make syntax-based models capable of hierar-
chical reordering and generalization. Due to these
advantages, syntax-based approaches are becoming
an active area of research in machine translation.
In this paper, we propose a novel model based on
dependency structures: Dependency Treelet String
Correspondence Model (DTSC). The DTSC model
maps source dependency structures to target strings.
It just needs a source language parser. In contrast to
the work by Lin (2004) and by Quirk et al (2005),
the DTSC model does not need to generate target
language dependency structures using source struc-
tures and word alignments. On the source side, we
extract treelets which are any connected subgraphs
and consistent with word alignments. While on the
target side, we allow the aligned target sequences
to be generalized and discontinuous by introducing
variables and gaps. The variables on the target side
are aligned to the corresponding variables of treelets,
while gaps between words or variables are corre-
sponding to the uncovered nodes which are not in-
cluded by treelets. To complete the translation pro-
cess, we design two basic operations for the decod-
ing: substituting and attaching. Substituting is used
to replace variable nodes which have been already
translated, while attaching is used to attach uncov-
40
ered nodes to treelets.
In the remainder of the paper, we first define de-
pendency treelet string correspondence in section
2 and describe an algorithm for extracting DTSCs
from the parsed and word-aligned corpus in section
3. Then we build our model based on DTSC in sec-
tion 4. The decoding algorithm and related pruning
strategies are introduced in section 5. We also spec-
ify the strategy to integrate phrases into our model
in section 6. In section 7 we evaluate our current
implementation of a simplified version of DTSC for
statistical machine translation. And finally, we dis-
cuss related work and conclude.
2 Dependency Treelet String
Correspondence
A dependency treelet string correspondence pi is a
triple < D,S,A > which describes a translation
pair < D,S > and their alignment A, where D is
the dependency treelet on the source side and S is
the translation string on the target side. < D,S >
must be consistent with the word alignment M of
the corresponding sentence pair
?(i, j) ? M, i ? D ? j ? S
A treelet is defined to be any connected subgraph,
which is similar to the definition in (Quirk et al,
2005). Treelet is more representatively flexible than
subtree which is widely used in models based on
phrase structures (Marcu et al, 2006; Liu et al,
2006). The most important distinction between the
treelet in (Quirk et al, 2005) and ours is that we al-
low variables at positions of subnodes. In our defini-
tion, the root node must be lexicalized but the subn-
odes can be replaced with a wild card. The target
counterpart of a wildcard node in S is also replaced
with a wild card. The wildcards introduced in this
way generalize DTSC to match dependency struc-
tures with the same head word but with different
modifiers or arguments.
Another unique feature of our DTSC is that we al-
low target strings with gaps between words or wild-
cards. Since source treelets may not cover all subn-
odes, the uncovered subnodes will generate a gap as
its counterpart on the target side. A sequence of con-
tinuous gaps will be merged to be one gap and gaps
at the beginning and the end of S will be removed
automatically.
??
eeeeeee
?
?
?
? ??eeeeeee
s s
s s
s s
s s??
??
? ?eeeeeee
?
?
?
?
]]]]]]]]]]
the conference cooperation of the ?
??
eeeeeeebbbbbbbbbbbbb
s s
s s
s s
s s?1
w w
w w
? YYYYYYY
SSS
SSS
S ?
?2 ]]]]]]]]]]?1 keep a G with the ?2
Figure 1: DTSC examples. Note that ? represents
variable and G represents gap.
Gap can be considered as a special kind of vari-
able whose counterpart on the source side is not
present. This makes the model more flexible to
match more partial dependency structures on the
source side. If only variables can be used, the model
has to match subtrees rather than treelets on the
source side. Furthermore, the positions of variables
on the target side are fixed so that some reorderings
related with them can be recorded in DTSC. The po-
sitions of gaps on the target side, however, are not
fixed until decoding. The presence of one gap and
its position can not be finalized until attaching op-
eration is performed. The introduction of gaps and
the related attaching operation in decoding is the
most important distinction between our model and
the previous syntax-based models.
Figure 1 shows several different DTSCs automat-
ically extracted from our training corpus. The top
left DTSC is totally lexicalized, while the top right
DTSC has one variable and the bottom has two vari-
ables and one gap. In the bottom DTSC, note that
the node ? which is aligned to the gap G of the
target string is an uncovered node and therefore not
included in the treelet actually. Here we just want
to show there is an uncovered node aligned with the
gap G.
Each node at the source treelet has three attributes
1. The head word
2. The category, i.e. the part of speech of the head
word
3. The node order which specifies the local order
of the current node relative to its parent node.
41
??/VV
eeeeeeebbbbbbbbbbbbbb ]]]]]]]]]]]]]]]]]]]]]
?
?
?
?
?
??/VV
?
?
?
DD
DD
D ?/P YYYYYYY
XXXXX
XXXXX
XXXX
??/NN
eeeeeee
z z
z z
z
????/NR
\\\\\\\\\\
\\\\ ??/NN
j j j
j??
go on providingfinancial aid to Palestine
1 2 3 4 5 6 7
Figure 2: An example dependency tree and its align-
ments
Note that the node order is defined at the context of
the extracted treelets but not the context of the orig-
inal tree. For example, the attributes for the node?
in the bottom DTSC of Figure 1 are {?, P, -1}. For
two treelets, if and only if their structures are iden-
tical and each corresponding nodes share the same
attributes, we say they are matched.
3 Extracting DTSCs
To extract DTSCs from the training corpus, firstly
the corpus must be parsed on the source side and
aligned at the word level. The source structures pro-
duced by the parser are unlabelled, ordered depen-
dency trees with each word annotated with a part-of-
speech. Figure 2 shows an example of dependency
tree really used in our extractor.
When the source language dependency trees and
word alignments between source and target lan-
guages are obtained, the DTSC extraction algorithm
runs in two phases along the dependency trees and
alignments. In the first step, the extractor annotates
each node with specific attributes defined in section
3.1. These attributes are used in the second step
which extracts all possible DTSCs rooted at each
node recursively.
3.1 Node annotation
For each source dependency node n, we define three
attributes: word span, node span and crossed.
Word span is defined to be the target word sequence
aligned with the head word of n, while node span is
defined to be the closure of the union of node spans
of all subnodes of n and its word span. These two at-
tributes are similar to those introduced by Lin (Lin,
2004). The third attribute crossed is an indicator that
has binary values. If the node span of n overlaps
the word span of its parent node or the node span
of its siblings, the crossed indicator of n is 1 and
n is therefore a crossed node, otherwise the crossed
indicator is 0 and n is a non-crossed node. Only
non-crossed nodes can generate DTSCs because the
target word sequence aligned with the whole subtree
rooted at it does not overlap any other sequences and
therefore can be extracted independently.
For the dependency tree and its alignments shown
in Figure 2, only the node ?? is a crossed node
since its node span ([4,5]) overlaps the word span
([5,5]) of its parent node??.
3.2 DTSCs extraction
The DTSC extraction algorithm (shown in Figure 3)
runs recursively. For each non-crossed node, the al-
gorithm generates all possible DTSCs rooted at it by
combining DTSCs from some subsets of its direct
subnodes. If one subnode n selected in the com-
bination is a crossed node, all other nodes whose
word/node spans overlap the node span of n must be
also selected in this combination. This kind of com-
bination is defined to be consistent with the word
alignment because the DTSC generated by this com-
bination is consistent with the word alignment. All
DTSCs generated in this way will be returned to the
last call and outputted. For each crossed node, the
algorithm generates pseudo DTSCs1 using DTSCs
from all of its subnodes. These pseudo DTSCs will
be returned to the last call but not outputted.
During the combination of DTSCs from subnodes
into larger DTSCs, there are two major tasks. One
task is to generate the treelet using treelets from
subnodes and the current node. This is a basic tree
generation operation. It is worth mentioning that
some non-crossed nodes are to be replaced with a
wild card so the algorithm can learn generalized
DTSCs described in section 2. Currently, we re-
place any non-crossed node alone or together with
their sibling non-crossed nodes. The second task
is to combine target strings. The word sequences
aligned with uncovered nodes will be replaced with
a gap. The word sequences aligned with wildcard
nodes will be replaced with a wild card.
If a non-crossed node n has m direct subnodes,
all 2m combinations will be considered. This will
generate a very large number of DTSCs, which is
1Some words in the target string are aligned with nodes
which are not included in the source treelet.
42
DTSCExtractor(Dnode n)
< := ? (DTSC container of n)
for each subnode k of n do
R := DTSCExtractor(k)
L := L?R
end for
if n.crossed! = 1 and there are no subnodes whose span
overlaps the word span of n then
Create a DTSC pi =< D,S,A > where the dependency
treelet D only contains the node n (not including any chil-
dren of it)
output pi
for each combination c of n?s subnodes do
if c is consistent with the word alignment then
Generate all DTSCs R by combining DTSCs (L)
from the selected subnodes with the current node n
< := <?R
end if
end for
output <
return <
else if n.crossed == 1 then
Create pseudo DTSCs P by combining all DTSCs from
n?s all subnodes.
< := <?P
return <
end if
Figure 3: DTSC Extraction Algorithm.
undesirable for training and decoding. Therefore we
filter DTSCs according to the following restrictions
1. If the number of direct subnodes of node n is
larger than 6, we only consider combining one
single subnode with n each time because in this
case reorderings of subnodes are always mono-
tone.
2. On the source side, the number of direct subn-
odes of each node is limited to be no greater
than ary-limit; the height of treelet D is limited
to be no greater than depth-limit.
3. On the target side, the length of S (including
gaps and variables) is limited to be no greater
than len-limit; the number of gaps in S is lim-
ited to be no greater than gap-limit.
4. During DTSC combination, the DTSCs from
each subnode are sorted by size (in descending
order). Only the top comb-limit DTSCs will be
selected to generate larger DTSCs.
As an example, for the dependency tree and its
alignments in Figure 2, all DTSCs extracted by the
Treelet String
(??/VV/0) go on
(????/NR/0) Palestine
(?/P/0) to
(?/P/0 (????/NR/1)) to Palestine
(?/P/0 (?/1)) to ?
(??/NN/0 (??/NN/-1)) financial aid
(??/VV/0) providing
(??/VV/0 (?/1)) providing ?
(??/VV/0 (?/-1)) providing G ?
(??/VV/0 (??/VV/-1)) go on providing
(??/VV/0 (?/-1)) ? providing
(??/VV/0 (?1/-1) (?2/1)) providing ?2 ?1
(??/VV/0 (?1/-1 ) (?2/1)) ?1 providing ?2
Table 1: Examples of DTSCs extracted from Figure
2. Alignments are not shown here because they are
self-evident.
algorithm with parameters { ary-limit = 2, depth-
limit = 2, len-limit = 3, gap-limit = 1, comb-limit
= 20 } are shown in the table 1.
4 The Model
Given an input dependency tree, the decoder gen-
erates translations for each dependency node in
bottom-up order. For each node, our algorithm will
search all matched DTSCs automatically learned
from the training corpus by the way mentioned in
section 3. When the root node is traversed, the trans-
lating is finished. This complicated procedure in-
volves a large number of sequences of applications
of DTSC rules. Each sequence of applications of
DTSC rules can derive a translation.
We define a derivation ? as a sequence of appli-
cations of DTSC rules, and let c(?) and e(?) be the
source dependency tree and the target yield of ?, re-
spectively. The score of ? is defined to be the prod-
uct of the score of the DTSC rules used in the trans-
lation, and timed by other feature functions:
?(?) =
?
i
?(i) ? plm(e)?lm ? exp(??apA(?)) (1)
where ?(i) is the score of the ith application of
DTSC rules, plm(e) is the language model score,
and exp(??apA(?)) is the attachment penalty,
where A(?) calculates the total number of attach-
ments occurring in the derivation ?. The attach-
ment penalty gives some control over the selection
of DTSC rules which makes the model prefer rules
43
with more nodes covered and therefore less attach-
ing operations involved.
For the score of DTSC rule pi, we define it as fol-
lows:
?(pi) =
?
j
fj(pi)?j (2)
where the fj are feature functions defined on DTSC
rules. Currently, we used features proved to be ef-
fective in phrase-based SMT, which are:
1. The translation probability p(D|S).
2. The inverse translation probability p(S|D).
3. The lexical translation probability plex(D|S)
which is computed over the words that occur
on the source and target sides of a DTSC rule
by the IBM model 1.
4. The inverse lexical translation probability
plex(S|D) which is computed over the words
that occur on the source and target sides of a
DTSC rule by the IBM model 1.
5. The word penalty wp.
6. The DTSC penalty dp which allows the model
to favor longer or shorter derivations.
It is worth mentioning how to integrate the N-
gram language mode into our DTSC model. During
decoding, we have to encounter many partial transla-
tions with gaps and variables. For these translations,
firstly we only calculate the language model scores
for word sequences in the translations. Later we up-
date the scores when gaps are removed or specified
by attachments or variables are substituted. Each up-
dating involves merging two neighbor substrings sl
(left) and sr (right) into one bigger string s. Let the
sequence of n ? 1 (n is the order of N-gram lan-
guage model used) rightmost words of sl be srl and
the sequence of n?1 leftmost words of sr be slr. we
have:
LM(s) = LM(sl) + LM(sr) + LM(srl slr)
?LM(srl )? LM(slr) (3)
where LM is the logarithm of the language model
probability. We only need to compute the increment
of the language model score:
4LM = LM(srl slr)? LM(srl )? LM(slr) (4)
for each node n of the input tree T , in bottom-up order do
Get al matched DTSCs rooted at n
for each matched DTSC pi do
for each wildcard node n? in pi do
Substitute the corresponding wildcard on the target
side with translations from the stack of n?
end for
for each uncovered node n@ by pi do
Attach the translations from the stack of n@ to the
target side at the attaching point
end for
end for
end for
Figure 4: Chart-style Decoding Algorithm for the
DTSC Model.
Melamed (2004) also used a similar way to integrate
the language model.
5 Decoding
Our decoding algorithm is similar to the bottom-up
chart parsing. The distinction is that the input is a
tree rather than a string and therefore the chart is in-
dexed by nodes of the tree rather than spans of the
string. Also, several other tree-based decoding al-
gorithms introduced by Eisner (2003), Quirk et al
(2005) and Liu et al (2006) can be classified as the
chart-style parsing algorithm too.
Our decoding algorithm is shown in Figure 4.
Given an input dependency tree, firstly we generate
the bottom-up order by postorder transversal. This
order guarantees that any subnodes of node n have
been translated before node n is done. For each
node n in the bottom-up order, all matched DTSCs
rooted at n are found, and a stack is also built for it to
store the candidate translations. A DTSC pi is said to
match the input dependency subtree T rooted at n if
and only if there is a treelet rooted at n that matches
2 the treelet of pi on the source side.
For each matched DTSC pi, two operations will
be performed on it. The first one is substituting
which replaces a wildcard node with the correspond-
ing translated node. The second one is attaching
which attaches an uncovered node to pi. The two op-
erations are shown in Figure 5. For each wildcard
node n?, translations from the stack of it will be se-
lected to replace the corresponding wildcard on the
2The words, categories and orders of each corresponding
nodes are matched. Please refer to the definition of matched
in section 2.
44
(a) A
eeeeeee YYYYYYYB
eeeeeee
? + D
C ? De
?e Ae Be Ce
Substitute ?
(b) A
eeeeeee YYYYYYYB
eeeeeee
D + E
C ? Ee
De Ae Be Ce
Attach ?
(c) A
eeeeeee YYYYYYYB
eeeeeee YYYYYYY
D
C E
De Ae Be Ee Ce
Figure 5: Substituting and attaching operations for
decoding. Xe is the translation of X . Node that ? is
a wildcard node to be substituted and node ? is an
uncovered node to be attached.
target side and the scores of new translations will be
calculated according to our model. For each uncov-
ered node n@, firstly we determine where transla-
tions from the stack of n@ should be attached on the
target side. There are several different mechanisms
for choosing attaching points. Currently, we imple-
ment a heuristic way: on the source side, we find the
node n@p which is the nearest neighbor of n@ from
its parent and sibling nodes, then the attaching point
is the left/right of the counterpart of n@p on the target
side according to their relative order. As an example,
see the uncovered node ? in Figure 5. The nearest
node to it is node B. Since node ? is at the right
of node B, the attaching point is the right of Be.
One can search all possible points using an ordering
model. And this ordering model can also use infor-
mation from gaps on the target side. We believe this
ordering model can improve the performance and let
it be one of directions for our future research.
Note that the gaps on the target side are not neces-
sarily attaching points in our current attaching mech-
anism. If they are not attaching point, they will be
removed automatically.
The search space of the decoding algorithm is
very large, therefore some pruning techniques have
to be used. To speed up the decoder, the following
pruning strategies are adopted.
1. Stack pruning. We use three pruning ways.
The first one is recombination which converts
the search to dynamic programming. When
two translations in the same stack have the
same w leftmost/rightmost words, where w de-
pends on the order of the language model, they
will be recombined by discarding the transla-
tion with lower score. The second one is the
threshold pruning which discards translations
that have a score worse than stack-threshold
times the best score in the same stack. The
last one is the histogram pruning which only
keeps the top stack-limit best translations for
each stack.
2. Node pruning. For each node, we only keep
the top node-limit matched DTSCs rooted at
that node, as ranked by the size of source
treelets.
3. Operation pruning. For each operation, sub-
stituting and attaching, the decoding will gen-
erate a large number of partial translations3
for the current node. We only keep the top
operation-limit partial translations each time
according to their scores.
6 Integrating Phrases
Although syntax-based models are good at dealing
with hierarchical reordering, but at the local level,
translating idioms and similar complicated expres-
sions can be a problem. However, phrase-based
models are good at dealing with these translations.
Therefore, integrating phrases into the syntax-based
models can improve the performance (Marcu et al,
2006; Liu et al, 2006). Since our DTSC model is
based on dependency structures and lexicalized nat-
urally, DTSCs are more similar to phrases than other
translation units based on phrase structures. This
means that phrases will be easier to be integrated
into our model.
The way to integrate phrases is quite straightfor-
ward: if there is a treelet rooted at the current node,
3There are wildcard nodes or uncovered nodes to be han-
dled.
45
of which the word sequence is continuous and iden-
tical to the source of some phrase, then a phrase-
style DTSC will be generated which uses the target
string of the phrase as its own target. The procedure
is finished during decoding. In our experiments, in-
tegrating phrases improves the performance greatly.
7 Current Implementation
To test our idea, we implemented the dependency
treelet string correspondence model in a Chinese-
English machine translation system. The current im-
plementation in this system is actually a simplified
version of the DTSC model introduced above. In
this version, we used a simple heuristic way for the
operation of attaching rather than a sophisticated sta-
tistical model which can learn ordering information
from the training corpus. Since dependency struc-
tures are more?flattened? compared with phrasal
structures, there are many subnodes which will not
be covered even by generalized matched DTSCs.
This means the attaching operation is very common
during decoding. Therefore better attaching model
which calculates the best point for attaching , we be-
lieve, will improve the performance greatly and is a
major goal for our future research.
To obtain the dependency structures of the source
side, one can parse the source sentences with a de-
pendency parser or parse them with a phrasal struc-
ture parser and then convert the phrasal structures
into dependency structures. In our experiments we
used a Chinese parser implemented by Xiong et
al. (2005) which generates phrasal structures. The
parser was trained on articles 1-270 of Penn Chinese
Treebank version 1.0 and achieved 79.4% (F1 mea-
sure). We then converted the phrasal structure trees
into dependency trees using the way introduced by
Xia (1999).
To obtain the word alignments, we use the way
of Koehn et al (2005). After running GIZA++
(Och and Ney, 2000) in both directions, we apply
the ?grow-diag-final? refinement rule on the in-
tersection alignments for each sentence pair.
The training corpus consists of 31, 149 sentence
pairs with 823K Chinese words and 927K English
words. For the language model, we used SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
trigram model with modified Kneser-Ney smooth-
Systems BLEU-4
PB 20.88 ? 0.87
DTSC 20.20 ? 0.81
DTSC + phrases 21.46 ? 0.83
Table 2: BLEU-4 scores for our system and a
phrase-based system.
ing on the 31, 149 English sentences. We selected
580 short sentences of length at most 50 characters
from the 2002 NIST MT Evaluation test set as our
development corpus and used it to tune ?s by max-
imizing the BLEU score (Och, 2003), and used the
2005 NIST MT Evaluation test set as our test corpus.
From the training corpus, we learned 2, 729,
964 distinct DTSCs with the configuration { ary-
limit = 4, depth-limit = 4, len-limit = 15, gap-limit
= 2, comb-limit = 20 }. Among them, 160,694
DTSCs are used for the test set. To run our de-
coder on the development and test set, we set stack-
thrshold = 0.0001, stack-limit = 100, node-limit =
100, operation-limit = 20.
We also ran a phrase-based system (PB) with a
distortion reordering model (Xiong et al, 2006) on
the same corpus. The results are shown in table 2.
For all BLEU scores, we also show the 95% confi-
dence intervals computed using Zhang?s significant
tester (Zhang et al, 2004) which was modified to
conform to NIST?s definition of the BLEU brevity
penalty. The BLEU score of our current system with
the DTSC model is lower than that of the phrase-
based system. However, with phrases integrated, the
performance is improved greatly, and the new BLEU
score is higher than that of the phrase-based SMT.
This difference is significant according to Zhang?s
tester. This result can be improved further using a
better parser (Quirk et al, 2006) or using a statisti-
cal attaching model.
8 Related Work
The DTSC model is different from previous work
based on dependency grammars by Eisner (2003),
Lin (2004), Quirk et al (2005), Ding et al (2005)
since they all deduce dependency structures on the
target side. Among them, the most similar work is
(Quirk et al, 2005). But there are still several major
differences beyond the one mentioned above. Our
46
treelets allow variables at any non-crossed nodes and
target strings allow gaps, which are not available in
(Quirk et al, 2005). Our language model is calcu-
lated during decoding while Quirk?s language model
is computed after decoding because of the complex-
ity of their decoding.
The DTSC model is also quite distinct from pre-
vious tree-string models by Marcu et al (2006)
and Liu et al (2006). Firstly, their models are
based on phrase structure grammars. Secondly, sub-
trees instead of treelets are extracted in their mod-
els. Thirdly, it seems to be more difficult to integrate
phrases into their models. And finally, our model al-
low gaps on the target side, which is an advantage
shared by (Melamed, 2004) and (Simard, 2005).
9 Conclusions and Future Work
We presented a novel syntax-based model using
dependency trees on the source side?dependency
treelet string correspondence model?for statistical
machine translation. We described an algorithm to
learn DTSCs automatically from the training corpus
and a chart-style algorithm for decoding.
Currently, we implemented a simple version of
the DTSC model. We believe that our performance
can be improved greatly using a more sophisticated
mechanism for determining attaching points. There-
fore the most important future work should be to de-
sign a better attaching model. Furthermore, we plan
to use larger corpora for training and n-best depen-
dency trees for decoding, which both are helpful for
the improvement of translation quality.
Acknowledgements
This work was supported by National Natural Sci-
ence Foundation of China, Contract No. 60603095
and 60573188.
References
David Chiang. 2005. A hierarchical phrase-based model for
statistical machine translation. In Proceedings of ACL.
Yuan Ding and Martha Palmer. 2005. Machine Translation Us-
ing Probabilistic Synchronous Dependency Insertion Gram-
mars. In Proceedings of ACL.
Jason Eisner. 2003. Learning non-isomorphic tree mappings
for machine translation. In Proceedings of ACL.
Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, Chris
Callison-Burch, Miles Osborne and David Talbot. 2005.
Edinburgh System Description for the 2005 IWSLT Speech
Translation Evaluation. In International Workshop on Spo-
ken Language Translation.
Daniel Marcu, Wei Wang, Abdessamad Echihabi, and Kevin
Knight. 2006. SPMT: Statistical Machine Translation with
Syntactified Target Language Phraases. In Proceedings of
EMNLP.
I. Dan Melamed. 2004. Algorithms for Syntax-Aware Statisti-
cal Machine Translation. In Proceedings of the Conference
on Theoretical and Methodological Issues in Machine Trans-
lation (TMI), Baltimore, MD.
Dekang Lin. 2004. A path-based transfer model for machine
translation. In Proceedings of COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-String
Alignment Template for Statistical Machine Translation. In
Proceedings of ACL.
Franz Josef Och. 2003. Minimum error rate training in statisti-
cal machine translation. In Proceedings of ACL.
Franz Josef Och and Hermann Ney. 2000. Improved statistical
alignment models. In Proceedings of ACL.
Chris Quirk, Arul Menezes and Colin Cherry. 2005. Depen-
dency Treelet Translation: Syntactically Informed Phrasal
SMT. In Proceedings of ACL.
Chris Quirk and Simon Corston-Oliver. 2006. The impact of
parse quality on syntactically-informed statistical machine
translation. In Proceedings of EMNLP, Sydney, Australia.
Michel Simard, Nicola Cancedda, Bruno Cavestro, Marc
Dymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.
2005. Translating with non-contiguous phrases. In Proceed-
ings of HLT-EMNLP.
Andreas Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proceedings of International Conference on
Spoken Language Processing, volume 2, pages 901-904.
Fei Xia. 1999. Automatic Grammar Generation from Two Dif-
ferent Perspectives. PhD thesis, University of Pennsylvania.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum
Entropy Based Phrase Reordering Model for Statistical Ma-
chine Translation. In Proceedings of COLING-ACL, Sydney,
Australia.
Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, Yueliang
Qian. 2005. Parsing the Penn Chinese Treebank with Se-
mantic Knowledge. In Proceedings of IJCNLP, Jeju Island,
Korea.
Ying Zhang, Stephan Vogel, and Alex Waibel. 2004. Inter-
preting BLEU/NIST scores: How much improvement do we
need to have a better system? In Proceedings of LREC,
pages 2051? 2054.
47
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1092?1100,
Beijing, August 2010
Dependency Forest for Statistical Machine Translation
Zhaopeng Tu ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{tuzhaopeng,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
We propose a structure called dependency
forest for statistical machine translation.
A dependency forest compactly represents
multiple dependency trees. We develop
new algorithms for extracting string-to-
dependency rules and training depen-
dency language models. Our forest-based
string-to-dependency system obtains sig-
nificant improvements ranging from 1.36
to 1.46 BLEU points over the tree-based
baseline on the NIST 2004/2005/2006
Chinese-English test sets.
1 Introduction
Dependency grammars have become increasingly
popular in syntax-based statistical machine trans-
lation (SMT). One important advantage of depen-
dency grammars is that they directly capture the
dependencies between words, which are key to re-
solving most parsing ambiguities. As a result, in-
corporating dependency trees proves to be effec-
tive in improving statistical machine translation
(Quirk et al, 2005; Ding and Palmer, 2005; Shen
et al, 2008).
However, most dependency-based translation
systems suffer from a major drawback: they only
use 1-best dependency trees for rule extraction,
dependency language model training, and decod-
ing, which potentially introduces translation mis-
takes due to the propagation of parsing errors
(Quirk and Corston-Oliver, 2006). While the
treelet system (Quirk et al, 2005) takes a de-
pendency tree as input, the string-to-dependency
system (Shen et al, 2008) decodes on a source-
language string. However, as we will show, the
string-to-dependency system still commits to us-
ing degenerate rules and dependency language
models learned from noisy 1-best trees.
To alleviate this problem, an obvious solu-
tion is to offer more alternatives. Recent studies
have shown that SMT systems can benefit from
widening the annotation pipeline: using packed
forests instead of 1-best trees (Mi and Huang,
2008), word lattices instead of 1-best segmenta-
tions (Dyer et al, 2008), and weighted alignment
matrices instead of 1-best alignments (Liu et al,
2009).
Along the same direction, we propose a struc-
ture called dependency forest, which encodes ex-
ponentially many dependency trees compactly, for
dependency-based translation systems. In this pa-
per, we develop two new algorithms for extracting
string-to-dependency rules and for training depen-
dency language models, respectively. We show
that using the rules and dependency language
models learned from dependency forests leads to
consistent and significant improvements over that
of using 1-best trees on the NIST 2004/2005/2006
Chinese-English test sets.
2 Background
Figure 1 shows a dependency tree of an English
sentence he saw a boy with a telescope. Arrows
point from the child to the parent, which is often
referred to as the head of the child. For example,
in Figure 1, saw is the head of he. A dependency
tree is more compact than its constituent counter-
part because there is no need to build a large su-
perstructure over a sentence.
Shen et al (2008) propose a novel string-to-
dependency translation model that features two
important advantages. First, they define that
a string-to-dependency rule must have a well-
formed dependency structure on the target side,
which makes efficient dynamic programming pos-
sible and manages to retain most useful non-
constituent rules. A well-formed structure can be
either fixed or floating . A fixed structure is a
1092
saw
he boy with
a telescope
a
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 1: A training example for tree-based rule
extraction.
dependency tree with all the children complete.
Floating structures consist of sibling nodes of a
common head, but the head itself is unspecified
or floating. For example, Figure 2(a) and Figure
2(b) are two fixed structures while Figure 2(c) is a
floating one.
Formally, for a given sentence w1:l = w1 . . . wl,
d1 . . . dl represent the parent word IDs for each
word. If wi is a root, we define di = 0.
Definition 1. A dependency structure di..j is fixed
on head h, where h /? [i, j], or fixed for short, if
and only if it meets the following conditions
? dh /? [i, j]
? ?k ? [i, j] and k 6= h, dk ? [i, j]
? ?k /? [i, j], dk = h or dk /? [i, j]
Definition 2. A dependency structure di..j is
floating with children C, for a non-empty set C
? {i, ..., j}, or floating for short, if and only if it
meets the following conditions
? ?h /? [i, j], s.t.?k ? C, dk = h
? ?k ? [i, j] and k /? C, dk ? [i, j]
? ?k /? [i, j], dk /? [i, j]
A dependency structure is well-formed if and
only if it is either fixed or floating.
2.1 Tree-based Rule Extraction
Figure 1 shows a training example consisting of an
English dependency tree, its Chinese translation,
boy
a
(a)
with
telescope
a
(b)
boy with
a telescope
a
(c)
Figure 2: Well-formed dependency structures cor-
responding to Figure 1. (a) and (b) are fixed and
(c) is floating.
and the word alignments between them. To facil-
itate identifying the correspondence between the
English and Chinese words, we also gives the En-
glish sentence. Extracting string-to-dependency
rules from aligned string-dependency pairs is sim-
ilar to extracting SCFG (Chiang, 2007) except that
the target side of a rule is a well-formed struc-
ture. For example, we can first extract a string-to-
dependency rule that is consistent with the word
alignment (Och and Ney, 2004):
with ((a) telescope) ? dai wangyuanjing de
Then a smaller rule
(a) telescope ? wangyuanjing
can be subtracted to obtain a rule with one non-
terminal:
with (X1) ? dai X1 de
where X is a non-terminal and the subscript indi-
cates the correspondence between non-terminals
on the source and target sides.
2.2 Tree-based Dependency Language Model
As dependency relations directly model the se-
mantics structure of a sentence, Shen et al (2008)
introduce dependency language model to better
account for the generation of target sentences.
Compared with the conventional n-gram language
models, dependency language model excels at
capturing non-local dependencies between words
(e.g., saw ... with in Figure 1). Given a depen-
dency tree, its dependency language model prob-
ability is a product of three sub-models defined
between headwords and their dependants. For ex-
ample, the probability of the tree in Figure 1 can
1093
saw0,7
he0,1 boy2,4 with4,7
a2,3 telescope5,7
a5,6
(a)
saw0,7
he0,1 boy2,7
a2,3 with4,7
telescope5,7
a5,6
(b)
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
(c)
Figure 3: (a) the dependency tree in Figure 1, (b) another dependency tree for the same sentence, and
(c) a dependency forest compactly represents the two trees.
be calculated as:
Prob = PT (saw)
?PL(he|saw-as-head)
?PR(boy|saw-as-head)
?PR(with|boy, saw-as-head)
?PL(a|boy-as-head)
?PR(telescope|with-as-head)
?PL(a|telescope-as-head)
where PT (x) is the probability of word x being
the root of a dependency tree. PL and PR are the
generative probabilities of left and right sides re-
spectively.
As the string-to-tree system relies on 1-best
trees for parameter estimation, the quality of rule
table and dependency language model might be
affected by parsing errors and therefore ultimately
results in translation mistakes.
3 Dependency Forest
We propose to encode multiple dependency trees
in a compact representation called dependency
forest, which offers an elegant solution to the
problem of parsing error propagation.
Figures 3(a) and 3(b) show two dependency
trees for the example English sentence in Figure
1. The prepositional phrase with a telescope could
either depend on saw or boy. Figure 3(c) is a
dependency forest compactly represents the two
trees by sharing common nodes and edges.
Each node in a dependency forest is a word.
To distinguish among nodes, we attach a span to
each node. For example, in Figure 1, the span of
the first a is (2, 3) because it is the third word in
the sentence. As the fourth word boy dominates
the node a2,3, it can be referred to as boy2,4. Note
that the position of boy itself is taken into consid-
eration. Similarly, the word boy in Figure 3(b) can
be represented as boy2,7.
The nodes in a dependency forest are connected
by hyperedges. While an edge in a dependency
tree only points from a dependent to its head, a
hyperedge groups all the dependants that have a
common head. For example, in Figure 3(c), the
hyperedge
e1: ?(he0,1, boy2,4,with4,7), saw0,7?
denotes that he0,1, boy2,4, and with4,7 are depen-
dants (from left to right) of saw0,7.
More formally, a dependency forest is a pair
?V,E?, where V is a set of nodes, and E
is a set of hyperedges. For a given sentence
w1:l = w1 . . . wl, each node v ? V is in the
form of wi,j , which denotes that w dominates
the substring from positions i through j (i.e.,
wi+1 . . . wj). Each hyperedge e ? E is a pair
?tails(e), head(e)?, where head(e) ? V is the
head and tails(e) ? V are its dependants.
A dependency forest has a structure of a hy-
pergraph such as packed forest (Klein and Man-
ning, 2001; Huang and Chiang, 2005). However,
while each hyperedge in a packed forest naturally
treats the corresponding PCFG rule probability as
its weight, it is challenging to make dependency
forest to be a weighted hypergraph because depen-
dency parsers usually only output a score, which
can be either positive or negative, for each edge
in a dependency tree rather than a hyperedge in a
1094
saw0,7
he0,1 boy2,4 boy2,7
with4,7
e1 e2
a2,3
e3 e4
telescope5,7
e5
a5,6
e6
he saw a boy with a telescope
ta kandao yige dai wangyuanjing de nanhai
Figure 4: A training example for forest-based rule
extraction.
dependency forest. For example, in Figure 3(a),
the scores for the edges he ? saw, boy ? saw,
and with ? saw could be 13, 22, and -12, respec-
tively.
To assign a probability to each hyperedge, we
can first obtain a positive number for a hyperedge
using the scores of the corresponding edges:1
c(e) = exp
(?
v?tails(e) s
(
v, head(e)
)
|tails(e)|
)
(1)
where c(e) is the count of a hyperedge e, head(e)
is a head, tails(e) is a set of dependants of the
head, v is one dependant, and s(v, head(e)) is the
score of an edge from v to head(e). For example,
the count of the hyperedge e1 in Figure 3(c) is
c(e1) = exp
(
13 + 22 ? 12
3
)
(2)
Then, the probability of a hyperedge can be ob-
tained by normalizing the count among all hyper-
edges with the same head collected from a training
corpus:
p(e) = c(e)?
e?:head(e?)=head(e) c(e?)
(3)
Therefore, we obtain a weighted dependency
forest in which each hyperedge has a probability.
1It is difficult to assign a probability to each hyperedge.
The current method is arbitrary, and we will improve it in the
future.
Algorithm 1 Forest-based Initial Phrase Extrac-
tion
Input: a source sentence ?, a forest F , an alignment a,
and k
Output: minimal initial phrase setR
1: for each node v ? V in a bottom-up order do
2: for each hyperedge e ? E and head(e) = v do
3: W ? ?
4: fixs? EnumFixed(v,modifiers(e))
5: floatings? EnumFloating(modifiers(e))
6: add structures fixs, floatings to W
7: for each ? ?W do
8: if ? is consistent with a then
9: generate a rule r
10: R.append(r)
11: keep k-best dependency structures for v
4 Forest-based Rule Extraction
In tree-based rule extraction, one just needs to first
enumerate all bilingual phrases that are consis-
tent with word alignment and then check whether
the dependency structures over the target phrases
are well-formed. However, this algorithm fails to
work in the forest scenario because there are usu-
ally exponentially many well-formed structures
over a target phrase.
The GHKM algorithm (Galley et al, 2004),
which is originally developed for extracting tree-
to-string rules from 1-best trees, has been suc-
cessfully extended to packed forests recently (Mi
and Huang, 2008). The algorithm distinguishes
between minimal and composed rules. Although
there are exponentially many composed rules, the
number of minimal rules extracted from each node
is rather limited (e.g., one or zero). Therefore, one
can obtain promising composed rules by combin-
ing minimal rules.
Unfortunately, the GHKM algorithm cannot be
applied to extracting string-to-dependency rules
from dependency forests. This is because the
GHKM algorithm requires a complete subtree to
exist in a rule while neither fixed nor floating de-
pendency structures ensure that all dependants of
a head are included. For example, the floating
structure shown in Figure 2(c) actually contains
two trees.
Alternatively, our algorithm searches for well-
formed structures for each node in a bottom-up
style. Algorithm 1 shows the algorithm for ex-
tracting initial phrases, that is, rules without non-
1095
terminals from dependency forests. The algorithm
maintains k-best well-formed structures for each
node (line 11). The well-formed structures of a
head can be constructed from those of its depen-
dants. For example, in Figure 4, as the fixed struc-
ture rooted at telescope5,7 is
(a) telescope
we can obtain a fixed structure rooted for the node
with4,7 by attaching the fixed structure of its de-
pendant to the node (EnumFixed in line 4). Figure
2(b) shows the resulting fixed structure.
Similarly, the floating structure for the node
saw0,7 can be obtained by concatenating the fixed
structures of its dependants boy2,4 and with4,7
(EnumFloating in line 5). Figure 2(c) shows the
resulting fixed structure. The algorithm is similar
to Wang et al (2007), which binarize each con-
stituent node to create some intermediate nodes
that correspond to the floating structures.
Therefore, we can find k-best fixed and float-
ing structures for a node in a dependency forest
by manipulating the fixed structures of its depen-
dants. Then we can extract string-to-dependency
rules if the dependency structures are consistent
with the word alignment.
How to judge a well-formed structure extracted
from a node is better than others? We follow Mi
and Huang (2008) to assign a fractional count to
each well-formed structure. Given a tree fragment
t, we use the inside-outside algorithm to compute
its posterior probability:
??(t) = ?(root(t)) ?
?
e?t
p(e)
?
?
v?leaves(t)
?(v) (4)
where root(t) is the root of the tree, e is an edge,
leaves(t) is a set of leaves of the tree, ?(?) is out-
side probability, and ?(?) is inside probability.
For example, the subtree rooted at boy2,7 in Fig-
ure 4 has the following posterior probability:
?(boy2,7) ? p(e4) ? p(e5)
?p(e6) ? ?(a2,3) ? ?(a5,6) (5)
Now the fractional count of the subtree t is
c(t) = ??(t)??(TOP ) (6)
where TOP denotes the root node of the forest.
As a well-formed structure might be non-
constituent, we approximate the fractional count
by taking that of the minimal constituent tree frag-
ment that contains the well-formed structure. Fi-
nally, the fractional counts of well-formed struc-
tures can be used to compute the relative frequen-
cies of the rules having them on the target side (Mi
and Huang, 2008):
?(r|lhs(r)) = c(r)?
r?:lhs(r?)=lhs(r) c(r?)
(7)
?(r|rhs(r)) = c(r)?
r?:rhs(r?)=rhs(r) c(r?)
(8)
Often, our approach extracts a large amount of
rules from training corpus as we usually retain ex-
ponentially many well-formed structures over a
target phrase. To maintain a reasonable rule ta-
ble size, we discard any rule that has a fractional
count lower that a threshold t.
5 Forest-based Dependency Language
Model Training
Dependency language model plays an important
role in string-to-dependency system. Shen et
al. (2008) show that string-to-dependency system
achieves 1.48 point improvement in BLEU along
with dependency language model, while no im-
provement without it. However, the string-to-
dependency system still commits to using depen-
dency language model from noisy 1-best trees.
We now turn to dependency forest for it encodes
multiple dependency trees.
To train a dependency language model from a
dependency forest, we need to collect all heads
and their dependants. This can be easily done by
enumerating all hyperedges. Similarly, we use the
inside-outside algorithm to compute the posterior
probability of each hyperedge e,
??(e) = ?(head(e)) ? p(e)
?
?
v?tailes(e)
?(v) (9)
For example, the posterior probability of the hy-
peredge e2 in Figure 4 is calculated as
??(e2) = ?(saw0,7) ? p(e2)
??(he0,1) ? ?(boy2,7) (10)
1096
Rule DepLM NIST 2004 NIST 2005 NIST 2006 time
tree tree 33.97 30.21 30.73 19.6
tree forest 34.42? 31.06? 31.37? 24.1
forest tree 34.60? 31.16? 31.45? 21.7
forest forest 35.33?? 31.57?? 32.19?? 28.5
Table 1: BLEU scores and average decoding time (second/sentence) on the Chinese-English test sets.
The baseline system (row 2) used the rule table and dependency language model learned both from
1-best dependency trees. We use ? *? and ?**? to denote a result is better than baseline significantly at
p < 0.05 and p < 0.01, respectively.
Then, we can obtain the fractional count of a
hyperedge e,
c(e) = ??(e)??(TOP ) (11)
Each n-gram (e.g., ?boy-as-head a?) is assigned
the same fractional count of the hyperedge it be-
longs to.
We also tried training dependency language
model as in (Shen et al, 2008), which means
all hyperedges were on equal footing without re-
garding probabilities. However, the performance
is about 0.8 point lower in BLEU. One possbile
reason is that hyperedges with probabilities could
distinguish high quality structures better.
6 Experiments
6.1 Results on the Chinese-English Task
We used the FBIS corpus (6.9M Chinese words
+ 8.9M English words) as our bilingual train-
ing corpus. We ran GIZA++ (Och and Ney,
2000) to obtain word alignments. We trained a
4-gram language model on the Xinhua portion
of GIGAWORD corpus using the SRI Language
Modeling Toolkit (Stolcke, 2002) with modi-
fied Kneser-Ney smoothing (Kneser and Ney,
1995). We optimized feature weights using the
minimum error rate training algorithm (Och and
Ney, 2002) on the NIST 2002 test set. We evalu-
ated the translation quality using case-insensitive
BLEU metric (Papineni et al, 2002) on the NIST
2004/2005/2006 test sets.
To obtain dependency trees and forests, we
parsed the English sentences of the FBIS corpus
using a shift-reduce dependency parser that en-
ables beam search (Huang et al, 2009). We only
Rules Size New Rules
tree 7.2M -
forest 7.6M 16.86%
Table 2: Statistics of rules. The last column shows
the ratio of rules extracted from non 1-best parses
being used in 1-best derivations.
retained the best well-formed structure for each
node when extracting string-to-tree rules from de-
pendency forests (i.e., k = 1). We trained two
3-gram depLMs (one from trees and another from
forests) on English side of FBIS corpus plus 2M
sentence pairs from other LDC corpus.
After extracting rules and training depLMs, we
ran our replication of string-to-dependency sys-
tem (Shen et al, 2008) to translate the develop-
ment and test sets.
Table 1 shows the BLEU scores on the test
sets. The first column ?Rule? indicates where
the string-to-dependency rules are learned from:
1-best dependency trees or dependency forests.
Similarly, the second column ?DepLM? also dis-
tinguish between the two sources for training de-
pendency language models. The baseline sys-
tem used the rule table and dependency lan-
guage model both learned from 1-best depen-
dency trees. We find that adding the rule table and
dependency language models obtained from de-
pendency forests improves string-to-dependency
translation consistently and significantly, ranging
from +1.3 to +1.4 BLEU points. In addition, us-
ing the rule table and dependency language model
trained from forest only increases decoding time
insignificantly.
How many rules extracted from non 1-best
1097
Rule DepLM BLEU
tree tree 22.31
tree forest 22.73?
forest tree 22.80?
forest forest 23.12??
Table 3: BLEU scores on the Korean-Chinese test
set.
parses are used by the decoder? Table 2 shows the
number of rules filtered on the test set. We observe
that the rule table size hardly increases. One pos-
sible reason is that we only keep the best depen-
dency structure for each node. The last row shows
that 16.86% of the rules used in 1-best deriva-
tions are extracted from non 1-best parses in the
forests, indicating that some useful rules cannot
be extracted from 1-best parses.
6.2 Results on the Korean-Chinese Task
To examine the efficacy of our approach on differ-
ent language pairs, we carried out an experiment
on Korean-Chinese translation. The training cor-
pus contains about 8.2M Korean words and 7.3M
Chinese words. The Chinese sentences were used
to train a 5-gram language model as well as a 3-
gram dependency language model. Both the de-
velopment and test sets consist of 1,006 sentences
with single reference. Table 3 shows the BLEU
scores on the test set. Again, our forest-based ap-
proach achieves significant improvement over the
baseline (p < 0.01).
6.3 Effect of K-best
We investigated the effect of different k-best
structures for each node on translation quality
(BLEU scores on the NIST 2005 set) and the rule
table size (filtered for the tuning and test sets), as
shown in Figure 5. To save time, we extracted
rules just from the first 30K sentence pairs of the
FBIS corpus. We trained a language model and
depLMs on the English sentences. We used 10
different k: 1, 2, 3, 4, 5, 6, 7, 8, 9 and 10. Ob-
viously, the higher the k is, the more rules are
extracted. When k=10, the number of rules used
on the tuning and test sets was 1,299,290 and the
BLEU score was 20.88. Generally, both the num-
ber of rules and the BLEU score went up with
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35
BL
EU
 s
co
re
rule table size(M)
k=1,2,...,10
Figure 5: Effect of k-best on rule table size and
translation quality.
20.4
20.5
20.6
20.7
20.8
20.9
21.0
21.1
21.2
21.3
21.4
21.5
21.6
21.7
21.8
0.98 1.00 1.02 1.04 1.06 1.08 1.10
BL
EU
 s
co
re
rule table size(M)
t=1.0,0.9,...,0.1
Figure 6: Effect of pruning threshold on rule table
size and translation quality.
the increase of k. However, this trend did not
hold within the range [4,10]. We conjecture that
when retaining more dependency structures for
each node, low quality structures would be intro-
duced, resulting in much rules of low quality.
An interesting finding is that the rule table grew
rapidly when k is in range [1,4], while gradually
within the range [4,10]. One possible reason is
that there are limited different dependency struc-
tures in the spans with a maximal length of 10,
which the target side of rules cover.
6.4 Effect of Pruning Threshold
Figure 6 shows the effect of pruning threshold on
translation quality and the rule table size. We
retained 10-best dependency structures for each
node in dependency forests. We used 10 different
1098
pruning thresholds: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
0.8, 0.9 and 1.0. Intuitively, the higher the prun-
ing threshold is, the less rules are extracted. When
t=0.1, the number of rules used on the tuning and
test sets was 1,081,841 and the BLEU score was
20.68.
Lots of rules are pruned when the pruning
threshold increases from 0.0 to 0.3 (around 20%).
After pruning away these rules, we achieved 0.6
point improvement in BLEU. However, when we
filtered more rules, the BLEU score went down.
Figures 5 and 6 show that using two parame-
ters that have to be hand-tuned achieves a small
improvement at the expense of an additional com-
plexity. To simplify the approach, we only keep
the best dependency structure for each node with-
out pruning any rule.
7 Related Works
While Mi and Huang (2008) and we both use
forests for rule extraction, there remain two ma-
jor differences. Firstly, Mi and Huang (2008) use
a packed forest, while we use a dependency forest.
Packed forest is a natural weighted hypergraph
(Klein and Manning, 2001; Huang and Chiang,
2005), for each hyperedge treats the correspond-
ing PCFG rule probability as its weight. However,
it is challenging to make dependency forest to be a
weighted hypergraph because dependency parsers
usually only output a score for each edge in a de-
pendency tree rather than a hyperedge in a depen-
dency forest. Secondly, The GHKM algorithm
(Galley et al, 2004), which is originally devel-
oped for extracting tree-to-string rules from 1-best
trees, has been successfully extended to packed
forests recently (Mi and Huang, 2008). Unfor-
tunately, the GHKM algorithm cannot be applied
to extracting string-to-dependency rules from de-
pendency forests, because the GHKM algorithm
requires a complete subtree to exist in a rule while
neither fixed nor floating dependency structures
ensure that all dependants of a head are included.
8 Conclusion and Future Work
In this paper, we have proposed to use dependency
forests instead of 1-best parses to extract string-to-
dependency tree rules and train dependency lan-
guage models. Our experiments show that our ap-
proach improves translation quality significantly
over a state-of-the-art string-to-dependency sys-
tem on various language pairs and test sets. We
believe that dependency forest can also be used to
improve the dependency treelet system (Quirk et
al., 2005) that takes 1-best trees as input.
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang for his invaluable help in dependency
forest.
References
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, pages 201?
228.
Ding, Yuan and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency
insertion grammars. In Proceedings of ACL.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proceedings of ACL.
Galley, Michel, Mark Hopkins, Kevin Knight, and
Daniel Marcu. 2004. What?s in a translation rule?
In Proceedings of NAACL.
Huang, Liang and David Chiang. 2005. Better k-best
parsing. In Proceedings of IWPT.
Huang, Liang, Wenbin Jiang, and Qun Liu. 2009.
Bilingually-constrained (monolingual) shift-reduce
parsing. In Proceedings of EMNLP.
Klein, Dan and Christopher D. Manning. 2001. Pars-
ing and hypergraphs. In Proceedings of IWPT.
Kneser, R. and H. Ney. 1995. Improved backing-off
for m-gram language modeling. In Proceedings of
Acoustics, Speech, and Signal.
Liu, Yang, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of EMNLP.
Mi, Haitao and Liang Huang. 2008. Forest-based
translation rule extraction. In Proceedings of
EMNLP.
1099
Och, Franz J. and Hermann Ney. 2000. Improved sta-
tistical alignment models. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proceedings of ACL.
Och, Franz J. and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of ACL.
Quirk, Chris and Simon Corston-Oliver. 2006. The
impact of parsing quality on syntactically-informed
statistical machine translation. In Proceedings of
EMNLP.
Quirk, Chris, Arul Menezes, and Colin Cherry. 2005.
Dependency treelet translation: syntactically in-
formed phrasal smt. In Proceedings of ACL.
Shen, Libin, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL.
Stolcke, Andreas. 2002. Srilm - an extensible lan-
guage modeling toolkit. In Proceedings of ICSLP.
Wang, Wei, Kevin Knight, and Daniel Marcu. 2007.
Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proceedings of
EMNLP.
1100
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1200?1208,
Beijing, August 2010
Joint Tokenization and Translation
Xinyan Xiao ? Yang Liu ? Young-Sook Hwang ? Qun Liu ? Shouxun Lin ?
?Key Lab. of Intelligent Info. Processing ?HILab Convergence Technology Center
Institute of Computing Technology C&I Business
Chinese Academy of Sciences SKTelecom
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn yshwang@sktelecom.com
Abstract
As tokenization is usually ambiguous for
many natural languages such as Chinese
and Korean, tokenization errors might po-
tentially introduce translation mistakes for
translation systems that rely on 1-best to-
kenizations. While using lattices to of-
fer more alternatives to translation sys-
tems have elegantly alleviated this prob-
lem, we take a further step to tokenize
and translate jointly. Taking a sequence
of atomic units that can be combined to
form words in different ways as input, our
joint decoder produces a tokenization on
the source side and a translation on the
target side simultaneously. By integrat-
ing tokenization and translation features
in a discriminative framework, our joint
decoder outperforms the baseline trans-
lation systems using 1-best tokenizations
and lattices significantly on both Chinese-
English and Korean-Chinese tasks. In-
terestingly, as a tokenizer, our joint de-
coder achieves significant improvements
over monolingual Chinese tokenizers.
1 Introduction
Tokenization plays an important role in statistical
machine translation (SMT) because tokenizing a
source-language sentence is always the first step
in SMT systems. Based on the type of input, Mi
and Huang (2008) distinguish between two cat-
egories of SMT systems : string-based systems
(Koehn et al, 2003; Chiang, 2007; Galley et al,
source
target
tokenize+translate
string tokenization
translation
source
target
string
tokenize
tokenization
translate
translation
(a)
(b)
Figure 1: (a) Separate tokenization and translation and (b)
joint tokenization and translation.
2006; Shen et al, 2008) that take a string as input
and tree-based systems (Liu et al, 2006; Mi et al,
2008) that take a tree as input. Note that a tree-
based system still needs to first tokenize the input
sentence and then obtain a parse tree or forest of
the sentence. As shown in Figure 1(a), we refer to
this pipeline as separate tokenization and transla-
tion because they are divided into single steps.
As tokenization for many languages is usually
ambiguous, SMT systems that separate tokeniza-
tion and translation suffer from a major drawback:
tokenization errors potentially introduce transla-
tion mistakes. As some languages such as Chi-
nese have no spaces in their writing systems, how
to segment sentences into appropriate words has
a direct impact on translation performance (Xu et
al., 2005; Chang et al, 2008; Zhang et al, 2008).
In addition, although agglutinative languages such
as Korean incorporate spaces between ?words?,
which consist of multiple morphemes, the gran-
ularity is too coarse and makes the training data
1200
considerably sparse. Studies reveal that seg-
menting ?words? into morphemes effectively im-
proves translating morphologically rich languages
(Oflazer, 2008). More importantly, a tokenization
close to a gold standard does not necessarily leads
to better translation quality (Chang et al, 2008;
Zhang et al, 2008). Therefore, it is necessary
to offer more tokenizations to SMT systems to
alleviate the tokenization error propagation prob-
lem. Recently, many researchers have shown that
replacing 1-best tokenizations with lattices im-
proves translation performance significantly (Xu
et al, 2005; Dyer et al, 2008; Dyer, 2009).
We take a next step towards the direction of
offering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), our approach tokenizes
and translates jointly to find a tokenization and
a translation for a source-language string simul-
taneously. We integrate translation and tokeniza-
tion models into a discriminative framework (Och
and Ney, 2002), within which tokenization and
translation models interact with each other. Ex-
periments show that joint tokenization and trans-
lation outperforms its separate counterparts (1-
best tokenizations and lattices) significantly on
the NIST 2004 and 2005 Chinese-English test
sets. Our joint decoder also reports positive results
on Korean-Chinese translation. As a tokenizer,
our joint decoder achieves significantly better to-
kenization accuracy than three monolingual Chi-
nese tokenizers.
2 Separate Tokenization and Translation
Tokenization is to split a string of characters into
meaningful elements, which are often referred to
as words. Typically, machine translation sepa-
rates tokenization from decoding as a preprocess-
ing step. An input string is first preprocessed by a
tokenizer, and then is translated based on the tok-
enized result. Take the SCFG-based model (Chi-
ang, 2007) as an example. Given the character
sequence of Figure 2(a), a tokenizer first splits it
into the word sequence as shown in Figure 2(b),
then the decoder translates the word sequence us-
ing the rules in Table 1.
This approach makes the translation process
simple and efficient. However, it may not be
? ? ? ? ? ? 0? 1 2 3 4 5 6 7
Figure 2: Chinese tokenization: (a) character sequence; (b)
and (c) tokenization instances; (d) lattice created from (b)
and (c). We insert ?-? between characters in a word just for
clarity.
r1 tao-fei-ke ?Taufik
r2 duo fen ? gain a point
r3 x1 you-wang x2 ? x1 will have the chance to x2
Table 1: An SCFG derivation given the tokenization of Fig-
ure 2(b).
optimal for machine translation. Firstly, optimal
granularity is unclear for machine translation. We
might face severe data sparseness problem by us-
ing large granularity, while losing much useful in-
formation with small one. Consider the example
in Figure 2. It is reasonable to split duo fen into
two words as duo and fen, since they have one-
to-one alignments to the target side. Nevertheless,
while you and wang also have one-to-one align-
ments, it is risky to segment them into two words.
Because the decoder is prone to translate wang as
a verb look without the context you. Secondly,
there may be tokenization errors. In Figure2(c),
tao fei ke is recognized as a Chinese person name
with the second name tao and the first name fei-ke,
but the whole string tao fei ke should be a name of
the Indonesian badminton player.
Therefore, it is necessary to offer more tok-
enizations to SMT systems to alleviate the tok-
enization error propagation problem. Recently,
many researchers have shown that replacing 1-
best tokenizations with lattices improves transla-
tion performance significantly. In this approach, a
lattice compactly encodes many tokenizations and
is fixed before decoding.
1201
0 1 2 3 4 5 6 7
1 2
3
Figure 3: A derivation of the joint model for the tokenization
in Figure 2(b) and the translation in Figure 2 by using the
rules in Table 1. N means tokenization while  represents
translation.
3 Joint Tokenization and Translation
3.1 Model
We take a next step towards the direction of of-
fering more tokenizations to SMT systems by
proposing joint tokenization and translation. As
shown in Figure 1(b), the decoder takes an un-
tokenized string as input, and then tokenizes the
source side string while building the correspond-
ing translation of the target side. Since the tradi-
tional rules like those in Table 1 natively include
tokenization information, we can directly apply
them for simultaneous construction of tokeniza-
tion and translation by the source side and target
side of rules respectively. In Figure 3, our joint
model takes the character sequence in Figure 2(a)
as input, and synchronously conducts both trans-
lation and tokenization using the rules in Table 1.
As our model conducts tokenization during de-
coding, we can integrate tokenization models as
features together with translation features under
the discriminative framework. We expect tok-
enization and translation could collaborate with
each other. Tokenization offers translation with
good tokenized results, while translation helps to-
kenization to eliminate ambiguity. Formally, the
probability of a derivation D is represented as
P (D) ?
?
i
?i(D)?i (1)
where ?i are features defined on derivations in-
cluding translation and tokenization, and ?i are
feature weights. We totally use 16 features:
? 8 traditional translation features (Chiang,
2007): 4 rule scores (direct and reverse trans-
lation scores; direct and reverse lexical trans-
lation scores); language model of the target
side; 3 penalties for word count, extracted
rule and glue rule.
? 8 tokenization features: maximum entropy
model, language model and word count of
the source side (Section 3.2). To handle
the Out Of Vocabulary (OOV) problem (Sec-
tion 3.3), we also introduce 5 OOV features:
OOV character count and 4 OOV discount
features.
Since our model is still a string-based model, the
CKY algorithm and cube pruning are still applica-
ble for our model to find the derivation with max
score.
3.2 Adding Tokenization Features
Maximum Entropy model (ME). We first intro-
duce ME model feature for tokenization by cast-
ing it as a labeling problem (Xue and Shen, 2003;
Ng and Low, 2004). We label a character with the
following 4 types:
? b: the begin of a word
? m: the middle of a word
? e: the end of a word
? s: a single-character word
Taking the tokenization you-wang of the string
you wang for example, we first create a label se-
quence b e for the tokenization you-wang and then
calculate the probability of tokenization by
P (you-wang | you wang)
= P (b e | you wang)
= P (b | you, you wang)
? P (e | wang, you wang)
Given a tokenization wL1 with L words for a
character sequence cn1 , we firstly create labels ln1
for every characters and then calculate the proba-
bility by
P (wL1 |cn1 ) = P (ln1 |cn1 ) =
n?
i=1
P (li|ci, cn1 ) (2)
1202
Under the ME framework, the probability of as-
signing the character c with the label l is repre-
sented as:
P (l|c, cn1 ) =
exp[?i ?ihi(l, c, cn1 )]?
l? exp[
?
i ?ihi(l?, c, cn1 )]
(3)
where hi is feature function, ?i is the feature
weight of hi. We use the feature templates the
same as Jiang et al, (2008) to extract features for
ME model. Since we directly construct tokeniza-
tion when decoding, it is straight to calculate the
ME model score of a tokenization according to
formula (2) and (3).
Language Model (LM). We also use the n-
gram language model to calculate the probability
of a tokenization wL1 :
P (wL1 ) =
L?
i=1
P (wi|wi?1i?n+1) (4)
For instance, we compute the probability of the
tokenization shown in Figure 2(b) under a 3-gram
model by
P (tao-fei-ke)
?P (you-wang | tao-fei-ke)
?P (duo | tao-fei-ke, you-wang)
?P (fen | you-wang, duo)
Word Count (WC). This feature counts the
number of words in a tokenization. Language
model is prone to assign higher probabilities to
short sentences in a biased way. This feature can
compensate this bias by encouraging long sen-
tences. Furthermore, using this feature, we can
optimize the granularity of tokenization for trans-
lation. If larger granularity is preferable for trans-
lation, then we can use this feature to punish the
tokenization containing more words.
3.3 Considering All Tokenizations
Obviously, we can construct the potential tok-
enizations and translations by only using the ex-
tracted rules, in line with traditional translation
decoding. However, it may limits the potential to-
kenization space. Consider a string you wang. If
you-wang is not reachable by the extracted rules,
the tokenization you-wang will never be consid-
ered under this way. However, the decoder may
still create a derivation by splitting the string as
small as possible with tokenization you wang and
translating you with a and wang with look, which
may hurt the translation performance. This case
happens frequently for named entity especially.
Overall, it is necessary to assure that the de-
coder can derive all potential tokenizations (Sec-
tion 4.1.3).
To assure that, when a span is not tokenized into
a single word by the extracted rules, we will add
an operation, which is considering the entire span
as an OOV. That is, we tokenize the entire span
into a single word with a translation that is the
copy of source side. We can define the set of all
potential tokenizations ?(cn1 ) for the character se-
quence cn1 in a recursive way by
?(cn1 ) =
n?1?
i
{?(ci1)
?
{w(cni+1)}} (5)
here w(cni+1) means a word contains characters
cni+1 and
?
means the times of two sets. Ac-
cording to this recursive definition, it is easy to
prove that all tokenizations is reachable by using
the glue rule (S ? SX,SX) and the added op-
eration. Here, glue rule is used to concatenate the
translation and tokenization of the two variables S
and X, which acts the role of the operator ? in
equation (5).
Consequently, this introduces a large number
of OOVs. In order to control the generation of
OOVs, we introduce the following OOV features:
OOV Character Count (OCC). This feature
counts the number of characters covered by OOV.
We can control the number of OOV characters by
this feature. It counts 3 when tao-fei-ke is an OOV,
since tao-fei-ke has 3 characters.
OOV Discount (OD). The chances to be OOVs
vary for words with different counts of characters.
We can directly attack this problem by adding
features ODi that reward or punish OOV words
which contains with i characters, or ODi,j for
OOVs contains with i to j characters. 4 OD fea-
tures are used in this paper: 1, 2, 3 and 4+. For
example, OD3 counts 1 when the word tao-fei-ke
is an OOV.
1203
Method Train #Rule Test TFs MT04 MT05 Speed
Separate
ICT 151M ICT ? 34.82 33.06 2.48
SF 148M SF ? 35.29 33.22 2.55
ME 141M ME ? 33.71 30.91 2.34
All 219M Lattice ? 35.79 33.95 3.83? 35.85 33.76 6.79
Joint
ICT 151M
Character
?
36.92 34.69 17.66
SF 148M 37.02 34.56 17.37
ME 141M 36.78 34.17 17.23
All 219M 37.25** 34.88** 17.52
Table 2: Comparison of Separate and Joint methods in terms of BLEU and speed (second per sentence). Columns Train
and Test represents the tokenization methods for training and testing respectively. Column TFs stands for whether the 8
tokenization features is used (?) or not (?). ICT, SF and ME are segmenter names for preprocessing. All means combined
corpus processed by the three segmenters. Lattice represent the system implemented as Dyer et al, (2008). ** means
significantly (Koehn, 2004) better than Lattice (p < 0.01).
4 Experiments
In this section, we try to answer the following
questions:
1. Does the joint method outperform conven-
tional methods that separate tokenization
from decoding. (Section 4.1)
2. How about the tokenization performance of
the joint decoder? (Section 4.2)
4.1 Translation Evaluation
We use the SCFG model (Chiang, 2007) for our
experiments. We firstly work on the Chinese-
English translation task. The bilingual training
data contains 1.5M sentence pairs coming from
LDC data.1 The monolingual data for training
English language model includes Xinhua portion
of the GIGAWORD corpus, which contains 238M
English words. We use the NIST evaluation sets
of 2002 (MT02) as our development data set, and
sets of 2004(MT04) and 2005(MT05) as test sets.
We use the corpus derived from the People?s Daily
(Renmin Ribao) in Feb. to Jun. 1998 containing
6M words for training LM and ME tokenization
models.
Translation Part. We used GIZA++ (Och and
Ney, 2003) to perform word alignment in both di-
rections, and grow-diag-final-and (Koehn et al,
2003) to generate symmetric word alignment. We
extracted the SCFG rules as describing in Chiang
(2007). The language model were trained by the
1including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and
LDC2005T06
SRILM toolkit (Stolcke, 2002).2 Case insensitive
NIST BLEU (Papineni et al, 2002) was used to
measure translation performance.
Tokenization Part. We used the toolkit imple-
mented by Zhang (2004) to train the ME model.
Three Chinese word segmenters were used for
comparing: ICTCLAS (ICT) developed by insti-
tute of Computing Technology Chinese Academy
of Sciences (Zhang et al, 2003); SF developed at
Stanford University (Huihsin et al, 2005) and ME
which exploits the ME model described in section
(3.2).
4.1.1 Joint Vs. Separate
We compared our joint tokenization and trans-
lation with the conventional separate methods.
The input of separate tokenization and translation
can either be a single segmentation or a lattice.
The lattice combines the 1-best segmentations of
segmenters. Same as Dyer et al, (2008), we also
extracted rules from a combined bilingual corpus
which contains three copies from different seg-
menters. We refer to this version of rules as All.
Table 2 shows the result.3 Using all rule ta-
ble, our joint method significantly outperforms the
best single system SF by +1.96 and +1.66 points
on MT04 and MT05 respectively, and also out-
performs the lattice-based system by +1.46 and
+0.93 points. However, the 8 tokenization fea-
tures have small impact on the lattice system,
probably because the tokenization space limited
2The calculation of LM probabilities for OOVs is done
by the SRILM without special treatment by ourself.
3The weights are retrained for different test conditions, so
do the experiments in other sections.
1204
ME LM WC OCC OD MT05
? ? ? ? ? 24.97? ? ? ? ? 25.30
? ? ? ? ? 24.70
? ? ? ? ? 24.84
? ? ? ? ? 25.51
? ? ? ? ? 25.34
? ? ? ? ? 25.74? ? ? ? ?
26.37
Table 3: Effect of tokenization features on Chinese-English
translation task. ?
?
? denotes using a tokenization feature
while ??? denotes that it is inactive.
by lattice has been created from good tokeniza-
tion. Not surprisingly, our decoding method is
about 2.6 times slower than lattice method with
tokenization features, since the joint decoder takes
character sequences as input, which is about 1.7
times longer than the corresponding word se-
quences tokenized by segmenters. (Section 4.1.4).
The number of extracted rules with different
segment methods are quite close, while the All
version contains about 45% more rules than the
single systems. With the same rule table, our joint
method improves the performance over separate
method up to +3.03 and +3.26 points (ME). In-
terestingly, comparing with the separate method,
the tokenization of training data has smaller effect
on joint method. The BLEU scores of MT04 and
MT05 fluctuate about 0.5 and 0.7 points when ap-
plying the joint method, while the difference of
separate method is up to 2 and 3 points respec-
tively. It shows that the joint method is more ro-
bust to segmentation performance.
4.1.2 Effect of Tokenization Model
We also investigated the effect of tokenization
features on translation. In order to reduce the time
for tuning weights and decoding, we extracted
rules from the FBIS part of the bilingual corpus,
and trained a 4-gram English language model on
the English side of FBIS.
Table 3 shows the result. Only using the 8 trans-
lation features, our system achieves a BLEU score
of 24.97. By activating all tokenization features,
the joint decoder obtains an absolute improve-
ment by 1.4 BLEU points. When only adding
one single tokenization feature, the LM and WC
fail to show improvement, which may result from
their bias to short or long tokenizations. How-
Method BLEU #Word Grau #OOV
ICT 33.06 30,602 1.65 644
SF 33.22 30,119 1.68 882
ME 30.91 29,717 1.70 1,614
Lattice 33.95 30,315 1.66 494
JointICT 34.69 29,723 1.70 996
JointSF 34.56 29,839 1.69 972
JointME 34.17 29,771 1.70 1,062
JointAll 34.88 29,644 1.70 883
Table 4: Granularity (Grau, counts of character per word)
and counts of OOV words of different methods on MT05.
The subscript of joint means the type of rule table.
ever, these two features have complementary ad-
vantages and collaborate well when using them to-
gether (line 8). The OCC and OD features also
contribute improvements which reflects the fact
that handling the generation of OOV is important
for the joint model.
4.1.3 Considering All Tokenizations?
In order to explain the necessary of considering
all potential tokenizations, we compare the perfor-
mances of whether to tokenize a span as a single
word or not as illustrated in section 3.3. When
only tokenizing by the extracted rules, we obtain
34.37 BLEU on MT05, which is about 0.5 points
lower than considering all tokenizations shown in
Table 2. This indicates that spuriously limitation
of the tokenization space may degenerate transla-
tion performance.
4.1.4 Results Analysis
To better understand why the joint method can
improve the translation quality, this section shows
some details of the results on the MT05 data set.
Table 4 shows the granularity and OOV word
counts of different configurations. The lattice
method reduces the OOV words quite a lot which
is 23% and 70% comparing with ICT and ME. In
contrast, the joint method gain an absolute im-
provement even thought the OOV count do not
decrease. It seems the lattice method prefers to
translate more characters (since smaller granular-
ity and less OOVs), while our method is inclined
to maintain integrity of words (since larger granu-
larity and more OOVs). This also explains the dif-
ficulty of deciding optimal tokenization for trans-
lation before decoding.
There are some named entities or idioms that
1205
Method Type F1 Time
Monolingual
ICT 97.47 0.010
SF 97.48 0.007
ME 95.53 0.008
Joint
ICT 97.68 9.382
SF 97.68 10.454
ME 97.60 10.451
All 97.70 9.248
Table 5: Comparison of segmentation performance in terms
of F1 score and speed (second per sentence). Type column
means the segmenter for monolingual method, while repre-
sents the rule tables used by joint method.
are split into smaller granularity by the seg-
menters. For example:???? which is an English
name ?Stone? or ??-g -u? which means
?teenage?. Although the separate method is possi-
ble to translate them using smaller granularity, the
translation results are in fact wrong. In contrast,
the joint method tokenizes them as entire OOV
words, however, it may result a better translation
for the whole sentence.
We also count the overlap of the segments
used by the JointAll system towards the single
segmentation systems. The tokenization result
of JointAll contains 29, 644 words, and shares
28, 159 , 27, 772 and 27, 407 words with ICT ,
SF and ME respectively. And 46 unique words
appear only in the joint method, where most of
them are named entity.
4.2 Chinese Word Segmentation Evaluation
We also test the tokenization performance of our
model on Chinese word segmentation task. We
randomly selected 3k sentences from the corpus
of People?s Daily in Jan. 1998. 1k sentences
were used for tuning weights, while the other 2k
sentences were for testing. We use MERT (Och,
2003) to tune the weights by minimizing the error
measured by F1 score.
As shown in Table 5, with all features activated,
our joint decoder achieves an F1 score of 97.70
which reduces the tokenization error comparing
with the best single segmenter ICT by 8.7%. Sim-
ilar to the translation performance evaluation, our
joint decoder outperforms the best segmenter with
any version of rule tables.
Feature F1
TFs 97.37
TFs + RS 97.65
TFs + LM 97.67
TFs + RS + LM 97.62
All 97.70
Table 6: Effect of the target side information on Chinese
word segmentation. TFs stands for the 8 tokenization fea-
tures. All represents all the 16 features.
4.2.1 Effect of Target Side Information
We compared the effect of the 4 Rule Scores
(RS), target side Language Model (LM) on tok-
enization. Table 6 shows the effect on Chinese
word segmentation. When only use tokenization
features, our joint decoder achieves an F1 score
of 97.37. Only integrating language model or rule
scores, the joint decoder achieves an absolute im-
provement of 0.3 point in F1 score, which reduces
the error rate by 11.4%. However, when combin-
ing them together, the F1 score deduces slightly,
which may result from the weight tuning. Us-
ing all feature, the performance comes to 97.70.
Overall, our experiment shows that the target side
information can improve the source side tokeniza-
tion under a supervised way, and outperform state-
of-the-art systems.
4.2.2 Best Tokenization = Best Translation?
Previous works (Zhang et al, 2008; Chang et
al., 2008) have shown that preprocessing the in-
put string for decoder by better segmenters do
not always improve the translation quality, we re-
verify this by testing whether the joint decoder
produces good tokenization and good translation
at the same time. To answer the question, we
used the feature weights optimized by maximiz-
ing BLEU for tokenization and used the weights
optimized by maximizing F1 for translation. We
test BLEU on MT05 and F1 score on the test data
used in segmentation evaluation experiments. By
tuning weights regarding to BLEU (the configura-
tion for JointAll in table 2), our decoder achieves
a BLEU score of 34.88 and an F1 score of 92.49.
Similarly, maximizing F1 (the configuration for
the last line in table 6) leads to a much lower
BLEU of 27.43, although the F1 is up to 97.70.
This suggests that better tokenization may not al-
ways lead to better translations and vice versa
1206
Rule #Rule Method Test Time
Morph 46M Separate 21.61 4.12Refined 55M 21.21 4.63
All 74M Joint 21.93* 5.10
Table 7: Comparison of Separate and Joint method in terms
of BLEU score and decoding speed (second per sentence) on
Korean-Chinese translation task.
even by the joint decoding. This also indicates the
hard of artificially defining the best tokenization
for translation.
4.3 Korean-Chinese Translation
We also test our model on a quite different task:
Korean-Chinese. Korean is an agglutinative lan-
guage, which comes from different language fam-
ily comparing with Chinese.
We used a newswire corpus containing 256k
sentence pairs as training data. The development
and test data set contain 1K sentence each with
one single reference. We used the target side of
training set for language model training. The Ko-
rean part of these data were tokenized into mor-
pheme sequence as atomic unit for our experi-
ments.
We compared three methods. First is directly
use morpheme sequence (Morph). The second
one is refined data (Refined), where we use selec-
tive morphological segmentation (Oflazer, 2008)
for combining morpheme together on the training
data. Since the selective method needs alignment
information which is unavailable in the decod-
ing, the test data is still of morpheme sequence.
These two methods still used traditional decoding
method. The third one extracting rules from com-
bined (All) data of methods 1 and 2, and using
joint decoder to exploit the different granularity
of rules.
Table 7 shows the result. Since there is no gold
standard data for tokenization, we do not use ME
and LM tokenization features here. However, our
joint method can still significantly (p < 0.05) im-
prove the performance by about +0.3 points. This
also reflects the importance of optimizing granu-
larity for morphological complex languages.
5 Related Work
Methods have been proposed to optimize tok-
enization for word alignment. For example, word
alignment can be simplified by packing (Ma et al,
2007) several consecutive words together. Word
alignment and tokenization can also be optimized
by maximizing the likelihood of bilingual corpus
(Chung and Gildea, 2009; Xu et al, 2008). In fact,
these work are orthogonal to our joint method,
since they focus on training step while we are con-
cerned of decoding. We believe we can further
the performance by combining these two kinds of
work.
Our work also has connections to multilingual
tokenization (Snyder and Barzilay, 2008). While
they have verified that tokenization can be im-
proved by multilingual learning, our work shows
that we can also improve tokenization by collabo-
rating with translation task in a supervised way.
More recently, Liu and Liu (2010) also shows
the effect of joint method. They integrate parsing
and translation into a single step and improve the
performance of translation significantly.
6 Conclusion
We have presented a novel method for joint tok-
enization and translation which directly combines
the tokenization model into the decoding phase.
Allowing tokenization and translation to collab-
orate with each other, tokenization can be opti-
mized for translation, while translation also makes
contribution to tokenization performance under a
supervised way. We believe that our approach can
be applied to other string-based model such as
phrase-based model (Koehn et al, 2003), string-
to-tree model (Galley et al, 2006) and string-to-
dependency model (Shen et al, 2008).
Acknowledgement
The authors were supported by SK Telecom C&I
Business, and National Natural Science Founda-
tion of China, Contracts 60736014 and 60903138.
We thank the anonymous reviewers for their in-
sightful comments. We are also grateful to Wen-
bin Jiang, Zhiyang Wang and Zongcheng Ji for
their helpful feedback.
1207
References
Chang, Pi-Chuan, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese word segmen-
tation for machine translation performance. In the
Third Workshop on SMT.
Chiang, David. 2007. Hierarchical phrase-based
translation. Computational Linguistics, 33(2):201?
228.
Chung, Tagyoung and Daniel Gildea. 2009. Unsuper-
vised tokenization for machine translation. In Proc.
EMNLP 2009.
Dyer, Christopher, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice transla-
tion. In Proc. ACL 2008.
Dyer, Chris. 2009. Using a maximum entropy model
to build segmentation lattices for mt. In Proc.
NAACL 2009.
Galley, Michel, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proc.
ACL 2006.
Huihsin, Tseng, Pichuan Chang, Galen Andrew,
Daniel Jurafsky, and Christopher Manning. 2005.
A conditional random field word segmenter. In
Fourth SIGHAN Workshop.
Jiang, Wenbin, Liang Huang, Qun Liu, and Yajuan Lu?.
2008. A cascaded linear model for joint chinese
word segmentation and part-of-speech tagging. In
Proc. ACL 2008.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Koehn, Philipp. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
Liu, Yang and Qun Liu. 2010. Joint parsing and trans-
lation. In Proc. Coling 2010.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proc. ACL 2006.
Ma, Yanjun, Nicolas Stroppa, and Andy Way. 2007.
Bootstrapping word alignment via word packing. In
Proc. ACL 2007.
Mi, Haitao, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Ng, Hwee Tou and Jin Kiat Low. 2004. Chinese part-
of-speech tagging: One-at-a-time or all-at-once?
word-based or character-based? In Proc. EMNLP
2004.
Och, Franz J. and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Och, Franz Josef and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19?51.
Och, Franz Josef. 2003. Minimum error rate train-
ing in statistical machine translation. In Proc. ACL
2003.
Oflazer, Kemal. 2008. Statistical machine translation
into a morphologically complex language. In Proc.
CICL 2008.
Papineni, Kishore, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In Proc.
ACL 2002.
Shen, Libin, Xu Jinxi, and Weischedel Ralph. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proc. ACL 2008.
Snyder, Benjamin and Regina Barzilay. 2008. Un-
supervised multilingual learning for morphological
segmentation. In Proc. ACL 2008.
Stolcke, Andreas. 2002. Srilm ? an extensible lan-
guage modeling toolkit.
Xu, Jia, Evgeny Matusov, Richard Zens, and Her-
mann Ney. 2005. Integrated chinese word segmen-
tation in statistical machine translation. In Proc.
IWSLT2005.
Xu, Jia, Jianfeng Gao, Kristina Toutanova, and Her-
mann Ney. 2008. Bayesian semi-supervised
chinese word segmentation for statistical machine
translation. In Proc. Coling 2008.
Xue, Nianwen and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In SIGHAN Work-
shop.
Zhang, Hua-Ping, Hong-Kui Yu, De-Yi Xiong, and
Qun Liu. 2003. Hhmm-based chinese lexical an-
alyzer ictclas. In the Second SIGHAN Workshop.
Zhang, Ruiqiang, Keiji Yasuda, and Eiichiro Sumita.
2008. Improved statistical machine translation by
multiple Chinese word segmentation. In the Third
Workshop on SMT.
Zhang, Le. 2004. Maximum entropy modeling toolkit
for python and c++.
1208
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2042?2051, Dublin, Ireland, August 23-29 2014.
RED: A Reference Dependency Based MT Evaluation Metric
Hui Yu
??
Xiaofeng Wu
?
Jun Xie
?
Wenbin Jiang
?
Qun Liu
??
Shouxun Lin
?
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?
University of Chinese Academy of Sciences
{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn
?
CNGL, School of Computing, Dublin City University
{xiaofengwu,qliu}@computing.dcu.ie
Abstract
Most of the widely-used automatic evaluation metrics consider only the local fragments of the
references and translations, and they ignore the evaluation on the syntax level. Current syntax-
based evaluation metrics try to introduce syntax information but suffer from the poor pars-
ing results of the noisy machine translations. To alleviate this problem, we propose a novel
dependency-based evaluation metric which only employs the dependency information of the ref-
erences. We use two kinds of reference dependency structures: headword chain to capture the
long distance dependency information, and fixed and floating structures to capture the local con-
tinuous ngram. Experiment results show that our metric achieves higher correlations with human
judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra
linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance
which is better than METEOR and SEMPOS on system level, and is comparable with METEOR
on sentence level on WMT 2012 and WMT 2013.
1 Introduction
Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not
only evaluates the performance of MT systems, but also makes the development of MT systems rapider
(Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics
can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based
metrics.
The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR
(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing
the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect
the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric
(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and
syntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentially
noisy machine translations, so the improvement of their performance is restricted due to the serious
parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the
similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in
translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and
Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not
achieve the state-of-the-art performance.
In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only
employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation.
We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and
Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating
structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching
score between the headword chain and the translation, we use a distance-based similarity. Experiment
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2042
results show that our metric achieves higher correlations with human judgments than BLEU, TER and
HWCM on WMT 2012 and WMT 2013. After introducing extra resources and tuning parameters on
WMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable with
METEOR on sentence level on WMT 2012 and WMT2013.
The remainder of this paper is organized as follows. Section 2 describes our new reference dependency
based MT evaluation metric. In Section 3, we introduce some extra resources to this new metric. Section
4 presents the parameter tuning for the new metric. Section 5 gives the experiment results. Conclusions
and future work are discussed in Section 6.
2 RED: A Reference Dependency Based MT Evaluation Metric
The new metric is a REference Dependency based automatic evaluation metric, so we name it RED.
We present the new metric detailedly in this section. The description of dependency ngrams is given in
Section 2.1. The method to score the dependency ngram is presented in Section 2.2. At last, the method
of calculating the final score is introduced in Section 2.3.
2.1 Two Kinds of Dependency Ngrams
To capture both the long distance dependency information and the local continuous ngrams, we use both
the headword chain and the fixed-floating structures in our new metric, which correspond to the two
kinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.
Figure 1: An example of dependency tree.
Figure 2: Different kinds of structures extracted
from the dependency tree in Figure 1. (a): Head-
word chain. (b): Fixed structure. (c): Floating struc-
ture.
2.1.1 Headword chain
Headword chain is a sequence of words which corresponds to a path in the dependency tree (Liu and
Gildea, 2005). For example, Figure 2(a) is a 3-word headword chain extracted from the dependency tree
in Figure 1. Headword chain can represent the long distance dependency information, but cannot capture
most of the continuous ngrams. In our metric, headword chain corresponds to the headword chain ngram
in which the positions of the words are considered. So the form of headword chain ngram is expressed
as (w1
pos1
, w2
pos2
, ..., wn
posn
), where n is the length of the headword chain ngram. For example, the
headword chain in Figure 2(a) is expressed as (saw
2
, with
5
,magnifier
7
).
2.1.2 Fixed and floating structures
Fixed and floating structures are defined in Shen et al. (2010). Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. They are called fixed dependency structures
because the head is known or fixed. For example, Figure 2(b) shows a fixed structure. Floating structures
consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.
Each of the siblings must be a complete constituent. Figure 2(c) shows a floating structure. Fixed-
floating structures correspond to fixed-floating ngrams in our metric. Fixed-floating ngrams don?t need
the position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the
2043
Figure 3: An example of calculating matching score for a headword chain ngram
(saw
2
, with
5
,magnifier
7
). dis r
1
and dis r
2
are the distances between the corresponding two
words in the reference. dis h
1
and dis h
2
are the distances between the corresponding two words in the
hypothesis.
fixed-floating ngram. For example, the fixed structure in Figure 2(b) and the floating structre in Figure
2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.
2.2 Scoring Dep-ngrams
Headword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous. So the
scoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methods
in Section 2.2.1 and Section 2.2.2 respectively.
2.2.1 Scoring headword chain ngram
For a headword chain ngram (w1
pos1
, w2
pos2
, ..., wn
posn
), if we can find all these n words in the string
of the translation with the same order as they appear in the reference sentence, we consider it a match and
the matching score is a distance-based similarity which is calculated by the relative distance, otherwise it
is not a match and the score is 0. The matching score is a decimal value between 0 and 1, which is more
suitable than just use integer 0 and 1. For example, if the distance between two words in reference is 1,
but the distance in two different hypotheses are 2 and 5 respectively. It?s more reasonable to score them
0.5 and 0.2 rather than 1 and 0.
The relative distance dis r
i
between every two adjacent words in this kind of dep-ngram is calculated
by Formula (1), where pos
wi
is the position of word wi in the sentence. In Formula (1), we have
1 ? i ? n ? 1 and n is the length of the dep-ngram. Then a vector (dis r
1
, dis r
2
, ..., dis r
n?1
) is
obtained. In the same way, we obtain vector (dis h
1
, dis h
2
, ..., dis h
n?1
) for the translation side.
dis r
i
= |pos
w(i+1)
? pos
wi
| (1)
The matching score p
(d,hyp)
for a headword chain ngram (d) and the translation (hyp) is calculated
according to Formula (2), where n > 1. When the length of the dep-ngram equals 1, the matching score
equals 1 if the translation has the same word, otherwise, the matching score equals 0.
p
(d,hyp)
=
?
?
?
exp(?
?
n?1
i=1
|dis r
i
? dis h
i
|
n? 1
) if match
0 if unmatch
(2)
An example illustrating the calculation of the matching score p
(d,hyp)
is shown in Figure 3. There is
a 3-word headword chain ngram (saw
2
, with
5
,magnifier
7
) in the dependency tree of the reference.
2044
For this dep-3gram, the words are represented with underline in the reference dependency tree and the
reference sentence in Figure 3. We can also find all the same three underlined words in the translation
with the same order as they appear in the reference. Therefore, there is a match for this dep-3gram. To
compute the matching score between this dep-3gram and the translation, we have:
? Calculate the distance
dis r
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis r
2
= |pos
magnifier
? pos
with
| = |7? 5| = 2
dis h
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis h
2
= |pos
magnifier
? pos
with
| = |6? 5| = 1
? Get the matching score as Formula (3) according to Formula (2). d denotes
(saw
2
, with
5
,magnifier
7
) and hyp denotes the translation in the example.
p
(d,hyp)
= exp(?
|dis r
1
? dis h
1
|+ |dis r
2
? dis h
2
|
3? 1
) = exp(?
|3? 3|+ |2? 1|
3? 1
) = exp(?0.5)
(3)
We also tried other methods to calculate the matching score, such as the cosine distance and the
absolute distance, but the relative distance performed best. For a headword chain ngram with more than
one matches in the translation, we choose the one with the highest matching score.
2.2.2 Scoring fixed-floating ngram
The words in the fixed-floating ngram are continuous, so we restrict the matched string in the translation
also to being continuous. That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all these
n words continuous in the translation with the same order as they appear in the reference, we think the
dep-ngram can match with the translation. The matching score can be obtained by Formula (4), where d
stands for a fixed-floating ngram and hyp stands for the translation.
p
(d,hyp)
=
{
1 if match
0 if unmatch
(4)
2.3 Scoring RED
In the new metric, we use Fscore to obtain the final score. Fscore is calculated by Formula (5), where ?
is a value between 0 and 1.
Fscore =
precision ? recall
? ? precision+ (1? ?) ? recall
(5)
The dep-ngrams of the reference and the string of the translation are used to calculate the precision and
recall. In order to calculate precision, the number of the dep-ngrams in the translation should be given,
but there is no dependency tree for the translation in our method. We know that the number of dep-
ngrams has an approximate linear relationship with the length of the sentence, so we use the length of
the translation to replace the number of the dep-ngrams in the translation dependency tree. Recall can
be calculated directly since we know the number of the dep-ngrams in the reference. The precision and
recall are computed as follows.
precision =
?
d?D
n
p
(d,hyp)
len
h
, recall =
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n. len
h
is the length of the translation. count
n(ref)
is the
number of the dep-ngrams with the length of n in the reference.
2045
The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?
Fscore is calculated. w
ngram
(0 ? w
ngram
? 1) is the weight of dep-ngram with the length of n. Fscore
n
is the Fscore for the dep-ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? Fscore
n
) (6)
3 Introducing Extra Resources
Many automatic evaluation metrics can only find the exact match between the reference and the transla-
tion, and the information provided by the limited number of references is not sufficient. Some evaluation
metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the
reference information. We also introduce some extra resources to RED, such as stem, synonym and
paraphrase. The words within a sentence can be classified into content words and function words. The
effects of the two kinds of words are different and they shouldn?t have the same matching score, so we
introduce a parameter to distinguish them. The methods of applying these resources are introduced as
follows.
? Stem and Synonym
Stem(Porter, 2001) and synonym (WordNet
1
) are introduced to RED in the following three steps.
First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only
exact match but also stem and synonym are considered. We use stem and synonym together with
exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We
think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each
of the words in the dep-ngram has a matched word in the translation according to the alignment;
2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The
matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,
the match module score of a dep-ngram is calculated according to Formula (7). Different match
modules have different effects, so we give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (7)
m
i
is the match module (exact, stem or synonym) of the ith word in a dep-ngram. w
m
i
is the match
module weight of the ith word in a dep-ngram. n is the number of words in a dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t consider the dependency tree of the reference, because
paraphrases may not be contained in the headword chain and fixed-floating structures. First, the
alignment is obtained with METEOR Aligner, only considering paraphrase. Second, the matched
paraphrases are extracted from the alignment and defined as paraphrase-ngram. The score of a
paraphrase is 1? w
par
, where w
par
is the weight of paraphrase-ngram.
? Function word
We introduce a parameter w
fun
(0 ? w
fun
? 1) to distinguish function words and content words.
w
fun
is the weight of function words. The function word score of a dep-ngram or paraphrase-ngram
is computed according to Formula (8).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(8)
C
fun
is the number of function words in the dep-ngram or paraphrase-ngram. C
con
is the number
of content words in the dep-ngram or paraphrase-ngram.
1
http://wordnet.princeton.edu/
2046
We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated as
Formula (9), in which Fscore
p
is obtained using precison
p
and recall
p
as Formula (10).
REDp =
N
?
n=1
(w
ngram
? Fscore
p
n
) (9)
Fscore
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(10)
precision
p
and recall
P
in Formula (10) are calculated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
, recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length of n
in the reference. count
n
(par) is the number of paraphrases with length of n in reference. score
par
n
is
the match score of paraphrase-ngrams with the length of n. score
dep
n
is the match score of dep-ngrams
with the length of n. score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
) , score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
P
n
is the set of paraphrase-ngrams with the length of n. D
n
is the set of dep-ngrams with the length of n.
4 Parameter Tuning
There are several parameters in REDp, and different parameter values can make the performance of
REDp different. For example,w
ngram
represents the weight of dep-ngram with the length of n. The
effect of ngrams with different lengths are different, and they shouldn?t have the same weight. So we can
tune the parameters to find their best values.
We try a preliminary optimization method to tune parameters in REDp. A heuristic search is employed
and the parameters are classified into two subsets. The parameter optimization is a grid search over the
two subsets of parameters. When searching Subset 1, the parameters in Subset 2 are fixed, and then
Subset 1 and Subset 2 are exchanged to finish this iteration. Several iterations are executed to finish the
parameter tuning process. This heuristic search may not find the global optimum but it can save a lot of
time compared with exhaustive search. The optimization goal is to maximize the sum of Spearman?s ?
rank correlation coefficient on system level and Kendall?s ? correlation coefficient on sentence level. ?
is calculated using the following equation.
? = 1?
6
?
d
2
i
n(n
2
? 1)
where d
i
is the difference between the human rank and metric?s rank for system i. n is the number of
systems. ? is calculated as follows.
? =
number of concordant pairs? number of discordant pairs
number of concordant pairs + number of discordant pairs
The data of into-English tasks in WMT 2010 are used to tune parameters. The tuned parameters are
listed in Table 1.
5 Experiments
5.1 Data
The test sets in experiments are WMT 2012 and WMT 2013. The language pairs are German-to-English
(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-
English (ru-en). The number of translation systems for each language pair are showed in Table 2. For
each language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.
2047
Parameter ? w
fun
w
exact
w
stem
w
syn
w
par
w
1gram
w
2gram
w
3gram
tuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1
Table 1: Parameter values after tuning on WMT 2010. ? is from Formula (10). w
fun
is the weight of
function word. w
exact
, w
stem
andw
syn
are the weights of the three match modules ?exact stem synonym?
respectively. w
par
is the weight of paraphrase-ngram. w
1gram
, w
2gram
and w
3gram
are the weights of
dep-ngram with the length of 1, 2 and 3 respectively.
Language pairs cz-en de-en es-en fr-en ru-en
WMT2012 6 16 12 15 -
WMT2013 12 23 17 19 23
Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.
We parsed the reference into constituent tree by Berkeley parser
2
and then converted the constituent
tree into dependency tree by Penn2Malt
3
. Presumably, the performance of the new metric will be better
if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can
be used forever so it will not increase costs.
5.2 Baselines
In the experiments, we compare the performance of our metric with the widely-used lexicon-based met-
rics such as BLEU
4
, TER
5
and METEOR
6
, dependency-based metric HWCM and semantic-based metric
SEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to the
published results of WMT 2012.
The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25.
The results of METEOR are obtained by Version 1.4 with task option ?rank?. We re-implement HWCM
which employs an epsilon value of 10
?3
to replace zero for smoothing purpose. The correlations of
SEMPOS are obtained from the published results of WMT 2012 and WMT 2013.
5.3 Experiment Results
The experiments on both system level and sentence level are carried out. On system level, the correlations
are calculated using Spearman?s rank correlation coefficient ? (Pirie, 1988). Kendall?s rank correlation
coefficient ? (Kendall, 1938) is employed to evaluate the sentence level correlation. Our method performs
best when the maximum length of dep-ngram is set to 3, so we only present the results with the maximum
length of 3. RED represents the new metric with exact match and the parameter values are set as follows.
? = 0.5. w
1gram
= w
2gram
= w
3gram
= 1/3. REDp represents the new metric with extra resources
and tuned parameter values which are listed in Table (1).
5.3.1 System level correlations
The system level correlations are shown in Table 3. RED is better than BLEU, TER and HWCM on
average on both WMT 2012 and WMT 2013, which reflects that using syntactic information and only
parsing the reference side are helpful. REDp gets the best result on all of the language pairs except
cz-en on WMT 2012. The significant improvement from RED to REDp illustrates the effect of extra
resources and the parameter tuning. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. So the performance can be optimized
through parameter tuning. SEMPOS got the best correlation according to the published results of WMT
2
http://code.google.com/p/berkeleyparser/downloads/list
3
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html
4
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
5
http://www.cs.umd.edu/
?
snover/tercom
6
http://www.cs.cmu.edu/
?
alavie/METEOR/download/meteor-1.4.tgz
2048
2012, and METEOR got the best correlation according to the published results of WMT 2013 on into-
English task on system level. REDp gets better result than SEMPOS and METEOR on both WMT 2012
and WMT 2013, so REDp achieves the state-of-the-art performance on system level.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876
TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798
HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880
METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935
SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913
RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912
REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947
Table 3: System level correlations on WMT 2012 and WMT 2013. The value in bold is the best result in
each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
5.3.2 Sentence level correlations
The sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4. RED is better than
BLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic information
and only parsing the reference. By introducing extra resources and parameter tuning, REDp achieves
significant improvement over RED. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. A better performance can be exploited
through parameter tuning. From the results of REDp and METEOR, we can see that REDp gets the
comparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213
HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209
METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277
RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237
REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271
Table 4: Sentence level correlations on WMT 2012 and WMT 2013. The value in bold is the best result
in each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
6 Conclusion and Future Work
In this paper, we propose a reference dependency based automatic MT evaluation metric RED. The
new metric only uses the dependency trees of the reference, which avoids the parsing of the potentially
noisy translations. Both long distance dependency information and the local continuous ngrams are
captured by the new metric. The experiment results indicate that RED achieves better correlations than
BLEU, TER and HWCM on both system level and sentence level. REDp, the improved version of RED
through adding extra resources and preliminary parameter tuning, gets state-of-the-art results which are
better than METEOR and SEMPOS on system level. On sentence level, REDp gets the comparable
performance with METEOR.
In the future, we will use the dependency forest instead of the dependency tree to reduce the effect
of parsing errors. We will also apply RED and REDp to the tuning process of SMT to improve the
translation quality.
2049
Acknowledgements
The authors were supported by National Natural Science Foundation of China (Contract 61202216)
and National Natural Science Foundation of China (Contract 61379086). Qun Liu?s work was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin
City University. Sincere thanks to the three anonymous reviewers for their thorough reviewing and
valuable suggestions.
References
Boxing Chen and Roland Kuhn. 2011. Amber: A modified bleu, enhanced ranking metric. In Proceedings of
the Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings
of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages
85?91. Association for Computational Linguistics.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264. Association for
Computational Linguistics.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81?93.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation,
StatMT ?07, pages 228?231, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,
pages 25?32.
Chi-kiu Lo and Dekai Wu. 2013. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based MT evaluation metric. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
422?428, Sofia, Bulgaria, August. Association for Computational Linguistics.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.
Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approximating a deep-syntactic metric for mt evaluation and tun-
ing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98. Association for
Computational Linguistics.
Dennis Mehay and Chris Brew. 2007. BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation. In
Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).
F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for Computa-
tional Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for
machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-
tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
W Pirie. 1988. Spearman rank correlation coefficient. Encyclopedia of statistical sciences.
2050
Martin F Porter. 2001. Snowball: A language for stemming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Compu-
tational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?:
exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 259?268. Association for Computational Linguistics.
2051
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 880?888,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
Fast Generation of Translation Forest
for Large-Scale SMT Discriminative Training
Xinyan Xiao, Yang Liu, Qun Liu, and Shouxun Lin
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology
Chinese Academy of Sciences
{xiaoxinyan,yliu,liuqun,sxlin}@ict.ac.cn
Abstract
Although discriminative training guarantees to
improve statistical machine translation by in-
corporating a large amount of overlapping fea-
tures, it is hard to scale up to large data due to
decoding complexity. We propose a new al-
gorithm to generate translation forest of train-
ing data in linear time with the help of word
alignment. Our algorithm also alleviates the
oracle selection problem by ensuring that a
forest always contains derivations that exactly
yield the reference translation. With millions
of features trained on 519K sentences in 0.03
second per sentence, our system achieves sig-
nificant improvement by 0.84 BLEU over the
baseline system on the NIST Chinese-English
test sets.
1 Introduction
Discriminative model (Och and Ney, 2002) can
easily incorporate non-independent and overlapping
features, and has been dominating the research field
of statistical machine translation (SMT) in the last
decade. Recent work have shown that SMT benefits
a lot from exploiting large amount of features (Liang
et al, 2006; Tillmann and Zhang, 2006; Watanabe
et al, 2007; Blunsom et al, 2008; Chiang et al,
2009). However, the training of the large number
of features was always restricted in fairly small data
sets. Some systems limit the number of training ex-
amples, while others use short sentences to maintain
efficiency.
Overfitting problem often comes when training
many features on a small data (Watanabe et al,
2007; Chiang et al, 2009). Obviously, using much
more data can alleviate such problem. Furthermore,
large data also enables us to globally train millions
of sparse lexical features which offer accurate clues
for SMT. Despite these advantages, to the best of
our knowledge, no previous discriminative training
paradigms scale up to use a large amount of training
data. The main obstacle comes from the complexity
of packed forests or n-best lists generation which
requires to search through all possible translations
of each training example, which is computationally
prohibitive in practice for SMT.
To make normalization efficient, contrastive esti-
mation (Smith and Eisner, 2005; Poon et al, 2009)
introduce neighborhood for unsupervised log-linear
model, and has presented positive results in various
tasks. Motivated by these work, we use a translation
forest (Section 3) which contains both ?reference?
derivations that potentially yield the reference trans-
lation and also neighboring ?non-reference? deriva-
tions that fail to produce the reference translation.1
However, the complexity of generating this transla-
tion forest is up to O(n6), because we still need bi-
parsing to create the reference derivations.
Consequently, we propose a method to fast gener-
ate a subset of the forest. The key idea (Section 4)
is to initialize a reference derivation tree with maxi-
mum score by the help of word alignment, and then
traverse the tree to generate the subset forest in lin-
ear time. Besides the efficiency improvement, such
a forest allows us to train the model without resort-
1Exactly, there are no reference derivations, since derivation
is a latent variable in SMT. We call them reference derivation
just for convenience.
880
0,4
0,1
2,4
3,4
1 30 4
21
3
5
4
6
2
hyper-
edge rule
e1 r1 X ? ?X1 bei X2, X1 was X2?
e2 r2 X ? ?qiangshou bei X1,
the gunman was X1?
e3 r3 X ? ?jingfang X1, X1 by the police?
e4 r4 X ? ?jingfang X1, police X1 ?
e5 r5 X ? ?qiangshou, the gunman?
e6 r6 X ? ?jibi, shot dead?
Figure 1: A translation forest which is the running example throughout this paper. The reference translation is ?the
gunman was killed by the police?. (1) Solid hyperedges denote a ?reference? derivation tree t1 which exactly yields
the reference translation. (2) Replacing e3 in t1 with e4 results a competing non-reference derivation t2, which fails to
swap the order ofX3,4. (3) Removing e1 and e5 in t1 and adding e2 leads to another reference derivation t3. Generally,
this is done by deleting a node X0,1.
ing to constructing the oracle reference (Liang et al,
2006; Watanabe et al, 2007; Chiang et al, 2009),
which is non-trivial for SMT and needs to be deter-
mined experimentally. Given such forests, we glob-
ally learn a log-linear model using stochastic gradi-
ent descend (Section 5). Overall, both the generation
of forests and the training algorithm are scalable, en-
abling us to train millions of features on large-scale
data.
To show the effect of our framework, we globally
train millions of word level context features moti-
vated by word sense disambiguation (Chan et al,
2007) together with the features used in traditional
SMT system (Section 6). Training on 519K sentence
pairs in 0.03 seconds per sentence, we achieve sig-
nificantly improvement over the traditional pipeline
by 0.84 BLEU.
2 Synchronous Context Free Grammar
We work on synchronous context free grammar
(SCFG) (Chiang, 2007) based translation. The el-
ementary structures in an SCFG are rewrite rules of
the form:
X ? ??, ??
where ? and ? are strings of terminals and nonter-
minals. We call ? and ? as the source side and the
target side of rule respectively. Here a rule means a
phrase translation (Koehn et al, 2003) or a transla-
tion pair that contains nonterminals.
We call a sequence of translation steps as a
derivation. In context of SCFG, a derivation is a se-
quence of SCFG rules {ri}. Translation forest (Mi
et al, 2008; Li and Eisner, 2009) is a compact repre-
sentation of all the derivations for a given sentence
under an SCFG (see Figure 1). A tree t in the forest
corresponds to a derivation. In our paper, tree means
the same as derivation.
More formally, a forest is a pair ?V,E?, where V
is the set of nodes, E is the set of hyperedge. For
a given source sentence f = fn1 , Each node v ? V
is in the form Xi,j , which denotes the recognition
of nonterminal X spanning the substring from the i
through j (that is fi+1...fj). Each hyperedge e ? E
connects a set of antecedent to a single consequent
node and corresponds to an SCFG rule r(e).
3 Our Translation Forest
We use a translation forest that contains both ?ref-
erence? derivations that potentially yield the refer-
ence translation and also some neighboring ?non-
reference? derivations that fail to produce the ref-
erence translation. Therefore, our forest only repre-
sents some of the derivations for a sentence given an
SCFG rule table. The motivation of using such a for-
est is efficiency. However, since this space contains
both ?good? and ?bad? translations, it still provides
evidences for discriminative training.
First see the example in Figure 1. The derivation
tree t1 represented by solid hyperedges is a reference
derivation. We can construct a non-reference deriva-
tion by making small change to t1. By replacing the
e3 of t1 with e4, we obtain a non-reference deriva-
881
tion tree t2. Considering the rules in each derivation,
the difference between t1 and t2 lies in r3 and r4. Al-
though r3 has a same source side with r4, it produces
a different translation. While r3 provides a swap-
ping translation, r4 generates a monotone transla-
tion. Thus, the derivation t2 fails to move the sub-
ject ?police? to the behind of verb ?shot dead?, re-
sulting a wrong translation ?the gunman was police
shot dead?. Given such derivations, we hope that
the discriminative model is capable to explain why
should use a reordering rule in this context.
Generally, our forest contains all the reference
derivationsRT for a sentence given a rule table, and
some neighboring non-reference derivations NT ,
which can be defined fromRT .
More formally, we call two hyperedges e1 and e2
are competing hyperedges, if their corresponding
rules r(e1) = ??1, ?1? and r(e2) = ??2, ?2? :
?1 = ?2 ? ?1 ?= ?2 (1)
This means they give different translations for a
same source side. We use C(e) to represent the set
of competing hyperedges of e.
Two derivations t1 = ?V 1, E1? and t2 =
?V 2, E2? are competing derivations if there exists
e1 ? E1 and e2 ? E2: 2
V 1 = V 2 ? E1 ? e1 = E2 ? e2
? e2 ? C(e1) (2)
In other words, derivations t1 and t2 only differ in
e1 and e2, and these two hyperedges are competing
hyperedges. We useC(t) to represent the set of com-
peting derivations of tree t, and C(t,e) to represent
the set of competing derivations of t if the competi-
tion occurs in hyperedge e in t.
Given a rule table, the set of reference derivations
RT for a sentence is determined. Then, the set of
non-reference derivations NT can be defined from
RT :
?t?RT C(t) (3)
Overall, our forest is the compact representation of
RT and NT .
2The definition of derivation tree is similar to forest, except
that the tree contains exactly one tree while forest contains ex-
ponentially trees. In tree, the hyperedge degrades to edge.
Algorithm 1 Forest Generation
1: procedure GENERATE(t)
2: list? t
3: for v ? t in post order do
4: e? incoming edge of v
5: append C(t, e) to list;
6: for u ? child(v) from left to right do
7: tn? OPERATE(t, u)
8: if tn ?= t then
9: append tn to list
10: for e? ? tn ? e? /? t do
11: append C(tn,e?) to list
12: if SCORE(t) < SCORE(tn) then
13: t? tn
14: return t,list
4 Fast Generation
It is still slow to calculate the entire forest defined
in Section 3, therefore we use a greedy decoding for
fast generating a subset of the forest. Starting form
a reference derivation, we try to slightly change the
derivation into a new reference derivation. During
this process, we collect the competing derivations
of reference derivations. We describe the details of
local operators for changing a derivation in section
4.1, and then introduce the creation of initial refer-
ence derivation with max score in Section 4.2.
For example, given derivation t1, we delete the
node X0,1 and the related hyperedge e1 and e5. Fix-
ing the other nodes and edges, we try to add a new
edge e2 to create a new reference translation. In this
case, if rule r2 really exists in our rule table, we get
a new reference derivation t3. After constructing t3,
we first collect the new tree and C(t3, e2). Then, we
will move to t3, if the score of t3 is higher than t2.
Notably, if r2 does not exist in the rule table, we fail
to create a new reference derivation. In such case,
we keep the origin derivation unchanged.
Algorithm 1 shows the process of generation.3
The input is a reference derivation t, and the out-
put is a new derivation and the generated derivations.
3For simplicity, we list all the trees, and do not compress
them into a forest in practice. It is straight to extent the algo-
rithm to get a compact forest for those generated derivations.
Actually, instead of storing the derivations, we call the generate
function twice to calculate gradient of log-linear model.
882
0,4
0,1 2,4
0,4
2,4
0,4
2,40,2
Figure 2: Lexicalize and generalize operators over t1 (part) in Figure 1. Although here only shows the nodes, we also
need to change relative edges actually. (1) Applying lexicalize operator on the non-terminal node X0,1 in (a) results a
new derivation shown in (b). (2) When visiting bei in (b), the generalize operator changes the derivation into (c).
The list used for storing forest is initialized with the
input tree (line 2). We visit the nodes in t in post-
order (line 3). For each node v, we first append the
competing derivations C(t,e) to list, where e is in-
coming edge of v (lines 4-5). Then, we apply oper-
ators on the child nodes of v from left to right (lines
6-13). The operators returns a reference derivation
tn (line 7). If it is new (line 8), we collect both the tn
(line 9), and also the competing derivationsC(tn, e?)
of the new derivation on those edges e? which only
occur in the new derivation (lines 10-11). Finally, if
the new derivation has a larger score, we will replace
the origin derivation with new one (lines 12-13).
Although there is a two-level loop for visiting
nodes (line 3 and 6), each node is visited only one
time in the inner loops. Thus, the complexity is
linear with the number of nodes #node. Consid-
ering that the number of source word (also leaf node
here) is less than the total number of nodes and is
more than ?(#node+1)/2?, the time complexity of
the process is also linear with the number of source
word.
4.1 Lexicalize and Generalize
The function OPERATE in Algorithm 1 uses two op-
erators to change a node: lexicalize and generalize.
Figure 2 shows the effects of the two operators. The
lexicalize operator works on nonterminal nodes. It
moves away a nonterminal node and attaches the
children of current node to its parent. In Figure 2(b),
the node X0,1 is deleted, requiring a more lexical-
ized rule to be applied to the parent node X0,4 (one
more terminal in the source side). We constrain the
lexicalize operator to apply on pre-terminal nodes
whose children are all terminal nodes. In contrast,
the generalize operator works on terminal nodes and
inserts a nonterminal node between current node and
its parent node. This operator generalizes over the
continuous terminal sibling nodes left to the current
node (including the current node). Generalizing the
node bei in Figure 2(b) results Figure 2(c). A new
node X0,2 is inserted as the parent of node qiang-
shou and node bei.
Notably, there are two steps when apply an oper-
ator. Suppose we want to lexicalize the node X0,1
in t1 of Figure 1, we first delete the node X0,1 and
related edge e1 and e5, then we try to add the new
edge e2. Since rule table is fixed, the second step
is a process of decoding. Therefore, sometimes we
may fail to create a new reference derivation (like
r2 may not exist in the rule table). In such case, we
keep the origin derivation unchanged.
The changes made by the two operators are local.
Considering the change of rules, the lexicalize oper-
ator deletes two rules and adds one new rule, while
the generalize operator deletes one rule and adds two
new rules. Such local changes provide us with a way
to incrementally calculate the scores of new deriva-
tions. We use this method motivated by Gibbs Sam-
pler (Blunsom et al, 2009) which has been used for
efficiently learning rules. The different lies in that
we use the operator for decoding where the rule ta-
ble is fixing.
4.2 Initialize a Reference Derivation
The generation starts from an initial reference
derivation with max score. This requires bi-parsing
(Dyer, 2010) over the source sentence f and the ref-
erence translation e. In practice, we may face three
problems.
First is efficiency problem. Exhaustive search
over the space under SCFG requires O(|f |3|e|3).
883
To parse quickly, we only visit the tight consistent
(Zhang et al, 2008) bi-spans with the help of word
alignment a. Only visiting tight consistent spans
greatly speeds up bi-parsing. Besides efficiency,
adoption of this constraint receives support from the
fact that heuristic SCFG rule extraction only extracts
tight consistent initial phrases (Chiang, 2007).
Second is degenerate problem. If we only use
the features as traditional SCFG systems, the bi-
parsing may end with a derivation consists of some
giant rules or rules with rare source/target sides,
which is called degenerate solution (DeNero et al,
2006). That is because the translation rules with rare
source/target sides always receive a very high trans-
lation probability. We add a prior score log(#rule)
for each rule, where #rule is the number of occur-
rence of a rule, to reward frequent reusable rules and
derivations with more rules.
Finally, we may fail to create reference deriva-
tions due to the limitation in rule extraction. We
create minimum trees for (f , e,a) using shift-reduce
(Zhang et al, 2008). Some minimum rules in the
trees may be illegal according to the definition of
Chiang (2007). We also add these rules to the rule
table, so as to make sure every sentence is reachable
given the rule table. A source sentence is reachable
given a rule table if reference derivations exists. We
refer these rules as added rules. However, this may
introduce rules with more than two variables and in-
crease the complexity of bi-parsing. To tackle this
problem, we initialize the chart with minimum par-
allel tree from the Zhang et al (2008) algorithm,
ensuring that the bi-parsing has at least one path to
create a reference derivation. Then we only need to
consider the traditional rules during bi-parsing.
5 Training
We use the forest to train a log-linear model with a
latent variable as describe in Blunsom et al(2008).
The probability p(e|f) is the sum over all possible
derivations:
p(e|f) =
?
t??(e,f)
p(t, e|f) (4)
where ?(e, f) is the set of all possible derivations
that translate f into e and t is one such derivation.4
4Although the derivation is typically represent as d, we de-
notes it by t since our paper use tree to represent derivation.
Algorithm 2 Training
1: procedure TRAIN(S)
2: Training Data S = {fn, en,an}Nn=1
3: Derivations T = {}Nn=1
4: for n = 1 to N do
5: tn ? INITIAL(fn, en,an)
6: i? 0
7: for m = 0 to M do
8: for n = 0 to N do
9: ? ? LEARNRATE(i)
10: (?L(wi, tn), tn)?GENERATE(tn)
11: wi ? wi + ? ??L(wi, tn)
12: i? i + 1
13: return
?MN
i=1 wi
MN
This model defines the conditional probability of
a derivation t and the corresponding translation e
given a source sentence f as:
p(t, e|f) = exp
?
i ?ihi(t, e, f)
Z(f) (5)
where the partition function is
Z(f) =
?
e
?
t??(e,f)
exp
?
i
?ihi(t, e, f) (6)
The partition function is approximated by our for-
est, which is labeled as Z?(f), and the derivations
that produce reference translation is approximated
by reference derivations in Z?(f).
We estimate the parameters in log-linear model
using maximum a posteriori (MAP) estimator. It
maximizes the likelihood of the bilingual corpus
S = {fn, en}Nn=1, penalized using a gaussian prior
(L2 norm) with the probability density function
p0(?i) ? exp(??2i /2?2). We set ?2 to 1.0 in our
experiments. This results in the following gradient:
?L
??i
= Ep(t|e,f)[hi]? Ep(e|f)[hi]?
?i
?2 (7)
We use an online learning algorithm to train the
parameters. We implement stochastic gradient de-
scent (SGD) recommended by Bottou.5 The dy-
namic learning rate we use is N(i+i0) , where N is the
5http://leon.bottou.org/projects/sgd
884
number of training example, i is the training itera-
tion, and i0 is a constant number used to get a initial
learning rate, which is determined by calibration.
Algorithm 2 shows the entire process. We first
create an initial reference derivation for every train-
ing examples using bi-parsing (lines 4-5), and then
online learn the parameters using SGD (lines 6-12).
We use the GENERATE function to calculate the gra-
dient. In practice, instead of storing all the deriva-
tions in a list, we traverse the tree twice. The first
time is calculating the partition function, and the
second time calculates the gradient normalized by
partition function. During training, we also change
the derivations (line 10). When training is finished
after M epochs, the algorithm returns an averaged
weight vector (Collins, 2002) to avoid overfitting
(line 13). We use a development set to select total
epoch m, which is set as M = 5 in our experiments.
6 Experiments
Our method is able to train a large number of fea-
tures on large data. We use a set of word context
features motivated by word sense disambiguation
(Chan et al, 2007) to test scalability. A word level
context feature is a triple (f, e, f+1), which counts
the number of time that f is aligned to e and f+1 oc-
curs to the right of f . Triple (f, e, f?1) is similar ex-
cept that f?1 locates to the left of f . We retain word
alignment information in the extracted rules to ex-
ploit such features. To demonstrate the importance
of scaling up the size of training data and the effect
of our method, we compare three types of training
configurations which differ in the size of features
and data.
MERT. We use MERT (Och, 2003) to training 8
features on a small data. The 8 features is the same
as Chiang (2007) including 4 rule scores (direct and
reverse translation scores; direct and reverse lexi-
cal translation scores); 1 target side language model
score; 3 penalties for word counts, extracted rules
and glue rule. Actually, traditional pipeline often
uses such configuration.
Perceptron. We also learn thousands of context
word features together with the 8 traditional features
on a small data using perceptron. Following (Chiang
et al, 2009), we only use 100 most frequent words
for word context feature. This setting use CKY de-
TRAIN RTRAIN DEV TEST
#Sent. 519,359 186,810 878 3,789
#Word 8.6M 1.3M 23K 105K
Avg. Len. 16.5 7.3 26.4 28.0
Lon. Len. 99 95 77 116
Table 1: Corpus statistics of Chinese side, where Sent.,
Avg., Lon., and Len. are short for sentence, longest,
average, and length respectively. RTRAIN denotes the
reachable (given rule table without added rules) subset of
TRAIN data.
coder to generate n-best lists for training. The com-
plexity of CKY decoding limits the training data into
a small size. We fix the 8 traditional feature weights
as MERT to get a comparable results as MERT.
Our Method. Finally, we use our method to train
millions of features on large data. The use of large
data promises us to use full vocabulary of training
data for the context word features, which results mil-
lions of fully lexicalized context features. During
decoding, when a context feature does not exit, we
simply ignore it. The weights of 8 traditional fea-
tures are fixed the same as MERT also. We fix these
weights because the translation feature weights fluc-
tuate intensely during online learning. The main rea-
son may come from the degeneration solution men-
tioned in Section 4.2, where rare rules with very high
translation probability are selected as the reference
derivations. Another reason could be the fact that
translation features are dense intensify the fluctua-
tion. We leave learning without fixing the 8 feature
weights to future work.
6.1 Data
We focus on the Chinese-to-English translation task
in this paper. The bilingual corpus we use con-
tains 519, 359 sentence pairs, with an average length
of 16.5 in source side and 20.3 in target side,
where 186, 810 sentence pairs (36%) are reach-
able (without added rules in Section 4.2). The
monolingual data includes the Xinhua portion of
the GIGAWORD corpus, which contains 238M En-
glish words. We use the NIST evaluation sets of
2002 (MT02) as our development set, and sets of
MT03/MT04/MT05 as test sets. Table 2 shows the
statistics of all bilingual corpus.
We use GIZA++ (Och and Ney, 2003) to perform
885
System #DATA #FEAT MT03 MT04 MT05 ALL
MERT 878 8 33.03 35.12 32.32 33.85
Perceptron 878 2.4K 32.89 34.88 32.55 33.76
Our Method 187K 2.0M 33.64 35.48 32.91* 34.41*519K 13.9M 34.19* 35.72* 33.09* 34.69*
Improvement over MERT +1.16 +0.60 +0.77 +0.84
Table 2: Effect of our method comparing with MERT and perceptron in terms of BLEU. We also compare our fast
generation method with different data (only reachable or full data). #Data is the size of data for training the feature
weights. * means significantly (Koehn, 2004) better than MERT (p < 0.01).
word alignment in both directions, and grow-diag-
final-and (Koehn et al, 2003) to generate symmet-
ric word alignment. We extract SCFG rules as de-
scribed in Chiang (2007) and also added rules (Sec-
tion 4.2). Our algorithm runs on the entire training
data, which requires to load all the rules into the
memory. To fit within memory, we cut off those
composed rules which only happen once in the train-
ing data. Here a composed rule is a rule that can be
produced by any other extracted rules. A 4-grams
language model is trained by the SRILM toolkit
(Stolcke, 2002). Case-insensitive NIST BLEU4 (Pa-
pineni et al, 2002) is used to measure translation
performance.
The training data comes from a subset of the
LDC data including LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06. Since the rule ta-
ble of the entire data is too large to be loaded to
the memory (even drop one-count rules), we remove
many sentence pairs to create a much smaller data
yet having a comparable performance with the entire
data. The intuition lies in that if most of the source
words of a sentence need to be translated by the
added rules, then the word alignment may be highly
crossed and the sentence may be useless. We cre-
ate minimum rules from a sentence pair, and count
the number of source words in those minimum rules
that are added rules. For example, suppose the result
minimum rules of a sentence contain r3 which is an
added rule, then we count 1 time for the sentence. If
the number of such source word is more than 10%
of the total number, we will drop the sentence pair.
We compare the performances of MERT setting
on three bilingual data: the entire data that contains
42.3M Chinese and 48.2M English words; 519K
data that contains 8.6M Chinese and 10.6M En-
glish words; FBIS (LDC2003E14) parts that con-
tains 6.9M Chinese and 9.1M English words. They
produce 33.11/32.32/30.47 BLEU tested on MT05
respectively. The performance of 519K data is com-
parable with that of entire data, and much higher
than that of FBIS data.
6.2 Result
Table 3 shows the performance of the three different
training configurations. The training of MERT and
perceptron run on MT02. For our method, we com-
pare two different training sets: one is trained on
all 519K sentence pairs, the other only uses 186K
reachable sentences.
Although the perceptron system exploits 2.4K
features, it fails to produce stable improvements
over MERT. The reason may come from overfitting,
since the training data for perceptron contains only
878 sentences. However, when use our method to
learn the word context feature on the 519K data,
we significantly improve the performance by 0.84
points on the entire test sets (ALL). The improve-
ments range from 0.60 to 1.16 points on MT03-
05. Because we use the full vocabulary, the num-
ber of features increased into 13.9 millions, which is
impractical to be trained on the small development
set. These results confirm the necessity of exploiting
more features and learning the parameters on large
data. Meanwhile, such results also demonstrate that
we can benefits from the forest generated by our fast
method instead of traditional CKY algorithm.
Not surprisingly, the improvements are smaller
when only use 186K reachable sentences. Some-
times we even fail to gain significant improvement.
This verifies our motivation to guarantee all sentence
886
 0
 30
 60
 90
 120
 150
 180
 0  10  20  30  40  50  60  70  80  90
Tr
ain
ing
 Ti
me
(M
illis
eco
nds
)
Sentence Length
Figure 3: Plot of training times (including forest genera-
tion and SGD training) versus sentence length. We ran-
domly select 1000 sentence from the 519K data for plot-
ting.
are reachable, so as to use all training data.
6.3 Speed
How about the speed of our framework? Our method
learns in 32 mlliseconds/sentence. Figure 3 shows
training times (including forest generation and SGD
training) versus sentence length. The plot confirms
that our training algorithm scales linearly. If we
use n-best lists which generated by CKY decoder
as MERT, it takes about 3105 milliseconds/sentence
for producing 100-best lists. Our method accelerates
the speed about 97 times (even though we search
twice to calculate the gradient). This shows the effi-
ciency of our method.
The procedure of training includes two steps. (1)
Bi-parsing to initialize a reference derivation with
max score. (2) Training procedure which generates
a set of derivations to calculate the gradient and up-
date parameters. Step (1) only runs once. The av-
erage time of processing a sentence for each step
is about 9.5 milliseconds and 30.2 milliseconds re-
spectively.
For simplicity we do not compress the generated
derivations into forests, therefore the size of result-
ing derivations is fairly small, which is about 265.8
for each sentence on average, where 6.1 of them are
reference derivations. Furthermore, we use lexical-
ize operator more often than generalize operator (the
ration between them is 1.5 to 1). Lexicalize operator
is used more frequently mainly dues to that the ref-
erence derivations are initialized with reusable (thus
small) rules.
7 Related Work
Minimum error rate training (Och, 2003) is perhaps
the most popular discriminative training for SMT.
However, it fails to scale to large number of features.
Researchers have propose many learning algorithms
to train many features: perceptron (Shen et al, 2004;
Liang et al, 2006), minimum risk (Smith and Eisner,
2006; Li et al, 2009), MIRA (Watanabe et al, 2007;
Chiang et al, 2009), gradient descent (Blunsom et
al., 2008; Blunsom and Osborne, 2008). The com-
plexity of n-best lists or packed forests generation
hamper these algorithms to scale to a large amount
of data.
For efficiency, we only use neighboring deriva-
tions for training. Such motivation is same as con-
trastive estimation (Smith and Eisner, 2005; Poon et
al., 2009). The difference lies in that the previous
work actually care about their latent variables (pos
tags, segmentation, dependency trees, etc), while
we are only interested in their marginal distribution.
Furthermore, we focus on how to fast generate trans-
lation forest for training.
The local operators lexicalize/generalize are use
for greedy decoding. The idea is related to ?peg-
ging? algorithm (Brown et al, 1993) and greedy de-
coding (Germann et al, 2001). Such types of local
operators are also used in Gibbs sampler for syn-
chronous grammar induction (Blunsom et al, 2009;
Cohn and Blunsom, 2009).
8 Conclusion and Future Work
We have presented a fast generation algorithm for
translation forest which contains both reference
derivations and neighboring non-reference deriva-
tions for large-scale SMT discriminative training.
We have achieved significantly improvement of 0.84
BLEU by incorporate 13.9M feature trained on 519K
data in 0.03 second per sentence.
In this paper, we define the forest based on com-
peting derivations which only differ in one rule.
There may be better classes of forest that can pro-
duce a better performance. It?s interesting to modify
the definition of forest, and use more local operators
to increase the size of forest. Furthermore, since the
generation of forests is quite general, it?s straight to
887
apply our forest on other learning algorithms. Fi-
nally, we hope to exploit more features such as re-
ordering features and syntactic features so as to fur-
ther improve the performance.
Acknowledgement
We would like to thank Yifan He, Xianhua Li, Daqi
Zheng, and the anonymous reviewers for their in-
sightful comments. The authors were supported by
National Natural Science Foundation of China Con-
tracts 60736014, 60873167, and 60903138.
References
Phil Blunsom and Miles Osborne. 2008. Probabilistic
inference for machine translation. In Proc. of EMNLP
2008.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. of ACL-08.
Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-
borne. 2009. A gibbs sampler for phrasal synchronous
grammar induction. In Proc. of ACL 2009.
Peter F. Brown, Vincent J.Della Pietra, Stephen A. Della
Pietra, and Robert. L. Mercer. 1993. The mathemat-
ics of statistical machine translation. Computational
Linguistics, 19:263?311.
Yee Seng Chan, Hwee Tou Ng, and David Chiang. 2007.
Word sense disambiguation improves statistical ma-
chine translation. In Proc. of ACL 2007, pages 33?40.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine translation.
In Proc. of NAACL 2009.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Trevor Cohn and Phil Blunsom. 2009. A Bayesian model
of syntax-directed tree to string grammar induction. In
Proc. of EMNLP 2009.
Michael Collins. 2002. Discriminative training methods
for hidden markov models: Theory and experiments
with perceptron algorithms. In Proc. of EMNLP 2002.
John DeNero, Dan Gillick, James Zhang, and Dan Klein.
2006. Why generative phrase models underperform
surface heuristics. In Proc. of the HLT-NAACL 2006
Workshop on SMT.
Chris Dyer. 2010. Two monolingual parses are better
than one (synchronous parse). In Proc. of NAACL
2010.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast decoding and
optimal decoding for machine translation. In Proc. of
ACL 2001.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
of HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. of EMNLP
2004.
Zhifei Li and Jason Eisner. 2009. First- and second-order
expectation semirings with applications to minimum-
risk training on translation forests. In Proc. of EMNLP
2009.
Zhifei Li, Jason Eisner, and Sanjeev Khudanpur. 2009.
Variational decoding for statistical machine transla-
tion. In Proc. of ACL 2009.
Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, and
Ben Taskar. 2006. An end-to-end discriminative ap-
proach to machine translation. In Proc. of ACL 2006.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proc. of ACL 2008.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. of ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. of ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. of ACL 2002.
Hoifung Poon, Colin Cherry, and Kristina Toutanova.
2009. Unsupervised morphological segmentation with
log-linear models. In Proc. of NAACL 2009.
Libin Shen, Anoop Sarkar, and Franz Josef Och. 2004.
Discriminative reranking for machine translation. In
Proc. of NAACL 2004.
Noah A. Smith and Jason Eisner. 2005. Contrastive esti-
mation: Training log-linear models on unlabeled data.
In Proc. of ACL 2005.
David A. Smith and Jason Eisner. 2006. Minimum risk
annealing for training log-linear models. In Proc. of
COLING/ACL 2006.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. of ICSLP 2002.
Christoph Tillmann and Tong Zhang. 2006. A discrim-
inative global training algorithm for statistical mt. In
Proc. of ACL 2006.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for statis-
tical machine translation. In Proc. of EMNLP-CoNLL
2007.
Hao Zhang, Daniel Gildea, and David Chiang. 2008. Ex-
tracting synchronous grammar rules from word-level
alignments in linear time. In Proc. of Coling 2008.
888
Discriminative Word Alignment by
Linear Modeling
Yang Liu?
Institute of Computing Technology
Chinese Academy of Sciences
Qun Liu?
Institute of Computing Technology
Chinese Academy of Sciences
Shouxun Lin?
Institute of Computing Technology
Chinese Academy of Sciences
Word alignment plays an important role in many NLP tasks as it indicates the correspondence
between words in a parallel text. Although widely used to align large bilingual corpora, gen-
erative models are hard to extend to incorporate arbitrary useful linguistic information. This
article presents a discriminative framework for word alignment based on a linear model. Within
this framework, all knowledge sources are treated as feature functions, which depend on a source
language sentence, a target language sentence, and the alignment between them. We describe a
number of features that could produce symmetric alignments. Our model is easy to extend and
can be optimized with respect to evaluation metrics directly. The model achieves state-of-the-art
alignment quality on three word alignment shared tasks for five language pairs with varying
divergence and richness of resources. We further show that our approach improves translation
performance for various statistical machine translation systems.
1. Introduction
Word alignment, which can be defined as an object for indicating the corresponding
words in a parallel text, was first introduced as an intermediate result of statistical
machine translation (Brown et al 1993).
Consider the following Chinese sentence and its English translation:
Published under Creative Commons Attribution-NonCommercial 3.0 Unported license
     
Zhongguo jianzhuye duiwaikaifang chengxian xin geju
The opening of China?s construction industry to the outside presents a new structure
? Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese
Academy of Sciences, No. 6 Kexueyuan South Road, Haidian District, P.O. Box 2704, Beijing 100190,
China. E-mail: {yliu, liuqun, sxlin}@ict.ac.cn.
Submission received: 13 September 2007; revised submission received: 7 January 2010; accepted for
publication: 21 February 2010.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 3
The Chinese word Zhongguo is aligned to the English word China because they are
translations of one another. Similarly, the Chinese word xin is aligned to the English
word new. These connections are not necessarily one-to-one. For example, one Chinese
word jianzhuye corresponds to two English words construction industry. In addition, the
English words (e.g., opening ... to the outside) connected to a Chinese word (e.g., dui-
waikaifang) could be discontinuous. Figure 1 shows an alignment for this sentence pair.
The Chinese and English words are listed horizontally and vertically, respectively. They
are numbered to facilitate identification. The dark points indicate the correspondence
between the words in two languages. The goal of word alignment is to identify such
correspondences in a parallel text.
Word alignment plays an important role in many NLP tasks. In statistical machine
translation, word-aligned corpora serve as an excellent source for translation-related
knowledge. The estimation of translation model parameters usually relies heavily on
word-aligned corpora, not only for phrase-based and hierarchical phrase-based models
(Koehn, Och, and Marcu 2003; Och and Ney 2004; Chiang 2005, 2007), but also for
syntax-based models (Quirk, Menezes, and Cherry 2005; Galley et al 2006; Liu, Liu,
and Lin 2006; Marcu et al 2006). Besides machine translation, many applications for
word-aligned corpora have been suggested, including machine-assisted translation,
Figure 1
Example of a word alignment between a Chinese?English sentence pair. The Chinese and
English words are listed horizontally and vertically, respectively. They are numbered to facilitate
identification. The dark points indicate the correspondences between the words in the two
languages.
304
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
translation assessment and critiquing tools, text generation, bilingual lexigraphy, and
word sense disambiguation.
Various methods have been proposed for finding word alignments between parallel
texts. Among them, generative alignment models (Brown et al 1993; Vogel and Ney
1996) have been widely used to produce word alignments for large bilingual corpora.
Describing the relationship of a bilingual sentence pair, a generative model treats word
alignment as a hidden process and maximizes the likelihood of a training corpus using
the expectation maximization (EM) algorithm. After the maximization process is com-
plete, the unknown model parameters are determined and the word alignments are set
to the maximum posterior predictions of the model.
However, one drawback of generative models is that they are hard to extend. Gen-
erative models usually impose strong independence assumptions between sub-models,
making it very difficult to incorporate arbitrary features explicitly. For example, when
considering whether to align two words, generative models cannot include information
about lexical and syntactic features such as part of speech and orthographic similarity in
an easy way. Such features would allow for more effective use of sparse data and result
in a model that is more robust in the presence of unseen words. Extending a generative
model requires that the interdependence of information sources be modeled explicitly,
which often makes the resulting system quite complex.
In this article, we introduce a discriminative framework for word alignment based
on the linear modeling approach. Within this framework, we treat all knowledge
sources as feature functions that depend on a source sentence, a target sentence,
and the alignment between them. Each feature function is associated with a feature
weight. The linear combination of features gives an overall score to each candidate
alignment. The best alignment is the one with the highest overall score. A linear
model not only allows for easy integration of new features, but also admits optimiz-
ing feature weights directly with respect to evaluation metrics. Experimental results
show that our approach improves both alignment quality and translation performance
significantly.
This article is organized as follows. Section 2 gives a formal description of our
model. We show how to train feature weights by taking evaluation metrics into account
and how to find the most probable alignment in an exponential search space efficiently.
Section 3 describes a number of features used in our experiments, focusing on the
features that produce symmetric alignments. In Section 4, we evaluate our model in
both alignment and translation tasks. Section 5 reviews previous work related to our
approach and the article closes with a conclusion in Section 6.
2. Approach
2.1 The Model
Given a source language sentence f = f1, . . . , fj, . . . , fJ and a target language sentence e =
e1, . . . , ei, . . . , eI, we define a link l = ( j, i) to exist if fj and ei are translations (or part of a
translation) of one another. Then, an alignment is defined as a subset of the Cartesian
product of the word positions:
a ? {( j, i) : j = 1, . . . , J; i = 1, . . . , I} (1)
305
Computational Linguistics Volume 36, Number 3
We propose a linear alignment model:
score(f, e, a) =
M
?
m=1
?mhm(f, e, a) (2)
where hm(f, e, a) is a feature function and ?m is its associated feature weight. The linear
combination of features gives an overall score score(f, e, a) to each candidate alignment
a for a given sentence pair ?f, e?.
2.2 Training
To achieve good alignment quality, it is essential to find a good set of feature weights
?M1 . Before discussing how to train ?
M
1 , we first describe two evaluation metrics that
measure alignment quality, because we will optimize ?M1 with respect to them directly.
2.2.1 Evaluation Metrics. The first metric is alignment error rate (AER), proposed by
Och and Ney (2003). AER has been used as official evaluation criterion in most word
alignment shared tasks. Och and Ney define two kinds of links in hand-aligned align-
ments: sure links for alignments that are unambiguous and possible links for ambiguous
alignments. Sure links usually connect content words such as Zhongguo and China.
In contrast, possible links often align words within idiomatic expressions and free
translations.
An AER score is given by
AER(S, P, A) = 1 ? |A ? S| + |A ? P||A| + |S| (3)
where S is a set of sure links in a reference alignment that is hand-aligned by human
experts, P is a set of possible links in the reference alignment, and A is a candidate
alignment. Note that S is a subset of P: S ? P. The lower the AER score is, the better the
alignment quality is.
Although widely used, AER has been criticized for correlating poorly with transla-
tion quality (Ayan and Dorr 2006a; Fraser and Marcu 2007b). In other words, lower AER
scores do not necessarily lead to better translation quality.1 Fraser and Marcu (2007b)
argue that reference alignments should consist of only sure links. They propose a new
measure called the balanced F-measure:
precision(S, A) =
|A ? S|
|S| (4)
recall(S, A) =
|A ? S|
|A| (5)
F-measure(S, ?, A) = 1
?
precision(S,A) +
1??
recall(S,A)
(6)
1 It has not yet been uniformly accepted that better word alignments yield better translations. Ayan and
Dorr (2006a) present a detailed discussion of the impact of word alignment on statistical machine
translation.
306
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
where ? is a parameter that sets the trade-off between precision and recall. Higher
F-measure means better alignment quality. Obviously, ? less than 0.5 weights recall
higher, whereas ? greater than 0.5 weights precision higher.
We use both AER and F-measure in our experiments. AER is used in experiments
evaluating alignment quality (Section 4.1) and F-measure is used in experiments evalu-
ating translation performance (Section 4.2).
2.2.2 Minimum Error Rate Training. Suppose we have three candidate alignments: a1, a2,
and a3. Their AER scores are 0.21, 0.20, and 0.22, respectively. Therefore, a2 is the best
candidate alignment, a1 is the second best, and a3 is the third best. We use three features
to score each candidate. Table 1 lists the feature values for each candidate.
If the set of feature weights is {1.0, 1.0, 1.0}, the model scores (see Equation (2)) of
the three candidates are ?71, ?74, and ?76, respectively. Whereas reference alignment
considers a2 as the best candidate, a1 has the maximal model score. This is undesirable
because the model fails to agree with the reference. If we change the feature weights
to {1.0,?2.0, 2.0}, the model scores become ?73, ?71, and ?83, respectively. Now, the
model chooses a2 as the best candidate correctly.
If a set of feature weights manages to make model predictions agree with refer-
ence alignments in training examples, we would expect the model to achieve good
alignment quality on unseen data as well. To do this, we adopt the minimum er-
ror rate training (MERT) algorithm proposed by Och (2003) to find feature weights
that minimize AER or maximize F-measure on a representative hand-aligned training
corpus.
Given a reference alignment r and a candidate alignment a, we use a loss func-
tion E(r, a) to measure alignment performance. Note that E(r, a) can be either AER or
1 ? F-measure. Given a bilingual corpus ?fS1, eS1? with a reference alignment rs and a set
of K different candidate alignments Cs = {as,1 . . . as,K} for each sentence pair ?fs, es?, our
goal is to find a set of feature weights ??M1 that minimizes the overall loss on the training
corpus:
??M1 = argmin
?M1
{ S
?
s=1
E(rs, a?(fs, es; ?
M
1 ))
}
(7)
= argmin
?M1
{ S
?
s=1
K
?
k=1
E(rs, as,k)?(a?(fs, es; ?
M
1 ), as,k)
}
(8)
Table 1
Example feature values and alignment error rates.
feature values
candidate h1 h2 h3 AER
a1 ?85 4 10 0.21
a2 ?89 3 12 0.20
a3 ?93 6 11 0.22
307
Computational Linguistics Volume 36, Number 3
where a?(fs, es; ?M1 ) is the best candidate alignment produced by the linear model:
a?(fs, es; ?
M
1 ) = argmax
a
{ M
?
m=1
?mhm(fs, es, a)
}
(9)
The basic idea of MERT is to optimize only one parameter (i.e., feature weight)
each time and keep all other parameters fixed. This process runs iteratively over M
parameters until the overall loss on the training corpus does not decrease.
Formally, suppose we tune a parameter and keep the other M ? 1 parameters fixed;
each candidate alignment corresponds to a line in the plane with ? as the independent
variable:
? ? ?(f, e, a) + ?(f, e, a) (10)
where ? denotes the parameter being tuned (i.e., ?m) and ?(f, e, a) and ?(f, e, a) are
constants with respect to ?:
?(f, e, a) = hm(f, e, a) (11)
?(f, e, a) =
M
?
m?=1,m? =m
?m?hm? (f, e, a) (12)
The set of candidates in Cs defines a set of lines. For example, given the candidate
alignments in Table 1, suppose we only tune ?2 and keep ?1 and ?3 fixed with an initial
set of parameters {1.0, 1.0, 1.0}. According to Equation (10), a1 corresponds to a line
4?? 75, a2 corresponds to a line 3?? 77, and a3 corresponds to a line 6?? 82.
The decision rule in Equation (9) states that a? is the line with the highest model
score for a given ?. The selection of ? for each sentence pair ultimately determines the
loss at ?. How do we find values of ? that could generate different loss values?
As the loss can only change if we move to a ? where the highest line is different
than before, Och (2003) suggests only evaluating the loss at values in between the
intersections that line the top surface of the cluster of lines. Figure 2 demonstrates eight
Figure 2
Candidate alignments in dimension ? and the critical intersections. Each candidate alignment is
represented as a line. ?1,?2, and ?3 are critical intersections where the best candidate a?
(highlighted in bold) will change: a? is a1 in (??, ?1], a2 in (?1, ?2], a7 in (?2, ?3], and a5 in
(?3, +?).
308
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
candidate alignments. The sequence of the topmost line segments highlighted in bold
constitutes an upper envelope, which indicates the best candidate alignments the model
predicts with various values of ?. Instead of computing all possible K2 intersections
between the lines in Cs, we just need to find the critical intersections where the topmost
line changes. In Figure 2, ?1, ?2, and ?3 are critical intersections. In the interval (??, ?1],
a1 has the highest score. Similarly, the best candidates are a2 for (?1, ?2], a7 for (?2, ?3],
and a5 for (?3, +?), respectively. The optimal ?? can be found by collecting all critical
intersections on the training corpus and choosing one ? that results in the minimal loss
value. Please refer to Och (2003) for more details.
2.3 Search
Given a source language sentence f and a target language sentence e, we try to find the
best candidate alignment with the highest model score:
a? = argmax
a
{
score(f, e, a)
}
(13)
= argmax
a
{ M
?
m=1
?mhm(f, e, a)
}
(14)
To do this, we begin with an empty alignment and keep adding new links until
the model score of the current alignment does not increase. Figure 3 illustrates this
search process. Given a source language sentence f1f2 and a target language sentence
e1e2, the initial alignment a1 is empty (i.e., all words are unaligned). Then, we obtain a
new alignment a2 by adding a link (1, 1) to a1. Similarly, the addition of (1, 2) to a1 leads
to a3. a2 and a3 can be further extended to produce more alignments.
Graphically speaking, the search space of a sentence pair can be organized as a
directed acyclic graph. Each node in the graph is a candidate alignment and each edge
corresponds to a link. We say that alignments that have the same number of links
constitute a level. There are 2J?I possible nodes and J ? I + 1 levels in a graph. In
Figure 3, a2, a3, a4, and a5 belong to the same level because they all contain one link.
The maximum level width is given by
( J?I
 J?I2 
)
. In Figure 3, the maximal level width is
(
4
2
)
= 6. Our goal is to find the node with the highest model score in a search graph.
As the search space of word alignment is exponential (although enumerable), it is
computationally prohibitive to explore all the graph. Instead, we can search efficiently
in a greedy way. In Figure 3, starting from a1, we add single links to a1 and obtain four
new alignments: a2, a3, a4, and a5. We retain the best new alignment that has a higher
score than a1, say a3, and discard the others. Then, we add single links to a3 and obtain
three new alignments: a7, a9, and a11. After choosing a9 as the current best alignment, the
next candidates are a12 and a14. Suppose the model scores of both a12 and a14 are lower
than that of a9. We terminate the search process and choose a9 as the best candidate
alignment.
During this search process, we expect that the addition of a single link l to the
current best alignment a will result in a new alignment a ? {l} with a higher score:
score(f, e, a ? {l}) > score(f, e, a) (15)
309
Computational Linguistics Volume 36, Number 3
Figure 3
Search space of a sentence pair: f1 f2 and e1e2. Each node in the directed graph is a candidate
alignment and each edge denotes a transition between two nodes by adding a link.
that is
M
?
m=1
?m
(
hm(f, e, a ? {l}) ? hm(f, e, a)
)
> 0 (16)
As a result, we can remove most of the computational overhead by calculating only
the difference of scores instead of the scores themselves. The difference of alignment
scores with the addition of a link, which we refer to as a link gain, is defined as
G(f, e, a, l) =
M
?
m=1
?mgm(f, e, a, l) (17)
where gm(f, e, a, l) is a feature gain, which is the incremental feature value after adding
a link l to the current alignment a:
gm(f, e, a, l) = hm(f, e, a ? {l}) ? hm(f, e, a) (18)
In our experiments, we use a beam search algorithm that is more general than the
above greedy algorithm. In the greedy algorithm, we retain at most one candidate in
each level of the space graph while traversing top-down. In the beam search algorithm,
we retain at most b candidates at each level.
Algorithm 1 shows the beam search algorithm. The input is a source language
sentence f and a target language sentence e (line 1). The algorithm maintains a list of
310
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Algorithm 1 A beam search algorithm for word alignment
1: procedure ALIGN(f, e)
2: open ? ?  a list of active alignments
3: N ? ?  n-best list
4: a ? ?  begin with an empty alignment
5: ADD(open, a, ?, b)  initialize the list
6: while open = ? do
7: closed ? ?  a list of promising alignments
8: for all a ? open do
9: for all l ? J ? I ? a do  enumerate all possible new links
10: a? ? a ? {l}  produce a new alignment
11: g ? GAIN(f, e, a, l)  compute the link gain
12: if g > 0 then  ensure that the score will increase
13: ADD(closed, a?, ?, b)  update promising alignments
14: end if
15: ADD(N , a?, 0, n)  update n-best list
16: end for
17: end for
18: open ? closed  update active alignments
19: end while
20: return N  return n-best list
21: end procedure
active alignments open (line 2) and an n-best list N (line 3). The aligning process begins
with an empty alignment a (line 4) and the procedure ADD(open, a, ?, b) adds a to open.
The procedure prunes the search space by discarding any alignment that has a score
worse than:
1. ? multiplied with the best score in the list, or
2. the score of b-th best alignment in the list.
For each iteration (line 6), we use a list closed to store promising alignments that
have higher scores than the current alignment. For every possible link l (line 9), we
produce a new alignment a? (line 10) and calculate the link gain G by calling the
procedure GAIN(f, e, a, l). If a? has a higher score (line 12), it is added to closed (line 13).
We also update N to keep the top n alignments explored during the search (line 15). The
n-best list will be used in training feature weights by MERT. This process iterates
until there are no promising alignments. The theoretical running time of this algorithm
is O(bJ2I2).
3. Feature Functions
The primary art in discriminative modeling is to define useful features that capture var-
ious characteristics of word alignments. Intuitively, we can include generative models
such as the IBM Models 1?5 (Brown et al 1993) as features in a discriminative model.
A straightforward way is to use a generative model itself as a feature directly (Liu, Liu,
311
Computational Linguistics Volume 36, Number 3
and Lin 2005). Another way is to treat each sub-model of a generative model as a feature
(Fraser and Marcu 2006). In either case, a generative model can be regarded as a special
case of a discriminative model where all feature weights are one. A detailed discussion
of the treatment of the IBM models as features can be found in Appendix B.
One major drawback of the IBM models is asymmetry. They are restricted such that
each source word is assigned to exactly one target word. This is not the case for many
language pairs. For example, in our running example, one Chinese word jianzhuye cor-
responds to two English words construction industry. As a result, our linear model will
produce only one-to-one alignments if the IBM models in two translation directions (i.e.,
source-to-target and target-to-source) are both used. Although some authors would use
the one-to-one assumption to simplify the modeling problem (Melamed 2000; Taskar,
Lacoste-Julien, and Klein 2005), many translation phenomena cannot be handled and
the recall cannot reach 100% in principle.
A more general way is to model alignment as an arbitrary relation between source
and target language word positions. As our linear model is capable of including many
overlapping features regardless of their interdependencies, it is easy to add features
that characterize symmetric alignments. In the following subsections, we will introduce
a number of symmetric features used in our experiments.
3.1 Translation Probability Product
To determine the correspondence of words in two languages, word-to-word translation
probabilities are always the most important knowledge source. To model a symmetric
alignment, a straightforward way is to compute the product of the translation probabil-
ities of each link in two directions.
For example, suppose that there is an alignment {(1, 2)} for a source language
sentence f1 f2 and a target language sentence e1e2; the translation probability prod-
uct is
t(e2| f1) ? t( f1|e2)
where t(e| f ) is the probability that f is translated to e and t( f |e) is the probability that e
is translated to f , respectively.
Unfortunately, the underlying model is biased: The more links added, the smaller
the product will be. For example, if we add a link (2, 2) to the current alignment and
obtain a new alignment {(1, 2), (2, 2)}, the resulting product will decrease after being
multiplied with t(e2| f2) ? t( f2|e2):
t(e2| f1) ? t( f1|e2) ? t(e2| f2) ? t( f2|e2)
The problem results from the absence of empty cepts. Following Brown et al (1993),
a cept in an alignment is either a single source word or it is empty. They assign cepts
to positions in the source sentence and reserve position zero for the empty cept. All
unaligned target words are assumed to be ?aligned? to the empty cept. For example,
in the current example alignment {(1, 2)}, the unaligned target word e1 is said to be
?aligned? to the empty cept f0. As our model is symmetric, we use f0 to denote the
empty cept on the source side and use e0 to denote the empty cept on the target side,
respectively.
312
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
If we take empty cepts into account, the product for {(1, 2)} can be rewritten as
t(e2| f1) ? t( f1|e2) ? t(e1| f0) ? t( f2|e0)
Similarly, the product for {(1, 2), (2, 2)} now becomes
t(e2| f1) ? t( f1|e2) ? t(e2| f2) ? t( f2|e2) ? t(e1| f0)
Note that after adding the link (2, 2), the new product still has more factors than the old
product. However, the new product is not necessarily always smaller than the old one.
In this case, the new product divided by the old product is
t(e2| f2) ? t( f2|e2)
t( f2|e0)
Whether a new product increases or not depends on actual translation probabilities.2
Depending on whether they are aligned or not, we divide the words in a sentence
pair into two categories: aligned and unaligned. For each aligned word, we use trans-
lation probabilities conditioned on its counterpart in two directions (i.e., t(ei| fj) and
t( fj|ei)). For each unaligned word, we use translation probabilities conditioned on empty
cepts on the other side in two directions (i.e., t(ei| f0) and t( fj|e0)).
Formally, the feature function for translation probability product is given by3
htpp(f, e, a) =
?
( j,i)?a
(
log
(
t(ei| fj)
)
+ log
(
t( fj|ei)
))
+
J
?
j=1
log
(
?(?j, 0) ? t( fj|e0) + 1 ? ?(?j, 0)
)
+
I
?
i=1
log
(
?(?i, 0) ? t(ei| f0) + 1 ? ?(?i, 0)
)
(19)
where ?(x, y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We define
the fertility of a source word fj as the number of aligned target words:
?j =
?
( j?,i)?a
?( j?, j) (20)
2 Even though we take empty cepts into account, the bias problem still exists because the product will
decrease by adding new links if there are no unaligned words. For example, the product will go down if
we further add a link (1, 1) to {(1, 2), (2, 2)} as all source words are aligned. This might not be a bad bias
because reference alignments usually do not have all words aligned and contain too many links.
Although translation probability product is degenerate as a generative model, the bias problem can be
alleviated when this feature is combined with other features such as link count (see Section 3.8).
3 We use the logarithmic form of translation probability product to avoid manipulating very small
numbers (e.g., 4.3 ? e?100) just for practical reasons.
313
Computational Linguistics Volume 36, Number 3
Table 2
Calculating feature values of translation probability product for a source sentence f1 f2 and a
target sentence e1e2.
alignment feature value
{} log
(
t(e1| f0) ? t(e2| f0) ? t( f1|e0) ? t( f2|e0)
)
{(1, 2)} log
(
t(e1| f0) ? t(e2| f1) ? t( f1|e2) ? t( f2|e0)
)
{(1, 2), (2, 2)} log
(
t(e1| f0) ? t(e2| f1) ? t(e2| f2) ? t( f1|e2) ? t( f2|e2)
)
Similarly, the fertility of a target word ei is the number of aligned source words:
?i =
?
( j,i? )?a
?(i?, i) (21)
For example, as only one English word China is aligned to the first Chinese word
Zhongguo in Figure 1, the fertility of Zhongguo is ?1 = 1. Similarly, the fertility of the
third Chinese word duiwaikaifang is ?3 = 4 because there are four aligned English
words. The fertility of the first English word The is ?1 = 0. Obviously, the words with
zero fertilities (e.g., The, ?s, and a in Figure 1) are unaligned.
In Equation (19), the first term calculates the product of aligned words, the second
term deals with unaligned source words, and the third term deals with unaligned target
words. Table 2 shows the feature values for some word alignments.
For efficiency, we need to calculate the difference of feature values instead of the
values themselves, which we call feature gain (see Equation (18)). The feature gain for
translation probability product is4
gtpp(f, e, a, j, i) = log
(
t(ei| fj)
)
+ log
(
t( fj|ei)
)
?
log
(
?(?j, 0) ? t( fj|e0) + 1 ? ?(?j, 0)
)
?
log
(
?(?i, 0) ? t(ei| f0) + 1 ? ?(?i, 0)
)
(22)
where ?j and ?i are the fertilities before adding the link ( j, i).
Although this feature is symmetric, we obtain the translation probabilities t( f |e) and
t(e| f ) by training the IBM models using GIZA++ (Och and Ney 2003).
3.2 Exact Match
Motivated by the fact that proper names (e.g., IBM) or specialized terms (e.g., DNA) are
often the same in both languages, Taskar, Lacoste-Julien, and Klein (2005) use a feature
that sums up the number of words linked to identical words. We adopt this exact match
feature in our model:
hem(f, e, a) =
?
( j,i)?a
?( fj, ei) (23)
4 For clarity, we use gtpp(f, e, a, j, i) instead of gtpp(f, e, a, l) because j and i appear in the equation.
314
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
gem(f, e, a, j, i) = ?( fj, ei) (24)
3.3 Cross Count
Due to the diversity of natural languages, word orders between two languages are usu-
ally different. For example, subject-verb-object (SVO) languages such as Chinese and
English often put an object after a verb while subject-object-verb (SOV) languages such
as Japanese and Turkish often put an object before a verb. Even between SVO languages
such as Chinese and English, word orders could be quite different too. In Figure 1,
while Zhongguo is the first Chinese word, its counterpart China is the fourth English
word. Meanwhile, the third Chinese word duiwaikaifang after Zhongguo is aligned to the
second English word opening before China. We say that there is a cross between the two
links (1, 4) and (3, 2) because (1 ? 3) ? (4 ? 2) < 0. In Figure 1, there is only one cross.
As a result, we could use the number of crosses in alignments to capture the divergence
of word orders between two languages.
Formally, the cross count feature function is given by
hcc(f, e, a) =
?
( j,i)?a
?
( j?,i? )?a
( j? j?) ? (i ? i?) < 0 (25)
gcc(f, e, a, j, i) =
?
( j?,i? )?a
( j? j?) ? (i ? i?) < 0 (26)
where expr is an indicator function that takes a boolean expression expr as the
argument:
expr =
{
1 if expr is true
0 otherwise
(27)
3.4 Neighbor Count
Moore (2005) finds that word alignments between closely related languages tend to be
approximately monotonic. Even for distantly related languages, the number of crossing
links is far less than chance since phrases tend to be translated as contiguous chunks.
In Figure 1, the dark points are positioned approximately in parallel with the diagonal
line, indicating that the alignment is approximately monotonic.
To capture such monotonicity, we follow Lacoste-Julien et al (2006) to encourage
strictly monotonic alignments by adding a bonus for any pair of links ( j, i) and ( j?, i?)
such that
j ? j? = 1 ? i ? i? = 1
In Figure 1, there is one such link pair: (3, 10) and (4, 11). We call these links
neighbors. Similarly, (5, 13) and (6, 14) are also neighbors.
Formally, the neighbor count feature function is given by
hnc(f, e, a) =
?
( j,i)?a
?
( j?,i? )?a
 j ? j? = 1 ? i ? i? = 1 (28)
315
Computational Linguistics Volume 36, Number 3
gnc(f, e, a, j, i) =
?
( j?,i? )?a
 j ? j? = 1 ? i ? i? = 1 (29)
3.5 Fertility Probability Product
Casual inspection of some word alignments quickly establishes that some Chinese
words such as Zhongguo and chengxian are often aligned to one English word whereas
other Chinese words such as duiwaikaifang tend to be translated into multiple English
words. Brown et al (1993) call the number of target words to which a source word f is
connected the fertility of f . Recall that we have given the formal definition of fertility in
the symmetric scenario in Equation (20) and Equation (21).
Besides word association (Sections 3.1 and 3.2) and word distortion (Sections 3.3
and 3.4), fertility also proves to be very important in modeling alignment because
sophisticated generative models such as the IBM Models 3?5 parameterize fertilities
directly. As our goal is to produce symmetric alignments, we calculate the product of
fertility probabilities in two directions.
Given an alignment {(1,2)} for a source sentence f1 f2 and a target sentence e1e2, the
fertility probability product is
n(1| f0) ? n(1| f1) ? n(0| f2) ? n(1|e0) ? n(0|e1) ? n(1|e2)
where n(?j| fj) is the probability that fj has a fertility of ?j and n(?i|ei) is the probability
that ei has a fertility of ?i, respectively.5 For example, n(1| f0) denotes the probability
that one target word is ?aligned? to the source empty cept f0 and n(1|e2) denotes the
probability that one source word is aligned to e2.
If we add a link (2, 2) to the current alignment and obtain a new alignment
{(1, 2), (2, 2)}, the resulting product will be
n(1| f0) ? n(1| f1) ? n(1| f2) ? n(0|e0) ? n(0|e1) ? n(2|e2)
The new product divided by the old product is
n(1| f2) ? n(0|e0) ? n(2|e2)
n(0| f2) ? n(1|e0) ? n(1|e2)
Formally, the feature function for fertility probability product is given by
hfpp(f, e, a) =
J
?
j=0
log(n(?j| fj)) +
I
?
i=0
log(n(?i|ei)) (30)
5 Brown et al (1993) treat the empty cept in a different way. They assume that at most half of the source
words in an alignment are not aligned (i.e., ?0 ? J/2) and define a binomial distribution relying on an
auxiliary parameter p0. Here, we use n(?0|e0) instead of the original form n0(?0|
?I
i=1 ?i ) just for
simplicity. See Appendix B for more details.
316
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
The corresponding feature gain is
gfpp(f, e, a, j, i) = log(n(?0 ? ?(?i, 0)| f0)) ? log(n(?0| f0)) +
log(n(?j + 1| fj) ? log(n(?j| fj)) +
log(n(?0 ? ?(?j, 0)|e0)) ? log(n(?0|e0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) (31)
where ?j and ?i are the fertilities before adding the link ( j, i).
Table 3 gives the feature values for some word alignments. In practice, we also
obtain all fertility probabilities n(?j| fj) and n(?i|ei) by using the output of GIZA++
directly.
3.6 Linked Word Count
We observe that there should not be too many unaligned words in good alignments.
For example, there are only three unaligned words on the target side in Figure 1: The,
?s, and a. Unaligned words are usually function words that have little lexical meaning
but instead serve to express grammatical relationships with other words or specify the
attitude or mood of the speaker. To control the number of unaligned words, we follow
Moore, Yih, and Bode (2006) to introduce a linked word count feature that simply counts
the number of aligned words:
hlwc(f, e, a) =
J
?
j=1
?j > 0 +
I
?
i=1
?i > 0 (32)
glwc(f, e, a, j, i) = ?(?j, 0) + ?(?i, 0) (33)
In Equation (33), ?j and ?i are the fertilities before adding l.
3.7 Sibling Distance
In word alignments, there are usually several words connected to the same word on the
other side. For example, in Figure 1, two English words construction and industry are
aligned to one Chinese word jianzhuye. We call the words aligned to the same word on
the other side siblings. In Figure 1, opening, to, the, and outside are also siblings because
they are aligned to duiwaikaifang. A word (e.g., jianzhuye) often tends to produce a series
of words in another language that belong together, whereas others (e.g., duiwaikaifang)
Table 3
Calculating feature values of fertility probability product for a source sentence f1 f2 and a target
sentence e1e2.
alignment feature value
{} log(n(2| f0) ? n(0| f1) ? n(0| f2) ? n(2|e0) ? n(0|e1) ? n(0|e2))
{(1, 2)} log(n(1| f0) ? n(1| f1) ? n(0| f2) ? n(1|e0) ? n(0|e1) ? n(1|e2))
{(1, 2), (2, 2)} log(n(1| f0) ? n(1| f1) ? n(1| f2) ? n(0|e0) ? n(0|e1) ? n(2|e2))
317
Computational Linguistics Volume 36, Number 3
tend to produce a series of words that should be separate. To model this tendency, we
introduce a feature that sums up the distances between siblings.
Formally, we use ?j,k to denote the position of the k-th target word aligned to a
source word fj and use ?i,k to denote the position of the k-th source word aligned to a
target word ei. For example, jianzhuye is the second source word (i.e., f2) in Figure 1.
As the first target word aligned to f2 is construction (i.e., e6), therefore we say that
?2,1 = 6. Similarly, ?2,2 = 7 because industry (i.e., e7) is the second target word aligned
to jianzhuye. Obviously, ?j,k+1 is always greater than ?j,k by definition.
As construction and industry are siblings, we define the distance between them
as ?2,2 ? ?2,1 ? 1 = 0. Note that we give no penalty to siblings that belong closely
together. In Figure 1, there are four siblings opening, to, the, and outside aligned to the
source word duiwaikaifang. The sum of distances between them is calculated as
?3,2 ? ?3,1 ? 1 + ?3,3 ? ?3,2 ? 1 + ?3,4 ? ?3,3 ? 1
= ?3,4 ? ?3,1 ? 3
= 10 ? 2 ? 3
= 5
Therefore, the distance sum of fj can be efficiently calculated as
?( j, ?j) =
{
?j,?j ? ?j,1 ? ?j + 1 if ?j > 1
0 otherwise
(34)
Accordingly, the distance sum of ei is
?(i, ?i) =
{
?i,?i ? ?i,1 ? ?i + 1 if ?i > 1
0 otherwise
(35)
Formally, the feature function for sibling distance is given by
hsd(f, e, a) =
J
?
j=1
?( j, ?j) +
I
?
i=1
?(i, ?i) (36)
The corresponding feature gain is
gsd(f, e, a, j, i) = ?( j, ?j + 1) ? ?( j, ?j) +
?(i, ?i + 1) ??(i, ?i) (37)
where ?j and ?i are the fertilities before adding the link ( j, i).
3.8 Link Count
Given a source sentence with J words and a target sentence with I words, there are
J ? I possible links. However, the actual number of links in a reference alignment is
usually far less. For example, there are only 10 links in Figure 1 although the maximum
is 6 ? 14 = 84. The number of links has an important effect on alignment quality because
318
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
more links result in higher recall while fewer links result in higher precision. A good
trade-off between recall and precision usually results from a reasonable number of links.
Using the number of links as a feature could also alleviate the bias problem posed by
the translation probability product feature (see Section 3.1). A negative weight of the
link count feature often leads to fewer links while a positive weight favors more links.
Formally, the feature function for link count is
hlc(f, e, a) = |a| (38)
glc(f, e, a, l) = 1 (39)
where |a| is the cardinality of a (i.e., the number of links in a).
3.9 Link Type Count
Due to the different fertilities of words, there are different types of links. For instance,
one-to-one links indicate that one source word (e.g., Zhongguo) is translated into ex-
actly one target word (e.g., China) while many-to-many links exist for phrase-to-phrase
translation. The distribution of link types differs for different language pairs. For ex-
ample, one-to-one links occur more frequently in closely related language pairs (e.g.,
French?English) and one-to-many links are more common in distantly related language
pairs (e.g., Chinese?English). To capture the distribution of link types independent of
languages, we use features to count different types of links.
Following Moore (2005), we divide links in an alignment into four categories:
1. one-to-one links, in which neither the source nor the target word
participates in other links;
2. one-to-many links, in which only the source word participates in other
links;
3. many-to-one links, in which only the target word participates in other
links;
4. many-to-many links, in which both the source and target words
participate in other links.
In Figure 1, (1, 4), (4, 11), (5, 13), and (6, 14) are one-to-one links and the others are
one-to-many links.
As a result, we introduce four features:
ho2o(f, e, a) =
?
( j,i)?a
?j = 1 ? ?i = 1 (40)
ho2m(f, e, a) =
?
( j,i)?a
?j > 1 ? ?i = 1 (41)
hm2o(f, e, a) =
?
( j,i)?a
?j = 1 ? ?i > 1 (42)
hm2m(f, e, a) =
?
( j,i)?a
?j > 1 ? ?i > 1 (43)
319
Computational Linguistics Volume 36, Number 3
Their feature gains cannot be calculated in a straightforward way because the
addition of a link might change the link types of its siblings on both the source and
target sides. For example, if we align the Chinese word chengxian and the English word
industry, the newly added link (4, 7) is a many-to-many link. Its source sibling (2, 7),
which was a one-to-many link, now becomes a many-to-many link. Meanwhile, its
target sibling (4, 11), which was a one-to-one link, now becomes a one-to-many link.
Algorithm 2 shows how to calculate the four feature gains. After initialization
(line 2), we first decide the type of l (lines 3?11). Then, we consider the siblings of l
on the target side (lines 12?24) and those on the source side (lines 25?38), respectively.
Note that the feature gains of siblings will not change if ?i = 1 or ?j = 1.
3.10 Bilingual Dictionary
A conventional bilingual dictionary can be considered an additional knowledge source.
The intuition is that a dictionary is expected to be more reliable than an automatically
trained lexicon. For example, if Zhongguo and China appear in an entry of a dictionary,
they should be more likely to be aligned. Thus, we use a single indicator feature to
encourage linking word pairs that occur in a dictionary D:
hbd(f, e, a, D) =
?
( j,i)?a
( fj, ei) ? D (44)
gbd(f, e, a, D, j, i) = ( fj, ei) ? D (45)
3.11 Link Co-Occurrence Count
The system combination technique that integrates predictions from multiple systems
proves to be effective in machine translation (Rosti, Matsoukas, and Schwartz 2007;
He et al 2008). In word alignment, a link should be aligned if it appears in most
system predictions. Taskar, Lacoste-Julien, and Klein (2005) include the IBM Model 4
predictions as features and obtain substantial improvements.
To enable system combination, we design a feature to favor links voted by most
systems. Given an alignment a? produced by another system, we use the number of
links of the intersection of a and a? as a feature:
hlcc(f, e, a, a
?) = |a ? a?| (46)
glcc(f, e, a, a
?, j, i) = l ? a ? a? (47)
4. Experiments
In this section, we try to answer two questions:
1. Does the proposed approach achieve higher alignment quality than
generative alignment models?
2. Do statistical machine translation systems produce better translations if
we replace generative alignment models with the proposed approach?
In Section 4.1, we evaluate our approach on three word alignment shared tasks for
five language pairs with varying divergence and richness of resources. Experimental
320
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Algorithm 2 Calculating gains for the link type count features
1: procedure GAINLINKTYPECOUNT(f, e, a, j, i)
2: {go2o, go2m, gm2o, gm2m} ? {0, 0, 0, 0}  initialize the feature gains
3: if ?j = 0 ? ?i = 0 then  consider ( j, i) first
4: go2o ? go2o + 1
5: else if ?j > 0 ? ?i = 0 then
6: go2m ? go2m + 1
7: else if ?j = 0 ? ?i > 0 then
8: gm2o ? gm2o + 1
9: else
10: gm2m ? gm2m + 1
11: end if
12: if ?j = 1 then  consider the siblings of ( j, i) on the target side
13: for i? = 1 . . . I do
14: if ( j, i?) ? a ? i? = i then  ( j, i?) is a sibling of ( j, i) on the target side
15: if ?i? = 1 then  ( j, i?) was a one-to-one link
16: go2o ? go2o ? 1
17: go2m ? go2m + 1  ( j, i?) now becomes a one-to-many link
18: else  ( j, i?) was a many-to-one link
19: gm2o ? gm2o ? 1
20: gm2m ? gm2m + 1  ( j, i?) now becomes a many-to-many link
21: end if
22: end if
23: end for
24: end if
25: if ?i = 1 then  consider the siblings of ( j, i) on the source side
26: for j? = 1 . . . J do
27: if ( j?, i) ? a ? j? = j then  ( j?, i) is a sibling of ( j, i) on the source side
28: if ?j? = 1 then  ( j?, i) was a one-to-one link
29: go2o ? go2o ? 1
30: gm2o ? gm2o + 1  ( j?, i) now becomes a many-to-one link
31: else  ( j?, i) was a one-to-many link
32: go2m ? go2m ? 1
33: gm2m ? gm2m + 1  ( j?, i) now becomes a many-to-many link
34: end if
35: end if
36: end for
37: end if
38: return {go2o, go2m, gm2o, gm2m}  return the four feature gains
39: end procedure
results show that our system outperforms systems participating in the three shared
tasks significantly and achieves comparable results with other state-of-the-art discrimi-
native alignment models.
In Section 4.2, we investigate the effect of our model on translation quality. By
training feature weights with respect to F-measure instead of AER, our model results
in superior translation quality over generative methods for phrase-based, hierarchical
phrase-based, and tree-to-string SMT systems.
321
Computational Linguistics Volume 36, Number 3
4.1 Evaluation of Alignment Quality
In this section, we present results of experiments on three word alignment shared tasks:
1. HLT/NAACL 2003 shared task (Mihalcea and Pedersen 2003). As part of
the HLT/NAACL 2003 workshop on ?Building and Using Parallel Texts:
Data Driven Machine Translation and Beyond,? this shared task includes
two language pairs: English?French and Romanian?English. Participants
can use both limited and unlimited resources.
2. ACL 2005 shared task (Martin, Mihalcea, and Pedersen 2005). As part of
the ACL 2005 workshop on ?Building and Using Parallel Texts: Data
Driven Machine Translation and Beyond,? this shared task includes three
language pairs to cover different language and data characteristics:
English?Inuktitut, Romanian?English, and English?Hindi. Participants
can use both limited and unlimited resources.
3. HTRDP 2005 shared task. As part of the 2005 HTRDP (National High
Technology Research and Development Program of China, also called
?863? Program) Evaluation on Chinese Information Processing and
Intelligent Human-Machine Interface Technology, this shared task
included only one language pair: Chinese?English. Participants can use
unlimited resources.
Among these, we choose two tasks, English?French and Chinese?English, to report
detailed experimental results. Results for the other tasks can also be found in Table 11.
Corpus statistics for the English?French and Chinese?English tasks are shown in
Tables 4 and 5. The English?French data from the HLT/NAACL 2003 shared task consist
of a training corpus of 1,130,104 sentence pairs, a development corpus of 37 sentence
pairs, and a test corpus of 447 sentence pairs. The development and test sets are manu-
ally aligned and marked with both sure and possible labels. Although the Canadian
Hansard bilingual corpus is widely used in the community, direct comparisons are
difficult due to the differences in splitting of training data, development data, and test
data. To make our results more comparable to previous work, we followed Lacoste-
Table 4
Corpus characteristics of the English?French task.
English French
Training corpus Sentences 1,130,104
Words 20.01M 23.61M
Vocabulary 68, 019 86, 591
Development corpus Sentences 37
Words 661 721
Vocabulary 322 344
Test corpus Sentences 447
Words 7, 020 7, 761
Vocabulary 1, 732 1, 943
322
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 5
Corpus characteristics of the Chinese?English task.
Chinese English
Training corpus Sentences 837,594
Words 10.32M 10.71M
Vocabulary 93, 532 134, 143
Development corpus Sentences 502
Words 9, 338 9, 364
Vocabulary 2, 608 2, 587
Test corpus Sentences 505
Words 9, 088 10, 224
Vocabulary 2, 319 2, 651
Julien et al (2006) splitting the original test set into two parts: the first 200 sentences
as the development set and the remaining 247 sentences as the test set. To compare
with systems participating in the 2003 NAACL shared task, we also used the small
development set of 37 sentences to optimize feature weights, and ran our system on the
original test set of 447 sentences. The results are shown in Table 11.
The Chinese?English data from the HTRDP 2005 shared task contains a develop-
ment corpus of 502 sentence pairs and a test corpus of 505 sentence pairs. We use
a training corpus of 837, 594 sentence pairs available from Chinese Linguistic Data
Consortium and a bilingual dictionary containing 415, 753 entries.
4.1.1 Comparison of the Search Algorithm with GIZA++. We develop a word alignment
system named Vigne based on the linear modeling approach. As we mentioned before,
our model can include the IBM models as features (see Appendix B). To investigate the
effectiveness of our search algorithm, we compare Vigne with GIZA++ by using the
same models.
Table 6 shows the alignment error rate percentages for various IBM models in
GIZA++ and Vigne. To make the results comparable, we ensured that Vigne shared
Table 6
Comparison of AER scores for various IBM models in GIZA++ and Vigne. These models are
trained only on development and test sets. The pruning setting for Vigne is ? = 0 and b = 1. All
differences are not statistically significant.
English?French Chinese?English
Model Training Scheme Direction GIZA++ Vigne GIZA++ Vigne
Model 1 15 S ? T 50.6 50.6 58.0 58.0
T ? S 46.2 46.2 56.1 56.1
Model 2 1525 S ? T 47.8 47.8 59.3 59.3
T ? S 43.6 43.6 57.4 57.4
Model 3 15H533 S ? T 31.6 31.4 45.0 44.5
T ? S 27.9 27.9 47.4 46.5
Model 4 15H53343 S ? T 34.5 34.2 44.9 44.6
T ? S 30.8 30.6 46.7 46.4
323
Computational Linguistics Volume 36, Number 3
the same parameters with GIZA++.6 Table 6 also gives the training schemes used for
GIZA++. For example, the training scheme for Model 4 is 15H53343. This notation
indicates that five iterations of Model 1, five iterations of HMM, three iterations of
Model 3, and three iterations of Model 4 are performed. As the two systems use the same
model parameters, the amount of training data will have no effect on the comparison.
Therefore, we trained the IBM Models only on the development and test sets. As a re-
sult, the AER scores in Table 6 look quite high.
In GIZA++, there exist simple polynomial algorithms to find the Viterbi alignments
for Models 1 and 2. We observe that the greedy search algorithm (? = 0 and b = 1)
used by Vigne can also find the optimal alignments. Note that the two systems achieve
identical AER scores because there are no search errors.
For Models 3 and 4, maximization over all alignments cannot be efficiently carried
out as the corresponding search problem is NP-complete. To alleviate the problem,
GIZA++ resorts to a greedy search algorithm. The basic idea is to compute a Viterbi
alignment of a simple model such as Model 2 or HMM. This alignment (an intermediate
node in the search space) is then iteratively improved with respect to the alignment
probability of the refined model by moving or swapping links. In contrast, our search
algorithm starts from an empty alignment and has only one operation: adding a link.
In addition, we treat the fertility probability of an empty cept in a different way (see
Equation B.7). Interestingly, Vigne achieves slightly better results than GIZA++ for both
models. All differences are not statistically significant.
4.1.2 Comparison to Generative Models Using Asymmetric Features. Table 7 compares the
AER scores achieved by GIZA++, Cross-EM (Liang, Taskar, and Klein 2006), and Vigne.
On both tasks, we lowercased all English words in the training, development, and
test sets as a preprocessing step. For GIZA++, we used the default training scheme
of 15H53545. We used the three symmetrization heuristics proposed by Och and Ney
(2003): intersection, union, and refined method. For Cross-EM, we also used the default
configuration and jointly trained Model 1 and HMM for five iterations. For Vigne, we
used a greedy search strategy by setting ? = 0 and b = 1. Note that both GIZA++ and
Cross-EM are unsupervised alignment methods.
On the English?French task, the refined combination of Model 4 alignments pro-
duced by GIZA++ in both translation directions yields an AER of 5.9%. Cross-EM
outperforms GIZA++ significantly by achieving 5.1%. For Vigne, we use Model 4 as
the primary feature. The linear combination of Model 4 in both directions achieves
a lower AER than either one separately. The link count feature controls the number
of links in the resulting alignments and leads to an absolute improvement of 0.1%.
With the addition of cross count and neighbor count features, the AER score drops to
5.4%. We attribute this to the fact that the two features are capable of capturing the
locality and monotonicity properties of natural languages, especially for closely related
language pairs such as English?French. After adding the linked word count feature, our
model achieves an AER of 5.2%. Finally, Vigne achieves an AER of 4.0% by combining
predictions from refined Model 4 and jointly trained HMM.
On the Chinese?English task, one-to-many and many-to-one relationships occur
more frequently in the reference alignments than the English?French task. As Cross-EM
6 In GIZA++ training, the final parameters are estimated on the final alignments, which are computed
using the parameters obtained in the previous iteration. As a result, Vigne made use of the parameters
generated by the iteration before the final iteration. In other experiments, Vigne used the final parameters.
324
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 7
Comparison of GIZA++, Cross-EM, and Vigne on both tasks. Note that Vigne yields only
one-to-one alignments if both ?Model 4 s2t? and ?Model 4 t2s? features are used. The pruning
setting for Vigne is ? = 0 and b = 1. While the final results of our system are better than the best
baseline generative models significantly at p < 0.01, adding a single feature will not always
produce a significant improvement, especially for English?French.
System Setting English?French Chinese?English
Model 4 s2t 7.7 20.9
Model 4 t2s 9.2 30.3
GIZA++ Intersection 6.8 21.8
Union 9.6 28.1
Refined method 5.9 18.4
Cross-EM HMM, joint 5.1 18.9
Model 4 s2t 7.8 20.5
+Model 4 t2s 5.6 18.3
+link count 5.5 17.7
+cross count 5.4 17.6
+neighbor count 5.2 17.4Vigne
+exact match 5.3 -
+linked word count 5.2 17.3
+bilingual dictionary - 17.1
+link co-occurrence count (GIZA++) 5.1 16.3
+link co-occurrence count (Cross-EM) 4.0 15.7
is prone to produce one-to-one alignments by encouraging agreement, symmetrizing
Model 4 by refined method yields better results than Cross-EM. We observe that the ad-
vantages of adding features such as link count, cross count, neighbor count, and linked
word count to our linear model continue to hold, resulting in a much lower AER than
both GIZA++ and Cross-EM. The addition of the bilingual dictionary is beneficial and
yields an AER of 17.1%. Further improvements were obtained by including predictions
from GIZA++ and Cross-EM.
As the IBM models do not allow a source word to be aligned with more than one
target word, the activation of the IBM models in both directions always yields one-
to-one alignments and thus has a loss in recall. To alleviate this problem, we use a
heuristic postprocessing step to produce many-to-one or one-to-many alignments. First,
we collect links that have higher translation probabilities than corresponding null links
in both directions. Then, these candidate links are sorted according to their translation
probabilities. Finally, they are added to the alignments under structural constraints
similar to those of Och and Ney (2003).
On the English?French task, this symmetrization method achieves relatively small
but very consistent improvements ranging from 0.1% to 0.2%. On the Chinese?English
task, the improvements are more significant, ranging from 0.1% to 0.8%. This differ-
ence also results from the fact that the reference alignments of the Chinese?English
task contain more one-to-many and many-to-one relationships than the English?French
task. After symmetrization, the final AER scores for the two tasks are 3.8% and 15.1%,
respectively.
4.1.3 Resulting Feature Weights. Table 8 shows the resulting feature weights of minimum
error rate training. We observe that adding new features has an effect on the weights
325
Computational Linguistics Volume 36, Number 3
Table 8
Resulting feature weights of minimum error rate training on the Chinese?English task (M4ST:
Model 4 s2t; M4TS: Model 4 t2s; LC: link count; CC: cross count; NC: neighbor count; LWC:
linked word count; BD: bilingual dictionary; LCCG: link co-occurrence count (GIZA++); LCCC:
link co-occurrence count (Cross-EM)).
M4ST M4TS LC CC NC LWC BD LCCG LCCC
M4ST 1.00 - - - - - - - -
+M4TS 0.63 0.37 - - - - - - -
+LC 0.18 0.07 ?0.75 - - - - - -
+CC 0.19 0.07 ?0.56 ?0.18 - - - - -
+NC 0.12 0.06 ?0.55 ?0.08 0.17 - - - -
+LWC 0.14 0.08 ?0.22 ?0.08 0.25 ?0.26 - - -
+BD 0.07 0.02 ?0.35 ?0.05 0.16 0.01 0.34 - -
+LCCG 0.03 0.04 ?0.13 ?0.05 0.20 ?0.16 0.28 0.11 -
+LCCC 0.02 0.02 0.14 ?0.03 0.10 ?0.26 0.30 0.04 0.09
of other features. The weights of the cross count feature are consistently negative,
suggesting that crossing links are always discouraged for Chinese?English. Also, the
positive weights of the neighbor count feature indicate that monotonic alignments
are encouraged. When the bilingual dictionary was included, the weights of Model 4
features in both directions dramatically decreased.
4.1.4 Results of the Symmetric Alignment Model. As we mentioned before, the linear model
can model many-to-many alignments directly without any postprocessing symmetriza-
tion heuristics.
Table 9 demonstrates the results of the symmetric alignment model on both tasks.
As the activation of translation and fertility probability products allows for arbitrary
relationships, the addition of the link count feature excludes most loosely related links
Table 9
AER scores achieved by the symmetric alignment model on both tasks. The pruning setting for
Vigne is ? = 0 and b = 1. Although the final model obviously outperforms the initial model
significantly at p < 0.01, adding a single feature will not always result in a significant
improvement, especially for English?French.
Features English?French Chinese?English
translation probability product 17.3 23.6
+fertility probability product 14.6 22.6
+link count 14.5 21.6
+cross count 5.8 18.5
+neighbor count 5.2 17.2
+exact match 5.1 -
+linked word count 5.2 17.0
+link types 5.0 16.9
+sibling distance 4.9 16.2
+bilingual dictionary - 15.9
+link co-occurrence count (GIZA++) 4.5 15.1
+link co-occurrence count (Cross-EM) 3.7 14.5
326
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
and results in more significant improvements than for asymmetric IBM models. One
interesting finding is that the cross count feature is very useful, leading to dramatic
absolute reduction of 8.7% on the English?French task and 3.1% on the Chinese?English
task, respectively. We find that the advantages of adding neighbor count and linked
word count still hold. By further including predictions from GIZA++ and Cross-EM,
our linear model achieves the best result: 3.7% on the English?French task and 14.5%
on the Chinese?English task.
We find that the symmetric linear model outperforms the asymmetric one, espe-
cially on the Chinese?English task. This suggests that although the asymmetric model
can produce symmetric alignments via symmetrization heuristics, the ?genuine? sym-
metric model produces many-to-many alignment in a more natural way.
4.1.5 Effect of Beam Search. Table 10 shows the effect of varying beam widths. The aligning
speed (words per second) decreases almost linearly with the increase of beam width b.
For simple alignment models such as using only the translation probability product
feature, enlarging the beam size fails to bring improvements due to modeling errors.
When more features are added, the model becomes more expressive. Therefore, our
system benefits from larger beam size consistently, although some benefits are not
significant statistically. When we set b = 10, the final AER scores for the English?French
and Chinese?English tasks are 3.6% and 14.3%, respectively.
4.1.6 Effect of Training Corpus Size. One disadvantage of our approach is that we need
a hand-aligned training corpus for training feature weights. However, compared with
building a treebank, manual alignment is far less expensive because one annotator only
needs to answer yes?no questions: Should this pair of words be aligned or not? If well
trained, even a non-linguist who is familiar with both source and target languages could
Table 10
Comparison of aligning speed (words per second) and AER score with varying beam widths for
the Chinese?English task. We fix ? = 0.01. Bold numbers refer to the results that are better than
the baseline but not significantly so. We use ?+? to denote the results that outperform the best
baseline (b = 1) and are statistically significant at p < 0.05. Similarly, we use ?++? to denote
significantly better than baseline at p < 0.01.
b=1 b=5 b=10
Features w/sec AER w/sec AER w/sec AER
translation probability product 3, 941 23.6 843 23.6 426 23.7
+fertility probability product 1, 418 22.6 300 22.7 150 22.9
+link count 1, 557 21.6 330 21.7 166 21.9
+cross count 1, 696 18.5 359 18.6 180 18.6
+neighbor count 1, 648 17.2 355 16.8+ 178 16.7+
+linked word count 1, 627 17.0 351 16.4+ 176 16.5+
+sibling distance 1, 531 16.9 326 16.5+ 165 16.4+
+link types 899 16.2 187 15.6+ 96 15.5++
+bilingual dictionary 890 15.9 187 15.6 94 15.5+
+link co-occurrence count (GIZA++) 877 15.1 182 15.0 92 14.9
+link co-occurrence count (Cross-EM) 867 14.5 183 14.4 92 14.3
327
Computational Linguistics Volume 36, Number 3
produce high-quality alignments. We estimate that aligning a sentence pair usually
takes only two minutes on average.
An interesting question is: How many training examples are needed to train a
good discriminative model? Figure 4 shows the learning curves with different numbers
of features on the Chinese?English task. We choose four feature groups with varying
numbers of features: 3, 6, 10, and 14. There are eight fractions of the training corpus: 10,
20, 50, 100, 200, 300, 400, and 502. Generally, the more features a model uses, the more
training examples are needed to train feature weights. Surprisingly, even when we use
14 features, 50 sentences seem to be good enough for minimum error rate training. This
finding suggests that our approach could work well even with a quite small training
corpus.
4.1.7 Summary of Results. Table 11 summarizes the results on all three shared tasks.
Vigne used the same configuration for all tasks. We used the symmetric linear model
and activated all features. The pruning setting is ? = 0.01 and b = 10. Our system
outperforms the systems participating in all the three shared tasks significantly.
Note that for the English?French task we used the small development set of 37
sentences to optimize feature weights, and ran our system on the original test set of
447 sentences. For the Romanian?English language pair, we follow Fraser and Marcu
(2006) in reducing the vocabulary by stemming Romanian and English words down to
their first four characters. For the other language pairs, English?Inuktitut and English?
Hindi, the symmetric linear model maintains its superiority over the asymmetric linear
model and yields better results than the other participants.
Figure 4
Effect of training corpus size on the Chinese?English task. We choose four feature groups with
varying numbers of features: 3, 6, 10, and 14. There are eight training corpora with varying
numbers of sentence pairs: 10, 20, 50, 100, 200, 300, 400, and 502.
328
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
Table 11
Comparison with the systems participating in the three shared tasks. ?non-null? denotes that the
reference alignments have no null links, ?null? denotes that the reference alignments have null
links, ?limited? denotes only limited resources can be used, and ?unlimited? denotes that there
are no restrictions on resources used.
Shared Task Task Participants Vigne
Romanian?English, non-null, limited 28.9?52.7 23.5
Romanian?English, null, limited 37.4?59.8 26.9HLT-NAACL 2003
English?French, non-null, limited 8.5?29.4 4.0
English?French, null, limited 18.5?51.7 4.6
English?Inuktitut, limited 9.5?71.3 8.9
ACL 2005 Romanian?English, limited 26.6?44.5 24.7
English?Hindi, limited 51.4 44.8
HTRDP 2005 Chinese?English, unlimited 23.5?49.2 14.3
4.1.8 Comparison to Other Work. In the word alignment literature, the Canadian Hansard
bilingual corpus is the most widely used data set. Table 12 lists alignment error rates
achieved by previous work and our system. Note that direct comparisons are problem-
atic due to the different configurations of training data, development data, and test data.
Our result matches the state-of-the-art performance on the Hansard data (Lacoste-Julien
et al 2006; Moore, Yih, and Bode 2006).
4.2 Evaluation of Translation Quality
In this section, we report on experiments with Chinese-to-English translation. To inves-
tigate the effect of our discriminative model on translation performance, we used three
translation systems:
1. Moses (Koehn and Hoang 2007), a state-of-the-art phrase-based SMT
system;
2. Hiero (Chiang 2007), a state-of-the-art hierarchical phrase-based system;
3. Lynx (Liu, Liu, and Lin 2006), a linguistically syntax-based system that
makes use of tree-to-string rules.
Table 12
Comparison of some word alignment systems on the Canadian Hansard data.
System Training Test AER
Och and Ney (2003) 1.5M 500 5.2
Moore (2005) 500K 223 7.5
Taskar, Lacoste-Julien, and Klein (2005) 1.1M 347 5.4
Liang, Taskar, and Klein (2006) 1.1M 347 4.9
Lacoste-Julien et al (2006) 1.1M 247 3.8
Blunsom and Cohn (2006) 1.1M 347 5.2
Moore, Yih, and Bode (2006) 1.1M 223 3.7
This work 1.1M 247 3.6
329
Computational Linguistics Volume 36, Number 3
For all three systems we trained the translation models on the FBIS corpus
(7.2M+9.2M words). For the language model, we used the SRI Language Modeling
Toolkit (Stolcke 2002) to train a trigram model with modified Kneser-Ney smoothing
on the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation
test set as the development set for training feature weights of translation systems, the
2005 test set as the devtest set for choosing optimal values of ? for different translation
systems, and the 2008 test set as the final test set. Our evaluation metric is case-sensitive
BLEU-4, as defined by NIST, that is, using the shortest (as opposed to closest) reference
length for brevity penalty.
We annotated the first 200 sentences of the FBIS corpus using the Blinker guidelines
(Melamed 1998). All links are sure ones. These hand-aligned sentences served as the
training corpus for Vigne. To train the feature weights in our discriminative model
using minimum-error-rate training (Och 2003), we adopt balanced F-measure (Fraser
and Marcu 2007b) as the optimization criterion.
The pipeline begins by running GIZA++ and Cross-EM on the FBIS corpus. We
used seven generative alignment methods based on IBM Model 4 and HMM as baseline
systems: (1) C?E, (2) E?C, (3) intersection, (4) union, (5) refined method (Och and
Ney 2003), (6) grow-diag-final (Koehn, Och, and Marcu 2003), and (7) Cross-EM (Liang,
Taskar, and Klein 2006). Instead of exploring the entire search space, our linear model
only searches within the union of baseline predictions, which enables our system to
align large bilingual corpus at a very fast speed of 3, 000 words per second. In other
words, our system is able to annotate the FBIS corpus in about 1.5 hours. Then, we train
the feature weights of the linear model on the training corpus with respect to F-measure
under different settings of ?. After that, our system runs on the FBIS corpus to produce
word alignments using the optimized weights. Finally, the three SMT systems train their
models on the word-aligned FBIS corpus.
Can our approach achieve higher F-measure scores than generative methods with
different values of ? (the weighting factor in F-measure)? Table 13 shows the results of
all the systems on the development set. To estimate the loss from restricting the search
Table 13
Maximization of F-measure with different settings of ? (the weighting factor in the balanced
F-measure). We use IBM Model 4 and HMM as baseline systems. Our system restricts the search
space by exploring only the union of baseline predictions. We compute the ?oracle? alignments
by intersecting the union with reference alignments. We use ?+? to denote the result that
outperforms the best baseline result with statistical significance at p < 0.05. Similarly, we use
?++? to denote significantly better than baseline at p < 0.01.
? = 0.1 ? = 0.3 ? = 0.5 ? = 0.7 ? = 0.9
IBM Model 4 C?E 82.6 81.3 80.1 79.0 77.8
IBM Model 4 E?C 68.2 70.5 73.0 75.7 78.5
IBM Model 4 intersection 63.6 68.9 75.2 82.8 92.1
IBM Model 4 union 86.6 82.0 77.9 74.2 70.8
IBM Model 4 refined method 75.4 78.5 81.8 85.4 89.4
IBM Model 4 grow-diag-final 82.4 82.1 81.7 81.4 81.1
Cross-EM HMM 70.4 73.7 77.3 81.2 85.5
oracle 91.9 93.6 95.3 97.1 99.0
Vigne 87.8++ 85.8++ 86.4++ 88.6++ 93.3++
330
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
space, we compute oracle alignments by intersecting the union of baseline predictions
with reference alignments. The F-measures achieved by oracle alignments range from
91.9 to 99.0, indicating that the union of baseline predictions is good enough to approx-
imate the true search space. We observe that C?E, union, and grow-diag-final weight
recall higher because F-measure decreases when ? increases. On the other hand, E?C,
intersection, refined method, and Cross-EM weight precision higher. In particular, ?
has a weak effect on grow-diag-final as its F-measure always keeps above 0.8 when ?
is varied. For each ?, we trained a set of feature weights to maximize the F-measure on
the development set. We find that our discriminative model outperforms the baseline
systems significantly at all values of ?.
Table 14 shows the BLEU scores of the three systems on the devtest set. For Moses
and Hiero, we used the default setting. For Lynx, we used the phrase pairs learned by
Moses to improve rule coverage (Liu, Liu, and Lin 2006). The best generative alignment
method is grow-diag-final, which is widely used in SMT. For all the three SMT systems,
our system outperforms the baseline systems statistically significantly. For Moses, the
best value of ? is 0.5. For Hiero and Lynx, the best ? is 0.3, suggesting that recall-
oriented alignments yield better translation performance.
Table 15 gives the BLEU scores of the three systems on the final test set. We used the
parameters optimized on the dev and devtest sets. More specifically, Moses used grow-
diag-final and ? = 0.5, Hiero used grow-diag-final and ? = 0.3, and Lynx used union
and ? = 0.3. We find that our discriminative alignment model improves the three
systems significantly.
5. Related Work
The first generative alignment models were the IBM Models 1?5 proposed by Brown
et al (1993). Vogel and Ney (1996) propose a first-order Hidden Markov model (HMM)
for word alignment. They show that it is beneficial to make the alignment probabilities
dependent on differences in position rather than on the absolute positions. Och and
Table 14
BLEU scores on the devtest set. We use ?+? to denote the result that outperforms the best
baseline result (highlighted in bold) statistically significantly at p < 0.05. Similarly, we use ?++?
to denote significantly better than baseline at p < 0.01.
Moses Hiero Lynx
IBM Model 4 C?E 24.7 25.7 24.8
IBM Model 4 E?C 20.6 23.5 21.6
IBM Model 4 intersection 20.1 23.2 21.2
IBM Model 4 union 24.3 24.1 25.1
IBM Model 4 refined method 24.2 24.0 24.2
IBM Model 4 grow-diag-final 25.0 25.8 24.3
Cross-EM HMM 23.6 24.9 24.8
? = 0.1 tuned 23.9 25.3 26.0++
? = 0.3 tuned 24.9 26.8++ 26.1++
Vigne ? = 0.5 tuned 25.7+ 26.6++ 24.3
? = 0.7 tuned 23.7 25.4 24.7
? = 0.9 tuned 21.9 24.7 23.9
331
Computational Linguistics Volume 36, Number 3
Table 15
BLEU scores on the final test set. We use the parameters optimized on the dev and devtest sets.
We use ?+? to denote the result that outperforms the best baseline result (indicated in bold)
statistically significantly at p < 0.05. Similarly, we use ?++? to denote significantly better than
baseline at p < 0.01.
Moses Hiero Lynx
generative 20.1 20.7 19.9
discriminative 20.8+ 21.6+ 21.0++
Ney (2003) re-implement the IBM models and the HMM model and compare them with
heuristic approaches systematically. The resulting toolkit GIZA++ developed by Franz
J. Och is the most popular alignment system nowadays. Liang, Taskar, and Klein (2006)
present an unsupervised way to produce symmetric alignments by training two simple
asymmetric models (e.g., IBM Model 1 and the HMM model) jointly to maximize a
combination of data likelihood and agreement between the models. Fraser and Marcu
(2007a) introduce a new generative model called LEAF that directly models many-
to-many non-consecutive word alignments. Their model can be trained using both
unsupervised and semi-supervised training methods.
Recent years have witnessed the rapid development of discriminative alignment
methods. As a first attempt, Och and Ney (2003) proposed the Model 6, which is a
log-linear combination of the IBM models and the HMM model. Cherry and Lin (2003)
develop a statistical model to find word alignments, which allows for easy integration
of context-specific features. Liu, Liu, and Lin (2005) apply the log-linear model used
in SMT (Och and Ney 2002) to word alignment and report significant improvements
over the IBM models. Moore (2005) presents a discriminative framework for word
alignment and uses averaged perceptron for parameter optimization. Taskar, Lacoste-
Julien, and Klein (2005) treat the alignment prediction task as a maximum weight
bipartite matching problem and use the large-margin method to train feature weights.
Neural networks and transformation-based learning have also been introduced to word
alignment (Ayan, Dorr, and Monz 2005a, 2005b). Blunsom and Cohn (2006) propose a
new discriminative model based on conditional random fields (CRF). Fraser and Marcu
(2006) use sub-models of IBM Model 4 as features and train feature weights using a
semi-supervised algorithm. Ayan and Dorr (2006b) use a maximum entropy model to
combine word alignments. Cherry and Lin (2006) show that introducing soft syntactic
constraints through discriminative training can improve alignment quality. Lacoste-
Julien et al (2006) extend the bipartite matching model of Taskar, Lacoste-Julien, and
Klein (2005) by including fertility and first-order interactions. Recently, max-product
belief propagation has been successfully applied to discriminative word alignment
(Niehues and Vogel 2008; Cromiere`s and Kurohashi 2009). Haghighi et al (2009) investi-
gate supervised word alignment methods that exploit inversion transduction grammar
(ITG) constraints.
Our work can be seen as an application of the linear model (Och 2003) in word
alignment. While aiming at producing symmetric word alignments in a discriminative
way, our approach uses asymmetric generative models (Brown et al 1993) as the major
information sources. Our linear model is similar to that of Moore, Yih, and Bode (2006).
They train two linear models called stage 1 and stage 2. The feature values are extracted
from word-aligned sentence pairs. After the stage 1 model aligns the entire training
corpus automatically, the stage 2 model uses features based not only on the parallel
332
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
sentences themselves but also on statistics of the alignments produced by the stage 1
model. They use average perceptron and support vector machine (SVM) to train feature
weights and use a beam search algorithm to find the most probable alignments. Table 12
shows that the two methods achieve comparable results on the Hansard data, confirm-
ing Moore, Yih, and Bode?s (2006) claim that model structure and feature selection are
more important than discriminative training method.
6. Conclusions and Future Work
We have presented a discriminative framework for word alignment based on the linear
modeling approach. This framework is easy to extend by including features that char-
acterize the aligning process. In addition, our approach supports symmetric alignment
modeling that allows for an arbitrary relationship between source and target language
positions. As the linear model offers excellent flexibility in using a large variety of
features and in combining information from various sources, it is able to produce good
predictions on language pairs that are either closely related (e.g., English?French) or dis-
tantly related (e.g., English?Inuktitut), either with rich resources (e.g., Chinese?English)
or with scarce resources (e.g., English?Hindi). We further show that our approach can
benefit different types of SMT systems: phrase-based, hierarchical phrase-based, and
syntax-based.
The real benefit of our model does not stem from the use of the linear model, but
rather from the discriminative training that optimizes feature weights with respect to
evaluation metrics on the gold-standard word alignments. One disadvantage of our
approach is the need for annotated training data. Although we have shown that a
very small number of training examples would be enough for parameter estimation
(Section 4.1.6), it is difficult to select such a representative training corpus to ensure that
the model will work well on unseen data, especially when the bilingual corpus to be
aligned consists of parallel texts from different domains.
Another problem is that it is hard to find an evaluation metric for word alignment
that correlates well with translation quality because the relationship between alignment
and translation is still not quite clear. Without a good loss function, discriminative
models cannot outperform generative models in large-scale applications. Therefore, it is
important to investigate how to select training examples and how to choose optimiza-
tion criterion.
The design of feature functions is most important for a discriminative alignment
model. Often, we need to try various feature groups manually on the development set
to determine the optimal feature group. Furthermore, a feature group optimized for one
language pair may not have the same effect on another one. In the future, we plan to
investigate an algorithm for automatic feature selection.
Appendix A: Table of Notation
f source sentence
fS1 sequence of source sentences: f1, . . . , fs, . . . , fS
f source word
J length of f
j position in f, j = 1, 2, . . . , J
fj the j-th word in f
f0 empty cept on the source side
333
Computational Linguistics Volume 36, Number 3
e target sentence
eS1 sequence of target sentences: e1, . . . , es, . . . , eS
e target word
I length of e
i position in e, i = 1, 2, . . . , I
ei the i-th word in e
e0 empty cept on the target side
a alignment
l a link ( j, i) in a
?j number of positions of e connected to position j of f
?i number of positions of f connected to position i of e
?j,k position of the k-th target word aligned to fj
?i,k position of the k-th source word aligned to ei
?( j, ?j) sum of sibling distances for fj
?(i, ?i) sum of sibling distances for ei
score(f, e, a) a score that indicates how well a is the alignment between f and e
a? the best candidate alignment
? feature weight
? the feature weight being optimized
h(f, e, a) feature function
G(f, e, a, l) link gain after adding l to a
g(f, e, a, l) feature gain after adding l to a
?(f, e, a) value of the feature being optimized
?(f, e, a) dot-product of fixed features
t(e| f ) the probability that f is translated to e
t( f |e) the probability that e is translated to f
n(?| f ) the probability that f has a fertility of ?
n(?|e) the probability that e has a fertility of ?
r reference alignment
Cs set of candidate alignments for the s-th training example
as,k the k-th candidate alignment for the s-th training example
E(r, a) loss function that measures alignment quality
? the precision/recall weighting factor in balanced F-measure
? pruning threshold in the beam search algorithm
b beam size in the beam search algorithm
?(x, y) the Kronecker function, which is 1 if x = y and 0 otherwise
expr an indicator function taking a boolean expression expr as the argument
Appendix B: Using the IBM Models as Feature Functions
In this article, we use IBM Models 1?4 as feature functions by taking the logarithm of the
models themselves rather than the sub-models just for simplicity. It is easy to separate
each sub-model as a feature as suggested by Fraser and Marcu (2006). We distinguish
334
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
between two translation directions (i.e., source-to-target and target-to-source) to use the
IBM models as feature functions. All model parameters are estimated by GIZA++ (Och
and Ney 2003).
The feature function for the IBM Model 1 is
hm1 (f, e, a) = log
(
( J|I)
(I + 1)J
J
?
j=1
t( fj|eaj )
)
(B.1)
where ( J|I) predicts the length of the source sentence conditioned on that of the target
sentence, (I + 1)?J defines a uniform distribution of the alignment between source and
target words, and t( fj|ei) is a translation sub-model. Note that aj = i, which means that
fj is connected to ei.
The corresponding feature gain is
gm1 (f, e, a, l) = log
(
t( fj|ei)) ? log(t( fj|e0)
)
(B.2)
where fj and ei are linked by l and e0 is the empty cept to which all unaligned source
words are ?aligned.?
Based on a similar generative story to Model 1, Model 2 replaces the uniform
alignment probability distribution with an alignment sub-model a(i| j, I, J). This sub-
model assumes that the position of ei depends on the position of its translation fj and
sentence lengths I and J.
The feature function for Model 2 is
hm2 (f, e, a) = log
(
( J|I)
J
?
j=1
t( fj|eaj )a(aj| j, I, J)
)
(B.3)
The corresponding feature gain is
gm2 (f, e, a, l) = log(t( fj|ei)) ? log(t( fj|e0)) +
log(a(i| j, I, J)) ? log(a(0| j, I, J)) (B.4)
where fj and ei are linked by l and 0 is the index of the empty cept e0.
Model 3 is a fertility-based model that parameterizes fertility of words. Unlike
Model 2, Model 3 uses a fertility sub-model n(?i|ei) and a distortion sub-model d( j|i, I, J).
Formally, the feature function of Model 3 is given by
hm3 (a, f, e) = log
(
n0
(
?0|
I
?
i=1
?i
)
I
?
i=1
n(?i|ei)?i!
J
?
j=1
t( fj|eaj )
?
?
j:aj =0
d( j|i, I, J)
)
(B.5)
Brown et al (1993) treat n0(?0|
?I
i=1 ?i), the fertility probability of e0, in a differ-
ent way. They assume that at most half of the source words in an alignment are not
335
Computational Linguistics Volume 36, Number 3
aligned (i.e., ?0 ? J2 ) and define a binomial distribution relying on an auxiliary parame-
ter p0:
n0
(
?0|
I
?
i=1
?i
)
=
{
(J??0
?0
)
pJ?2?00 (1 ? p0)?0 if ?0 ?
J
2
0 otherwise
(B.6)
Note that we follow Brown et al (1993) in replacing
?I
i=1 ?i with J ? ?0 for simplicity.
The original form should be (
?I
i=1?i
?0
)
p
?I
i=1?i??0
0 (1 ? p0 )?0 .
However, this assumption results in a problem for our search algorithm that begins
with an empty alignment (see Algorithm 1), for which ?0 is J and the feature value
hm3 (f, e, a) is negative infinity. To alleviate this problem, we modify Equation B.6 slightly
by adding a smoothing parameter pn ? (0, 1):
n0
(
?0|
I
?
i=1
?i
)
=
{
(J??0
?0
)
pJ?2?00 (1 ? p0)?0pn if ?0 ?
J
2
1?pn
 J2 
otherwise
(B.7)
Therefore, the feature gain for Model 3 is
gm3 (f, e, a, l) = log(gn0 ( J, ?0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) +
log(?i + 1) +
log(t( fj|ei)) ? log(t( fj|e0)) +
log(d( j|i, I, J)) (B.8)
where fj and ei are linked by l, ?i is the fertility before adding l, and gn0 ( J, ?0) is the gain
for n0(?0|
?I
i=1 ?i):
gn0 ( J, ?0) =
?
?
?
?
?
?
?
?0?( J??0+1)
( J?2?0+1)?( J?2?0+2) if ?0 ?
J
2
1?pn
(J??0+1?0?1 )?p
J?2?0+2
0 ?(1?p0 )?0?1?pn?
J
2 
J
2 < ?0 ?
J
2 + 1
1 otherwise
(B.9)
Model 4 defines a new distortion sub-model D(a) that relies on word classes A and
B to capture movement of phrases. The feature function for Model 4 is
hm4 (a, f, e) = log
(
n0(?0|
I
?
i=1
?i)
I
?
i=1
n(?i|ei)
J
?
j=1
t( fj|eaj )
1
?0!
D(a)
)
(B.10)
where
D(a) =
I
?
i=1
?i
?
k=1
pik(?ik) (B.11)
336
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
pik( j) =
{
d1( j ? c?i |A(e?i ),B(?i1)) if k = 1
d>1( j? ?i,k?1|B(?ik)) otherwise
(B.12)
Brown et al (1993) propose two distortion models for Model 4: d1(?) for the first
word of a tablet ? and d>1(?) for the other words of the tablet. In Equation B.12, ?i is
the first position to the left of i for which ?i > 0, c?i is the ceiling of the average position
of the words in ??, ?ik denotes the k-th French word aligned to ek, ?i,k?1 denotes the
position of the k ? 1-th French word aligned to ei, and A(?) and B(?) are word classes
for the source and target languages, respectively. Please refer to Brown et al (1993) for
more details.
The corresponding feature gain is
gm4 (f, e, a, l) = log(gn0 ( J, ?0)) +
log(n(?i + 1|ei)) ? log(n(?i|ei)) +
log(t( fj|ei)) ? log(t( fj|e0)) +
log(?0) +
log(D(a ? {l})) ? log(D(a)) (B.13)
where fj and ei are linked by l and ?i is the fertility before adding l.
In Model 4, the addition of a single link might change the distortion probabilities
pik( j) of other links. As a result, we have to compute the overall distortion probabilities
D(a) every time.
Acknowledgments
This work was supported by National
Natural Science Foundation of China,
Contract No. 60603095 and 60573188.
Thanks to the three anonymous reviewers
for their insightful and constructive
comments and suggestions. We are
grateful to Rada Mihalcea for giving us
the Romanian?English training data and
David Chiang for allowing us to use Hiero.
Stephan Vogel, Vamshi Ambati, and
Kelly Widmaier offered valuable feedback
on an earlier version of this article.
References
Ayan, Necip Fazil and Bonnie J. Dorr. 2006a.
Going beyond AER: An extensive analysis
of word alignments and their impact on
MT. In Proceedings of COLING-ACL 2006,
pages 9?16, Sydney.
Ayan, Necip Fazil and Bonnie J. Dorr. 2006b.
A maximum entropy approach to
combining word alignments. In Proceedings
of HLT-NAACL 2006, pages 96?103, New
York, NY.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005a. Alignment link
projection using transformation-based
learning. In Proceedings of HLT-EMNLP
2005, pages 185?192, Vancouver.
Ayan, Necip Fazil, Bonnie J. Dorr, and
Christof Monz. 2005b. Neuralign:
Combining word alignments using neural
networks. In Proceedings of HLT-EMNLP
2005, pages 65?72, Vancouver.
Blunsom, Phil and Trevor Cohn. 2006.
Discriminative word alignment with
conditional random fields. In Proceedings
of COLING-ACL 2006, pages 65?72,
Sydney.
Brown, Peter F., Stephen A. Della Pietra,
Vincent J. Della Pietra, and Robert L.
Mercer. 1993. The mathematics of
statistical machine translation: Parameter
estimation. Computational Linguistics,
19(2):263?311.
Cherry, Colin and Dekang Lin. 2003. A
probability model to improve word
alignment. In Proceedings of ACL 2003,
pages 88?95, Sapporo.
Cherry, Colin and Dekang Lin. 2006. Soft
syntactic constraints for word alignment
through discriminative training. In
Proceedings of COLING-ACL 2006 (poster),
pages 105?112, Sydney.
Chiang, David. 2005. A hierarchical
phrase-based model for statistical machine
337
Computational Linguistics Volume 36, Number 3
translation. In Proceedings of ACL 2005,
pages 263?270, Ann Arbor, MI.
Chiang, David. 2007. Hierarchical
phrase-based translation. Computational
Linguistics, 33(2):201?228.
Cromiere`s, Fabien and Sadao Kurohashi.
2009. An alignment algorithm using belief
propagation and a structure-based
distortion model. In Proceedings of EACL
2009, pages 166?174, Athens.
Fraser, Alexander and Daniel Marcu. 2006.
Semi-supervised training for statistical
word alignment. In Proceedings of
COLING-ACL 2006, pages 769?776,
Sydney.
Fraser, Alexander and Daniel Marcu. 2007a.
Getting the structure right for word
alignment: LEAF. In Proceedings of
EMNLP-CoNLL 2007, pages 51?60, Prague.
Fraser, Alexander and Daniel Marcu. 2007b.
Measuring word alignment quality for
statistical machine translation.
Computational Linguistics, 33(3):293?303.
Galley, Michel, Jonathan Graehl, Kevin
Knight, Daniel Marcu, Steve DeNeefe, Wei
Wang, and Ignacio Thayer. 2006. Scalable
inference and training of context-rich
syntactic translation models. In Proceedings
of COLING-ACL 2006, pages 961?968,
Sydney.
Haghighi, Aria, John Blitzer, John DeNero,
and Dan Klein. 2009. Better word
alignments with supervised ITG models.
In Proceedings of ACL-IJCNLP 2009,
pages 923?931, Suntec.
He, Xiaodong, Mei Yang, Jianfeng Gao,
Patrick Nguyen, and Robert Moore. 2008.
Indirect-HMM-based hypothesis
alignment for combining outputs from
machine translation systems. In
Proceedings of EMNLP 2008, pages 98?107,
Honolulu, HI.
Koehn, Philipp and Hieu Hoang. 2007.
Factored translation models. In Proceedings
of EMNLP-CoNLL 2007, pages 868?876,
Prague.
Koehn, Philipp, Franz J. Och, and Daniel
Marcu. 2003. Statistical phrase-based
translation. In Proceedings of HLT-NAACL
2003, pages 127?133, Edmonton.
Lacoste-Julien, Simon, Ben Taskar, Dan Klein,
and Michael I. Jordan. 2006. Word
alignment via quadratic assignment.
In Proceedings of HLT-NAACL 2007,
pages 112?119, New York, NY.
Liang, Percy, Ben Taskar, and Dan Klein.
2006. Alignment by agreement. In
Proceedings of HLT-NAACL 2006,
pages 104?111, New York, NY.
Liu, Yang, Qun Liu, and Shouxun Lin. 2005.
Log-linear models for word alignment. In
Proceedings of ACL 2005, pages 459?466,
Ann Arbor, MI.
Liu, Yang, Qun Liu, and Shouxun Lin. 2006.
Tree-to-string alignment template for
statistical machine translation. In
Proceedings of COLING-ACL 2006,
pages 609?616, Sydney.
Marcu, Daniel, Wei Wang, Abdessamad
Echihabi, and Kevin Knight. 2006. SPMT:
Statistical machine translation with
syntactified target language phrases. In
Proceedings of EMNLP 2006, pages 44?52,
Sydney.
Martin, Joel, Rada Mihalcea, and Ted
Pedersen. 2005. Word alignment for
languages with scarce resources. In
Proceedings of the ACL 2005 Workshop on
Building and Using Parallel Texts,
pages 65?74, Ann Arbor, MI.
Melamed, I. Dan. 1998. Annotation style
guide for the blinker project. Technical
report No. 98-06, University of
Pennsylvania, Philadelphia.
Melamed, I. Dan. 2000. Models for
translational equivalence among words.
Computational Linguistics, 26(2):221?249.
Mihalcea, Rada and Ted Pedersen. 2003.
An evaluation exercise for word
alignment. In Proceedings of HLT-NAACL
2003 Workshop on Building and Using
Parallel Texts, pages 1?10, Edmonton.
Moore, Robert C. 2005. A discriminative
framework for bilingual word alignment.
In Proceedings of HLT-EMNLP 2005,
pages 81?88, Vancouver.
Moore, Robert C., Wen-tau Yih, and Andreas
Bode. 2006. Improved discriminative
bilingual word alignment. In Proceedings
of COLING-ACL 2006, pages 513?520,
Sydney.
Niehues, Jan and Stephan Vogel. 2008.
Discriminative word alignment via
alignment matrix modeling. In
Proceedings of the Third Workshop on
Statistical Machine Translation, pages 18?25,
Columbus, OH.
Och, Franz J. 2003. Minimum error rate
training in statistical machine translation.
In Proceedings of ACL 2003, pages 160?167,
Sapporo.
Och, Franz J. and Hermann Ney. 2002.
Discriminative training and maximum
entropy models for statistical machine
translation. In Proceedings of ACL 2002,
pages 295?302, Philadephia, PA.
Och, Franz J. and Hermann Ney. 2003. A
systematic comparison of various
338
Liu, Liu, and Lin Discriminative Word Alignment by Linear Modeling
statistical alignment models.
Computational Linguistics, 29(1):19?51.
Och, Franz J. and Hermann Ney. 2004.
The alignment template approach
to statistical machine translation.
Computational Linguistics,
30(4):417?449.
Quirk, Chris, Arul Menezes, and Colin
Cherry. 2005. Dependency treelet
translation: Syntactically informed
phrasal SMT. In Proceedings of ACL 2005,
pages 271?279, Ann Arbor, MI.
Rosti, Antti-Veikko, Spyros Matsoukas, and
Richard Schwartz. 2007. Improved
word-level system combination for
machine translation. In Proceedings of ACL
2007, pages 312?319, Prague.
Stolcke, Andreas. 2002. SRILM?an
extensible language modeling toolkit. In
Proceedings of ICSLP 2002, pages 901?904,
Denver, CO.
Taskar, Ben, Simon Lacoste-Julien, and Dan
Klein. 2005. A discriminative matching
approach to word alignment. In
Proceedings of HLT-EMNLP 2005,
pages 73?80, Vancouver.
Vogel, Stephan and Hermann Ney. 1996.
HMM-based word alignment in statistical
translation. In Proceedings of COLING 1996,
pages 836?841, Copenhagen.
339

Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 750?758,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
A Topic Similarity Model
for Hierarchical Phrase-based Translation
Xinyan Xiao? Deyi Xiong? Min Zhang?? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Human Language Technology
Institute of Computing Technology Institute for Infocomm Research
Chinese Academy of Sciences
{xiaoxinyan, liuqun, sxlin}@ict.ac.cn {dyxiong, mzhang?}@i2r.a-star.edu.sg
Abstract
Previous work using topic model for statis-
tical machine translation (SMT) explore top-
ic information at the word level. Howev-
er, SMT has been advanced from word-based
paradigm to phrase/rule-based paradigm. We
therefore propose a topic similarity model to
exploit topic information at the synchronous
rule level for hierarchical phrase-based trans-
lation. We associate each synchronous rule
with a topic distribution, and select desirable
rules according to the similarity of their top-
ic distributions with given documents. We
show that our model significantly improves
the translation performance over the baseline
on NIST Chinese-to-English translation ex-
periments. Our model also achieves a better
performance and a faster speed than previous
approaches that work at the word level.
1 Introduction
Topic model (Hofmann, 1999; Blei et al, 2003) is
a popular technique for discovering the underlying
topic structure of documents. To exploit topic infor-
mation for statistical machine translation (SMT), re-
searchers have proposed various topic-specific lexi-
con translation models (Zhao and Xing, 2006; Zhao
and Xing, 2007; Tam et al, 2007) to improve trans-
lation quality.
Topic-specific lexicon translation models focus
on word-level translations. Such models first esti-
mate word translation probabilities conditioned on
topics, and then adapt lexical weights of phrases
?Corresponding author
by these probabilities. However, the state-of-the-
art SMT systems translate sentences by using se-
quences of synchronous rules or phrases, instead of
translating word by word. Since a synchronous rule
is rarely factorized into individual words, we believe
that it is more reasonable to incorporate the topic
model directly at the rule level rather than the word
level.
Consequently, we propose a topic similari-
ty model for hierarchical phrase-based translation
(Chiang, 2007), where each synchronous rule is as-
sociated with a topic distribution. In particular,
? Given a document to be translated, we cal-
culate the topic similarity between a rule and
the document based on their topic distributions.
We augment the hierarchical phrase-based sys-
tem by integrating the proposed topic similarity
model as a new feature (Section 3.1).
? As we will discuss in Section 3.2, the similarity
between a generic rule and a given source docu-
ment computed by our topic similarity model is
often very low. We don?t want to penalize these
generic rules. Therefore we further propose a
topic sensitivity model which rewards generic
rules so as to complement the topic similarity
model.
? We estimate the topic distribution for a rule
based on both the source and target side topic
models (Section 4.1). In order to calculate sim-
ilarities between target-side topic distributions
of rules and source-side topic distributions of
given documents during decoding, we project
750
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(a) ?? ?? ? opera-
tional capability
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(b) ??X1 ? grandsX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(c) ??X1 ? giveX1
 0
 0.2
 0.4
 0.6
1 5 10 15 20 25 30
(d) X1 ?? ?? X2 ?
held talksX1 X2
Figure 1: Four synchronous rules with topic distributions. Each sub-graph shows a rule with its topic distribution,
where the X-axis means topic index and the Y-axis means the topic probability. Notably, the rule (b) and rule (c) shares
the same source Chinese string, but they have different topic distributions due to the different English translations.
the target-side topic distributions of rules into
the space of source-side topic model by one-to-
many projection (Section 4.2).
Experiments on Chinese-English translation tasks
(Section 6) show that, our method outperforms the
baseline hierarchial phrase-based system by +0.9
BLEU points. This result is also +0.5 points high-
er and 3 times faster than the previous topic-specific
lexicon translation method. We further show that
both the source-side and target-side topic distribu-
tions improve translation quality and their improve-
ments are complementary to each other.
2 Background: Topic Model
A topic model is used for discovering the topics
that occur in a collection of documents. Both La-
tent Dirichlet Allocation (LDA) (Blei et al, 2003)
and Probabilistic Latent Semantic Analysis (PLSA)
(Hofmann, 1999) are types of topic models. LDA
is the most common topic model currently in use,
therefore we exploit it for mining topics in this pa-
per. Here, we first give a brief description of LDA.
LDA views each document as a mixture pro-
portion of various topics, and generates each word
by multinomial distribution conditioned on a topic.
More specifically, as a generative process, LDA first
samples a document-topic distribution for each doc-
ument. Then, for each word in the document, it sam-
ples a topic index from the document-topic distribu-
tion and samples the word conditioned on the topic
index according the topic-word distribution.
Generally speaking, LDA contains two types of
parameters. The first one relates to the document-
topic distribution, which records the topic distribu-
tion of each document. The second one is used for
topic-word distribution, which represents each topic
as a distribution over words. Based on these param-
eters (and some hyper-parameters), LDA can infer a
topic assignment for each word in the documents. In
the following sections, we will use these parameters
and the topic assignments of words to estimate the
parameters in our method.
3 Topic Similarity Model
Sentences should be translated in consistence with
their topics (Zhao and Xing, 2006; Zhao and Xing,
2007; Tam et al, 2007). In the hierarchical phrase
based system, a synchronous rule may be related to
some topics and unrelated to others. In terms of
probability, a rule often has an uneven probability
distribution over topics. The probability over a topic
is high if the rule is highly related to the topic, other-
wise the probability will be low. Therefore, we use
topic distribution to describe the relatedness of rules
to topics.
Figure 1 shows four synchronous rules (Chiang,
2007) with topic distributions, some of which con-
tain nonterminals. We can see that, although the
source part of rule (b) and (c) are identical, their top-
ic distributions are quite different. Rule (b) contains
a highest probability on the topic about ?China-U.S.
relationship?, which means rule (b) is much more
related to this topic. In contrast, rule (c) contains
an even distribution over various topics. Thus, giv-
en a document about ?China-U.S. relationship?, we
hope to encourage the system to apply rule (b) but
penalize the application of rule (c). We achieve this
by calculating similarity between the topic distribu-
tions of a rule and a document to be translated.
More formally, we associate each rule with a rule-
topic distribution P (z|r), where r is a rule, and z is
a topic. Suppose there are K topics, this distribution
751
can be represented by a K-dimension vector. The
k-th component P (z = k|r) means the probability
of topic k given the rule r. The estimation of such
distribution will be described in Section 4.
Analogously, we represent the topic information
of a document d to be translated by a document-
topic distribution P (z|d), which is also a K-
dimension vector. The k-th dimension P (z = k|d)
means the probability of topic k given document d.
Different from rule-topic distribution, the document-
topic distribution can be directly inferred by an off-
the-shelf LDA tool.
Consequently, based on these two distribution-
s, we select a rule for a document to be translat-
ed according to their topic similarity (Section 3.1),
which measures the relatedness of the rule to the
document. In order to encourage the application
of generic rules which are often penalized by our
similarity model, we also propose a topic sensitivity
model (Section 3.2).
3.1 Topic Similarity
By comparing the similarity of their topic distribu-
tions, we are able to decide whether a rule is suitable
for a given source document. The topic similarity
computes the distance of two topic distributions. We
calculate the topic similarity by Hellinger function:
Similarity(P (z|d), P (z|r))
=
K
?
k=1
(
?
P (z = k|d) ?
?
P (z = k|r)
)2
(1)
Hellinger function is used to calculate distribution
distance and is popular in topic model (Blei and Laf-
ferty, 2007).1 By topic similarity, we aim to encour-
age or penalize the application of a rule for a giv-
en document according to their topic distributions,
which then helps the SMT system make better trans-
lation decisions.
3.2 Topic Sensitivity
Domain adaptation (Wu et al, 2008; Bertoldi and
Federico, 2009) often distinguishes general-domain
data from in-domain data. Similarly, we divide the
rules into topic-insensitive rules and topic-sensitive
1We also try other distance functions, including Euclidean
distance, Kullback-Leibler divergence and cosine function.
They produce similar results in our preliminary experiments.
rules according to their topic distributions. Let?s
revisit Figure 1. We can easily find that the topic
distribution of rule (c) distribute evenly. This in-
dicates that it is insensitive to topics, and can be
applied in any topics. We call such a rule a topic-
insensitive rule. In contrast, the distributions of the
rest rules peak on a few topics. Such rules are called
topic-sensitive rules. Generally speaking, a topic-
insensitive rule has a fairly flat distribution, while a
topic-sensitive rule has a sharp distribution.
A document typically focuses on a few topics, and
has a sharp topic distribution. In contrast, the distri-
bution of topic-insensitive rule is fairly flat. Hence,
a topic-insensitive rule is always less similar to doc-
uments and is punished by the similarity function.
However, topic-insensitive rules may be more
preferable than topic-sensitive rules if neither of
them are similar to given documents. For a doc-
ument about the ?military? topic, the rule (b) and
(c) in Figure 1 are both dissimilar to the document,
because rule (b) relates to the ?China-U.S. relation-
ship? topic and rule (c) is topic-insensitive. Never-
theless, since rule (c) occurs more frequently across
various topics, it may be better to apply rule (c).
To address such issue of the topic similarity mod-
el, we further introduce a topic sensitivity model to
describe the topic sensitivity of a rule using entropy
as a metric:
Sensitivity(P (z|r))
= ?
K
?
k=1
P (z = k|r) ? log (P (z = k|r)) (2)
According to the Eq. (2), a topic-insensitive rule has
a large entropy, while a topic-sensitive rule has a s-
maller entropy. By incorporating the topic sensitivi-
ty model with the topic similarity model, we enable
our SMT system to balance the selection of these t-
wo types of rules. Given rules with approximately
equal values of Eq. (1), we prefer topic-insensitive
rules.
4 Estimation
Unlike document-topic distribution that can be di-
rectly learned by LDA tools, we need to estimate the
rule-topic distribution according to our requirement.
In this paper, we try to exploit the topic information
752
of both source and target language. To achieve this
goal, we use both source-side and target-side mono-
lingual topic models, and learn the correspondence
between the two topic models from word-aligned
bilingual corpus.
Specifically, we use two types of rule-topic dis-
tributions: one is source-side rule-topic distribution
and the other is target-side rule-topic distribution.
These two rule-topic distributions are estimated by
corresponding topic models in the same way (Sec-
tion 4.1). Notably, only source language documents
are available during decoding. In order to compute
the similarity between the target-side topic distribu-
tion of a rule and the source-side topic distribution
of a given document?we need to project the target-
side topic distribution of a synchronous rule into the
space of the source-side topic model (Section 4.2).
A more principle way is to learn a bilingual topic
model from bilingual corpus (Mimno et al, 2009).
However, we may face difficulty during decoding,
where only source language documents are avail-
able. It requires a marginalization to infer the mono-
lingual topic distribution using the bilingual topic
model. The high complexity of marginalization pro-
hibits such a summation in practice. Previous work
on bilingual topic model avoid this problem by some
monolingual assumptions. Zhao and Xing (2007)
assume that the topic model is generated in a mono-
lingual manner, while Tam et al, (2007) construct
their bilingual topic model by enforcing a one-to-
one correspondence between two monolingual topic
models. We also estimate our rule-topic distribution
by two monolingual topic models, but use a differ-
ent way to project target-side topics onto source-side
topics.
4.1 Monolingual Topic Distribution Estimation
We estimate rule-topic distribution from word-
aligned bilingual training corpus with documen-
t boundaries explicitly given. The source and tar-
get side distributions are estimated in the same way.
For simplicity, we only describe the estimation of
source-side distribution in this section.
The process of rule-topic distribution estimation
is analogous to the traditional estimation of rule
translation probability (Chiang, 2007). In addition
to the word-aligned corpus, the input for estimation
also contains the source-side topic-document distri-
bution of every documents inferred by LDA tool.
We first extract synchronous rules from training
data in a traditional way. When a rule r is extracted
from a document d with topic distribution P (z|d),
we collect an instance (r, P (z|d), c), where c is the
fraction count of an instance as described in Chiang,
(2007). After extraction, we get a set of instances
I = {(r, P (z|d), c)} with different document-topic
distributions for each rule. Using these instances,
we calculate the topic probability P (z = k|r) as
follows:
P (z = k|r) =
?
I?I c? P (z = k|d)
?K
k?=1
?
I?I c? P (z = k?|d)
(3)
By using both source-side and target-side
document-topic distribution, we obtain two rule-
topic distributions for each rule in total.
4.2 Target-side Topic Distribution Projection
As described in the previous section, we also esti-
mate the target-side rule-topic distribution. How-
ever, only source document-topic distributions are
available during decoding. In order to calculate
the similarity between the target-side rule-topic dis-
tribution of a rule and the source-side document-
topic distribution of a source document, we need to
project target-side topics into the source-side topic
space. The projection contains two steps:
? In the first step, we learn the topic-to-topic cor-
respondence probability p(zf |ze) from target-
side topic ze to source-side topic zf .
? In the second step, we project the target-side
topic distribution of a rule into source-side top-
ic space using the correspondence probability.
In the first step, we estimate the correspondence
probability by the co-occurrence of the source-side
and the target-side topic assignment of the word-
aligned corpus. The topic assignments are output
by LDA tool. Thus, we denotes each sentence pair
by (zf , ze,a), where zf and ze are the topic as-
signments of source-side and target-side sentences
respectively, and a is a set of links {(i, j)}. A
link (i, j) means a source-side position i aligns to
a target-side position j. Thus, the co-occurrence of
a source-side topic with index kf and a target-side
753
e-topic f-topic 1 f-topic 2 f-topic 3
enterprises ??(agricultural) ??(enterprise) ??(develop)
rural ??(rural) ??(market) ??(economic)
state ??(peasant) ??(state) ??(technology )
agricultural ??(reform) ??(company) ??(China)
market ??(finance) ??(finance) ??(technique)
reform ??(social) ??(bank) ??(industry)
production ??(safety) ??(investment) ??(structure)
peasants ??(adjust) ??(manage) ??(innovation)
owned ??(policy) ??(reform) ??(accelerate)
enterprise ??(income) ??(operation) ??(reform)
p(zf |ze) 0.38 0.28 0.16
Table 1: Example of topic-to-topic correspondence. The
last line shows the correspondence probability. Each col-
umnmeans a topic represented by its top-10 topical word-
s. The first column is a target-side topic, while the rest
three columns are source-side topics.
topic ke is calculated by:
?
(zf ,ze,a)
?
(i,j)?a
?(zfi , kf ) ? ?(zej , ke) (4)
where ?(x, y) is the Kronecker function, which is 1
if x = y and 0 otherwise. We then compute the
probability of P (z = kf |z = ke) by normalizing
the co-occurrence count. Overall, after the first step,
we obtain an correspondence matrix MKe?Kf from
target-side topic to source-side topic, where the item
Mi,j represents the probability P (zf = i|ze = j).
In the second step, given the correspondence ma-
trix MKe?Kf , we project the target-side rule-topic
distribution P (ze|r) to the source-side topic space
by multiplication as follows:
T (P (ze|r)) = P (ze|r) ?MKe?Kf (5)
In this way, we get a second distribution for a rule
in the source-side topic space, which we called pro-
jected target-side topic distribution T (P (ze|r)).
Obviously, our projection method allows one
target-side topic to align to multiple source-side top-
ics. This is different from the one-to-one correspon-
dence used by Tam et al, (2007). From the training
result of the correspondence matrix MKe?Kf , we
find that the topic correspondence between source
and target language is not necessarily one-to-one.
Typically, the probability P (z = kf |z = ke) of a
target-side topic mainly distributes on two or three
source-side topics. Table 1 shows an example of
a target-side topic with its three mainly aligned
source-side topics.
5 Decoding
We incorporate our topic similarity model as a
new feature into a traditional hiero system (Chi-
ang, 2007) under discriminative framework (Och
and Ney, 2002). Considering there are a source-
side rule-topic distribution and a projected target-
side rule-topic distribution, we add four features in
total:
? Similarity (P (zf |d), P (zf |r))
? Similarity(P (zf |d), T (P (ze|r)))
? Sensitivity(P (zf |r))
? Sensitivity(T (P (ze|r))
To calculate the total score of a derivation on each
feature listed above during decoding, we sum up the
correspondent feature score of each applied rule.2
The source-side and projected target-side rule-
topic distribution are calculated before decoding.
During decoding, we first infer the topic distribution
P (zf |d) for a given document on source language.
When applying a rule, it is straightforward to calcu-
late these topic features. Obviously, the computa-
tional cost of these features is rather small.
In the topic-specific lexicon translation model,
given a source document, it first calculates the topic-
specific translation probability by normalizing the
entire lexicon translation table, and then adapts the
lexical weights of rules correspondingly. This makes
the decoding slower. Therefore, comparing with the
previous topic-specific lexicon translation method,
our method provides a more efficient way for incor-
porating topic model into SMT.
6 Experiments
We try to answer the following questions by experi-
ments:
1. Is our topic similarity model able to improve
translation quality in terms of BLEU? Further-
more, are source-side and target-side rule-topic
distributions complementary to each other?
2Since glue rule and rules of unknown words are not extract-
ed from training data, here, we just ignore the calculation of the
four features for them.
754
System MT06 MT08 Avg Speed
Baseline 30.20 21.93 26.07 12.6
TopicLex 30.65 22.29 26.47 3.3
SimSrc 30.41 22.69 26.55 11.5
SimTgt 30.51 22.39 26.45 11.7
SimSrc+SimTgt 30.73 22.69 26.71 11.2
Sim+Sen 30.95 22.92 26.94 10.2
Table 2: Result of our topic similarity model in terms of BLEU and speed (words per second), comparing with the
traditional hierarchical system (?Baseline?) and the topic-specific lexicon translation method (?TopicLex?). ?SimSrc?
and ?SimTgt? denote similarity by source-side and target-side rule-distribution respectively, while ?Sim+Sen? acti-
vates the two similarity and two sensitivity features. ?Avg? is the average BLEU score on the two test sets. Scores
marked in bold mean significantly (Koehn, 2004) better than Baseline (p < 0.01).
2. Is it helpful to introduce the topic sensitivi-
ty model to distinguish topic-insensitive and
topic-sensitive rules?
3. Is it necessary to project topics by one-to-many
correspondence instead of one-to-one corre-
spondence?
4. What is the effect of our method on various
types of rules, such as phrase rules and rules
with non-terminals?
6.1 Data
We present our experiments on the NIST Chinese-
English translation tasks. The bilingual training da-
ta contains 239K sentence pairs with 6.9M Chinese
words and 9.14M English words, which comes from
the FBIS portion of LDC data. There are 10,947
documents in the FBIS corpus. The monolingual da-
ta for training English language model includes the
Xinhua portion of the GIGAWORD corpus, which
contains 238M English words. We used the NIST
evaluation set of 2005 (MT05) as our development
set, and sets of MT06/MT08 as test sets. The num-
bers of documents in MT05, MT06, MT08 are 100,
79, and 109 respectively.
We obtained symmetric word alignments of train-
ing data by first running GIZA++ (Och and Ney,
2003) in both directions and then applying re-
finement rule ?grow-diag-final-and? (Koehn et al,
2003). The SCFG rules are extracted from this
word-aligned training data. A 4-gram language
model was trained on the monolingual data by the
SRILM toolkit (Stolcke, 2002). Case-insensitive
NIST BLEU (Papineni et al, 2002) was used to mea-
sure translation performance. We used minimum er-
ror rate training (Och, 2003) for optimizing the fea-
ture weights.
For the topic model, we used the open source L-
DA tool GibbsLDA++ for estimation and inference.3
GibssLDA++ is an implementation of LDA using
gibbs sampling for parameter estimation and infer-
ence. The source-side and target-side topic models
are estimated from the Chinese part and English part
of FBIS corpus respectively. We set the number of
topic K = 30 for both source-side and target-side,
and use the default setting of the tool for training and
inference.4 During decoding, we first infer the top-
ic distribution of given documents before translation
according to the topic model trained on Chinese part
of FBIS corpus.
6.2 Effect of Topic Similarity Model
We compare our method with two baselines. In addi-
tion to the traditional hiero system, we also compare
with the topic-specific lexicon translation method in
Zhao and Xing (2007). The lexicon translation prob-
ability is adapted by:
p(f |e,DF ) ? p(e|f,DF )P (f |DF ) (6)
=
?
k
p(e|f, z = k)p(f |z = k)p(z = k|DF ) (7)
However, we simplify the estimation of p(e|f, z =
k) by directly using the word alignment corpus with
3http://gibbslda.sourceforge.net/
4We determine K by testing {15, 30, 50, 100, 200} in our
preliminary experiments. We find that K = 30 produces a s-
lightly better performance than other values.
755
Type Count Src% Tgt%
Phrase-rule 3.9M 83.4 84.4
Monotone-rule 19.2M 85.3 86.1
Reordering-rule 5.7M 85.9 86.8
All-rule 28.8M 85.1 86.0
Table 3: Percentage of topic-sensitive rules of various
types of rule according to source-side (?Src?) and target-
side (?Tgt?) topic distributions. Phrase rules are fully
lexicalized, while monotone and reordering rules contain
nonterminals (Section 6.5).
topic assignment that is inferred by the GibbsL-
DA++. Despite the simplification of estimation, the
improvement of our implementation is comparable
with the improvement in Zhao et al,(2007). Given a
new document, we need to adapt the lexical transla-
tion weights of the rules based on topic model. The
adapted lexicon translation model is added as a new
feature under the discriminative framework.
Table 2 shows the result of our method compar-
ing with the traditional system and the topic-lexicon
specific translation method described as above. By
using all the features (last line in the table), we im-
prove the translation performance over the baseline
system by 0.87 BLEU point on average. Our method
also outperforms the topic-lexicon specific transla-
tion method by 0.47 points. This verifies that topic
similarity model can improve the translation quality
significantly.
In order to gain insights into why our model is
helpful, we further investigate how many rules are
topic-sensitive. As described in Section 3.2, we use
entropy to measure the topic sensitivity. If the en-
tropy of a rule is smaller than a certain threshold,
then the rule is topic sensitive. Since documents of-
ten focus on some topics, we use the average entropy
of document-topic distribution of all training docu-
ments as the threshold. We compare both source-
side and target-side distribution shown in Table 3.
We find that more than 80 percents of the rules are
topic-sensitive, thus provides us a large space to im-
prove the translation by exploiting topics.
We also compare these methods in terms of the
decoding speed (words/second). The baseline trans-
lates 12.6 words per second, while the topic-specific
lexicon translation method only translates 3.3 word-
s in one second. The overhead of the topic-specific
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
One-to-One 30.27 22.12 26.20
One-to-Many 30.51 22.39 26.45
Table 4: Effects of one-to-one and one-to-many topic pro-
jection.
lexicon translation method mainly comes from the
adaptation of lexical weights. It takes 72.8% of
the time to do the adaptation, despite only lexical
weights of the used rules are adapted. In contrast,
our method has a speed of 10.2 words per second for
each sentence on average, which is three times faster
than the topic-specific lexicon translation method.
Meanwhile, we try to separate the effects of
source-side topic distribution from the target-side
topic distribution. From lines 4-6 of Table 2. We
clearly find that the two rule-topic distributions im-
prove the performance by 0.48 and 0.38 BLEU
points over the baseline respectively. It seems that
the source-side topic model is more helpful. Fur-
thermore, when combine these two distributions, the
improvement is increased to 0.64 points. This indi-
cates that the effects of source-side and target-side
distributions are complementary.
6.3 Effect of Topic Sensitivity Model
As described in Section 3.2, because the similari-
ty features always punish topic-insensitive rules, we
introduce topic sensitivity features as a complemen-
t. In the last line of Table 2, we obtain a fur-
ther improvement of 0.23 points, when incorporat-
ing topic sensitivity features with topic similarity
features. This suggests that it is necessary to dis-
tinguish topic-insensitive and topic-sensitive rules.
6.4 One-to-One Vs. One-to-Many Topic
Projection
In Section 4.2, we find that source-side topic and
target-side topics may not exactly match, hence we
use one-to-many topic correspondence. Yet anoth-
er method is to enforce one-to-one topic projection
(Tam et al, 2007). We achieve one-to-one projection
by aligning a target topic to the source topic with the
largest correspondence probability as calculated in
Section 4.2.
Table 4 compares the effects of these two method-
756
System MT06 MT08 Avg
Baseline 30.20 21.93 26.07
Phrase-rule 30.53 22.29 26.41
Monotone-rule 30.72 22.62 26.67
Reordering-rule 30.31 22.40 26.36
All-rule 30.95 22.92 26.94
Table 5: Effect of our topic model on three types of rules.
Phrase rules are fully lexicalized, while monotone and
reordering rules contain nonterminals.
s. We find that the enforced one-to-one topic method
obtains a slight improvement over the baseline sys-
tem, while one-to-many projection achieves a larger
improvement. This confirms our observation of the
non-one-to-one mapping between source-side and
target-side topics.
6.5 Effect on Various Types of Rules
To get a more detailed analysis of the result, we
further compare the effect of our method on differ-
ent types of rules. We divide the rules into three
types: phrase rules, which only contain terminal-
s and are the same as the phrase pairs in phrase-
based system; monotone rules, which contain non-
terminals and produce monotone translations; re-
ordering rules, which also contain non-terminals but
change the order of translations. We define the
monotone and reordering rules according to Chiang
et al, (2008).
Table 5 show the results. We can see that our
method achieves improvements on all the three type-
s of rules. Our topic similarity method on mono-
tone rule achieves the most improvement which is
0.6 BLEU points, while the improvement on reorder-
ing rules is the smallest among the three types. This
shows that topic information also helps the selec-
tions of rules with non-terminals.
7 Related Work
In addition to the topic-specific lexicon transla-
tion method mentioned in the previous sections,
researchers also explore topic model for machine
translation in other ways.
Foster and Kunh (2007) describe a mixture-model
approach for SMT adaptation. They first split a
training corpus into different domains. Then, they
train separate models on each domain. Finally, they
combine a specific domain translation model with a
general domain translation model depending on var-
ious text distances. One way to calculate the dis-
tance is using topic model.
Gong et al (2010) introduce topic model for fil-
tering topic-mismatched phrase pairs. They first as-
sign a specific topic for the document to be translat-
ed. Similarly, each phrase pair is also assigned with
one specific topic. A phrase pair will be discarded if
its topic mismatches the document topic.
Researchers also introduce topic model for cross-
lingual language model adaptation (Tam et al, 2007;
Ruiz and Federico, 2011). They use bilingual topic
model to project latent topic distribution across lan-
guages. Based on the bilingual topic model, they ap-
ply the source-side topic weights into the target-side
topic model, and adapt the n-gram language model
of target side.
Our topic similarity model uses the document top-
ic information. From this point, our work is related
to context-dependent translation (Carpuat and Wu,
2007; He et al, 2008; Shen et al, 2009). Previous
work typically use neighboring words and sentence
level information, while our work extents the con-
text into the document level.
8 Conclusion and Future Work
We have presented a topic similarity model which
incorporates the rule-topic distributions on both the
source and target side into traditional hierarchical
phrase-based system. Our experimental results show
that our model achieves a better performance with
faster decoding speed than previous work on topic-
specific lexicon translation. This verifies the advan-
tage of exploiting topic model at the rule level over
the word level. Further improvement is achieved by
distinguishing topic-sensitive and topic-insensitive
rules using the topic sensitivity model.
In the future, we are interesting to find ways to
exploit topic model on bilingual data without docu-
ment boundaries, thus to enlarge the size of training
data. Furthermore, our training corpus mainly focus
on news, it is also interesting to apply our method on
corpus with more diverse topics. Finally, we hope to
apply our method to other translation models, espe-
cially syntax-based models.
757
Acknowledgement
The authors were supported by High-Technology
R&D Program (863) Project No 2011AA01A207
and 2012BAH39B03. This work was done dur-
ing Xinyan Xiao?s internship at I2R. We would like
to thank Yun Huang, Zhengxian Gong, Wenliang
Chen, Jun lang, Xiangyu Duan, Jun Sun, Jinsong
Su and the anonymous reviewers for their insightful
comments.
References
Nicola Bertoldi and Marcello Federico. 2009. Do-
main adaptation for statistical machine translation with
monolingual resources. In Proc of WMT 2009.
David M. Blei and John D. Lafferty. 2007. A correlated
topic model of science. AAS, 1(1):17?35.
David M. Blei, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. JMLR, 3:993?1022.
Marine Carpuat and Dekai Wu. 2007. Context-
dependent phrasal translation lexicons for statistical
machine translation. In Proceedings of the MT Sum-
mit XI.
David Chiang, Yuval Marton, and Philip Resnik. 2008.
Online large-margin training of syntactic and struc-
tural translation features. In Proc. EMNLP 2008.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
George Foster and Roland Kuhn. 2007. Mixture-model
adaptation for SMT. In Proc. of the Second Work-
shop on Statistical Machine Translation, pages 128?
135, Prague, Czech Republic, June.
Zhengxian Gong, Yu Zhang, and Guodong Zhou. 2010.
Statistical machine translation based on lda. In Proc.
IUCS 2010, page 286?290, Oct.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proc. EMNLP 2008.
Thomas Hofmann. 1999. Probabilistic latent semantic
analysis. In Proc. of UAI 1999, pages 289?296.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
HLT-NAACL 2003.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proc. EMNLP
2004.
David Mimno, Hanna M. Wallach, Jason Naradowsky,
David A. Smith, and Andrew McCallum. 2009.
Polylingual topic models. In Proc. of EMNLP 2009.
Franz J. Och and Hermann Ney. 2002. Discriminative
training and maximum entropy models for statistical
machine translation. In Proc. ACL 2002.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL 2003.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalua-
tion of machine translation. In Proc. ACL 2002.
Nick Ruiz and Marcello Federico. 2011. Topic adapta-
tion for lecture translation through bilingual latent se-
mantic models. In Proceedings of the Sixth Workshop
on Statistical Machine Translation, July.
Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,
and Ralph Weischedel. 2009. Effective use of linguis-
tic and contextual information for statistical machine
translation. In Proc. EMNLP 2009.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proc. ICSLP 2002.
Yik-Cheung Tam, Ian R. Lane, and Tanja Schultz. 2007.
Bilingual lsa-based adaptation for statistical machine
translation. Machine Translation, 21(4):187?207.
Hua Wu, Haifeng Wang, and Chengqing Zong. 2008.
Domain adaptation for statistical machine translation
with domain dictionary and monolingual corpora. In
Proc. Coling 2008.
Bing Zhao and Eric P. Xing. 2006. BiTAM: Bilingual
topic admixture models for word alignment. In Proc.
ACL 2006.
Bin Zhao and Eric P. Xing. 2007. HM-BiTAM: Bilingual
topic exploration, word alignment, and translation. In
Proc. NIPS 2007.
758
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Identifying High-Impact Sub-Structures for Convolution Kernels in
Document-level Sentiment Classification
Zhaopeng Tu? Yifan He?? Jennifer Foster? Josef van Genabith? Qun Liu? Shouxun Lin?
?Key Lab. of Intelligent Info. Processing ?Computer Science Department ?School of Computing
Institute of Computing Technology, CAS New York University Dublin City University
?{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,
?yhe@cs.nyu.edu, ?{jfoster,josef}@computing.dcu.ie
Abstract
Convolution kernels support the modeling of
complex syntactic information in machine-
learning tasks. However, such models are
highly sensitive to the type and size of syntac-
tic structure used. It is therefore an importan-
t challenge to automatically identify high im-
pact sub-structures relevant to a given task. In
this paper we present a systematic study inves-
tigating (combinations of) sequence and con-
volution kernels using different types of sub-
structures in document-level sentiment classi-
fication. We show that minimal sub-structures
extracted from constituency and dependency
trees guided by a polarity lexicon show 1.45
point absolute improvement in accuracy over a
bag-of-words classifier on a widely used sen-
timent corpus.
1 Introduction
An important subtask in sentiment analysis is sen-
timent classification. Sentiment classification in-
volves the identification of positive and negative
opinions from a text segment at various levels of
granularity including document-level, paragraph-
level, sentence-level and phrase-level. This paper
focuses on document-level sentiment classification.
There has been a substantial amount of work
on document-level sentiment classification. In ear-
ly pioneering work, Pang and Lee (2004) use a
flat feature vector (e.g., a bag-of-words) to rep-
resent the documents. A bag-of-words approach,
however, cannot capture important information ob-
tained from structural linguistic analysis of the doc-
uments. More recently, there have been several ap-
proaches which employ features based on deep lin-
guistic analysis with encouraging results including
Joshi and Penstein-Rose (2009) and Liu and Senef-
f (2009). However, as they select features manually,
these methods would require additional labor when
ported to other languages and domains.
In this paper, we study and evaluate diverse lin-
guistic structures encoded as convolution kernels for
the document-level sentiment classification prob-
lem, in order to utilize syntactic structures without
defining explicit linguistic rules. While the applica-
tion of kernel methods could seem intuitive for many
tasks, it is non-trivial to apply convolution kernels
to document-level sentiment classification: previous
work has already shown that categorically using the
entire syntactic structure of a single sentence would
produce too many features for a convolution ker-
nel (Zhang et al, 2006; Moschitti et al, 2008). We
expect the situation to be worse for our task as we
work with documents that tend to comprise dozens
of sentences.
It is therefore necessary to choose appropriate
substructures of a sentence as opposed to using the
whole structure in order to effectively use convolu-
tion kernels in our task. It has been observed that
not every part of a document is equally informa-
tive for identifying the polarity of the whole doc-
ument (Yu and Hatzivassiloglou, 2003; Pang and
Lee, 2004; Koppel and Schler, 2005; Ferguson et
al., 2009): a film review often uses lengthy objective
paragraphs to simply describe the plot. Such objec-
tive portions do not contain the author?s opinion and
are irrelevant with respect to the sentiment classifi-
338
cation task. Indeed, separating objective sentences
from subjective sentences in a document produces
encouraging results (Yu and Hatzivassiloglou, 2003;
Pang and Lee, 2004; Koppel and Schler, 2005; Fer-
guson et al, 2009). Our research is inspired by these
observations. Unlike in the previous work, however,
we focus on syntactic substructures (rather than en-
tire paragraphs or sentences) that contain subjective
words.
More specifically, we use the terms in the lexi-
con constructed from (Wilson et al, 2005) as the
indicators to identify the substructures for the con-
volution kernels, and extract different sub-structures
according to these indicators for various types of
parse trees (Section 3). An empirical evaluation on
a widely used sentiment corpus shows an improve-
ment of 1.45 point in accuracy over the baseline
resulting from a combination of bag-of-words and
high-impact parse features (Section 4).
2 Related Work
Our research builds on previous work in the field
of sentiment classification and convolution kernel-
s. For sentiment classification, the design of lexi-
cal and syntactic features is an important first step.
Several approaches propose feature-based learning
algorithms for this problem. Pang and Lee (2004)
and Dave et al (2003) represent a document as a
bag-of-words; Matsumoto et al, (2005) extract fre-
quently occurring connected subtrees from depen-
dency parsing; Joshi and Penstein-Rose (2009) use
a transformation of dependency relation triples; Liu
and Seneff (2009) extract adverb-adjective-noun re-
lations from dependency parser output.
Previous research has convincingly demonstrat-
ed a kernel?s ability to generate large feature set-
s, which is useful to quickly model new and not
well understood linguistic phenomena in machine
learning, and has led to improvements in various
NLP tasks, including relation extraction (Bunescu
and Mooney, 2005a; Bunescu and Mooney, 2005b;
Zhang et al, 2006; Nguyen et al, 2009), question
answering (Moschitti and Quarteroni, 2008), seman-
tic role labeling (Moschitti et al, 2008).
Convolution kernels have been used before in sen-
timent analysis: Wiegand and Klakow (2010) use
convolution kernels for opinion holder extraction,
Johansson and Moschitti (2010) for opinion expres-
sion detection and Agarwal et al (2011) for sen-
timent analysis of Twitter data. Wiegand and K-
lakow (2010) use e.g. noun phrases as possible can-
didate opinion holders, in our work we extract any
minimal syntactic context containing a subjective
word. Johansson and Moschitti (2010) and Agarwal
et al (2011) process sentences and tweets respec-
tively. However, as these are considerably shorter
than documents, their feature space is less complex,
and pruning is not as pertinent.
3 Kernels for Sentiment Classification
3.1 Linguistic Representations
We explore both sequence and convolution kernels
to exploit information on surface and syntactic lev-
els. For sequence kernels, we make use of lexical
words with some syntactic information in the form
of part-of-speech (POS) tags. More specifically, we
define three types of sequences:
? SW, a sequence of lexical words, e.g.: A tragic
waste of talent and incredible visual effects.
? SP, a sequence of POS tags, e.g.: DT JJ NN IN
NN CC JJ JJ NNS.
? SWP, a sequence of words and POS tags,
e.g.: A/DT tragic/JJ waste/NN of/IN talent/NN
and/CC incredible/JJ visual/JJ effects/NNS.
In addition, we experiment with constituency tree
kernels (CON), and dependency tree kernels (D),
which capture hierarchical constituency structure
and labeled dependency relations between words,
respectively. For dependency kernels, we test with
word (DW), POS (DP), and combined word-and-
POS settings (DWP), and similarly for simple se-
quence kernels (SW, SP and SWP). We also use a
vector kernel (VK) in a bag-of-words baseline. Fig-
ure 1 shows the constituent and dependency struc-
ture for the above sentence.
3.2 Settings
As kernel-based algorithms inherently explore the
whole feature space to weight the features, it is im-
portant to choose appropriate substructures to re-
move unnecessary features as much as possible.
339
NP
PP
NP
DT JJ NN
A tragic waste
NP
IN
of
NP NP
NN
talent
CC
and
JJ JJ NNS
incredible visual effect
(a)
waste
det amod prep of
A tragic talent
conj and
effects
amod amod
incredible visual
(b)
waste
det amod prep of
DT JJ NN
conj and
NNS
amod amod
JJ JJ
(c)
waste
det amod prep of
DT
A
JJ
tragic
NN
talent
conj and
NNS
effects
amod amod
JJ
incredible
visual
visual
(d)
Figure 1: Illustration of the different tree structures employed for convolution kernels. (a) Constituent parse tree
(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)
with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).
NP
DT JJ NN
A tragic waste
(a)
waste
amod
JJ
tragic
(b)
Figure 2: Illustration of the different settings on con-
stituency (CON) and dependency (DWP) parse trees with
tragic as the indicator word.
Unfortunately, in our task there exist several cues
indicating the polarity of the document, which are
distributed in different sentences. To solve this prob-
lem, we define the indicators in this task as subjec-
tive words in a polarity lexicon (Wilson et al, 2005).
For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at least
one subjective word) of each indicator for different
representations as follows:
For a constituent tree, a node and its children
correspond to a grammatical production. There-
fore, considering the terminal node tragic in the con-
stituent structure tree in Figure 1(a), we extract the
subtree rooted at the grandparent of the terminal, see
Figure 2(a). We also use the corresponding sequence
Scopes Trees Size
Document 32 24
Subjective Sentences 22 27
Constituent Substructures 30 10
Dependency Substructures 40 3
Table 1: The detail of the corpus. Here Trees denotes the
average number of trees, and Size denotes the averaged
number of words in each tree.
of words in the subtree for the sequential kernel.
For a dependency tree, we only consider the sub-
tree containing the lexical items that are directly
connected to the subjective word. For instance, giv-
en the node tragic in Figure 1(d), we will extract its
direct parent waste integrated with dependency rela-
tions and (possibly) POS, as in Figure 2(b).
We further add two background scopes, one be-
ing subjective sentences (the sentences that contain
subjective words), and the entire document.
4 Experiments
4.1 Setup
We carried out experiments on the movie review
dataset (Pang and Lee, 2004), which consists of
340
1000 positive reviews and 1000 negative reviews.
To obtain constituency trees, we parsed the docu-
ment using the Stanford Parser (Klein and Man-
ning, 2003). To obtain dependency trees, we passed
the Stanford constituency trees through the Stanford
constituency-to-dependency converter (de Marneffe
and Manning, 2008).
We exploited Subset Tree (SST) (Collins and
Duffy, 2001) and Partial Tree (PT) kernels (Mos-
chitti, 2006) for constituent and dependency parse
trees1, respectively. A sequential kernel is applied
for lexical sequences. Kernels were combined using
plain (unweighted) summation. Corpus statistics are
provided in Table 1.
We use a manually constructed polarity lexicon
(Wilson et al, 2005), in which each entry is annotat-
ed with its degree of subjectivity (strong, weak), as
well as its sentiment polarity (positive, negative and
neutral). We only take into account the subjective
terms with the degree of strong subjectivity.
We consider two baselines:
? VK: bag-of-words features using a vector ker-
nel (Pang and Lee, 2004; Ng et al, 2006)
? Rand: a number of randomly selected sub-
structures similar to the number of extracted
substructures defined in Section 3.2
All experiments were carried out using the SVM-
Light-TK toolkit2 with default parameter settings.
All results reported are based on 10-fold cross vali-
dation.
4.2 Results and Discussions
Table 2 lists the results of the different kernel type
combinations. The best performance is obtained by
combining VK and DW kernels, gaining a signifi-
cant improvement of 1.45 point in accuracy. As far
as PT kernels are concerned, we find dependency
trees with simple words (DW) outperform both de-
pendency trees with POS (DP) and those with both
words and POS (DWP). We conjecture that in this
case, as syntactic information is already captured by
1A SubSet Tree is a structure that satisfies the constraint that
grammatical rules cannot be broken, while a Partial Tree is a
more general form of substructures obtained by the application
of partial production rules of the grammar.
2available at http://disi.unitn.it/moschitti/
Kernels Doc Sent Rand Sub
VK 87.05
VK + SW 87.25 86.95 87.25 87.40
VK + SP 87.35 86.95 87.45 87.35
VK + SWP 87.30 87.45 87.30 88.15*
VK + CON 87.45 87.65 87.45 88.30**
VK + DW 87.35 87.50 87.30 88.50**
VK + DP 87.75* 87.20 87.35 87.75
VK + DWP 87.70* 87.30 87.65 87.80*
Table 2: Results of kernels. Here Doc denotes the whole
document of the text, Sent denotes the sentences that con-
tains subjective terms in the lexicon, Rand denotes ran-
domly selected substructures, and Sub denotes the sub-
structures defined in Section 3.2. We use ?*? and ?**? to
denote a result is better than baseline VK significantly at
p < 0.05 and p < 0.01 (sign test), respectively.
the dependency representation, POS tags can intro-
duce little new information, and will add unneces-
sary complexity. For example, given the substruc-
ture (waste (amod (JJ (tragic)))), the PT kernel will
use both (waste (amod (JJ))) and (waste (amod (JJ
(tragic)))). We can see that the former is adding no
value to the model, as the JJ tag could indicate ei-
ther positive words (e.g. good) or negative words
(e.g. tragic). In contrast, words are good indicators
for sentiment polarity.
The results in Table 2 confirm two of our hy-
potheses. Firstly, it clearly demonstrates the val-
ue of incorporating syntactic information into the
document-level sentiment classifier, as the tree k-
ernels (CON and D*) generally outperforms vector
and sequence kernels (VK and S*). More impor-
tantly, it also shows the necessity of extracting ap-
propriate substructures when using convolution ker-
nels in our task: when using the dependency kernel
(VK+DW), the result on lexicon guided substruc-
tures (Sub) outperforms the results on document,
sentence, or randomly selected substructures, with
statistical significance (p<0.05).
5 Conclusion and Future Work
We studied the impact of syntactic information on
document-level sentiment classification using con-
volution kernels, and reduced the complexity of the
kernels by extracting minimal high-impact substruc-
tures, guided by a polarity lexicon. Experiments
341
show that our method outperformed a bag-of-words
baseline with a statistically significant gain of 1.45
absolute point in accuracy.
Our research focuses on identifying and using
high-impact substructures for convolution kernels in
document-level sentiment classification. We expect
our method to be complementary with sophisticated
methods used in state-of-the-art sentiment classifica-
tion systems, which is to be explored in future work.
Acknowledgement
The authors were supported by 863 State Key
Project No. 2006AA010108, the EuroMatrixPlus F-
P7 EU project (grant No 231720) and Science Foun-
dation Ireland (Grant No. 07/CE/I1142). Part of the
research was done while Zhaopeng Tu was visiting,
and Yifan He was at the Centre for Next Generation
Localisation (www.cngl.ie), School of Computing,
Dublin City University. We thank the anonymous
reviewers for their insightful comments. We are al-
so grateful to Junhui Li for his helpful feedback.
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment analysis
of twitter data. In Proceedings of the Workshop on
Languages in Social Media, pages 30?38. Association
for Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005a. A
Shortest Path Dependency Kernel for Relation Extrac-
tion. In Proceedings of Human Language Technolo-
gy Conference and Conference on Empirical Methods
in Natural Language Processing, pages 724?731, Van-
couver, British Columbia, Canada, oct. Association for
Computational Linguistics.
Razvan Bunescu and Raymond Mooney. 2005b. Sub-
sequence Kernels for Relation Extraction. In Y Weis-
s, B Sch o lkopf, and J Platt, editors, Proceedings of
the 19th Conference on Neural Information Processing
Systems, pages 171?178, Cambridge, MA. MIT Press.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Proceedings of Neural
Information Processing Systems, pages 625?632.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the COLING Workshop
on Cross-Framework and Cross-Domain Parser Eval-
uation, Manchester, August.
Paul Ferguson, Neil O?Hare, Michael Davy, Adam
Bermingham, Paraic Sheridan, Cathal Gurrin, and
Alan F. Smeaton. 2009. Exploring the use of
paragraph-level annotations for sentiment analysis of
financial blogs. In Proceedings of the Workshop on
Opinion Mining and Sentiment Analysis.
Richard Johansson and Alessandro Moschitti. 2010.
Syntactic and semantic structure for opinion expres-
sion detection. In Proceedings of the Fourteenth Con-
ference on Computational Natural Language Learn-
ing, pages 67?76, Uppsala, Sweden, July.
Mahesh Joshi and Carolyn Penstein-Rose. 2009. Gen-
eralizing Dependency Features for Opinion Mining.
In Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers, pages 313?316, Suntec, Singapore, jul.
Suntec, Singapore.
Dan Klein and Christopher D Manning. 2003. Accu-
rate Unlexicalized Parsing. In Proceedings of the 41st
Annual Meeting of the Association for Computational
Linguistics, pages 423?430, Sapporo, Japan, jul. As-
sociation for Computational Linguistics.
Moshe Koppel and Jonathan Schler. 2005. Using neutral
examples for learning polarity. In Proceedings of In-
ternational Joint Conferences on Artificial Intelligence
(IJCAI) 2005, pages 1616?1616.
Steve Lawrence Kushal Dave and David Pennock. 2003.
Mining the peanut gallery: Opinion extraction and se-
mantic classification of product reviews. In Proceed-
ings of the 12th International Conference on World
Wide Web, pages 519?528, ACM. ACM.
Jingjing Liu and Stephanie Seneff. 2009. Review Sen-
timent Scoring via a Parse-and-Paraphrase Paradigm.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 161?
169, Singapore, aug. Singapore.
Shotaro Matsumoto, Hiroya Takamura, and Manabu
Okumura. 2005. Sentiment classification using word
sub-sequences and dependency sub-trees. Proceed-
ings of PAKDD?05, the 9th Pacific-Asia Conference on
Advances in Knowledge Discovery and Data Mining,
3518/2005:21?32.
Alessandro Moschitti and Silvia Quarteroni. 2008. K-
ernels on Linguistic Structures for Answer Extraction.
In Proceedings of ACL-08: HLT, Short Papers, pages
113?116, Columbus, Ohio, jun. Association for Com-
putational Linguistics.
Alessandro Moschitti, Daniele Pighin, and Roberto
Basili. 2008. Tree kernels for semantic role labeling.
Computational Linguistics, 34(2):193?224.
Alessandro Moschitti. 2006. Efficient Convolution Ker-
nels for Dependency and Constituent Syntactic Trees.
In Proceedings of the 17th European Conference on
Machine Learning, pages 318?329, Berlin, Germany,
342
sep. Machine Learning: ECML 2006, 17th European
Conference on Machine Learning, Proceedings.
Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin. 2006.
Examining the Role of Linguistic Knowledge Sources
in the Automatic Identification and Classification of
Reviews. In Proceedings of the COLING/ACL 2006
Main Conference Poster Sessions, pages 611?618,
Sydney, Australia, jul. Sydney, Australia.
Truc-Vien T Nguyen, Alessandro Moschitti, and
Giuseppe Riccardi. 2009. Convolution kernels on
constituent, dependency and sequential structures for
relation extraction. Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1378?1387.
Bo Pang and Lillian Lee. 2004. A Sentimental Educa-
tion: Sentiment Analysis Using Subjectivity Summa-
rization Based on Minimum Cuts. In Proceedings of
the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 271?278, Barcelona, S-
pain, jun. Barcelona, Spain.
Michael Wiegand and Dietrich Klakow. 2010. Convolu-
tion Kernels for Opinion Holder Extraction. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 795?803, Los An-
geles, California, jun. Los Angeles, California.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing Contextual Polarity in Phrase-
Level Sentiment Analysis. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing,
pages 347?354, Vancouver, British Columbia, Cana-
da, oct. Association for Computational Linguistics.
Hong Yu and Vasileios Hatzivassiloglou. 2003. Toward-
s answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the 2003 Conference on
Empirical Methods in Natural Language Processing,
pages 129?136, Association for Computational Lin-
guistics. Association for Computational Linguistics.
Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.
2006. A Composite Kernel to Extract Relations be-
tween Entities with Both Flat and Structured Features.
In Proceedings of the 21st International Conference
on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics,
pages 825?832, Sydney, Australia, jul. Association for
Computational Linguistics.
343
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 358?363,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
A Novel Graph-based Compact Representation of Word Alignment
Qun Liu?? Zhaopeng Tu? Shouxun Lin?
?Centre for Next Generation Locolisation ?Key Lab. of Intelligent Info. Processing
Dublin City University Institute of Computing Technology, CAS
qliu@computing.dcu.ie {tuzhaopeng,sxlin}@ict.ac.cn
Abstract
In this paper, we propose a novel compact
representation called weighted bipartite
hypergraph to exploit the fertility model,
which plays a critical role in word align-
ment. However, estimating the probabili-
ties of rules extracted from hypergraphs is
an NP-complete problem, which is com-
putationally infeasible. Therefore, we pro-
pose a divide-and-conquer strategy by de-
composing a hypergraph into a set of inde-
pendent subhypergraphs. The experiments
show that our approach outperforms both
1-best and n-best alignments.
1 Introduction
Word alignment is the task of identifying trans-
lational relations between words in parallel cor-
pora, in which a word at one language is usually
translated into several words at the other language
(fertility model) (Brown et al, 1993). Given that
many-to-many links are common in natural lan-
guages (Moore, 2005), it is necessary to pay atten-
tion to the relations among alignment links.
In this paper, we have proposed a novel graph-
based compact representation of word alignment,
which takes into account the joint distribution of
alignment links. We first transform each align-
ment to a bigraph that can be decomposed into a
set of subgraphs, where all interrelated links are
in the same subgraph (? 2.1). Then we employ
a weighted partite hypergraph to encode multiple
bigraphs (? 2.2).
The main challenge of this research is to effi-
ciently calculate the fractional counts for rules ex-
tracted from hypergraphs. This is equivalent to the
decision version of set covering problem, which is
NP-complete. Observing that most alignments are
not connected, we propose a divide-and-conquer
strategy by decomposing a hypergraph into a set
Figure 1: A bigraph constructed from an align-
ment (a), and its disjoint MCSs (b).
of independent subhypergraphs, which is compu-
tationally feasible in practice (? 3.2). Experimen-
tal results show that our approach significantly im-
proves translation performance by up to 1.3 BLEU
points over 1-best alignments (? 4.3).
2 Graph-based Compact Representation
2.1 Word Alignment as a Bigraph
Each alignment of a sentence pair can be trans-
formed to a bigraph, in which the two disjoint ver-
tex sets S and T are the source and target words re-
spectively, and the edges are word-by-word links.
For example, Figure 1(a) shows the corresponding
bigraph of an alignment.
The bigraph usually is not connected. A graph
is called connected if there is a path between every
pair of distinct vertices. In an alignment, words in
a specific portion at the source side (i.e. a verb
phrase) usually align to those in the corresponding
portion (i.e. the verb phrase at the target side), and
would never align to other words; and vice versa.
Therefore, there is no edge that connects the words
in the portion to those outside the portion.
Therefore, a bigraph can be decomposed into
a unique set of minimum connected subgraphs
(MCSs), where each subgraph is connected and
does not contain any other MCSs. For example,
the bigraph in Figure 1(a) can be decomposed into
358
the
book
is
on
the
desk
?
?
??
?
D
the
book
is
on
the
desk
?
?
??
?
e1
F
the
book
is
on
the
desk
?
?
??
?
E
e2 e3
e4
e5
Figure 2: (a) One alignment of a sentence pair; (b) another alignment of the same sentence pair; (c) the
resulting hypergraph that takes the two alignments as samples.
the MCSs in Figure 1(b). We can see that all in-
terrelated links are in the same MCS. These MCSs
work as fundamental units in our approach to take
advantage of the relations among the links. Here-
inafter, we use bigraph to denote the alignment of
a sentence pair.
2.2 Weighted Bipartite Hypergraph
We believe that offering more alternatives to ex-
tracting translation rules could help improve trans-
lation quality. We propose a new structure called
weighted bipartite hypergraph that compactly en-
codes multiple alignments.
We use an example to illustrate our idea. Fig-
ures 2(a) and 2(b) show two bigraphs of the same
sentence pair. Intuitively, we can encode the
union set of subgraphs in a bipartite hypergraph,
in which each MCS serves as a hyperedge, as in
Figure 2(c). Accordingly, we can calculate how
well a hyperedge is by calculating its relative fre-
quency, which is the probability sum of bigraphs
in which the corresponding MCS occurs divided
by the probability sum of all possible bigraphs.
Suppose that the probabilities of the two bigraphs
in Figures 2(a) and 2(b) are 0.7 and 0.3, respec-
tively. Then the weight of e1 is 1.0 and e2 is
0.7. Therefore, each hyperedge is associated with
a weight to indicate how well it is.
Formally, a weighted bipartite hypergraph H is
a triple ?S, T,E? where S and T are two sets of
vertices on the source and target sides, and E are
hyperedges associated with weights. Currently,
we estimate the weights of hyperedges from an n-
best list by calculating relative frequencies:
w(ei) =
?
BG?N p(BG) ? ?(BG, gi)?
BG?N p(BG)
Here N is an n-best bigraph (i.e., alignment) list,
p(BG) is the probability of a bigraph BG in the n-
best list, gi is the MCS that corresponds to ei, and
?(BG, gi) is an indicator function which equals 1
when gi occurs in BG, and 0 otherwise.
It is worthy mentioning that a hypergraph en-
codes much more alignments than the input n-best
list. For example, we can construct a new align-
ment by using hyperedges from different bigraphs
that cover all vertices.
3 Graph-based Rule Extraction
In this section we describe how to extract transla-
tion rules from a hypergraph (? 3.1) and how to
estimate their probabilities (? 3.2).
3.1 Extraction Algorithm
We extract translation rules from a hypergraph
for the hierarchical phrase-based system (Chiang,
2007). Chiang (2007) describes a rule extrac-
tion algorithm that involves two steps: (1) extract
phrases from 1-best alignments; (2) obtain vari-
able rules by replacing sub-phrase pairs with non-
terminals. Our extraction algorithm differs at the
first step, in which we extract phrases from hyper-
graphs instead of 1-best alignments. Rather than
restricting ourselves by the alignment consistency
in the traditional algorithm, we extract all possible
candidate target phrases for each source phrase.
To maintain a reasonable rule table size, we fil-
ter out less promising candidates that have a frac-
tional count lower than a threshold.
3.2 Calculating Fractional Counts
The fractional count of a phrase pair is the proba-
bility sum of the alignments with which the phrase
pair is consistent (?3.2.2), divided by the probabil-
ity sum of all alignments encoded in a hypergraph
(?3.2.1) (Liu et al, 2009).
359
Intuitively, our approach faces two challenges:
1. How to calculate the probability sum of all
alignments encoded in a hypergraph (?3.2.1)?
2. How to efficiently calculate the probability
sum of all consistent alignments for each
phrase pair (?3.2.2)?
3.2.1 Enumerating All Alignments
In theory, a hypergraph can encode all possible
alignments if there are enough hyperedges. How-
ever, since a hypergraph is constructed from an n-
best list, it can only represent partial space of all
alignments (p(A|H) < 1) because of the limiting
size of hyperedges learned from the list. There-
fore, we need to enumerate all possible align-
ments in a hypergraph to obtain the probability
sum p(A|H).
Specifically, generating an alignment from a hy-
pergraph can be modelled as finding a complete
hyperedge matching, which is a set of hyperedges
without common vertices that matches all vertices.
The probability of the alignment is the product of
hyperedge weights. Thus, enumerating all possi-
ble alignments in a hypergraph is reformulated as
finding all complete hypergraph matchings, which
is an NP-complete problem (Valiant, 1979).
Similar to the bigraph, a hypergraph is also usu-
ally not connected. To make the enumeration prac-
tically tractable, we propose a divide-and-conquer
strategy by decomposing a hypergraph H into a set
of independent subhypergraphs {h1, h2, . . . , hn}.
Intuitively, the probability of an alignment is the
product of hyperedge weights. According to the
divide-and-conquer strategy, the probability sum
of all alignments A encoded in a hypergraph H is:
p(A|H) =
?
hi?H
p(Ai|hi)
Here p(Ai|hi) is the probability sum of all sub-
alignments Ai encoded in the subhypergraph hi.
3.2.2 Enumerating Consistent Alignments
Since a hypergraph encodes many alignments, it is
unrealistic to enumerate all consistent alignments
explicitly for each phrase pair.
Recall that a hypergraph can be decomposed
to a list of independent subhypergraphs, and an
alignment is a combination of the sub-alignments
from the decompositions. We observe that a
phrase pair is absolutely consistent with the sub-
alignments from some subhypergraphs, while pos-
sibly consistent with the others. As an example,
E
the
book
is
on
the
desk
?
?
??
?
e1
D
e2 e3
e4
e5
the
book
is
on
the
desk
?
?
??
?
e1
e2 e3
e4
e5
h1
h3
h2
Figure 3: A hypergraph with a candidate phrase
in the grey shadow (a), and its independent subhy-
pergraphs {h1, h2, h3}.
consider the phrase pair in the grey shadow in Fig-
ure 3(a), it is consistent with all sub-alignments
from both h1 and h2 because they are outside and
inside the phrase pair respectively, while not con-
sistent with the sub-alignment that contains hyper-
edge e2 from h3 because it contains an alignment
link that crosses the phrase pair.
Therefore, to calculate the probability sum of all
consistent alignments, we only need to consider
the overlap subhypergraphs, which have at least
one hyperedge that crosses the phrase pair. Given
a overlap subhypergraph, the probability sum of
consistent sub-alignments is calculated by sub-
tracting the probability sum of the sub-alignments
that contain crossed hyperedges, from the proba-
bility sum of all sub-alignments encoded in a hy-
pergraph.
Given a phrase pair P , let OS and NS de-
notes the sets of overlap and non-overlap subhy-
pergraphs respectively (NS = H ?OS). Then
p(A|H,P ) =
?
hi?OS
p(Ai|hi, P )
?
hj?NS
p(Aj|hj)
Here the phrase pair is absolutely consistent with
the sub-alignments from non-overlap subhyper-
graphs (NS), and we have p(A|h, P ) = p(A|h).
Then the fractional count of a phrase pair is:
c(P |H) = p(A|H,P )p(A|H) =
?
hi?OS p(A|hi, P )?
hi?OS p(A|hi)
After we get the fractional counts of transla-
tion rules, we can estimate their relative frequen-
cies (Och and Ney, 2004). We follow (Liu et al,
2009; Tu et al, 2011) to learn lexical tables from
n-best lists and then calculate the lexical weights.
360
Rules from. . . Rules MT03 MT04 MT05 Avg.
1-best 257M 33.45 35.25 33.63 34.11
10-best 427M 34.10 35.71 34.04 34.62
Hypergraph 426M 34.71 36.24 34.41 35.12
Table 1: Evaluation of translation quality.
4 Experiments
4.1 Setup
We carry out our experiments on Chinese-English
translation tasks using a reimplementation of the
hierarchical phrase-based system (Chiang, 2007).
Our training data contains 1.5 million sentence
pairs from LDC dataset.1 We train a 4-gram
language model on the Xinhua portion of the
GIGAWORD corpus using the SRI Language
Toolkit (Stolcke, 2002) with modified Kneser-Ney
Smoothing (Kneser and Ney, 1995). We use min-
imum error rate training (Och, 2003) to optimize
the feature weights on the MT02 testset, and test
on the MT03/04/05 testsets. For evaluation, case-
insensitive NIST BLEU (Papineni et al, 2002) is
used to measure translation performance.
We first follow Venugopal et al (2008) to pro-
duce n-best lists via GIZA++. We produce 10-best
lists in two translation directions, and use ?grow-
diag-final-and? strategy (Koehn et al, 2003) to
generate the final n-best lists by selecting the
top n alignments. We re-estimated the probabil-
ity of each alignment in the n-best list using re-
normalization (Venugopal et al, 2008). Finally we
construct weighted alignment hypergraphs from
these n-best lists.2 When extracting rules from hy-
pergraphs, we set the pruning threshold t = 0.5.
4.2 Tractability of Divide-and-Conquer
Strategy
Figure 4 shows the distribution of vertices (hy-
peredges) number of the subhypergraphs. We can
see that most of the subhypergraphs have just less
than two vertices and hyperedges.3 Specifically,
each subhypergraph has 2.0 vertices and 1.4 hy-
1The corpus includes LDC2002E18, LDC2003E07,
LDC2003E14, Hansards portion of LDC2004T07,
LDC2004T08 and LDC2005T06.
2Here we only use 10-best lists, because the alignments
beyond top 10 have very small probabilities, thus have negli-
gible influence on the hypergraphs.
3It?s interesting that there are few subhypergraphs that
have exactly 2 hyperedges. In this case, the only two hy-
peredges fully cover the vertices and they differ at the word-
by-word links, which is uncommon in n-best lists.
 0
 0.2
 0.4
 0.6
 0.8
 1
1 2 3 4 5 6 7 8 9 10
pe
rc
en
ta
ge
number of vertices (hyperedges)
vertices
hyperedges
Figure 4: The distribution of vertices (hyperedges)
number of the subhypergraphs.
peredges on average. This suggests that the divide-
and-conquer strategy makes the extraction compu-
tationally tractable, because it greatly reduces the
number of vertices and hyperedges. For computa-
tional tractability, we only allow a subhypergraph
has at most 5 hyperedges. 4
4.3 Translation Performance
Table 1 shows the rule table size and transla-
tion quality. Using n-best lists slightly improves
the BLEU score over 1-best alignments, but at
the cost of a larger rule table. This is in ac-
cord with intuition, because all possible transla-
tion rules would be extracted from different align-
ments in n-best lists without pruning. This larger
rule table indeed leads to a high rule coverage, but
in the meanwhile, introduces translation errors be-
cause of the low-quality rules (i.e., rules extracted
only from low-quality alignments in n-best lists).
By contrast, our approach not only significantly
improves the translation performance over 1-best
alignments, but also outperforms n-best lists with
a similar-scale rule table. The absolute improve-
ments of 1.0 BLEU points on average over 1-best
alignments are statistically significant at p < 0.01
using sign-test (Collins et al, 2005).
4If a subhypergraph has more than 5 hyperedges, we
forcibly partition it into small subhypergraphs by iteratively
removing lowest-probability hyperedges.
361
Rules from. . . Shared Non-shared AllRules BLEU Rules BLEU Rules BLEU
10-best 1.83M 32.75 2.81M 30.71 4.64M 34.62
Hypergraph 1.83M 33.24 2.89M 31.12 4.72M 35.12
Table 2: Comparison of rule tables learned from n-best lists and hypergraphs. ?All? denotes the full rule
table, ?Shared? denotes the intersection of two tables, and ?Non-shared? denotes the complement. Note
that the probabilities of ?Shared? rules are different for the two approaches.
Why our approach outperforms n-best lists? In
theory, the rule table extracted from n-best lists
is a subset of that from hypergraphs. In prac-
tice, however, this is not true because we pruned
the rules that have fractional counts lower than a
threshold. Therefore, the question arises as to how
many rules are shared by n-best and hypergraph-
based extractions. We try to answer this ques-
tion by comparing the different rule tables (filtered
on the test sets) learned from n-best lists and hy-
pergraphs. Table 2 gives some statistics. ?All?
denotes the full rule table, ?Shared? denotes the
intersection of two tables, and ?Non-shared? de-
notes the complement. Note that the probabil-
ities of ?Shared? rules are different for the two
approaches. We can see that both the ?Shared?
and ?Non-shared? rules learned from hypergraphs
outperform n-best lists, indicating: (1) our ap-
proach has a better estimation of rule probabili-
ties because we estimate the probabilities from a
much larger alignment space that can not be rep-
resented by n-best lists, (2) our approach can ex-
tract good rules that cannot be extracted from any
single alignments in the n-best lists.
5 Related Work
Our research builds on previous work in the field
of graph models and compact representations.
Graph models have been used before in word
alignment: the search space of word alignment can
be structured as a graph and the search problem
can be reformulated as finding the optimal path
though this graph (e.g., (Och and Ney, 2004; Liu et
al., 2010)). In addition, Kumar and Byrne (2002)
define a graph distance as a loss function for
minimum Bayes-risk word alignment, Riesa and
Marcu (2010) open up the word alignment task to
advances in hypergraph algorithms currently used
in parsing. As opposed to the search problem, we
propose a graph-based compact representation that
encodes multiple alignments for machine transla-
tion.
Previous research has demonstrated that com-
pact representations can produce improved re-
sults by offering more alternatives, e.g., using
forests over 1-best trees (Mi and Huang, 2008;
Tu et al, 2010; Tu et al, 2012a), word lattices
over 1-best segmentations (Dyer et al, 2008),
and weighted alignment matrices over 1-best word
alignments (Liu et al, 2009; Tu et al, 2011; Tu et
al., 2012b). Liu et al, (2009) estimate the link
probabilities from n-best lists, while Gispert et
al., (2010) learn the alignment posterior probabil-
ities directly from IBM models. However, both of
them ignore the relations among alignment links.
By contrast, our approach takes into account the
joint distribution of alignment links and explores
the fertility model past the link level.
6 Conclusion
We have presented a novel compact representa-
tion of word alignment, named weighted bipar-
tite hypergraph, to exploit the relations among
alignment links. Since estimating the probabil-
ities of rules extracted from hypergraphs is an
NP-complete problem, we propose a computation-
ally tractable divide-and-conquer strategy by de-
composing a hypergraph into a set of independent
subhypergraphs. Experimental results show that
our approach outperforms both 1-best and n-best
alignments.
Acknowledgement
The authors are supported by 863 State Key
Project No. 2011AA01A207, National Key Tech-
nology R&D Program No. 2012BAH39B03 and
National Natural Science Foundation of China
(Contracts 61202216). Qun Liu?s work is partially
supported by Science Foundation Ireland (Grant
No.07/CE/I1142) as part of the CNGL at Dublin
City University. We thank Junhui Li, Yifan He
and the anonymous reviewers for their insightful
comments.
362
References
Peter E. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993.
The mathematics of statistical machine translation:
Parameter estimation. Computational linguistics,
19(2):263?311.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228.
M. Collins, P. Koehn, and I. Kuc?erova?. 2005. Clause
restructuring for statistical machine translation. In
Proceedings of the 43rd Annual Meeting on Associa-
tion for Computational Linguistics, pages 531?540.
Adria` de Gispert, Juan Pino, and William Byrne. 2010.
Hierarchical phrase-based translation grammars ex-
tracted from alignment posterior probabilities. In
Proceedings of the 2010 Conference on Empirical
Methods in Natural Language Processing, pages
545?554.
Christopher Dyer, Smaranda Muresan, and Philip
Resnik. 2008. Generalizing word lattice translation.
In Proceedings of ACL-08: HLT, pages 1012?1020.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, volume 1, pages
181?184.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics, pages 48?54.
Shankar Kumar and William Byrne. 2002. Mini-
mum Bayes-risk word alignments of bilingual texts.
In Proceedings of the 2002 Conference on Empiri-
cal Methods in Natural Language Processing, pages
140?147.
Yang Liu, Tian Xia, Xinyan Xiao, and Qun Liu. 2009.
Weighted alignment matrices for statistical machine
translation. In Proceedings of the 2009 Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 1017?1026.
Yang Liu, Qun Liu, and Shouxun Lin. 2010. Discrim-
inative word alignment by linear modeling. Compu-
tational Linguistics, 36(3):303?339.
Haitao Mi and Liang Huang. 2008. Forest-based trans-
lation rule extraction. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 206?214.
Robert C. Moore. 2005. A discriminative framework
for bilingual word alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 81?88, October.
Franz J. Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Computational Linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318.
Jason Riesa and Daniel Marcu. 2010. Hierarchical
search for word alignment. In Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 157?166.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of Seventh Inter-
national Conference on Spoken Language Process-
ing, volume 3, pages 901?904. Citeseer.
Zhaopeng Tu, Yang Liu, Young-Sook Hwang, Qun
Liu, and Shouxun Lin. 2010. Dependency forest
for statistical machine translation. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics, pages 1092?1100.
Zhaopeng Tu, Yang Liu, Qun Liu, and Shouxun Lin.
2011. Extracting hierarchical rules from a weighted
alignment matrix. In Proceedings of 5th Interna-
tional Joint Conference on Natural Language Pro-
cessing, pages 1294?1303.
Zhaopeng Tu, Wenbin Jiang, Qun Liu, and Shouxun
Lin. 2012a. Dependency forest for sentiment anal-
ysis. In Springer-Verlag Berlin Heidelberg, pages
69?77.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012b. Combining mul-
tiple alignments to improve machine translation. In
Proceedings of the 24th International Conference on
Computational Linguistics, pages 1249?1260.
Leslie G Valiant. 1979. The complexity of comput-
ing the permanent. Theoretical Computer Science,
8(2):189?201.
Ashish Venugopal, Andreas Zollmann, Noah A. Smith,
and Stephan Vogel. 2008. Wider pipelines: n-best
alignments and parses in mt training. In Proceed-
ings of AMTA, pages 192?201.
363
