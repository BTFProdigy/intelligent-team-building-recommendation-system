Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1433?1437,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Naive Bayes Word Sense Induction
Do Kook Choe
Brown University
Providence, RI
dc65@cs.brown.edu
Eugene Charniak
Brown University
Providence, RI
ec@cs.brown.edu
Abstract
We introduce an extended naive Bayes model
for word sense induction (WSI) and apply it to
a WSI task. The extended model incorporates
the idea the words closer to the target word are
more relevant in predicting its sense. The pro-
posed model is very simple yet effective when
evaluated on SemEval-2010 WSI data.
1 Introduction
The task of word sense induction (WSI) is to find
clusters of tokens of an ambiguous word in an un-
labeled corpus that have the same sense. For in-
stance, given a target word ?crane,? a good WSI sys-
tem should find a cluster of tokens referring to avian
cranes and another referring to mechanical cranes.
We believe that neighboring words contain enough
information that these clusters can be found from
plain texts.
WSI is related to word sense disambiguation
(WSD). In a WSD task, a system learns a sense clas-
sifier in a supervised manner from a sense-labeled
corpus. The performance of the learned classifier
is measured on some unseen data. WSD systems
perform better than WSI systems, but building la-
beled data can be prohibitively expensive. In addi-
tion, WSD systems are not suitable for newly cre-
ated words, new senses of existing words, or domain-
specific words. On the other hand, WSI systems can
learn new senses of words directly from texts because
these programs do not rely on a predefined set of
senses.
In Section 2 we describe relevant previous work. In
Section 3 and 4 we introduce the naive Bayes model
for WSI and inference schemes for the model. In Sec-
tion 5 we evaluate the model on SemEval-2010 data.
In Section 6 we conclude.
2 Related Work
Yarowsky (1995) introduces a semi-supervised
bootstrapping algorithm with two assumptions
that rivals supervised algorithms: one-sense-per-
collocation and one-sense-per-discourse. But this
algorithm cannot easily be scaled up because for
any new ambiguous word humans need to pick
a few seed words, which initialize the algorithm.
In order to automate the semi-supervised system,
Eisner and Karakos (2005) propose an unsupervised
bootstrapping algorithm. Their system tries many
different seeds for bootstrapping and chooses the
?best? classifier at the end. Eisner and Karakos?s
algorithm is limited in that their system is designed
for disambiguating words that have only 2 senses.
Bayesian WSI systems have been developed by
several authors. Brody and Lapata (2009) apply
Latent Dirichlet Allocation (LDA) (Blei et al,
2003) to WSI. They run a topic modeling algorithm
on texts with some fixed number of topics that
correspond to senses and induce a cluster by finding
target words assigned to the same topic. Their
system is evaluated on SemEval-2007 noun data
(Agirre and Soroa, 2007). Lau et al (2012) apply
a nonparametric model, Hierarchical Dirichlet Pro-
cesses (HDP), to SemEval-2010 data (Manandhar et
al., 2010).
3 Model
Following Yarowsky (1995), we assume that a word
in a document has one sense. Multiple occurrences
of a word in a document refer to the same object
or concept. The naive Bayes model is well suited
for this one-sense-per-document assumption. Each
document has one topic corresponding to the sense of
the target word that needs disambiguation. Context
words in a document are drawn from the conditional
distribution of words given the sense. Context words
are assumed to be independent from each other given
1433
the sense, which is far from being true yet effective.
3.1 Naive Bayes
The naive Bayes model assumes that every word in a
document is generated independently from the con-
ditional distribution of words given a sense, p(w|s).
The mathematical definition of the naive Bayes
model is as follows:
p(w) =
?
s
p(s,w) =
?
s
p(s)p(w |s)
=
?
s
p(s)
?
w
p(w|s), (1)
where w is a vector of words in the document. With
the model, a new document can be easily labeled
using the following classifier:
s? = argmax
s
p(s)
?
w
p(w|s), (2)
where s? is the label of the new document. In con-
trast to LDA-like models, it is easy to construct
the closed form classifier from the model. The pa-
rameters of the model, p(s) and p(w|s), can be
learned by maximizing the probability of the corpus,
p(d) =
?
d p(d) =
?
w p(w) where d is a vector of
documents and d = w .
3.2 Distance Incorporated Naive Bayes
Intuitively, context words near a target word are
more indicative of its sense than ones that are far-
ther away. To account for this intuition, we propose
a more sophisticated model that uses the distance
between a context word and a target word. Before
introducing the new model, we define a probability
distribution, f(w|s), that incorporates distances as
follows:
f(w|s) =
p(w|s)l(w)
?
w??W p(w
?|s)l(w)
, (3)
where l(w) = 1dist(w)x . W is a set of types in the cor-
pus. x is a tunable parameter that takes nonnegative
real values. With the new probability distribution,
the model and the classifier become:
p(w) =
?
s
p(s)
?
w
f(w|s) (4)
s? = argmax
s
p(s)
?
w
f(w|s), (5)
where f(w|s) replaces p(w|s). The naive Bayes
model is a special case; set x = 0. The new model
puts more weight on context words that are close
to the target word. The distribution of words that
are farther away approaches the uniform distribu-
tion. l(w) smoothes the distribution more as x be-
comes larger.
4 Inference
Given the generative model, we employ two inference
algorithms to learn the sense distribution and word
distributions given a sense. Expectation Maximiza-
tion (EM) is a natural choice for the naive Bayes
(Dempster et al, 1977). When initialized with ran-
dom parameters, EM gets stuck at local maxima. To
avoid local maxima, we use a Gibbs sampler for the
plain naive Bayes to learn parameters that initialize
EM.
5 Experiments
5.1 Data
We evaluate the model on SemEval-2010 WSI task
data (Manandhar et al, 2010). The task has 100
target words, 50 nouns and 50 verbs. For each target
word, there are training and test documents. Table
1 have details. The training and test data are plain
texts without sense tags. For evaluation, the inferred
sense labels are compared with human annotations.
To tune some parameters we use the trial data of
Training Testing Senses (#)
All 879807 8915 3.79
Nouns 716945 5285 4.46
Verbs 162862 3630 3.12
Table 1: Details of SemEval-2010 data
SemEval-2010. The trial data consists of training
and test portions of 4 verbs. On average there are
137 documents for each target word in the training
part of the trial data.
5.2 Task
Participants induce clusters from the training data
and use them to label the test data. Resources other
than NLP tools for morphology and syntax such as
lemmatizer, POS-tagger, and parser are not allowed.
Tuning parameters and inducing clusters are only
allowed during the training phase. After training,
participants submit their sense-labeled test data to
organizers.
LDA models are not compatible with the scoring
rules for the SemEval-2010 competition, and that is
the work against which we most want to compare.
These rules require that training be done strictly be-
fore the testing is done. Note however that LDA re-
quires learning the mixture weights of topics for each
1434
individual document p(topic | document). These are,
of course, learned during training. But the docu-
ments in the testing corpus have never been seen
before, so clearly their topic mixture weights are not
learned during training, and thus not learned at all.
The way to overcome this is by training on both
train and test documents, but this is exactly what
SemEval-2010 forbids.
5.3 Implementation Details
The documents are tokenized and stemmed by
Stanford tokenizer and stemmer. Stop words and
punctuation in the training and test data are
discarded. Words that occur at most 10 times are
discarded from the training data. Context words
within a window of 50 about a target word are used
to construct a bag-of-words.
When a target word appears more than once
in a document, the distance between that target
word and a context word is ambiguous. We define
this distance to be minimum distance between a
context word and an instance of the target word.
For example, the word ?chip? appears 3 times. For
? ? ? of memory chips . Currently , chips are pro-
duced by shining light through a mask to produce
an image on the chip , much as ? ? ?
Example 1: an excerpt from ?chip? test data
a context word, e.g., ?shining? there are three pos-
sible distances: 8 away from the first ?chip,? 4 away
from the second ?chip? and 11 away from the last
?chip.? We set the distance of ?shining? from the
target to 4.
We model each target word individually. We set ?,
a Dirichlet prior for senses, to 0.02 and ?, a Dirichlet
prior for contextual words, to 0.1 for the Gibbs sam-
pler as in Brody and Lapata (2009). We initialize
EM with parameters learned from the sampler. We
run EM until the likehood changes less than 1%. We
run the sampler 2000 iterations including 1000 itera-
tions of burn-in: 10 samples at an interval of 100 are
averaged. For comparison, we also evaluate EM with
random initialization. All reported scores (described
in Section 5.4) are averaged over ten different runs
of the program.1
5.3.1 Tuning Parameters
Two parameters, the number of senses and x of
the function l(w), need to be determined before run-
ning the program. To find a good setting we do grid
search on the trial data with the number of senses
1Code used for experiments is available for download at
http://cs.brown.edu/~dc65/.
ranging from 2 to 5 and x ranging from 0 to 1.1 with
an interval 0.1. Due to the small size of the training
portion of the trial data, words that occur once are
thrown out in the training portion. All the other pa-
rameters are as described in Section 5.3. We choose
(4, 0.4), which achieves the highest supervised recall.
See Table 2 for the performance of the model with
various parameter settings. With a fixed value of x,
a column is nearly unimodal in the number of senses
and vice versa. x = 0 is not optimal and there is
some noticeable difference between scores with opti-
mal x and scores with x = 0.
5.4 Evaluation
We compare our system to other WSI systems and
discuss two metrics for unsupervised evaluation (V-
Measure, paired F-Score) and one metric for super-
vised evaluation (supervised recall). We refer to the
true group of tokens as a gold class and to an induced
group of tokens as a cluster. We refer to the model
learned with the sampler and EM as NB, and to the
model learned with EM only as NB0.
5.4.1 Short Descriptions of Other WSI
Systems Evaluated on SemEval-2010
The baseline assigns every instance of a target
word with the most frequent sense (MFS). UoY runs
a clustering algorithm on a graph with words as
nodes and co-occurrences between words as edges
(Korkontzelos and Manandhar, 2010). Hermit ap-
proximates co-occurrence space with Random Index-
ing and applies a hybrid of k-means and Hierarchical
Agglomerate Clustering to co-occurrence space (Ju-
rgens and Stevens, 2010). NMFlib factors a matrix
using nonnegative matrix factorization and runs a
clustering algorithm on test instances represented by
factors (Van de Cruys et al, 2011).
5.4.2 V-Measure
V-Measure computes the quality of induced clus-
ters as the harmonic mean of two values, homo-
geneity and completeness. Homogeneity measures
whether instances of a cluster belong to a single gold
class. Completeness measures whether instances of a
gold class belong to a cluster. V-Measure is between
0 and 1; higher is better. See Table 3 for details of
V-Measure evaluation (#cl is the number of induced
clusters).
With respect to V-Measure, NB performs much
better than NB0. This holds for paired F-Score and
supervised recall evaluations. The sampler improves
the log-likelihood of NB by 3.8% on average (4.8%
on nouns and 2.9% on verbs).
Pedersen (2010) points out that it is possible to
increase the V-Measure of bad models by increasing
1435
#s \ x 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1
2 74.73 74.76 74.41 74.57 74.06 74.07 74.18 74.33 74.14 74.22 74.15 74.52
3 74.60 74.71 75.21 75.46 75.21 75.57 75.61 75.32 75.53 75.56 74.98 74.79
4 74.52 75.06 74.97 75.14 76.02 75.51 75.74 75.51 75.59 75.51 75.37 75.35
5 73.40 73.88 74.93 75.13 74.79 74.68 74.71 74.49 75.11 74.94 74.86 75.25
Table 2: Performance of the model with various parameters: supervised recall on the trial data. The best value from
each row is bold-faced. The scores are averaged over 100 runs.
VM(%) all nouns verbs #cl
NB 18.0 23.7 9.9 3.42
NB0 14.9 19.0 9.0 3.77
Hermit 16.2 16.7 15.6 10.78
UoY 15.7 20.6 8.5 11.54
NMFlib 11.8 13.5 9.4 4.80
MFS 0.0 0.0 0.0 1.00
Table 3: Unsupervised evaluation: V-Measure
the number of clusters. But increasing the number
of clusters harms paired F-Score, which results in
bad supervised recalls. NB attains a very high V-
Measure with few induced clusters, which indicates
that those clusters are high quality. Other systems
use more induced clusters but fail to attain the V-
Measure of NB.
5.4.3 Paired F-Score
Paired F-Score is the harmonic mean of paired re-
call and paired precision. Paired recall is fraction of
pairs belonging to the same gold class that belong
to the same cluster. Paired precision is fraction of
pairs belonging to the same cluster that belong to
the same class. See Table 4 for details of paired F-
Score evaluation.
As with V-Measure, it is possible to attain a high
paired F-Score by producing only one cluster. The
baseline, MFS, attains 100% paired recall, which to-
gether with the poor performance of WSI systems
makes its paired F-Score difficult to beat. V-Measure
and paired F-Score are meaningful when systems
produce about the same numbers of clusters as the
numbers of classes and attain high scores on these
metrics.
FS(%) all nouns verbs #cl
MFS 63.5 57.0 72.7 1.00
NB 52.9 52.5 53.5 3.42
NB0 46.8 47.4 46.0 3.77
UoY 49.8 38.2 66.6 11.54
NMFlib 45.3 42.2 49.8 4.80
Hermit 26.7 24.4 30.1 10.78
Table 4: Unsupervised evaluation: paired F-Score
5.4.4 Supervised Recall
For the supervised task, the test data is split into
two groups: one for mapping clusters to classes and
the other for standard WSD evaluation. 2 differ-
ent split schemes (80% mapping, 20% evaluation and
60% mapping, 40% evaluation) are evaluated. 5 ran-
dom splits are averaged for each split scheme. Map-
ping is induced automatically by the program pro-
vided by organizers. See Table 5 for details of super-
vised recall evaluation (#s is the average number of
classes mapped from clusters).2
SR(%) all nouns verbs #s
NB 65.4 62.6 69.5 1.72
NB0 63.5 59.8 69.0 1.76
NMFlib 62.6 57.3 70.2 1.82
UoY 62.4 59.4 66.8 1.51
MFS 58.7 53.2 66.6 1.00
Hermit 58.3 53.6 65.3 2.06
Table 5: Supervised evaluation: supervised recall, 80%
mapping and 20% evaluation
Overall our system performs better than other sys-
tems with respect to supervised recall. When a sys-
tem has higher V-Measure and paired F-Score on
nouns than another system, it achieves a higher su-
pervised recall on nouns too. However, this behav-
ior is not observed on verbs. For example, NB has
higher V-Measure and paired F-Score on verbs than
NMFlib but NB attains a lower supervised recall on
verbs than NMFlib. It is difficult to see which verbs
clusters are better than some other clusters.
6 Conclusion
Of the four SemEval-2010 evaluation metrics, and
restricting ourselves to systems obeying the evalua-
tion conditions for that competition, our new model
achieves new best results on three. The exception is
paired F-Score. As we note earlier, this metric tends
to assign very high scores when every word receives
only one sense, and our model is bested by the base-
line system that does exactly that.
260-40 split is omitted here due to almost identical result.
1436
If we loosen possible comparison systems, the
LDA/HDP model of Lau et al (2012) achieves supe-
rior numbers to ours for the two supervised metrics,
but at the expense of requiring LDA type processing
on the test data, something that the SemEval or-
ganizers ruled out, presumably with the reasonable
idea that such processing would not be feasible in
the real world. More generally, their system assigns
many senses (about 10) to each word, and thus no-
doubt does poorly on the paired F-Score (they do not
report results on V-Measure and paired F-Score).
References
Eneko Agirre and Aitor Soroa. 2007. Semeval-2007 task
02: Evaluating word sense induction and discrimina-
tion systems. In Proceedings of the 4th International
Workshop on Semantic Evaluations, pages 7?12. Asso-
ciation for Computational Linguistics.
David M Blei, Andrew Y Ng, and Michael I Jordan. 2003.
Latent dirichlet alocation. the Journal of machine
Learning research, 3:993?1022.
Samuel Brody and Mirella Lapata. 2009. Bayesian word
sense induction. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for
Computational Linguistics, EACL ?09, pages 103?111,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Arthur P Dempster, Nan M Laird, and Donald B Ru-
bin. 1977. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical
Society. Series B (Methodological), pages 1?38.
Jason Eisner and Damianos Karakos. 2005. Bootstrap-
ping without the boot. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, HLT ?05, pages
395?402, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David Jurgens and Keith Stevens. 2010. Hermit: Flex-
ible clustering for the semeval-2 wsi task. In Pro-
ceedings of the 5th international workshop on semantic
evaluation, pages 359?362. Association for Computa-
tional Linguistics.
Ioannis Korkontzelos and Suresh Manandhar. 2010. Uoy:
Graphs of unambiguous vertices for word sense induc-
tion and disambiguation. In Proceedings of the 5th
international workshop on semantic evaluation, pages
355?358. Association for Computational Linguistics.
Jey Han Lau, Paul Cook, Diana McCarthy, David New-
man, and Timothy Baldwin. 2012. Word sense induc-
tion for novel sense detection. In Proceedings of the
13th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics, pages 591?601.
Association for Computational Linguistics.
Suresh Manandhar, Ioannis P Klapaftis, Dmitriy Dli-
gach, and Sameer S Pradhan. 2010. Semeval-2010
task 14: Word sense induction & disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 63?68. Association for Com-
putational Linguistics.
Ted Pedersen. 2010. Duluth-wsi: Senseclusters applied
to the sense induction task of semeval-2. In Proceed-
ings of the 5th international workshop on semantic
evaluation, pages 363?366. Association for Computa-
tional Linguistics.
Tim Van de Cruys, Marianna Apidianaki, et al 2011.
Latent semantic word sense induction and disambigua-
tion. In Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Hu-
man Language Technologies (ACL/HLT), pages 1476?
1485.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceedings
of the 33rd annual meeting on Association for Compu-
tational Linguistics, ACL ?95, pages 189?196, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
1437
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 512?516,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Humans Require Context to Infer Ironic Intent
(so Computers Probably do, too)
Byron C. Wallace, Do Kook Choe, Laura Kertz and Eugene Charniak
Brown University
{byron wallace, do kook choe, laura kertz, eugene charniak}@brown.edu
Abstract
Automatically detecting verbal irony
(roughly, sarcasm) is a challenging task
because ironists say something other
than ? and often opposite to ? what they
actually mean. Discerning ironic intent
exclusively from the words and syntax
comprising texts (e.g., tweets, forum
posts) is therefore not always possible:
additional contextual information about
the speaker and/or the topic at hand is
often necessary. We introduce a new
corpus that provides empirical evidence
for this claim. We show that annota-
tors frequently require context to make
judgements concerning ironic intent, and
that machine learning approaches tend
to misclassify those same comments for
which annotators required additional
context.
1 Introduction & Motivation
This work concerns the task of detecting verbal
irony online. Our principal argument is that sim-
ple bag-of-words based text classification models
? which, when coupled with sufficient data, have
proven to be extremely successful for many natu-
ral language processing tasks (Halevy et al, 2009)
? are inadequate for irony detection. In this paper
we provide empirical evidence that context is often
necessary to recognize ironic intent.
This is consistent with the large body of prag-
matics/linguistics literature on irony and its us-
age, which has emphasized the role that context
plays in recognizing and decoding ironic utter-
ances (Grice, 1975; Clark and Gerrig, 1984; Sper-
ber and Wilson, 1981). But existing work on au-
tomatic irony detection ? reviewed in Section 2
? has not explicitly attempted to operationalize
such theories, and has instead relied on features
(mostly word counts) intrinsic to the texts that are
to be classified as ironic. These approaches have
achieved some success, but necessarily face an
upper-bound: the exact same sentence can be both
intended ironically and unironically, depending on
the context (including the speaker and the topic at
hand). Only obvious verbal ironies will be recog-
nizable from intrinsic features alone.
Here we provide empirical evidence for the
above claims. We also introduce a new annotated
corpus that will allow researchers to build models
that augment existing approaches to irony detec-
tion with contextual information regarding the text
(utterance) to be classified and its author. Briefly,
our contributions are summarized as follows.
? We introduce the first version of the reddit
irony corpus, composed of annotated com-
ments from the social news website reddit.
Each sentence in every comment in this cor-
pus has been labeled by three independent an-
notators as having been intended by the au-
thor ironically or not. This dataset is publicly
available.
1
? We provide empirical evidence that human
annotators consistently rely on contextual in-
formation to make ironic/unironic sentence
judgements.
? We show that the standard ?bag-of-words? ap-
proach to text classification fails to accurately
judge ironic intent on those cases for which
humans required additional context. This
suggests that, as humans require context to
make their judgements for this task, so too do
computers.
Our hope is that these observations and this
dataset will spur innovative new research on meth-
ods for verbal irony detection.
1
https://github.com/bwallace/
ACL-2014-irony
512
2 Previous Work
There has recently been a flurry of interesting
work on automatic irony detection (Tepperman
et al, 2006; Davidov et al, 2010; Carvalho et
al., 2009; Burfoot and Baldwin, 2009; Tsur et
al., 2010; Gonz?alez-Ib?a?nez et al, 2011; Filatova,
2012; Reyes et al, 2012; Lukin and Walker, 2013;
Riloff et al, 2013). In these works, verbal irony
detection has mostly been treated as a standard
text classification task, though with some innova-
tive approaches specific to detecting irony.
The most common data source used to experi-
ment with irony detection systems has been Twit-
ter (Reyes et al, 2012; Gonz?alez-Ib?a?nez et al,
2011; Davidov et al, 2010), though Amazon prod-
uct reviews have been used experimentally as well
(Tsur et al, 2010; Davidov et al, 2010; Reyes et
al., 2012; Filatova, 2012). Walker et al (2012)
also recently introduced the Internet Argument
Corpus (IAC), which includes a sarcasm label
(among others).
Some of the findings from these previous ef-
forts have squared with intuition: e.g., overzealous
punctuation (as in ?great idea!!!!?) is indicative of
ironic intent (Carvalho et al, 2009). Other works
have proposed novel approaches specifically for
irony detection: Davidov et al (2010), for ex-
ample, proposed a semi-supervised approach in
which they look for sentence templates indicative
of irony. Elsewhere, Riloff et al (2013) proposed
a method that exploits contrasting sentiment in the
same utterance to detect irony.
To our knowledge, however, no previous work
on irony detection has attempted to leverage
contextual information regarding the author or
speaker (external to the utterance). But this is nec-
essary in some cases, however. For example, in
the case of Amazon product reviews, knowing the
kinds of books that an individual typically likes
might inform our judgement: someone who tends
to read and review Dostoevsky is probably be-
ing ironic if she writes a glowing review of Twi-
light. Of course, many people genuinely do enjoy
Twilight and so if the review is written subtly it
will likely be difficult to discern the author?s in-
tent without this background. In the case of Twit-
ter, it is likely to be difficult to classify utterances
without considering the contextualizing exchange
of tweets (i.e., the conversation) to which they be-
long.
1
2
3
4
Figure 1: The web-based tool used by our annotators to la-
bel reddit comments. Enumerated interface elements are de-
scribed as follows: 1 the text of the comment to be anno-
tated ? sentences marked as ironic are highlighted; 2 buttons
to label sentences as ironic or unironic; 3 buttons to request
additional context (the embedding discussion thread or asso-
ciated webpage ? see Section 3.2); 4 radio button to provide
confidence in comment labels (low, medium or high).
3 Introducing the reddit Irony Dataset
Here we introduce the first version (? 1.0) of
our irony corpus. Reddit (http://reddit.
com) is a social-news website to which news
stories (and other links) are posted, voted on
and commented upon. The forum compo-
nent of reddit is extremely active: popular
posts often have well into 1000?s of user com-
ments. Reddit comprises ?sub-reddits?, which fo-
cus on specific topics. For example, http://
reddit.com/r/politics features articles
(and hence comments) centered around political
news. The current version of the corpus is avail-
able at: https://github.com/bwallace/
ACL-2014-irony. Data collection and annota-
tion is ongoing, so we will continue to release new
(larger) versions of the corpus in the future. The
present version comprises 3,020 annotated com-
ments scraped from the six subreddits enumerated
in Table 1. These comments in turn comprise a
total of 10,401 labeled sentences.
2
3.1 Annotation Process
Three university undergraduates independently
annotated each sentence in the corpus. More
specifically, annotators have provided binary ?la-
bels? for each sentence indicating whether or not
they (the annotator) believe it was intended by the
author ironically (or not). This annotation was
provided via a custom-built browser-based anno-
tation tool, shown in Figure 1.
We intentionally did not provide much guid-
ance to annotators regarding the criteria for what
2
We performed na??ve ?segmentation? of comments based
on punctuation.
513
sub-reddit (URL) description number of labeled comments
politics (r/politics) Political news and editorials; focus on the US. 873
conservative (r/conservative) A community for political conservatives. 573
progressive (r/progressive) A community for political progressives (liberals). 543
atheism (r/atheism) A community for non-believers. 442
Christianity (r/Christianity) News and viewpoints on the Christian faith. 312
technology (r/technology) Technology news and commentary. 277
Table 1: The six sub-reddits that we have downloaded comments from and the corresponding number of comments for which
we have acquired annotations in this ? version of the corpus. Note that we acquired labels at the sentence level, whereas the
counts above reflect comments, all of which contain at least one sentence.
constitutes an ?ironic? statement, for two reasons.
First, verbal irony is a notoriously slippery concept
(Gibbs and Colston, 2007) and coming up with an
operational definition to be consistently applied is
non-trivial. Second, we were interested in assess-
ing the extent of natural agreement between an-
notators for this task. The raw average agreement
between all annotators on all sentences is 0.844.
Average pairwise Cohen?s Kappa (Cohen, 1960)
is 0.341, suggesting fair to moderate agreement
(Viera and Garrett, 2005), as we might expect for
a subjective task like this one.
3.2 Context
Reddit is a good corpus for the irony detection
task in part because it provides a natural prac-
tical realization of the otherwise ill-defined con-
text for comments. In particular, each comment is
associated with a specific user (the author), and
we can view their previous comments. More-
over, comments are embedded within discussion
threads that pertain to the (usually external) con-
tent linked to in the corresponding submission (see
Figure 2). These pieces of information (previous
comments by the same user, the external link of
the embedding reddit thread, and the other com-
ments in this thread) constitute our context. All
of this is readily accessible. Labelers can opt to
request these pieces of context via the annotation
tool, and we record when they do so.
Consider the following example comment taken
from our dataset: ?Great idea on the talkathon
Cruz. Really made the republicans look like the
sane ones.? Did the author intend this statement
ironically, or was this a subtle dig on Senator
Ted Cruz? Without additional context it is diffi-
cult to know. And indeed, all three annotators re-
quested additional context for this comment. This
context at first suggests that the comment may
have been intended literally: it was posted in the
r/conservative subreddit (Ted Cruz is a conserva-
tive senator). But if we peruse the author?s com-
Figure 2: An illustrative reddit comment (highlighted). The
title (?Virginia Republican ...?) links to an article, providing
one example of contextualizing content. The conversational
thread in which this comment is embedded provides addi-
tional context. The comment in question was presumably in-
tended ironically, though without the aforementioned context
this would be difficult to conclude with any certainty.
ment history, we see that he or she repeatedly de-
rides Senator Cruz (e.g., writing ?Ted Cruz is no
Ronald Reagan. They aren?t even close.?). From
this contextual information, then, we can reason-
ably assume that the comment was intended iron-
ically (and all three annotators did so after assess-
ing the available contextual information).
4 Humans Need Context to Infer Irony
We explore the extent to which human annotators
rely on contextual information to decide whether
or not sentences were intended ironically. Recall
that our annotation tool allows labelers to request
additional context if they cannot make a decision
based on the comment text alone (Figure 1). On
average, annotators requested additional context
for 30% of comments (range across annotators of
12% to 56%). As shown in Figure 3, annotators
are consistently more confident once they have
consulted this information.
We tested for a correlation between these re-
quests for context and the final decisions regard-
ing whether comments contain at least one ironic
sentence. We denote the probability of at least one
annotator requesting additional context for com-
ment i by P (C
i
). We then model the probability
of this event as a linear function of whether or not
514
64
86
174
forced decision30 final decision
152
90
529
forced decision51 final decision
176
207
364
forced decision25 final decision
ironic ?ironic
ironic ?unironic
unironic ?unironic
unironic ?ironic
annotator 1
annotator 2 annotator 3
Figure 3: This plot illustrates the effect of viewing contextual information for three annotators (one table for each annotator).
For all comments for which these annotators requested context, we show forced (before viewing the requested contextual
content) and final (after) decisions regarding perceived ironic intent on behalf of the author. Each row shows one of four
possible decision sequences (e.g., a judgement of ironic prior to seeing context and unironic after). Numbers correspond to
counts of these sequences for each annotator (e.g., the first annotator changed their mind from ironic to unironic 86 times).
Cases that involve the annotator changing his or her mind are shown in red; those in which the annotator stuck with their initial
judgement are shown in blue. Color intensity is proportional to the average confidence judgements the annotator provided:
these are uniformly stronger after they have consulted contextualizing information. Note also that the context frequently results
in annotators changing their judgement.
any annotator labeled any sentence in comment i
as ironic. We code this via the indicator variable
I
i
which is 1 when comment i has been deemed
to contain an ironic sentence (by any of the three
annotators) and 0 otherwise.
logit{P (C
i
)} = ?
0
+ ?
1
I
i
(1)
We used the regression model shown in Equa-
tion 1, where ?
0
is an intercept and ?
1
captures
the correlation between requests for context for a
given comment and its ultimately being deemed
to contain at least one ironic sentence. We fit this
model to the annotated corpus, and found a signif-
icant correlation:
?
?
1
= 1.508 with a 95% confi-
dence interval of (1.326, 1.690); p < 0.001.
In other words, annotators request context sig-
nificantly more frequently for those comments
that (are ultimately deemed to) contain an ironic
sentence. This would suggest that the words
and punctuation comprising online comments
alone are not sufficient to distinguish ironic from
unironic comments. Despite this, most machine
learning based approaches to irony detection have
relied nearly exclusively on such intrinsic features.
5 Machines Probably do, too
We show that the misclassifications (with respect
to whether comments contain irony or not) made
by a standard text classification model signifi-
cantly correlate with those comments for which
human annotators requested additional context.
This provides evidence that bag-of-words ap-
proaches are insufficient for the general task of
irony detection: more context is necessary.
We implemented a baseline classification ap-
proach using vanilla token count features (binary
bag-of-words). We removed stop-words and lim-
ited the vocabulary to the 50,000 most frequently
occurring unigrams and bigrams. We added ad-
ditional binary features coding for the presence
of punctuational features, such as exclamation
points, emoticons (for example, ?;)?) and question
marks: previous work (Davidov et al, 2010; Car-
valho et al, 2009) has found that these are good
indicators of ironic intent.
For our predictive model, we used a linear-
kernel SVM (tuning the C parameter via grid-
search over the training dataset to maximize F1
score). We performed five-fold cross-validation,
recording the predictions y?
i
for each (held-out)
comment i. Average F1 score over the five-folds
was 0.383 with range (0.330, 0.412); mean recall
was 0.496 (0.446, 0.548) and average precision
was 0.315 (0.261, 0.380). The five most predictive
tokens were: !, yeah, guys, oh and shocked. This
represents reasonable performance (with intuitive
predictive tokens); but obviously there is quite a
bit of room for improvement.
3
We now explore empirically whether these mis-
classifications are made on the same comments for
which annotators requested context. To this end,
we introduce a variable M
i
for each comment i
such that M
i
= 1 if y?
i
6= y
i
, i.e., M
i
is an in-
3
Some of the recently proposed strategies mentioned in
Section 2 may improve performance here, but none of these
address the fundamental issue of context.
515
dicator variable that encodes whether or not the
classifier misclassified comment i. We then ran
a second regression in which the output variable
was the logit-transformed probability of the model
misclassifying comment i, i.e., P (M
i
). Here we
are interested in the correlation of the event that
one or more annotators requested additional con-
text for comment i (denoted by C
i
) and model mis-
classifications (adjusting for the comment?s true
label). Formally:
logit{P (M
i
)} = ?
0
+ ?
1
I
i
+ ?
2
C
i
(2)
Fitting this to the data, we estimated
?
?
2
= 0.971
with a 95% CI of (0.810, 1.133); p < 0.001. Put
another way, the model makes mistakes on those
comments for which annotators requested addi-
tional context (even after accounting for the an-
notator designation of comments).
6 Conclusions and Future Directions
We have described a new (publicly available) cor-
pus for the task of verbal irony detection. The
data comprises comments scraped from the so-
cial news website reddit. We recorded confidence
judgements and requests for contextualizing infor-
mation for each comment during annotation. We
analyzed this corpus to provide empirical evidence
that annotators quite often require context beyond
the comment under consideration to discern irony;
especially for those comments ultimately deemed
as being intended ironically. We demonstrated
that a standard token-based machine learning ap-
proach misclassified many of the same comments
for which annotators tend to request context.
We have shown that annotators rely on contex-
tual cues (in addition to word and grammatical fea-
tures) to discern irony and argued that this implies
computers should, too. The obvious next step is to
develop new machine learning models that exploit
the contextual information available in the corpus
we have curated (e.g., previous comments by the
same user, the thread topic).
7 Acknowledgement
This work was made possible by the Army Re-
search Office (ARO), grant #64481-MA.
References
C Burfoot and T Baldwin. 2009. Automatic satire de-
tection: are you having a laugh? In ACL-IJCNLP,
pages 161?164. ACL.
P Carvalho, L Sarmento, MJ Silva, and E de Oliveira.
2009. Clues for detecting irony in user-generated
contents: oh...!! it?s so easy;-). In CIKM workshop
on Topic-sentiment analysis for mass opinion, pages
53?56. ACM.
HH Clark and RJ Gerrig. 1984. On the pretense the-
ory of irony. Journal of Experimental Psychology,
113:121?126.
J Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20:37?46.
D Davidov, O Tsur, and A Rappoport. 2010. Semi-
supervised recognition of sarcastic sentences in twit-
ter and amazon. pages 107?116.
E Filatova. 2012. Irony and sarcasm: Corpus gener-
ation and analysis using crowdsourcing. In LREC,
volume 12, pages 392?398.
RW Gibbs and HL Colston. 2007. Irony in language
and thought: a cognitive science reader. Lawrence
Erlbaum.
R Gonz?alez-Ib?a?nez, S Muresan, and N Wacholder.
2011. Identifying sarcasm in twitter: a closer look.
In ACL, volume 2, pages 581?586. Citeseer.
HP Grice. 1975. Logic and conversation. 1975, pages
41?58.
A Halevy, P Norvig, and F Pereira. 2009. The unrea-
sonable effectiveness of data. Intelligent Systems,
IEEE, 24(2):8?12.
S Lukin and M Walker. 2013. Really? well. ap-
parently bootstrapping improves the performance of
sarcasm and nastiness classifiers for online dialogue.
NAACL, pages 30?40.
A Reyes, P Rosso, and T Veale. 2012. A multidimen-
sional approach for detecting irony in twitter. LREC,
pages 1?30.
E Riloff, A Qadir, P Surve, LD Silva, N Gilbert, and
R Huang. 2013. Sarcasm as contrast between a pos-
itive sentiment and negative situation. In EMNLP,
pages 704?714.
D Sperber and D Wilson. 1981. Irony and the use-
mention distinction. 1981.
J Tepperman, D Traum, and S Narayanan. 2006.
?Yeah Right?: Sarcasm Recognition for Spoken Di-
alogue Systems.
O Tsur, D Davidov, and A Rappoport. 2010. ICWSM-
a great catchy name: Semi-supervised recognition
of sarcastic sentences in online product reviews. In
AAAI Conference on Weblogs and Social Media.
AJ Viera and JM Garrett. 2005. Understanding in-
terobserver agreement: the kappa statistic. Family
Medicine, 37(5):360?363.
MA Walker, JEF Tree, P Anand, R Abbott, and J King.
2012. A corpus for research on deliberation and de-
bate. In LREC, pages 812?817.
516
