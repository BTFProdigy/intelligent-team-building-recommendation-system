Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 208?216,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Automatically Generating Wikipedia Articles:
A Structure-Aware Approach
Christina Sauper and Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{csauper,regina}@csail.mit.edu
Abstract
In this paper, we investigate an ap-
proach for creating a comprehensive tex-
tual overview of a subject composed of in-
formation drawn from the Internet. We use
the high-level structure of human-authored
texts to automatically induce a domain-
specific template for the topic structure of
a new overview. The algorithmic innova-
tion of our work is a method to learn topic-
specific extractors for content selection
jointly for the entire template. We aug-
ment the standard perceptron algorithm
with a global integer linear programming
formulation to optimize both local fit of
information into each topic and global co-
herence across the entire overview. The
results of our evaluation confirm the bene-
fits of incorporating structural information
into the content selection process.
1 Introduction
In this paper, we consider the task of automatically
creating a multi-paragraph overview article that
provides a comprehensive summary of a subject of
interest. Examples of such overviews include ac-
tor biographies from IMDB and disease synopses
from Wikipedia. Producing these texts by hand is
a labor-intensive task, especially when relevant in-
formation is scattered throughout a wide range of
Internet sources. Our goal is to automate this pro-
cess. We aim to create an overview of a subject ?
e.g., 3-M Syndrome ? by intelligently combining
relevant excerpts from across the Internet.
As a starting point, we can employ meth-
ods developed for multi-document summarization.
However, our task poses additional technical chal-
lenges with respect to content planning. Gen-
erating a well-rounded overview article requires
proactive strategies to gather relevant material,
such as searching the Internet. Moreover, the chal-
lenge of maintaining output readability is mag-
nified when creating a longer document that dis-
cusses multiple topics.
In our approach, we explore how the high-
level structure of human-authored documents can
be used to produce well-formed comprehensive
overview articles. We select relevant material for
an article using a domain-specific automatically
generated content template. For example, a tem-
plate for articles about diseases might contain di-
agnosis, causes, symptoms, and treatment. Our
system induces these templates by analyzing pat-
terns in the structure of human-authored docu-
ments in the domain of interest. Then, it produces
a new article by selecting content from the Internet
for each part of this template. An example of our
system?s output1 is shown in Figure 1.
The algorithmic innovation of our work is a
method for learning topic-specific extractors for
content selection jointly across the entire template.
Learning a single topic-specific extractor can be
easily achieved in a standard classification frame-
work. However, the choices for different topics
in a template are mutually dependent; for exam-
ple, in a multi-topic article, there is potential for
redundancy across topics. Simultaneously learn-
ing content selection for all topics enables us to
explicitly model these inter-topic connections.
We formulate this task as a structured classifica-
tion problem. We estimate the parameters of our
model using the perceptron algorithm augmented
with an integer linear programming (ILP) formu-
lation, run over a training set of example articles
in the given domain.
The key features of this structure-aware ap-
proach are twofold:
1This system output was added to Wikipedia at http://
en.wikipedia.org/wiki/3-M syndrome on June
26, 2008. The page?s history provides examples of changes
performed by human editors to articles created by our system.
208
Diagnosis . . . No laboratories offering molecular genetic testing for prenatal diagnosis of 3-M syndrome are listed in the
GeneTests Laboratory Directory. However, prenatal testing may be available for families in which the disease-causing mutations
have been identified in an affected family member in a research or clinical laboratory.
Causes Three M syndrome is thought to be inherited as an autosomal recessive genetic trait. Human traits, including the classic
genetic diseases, are the product of the interaction of two genes, one received from the father and one from the mother. In recessive
disorders, the condition does not occur unless an individual inherits the same defective gene for the same trait from each parent. . . .
Symptoms . . .Many of the symptoms and physical features associated with the disorder are apparent at birth (congenital). In
some cases, individuals who carry a single copy of the disease gene (heterozygotes) may exhibit mild symptoms associated with
Three M syndrome.
Treatment . . . Genetic counseling will be of benefit for affected individuals and their families. Family members of affected indi-
viduals should also receive regular clinical evaluations to detect any symptoms and physical characteristics that may be potentially
associated with Three M syndrome or heterozygosity for the disorder. Other treatment for Three M syndrome is symptomatic and
supportive.
Figure 1: A fragment from the automatically created article for 3-M Syndrome.
? Automatic template creation: Templates
are automatically induced from human-
authored documents. This ensures that the
overview article will have the breadth ex-
pected in a comprehensive summary, with
content drawn from a wide variety of Inter-
net sources.
? Joint parameter estimation for content se-
lection: Parameters are learned jointly for
all topics in the template. This procedure op-
timizes both local relevance of information
for each topic and global coherence across
the entire article.
We evaluate our approach by creating articles in
two domains: Actors and Diseases. For a data set,
we use Wikipedia, which contains articles simi-
lar to those we wish to produce in terms of length
and breadth. An advantage of this data set is that
Wikipedia articles explicitly delineate topical sec-
tions, facilitating structural analysis. The results
of our evaluation confirm the benefits of structure-
aware content selection over approaches that do
not explicitly model topical structure.
2 Related Work
Concept-to-text generation and text-to-text gener-
ation take very different approaches to content se-
lection. In traditional concept-to-text generation,
a content planner provides a detailed template for
what information should be included in the output
and how this information should be organized (Re-
iter and Dale, 2000). In text-to-text generation,
such templates for information organization are
not available; sentences are selected based on their
salience properties (Mani and Maybury, 1999).
While this strategy is robust and portable across
domains, output summaries often suffer from co-
herence and coverage problems.
In between these two approaches is work on
domain-specific text-to-text generation. Instances
of these tasks are biography generation in sum-
marization and answering definition requests in
question-answering. In contrast to a generic sum-
marizer, these applications aim to characterize
the types of information that are essential in a
given domain. This characterization varies greatly
in granularity. For instance, some approaches
coarsely discriminate between biographical and
non-biographical information (Zhou et al, 2004;
Biadsy et al, 2008), while others go beyond binary
distinction by identifying atomic events ? e.g., oc-
cupation and marital status ? that are typically in-
cluded in a biography (Weischedel et al, 2004;
Filatova and Prager, 2005; Filatova et al, 2006).
Commonly, such templates are specified manually
and are hard-coded for a particular domain (Fujii
and Ishikawa, 2004; Weischedel et al, 2004).
Our work is related to these approaches; how-
ever, content selection in our work is driven by
domain-specific automatically induced templates.
As our experiments demonstrate, patterns ob-
served in domain-specific training data provide
sufficient constraints for topic organization, which
is crucial for a comprehensive text.
Our work also relates to a large body of recent
work that uses Wikipedia material. Instances of
this work include information extraction, ontology
induction and resource acquisition (Wu and Weld,
2007; Biadsy et al, 2008; Nastase, 2008; Nastase
and Strube, 2008). Our focus is on a different task
? generation of new overview articles that follow
the structure of Wikipedia articles.
209
3 Method
The goal of our system is to produce a compre-
hensive overview article given a title ? e.g., Can-
cer. We assume that relevant information on the
subject is available on the Internet but scattered
among several pages interspersed with noise.
We are provided with a training corpus consist-
ing of n documents d1 . . . dn in the same domain
? e.g., Diseases. Each document di has a title and
a set of delineated sections2 si1 . . . sim. The num-
ber of sectionsm varies between documents. Each
section sij also has a corresponding heading hij ?
e.g., Treatment.
Our overview article creation process consists
of three parts. First, a preprocessing step creates
a template and searches for a number of candidate
excerpts from the Internet. Next, parameters must
be trained for the content selection algorithm us-
ing our training data set. Finally, a complete ar-
ticle may be created by combining a selection of
candidate excerpts.
1. Preprocessing (Section 3.1) Our prepro-
cessing step leverages previous work in topic
segmentation and query reformulation to pre-
pare a template and a set of candidate ex-
cerpts for content selection. Template gen-
eration must occur once per domain, whereas
search occurs every time an article is gener-
ated in both learning and application.
(a) Template Induction To create a con-
tent template, we cluster all section
headings hi1 . . . him for all documents
di. Each cluster is labeled with the most
common heading hij within the clus-
ter. The largest k clusters are selected to
become topics t1 . . . tk, which form the
domain-specific content template.
(b) Search For each document that we
wish to create, we retrieve from the In-
ternet a set of r excerpts ej1 . . . ejr for
each topic tj from the template. We de-
fine appropriate search queries using the
requested document title and topics tj .
2. Learning Content Selection (Section 3.2)
For each topic tj , we learn the corresponding
topic-specific parameters wj to determine the
2In data sets where such mark-up is not available, one can
employ topical segmentation algorithms as an additional pre-
processing step.
quality of a given excerpt. Using the percep-
tron framework augmented with an ILP for-
mulation for global optimization, the system
is trained to select the best excerpt for each
document di and each topic tj . For train-
ing, we assume the best excerpt is the original
human-authored text sij .
3. Application (Section 3.2) Given the title of
a requested document, we select several ex-
cerpts from the candidate vectors returned by
the search procedure (1b) to create a com-
prehensive overview article. We perform the
decoding procedure jointly using learned pa-
rameters w1 . . .wk and the same ILP formu-
lation for global optimization as in training.
The result is a new document with k excerpts,
one for each topic.
3.1 Preprocessing
Template Induction A content template speci-
fies the topical structure of documents in one do-
main. For instance, the template for articles about
actors consists of four topics t1 . . . t4: biography,
early life, career, and personal life. Using this
template to create the biography of a new actor
will ensure that its information coverage is con-
sistent with existing human-authored documents.
We aim to derive these templates by discovering
common patterns in the organization of documents
in a domain of interest. There has been a sizable
amount of research on structure induction ranging
from linear segmentation (Hearst, 1994) to content
modeling (Barzilay and Lee, 2004). At the core
of these methods is the assumption that fragments
of text conveying similar information have simi-
lar word distribution patterns. Therefore, often a
simple segment clustering across domain texts can
identify strong patterns in content structure (Barzi-
lay and Elhadad, 2003). Clusters containing frag-
ments from many documents are indicative of top-
ics that are essential for a comprehensive sum-
mary. Given the simplicity and robustness of this
approach, we utilize it for template induction.
We cluster all section headings hi1 . . . him from
all documents di using a repeated bisectioning
algorithm (Zhao et al, 2005). As a similarity
function, we use cosine similarity weighted with
TF*IDF. We eliminate any clusters with low in-
ternal similarity (i.e., smaller than 0.5), as we as-
sume these are ?miscellaneous? clusters that will
not yield unified topics.
210
We determine the average number of sections
k over all documents in our training set, then se-
lect the k largest section clusters as topics. We or-
der these topics as t1 . . . tk using a majority order-
ing algorithm (Cohen et al, 1998). This algorithm
finds a total order among clusters that is consistent
with a maximal number of pairwise relationships
observed in our data set.
Each topic tj is identified by the most frequent
heading found within the cluster ? e.g., Causes.
This set of topics forms the content template for a
domain.
Search To retrieve relevant excerpts, we must
define appropriate search queries for each topic
t1 . . . tk. Query reformulation is an active area of
research (Agichtein et al, 2001). We have exper-
imented with several of these methods for draw-
ing search queries from representative words in the
body text of each topic; however, we find that the
best performance is provided by deriving queries
from a conjunction of the document title and topic
? e.g., ?3-M syndrome? diagnosis.
Using these queries, we search using Yahoo!
and retrieve the first ten result pages for each topic.
From each of these pages, we extract all possible
excerpts consisting of chunks of text between stan-
dardized boundary indicators (such as <p> tags).
In our experiments, there are an average of 6 ex-
cerpts taken from each page. For each topic tj of
each document we wish to create, the total number
of excerpts r found on the Internet may differ. We
label the excerpts ej1 . . . ejr.
3.2 Selection Model
Our selection model takes the content template
t1 . . . tk and the candidate excerpts ej1 . . . ejr for
each topic tj produced in the previous steps. It
then selects a series of k excerpts, one from each
topic, to create a coherent summary.
One possible approach is to perform individ-
ual selections from each set of excerpts ej1 . . . ejr
and then combine the results. This strategy is
commonly used in multi-document summariza-
tion (Barzilay et al, 1999; Goldstein et al, 2000;
Radev et al, 2000), where the combination step
eliminates the redundancy across selected ex-
cerpts. However, separating the two steps may not
be optimal for this task ? the balance between
coverage and redundancy is harder to achieve
when a multi-paragraph summary is generated. In
addition, a more discriminative selection strategy
is needed when candidate excerpts are drawn di-
rectly from the web, as they may be contaminated
with noise.
We propose a novel joint training algorithm that
learns selection criteria for all the topics simulta-
neously. This approach enables us to maximize
both local fit and global coherence. We implement
this algorithm using the perceptron framework, as
it can be easily modified for structured prediction
while preserving convergence guarantees (Daume?
III and Marcu, 2005; Snyder and Barzilay, 2007).
In this section, we first describe the structure
and decoding procedure of our model. We then
present an algorithm to jointly learn the parame-
ters of all topic models.
3.2.1 Model Structure
The model inputs are as follows:
? The title of the desired document
? t1 . . . tk ? topics from the content template
? ej1 . . . ejr ? candidate excerpts for each
topic tj
In addition, we define feature and parameter
vectors:
? ?(ejl) ? feature vector for the lth candidate
excerpt for topic tj
? w1 . . .wk ? parameter vectors, one for each
of the topics t1 . . . tk
Our model constructs a new article by following
these two steps:
Ranking First, we attempt to rank candidate
excerpts based on how representative they are of
each individual topic. For each topic tj , we induce
a ranking of the excerpts ej1 . . . ejr by mapping
each excerpt ejl to a score:
scorej(ejl) = ?(ejl) ?wj
Candidates for each topic are ranked from high-
est to lowest score. After this procedure, the posi-
tion l of excerpt ejl within the topic-specific can-
didate vector is the excerpt?s rank.
Optimizing the Global Objective To avoid re-
dundancy between topics, we formulate an opti-
mization problem using excerpt rankings to create
the final article. Given k topics, we would like to
select one excerpt ejl for each topic tj , such that
the rank is minimized; that is, scorej(ejl) is high.
To select the optimal excerpts, we employ inte-
ger linear programming (ILP). This framework is
211
commonly used in generation and summarization
applications where the selection process is driven
by multiple constraints (Marciniak and Strube,
2005; Clarke and Lapata, 2007).
We represent excerpts included in the output
using a set of indicator variables, xjl. For each
excerpt ejl, the corresponding indicator variable
xjl = 1 if the excerpt is included in the final doc-
ument, and xjl = 0 otherwise.
Our objective is to minimize the ranks of the
excerpts selected for the final document:
min
k?
j=1
r?
l=1
l ? xjl
We augment this formulation with two types of
constraints.
Exclusivity Constraints We want to ensure that
exactly one indicator xjl is nonzero for each topic
tj . These constraints are formulated as follows:
r?
l=1
xjl = 1 ?j ? {1 . . . k}
Redundancy Constraints We also want to pre-
vent redundancy across topics. We define
sim(ejl, ej?l?) as the cosine similarity between ex-
cerpts ejl from topic tj and ej?l? from topic tj? .
We introduce constraints that ensure no pair of ex-
cerpts has similarity above 0.5:
(xjl + xj?l?) ? sim(ejl, ej?l?) ? 1
?j, j? = 1 . . . k ?l, l? = 1 . . . r
If excerpts ejl and ej?l? have cosine similarity
sim(ejl, ej?l?) > 0.5, only one excerpt may be
selected for the final document ? i.e., either xjl
or xj?l? may be 1, but not both. Conversely, if
sim(ejl, ej?l?) ? 0.5, both excerpts may be se-
lected.
Solving the ILP Solving an integer linear pro-
gram is NP-hard (Cormen et al, 1992); however,
in practice there exist several strategies for solving
certain ILPs efficiently. In our study, we employed
lp solve,3 an efficient mixed integer programming
solver which implements the Branch-and-Bound
algorithm. On a larger scale, there are several al-
ternatives to approximate the ILP results, such as a
dynamic programming approximation to the knap-
sack problem (McDonald, 2007).
3http://lpsolve.sourceforge.net/5.5/
Feature Value
UNI wordi count of word occurrences
POS wordi first position of word in excerpt
BI wordi wordi+1 count of bigram occurrences
SENT count of all sentences
EXCL count of exclamations
QUES count of questions
WORD count of all words
NAME count of title mentions
DATE count of dates
PROP count of proper nouns
PRON count of pronouns
NUM count of numbers
FIRST word1 1?
FIRST word1 word2 1?
SIMS count of similar excerpts?
Table 1: Features employed in the ranking model.
? Defined as the first unigram in the excerpt.
? Defined as the first bigram in the excerpt.
? Defined as excerpts with cosine similarity > 0.5
Features As shown in Table 1, most of the fea-
tures we select in our model have been employed
in previous work on summarization (Mani and
Maybury, 1999). All features except the SIMS
feature are defined for individual excerpts in isola-
tion. For each excerpt ejl, the value of the SIMS
feature is the count of excerpts ejl? in the same
topic tj for which sim(ejl, ejl?) > 0.5. This fea-
ture quantifies the degree of repetition within a
topic, often indicative of an excerpt?s accuracy and
relevance.
3.2.2 Model Training
Generating Training Data For training, we are
given n original documents d1 . . . dn, a content
template consisting of topics t1 . . . tk, and a set of
candidate excerpts eij1 . . . eijr for each document
di and topic tj . For each section of each docu-
ment, we add the gold excerpt sij to the corre-
sponding vector of candidate excerpts eij1 . . . eijr.
This excerpt represents the target for our training
algorithm. Note that the algorithm does not re-
quire annotated ranking data; only knowledge of
this ?optimal? excerpt is required. However, if
the excerpts provided in the training data have low
quality, noise is introduced into the system.
Training Procedure Our algorithm is a
modification of the perceptron ranking algo-
rithm (Collins, 2002), which allows for joint
learning across several ranking problems (Daume?
III and Marcu, 2005; Snyder and Barzilay, 2007).
Pseudocode for this algorithm is provided in
Figure 2.
First, we define Rank(eij1 . . . eijr,wj), which
212
ranks all excerpts from the candidate excerpt
vector eij1 . . . eijr for document di and topic
tj . Excerpts are ordered by scorej(ejl) using
the current parameter values. We also define
Optimize(eij1 . . . eijr), which finds the optimal
selection of excerpts (one per topic) given ranked
lists of excerpts eij1 . . . eijr for each document di
and topic tj . These functions follow the ranking
and optimization procedures described in Section
3.2.1. The algorithm maintains k parameter vec-
tors w1 . . .wk, one associated with each topic tj
desired in the final article. During initialization,
all parameter vectors are set to zeros (line 2).
To learn the optimal parameters, this algorithm
iterates over the training set until the parameters
converge or a maximum number of iterations is
reached (line 3). For each document in the train-
ing set (line 4), the following steps occur: First,
candidate excerpts for each topic are ranked (lines
5-6). Next, decoding through ILP optimization is
performed over all ranked lists of candidate ex-
cerpts, selecting one excerpt for each topic (line
7). Finally, the parameters are updated in a joint
fashion. For each topic (line 8), if the selected
excerpt is not similar enough to the gold excerpt
(line 9), the parameters for that topic are updated
using a standard perceptron update rule (line 10).
When convergence is reached or the maximum it-
eration count is exceeded, the learned parameter
values are returned (line 12).
The use of ILP during each step of training
sets this algorithm apart from previous work. In
prior research, ILP was used as a postprocess-
ing step to remove redundancy and make other
global decisions about parameters (McDonald,
2007; Marciniak and Strube, 2005; Clarke and La-
pata, 2007). However, in our training, we inter-
twine the complete decoding procedure with the
parameter updates. Our joint learning approach
finds per-topic parameter values that are maxi-
mally suited for the global decoding procedure for
content selection.
4 Experimental Setup
We evaluate our method by observing the quality
of automatically created articles in different do-
mains. We compute the similarity of a large num-
ber of articles produced by our system and sev-
eral baselines to the original human-authored arti-
cles using ROUGE, a standard metric for summary
quality. In addition, we perform an analysis of edi-
Input:
d1 . . . dn: A set of n documents, each containing
k sections si1 . . . sik
eij1 . . . eijr: Sets of candidate excerpts for each topic
tj and document di
Define:
Rank(eij1 . . . eijr,wj):
As described in Section 3.2.1:
Calculates scorej(eijl) for all excerpts for
document di and topic tj , using parameterswj .
Orders the list of excerpts by scorej(eijl)
from highest to lowest.
Optimize(ei11 . . . eikr):
As described in Section 3.2.1:
Finds the optimal selection of excerpts to form a
final article, given ranked lists of excerpts
for each topic t1 . . . tk.
Returns a list of k excerpts, one for each topic.
?(eijl):
Returns the feature vector representing excerpt eijl
Initialization:
1 For j = 1 . . . k
2 Set parameterswj = 0
Training:
3 Repeat until convergence or while iter < itermax:
4 For i = 1 . . . n
5 For j = 1 . . . k
6 Rank(eij1 . . . eijr,wj)
7 x1 . . . xk = Optimize(ei11 . . . eikr)
8 For j = 1 . . . k
9 If sim(xj , sij) < 0.8
10 wj = wj + ?(sij)? ?(xi)
11 iter = iter + 1
12 Return parametersw1 . . .wk
Figure 2: An algorithm for learning several rank-
ing problems with a joint decoding mechanism.
tor reaction to system-produced articles submitted
to Wikipedia.
Data For evaluation, we consider two domains:
American Film Actors and Diseases. These do-
mains have been commonly used in prior work
on summarization (Weischedel et al, 2004; Zhou
et al, 2004; Filatova and Prager, 2005; Demner-
Fushman and Lin, 2007; Biadsy et al, 2008). Our
text corpus consists of articles drawn from the cor-
responding categories in Wikipedia. There are
2,150 articles in American Film Actors and 523
articles in Diseases. For each domain, we ran-
domly select 90% of articles for training and test
on the remaining 10%. Human-authored articles
in both domains contain an average of four top-
ics, and each topic contains an average of 193
words. In order to model the real-world scenario
where Wikipedia articles are not always available
(as for new or specialized topics), we specifically
exclude Wikipedia sources during our search pro-
213
Avg. Excerpts Avg. Sources
Amer. Film Actors
Search 2.3 1
No Template 4 4.0
Disjoint 4 2.1
Full Model 4 3.4
Oracle 4.3 4.3
Diseases
Search 3.1 1
No Template 4 2.5
Disjoint 4 3.0
Full Model 4 3.2
Oracle 5.8 3.9
Table 2: Average number of excerpts selected and
sources used in article creation for test articles.
cedure (Section 3.1) for evaluation.
Baselines Our first baseline, Search, relies
solely on search engine ranking for content selec-
tion. Using the article title as a query ? e.g., Bacil-
lary Angiomatosis, this method selects the web
page that is ranked first by the search engine. From
this page we select the first k paragraphs where k
is defined in the same way as in our full model. If
there are less than k paragraphs on the page, all
paragraphs are selected, but no other sources are
used. This yields a document of comparable size
with the output of our system. Despite its sim-
plicity, this baseline is not naive: extracting ma-
terial from a single document guarantees that the
output is coherent, and a page highly ranked by a
search engine may readily contain a comprehen-
sive overview of the subject.
Our second baseline, No Template, does not
use a template to specify desired topics; there-
fore, there are no constraints on content selection.
Instead, we follow a simplified form of previous
work on biography creation, where a classifier is
trained to distinguish biographical text (Zhou et
al., 2004; Biadsy et al, 2008).
In this case, we train a classifier to distinguish
domain-specific text. Positive training data is
drawn from all topics in the given domain cor-
pus. To find negative training data, we perform
the search procedure as in our full model (see
Section 3.1) using only the article titles as search
queries. Any excerpts which have very low sim-
ilarity to the original articles are used as negative
examples. During the decoding procedure, we use
the same search procedure. We then classify each
excerpt as relevant or irrelevant and select the k
non-redundant excerpts with the highest relevance
confidence scores.
Our third baseline, Disjoint, uses the ranking
perceptron framework as in our full system; how-
ever, rather than perform an optimization step
during training and decoding, we simply select
the highest-ranked excerpt for each topic. This
equates to standard linear classification for each
section individually.
In addition to these baselines, we compare
against an Oracle system. For each topic present
in the human-authored article, the Oracle selects
the excerpt from our full model?s candidate ex-
cerpts with the highest cosine similarity to the
human-authored text. This excerpt is the optimal
automatic selection from the results available, and
therefore represents an upper bound on our excerpt
selection task. Some articles contain additional
topics beyond those in the template; in these cases,
the Oracle system produces a longer article than
our algorithm.
Table 2 shows the average number of excerpts
selected and sources used in articles created by our
full model and each baseline.
Automatic Evaluation To assess the quality of
the resulting overview articles, we compare them
with the original human-authored articles. We
use ROUGE, an evaluation metric employed at the
Document Understanding Conferences (DUC),
which assumes that proximity to human-authored
text is an indicator of summary quality. We
use the publicly available ROUGE toolkit (Lin,
2004) to compute recall, precision, and F-score for
ROUGE-1. We use theWilcoxon Signed Rank Test
to determine statistical significance.
Analysis of Human Edits In addition to our auto-
matic evaluation, we perform a study of reactions
to system-produced articles by the general pub-
lic. To achieve this goal, we insert automatically
created articles4 into Wikipedia itself and exam-
ine the feedback of Wikipedia editors. Selection
of specific articles is constrained by the need to
find topics which are currently of ?stub? status that
have enough information available on the Internet
to construct a valid article. After a period of time,
we analyzed the edits made to the articles to deter-
mine the overall editor reaction. We report results
on 15 articles in the Diseases category5.
4In addition to the summary itself, we also include proper
citations to the sources from which the material is extracted.
5We are continually submitting new articles; however, we
report results on those that have at least a 6 month history at
time of writing.
214
Recall Precision F-score
Amer. Film Actors
Search 0.09 0.37 0.13 ?
No Template 0.33 0.50 0.39 ?
Disjoint 0.45 0.32 0.36 ?
Full Model 0.46 0.40 0.41
Oracle 0.48 0.64 0.54 ?
Diseases
Search 0.31 0.37 0.32 ?
No Template 0.32 0.27 0.28 ?
Disjoint 0.33 0.40 0.35 ?
Full Model 0.36 0.39 0.37
Oracle 0.59 0.37 0.44 ?
Table 3: Results of ROUGE-1 evaluation.
? Significant with respect to our full model for p ? 0.05.
? Significant with respect to our full model for p ? 0.10.
Since Wikipedia is a live resource, we do not
repeat this procedure for our baseline systems.
Adding articles from systems which have previ-
ously demonstrated poor quality would be im-
proper, especially in Diseases. Therefore, we
present this analysis as an additional observation
rather than a rigorous technical study.
5 Results
Automatic Evaluation The results of this evalu-
ation are shown in Table 3. Our full model outper-
forms all of the baselines. By surpassing the Dis-
joint baseline, we demonstrate the benefits of joint
classification. Furthermore, the high performance
of both our full model and the Disjoint baseline
relative to the other baselines shows the impor-
tance of structure-aware content selection. The
Oracle system, which represents an upper bound
on our system?s capabilities, performs well.
The remaining baselines have different flaws:
Articles produced by the No Template baseline
tend to focus on a single topic extensively at the
expense of breadth, because there are no con-
straints to ensure diverse topic selection. On the
other hand, performance of the Search baseline
varies dramatically. This is expected; this base-
line relies heavily on both the search engine and
individual web pages. The search engine must cor-
rectly rank relevant pages, and the web pages must
provide the important material first.
Analysis of Human Edits The results of our ob-
servation of editing patterns are shown in Table
4. These articles have resided on Wikipedia for
a period of time ranging from 5-11 months. All
of them have been edited, and no articles were re-
moved due to lack of quality. Moreover, ten au-
tomatically created articles have been promoted
Type Count
Total articles 15
Promoted articles 10
Edit types
Intra-wiki links 36
Formatting 25
Grammar 20
Minor topic edits 2
Major topic changes 1
Total edits 85
Table 4: Distribution of edits on Wikipedia.
by human editors from stubs to regular Wikipedia
entries based on the quality and coverage of the
material. Information was removed in three cases
for being irrelevant, one entire section and two
smaller pieces. The most common changes were
small edits to formatting and introduction of links
to other Wikipedia articles in the body text.
6 Conclusion
In this paper, we investigated an approach for cre-
ating a multi-paragraph overview article by select-
ing relevant material from the web and organiz-
ing it into a single coherent text. Our algorithm
yields significant gains over a structure-agnostic
approach. Moreover, our results demonstrate the
benefits of structured classification, which out-
performs independently trained topical classifiers.
Overall, the results of our evaluation combined
with our analysis of human edits confirm that the
proposed method can effectively produce compre-
hensive overview articles.
This work opens several directions for future re-
search. Diseases and American Film Actors ex-
hibit fairly consistent article structures, which are
successfully captured by a simple template cre-
ation process. However, with categories that ex-
hibit structural variability, more sophisticated sta-
tistical approaches may be required to produce ac-
curate templates. Moreover, a promising direction
is to consider hierarchical discourse formalisms
such as RST (Mann and Thompson, 1988) to sup-
plement our template-based approach.
Acknowledgments
The authors acknowledge the support of the NSF (CA-
REER grant IIS-0448168, grant IIS-0835445, and grant IIS-
0835652) and NIH (grant V54LM008748). Thanks to Mike
Collins, Julia Hirschberg, and members of the MIT NLP
group for their helpful suggestions and comments. Any opin-
ions, findings, conclusions, or recommendations expressed in
this paper are those of the authors, and do not necessarily re-
flect the views of the funding organizations.
215
References
Eugene Agichtein, Steve Lawrence, and Luis Gravano. 2001.
Learning search engine specific query transformations for
question answering. In Proceedings of WWW, pages 169?
178.
Regina Barzilay and Noemie Elhadad. 2003. Sentence align-
ment for monolingual comparable corpora. In Proceed-
ings of EMNLP, pages 25?32.
Regina Barzilay and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to genera-
tion and summarization. In Proceedings of HLT-NAACL,
pages 113?120.
Regina Barzilay, Kathleen R. McKeown, and Michael El-
hadad. 1999. Information fusion in the context of multi-
document summarization. In Proceedings of ACL, pages
550?557.
Fadi Biadsy, Julia Hirschberg, and Elena Filatova. 2008.
An unsupervised approach to biography production using
wikipedia. In Proceedings of ACL/HLT, pages 807?815.
James Clarke and Mirella Lapata. 2007. Modelling com-
pression with discourse constraints. In Proceedings of
EMNLP-CoNLL, pages 1?11.
William W. Cohen, Robert E. Schapire, and Yoram Singer.
1998. Learning to order things. In Proceedings of NIPS,
pages 451?457.
Michael Collins. 2002. Ranking algorithms for named-entity
extraction: Boosting and the voted perceptron. In Pro-
ceedings of ACL, pages 489?496.
Thomas H. Cormen, Charles E. Leiserson, and Ronald L.
Rivest. 1992. Intoduction to Algorithms. The MIT Press.
Hal Daume? III and Daniel Marcu. 2005. A large-scale explo-
ration of effective global features for a joint entity detec-
tion and tracking model. In Proceedings of HLT/EMNLP,
pages 97?104.
Dina Demner-Fushman and Jimmy Lin. 2007. Answer-
ing clinical questions with knowledge-based and statisti-
cal techniques. Computational Linguistics, 33(1):63?103.
Elena Filatova and John M. Prager. 2005. Tell me what you
do and I?ll tell you what you are: Learning occupation-
related activities for biographies. In Proceedings of
HLT/EMNLP, pages 113?120.
Elena Filatova, Vasileios Hatzivassiloglou, and Kathleen
McKeown. 2006. Automatic creation of domain tem-
plates. In Proceedings of ACL, pages 207?214.
Atsushi Fujii and Tetsuya Ishikawa. 2004. Summarizing en-
cyclopedic term descriptions on the web. In Proceedings
of COLING, page 645.
Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark
Kantrowitz. 2000. Multi-document summarization by
sentence extraction. In Proceedings of NAACL-ANLP,
pages 40?48.
Marti A. Hearst. 1994. Multi-paragraph segmentation of ex-
pository text. In Proceedings of ACL, pages 9?16.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proceedings of ACL, pages
74?81.
Inderjeet Mani and Mark T. Maybury. 1999. Advances in
Automatic Text Summarization. The MIT Press.
William C. Mann and Sandra A. Thompson. 1988. Rhetor-
ical structure theory: Toward a functional theory of text
organization. Text, 8(3):243?281.
Tomasz Marciniak and Michael Strube. 2005. Beyond the
pipeline: Discrete optimization in NLP. In Proceedings
of CoNLL, pages 136?143.
Ryan McDonald. 2007. A study of global inference algo-
rithms in multi-document summarization. In Proceedings
of EICR, pages 557?564.
Vivi Nastase and Michael Strube. 2008. Decoding wikipedia
categories for knowledge acquisition. In Proceedings of
AAAI, pages 1219?1224.
Vivi Nastase. 2008. Topic-driven multi-document summa-
rization with encyclopedic knowledge and spreading acti-
vation. In Proceedings of EMNLP, pages 763?772.
Dragomir R. Radev, Hongyan Jing, and Malgorzata
Budzikowska. 2000. Centroid-based summarization
of multiple documents: sentence extraction, utility-
based evaluation, and user studies. In Proceedings of
ANLP/NAACL, pages 21?29.
Ehud Reiter and Robert Dale. 2000. Building Natural Lan-
guage Generation Systems. Cambridge University Press,
Cambridge.
Benjamin Snyder and Regina Barzilay. 2007. Multiple as-
pect ranking using the good grief algorithm. In Proceed-
ings of HLT-NAACL, pages 300?307.
Ralph M. Weischedel, Jinxi Xu, and Ana Licuanan. 2004. A
hybrid approach to answering biographical questions. In
New Directions in Question Answering, pages 59?70.
Fei Wu and Daniel S. Weld. 2007. Autonomously semanti-
fying wikipedia. In Proceedings of CIKM, pages 41?50.
Ying Zhao, George Karypis, and Usama Fayyad. 2005.
Hierarchical clustering algorithms for document datasets.
Data Mining and Knowledge Discovery, 10(2):141?168.
L. Zhou, M. Ticrea, and Eduard Hovy. 2004. Multi-
document biography summarization. In Proceedings of
EMNLP, pages 434?441.
216
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 377?387,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Incorporating Content Structure into Text Analysis Applications
Christina Sauper, Aria Haghighi, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
{csauper, aria42, regina}@csail.mit.edu
Abstract
In this paper, we investigate how modeling
content structure can benefit text analysis ap-
plications such as extractive summarization
and sentiment analysis. This follows the lin-
guistic intuition that rich contextual informa-
tion should be useful in these tasks. We
present a framework which combines a su-
pervised text analysis application with the in-
duction of latent content structure. Both of
these elements are learned jointly using the
EM algorithm. The induced content struc-
ture is learned from a large unannotated cor-
pus and biased by the underlying text analysis
task. We demonstrate that exploiting content
structure yields significant improvements over
approaches that rely only on local context.1
1 Introduction
In this paper, we demonstrate that leveraging doc-
ument structure significantly benefits text analysis
applications. As a motivating example, consider
the excerpt from a DVD review shown in Table 1.
This review discusses multiple aspects of a product,
such as audio and video properties. While the word
?pleased? is a strong indicator of positive sentiment,
the sentence in which it appears does not specify the
aspect to which it relates. Resolving this ambiguity
requires information about global document struc-
ture.
A central challenge in utilizing such informa-
tion lies in finding a relevant representation of con-
tent structure for a specific text analysis task. For
1Code and processed data presented here are available at
http://groups.csail.mit.edu/rbg/code/content structure.html
Audio Audio choices are English, Spanish and French
Dolby Digital 5.1 ... Bass is still robust and powerful,
giving weight to just about any scene ? most notably
the film?s exciting final fight. Fans should be pleased
with the presentation.
Extras This single-disc DVD comes packed in a black
amaray case with a glossy slipcover. Cover art has
clearly been designed to appeal the Twilight crowd ...
Finally, we?ve got a deleted scenes reel. Most of the
excised scenes are actually pretty interesting.
Table 1: An excerpt from a DVD review.
instance, when performing single-aspect sentiment
analysis, the most relevant aspect of content struc-
ture is whether a given sentence is objective or sub-
jective (Pang and Lee, 2004). In a multi-aspect
setting, however, information about the sentence
topic is required to determine the aspect to which
a sentiment-bearing word relates (Snyder and Barzi-
lay, 2007). As we can see from even these closely re-
lated applications, the content structure representa-
tion should be intimately tied to a specific text anal-
ysis task.
In this work, we present an approach in which a
content model is learned jointly with a text analy-
sis task. We assume complete annotations for the
task itself, but we learn the content model from raw,
unannotated text. Our approach is implemented in
a discriminative framework using latent variables to
represent facets of content structure. In this frame-
work, the original task features (e.g., lexical ones)
are conjoined with latent variables to enrich the fea-
tures with global contextual information. For ex-
ample, in Table 1, the feature associated with the
377
word ?pleased? should contribute most strongly to
the sentiment of the audio aspect when it is aug-
mented with a relevant topic indicator.
The coupling of the content model and the task-
specific model allows the two components to mutu-
ally influence each other during learning. The con-
tent model leverages unannotated data to improve
the performance of the task-specific model, while
the task-specific model provides feedback to im-
prove the relevance of the content model. The com-
bined model can be learned effectively using a novel
EM-based method for joint training.
We evaluate our approach on two complementary
text analysis tasks. Our first task is a multi-aspect
sentiment analysis task, where a system predicts the
aspect-specific sentiment ratings (Snyder and Barzi-
lay, 2007). Second, we consider a multi-aspect ex-
tractive summarization task in which a system ex-
tracts key properties for a pre-specified set of as-
pects. On both tasks, our method for incorporating
content structure consistently outperforms structure-
agnostic counterparts. Moreover, jointly learning
content and task parameters yields additional gains
over independently learned models.
2 Related Work
Prior research has demonstrated the usefulness of
content models for discourse-level tasks. Examples
of such tasks include sentence ordering (Barzilay
and Lee, 2004; Elsner et al, 2007), extraction-based
summarization (Haghighi and Vanderwende, 2009)
and text segmentation (Chen et al, 2009). Since
these tasks are inherently tied to document structure,
a content model is essential to performing them suc-
cessfully. In contrast, the applications considered in
this paper are typically developed without any dis-
course information, focusing on capturing sentence-
level relations. Our goal is to augment these models
with document-level content information.
Several applications in information extraction
and sentiment analysis are close in spirit to our
work (Pang and Lee, 2004; Patwardhan and Riloff,
2007; McDonald et al, 2007). These approaches
consider global contextual information when de-
termining whether a given sentence is relevant to
the underlying analysis task. All assume that rele-
vant sentences have been annotated. For instance,
Pang and Lee (2004) refine the accuracy of sen-
timent analysis by considering only the subjective
sentences of a review as determined by an indepen-
dent classifier. Patwardhan and Riloff (2007) take
a similar approach in the context of information ex-
traction. Rather than applying their extractor to all
the sentences in a document, they limit it to event-
relevant sentences. Since these sentences are more
likely to contain information of interest, the extrac-
tion performance increases.
Another approach, taken by Choi and Cardie
(2008) and Somasundaran et al (2009) uses lin-
guistic resources to create a latent model in a task-
specific fashion to improve performance, rather than
assuming sentence-level task relevancy. Choi and
Cardie (2008) address a sentiment analysis task by
using a heuristic decision process based on word-
level intermediate variables to represent polarity.
Somasundaran et al (2009) similarly uses a boot-
strapped local polarity classifier to identify sentence
polarity.
McDonald et al (2007) propose a model
which jointly identifies global polarity as well as
paragraph- and sentence-level polarity, all of which
are observed in training data. While our approach
uses a similar hierarchy, McDonald et al (2007) is
concerned with recovering the labels at all levels,
whereas in this work we are interested in using la-
tent document content structure as a means to benefit
task predictions.
While our method also incorporates contextual
information into existing text analysis applications,
our approach is markedly different from the above
approaches. First, our representation of context en-
codes more than the relevance-based binary distinc-
tion considered in the past work. Our algorithm ad-
justs the content model dynamically for a given task
rather than pre-specifying it. Second, while previ-
ous work is fully supervised, in our case relevance
annotations are readily available for only a few ap-
plications and are prohibitively expensive to obtain
for many others. To overcome this drawback, our
method induces a content model in an unsupervised
fashion and connects it via latent variables to the
target model. This design not only eliminates the
need for additional annotations, but also allows the
algorithm to leverage large quantities of raw data for
training the content model. The tight coupling of rel-
378
evance learning with the target analysis task leads to
further performance gains.
Finally, our work relates to supervised topic mod-
els in Blei and McAullife (2007). In this work, la-
tent topic variables are used to generate text as well
as a supervised sentiment rating for the document.
However, this architecture does not permit the usage
of standard discriminative models which condition
freely on textual features.
3 Model
3.1 Problem Formulation
In this section, we describe a model which incorpo-
rates content information into a multi-aspect sum-
marization task.2 Our approach assumes that at
training time we have a collection of labeled doc-
uments DL, each consisting of the document text
s and true task-specific labeling y?. For the multi-
aspect summarization task, y? consists of sequence
labels (e.g., value or service) for the tokens of a
document. Specifically, the document text s is
composed of sentences s1, . . . , sn and the label-
ings y? consists of corresponding label sequences
y1, . . . , yn.3
As is common in related work, we model each yi
using a CRF which conditions on the observed doc-
ument text. In this work, we also assume a content
model, which we fix to be the document-level HMM
as used in Barzilay and Lee (2004). In this content
model, each sentence si is associated with a hidden
topic variable Ti which generates the words of the
sentence. We will use T = (T1, . . . , Tn) to refer to
the hidden topic sequence for a document. We fix
the number of topics to a pre-specified constant K.
3.2 Model Overview
Our model, depicted in Figure 1, proceeds as fol-
lows: First the document-level HMM generates
a hidden content topic sequence T for the sen-
tences of a document. This content component is
parametrized by ? and decomposes in the standard
2In Section 3.6, we discuss how this framework can be used
for other text analysis applications.
3Note that each yi is a label sequence across the words in si,
rather than an individual label.
y
1
i
y
m
i
y
2
i
. . .
T
i
w
1
i
w
m
i
w
2
i
. . .
T
i?1
T
i+1
(w
2
i
= pleased) ? (T
i
= 3)
w
2
i
= pleased
...
s
i
Figure 1: A graphical depiction of our model for
sequence labeling tasks. The Ti variable represents
the content model topic for the ith sentence si. The
words of si, (w1i , . . . , w
m
i ), each have a task label
(y
1
i , . . . , y
m
i ). Note that each token label has an
undirected edge to a factor containing the words of
the current sentence, si as well as the topic of the
current sentence Ti.
HMM fashion:4
P?(s,T ) =
n?
i=1
P?(Ti|Ti?1)
?
w?si
P?(w|Ti) (1)
Then the label sequences for each sentence in
the document are independently modeled as CRFs
which condition on both the sentence features and
the sentence topic:
P?(y|s,T ) =
n?
i=1
P?(yi|si, Ti) (2)
Each sentence CRF is parametrized by ? and takes
the standard form:
P?(y|s, T ) ?
exp
?
?
?
?
j
?
T [
fN (y
j
, s, T ) + fE(y
j
, y
j+1
)
]
?
?
?
4We also utilize a hierarchical emission model so that each
topic distribution interpolates between a topic-specific distribu-
tion as well as a shared background model; this is intended to
capture domain-specific stop words.
379
Ts
y
?
?
?
Content
Parameters
Task
Parameters
Task Labels
Text
Content
Structure
Figure 2: A graphical depiction of the generative
process for a labeled document at training time (See
Section 3); shaded nodes indicate variables which
are observed at training time. First the latent un-
derlying content structure T is drawn. Then, the
document text s is drawn conditioned on the content
structure utilizing content parameters ?. Finally, the
observed task labels for the document are modeled
given s and T using the task parameters ?. Note that
the arrows for the task labels are undirected since
they are modeled discriminatively.
where fN (?) and fE(?) are feature functions associ-
ated with CRF nodes and transitions respectively.
Allowing the CRF to condition on the sentence
topic Ti permits predictions to be more sensitive to
content. For instance, using the example from Ta-
ble 1, we could have a feature that indicates the word
?pleased? conjoined with the segment topic (see Fig-
ure 1). These topic-specific features serve to disam-
biguate word usage.
This joint process, depicted graphically in Fig-
ure 2, is summarized as:
P (T , s,y?) = P?(T , s)P?(y
?
|s,T ) (3)
Note that this probability decomposes into a
document-level HMM term (the content component)
as well as a product of CRF terms (the task compo-
nent).
3.3 Learning
During learning, we would like to find the
document-level HMM parameters ? and the summa-
rization task CRF parameters ? which maximize the
likelihood of the labeled documents. The only ob-
served elements of a labeled document are the docu-
ment text s and the aspect labels y?. This objective
is given by:
LL(?, ?) =
?
(s,y?)?DL
logP (s,y?)
=
?
(s,y?)?DL
log
?
T
P (T , s,y?)
We use the EM algorithm to optimize this objec-
tive.
E-Step The E-Step in EM requires computing the
posterior distribution over latent variables. In this
model, the only latent variables are the sentence top-
ics T . To compute this term, we utilize the decom-
position in Equation (3) and rearrange HMM and
CRF terms to obtain:
P (T , s,y?) = P?(T , s)P?(y
?
|T , s)
=
(
n?
i=1
P?(Ti|Ti?1)
?
w?si
P?(w|Ti)
)
?
(
n?
i=1
P?(y
?
i |si, Ti)
)
=
n?
i=1
P?(Ti|Ti?1)?
(
?
w?si
P?(w|Ti)P?(y
?
i |si, Ti)
)
We note that this expression takes the same form as
the document-level HMM, except that in addition to
emitting the words of a sentence, we also have an
observation associated with the sentence sequence
labeling. We treat each P?(y?i |si, Ti) as part of the
node potential associated with the document-level
HMM. We utilize the Forward-Backward algorithm
as one would with the document-level HMM in iso-
lation, except that each node potential incorporates
this CRF term.
M-Step We perform separate M-Steps for content
and task parameters. The M-Step for the content pa-
rameters is identical to the document-level HMM
380
content model: topic emission and transition dis-
tributions are updated with expected counts derived
from E-Step topic posteriors.
The M-Step for the task parameters does not have
a closed-form solution. Recall that in the M-Step,
we maximize the log probability of all random vari-
ables given expectations of latent variables. Using
the decomposition in Equation (3), it is clear that
the only component of the joint labeled document
probability which relies upon the task parameters is
logP?(y?|s,T ). Thus for the M-Step, it is sufficient
to optimize the following with respect to ?:
ET |s,y? logP?(y
?
|s,T )
=
n?
i=1
E
T
i
|s
i
, y
?
i
logP?(y
?
i |si, Ti)
=
n?
i=1
K?
k=1
P (Ti = k|si, y
?
i ) logP?(y
?
i |si, Ti)
The first equality follows from the decomposition
of the task component into independent CRFs (see
Equation (2)). Optimizing this objective is equiva-
lent to a weighted version of the conditional likeli-
hood objective used to train the CRF in isolation. An
intuitive explanation of this process is that there are
multiple CRF instances, one for each possible hid-
den topic T . Each utilizes different content features
to explain the sentence sequence labeling. These in-
stances are weighted according to the posterior over
T obtained during the E-Step. While this objective
is non-convex due to the summation over T , we can
still optimize it using any gradient-based optimiza-
tion solver; in our experiments, we used the LBFGS
algorithm (Liu et al, 1989).
3.4 Inference
We must predict a label sequence y for each sen-
tence s of the document. We assume a loss function
over a sequence labeling y and a proposed labeling
y?, which decomposes as:
L(y, y?) =
?
j
L(y
j
, y?
j
)
where each position loss is sensitive to the kind of
error which is made. Failing to extract a token is
penalized to a greater extent than extracting it with
an incorrect label:
L(y
j
, y?
j
) =
?
??
??
0 if y?j = yj
c if yj 6= NONE and y?j = NONE
1 otherwise
In this definition, NONE represents the background
label which is reserved for tokens which do not cor-
respond to labels of interest. The constant c repre-
sents a user-defined trade-off between precision and
recall errors. For our multi-aspect summarization
task, we select c = 4 for Yelp and c = 5 for Amazon
to combat the high-precision bias typical of condi-
tional likelihood models.
At inference time, we select the single labeling
which minimizes the expected loss with respect to
model posterior over label sequences:
y? = min
y?
E
y|sL(y, y?)
= min
y?
?
j=1
E
y
j
|sL(y
j
, y?
j
)
In our case, we must marginalize out the sentence
topic T :
P (y
j
|s) =
?
T
P (y
j
, T |s)
=
?
T
P?(T |s)P?(y
j
|s, T )
This minimum risk criterion has been widely used in
NLP applications such as parsing (Goodman, 1999)
and machine translation (DeNero et al, 2009). Note
that the above formulation differs from the stan-
dard CRF due to the latent topic variables. Other-
wise the inference task could be accomplished by
directly obtaining posteriors over each yj state using
the Forward-Backwards algorithm on the sentence
CRF.
Finding y? can be done efficiently. First, we ob-
tain marginal token posteriors as above. Then, the
expected loss of a token prediction is computed as
follows: ?
y?j
P (y
j
|s)L(y
j
, y?
j
)
Once we obtain expected losses of each token pre-
diction, we compute the minimum risk sequence la-
beling by running the Viterbi algorithm. The po-
tential for each position and prediction is given by
381
the negative expected loss. The maximal scoring se-
quence according to these potentials minimizes the
expected risk.
3.5 Leveraging unannotated data
Our model allows us to incorporate unlabeled doc-
uments, denoted DU , to improve the learning of the
content model. For an unlabeled document we only
observe the document text s and assume it is drawn
from the same content model as our labeled docu-
ments. The objective presented in Section 3.3 as-
sumed that all documents were labeled; here we sup-
plement this objective by capturing the likelihood
of unlabeled documents according to the content
model:
LU (?) =
?
s?DU
logP?(s)
=
?
s?DU
log
?
T
P?(s,T )
Our overall objective function is to maximize the
likelihood of both our labeled and unlabeled data.
This objective corresponds to:
L(?, ?) =LU (?) + LL(?, ?)
This objective can also be optimized using the EM
algorithm, where the E-Step for labeled and unla-
beled documents is outlined above.
3.6 Generalization
The approach outlined can be applied to a wider
range of task components. For instance, in Sec-
tion 4.1 we apply this approach to multi-aspect sen-
timent analysis. In this task, the target y consists of
numeric sentiment ratings (y1, . . . , yK) for each of
K aspects. The task component consists of indepen-
dent linear regression models for each aspect sen-
timent rating. For the content model, we associate
a topic with each paragraph; T consists of assign-
ments of topics to each document paragraph.
The model structure still decomposes as in Fig-
ure 2, but the details of learning are slightly differ-
ent. For instance, because the task label (aspect sen-
timent ratings) is not localized to any region of the
document, all content model variables influence the
target response. Conditioned on the target label, all
topic variables become correlated. Thus when learn-
ing, the E-Step requires computing a posterior over
paragraph topic tuples T :
P (T |y, s) ? P (s,T )P (y|T , s)
For the case of our multi-aspect sentiment task, this
computation can be done exactly by enumerating
T tuples, since the number of sentences and pos-
sible topics is relatively small. If summation is in-
tractable, the posterior may be approximated using
variational techniques (Bishop, 2006), which is ap-
plicable to a broad range of potential applications.
4 Experimental Set-Up
We apply our approach to two text analysis tasks that
stand to benefit from modeling content structure:
multi-aspect sentiment analysis and multi-aspect re-
view summarization.
4.1 Tasks
In the following section, we define each task in de-
tail, explain the task-specific adaptation of the model
and describe the data sets used in the experiments.
Table 2 summarizes statistics for all the data sets.
For all tasks, when using a content model with a
task model, we utilize a new set of features which
include all the original features as well as a copy
of each feature conjoined with the content topic as-
signment (see Figure 1). We also include a fea-
ture which indicates whether a given word was most
likely emitted from the underlying topic or from a
background distribution.
Multi-Aspect Sentiment Ranking The goal of
multi-aspect sentiment classification is to predict a
set of numeric ranks that reflects the user satisfaction
for each aspect (Snyder and Barzilay, 2007). One of
the challenges in this task is to attribute sentiment-
bearing words to the aspects they describe. Informa-
tion about document structure has the potential to
greatly reduce this ambiguity.
Following standard sentiment ranking ap-
proaches (Wilson et al, 2004; Pang and Lee, 2005;
Goldberg and Zhu, 2006; Snyder and Barzilay,
2007), we employ ordinary linear regression to
independently map bag-of-words representations
into predicted aspect ranks. In addition to com-
monly used lexical features, this set is augmented
382
Task
Labeled
Unlabeled
Avg. Size
Train Test Words Sents
Multi-aspect sentiment 600 65 ? 1,027 20.5
Multi-aspect summarization
Amazon 35 24 12,684 214 11.7
Yelp 48 48 33,015 178 11.2
Table 2: This table summarizes the size of each corpus. In each case, the unlabeled texts of both labeled and
unlabeled documents are used for training the content model, while only the labeled training corpus is used
to train the task model. Note that the entire data set for the multi-aspect sentiment analysis task is labeled.
with content features as described above. For this
application, we fix the number of HMM states to be
equal to the predefined number of aspects.
We test our sentiment ranker on a set of DVD re-
views from the website IGN.com.5 Each review is
accompanied by 1-10 scale ratings in four categories
that assess the quality of a movie?s content, video,
audio, and DVD extras. In this data set, segments
corresponding to each of the aspects are clearly de-
lineated in each document. Therefore, we can com-
pare the performance of the algorithm using auto-
matically induced content models against the gold
standard structural information.
Multi-Aspect Review Summarization The goal
of this task is to extract informative phrases that
identify information relevant to several predefined
aspects of interest. In other words, we would like our
system to both extract important phrases (e.g., cheap
food) and label it with one of the given aspects (e.g.,
value). For concrete examples and lists of aspects
for each data set, see Figures 3b and 3c. Variants of
this task have been considered in review summariza-
tion in previous work (Kim and Hovy, 2006; Brana-
van et al, 2009).
This task has elements of both information extrac-
tion and phrase-based summarization ? the phrases
we wish to extract are broader in scope than in stan-
dard template-driven IE, but at the same time, the
type of selected information is restricted to the de-
fined aspects, similar to query-based summarization.
The difficulty here is that phrase selection is highly
context-dependent. For instance, in TV reviews such
as in Figure 3b, the highlighted phrase ?easy to read?
might refer to either the menu or the remote; broader
5http://dvd.ign.com/index/reviews.html
context is required for correct labeling.
We evaluated our approach for this task on two
data sets: Amazon TV reviews (Figure 3b) and Yelp
restaurant reviews (Figure 3c). To eliminate noisy
reviews, we only retain documents that have been
rated ?helpful? by the users of the site; we also re-
move reviews which are abnormally short or long.
Each data set was manually annotated with aspect
labels using Mechanical Turk, which has been used
in previous work to annotate NLP data (Snow et al,
2008). Since we cannot select high-quality annota-
tors directly, we included a control document which
had been previously annotated by a native speaker
among the documents assigned to each annotator.
The work of any annotator who exhibited low agree-
ment on the control document annotation was ex-
cluded from the corpus. To test task annotation
agreement, we use Cohen?s Kappa (Cohen, 1960).
On the Amazon data set, two native speakers anno-
tated a set of four documents. The agreement be-
tween the judges was 0.54. On the Yelp data set, we
simply computed the agreement between all pairs of
reviewers who received the same control documents;
the agreement was 0.49.
4.2 Baseline Comparison and Evaluation
Baselines For all the models, we obtain a baseline
system by eliminating content features and only us-
ing a task model with the set of features described
above. We also compare against a simplified vari-
ant of our method wherein a content model is in-
duced in isolation rather than learned jointly in the
context of the underlying task. In our experiments,
we refer to the two methods as the No Content
Model (NoCM) and Independent Content Model
(IndepCM) settings, respectively. The Joint Content
383
M = Movie
V = Video
A = Audio
E = Extras
M This collection certainly offers some nostalgic 
fun, but at the end of the day, the shows themselves, 
for the most part, just don't hold up. (5)
V Regardless, this is a fairly solid presentation, but 
it's obvious there was room for improvement.  (7)
A Bass is still robust and powerful. Fans should be 
pleased with this presentation. (8)
E The deleted scenes were quite lengthy, but only 
shelled out a few extra laughs. (4) 
(a) Sample labeled text from the multi-aspect sentiment corpus
[R Big multifunction remote] with [R easy-to-
read keys].   The on-screen menu is [M easy to 
use] and you [M can rename the inputs] to one 
of several options (DVD, Cable, etc.).
R = Remote
M = Menu
I = Inputs
E = Economy
V = Video
S = Sound
A = Appearance
F = Features
I bought this TV because the [V overall picture 
quality is good] and it's [A unbelievably thin].
[I Plenty of inputs], including [I 2 HDMI ports], 
which is [E unheard of in this price range].
(b) Sample labeled text from the Amazon multi-aspect summa-
rization corpus
[F All the ingredients are fresh], [V the sizes are 
huge] and [V the price is cheap]. 
F = Food
A = Atmosphere
V = Value
S = Service
O = Overall
[O This place rocks!]  [V Pricey, but worth it] .
[A The place is a pretty good size] and
[S the staff is super friendly].
(c) Sample labeled text from the Yelp multi-aspect summarization
corpus
Figure 3: Excerpts from the three corpora with the
corresponding labels. Note that sentences from the
multi-aspect summarization corpora generally focus
on only one or two aspects. The multi-aspect senti-
ment corpus has labels per paragraph rather than per
sentence.
Model (JointCM) setting refers to our full model de-
scribed in Section 3, where content and task compo-
nents are learned jointly.
Evaluation Metrics For multi-aspect sentiment
ranking, we report the average L2 (squared differ-
ence) and L1 (absolute difference) between system
prediction and true 1-10 sentiment rating across test
documents and aspects.
For the multi-aspect summarization task, we mea-
sure average token precision and recall of the label
assignments (Multi-label). For the Amazon corpus,
we also report a coarser metric which measures ex-
traction precision and recall while ignoring labels
(Binary labels) as well as ROUGE (Lin, 2004). To
compute ROUGE, we control for length by limiting
L1 L2
NoCM 1.37 3.15
IndepCM 1.28?* 2.80?*
JointCM 1.25? 2.65?*
Gold 1.18?* 2.48?*
Table 3: The error rate on the multi-aspect sentiment
ranking. We report mean L1 and L2 between system
prediction and true values over all aspects. Marked
results are statistically significant with p < 0.05: *
over the previous model and ? over NoCM.
F1 F2 Prec. Recall
NoCM 28.8% 34.8% 22.4% 40.3%
IndepCM 37.9% 43.7% 31.1%?* 48.6%?*
JointCM 39.2% 44.4% 32.9%?* 48.6%?
Table 4: Results for multi-aspect summarization on
the Yelp corpus. Marked precision and recall are
statistically significant with p < 0.05: * over the
previous model and ? over NoCM.
each system to predict the same number of tokens as
the original labeled document.
Our metrics of statistical significance vary by
task. For the sentiment task, we use Student?s t-
test. For the multi-aspect summarization task, we
perform chi-square analysis on the ROUGE scores
as well as on precision and recall separately, as
is commonly done in information extraction (Fre-
itag, 2004; Weeds et al, 2004; Finkel and Manning,
2009).
5 Results
In this section, we present the results of the methods
on the tasks described above (see Tables 3, 4, and 5).
Baseline Comparisons Adding a content model
significantly outperforms the NoCM baseline on
both tasks. The highest F1 error reduction ? 14.7%
? is achieved on multi-aspect summarization on the
Yelp corpus, followed by the reduction of 11.5% and
8.75%, on multi-aspect summarization on the Ama-
zon corpus and multi-aspect sentiment ranking, re-
spectively.
We also observe a consistent performance boost
when comparing against the IndepCM baseline.
This result confirms our hypothesis about the ad-
384
Multi-label Binary labels
F1 F2 Prec. Recall F1 F2 Prec. Recall ROUGE
NoCM 18.9% 18.0% 20.4% 17.5% 35.1% 33.6% 38.1% 32.6% 43.8%
IndepCM 24.5% 23.8% 25.8%?* 23.3%?* 43.0% 41.8% 45.3%?* 40.9%?* 47.4%?*
JointCM 28.2% 31.3% 24.3%? 33.7%?* 47.8% 53.0% 41.2%? 57.1%?* 47.6%?*
Table 5: Results for multi-aspect summarization on the Amazon corpus. Marked ROUGE, precision, and
recall are statistically significant with p < 0.05: * over the previous model and ? over NoCM.
vantages of jointly learning the content model in the
context of the underlying task.
Comparison with additional context features
One alternative to an explicit content model is to
simply incorporate additional features into NoCM
as a proxy for contextual information. In the
multi-aspect summarization case, this can be accom-
plished by adding unigram features from the sen-
tences before and after the current one.6
When testing this approach, however, the perfor-
mance of NoCM actually decreases on both Ama-
zon (to 15.0% F1) and Yelp (to 24.5% F1) corpora.
This result is not surprising for this particular task ?
by adding these features, we substantially increase
the feature space without increasing the amount of
training data. An advantage of our approach is
that our learned representation of context is coarse,
and we can leverage large quantities of unannotated
training data.
Impact of content model quality on task per-
formance In the multi-aspect sentiment ranking
task, we have access to gold standard document-
level content structure annotation. This affords us
the ability to compare the ideal content structure,
provided by the document authors, with one that is
learned automatically. As Table 3 shows, the manu-
ally created document structure segmentation yields
the best results. However, the performance of our
JointCM model is not far behind the gold standard
content structure.
The quality of the induced content model is de-
termined by the amount of training data. As Fig-
ure 4 shows, the multi-aspect summarizer improves
with the increase in the size of raw data available for
learning content model.
6This type of feature is not applicable to our multi-aspect
sentiment ranking task, as we already use unigram features from
the entire document.
10
20
30
0% 50% 100%
Multi
-label
 F 1
Percentage of unlabeled data
22.8 26.0
28.2
Figure 4: Results on the Amazon corpus using the
complete annotated set with varying amounts of ad-
ditional unlabeled data.7
Compensating for annotation sparsity We hy-
pothesize that by incorporating rich contextual in-
formation, we can reduce the need for manual task
annotation. We test this by reducing the amount of
annotated data available to the model and measur-
ing performance at several quantities of unannotated
data. As Figure 5 shows, the performance increase
achieved by doubling the amount of annotated data
can also be achieved by adding only 12.5% of the
unlabeled data.
6 Conclusion
In this paper, we demonstrate the benefits of incor-
porating content models in text analysis tasks. We
also introduce a framework to allow the joint learn-
ing of an unsupervised latent content model with a
supervised task-specific model. On multiple tasks
and datasets, our results empirically connect model
quality and task performance, suggesting that fur-
7Because we append the unlabeled versions of the labeled
data to the unlabeled set, even with 0% additional unlabeled
data, there is a small data set to train the content model.
385
10
12
32
0% 1352% 32%
Multi
-label
 F 1
Percentage of unlabeled data
30
2.82
608 66828
Figure 5: Results on the Amazon corpus using half
of the annotated training documents. The content
model is trained with 0%, 12.5%, and 25% of addi-
tional unlabeled data.7 The dashed horizontal line
represents NoCM with the complete annotated set.
ther improvements in content modeling may yield
even further gains.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168) and NIH (grant 5-
R01-LM009723-02). Thanks to Peter Szolovits and
the MIT NLP group for their helpful comments.
Any opinions, findings, conclusions, or recommen-
dations expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of the
funding organizations.
References
Regina Barzilay and Lillian Lee. 2004. Catching the
drift: Probabilistic content models, with applications
to generation and summarization. In Proceedings of
the NAACL/HLT, pages 113?120.
Christopher M. Bishop. 2006. Pattern Recognition and
Machine Learning (Information Science and Statis-
tics). Springer-Verlag New York, Inc.
David M. Blei and Jon D. McAullife. 2007. Supervised
Topic Models. In NIPS.
S. R. K. Branavan, Harr Chen, Jacob Eisenstein, and
Regina Barzilay. 2009. Learning document-level se-
mantic properties from free-text annotations. JAIR,
34:569?603.
Harr Chen, S. R. K. Branavan, Regina Barzilay, and
David R. Karger. 2009. Content modeling using la-
tent permutations. JAIR, 36:129?163.
Yejin Choi and Claire Cardie. 2008. Learning with com-
positional semantics as structural inference for sub-
sentential sentiment analysis. In Proceedings of the
EMNLP, pages 793?801.
J. Cohen. 1960. A Coefficient of Agreement for Nominal
Scales. Educational and Psychological Measurement,
20(1):37.
John DeNero, David Chiang, and Kevin Knight. 2009.
Fast consensus decoding over translation forests. In
Proceedings of the ACL/IJCNLP, pages 567?575.
Micha Elsner, Joseph Austerweil, and Eugene Charniak.
2007. A unified local and global model for discourse
coherence. In Proceedings of the NAACL/HLT, pages
436?443.
Jenny Rose Finkel and Christopher D. Manning. 2009.
Joint parsing and named entity recognition. In Pro-
ceedings of the NAACL.
Dayne Freitag. 2004. Trained named entity recogni-
tion using distributional clusters. In Proceedings of
the EMNLP, pages 262?269.
Andrew B. Goldberg and Xiaojin Zhu. 2006. See-
ing stars when there aren?t many stars: Graph-based
semi-supervised learning for sentiment categoriza-
tion. In Proceedings of the NAACL/HLT Workshop on
TextGraphs, pages 45?52.
Joshua Goodman. 1999. Semiring parsing. Computa-
tional Linguistics, 25(4):573?605.
Aria Haghighi and Lucy Vanderwende. 2009. Exploring
content models for multi-document summarization. In
Proceedings of the NAACL/HLT, pages 362?370.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of the COLING/ACL, pages 483?490.
Chin-Yew Lin. 2004. ROUGE: A package for automatic
evaluation of summaries. In Proceedings of the ACL,
pages 74?81.
Dong C. Liu, Jorge Nocedal, Dong C. Liu, and Jorge No-
cedal. 1989. On the limited memory bfgs method for
large scale optimization. Mathematical Programming,
45:503?528.
Ryan McDonald, Kerry Hannan, Tyler Neylon, Mike
Wells, and Jeff Reynar. 2007. Structured models for
fine-to-coarse sentiment analysis. In Proceedings of
the ACL, pages 432?439.
Bo Pang and Lillian Lee. 2004. A sentimental education:
Sentiment analysis using subjectivity summarization
based on minimum cuts. In Proceedings of the ACL,
pages 271?278.
Bo Pang and Lillian Lee. 2005. Seeing stars: Exploiting
class relationships for sentiment categorization with
respect to rating scales. In Proceedings of the ACL,
pages 115?124.
386
Siddharth Patwardhan and Ellen Riloff. 2007. Effec-
tive information extraction with semantic affinity pat-
terns and relevant regions. In Proceedings of the
EMNLP/CoNLL, pages 717?727.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast - but is it good?
evaluating non-expert annotations for natural language
tasks. In Proceedings of the EMNLP.
Benjamin Snyder and Regina Barzilay. 2007. Multiple
aspect ranking using the good grief algorithm. In Pro-
ceedings of the NAACL/HLT, pages 300?307.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the EMNLP, pages 170?179.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional simi-
larity. In Proceedings of the COLING, page 1015.
Theresa Wilson, Janyce Wiebe, and Rebecca Hwa. 2004.
Just how mad are you? finding strong and weak opin-
ion clauses. In Proceedings of the AAAI, pages 761?
769.
387
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 350?358,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Content Models with Attitude
Christina Sauper, Aria Haghighi, Regina Barzilay
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
csauper@csail.mit.edu, me@aria42.com, regina@csail.mit.edu
Abstract
We present a probabilistic topic model for
jointly identifying properties and attributes of
social media review snippets. Our model
simultaneously learns a set of properties of
a product and captures aggregate user senti-
ments towards these properties. This approach
directly enables discovery of highly rated or
inconsistent properties of a product. Our
model admits an efficient variational mean-
field inference algorithm which can be paral-
lelized and run on large snippet collections.
We evaluate our model on a large corpus of
snippets from Yelp reviews to assess property
and attribute prediction. We demonstrate that
it outperforms applicable baselines by a con-
siderable margin.
1 Introduction
Online product reviews have become an increasingly
valuable and influential source of information for
consumers. Different reviewers may choose to com-
ment on different properties or aspects of a product;
therefore their reviews focus on different qualities of
the product. Even when they discuss the same prop-
erties, their experiences and, subsequently, evalua-
tions of the product can differ dramatically. Thus,
information in any single review may not provide
a complete and balanced view representative of the
product as a whole. To address this need, online re-
tailers often use simple aggregation mechanisms to
represent the spectrum of user sentiment. For in-
stance, product pages on Amazon prominently dis-
play the distribution of numerical scores across re-
Coherent property cluster
+
The martinis were very good.
The drinks - both wine and martinis - were tasty.
-
The wine list was pricey.
Their wine selection is horrible.
Incoherent property cluster
+
The sushi is the best I?ve ever had.
Best paella I?d ever had.
The fillet was the best steak we?d ever had.
It?s the best soup I?ve ever had.
Table 1: Example clusters of restaurant review snippets.
The first cluster represents a coherent property of the un-
derlying product, namely the cocktail property, and as-
sesses distinctions in user sentiment. The latter cluster
simply shares a common attribute expression and does
not represent snippets discussing the same product prop-
erty. In this work, we aim to produce the first type of
property cluster with correct sentiment labeling.
views, providing access to reviews at different levels
of satisfaction.
The goal of our work is to provide a mechanism
for review content aggregation that goes beyond nu-
merical scores. Specifically, we are interested in
identifying fine-grained product properties across
reviews (e.g., battery life for electronics or pizza for
restaurants) as well as capturing attributes of these
properties, namely aggregate user sentiment.
For this task, we assume as input a set of prod-
uct review snippets (i.e., standalone phrases such as
?battery life is the best I?ve found?) rather than com-
plete reviews. There are many techniques for ex-
tracting this type of snippet in existing work; we use
the Sauper et al (2010) system.
350
At first glance, this task can be solved using ex-
isting methods for review analysis. These methods
can effectively extract product properties from indi-
vidual snippets along with their corresponding sen-
timent. While the resulting property-attribute pairs
form a useful abstraction for cross-review analysis,
in practice direct comparison of these pairs is chal-
lenging.
Consider, for instance, the two clusters of restau-
rant review snippets shown in Figure 1. While both
clusters have many words in common among their
members, only the first describes a coherent prop-
erty cluster, namely the cocktail property. The snip-
pets of the latter cluster do not discuss a single prod-
uct property, but instead share similar expressions
of sentiment. To solve this issue, we need a method
which can correctly identify both property and sen-
timent words.
In this work, we propose an approach that jointly
analyzes the whole collection of product review
snippets, induces a set of learned properties, and
models the aggregate user sentiment towards these
properties. We capture this idea using a Bayesian
topic model where a set of properties and corre-
sponding attribute tendencies are represented as hid-
den variables. The model takes product review snip-
pets as input and explains how the observed text
arises from the latent variables, thereby connecting
text fragments with corresponding properties and at-
tributes.
The advantages of this formulation are twofold.
First, this encoding provides a common ground for
comparing and aggregating review content in the
presence of varied lexical realizations. For instance,
this representation allows us to directly compare
how many reviewers liked a given property of a
product. Second, our model yields an efficient
mean-field variational inference procedure which
can be parallelized and run on a large number of re-
view snippets.
We evaluate our approach in the domain of snip-
pets taken from restaurant reviews on Yelp. In this
collection, each restaurant has on average 29.8 snip-
pets representing a wide spectrum of opinions about
a restaurant. The evaluation we present demon-
strates that the model can accurately retrieve clusters
of review fragments that describe the same property,
yielding 20% error reduction over a standalone clus-
tering baseline. We also show that the model can ef-
fectively identify binary snippet attributes with 9.2%
error reduction over applicable baselines, demon-
strating that learning to identify attributes in the con-
text of other product reviews yields significant gains.
Finally, we evaluate our model on its ability to iden-
tify product properties for which there is significant
sentiment disagreement amongst user snippets. This
tests our model?s capacity to jointly identify proper-
ties and assess attributes.
2 Related Work
Our work on review aggregation has connections to
three lines of work in text analysis.
First, our work relates to research on extraction of
product properties with associated sentiment from
review text (Hu and Liu, 2004; Liu et al, 2005a;
Popescu et al, 2005). These methods identify rele-
vant information in a document using a wide range
of methods such as association mining (Hu and Liu,
2004), relaxation labeling (Popescu et al, 2005) and
supervised learning (Kim and Hovy, 2006). While
our method also extracts product properties and sen-
timent, our focus is on multi-review aggregation.
This task introduces new challenges which were
not addressed in prior research that focused on per-
document analysis.
A second related line of research is multi-
document review summarization. Some of
these methods directly apply existing domain-
independent summarization methods (Seki et al,
2006), while others propose new methods targeted
for opinion text (Liu et al, 2005b; Carenini et al,
2006; Hu and Liu, 2006; Kim and Zhai, 2009). For
instance, these summaries may present contrastive
view points (Kim and Zhai, 2009) or relay average
sentiment (Carenini et al, 2006). The focus of this
line of work is on how to select suitable sentences,
assuming that relevant review features (such as nu-
merical scores) are given. Since our emphasis is on
multi-review analysis, we believe that the informa-
tion we extract can benefit existing summarization
systems.
Finally, a number of approaches analyze review
documents using probabilistic topic models (Lu and
Zhai, 2008; Titov and McDonald, 2008; Mei et al,
2007). While some of these methods focus primar-
351
ily on modeling ratable aspects (Titov and McDon-
ald, 2008), others explicitly capture the mixture of
topics and sentiments (Mei et al, 2007). These ap-
proaches are capable of identifying latent topics in
the collection in opinion text (e.g., weblogs) as well
as associated sentiment. While our model captures
similar high-level intuition, it analyzes fine-grained
properties expressed at the snippet level, rather than
document-level sentiment. Delivering analysis at
such a fine granularity requires a new technique.
3 Problem Formulation
In this section, we discuss the core random variables
and abstractions of our model. We describe the gen-
erative models over these elements in Section 4.
Product: A product represents a reviewable ob-
ject. For the experiments in this paper, we use
restaurants as products.
Snippets: A snippet is a user-generated short se-
quence of tokens describing a product. Input snip-
pets are deterministically taken from the output of
the Sauper et al (2010) system.
Property: A property corresponds to some fine-
grained aspect of a product. For instance, the snippet
?the pad thai was great? describes the pad thai prop-
erty. We assume that each snippet has a single prop-
erty associated with it. We assume a fixed number
of possible properties K for each product.
For the corpus of restaurant reviews, we assume
that the set of properties are specific to a given prod-
uct, in order to capture fine-grained, relevant proper-
ties for each restaurant. For example, reviews from a
sandwich shop may contrast the club sandwich with
the turkey wrap, while for a more general restau-
rant, the snippets refer to sandwiches in general. For
other domains where the properties are more consis-
tent, it is straightforward to alter our model so that
properties are shared across products.
Attribute: An attribute is a description of a prop-
erty. There are multiple attribute types, which may
correspond to semantic differences. We assume a
fixed, pre-specified number of attributes N . For
example, in the case of product reviews, we select
N = 2 attributes corresponding to positive and neg-
ative sentiment. In the case of information extrac-
tion, it may be beneficial to use numeric and alpha-
betic types.
One of the goals of this work in the review do-
main is to improve sentiment prediction by exploit-
ing correlations within a single property cluster. For
example, if there are already many snippets with the
attribute representing positive sentiment in a given
property cluster, additional snippets are biased to-
wards positive sentiment as well; however, data can
always override this bias.
Snippets themselves are always observed; the
goal of this work is to induce the latent property and
attribute underlying each snippet.
4 Model
Our model generates the words of all snippets for
each product in a collection of products. We use
s
i,j,w to represent the wth word of the jth snippet
of the ith product. We use s to denote the collec-
tion of all snippet words. We also assume a fixed
vocabulary of words V .
We present an overview of our generative model
in Figure 1 and describe each component in turn:
Global Distributions: At the global level, we
draw several unigram distributions: a global back-
ground distribution ?B and attribute distributions
?
a
A for each attribute. The background distribution
is meant to encode stop-words and domain white-
noise, e.g., food in the restaurants domain. In this
domain, the positive and negative attribute distribu-
tions encode words with positive and negative senti-
ments (e.g., delicious or terrible).
Each of these distributions are drawn from Dirich-
let priors. The background distribution is drawn
from a symmetric Dirichlet with concentration
?B = 0.2. The positive and negative attribute dis-
tributions are initialized using seed words (Vseeda
in Figure 1). These seeds are incorporated into
the attribute priors: a non-seed word gets  hyper-
parameter and a seed word gets  + ?A, where
 = 0.25 and ?A = 1.0.
Product Level: For the ith product, we draw
property unigram distributions ?i,1P , . . . , ?
i,K
P for
each of the possibleK product properties. The prop-
erty distribution represents product-specific content
distributions over properties discussed in reviews of
the product; for instance in the restaurant domains,
properties may correspond to distinct menu items.
Each ?i,kP is drawn from a symmetric Dirichlet prior
352
Global Level:
- Draw background distribution ?B ? DIRICHLET(?BV )
- For each attribute type a,
- Draw attribute distribution ?aA ? DIRICHLET(V + ?AVseeda)
Product Level:
- For each product i,
- Draw property distributions ?kP ? DIRICHLET(?PV ) for k = 1, . . . ,K
- Draw property attribute binomial ?i,k ? BETA(?A, ?A) for k = 1, . . . ,K
- Draw property multinomial ?i ? DIRICHLET(?MK)
Snippet Level:
- For each snippet j in ith product,
- Draw snippet property Zi,jP ? ?i
- Draw snippet attribute Zi,jA ? ?Z
ij
P
- Draw sequence of word topic indicators Zi,j,wW ? ?|Z
i,j,w?1
W
- Draw snippet word given property Zi,jP and attribute Z
i,j
A
si,j,w ?
?
???
???
?
i,Zi,jP
P , when Z
i,j,w
W = P
?
Zi,jA
A , when Z
i,j,w
W = A
?B, when Zi,j,wW = B
?
B
?
a
A
?
?
k
Z
i?1
W
Z
i
W
Z
i+1
W
w
i?1
w
i
w
i+1
HMM over snippet words
Background word 
distribution 
Attribute word 
distributions 
Product
Snippet
Z
P
Z
A
Property
multinomial
Property attribute 
binomials
?
k
P
Property word
distributions
Property
Snippet attributeSnippet property
?
a
A
Z
P
, ?
P
Z
A
, ?
A
?
B
Attribute
Figure 1: A high-level verbal and graphical description for our model in Section 4. We use DIRICHLET(?V ) to denote
a finite Dirichlet prior where the hyper-parameter counts are a scalar times the unit vector of vocabulary items. For
the global attribute distribution, the prior hyper-parameter counts are  for all vocabulary items and ?
A
for V
seeda , the
vector of vocabulary items in the set of seed words for attribute a.
with hyper-parameter ?P = 0.2.
For each property k = 1, . . . ,K. ?i,k, we draw a
binomial distribution ?i,k. This represents the dis-
tribution over positive and negative attributes for
that property; it is drawn from a beta prior using
hyper-parameters ?A = 2 and ?A = 2. We also
draw a multinomial ?i over K possible properties
from a symmetric Dirichlet distribution with hyper-
parameter ?M = 1, 000. This distribution is used to
draw snippet properties.
Snippet Level: For the jth snippet of the ith prod-
uct, a property random variable Zi,jP is drawn ac-
cording to the multinomial ?i. Conditioned on this
choice, we draw an attribute Zi,jA (positive or nega-
tive) from the property attribute distribution ?i,Z
j,j
P .
Once the property Zi,jP and attribute Z
i,j
A have
been selected, the tokens of the snippet are gener-
ated using a simple HMM. The latent state underly-
ing a token, Zi,j,wW , indicates whether the wth word
comes from the property distribution, attribute dis-
tribution, or background distribution; we use P , A,
or B to denote these respective values of Zi,j,wW .
The sequence Zi,j,1W , . . . , Z
i,j,m
W is generated us-
ing a first-order Markov model. The full transition
parameter matrix ? parametrizes these decisions.
Conditioned on the underlying Zi,j,wW , a word, s
i,j,w
is drawn from ?i,jP , ?
i,Zi,jP
A , or ?B for the values P ,A,
or B respectively.
5 Inference
The goal of inference is to predict the snippet prop-
erty and attribute distributions over each snippet
given all the observed snippets P (Zi,jP , Z
i,j
A |s) for
all products i and snippets j. Ideally, we would like
to marginalize out nuisance random variables and
distributions. Specifically, we approximate the full
353
model posterior using variational inference:1
P (?,?P , ?B,?A,?, |s) ?
Q(?,?P , ?B,?A,?)
where?, ?P ,? denote the collection of latent distri-
butions in our model. Here, we assume a full mean-
field factorization of the variational distribution; see
Figure 2 for the decomposition. Each variational
factor q(?) represents an approximation of that vari-
able?s posterior given observed random variables.
The variational distribution Q(?) makes the (incor-
rect) assumption that the posteriors amongst factors
are independent. The goal of variational inference is
to set factors q(?) so that it minimizes the KL diver-
gence to the true model posterior:
min
Q(?)
KL(P (?,?P , ?B,?A,?, |s)?
Q(?,?P , ?B,?A,?)
We optimize this objective using coordinate descent
on the q(?) factors. Concretely, we update each fac-
tor by optimizing the above criterion with all other
factors fixed to current values. For instance, the up-
date for the factor q(Zi,j,wW ) takes the form:
q(Zi,j,wW )?
EQ/q(Zi,j,wW )
lgP (?,?P , ?B,?A,?, s)
The full factorization of Q(?) and updates for
all random variable factors are given in Figure 2.
Updates of parameter factors are omitted; however
these are derived through simple counts of the ZA,
ZP , and ZW latent variables. For related discussion,
see Blei et al (2003).
6 Experiments
In this section, we describe in detail our data set and
present three experiments and their results.
Data Set Our data set consists of snippets from
Yelp reviews generated by the system described in
Sauper et al (2010). This system is trained to ex-
tract snippets containing short descriptions of user
sentiment towards some aspect of a restaurant.2 We
1See Liang and Klein (2007) for an overview of variational tech-
niques.
2For exact training procedures, please reference that paper.
The [P noodles ] and the [P meat ] were actually [+ pretty good ].
I [+ recommend ] the [P chicken noodle pho ].
The [P noodles ] were [- soggy ].
The [P chicken pho ] was also [+ good ].
The [P spring rolls ] and [P coffee ] were [+ good ] though.
The [P spring roll wrappers ] were a [- little dry tasting ].
My [+ favorites ] were the [P crispy spring rolls ].
The [P Crispy Tuna Spring Rolls ] are [+ fantastic ]!
The [P lobster roll ] my mother ordered was [- dry ] and [- scant ].
The [P portabella mushroom ] is my [+ go-to ] [P sandwich ].
The [P bread ] on the [P sandwich ] was [- stale ].
The slice of [P tomato ] was [- rather measly ].
The [P shumai ] and [P California maki sushi ] were [+ decent ].
The [P spicy tuna roll ] and [P eel roll ] were [+ perfect ].
The [P rolls ] with [P spicy mayo ] were [- not so great ].
I [+ love ] [P Thai rolls ].
Figure 3: Example snippets from our data set, grouped
according to property. Property words are labeled P and
colored blue, NEGATIVE attribute words are labeled - and
colored red, and POSITIVE attribute words are labeled +
and colored green. The grouping and labeling are not
given in the data set and must be learned by the model.
select only the snippets labeled by that system as ref-
erencing food, and we ignore restaurants with fewer
than 20 snippets. There are 13,879 snippets in to-
tal, taken from 328 restaurants in and around the
Boston/Cambridge area. The average snippet length
is 7.8 words, and there are an average of 42.1 snip-
pets per restaurant, although there is high variance
in number of snippets for each restaurant. Figure 3
shows some example snippets.
For sentiment attribute seed words, we use 42 and
33 words for the positive and negative distributions
respectively. These are hand-selected based on the
restaurant review domain; therefore, they include
domain-specific words such as delicious and gross.
Tasks We perform three experiments to evaluate
our model?s effectiveness. First, a cluster predic-
tion task is designed to test the quality of the learned
property clusters. Second, an attribute analysis task
will evaluate the sentiment analysis portion of the
model. Third, we present a task designed to test
whether the system can correctly identify properties
which have conflicting attributes, which tests both
clustering and sentiment analysis.
354
Mean-field Factorization
Q(?,?P , ?B,?A,?) = q(?B)
(
N?
a=1
q(?aA)
)?
?
n?
i
(
K?
k=1
q(?i,kP )q(?
i,k)
)?
?
?
j
q(Zi,jA )q(Z
i,j
P )
?
w
q(Zi,j,wW )
?
?
?
?
Snippet Property Indicator
lg q(Zi,jP = k) ? Eq(?i) lg?
i(p) +
?
w
q(Zi,j,wW = P )Eq(?i,kP )
lg ?i,kP (s
i,j,w) +
N?
a=1
q(Zi,jA = a)Eq(?i,k) lg ?
i,k(a)
Snippet Attribute Indicator
lg q(Zi,jA = a) =
?
k
q(Zi,jP = k)Eq(?i,k) lg ?
i,k(a) +
?
w
q(Zi,j,wW = A)Eq(?aA) lg ?
a
A(s
i,j,w)
Word Topic Indicator
lg q(Zi,j,wW = P ) ? lgP (ZW = P ) +
?
k
q(Zi,jP = k)Eq(?i,kP )
lg ?i,jP (s
i,j,w)
lg q(Zi,j,wW = A) ? lgP (ZW = A) +
?
a?{+,?}
q(Zi,jA = a)Eq(?aA) lg ?
a
A(s
i,j,w)
lg q(Zi,j,wW = B) ? lgP (ZW = B) + Eq(?B) lg ?B(s
i,j,w)
Figure 2: The mean-field variational algorithm used during learning and inference to obtain posterior predictions over
snippet properties and attributes, as described in Section 5. Mean-field inference consists of updating each of the latent
variable factors as well as a straightforward update of latent parameters in round robin fashion.
6.1 Cluster prediction
The goal of this task is to evaluate the quality of
property clusters; specifically the Zi,jP variable in
Section 4. In an ideal clustering, the predicted clus-
ters will be cohesive (i.e., all snippets predicted for
a given property are related to each other) and com-
prehensive (i.e., all snippets which are related to a
property are predicted for it). For example, a snip-
pet will be assigned the property pad thai if and only
if that snippet mentions some aspect of the pad thai.
Annotation For this task, we use a set of gold
clusters over 3,250 snippets across 75 restaurants
collected through Mechanical Turk. In each task, a
worker was given a set of 25 snippets from a single
restaurant and asked to cluster them into as many
clusters as they desired, with the option of leaving
any number unclustered. This yields a set of gold
clusters and a set of unclustered snippets. For verifi-
cation purposes, each task was provided to two dif-
ferent workers. The intersection of both workers?
judgments was accepted as the gold standard, so the
model is not evaluated on judgments which disagree.
In total, there were 130 unique tasks, each of which
were provided to two workers, for a total output of
210 generated clusters.
Baseline The baseline for this task is a cluster-
ing algorithm weighted by TF*IDF over the data set
as implemented by the publicly available CLUTO
package.3 This baseline will put a strong connec-
tion between things which are lexically similar. Be-
cause our model only uses property words to tie
together clusters, it may miss correlations between
words which are not correctly identified as property
words. The baseline is allowed 10 property clusters
per restaurant.
We use the MUC cluster evaluation metric for
this task (Vilain et al, 1995). This metric measures
the number of cluster merges and splits required to
recreate the gold clusters given the model?s output.
3Available at http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview
with agglomerative clustering, using the cosine similarity
distance metric.
355
Precision Recall F1
Baseline 80.2 61.1 69.3
Our model 72.2 79.1 75.5
Table 2: Results using the MUC metric on the cluster
prediction task. Note that while the precision of the base-
line is higher, the recall and overall F1 of our model out-
weighs that. While MUC has a deficiency in that putting
everything into a single cluster will artificially inflate the
score, parameters on our model are set so that the model
uses the same number of clusters as the baseline system.
Therefore, it can concisely show how accurate our
clusters are as a whole. While it would be possible
to artificially inflate the score by putting everything
into a single cluster, the parameters on our model
and the likelihood objective are such that the model
prefers to use all available clusters, the same number
as the baseline system.
Results Results for our cluster prediction task are
in Table 2. While our system does suffer on preci-
sion in comparison to the baseline system, the recall
gains far outweigh this loss, for a total error reduc-
tion of 20% on the MUC measure.
The most common cause of poor cluster choices
in the baseline system is its inability to distinguish
property words from attribute words. For example,
if many snippets in a given restaurant use the word
delicious, there may end up being a cluster based on
that alone. Because our system is capable of dis-
tinguishing which words are property words (i.e.,
words relevant to clustering), it can choose clusters
which make more sense overall. We show an exam-
ple of this in Table 3.
6.2 Attribute analysis
We also evaluate the system?s predictions of snip-
pet attribute using the predicted posterior over the
attribute distribution for the snippet (i.e., Zi,jA ). For
this task, we consider the binary judgment to be sim-
ply the one with higher value in q(Zi,jA ) (see Sec-
tion 5). The goal of this task is to evaluate whether
our model correctly distinguishes attribute words.
Annotation For this task, we use a set of 260 to-
tal snippets from the Yelp reviews for 30 restaurants,
evenly split into a training and test sets of 130 snip-
pets each. These snippets are manually labeled POS-
The martini selection looked delicious
The s?mores martini sounded excellent
The martinis were good
The martinis are very good
The mozzarella was very fresh
The fish and various meets were very well made
The best carrot cake I?ve ever eaten
Carrot cake was deliciously moist
The carrot cake was delicious.
It was rich, creamy and delicious.
The pasta Bolognese was rich and robust.
Table 3: Example phrases from clusters in both the base-
line and our model. For each pair of clusters, the dashed
line indicates separation by the baseline model, while the
solid line indicates separation by our model. In the first
example, the baseline mistakenly clusters some snippets
about martinis with those containing the word very. In
the second example, the same occurs with the word deli-
cious.
ITIVE or NEGATIVE. Neutral snippets are ignored
for the purpose of this experiment.
Baseline We use two baselines for this task, one
based on a standard discriminative classifier and one
based on the seed words from our model.
The DISCRIMINATIVE baseline for this task is
a standard maximum entropy discriminative bi-
nary classifier over unigrams. Given enough snip-
pets from enough unrelated properties, the classifier
should be able to identify that words like great in-
dicate positive sentiment and those like bad indi-
cate negative sentiment, while words like chicken
are neutral and have no effect.
The SEED baseline simply counts the number of
words from the positive and negative seed lists used
by the model, Vseed+ and Vseed? . If there are more
words from Vseed+ , the snippet is labeled positive,
and if there are more words from Vseed? , the snip-
pet is labeled negative. If there is a tie or there are
no seed words, we split the prediction. Because
the seed word lists are specifically slanted toward
restaurant reviews (i.e., they contain words such as
delicious), this baseline should perform well.
Results For this experiment, we measure the over-
all classification accuracy of each system (see Table
356
Accuracy
DISCRIMINATIVE baseline 75.9
SEED baseline 78.2
Our model 80.2
Table 4: Attribute prediction accuracy of the full system
compared to the DISCRIMINATIVE and SEED baselines.
The advantage of our system is its ability to distinguish
property words from attribute words in order to restrict
judgment to only the relevant terms.
The naan was hot and fresh
All the veggies were really fresh and crisp.
Perfect mix of fresh flavors and comfort food
The lo main smelled and tasted rancid
My grilled cheese sandwich was a little gross
Table 5: Examples of sentences correctly labeled by our
system but incorrectly labeled by the DISCRIMINATIVE
baseline; the key sentiment words are highlighted. No-
tice that these words are not the most common sentiment
words; therefore, it is difficult for the classifier to make a
correct generalization. Only two of these words are seed
words for our model (fresh and gross).
4). Our system outperforms both supervised base-
lines.
As in the cluster prediction case, the main flaw
with the DISCRIMINATIVE baseline system is its in-
ability to recognize which words are relevant for the
task at hand, in this case the attribute words. By
learning to separate attribute words from the other
words in the snippets, our full system is able to more
accurately judge their sentiment. Examples of these
cases are found in Table 5.
The obvious flaw in the SEED baseline is the in-
ability to pre-specify every possible sentiment word;
our model?s performance indicates that it is learning
something beyond just these basic words.
6.3 Conflict identification
Our final task requires both correct cluster prediction
and correct sentiment judgments. In many domains,
it is interesting to know not only whether a product
is rated highly, but also whether there is conflicting
sentiment or debate. In the case of restaurant re-
views, it is relevant to know whether the dishes are
consistently good or whether there is some variation
in quality.
Judgment
P A Attribute / Snippet
Yes Yes
- The salsa isn?t great
+ Chips and salsa are sublime
- The grits were good, but not great.
+ Grits were the perfect consistency
- The tom yum kha was bland
+ It?s the best Thai soup I ever had
- The naan is a bit doughy and undercooked
+ The naan was pretty tasty
- My reuben was a little dry.
+ The reuben was a good reuben.
Yes No
- Belgian frites are crave-able
+ The frites are very, very good.
No Yes
- The blackened chicken was meh
+ Chicken enchiladas are yummy!
- The taste overall was mediocre
+ The oysters are tremendous
No No - The cream cheese wasn?t bad+ Ice cream was just delicious
Table 6: Example property-attribute correctness for the
conflict identification task, over both property and at-
tribute. Property judgment (P) indicates whether the snip-
pets are discussing the same item; attribute judgment (A)
indicates whether there is a correct difference in attribute
(sentiment), regardless of properties.
To evaluate this, we examine the output clusters
which contain predictions of both positive and neg-
ative snippets. The goal is to identify whether these
are true conflicts of sentiment or there was a failure
in either property clustering or attribute classifica-
tion.
For this task, the output clusters are manually an-
notated for correctness of both property and attribute
judgments, as in Table 6. As there is no obvious
baseline for this experiment, we treat it simply as an
analysis of errors.
Results For this task, we examine the accuracy of
conflict prediction, both with and without the cor-
rectly identified properties. The results by property-
attribute correctness are shown in Table 7. From
these numbers, we can see that 50% of the clusters
are correct in both property (cohesiveness) and at-
tribute (difference in sentiment) dimensions.
Overall, the properties are correctly identified
(subject of NEG matches the subject of POS) 68%
of the time and a correct difference in attribute is
identified 67% of the time. Of the clusters which
are correct in property, 74% show a correctly labeled
357
Judgment
P A # Clusters
Yes Yes 52
Yes No 18
No Yes 17
No No 15
Table 7: Results of conflict analysis by correctness of
property label (P) and attribute conflict (A). Examples
of each type of correctness pair are show in in Table 6.
50% of the clusters are correct in both labels, and there
are approximately the same number of errors toward both
property and attribute.
difference in attribute.
7 Conclusion
We have presented a probabilistic topic model for
identifying properties and attitudes of product re-
view snippets. The model is relatively simple and
admits an efficient variational mean-field inference
procedure which is parallelized and can be run on
a large number of snippets. We have demonstrated
on multiple evaluation tasks that our model outper-
forms applicable baselines by a considerable mar-
gin.
Acknowledgments
The authors acknowledge the support of the NSF
(CAREER grant IIS-0448168), NIH (grant 5-
R01-LM009723-02), Nokia, and the DARPA Ma-
chine Reading Program (AFRL prime contract no.
FA8750-09-C-0172). Thanks to Peter Szolovits and
the MIT NLP group for their helpful comments.
Any opinions, findings, conclusions, or recommen-
dations expressed in this paper are those of the au-
thors, and do not necessarily reflect the views of the
funding organizations.
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Giuseppe Carenini, Raymond Ng, and Adam Pauls.
2006. Multi-document summarization of evaluative
text. In Proceedings of EACL, pages 305?312.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of SIGKDD,
pages 168?177.
Minqing Hu and Bing Liu. 2006. Opinion extraction and
summarization on the web. In Proceedings of AAAI.
Soo-Min Kim and Eduard Hovy. 2006. Automatic iden-
tification of pro and con reasons in online reviews. In
Proceedings of COLING/ACL, pages 483?490.
Hyun Duk Kim and ChengXiang Zhai. 2009. Generat-
ing comparative summaries of contradictory opinions
in text. In Proceedings of CIKM, pages 385?394.
P. Liang and D. Klein. 2007. Structured Bayesian non-
parametric models with variational inference (tutorial).
In Proceedings of ACL.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005a.
Opinion observer: Analyzing and comparing opinions
on the web. In Proceedings of WWW, pages 342?351.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005b.
Opinion observer: analyzing and comparing opinions
on the web. In Proceedings of WWW, pages 342?351.
Yue Lu and ChengXiang Zhai. 2008. Opinion integra-
tion through semi-supervised topic modeling. In Pro-
ceedings of WWW, pages 121?130.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Proceed-
ings of WWW, pages 171?180.
Ana-Maria Popescu, Bao Nguyen, and Oren Etzioni.
2005. OPINE: Extracting product features and opin-
ions from reviews. In Proceedings of HLT/EMNLP,
pages 339?346.
Christina Sauper, Aria Haghighi, and Regina Barzilay.
2010. Incorporating content structure into text anal-
ysis applications. In Proceedings of EMNLP, pages
377?387.
Yohei Seki, Koji Eguchi, Noriko K, and Masaki Aono.
2006. Opinion-focused summarization and its analysis
at DUC 2006. In Proceedings of DUC, pages 122?
130.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of ACL, pages 308?316.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
of MUC, pages 45?52.
358
