Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 549?553, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
SZTE-NLP: Sentiment Detection on Twitter Messages
Viktor Hangya, Ga?bor Berend, Richa?rd Farkas
University of Szeged
Department of Informatics
hangyav@gmail.com, {berendg,rfarkas}@inf.u-szeged.hu
Abstract
In this paper we introduce our contribution
to the SemEval-2013 Task 2 on ?Sentiment
Analysis in Twitter?. We participated in ?task
B?, where the objective was to build mod-
els which classify tweets into three classes
(positive, negative or neutral) by their con-
tents. To solve this problem we basically fol-
lowed the supervised learning approach and
proposed several domain (i.e. microblog) spe-
cific improvements including text preprocess-
ing and feature engineering. Beyond the su-
pervised setting we also introduce some early
results employing a huge, automatically anno-
tated tweet dataset.
1 Introduction
In the past few years, the popularity of social me-
dia has increased. Many studies have been made in
the area (Jansen et al, 2009; O?Connor et al, 2010;
Bifet and Frank, 2010; Sang and Bos, 2012). Peo-
ple post messages on a variety of topics, for example
products, political issues, etc. Thus a big amount of
user generated data is created day-by-day. The man-
ual processing of this data is impossible, therefore
automatic procedures are needed.
In this paper we introduce an approach which is
able to assign sentiment labels to Twitter messages.
More precisely, it classifies tweets into positive, neg-
ative or neutral polarity classes. The system partici-
pated in the SemEval-2013 Task 2: Sentiment Anal-
ysis in Twitter, Task?B Message Polarity Classifica-
tion (Wilson et al, 2013). In our approach we used
a unigram based supervised model because it has
been shown that it works well on short messages like
tweets (Jiang et al, 2011; Barbosa and Feng, 2010;
Agarwal et al, 2011; Liu, 2010). We reduced the
size of the dictionary by normalizing the messages
and by stop word filtering. We also explored novel
features which gave us information on the polarity of
a tweet, for example we made use of the acronyms
in messages.
In the ?constrained? track of Task?B we used the
given training and development data only. For the
?unconstrained? track we downloaded tweets using
the Twitter Streaming API1 and automatically anno-
tated them. We present some preliminary results on
exploiting this huge dataset for training our classi-
fier.
2 Approach
At the beginning of our experiments we used a
unigram-based supervised model. Later on, we re-
alized that the size of our dictionary is huge, so
in the normalization phase we tried to reduce the
number of words in it. We investigated novel fea-
tures which contain information on the polarity of
the messages. Using these features we were able to
improve the precision of our classifier. For imple-
mentation we used the MALLET toolkit, which is a
Java-based package for natural language processing
(McCallum, 2002).
2.1 Normalization
One reason for the unusually big dictionary size is
that it contains one word in many forms, for exam-
1https://dev.twitter.com/docs/
streaming-apis/streams/public
549
ple in upper and lower case, in a misspelled form,
with character repetition, etc. On the other hand, it
contained numerous special annotations which are
typical for blogging, such as Twitter-specific anno-
tations, URL?s, smileys, etc. Keeping these in mind
we made the following normalization steps:
? First, in order to get rid of the multiple forms
of a single word we converted them into lower
case form then we stemmed them. For this pur-
pose we used the Porter Stemming Algorithm.
? We replaced the @ and # Twitter-specific tags
with the [USER] and [TAG] notations, respec-
tively. Besides we converted every URL in the
messages to the [URL] notation.
? Smileys in messages play an important role
in polarity classification. For this reason we
grouped them into positive and negative smi-
ley classes. We considered :), :-),: ), :D, =), ;),
; ), (: and :(, :-(, : (, ):, ) : smileys as positive
and negative, respectively.
? Since numbers do not contain information re-
garding a message polarity, we converted them
as well to the [NUMBER] form. In ad-
dition, we replaced the question and excla-
mation marks with the [QUESTION MARK]
and [EXCLAMATION MARK] notations. Af-
ter this we removed the unnecessary char-
acters ?"#$%&()*+,./:;<=>\?{}?, with
the exception that we removed the ? character
only if a word started or ended with it.
? In the case of words which contained character
repetitions ? more precisely those which con-
tained the same character at least three times
in a row ?, we reduced the length of this se-
quence to three. For instance, in the case
of the word yeeeeahhhhhhh we got the form
yeeeahhh. This way we unified these charac-
ter repetitions, but we did not loose this extra
information.
? Finally we made a stop word filtering in order
to get rid of the undesired words. To identify
these words we did not use a stop word dictio-
nary, rather we filtered out those words which
appeared too frequently in the training corpus.
We have chosen this method because we would
like to automatically detect those words which
are not relevant in the classification.
Before the normalization step, the dictionary con-
tained approximately 41, 000 words. After the above
introduced steps we managed to reduce the size of
the dictionary to 15, 000 words.
2.2 Features
After normalizing Twitter messages, we searched
for special features which characterize the polarity
of the tweets. One such feature is the polarity of
each word in a message. To determine the polarity
of a word, we used the SentiWordNet sentiment lex-
icon (Baccianella et al, 2010). In this lexicon, a pos-
itive, negative and an objective real value belong to
each word, which describes the polarity of the given
word. We consider a word as positive if the related
positive value is greater than 0.3, we consider it as
negative if the related negative value is greater than
0.2 and we consider it as objective if the related ob-
jective value is greater than 0.8. The threshold of the
objective value is high because most words are ob-
jective in this lexicon. After calculating the polarity
of each word we created three new features for each
tweet which are the number of positive, negative and
objective words, respectively. We also checked if a
negation word precedes a positive or negative word
and if so we inverted its polarity.
We also tried to group acronyms by their polarity.
For this purpose we used an acronym lexicon which
can be found on the www.internetslang.com
website. For each acronym we used the polarity of
each word in the acronym?s description and we de-
termined the polarity of the acronym by calculat-
ing the rate of positive and negative words in the
description. This way we created two new fea-
tures which are the number of positive and negative
acronyms in a given message.
Our intuition was that people like to use character
repetitions in their words for expressing their happi-
ness or sadness. Besides normalizing these tokens
(see Section 2.1), we created a new feature as well,
which represents the number of this kind of words
in a tweet.
Beyond character repetitions people like to write
words or a part of the text in upper case in order to
550
call the reader?s attention. Because of this we cre-
ated another feature which is the number of upper
case words in the given text.
3 Collected Data
In order to achieve an appropriate precision with su-
pervised methods we need a big amount of training
data. Creating this database manually is a hard and
time-consuming task. In many cases it is hard even
for humans to determine the polarity of a message,
for instance:
After a whole 5 hours away from work, I
get to go back again, I?m so lucky!
In the above tweet we cannot decide precisely the
polarity because the writer can be serious or just sar-
castic.
In order to increase the size of the training data
we acquired additional tweets, which we used in
the unconstrained run for Task?B. We created an ap-
plication which downloads tweets using the Twitter
Streaming API. The API supports language filter-
ing, which was used to get rid of non-English mes-
sages. Our manual investigations of the downloaded
tweets revealed, however, that this filter allows a big
amount of non-English tweets, which is probably
due to the fact that some Twitter users did not set
their language. We used Twitter4J2 API (which is
a Java library for the Twitter API) for downloading
these tweets. We automatically annotated the down-
loaded tweets using the Twitter Sentiment3 web ap-
plication, similar to Barbosa and Feng (2010) but
we used only one annotator. This web application
also supports language detection, but after this extra
filtration, our dataset still contained a considerable
amount of non-English messages. After 16 hours
of data collection we got 350, 000 annotated tweets,
where the distribution of neutral, positive and neg-
ative classes was approximately 60%, 20%, 20%,
respectively. For further testing purposes we have
chosen 10, 000 tweets from each class.
4 Results
We report results on the two official test sets of the
shared task. The ?twitter? test set consists of 3, 813
2http://twitter4j.org
3http://www.sentiment140.com
tweets while the ?sms? set consists of 2, 094 sms
messages. We evaluated both test databases in two
ways, in the so-called constrained run we only used
the official training database, while in the uncon-
strained run we also used a part of the additional
data, which was mentioned in the 3 section. The
official training database contained 4, 028 positive,
1, 655 negative and 3, 821 neutral tweets while for
the unconstrained run we used an additional 10, 000
tweets from each class. This way in each phase we
got four kinds of runs, which were evaluated with
the Na??ve Bayes and Maximum Entropy classifiers.
In Table 1 the evaluation of the unigram-based
model with the Na??ve Bayes learner can be seen.
The table contains the F-scores for the positive, neg-
ative and neutral labels for each of the four runs.
The avg column contains the average F-score for the
positive and negative labels, which was the official
evaluation metric for SemEval-2013 Task 2. We got
the best scores for the neutral label whilst the worst
scores are obtained for the negative label, which is
due to the fact that there were much less negative
instances in the training database. It can be seen
that the F-scores for the unconstrained run are better
both for the tweet and sms test databases. For the
unigram-based model the F-scores are higher when
we used the Maximum Entropy model (see Table 2).
pos neg neut avg
twitter-constrained 0.59 0.09 0.65 0.34
twitter-unconstrained 0.60 0.17 0.65 0.38
sms-constrained 0.46 0.16 0.63 0.31
sms-unconstrained 0.47 0.38 0.53 0.42
Table 1: Unigram-based model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.60 0.33 0.67 0.46
twitter-unconstrained 0.60 0.40 0.66 0.50
sms-constrained 0.47 0.31 0.69 0.39
sms-unconstrained 0.52 0.47 0.66 0.49
Table 2: Unigram-based model, Maximum Entropy
learner
In Tables 3 and 4 the evaluation results can be
seen for the normalized model. The normalization
551
step increased the precision for both learning al-
gorithms and the Maximum Entropy learner is still
better than Na??ve Bayes. Besides this we noticed
that for both learners in the case of the tweet test
database, the unconstrained run had lower scores
than the constrained whilst in the case of the sms
test database this phenomenon did not appear.
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.21 0.63 0.41
sms-constrained 0.56 0.27 0.72 0.41
sms-unconstrained 0.52 0.35 0.66 0.43
Table 3: Normalized model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.66 0.40 0.68 0.53
twitter-unconstrained 0.61 0.42 0.64 0.51
sms-constrained 0.61 0.38 0.77 0.49
sms-unconstrained 0.57 0.47 0.72 0.52
Table 4: Normalized model, Maximum Entropy
learner
The evaluation results of the feature-based model
can be seen in Tables 5 and 6. In the case of the
Na??ve Bayes learner, the features did not increase the
F-scores, only for the sms-unconstrained run. For
the other runs the achieved scores decreased. In the
case of the Maximum Entropy learner the features
increased the F-scores, slightly for the constrained
runs and a bit more for the unconstrained runs.
From this analysis we can conclude that the nor-
malization of the messages yielded a considerable
increase in the F-scores. We discussed above that
this step also significantly reduced the size of the
dictionary. The features increased the precision too,
especially for the unconstrained run. This means
that these features represent properties which are
useful for those training data which are not from the
same corpus as the test messages. We compared two
machine learning algorithms and from the results we
concluded that the Maximum Entropy learner per-
forms better than the Na??ve Bayes on this task. Our
experiments also showed that the external, automat-
ically labeled training database helped only in the
classification of sms messages. This is due to the
fact that the smses and our external database are
from a different distribution than the official tweet
database.
pos neg neut avg
twitter-constrained 0.65 0.32 0.67 0.48
twitter-unconstrained 0.62 0.17 0.79 0.39
sms-constrained 0.56 0.38 0.74 0.47
sms-unconstrained 0.54 0.29 0.70 0.41
Table 5: Feature-based model, Na??ve Bayes learner
pos neg neut avg
twitter-constrained 0.66 0.41 0.69 0.54
twitter-unconstrained 0.63 0.43 0.65 0.53
sms-constrained 0.62 0.39 0.79 0.50
sms-unconstrained 0.61 0.49 0.75 0.55
Table 6: Feature-based model, Maximum Entropy
learner
5 Conclusions and Future Work
Recently, sentiment analysis on Twitter messages
has gained a lot of attention due to the huge amount
of Twitter users and their tweets. In this paper we ex-
amined different methods for classifying Twitter and
sms messages. We proposed special features which
characterize the polarity of the messages and we
concluded that due to the informality (slang, spelling
mistakes, etc.) of the messages it is crucial to nor-
malize them properly.
In the future, we plan to investigate the utility of
relations between Twitter users and between their
tweets and we are interested in topic-dependent sen-
timent analysis.
Acknowledgments
This work was supported in part by the Euro-
pean Union and the European Social Fund through
project FuturICT.hu (grant no.: TA?MOP-4.2.2.C-
11/1/KONV-2012-0013).
References
Apoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,
and Rebecca Passonneau. 2011. Sentiment Analysis
552
of Twitter Data. In Proceedings of the Workshop on
Language in Social Media (LSM 2011), pages 30?38,
June.
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Bente Maegaard, Joseph Mariani,
Jan Odijk, Stelios Piperidis, Mike Rosner, and Daniel
Tapias, editors, Proceedings of the Seventh Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Luciano Barbosa and Junlan Feng. 2010. Robust Sen-
timent Detection on Twitter from Biased and Noisy
Data. In Poster volume, Coling 2010, pages 36?44,
August.
Albert Bifet and Eibe Frank. 2010. Sentiment Knowl-
edge Discovery in Twitter Streaming Data.
Bernard J. Jansen, Mimi Zhang, Kate Sobel, and Abdur
Chowdury. 2009. Twitter Power: Tweets as Electronic
Word of Mouth. In Journal of the American society
for information science and technology, pages 2169?
2188.
Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun
Zhao. 2011. Target-dependent Twitter Sentiment
Classification. In Proceedings of the 49th Annual
Meeting of the Association for Computational Linguis-
tics, pages 151?160, June.
Bing Liu. 2010. Sentiment Analysis and Subjectivity. In
N. Indurkhya and F. J. Damerau, editors, Handbook of
Natural Language Processing.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Brendan O?Connor, Ramnath Balasubramanyan,
Bryan R. Routledge, and Noah A. Smith. 2010.
From Tweets to Polls: Linking Text Sentiment to
Public Opinion Time Series. In Proceedings of the
International AAAI Conference on Weblogs and Social
Media, May.
Erik Tjong Kim Sang and Johan Bos. 2012. Predicting
the 2011 Dutch Senate Election Results with Twitter.
In Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 53?60, April.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 Task 2: Sentiment Analysis in Twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
553
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 610?614,
Dublin, Ireland, August 23-24, 2014.
SZTE-NLP: Aspect Level Opinion Mining Exploiting Syntactic Cues
Viktor Hangya
1
, G
?
abor Berend
1
, Istv
?
an Varga
2?
, Rich
?
ard Farkas
1
1
University of Szeged
Department of Informatics
{hangyav,berendg,rfarkas}@inf.u-szeged.hu
2
NEC Corporation, Japan
Knowledge Discovery Research Laboratories
vistvan@az.jp.nec.com
Abstract
In this paper, we introduce our contribu-
tions to the SemEval-2014 Task 4 ? As-
pect Based Sentiment Analysis (Pontiki et
al., 2014) challenge. We participated in
the aspect term polarity subtask where
the goal was to classify opinions related
to a given aspect into positive, negative,
neutral or conflict classes. To solve this
problem, we employed supervised ma-
chine learning techniques exploiting a rich
feature set. Our feature templates ex-
ploited both phrase structure and depen-
dency parses.
1 Introduction
The booming volume of user-generated content
and the consequent popularity growth of online re-
view sites has led to vast amount of user reviews
that are becoming increasingly difficult to grasp.
There is desperate need for tools that can automat-
ically process and organize information that might
be useful for both users and commercial agents.
Such early approaches have focused on deter-
mining the overall polarity (e.g., positive, nega-
tive, neutral, conflict) or sentiment rating (e.g.,
star rating) of various entities (e.g., restaurants,
movies, etc.) cf. (Ganu et al., 2009). While the
overall polarity rating regarding a certain entity
is, without question, extremely valuable, it fails
to distinguish between various crucial dimensions
based on which an entity can be evaluated. Evalu-
ations targeting distinct key aspects (i.e., function-
ality, price, design, etc) provide important clues
that may be targeted by users with different priori-
ties concerning the entity in question, thus holding
?
The work was done while this author was working as a
guest researcher at the University of Szeged
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
much greater value in one?s decision making pro-
cess.
In this paper, we introduce our contribution to
the SemEval-2014 Task 4 ? Aspect Based Sen-
timent Analysis (Pontiki et al., 2014) challenge.
We participated in the aspect term polarity sub-
task where the goal was to classify opinions which
are related to a given aspect into positive, nega-
tive, neutral or conflict classes. We employed su-
pervised machine learning techniques exploiting a
rich feature set for target polarity detection, with
a special emphasis on features that deal with the
detection of aspect scopes. Our system achieved
an accuracy of 0.752 and 0.669 for the restaurant
and laptop domains, respectively.
2 Approach
We employed a four-class supervised (positive,
negative, neutral and conflict) classifier here. As
a normalization step, we converted the given texts
into their lowercased forms. Bag-of-words fea-
tures comprised the basic feature set for our max-
imum entropy classifier, which was shown to be
helpful in polarity detection (Hangya and Farkas,
2013).
In the case of aspect-oriented sentiment detec-
tion, we found it important to locate text parts
that refer to particular aspects. For this, we used
several syntactic parsing methods and introduced
parse tree based features.
2.1 Distance-weighted Bag-of-words Features
Initially, we used n-gram token features (unigrams
and bigrams). It could be helpful to take into con-
sideration the distance between the token in ques-
tion and the mention of the target aspect. The
closer a token is to an entity the more plausible
that the given token is related to the aspect.
610
<ROOT> The food was great but the service was awful .
DT NN VBD JJ CC DT NN VBD JJ .
ROOT
SBJNMOD PRD
COORD
P
CONJ
NMOD SBJ PRD
Figure 1: Dependency parse tree (MATE parser).
For this we used weighted feature vectors, and
weighted each n-gram feature by its distance in to-
kens from the mention of the given aspect:
1
e
1
n
|i?j|
,
where n is the length of the review and the values
i, j are the positions of the actual word and the
mentioned aspect.
2.2 Polarity Lexicon
To examine the polarity of the words comprising
a review, we incorporated the SentiWordNet sen-
timent lexicon (Baccianella et al., 2010) into our
feature set.
In this resource, synsets ? i.e. sets of word
forms sharing some common meaning ? are as-
signed positivity, negativity and objectivity scores.
These scores can be interpreted as the probabilities
of seeing some representatives of the synsets in
a positive, negative and neutral meaning, respec-
tively. However, it is not unequivocal to deter-
mine automatically which particular synset a given
word belongs to with respect its context. Consider
the word form great for instance, which might
have multiple, fundamentally different sentiment
connotations in different contexts, e.g. in expres-
sions such as ?great food? and ?great crisis?.
We determined the most likely synset a particu-
lar word form belonged to based on its contexts by
selecting the synset, the members of which were
the most appropriate for the lexical substitution
of the target word. The extent of the appropri-
ateness of a word being a substitute for another
word was measured relying on Google?s N-Gram
Corpus, using the indexing framework described
in (Ceylan and Mihalcea, 2011).
We look up the frequencies of the n-grams that
we derive from the context by replacing the tar-
get words with its synonyms(great) from various
synsets, e.g. good versus big. We count down the
frequency of the phrases food is good and food is
big in a huge set of in-domain documents (Cey-
lan and Mihalcea, 2011). Than we choose the
meaning which has the highest probability, good
in this case. This way we assign a polarity value
for each word in a text and created three new fea-
tures for the machine learning algorithm, which
are the number of positive, negative and objective
words in the given document.
2.3 Negation Scope Detection
Since negations are quite frequent in user reviews
and have the tendency to flip polarities, we took
special care of negation expressions. We collected
a set of negation expressions, like not, don?t, etc.
and a set of delimiters and, or, etc. It is reasonable
to think that the scope of a negation starts when
we detect a negation word in the sentence and it
lasts until the next delimiter. If an n-gram was in
a negation scope we added a NOT prefix to that
feature.
2.4 Syntax-based Features
It is very important to discriminate between text
fragments that are referring to the given aspect and
the fragments that do not, within the same sen-
tence. To detect the relevant text fragments, we
used dependency and constituency parsers. Since
adjectives are good indicators of opinion polarity,
we add the ones to our feature set which are in
close proximity with the given aspect term. We
define proximity between an adjective and an as-
pect term as the length of the non-directional path
between them in the dependency tree. We gather
adjectives in proximity less than 6.
Another feature, which is not aspect specific but
can indicate the polarity of an opinion, is the polar-
ity of words? modifiers. We defined a feature tem-
plate for tokens whose syntactic head is present in
611
ROOT
S
.
.
S
VP
ADJP
JJ
awful
VBD
was
NP
NN
service
DT
the
CC
but
S
VP
ADJP
JJ
great
VBD
was
NP
NN
food
DT
The
Figure 2: Constituency parse tree (Stanford parser).
our positive or negative lexicon. For dependency
parsing we used the MATE parser (Bohnet, 2010)
trained on the Penn Treebank (penn2malt conver-
sion), an example can be seen on Figure 1.
Besides using words that refer to a given aspect,
we tried to identify subsentences which refers to
the aspect mention. In a sentence we can express
our opinions about more than one aspect, so it is
important not to use subsentences containing opin-
ions about other aspects. We developed a sim-
ple rule based method for selecting the appropri-
ate subtree from the constituent parse of the sen-
tence in question (see Figure 2). In this method,
the root of this subtree is the leaf which contains
the given aspect initially. In subsequent steps the
subtree containing the aspect in its yield gets ex-
panded until the following conditions are met:
? The yield of the subtree consists of at least
five tokens.
? The yield of the subtree does not contain any
other aspect besides the five-token window
frame relative to the aspect in question.
? The current root node of the subtree is either
the non-terminal symbol PP or S.
Relying on these identified subtrees, we intro-
duced a few more features. First, we created
new n-gram features from the yield of the sub-
tree. Next, we determined the polarity of this sub-
tree with a method proposed by Socher et al. ()
and used it as a feature. We also detected those
words which tend to take part in sentences con-
veying subjectivity, using the ?
2
statistics calcu-
lated from the training data. With the help of these
words, we counted the number of opinion indica-
tor words in the subtree as additional features. We
used the Stanford constituency parser (Klein and
Manning, 2003) trained on the Penn Treebank for
these experiments.
2.5 Clustering
Aspect mentions can be classified into a few dis-
tinct topical categories, such as aspects regarding
the price, service or ambiance of some product or
service. Our hypothesis was that the distribution
of the sentiment categories can differ significantly
depending on the aspect categories. For instance,
people might tend to share positive ideas on the
price of some product rather than expressing neg-
ative, neutral or conflicting ideas towards it. In
order to make use of this assumption, we automat-
ically grouped aspect mentions based on their con-
texts as different aspect target words can still refer
to the very same aspect category (e.g. ?delicious
food? and ?nice dishes?).
Clustering of aspect mentions was performed
by determining a vector for each aspect term based
on the words co-occurring with them. 6, 485 dis-
tinct lemmas were found to co-occur with any of
the aspect phrases in the two databases, thus con-
text vectors originally consisted of that many el-
ements. Singular value decomposition was then
used to project these aspect vectors into a lower di-
mensional ?semantic? space, where k-means clus-
tering (with k = 10) was performed over the data
points. For each classification instance, we re-
garded the cluster ID of the particular aspect term
as a nominal feature.
612
3 Results
In this section, we will report our results on the
shared task database which consists of English
product reviews. There are 3, 000 laptop and
restaurant related sentences, respectively. Aspects
were annotated in these sentences, resulting in a
total of 6, 051 annotated aspects. In our experi-
ments, we used maximum entropy classifier with
the default parameter settings of the Java-based
machine learning framework MALLET (McCal-
lum, ).
w
e
i
g
h
t
i
n
g
c
l
u
s
t
e
r
-
p
o
l
a
r
i
t
y
p
a
r
s
e
r
s
s
e
n
t
i
m
e
n
t
0.7
0.72
0.74
0.76
0.78
systems
full-system
baseline
Figure 3: Accuracy on the restaurant test data.
w
e
i
g
h
t
i
n
g
c
l
u
s
t
e
r
-
p
o
l
a
r
i
t
y
p
a
r
s
e
r
s
s
e
n
t
i
m
e
n
t
0.62
0.64
0.66
0.68
0.7
systems
full-system
baseline
Figure 4: Accuracy on the laptop test data.
Our accuracy measured on the restaurant and
laptop test databases can be seen on figures 3 and
4. On the x-axis the accuracy loss can be seen
comparing to our baseline (n-gram features only)
and full-system, while turning off various sets of
features. Firstly, the weighting of n-gram features
are absent, then features based on aspect clustering
and words which indicate polarity in texts. After-
wards, features that are created using dependency
and constituency parsing are turned off and lastly
sentiment features based on the SentiWordNet lex-
icon are ignored. It can be seen that omitting the
features based on parsing results in the most seri-
ous drop in performance. We achieved 1.1 and 2.6
error reduction on the restaurant and laptop test
data using these features, respectively.
In Table 1 the results of several other participat-
ing teams can be seen on the restaurant and laptop
test data. There were more than 30 submissions,
from which we achieved the sixth and third best
results on the restaurants and laptop domains, re-
spectively. At the bottom of the table the official
baselines for each domain can be seen.
Team restaurant laptop
DCU 0.809 0.704
NRC-Canada 0.801 0.704
SZTE-NLP 0.752 0.669
UBham 0.746 0.666
USF 0.731 0.645
ECNU 0.707 0.611
baseline 0.642 0.510
Table 1: Accuracy results of several other partici-
pants. Our system is named SZTE-NLP.
4 Conclusions
In this paper, we presented our contribution to the
aspect term polarity subtask of the SemEval-2014
Task 4 ? Aspect Based Sentiment Analysis chal-
lenge. We proposed a supervised machine learn-
ing technique that employs a rich feature set tar-
geting aspect term polarity detection. Among the
features designed here, the syntax-based feature
group for the determination of the scopes of the as-
pect terms showed the highest contribution. In the
end, our system was ranked as 6
th
and 3
rd
, achiev-
ing an 0.752 and 0.669 accuracies for the restau-
rant and laptop domains, respectively.
613
Acknowledgments
Viktor Hangya and Istv?an Varga were funded in
part by the European Union and the European
Social Fund through the project FuturICT.hu
(T
?
AMOP-4.2.2.C-11/1/KONV-2012-0013).
G?abor Berend and Rich?ard Farkas was partially
funded by the ?Hungarian National Excellence
Program? (T
?
AMOP 4.2.4.A/2-11-1-2012-0001),
co-financed by the European Social Fund.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An Enhanced Lex-
ical Resource for Sentiment Analysis and Opinion
Mining. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10).
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August. Coling 2010 Organizing Com-
mittee.
Hakan Ceylan and Rada Mihalcea. 2011. An efficient
indexer for large n-gram corpora. In ACL (System
Demonstrations), pages 103?108. The Association
for Computer Linguistics.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In WebDB.
Viktor Hangya and Richard Farkas. 2013. Target-
oriented opinion mining from tweets. In Cognitive
Infocommunications (CogInfoCom), 2013 IEEE 4th
International Conference on, pages 251?254. IEEE.
Dan Klein and Christopher D. Manning. 2003. Ac-
curate unlexicalized parsing. In Proceedings of the
41st ACL, pages 423?430.
Andrew Kachites McCallum. Mallet: A machine
learning for language toolkit.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the International Workshop on Semantic Evaluation,
SemEval ?14.
Richard Socher, Alex Perelygin, Jean Wu, Jason
Chuang, Christopher D. Manning, Andrew Y. Ng,
and Christopher Potts. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, October.
614
