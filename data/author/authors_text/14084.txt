Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 216?224,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Contextual Information Improves OOV Detection in Speech
Carolina Parada, Mark Dredze
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
carolinap@jhu.edu
mdredze@cs.jhu.edu
Denis Filimonov
HLTCOE
University of Maryland,
College Park, MD 20742 USA
den@cs.umd.edu
Frederick Jelinek
HLTCOE
Johns Hopkins University
3400 North Charles Street,
Baltimore MD 21210, USA
jelinek@jhu.edu
Abstract
Out-of-vocabulary (OOV) words represent an
important source of error in large vocabulary
continuous speech recognition (LVCSR) sys-
tems. These words cause recognition failures,
which propagate through pipeline systems im-
pacting the performance of downstream ap-
plications. The detection of OOV regions in
the output of a LVCSR system is typically ad-
dressed as a binary classification task, where
each region is independently classified using
local information. In this paper, we show that
jointly predicting OOV regions, and includ-
ing contextual information from each region,
leads to substantial improvement in OOV de-
tection. Compared to the state-of-the-art, we
reduce the missed OOV rate from 42.6% to
28.4% at 10% false alarm rate.
1 Introduction
Even with a vocabulary of one hundred thou-
sand words, a large vocabulary continuous speech
recognition (LVCSR) system encounters out-of-
vocabulary (OOV) words, especially in new do-
mains or genres. New words often include named
entities, foreign words, rare and invented words.
Since these words were not seen during training, the
LVCSR system has no way to recognize them.
OOV words are an important source of error in
LVCSR systems for three reasons. First, OOVs can
never be recognized by the LVCSR system, even if
repeated. Second, OOV words contribute to recog-
nition errors in surrounding words, which propagate
into to later processing stages (translation, under-
standing, document retrieval, etc.). Third, OOVs
are often information-rich nouns ? mis-recognized
OOVs can have a greater impact on the understand-
ing of the transcript than other words.
One solution is to simply increase the LVCSR
system?s vocabulary, but there are always new
words. Additionally, increasing the vocabulary size
without limit can sometimes produce higher word
error rates (WER), leading to a tradeoff between
recognition accuracy of frequent and rare words.
A more effective solution is to detect the presence
of OOVs directly. Once identified, OOVs can be
flagged for annotation and addition to the system?s
vocabulary, or OOV segments can be transcribed
with a phone recognizer, creating an open vocabu-
lary LVCSR system. Identified OOVs prevent error
propagation in the application pipeline.
In the literature, there are two basic approaches
to OOV detection: 1) filler models, which explicitly
represent OOVs using a filler, sub-word, or generic
word model (Bazzi, 2002; Schaaf, 2001; Bisani and
Ney, 2005; Klakow et al, 1999; Wang, 2009); and
2) confidence estimation models, which use differ-
ent confidence scores to find unreliable regions and
label them as OOV (Lin et al, 2007; Burget et al,
2008; Sun et al, 2001; Wessel et al, 2001).
Recently, Rastrow et al (2009a) presented an ap-
proach that combined confidence estimation models
and filler models to improve state-of-the-art results
for OOV detection. This approach and other confi-
dence based systems (Hazen and Bazzi, 2001; Lin
et al, 2007), treat OOV detection as a binary clas-
sification task; each region is independently classi-
fied using local information as IV or OOV. This
work moves beyond this independence assumption
216
that considers regions independently for OOV de-
tection. We treat OOV detection as a sequence la-
beling problem and add features based on the local
lexical context of each region as well as global fea-
tures from a language model using the entire utter-
ance. Our results show that such information im-
proves OOV detection and we obtain large reduc-
tions in error compared to the best previously re-
ported results. Furthermore, our approach can be
combined with any confidence based system.
We begin by reviewing the current state-of-the-art
results for OOV detection. After describing our ex-
perimental setup, we generalize the framework to a
sequence labeling problem, which includes features
from the local context, lexical context, and entire ut-
terance. Each stage yields additional improvements
over the baseline system. We conclude with a review
of related work.
2 Maximum Entropy OOV Detection
Our baseline system is the Maximum Entropy model
with features from filler and confidence estimation
models proposed by Rastrow et al (2009a). Based
on filler models, this approach models OOVs by
constructing a hybrid system which combines words
and sub-word units. Sub-word units, or fragments,
are variable length phone sequences selected using
statistical methods (Siohan and Bacchiani, 2005).
The vocabulary contains a word and a fragment lex-
icon; fragments are used to represent OOVs in the
language model text. Language model training text
is obtained by replacing low frequency words (as-
sumed OOVs) by their fragment representation. Pro-
nunciations for OOVs are obtained using grapheme
to phoneme models (Chen, 2003).
This approach also includes properties from con-
fidence estimation systems. Using a hybrid LVCSR
system, they obtain confusion networks (Mangu et
al., 1999), compact representations of the recog-
nizer?s most likely hypotheses. For an utterance,
the confusion network is composed of a sequence
of confused regions, indicating the set of most likely
word/sub-word hypotheses uttered and their poste-
rior probabilities1 in a specific time interval.
1P (wi|A): posterior probability of word i given the acous-
tics, which includes the language model and acoustic model
scores, as described in (Mangu et al, 1999).
Figure 1 depicts a confusion network decoded by
the hybrid system for a section of an utterance in our
test-set. Below the network we present the reference
transcription. In this example, two OOVs were ut-
tered: ?slobodan? and ?milosevic? and decoded as
four and three in-vocabulary words, respectively. A
confused region (also called ?bin?) corresponds to
a set of competing hypothesis between two nodes.
The goal is to correctly label each of the ?bins? as
OOV or IV. Note the presence of both fragments
(e.g. s l ow, l aa s) and words in some of the
hypothesis bins.
For any bin of the confusion network, Rastrow et
al. combine features from that region using a binary
Maximum Entropy classifier (White et al, 2007).
Their most effective features were:
Fragment-Posterior =
?
f?tj
p(f |tj)
Word-Entropy = ?
?
w?tj
p(w|tj) log p(w|tj)
tj is the current bin in the confusion network and f
is a fragment in the hybrid dictionary.
We obtained confusion networks for a standard
word based system and the hybrid system described
above. We re-implemented the above features, ob-
taining nearly identical results to Rastrow et al us-
ing Mallet?s MaxEnt classifier (McCallum, 2002). 2
All real-valued features were normalized and quan-
tized using the uniform-occupancy partitioning de-
scribed in White et al (2007).3 The MaxEnt model
is regularized using a Gaussian prior (?2 = 100),
but we found results generally insensitive to ?.
3 Experimental Setup
Before we introduce and evaluate our context ap-
proach, we establish an experimental setup. We used
the dataset constructed by Can et al (2009) to eval-
uate Spoken Term Detection (STD) of OOVs; we
refer to this corpus as OOVCORP. The corpus con-
tains 100 hours of transcribed Broadcast News En-
glish speech emphasizing OOVs. There are 1290
unique OOVs in the corpus, which were selected
with a minimum of 5 acoustic instances per word.
2Small differences are due to a change in MaxEnt library.
3All experiments use 50 partitions with a minimum of 100
training values per partition.
217
Figure 1: Example confusion network from the hybrid system with OOV regions and BIO encoding. Hypothesis are
ordered by decreasing value of posterior probability. Best hypothesis is the concatenation of the top word/fragments
in each bin. We omit posterior probabilities due to spacing.
Common English words were filtered out to ob-
tain meaningful OOVs: e.g. NATALIE, PUTIN,
QAEDA, HOLLOWAY. Since the corpus was de-
signed for STD, short OOVs (less than 4 phones)
were explicitly excluded. This resulted in roughly
24K (2%) OOV tokens.
For a LVCSR system we used the IBM Speech
Recognition Toolkit (Soltau et al, 2005)4 with
acoustic models trained on 300 hours of HUB4 data
(Fiscus et al, 1998) and excluded utterances con-
taining OOV words as marked in OOVCORP. The lan-
guage model was trained on 400M words from var-
ious text sources with a 83K word vocabulary. The
LVCSR system?s WER on the standard RT04 BN
test set was 19.4%. Excluded utterances were di-
vided into 5 hours of training and 95 hours of test
data for the OOV detector. Both train and test sets
have a 2% OOV rate. We used this split for all exper-
iments. Note that the OOV training set is different
from the LVCSR training set.
In addition to a word-based LVCSR system, we
use a hybrid LVCSR system, combining word and
sub-word (fragments) units. Combined word/sub-
word systems have improved OOV Spoken Term
Detection performance (Mamou et al, 2007; Parada
et al, 2009), better phone error rates, especially in
OOV regions (Rastrow et al, 2009b), and state-of-
the-art performance for OOV detection. Our hybrid
system?s lexicon has 83K words and 20K fragments
derived using Rastrow et al (2009a). The 1290 ex-
cluded words are OOVs to both the word and hybrid
4We use the IBM system with speaker adaptive training
based on maximum likelihood with no discriminative training.
systems.
Note that our experiments use a different dataset
than Rastrow et. al., but we have a larger vocabu-
lary (83K vs 20K), which is closer to most modern
LVCSR system vocabularies; the resulting OOVs
are more challenging but more realistic.
3.1 Evaluation
Confusion networks are obtained from both the
word and hybrid LVCSR systems. In order to eval-
uate the performance of the OOV detector, we align
the reference transcript to the audio. The LVCSR
transcript is compared to the reference transcript at
the confused region level, so each confused region
is tagged as either OOV or IV. The OOV detector
assigns a score/probability for IV/OOV to each of
these regions.
Previous research reported OOV detection accu-
racy on all test data. However, once an OOV word
has been observed in the training data for the OOV
detector, even if it never appeared in the LVCSR
training data, it is no longer truly OOV. The fea-
tures used in previous approaches did not necessar-
ily provide an advantage on observed versus unob-
served OOVs, but our features do yield an advan-
tage. Therefore, in the sections that follow we re-
port unobserved OOV accuracy: OOV words that
do not appear in either the OOV detector?s or the
LVCSR?s training data. While this penalizes our re-
sults, it is a more informative metric of true system
performance.
We present results using standard detection error
tradeoff (DET) curves (Martin et al, 1997). DET
218
curves measure tradeoffs between misses and false
alarms and can be used to determine the optimal op-
erating point of a system. The x-axis varies the false
alarm rate (false positive) and the y-axis varies the
miss (false negative) rate; lower curves are better.
4 From MaxEnt to CRFs
As a classification algorithm, Maximum Entropy as-
signs a label to each region independently. However,
OOV words tend to be recognized as two or more IV
words, hence OOV regions tend to co-occur. In the
example of Figure 1, the OOV word ?slobodan? was
recognized as four IV words: ?slow vote i mean?.
This suggests that sequence models, which jointly
assign all labels in a sequence, may be more appro-
priate. Therefore, we begin incorporating context by
moving from classification to sequence models.
MaxEnt classification models the target label as
p(yi|xi), where yi is a discrete variable representing
the ith label (?IV? or ?OOV?) and xi is a feature
vector representing information for position i. The
conditional distribution for yi takes the form
p(yi|xi) =
1
Z(xi)
exp(
K?
k=1
?kfk(yi,xi)) ,
Z(xi) is a normalization term and f(yi,xi) is a vec-
tor ofK features, such as those defined in Section 2.
The model is trained discriminatively: parameters ?
are chosen to maximize conditional data likelihood.
Conditional Random Fields (CRF) (Lafferty et
al., 2001) generalize MaxEnt models to sequence
tasks. While having the same model structure as
Hidden Markov Models (HMMs), CRFs are trained
discriminatively and can use large numbers of corre-
lated features. Their primary advantage over Max-
Ent models is their ability to find an optimal labeling
for the entire sequence rather than greedy local deci-
sions. CRFs have been used successfully used in nu-
merous text processing tasks and while less popular
in speech, still applied successfully, such as sentence
boundary detection (Liu et al, 2005).
A CRF models the entire label sequence y as:
p(y|x) =
1
Z(x)
exp(?F (y,x)) ,
where F (y,x) is a global feature vector for input
sequence x and label sequence y and Z(x) is a nor-
malization term.5
5 Context for OOV Detection
We begin by including a minimal amount of local
context in making OOV decisions: the predicted la-
bels for adjacent confused regions (bins). This infor-
mation helps when OOV bins occur in close proxim-
ity, such as successive OOV bins. This is indeed the
case: in the OOV detector training data only 48% of
OOV sequences contained a single bin; sequences
were of length 2 (40%), 3 (9%) and 4 (2%). We
found similar results in the test data. Therefore, we
expect that even a minimal amount of context based
on the labels of adjacent bins will help.
A natural way of incorporating contextual infor-
mation is through a CRF, which introduces depen-
dencies between each label and its neighbors. If a
neighboring bin is likely an OOV, it increases the
chance that the current bin is OOV.
In sequence models, another technique for cap-
turing contextual dependence is the label encoding
scheme. In information extraction, where sequences
of adjacent tokens are likely to receive the same
tag, the beginning of each sequence receives a dif-
ferent tag from words that continue the sequence.
For example, the first token in a person name is
labeled B-PER and all subsequent tokens are la-
beled I-PER. This is commonly referred to as BIO
encoding (beginning, inside, outside). We applied
this encoding technique to our task, labeling bins
as either IV (in vocabulary), B-OOV (begin OOV)
and I-OOV (inside OOV), as illustrated in Figure 1.
This encoding allows the algorithm to identify fea-
tures which might be more indicative of the begin-
ning of an OOV sequence. We found that this en-
coding achieved a superior performance to a simple
IV/OOV encoding. We therefore utilize the BIO en-
coding in all CRF experiments.
Another means of introducing context is through
the order of the CRF model. A first order model
(n = 1) adds dependencies only between neighbor-
ing labels, whereas an n order model creates depen-
dencies between labels up to a distance of n posi-
tions. Higher order models capture length of label
5CRF experiments used the CRF++ package
http://crfpp.sourceforge.net/
219
regions (up to length n). We experiment with both
a first order and a second order CRF. Higher order
models did not provide any improvements.
In order to establish a comparative baseline, we
first present results using the same features from
the system described in Section 2 (Word-Entropy
and Fragment-Posterior). All real-valued features
were normalized and quantized using the uniform-
occupancy partitioning described in White et al
(2007).6 Quantization of real valued features is stan-
dard for log-linear models as it allows the model to
take advantage of non-linear characteristics of fea-
ture values and is better handled by the regulariza-
tion term. As in White et. al. we found it improved
performance.
Figure 2 depicts DET curves for OOV detection
for the MaxEnt baseline and first and second order
CRFs with BIO encoding on unobserved OOVs in
the test data. We generated predictions at different
false alarm rates by varying a probability threshold.
For MaxEnt we used the predicted label probability
and for CRFs the marginal probability of each bin?s
label. While the first order CRF achieves nearly
identical performance to the MaxEnt baseline, the
second order CRF shows a clear improvement. The
second order model has a 5% absolute improvement
at 10% false alarm rate, despite using the identi-
cal features as the MaxEnt baseline. Even a small
amount of context as expressed through local label-
ing decisions improves OOV detection.
The quantization of the features yields quan-
tized prediction scores, resulting in the non-smooth
curves for the MaxEnt and 1st order CRF results.
However, when using a second order CRF the OOV
score varies more smoothly since more features
(context labels) are considered in the prediction of
the current label.
6 Local Lexical Context
A popular approach in sequence tagging, such as in-
formation extraction or part of speech tagging, is to
include features based on local lexical content and
context. In detecting a name, both the lexical form
?John? and the preceding lexical context ?Mr.? pro-
vide clues that ?John? is a name. While we do not
6All experiments use 50 partitions with a minimum of 100
training values per partition.
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
MaxEnt (Baseline)CRF (First Order)CRF (Second Order)
Figure 2: DET curves for OOV detection using a Max-
imum Entropy (MaxEnt) classifier and contextual infor-
mation using a 1st order and 2nd order CRF. All models
use the same baseline features (Section 2).
know the actual lexical items in the speech sequence,
the speech recognizer output can be used as a best
guess. In the example of Figure 1, the words ?for-
mer president? are good indicators that the following
word is either the word ?of? or a name, and hence a
potential OOV. Combining this lexical context with
hypothesized words can help label the subsequent
regions as OOVs (note that none of the hypothesized
words in the third bin are ?of?, names, or nouns).
Words from the LVCSR decoding of the sentence
are used in the CRF OOV detector. For each bin in
the confusion network, we select the word with the
highest probability (best hypothesis). We then add
the best hypothesis word as a feature of the form:
current word=X. These features capture how the
LVCSR system incorrectly recognizes OOV words.
However, since detection is measured on unobserved
OOVs, these features alone may not help.
Instead, we turn to lexical context, which includes
correctly recognized IV words. We evaluate the fol-
lowing sets of features derived from lexical context:
? Current bin?s best hypothesis. (Current-Word)
? Unigrams and bigrams from the best hypoth-
esis in a window of 5 words around current
bin. This feature ignores the best hypothesis in
the current bin, i.e., word[-2],word[-1]
is included, but word[-1],word[0] is not.
(Context-Bigrams)
220
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
CRF (Second Order)+Current-Word+Context-Bigrams+Current-Trigrams+All-Words+All-Words-Stemmed
Figure 3: A second order CRF (Section 5) and additional
features including including word identities from current
and neighboring bins (Section 6).
? Unigrams, bigrams, and trigrams in a window
of 5 words around and including current bin.
(Current-Trigrams)
? All of the above features. (All-Words)
? All above features and their stems.7 (All-
Words-Stemmed)
We added these features to the second order CRF
with BIO encoding and baseline features (Figure 3).
As expected, the current words did not improve per-
formance on unobserved OOVs. When the current
words are combined with the lexical context and
their lemmas, they give a significant boost in perfor-
mance: a 4.2% absolute improvement at 10% false
alarm rate over the previous CRF system, and 9.3%
over the MaxEnt baseline. Interestingly, only com-
bining context and current word gives a substantial
gain. This indicates that OOVs tend to occur with
certain distributional characteristics that are inde-
pendent of the OOV word uttered (since we consider
only unobserved OOVs), perhaps because OOVs
tend to be named entities, foreign words, or rare
nouns. The importance of distributional features is
well known for named entity recognition and part
of speech tagging (Pereira et al, 1993). Other fea-
tures such as sub-strings or baseline features (Word-
7To obtain stemmed words, we use the CPAN package:
http://search.cpan.org/~snowhare/Lingua-Stem-0.83.
Entropy, Fragment-Posterior) from neighboring bins
did not provide further improvement.
7 Global Utterance Context
We now include features that incorporate informa-
tion from the entire utterance. The probability of an
utterance as computed by a language model is of-
ten used as a measure of fluency of the utterance.
We also observe that OOV words tend to take very
specific syntactic roles (more than half of them are
proper nouns), which means the surrounding context
will have predictive lexical and syntactic properties.
Therefore, we use a syntactic language model.
7.1 Language Models
We evaluated both a standard trigram language
model and a syntactic language model (Filimonov
and Harper, 2009a). The syntactic model estimates
the joint probability of the word and its syntactic tag
based on the preceding words and tags. The proba-
bility of an utterance wn1 of length n is computed by
summing over all latent syntactic tag assignments:
p(utt) = p(wn1 ) =
?
t1...tn
n?
i?1
p(wi, ti|w
i?1
1 , t
i?1
1 )
(1)
where wi and ti are the word and tag at posi-
tion i, and wi?11 and t
i?1
1 are sequences of words
and tags of length i ? 1 starting a position 1.
The model is restricted to a trigram context, i.e.,
p(wi, ti|w
i?1
i?2, t
i?1
i?2); experiments that increased the
order yielded no improvement.
We trained the language model on 130 million
words from Hub4 CSR 1996 (Garofolo et al, 1996).
The corpus was parsed using a modified Berkeley
parser (Huang and Harper, 2009) and tags extracted
from parse trees incorporated the word?s POS, the
label of its immediate parent, and the relative posi-
tion of the word among its siblings. 8 The parser
required separated contractions and possessives, but
we recombined those words after parsing to match
the LVCSR tokenization, merging their tags. Since
we are considering OOV detection, the language
model was restricted to LVCSR system?s vocabu-
lary.
8The parent tagset of Filimonov and Harper (2009a).
221
0 2 4 6 8 10 12 14P(FA)10
20
30
40
50
60
P(M
iss)
All-Words-Lemmas+3gram-LM+Syntactic-LM+Syntactic-LM+Tags
Figure 4: Features from a language model added to the
best CRF from Section 6 (All-Words-Stemmed).
We also used the standard trigram LM for refer-
ence. It was trained on the same data and with the
same vocabulary using the SRILM toolkit. We used
interpolated modified KN discounting.
7.2 Language Model Features
We designed features based on the entire utterance
using the language model to measure how the utter-
ance is effected by the current token: whether the
utterance is more likely given the recognized word
or some OOV word.
Likelihood-ratio = log
p(utt)
p(utt|wi = unknown)
Norm-LM-score =
log p(utt)
length(utt)
where p(utt) represents the probability of the ut-
terance using the best path hypothesis word of the
LVCSR system, and p(utt|wi = unknown) is the
probability of the entire utterance with the current
word in the LVCSR output replaced by the token
<unk>, used to represent OOVs. Intuitively, when
an OOV word is recognized as an IV word, the flu-
ency of the utterance is disrupted, especially if the
IV is a function word. The Likelihood-ratio is de-
signed to show whether the utterance is more fluent
(more likely) if the current word is a misrecognized
OOV. 9 The second feature (Norm-LM-score) is the
9Note that in the standard n-gram LM the feature reduces to
log
Qi+n?1
k=i p(wk|w
k?1
k?n+1)
Qi+n?1
k=i p(wk|w
k?1
k?n+1,wi=unknown)
, i.e., only n n-grams actu-
0 5 10 15 20 25 30 35 40P(FA)0
10
20
30
40
50
60
70
80
P(M
iss)
MaxEnt (Baseline)CRF All FeaturesCRF All Features (Unobserved)CRF All Features (Observed)
Figure 5: A CRF with all context features compared to
the state-of-the-art MaxEnt baseline. Results for the CRF
are shown for unobserved, observed and both OOVs.
normalized likelihood of the utterance. An unlikely
utterance biases the system to predicting OOVs.
We evaluated a CRF with these features and
all lexical context features (Section 6) using both
the trigram model and the joint syntactic language
model (Figure 4). Each model improved perfor-
mance, but the syntactic model provided the largest
improvement. At 10% false alarm rate it yields a
4% absolute improvement with respect to the pre-
vious best result (All-Words-Stemmed) and 13.3%
over the MaxEnt baseline. Higher order language
models did not improve.
7.3 Additional Syntactic Features
We explored other syntactic features; the most ef-
fective was the 5-tag window of POS tags of the
best hypothesis.10 The additive improvement of this
feature is depicted in Figure 4 labeled ?+Syntactic-
LM+Tags.? With this feature, we achieve a small ad-
ditional gain. We tried other syntactic features with-
out added benefit, such as the most likely POS tag
for <unk>in the utterance.
ally contribute. However, in the syntactic LM, the entire utter-
ance is affected by the change of one word through the latent
states (tags) (Eq. 1), thus making it a truly global feature.
10The POS tags were generated by the same syntactic LM
(see Section 7.1) as described in (Filimonov and Harper,
2009b). In this case, POS tags include merged tags, i.e., the vo-
cabulary word fred?s may be tagged as NNP-POS or NNP-VBZ.
222
8 Final System
Figure 5 summarizes all of the context features in a
single second order BIO encoded CRF. Results are
shown for state-of-the-art MaxEnt (Rastrow et al,
2009a) as well as for the CRF on unobserved, ob-
served and combined OOVs. For unobserved OOVs
our final system achieves a 14.2% absolute improve-
ment at 10% FA rate. The absolute improvement
on all OOVs was 23.7%. This result includes ob-
served OOVs: words that are OOV for the LVCSR
but are encountered in the OOV detector?s training
data. MaxEnt achieved similar performance for ob-
served and unobserved OOVs so we only include a
single combined result.
Note that the MaxEnt curve flattens at 26% false
alarms, while the CRF continues to decrease. The
elbow in the MaxEnt curve corresponds to the prob-
ability threshold at which no other labeled OOV re-
gion has a non-zero OOV score (regions with zero
entropy and no fragments). In this case, the CRF
model can still rely on the context to predict a non-
zero OOV score. This helps applications where
misses are more heavily penalized than false alarms.
9 Related Work
Most approaches to OOV detection in speech can
be categorized as filler models or confidence esti-
mation models. Filler models vary in three dimen-
sions: 1) The type of filler units used: variable-
length phoneme units (as the baseline system) vs
joint letter sound sub-words; 2) Method used to de-
rive units: data-driven (Bazzi and Glass, 2001) or
linguistically motivated (Choueiter, 2009); 3) The
method for incorporating the LVCSR system: hi-
erarchical (Bazzi, 2002) or flat models (Bisani and
Ney, 2005). Our approach can be integrated with
any of these systems.
We have shown that combining the presence of
sub-word units with other measures of confidence
can provided significant improvements, and other
proposed local confidence measures could be in-
cluded in our system as well. Lin et al (2007)
uses joint word/phone lattice alignments and clas-
sifies high local miss-alignment regions as OOVs.
Hazen and Bazzi (2001) combines filler models with
word confidence scores, such as the minimum nor-
malized log-likelihood acoustic model score for a
word and, the fraction of the N-best utterance hy-
potheses in which a hypothesized word appears.
Limited contextual information has been pre-
viously exploited (although maintaining indepen-
dence assumptions on the labels). Burget et al
(2008) used a neural-network (NN) phone-posterior
estimator as a feature for OOV detection. The
network is fed with posterior probabilities from
weakly-constrained (phonetic-based) and strongly-
constrained (word-based) recognizers. Their sys-
tem estimates frame-based scores, and interestingly,
they report large improvements when using tempo-
ral context in the NN input. This context is quite lim-
ited; it refers to posterior scores from one frame on
each side. Other features are considered and com-
bined using a MaxEnt model. They attribute this
gain to sampling from neighboring phonemes. Sun
et al (2001) combines a filler-based model with a
confidence approach by using several acoustic fea-
tures along with context based features, such as
whether the next word is a filler, acoustic confidence
features for next word, number of fillers, etc.
None of these approaches consider OOV detec-
tion as a sequence labeling problem. The work of
Liu et al (2005) is most similar to the approach pre-
sented here, but applies a CRF to sentence boundary
detection.
10 Conclusion and Future Work
We have presented a novel and effective approach to
improve OOV detection in the output confusion net-
works of a LVCSR system. Local and global con-
textual information is integrated with sub-word pos-
terior probabilities obtained from a hybrid LVCSR
system in a CRF to detect OOV regions effectively.
At a 10% FA rate, we reduce the missed OOV rate
from 42.6% to 28.4%, a 33.3% relative error reduc-
tion. Our future work will focus on additional fea-
tures from the recognizer aside from the single best-
hypothesis, as well as other applications of contex-
tual sequence prediction to speech tasks.
Acknowledgments
The authors thank Ariya Rastrow for providing the
baseline system code, Abhinav Sethy and Bhuvana
Ramabhadran for providing the data used in the ex-
periments and for many insightful discussions.
223
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
elling. In Eurospeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flag hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
Dogan Can, Erica Cooper, Abhinav Sethy, Chris White,
Bhuvana Ramabhadran, and Murat Saraclar. 2009.
Effect of pronounciations on OOV queries in spoken
term detection. ICASSP.
Stanley F. Chen. 2003. Conditional and joint models for
grapheme-to-phoneme conversion. In Eurospeech.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Denis Filimonov and Mary Harper. 2009a. A joint
language model with fine-grain syntactic tags. In
EMNLP.
Denis Filimonov and Mary Harper. 2009b. Measuring
tagging performance of a joint language model. In
Proceedings of the Interspeech 2009.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
John Garofolo, Jonathan Fiscus, William Fisher, and
David Pallett, 1996. CSR-IV HUB4. Linguistic Data
Consortium, Philadelphia.
Timothy J. Hazen and Issam Bazzi. 2001. A comparison
and combination of methods for OOV word detection
and word confidence scoring. In Proceedings of the
International Conference on Acoustics.
Zhongqiang Huang and Mary Harper. 2009. Self-
Training PCFG grammars with latent annotations
across languages. In EMNLP.
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Interna-
tional Conference on Machine Learning (ICML).
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and Mary
Harper. 2005. Using conditional random fields for
sentence boundary detection in speech. In ACL.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The DET curve in assessment of
detection task performance. In Eurospeech.
Andrew McCallum. 2002. MALLET: A machine learn-
ing for language toolkit. http://mallet.cs.
umass.edu.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for OOV terms. In ASRU.
Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993.
Distributional clustering of english words. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramabhad-
ran. 2009a. A new method for OOV detection using
hybrid word/fragment system. ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
O. Siohan and M. Bacchiani. 2005. Fast vocabulary-
independent audio search using path-based graph in-
dexing. In INTERSPEECH.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The IBM 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
224
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 712?721,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Learning Sub-Word Units for Open Vocabulary Speech Recognition
Carolina Parada1, Mark Dredze1, Abhinav Sethy2, and Ariya Rastrow1
1Human Language Technology Center of Excellence, Johns Hopkins University
3400 N Charles Street, Baltimore, MD, USA
carolinap@jhu.edu, mdredze@cs.jhu.edu, ariya@jhu.edu
2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA
asethy@us.ibm.com
Abstract
Large vocabulary speech recognition systems
fail to recognize words beyond their vocab-
ulary, many of which are information rich
terms, like named entities or foreign words.
Hybrid word/sub-word systems solve this
problem by adding sub-word units to large vo-
cabulary word based systems; new words can
then be represented by combinations of sub-
word units. Previous work heuristically cre-
ated the sub-word lexicon from phonetic rep-
resentations of text using simple statistics to
select common phone sequences. We pro-
pose a probabilistic model to learn the sub-
word lexicon optimized for a given task. We
consider the task of out of vocabulary (OOV)
word detection, which relies on output from
a hybrid model. A hybrid model with our
learned sub-word lexicon reduces error by
6.3% and 7.6% (absolute) at a 5% false alarm
rate on an English Broadcast News and MIT
Lectures task respectively.
1 Introduction
Most automatic speech recognition systems operate
with a large but limited vocabulary, finding the most
likely words in the vocabulary for the given acoustic
signal. While large vocabulary continuous speech
recognition (LVCSR) systems produce high quality
transcripts, they fail to recognize out of vocabulary
(OOV) words. Unfortunately, OOVs are often infor-
mation rich nouns, such as named entities and for-
eign words, and mis-recognizing them can have a
disproportionate impact on transcript coherence.
Hybrid word/sub-word recognizers can produce a
sequence of sub-word units in place of OOV words.
Ideally, the recognizer outputs a complete word for
in-vocabulary (IV) utterances, and sub-word units
for OOVs. Consider the word ?Slobodan?, the given
name of the former president of Serbia. As an un-
common English word, it is unlikely to be in the vo-
cabulary of an English recognizer. While a LVCSR
system would output the closest known words (e.x.
?slow it dawn?), a hybrid system could output a
sequence of multi-phoneme units: s l ow, b ax,
d ae n. The latter is more useful for automatically
recovering the word?s orthographic form, identify-
ing that an OOV was spoken, or improving perfor-
mance of a spoken term detection system with OOV
queries. In fact, hybrid systems have improved OOV
spoken term detection (Mamou et al, 2007; Parada
et al, 2009), achieved better phone error rates, espe-
cially in OOV regions (Rastrow et al, 2009b), and
obtained state-of-the-art performance for OOV de-
tection (Parada et al, 2010).
Hybrid recognizers vary in a number of ways:
sub-word unit type: variable-length phoneme
units (Rastrow et al, 2009a; Bazzi and Glass, 2001)
or joint letter sound sub-words (Bisani and Ney,
2005); unit creation: data-driven or linguistically
motivated (Choueiter, 2009); and how they are in-
corporated in LVCSR systems: hierarchical (Bazzi,
2002) or flat models (Bisani and Ney, 2005).
In this work, we consider how to optimally cre-
ate sub-word units for a hybrid system. These units
are variable-length phoneme sequences, although in
principle our work can be use for other unit types.
Previous methods for creating the sub-word lexi-
712
con have relied on simple statistics computed from
the phonetic representation of text (Rastrow et al,
2009a). These units typically represent the most fre-
quent phoneme sequences in English words. How-
ever, it isn?t clear why these units would produce the
best hybrid output. Instead, we introduce a prob-
abilistic model for learning the optimal units for a
given task. Our model learns a segmentation of a
text corpus given some side information: a mapping
between the vocabulary and a label set; learned units
are predictive of class labels.
In this paper, we learn sub-word units optimized
for OOV detection. OOV detection aims to identify
regions in the LVCSR output where OOVs were ut-
tered. Towards this goal, we are interested in select-
ing units such that the recognizer outputs them only
for OOV regions while prefering to output a com-
plete word for in-vocabulary regions. Our approach
yields improvements over state-of-the-art results.
We begin by presenting our log-linear model for
learning sub-word units with a simple but effective
inference procedure. After reviewing existing OOV
detection approaches, we detail how the learned
units are integrated into a hybrid speech recognition
system. We show improvements in OOV detection,
and evaluate impact on phone error rates.
2 Learning Sub-Word Units
Given raw text, our objective is to produce a lexicon
of sub-word units that can be used by a hybrid sys-
tem for open vocabulary speech recognition. Rather
than relying on the text alone, we also utilize side
information: a mapping of words to classes so we
can optimize learning for a specific task.
The provided mapping assigns labels Y to the cor-
pus. We maximize the probability of the observed
labeling sequence Y given the text W : P (Y |W ).
We assume there is a latent segmentation S of this
corpus which impacts Y . The complete data likeli-
hood becomes: P (Y |W ) =
?
S P (Y, S|W ) during
training. Since we are maximizing the observed Y ,
segmentation S must discriminate between different
possible labels.
We learn variable-length multi-phone units by
segmenting the phonetic representation of each word
in the corpus. Resulting segments form the sub-
word lexicon.1 Learning input includes a list of
words to segment taken from raw text, a mapping
between words and classes (side information indi-
cating whether token is IV or OOV), a pronuncia-
tion dictionaryD, and a letter to sound model (L2S),
such as the one described in Chen (2003). The cor-
pus W is the list of types (unique words) in the raw
text input. This forces each word to have a unique
segmentation, shared by all common tokens. Words
are converted into phonetic representations accord-
ing to their most likely dictionary pronunciation;
non-dictionary words use the L2S model.2
2.1 Model
Inspired by the morphological segmentation model
of Poon et al (2009), we assume P (Y, S|W ) is a
log-linear model parameterized by ?:
P?(Y, S|W ) =
1
Z(W )
u?(Y, S,W ) (1)
where u?(Y, S,W ) defines the score of the pro-
posed segmentation S for words W and labels Y
according to model parameters ?. Sub-word units
? compose S, where each ? is a phone sequence, in-
cluding the full pronunciation for vocabulary words;
the collection of ?s form the lexicon. Each unit
? is present in a segmentation with some context
c = (?l, ?r) of the form ?l??r. Features based on
the context and the unit itself parameterize u?.
In addition to scoring a segmentation based on
features, we include two priors inspired by the Min-
imum Description Length (MDL) principle sug-
gested by Poon et al (2009). The lexicon prior
favors smaller lexicons by placing an exponential
prior with negative weight on the length of the lex-
icon
?
? |?|, where |?| is the length of the unit ?
in number of phones. Minimizing the lexicon prior
favors a trivial lexicon of only the phones. The
corpus prior counters this effect, an exponential
prior with negative weight on the number of units
in each word?s segmentation, where |si| is the seg-
mentation length and |wi| is the length of the word
in phones. Learning strikes a balance between the
two priors. Using these definitions, the segmenta-
tion score u?(Y, S,W ) is given as:
1Since sub-word units can expand full-words, we refer to
both words and sub-words simply as units.
2The model can also take multiple pronunciations (?3.1).
713
s l ow b ax d ae n
s l ow
(#,#, , b, ax)
b ax
(l,ow, , d, ae)
d ae n
(b,ax, , #, #)
Figure 1: Units and bigram phone context (in parenthesis)
for an example segmentation of the word ?slobodan?.
u?(Y, S,W ) = exp
(
?
?,y
??,yf?,y(S, Y )
+
?
c,y
?c,yfc,y(S, Y )
+ ? ?
?
??S
|?|
+ ? ?
?
i?W
|si|/|wi|
)
(2)
f?,y(S, Y ) are the co-occurrence counts of the pair
(?, y) where ? is a unit under segmentation S and y
is the label. fc,y(S, Y ) are the co-occurrence counts
for the context c and label y under S. The model
parameters are ? = {??,y, ?c,y : ??, c, y}. The neg-
ative weights for the lexicon (?) and corpus priors
(?) are tuned on development data. The normalizer
Z sums over all possible segmentations and labels:
Z(W ) =
?
S?
?
Y ?
u?(Y
?, S?,W ) (3)
Consider the example segmentation for the word
?slobodan? with pronunciation s,l,ow,b,ax,d,ae,n
(Figure 1). The bigram phone context as a four-tuple
appears below each unit; the first two entries corre-
spond to the left context, and last two the right con-
text. The example corpus (Figure 2) demonstrates
how unit features f?,y and context features fc,y are
computed.
3 Model Training
Learning maximizes the log likelihood of the ob-
served labels Y ? given the words W :
`(Y ?|W ) = log
?
S
1
Z(W )
u?(Y
?, S,W ) (4)
We use the Expectation-Maximization algorithm,
where the expectation step predicts segmentations S
Labeled corpus: president/y = 0 milosevic/y = 1
Segmented corpus: p r eh z ih d ih n t/0 m ih/1 l aa/1
s ax/1 v ih ch/1
Unit-feature:Value p r eh z ih d ih n t/0:1 m ih/1:1
l aa/1:1 s ax/1:1 v ih ch/1:1
Context-feature:Value
(#/0,#/0, ,l/1,aa/1):1,
(m/1,ih/1, ,s/1,ax/1):1,
(l/1,aa/1, ,v/1,ih/1):1,
(s/1,ax/1, ,#/0,#/0):1,
(#/0,#/0, ,#/0,#/0):1
Figure 2: A small example corpus with segmentations
and corresponding features. The notation m ih/1:1
represents unit/label:feature-value. Overlapping context
features capture rich segmentation regularities associated
with each class.
given the model?s current parameters ? (?3.1), and
the maximization step updates these parameters us-
ing gradient ascent. The partial derivatives of the
objective (4) with respect to each parameter ?i are:
?`(Y ?|W )
??i
= ES|Y ?,W [fi]? ES,Y |W [fi] (5)
The gradient takes the usual form, where we en-
courage the expected segmentation from the current
model given the correct labels to equal the expected
segmentation and expected labels. The next section
discusses computing these expectations.
3.1 Inference
Inference is challenging since the lexicon prior ren-
ders all word segmentations interdependent. Con-
sider a simple two word corpus: cesar (s,iy,z,er),
and cesium (s,iy,z,iy,ax,m). Numerous segmen-
tations are possible; each word has 2N?1 possible
segmentations, where N is the number of phones in
its pronunciation (i.e., 23 ? 25 = 256). However,
if we decide to segment the first word as: {s iy,
z er}, then the segmentation for ?cesium?:{s iy,
z iy ax m} will incur a lexicon prior penalty for
including the new segment z iy ax m. If instead
we segment ?cesar? as {s iy z, er}, the segmen-
tation {s iy, z iy ax m} incurs double penalty
for the lexicon prior (since we are including two new
units in the lexicon: s iy and z iy ax m). This
dependency requires joint segmentation of the entire
corpus, which is intractable. Hence, we resort to ap-
proximations of the expectations in Eq. (5).
One approach is to use Gibbs Sampling: it-
erating through each word, sampling a new seg-
714
mentation conditioned on the segmentation of all
other words. The sampling distribution requires
enumerating all possible segmentations for each
word (2N?1) and computing the conditional prob-
abilities for each segmentation: P (S|Y ?,W ) =
P (Y ?, S|W )/P (Y ?|W ) (the features are extracted
from the remaining words in the corpus). Using M
sampled segmentations S1, S2, . . . Sm we compute
ES|Y ?,W [fi] as follows:
ES|Y ?,W [fi] ?
1
M
?
j
fi[Sj ]
Similarly, to compute ES,Y |W we sample a seg-
mentation and a label for each word. We com-
pute the joint probability of P (Y, S|W ) for each
segmentation-label pair using Eq. (1). A sampled
segmentation can introduce new units, which may
have higher probability than existing ones.
Using these approximations in Eq. (5), we update
the parameters using gradient ascent:
??new = ??old + ??`??(Y
?|W )
where ? > 0 is the learning rate.
To obtain the best segmentation, we use determin-
istic annealing. Sampling operates as usual, except
that the parameters are divided by a value, which
starts large and gradually drops to zero. To make
burn in faster for sampling, the sampler is initialized
with the most likely segmentation from the previous
iteration. To initialize the sampler the first time, we
set al the parameters to zero (only the priors have
non-zero values) and run deterministic annealing to
obtain the first segmentation of the corpus.
3.2 Efficient Sampling
Sampling a segmentation for the corpus requires
computing the normalization constant (3), which
contains a summation over all possible corpus seg-
mentations. Instead, we approximate this constant
by sampling words independently, keeping fixed all
other segmentations. Still, even sampling a single
word?s segmentation requires enumerating probabil-
ities for all possible segmentations.
We sample a segmentation efficiently using dy-
namic programming. We can represent all possible
segmentations for a word as a finite state machine
(FSM) (Figure 3), where arcs weights arise from
scoring the segmentation?s features. This weight is
the negative log probability of the resulting model
after adding the corresponding features and priors.
However, the lexicon prior poses a problem for
this construction since the penalty incurred by a new
unit in the segmentation depends on whether that
unit is present elsewhere in that segmentation. For
example, consider the segmentation for the word
ANJANI: AA N, JH, AA N, IY. If none of these units
are in the lexicon, this segmentation yields the low-
est prior penalty since it repeats the unit AA N. 3 This
global dependency means paths must encode the full
unit history, making computing forward-backward
probabilities inefficient.
Our solution is to use the Metropolis-Hastings al-
gorithm, which samples from the true distribution
P (Y, S|W ) by first sampling a new label and seg-
mentation (y?, s?) from a simpler proposal distribu-
tion Q(Y, S|W ). The new assignment (y?, s?) is ac-
cepted with probability:
?(Y ?, S?|Y, S,W )=min
?
1,
P (Y ?, S?|W )Q(Y, S|Y ?, S?,W )
P (Y, S|W )Q(Y ?, S?|Y, S,W )
?
We choose the proposal distribution Q(Y, S|W )
as Eq. (1) omitting the lexicon prior, removing the
challenge for efficient computation. The probability
of accepting a sample becomes:
?(Y ?, S?|Y, S,W )=min
?
1,
P
??S? |?|P
??S |?|
?
(6)
We sample a path from the FSM by running the
forward-backward algorithm, where the backward
computations are carried out explicitly, and the for-
ward pass is done through sampling, i.e. we traverse
the machine only computing forward probabilities
for arcs leaving the sampled state.4 Once we sample
a segmentation (and label) we accept it according to
Eq. (6) or keep the previous segmentation if rejected.
Alg. 1 shows our full sub-word learning proce-
dure, where sampleSL (Alg. 2) samples a segmen-
tation and label sequence for the entire corpus from
P (Y, S|W ), and sampleS samples a segmentation
from P (S|Y ?,W ).
3Splitting at phone boundaries yields the same lexicon prior
but a higher corpus prior.
4We use OpenFst?s RandGen operation with a costumed arc-
selector (http://www.openfst.org/).
715
0 1AA
5
AA_N_JH_AA_N
4
AA_N_JH_AA
3
AA_N_JH 2
AA_N
N_JH_AA_N
N_JH_AA
N_JH
N
6
N_JH_AA_N_IY
IY
N
AA_NAA
AA_N_IY
JH_AA_N
JH_AA
JH
JH_AA_N_IY
Figure 3: FSM representing all segmentations for the word ANJANI with pronunciation: AA,N,JH,AA,N,IY
Algorithm 1 Training
Input: Lexicon L from training text W , Dictionary D,
Mapping M , L2S pronunciations, Annealing temp T .
Initialization:
Assign label y?m = M [wm]. ??0 = 0?
S0 = random segmentation for each word in L.
for i = 1 to K do
/* E-Step */
Si = bestSegmentation(T, ?i?1, Si?1).
for k = 1 to NumSamples do
(S?k, Y
?
k) = sampleSL(P (Y, Si|W ),Q(Y, Si|W ))
S?k = sampleS(P (Si|Y ?,W ),Q(Si|Y ?,W ))
end for
/* M-Step */
ES,Y |W [fi] =
1
NumSamples
?
k f?,l[S
?
k, Y
?
k]
ES|Y ?,W [f?,l] =
1
NumSamples
?
k f?,l[S?k, Y
?]
??i = ??i?1 + ??L??(Y
?|W )
end for
S = bestSegmentation(T, ?K , S0)
Output: Lexicon Lo from S
4 OOV Detection Using Hybrid Models
To evaluate our model for learning sub-word units,
we consider the task of out-of-vocabulary (OOV)
word detection. OOV detection for ASR output can
be categorized into two broad groups: 1) hybrid
(filler) models: which explicitly model OOVs us-
ing either filler, sub-words, or generic word mod-
els (Bazzi, 2002; Schaaf, 2001; Bisani and Ney,
2005; Klakow et al, 1999; Wang, 2009); and
2) confidence-based approaches: which label un-
reliable regions as OOVs based on different con-
fidence scores, such as acoustic scores, language
models, and lattice scores (Lin et al, 2007; Burget
et al, 2008; Sun et al, 2001; Wessel et al, 2001).
In the next section we detail the OOV detection
approach we employ, which combines hybrid and
Algorithm 2 sampleSL(P (S, Y |W ), Q(S, Y |W ))
for m = 1 to M (NumWords) do
(s?m, y
?
m) = Sample segmentation/label pair for
word wm according to Q(S, Y |W )
Y ? = {y1 . . . ym?1y?mym+1 . . . yM}
S? = {s1 . . . sm?1s?msm+1 . . . sM}
?=min
(
1,
P
??S? |?|P
??S |?|
)
with prob ? : ym,k = y?m, sm,k = s
?
m
with prob (1? ?) : ym,k = ym, sm,k = sm
end for
return (S?k, Y
?
k) = [(s1,k, y1,k) . . . (sM,k, yM,k)]
confidence-based models, achieving state-of-the art
performance for this task.
4.1 OOV Detection Approach
We use the state-of-the-art OOV detection model of
Parada et al (2010), a second order CRF with fea-
tures based on the output of a hybrid recognizer.
This detector processes hybrid recognizer output, so
we can evaluate different sub-word unit lexicons for
the hybrid recognizer and measure the change in
OOV detection accuracy.
Our model (?2.1) can be applied to this task by
using a dictionary D to label words as IV (yi = 0 if
wi ? D) and OOV (yi = 1 if wi /? D). This results
in a labeled corpus, where the labeling sequence Y
indicates the presence of out-of-vocabulary words
(OOVs). For comparison we evaluate a baseline
method (Rastrow et al, 2009b) for selecting units.
Given a sub-word lexicon, the word and sub-
words are combined to form a hybrid language
model (LM) to be used by the LVCSR system. This
hybrid LM captures dependencies between word and
sub-words. In the LM training data, all OOVs are
represented by the smallest number of sub-words
which corresponds to their pronunciation. Pronun-
ciations for all OOVs are obtained using grapheme
716
to phone models (Chen, 2003).
Since sub-words represent OOVs while building
the hybrid LM, the existence of sub-words in ASR
output indicate an OOV region. A simple solution to
the OOV detection problem would then be reduced
to a search for the sub-words in the output of the
ASR system. The search can be on the one-best
transcripts, lattices or confusion networks. While
lattices contain more information, they are harder
to process; confusion networks offer a trade-off be-
tween richness (posterior probabilities are already
computed) and compactness (Mangu et al, 1999).
Two effective indications of OOVs are the exis-
tence of sub-words (Eq. 7) and high entropy in a
network region (Eq. 8), both of which are used as
features in the model of Parada et al (2010).
Sub-word Posterior =
?
??tj
p(?|tj) (7)
Word-Entropy =?
?
w?tj
p(w|tj) log p(w|tj) (8)
tj is the current bin in the confusion network and
? is a sub-word in the hybrid dictionary. Improving
the sub-word unit lexicon, improves the quality of
the confusion networks for OOV detection.
5 Experimental Setup
We used the data set constructed by Can et al
(2009) (OOVCORP) for the evaluation of Spoken
Term Detection of OOVs since it focuses on the
OOV problem. The corpus contains 100 hours of
transcribed Broadcast News English speech. There
are 1290 unique OOVs in the corpus, which were
selected with a minimum of 5 acoustic instances per
word and short OOVs inappropriate for STD (less
than 4 phones) were explicitly excluded. Example
OOVs include: NATALIE, PUTIN, QAEDA,
HOLLOWAY, COROLLARIES, HYPERLINKED,
etc. This resulted in roughly 24K (2%) OOV tokens.
For LVCSR, we used the IBM Speech Recogni-
tion Toolkit (Soltau et al, 2005)5 to obtain a tran-
script of the audio. Acoustic models were trained
on 300 hours of HUB4 data (Fiscus et al, 1998)
and utterances containing OOV words as marked in
OOVCORP were excluded. The language model was
trained on 400M words from various text sources
5The IBM system used speaker adaptive training based on
maximum likelihood with no discriminative training.
with a 83K word vocabulary. The LVCSR system?s
WER on the standard RT04 BN test set was 19.4%.
Excluded utterances amount to 100hrs. These were
divided into 5 hours of training for the OOV detec-
tor and 95 hours of test. Note that the OOV detector
training set is different from the LVCSR training set.
We also use a hybrid LVCSR system, combin-
ing word and sub-word units obtained from ei-
ther our approach or a state-of-the-art baseline ap-
proach (Rastrow et al, 2009a) (?5.2). Our hybrid
system?s lexicon has 83K words and 5K or 10K
sub-words. Note that the word vocabulary is com-
mon to both systems and only the sub-words are se-
lected using either approach. The word vocabulary
used is close to most modern LVCSR system vo-
cabularies for English Broadcast News; the result-
ing OOVs are more challenging but more realistic
(i.e. mostly named entities and technical terms). The
1290 words are OOVs to both the word and hybrid
systems.
In addition we report OOV detection results on a
MIT lectures data set (Glass et al, 2010) consisting
of 3 Hrs from two speakers with a 1.5% OOV rate.
These were divided into 1 Hr for training the OOV
detector and 2 Hrs for testing. Note that the LVCSR
system is trained on Broadcast News data. This out-
of-domain test-set help us evaluate the cross-domain
performance of the proposed and baseline hybrid
systems. OOVs in this data set correspond mainly to
technical terms in computer science and math. e.g.
ALGORITHM, DEBUG, COMPILER, LISP.
5.1 Learning parameters
For learning the sub-words we randomly selected
from training 5,000 words which belong to the 83K
vocabulary and 5,000 OOVs6. For development we
selected an additional 1,000 IV and 1,000 OOVs.
This was used to tune our model hyper parameters
(set to ? = ?1, ? = ?20). There is no overlap
of OOVs in training, development and test sets. All
feature weights were initialized to zero and had a
Gaussian prior with variance ? = 100. Each of the
words in training and development was converted to
their most-likely pronunciation using the dictionary
6This was used to obtain the 5K hybrid system. To learn sub-
words for the 10K hybrid system we used 10K in-vocabulary
words and 10K OOVs. All words were randomly selected from
the LM training text.
717
for IV words or the L2S model for OOVs.7
The learning rate was ?k =
?
(k+1+A)? , where k is
the iteration,A is the stability constant (set to 0.1K),
? = 0.4, and ? = 0.6. We used K = 40 itera-
tions for learning and 200 samples to compute the
expectations in Eq. 5. The sampler was initialized
by sampling for 500 iterations with deterministic an-
nealing for a temperature varying from 10 to 0 at 0.1
intervals. Final segmentations were obtained using
10, 000 samples and the same temperature schedule.
We limit segmentations to those including units of at
most 5 phones to speed sampling with no significant
degradation in performance. We observed improved
performance by dis-allowing whole word units.
5.2 Baseline Unit Selection
We used Rastrow et al (2009a) as our baseline
unit selection method, a data driven approach where
the language model training text is converted into
phones using the dictionary (or a letter-to-sound
model for OOVs), and a N-gram phone LM is es-
timated on this data and pruned using a relative en-
tropy based method. The hybrid lexicon includes
resulting sub-words ? ranging from unigrams to 5-
gram phones, and the 83K word lexicon.
5.3 Evaluation
We obtain confusion networks from both the word
and hybrid LVCSR systems. We align the LVCSR
transcripts with the reference transcripts and tag
each confusion region as either IV or OOV. The
OOV detector classifies each region in the confusion
network as IV/OOV. We report OOV detection accu-
racy using standard detection error tradeoff (DET)
curves (Martin et al, 1997). DET curves measure
tradeoffs between false alarms (x-axis) and misses
(y-axis), and are useful for determining the optimal
operating point for an application; lower curves are
better. Following Parada et al (2010) we separately
evaluate unobserved OOVs.8
7In this work we ignore pronunciation variability and sim-
ply consider the most likely pronunciation for each word. It
is straightforward to extend to multiple pronunciations by first
sampling a pronunciation for each word and then sampling a
segmentation for that pronunciation.
8Once an OOV word has been observed in the OOV detector
training data, even if it was not in the LVCSR training data, it is
no longer truly OOV.
6 Results
We compare the performance of a hybrid sys-
tem with baseline units9 (?5.2) and one with units
learned by our model on OOV detection and phone
error rate. We present results using a hybrid system
with 5k and 10k sub-words.
We evaluate the CRF OOV detector with two dif-
ferent feature sets. The first uses only Word En-
tropy and Sub-word Posterior (Eqs. 7 and 8) (Fig-
ure 4)10. The second (context) uses the extended
context features of Parada et al (2010) (Figure 5).
Specifically, we include all trigrams obtained from
the best hypothesis of the recognizer (a window of 5
words around current confusion bin). Predictions at
different FA rates are obtained by varying a proba-
bility threshold.
At a 5% FA rate, our system (This Paper 5k) re-
duces the miss OOV rate by 6.3% absolute over the
baseline (Baseline 5k) when evaluating all OOVs.
For unobserved OOVs, it achieves 3.6% absolute
improvement. A larger lexicon (Baseline 10k and
This Paper 10k ) shows similar relative improve-
ments. Note that the features used so far do not nec-
essarily provide an advantage for unobserved ver-
sus observed OOVs, since they ignore the decoded
word/sub-word sequence. In fact, the performance
on un-observed OOVs is better.
OOV detection improvements can be attributed to
increased coverage of OOV regions by the learned
sub-words compared to the baseline. Table 1 shows
the percent of Hits: sub-word units predicted in
OOV regions, and False Alarms: sub-word units
predicted for in-vocabulary words. We can see
that the proposed system increases the Hits by over
8% absolute, while increasing the False Alarms by
0.3%. Interestingly, the average sub-word length
for the proposed units exceeded that of the baseline
units by 0.3 phones (Baseline 5K average length
was 2.92, while that of This Paper 5K was 3.2).
9Our baseline results differ from Parada et al (2010). When
implementing the lexicon baseline, we discovered that their hy-
brid units were mistakenly derived from text containing test
OOVs. Once excluded, the relative improvements of previous
work remain, but the absolute error rates are higher.
10All real-valued features were normalized and quantized us-
ing the uniform-occupancy partitioning described in White et
al. (2007). We used 50 partitions with a minimum of 100 train-
ing values per partition.
718
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 4: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on OOVCORP data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
35
40
45
50
55
60
65
70
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA10
20
30
40
50
60
70
80
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 5: Effect of adding context features to baseline and discriminative hybrid systems on OOVCORP data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
Consistent with previously published results, in-
cluding context achieves large improvement in per-
formance. The proposed hybrid system (This Pa-
per 10k + context-features) still improves over the
baseline (Baseline 10k + context-features), however
the relative gain is reduced. In this case, we ob-
tain larger gains for un-observed OOVs which ben-
efit less from the context clues learned in training.
Lastly, we report OOV detection performance on
MIT Lectures. Both the sub-word lexicon and the
LVCSR models were trained on Broadcast News
data, helping us evaluate the robustness of learned
sub-words across domains. Note that the OOVs
in these domains are quite different: MIT Lec-
tures? OOVs correspond to technical computer sci-
Hybrid System Hits FAs
Baseline (5k) 18.25 1.49
This Paper (5k) 26.78 1.78
Baseline (10k) 24.26 1.82
This Paper (10k) 28.96 1.92
Table 1: Coverage of OOV regions by baseline and pro-
posed sub-words in OOVCORP.
ence and math terms, while in Broadcast News they
are mainly named-entities.
Figure 6 and 7 show the OOV detection results in
the MIT Lectures data set. For un-observed OOVs,
the proposed system (This Paper 10k) reduces the
miss OOV rate by 7.6% with respect to the base-
line (Baseline 10k) at a 5% FA rate. Similar to
Broadcast News results, we found that the learned
sub-words provide larger coverage of OOV regions
in MIT Lectures domain. These results suggest that
the proposed sub-words are not simply modeling the
training OOVs (named-entities) better than the base-
line sub-words, but also describe better novel unex-
pected words. Furthermore, including context fea-
tures does not seem as helpful. We conjecture that
this is due to the higher WER11 and the less struc-
tured nature of the domain: i.e. ungrammatical sen-
tences, disfluencies, incomplete sentences, making
it more difficult to predict OOVs based on context.
11WER = 32.7% since the LVCSR system was trained on
Broadcast News data as described in Section 5.
719
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (5k) This Paper (5k) Baseline (10k) This Paper (10k)
(b)
Figure 6: DET curves for OOV detection using baseline hybrid systems for different lexicon size and proposed dis-
criminative hybrid system on MIT Lectures data set. Evaluation on un-observed OOVs (a) and all OOVs (b).
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(a)
0 5 10 15 20%FA30
40
50
60
70
80
90
%M
isses
Baseline (10k) Baseline (10k) + context-features This Paper (10k) This Paper (10k) + context-features
(b)
Figure 7: Effect of adding context features to baseline and discriminative hybrid systems on MIT Lectures data set.
Evaluation on un-observed OOVs (a) and all OOVs (b).
6.1 Improved Phonetic Transcription
We consider the hybrid lexicon?s impact on Phone
Error Rate (PER) with respect to the reference tran-
scription. The reference phone sequence is obtained
by doing forced alignment of the audio stream to the
reference transcripts using acoustic models. This
provides an alignment of the pronunciation variant
of each word in the reference and the recognizer?s
one-best output. The aligned words are converted to
the phonetic representation using the dictionary.
Table 2 presents PERs for the word and differ-
ent hybrid systems. As previously reported (Ras-
trow et al, 2009b), the hybrid systems achieve bet-
ter PER, specially in OOV regions since they pre-
dict sub-word units for OOVs. Our method achieves
modest improvements in PER compared to the hy-
brid baseline. No statistically significant improve-
ments in PER were observed on MIT Lectures.
7 Conclusions
Our probabilistic model learns sub-word units for
hybrid speech recognizers by segmenting a text cor-
pus while exploiting side information. Applying our
System OOV IV All
Word 1.62 6.42 8.04
Hybrid: Baseline (5k) 1.56 6.44 8.01
Hybrid: Baseline (10k) 1.51 6.41 7.92
Hybrid: This Paper (5k) 1.52 6.42 7.94
Hybrid: This Paper (10k) 1.45 6.39 7.85
Table 2: Phone Error Rate for OOVCORP.
method to the task of OOV detection, we obtain an
absolute error reduction of 6.3% and 7.6% at a 5%
false alarm rate on an English Broadcast News and
MIT Lectures task respectively, when compared to a
baseline system. Furthermore, we have confirmed
previous work that hybrid systems achieve better
phone accuracy, and our model makes modest im-
provements over a baseline with a similarly sized
sub-word lexicon. We plan to further explore our
new lexicon?s performance for other languages and
tasks, such as OOV spoken term detection.
Acknowledgments
We gratefully acknowledge Bhuvaha Ramabhadran
for many insightful discussions and the anonymous
reviewers for their helpful comments. This work
was funded by a Google PhD Fellowship.
720
References
Issam Bazzi and James Glass. 2001. Learning units
for domain-independent out-of-vocabulary word mod-
eling. In EuroSpeech.
Issam Bazzi. 2002. Modelling out-of-vocabulary words
for robust speech recognition. Ph.D. thesis, Mas-
sachusetts Institute of Technology.
M. Bisani and H. Ney. 2005. Open vocabulary speech
recognition with flat hybrid models. In INTER-
SPEECH.
L. Burget, P. Schwarz, P. Matejka, M. Hannemann,
A. Rastrow, C. White, S. Khudanpur, H. Hermansky,
and J. Cernocky. 2008. Combination of strongly and
weakly constrained recognizers for reliable detection
of OOVS. In ICASSP.
D. Can, E. Cooper, A. Sethy, M. Saraclar, and C. White.
2009. Effect of pronounciations on OOV queries in
spoken term detection. Proceedings of ICASSP.
Stanley F. Chen. 2003. Conditional and joint models
for grapheme-to-phoneme conversion. In Eurospeech,
pages 2033?2036.
G. Choueiter. 2009. Linguistically-motivated sub-
word modeling with applications to speech recogni-
tion. Ph.D. thesis, Massachusetts Institute of Technol-
ogy.
Jonathan Fiscus, John Garofolo, Mark Przybocki,
William Fisher, and David Pallett, 1998. 1997 En-
glish Broadcast News Speech (HUB4). Linguistic
Data Consortium, Philadelphia.
James Glass, Timothy Hazen, Lee Hetherington, and
Chao Wang. 2010. Analysis and processing of lec-
ture audio data: Preliminary investigations. In North
American Chapter of the Association for Computa-
tional Linguistics (NAACL).
Dietrich Klakow, Georg Rose, and Xavier Aubert. 1999.
OOV-detection in large vocabulary system using au-
tomatically defined word-fragments as fillers. In Eu-
rospeech.
Hui Lin, J. Bilmes, D. Vergyri, and K. Kirchhoff. 2007.
OOV detection by joint word/phone lattice alignment.
In ASRU, pages 478?483, Dec.
Jonathan Mamou, Bhuvana Ramabhadran, and Olivier
Siohan. 2007. Vocabulary independent spoken term
detection. In Proceedings of SIGIR.
L. Mangu, E. Brill, and A. Stolcke. 1999. Finding con-
sensus among words. In Eurospeech.
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and
M. Przybocky. 1997. The det curve in assessment of
detection task performance. In Eurospeech.
Carolina Parada, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009. Query-by-example spoken term detec-
tion for oov terms. In ASRU.
Carolina Parada, Mark Dredze, Denis Filimonov, and
Fred Jelinek. 2010. Contextual information improves
oov detection in speech. In North American Chap-
ter of the Association for Computational Linguistics
(NAACL).
H. Poon, C. Cherry, and K. Toutanova. 2009. Unsu-
pervised morphological segmentation with log-linear
models. In ACL.
Ariya Rastrow, Abhinav Sethy, and Bhuvana Ramab-
hadran. 2009a. A new method for OOV detection
using hybrid word/fragment system. Proceedings of
ICASSP.
Ariya Rastrow, Abhinav Sethy, Bhuvana Ramabhadran,
and Fred Jelinek. 2009b. Towards using hybrid,
word, and fragment units for vocabulary independent
LVCSR systems. INTERSPEECH.
T. Schaaf. 2001. Detection of OOV words using gen-
eralized word models and a semantic class language
model. In Eurospeech.
H. Soltau, B. Kingsbury, L. Mangu, D. Povey, G. Saon,
and G. Zweig. 2005. The ibm 2004 conversational
telephony system for rich transcription. In ICASSP.
H. Sun, G. Zhang, f. Zheng, and M. Xu. 2001. Using
word confidence measure for OOV words detection in
a spontaneous spoken dialog system. In Eurospeech.
Stanley Wang. 2009. Using graphone models in au-
tomatic speech recognition. Master?s thesis, Mas-
sachusetts Institute of Technology.
F. Wessel, R. Schluter, K. Macherey, and H. Ney. 2001.
Confidence measures for large vocabulary continuous
speech recognition. IEEE Transactions on Speech and
Audio Processing, 9(3).
Christopher White, Jasha Droppo, Alex Acero, and Ju-
lian Odell. 2007. Maximum entropy confidence esti-
mation for speech recognition. In ICASSP.
721
