The Robustness of Domain Lexico-Taxonomy:  
Expanding Domain Lexicon with CiLin  
Chu-Ren Huang
Institute of Linguistics, 
Academia Sinica, Taipei 
churen@sinica.edu.tw
Xiang-Bing Li 
Institute of Information Science, 
Academia Sinica, Taipei  
dreamer@hp.iis.sinica.edu.tw 
Jia-Fei Hong
Institute of Linguistics, 
Academia Sinica, Taipei
jiafei@gate.sinica.edu.tw
Abstract.
This paper deals with the robust 
expansion of Domain Lexico- 
Taxonomy (DLT). DLT is a domain 
taxonomy enriched with domain lexica. 
DLT was proposed as an infrastructure 
for crossing domain barriers (Huang et 
al. 2004). The DLT proposal is based 
on the observation that domain lexica 
contain entries that are also part of a 
general lexicon. Hence, when entries of 
a general lexicon are marked with their 
associated domain attributes, this 
information can have two important 
applications. First, the DLT will serve 
as seeds for domain lexica. Second, the 
DLT offers the most reliable evidence 
for deciding the domain of a new text 
since these lexical clues belong to the 
general lexicon and do occur reliably in 
all texts. Hence general lexicon 
lemmas are extracted to populate 
domain lexica, which are situated in 
domain taxonomy. Based on this 
previous work, we show in this paper 
that the original DLT can be further 
expanded when a new language 
resource is introduced. We applied 
CiLin, a Chinese thesaurus, and added 
more than 1000 new entries for DLT 
and show with evaluation that the DLT 
approach is robust since the size and 
number of domain lexica increased 
effectively. 
1.  Introduction 
Domain-based language processing has an 
inherent research dilemma when the 
construction of domain lexicons is involved. 
The standard approach of building domain 
lexicon from domain corpora requires a very 
high threshold of existing domain resources and 
knowledge. Since only well-documented 
domains can provide enough quality corpora, it 
is likely these fields already have good manually 
constructed domain lexica. Hence this approach 
is can only deal with domains where only 
marginal benefit can be achieved, while it 
cannot deal with domains where it can make 
most contribution since there is not enough 
resources to work with. 
It was observed that the type of domain 
language processing that has the widest 
application and best potentials are cross-domain 
and multi-domain in nature. For instance, a 
typical web-search is a search for specific 
domain information from the www as an archive 
of mixed and heterogeneous domains. The 
contribution will be immediate and salient to be 
able to acquire resources and information for a 
new domain that is not well documented yet. 
A new approach towards domain language 
processing by constructing an infrastructure for 
multi-domain language processing called the 
Domain Lexico-Taxonomy (DLT) was proposed 
in Huang et al (2004). In the DLT approach, 
domain lexica are semi-automatically acquired 
to populate domain taxonomy. This lexically 
populated domain taxonomy serves two 
purposes: as the basis of stylo-statistical 
prediction of the domain of a new text, and as 
the core seed of complete domain lexica. For the 
first purpose, the DLT approach relies crucially 
on the ability to effectively identify words that 
are good indicators of specific domains. For the 
second purpose, the DLT needs to be robust 
enough to allow incremental expansion when 
new content resources are integrated. In this 
study, we integrate CiLin, a Chinese thesaurus, 
to show that the DLT architecture is indeed 
robust.
103
2.  Related Work
Typical studies on domain lexica focuses on 
assigning texts to specific classes, hence they
use a limited taxonomy augmented with a small 
set of features (e.g. Avancini et al 2003,
Sebastiani 2002, and Yand and Pederson 1997).
However, specialized lemmas cannot be useful 
in multi-domain processing. To achieve domain
versatility in processing, it is necessary to 
identify lemmas with wider distributions and yet
is associated with particular domain(s). We
follow the DLT architecture (Huang et al 2004), 
which was shown to be effective in predicting 
the domain of documents extracted from the 
web. We aim to elaborate that framework by 
proposing a domain lexica can be incrementally 
expanded with knowledge from a new resource. 
3.  Domain Taxonomy
A domain taxonomy containing 549 nodes was 
manually constructed. The main sources of
domain classification are from Chinese Library 
Classification system, Encyclopedia Britannica 
and the Global View English-Chinese dictionary.
Two important criteria were chosen: that the 
taxonomy is bilingual and that it is maintained
locally. First, the bilingual taxonomy is essential
for future cross-lingual processing but also 
allows us to access relevant resources in both 
languages. Second, since our emphasis was not 
on the correctness of a dogmatic taxonomy but
on the flexibility that allows monotonic
extensions, it is essential to be able to monitor
any changes in the taxonomy.
There are four layers in the constructed 
domain taxonomy. Fourteen (14) domains are in 
the upper layer, including Humanities, Social 
Science, Formal Science, Natural Science,
Medical Science, Engineering Science,
Agriculture and Industry, Fine Arts, Recreation,
Proper Name, Genre/Strata, Etymology, Country
Name, Country People. The Second layer has 
147 domains. The third layer has 279 domains.
Lastly the fourth layer has only 109 domains
since not all branches need to be expanded at 
this level. In sum, there are 549 possible domain
tags when the hierarchy is ignored. The domain
taxonomy is available online at the Sinica BOW
website (http://BOW.sinica.edu.tw/, Huang and
Chang 2004).
4. Detection of Domain Lexicon in DLT
The challenge in integrating heterogeneous 
language resources for domain information is 
that conceptual classification varies from one 
resource to another and hence cannot be directly
harvested. We propose to utilize the inheritance 
relations of these resources, instead of their 
hierarchy. In other words, lexical (and hence 
conceptual) identity is established first, 
following by expanding this matching with 
logical inheritance but without branching out on
the conceptual hierarchy.
DLT establish the correspondences 
between the taxonomic nodes of domains and 
the linguistic resources of sub-lexica. Note that 
a lexical knowledgebase, in a Wordnet fashion,
also contains hierarchical relations. The domain
taxonomy can be enriched by taking the
hierarchical information internal to the lexica. If 
these resources directly encodes the ?is-a?
relation by hyponymy, we assume that both the
node (lexicons) and their hyponym node 
(lexicons) belong to that domain. Using the
simple supposition, we can observe the domain
knowledge with various resources, and
strengthens the domain lexica for domain
taxonomy. The process of populating DLT is
shown in Fig. 1. 
Figure. 1. Populating DLT from Linguistic Resource
104
5.  Experiment 
5.1. The Original Study with Bilingual 
WordNet
The original DLT work was based on bilingual 
Wordnet (Huang et al 2004). This is because of 
the Wordnet lexical knowledgebase is highly 
enriched with lexical semantic relation 
information. In addition, the bilingual Wordnet 
adds an unparallel dimension of knowledge 
coverage. The bilingual Wordnet used is Sinica 
BOW (The Academia Sinica Bilingual 
Ontological WordNet, Huang and Chang 
(2004)). Sinica BOW is bilingual lexical 
knowledgebase connecting WordNet and SUMO 
and mapping both between English and Chinese. 
The study reported in Huang et al (2004) also 
contains a small domain identification 
experiment to show the application of DLT.  
5.1.1 Description of WordNet and Sinica 
BOW
WordNet is inspired by current psycholinguistic 
and computational theories of human lexical 
memory (Fellbaum (1998), Miller et al (1993)). 
English nouns, verbs, adjectives, and adverbs 
are organized into synonym sets, each 
representing one underlying lexicalized concept. 
Different semantic relations link the synonym 
sets (synsets). The version of WordNet that 
Sinica BOW implemented is version 1.6, with 
nearly 100,000 synsets. 
In Sinica BOW, ach English synset was 
given up to 3 most appropriate Chinese 
translation equivalents. And in cases where the 
translation pairs are not synonyms, their 
semantic relations are marked (Huang et al 
2003). The bilingual WordNet is further linked 
to the SUMO ontology. We use the semantic 
relations in bilingual resource to expand and 
predict domain classification when it cannot be 
judged directly from a lexical lemma. 
5.1.2 Experiment and Result with WordNet 
463 of the 549 nodes in the domain taxonomy 
were successfully mapped to a WordNet synset 
through an identical lemma. 452 or 463 
mappings were manually confirmed to be 
correct, a precision score of over 97%. These 
domains were expanded to cover a total of 
11,918 synsets corresponding to 15,160 Chinese 
lemmas. Note that both English and Chinese 
correspondences are used since our resources 
(WordNet and domain taxonomy) are both 
bilingual.
Due mostly to hyponymy expansion, each 
lemma is mapped to 1.38 domains in average. 
While each lemma is assigned to no more than 8 
domains, with the majority (6,464) assigned to 
only one. These mapped lemmas populate a set 
of domain lexica. The number of entries in these 
domain lexica ranges from 1 to 3762. The 
average size of these domain lexica is 32.8 
lemmas. Only 41 domains lexical contain 33 or 
more lemmas. Since we cannot know the 
effective of the lexicon of a domain a priori, we 
take those whose size are above average as the 
effective domain lexica. 
These domain lexica and their sizes are 
shown in Table 1. 
5.1.3 Evaluation: precision of domain lexica 
It is impossible to formally evaluate the recall 
rate of this domain lexica study since we do not 
know the total number of entries to be recalled. 
However, it is possible to evaluate the precision 
rate of the constructed domain lexica. First, the 
precision of all recalled lemmas is tested. 
Among the mapped lemmas, 8696 (out of 
15,160) lemmas are assigned to multiple 
domains, while 6,464 are assigned to single 
domain. The single domain mappings were 
spot-checked to be correct. On the other hand, 
the precision of all 8,696 multi-domain lemmas 
are carefully evaluated. Among these lemmas, 
only 4.81% (418) proves to be wrong; and an 
overwhelming majority of 95.19% turns out to 
be correct (8278). 
Second, a more meaningful test is to 
evaluate how well the domain lexica are defined. 
Five effective domain lexica with over 100 
entries were randomly chosen for evaluation: 
Insect (515 entries), Natural Science (262 
entries), Sports (180 entries), Dance (124 entries) 
and Religious Music (48 entries).  
The manually checked precision of these 
domain lexica is listed below the Table 2: 
105
Domain Domain Domain Domain
Vertebrates 
???3676 ? Food ?2968 ? Bird ??  1059 Fish ?? 729 
Language ??
699 
Recreation
???? 548 Insect ??  515 
Natural Science 
???? 262 
Country?? 250 contest?? 207 music?? 192 Indian???188 
Sports!?? 180 commerce?? 144 Business ?? 144 Dance?? 124 
Heraldic design 
???? 120 
Medical Science 
????  85 Medicine ?? 76 
Pathological 
medicine ???
? 76 
Clinical medicine 
???? 76 
Mathematics 
??  69 
Humanities 
???64 ? 
Social Science 
????  62 
physics??? 56 Religion?? 52 
Religious Music 
???? 48 
Plastic art 
???? 45 
Pure mathematics 
??? 44 
Anthropology 
??? 42 
Earth science 
???? 39 drawing?? 4:
Norse Mythology 
???? 39 Philosophy ?? 37
Telecommunication
???? 35 theater?? 34 
Fine Arts?? 33 
Table 1. Domain lexica containing 33 or more lemmas 
Domain Label # of entries Precision (%) 
Insect 515 99.03 
Natural Science 262 69.85 
Sports 180 86.11 
Dance 124 100.00 
Religious Music 48 93.75 
Table 2. Size and Precision of selected domain lexica 
Table 2 shows an overall precision of over 95%, 
while no other lexica has precision lower than 
86%, natural science is lowest at just below 70%. 
This is because ?Natural Science? is a higher 
level domain and hence open to more noises in 
the detection process. This study clearly showed 
that the WordNet helped to effectively build 
core domain lexica. 
We take the domain ?Dance? as an 
example to explain the process. First, we map 
?Dance? to the Wordnet synset??dance?, and 
we look for the hyponym synsets. Table3 will be 
shown the expanding lexica of one of hyponym 
synsets. These lexical entries are associated with 
domain ?Dance? and populate the domain 
lexicon.
Level synset 
1 social_dancing 
2 folk_dancing, folk_dance 
3 country-dance, country_dancing, 
4          square_dance, square_dancing 
5             quadrille 
5.2 For CiLin 
5.2.1 Description of CiLin 
CiLin, a short name for Tongyici CiLin, is a 
Chinese thesaurus published in 1984 (Mei et al 
1984). The terms in CiLin are organized in a 
conceptual hierarchy, with near-synonym terms 
forming a set. There are five levels in the 
taxonomy structure of CiLin. The CiLin terms 
between Level1 to Level4 are taxonomy 
categories. Level1 is the upper class, and it 
includes 12 categories, like as people, object, 
time and space, abstract etc. Level2 has 106 
categories. Level3 has 3,948 categories. Level4 
has 4,014 categories. There are 64,157 terms in 
Level5 since all branches need to be expanded 
at this level. These terms are classified to 12,193 
sets by the meaning. The average number of 
terms in each set is 5.34. Fig. 2 shows the 
structure of CiLin. 
Table 3. The expanding hyponym synsets of ?dance? 
106
Figure. 2. The structure of CiLin
5.2.2. Experiment and Result with CiLin
First, we map the 549 domains to CiLin?s
taxonomy. Unlike the previous study, only
Chinese terms were available on CiLin. The
result is given in Table 4.
# of
entries
# of
domains entries/domains
Leve1 1 146 1 146
Level 2 1,587 3 529
Level 4 1,222 32 38.19
Table 4. Number of expanding entries and 
mapping domains
Manual checking showed that mappings to 
Level 1 and Level 2 are both imprecise and 
small in number. Hence we take Level 4 as the 
lexical anchor for enriching domain lexica.
1,222 lexical items are expanded from 32 
domains, and these domain lexica and their sizes
are shown in Table 5. 
Domain Domain
Insect(??) -- 146 Sewing(??) -- 25
Country(??) -- 128 Movie(??) -- 25
Theater(??) -- 116 Game(??) -- 25
Painting(??) -- 88 Photography(??) -- 21
Capital(??) -- 54 Payment(??) -- 20
Cookery(??) -- 52 Printing(20 -- (??
Dance(??) -- 52 Literature(??) -- 18
Law(??) -- 50 Investment(??) -- 14
Education(??) -- 47 Swimming(??) -- 12
Martial_art(??) -- 45 Broadcasting(??) -- 11
Religion(??) -- 39 Ranching_and_animal_husbandry(??) -- 10
Architecture(??) -- 38 Textile_industry?? 10
Carving(?37 -- (? Boating(??) -- 8 
Language(??) -- 37 Trade(?7 -- (? 
Table 5. Domain lexica 
When all mappings are evaluated, 873(71.44%)
of them are correct, and 349 (28.56%) incorrect. 
Five effective domain lexica are evaluated, as
shown below in Table 6: 
Domain Label # of entries Precision (%)
Insect 146 58.9
Country 128 55.47
Theater 116 80.17
Painting 88 80.68
Dance 52 80.77
Table 6. Size and Precision of selected domain lexica
Compared with the work reported in (Huang et 
al. 2004), both the number of lemma (1,222 vs.
15,160) and precision (71.44% vs. nearly 95%)
are lower. This result is expected since CiLin
has a simple taxonomy without the rich lexical 
information of a Wordnet. The crucial fact
shown, however, is that DLT can be
incrementally enhanced with the new mappings. 
Of the 873 correct domain lexica entries, 79.5%
(694) are new entries that were not identified
previously. Even more impressive is the 
effectiveness of increase in lexica sizes for 
applicable domains, as shown below in Table 7. 
107
domain WN/old CiLin/new increase domain WN/old CiLin/new!increase!
?? 34 80 0.7018?? 12 15 0.5556
?? 515 65 0.1121?? 22 15 0.4054
?? 17 61 0.7821?? 28 14 0.3333
?? 250 44 0.1497?? 192 12 0.0588
?? 124 34 0.2152?? 20 11 0.3548
?? 7 33 0.8250?? 23 10 0.3030
?? 26 33 0.5593?? 15 9 0.3750
?? 14 32 0.6957?? 5 8 0.6154
?? 2 29 0.9355?? 9 7 0.4375
?? 2 27 0.9310?? 2 7 0.7778
?? 22 24 0.5217?? 0 5 1.0000
?? 26 23 0.4694?? 16 5 0.2381
?? 699 22 0.0305?? 27 4 0.1290
?? 52 21 0.2877?? 4 4 0.5000
?? 55 19 0.2568?? 6 2 0.2500
?? 21 17 0.4474?? 1 2 0.6667
Table 7. Increase in Domain Lexicon Size after CiLin Integration 
Table 7 shows that, even though adding CiLin 
only helped 32 domain lexica, 14 of them have 
their lexicon size more than doubled. One of 
them, ranching and animal husbandry is a new 
domain lexicon where no mapping was possible 
with WordNet. In other words, adding the CiLin 
resource substantially enhanced effective 
domain coverage of DLT. 
6.  Conclusion 
In this paper, test the robustness of the DLT 
architecture. We show both the coverage and the 
sizes of the domain lexica on DLT can be 
effectively expanded by integrating a new 
language resource. The robustness is convincing 
given that the coverage and quality of the new 
resource is actually not as good as the original 
reference resources. In other words, we showed 
the open architecture of DLT facilitates 
integration of new domain information without 
imposing any high threshold on the format and 
quality of new resources. We also verify partial 
results of previous work since 205 lemma 
mappings were repeated. For future work, we 
plan to continue to populate DLT, as well as to 
explore other possibilities for putting DLT to 
actual applications. 
References 
Chu-Ren Huang, Elanna I. J. Tseng, Dylan B. S. Tsai, 
Brian Murphy.  Cross-lingual Portability of 
Semantic relations: Bootstrapping Chinese 
WordNet with English WordNet Relations. 
Languages and Linguistics. 4.3. (2003)509-532 
Chu-Ren, Huang and Ru-Yng Chang. Sinica BOW 
(Bilingual Ontological Wordnet): Integration of 
Bilingual WordNet and SUMO?. Presented at the 
4th International Conference on Language 
Resources and Evaluation (LREC2004). Lisbon. 
Portugal. 26-28 May (2004) 
Chu-Ren Huang, Xiang-Bing Li, Jia-Fei Hong. 
"Domain Lexico-Taxonomy:An Approach 
Towards Multi-domain Language Processing",
Asian Symposium on Natural Language 
Processing to Overcome Language Barriers, The 
First International Joint Conference on Natural 
Language Processing (IJCNLP-04). Sanya City, 
Hainan Island, China. 22-24 March (2004) 
F. Sebastiani., ?Machine learning in automated text 
categorization?. ACM Computing Surveys, 34(1) 
(2002)1-47 
Fellbaum C.. WordNet: An Electronic Lexical 
Database. Cambridge: MIT Press (1998) 
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross 
and K. Miller. ?Introduction to WordNet: An 
On-line Lexical Database,? In Proceedings of the 
fifteenth International Joint Conference on 
108
Artificial Intelligence. Chamb?ry, France. 28 
August- 3 September (1993) 
Henri Avancini, Alberto Lavelli, Bernardo Magnini, 
Fabrizio Sebastiani, Roberto Zanoli. Expanding 
Domain-Specific Lexicons by Term 
Categorization. Proceedings of the 2003 ACM 
symposium on Applied computing. Melbourne, 
Florida, USA. 9-12 March (2003) 
Jia-ju Mei, Yi-Ming Zheng, Yun- Qi Gao and Hung- 
Xiang Yin. TongYiCi CiLin. Shanghai: the 
COMMERCIAL Press (1984) 
Y. Yang and J. O. Pedersen. A comparative study on 
feature selection in text categorization. In D. H. 
Fisher, editor, Proceedings of ICML-97, 14th 
International Conference on Machine Learning, 
pages 412 420. San Francisco: Morgan Kaufmann 
(1997) 
109
Cross-lingual Conversion of Lexical Semantic Relations: Building 
Parallel Wordnets 
Chu-Ren Huang1 , I-Li Su1, Jia-Fei Hong1, Xiang-Bing Li2
1. Institute of Linguistics 
2. Institute of Information Science 
Academia Sinica, 
No.128 Academic Sinica Road, SEC.2 Nankang, 
Taipei 115, Taiwan 
1.{ churen, isu, jiafei }@gate.sinica.edu.tw 
Abstract
Parallel wordnets built upon 
correspondences between different 
languages can play a crucial role in 
multilingual knowledge processing. Since 
there is no homomorphism between pairs of 
monolingual wordnets, we must rely on 
lexical semantic relation (LSR) mappings to 
ensure conceptual cohesion. In this paper, 
we propose and implement a model for 
bootstrapping parallel wordnets based on 
one monolingual wordnet and a set of 
cross-lingual lexical semantic relations. In 
particular, we propose a set of inference 
rules to predict Chinese wordnet structure 
based on English wordnet and 
English-Chinese translation relations. We 
show that this model of parallel wordnet 
building is effective and achieves higher 
precision in LSR prediction. 
1 Introduction 
A knowledgebase which systemizes 
lexical and conceptual information of 
human knowledge is a basic infrastructure 
for Natural Language Processing (NLP) 
applications. Wordnets, pioneered by the 
Princeton WordNet (WN, Fellbaum 1998), 
and greatly enriched by EuroWordnet (EWN, 
Vossen 1998), have become the standard for 
a lexical knowledgebase enriched with 
lexical semantic relations. In addition to the 
multilingual architecture of EWN, there are 
some proposals to construct the expansion 
for monolingual wordnets to parallel 
wordnet systems, such as Pianta and Girardi 
(2002). However, the construction of 
multilingual wordnets eventually faces the 
challenge of low-density languages, which 
is dealt with in Huang, et al (2002). 
Low-density languages, as opposed to 
high-density languages, usually refer to 
languages that are not spoken by a large 
number of people. However, there is neither 
a direct correspondence between language 
population and language technology, nor an 
objective population number that defines 
density level. In this work, we use the 
availability of language resources instead to 
define language density. That is, low-density 
languages are languages that do not have 
enough language resources to support fully 
automated language processing, such as 
machine translation. In our current line of 
work, we (Huang et al 2002) refer to 
low-density languages as those which do not 
have enough existing resources for 
semi-automatic construction of monolingual 
wordnet.
There are two alternative approaches to 
build parallel wordnets, as shown in Figure 
1. The first approach relies on two fully 
annotated monolingual wordnets with 
synsets and LSR?s. The second approach 
requires only one fully annotated WN in 
addition to LSR-based cross-lingual 
translation correspondences.  
48
Figure1. Two Approaches to Building Bilingual Wordnets
Approach I maps and pairs Language A
synsets with Language B synsets and 
annotates cross-lingual LSR?s. The result is 
a fully annotated parallel wordnet. Approach
II maps language A synsets to language B 
through translation equivalents. After 
language B synsets are thus established, 
language B LSR?s are predicted based on
corresponding LSR?s in language A. A new 
set of monolingual LSR?s is bootstrapped
and predicted basing on inference rules 
governed by translation LSR?s (T-LSR?s). In 
general, approach I applies to high-density
languages while approach II applies to
low-density languages. In this paper, we will 
focus on the application of approach II to
build a Chinese Wordnet with conceptual
cohesion.
The current model was first explored in
Huang et al (2003). This previous study
covered 210 lemmas, consisted of the top
ranked lemmas in each part-of-speech 
(POS). The translation LSR?s discussed in 
the previous model were antonymy,
hypernymy and hyponymy. In this current
work, we expand our study to all possible
LSR?s as well as to all the bilingual lexical 
pairs in our English-Chinese translation
equivalents databases. Moreover, the LSR?s
in Princeton WordNet are again used as the 
basis for bootstrapping. In addition, we 
establish a set of evaluation for the results.
The approach will be evaluated in term of 
both the precision of prediction and the
confidence of prediction. We aim to show 
that T-LSR?s bootstrapped approach does
provide an effective model for building
parallel wordnets for low-density languages. 
After the introduction, the main part of 
this paper consists of the following sections:
in section 2, we briefly introduce the 
existing resources required for this work. 
We discuss methodology of T-LSR
bootstrapping step by step in section 3. A 
series of LSR-predicting inference rules are
also given in this section. In section 4, we 
plan to evaluate the results of our 
experiment and demonstrate the feasibility
of maintaining conceptual cohesion in
cross-lingual LSR mapping.
2 Required Resources: ECTED and 
WN
As we mentioned above, the T-LSR
approach to parallel wordnet requires two
language resources: a fully annotated 
monolingual wordnet and a set of translation
LSR?s to map the wordnet information to 
the target language. In our current study, we 
use the English WN as the source of synset
and LSR information. The semantic relation 
between an English synset and its Chinese 
translation is based on The English-Chinese 
Translation Equivalents Database (ECTED, 
Huang et al 2002). 
2.1 The English-Chinese Translation
Equivalents Databases (ECTED)
The basic idea of ECTED is to provide 
the Chinese translation equivalents for each 
APPROACH I
Given fully annotated
monolingual wordents
with synsets and LSRs
Fully annotated
parallel wordnet
APPROACH II
Given fully annotated WN 
of language A; and
bilingual translation
equivalents annotated
with LSR
Map LSR-annotated
synsets in Language A to
Language B through
translation LSRs (T-LSR?s)
Grow LSR links among
Language B synsets by 
using language A LSR 
and cross-lingual LSR
inference rules
Map and pair Language A and
Language B synsets with 
cross-lingual LSRs
49
WN English synset. Our ECTED was 
bootstrapped with a combined lexical
knowledgebase integrating at least four
English-Chinese or Chinese-English 
bilingual resources. Based on this combined
LKB, a group of translators chose (or
created) up to three best translation
equivalents for each WN synset. In addition, 
for each English-Chinese translation
equivalent, a lexical semantic relation is
annotated. In addition to synonym, the 
semantic relations marked including
antonym, hypernym, hyponym, holonym,
meronym, and near-synonym. We use all 
semantic relations, with the exception of
antonymy, in this study.
2.2 Wordnet (WN)
The Cognitive Science Laboratory of
Princeton University created WN, a lexical
knowledgebase for English, in 1990
(Fellbaum, 1998). Synsets (a group of
form-meaning pairs sharing same sense) are
the main units used in WN to organize the 
lexicon conceptually. Each sense can be
expanded either by gloss or context. It is 
easy for users to distinguish each sense by 
simply checking the synonyms, the example
sentences or explanation. Nouns, verbs,
adjectives and adverbs are the main lexical
categories to classify all the lexicons. Such 
classification of lexicons is based on the 
principles in psycholinguistics. Besides, the 
semantic relations of each sense in WN are 
also expressed like a Word-network. In 
other words, WN resembles an ontology
system and links all the semantic relations
of words. Therefore, English WN is not just 
a lexical knowledgebase but also an 
ontological system that expresses the 
semantic relations and the concepts of
words.
The current version of WN is Wordnet
2.0, but Wordnet 1.6 is more widely used by
the most applications in NLP and linguistic 
research. Therefore, after considering the 
compatibility with other applications, we 
connected the ECTED with Wordnet 1.6.
However, we are still working on keeping 
updating our systems by using the content in 
the new version of WN. We believe this will 
keep the information updated and shorten
the gap caused by the different versions of 
WN.
3 Inferring Lexical Semantic 
Relations for WN and ECTED 
As we mentioned above, WN does not 
only express the knowledge of lexicons but 
also cover the semantic relations of lexicons.
Therefore, in order to present such semantic
relations clearly and logically, Huang (2002)
proposed to use cross-lingual Lexical
Semantic Relations (LSRs) to predict the
semantic relations in the target language. 
The proposed framework is shown in
Diagram 1. 
Diagram 1. Translation-mediated LSR (the complete model)
In Diagram1, EW1 and EW2 are head 
words for two different English synsets.
CW1 and CW2 are translation equivalents
in ECTED for these two head words. LSR i 
and ii are the T-LSRs stipulating the 
semantic relations between the head words 
and their Chinese TEs. In WN, each synset
is linked to a network of their synsets
through a number of LSR?s. Hence, we use 
LSR x to represent the semantic relation 
CW1 EW1(Synset number)
EW2(Synset number)CW2
y
i
x
ii
x = EW1-EW2
y = CW1-CW2
i = Translation LSR
ii = Translation LSR
The unknown LSR y = i+x+ii 
50
between EW1 and EW2. The four LSR?s
form a closed network that includes three 
know LSR?s: two T-LSRs, i and ii, and one
English LSR, x, from WN. The only
unknown LSR is y, the semantic relation
between CW1 and CW2. Huang et al(2002) 
claimed that LSR y can be inferred as a 
functional combination of the three LSRs - i, 
x and ii. 
Language translation does not only
involve the semantic correspondences but 
also the human decision in choosing
translation equivalents that are affected by
the social and cultural factors. Our main
priority in this paper is to infer the lexical 
semantic information across different
language rather than the translational 
idiosyncrasies, so the elements regarding 
translational idiosyncrasies are excluded 
here. In order to simplify the complexity of 
LSR combination and get a better prediction 
of LSR, here, we only take account of the
situations when LSR ii is exactly equivalent,
EW2=CW2 or ii=0. Therefore, we have a 
reduced model of the translation-mediated 
LSR Prediction as shown in diagram 2. 
Diagram 2. Translation-mediated LSR (the reduced model)
Synonym, hypernym, hyponym,
holonym, meronym and near-synonym are 
the main semantic relations that we will
discuss in the following sections. First of all, 
we would like to discuss the foundational
situation of LSR prediction, synonym, as 
shown in diagram 3. When translation LSR i 
is exactly equivalent, i.e. CW1=EW1, and 
LSR ii is also exactly equivalent, i.e. 
EW2=CW2, the LSR combination, LSR y,
is directly inherited the semantic relation of
LSR x. 
Diagram 3. Translation-mediated LSR (When TEs are synonymous)
CW1 EW1(Synset number)
EW2(Synset number)=CW2 (ii=0)
y
i
x
The unknown LSR y= i + x
CW1 EW1(Synset number)
EW2(Synset number)=CW2 (ii=0)
y
CW1=EW1(i=0)
x
The unknown LSR y= 0 + x = x 
51
Diagram 4. Examples of the LSR (When TEs are synonymous)
As shown in diagram 4 above,
according to the ECTED, the English head 
word ?thin? is exactly equivalent with
?shou4? in Chinese. The LSR x between 
EW1 and EW2 in WN is marked ?ANT?
which means ?fat? is the antonym of ?thin.?
Therefore, according to the prediction in 
diagram 3, we can infer that the CW2
(fei2pang4de5) is the antonym of CW1 
(shou4). The above inference can also be
applied to another example in diagram 4. 
The LSR prediction in WN plays a very
crucial role in determining the unknown 
LSR y. Even an English head word may
have more than one sense, it is still very
clear to infer the LSR between the TEs. 
However, there is a potential problem within
this inference. If a head word has more than 
one Chinese TEs which can all correspond 
to the head word, there might be a problem 
to consider whether those TEs are really
synonyms.
However, the situation is not always
that ideal as above. When the Chinese 
translation equivalents and the corresponded 
English synset have a non-identical
semantic relation, CW1?EW1, the 
prediction of LSR y needs to be considered
further and carefully.
fei2pang4d fat (00934421A)
chubby(00935062A) = feng1man3de5
y =NSYN
CW1=EW1(i=0)
x = NSYN 
shou4 thin (00936334A)
fat(00934421A) = fei2pang4de5
y = ANT
CW1=EW1(i=0)
x= ANT
52
Diagram 5. Predicting LSR (Hypernym) and its example
Diagram 6. Predicting LSR(Hyponym) and its example 
Logically, hypernym and hyponym are 
symmetric semantic relations. For instance,
if A is a hypernym of B, B is a hyponym of 
A. For instance, as shown in diagram 5, the
English word ?nick? is the hypernym of the 
Chinese term ?shang1kou3? and ?cut? is the 
hypernym of ?nick? in WN and the exact 
translation equivalent of ?cut? in Chinese is 
?jian3kai1.? According to the logicality,
?jian3kai1? is the hypernym of 
?shang1kou3.? The example of hyponym is 
shown in diagram 6. Due to the varied
semantic relations in WN, the inferences of 
LSRs , the unknown LSR y = i + x ,for
hypernym, hyponym, near-synonym,
holonym, and mernoym are listed as below: 
Hypernym(HYP)
(a) IF x=ANT 
LSR y =HYP +ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HYP 
LSR y = HYP+HYP =HYP (CW2 is the 
hypernym of CW1.) 
(c) IF x= NSYN 
LSR y = HYP+NSYN =HYP (CW2 is the
hypernym of CW1.) 
(d) IF x = HOL 
LSR y = HYP+HOL =HOL (CW2 is the 
holonym of CW1.)
(e) IF x = all other LSR 
LSR y = HYP +all other LSRs = ? 
(Undecided)
Hyponym(HPO)
(a) IF x=ANT 
LSR y =HPO +ANT =ANT (CW2 is the
antonym of CW1.) 
(b) IF x=HPO 
LSR y = HPO+HPO =HPO (CW2 is the
hyponym of CW1.)
(c) IF x= NSYN 
LSR y = HPO+NSYN =HPO (CW2 is the 
hyponym of CW1.)
(d) IF x = MER 
LSR y = HPO+MER =MER (CW2 is the 
meronym of CW1.) 
(e) IF x = all other LSR 
LSR y = HPO +all other LSRs = ? 
(Undecided)
gao1dian3 pastry(05670938N)
baklava(05674827N)=guo3ren2mi4tang2qian1ceng2bing3
y
i= HPO 
x= HPO 
The unknown LSR y
= i + x
=HPO +HPO =HPO
(?guo3ren2mi4tang2qian1ceng2bing3? is the
hyponym of ?gao1dian3?)
shang1kou3 nick(00248910N)
cut(00248688N)=jian3kai1
y
i= HYP (?nick? is the hypernym of ?shang1kou3? )
x= HYP (?cut? is the hypernym of ?nick?)
The unknown LSR y 
= i + x
=HYP +HYP =HYP
(?jian3kai1? is the hypernym of ?shang1kou3?)
53
Near-Synonym(NSYN) 
(a) IF x=ANT 
LSR y = NSYN+ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HYP 
LSR y = NSYN+HYP =HYP (CW2 is the 
hypernym of CW1.) 
(c) IF x=HPO 
LSR y = NSYN+HPO =HPO (CW2 is the 
hyponym of CW1.) 
(d) IF x= NSYN 
LSR y = NSYN+NSYN =NSYN (CW2 is 
the near-synonym of CW1.) 
(e) IF x = MER 
LSR y = NSYN+MER =MER (CW2 is the 
meronym of CW1.) 
(f) IF x = HOL 
LSR y = NSYN+HOL =HOL (CW2 is the 
holonym of CW1.) 
Holonym(HOL) 
(a) IF x=ANT 
LSR y = HOL+ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HYP 
LSR y = HOL+HYP =HYP (CW2 is the 
hypernym of CW1.) 
(c) IF x= NSYN 
LSR y = HOL+NSYN =HOL (CW2 is the 
holonym of CW1.) 
(d) IF x = HOL 
LSR y = HOL+HOL =HOL (CW2 is the 
holonym of CW1.) 
(e) IF x = all other LSR 
LSR y = HPO +all other LSRs = ? 
(Undecided)
Meronym(MER) 
(a) IF x=ANT 
LSR y = MER+ANT =ANT (CW2 is the 
antonym of CW1.) 
(b) IF x=HPO 
LSR y = MER+HPO =HPO (CW2 is the 
hyponym of CW1.) 
(c) IF x= NSYN 
LSR y = MER+NSYN =MER (CW2 is the 
meronym of CW1.) 
(d) IF x = MER 
LSR y = MER+MER =MER (CW2 is the 
meronym of CW1.) 
(e) IF x = all other LSR 
LSR y = HPO +all other LSRs = ? 
(Undecided)
4 Implementation and Evaluation 
WN 1.6 contains 99,642 English 
synsets and expands to 157,507 English 
lemma tokens. On the other hand, the total 
number of Chinese lemma types found in 
our ECTED is 108,533. Hence, each 
Chinese lemma type translates roughly 1.1 
English synsets in average.  
In comparing the two approaches to 
parallel wordnet building, we treat at 
baseline the cases where the translation LSR 
is synonymy. In others words, these are the 
cases where both approach I and approach II 
will make highly accurate predictions (e.g. 
Huang, et al 2003). However, if the T-LSR 
is other than synonymy, we expect the 
prediction based on source language LSR 
will be much lower.  
In our study, there are in total 372,927 
lexical semantic relations that can 
potentially be bootstrapped when the T-LSR 
is one of the five semantic relations in study. 
These are expanded from the following 
types of translations equivalence relations: 
11,396 translation near-synonyms, 2,782 
translation hypernyms, 2,106 translation 
hyponyms, 252 translation meronyms and 
145 translations holonyms. For evaluation, 
due to constraints on resources, we 
exhaustively check the types with less than 
300 lemmas, while randomly checked close 
to 300 lemmas for the other types. 
 We first introduce the baseline model 
where synonym is assumed. This is where 
source language LSR?s will be mapped 
directly to target languages. We have shown 
that if the T-LSR is really synonymy, the 
precision will be 62.7%. However, when the 
T-LSR?s are different, the baseline precision 
is much lower. In Table 1, such na?ve 
prediction is manually classes into three 
types: Correct, Incorrect, and Others. 
?Correct? means that the prediction is 
verified. ?Incorrect? means the assigned 
LSR is wrong. Two scenarios are possible. 
One is that there is a possible prediction and 
another one is the correct LSR is different 
from the predicted one. ?Others? refers to 
exceptional cases where these is no lexical 
translation, or the source language LSR is 
wrongly assigned and so on. Table 1 shows 
that the baseline for non-synonymous 
T-LSR is only 47% in average, and range 
from 30% to 65% for each semantic relation.
54
Correct Incorrect Others Total 
NSYN 400 51% 379 49% 0 0% 779 100% 
HYP 178 65% 72 27% 22 8% 272 100% 
HPO 402 40% 285 28% 330 32% 1017 100% 
HOL 48 30%  108 69% 2 1% 158 100% 
MER 52 56% 32 34% 9 10% 93 100% 
Total 1079 47% 877 37% 363 16% 2319 100% 
Table 1 Baseline Results (assuming synonym)
Table 2 shows the comparison between 
the T-LSR model and the baseline. It shows 
that there is improvement of 17.8% in 
average and that there is gain in precision 
for each LSR type. The improvement varies 
from just below 2% to 39%.  
Baseline T-LSR Difference Improvement 
NSYN 400 51% 556 71% 156 20 % 156/400 39 % 
HYP 178 65% 184 66%  6 2.2% 6/178  3.4% 
HPO 402 40% 409 40%  7 0.7% 7/402  1.7% 
HOL 48 30% 64 41% 16 10.1% 16/48 33.3% 
MER 52 56% 58 62% 6 6.5% 6/52 11.5% 
Total 1079 47% 1271 55% 191 8.2% 191/1080 17.7% 
Table 2 Precision of using the LSR inferences
5 Conclusion
It is interesting to note that the classes 
with least improvements are hypernymy and 
hyponymy. Since these are the classical 
IS-A relations, we hypothesize that their 
predictions are similar to the baseline 
relation of synonym. If we take these two 
relations out, the T-LSR model with 
inference rules has a precision difference of 
17.3% (178/1030), as well as an 
improvement of 35.6% (178/500). These are 
substantial improvements over the baseline 
model. The result will be reinforced when 
the evaluation is completed. We will also 
analyze the prediction based on each T-LSR 
to give a more explanatory account as well a 
measure confidence or prediction. The result 
offers strong support for T-LSR as a model 
for bootstrapping parallel wordnets with a 
low-density target language. 
References 
Fellbaum, C. (ed.) 1998. Wordent: An Electronic 
Lexical Database. Cambridge, MA: MIT Press. 
Huang, Chu-Ren, D.B. Tsai, J.Lin, S. Tseng, K.J. 
Chen and Y. Chuang. 2001 Definition and Test 
for Lexical Semantic Relation in Chinese. [in 
Chinese] Paper presented at the Second Chinese 
Lexical Semantics Workshop. May 2001,  
Beijing, China. 
Huang, Chu-Ren, I-Ju E. Tseng, Dylan B.S. Tsai. 
2002. Translating Lexical Semantic Relations: 
The first step towards Multilingual Wordnets.
Proceedings of the COLONG2002 Workshop 
?SemaNet:Building and Using Semantic 
Networks?, ed. By Grace Ngai, Pascale Fung, 
and Kenneth W. Church, 2-8. 
Huang, Chu-Ren, Elanna I. J. Tseng, Dylan B.S. 
Tsai, Brian Murphy. 2003 Cross-lingual 
Portability of Semantic Relations: Bootstrapping 
Chinese WordNet with English WordNet 
Relations. pp.509-531. 
Pianta, Emanuel, L. Benitivogli, C. Girardi. 
2002 MultiWordnet: Developing an aligned 
nultilingual database. Proceedings of the 1st
International WordNet Conference, Maysore, 
Inda, pp.293-302. 
Tsai, D.B.S., Chu-Ren Huang, J.Lin, K.J. Chen 
and Y. Chuang. 2002. Definition and Test for 
Lexical Semantic Relation in Chinese. [???
??????????!
?] Journal of Chinese Information Processing 
[??????]. 16.4.21-31. 
Vossen P. (ed.). 1998. EuroWordNet: A 
multilingual database with lexical semantic 
networks. Norwell, MA: Kluwer Academic 
Publisher 
55
