Felix Bildhauer & Roland Sch?fer (eds.), Proceedings of the 9th Web as Corpus Workshop (WaC-9) @ EACL 2014, pages 29?35,
Gothenburg, Sweden, April 26 2014.
c?2014 Association for Computational Linguistics
{bs,hr,sr}WaC ? Web corpora of Bosnian, Croatian and Serbian
Nikola Ljube
?
si
?
c
University of Zagreb
Ivana Lu?ci?ca 3, 10000 Zagreb, Croatia
nljubesi@ffzg.hr
Filip Klubi
?
cka
University of Zagreb
Ivana Lu?ci?ca 3, 10000 Zagreb, Croatia
fklubick@ffzg.hr
Abstract
In this paper we present the construction
process of top-level-domain web corpora
of Bosnian, Croatian and Serbian. For
constructing the corpora we use the Spi-
derLing crawler with its associated tools
adapted for simultaneous crawling and
processing of text written in two scripts,
Latin and Cyrillic. In addition to the mod-
ified collection process we focus on two
sources of noise in the resulting corpora:
1. they contain documents written in the
other, closely related languages that can
not be identified with standard language
identification methods and 2. as most web
corpora, they partially contain low-quality
data not suitable for the specific research
and application objectives. We approach
both problems by using language mod-
eling on the crawled data only, omitting
the need for manually validated language
samples for training. On the task of dis-
criminating between closely related lan-
guages we outperform the state-of-the-art
Blacklist classifier reducing its error to a
fourth.
1 Introduction
Building web corpora for various NLP tasks has
become quite a standard approach, especially if
funding is limited and / or there is need for large
amounts of textual data.
Although off-the-shelf solutions for compiling
web corpora have emerged recently, there are still
specific challenges that have to be addressed in
most corpus construction processes. One such
challenge that we face while constructing the cor-
pora described in this paper is simultaneous us-
age of two scripts on two out of three top-level
domains (TLDs) crawled.
Additionally, there are still many open ques-
tions and possibilities for improvement in the
process of collecting data as well as data post-
processing. We address two of the latter kind ?
discrimination between similar, neighboring lan-
guages that are used on all selected TLDs, and
the question of text quality in corpora collected in
such a fully automated fashion.
In the paper we present the process of building
web corpora of Bosnian, Croatian and Serbian by
crawling the .ba, .hr and .rs TLDs. The three
languages belong to the South Slavic language
branch and are very similar to each other. The
biggest differences between Croatian and Serbian
are the proto-Slavic vowel jat (Croatian ?covjek
vs. Serbian ?covek), way of handling proper nouns
(Croatian New York vs. Serbian Nju Jork), specific
syntactic constructions (Croatian ho?cu raditi vs.
Serbian ho?cu da radim) and a series of lexical dif-
ferences (Croatian mrkva vs. Serbian ?sargarepa).
Bosnian is mostly seen as a mixture of those two
and allows, beside its own lexical specificities, so-
lutions from one or both languages.
1
This paper is structured as follows: in Section
2 we give an overview of related work regarding
existing (web) corpora of the languages in ques-
tion, language identification and web text quality
estimation. Section 3 shows the process of col-
lecting the three TLD corpora with emphasis on
the problem of collecting data written in various
scripts, while in Section 4 we describe the linguis-
tic annotation layers added to the corpora. Section
5 depicts our approach to discriminating between
very similar languages while in Section 6 we de-
scribe our approach to identifying documents of
low text quality, and both approaches use recently
crawled data only.
1
A more thorough comparison of the three lan-
guages is available at http://en.wikipedia.org/
wiki/Comparison_of_standard_Bosnian,
_Croatian_and_Serbian
29
2 Related work
The only two South Slavic languages for which
web corpora were previously built are Croatian
and Slovene (Ljube?si?c and Erjavec, 2011). The
Croatian corpus presented in this paper is actually
an extension of the existing corpus, representing
its second version. hrWaC v1.0 was, until now,
the biggest available corpus of Croatian.
For Bosnian, almost no corpora are available
except the SETimes corpus
2
, which is a 10-
languages parallel corpus with its Bosnian side
consisting of 2.2 million words, and The Oslo
Corpus of Bosnian Texts
3
, which is a 1.5 mil-
lion words corpus consisting of different genres of
texts that were published in the 1990s.
For the Serbian language, until now, the largest
corpus was the SrpKor corpus
4
, consisting of 118
million words that are annotated with part-of-
speech information (16 tags) and lemmatized. The
corpus is available for search through an interface
for non-commercial purposes.
Until now, no large freely downloadable cor-
pora of Bosnian and Serbian were available, and
this was one of the strongest motivations for our
work.
Multiple pipelines for building web corpora
were described in many papers in the last decade
(Baroni et al., 2009; Ljube?si?c and Erjavec, 2011;
Sch?afer and Bildhauer, 2012), but, to the best of
our knowledge, only one pipeline is freely avail-
able as a complete, ready-to-use tool: the Brno
pipeline (Suchomel and Pomik?alek, 2012), con-
sisting of the SpiderLing crawler
5
, the Chared en-
coding detector
6
, the jusText content extractor
7
and the Onion near-deduplicator
8
. Although we
have our own pipeline set up (this is the pipeline
the first versions of hrWaC and slWaC were built
with), we decided to compile these versions of
web corpora with the Brno pipeline for two rea-
sons: 1. to inspect the pipeline?s capabilities, and
2. to extend the Croatian web corpus as much as
possible by using a different crawler.
Although language identification is seen as a
2
http://nlp.ffzg.hr/resources/corpora/
setimes/
3
http://www.tekstlab.uio.no/Bosnian/
Corpus.html
4
http://tinyurl.com/mocnzna
5
http://nlp.fi.muni.cz/trac/spiderling
6
https://code.google.com/p/chared/
7
http://code.google.com/p/justext/
8
http://code.google.com/p/onion/
solved problem by many, the recently growing in-
terest for it indicates the opposite. Recently, re-
searchers focused on improving off-the-shelf tools
for identifying many languages (Lui and Bald-
win, 2012), discriminating between similar lan-
guages where standard tools fail (Tiedemann and
Ljube?si?c, 2012), identifying documents written in
multiple languages and identifying the languages
in such multilingual documents (Lui et al., 2014).
Text quality in automatically constructed web
corpora is quite an underresearched topic, with the
exception of boilerplate removal / content extrac-
tion approaches that deal with this problem implic-
itly (Baroni et al., 2008; Kohlsch?utter et al., 2010),
but quite drastically, by removing all content that
does not conform to the criteria set. A recent ap-
proach to assessing text quality in web corpora in
an unsupervised manner (Sch?afer et al., 2013) cal-
culates the weighted mean and standard deviation
of n most frequent words in a corpus sample and
measures how much a specific document deviates
from the estimated means. This approach is in its
basic idea quite similar to ours because it assumes
that most of the documents in the corpus contain
content of good quality. The main difference in
our approach is that we do not constrain ourselves
to most frequent words as features, but use char-
acter and word n-grams of all available text.
3 Corpus construction
For constructing the corpora we used the Spi-
derLing crawler
9
along with its associated tools
for encoding guessing, content extraction, lan-
guage identification and near-duplicate removal
(Suchomel and Pomik?alek, 2012). Seed URLs
for Bosnian and Serbian were obtained via the
Google Search API queried with bigrams of mid-
frequency terms. Those terms were obtained from
corpora that were built with focused crawls of
newspaper sites as part of our previous research
(Tiedemann and Ljube?si?c, 2012). For Croatian
seed URLs, we used the home pages of web do-
mains obtained during the construction of the first
version of the hrWaC corpus. The number of seed
URLs was 8,388 for bsWaC, 11,427 for srWaC
and 14,396 for hrWaC. Each TLD was crawled for
21 days with 16 cores used for document process-
ing.
Because Serbian ? which is frequently used on
the Serbian and Bosnian TLDs ? uses two scripts
9
http://nlp.fi.muni.cz/trac/spiderling
30
? Latin and Cyrillic ? we had to adjust the stan-
dard corpus construction process to cope with both
scripts. This was done by 1. building new two-
script models for encoding guessing with Chared,
2. defining stop-words used in content extraction
in both scripts and 3. transforming extracted text
from Cyrillic to Latin with serbian.py
10
before
performing language identification and duplicate
removal. We kept all content of the final corpora in
the Latin script to simplify further processing, es-
pecially because linguistic annotation layers were
added with models developed for Croatian which
uses the Latin script exclusively. The information
about the amount of Cyrillic text in each document
is still preserved as an attribute of the <doc> el-
ement. Overall the percentage of documents writ-
ten >90% in the Cyrillic script was 3.2% on the
Bosnian TLD and 16.7% on the Serbian TLD.
Near-duplicate identification was performed
both on the document and the paragraph level.
The document-level near-duplicates were removed
from the corpus cutting its size in half, while
paragraph-level near-duplicates were labeled by
the neardupe binary attribute in the <p> el-
ement enabling the corpus users to decide what
level of near-duplicate removal suits their needs.
The resulting size of the three corpora (in mil-
lions of tokens) after each of the three duplicate re-
moval stages is given in Table 1. Separate numbers
are shown for the new crawl of the Croatian TLD
and the final corpus consisting of both crawls.
PHYS DOCN PARN
bsWaC 1.0 722 429 288
hrWaC new 1,779 1,134 700
hrWaC 2.0 2,686 1,910 1,340
srWaC 1.0 1,554 894 557
Table 1: Size of the corpora in Mtokens after phys-
ical duplicate (PHY), document near-duplicate
(DOCN) and paragraph near-duplicate removal
(PARN)
At this point of the corpus construction process
the <doc> element contained the following at-
tributes:
? domain ? the domain the document is pub-
lished on (e.g. zkvh.org.rs)
? url ? the URL of the document
10
http://klaus.e175.net/code/serbian.py
? crawl_date ? date the document was
crawled
? cyrillic_num ? number of Cyrillic let-
ters in the document
? cyrillic_perc ? percentage of letters
that are Cyrillic
4 Corpus annotation
We annotated all three corpora on the level of
lemmas, morphosyntactic description (675 tags)
and dependency syntax (15 tags). Lemmatiza-
tion was performed with the CST?s Lemmatiser
11
(Jongejan and Dalianis, 2009), morphosyntactic
tagging with HunPos
12
(Hal?acsy et al., 2007) and
dependency syntax with mate-tools
13
(Bohnet,
2010). All models were trained on the Croa-
tian 90k-token annotated corpus SETimes.HR
14
(Agi?c and Ljube?si?c, 2014) that we recently ex-
panded with 50k additional tokens from vari-
ous newspaper domains (at this point we call
it simply SETimes.HR+). Although the anno-
tated training corpora are Croatian, previous re-
search (Agi?c et al., 2013a; Agi?c et al., 2013b) has
shown that on this level of tagging accuracy on
in-domain test sets (lemma?96%, morphosyntac-
tic description (MSD) ?87%, labeled attachment
score (LAS) ?73%), annotating Serbian text with
models trained on Croatian data produced perfor-
mance loss of only up to 3% on all three levels
of annotation, while on out-of-domain test sets
(lemma ?92%, MSD ?81%, LAS ?65%) there
was no loss in accuracy.
We nevertheless performed an intervention in
the SETimes.HR+ corpus before training the mod-
els used for annotating the Bosnian and the Ser-
bian TLD corpora. Namely, on the morphosyn-
tactic level the tagsets of Croatian and Serbian
are identical, except for one subset of tags for
the future tense which is present in Serbian and
not present in Croatian. This is because Croatian
uses the complex, analytic future tense consisting
of the infinitive of the main verb and the present
tense of the auxiliary verb have (radit ?cemo) while
Serbian uses both the analytic and the synthetic
form where the two words are conflated into one
(radi?cemo).
11
https://github.com/kuhumcst/cstlemma
12
https://code.google.com/p/hunpos/
13
https://code.google.com/p/mate-tools/
14
http://nlp.ffzg.hr/resources/corpora/
setimes-hr/
31
To enable models to correctly handle both the
analytic and synthetic form of the future tense,
we simply repeated the sentences containing the
analytic form that we automatically transformed
to the synthetic one. By annotating the bsWaC
and srWaC corpora with the models trained on
the modified SETimes.HR+ corpus, we annotated
610k word forms in srWaC and 115k word forms
in bsWaC with the synthetic future tense. Manual
inspection showed that most of the tokens actually
do represent the future tense, proving that the in-
tervention was well worth it.
The lemmatization and morphosyntactic anno-
tation of all three corpora took just a few hours
while the full dependency parsing procedure on 40
server grade cores took 25 days.
5 Language identification
Because each of the three languages of interest is
used to some extent on each of the three TLDs and,
additionally, these languages are very similar, dis-
criminating between them presented both a neces-
sity and a challenge.
In previous work on discriminating between
closely related languages, the Blacklist (BL) clas-
sifier (Tiedemann and Ljube?si?c, 2012) has shown
to be, on a newspaper-based test set, 100% accu-
rate in discriminating between Croatian and Ser-
bian, and 97% accurate on all three languages of
interest.
Our aim at this stage was twofold: 1. to put the
existing BL classifier on a realistic test on (noisy)
web data and 2. to propose an alternative, simple,
data-intense, but noise-resistant method which can
be used for discriminating between closely related
languages or language varieties that are predomi-
nantly used on specific sections of the Web.
Our method (LM1) uses the whole content of
each of the three TLD web corpora (so large
amounts of automatically collected, noisy data) to
build unigram-level language models. Its advan-
tage over the BL classifier is that it does not re-
quire any clean, manually prepared samples for
training. The probability estimate for each word w
given the TLD, using add-one smoothing is this:
?
P (w|TLD) =
c(w, TLD) + 1
?
w
i
?V
(c(w
i
, TLD) + 1)
(1)
where c(w, TLD) is the number of times word w
occurred on the specific TLD and V is the vocab-
ulary defined over all TLDs.
We perform classification on each document as
a maximum-a-posteriori (MAP) decision, i.e. we
choose the language of the corresponding TLD
(l ? TLD) that produces maximum probability
with respect to words occurring in the document
(w
1
...w
n
):
l
map
= argmax
l?TLD
?
i=1..n
?
P (w
i
|l) (2)
We should note here that our approach is identi-
cal to using the Na??ve Bayes classifier without the
a priori probability for each class, i.e. language.
Speaking in loose terms, what we do is that for
each document of each TLD, we identify, on the
word level, to which TLD data collection the doc-
ument corresponds best.
Because Bosnian is mostly a mixture of Croat-
ian and Serbian and actually represents a contin-
uum between those two languages, we decided
to compare the BL and the LM1 classifier on a
much more straight-forward task of discriminat-
ing between Croatian and Serbian. The results of
classifying each document with both classifiers are
given in Table 2. They show that both classifiers
agree on around 75% of decisions and that around
0.4 percent of documents from hrWaC are identi-
fied as Serbian and 1.5 percent of document from
srWaC as Croatian.
BL LM1 agreement
hrWaC 0.42% 0.3% 73.15%
srWaC 1.93 % 1.28% 80.53%
Table 2: Percentage of documents identified by
each classifier as belonging to the other language
We compared the classifiers by manually in-
specting 100 random documents per corpus where
the two classifiers were not in agreement. The re-
sults of this tool-oriented evaluation are presented
in Table 3 showing that the LM1 classifier pro-
duced the correct answer in overall 4 times more
cases than the BL classifier.
If we assume that the decisions where the two
classifiers agree are correct (and manual inspec-
tion of data samples points in that direction) we
can conclude that our simple, data-intense, noise-
resistant LM1 method cuts the BL classification
error to a fourth. We consider a more thorough
evaluation of the two classifiers, probably by pool-
ing and annotating documents that were identified
32
BL LM1 NA
hrWaC 18% 62% 20%
srWaC 10% 48% 42%
Table 3: Percentage of correct decisions of each
classifier on documents where the classifiers dis-
agreed (NA represents documents that are a mix-
ture of both languages)
as belonging to the other TLD language by some
classifier, as future work.
Due to the significant reduction in error by the
LM1 classifier, we annotated each document in the
hrWaC and srWaC corpora with the LM1 binary
hr-sr language identifier while on bsWaC we used
the LM1 ternary bs-hr-sr classifier. This decision
is based on the fact that discriminating between all
three languages is very hard even for humans and
that for most users the hr-sr discrimination on the
two corpora will be informative enough. In each
document we encoded the normalized distribution
of log-probabilities for the considered languages,
enabling the corpus user to redefine his own lan-
guage criterion.
The percentage of documents from each corpus
being identified as a specific language is given in
Table 4.
bs hr sr
bsWaC 78.0% 16.5% 5.5%
hrWaC - 99.7% 0.3%
srWaC - 1.3% 98.7%
Table 4: Distribution of identified languages
throughout the three corpora
Additional attributes added to the <doc> ele-
ment during language identification are these:
? lang ? language code of the language iden-
tified by maximum-a-posteriori
? langdistr ? normalized distri-
bution of log probabilities of lan-
guages taken under consideration (e.g.
bs:-0.324|hr:-0.329|sr:-0.347
for a document from bsWaC)
6 Identifying text of low quality
Finally, we tackled the problem of identifying doc-
uments of low text quality in an unsupervised
manner by assuming that most of the content of
each web corpus is of good quality and that low
quality content can be identified as data points
of lowest probability regarding language models
built on the whole data collection. We pragmati-
cally define low quality content as content not de-
sirable for a significant number of research or ap-
plication objectives.
For each TLD we calculated character n-gram
and word n-gram language models in the same
manner as in the previous section (Equation 1) for
language identification. We scored each TLD doc-
ument with each language model that was built on
that TLD. To get a probability estimate which does
not depend on the document length, we calculated
probabilities of subsequences of identical length
and computed the average of those.
We manually inspected documents with low
probability regarding character n-gram models
from level 1 to level 15 and word n-gram mod-
els from level 1 to level 5. Word n-gram mod-
els proved to be much less appropriate for cap-
turing low quality documents by lowest probabil-
ity scores than character n-gram models. Among
character n-gram models, 3-gram models were
able to identify documents with noise on the token
level while 12-gram models assigned low proba-
bilities to documents with noise above the token
level.
The most frequent types of potential noise
found in lowest scored documents in all three cor-
pora are the following:
? 3-gram models
? non-standard usage of uppercase, lower-
case and punctuation
? URL-s
? uppercase want ads
? formulas
? 12-gram models
? words split into multiple words (due to
soft hyphen usage or HTML tags inside
words)
? enumerated and bulleted lists
? uppercase want ads
? non-standard text (slang, no uppercased
words, emoticons)
? dialects
? lyric, epic, historical texts
33
The character 3-gram method has additionally
proven to be a very good estimate of text quality on
the lexical level by strongly correlating (0.74) with
the knowledge-heavy method of calculating lexi-
cal overlap of each document with a morphologi-
cal dictionary which is available for Croatian
15
.
An interesting finding is that word-level models
perform much worse for this task than character-
level models. We hypothesize that this is due to
feature space sparsity on the word level which is
much lower on the character level.
We decided to postpone any final decisions (like
discretizing these two variables and defining one
or two categorical ones) and therefore encoded
both log-probabilities as attributes in each doc-
ument element in the corpus leaving to the fi-
nal users to define their own cut-off criteria. To
make that decision easier, for each document and
each character n-gram method we computed the
percentage of documents in the corpus that have
an equal or lower result of that character n-gram
method. This makes removing a specific percent-
age of documents with lowest scores regarding a
method much easier.
We also computed one very simple estimate of
text quality ? the percentage of characters that are
diacritics. Namely, for some tasks, like lexicon en-
richment, working on non-diacritized text is not an
option. Additionally, it is to expect that lower us-
age of diacritics points to less standard language
usage. The distribution of this text quality esti-
mate in the hrWaC corpus (all three corpora fol-
low the same pattern) is depicted in Figure 1 show-
ing that the estimate is rather normally distributed
with a small peak at value zero representing non-
diacritized documents.
In each <doc> element we finally encoded 5
attributes regarding text quality:
? 3graph ? average log-probability on 100-
character sequences regarding the character
3-gram model trained on the whole TLD cor-
pus
? 3graph_cumul ? percentage of documents
with equal or lower 3graph attribute value
? 12graph ? same as 3graph, but computed
with the character 12-gram model
? 12graph_cumul ? like 3graph_cumul,
but for the 12graph attribute
15
http://bit.ly/1mRjMrP
Percentage of diacritics
Fre
que
ncy
0.00 0.02 0.04 0.06 0.08 0.10
0
500
00
100
000
150
000
Figure 1: Distribution of the percentage of charac-
ters of a document being diacritics
? diacr_perc ? percentage of non-
whitespace characters that are diacritics
We plan to perform extrinsic evaluation of the
three estimates of text quality on various NLP
tasks such as language modeling for statistical
machine translation, morphological lexicon induc-
tion, distributional lexicon induction of closely re-
lated languages and multi-word expression extrac-
tion.
7 Conclusion
In this paper we described the process of con-
structing three TLD corpora of Bosnian, Croatian
and Serbian.
After presenting the construction and annota-
tion process of the largest existing corpora for
each of the three languages, we focused on the
issue that all three languages are to some extent
used on all three TLDs. We presented a method
for discriminating between similar languages that
is based on unigram language modeling on the
crawled data only, which exploits the fact that the
majority of the data published on each TLD is
written in the language corresponding to that TLD.
We reduced the error of a state-of-the-art classifier
to a fourth on documents where the two classifiers
disagree on.
We dealt with the problem of identifying low
quality content as well, again using language mod-
eling on crawled data only, showing that document
probability regarding a character 3-gram model is
a very good estimate of lexical quality, while low
34
character 12-gram probabilities identify low qual-
ity documents beyond the word boundary.
We encoded a total of 12 attributes in the docu-
ment element and the paragraph-near-duplicate in-
formation in the paragraph element enabling each
user to search for and define his own criteria.
We plan on experimenting with those attributes
on various tasks, from language modeling for sta-
tistical machine translation, to extracting various
linguistic knowledge from those corpora.
Acknowledgement
The research leading to these results has re-
ceived funding from the European Union Sev-
enth Framework Programme FP7/2007-2013 un-
der grant agreement no. PIAP-GA-2012-324414
(project Abu-MaTran).
References
[Agi?c and Ljube?si?c2014]
?
Zeljko Agi?c and Nikola
Ljube?si?c. 2014. The SETimes.HR linguistically
annotated corpus of Croatian. In Proceedings of
LREC 2014.
[Agi?c et al.2013a]
?
Zeljko Agi?c, Nikola Ljube?si?c, and
Danijela Merkler. 2013a. Lemmatization and mor-
phosyntactic tagging of Croatian and Serbian. In
Proceedings of the 4th Biennial International Work-
shop on Balto-Slavic Natural Language Processing,
pages 48?57, Sofia, Bulgaria, August. Association
for Computational Linguistics.
[Agi?c et al.2013b]
?
Zeljko Agi?c, Danijela Merkler, and
Da?sa Berovi?c. 2013b. Parsing Croatian and Serbian
by using Croatian dependency treebanks. In Pro-
ceedings of the Fourth Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL
2013).
[Baroni et al.2008] Marco Baroni, Francis Chantree,
Adam Kilgarriff, and Serge Sharoff. 2008.
Cleaneval: a competition for cleaning web pages.
In Proceedings of the Sixth International Language
Resources and Evaluation (LREC?08), Marrakech,
Morocco. European Language Resources Associa-
tion (ELRA).
[Baroni et al.2009] Marco Baroni, Silvia Bernardini,
Adriano Ferraresi, and Eros Zanchetta. 2009. The
WaCky wide web: a collection of very large linguis-
tically processed web-crawled corpora. Language
Resources and Evaluation, pages 209?226.
[Bohnet2010] Bernd Bohnet. 2010. Very high accuracy
and fast dependency parsing is not a contradiction.
In The 23rd International Conference on Computa-
tional Linguistics (COLING 2010).
[Hal?acsy et al.2007] P?eter Hal?acsy, Andr?as Kornai, and
Csaba Oravecz. 2007. HunPos: an open source
trigram tagger. In Proceedings of the 45th An-
nual Meeting of the ACL on Interactive Poster and
Demonstration Sessions, ACL ?07, pages 209?212,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
[Jongejan and Dalianis2009] Bart Jongejan and Her-
cules Dalianis. 2009. Automatic training of lemma-
tization rules that handle morphological changes in
pre-, in- and suffixes alike. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP, pages
145?153.
[Kohlsch?utter et al.2010] Christian Kohlsch?utter, Peter
Fankhauser, and Wolfgang Nejdl. 2010. Boilerplate
detection using shallow text features. In Brian D.
Davison, Torsten Suel, Nick Craswell, and Bing Liu,
editors, WSDM, pages 441?450. ACM.
[Ljube?si?c and Erjavec2011] Nikola Ljube?si?c and
Toma?z Erjavec. 2011. hrWaC and slWac: Com-
piling Web Corpora for Croatian and Slovene. In
Text, Speech and Dialogue - 14th International
Conference, TSD 2011, Pilsen, Czech Republic,
Lecture Notes in Computer Science, pages 395?402.
Springer.
[Lui and Baldwin2012] Marco Lui and Timothy Bald-
win. 2012. langid.py: An off-the-shelf language
identification tool. In ACL (System Demonstra-
tions), pages 25?30.
[Lui et al.2014] Marco Lui, Jey Han Lau, and Timothy
Baldwin. 2014. Automatic detection and language
identification of multilingual documents. Transac-
tions of the Association for Computational Linguis-
tics.
[Sch?afer and Bildhauer2012] Roland Sch?afer and Felix
Bildhauer. 2012. Building large corpora from the
web using a new efficient tool chain. In Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC?12), Istan-
bul, Turkey. European Language Resources Associ-
ation (ELRA).
[Sch?afer et al.2013] Roland Sch?afer, Adrien Barbaresi,
and Felix Bildhauer. 2013. The good, the bad, and
the hazy: Design decisions in web corpus construc-
tion. In Proceedings of the 8th Web as Corpus Work-
shop (WAC8).
[Suchomel and Pomik?alek2012] V??t Suchomel and Jan
Pomik?alek. 2012. Efficient web crawling for large
text corpora. In Serge Sharoff Adam Kilgarriff, edi-
tor, Proceedings of the seventh Web as Corpus Work-
shop (WAC7), pages 39?43, Lyon.
[Tiedemann and Ljube?si?c2012] J?org Tiedemann and
Nikola Ljube?si?c. 2012. Efficient discrimination be-
tween closely related languages. In Proceedings of
COLING 2012, pages 2619?2634, Mumbai, India.
35
Language Technology for Closely Related Languages and Language Variants (LT4CloseLang), pages 76?84,
October 29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Exploring cross-language statistical machine translation
for closely related South Slavic languages
Maja Popovi
?
c
DFKI Berlin, Germany
maja.popovic@dfki.de
Nikola Ljube?i
?
c
University of Zagreb, Croatia
nikola.ljubesic@ffzg.hr
Abstract
This work investigates the use of cross-
language resources for statistical machine
translation (SMT) between English and
two closely related South Slavic lan-
guages, namely Croatian and Serbian. The
goal is to explore the effects of translating
from and into one language using an SMT
system trained on another. For translation
into English, a loss due to cross-translation
is about 13% of BLEU and for the other
translation direction about 15%. The per-
formance decrease for both languages in
both translation directions is mainly due
to lexical divergences. Several language
adaptation methods are explored, and it is
shown that very simple lexical transforma-
tions already can yield a small improve-
ment, and that the most promising adap-
tation method is using a Croatian-Serbian
SMT system trained on a very small cor-
pus.
1 Introduction
Statistical machine translation has become widely
used over the last decade ? open source tools such
as Moses (Koehn et al., 2007) make it possible
to build translation systems for any language pair
within days, or even hours. However, the prereq-
uisite is that appropriate bilingual training data is
available, which is actually one of the most se-
vere limitations of the statistical approach ? large
resources are only available for a few language
pairs and domains. Therefore exploiting language
closeness can be very convenient if there are no
appropriate corpora containing the desired lan-
guage, but it is possible to acquire corpora con-
taining a closely related one. Croatian and Ser-
bian are very close languages, and both
1
are under-
1
as well as other South Slavic languages
resourced in terms of free/open-source language
resources and tools, especially in terms of paral-
lel bilingual corpora. On the other hand, Croatian
has recently become the third official South Slavic
language in the EU
2
, and Serbian
3
is the official
language of a candidate member state. Therefore
investigating cross-language translation for these
two languages can be considered very useful.
Both languages belong to the South-Western
Slavic branch. As Slavic languages, they have
a free word order and are highly inflected. Al-
though they exhibit a large overlap in vocabulary
and a strong morphosyntactic similarity so that the
speakers can understand each other without diffi-
culties, there is a number of small, but notable and
frequently occurring differences between them.
In this paper, we investigate the impact of these
differences on cross-language translation. The
main questions are:
? How much will the translation performance
decrease if a Serbian-English SMT system is
used for translation from and into Croatian?
(and the other way round)
? What are the possibilities for diminishing this
performance decrease?
1.1 Related work
First publications dealing with statistical machine
translation systems for Serbian-English (Popovi
?
c
et al., 2005) and for Croatian-English (Ljube?i
?
c
et al., 2010) are reporting results of first steps
on small bilingual corpora. Recent work on
Croatian-English pair describes building a paral-
lel corpus in the tourism domain by automatic
web harvesting (Espl?-Gomis et al., 2014) and re-
sults of a SMT system built on this parallel cor-
pus which yielded significant improvement (10%
2
together with Slovenian and Bulgarian
3
together with Bosnian and Montenegrin
76
BLEU) over the Google baseline in the tourism do-
main (Toral et al., 2014). A rule-based Apertium
system (Peradin et al., 2014) has been recently de-
veloped for translation from and into Slovenian
(also closely related language, but more distant).
Techniques simpler than general SMT such
as character-level translation have been investi-
gated for translation between various close lan-
guage pairs, where for the South Slavic group
the Bulgarian-Macedonian pair has been ex-
plored (Nakov and Tiedemann, 2012). Character-
based translation has also been used for translat-
ing between Bosnian and Macedonian in order to
build pivot translation systems from and into En-
glish (Tiedemann, 2012).
Developing POS taggers and lemmatizers for
Croatian and Serbian and using Croatian models
on Serbian data has been explored in (Agi
?
c et al.,
2013).
To the best of our knowledge, a systematic in-
vestigation of cross-language translation systems
involving Croatian and Serbian, thereby exploit-
ing benefits from the language closeness and ana-
lyzing problems induced by language differences
has not been carried out yet.
2 Language characteristics
2.1 General characteristics
Croatian and Serbian, as Slavic languages, have
a very rich inflectional morphology for all word
classes. There are six distinct cases affecting
not only common nouns but also proper nouns
as well as pronouns, adjectives and some num-
bers. Some nouns and adjectives have two dis-
tinct plural forms depending on the number (less
than five or not). There are also three genders for
the nouns, pronouns, adjectives and some numbers
leading to differences between the cases and also
between the verb participles for past tense and pas-
sive voice.
As for verbs, person and many tenses are ex-
pressed by the suffix, and the subject pronoun (e.g.
I, we, it) is often omitted (similarly as in Spanish
and Italian). In addition, negation of three quite
important verbs, ?biti? (to be, auxiliary verb for
past tense, conditional and passive voice), ?imati?
(to have) and ?ht(j)eti? (to want, auxiliary verb for
the future tense), is formed by adding the negative
particle to the verb as a prefix.
As for syntax, both languages have a quite free
word order, and there are no articles.
2.2 Differences
The main differences between the languages are
illustrated by examples in Table 1.
The largest differences between the two lan-
guages are in the vocabulary. Months have Slavic-
derived names in Croatian whereas Serbian uses
standard set of international Latin-derived names.
A number of other words are also completely dif-
ferent (1), and a lot of words differ only by one
or two letters (2). In addition, Croatian language
does not transcribe foreign names and words,
whereas phonetical transcriptions are usual in Ser-
bian although original writing is allowed too (3).
Apart from lexical differences, there are also
structural differences mainly concerning verbs.
After modal verbs such as ?morati? (to have to)
or ?mo
?
ci? (can) (4), the infinitive is prescribed in
Croatian (?moram raditi?), whereas the construc-
tion with particle ?da? (that/to) and present tense
(?moram da radim?) is preferred in Serbian. An
inspection of the Croatian and Serbian web cor-
pora
4
(Ljube?i
?
c and Klubi
?
cka., 2014) shows the
prescription being followed by identifying 1286
vs. 29 occurrences of the two phrases in the Croat-
ian and 40 vs. 322 occurrences in the Serbian cor-
pus. It is important to note that the queried cor-
pora consist of texts from the Croatian and Ser-
bian top-level web domain and that the results in
discriminating between Croatian and Serbian lan-
guage applied to these corpora are not used at this
point.
The mentioned difference partly extends to the
future tense (5), which is formed in a similar
manner to English, using present of the verb
"ht(j)eti" as auxiliary verb. The infinitive is for-
mally required in both variants, however, when
?da?+present is used instead, it can additionally
express the subject?s will or intention to perform
the action. This form is frequent in Serbian (?ja
?
cu da radim?), whereas in Croatian only the infini-
tive form is used (?ja
?
cu raditi?). This is, again,
followed by corpus evidence with 0 vs. 71 occur-
rences of the phrases in the Croatian corpus and 13
vs. 22 occurrences in the Serbian corpus. Another
difference regarding future tense exists when the
the auxiliary and main verb are reversed (5b): in
Croatian the final "i" of the infinitive is removed
(?radit
?
cu?), whereas in Serbian the main and the
auxiliary verb merge into a single word (?radi
?
cu?).
4
the corpora can be queried via http://nl.ijs.si/
noske/
77
Croatian Serbian English
vocabulary
1) word level gospodarstvo ekonomija economy
tjedan nedelja week
tisu
?
ca hiljada one thousand
months sije
?
canj januar January
2) character level to
?
cno ta
?
cno accurate
Europa Evropa Europe
vjerojatno verovatno probably
vijesti vesti news
terorist terorista terrorist
3) transcription Washington Va?ington Washington
structure (verbs)
4) modal verbs moram raditi moram da radim I have to work
mogu raditi mogu da radim I can work
5) future tense a) ja
?
cu raditi ja
?
cu da radim I will work
b) radit
?
cu radi
?
cu I will work
6) ?trebati? = should a) trebam raditi treba da radim I should work
treba? raditi treba da radi? you should work
= need b) trebam posao treba mi posao I need a job
Petar treba knjige Petru trebaju knjige Petar needs books
Table 1: Examples of main differences between Croatian and Serbian.
Corpus evidence follows this as well with 611 vs.
9 occurrences in the Croatian corpus and 4 vs.
103 occurrences in the Serbian one. A very im-
portant difference concerns the verb ?trebati? (to
need, should) (6). In Croatian, the verb takes the
tense according to the subject and it is transitive as
in English. In Serbian, when it means ?should?
(6a) it is impersonal followed by ?da? and the
present of the main verb (?treba da radim?). When
it means ?to need? (6b), the verb is conjugated ac-
cording to the needed object (?treba? (job), ?tre-
baju? (books)), and the subject which needs some-
thing (I, Petar) is an indirect grammatical object in
dative case (?meni?, ?Petru?).
Apart from the described differences, there is
also a difference in scripts: Croatian uses only the
Latin alphabet whereas Serbian uses both Latin
and Cyrillic scripts
5
. However, this poses no prob-
lem regarding corpora because a Cyrillic Serbian
5
During the compilation process of the Serbian web cor-
pus (Ljube?i?c and Klubi?cka., 2014), 16.7% of retrieved text
was written in the Cyrillic script.
text can be easily transliterated into Latin.
 0
 20
 40
 60
 80
 100
 1  2  3  4  5  6
n-gram length
n-gram overlap [F-score, %]
sr-hr, char
sr-hr, word 
hr-en, char
sr-en, char
hr-en, word 
sr-en, word 
Figure 1: n-gram overlap on word level and
on character level between Croatian-Serbian,
Croatian-English and Serbian-English.
The idea of Figure 1 is to illustrate the close-
ness and the differences between the two close
languages of interest by numbers: overlapping of
78
word level and character level n-grams for n =
1, ...6 in training, development and test corpora to-
gether is presented via the F-score. In order to give
a better insight, overlaps with English are calcu-
lated as well. It can be seen that the Croatian-
Serbian overlap on character level is very high,
and still rather high on the word level. Charac-
ter overlaps with English are below the Croatian-
Serbian overlap on the word level, whereas the
word level overlaps with English are very low.
3 Translation experiments
In order to explore effects of the described
language differences on cross-language
SMT, four translation systems have been
built: Croatian?English, Serbian?English,
English?Croatian and English?Serbian. For
the sake of brevity and clarity, we will use the
terms ?corresponding source/output? when the
test language is same as the language used for
training, and ?other source/output? when the
cross-language translation is performed. For
translation into English, the translation outputs
of the other source text and its adapted variants
are compared to the translation output of the
corresponding source test with respect to the
English reference. For translation from English,
the other translation output and its adapted ver-
sions are compared to the corresponding output
with respect to the corresponding reference. The
investigated adaptation methods are described in
the next section.
3.1 Language adaptation methods
The following methods were investigated for
adaptation of the test set in the other language:
? lexical conversion of the most frequent words
(conv);
The most frequent
6
different words together
with simple morphological variations are re-
placed by the words in the corresponding lan-
guage. This method is simple and fast, how-
ever it is very basic and also requires knowl-
edge of the involved languages to be set up.
It can be seen as a very first step towards the
use of a rule-based Croatian-Serbian system.
? Croatian-Serbian translation system trained
on three thousand parallel sentences (3k);
6
occurring ? 1000 times in the training corpus
This method does not require any language
knowledge, and a small bilingual corpus is
often not very difficult to acquire. It is even
not very difficult to create it manually from
a monolingual corpus by translating it, al-
though in that case the language knowledge
is needed.
? Croatian-Serbian translation system trained
on the large parallel corpus (200k);
This method is interesting in order to see the
upper limits of the adaptation, however it is
not realistic ? if a large in-domain corpus is
available in both languages, there is no need
for cross-language translation, but pivoting or
synthetic corpora can be used.
The language adaptation is performed in the fol-
lowing way: for translation into English, the other
language test set is first preprocesssed, i.e. con-
verted or translated into the corresponding lan-
guage, and then translated. For the other transla-
tion direction, the English test is translated into the
other language and then converted/translated into
the corresponding one.
In addition, training a system using the con-
verted corpus has also been investigated for all
translation directions.
4 Experimental set-up
The enhanced version
7
of the SEtimes corpus (Ty-
ers and Alperen, 2010) is used for translation
experiments. The corpus is based on the con-
tent published on the SETimes.com news portal
which publishes ?news and views from Southeast
Europe? in ten languages: Bulgarian, Bosnian,
Greek, English, Croatian, Macedonian, Roma-
nian, Albanian and Serbian. We used the paral-
lel trilingual Croatian-English-Serbian part of the
corpus. The detailed corpus statistic is shown in
Table 2. The Croatian language is further referred
to as hr, Serbian as sr and English as en.
The translation system used is the phrase-based
Moses system (Koehn et al., 2007). The evalu-
ation metrics used for assessment of the transla-
tions are the BLEU score (Papineni et al., 2002)
and the F-score, which also takes recall into ac-
count and generally better correlates with human
rankings which has been shown in (Melamed et
al., 2003) and confirmed in (Popovi
?
c, 2011). For
7
http://nlp.ffzg.hr/resources/corpora/setimes/
79
Croatian (hr) Serbian (sr) English (en)
Train sentences 197575
avg sent length 22.3 22.5 23.9
running words 4410721 4453579 4731746
vocabulary 149416 144544 76242
Dev sentences 995
avg sent length 22.2 22.5 24.0
running words 22125 22343 23896
running OOVs 1.7% 1.6% 0.8%
Test sentences 1000
avg sent length 22.3 22.4 23.8
running words 22346 22428 23825
running OOVs 1.5% 1.4% 0.7%
Table 2: Corpus statistics
translation into Croatian and Serbian, F-scores on
character level are also calculated.
5 Results
5.1 Croatian?Serbian language adaptation
This section presents the results of conversion and
translation between Croatian and Serbian in order
to better understand advantages and disadvantages
of each of the adaptation methods. The effects of
each method on translation into and from English
will be reported in the next section.
Table 3 shows the BLEU and F-scores as well
as the percentage of running OOVs for each adap-
tation method. If no adaptation is performed (first
row), the word level scores are about 40%, CHARF
score is close to 75% , and a large number of OOVs
is present ? 13% of running words are unseen. A
large portion of these words differ only by one
or two characters, and for a standard SMT sys-
tem there is no difference between such words and
completely distinct ones.
The conv method, i.e. simple replacement of a
set of words, already makes the text more close:
it reduces the number of OOVs by 3-5% and im-
proves the scores by 3%. The best results are ob-
tained, as it can be expected, by 200k adaptation,
i.e. translation using the large Croatian-Serbian
training corpus; the amount of OOVs in the adapted
text is comparable with the text in the correspond-
ing language (presented in Table 2). The 3k trans-
lation system, being the most suitable for ?real-
word? tasks and improving significantly the text in
the other language (almost 10% reduction of OOVs
and 13% increase of scores) seems to be the most
promising adaptation method.
5.2 Croatian/Serbian?English translation
The translation results into and from English
are presented in Table 4. It can be seen that
the BLEU/WORDF loss induced by cross-language
translation is about 12-13% for translation into
English and about 13-15% for the other direc-
tion. The effects of language adaptation meth-
ods are similar for all translation directions: the
simple lexical conversion conv slightly improves
the translation outputs, and the best option is to
use the 200k translation system. The small train-
ing corpus achieves, of course, less improvement
than the large corpus. On the other hand, taking
into account the significant improvement over the
original of the text of the other language (about
9%) and the advantages of the method discussed
in Sections 3.1 and 5.1, this performance differ-
ence is actually not too large. Future work should
explore techniques for improvement of such sys-
tems.
Last two rows in each table represent the re-
sults of the additional experiment, namely us-
ing the converted other language corpus for train-
ing. However, the results do not outperform
those obtained by (much faster) conversion of the
source/output, meaning that there is no need for
retraining the translation system ? it is sufficient
to adapt only the test source/output.
Translation examples
Table 5 presents two translation examples: the
source/reference sentence in all three languages,
the cross-language translation output, the trans-
80
direction method BLEU WORDF CHARF OOV
none 40.1 43.1 74.7 13.3
hr?sr conv 43.7 46.3 76.4 10.7
3k 54.8 55.9 80.8 4.6
200k 64.3 65.4 85.2 1.4
sr?hr conv 43.5 46.1 76.3 8.5
3k 54.0 55.9 80.9 4.3
200k 64.1 65.3 85.1 1.4
Table 3: BLEU and F-scores for Croatian-Serbian conversion and translation used for adaptation.
lation outputs of adapted sources, as well as the
translation output of the corresponding source.
The examples are given only for translation into
English, and the effects for the other translation di-
rection can be observed implicitly. Generally, the
main source of errors are OOV words, but struc-
tural differences also cause problems.
For the first sentence (1), the conv method is
sufficient for obtaining a perfect cross-translation
output: the obstacles are three OOV words, all of
them being frequent and thus converted. The out-
puts obtained by 3k and 200k methods as well as
the output for the corresponding language are ex-
actly the same and therefore not presented.
The second sentence (2) is more complex: it
contains three OOV words, two of which are not
frequent and thus not adapted by conv, and one
future tense i.e. a structural difference. The OOV
words do not only generate lexical errors (untrans-
lated words) but also incorrect word order (?from
17 dje
?
cjih kazali?ta?). The conv method is able to
repair only the month name, whereas other errors
induced by language differences
8
are still present.
The 3k translation system resolves one more OOV
word (?theater?) together with its position, as well
as the future tense problem, but the third OOV
word ?children?s? is still untranslated and in the
wrong position. This error is fixed only when 200k
translation system is used, since the word occurs in
the large corpus but not in the small one. It should
be noted that the word is, though, an OOV only
due to the one single letter and probably could be
dealt with by character-based techniques (Nakov
and Tiedemann, 2012) which should be investi-
gated in future work.
8
It should be noted that errors not related to the language
differences are out of the scope of this work.
6 Conclusions
In this work, we have examined the possibilities
for using a statistical machine translation system
built on one language and English for translation
from and into another closely related language.
Our experiments on Croatian and Serbian showed
that the loss by cross-translation is about 13% of
BLEU for translation into English and 15% for
translation from English.
We have systematically investigated several
methods for language adaptation. It is shown that
even a simple lexical conversion of limited num-
ber of words yields improvements of about 2%
BLEU, and the Croatian-Serbian translation system
trained on three thousand sentences yields a large
improvement of about 6-9%. The best results are
obtained when the translation system built on the
large corpus is used; however, it should be taken
into account that such scenario is not realistic.
We believe that the use of a small parallel cor-
pus is a very promising method for language adap-
tation and that the future work should concen-
trate in improving such systems, for example by
character-based techniques. We also believe that a
rule-based Croatian-Serbian system could be use-
ful for adaptation, since the translation perfor-
mance has been improved already by applying a
very simple lexical transfer rule. Both approaches
will be investigated in the framework of the ABU-
MATRAN project
9
.
Depending on the availability of resources and
tools, we plan to examine texts in other related lan-
guages such as Slovenian, Macedonian and Bul-
garian (the last already being part of ongoing work
in the framework of the QTLEAP project
10
), and
also to do further investigations on the Croatian-
Serbian language pair.
9
http://abumatran.eu/
10
http://qtleap.eu/
81
(a) translation into English
training source BLEU WORDF
sr?en hr 29.8 34.1
hr-sr.conv 32.3 36.4
hr-sr.3k 37.6 41.1
hr-sr.200k 42.3 45.6
sr 42.9 46.0
hr?en sr 31.4 35.5
sr-hr.conv 32.8 36.8
sr-hr.3k 37.2 40.8
sr-hr.200k 41.7 44.9
hr 43.2 46.3
sr-hr.conv?en hr 32.2 36.2
hr-sr.conv?en sr 33.5 37.4
(b) translation from English
reference output BLEU WORDF CHARF
hr sr 20.6 25.4 62.7
sr-hr.conv 22.8 27.4 64.2
sr-hr.3k 29.3 33.4 68.5
sr-hr.200k 33.5 37.2 71.2
hr 35.5 38.9 72.1
sr hr 20.3 25.3 62.7
hr-sr.conv 22.6 27.4 64.2
hr-sr.3k 29.8 33.7 68.4
hr-sr.200k 34.0 37.5 71.3
sr 35.3 38.5 72.1
sr en?hr-sr.conv 22.6 27.4 64.2
hr en?sr-hr.conv 23.2 27.7 64.2
Table 4: BLEU, WORDF and CHARF scores for translation (a) into English; (b) from English.
Acknowledgments
This work has been supported by the QTLEAP
project ? EC?s FP7 (FP7/2007-2013) under grant
agreement number 610516: ?QTLEAP: Qual-
ity Translation by Deep Language Engineering
Approaches? and the ABU-MATRAN project ?
EC?s FP7 (FP7/2007-2013) under grant agree-
ment number PIAP-GA-2012-324414: ?ABU-
MATRAN: Automatic building of Machine Trans-
lation?.
References
?eljko Agi
?
c, Nikola Ljube?i
?
c and Danijela Merkler.
2013. Lemmatization and Morphosyntactic Tag-
ging of Croatian and Serbian, In Proceedings of
the 4th Biennial International Workshop on Balto-
Slavic Natural Language Processing, pages 48?57,
Sofia, Bulgaria, August.
Miquel Espl?-Gomis and Filip Klubi
?
cka and Nikola
Ljube?i
?
c and Sergio Ortiz-Rojas and Vassilis Pa-
pavassiliou and Prokopis Prokopidis 2014. Com-
paring Two Acquisition Systems for Automatically
Building an English-Croatian Parallel Corpus from
Multilingual Websites, In Proceedings of the 9th In-
ternational Conference on Language Resources and
Evaluation (LREC), Reykjavik, Iceland, May.
Nikola Ljube?i?c and Filip Klubi?cka 2014.
{bs,hr,sr}WaC ? Web corpora of Bosnian, Croatian
and Serbian, In Proceedings of the 9th Web as
Corpus Workshop (WaC-9), Gothenburg, Sweden,
April.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
82
sr Pregled poslovnih i ekonomskih vesti sa Balkana od 15. avgusta.
1) hr Pregled poslovnih i gospodarskih vijesti s Balkana od 15. kolovoza.
en A review of business and economic news from the Balkans since 15 August.
training: sr?en
source: hr A review of business and gospodarskih vijesti from the Balkans since 15 kolovoza .
hr-sr.conv A review of business and economic news from the Balkans since 15 August .
sr Srpski grad Subotica bic?e doma
?
cin 16. izdanja Me
?
dunarodnog festivala
de
?
cjih pozori?ta od 17. do 23. maja.
2) hr Subotica u Srbiji bit c?e doma
?
cin 16 . Me
?
dunarodnog festivala
dje
?
cjih kazali?ta od 17. do 23. svibnja.
en Subotica, Serbia, will host the 16th edition of the International Festival of
Children?s Theatres from May 17th to May 23rd .
training: sr?en
source: hr Subotica in Serbia will be will host the 16th International Festival
from 17 dje
?
cjih kazali?ta to 23 svibnja.
hr-sr.conv Subotica in Serbia will be will host the 16th International Festival
from 17 dje
?
cjih kazali?ta to 23 May.
hr-sr.3k Subotica in Serbia will host the 16th International Theatre Festival
from 17 dje
?
cjih to 23 May.
hr-sr.200k Subotica in Serbia will host the 16th International Children?s
Theatre Festival from 17 to 23 May.
sr The Serbian town of Subotica will host the 16th edition of the
International Children?s Theatre Festival from 17 to 23 May.
Table 5: Two examples of cross-translation of Croatian source sentence into English using
Serbian?English translation system.
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation, In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
pages 177?180, Prague, Czech Republic, June.
Nikola Ljube?i
?
c, Petra Bago and Damir Boras. 2010.
Statistical machine translation of Croatian weather
forecast: How much data do we need?, In Proceed-
ings of the ITI 2010 32nd International Conference
on Information Technology Interfaces, pages 91?96,
Cavtat, Croatia, June.
I.Dan Melamed, Ryan Green and Joseph P. Turian.
2003. Precision and Recall of Machine Transla-
tion. In Proceedings of the Human Language Tech-
nology Conference (HLT-NAACL), pages 61?63, Ed-
monton, Canada, May/June.
Preslav Nakov and J?rg Tiedemann. 2012. Combin-
ing word-level and character-level models for ma-
chine translation between closely-related languages,
In Proceedings of the 50th Annual Meeting of the As-
sociation of Computational Linguistics (ACL), pages
301?305, Jeju, Republic of Korea, July.
Kishore Papineni, Salim Roukos, Todd Ward and Wie-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation, In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 311?318,
Philadelphia, PA, July.
Hrvoje Peradin, Filip Petkovski and Francis Tyers.
2014. Shallow-transfer rule-based machine trans-
lation for the Western group of South Slavic lan-
guages, In Proceedings of the 9th SaLTMiL Work-
shop on Free/open-Source Language Resources for
the Machine Translation of Less-Resourced Lan-
guages, pages 25?30, Reykjavik, Iceland, May.
Maja Popovi
?
c, David Vilar, Hermann Ney, Slobo-
dan Jovi?ci?c and Zoran ?ari?c. 2005. Augment-
ing a Small Parallel Text with Morpho-syntactic
Language Resources for Serbian?English Statisti-
cal Machine Translation In Proceedings of the
ACL-05 Workshop on Building and Using Parallel
83
Texts: Data-Driven Machine Translation and Be-
yond, pages 119?124, Ann Arbor, MI, June.
Maja Popovi?c. 2011. Morphemes and POS tags for
n-gram based evaluation metrics, In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion (WMT 2011), pages 104?107, Edinburgh, Scot-
land, July.
J?rg Tiedemann. 2012. Character-based pivot transla-
tion for under-resourced languages and domains, In
Proceedings of the 13th Conference of the European
Chapter of the Association for Computational Lin-
guistics (EACL), pages 141?151, Avignon, France,
April.
Antonio Toral, Raphael Rubino, Miquel Espl?-Gomis,
Tommi Pirinen, Andy Way and Gema Ramirez-
Sanchez 2014. Extrinsic Evaluation of Web-
Crawlers in Machine Translation: a Case Study on
Croatian?English for the Tourism Domain, In Pro-
ceedings of the 17th Conference of the European As-
sociation for Machine Translation (EAMT), pages
221?224, Dubrovnik, Croatia, June.
Francis M. Tyers and Murat Alperen. 2010. South-
East European Times: A parallel corpus of the
Balkan languages, In Proceedings of the LREC
Workshop on Exploitation of Multilingual Resources
and Tools for Central and (South-) Eastern Euro-
pean Languages, pages 49?53, Valetta, Malta, May.
84
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 58?67,
Dublin, Ireland, August 23 2014.
A Report on the DSL Shared Task 2014
Marcos Zampieri
1
, Liling Tan
2
, Nikola Ljube
?
si
?
c
3
, J
?
org Tiedemann
4
Saarland University, Germany
1,2
University of Zagreb, Croatia
3
Uppsala University, Sweden
4
marcos.zampieri@uni-saarland.de, liling.tan@uni-saarland.de
jorg.tiedemann@lingfil.uu.se, nljubesi@ffzg.hr
Abstract
This paper summarizes the methods, results and findings of the Discriminating between Similar
Languages (DSL) shared task 2014. The shared task provided data from 13 different languages
and varieties divided into 6 groups. Participants were required to train their systems to discrimi-
nate between languages on a training and development set containing 20,000 sentences from each
language (closed submission) and/or any other dataset (open submission). One month later, a test
set containing 1,000 unidentified instances per language was released for evaluation. The DSL
shared task received 22 inscriptions and 8 final submissions. The best system obtained 95.7%
average accuracy.
1 Introduction
Discriminating between similar languages is one of the bottlenecks of state-of-the-art language iden-
tification systems. Although in recent years systems have been trained to discriminate between more
languages
1
, they still struggle to discriminate between similar languages such as Croatian and Serbian or
Malay and Indonesian.
From an NLP point of view, the difficulty systems face when discriminating between closely related
languages is similar to the problem of discriminating between standard national language varieties (e.g.
American English and British English or Brazilian Portuguese and European Portuguese), henceforth
varieties. Recent studies show that language varieties can be discriminated automatically using words or
characters as features (Zampieri and Gebre, 2012; Lui and Cook, 2013) . However, due to performance
limitations, state-of-the-art general-purpose language identification systems do not distinguish texts from
different national varieties, modelling pluricentric languages as unique classes.
To evaluate how state-of-the-art systems perform in identifying similar languages and varieties, we
decided to organize the Discriminating between Similar Languages (DSL)
2
shared task. This shared task
was organized within the scope of the workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial) in the 2014 edition of COLING.
The motivation behind the DSL shared task is two-fold. Firstly, we have observed an increase of
interest in the topic. This is reflected by a number of papers that have been published about this task in
recent years starting with Ranaivo-Malanc?on (2006) for Malay and Indonesian and Ljube?si?c et al. (2007)
for South Slavic languages. In the DSL shared task we tried to include (depending on the availability of
data) languages that have been studied in previous experiments, such as Croatian, English, Indonesian,
Malay, Portuguese and Spanish.
The second aspect that motivated us to organize this shared task is that, to our knowledge, no shared
task focusing on the discrimination of similar languages has been organized previously. The most sim-
ilar shared tasks to DSL are the DEFT 2010 shared task (Grouin et al., 2010), in which systems were
required to classify French journalistic texts with respect to their geographical location as well as the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1
Brown (2013) reports results on a system trained to recognize more than 1,100 languages
2
http://corporavm.uni-koeln.de/vardial/sharedtask.html
58
decade in which they were published. Other related shared tasks include the ALTW 2010 multilingual
language identification shared task, a general-purpose language identification task containing data from
74 languages (Baldwin and Lui, 2010) and finally the Native Language Identification (NLI) shared task
(Tetreault et al., 2013) where participants were provided English essays written by foreign students of 11
different mother tongues (Blanchard et al., 2013). Participants had to train their systems to identify the
native language of the writer of each text.
2 Related Work
Among the first studies to investigate the question of discriminating between similar languages is the
study published by Ranaivo-Malanc?on (2006). The author presents a semi-supervised model to dis-
tinguish between Indonesian and Malay, two closely related languages from the Austronesian family
represented in the DSL shared task. The study uses the frequency and rank of character trigrams derived
from the most frequent words in each language, lists of exclusive words, and the format of numbers
(Malay uses decimal points whereas Indonesian uses commas). The author compares the performance
of this method with the performance obtained by TextCat (Cavnar and Trenkle, 1994).
Ljube?si?c et al. (2007) proposed a computational model for the identification of Croatian texts in
comparison to Slovene and Serbian, reporting 99% recall and precision in three processing stages. The
approach includes a ?black list?, which increases the performance of the algorithm. Tiedemann and
Ljube?si?c (2012) improved this method and applied it to Bosnian, Croatian and Serbian texts. The study
reports significantly higher performance compared to general purpose language identification methods.
The methods applied to discriminate between texts from different language varieties and dialects are
similar to those applied to similar languages
3
. One of the methods proposed to identify language varieties
is by Huang and Lee (2008). This study presented a bag-of-words approach to classify Chinese texts from
the mainland and Taiwan with results of up to 92% accuracy.
Another study that focused on language varieties is the one published by Zampieri and Gebre (2012).
In this study, the authors proposed a log-likelihood estimation method along with Laplace smoothing to
identify two varieties of Portuguese (Brazilian and European). Their approach was trained and tested in
a binary setting using journalistic texts with accuracy results above 99.5% for character n-grams. The
algorithm was later adapted to classify Spanish texts using not only the classical word and character
n-grams but also POS distribution (Zampieri et al., 2013).
The aforementioned study by Lui and Cook (2013) investigates computational methods to discriminate
between texts from three different English varieties (Canadian, Australian and British) across different
domains. The authors state that the results obtained suggest that each variety contains characteristics that
are consistent across multiple domains, which enables algorithms to distinguish them regardless of the
data source.
Zaidan and Callison-Burch (2013) propose computational methods for the identification of Arabic
language varieties
4
using character and word n-grams. The authors built their own dataset using crowd-
sourcing and investigated annotators? behaviour, agreement and performance when manually tagging
instances with the correct label (variety).
3 Methods
In the following subsections we will describe the methodology adopted for the DSL shared task. Due to
the lack of comparable resources, the first decision we had to take was to create a dataset that could be
used in the shared task and also redistributed to be used in other experiments. We opted for the creation
of a corpus collection based on existing datasets as discussed in 3.1 (Tan et al., 2014).
Groups interested in participating in the DSL shared task had to register themselves in the shared
task website to receive the training and test data. Each group could participate in one or two types of
3
In the DSL shared task and in this paper we did not distinguish between language varieties and similar languages. More
on this discussion can be found in Clyne (1992) and Chamber and Trudgill (1998).
4
Zaidan and Callison-Burch (2013) use the terms ?varieties? and ?dialects? interchangeably whereas Lui and Cook (2013)
use the term ?national dialect? to refer to what previous work describes as ?national variety?.
59
submission as follows:
? Closed Submission: Using only the DSL corpus collection for training.
? Open Submission: Using any other dataset including or not the DSL collection for training.
In the open submission we did not make any distinction between systems using the DSL corpus col-
lection and those that did not. This is different from the types of submissions for the NLI shared task
2013. The NLI shared task offered proposed two types of open submissions: open submission 1 - any
dataset including the aforementioned TOEFL11 dataset (Blanchard et al., 2013) and open submission 2
- any dataset excluding TOEFL11.
For each of these submission types, participants were allowed to submit up to three runs, resulting in
a maximum of six runs in total (three closed submissions and three open submissions).
3.1 Data
As previously mentioned, we decided to compile our own dataset for the shared task. The dataset was
entitled DSL corpus collection and its compilation was motivated by the absence of a resource that
allowed us to evaluate systems on discriminating similar languages. The methods behind the compilation
of this collection and the preliminary baseline experiments are described in Tan et al. (2014).
The DSL corpus collection consists of 18,000 randomly sampled training sentences, 2,000 develop-
ment sentences and 1,000 test sentences for each language (or variety) containing at least 20 tokens
5
each.
The languages are presented in table 1 with their ISO 639-1 language codes
6
. For language varieties the
country code is appended to the ISO code (e.g. en-GB refers to the British variety of English).
Group Language/Variety Code
Bosnian bs
A Croatian hr
Serbian sr
Indonesian id
B Malay my
Czech cz
C Slovak sk
Brazilian Portuguese pt-BR
D European Portuguese pt-PT
Argentine Spanish es-AR
E Castilian Spanish es-ES
British English en-GB
F American English en-US
Table 1: Language Groups - DSL 2014 Shared Task
For this collection, randomly sampled sentences from journalistic corpora (and corpora collections) were
selected for each of the 13 classes. Journalistic corpora were preferred because they represent standard
language, which is an important factor to be considered when working with language varieties. Other data
sources (e.g. Wikipedia) do not make any distinction between language varieties and they are therefore
not suitable for the purpose of the shared task. A number of studies mentioned in the related work section
use journalistic texts for similar reasons (Huang and Lee, 2008; Grouin et al., 2010; Zampieri and Gebre,
2012)
Given what has been said in this section, we consider the collection to be a suitable comparable corpora
from this task, which was compiled to avoid bias in classification towards source, register and topics. The
5
We considered a token as orthographic units delimited by white spaces.
6
http://www.loc.gov/standards/iso639-2/php/English_list.php
60
DSL corpus collection was distributed in tab delimited format; the first column contains a sentence in
the language/variety, the second column states its group and the last column refers to its language code.
7
3.1.1 Problems with Group F
There are no major problems to report regarding the organization of the shared task nor with the com-
pilation of the DSL corpus collection apart from some issues in the Group F data. The organizers and
a couple of teams participating in the shared task observed very poor performance when distinguishing
instances from group F (British English - American British). For example, the baseline experiments
described in Tan et al. (2014) report a very low 0.59 F-measure for Group F (the lowest score) and 0.84
for Group E (the second lowest score). Some of the teams asked human annotators to try to distinguish
the sentences manually and they concluded that some instances were probably misclassified.
We decided to look more carefully at the data and noticed that the instances were originally tagged
based on the websites (newspapers) that they were retrieved from and not the country of the original
publication. There are, however, many cases of cross citation and republication of texts that the original
data sources did not take into account (e.g. British texts that were later republished by an American
website). As the DSL is a corpus collection and manually checking all 20,000 training and development
instances per language was not feasible, we assumed that the original sources
8
from which the texts were
retrieved provided the correct country of origin. The assumption was correct for all language groups but
English.
To illustrate the issues above we present next some misclassified examples. Two particular cases raised
by the UMich team are the following:
(1) I think they can afford to give North another innings and some time in Shield cricket and take
another middle order batsman. (en-US)
(2) ATHENS, Ohio (AP) Albuquerque will continue its four-game series in Nashville Thursday night
when it takes on the Sounds behind starter Les Walrond (3-4, 4.50) against Gary Glover, who is
making his first Triple-A start after coming down from Milwaukee. (en-GB)
Example number one was tagged as American English because it was retrieved from the online edition
of The New York Times but it was in fact first published in Australia. The second example is a text
published by Associated Press describing an event that took place in Ohio, United States, but it was
tagged as British English because it was retrieved by the UK Yahoo! sports section.
Our solution was to exclude the language group F from the final scores and perform a manual check in
all its 1,000 test instances
9
, thus giving the chance to participants to train their algorithms on other data
sources (open submission).
3.2 Schedule
The DSL shared task spanned from March 20
th
when the training set was released, to June 6
th
when
participants could submit a paper (up to 10 pages) describing their system. We provided one month
between the release of the training and the test set. The schedule of the DSL shared task 2014 can be
seen below.
Event Date
Training Set Release March 20
th
, 2014
Test Set Release April 21
st
, 2014
Submissions Due April 23
rd
, 2014
Results Announced April 30
th
, 2014
Paper Submission June 6
th
, 2014
Table 2: DSL 2014 Shared Task Schedule
7
To obtain the data please visit: https://bitbucket.org/alvations/dslsharedtask2014
8
See Tan et al. (2014) for a complete description of the data sources of the DSL corpus collection.
9
Our manual check suggests that about 25% of the instances in the English dataset was likely to have been misclassified.
61
4 Results
This section summarises the results obtained by all participants of the shared task who submitted final
results.
10
The DSL shared task included 22 enrolled teams from different countries (e.g. Australia,
Estonia, Holland, Germany, United Kingdom and United States). From the 22 enrolled teams, eight of
them submitted their final results. Most of the groups opted to exclusively use the DSL corpus collection
and therefore participated solely in the closed submission track. Two of them compiled comparable
datasets and also participated in the open submission.
Given that the dataset contained misclassified instances, group F (English) was not taken into account
to compute the final shared task scores. In the next subsections we report results in terms of macro-
average F-measure and accuracy.
4.1 Closed Submission
Table 3 presents the best F-measure and Accuracy results obtained by the eight teams that submitted their
results for the closed submission track ordered by accuracy.
Team Macro-avg F-score Overall Accuracy
NRC-CNRC 0.957 0.957
RAE 0.947 0.947
UMich 0.932 0.932
UniMelb-NLP 0.918 0.918
QMUL 0.925 0.906
LIRA 0.721 0.766
UDE 0.657 0.681
CLCG 0.405 0.453
Table 3: Open Submission - Results
In the closed submissions, we observed a group of five teams whose systems (best runs) obtained re-
sults over 90% accuracy. This is comparable to what is described in the state-of-the-art literature for
discriminating similar languages and language varieties (Tiedemann and Ljube?si?c, 2012; Lui and Cook,
2013). These five teams submitted system descriptions that allowed us to look in more detail at successful
approaches for this task. System descriptions will be discussed in section 5.
Three of the eight teams obtained substantially lower scores, from 45.33% to 76.64% accuracy. These
three groups unfortunately did not submit system description papers. From our point of view, this would
create an interesting opportunity to look more carefully at the weaknesses of approaches that did not
obtain good results in this task.
4.2 Open Submission
Only two systems submitted results for the open submission track and their F-measure and Accuracy
results are presented in table 3.
Team Macro-avg F-score Overall Accuracy
UniMelb-NLP 0.878 0.880
UMich 0.858 0.859
Table 4: Closed Submission - Results
The UniMelb-NLP (Lui et al., 2014) group used data from different corpora such as the BNC, EU-
ROPARL and Open Subtitles whereas UMich (King et al., 2014) compiled journalistic corpora from dif-
ferent sizes for each language ranging from 695,597 tokens for Malay to 20,288,294 tokens for British
English.
10
Visit https://bitbucket.org/alvations/dslsharedtask2014/downloads/dsl-results.html
for more detail on the shared task results or at the aforementioned DSL shared task website.
62
Comparing the results of the closed to the open submissions, we observed that the UniMelb-NLP sub-
mission was outperformed by UMich system by about 1.5% accuracy in the closed submission, but in
the open submission they scored 2.1% better than UMich. This difference can be explained by investi-
gating these two factors: 1) the quality and amount of the collected training data; 2) the robustness of
the method to obtain correct predictions across different datasets and domains as previously discussed
by Lui and Cook (2013) for English varieties.
4.3 Accuracy per Language Group
In this subsection we look more carefully at the performance of systems in discriminating each class
within groups A to E. Table 5 presents the accuracy scores obtained per language group for each team
sorted alphabetically. The best score per group is displayed in bold.
CLCG LIRA NRC-CNRC QMUL RAE UDE UMich UniMelb-NLP
A 0.338 0.333 0.936 0.879 0.919 0.785 0.919 0.915
B 0.503 0.982 0.996 0.935 0.994 0.892 0.992 0.972
C 0.500 1.000 1.000 0.962 1.000 0.493 0.999 1.000
D 0.496 0.892 0.956 0.905 0.948 0.493 0.926 0.896
E 0.503 0.843 0.910 0.865 0.888 0.694 0.876 0.807
Table 5: Language Groups A to E - Accuracy Results
The top 5 systems plus the LIRA team obtained very good results for groups B (Malay and Indonesian)
and C (Czech and Slovak). Four out of eight systems obtained perfect performance when discriminating
Czech and Slovak texts. Perfect performance was not achieved by any of the systems when distinguishing
Malay from Indonesian texts, but even so, results were fairly high and the best result was 99.6% accuracy
obtained by the NRC-CNRC group. The perfect results obtained by four groups when distinguishing
texts from group C suggest that Czech and Slovak texts are not as similar as we assumed before the
shared task, and that they therefore possess strong systemic and/or orthographic differences that allow
well-trained classifiers to perform perfectly. Figure 1 presents the accuracy results of the top 5 groups.
Figure 1: Language Groups A to E Accuracy - Top 5 Systems
Distinguishing between languages from group A (Bosnian, Croatian and Serbian), the only group con-
taining 3 languages, proved to be a challenging task as discussed in previous research (Ljube?si?c et al.,
2007; Tiedemann and Ljube?si?c, 2012). The best result was again obtained by the NRC-CNRC group
with 93.5% accuracy. The groups containing texts written in different language varieties, namely D (Por-
tuguese) and E (Spanish) were the most difficult to discriminate, particularly the Spanish varieties. These
results also corroborate the findings of previous studies (Zampieri et al., 2013).
63
The QMUL system that was the 5
th
best system in the closed submission track did not outperform any
of the other top 5 systems in groups A, B or C. However, the system did better when distinguishing texts
from the two most difficult language groups (D and E), outperforming the UniMelb-NLP submission on
two occasions. The simplicity of the approach proposed by the QMUL, which the author describes as
?a simple baseline? (Purver, 2014) may be an explanation for the regular performance across different
language groups.
4.4 Results Group F
To document the problems in the group F (British and American English) dataset we included the results
of both the open and closed submissions for this language group in table 6. As previously mentioned,
submitting group F results was optional and we did not include these results in the final shared task
results. Six out of eight systems decided to submit their predictions as closed submissions and the two
groups participating in the open submission track also submitted their group F results.
Team F-score Accuracy Type
UMich 0.639 0.639 Open
UniMelb- NLP 0.581 0.583 Open
NRC-CNRC 0.522 0.524 Closed
LIRA 0.450 0.493 Closed
RAE 0.451 0.481 Closed
UMich 0.463 0.464 Closed
UDE 0.451 0.451 Closed
UniMelb-NLP 0.435 0.435 Closed
Table 6: Group F - Accuracy Results
The results confirm the problems in the DSL dataset discussed in section 3.1.1. After a careful manual
check of the 1,000 test instances, open submissions scores were still substantially lower than the other
groups: 69.9% and 58.3% accuracy. Closed submissions proved to be impossible and only one of the six
systems scored slightly above the 50% baseline.
It should be investigated more carefully in future research whether the poor results for group F reflect
only the problems in the dataset or also the actual difficulty in discriminating between these two varieties
of English. Moderate differences in orthography (e.g. neighbour (UK) and neighbor (US)) as well
as lexical choices (e.g. rubbish (UK) and garbage (US) or trousers (UK) and pants (US)) are present
in texts from these two varieties and these can be informative features for algorithms to discriminate
between them. Discriminating between other English varieties already proved to be a challenging yet
feasible task in previous research (Lui and Cook, 2013).
5 System Descriptions
All eight systems that submitted their final results to the shared task were invited to submit papers de-
scribing their systems and the top 5 systems in the closed track submitted their papers, namely: NRC-
CNRC, RAE, UMich, UniMelb-NLP and QMUL.
The best scores were obtained by the NRC-CNRC (Goutte et al., 2014) team which proposed a two-
step approach to predict first the language group than the language of each instance. The language group
was predicted in a 6-way classification using a probabilistic model similar to a Naive Bayes classifier,
and later the method applied SVM classifiers to discriminate within each group: binary for groups B-F
and one versus all for group A, which contains three classes (Bosnian, Croatian and Serbian).
An interesting contribution proposed by the RAE team (Porta and Sancho, 2014) are the so-called
?white lists? inspired by the ?blacklist? classifier (Tiedemann and Ljube?si?c, 2012). These lists are word
lists exclusive to a language or variety, similar to one of the features that Ranaivo-Malanc?on (2006)
proposed to discriminate between Malay and Indonesian.
64
Two groups used Information Gain (IG) to select the best features for classification, namely UMich
(King et al., 2014) and UniMelb-NLP (Lui et al., 2014). These teams were also the only ones to submit
open submissions. The UniMelb-NLP team tried different classification methods and features (including
delexicalized models) in each run. The best results were obtained by their own method, the off-the-shelf
general-purpose language identification software langid.py (Lui and Baldwin, 2012). This method has
been widely used for general-purpose language identification and its performance is regarded superior
to similar general-purpose methods such as TextCat. In the shared task, the system was modelled hier-
archically firstly identifying the language group that a sentence belongs to and subsequently the specific
language, achieving performance comparable to the state-of-the-art, but still slightly below the other
three systems.
The QMUL team (Purver, 2014) proposed a linear SVM classifier using words and characters as fea-
tures. The author investigated the influence of the cost parameter c (from 1.0 to 100.0), in the classifiers?
performance. The cost parameter c is responsible for the trade-off between maximum margin and clas-
sification errors. According to the system description the optimal parameter for this task lies between
30.0 and 50.0. Purver (2014) also notes that the linear SVM classifier performs well with word uni-gram
language models in comparison to methods using character n-grams. This observation corroborates the
findings of previous experiments that rely on words as important features to distinguish similar languages
and varieties (Huang and Lee, 2008; Zampieri, 2013)
The features and algorithms presented so far, as well as the system paper descriptions, are summarised
in table7.
11
Team Algorithm Features System Paper
NRC-CNRC Prob. Class. and Linear SVM Words 1-2, Char. 2-6 (Goutte et al., 2014)
RAE MaxEnt Words 1-2, Char. 1-5, ?Whitelist? (Porta and Sancho, 2014)
UMich Naive Bayes Words 1-2, Char. 2-6 (IG Feat. Selection) (King et al., 2014)
UniMelb-NLP langid.py Words, Char., POS (IG Feat. Selection) (Lui et al., 2014)
QMUL Linear SVM Words 1, Char. 1-3 (Purver, 2014)
Table 7: Top 5 Systems - Features and Algorithms at a Glance
6 Conclusion
Shared tasks are an interesting way of comparing algorithms, computational methods and features using
the same dataset. Given what has been presented in this paper, we believe that the DSL shared task filled
an important gap in language identification and will allow other researchers to look in more detail at the
problem of discriminating similar languages. Accurate methods for discriminating similar languages can
help to improve performance not only in language identification but also in a number of NLP tasks and
applications such as part-of-speech-tagging, spell checking and machine translation.
The best system obtained 95.71% accuracy and F-measure for a set of 11 languages and varieties
divided into 5 groups (A to E), using only the DSL corpus collection. Systems that performed best
modelled their algorithms to perform two-step predictions: first the language group, then the actual class
and used characters and words as features. As we regard the corpus to be a balanced sample of the
news domain, the results obtained confirm the assumption that similar languages and varieties possess
systemic characteristics that can be modelled by algorithms in order to distinguish languages from other
similar languages or varieties using lexical or orthographical features.
Another lesson learned from this shared task is regarding the compilation of group F (English) data.
Researchers, including us, often rely on previously annotated meta-data which sometimes may contain
inaccurate information and errors. Corpus collection for this purpose should be thoroughly checked
(manually if possible). The issues with the group F might have discouraged some of the participants to
continue in the shared task (particularly those who were interested only in the discrimination of English
varieties).
11
UniMelb-NLP experimented different methods in their 6 runs. In this report we commented on the algorithm that achieved
the best performance.
65
6.1 Future Perspectives
The shared task was a very fruitful and positive experience for the organizers. We would like to organize
a second edition of the shared task containing, for example, new language groups for which we could
not find suitable corpora before the 2014 edition. This includes, most notably, the cases of Dutch and
Flemish or the varieties of French and German which could not be included in the DSL shared task due
to the lack of available data.
The DSL corpus collection is freely available and can be used as a gold standard for language iden-
tification or to train algorithms for other NLP tasks involving similar languages. We would like to use
the dataset to investigate, for example, lexical variation between similar languages and varieties as pro-
posed by Piersman et al. (2010) and Soares da Silva (2010) or syntactic variation using annotated data
as discussed in Anstein (2013).
At present, we are investigating the influence of the length of texts in the discrimination of similar
languages. It is a well known fact that the longer texts are, the more likely they are to contain features
that allow algorithms to identify their language. However, this variable was not explored within the
scope of the DSL shared task and we are using the DSL dataset and the results for this purpose. Another
direction that our work may take is the linguistic analysis of the most informative features in classification
as was done recently by Diwersy et al. (2014).
Acknowledgements
The authors would like to thank all participants of the DSL shared task for their comments and sugges-
tions throughout the organization of this shared task. We would also like to thank Joel Tetreault and
Binyam Gebrekidan Gebre for their valuable feedback on this report.
References
Stefanie Anstein. 2013. Computational approaches to the comparison of regional variety corpora : prototyping a
semi-automatic system for German. Ph.D. thesis, University of Stuttgart.
Timothy Baldwin and Marco Lui. 2010. Multilingual language identification: ALTW 2010 shared task data. In
Proceedings of Australasian Language Technology Association Workshop, pages 4?7.
Daniel Blanchard, Joel Tetreault, Derrick Higgins, Aoife Cahill, and Martin Chodorow. 2013. TOEFL11: A
Corpus of Non-Native English. Technical report, Educational Testing Service.
Ralf Brown. 2013. Selecting and weighting n-grams to identify 1100 languages. In Proceedings of the 16th In-
ternational Conference on Text Speech and Dialogue (TSD2013), Lecture Notes in Artificial Intelligence (LNAI
8082), pages 519?526, Pilsen, Czech Republic. Springer.
William Cavnar and John Trenkle. 1994. N-gram-based text catogorization. 3rd Symposium on Document Analy-
sis and Information Retrieval (SDAIR-94).
Jack Chambers and Peter Trudgill. 1998. Dialectology (2nd Edition). Cambridge University Press.
Michael Clyne. 1992. Pluricentric Languages: Different Norms in Different Nations. CRC Press.
Sascha Diwersy, Stefan Evert, and Stella Neumann. 2014. A semi-supervised multivariate approach to the study
of language variation. Linguistic Variation in Text and Speech, within and across Languages.
Cyril Goutte, Serge L?eger, and Marine Carpuat. 2014. The NRC system for discriminating similar languages. In
Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),
Dublin, Ireland.
Cyril Grouin, Dominic Forest, Lyne Da Sylva, Patrick Paroubek, and Pierre Zweigenbaum. 2010. Pr?esentation et
r?esultats du d?efi fouille de texte DEFT2010 o`u et quand un article de presse a-t-il ?et?e ?ecrit? Actes du sixi`eme
D
?
Efi Fouille de Textes.
Chu-ren Huang and Lung-hao Lee. 2008. Contrastive approach towards text source classification based on top-
bag-of-word similarity. In Proceedings of PACLIC 2008, pages 404?410.
66
Ben King, Dragomir Radev, and Steven Abney. 2014. Experiments in sentence language identification with
groups of similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages,
Varieties and Dialects (VarDial), Dublin, Ireland.
Nikola Ljube?si?c, Nives Mikelic, and Damir Boras. 2007. Language identification: How to distinguish similar
languages? In Proceedings of the 29th International Conference on Information Technology Interfaces.
Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings
of the 50th Meeting of the ACL.
Marco Lui and Paul Cook. 2013. Classifying English documents by national dialect. In Proceedings of Aus-
tralasian Language Tchnology Workshop, pages 5?15.
Marco Lui, Ned Letcher, Oliver Adams, Long Duong, Paul Cook, and Timothy Baldwin. 2014. Exploring methods
and resources for discriminating similar languages. In Proceedings of the 1st Workshop on Applying NLP Tools
to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Yves Piersman, Dirk Geeraerts, and Dirk Speelman. 2010. The automatic identification of lexical variation
between language varieties. Natural Language Engineering, 16:469?491.
Jordi Porta and Jos?e-Luis Sancho. 2014. Using maximum entropy models to discriminate between similar lan-
guages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties
and Dialects (VarDial), Dublin, Ireland.
Matthew Purver. 2014. A simple baseline for discriminating similar language. In Proceedings of the 1st Workshop
on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial), Dublin, Ireland.
Bali Ranaivo-Malanc?on. 2006. Automatic identification of close languages - case study: Malay and Indonesian.
ECTI Transactions on Computer and Information Technology, 2:126?134.
Augusto Soares da Silva. 2010. Measuring and parameterizing lexical convergence and divergence between
European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in
Cognitive Sociolinguistics.
Liling Tan, Marcos Zampieri, Nikola Ljube?si?c, and J?org Tiedemann. 2014. Merging comparable data sources
for the discrimination of similar languages: The DSL corpus collection. In Proceedings of The Workshop on
Building and Using Comparable Corpora (BUCC), Reykjavik, Iceland.
Joel Tetreault, Daniel Blanchard, and Aoife Cahill. 2013. A report on the first native language identification shared
task. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications,
Atlanta, GA, USA, June. Association for Computational Linguistics.
J?org Tiedemann and Nikola Ljube?si?c. 2012. Efficient discrimination between closely related languages. In
Proceedings of COLING 2012, pages 2619?2634, Mumbai, India.
Omar F Zaidan and Chris Callison-Burch. 2013. Arabic dialect identification. Computational Linguistics.
Marcos Zampieri and Binyam Gebrekidan Gebre. 2012. Automatic identification of language varieties: The case
of Portuguese. In Proceedings of KONVENS2012, pages 233?237, Vienna, Austria.
Marcos Zampieri, Binyam Gebrekidan Gebre, and Sascha Diwersy. 2013. N-gram language models and POS
distribution for the identification of Spanish varieties. In Proceedings of TALN2013, pages 580?587, Sable
d?Olonne, France.
Marcos Zampieri. 2013. Using bag-of-words to distinguish similar languages: How efficient are they?
In Proceedings of the 14th IEEE International Symposium on Computational Intelligence and Informatics
(CINTI2013), pages 37?41, Budapest, Hungary.
67
