Proceedings of NAACL HLT 2007, pages 236?243,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
Joint Determination of Anaphoricity and Coreference Resolution using
Integer Programming
Pascal Denis and Jason Baldridge
Department of Linguistics
University of Texas at Austin
{denis,jbaldrid}@mail.utexas.edu
Abstract
Standard pairwise coreference resolution
systems are subject to errors resulting
from their performing anaphora identifi-
cation as an implicit part of coreference
resolution. In this paper, we propose
an integer linear programming (ILP) for-
mulation for coreference resolution which
models anaphoricity and coreference as a
joint task, such that each local model in-
forms the other for the final assignments.
This joint ILP formulation provides f -
score improvements of 3.7-5.3% over a
base coreference classifier on the ACE
datasets.
1 Introduction
The task of coreference resolution involves impos-
ing a partition on a set of entity mentions in a docu-
ment, where each partition corresponds to some en-
tity in an underlying discourse model. Most work
treats coreference resolution as a binary classifica-
tion task in which each decision is made in a pair-
wise fashion, independently of the others (McCarthy
and Lehnert, 1995; Soon et al, 2001; Ng and Cardie,
2002b; Morton, 2000; Kehler et al, 2004).
There are two major drawbacks with most sys-
tems that make pairwise coreference decisions. The
first is that identification of anaphora is done implic-
itly as part of the coreference resolution. Two com-
mon types of errors with these systems are cases
where: (i) the system mistakenly identifies an an-
tecedent for non-anaphoric mentions, and (ii) the
system does not try to resolve an actual anaphoric
mention. To reduce such errors, Ng and Cardie
(2002a) and Ng (2004) use an anaphoricity classi-
fier ?which has the sole task of saying whether or
not any antecedents should be identified for each
mention? as a filter for their coreference system.
They achieve higher performance by doing so; how-
ever, their setup uses the two classifiers in a cascade.
This requires careful determination of an anaphoric-
ity threshold in order to not remove too many men-
tions from consideration (Ng, 2004). This sensi-
tivity is unsurprising, given that the tasks are co-
dependent.
The second problem is that most coreference sys-
tems make each decision independently of previous
ones in a greedy fashion (McCallum and Wellner,
2004). Clearly, the determination of membership of
a particular mention into a partition should be condi-
tioned on how well it matches the entity as a whole.
Since independence between decisions is an unwar-
ranted assumption for the task, models that consider
a more global context are likely to be more appropri-
ate. Recent work has examined such models; Luo et
al. (2004) using Bell trees, and McCallum and Well-
ner (2004) using conditional random fields, and Ng
(2005) using rerankers.
In this paper, we propose to recast the task of
coreference resolution as an optimization problem,
namely an integer linear programming (ILP) prob-
lem. This framework has several properties that
make it highly suitable for addressing the two afore-
mentioned problems. The first is that it can uti-
lize existing classifiers; ILP performs global infer-
ence based on their output rather than formulating a
236
new inference procedure for solving the basic task.
Second, the ILP approach supports inference over
multiple classifiers, without having to fiddle with
special parameterization. Third, it is much more
efficient than conditional random fields, especially
when long-distance features are utilized (Roth and
Yih, 2005). Finally, it is straightforward to create
categorical global constraints with ILP; this is done
in a declarative manner using inequalities on the as-
signments to indicator variables.
This paper focuses on the first problem, and
proposes to model anaphoricity determination and
coreference resolution as a joint task, wherein the
decisions made by each locally trained model are
mutually constrained. The presentation of the ILP
model proceeds in two steps. In the first, interme-
diary step, we simply use ILP to find a global as-
signment based on decisions made by the corefer-
ence classifier alone. The resulting assignment is
one that maximally agrees with the decisions of the
classifier, that is, where all and only the links pre-
dicted to be coreferential are used for constructing
the chains. This is in contrast with the usual clus-
tering algorithms, in which a unique antecedent is
typically picked for each anaphor (e.g., the most
probable or the most recent). The second step pro-
vides the joint formulation: the coreference classi-
fier is now combined with an anaphoricity classifier
and constraints are added to ensure that the ultimate
coreference and anaphoricity decisions are mutually
consistent. Both of these formulations achieve sig-
nificant performance gains over the base classifier.
Specifically, the joint model achieves f -score im-
provements of 3.7-5.3% on three datasets.
We begin by presenting the basic coreference
classifier and anaphoricity classifier and their per-
formance, including an upperbound that shows the
limitation of using them in a cascade. We then give
the details of our ILP formulations and evaluate their
performance with respect to each other and the base
classifier.
2 Base models: coreference classifier
The classification approach tackles coreference
in two steps by: (i) estimating the probability,
PC(COREF|?i, j?), of having a coreferential out-
come given a pair of mentions ?i, j?, and (ii) apply-
ing a selection algorithm that will single out a unique
candidate out of the subset of candidates i for which
the probability PC(COREF|?i, j?) reaches a particu-
lar value (typically .5).
We use a maximum entropy model for the coref-
erence classifier. Such models are well-suited for
coreference, because they are able to handle many
different, potentially overlapping learning features
without making independence assumptions. Previ-
ous work on coreference using maximum entropy
includes (Kehler, 1997; Morton, 1999; Morton,
2000). The model is defined in a standard fashion
as follows:
PC(COREF|?i, j?) =
exp(
n?
k=1
?kfk(?i, j?, COREF))
Z(?i, j?)
(1)
Z(?i, j?) is a normalization factor over both out-
comes (COREF and ?COREF). Model parameters
are estimated using maximum entropy (Berger et al,
1996). Specifically, we estimate parameters with
the limited memory variable metric algorithm imple-
mented in the Toolkit for Advanced Discriminative
Modeling1 (Malouf, 2002). We use a Gaussian prior
with a variance of 1000 ? no attempt was made to
optimize this value.
Training instances for the coreference classifier
are constructed based on pairs of mentions of the
form ?i, j?, where j and i are the descriptions for
an anaphor and one of its candidate antecedents, re-
spectively. Each such pair is assigned either a label
COREF (i.e. a positive instance) or a label ?COREF
(i.e. a negative instance) depending on whether or
not the two mentions corefer. In generating the train-
ing data, we followed the method of (Soon et al,
2001) creating for each anaphor: (i) a positive in-
stance for the pair ?i, j? where i is the closest an-
tecedent for j, and (ii) a negative instance for each
pair ?i, k? where k intervenes between i and j.
Once trained, the classifier is used to create a set
of coreferential links for each test document; these
links in turn define a partition over the entire set of
mentions. In the system of Soon et. al. (2001) sys-
tem, this is done by pairing each mention j with each
preceding mention i. Each test instance ?i, j? thus
1Available from tadm.sf.net.
237
formed is then evaluated by the classifier, which re-
turns a probability representing the likelihood that
these two mentions are coreferential. Soon et. al.
(2001) use ?Closest-First? selection: that is, the pro-
cess terminates as soon as an antecedent (i.e., a test
instance with probability > .5) is found or the be-
ginning of the text is reached. Another option is to
pick the antecedent with the best overall probability
(Ng and Cardie, 2002b).
Our features for the coreference classifier fall into
three main categories: (i) features of the anaphor, (ii)
features of antecedent mention, and (iii) relational
features (i.e., features that describe properties which
hold between the two mentions, e.g. distance). This
feature set is similar (though not equivalent) to that
used by Ng and Cardie (2002a). We omit details
here for the sake of brevity ? the ILP systems we
employ here could be equally well applied to many
different base classifiers using many different fea-
ture sets.
3 Base models: anaphoricity classifier
As mentioned in the introduction, coreference clas-
sifiers such as that presented in section 2 suf-
fer from errors in which (a) they assign an an-
tecedent to a non-anaphor mention or (b) they as-
sign no antecedents to an anaphoric mention. Ng
and Cardie (2002a) suggest overcoming such fail-
ings by augmenting their coreference classifier with
an anaphoricity classifier which acts as a filter dur-
ing model usage. Only the mentions that are deemed
anaphoric are considered for coreference resolu-
tion. Interestingly, they find a degredation in per-
formance. In particular, they obtain significant im-
provements in precision, but with larger losses in
recall (especially for proper names and common
nouns). To counteract this, they add ad hoc con-
straints based on string matching and extended men-
tion matching which force certain mentions to be
resolved as anaphors regardless of the anaphoric-
ity classifier. This allows them to improve overall
f -scores by 1-3%. Ng (2004) obtains f -score im-
provements of 2.8-4.5% by tuning the anaphoricity
threshold on held-out data.
The task for the anaphoricity determination com-
ponent is the following: one wants to decide for each
mention i in a document whether i is anaphoric or
not. That is, this task can be performed using a sim-
ple binary classifier with two outcomes: ANAPH and
?ANAPH. The classifier estimates the conditional
probabilities P (ANAPH|i) and predicts ANAPH for i
when P (ANAPH|i) > .5.
We use the following model for our anaphoricity
classifier:
PA(ANAPH|i) =
exp(
n?
k=1
?kfk(i, ANAPH))
Z(i)
(2)
This model is trained in the same manner as the
coreference classifier, also with a Gaussian prior
with a variance of 1000.
The features used for the anaphoricity classifier
are quite simple. They include information regard-
ing (1) the mention itself, such as the number of
words and whether it is a pronoun, and (2) properties
of the potential antecedent set, such as the number of
preceding mentions and whether there is a previous
mention with a matching string.
4 Base model results
This section provides the performance of the pair-
wise coreference classifier, both when used alone
(COREF-PAIRWISE) and when used in a cascade
where the anaphoricity classifier acts as a filter on
which mentions should be resolved (AC-CASCADE).
In both systems, antecedents are determined in the
manner described in section 2.
To demonstrate the inherent limitations of cas-
cading, we also give results for an oracle sys-
tem, ORACLE-LINK, which assumes perfect linkage.
That is, it always picks the correct antecedent for
an anaphor. Its only errors are due to being un-
able to resolve mentions which were marked as non-
anaphoric (by the imperfect anaphoricity classifier)
when in fact they were anaphoric.
We evaluate these systems on the datasets from
the ACE corpus (Phase 2). This corpus is di-
vided into three parts, each corresponding to a dif-
ferent genre: newspaper texts (NPAPER), newswire
texts (NWIRE), and broadcasted news transcripts
(BNEWS). Each of these is split into a train
part and a devtest part. Progress during the de-
velopment phase was determined by using cross-
validation on only the training set for the NPAPER
238
System BNEWS NPAPER NWIRE
R P F R P F R P F
COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8
AC-CASCADE 51.1 79.7 62.3 53.7 79.0 63.9 53.0 81.8 64.3
ORACLE-LINK 69.4 100 82.0 71.2 100 83.1 66.7 100 80.0
Table 1: Recall (R), precision (P), and f -score (F) on the three ACE datasets for the basic coreference system
(COREF-PAIRWISE), the anaphoricity-coreference cascade system (AC-CASCADE), and the oracle which
performs perfect linkage (ORACLE-LINK). The first two systems make strictly local pairwise coreference
decisions.
section. No human-annotated linguistic information
is used in the input. The corpus text was prepro-
cessed with the OpenNLP Toolkit2 (i.e., a sentence
detector, a tokenizer, a POS tagger, and a Named
Entity Recognizer).
In our experiments, we consider only the true
ACE mentions. This is because our focus is on eval-
uating pairwise local approaches versus the global
ILP approach rather than on building a full coref-
erence resolution system. It is worth noting that
previous work tends to be vague in both these re-
spects: details on mention filtering or providing
performance figures for markable identification are
rarely given.
Following common practice, results are given in
terms of recall and precision according to the stan-
dard model-theoretic metric (Vilain et al, 1995).
This method operates by comparing the equivalence
classes defined by the resolutions produced by the
system with the gold standard classes: these are the
two ?models?. Roughly, the scores are obtained by
determining the minimal perturbations brought to
one model in order to map it onto the other model.
Recall is computed by trying to map the predicted
chains onto the true chains, while precision is com-
puted the other way around. We test significant dif-
ferences with paired t-tests (p < .05).
The anaphoricity classifier has an average accu-
racy of 80.2% on the three ACE datasets (using a
threshold of .5). This score is slightly lower than
the scores reported by Ng and Cardie (2002a) for
another data set (MUC).
Table 1 summarizes the results, in terms of recall
(R), precision (P), and f -score (F) on the three ACE
data sets. As can be seen, the AC-CASCADE system
2Available from opennlp.sf.net.
generally provides slightly better precision at the ex-
pense of recall than the COREF-PAIRWISE system,
but the performance varies across the three datasets.
The source of this variance is likely due to the fact
that we applied a uniform anaphoricity threshold
of .5 across all datasets; Ng (2004) optimizes this
threshold for each of the datasets: .3 for BNEWS
and NWIRE and .35 for NPAPER. This variance re-
inforces our argument for determining anaphoricity
and coreference jointly.
The limitations of the cascade approach are also
shown by the oracle results. Even if we had a sys-
tem that can pick the correct antecedents for all truly
anaphoric mentions, it would have a maximum re-
call of roughly 70% for the different datasets.
5 Integer programming formulations
The results in the previous section demonstrate the
limitations of a cascading approach for determin-
ing anaphoricity and coreference with separate mod-
els. The other thing to note is that the results in
general provide a lot of room for improvement ?
this is true for other state-of-the-art systems as well.
The integer programming formulation we provide
here has qualities which address both of these is-
sues. In particular, we define two objective func-
tions for coreference resolution to be optimized with
ILP. The first uses only information from the coref-
erence classifier (COREF-ILP) and the second inte-
grates both anaphoricity and coreference in a joint
formulation (JOINT-ILP). Our problem formulation
and use of ILP are based on both (Roth and Yih,
2004) and (Barzilay and Lapata, 2006).
For solving the ILP problem, we use lp solve,
an open-source linear programming solver which
implements the simplex and the Branch-and-Bound
239
methods.3 In practice, each test document is pro-
cessed to define a distinct ILP problem that is then
submitted to the solver.
5.1 COREF-ILP: coreference-only formulation
Barzilay and Lapata (2006) use ILP for the problem
of aggregation in natural language generation: clus-
tering sets of propositions together to create more
concise texts. They cast it as a set partitioning prob-
lem. This is very much like coreference, where
each partition corresponds to an entity in a discourse
model.
COREF-ILP uses an objective function that is
based on only the coreference classifier and the
probabilities it produces. Given that the classifier
produces probabilities pC = PC(COREF|i, j), the
assignment cost of commiting to a coreference link
is cC?i,j? = ?log(pC). A complement assignment
cost cC?i,j? = ?log(1?pC) is associated with choos-
ing not to establish a link. In what follows, M de-
notes the set of mentions in the document, and P the
set of possible coreference links over these mentions
(i.e., P = {?i, j?|?i, j? ? M ? M and i < j}). Fi-
nally, we use indicator variables x?i,j? that are set to
1 if mentions i and j are coreferent, and 0 otherwise.
The objective function takes the following form:
min
?
?i,j??P
cC?i,j? ? x?i,j? + c
C
?i,j? ? (1? x?i,j?) (3)
subject to:
x?i,j? ? {0, 1} ??i, j? ? P (4)
This is essentially identical to Barzilay and Lapata?s
objective function, except that we consider only
pairs in which the i precedes the j (due to the struc-
ture of the problem). Also, we minimize rather than
maximize due to the fact we transform the model
probabilities with ?log (like Roth and Yih (2004)).
This preliminary objective function merely guar-
antees that ILP will find a global assignment that
maximally agrees with the decisions made by the
coreference classifier. Concretely, this amounts to
taking all (and only) those links for which the classi-
fier returns a probability above .5. This formulation
does not yet take advantage of information from a
classifier that specializes in anaphoricity; this is the
subject of the next section.
3Available from http://lpsolve.sourceforge.net/.
5.2 JOINT-ILP: joint anaphoricity-coreference
formulation
Roth and Yih (2004) use ILP to deal with the joint
inference problem of named entity and relation iden-
tification. This requires labeling a set of named enti-
ties in a text with labels such as person and loca-
tion, and identifying relations between them such
as spouse of and work for. In theory, each of these
tasks would likely benefit from utilizing the infor-
mation produced by the other, but if done as a cas-
cade will be subject to propogation of errors. Roth
and Yih thus set this up as problem in which each
task is performed separately; their output is used to
assign costs associated with indicator variables in an
objective function, which is then minimized subject
to constraints that relate the two kinds of outputs.
These constraints express qualities of what a global
assignment of values for these tasks must respect,
such as the fact that the arguments to the spouse of
relation must be entities with person labels. Impor-
tantly, the ILP objective function encodes not only
the best label produced by each classifier for each
decision; it utilizes the probabilities (or scores) as-
signed to each label and attempts to find a global
optimum (subject to the constraints).
The parallels to our anaphoricity/coreference sce-
nario are straightforward. The anaphoricity problem
is like the problem of identifying the type of entity
(where the labels are now ANAPH and ?ANAPH),
and the coreference problem is like that of determin-
ing the relations between mentions (where the labels
are now COREF or ?COREF).
Based on these parallels, the JOINT-ILP system
brings the two decisions of anaphoricity and corefer-
ence together by including both in a single objective
function and including constraints that ensure the
consistency of a solution for both tasks. Let cAj and
cAj be defined analogously to the coreference clas-
sifier costs for pA = PA(ANAPH|j), the probability
the anaphoricity classifier assigns to a mention j be-
ing anaphoric. Also, we have indicator variables yj
that are set to 1 if mention j is anaphoric and 0 oth-
erwise. The objective function takes the following
240
form:
min
?
?i,j??P
cC?i,j? ? x?i,j? + c
C
?i,j? ? (1?x?i,j?)
+
?
j?M
cAj ? yj + c
A
j ? (1?yj) (5)
subject to:
x?i,j? ? {0, 1} ??i, j? ? P (6)
yj ? {0, 1} ?j ? M (7)
The structure of this objective function is very sim-
ilar to Roth and Yih?s, except that we do not uti-
lize constraint costs in the objective function itself.
Roth and Yih use these to make certain combina-
tions impossible (like a location being an argument
to a spouse of relation); we enforce such effects in
the constraint equations instead.
The joint objective function (5) does not constrain
the assignment of the x?i,j? and yj variables to be
consistent with one another. To enforce consistency,
we add further constraints. In what follows, Mj is
the set of all mentions preceding mention j in the
document.
Resolve only anaphors: if a pair of mentions ?i, j?
is coreferent (x?i,j?=1), then mention j must be
anaphoric (yj=1).
x?i,j? ? yj ??i, j? ? P (8)
Resolve anaphors: if a mention is anaphoric
(yj=1), it must be coreferent with at least one an-
tecedent.
yj ?
?
i?Mj
x?i,j? ?j ? M (9)
Do not resolve non-anaphors: if a mention is non-
anaphoric (yj=0), it should have no antecedents.
yj ?
1
|Mj |
?
i?Mj
x?i,j? ?j ? M (10)
These constraints thus directly relate the two
tasks. By formulating the problem this way, the de-
cisions of the anaphoricity classifier are not taken
on faith as they were with AC-CASCADE. Instead,
we optimize over consideration of both possibilities
in the objective function (relative to the probability
output by the classifier) while ensuring that the final
assignments respect the signifance of what it is to be
anaphoric or non-anaphoric.
6 Joint Results
Table 2 summarizes the results for these different
systems. Both ILP systems are significantly better
than the baseline system COREF-PAIRWISE. Despite
having lower precision than COREF-PAIRWISE, the
COREF-ILP system obtains very large gains in recall
to end up with overall f -score gains of 4.3%, 4.2%,
and 3.0% across BNEWS, NPAPER, and NWIRE, re-
spectively. The fundamental reason for the increase
in recall and drop in precision is that COREF-ILP can
posit multiple antecedents for each mention. This
is an extra degree of freedom that allows COREF-
ILP to cast a wider net, with a consequent risk of
capturing incorrect antecedents. Precision is not
completely degraded because the optimization per-
formed by ILP utilizes the pairwise probabilities of
mention pairs as weights in the objective function
to make its assignments. Thus, highly improbable
links are still heavily penalized and are not chosen
as coreferential.
The JOINT-ILP system demonstrates the benefit
ILP?s ability to support joint task formulations. It
produces significantly better f -scores by regaining
some of the ground on precision lost by COREF-
ILP. The most likely source of the improved pre-
cision of JOINT-ILP is that weights corresponding
to the anaphoricity probabilities and constraints (8)
and (10) reduce the number of occurrences of non-
anaphors being assigned antecedents. There are also
improvements in recall over COREF-ILP for NPAPER
and NWIRE. A possible source of this difference is
constraint (9), which ensures that mentions which
are considered anaphoric must have at least one an-
tecedent.
Compared to COREF-PAIRWISE, JOINT-ILP dra-
matically improves recall with relatively small
losses in precision, providing overall f -score gains
of 5.3%, 4.9%, and 3.7% on the three datasets.
7 Related Work
As was just demonstrated, ILP provides a principled
way to model dependencies between anaphoricity
decisions and coreference decisions. In a simi-
lar manner, this framework could also be used to
capture dependencies among coreference decisions
themselves. This option ?which we will leave for
future work? would make such an approach akin to
241
System BNEWS NPAPER NWIRE
R P F R P F R P F
COREF-PAIRWISE 54.4 77.4 63.9 58.1 80.7 67.6 53.8 78.2 63.8
COREF-ILP 62.2 75.5 68.2 67.1 77.3 71.8 60.1 74.8 66.8
JOINT-ILP 62.1 78.0 69.2 68.0 77.6 72.5 60.8 75.8 67.5
Table 2: Recall (R), precision (P), and f -score (F) on the three ACE datasets for the basic coreference system
(COREF-PAIRWISE), the coreference only ILP system (COREF-ILP), and the joint anaphoricity-coreference
ILP system (JOINT-ILP). All f -score differences are significant (p < .05).
a number of recent global approaches.
Luo et al (2004) use Bell trees to represent the
search space of the coreference resolution problem
(where each leaf is possible partition). The prob-
lem is thus recast as that of finding the ?best? path
through the tree. Given the rapidly growing size of
Bell trees, Luo et al resort to a beam search al-
gorithm and various pruning strategies, potentially
resulting in picking a non-optimal solution. The re-
sults provided by Luo et al are difficult to compare
with ours, since they use a different evaluation met-
ric.
Another global approach to coreference is the
application of Conditional Random Fields (CRFs)
(McCallum and Wellner, 2004). Although both are
global approaches, CRFs and ILP have important
differences. ILP uses separate local classifiers which
are learned without knowledge of the output con-
straints and are then integrated into a larger infer-
ence task. CRFs estimate a global model that di-
rectly uses the constraints of the domain. This in-
volves heavy computations which cause CRFs to
generally be slow and inefficient (even using dy-
namic programming). Again, the results presented
in McCallum and Wellner (2004) are hard to com-
pare with our own results. They only consider
proper names, and they only tackled the task of
identifying the correct antecedent only for mentions
which have a true antecedent.
A third global approach is offered by Ng (2005),
who proposes a global reranking over partitions gen-
erated by different coreference systems. This ap-
proach proceeds by first generating 54 candidate
partitions, which are each generated by a differ-
ent system. These different coreference systems
are obtained as combinations over three different
learners (C4.5, Ripper, and Maxent), three sam-
pling methods, two feature sets (Soon et al, 2001;
Ng and Cardie, 2002b), and three clustering al-
gorithms (Best-First, Closest-First, and aggressive-
merge). The features used by the reranker are of
two types: (i) partition-based features which are
here simple functions of the local features, and (ii)
method-based features which simply identify the
coreference system used for generating the given
partition. Although this approach leads to significant
gains on the both the MUC and the ACE datasets,
it has some weaknesses. Most importantly, the dif-
ferent systems employed for generating the different
partitions are all instances of the local classification
approach, and they all use very similar features. This
renders them likely to make the same types of errors.
The ILP approach could in fact be integrated with
these other approaches, potentially realizing the ad-
vantages of multiple global systems, with ILP con-
ducting their interactions.
8 Conclusions
We have provided two ILP formulations for resolv-
ing coreference and demonstrated their superiority
to a pairwise classifier that makes its coreference as-
signments greedily.
In particular, we have also shown that ILP pro-
vides a natural means to express the use of both
anaphoricity classification and coreference classifi-
cation in a single system, and that doing so provides
even further performance improvements, specifi-
cally f -score improvements of 5.3%, 4.9%, and
3.7% over the base coreference classifier on the ACE
datasets.
With ILP, it is not necessary to carefully control
the anaphoricity threshold. This is in stark contrast
to systems which use the anaphoricity classifier as a
filter for the coreference classifier in a cascade setup.
242
The ILP objective function incorporates the proba-
bilities produced by both classifiers as weights on
variables that indicate the ILP assignments for those
tasks. The indicator variables associated with those
assignments allow several constraints between the
tasks to be straightforwardly stated to ensure consis-
tency to the assignments. We thus achieve large im-
provements with a simple formulation and no fuss.
ILP solutions are also obtained very quickly for the
objective functions and constraints we use.
In future work, we will explore the use of global
constraints, similar to those used by (Barzilay and
Lapata, 2006) to improve both precision and recall.
For example, we expect transitivity constraints over
coreference pairs, as well as constraints on the en-
tire partition (e.g., the number of entities in the doc-
ument), to help considerably. We will also consider
linguistic constraints (e.g., restrictions on pronouns)
in order to improve precision.
Acknowledgments
We would like to thank Ray Mooney, Rohit Kate,
and the three anonymous reviewers for their com-
ments. This work was supported by NSF grant IIS-
0535154.
References
Regina Barzilay and Mirella Lapata. 2006. Aggregation
via set partitioning for natural language generation. In
Proceedings of the HLT/NAACL, pages 359?366, New
York, NY.
A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A
maximum entropy approach to natural language pro-
cessing. Computational Linguistics, 22(1):39?71.
A. Kehler, D. Appelt, L. Taylor, and A. Simma.
2004. The (non)utility of predicate-argument frequen-
cies for pronoun interpretation. In Proceedings of
HLT/NAACL, pages 289?296.
Andrew Kehler. 1997. Probabilistic coreference in infor-
mation extraction. In Proceedings of EMNLP, pages
163?173.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, , and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the Bell tree. In Proceedings of the ACL.
Robert Malouf. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Language
Learning, pages 49?55, Taipei, Taiwan.
Andrew McCallum and Ben Wellner. 2004. Conditional
models of identity uncertainty with application to noun
coreference. In Proceedings of NIPS.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Using
decision trees for coreference resolution. In Proceed-
ings of IJCAI, pages 1050?1055.
Thomas Morton. 1999. Using coreference for ques-
tion answering. In Proceedings of ACL Workshop on
Coreference and Its Applications.
Thomas Morton. 2000. Coreference for NLP applica-
tions. In Proceedings of ACL, Hong Kong.
Vincent Ng and Claire Cardie. 2002a. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings of COLING.
Vincent Ng and Claire Cardie. 2002b. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of ACL, pages 104?111.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings of ACL.
Vincent Ng. 2005. Machine learning for coreference res-
olution: From local classification to global ranking. In
Proceedings of ACL.
Dan Roth and Wen-tau Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proceedings of CoNLL.
Dan Roth and Wen-tau Yih. 2005. Integer linear pro-
gramming inference for conditional random fields. In
Proceedings of ICML, pages 737?744.
W. Soon, H. Ng, and D. Lim. 2001. A machine learning
approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceedings
fo the 6th Message Understanding Conference (MUC-
6), pages 45?52, San Mateo, CA. Morgan Kaufmann.
243
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660?669,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Specialized models and ranking for coreference resolution
Pascal Denis
ALPAGE Project Team
INRIA Rocquencourt
F-78153 Le Chesnay, France
pascal.denis@inria.fr
Jason Baldridge
Department of Linguistics
University of Texas at Austin
Austin, TX 78712-0198, USA
jbaldrid@mail.utexas.edu
Abstract
This paper investigates two strategies for im-
proving coreference resolution: (1) training
separate models that specialize in particu-
lar types of mentions (e.g., pronouns versus
proper nouns) and (2) using a ranking loss
function rather than a classification function.
In addition to being conceptually simple, these
modifications of the standard single-model,
classification-based approach also deliver sig-
nificant performance improvements. Specifi-
cally, we show that on the ACE corpus both
strategies produce f -score gains of more than
3% across the three coreference evaluation
metrics (MUC, B3, and CEAF).
1 Introduction
Coreference resolution is the task of partitioning a
set of entity mentions in a text, where each par-
tition corresponds to some entity in an underlying
discourse model. While early machine learning ap-
proaches for the task relied on local, discriminative
classifiers (Soon et al, 2001; Ng and Cardie, 2002b;
Morton, 2000; Kehler et al, 2004), more recent ap-
proaches use joint and/or global models (McCallum
and Wellner, 2004; Ng, 2004; Daume? III and Marcu,
2005; Denis and Baldridge, 2007a). This shift im-
proves performance, but the systems are consider-
ably more complex and often less efficient. Here,
we explore two simple modifications of the first type
of approach that yield performance gains which are
comparable, and sometimes better, to those obtained
with these more complex systems. These modifica-
tions involve: (i) the use of rankers instead of clas-
sifiers, and (ii) the use of linguistically motivated,
specialized models for different types of mentions.
Ranking models provide a theoretically more ad-
equate and empirically better alternative approach
to pronoun resolution than standard classification-
based approaches (Denis and Baldridge, 2007b).
In essence, ranking models directly capture during
training the competition among potential antecedent
candidates, instead of considering them indepen-
dently. This gives the ranker additional discrimina-
tive power and in turn better antecedent selection ac-
curacy. Here, we show that ranking is also effective
for the wider task of coreference resolution.
Coreference resolution involves several different
types of anaphoric expressions: third-person pro-
nouns, speech pronouns (i.e., first and second person
pronouns), proper names, definite descriptions and
other types of nominals (e.g., anaphoric uses of in-
definite, quantified, and bare noun phrases). Differ-
ent anaphoric expressions exhibit different patterns
of resolution and are sensitive to different factors
(Ariel, 1988; van der Sandt, 1992; Gundel et al,
1993), yet most machine learning approaches have
ignored these differences and handle these different
phenomena with a single, monolithic model. A few
exceptions are worth noting. Morton (2000) and Ng
(2005b) propose different classifiers models for dif-
ferent NPs for coreference resolution and pronoun
resolution, respectively. Other partially capture the
differential preferences between different anaphors
via different sample selection strategies during train-
ing (Ng and Cardie, 2002b; Uryupina, 2004). More
recently, Haghighi and Klein (2007) use the distinc-
tion between pronouns, nominals and proper nouns
660
in their unsupervised, generative model for corefer-
ence resolution; for their model, this is absolutely
critical for achieving better accuracy. Here, we show
that using specialized models for different types
of referential expressions improves performance for
supervised models (both classifiers and rankers).
Both these strategies lead to improvements for
all three standard coreference metrics: MUC (Vilain
et al, 1995), B3 (Bagga and Baldwin, 1998), and
CEAF (Luo, 2005). In particular, our specialized
ranker system provides absolute f -score improve-
ments against an otherwise identical standard clas-
sifier system by 3.2%, 3.1%, and 3.6% for MUC, B3,
and CEAF, respectively.
2 Ranking
Numerous approaches to anaphora and coreference
resolution reduce these tasks to a binary classifica-
tion task, whereby pairs of mentions are classified as
coreferential or not (McCarthy and Lehnert, 1995;
Soon et al, 2001; Ng and Cardie, 2002b). Usually
used in combination with a greedy right-to-left clus-
tering, these approaches make very strong indepen-
dence assumptions. Not only do they model each
coreference decision separately, they actually model
each pair of mentions as a separate event. Recast-
ing these tasks as ranking tasks partly addresses this
problem by directly making the comparison between
different candidate antecedents for an anaphor part
of the training criterion. Each candidate is assigned
a conditional probability with respect to the entire
candidate set. (Re)rankers have been successfully
applied to numerous NLP tasks, such as parse se-
lection (Osborne and Baldridge, 2004; Toutanova et
al., 2004), parse reranking (Collins and Duffy, 2002;
Charniak and Johnson, 2005), question-answering
(Ravichandran et al, 2003).
The twin-candidate classification approach pro-
posed by (Yang et al, 2003) shares some similarities
with the ranker in making the comparison between
candidate antecedents part of training. An important
difference however is that under the twin-candidate
approach, candidates are compared in pairwise fash-
ion (and the best overall candidate is the one that has
won the most round robin contests), while the ranker
considers the entire candidate set at once. Another
advantage of the ranking approach is that its com-
plexity is only square in the number of mentions,
while that of the twin-candidate model is cubic (see
Denis and Baldridge (2007b) for a more detailed
comparison in the context of pronoun resolution).
Our ranking models for coreference take the fol-
lowing log-linear form:
Prk(?i|pi) =
exp
m?
j=1
wjfj(pi, ?i)
?
k
exp
m?
j=1
wjfj(pi, ?k)
(1)
where pi stands for the anaphoric expression, ?i for
an antecedent candidate, fj the weighted features of
the model. The denominator consists of a normal-
ization factor over the k candidate mentions. Model
parameters were estimated with the limited memory
variable metric algorithm and Gaussian smoothing
(?2=1000), using TADM (Malouf, 2002).
For the training of the different ranking models,
we use the following procedure. For each model, in-
stances are created by pairing each anaphor of the
proper type (e.g., definite description) with a set of
candidates which contains: (i) a true antecedent, and
(ii) a set of non-antecedents. The selection of the
true antecedent varies depending on the model we
are training: for pronominal forms, the antecedent
is selected as the closest preceding mention in the
chain; for non-pronominal forms, we used the clos-
est preceding non-pronominal mention in the chain
as the antecedent. For the creation of the non-
antecedent set, we collect all the non-antecedents
that appear in a window of two sentences around the
antecedent.1 At test time, we consider all preceding
mentions as potential antecedents.
Not all referential expressions in a given docu-
ment are anaphors: some expressions introduce a
discourse entity, rather than accessing an existing
one. Thus, coreference resolvers must have a way of
identifying such ?discourse-new? expressions. This
is easily handled in the standard classification ap-
proach: a mention will not be resolved if none of its
candidates is classified positively (i.e., as coreferen-
tial). The problem is more troublesome for rankers,
which always pick an antecedent from the candidate
1We suspect that different varying windows might be more
appropriate for different types of expressions, but leaves this for
further investigations.
661
set. A natural solution is to use a model that specifi-
cally predicts the discourse status (discourse-new vs.
discourse-old) of each expression: only expressions
that are classified as ?discourse-old? by this model
are considered by rankers.
Ng and Cardie (Ng and Cardie, 2002a) introduced
the use of an ?anaphoricity? classifier to act as a fil-
ter for coreference resolution in order to correct er-
rors where antecedents are mistakenly identified for
non-anaphoric mentions or antecedents are not de-
termined for mentions which are indeed anaphoric.
Their approach produced significant improvements
in precision, but with consequent larger losses in re-
call. Ng (2004) improves recall by optimizing the
anaphoricity threshold. By using joint inference for
anaphoricity and coreference, Denis and Baldridge
(2007a) avoid cascade-induced errors without the
need to separately optimize the threshold.
We use a similar discourse status classifier to Ng
and Cardie?s as a filter on mentions for our rankers.
We rely on three main types of information sources:
(i) the form of mention (e.g., type of linguistic ex-
pression, number of tokens), (ii) positional features
in the text, (iii) comparisons of the given mention to
the mentions that precede it in the text. Evaluated on
the ACE datasets, training the model on the train
texts, and applying the classifier to the devtest
texts, the model achieves an overall accuracy score
of 80.8%, compared to a baseline of 59.7% when
predicting the majority class (?discourse-old?).
3 Specialized models
Our second strategy is to use different, specialized
models for different referential expressions, simi-
larly to Elwell and Baldridge?s (2008) use of connec-
tive specific models for identifying the arguments of
discourse connectives. For this, one must determine
along which dimension to split such expressions.
For example, Ng (2005b) learns models for each set
of anaphors that are lexically identical (e.g., I, he,
they, etc.). This option is possible for closed sets
like pronouns, but not for other types of anaphors
like proper names and definite descriptions. Another
option is to rely on the particular linguistic form of
the different expressions, as signaled by the head
word category and the determiner (if any). More
concretely, we use separate models for the follow-
ing types: (i) third person pronouns, (ii) speech pro-
nouns, (iii) proper names, (iv) definite descriptions,
and (v) others (i.e., all expressions that don?t fall into
the previous categories).
The correlation between the form of a referen-
tial expression and its anaphoric behavior is actually
central to various linguistic accounts (Prince, 1981;
Ariel, 1988; Gundel et al, 1993). Basically, the idea
is that linguistic form is an indicator of the status of
the corresponding referent in the discourse model.
That is, the use by the speaker of a particular lin-
guistic form corresponds to a particular level of acti-
vation (or familiarity or salience or accessibility) in
(what she thinks is) the addressee?s discourse model.
For many authors, the relation takes the form of a
continuum and is often represented in the form of a
referential hierarchy, such as:
Accessibility Hierarchy (Ariel, 1988)
Zero pronouns >> Pronouns >> Demonstra-
tive pronouns >> Demonstrative NPs >>
Short PNs >> Definite descriptions >> Full
PNs >> Full PNs + appositive
The higher up, the more accessible (or salient) the
entity is. At the extremes are pronouns (these forms
typically require a previous mention in the local con-
text) and proper names (these forms are often used
without previous mentions of the entity). This type
of hierarchy is validated by corpus studies of the
distribution of different types of expressions. For
instance, pronouns find their antecedents very lo-
cally (in a window of 1-2 sentences), while proper
names predominantly find theirs at longer distances
(Ariel, 1988).2 Using discourse structure, Asher et
al. (2006) show that while anaphoric pronouns sys-
tematically obey the right-frontier constraint (i.e.,
their antecedents have to appear on the right edge
of the discourse graph), this is less so for definites,
and even less so for proper names.
From a machine learning perspective, these find-
ings suggest that features encoding some aspect of
salience (e.g., distance, syntactic context) are likely
to receive different sets of parameters depending on
the form of the anaphor. This therefore suggests
that better parameters are likely to be learned in the
2Haghighi and Klein?s (2007) generative coreference model
mirrors this in the posterior distribution which it assigns to men-
tion types given their salience (see their Table 1).
662
Type/Count train test
3rd pron. 4, 389 1, 093
speech pron. 2, 178 610
proper names 7, 868 1, 532
def. NPs 3, 124 796
others 1, 763 568
Total 19, 322 4, 599
Table 1: Distribution of the different anaphors in ACE
context of different models.3 While the above stud-
ies focus primarily on salience, there are of course
other dimensions according to which anaphors differ
in their resolution preferences. Thus, the resolution
of lexical expressions like definite descriptions and
proper names is likely to benefit from the inclusion
of features that compare the strings of the anaphor
and the candidate antecedent (e.g., string matching)
and features that identify particular syntactic config-
urations like appositive structures. This type of in-
formation is however much less likely to help in the
resolution of pronominal forms. The problem is that,
within a single model, such features are likely to re-
ceive strong parameters (due to the fact that they are
good predictors for lexical anaphors) in a way that
might eventually hurt pronominal resolutions.
Note that our split of referential types only
partially cover the referential hierarchies of Ariel
(1988) or Gundel et al (1993). Thus, there is no sep-
arate model for demonstrative noun phrases and pro-
nouns: these are very rare in the corpus we used (i.e.,
the ACE corpus).4 These expressions were therefore
handled through the ?others? model. There is how-
ever a model for first and second person pronouns
(i.e., speech pronouns): this is justified by the fact
that these pronouns behave differently from their
third person counterparts. These forms indeed of-
ten behave like deictics (i.e., they refer to discourse
participants) or they appear within a quote.
The total number of anaphors (i.e., of mentions
that are not chain heads) in the data is 19, 322 and
4, 599 for training and testing, respectively. The dis-
tribution of each anaphoric type is presented in Ta-
ble 1. Roughly, third person pronouns account for
3Another possible approach would consist in introducing
different salience-based features encoding the form of the
anaphor.
4There are only 114 demonstrative NPs and 12 demonstra-
tive pronouns in the entire ACE training.
Linguistic Form
pn ? is a proper name {1,0}
def np ? is a definite description {1,0}
indef np ? is an indefinite description {1,0}
pro ? is a pronoun {1,0}
Context
left pos POS of the token preceding ?
right pos POS of the token following ?
surr pos pair of POS for the tokens surrounding ?
Distance
s dist Binned values for sentence distance between pi and ?
np dist Binned values for mention distance between pi and ?
Morphosyntactic Agreement
gender pairs of attributes {masc, fem, neut, unk} for pi and ?
number pairs of attributes {sg, pl} for pi and ?
person pairs of attributes {1, 2, 3, 4, 5, 6} for pi and ?
Semantic compatibility
wn sense pairs of Wordnet senses for pi and ?
String similarity
str match pi and ? have identical strings {1,0}
left substr one mention is a left substring of the other {1,0}
right substr one mention is a right substring of the other {1,0}
hd match pi and ? have the same head word {1,0}
Apposition
apposition pi and ? are in an appositive structure {1,0}
Acronym
acronym pi is an acronym of ? or vice versa {1,0}
Table 2: Features used by coreference models.
22-24% of all anaphors in the entire corpus, speech
pronouns for 11-13%, proper names for 33-40%,
and definite descriptions for 16-17%. The distribu-
tion is slightly different from one dataset to another,
probably reflecting genre differences. For instance,
BNEWS shows a larger proportion of pronouns in
general (pronominal forms account for 40-44% of
all the anaphoric forms).
We use five broad types of features for all mention
types, plus three others used by specific types, sum-
marized in Table 3. Our feature extraction relies on
limited linguistic processing: we only made use of a
sentence detector, a tokenizer, a POS tagger (as pro-
vided by the OpenNLP Toolkit5) and the WordNet6
database. Since we did not use parser, lexical heads
for the NP mentions were computed using simple
heuristics relying solely on POS sequences. Table 2
describes in detail the entire feature set, and Table 3
shows which features were used for which models.
Linguistic form: the referential form of the an-
tecedent candidate: a proper name, a definite de-
5http://opennlp.sf.net.
6http://wordnet.princeton.edu/
663
Features/Types 3P SP PN Def-NP Oth
Ling. form
? ? ? ? ?
Context
? ? ? ? ?
Distance
? ? ? ? ?
Agreement.
? ? ? ? ?
Sem. compat.
? ? ? ? ?
Str. sim.
? ? ?
Apposition
? ?
Acronym
?
Table 3: Features for each type of referential expression.
scription, an indefinite NP, or a pronoun.
Context: the context of the antecedent candidate:
these features can be seen as approximations of the
grammatical roles, as indicators of the salience of
the potential candidate (Grosz et al, 1995). For
instance, this includes the part of speech tags sur-
rounding the candidate, as well as a feature that
indicates whether the potential antecedent is the
first mention in a sentence (approximating subject-
hood), and a feature indicating whether the candi-
date is embedded inside another mention.
Distance: the distance between the anaphor and
the candidate, measured by the number of sentences
and mentions between them.
Morphosyntactic agreement: indicators of the
gender, number, and person of the two mentions.
These are determined for non-pronominal NPs with
heuristics based on POS tags (e.g., NN vs. NNS for
number) and actual mention strings (e.g., whether
the mention contains a male/female first name or
honorific for gender). These features consist of pairs
of attributes, ensuring that not only strict agreement
(e.g., singular-singular) but also mere compatibility
(e.g., masculine-unknown) is captured.
Semantic compatibility: features designed to as-
sess whether the two mentions are semantically
compatible. For these features, we use the Word-
Net database: in particular, we collected the syn-
onym set (or synset) as well as the synset of their
direct hypernyms associated with each mention. In
the case of common nouns, we used the synset asso-
ciated with the first sense associated with the men-
tion?s head word. In the case of proper names, we
used the synset associated with the name if avail-
able, and the string itself otherwise. For pronouns
(which are not part of Wordnet), we simply used the
pronominal form.
All these features were used in all five models.
While one may question the use of distance for non-
pronominal anaphors,7 their inclusion can be justi-
fied in that they might predict some ?obviation? ef-
fects. Definite descriptions and proper names are
sensitive to distance too, although not in the same
way as pronouns are: they show a preference for an-
tecedents that appear outside a window of one or two
sentences (Ariel, 1988).
Several features are used only for particular men-
tion types:
String similarity: similarity of the anaphor and
the candidate strings. Examples are perfect string
match, substring matches, and head match (i.e., the
two mentions share the same head word).
Appositive: whether the anaphor is an appositive
of the antecedent candidate. Since we do not have
access to syntactic structure, we use heuristics (e.g.,
the presence of a comma between the two mentions)
to extract this feature.
Acronym: whether the anaphor string is an
acronym of the candidate string (or vice versa): e.g.,
NSF and National Science Foundation.
4 Coreference systems
We evaluate several systems to explore the effect of
ranking versus classification and specialized versus
monolithic models. The different systems follow a
generic architecture. Let M be the set of mentions
present in a document. For all models, each mention
m ? M is associated at test time with a set of an-
tecedent candidates Cm, which includes all the men-
tions that linearly precede m. The best candidate is
determined by the model in use. The final output of
each system consists in a list of mention pairs (i.e.,
the coreference links) which in turn defines (through
reflexive, transitive closure) a partition over the set
M. Our models are summarized in Table 4.
The use of the discourse status filter is straightfor-
ward. For each mention m?M, the discourse status
7In fact, Morton (2000) does not use distance in this case.
664
Model Disc.
Model Name Type Specialized? Status
CLASS class No No
CLASS+DS class No Yes
CLASS+SP class Yes No
CLASS+DS+SP class Yes Yes
RANK+DS+SP rank Yes Yes
Table 4: Model names and their properties.
model is first applied to determine whether m intro-
duces a new discourse entity (i.e., it is classified as
?new?) or refers back to an existing entity (i.e., it
is classified as ?old?). If m is classified as ?new?,
the process terminates and goes to the next mention.
If m is classified as ?old?, m along with its set of
antecedent candidates Cm is sent to the model.
For classifiers, we replicate the procedures of Ng
and Cardie (2002b). During training, instances are
formed by pairing each anaphor with each of its pre-
ceding candidates, until the antecedent is reached:
the closest preceding antecedent in the case of a
pronominal anaphor, or the closest non-pronominal
antecedent for other anaphor types. For classifiers,
the use of a discourse status filter at test time is op-
tional. When a filter is not used, then a mention
is left unresolved if none of the pairs created for a
given mention is classified positively. If several pairs
for a given mention are classified positively, then the
pair with the highest score is selected (i.e., ?Best-
First? link selection). If a filter is used, then the can-
didate with the highest score is selected, even if the
probability of coreference is less than one-half.8
The use of specialized models is simple, for both
classifiers and rankers. Specialized models are cre-
ated for: (i) third person pronouns, (ii) speech pro-
nouns, (iii) proper names, (iv) definite descriptions,
(v) other types of phrases. The mention type is de-
8This is very similar to the approach of Ng and Cardie
(2002a). An important difference is that their system does not
necessarily yield an antecedent for each of the anaphors pro-
posed by the discourse status model. In their system, if the
coreference classifier finds that none of the candidates for a
?new? mention are coreferential, it leaves it unresolved. In this
case, the coreference model acts as an additional filter. Not sur-
prisingly, these authors report gains in precision but compar-
atively larger losses in recall. Our development experiments
revealed that forcing a decision on items identified as new pro-
vided performed better across all metrics.
System Accuracy
3rd pron. 82.2
speech pron. 66.9
proper names 83.5
def. NPs 66.5
others 63.6
Table 5: Accuracy of the different ranker models.
termined and the best candidate is chosen by the
appropriate model Following Elwell and Baldridge
(2008), these models could be interpolated with a
monolithic model, or even word specific models, but
we have not explored that option here.
The feature sets for the classifiers in the base-
line systems includes all the features that were used
for the described in Section 3. For the classi-
fiers that do not use specialized models (CLASS and
CLASS+DS), we have also added extra features de-
scribing the linguistic form of the potential anaphor
(whether it is a pronoun, a proper name, and so
on). This is in accordance with standard feature sets
in the pairwise approach. It gives these models a
chance to learn weights more appropriately for the
different types within a single, monolithic model.
5 Experiments
We use the ACE corpus (Phase 2). The corpus has
three parts, each corresponding to a different genre:
newspaper texts (NPAPER), newswire texts (NWIRE),
and broadcast news (BNEWS). Each set is split into
a train part and a devtest part. In our experi-
ments, we consider only true ACE mentions.
5.1 Antecedent selection results
We first evaluate the specialized ranker models
individually on the task of anaphora resolution:
their ability to select a correct antecedent for each
anaphor. Following common practice in this task,
we report results in terms of accuracy, which is sim-
ply the ratio of correctly resolved anaphors. The
candidate set during testing was formed by taking
all the mentions that appear before the anaphor.
Also, we assume that correctly resolving an anaphor
amounts to selecting any of the previous mentions in
the entity as the antecedent. The accuracy scores for
the different models are presented in Table 5.
665
The best accuracy results on the entire ACE cor-
pus are found first for the proper name resolver with
a score of 83.5%, then for the third person pronoun
resolver with 82.2%, then for the definite descrip-
tion and speech pronoun resolvers with 66.9% and
66.5% respectively. The worst scores are obtained
for the ?others? category. The high scores for the
third person pronoun and the proper name rankers
most likely follow from the fact that the resolution
of these expressions relies on simple, reliable pre-
dictors, such as distance and morphosyntactic agree-
ment for pronouns, and string similarity features for
proper names. The resolution of definite descrip-
tions and other types of lexical NPs (which are han-
dled through the ?others? model) are much more
challenging: they rely on lexical semantic and world
knowledge, which is only partially encoded via our
WordNet-based features. Finally, note that the reso-
lution of speech pronouns is also much harder than
that of the other pronominal forms: these expres-
sions are much less (if at all) constrained by re-
cency and agreement. Furthermore, these expres-
sions show a lot of cataphoric uses, which are not
considered by our models. The low scores for the
?others? category is likely due to the fact that it en-
compasses very different referential expressions.
5.2 Coreference Results
For evaluating the coreference performance, we rely
on three primary metrics: (i) the link based MUC
metric (Vilain et al, 1995), the mention based B3
metric (Bagga and Baldwin, 1998), and the entity
based CEAF metric (Luo, 2005). Common to these
metrics is: (i) they operate by comparing the set of
chains S produced by the system against the true
chains T , and (ii) they report performance in terms
of recall and precision. There are however impor-
tant differences in how each metric computes these
scores, each producing a different bias.
MUC scores are based on the number of links
(pairs of mentions) common to S and T . Recall
is the number of common links divided by the to-
tal number of links in T ; precision is the number of
common links divided by the total number of links
in S. This focus gives MUC two main biases. First,
it favors systems that create large chains (and thus
fewer entities). For instance, a system that produces
a single chain achieves 100% recall without severe
degradation in precision. Second, it ignores single
mention entities, which are involved in no links.9
The B3 metric was designed to address the MUC
metric?s shortcomings. It is mention-based: it com-
putes both recall and precision scores for each men-
tion i. Let S be the system chain containing m, T
be the true chain containing m. The set of correct
elements in S is thus |S ? T |. The recall score for
a mention i is |S?T ||T | , while the precision score for i
is |S?T ||S| . Overall recall/precision is obtained by av-
eraging over the individual mention scores. The fact
that this metric is mention-based by definition solves
the problem of single mention entities. Also solved
is the bias favoring larger chains, since this will be
penalized in the precision score of each mention.
The Constrained Entity Aligned F-Measure
(CEAF) (Luo, 2005). aligns each system chain S
with at most one true chain T . It finds the best one-
to-one mapping between the set of chains S and T ,
which is equivalent to finding the optimal alignment
in a bipartite graph. The best mapping maximizes
the similarity over pairs of chains (Si, Ti), where
the similarity between two chains is the number of
common mentions to the two chains. With CEAF,
recall is computed as the total similarity divided by
the number of mentions in all the T (i.e., the self-
similarity), while precision is the total similarity di-
vided by the number of mentions in S.
Table 6 gives scores for all three metrics
for the different models on the entire ACE
corpus. Two main patterns emerge: sig-
nificant improvements are obtained by using
specialized models (CLASS vs CLASS+SP and
CLASS+DS vs CLASS+DS+SP) and by using a
ranker (CLASS+DS+SP vs RANK+DS+SP). Overall,
the RANK+DS+SP system significantly outperforms
the other systems on the three different metrics.10
The f -scores for RANK+DS+SP are 71.6% with
the MUC metric, 72.7% with the B3, and 67.0%
with the CEAF metric. These scores place the
RANK+DS+SP among the best coreference resolu-
tion systems, since most existing systems are typi-
cally under the bar of the 70% in f -score with the
9It is worth noting that the MUC corpus does not annotate
single mention entities.
10Statistical significance was determined with t-tests for both
recall and precision scores, with p < 0.05.
666
System MUC B3 CEAF
R P F R P F F
CLASS 60.8 72.6 66.2 62.4 77.7 69.2 62.3
CLASS+DS 64.9 72.3 68.4 65.6 74.1 69.6 63.4
CLASS+SP 64.8 74.5 69.3 65.3 79.1 71.5 65.0
CLASS+DS+SP 66.8 74.4 70.4 66.4 77.0 71.3 65.3
RANK+DS+SP 67.9 75.7 71.6 66.8 79.8 72.7 67.0
Table 6: Recall (R), Precision (P), and f -score (F) results on the entire ACE corpus using the MUC, B3, and CEAF
metrics. Note that R=P=F for CEAF when using true mentions, as we do here.
MUC and B3 metrics (Ng, 2005a). An interesting
point of comparison is provided by Ng (2007), who
also relies on true mentions and reports MUC f -
scores only slightly superior to ours (73.8%) while
relying on perfect semantic class information. His
best results otherwise are 64.6%. The fact that
our improvements are consistent across the different
evaluation metrics is remarkable, especially given
that these three metrics are quite different in the
way they compute their scores. The gains in f -
score range from 1.2 to 5.4% on the MUC metric
(i.e., error reductions of 4 to 15.9%), from 1.4 to
3.5% on the B3 metric (i.e., error reductions of 4.8
to 11.4%), and from 1.7 to 4.7% on the CEAF met-
ric (i.e., error reductions of 6.9 to 17%). The larger
improvements come from recall, with improvements
ranging from 1.9 to 7.1% with MUC, from 2.4 to
5.6% with B3.11 This suggests that RANK+DS+SP
predicts many more valid coreference links than the
other systems. Smaller but still significant gains are
made in precision: RANK+DS+SP is also able to re-
duce the proportion of invalid links.
The overall improvements found with
RANK+DS+SP suggest that it is able to capi-
talize on the better antecedent selection capabilities
offered by the ranking approach. This is supported
by the error analysis on the development data.
Errors made by a coreference system can be con-
ceptualized as falling into three main classes: (i)
?missed anaphors? (i.e., an anaphoric mention that
fails to be linked to a previous mention), (ii) ?spuri-
ous anaphors? (i.e., an non-anaphoric mention that
is linked to a previous mention), and (iii) ?invalid
resolutions? (i.e., a true anaphor that is linked to a
11Recall that recall and precision scores are identical with
CEAF, due to the fact that we are using true mention boundaries.
incorrect antecedent). The two first types of error
pertain to the determination of the discourse status
of the mention, while the third regards the selection
of an antecedent (i.e., anaphora resolution). Con-
sidering the systems? invalid resolutions, we found
that the RANK+DS+SP had a much lower error rate:
only 17.9% of all true anaphors were incorrectly
resolved by this system, against 23.1% for CLASS,
24.9% for CLASS+DS, 20.4% for CLASS+SP, and
22.1% for CLASS+DS+SP.
Denis (2007) provides multi-metric scores for the
JOINT-ILP model of Denis and Baldridge (2007a),
which uses integer linear programming for joint in-
ference over coreference resolution and discourse
status: f -scores of 73.3%, 68.0%, and 58.9% for
MUC, B3, and CEAF, respectively. Despite the fact
that this MUC score beats RANK+DS+SP?s, it is ac-
tually worse than even the basic model CLASS for
B3 and CEAF. This difference fact that MUC gives
more recall credit for large chains without a conse-
quent precision reduction, and shows the importance
of using B3 and CEAF scores in addition to MUC.
Denis (2007) also extends the JOINT-ILP system
by adding named entity resolution and constraints
on transitivity with respect to coreference links. The
best model reported there (JOINT-DS-NE-AE-ILP)
obtains f -scores of 70.1%, 72.7%, and 66.2% for
MUC, B3, and CEAF, respectively. Interestingly,
RANK+DS+SP actually performs better across all
metrics despite being a simpler model with fewer
sources of information.
5.3 Oracle results
Using specialized rankers with a discourse status
classifier yields coreference performance superior to
that given by various classification-based baseline
systems. Crucially, these improvements have been
667
System MUC B3 CEAF
R P F R P F F
RANK+DS+SP 67.9 75.7 71.6 66.8 79.8 72.7 67.0
RANK+DS-ORACLE+SP 79.1 79.1 79.1 75.4 76.0 75.7 76.9
LINK-ORACLE 78.8 100.0 88.1 74.3 100.0 85.2 79.7
Table 7: Recall (R), Precision (P), and f -score (F) results for RANK+DS-ORACLE+SP and LINK-ORACLE on the
entire ACE corpus.
possible using a discourse status model that has an
accuracy of just 80.8%. Clearly, the performance
of the discourse status module has a direct impact
on the performance of the entire coreference sys-
tem. On the one hand, misclassified anaphors are
simply not resolved by the rankers: this limits the
recall of the coreference system. On the other hand,
misclassified non-anaphors are linked to a previous
mention: this limits precision.
In order to further assess the impact of the er-
rors made by the discourse status classifier, we build
two different oracle systems. The first oracle sys-
tem, RANK+DS-ORACLE+SP, uses the specialized
rankers in combination with a perfect discourse sta-
tus classifier. That is, this system knows for each
mention whether it is anaphoric or not: the only er-
rors made by such a system are invalid resolutions.
RANK+DS-ORACLE+SP thus provides an upper-
bound for the RANK+DS+SP model. The results for
this oracle are given in Table 7: they show substan-
tial improvements over RANK+DS+SP, which sug-
gests that the RANK+DS+SP has also the potential
to be further improved if used in combination with a
more accurate discourse status classifier.
The second oracle system, LINK-ORACLE, uses
the discourse status classifier with a perfect corefer-
ence resolver. That is, this system has perfect knowl-
edge regarding the antecedents of anaphors: the er-
rors made by such a system are only errors in the
discourse status of mentions. The results for LINK-
ORACLE are also reported in Table 7. These figures
show that however accurate our models are at pick-
ing a correct antecedent for a true anaphor, the best
they can achieve in terms of f -scores is 88.1% with
MUC, 85.2% with B3, and 79.7% with CEAF.
6 Conclusion
We present and evaluate two straight-forward tac-
tics for improving coreference resolution: (i) rank-
ing models, and (ii) separate, specialized models
for different types of referring expressions. The
specialized rankers are used in combination with
a discourse status classifier which determines the
mentions that are sent to the rankers. This simple
pipeline architecture produces significant improve-
ments over various implementations of the standard,
classifier-based coreference system. In turn, these
strategies could be integrated with the joint infer-
ence models we have explored elsewhere (Denis and
Baldridge, 2007a; Denis, 2007) and which have ob-
tained performance improvements that are orthogo-
nal to those obtained here.
This paper?s improvements are consistent across
the three main coreference evaluation metrics: MUC,
B3, and CEAF.12 We attribute improvements to: (i)
the better antecedent selection capabilities offered
by the ranking approach, and (ii) the division of la-
bor between specialized models, allowing each one
to better model the corresponding distribution.
Acknowledgments
We would like to thank Nicholas Asher, Andy
Kehler, Ray Mooney, and the three anonymous re-
viewers for their comments. This work was sup-
ported by NSF grant IIS-0535154.
References
M. Ariel. 1988. Referring and accessibility. Journal of
Linguistics, pages 65?87.
N. Asher, P. Denis, and B. Reese. 2006. Names and pops
and discourse structure. In Workshop on Constraints
in Discourse, Maynooth, Ireland.
A. Bagga and B. Baldwin. 1998. Algorithms for scor-
ing coreference chains. In Proceedings of LREC 1998,
pages 563?566.
12We strongly advocate that coreference results should never
be presented in terms of MUC scores alone.
668
E. Charniak and M. Johnson. 2005. Coarse-to-fine n-
best parsing and maxent discriminative reranking. In
Proceedings of ACL 2005, Ann Arbor, Michigan.
M. Collins and N. Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete
structures and the voted perceptron. In Proceedings of
ACL 2002, pages 263?270, Philadelphia, PA.
H. Daume? III and D. Marcu. 2005. A large-scale ex-
ploration of effective global features for a joint entity
detection and tracking model. In Proceedings of HLT-
EMNLP 2005, Vancouver, Canada.
P. Denis and J. Baldridge. 2007a. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proceedings of HLT-NAACL 2007,
Rochester, NY.
P. Denis and J. Baldridge. 2007b. A ranking approach
to pronoun resolution. In Proceedings of IJCAI 2007,
Hyderabad, India.
Pascal Denis. 2007. New Learning Models for Robust
Reference Resolution. Ph.D. thesis, The University of
Texas at Austin.
R. Elwell and J. Baldridge. 2008. Discourse connec-
tive argument identification with connective specific
rankers. In Proceedings of the International Confer-
ence on Semantic Computing, Santa Clara, CA.
B. Grosz, A. Joshi, and S. Weinstein. 1995. Centering:
A framework for modelling the local coherence of dis-
course. Computational Linguistics, 2(21).
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, 69:274?307.
A. Haghighi and D. Klein. 2007. Unsupervised coref-
erence resolution in a nonparametric Bayesian model.
In Proceedings ACL 2007, pages 848?855, Prague,
Czech Republic.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proceedings of HLT-
NAACL 2004.
X. Luo. 2005. On coreference resolution performance
metrics. In Proceedings of HLT-NAACL 2005, pages
25?32.
R. Malouf. 2002. A comparison of algorithms for max-
imum entropy parameter estimation. In Proceedings
of the Sixth Workshop on Natural Language Learning,
pages 49?55, Taipei, Taiwan.
A. McCallum and B. Wellner. 2004. Conditional models
of identity uncertainty with application to noun coref-
erence. In Proceedings of NIPS 2004.
J. F. McCarthy and W. G. Lehnert. 1995. Using deci-
sion trees for coreference resolution. In IJCAI, pages
1050?1055.
T. Morton. 2000. Coreference for NLP applications. In
Proceedings of ACL 2000, Hong Kong.
V. Ng and C. Cardie. 2002a. Identifying anaphoric and
non-anaphoric noun phrases to improve coreference
resolution. In Proceedings of COLING 2002.
V. Ng and C. Cardie. 2002b. Improving machine learn-
ing approaches to coreference resolution. In Proceed-
ings of ACL 2002, pages 104?111.
V. Ng. 2004. Learning noun phrase anaphoricity to im-
prove coreference resolution: Issues in representation
and optimization. In Proceedings of ACL 2004.
V. Ng. 2005a. Machine learning for coreference reso-
lution: From local classification to global ranking. In
Proceedings of ACL 2005, pages 157?164, Ann Arbor,
MI.
V. Ng. 2005b. Supervised ranking for pronoun resolu-
tion: Some recent improvements. In Proceedings of
AAAI 2005.
V. Ng. 2007. Semantic class induction and coreference
resolution. In Proceedings of ACL 2007.
M. Osborne and J. Baldridge. 2004. Ensemble-based
active learning for parse selection. In Proceedings of
HLT-NAACL 2004, pages 89?96, Boston, MA.
E. F. Prince. 1981. Toward a taxonomy of given-new
information. In P. Cole, editor, Radical Pragmatics,
pages 223?255. Academic Press, New York.
D. Ravichandran, E. Hovy, and F. J. Och. 2003. Sta-
tistical QA - classifier vs re-ranker: What?s the differ-
ence? In Proceedings of the ACL Workshop on Mul-
tilingual Summarization and Question Answering?
Machine Learning and Beyond.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A machine
learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
K. Toutanova, P. Markova, and C. Manning. 2004. The
leaf projection path view of parse trees: Exploring
string kernels for HPSG parse selection. In Proceed-
ings of EMNLP 2004, pages 166?173, Barcelona.
O. Uryupina. 2004. Linguistically motivated sample se-
lection for coreference resolution. In Proceedings of
DAARC 2004, Furnas.
R. van der Sandt. 1992. Presupposition projection as
anaphora resolution. Journal of Semantics, 9:333?
377.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic corefer-
ence scoring scheme. In Proceedings fo the 6th Mes-
sage Understanding Conference (MUC-6), pages 45?
52, San Mateo, CA. Morgan Kaufmann.
X. Yang, G. Zhou, J. Su, and C.L. Tan. 2003. Corefer-
ence resolution using competitive learning approach.
In Proceedings of ACL 2003, pages 176?183.
669
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 250?258,
Beijing, August 2010
Comparison of different algebras for inducing
the temporal structure of texts
Pascal Denis?
? Alpage Project-Team
INRIA & Universit? Paris 7
pascal.denis@inria.fr
Philippe Muller?,
 IRIT
Universit? de Toulouse
muller@irit.fr
Abstract
This paper investigates the impact of us-
ing different temporal algebras for learn-
ing temporal relations between events.
Specifically, we compare three interval-
based algebras: Allen (1983) algebra,
Bruce (1972) algebra, and the algebra de-
rived from the TempEval-07 campaign.
These algebras encode different granular-
ities of relations and have different infer-
ential properties. They in turn behave dif-
ferently when used to enforce global con-
sistency constraints on the building of a
temporal representation. Through various
experiments on the TimeBank/AQUAINT
corpus, we show that although the TempE-
val relation set leads to the best classifica-
tion accuracy performance, it is too vague
to be used for enforcing consistency. By
contrast, the other two relation sets are
similarly harder to learn, but more use-
ful when global consistency is important.
Overall, the Bruce algebra is shown to
give the best compromise between learn-
ability and expressive power.
1 Introduction
Being able to recover the temporal relations (e.g.,
precedence, inclusion) that hold between events
and other time-denoting expressions in a docu-
ment is an essential part of natural language un-
derstanding. Success in this task has important
implications for other NLP applications, such as
text summarization, information extraction, and
question answering.
Interest for this problem within the NLP com-
munity is not new (Passonneau, 1988; Webber,
1988; Lascarides and Asher, 1993), but has been
recently revived by the creation of the TimeBank
corpus (Pustejovsky et al, 2003), and the orga-
nization of the TempEval-07 campaign (Verhagen
et al, 2007). These have seen the development
of machine learning inspired systems (Bramsen et
al., 2006; Mani et al, 2006; Tatu and Srikanth,
2008; Chambers and Jurafsky, 2008).
Learning the temporal stucture from texts is a
difficult problem because there are numerous in-
formation sources at play (in particular, seman-
tic and pragmatic ones) (Lascarides and Asher,
1993). An additional difficulty comes from the
fact that temporal relations have logical proper-
ties that restrict the consistent graphs that can be
built for a set of temporal entities (for instance
the transitivity of inclusion and temporal prece-
dence). Previous work do not attempt to directly
predict globally coherent temporal graphs, but in-
stead focus on the the simpler problem of label-
ing pre-selected pairs of events (i.e., a task that
directly lends itself to the use of standard classifi-
cation techniques). That is, they do not consider
the problem of linking pairs of events (i.e., of de-
termining which pairs of events are related).
Given the importance of temporal reasoning
for determining the temporal structure of texts,
a natural question is how to best use it within
a machine-based learning approach. Following
(Mani et al, 2006), prior approaches exploit tem-
poral inferences to enrich the set of training in-
stances used for learning. By contrast, (Bramsen
et al, 2006) use temporal relation compositions to
provide constraints in a global inference problem
(on the slightly different task of ordering passages
in medical history records). (Tatu and Srikanth,
2008) and (Chambers and Jurafsky, 2008) com-
bine both approaches and use temporal reasoning
both during training and decoding. Interestingly,
these approaches use different inventories of re-
lations: (Mani et al, 2006) use the TimeML 13
relation set, while (Chambers and Jurafsky, 2008;
250
Bramsen et al, 2006) use subset of these relations,
namely precedence and the absence of relation.
This paper adopts a more systematic perspec-
tive and directly assesses the impact of differ-
ent relation sets (and their underlying algebras)
in terms of learning and inferential properties.
Specifically, we compare three interval-based al-
gebras for building classification-based systems,
namely: Allen (1983)?s 13 relation algebra, Bruce
(1972)?s 7 relations algebra, and the algebra
underlying Tempeval-07 3 relations (henceforth,
TempEval algebra). We wish to determine the
best trade-off between: (i) how easy it is to learn
a given set of relations, (ii) how informative are
the representations produced by each relation set,
and (iii) how much information can be drawn from
the predicted relations using knowledge encoded
in the representation. These algebras indeed dif-
fer in the number of relations they encode, and in
turn in how expressive each of these relations is.
From a machine learning point of view of learn-
ing, it is arguably easier to learn a model that
has to decide among fewer relations (i.e., that has
fewer classes). But from a representational point
of view, it is better to predict relations that are as
specific as possible, for composing them may re-
strict the prediction to more accurate descriptions
of the situation. However, while specific relations
potentially trigger more inferences, they are also
more likely to predict inconsistent constraints. In
order to evaluate these differences, we design a set
of experiments on the Timebank/AQUAINT cor-
pus, wherein we learn precise relations and vaguer
ones, and evaluate them with respect to each other
(when a correspondence is possible).
Section 2 briefly presents the Time-
bank/AQUAINT corpus. In section 3, we
describe the task of temporal ordering through an
example, and discuss how it should be evaluated.
Section 4 then goes into more detail about the
different representation possibilities for temporal
relations, and some of their formal properties.
Section 5 presents our methods for building tem-
poral structures, that combines relation classifiers
with global constraints on whole documents.
Finally, we discuss our experimental results in
section 6.
2 The Timebank/AQUAINT corpus
Like (Mani et al, 2006) and (Chambers and Ju-
rafsky, 2008), we use the so-called OTC corpus,
a corpus of 259 documents obtained by com-
bining the Timebank corpus (Pustejovsky et al,
2003) (we use version 1.1 of the corpus) and the
AQUAINT corpus.1 The Timebank corpus con-
sists of 186 newswire articles (and around 65, 000
words), while AQUAINT has 73 documents (and
around 40, 000 words).
Both corpora are annotated using the TimeML
scheme for tagging eventualities (events and
states), dates/times, and their temporal relations.
Eventualities can be denoted by verbs, nouns, and
some specific constructions. The temporal rela-
tions (i.e., the so-called TLINKS) encode topolog-
ical information between the time intervals of oc-
curring eventualities. TimeML distinguishes three
types of TLINKS: event-event, event-time, and
time-time, giving rise to different subtasks. In this
paper, we will focus on predicting event-event re-
lations (see (Filatova and Hovy, 2001; Boguraev
and Ando, 2005) for work on the other tasks). The
set of temporal relations used in TLINKS mirrors
the 13 Allen relations (see next section), and in-
cludes the following six relations: before, begins,
ends, ibefore, includes, simultaneous and their in-
verses. The combined OTC corpus comprises a
total of 6, 139 annotated event-event TLINKS. We
also make use of the additional TLINKS indepen-
dently provided by (Bethard et al, 2007) for 129
of the 186 Timebank documents.
3 Task presentation and evaluation
3.1 An example
We illustrate the task of event ordering using a
small fabricated, simplified example:
Fortis bank investede1 in junk bonds
before the financial crisise2 , but
got ride3 of most of them during
the crisise2bis . However, the insti-
tution still went bankrupte4 a year
later.
1Both corpora are freely available from http://www.
timeml.org/site/timebank/timebank.html.
251
The annotation for this temporal structure would
include the following relations: e1 is temporally
before e2, e3 is temporally included in e2, and e3
is before e4. The coreference relation between e2
and e2bis implies the equality of their temporal ex-
tension. Of course all these events may in theory
be related temporally to almost any other event in
the text. Events are also anchored to temporal ex-
pressions explicitly, and this is usually considered
as a separate, much easier task. We will use this
example throughout the rest of our presentation.
3.2 Comparing temporal annotations
Due to possible inferences, there are often many
equivalent ways to express the same ordering of
events, so comparisons between annotation and
reference event-event pairs cannot rely on simple
precision/recall measures.
Consider the above example and assume the
following annotation: e1 is before e2, e3 is in-
cluded in e2, and e3 is before e3. Without going
into too much detail about the semantics of the re-
lations used, one expects annotators to agree with
the fact that it entails that e1 is before e3, among
other things. So the annotation is equivalent to a
larger set of relations. In some cases, the inferred
information is disjunctive (the relation holding be-
tween two events is a subset of possible ?simple?
relations, such as ?before or included?).
Nowadays, the given practice is to compute
some sort of transitive closure over the network of
constraints on temporal events (usually expressed
in the well-studied Allen algebra (Allen, 1983)),
and compute agreements over the saturated struc-
tures. Specifically, we can compare the sets of
simple temporal relations that are deduced from
it (henceforth, the ?strict? metric), or measure the
agreement between the whole graphs, including
disjunctions (Verhagen et al, 2007) (henceforth,
the ?relaxed? metric).2 Under this latter met-
ric, precision (resp. recall) of a prediction for a
pair of events consisting of a set S of relations
with respect to a set of relations R inferred from
the reference, is computed as |S ? R|/|S| (resp.
|S ?R|/|R|).
2Taking into account disjunctions means giving partial
credit to disjunctions approximating the reference relation
(possibly disjunctive itself), see next section.
e1
e2
e3
e4
b
di
b
e1
e2
e3
e4
b
b
di,fi,o,m
e1
e2
e3
e4
b
di
b
b
e1
e2
e3
e4
b
b
Figure 1: Two non-equivalent annotations of the
same situations (left) and their transitive closure
in Allen?s algebra (right, with new relations only).
b stands for Allen?s before relation, m for meet, o
for overlap, di and fi for the inverses of during and
finish, respectively.
Figure 1 illustrates the point of these ?satu-
rated? representations, showing two raw annota-
tions of our example on the left (top and bottom)
and their closures on the right. The raw annota-
tions share only 2 relations (between e1 and e2,
and e3 and e4), but their transitive closures agree
also on the relations between e1 and e3, e1 and
e4, and e3 and e4. They still differ on the rela-
tion between e2 and e4, but only because one is
much more specific than the other, something that
can only be taken into account by a partial credit
scoring function.
For this example, the ?strict? metric yields pre-
cision and recall scores of 5/5 and 5/6, when
comparing the top annotation against the bottom
one. By contrast, the ?relaxed? metric (introduced
in the TempEval-07) yields precision and recall
scores of (5+0.2)/6 and 6/6, respectively.
We now turn to the issue of the set of relations
chosen for the task of expressing temporal infor-
mation in texts.
4 Temporal representations
Because of the inferential properties of temporal
relations, we have seen that the same situation can
be expressed in different ways, and some rela-
tions can be deduced from others. The need for
252
a precise reasoning framework has been present
in previous attempts at the task (Setzer et al,
2006), and people have moved to a set of hand-
made rules over ad hoc relations to more widely
accepted temporal reasoning frameworks, such as
algebras of temporal relations, the most famous
being Allen?s interval algebra.
An algebra of relations can be defined on any
set of relations that are mutually exclusive (two
relations cannot hold at the same time between
two entities) and exhaustive (at least one relation
must hold between two given entities). The alge-
bra starts from a set of simple, atomic, relations
U = {r1, r2, ...}, and a general relation is a sub-
set of U , interpreted as a disjunction of the rela-
tions it contains. From there, we can define union
and intersection of relations as classical set union
and intersection of the base relations they consist
of. Moreover, one can define a composition of re-
lations as follows:
(r1 ? r2)(x, z)? ?y r1(x, y) ? r2(y, z)
In words, a relation between x and z can be
computed from what is known between (x and
y) and (y and z). By computing beforehand the
n?n compositions of base relations of U , we can
compute the composition of any two general rela-
tions (because r ? r? =? when r, r? are basic and
r 6= r?):
{r1, r2, ...rk} ? {s1, s2, ...sm} =
?
i,j
(ri ? sj)
Saturating the graph of temporal constraints
means applying these rules to all compatible pairs
of constraints in the graph and iterating until a
fixpoint is reached. In Allen?s algebra there are
13 relations, determined by the different relations
that can hold between two intervals endpoints (be-
fore, equals, after). These relations are: b (be-
fore), m (meet), o (overlap), s (start), f (finish), d
(during), their inverses (bi, mi, oi, si, fi, di) and =
(equal), see figure 2.3
It is important to see that a general approach
to temporal ordering of events cannot restrict it-
self to a subset of these and still use the power of
3TimeML uses somewhat different names, with obvious
mappings, except ibefore (?immediately before?) for m, and
iafter (?immediately after?) for mi.
X
Y
X
X
X
Y
Y
Yfinishes
before
meets
overlaps
X
X
Y
Y
equals
during
starts
X
Y
Figure 2: Allen?s thirteen relations between two
temporal intervals
inferences to complete a situation, because com-
position of information is stable only on restricted
subsets. And using all of them means generating
numerous disjunctions of relations.
Allen relations are convenient for reason-
ing purposes, but might too precise for rep-
resenting natural language expressions, and
that?s why recent evaluation campaigns such as
TempEval-07 have settled on vaguer representa-
tions. TempEval-07 uses three relations called be-
fore, overlaps and after, which we note bt, ot,
and bit.4 These all correspond to disjunctions
of Allen relations: {b,m}a, {o,d,s,=,f}a and its
inverse, and {bi,mi}a, respectively. These rep-
resentations can be converted to Allen relations,
over which the same inference procedures can be
applied, and then expressed back as (potentially
disjunctive) TempEval relations. They thus form
a sub-algebra of Allen?s algebra, if we add their
possible disjunctions.
In fact, starting from the base relations, only
{b,o}t, {bi,o}t, and vague (i.e., the disjunction of
all relations) can be inferred (besides the base re-
lations). This is a consequence of the stability of
so-called convex relations in Allen algebra. Note
that an even simpler schema is used in (Chambers
and Jurafsky, 2008), where only TempEval before
and after and the vague relation are used.
We propose to consider yet another set of rela-
tion, namely relations from (Bruce, 1972). These
provide an intermediate level of representation,
since they include 7 simple relations. These are
4When it is not obvious, we will use subscript symbols
to indicate the particular algebra that is used (e.g., bt is the
before relation in TempEval).
253
also expressible as disjunctions of Allen relations;
they are: before (bb), after (bib) (with the same
semantics as TempEval?s bt and bit), equals (=b,
same as =a), includes (i, same as Allen?s {s,d,f}a),
overlaps (ob, same as oa), included (ii) and is-
overlapped (oib), their inverse relations. The
equivalences between the three algebras is shown
table 1.
Allen Bruce Tempeval
before before beforemeet
overlaps overlaps
overlaps
starts
includedduring
finishes
overlapsi is-overlapped
startsi
includesduringi
finishesi
meeti after afterbeforei
equals equals equals
Table 1: Correspondances between temporal al-
gebras. A relation ranging over multiple cells
is equivalent to a disjunction of all the relations
within these cells.
Considering a vaguer set is arguably more ad-
equate for natural language expressions while at
the same time this specific set preserves at least
the notions of temporal order and inclusion (con-
trary to the TempEval scheme), which have strong
inferential properties: they are both transitive, and
their composition yields simple relations; over-
lap allows for much weaker inferences. Figure 3
shows part of our example from the introduction
expressed in the three cases: with Allen relations,
the most precise, with Bruce relations and Tem-
pEval relations, with dotted lines showing the ex-
tent of the vagueness of the temporal situations in
each case (with respect to the most precise Allen
description). We can see that TempEval relations
lose quickly all information that is not before or
after, while Bruce preserves inference combining
precedence and temporal inclusion.
Information can be converted from one algebra
to the other, since vaguer algebras are based on re-
lations equivalent to disjunctions in Allen algebra.
But conversion from a precise relation to a vaguer
one and back to a more precise algebra leads to
information loss. Hence on figure 3, the original
Allen relation: e3 da e2 is converted to: e3 ot e2
in TempEval, which converts back into the much
less informative: e3 {o,d, s,=, f,oi, si, fi,di}a e2.
We will use these translations during our system
evaluation to have a common comparison point
between representations.
5 Models
5.1 Algebra-based classifiers
In order to compare the impact of the different al-
gebras described in section 4, we build three event
pair classification models corresponding to each
relation set. The resulting Allen-based, Bruce-
based, and Tempeval-based models therefore con-
tain 13, 7, and 3 class labels, respectively.5 For
obvious sparsity issues, we did not include classes
corresponding to disjunctive relations, as there are
2|R| possible disjunctions for each relation set R.
For training our models, we experiment with 4
various configurations that correspond to ways of
expanding the set of training examples. Specifi-
cally, these configurations vary in: (i) whether or
not we added the additional ?Bethard relations? to
the initial OTC annotations (Bethard et al, 2007),
(ii) whether or not we applied saturation over the
set of annotated relations.
5.2 Features
Our feature set for the various models is similar
to that used by previous work, including binary
features that encode event string as well as the five
TimeML attributes and their possible values:
? aspect: none, prog, perfect, prog perfect
? class: report, aspectual, state, I-state I-
action, perception, occurrence
? modality: none, to, should, would, could
can, might
? polarity: positive, negative
? tense: none, present, past, future
5Our TempEval model actually has a fourth label for the
identity relation. The motivations behind the inclusion of this
extra label are: (i) this relation is linguistically motivated and
comparatively easy to learn (for a lot of instances of this rela-
tion are cases of anaphora, which are often signaled by iden-
tical strings) (ii) this relation triggers a lot of specific infer-
ences.
254
e1
Time
e3
e2
(a) Allen:
(e1bae2 ? e3dae2) ? e1bae3
e1
Time
e3
e2
(b) Bruce:
(e1bbe2 ? e3dbe2) ? e1bbe3
e1
Time
e3
e2
(c) Tempeval:
(e1bte2 ? e3ote2) ? e1{bt, ot}e3
Figure 3: Comparing loss of inferential power in algebras: hard lines show the actual temporal
model, exactly expressed in Allen relations (a); dotted lines show the vagueness induced by alterna-
tive schemes, and the inference that can or cannot still be made in each algebra, (b) and (c).
Additional binary features check agreement for
same attribute (e.g., the same tense). Finally, we
add features that represent the distance between
two events (in number of sentences, and in num-
ber of intervening events). 6
5.3 Training set generation
Our generic training procedure works as follows.
For each document, we scan events in their order
of appearance in the text. We create a training
instance inst(ei,ej) for each ordered pair of events
(ei, ej): if (ei, ej) (resp. (ej , ei)) corresponds to
an annotated relation r, then we label inst(ei,ej)
with the label r (resp. its inverse r?1).
5.4 Parameter estimation
All of these classifiers are maximum entropy mod-
els (Berger et al, 1996). Parameter estimation
was performed with the Limited Memory Variable
Metric algorithm (Malouf, 2002) implemented in
the Megam package.7
5.5 Decoding
We consider two different decoding procedures.
The first one simply mirrors the training proce-
dure just described, scanning pairs of events in the
order of the text, and sending each pair to the clas-
sifier. The pair is then labeled with the label out-
putted by the classifier (i.e., the label receiving the
6These were also encoded as binary features, and the var-
ious feature values were binned in order to avoid sparseness.
7Available from http://www.cs.utah.edu/
~hal/megam/.
highest probability). No attempt is made to guar-
antee the consistency of the final temporal graph.
Our second inference procedure works as fol-
lows. As in the previous method, we scan the
events in the order of the text, and create ordered
pairs of events that we then submit to the classifier.
But the difference is that we saturate the graph af-
ter each classification decision to make sure that
the graph created so far is coherent. In case where
the classifier predicts a relation whose addition re-
sults in an incoherent graph, we try the next high-
est probability relation, and so on, until we find
a coherent graph. This greedy procedure is simi-
lar to the Natural Reading Order (NRO) inference
procedure described by (Bramsen et al, 2006).
6 Experiments and results
We perform two main series of experiments for
comparing our different models. In the first series,
we measure the accuracy of the Allen-, Bruce-
, and Tempeval-based models on predicting the
correct relation for the event-event TLINKS an-
notated in the corpus. In the second series, we
saturate the event pair relations produced by the
classifiers (combined with NRO search to en-
force global coherence) and compare the pre-
dicted graphs against the saturated event-event
TLINKS.
6.1 Experiment settings
All our models are trained and tested with 5-fold
cross-validation on the OTC documents. For eval-
255
uation, we use simple accuracy for the first se-
ries of experiments, and two ?strict? and ?relaxed?
precision/recall measures described in section 3
for the other series. For each type of measures,
we report scores with respect to both Allen and
TemEval relation sets. All scores are reported
using macro-averaging. Out of the 259 tempo-
ral graphs present in OTC, we found that 54 of
them were actually inconsistent when saturated;
the corresponding documents were therefore left
out of the evaluation.8 Given the rather expensive
procedure involved in the NRO decoding (saturat-
ing an inconsistent graph ?erases? all relations),
we skipped 8 documents wich were much longer
than the rest, leaving us with 197 documents for
our final experiments.
6.2 Event-event classification
Table 2 summarizes the accuracy scores of the
different classifiers on the event-event TLINKS
of OTC. We only report the best configuration
for each model. For the TempEval-based model,
we found that the best training setting was when
Bethard annotations were added to the original
TimeML annotations, but with no saturation.9 For
Allen and Bruce models, neither Bethard?s re-
lations nor saturation helps improve classifica-
tion accuracy. In fact, saturation degrades per-
formance, which can be explained by the fact
that saturation reinforces the bias towards already
over-represented relations.10 The best accuracy
performances are obtained by the Allen-based and
TempEval-based classifiers, each one performing
better in its own algebra (with 47.0% and 54.0%).
This is not surprising, since these classifiers were
specifically trained to optimize their respective
metrics. The Bruce-based classifier is slightly bet-
ter than the Allen-based one in TempEval, but also
slightly worse than TempEval-based classifier in
Allen.
8Because there is no way to trace the relation(s) respon-
sible for an inconsistency without analysing the whole set of
annotations of a text, and considering that it usually happens
on very long texts, we did not attempt to manually correct
the annotations.
9This is actually consistent with similar findings made by
(Chambers and Jurafsky, 2008).
10For instance, for Allen relations, there are roughly 50%
of before-after relations before saturation but 73% of them
after saturation.
Allen Acc. TempEval Acc.
Allen 47.0 48.9
Bruce N/A 49.3
TempEval N/A 54.0
Table 2: Accuracy scores for Allen, Bruce, and
TempEval classifiers on event-event TLINKS, ex-
pressed in Allen or TempEval algebra. Scores for
Bruce and TempEval models into Allen are left
out, since they predict (through conversion) dis-
junctive relations for all relations but equality.
Our accuracy scores for Allen, and TempEval-
based classifiers are somewhat lower than the ones
reported for similar systems by (Mani et al, 2006)
and (Chambers and Jurafsky, 2008), respectively.
These differences are likely to come from the fact
that: (i) (Mani et al, 2006) perform a 6-way clas-
sification, and not a 13-way classification11, and
(ii) (Chambers and Jurafsky, 2008) use a relation
set that is even more restrictive than TempEval?s.
6.3 Saturated graphs
Table 3 summarizes the various precision/recall
scores of the graph obtained by saturating the clas-
sifiers predictions (potentially altered by NRO)
against the event-event saturated graph. These re-
sults contrast with the accuracy results presented
in table 2: while the TempEval-based model was
the best model in classification accuracy in Tem-
pEval, it is now outperformed by both the Allen-
and Bruce-based systems (this with or with us-
ing NRO). The best system in TempEval is actu-
ally Bruce-based system, with 52.9 and 62.8 for
the strict/relaxed metrics, respectively. The re-
sults suggest that this algebra might actually of-
fer the best trade-off between learnanility and ex-
pressive power. The use of NRO to restore global
coherence yields important gains (10 points) in
the relaxed metric for both Allen- and Bruce-
based systems (although they do not convert into
gains in the strict metric). Unsuprisingly, the
best model on the Allen set remains Allen-based
model (and this time the use of NRO results in
gains on the strict metric). Predictions without
11This is only possible because they order the event-event
pairs before submitting them to the classifier.
256
System Allen Tempeval
RELAX STRICT RELAX STRICT
R P F1 R P F1 R P F1 R P F1
Allen 57.5 46.7 51.5 49.6 56.2 52.7 62.0 50.3 55.5 50.4 57.1 53.6
Bruce 46.0 39.0 42.1 18.0 44.0 25.9 62.9 52.6 57.3 50.9 57.0 53.8
Tempeval 37.1 35.9 36.5 14.0 44.0 21.2 49.3 47.1 48.2 21.7 44.2 29.1
AllenNRO 44.8 60.1 51.3 57.2 62.9 59.9 63.8 67.0 65.3 45.2 60.6 51.8
BruceNRO 46.3 53.1 49.5 13.9 45.3 21.2 65.5 71.8 68.5 46.6 61.1 52.9
TempevalNRO 37.1 35.9 36.5 13.9 44.3 21.2 49.3 47.1 48.2 21.7 44.2 29.1
Table 3: Comparing Allen-, Bruce-, Tempeval-based classifiers saturated predictions on saturated event-
event graph. The NRO subscript indicates whether the system uses NRO or not. Evaluation are given
with respect to both Allen and Tempeval relation sets.
NRO yielded between 7.5 and 9% of inconsistent
saturated graphs that were ignored by the evalua-
tion, which means this impacted recall measures
only.
7 Related work
Early work on temporal ordering (Passonneau,
1988; Webber, 1988; Lascarides and Asher, 1993)
concentrated on studying the knowledge sources
at play (such as tense, aspect, lexical semantics,
rhetorical relations). The development of anno-
tated resources like the TimeBank corpus (Puste-
jovsky et al, 2003) has triggered the development
of machine learning systems (Mani et al, 2006;
Tatu and Srikanth, 2008; Chambers and Jurafsky,
2008).
More recent work uses automatic classifica-
tion methods, based on the TimeBank and Ac-
quaint corpus, either as is, with inferential enrich-
ment for training (Mani et al, 2006; Chambers
et al, 2007), or supplied with the corrections of
(Bethard et al, 2007), or are restricted to selected
contexts, such as intra-sentential event relations
(Li et al, 2004; Lapata and Lascarides, 2006). All
of these assume that event pairs are preselected,
so the task is only to determine what is the most
likely relation between them. The best scores
are obtained with the added assumption that the
event-event pair can be pre-ordered (thus reduc-
ing the number of possible labels by 2).
More recently, (Bramsen et al, 2006) and sub-
sequently (Chambers and Jurafsky, 2008) pro-
pose to use an Integer Linear Programming solver
to enforce the consistency of a network of con-
straints while maximizing the score of local clas-
sification decisions. But these are restricted to the
relations BEFORE and AFTER, which have very
strong inference properties that cannot be gener-
alised to other relations. The ILP strategy is not
likely to scale up very well for richer relation sets,
for the number of possible relations between two
events (and thus the number of variables to put in
the LP solver for each pair) is the order of 2|R|
(where R is the relation set), and each transitiv-
ity constraints generates an enormous amount of
constraints.
8 Conclusion
We have investigated the role played by ontolog-
ical choices in temporal representations by com-
paring three algebras with different granularities
of relations and inferential powers. Our experi-
ments on the Timebank/AQUAINT reveal that the
TempEval relation set provides the best overall
classification accuracy, but it provides much less
informative temporal structures, and it does not
provide enough inferences for being useful for en-
forcing consistency. By contrast, the other two
relation sets are significantly harder to learn, but
provide more richer inferences and are therefore
more useful when global consistency is important.
Bruce?s 7 relations-based model appears to per-
form best in the TempEval evaluation, suggesting
that this algebra provides the best trade-off be-
tween learnability and expressive power.
257
References
Allen, James. 1983. Maintaining Knowledge about
Temporal Intervals. Communications of the ACM,
pages 832?843.
Berger, A., S. Della Pietra, and V. Della Pietra. 1996.
A maximum entropy approach to natural language
processing. Computational Linguistics, 22(1):39?
71.
Bethard, Steven, James H. Martin, and Sara Klingen-
stein. 2007. Timelines from text: Identification of
syntactic temporal relations. In International Con-
ference on Semantic Computing, pages 11?18, Los
Alamitos, CA, USA. IEEE Computer Society.
Boguraev, Branimir and Rie Ando. 2005. TimeML-
compliant text analysis for temporal reasoning. In
Kaelbling, Leslie Pack and Fausto Giunchiglia, edi-
tors, Proceedings of IJCAI05, pages 997?1003.
Bramsen, Philip, Pawan Deshpande, Yoong Keok Lee,
and Regina Barzilay. 2006. Inducing temporal
graphs. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 189?198, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Bruce, B. 1972. A model for temporal references and
its application in a question answering program. Ar-
tificial Intelligence, 3(1-3):1?25.
Chambers, Nathanael and Daniel Jurafsky. 2008.
Jointly combining implicit constraints improves
temporal ordering. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, pages 698?706, Honolulu, Hawaii, Oc-
tober. Association for Computational Linguistics.
Chambers, Nathanael, Shan Wang, and Daniel Juraf-
sky. 2007. Classifying temporal relations between
events. In ACL. The Association for Computer Lin-
guistics.
Filatova, Elena and Eduard Hovy. 2001. Assigning
time-stamps to event-clauses. In Mani, I., J. Puste-
jovsky, and R Gaizauskas, editors, The Language of
Time: A Reader. Oxford University Press.
Lapata, Maria and Alex Lascarides. 2006. Learning
sentence-internal temporal relations. J. Artif. Intell.
Res. (JAIR), 27:85?117.
Lascarides, Alex and Nicholas Asher. 1993. Tem-
poral interpretation, discourse relations and com-
mon sense entailment. Linguistics and Philosophy,
16:437?493.
Li, Wenjie, Kam-Fai Wong, Guihong Cao, and Chunfa
Yuan. 2004. Applying machine learning to chi-
nese temporal relation resolution. In Proceedings
of the 42nd Meeting of the Association for Compu-
tational Linguistics (ACL?04), Main Volume, pages
582?588, Barcelona, Spain, July.
Malouf, Robert. 2002. A comparison of algorithms
for maximum entropy parameter estimation. In Pro-
ceedings of the Sixth Workshop on Natural Lan-
guage Learning, pages 49?55, Taipei, Taiwan.
Mani, Inderjeet, Marc Verhagen, Ben Wellner,
Chong Min Lee, and James Pustejovsky. 2006. Ma-
chine learning of temporal relations. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, pages
753?760, Sydney, Australia, July. Association for
Computational Linguistics.
Passonneau, Rebecca J. 1988. A computational model
of the semantics of tense and aspect. Computational
Linguistics, 14(2):44?60.
Pustejovsky, James, Patrick Hanks, Roser Saur?,
Andrew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003. The TIMEBANK
Corpus. In Proceedings of Corpus Linguistics,
pages 647?656, Lancaster University, UK, March.
Setzer, Andrea, Robert Gaizauskas, and Mark Hepple.
2006. The Role of Inference in the Temporal An-
notation and Analysis of Text. Language Resources
and Evaluation, 39:243?265.
Tatu, Marta and Munirathnam Srikanth. 2008. Ex-
periments with reasoning for temporal relations be-
tween events. In Proceedings of the 22nd Inter-
national Conference on Computational Linguistics
(Coling 2008), pages 857?864, Manchester, UK,
August. Coling 2008 Organizing Committee.
Verhagen, Marc, Robert Gaizauskas, Franck Schilder,
Mark Hepple, Graham Katz, and James Puste-
jovsky. 2007. SemEval-2007 - 15: TempEval Tem-
poral Relation Identification. In Proceedings of Se-
mEval workshop at ACL 2007, Prague, Czech Re-
public, June. Association for Computational Lin-
guistics, Morristown, NJ, USA.
Webber, Bonnie Lynn. 1988. Tense as discourse
anaphor. Computational Linguistics, 14(2):61?73.
258
Coling 2010: Poster Volume, pages 108?116,
Beijing, August 2010
Benchmarking of Statistical Dependency Parsers for French
Marie Candito!, Joakim Nivre!, Pascal Denis! and Enrique Henestroza Anguiano!
! Alpage (Universit? Paris 7/INRIA)
! Uppsala University, Department of Linguistics and Philology
marie.candito@linguist.jussieu.fr {pascal.denis, henestro}@inria.fr joakim.nivre@ling?l.uu.se
Abstract
We compare the performance of three
statistical parsing architectures on the
problem of deriving typed dependency
structures for French. The architectures
are based on PCFGs with latent vari-
ables, graph-based dependency parsing
and transition-based dependency parsing,
respectively. We also study the in?u-
ence of three types of lexical informa-
tion: lemmas, morphological features,
and word clusters. The results show that
all three systems achieve competitive per-
formance, with a best labeled attachment
score over 88%. All three parsers bene?t
from the use of automatically derived lem-
mas, while morphological features seem
to be less important. Word clusters have a
positive effect primarily on the latent vari-
able parser.
1 Introduction
In this paper, we compare three statistical parsers
that produce typed dependencies for French. A
syntactic analysis in terms of typed grammatical
relations, whether encoded as functional annota-
tions in syntagmatic trees or in labeled depen-
dency trees, appears to be useful for many NLP
tasks including question answering, information
extraction, and lexical acquisition tasks like collo-
cation extraction.
This usefulness holds particularly for French,
a language for which bare syntagmatic trees
are often syntactically underspeci?ed because
of a rather free order of post-verbal comple-
ments/adjuncts and the possibility of subject in-
version. Thus, the annotation scheme of the
French Treebank (Abeill? and Barrier, 2004)
makes use of ?at syntagmatic trees without VP
nodes, with no structural distinction between
complements, adjuncts or post-verbal subjects,
but with additional functional annotations on de-
pendents of verbs.
Parsing is commonly enhanced by using more
abstract lexical information, in the form of mor-
phological features (Tsarfaty, 2006), lemmas
(Seddah et al, 2010), or various forms of clusters
(see (Candito and Seddah, 2010) for references).
In this paper, we explore the integration of mor-
phological features, lemmas, and linear context
clusters.
Typed dependencies can be derived using many
different parsing architectures. As far as statistical
approaches are concerned, the dominant paradigm
for English has been to use constituency-based
parsers, the output of which can be converted
to typed dependencies using well-proven conver-
sion procedures, as in the Stanford parser (Klein
and Manning, 2003). In recent years, it has
also become popular to use statistical dependency
parsers, which are trained directly on labeled de-
pendency trees and output such trees directly, such
as MSTParser (McDonald, 2006) and MaltParser
(Nivre et al, 2006). Dependency parsing has been
applied to a fairly broad range of languages, espe-
cially in the CoNLL shared tasks in 2006 and 2007
(Buchholz and Marsi, 2006; Nivre et al, 2007).
We present a comparison of three statistical
parsing architectures that output typed dependen-
cies for French: one constituency-based architec-
ture featuring the Berkeley parser (Petrov et al,
2006), and two dependency-based systems using
radically different parsing methods, MSTParser
(McDonald et al, 2006) and MaltParser (Nivre et
al., 2006). These three systems are compared both
in terms of parsing accuracy and parsing times, in
realistic settings that only use predicted informa-
tion. By using freely available software packages
that implement language-independent approaches
108
and applying them to a language different from
English, we also hope to shed some light on the
capacity of different methods to cope with the
challenges posed by different languages.
Comparative evaluation of constituency-based
and dependency-based parsers with respect to la-
beled accuracy is rare, despite the fact that parser
evaluation on typed dependencies has been ad-
vocated for a long time (Lin, 1995; Carroll et
al., 1998). Early work on statistical dependency
parsing often compared constituency-based and
dependency-based methods with respect to their
unlabeled accuracy (Yamada and Matsumoto,
2003), but comparison of different approaches
with respect to labeled accuracy is more recent.
Cer et al (2010) present a thorough analysis of
the best trade-off between speed and accuracy in
deriving Stanford typed dependencies for English
(de Marneffe et al, 2006), comparing a number of
constituency-based and dependency-based parsers
on data from the Wall Street Journal. They con-
clude that the highest accuracy is obtained using
constituency-based parsers, although some of the
dependency-based parsers are more ef?cient.
For German, the 2008 ACL workshop on pars-
ing German (K?bler, 2008) featured a shared task
with two different tracks, one for constituency-
based parsing and one for dependency-based pars-
ing. Both tracks had their own evaluation metrics,
but the accuracy with which parsers identi?ed
subjects, direct objects and indirect objects was
compared across the two tracks, and the results
in this case showed an advantage for dependency-
based parsing.
In this paper, we contribute results for a
third language, French, by benchmarking both
constituency-based and dependency-based meth-
ods for deriving typed dependencies. In addi-
tion, we investigate the usefulness of morphologi-
cal features, lemmas and word clusters for each of
the different parsing architectures. The rest of the
paper is structured as follows. Section 2 describes
the French Treebank, and Section 3 describes the
three parsing systems. Section 4 presents the ex-
perimental evaluation, and Section 5 contains a
comparative error analysis of the three systems.
Section 6 concludes with suggestions for future
research.
2 Treebanks
For training and testing the statistical parsers, we
use treebanks that are automatically converted
from the French Treebank (Abeill? and Barrier,
2004) (hereafter FTB), a constituency-based tree-
bank made up of 12, 531 sentences from the Le
Monde newspaper. Each sentence is annotated
with a constituent structure and words bear the
following features: gender, number, mood, tense,
person, de?niteness, wh-feature, and clitic case.
Nodes representing dependents of a verb are la-
beled with one of 8 grammatical functions.1
We use two treebanks automatically obtained
from FTB, both described in Candito et al
(2010). FTB-UC is a modi?ed version of the
original constituency-based treebank, where the
rich morphological annotation has been mapped
to a simple tagset of 28 part-of-speech tags, and
where compounds with regular syntax are bro-
ken down into phrases containing several simple
words while remaining sequences annotated as
compounds in FTB are merged into a single token.
Function labels are appended to syntactic category
symbols and are either used or ignored, depending
on the task.
FTB-UC-DEP is a dependency treebank de-
rived from FTB-UC using the classic technique of
head propagation rules, ?rst proposed for English
by Magerman (1995). Function labels that are
present in the original treebank serve to label the
corresponding dependencies. The remaining un-
labeled dependencies are labeled using heuristics
(for dependents of non-verbal heads). With this
conversion technique, output dependency trees are
necessarily projective, and extracted dependen-
cies are necessarily local to a phrase, which means
that the automatically converted trees can be re-
garded as pseudo-projective approximations to the
correct dependency trees (Kahane et al, 1998).
Candito et al (2010) evaluated the converted trees
for 120 sentences, and report a 98% labeled at-
tachment score when comparing the automatically
converted dependency trees to the manually cor-
rected ones.
1These are SUJ (subject), OBJ (object), A-OBJ/DE-OBJ
(indirect object with preposition ? / de), P-OBJ (indirect
object with another preposition / locatives), MOD (modi?er),
ATS/ATO (subject/object predicative complement).
109
SNP-SUJ
DET
une
NC
lettre
VN
V
avait
VPP
?t?
VPP
envoy?e
NP-MOD
DET
la
NC
semaine
ADJ
derni?re
PP-A_OBJ
P+D
aux
NP
NC
salari?s
une lettre avait ?t? envoy?e la semaine derni?re aux salari?s
de
t
suj
au
x-t
ps
au
x-
pa
ss mod
de
t mod
a_obj
obj
Figure 1: An example of constituency tree of the FTB-UC (left), and the corresponding dependency tree
(right) for A letter had been sent the week before to the employees.
Figure 1 shows two parallel trees from FTB-UC
and FTB-UC-DEP. In all reported experiments in
this paper, we use the usual split of FTB-UC: ?rst
10% as test set, next 10% as dev set, and the re-
maining sentences as training set.
3 Parsers
Although all three parsers compared are statis-
tical, they are based on fairly different parsing
methodologies. The Berkeley parser (Petrov et
al., 2006) is a latent-variable PCFG parser, MST-
Parser (McDonald et al, 2006) is a graph-based
dependency parser, and MaltParser (Nivre et al,
2006) is a transition-based dependency parser.
The choice to include two different dependency
parsers but only one constituency-based parser is
motivated by the study of Seddah et al (2009),
where a number of constituency-based statisti-
cal parsers were evaluated on French, including
Dan Bikel?s implementation of the Collins parser
(Bikel, 2002) and the Charniak parser (Charniak,
2000). The evaluation showed that the Berke-
ley parser had signi?cantly better performance for
French than the other parsers, whether measured
using a parseval-style labeled bracketing F-score
or a CoNLL-style unlabeled attachment score.
Contrary to most of the other parsers in that study,
the Berkeley parser has the advantage of a strict
separation of parsing model and linguistic con-
straints: linguistic information is encoded in the
treebank only, except for a language-dependent
suf?x list used for handling unknown words.
In this study, we compare the Berkeley parser
to MSTParser and MaltParser, which have the
same separation of parsing model and linguistic
representation, but which are trained directly on
labeled dependency trees. The two dependency
parsers use radically different parsing approaches
but have achieved very similar performance for a
wide range of languages (McDonald and Nivre,
2007). We describe below the three architectures
in more detail.2
3.1 The Berkeley Parser
The Berkeley parser is a freely available imple-
mentation of the statistical training and parsing
algorithms described in (Petrov et al, 2006) and
(Petrov and Klein, 2007). It exploits the fact that
PCFG learning can be improved by splitting sym-
bols according to structural and/or lexical proper-
ties (Klein and Manning, 2003). Following Mat-
suzaki et al (2005), the Berkeley learning algo-
rithm uses EM to estimate probabilities on sym-
bols that are automatically augmented with la-
tent annotations, a process that can be viewed
as symbol splitting. Petrov et al (2006) pro-
posed to score the splits in order to retain only the
most bene?cial ones, and keep the grammar size
manageable: the splits that induce the smallest
losses in the likelihood of the treebank are merged
back. The algorithm starts with a very general
treebank-induced binarized PCFG, with order h
horizontal markovisation. created, where at each
level a symbol appears without track of its orig-
inal siblings. Then the Berkeley algorithm per-
forms split/merge/smooth cycles that iteratively
re?ne the binarized grammar: it adds two latent
annotations on each symbol, learns probabilities
for the re?ned grammar, merges back 50% of the
splits, and smoothes the ?nal probabilities to pre-
vent over?tting. All our experiments are run us-
ing BerkeleyParser 1.0,3 modi?ed for handling
2For replicability, models, preprocessing tools and ex-
perimental settings are available at http://alpage.
inria.fr/statgram/frdep.html.
3http://www.eecs.berkeley.edu/
\~petrov/berkeleyParser
110
French unknown words by Crabb? and Candito
(2008), with otherwise default settings (order 0
horizontal markovisation, order 1 vertical marko-
visation, 5 split/merge cycles).
The Berkeley parser could in principle be
trained on functionally annotated phrase-structure
trees (as shown in the left half of ?gure 1), but
Crabb? and Candito (2008) have shown that this
leads to very low performance, because the split-
ting of symbols according to grammatical func-
tions renders the data too sparse. Therefore, the
Berkeley parser was trained on FTB-UC without
functional annotation. Labeled dependency trees
were then derived from the phrase-structure trees
output by the parser in two steps: (1) function la-
bels are assigned to phrase structure nodes that
have functional annotation in the FTB scheme;
and (2) dependency trees are produced using the
same procedure used to produce the pseudo-gold
dependency treebank from the FTB (cf. Section 2).
The functional labeling relies on the Maximum
Entropy labeler described in Candito et al (2010),
which encodes the problem of functional label-
ing as a multiclass classi?cation problem. Specif-
ically, each class is of the eight grammatical func-
tions used in FTB, and each head-dependent pair
is treated as an independent event. The feature
set used in the labeler attempt to capture bilexi-
cal dependencies between the head and the depen-
dent (using stemmed word forms, parts of speech,
etc.) as well as more global sentence properties
like mood, voice and inversion.
3.2 MSTParser
MSTParser is a freely available implementation
of the parsing models described in McDonald
(2006). These models are often described as
graph-based because they reduce the problem
of parsing a sentence to the problem of ?nding
a directed maximum spanning tree in a dense
graph representation of the sentence. Graph-based
parsers typically use global training algorithms,
where the goal is to learn to score correct trees
higher than incorrect trees. At parsing time a
global search is run to ?nd the highest scoring
dependency tree. However, unrestricted global
inference for graph-based dependency parsing
is NP-hard, and graph-based parsers like MST-
Parser therefore limit the scope of their features
to a small number of adjacent arcs (usually two)
and/or resort to approximate inference (McDon-
ald and Pereira, 2006). For our experiments, we
use MSTParser 0.4.3b4 with 1-best projective de-
coding, using the algorithm of Eisner (1996), and
second order features. The labeling of dependen-
cies is performed as a separate sequence classi?-
cation step, following McDonald et al (2006).
To provide part-of-speech tags to MSTParser,
we use the MElt tagger (Denis and Sagot, 2009),
a Maximum Entropy Markov Model tagger en-
riched with information from a large-scale dictio-
nary.5 The tagger was trained on the training set
to provide POS tags for the dev and test sets, and
we used 10-way jackkni?ng to generate tags for
the training set.
3.3 MaltParser
MaltParser6 is a freely available implementation
of the parsing models described in (Nivre, 2006)
and (Nivre, 2008). These models are often char-
acterized as transition-based, because they reduce
the problem of parsing a sentence to the prob-
lem of ?nding an optimal path through an abstract
transition system, or state machine. This is some-
times equated with shift-reduce parsing, but in
fact includes a much broader range of transition
systems (Nivre, 2008). Transition-based parsers
learn models that predict the next state given the
current state of the system, including features over
the history of parsing decisions and the input sen-
tence. At parsing time, the parser starts in an ini-
tial state and greedily moves to subsequent states
? based on the predictions of the model ? until a
terminal state is reached. The greedy, determinis-
tic parsing strategy results in highly ef?cient pars-
ing, with run-times often linear in sentence length,
and also facilitates the use of arbitrary non-local
features, since the partially built dependency tree
is ?xed in any given state. However, greedy in-
ference can also lead to error propagation if early
predictions place the parser in incorrect states. For
the experiments in this paper, we use MaltParser
4http://mstparser.sourceforge.net
5Denis and Sagot (2009) report a tagging accuracy of
97.7% (90.1% on unknown words) on the FTB-UC test set.
6http://www.maltparser.org
111
1.3.1 with the arc-eager algorithm (Nivre, 2008)
and use linear classi?ers from the LIBLINEAR
package (Fan et al, 2008) to predict the next state
transitions. As for MST, we used the MElt tagger
to provide input part-of-speech tags to the parser.
4 Experiments
This section presents the parsing experiments that
were carried out in order to assess the state of the
art in labeled dependency parsing for French and
at the same time investigate the impact of different
types of lexical information on parsing accuracy.
We present the features given to the parsers, dis-
cuss how they were extracted/computed and inte-
grated within each parsing architecture, and then
summarize the performance scores for the differ-
ent parsers and feature con?gurations.
4.1 Experimental Space
Our experiments focus on three types of lexical
features that are used either in addition to or as
substitutes for word forms: morphological fea-
tures, lemmas, and word clusters. In the case
of MaltParser and MSTParser, these features are
used in conjunction with POS tags. Motivations
for these features are rooted in the fact that French
has a rather rich in?ectional morphology.
The intuition behind using morphological fea-
tures like tense, mood, gender, number, and per-
son is that some of these are likely to provide ad-
ditional cues for syntactic attachment or function
type. This is especially true given that the 29 tags
used by the MElt tagger are rather coarse-grained.
The use of lemmas and word clusters, on the
other hand, is motivated by data sparseness con-
siderations: these provide various degrees of gen-
eralization over word forms. As suggested by Koo
et al (2008), the use of word clusters may also re-
duce the need for annotated data.
All our features are automatically produced:
no features except word forms originate from the
treebank. Our aim was to assess the performance
currently available for French in a realistic setting.
Lemmas Lemmatized forms are extracted us-
ing Lefff (Sagot, 2010), a large-coverage morpho-
syntactic lexicon for French, and a set of heuristics
for unknown words. More speci?cally, Lefff is
queried for each (word, pos), where pos is the
tag predicted by the MElt tagger. If the pair is
found, we use the longest lemma associated with
it in Lefff. Otherwise, we rely on a set of simple
stemming heuristics using the form and the pre-
dicted tag to produce the lemma. We use the form
itself for all other remaining cases.7
Morphological Features Morphological fea-
tures were extracted in a way similar to lemmas,
again by querying Lefff and relying on heuristics
for out-of-dictionary words. Here are the main
morphological attributes that were extracted from
the lexicon: mood and tense for verbs; person
for verbs and pronouns; number and gender for
nouns, past participles, adjectives and pronouns;
whether an adverb is negative; whether an adjec-
tive, pronoun or determiner is cardinal, ordinal,
de?nite, possessive or relative. Our goal was to
predict all attributes found in FTB that are recov-
erable from the word form alone.
Word Form Clusters Koo et al (2008) have
proposed to use unsupervised word clusters as
features in MSTParser, for parsing English and
Czech. Candito and Crabb? (2009) showed that,
for parsing French with the Berkeley parser, us-
ing the same kind of clusters as substitutes for
word forms improves performance. We now ex-
tend their work by comparing the impact of such
clusters on two additional parsers.
We use the word clusters computed by Can-
dito and Crabb? (2009) using Percy Liang?s im-
plementation8 of the Brown unsupervised cluster-
ing algorithm (Brown et al, 1992). It is a bottom-
up hierarchical clustering algorithm that uses a bi-
gram language model over clusters. The result-
ing cluster ids are bit-strings, and various lev-
els of granularity can be obtained by retaining
only the ?rst x bits. Candito and Crabb? (2009)
used the L?Est R?publicain corpus, a 125 mil-
lion word journalistic corpus.9 To reduce lexi-
7Candito and Seddah (2010) report the following cover-
age for the Lefff : around 96% of the tokens, and 80.1% of
the token types are present in the Lefff (leaving out punctua-
tion and numeric tokens, and ignoring case differences).
8http://www.eecs.berkeley.edu/~pliang/
software
9http://www.cnrtl.fr/corpus/
estrepublicain
112
cal data sparseness caused by in?ection, they ran
a lexicon-based stemming process on the corpus
that removes in?ection marks without adding or
removing lexical ambiguity. The Brown algo-
rithm was then used to compute 1000 clusters of
stemmed forms, limited to forms that appeared at
least 20 times.
We tested the use of clusters with different val-
ues for two parameters: nbbits = the cluster pre-
?x length in bits, to test varying granularities, and
minocc = the minimum number of occurrences in
the L?Est R?publicain corpus for a form to be re-
placed by a cluster or for a cluster feature to be
used for that form.
4.2 Parser-Specific Configurations
Since the three parsers are based on different ma-
chine learning algorithms and parsing algorithms
(with different memory requirements and parsing
times), we cannot integrate the different features
described above in exactly the same way. For the
Berkeley parser we use the setup of Candito and
Seddah (2010), where additional information is
encoded within symbols that are used as substi-
tutes for word forms. For MaltParser and MST-
Parser, which are based on discriminative models
that permit the inclusion of interdependent fea-
tures, additional information may be used either
in addition to or as substitutes for word forms.
Below we summarize the con?gurations that have
been explored for each parser:
? Berkeley:
1. Morphological features: N/A.
2. Lemmas: Concatenated with POS tags
and substituted for word forms.
3. Clusters: Concatenated with morpho-
logical suf?xes and substituted for word
forms; grid search for optimal values of
nbbits and minocc.
? MaltParser and MSTParser:
1. Morphological features: Added as
features.
2. Lemmas: Substituted for word forms
or added as features.
3. Clusters: Substituted for word forms or
added as features; grid search for opti-
mal values of nbbits and minocc.
4.3 Results
Table 1 summarizes the experimental results. For
each parser we give results on the development
set for the baseline (no additional features), the
best con?guration for each individual feature type,
and the best con?guration for any allowed combi-
nation of the three features types. For the ?nal
test set, we only evaluate the baseline and the best
combination of features. Scores on the test set
were compared using a ?2-test to assess statisti-
cal signi?cance: unless speci?ed, all differences
therein were signi?cant at p ? 0.01.
The MSTParser system achieves the best la-
beled accuracy on both the development set and
the test set. When adding lemmas, the best con-
?guration is to use them as substitutes for word
forms, which slightly improves the UAS results.
For the clusters, their use as substitutes for word
forms tends to degrade results, whereas using
them as features alone has almost no impact. This
means that we could not replicate the positive ef-
fect10 reported by Koo et al (2008) for English
and Czech. However, the best combined con-
?guration is obtained using lemmas instead of
words, a reduced set of morphological features,11
and clusters as features, with minocc=50, 000 and
nbbits=10.
MaltParser has the second best labeled accu-
racy on both the development set and the test set,
although the difference with Berkeley is not sig-
ni?cant on the latter. MaltParser has the lowest
unlabeled accuracy of all three parsers on both
datasets. As opposed to MSTParser, all three fea-
ture types work best for MaltParser when used in
addition to word forms, although the improvement
is statistically signi?cant only for lemmas and
clusters. Again, the best model uses all three types
of features, with cluster features minocc=600 and
nbbits=7. MaltParser shows the smallest discrep-
ancy from unlabeled to labeled scores. This might
be because it is the only architecture where label-
ing is directly done as part of parsing.
10Note that the two experiments cannot be directly com-
pared. Koo et al (2008) use their own implementation of an
MST parser, which includes extra second-order features (e.g.
grand-parent features on top of sibling features).
11As MSTParser training is memory-intensive, we re-
moved the features containing information already encoded
part-of-speech tags.
113
Development Set Test Set
Baseline Morpho Lemma Cluster Best Baseline Best
Parser LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS
Berkeley 85.1 89.3 ? ? 85.9 90.0 86.5 90.8 86.5 90.8 85.6 89.6 86.8 91.0
MSTParser 87.2 90.0 87.2 90.2 87.2 90.1 87.2 90.1 87.5 90.3 87.6 90.3 88.2 90.9
MaltParser 86.2 89.0 86.3 89.0 86.6 89.2 86.5 89.2 86.9 89.4 86.7 89.3 87.3 89.7
Table 1: Experimental results for the three parsing systems. LAS=labeled accuracy, UAS=unlabeled accuracy, for sentences
of any length, ignoring punctuation tokens. Morpho/Lemma/Cluster=best con?guration when using morphological features
only (resp. lemmas only, clusters only), Best=best con?guration using any combination of these.
For Berkeley, the lemmas improve the results
over the baseline, and its performance reaches that
of MSTParser for unlabeled accuracy (although
the difference between the two parsers is not sig-
ni?cant on the test set). The best setting is ob-
tained with clusters instead of word forms, using
the full bit strings. It also gives the best unlabeled
accuracy of all three systems on both the devel-
opment set and the test set. For the more impor-
tant labeled accuracy, the point-wise labeler used
is not effective enough.
Overall, MSTParser has the highest labeled ac-
curacy and Berkeley the highest unlabeled ac-
curacy. However, results for all three systems
on the test set are roughly within one percent-
age point for both labeled and unlabeled ac-
curacy, which means that we do not ?nd the
same discrepancy between constituency-based
and dependency-based parser that was reported
for English by Cer et al (2010).
Table 2 gives parsing times for the best con?g-
uration of each parsing architecture. MaltParser
runs approximately 9 times faster than the Berke-
ley system, and 10 times faster than MSTParser.
The difference in ef?ciency is mainly due to the
fact that MaltParser uses a linear-time parsing al-
gorithm, while the other two parsers have cubic
time complexity. Given the rather small differ-
ence in labeled accuracy, MaltParser seems to be
a good choice for processing very large corpora.
5 Error Analysis
We provide a brief analysis of the errors made by
the best performing models for Berkeley, MST-
Parser and MaltParser on the development set, fo-
cusing on labeled and unlabeled attachment for
nouns, prepositions and verbs. For nouns, Berke-
Bky Malt MST
Tagging _ 0:27 0:27
Parsing 12:19 0:58 (0:18) 14:12 (12:44)
Func. Lab. 0:23 _ _
Dep. Conv. 0:4 _ _
Total 12:46 1:25 14:39
Table 2: Parsing times (min:sec) for the dev set, for the
three architectures, on an imac 2.66GHz. The ?gures within
brackets show the pure parsing time without the model load-
ing time, when available.
ley has the best unlabeled attachment, followed by
MSTParser and then MaltParser, while for labeled
attachment Berkeley and MSTParser are on a par
with MaltParser a bit behind. For prepositions,
MSTParser is by far the best for both labeled and
unlabeled attachment, with Berkeley and Malt-
Parser performing equally well on unlabeled at-
tachment and MaltParser performing better than
Berkeley on labeled attachment.12 For verbs,
Berkeley has the best performance on both labeled
and unlabeled attachment, with MSTParser and
MaltParser performing about equally well. Al-
though Berkeley has the best unlabeled attach-
ment overall, it also has the worst labeled attach-
ment, and we found that this is largely due to the
functional role labeler having trouble assigning
the correct label when the dependent is a prepo-
sition or a clitic.
For errors in attachment as a function of word
distance, we ?nd that precision and recall on de-
pendencies of length > 2 tend to degrade faster
for MaltParser than for MSTParser and Berkeley,
12In the dev set, for MSTParser, 29% of the tokens that
do not receive the correct governor are prepositions (883 out
of 3051 errors), while these represent 34% for Berkeley (992
out of 2914), and 30% for MaltParser (1016 out of 3340).
114
with Berkeley being the most robust for depen-
dencies of length > 6. In addition, Berkeley is
best at ?nding the correct root of sentences, while
MaltParser often predicts more than one root for a
given sentence. The behavior of MSTParser and
MaltParser in this respect is consistent with the re-
sults of McDonald and Nivre (2007).
6 Conclusion
We have evaluated three statistical parsing ar-
chitectures for deriving typed dependencies for
French. The best result obtained is a labeled at-
tachment score of 88.2%, which is roughly on a
par with the best performance reported by Cer et
al. (2010) for parsing English to Stanford depen-
dencies. Note two important differences between
their results and ours: First, the Stanford depen-
dencies are in a way deeper than the surface de-
pendencies tested in our work. Secondly, we ?nd
that for French there is no consistent trend fa-
voring either constituency-based or dependency-
based methods, since they achieve comparable re-
sults both for labeled and unlabeled dependencies.
Indeed, the differences between parsing archi-
tectures are generally small. The best perfor-
mance is achieved using MSTParser, enhanced
with predicted part-of-speech tags, lemmas, mor-
phological features, and unsupervised clusters of
word forms. MaltParser achieves slightly lower
labeled accuracy, but is probably the best option
if speed is crucial. The Berkeley parser has high
accuracy for unlabeled dependencies, but the cur-
rent labeling method does not achieve a compara-
bly high labeled accuracy.
Examining the use of lexical features, we ?nd
that predicted lemmas are useful in all three ar-
chitectures, while morphological features have a
marginal effect on the two dependency parsers
(they are not used by the Berkeley parser). Unsu-
pervised word clusters, ?nally, give a signi?cant
improvement for the Berkeley parser, but have a
rather small effect for the dependency parsers.
Other results for statistical dependency pars-
ing of French include the pilot study of Candito
et al (2010), and the work ofSchluter and van
Genabith (2009), which resulted in an LFG sta-
tistical French parser. However, the latter?s re-
sults are obtained on a modi?ed subset of the FTB,
and are expressed in terms of F-score on LFG f-
structure features, which are not comparable to
our attachment scores. There also exist a num-
ber of grammar-based parsers, evaluated on gold
test sets annotated with chunks and dependen-
cies (Paroubek et al, 2005; de la Clergerie et al,
2008). Their annotation scheme is different from
that of the FTB, but we plan to evaluate the statis-
tical parsers on the same data in order to compare
the performance of grammar-based and statistical
approaches.
Acknowledgments
The ?rst, third and fourth authors? work was sup-
ported by ANR Sequoia (ANR-08-EMER-013).
We are grateful to our anonymous reviewers for
their comments.
References
Abeill?, A. and N. Barrier. 2004. Enriching a french
treebank. In LREC?04.
Bikel, D. M. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In HLT-02.
Brown, P., V. Della Pietra, P. Desouza, J. Lai, and
R. Mercer. 1992. Class-based n-gram models of
natural language. Computational linguistics, 18(4).
Buchholz, S. and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In CoNLL
2006.
Candito, M. and B. Crabb?. 2009. Improving gener-
ative statistical parsing with semi-supervised word
clustering. In IWPT?09.
Candito, M. and D. Seddah. 2010. Parsing word clus-
ters. In NAACL/HLT Workshop SPMRL 2010.
Candito, M., B. Crabb?, and P. Denis. 2010. Statis-
tical french dependency parsing : treebank conver-
sion and ?rst results. In LREC 2010.
Carroll, J., E. Briscoe, and A. San?lippo. 1998. Parser
evaluation: A survey and a new proposal. In LREC
1998.
Cer, D., M.-C. de Marneffe, D. Jurafsky, and C. Man-
ning. 2010. Parsing to stanford dependencies:
Trade-offs between speed and accuracy. In LREC
2010.
Charniak, E. 2000. A maximum entropy inspired
parser. In NAACL 2000.
115
Crabb?, B. and M. Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In
TALN 2008.
de la Clergerie, E. V., C. Ayache, G. de Chalendar,
G. Francopoulo, C. Gardent, and P. Paroubek. 2008.
Large scale production of syntactic annotations for
french. In First International Workshop on Auto-
mated Syntactic Annotations for Interoperable Lan-
guage Resources.
de Marneffe, M.-C., B. MacCartney, and C. D. Man-
ning. 2006. Generating typed dependency parses
from phrase structure parses. In LREC 2006.
Denis, P. and B. Sagot. 2009. Coupling an an-
notated corpus and a morphosyntactic lexicon for
state-of-the-art pos tagging with less human effort.
In PACLIC 2009.
Eisner, J. 1996. Three new probabilistic models for
dependency parsing: An exploration. In COLING
1996.
Fan, R.-E., K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and
C.-J. Lin. 2008. LIBLINEAR: A library for large
linear classi?cation. Journal of Machine Learning
Research, 9.
Kahane, S., A. Nasr, and O. Rambow. 1998.
Pseudo-projectivity: A polynomially parsable non-
projective dependency grammar. In ACL/COLING
1998.
Klein, D. and C. D. Manning. 2003. Accurate unlexi-
calized parsing. In ACL 2003.
Koo, T., X. Carreras, and M. Collins. 2008. Sim-
ple semi-supervised dependency parsing. In ACL-
08:HLT.
K?bler, S. 2008. The PaGe 2008 shared task on pars-
ing german. In ACL-08 Workshop on Parsing Ger-
man.
Lin, D. 1995. A dependency-based method for evalu-
ating broad-coverage parsers. In IJCAI-95.
Magerman, D. M. 1995. Statistical decision-tree mod-
els for parsing. In ACL 1995.
Matsuzaki, T., Y. Miyao, and J. Tsujii. 2005. Proba-
bilistic cfg with latent annotations. In ACL 2005.
McDonald, R. and J. Nivre. 2007. Characterizing
the errors of data-driven dependency parsing mod-
els. In EMNLP-CoNLL 2007.
McDonald, R. and F. Pereira. 2006. Online learning
of approximate dependency parsing algorithms. In
EACL 2006.
McDonald, R., K. Lerman, and F. Pereira. 2006. Mul-
tilingual dependency analysis with a two-stage dis-
criminative parser. In CoNLL 2006.
McDonald, R. 2006. Discriminative Learning and
Spanning Tree Algorithms for Dependency Parsing.
Ph.D. thesis, University of Pennsylvania.
Nivre, J., Johan Hall, and Jens Nilsson. 2006. Malt-
parser: A data-driven parser-generator for depen-
dency parsing. In LREC 2006.
Nivre, J., J. Hall, S. K?bler, R. McDonald, J. Nils-
son, S. Riedel, and D. Yuret. 2007. The CoNLL
2007 shared task on dependency parsing. In CoNLL
Shared Task of EMNLP-CoNLL 2007.
Nivre, J. 2006. Inductive Dependency Parsing.
Springer.
Nivre, J. 2008. Algorithms for deterministic incre-
mental dependency parsing. Computational Lin-
guistics, 34.
Paroubek, P., L.-G. Pouillot, I. Robba, and A. Vilnat.
2005. Easy : Campagne d??valuation des analy-
seurs syntaxiques. In TALN 2005, EASy workshop :
campagne d??valuation des analyseurs syntaxiques.
Petrov, S. and D. Klein. 2007. Improved inference for
unlexicalized parsing. In NAACL-07: HLT.
Petrov, S., L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In ACL 2006.
Sagot, B. 2010. The Lefff, a freely available and large-
coverage morphological and syntactic lexicon for
french. In LREC 2010.
Schluter, N. and J. van Genabith. 2009. Dependency
parsing resources for french: Converting acquired
lfg f-structure. In NODALIDA 2009.
Seddah, D., M. Candito, and B. Crabb?. 2009. Cross
parser evaluation and tagset variation: a french tree-
bank study. In IWPT 2009.
Seddah, D., G. Chrupa?a, O. Cetinoglu, J. van Gen-
abith, and M. Candito. 2010. Lemmatization and
statistical lexicalized parsing of morphologically-
rich languages. In NAACL/HLT Workshop SPMRL
2010.
Tsarfaty, R. 2006. Integrated morphological and syn-
tactic disambiguation for modern hebrew. In COL-
ING/ACL 2006 Student Research Workshop.
Yamada, H. and Y. Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In
IWPT 2003.
116
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1694?1705, Dublin, Ireland, August 23-29 2014.
Combining Natural and Artificial Examples to Improve Implicit
Discourse Relation Identification
Chlo? Braud
ALPAGE, Univ Paris Diderot
& INRIA Paris-Rocquencourt
75013 Paris - France
chloe.braud@inria.fr
Pascal Denis
MAGNET, INRIA Lille Nord-Europe
59650 Villeneuve d?Ascq - France
pascal.denis@inria.fr
Abstract
This paper presents the first experiments on identifying implicit discourse relations (i.e., relations
lacking an overt discourse connective) in French. Given the little amount of annotated data for
this task, our system resorts to additional data automatically labeled using unambiguous connec-
tives, a method introduced by (Marcu and Echihabi, 2002). We first show that a system trained
solely on these artificial data does not generalize well to natural implicit examples, thus echoing
the conclusion made by (Sporleder and Lascarides, 2008) for English. We then explain these ini-
tial results by analyzing the different types of distribution difference between natural and artificial
implicit data. This finally leads us to propose a number of very simple methods, all inspired from
work on domain adaptation, for combining the two types of data. Through various experiments
on the French ANNODIS corpus, we show that our best system achieves an accuracy of 41.7%,
corresponding to a 4.4% significant gain over a system solely trained on manually labeled data.
1 Introduction
An important bottleneck for automatic discourse understanding is the proper identification of implicit
relations between discourse units. What makes these relations difficult is that they lack strong surface
cues like a discourse marker. This point is illustrated in the French examples (1) and (2).
1
In (1), the
connective mais (but) triggers a relation of contrast, whereas in (2), there is no explicit connective to
signal the explanation relation, and the relation has to be inferred through other ways (in this case, a
causal relation between having injured players and loosing).
(1) La hulotte est un rapace nocturne, mais elle peut vivre le jour.
The tawny owl is a nocturnal bird of prey, but it can live in the daytime.
(2) L??quipe a perdu lamentablement hier. Elle avait trop de bless?s.
The team lost miserably yesterday. It had too many injured players.
Implicit relations are very widespread in naturally-occurring data. Thus, they make up between 39.5%
and 54% of the annotated examples in the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008),
depending on the relation types used.
2
A quick look at other discourse corpora suggests that the problem
is as pervasive (if not more) in other languages. The French ANNODIS corpus does not annotate the
distinction between explicit and implicit relations, but a projection of a French connective lexicon on the
data gives a proportion of 47.4 to 71% of implicit relations, depending on the set of relations.
3
For the
German discourse corpus of (Gastel et al., 2011), (Versley, 2013) report 65% of implicit relations.
In this paper, we tackle the problem of automatically identifying implicit discourse relations in French.
To date, the large majority of studies on this task have focused on English, and to a lesser extent on
German. Performance remain relatively low compared to explicit relations, due to the lack of strong
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organisers. License details: http://creativecommons.org/licenses/by/4.0/
1
All our examples are taken from the ANNODIS corpus: http://redac.univ-tlse2.fr/corpus/annodis/.
2
The former count does not include AltLex, EntRel and NoRel as implicit examples, whereas the latter does.
3
The first count does not include attribution, e-elaboration and frame examples.
1694
predictors. Because it relies on more complex, interacting factors, the identification of implicit relations
requires a lot of data. But the available annotated for French is scarce: while the PDTB contains about
40, 000 examples, the French ANNODIS only has about 3, 000 examples. An additional challenge for
building such a system for French compared to English is the lack of external lexical ressources (e.g.,
semantic verb classification, polarity database).
A natural approach to deal with the lack of annotated implicit data is to resort to additional data
automatically obtained from explicit examples in which the connective is removed (Marcu and Echihabi,
2002). Provided that one could reliably identify discourse connectives, this approach makes it possible to
create large amounts of additional implicit data from raw texts. Unfortunately, (Sporleder and Lascarides,
2008) show that a system trained on this type of artificially generated data does not generalize well,
leading to important performance degradation compared to a system solely trained on natural data.
The central question we address in this paper is how to better leverage the large amount of automat-
ically generated data. We first show that the bad generalization performance of the system trained on
artificial data lies in important distribution differences between the two datasets. This analysis in turn
leads us to investigate various simple schemes for combining natural and artificial data methods inspired
from the field of domain adaptation. Our best combined system yields a significant improvement of 4.4%
over a system solely trained on the available manually annotated data.
The rest of this paper is organized as follows. Section 2 summarizes previous works on implicit
relation identification. In section 3, we describe the problems introduced by the use of artificial data and
the methods we develop to deal with them. In section 4, we give a description of the data used, and in
section 5, we detail our feature set. Our experiments are then summarized in section 6.
2 Related Work
To date, there have been only a few attempts at building full document-based discourse parsers. On the
RST-DT (Carlson et al., 2001), the best performing system is (Joty et al., 2013), who report an F
1
score
of 55.71 for labeled stuctures (with 23 relations). On the same corpus, (Sagae, 2009) and (Hernault et
al., 2010) report F
1
scores of 44.5 and 47.3, respectively. On the PDTB, the parser of Lin et al. (2010)
obtains an F
1
score of 33 (16 explicit relations, 11 implicit relations). On the ANNODIS corpus, Muller
et al. (2012) reports F
1
scores of 36.1 (17 relations) and 46.8 (4 relations).
These still modest performance are due to wrong attachment decisions, as well as to errors in relation
labeling. Most of these latter errors are mostly imputable to wrong classifications of implicit relations.
Thus, the current best accuracy performance on explicit PDTB relations are 94.15% on 4 relations (Pitler
and Nenkova, 2009), and 86.77% on 16 relations (Lin et al., 2010). By contrast, the best identification
system for implicit PDTB relations obtains an accuracy of 65.4% on 4 relations in (Pitler et al., 2009), and
down to 40.2% for 11 of the level 2 relations of PDTB (Lin et al., 2009). For German, Versley (2013)?s
study on implicit relations reports 42.5 in F
1
for 5 relations and 18.7 for 21 relations. For French, Muller
et al. (2012) report an accuracy score of 63.6% for their relation labeling system (over 17 relations), but
they do not provide separate scores for explicit vs. implicit relations.
This performance drop reflects the difficulty of identifying a rhetorical relation in the absence of an
explicit discourse marker. As shown by (Park and Cardie, 2012), the identification of implicit relations
relies on more diverse and noisy predictors from syntax (in the form of prediction rules) and (lexical)
semantics (e.g., polarity, semantic classes and fine-grained semantic tags for verbs). Unfortunately, most
of the semantic resources used to derive features for English (polarity database, Inquirer tags) are not
available for French. Zhou et al. (2010) try to predict the implicit connectives annotated in the PDTB
as a way of predicting the relation, a method only possible with this corpus. They obtain results lower
than those reported by (Park and Cardie, 2012). In another context, Sporleder (2008) shows that using
WordNet is less effective than lemmatisation for capturing semantic generalization, and (Wang et al.,
2010) use tree kernels in order to better capture important syntactic information. In another context,
Sporleder (2008) shows that using WordNet is less effective than lemmatisation for capturing semantic
generalization, and (Wang et al., 2010) use tree kernels in order to better capture important syntactic
information.
1695
Another set of studies we directly build upon explore the idea that many connectives unambiguously
trigger a unique relation, thus allowing to construct massive amount of (artificially) labelled implicit
examples from raw data. Marcu and Echihabi (2002) were the first to use this method: they were mainly
interested in showing that a removed connective could be recovered from its linguistic context. In turn,
they only tested their approach on examples that were also generated automatically, and not on manually
annotated implicit examples. In this setting, they report an accuracy of 49.7 (6 classes), significantly
above luck. Reusing the same approach, Sporleder and Lascarides (2008) then showed that a system
trained on a large amount of artificial examples (72000 examples) performs much worse than the same
system trained on a much smaller amount of natural examples (1, 051 examples) implicit examples, with
accuracies of 25.8 and 40.3, respectively.
Marcu and Echihabi?s (2002) original approach was based on the idea of finding pairs of semantically
related words that together trigger a relation (such as ?nocturne/jour? (?nocturnal/daytime?) in example
1 of contrast). Interestingly, Pitler et al. (2009) showed that word pairs extracted from artificial data are
not helpful for implicit relation identification and, moreover, that the most informative word pairs are not
semantically related. Blair-Goldensohn et al. (2007) showed that, for cause and contrast at least, results
can be enhanced by improving the quality of the artificial data. Finally, Wang et al. (2012) propose a
first approach that exploits both natural and artificial data. Specifically, they select the most informative
training points among natural and artificial examples, both coming from the PDTB or the RST DT. They
define deterministic rules for identifying so-called ?typical? examples of a relation, the ?seed? sets that
are then expanded using a simple clustering algorithm. They report performance results well over those
of (Pitler et al., 2009), but using a different evaluation protocole.
4
Also, their method is not easy
to repoduce, especially for French, where we can not define the same deterministic rules as some of
these depend on polarity information, for which we do not have external resources. Furthermore, their
approach only extracts 1 to 5% of the data as seed examples, which would represent too few examples on
our corpus. Finally, we are interested in finer-grained relations, thus more difficult to discriminate using
these kind of rules.
3 Proposed Approach
Our approach builds upon and extends the method of (Marcu and Echihabi, 2002) and (Sporleder and
Lascarides, 2008) by investigating different strategies for combining natural and artificial examples of
implicit discourse relations. These different combination schemes are inspired from domain adaptation
and are motivated by the fact that artificial and natural examples follow different probability distributions.
3.1 Distribution Differences
Most machine learning algorithms are based on the assumption that data from training and test samples
are independently and identically distributed (i.e., the i.i.d. sampling assumption). Yet, it seems that the
use of artificial data clearly undermines this assumption. There is indeed no guarantee that our artificial
examples should follow a distribution similar to that of the manual examples. This leads to the problem
of learning from non-iid data, a problem that has attracted growing attention these last years in machine
learning and NLP (Sogaard, 2013), (Hand, 2006).
In this particular context, we have two sets of data with the same output space (i.e., the discourse
relations), and the same kind of inputs space (i.e., spans of text). But our data samples can differ in a
number of ways. Following the terminology in (Moreno-Torres et al., 2012), we may encounter all the
different kinds of shift that can appear in a classification problem.
Prior Probability Shift This shift describes changes in the marginal distribution of the output (i.e., the
relations). The artificial data do not have the same class distribution as the natural ones (see section 4).
Neither do they have the same distribution as the natural explicit, because of the automatic extraction.
This problem can be easily handled by resampling artificial data (see section 4).
4
Wang et al. (2012) only use the first annotated relation and ignore the Entity relation, whereas Pitler et al. (2009) keep all
the annotations and map Entity examples to the Expansion class.
1696
Covariate Shift This shift describes changes in the marginal distribution of the input (i.e., the pairs of
spans of text). Artifical examples are originally explicit examples minus their connective, so it is rea-
sonnable to think that these examples will have a different distribution from the natural implicit examples.
Moreover, it is possible that, by removing the connective, we have made these examples semantically
unfelicitous or even ungrammatical. Segmentation is another issue, since it is automatic and based on
heuristics (see section 4). For example, artificial examples can not be multi-sentential whereas it can be
the case for natural ones.
Concept Shift This shift describes changes in the joint distribution of inputs and outputs. Consider
for instance the occurrences of relations within inter- and intra-sentential contexts. The proportion of
inter-sentential examples in natural and artificial datasets is the same for contrast (57.1%), it is similar
for result (resp. 45.7% and 39.8%), but very different for continuation (resp. 70% and 96.5%) and for
explanation (resp. 21.4% and 53.0%). Moreover, the extraction method is prone to errors, and it may
be the case that we wrongly identify a word form as a discourse connective. Thus, we may produce
examples annotated with a wrong relation or that do not involve any discourse relation at all. Finally,
deleting a connective can make the discourse ackward or even incoherent (Asher and Lascarides, 2003).
We can actually witness this with example (1). As shown by (Sporleder and Lascarides, 2008), deleting
the connective can also change the inferred relation. They found examples of explanation in which an
implicit relation becomes the only one inferable after removing the explicit marker.The deletion can
also change the inferred relation (Sporleder and Lascarides, 2008). We found an even worse effect in
our French corpus. In example (3), the connective puisqu(e) (because) triggers an explanation, thus the
events are ordered following the causal law. The cause, ?migrer? (?migrate?), comes before the effect,
?deviennent? (?becomes?). But when we delete the connective, the order of the events seems to be
reversed. Keeping the first clause as the first argument, we then obtain a result relation in this sentence.
(3) Les Amorrites deviennent ? la p?riode suivante de s?rieux adversaires des souverains d?Ur,
puisqu?ils commencent alors ? migrer en grand nombre vers la M?sopotamie.
In the next period, Amorrites become severe opponents of the sovereigns of Ur, because they then
begin to migrate in large numbers to Mesopotamia.
3.2 Methods Inspired by Domain Adaptation
A way to deal with all the distribution differences observed is to reframe our problem within the frame-
work of domain adaptation. Informally, the task of domain adaptation is to port some system from one
domain, the source, to another, the target. Informally, we have a distribution D
s
for the source data and
a distribution D
t
for the target data. The goal of the classifier is to build a good approximation of D
t
. If
one uses data following the distribution D
s
in order to build this approximation, then the performance
will depend of the similarity between D
s
and D
t
. If these distributions are too dissimilar, the approxi-
mation will be bad and so will be the performance. It is the case in particular when the domains (e.g.,
text genres) are different. The goal of domain adaptation is precisely to deal with data from different
distributions (Jiang, 2008), (Mansour et al., 2009). We are not exactly in the same setting, but we can
regard the artificial data as the source, and the natural data, on which we evaluate, as the target.
As a first step, we decided to investigate the simplest domain adaptation methods there is, such as
those described in (Daum? III, 2007). These methods either combine directly the data or the models
built on each set of data. Performance of all these systems will be compared to the base systems trained
on only one set of data, in section 6.
Data combination The first possibility is to combine the data. The first model is trained on all natural
and artificial data together (UNION). This method does not allow us to control the importance of the two
sets of data nor to evaluate their influence on the system. We thus refine it in two ways. First, we only
add to the manual data randomly selected samples from the artificial data (ARTSUB). Alternatively, we
keep all the artificial examples but reweight (or, equivalently, duplicate) the manual examples (NATW).
Both these schemes allow us to avoid a massive imbalance between the two kinds of data.
1697
Model combination The second strategy consists in combining the models. A first set of methods
involve adding new features. That is, we train a model on the artificial data, then run it on the natural
examples. We use these predictions as new attributes for the natural model (ADDPRED). The parameter
associated to the attribute therefore measures the importance to be given to the predictions made by the
model trained on artificial data. We propose a variation of this method by adding the probabilities of each
prediction as supplementary attributes (ADDPROB). The intuition is that even if the classifier is wrong, it
could still be consistent in its errors. Yet another model combination consists in using the parameters of
the artificial model as initial values for the manual model parameters (ARTINIT). This method allows to
give an initial information to the natural model rather than a random intialization. Finally, we also build
a model by linearly interpolating the two basic models (LININT).
In addition to these combination schemes, we also add a method to automatically select examples
among the artificial set based on the confidence of the artificial model. Its aim is to filter out noisy
examples, our hypothesis being that the more confident the model, the less noisy the example.
4 Data
In this work, we choose to focus on 4 relations, contrast, result, continuation and explanation, each
of which can be either explicit or implicit. These are the same as the relations used in (Sporleder and
Lascarides, 2008), allowing for easy comparison across languages, with the exception of the relation
summary which does not appear in the ANNODIS corpus. Although it is difficult to map these relations
onto the relation set of the PDTB, we can say that our relations are closer to level 2 and level 3 (i.e.,
fine-grained) PDTB relations than level 1 (i.e., coarse-grained) ones.
4.1 Manually Annotated Data: ANNODIS
Our natural implicit examples are taken from the ANNODIS corpus, which is to date the only available
French corpus annotated at the discourse level. Its annotations are based on the SDRT framework (Asher
and Lascarides, 2003). It consists of 86 newspaper and Wikipedia articles. 3, 339 examples have been
annotated using 17 relations. In way of comparison, note that the PDTB has roughly 12 times more
annotated relations than ANNODIS. Documents are segmented in Elementary Discourse Units (EDUs)
which can be clauses, prepositionnal phrases and some adverbials and parentheticals if the span of text
describes an event. The relations link EDUs and complex segments, adjacent or not. The connectives are
not annotated, which means that the examples of implicit relations had to be extracted automatically.
The corpus has been pre-processed using the MELt tagger (Denis and Sagot, 2009) for POS-tagging,
lemmatization and morphological markings. Then, the documents have been parsed using the the MST-
Parser (McDonald and Pereira, 2006) trained for French by (Candito et al., 2010). In order to identify
implicit examples, we used the French lexicon of connectives (LexConn) developed by Roze et al. (2012).
We simply matched all possible connective forms associated with the annotated relations (discarding ?,
which is too ambiguous). We did not add constraints on the connective position, as we wanted to be
sure to exclude all explicit examples, this method led us to miss a few implicit examples. Out of 1, 108
examples annotated with one of the 4 relations considered, 494 were found to be implicit (see table 2).
4.2 Automatically Annotated Data
The artificial data are automatically extracted from raw data using heuristic rules. We use LexConn to
mine explicit instances in the corpus Est R?publicain composed of newspaper articles (9M sentences),
with the same pre-processings as ANNODIS. LexConn contains 329 connectives, among them, 131 are
unambiguous for our 4 relations. We grouped pragmatic relations (i.e., the relation is between speech
acts) and non pragmatic relations (i.e., the relation is between facts) relations, assuming they involve the
same kind of predictors, and the 3 contrastive relations, as only one type of contrast is annotated in ANN-
ODIS. We did not take into account 3 connectives corresponding to unknown part-of-speech. Our first
evaluation led us to delete 6 connectives, very ambiguous between discourse and non discourse readings,
such as ?maintenant? (?now?). We eventually settled on 122 connectives, among which 100 were seen
in the corpus in a configuration matching one of our pre-defined patterns. As a comparison, (Sporleder
1698
and Lascarides, 2008) only had 50 such connectives. We finally use 122 connectives, among which 100
were seen in a correct configuration in the corpus. As a comparison, 50 were used in (Sporleder and
Lascarides, 2008).
Position Part-of-speech Patterns Examples
Inter-sentential All POS A1. C(,) A2. A1. Malheureusement(,) A2
A1. Surtout, A2.
Adv.
A1. beg-A2(,) C(,) end-A2. A1. beg-A2, de plus, end-A2.
A1. beg-A2(,) en outre(,) end-A2.
A1. A2, C. A1. A2, remarque.
Intra-sentential All POS A1, C(,) A2. A1, de plus(,) A2.
A1(,) donc(,) A2.
SC and Prep. C A1, A2. Preuve que A1, A2.
Puisque A1, A2.
Adv.
A1, beg-A2(,) C (,) end-A2. A1, beg-A2, de plus, end-A2.
A1, beg-A2(,) en outre(,) A2.
A1, A2, C. A1, A2, r?flexion faite.
Table 1: Defined patterns with some examples. ?A1? stands for the first argument, ?A2? for the second
and ?C? stands for the connective ; ?beg? and ?end? stand resp. for the beginning and the end of an
argument ; ?(x)? indicates that ?x? is not necessary, depending on the connective form. Some patterns
are only possible for some sets of connectives based on their part-of-speech (Subordinating Conjunction
(SC), Preposition (Prep.), Averbials (Adv.)).
The heuristic used to extract the examples has two main steps. First, we search forms used in discourse
readings using patterns (see table 1) that were manually defined for each connective based on its position,
its part-of-speechand the punctuation around it. Second, we identify the connectives arguments using the
same information. We make the same simplifying assumptions as in the previous studies: an argument
covers at most one sentence, and we have at most 2 EDUs within a sentence. As additional constraint,
we also require the presence of a verb in each relation argument. When two connectives occur in the
same segment, it is possible that one modifies the other. In turn, a naive extraction could produce two
examples with different relations but the same arguments. To avoid the creation of spurious examples,
we extract two examples in these cases only if one is inter- and the other intra-sentential according to our
extraction patterns.
Natural dataset Artificial dataset
Relation Explicit Implicit Available Training Test
contrast 100 42 252 793 23 409 2 926
result 52 110 50 297 23 409 2 926
continuation 404 272 29 261 23 409 2 926
explanation 58 70 59 909 23 409 2 926
All 614 494 392 260 93 636 11 704
Table 2: Number of examples in our corpora, for the natural dataset, only the implicit examples are used.
This simple method allows to quickly generate a large amount of data. In total, we extracted 392, 260
examples (see table 2). This initial dataset was rebalanced in a way to keep the maximum number of
available examples (thus dealing with the prior probability shift). We used 80% of the data as training
set, and 10% the development and test set. Note that there are some important differences in the label
distributions between natural and artificial data. For instance, the most represented relation in the natural
data (continuation) is the least represented in the artificial data. This is because the connectives that
trigger this relation are highly ambiguous between discourse and non-discourse readings. Finally, this
method generates some noise: out of 250 random examples, we found 37 errors in span boudaries and
1699
18 cases in which the connective form does not have a discourse reading.
5 Features
We adapted various features used in previous studies. The lack of ressources for French prevented us
from using them all, especially the semantic ones. These features correspond to surface information and
others more linguistic. As a comparison, (Marcu and Echihabi, 2002) only used pairs of words.
Sporleder and Lascarides (2008) used various linguistic features but no syntaxic ones. (Wang et al.,
2012) used semantic, syntaxic and lexical information. We used lexico-syntaxic information. Finally,
note that our goal is to evaluate the efficiency of data combinations. Thus we did not try to optimize this
feature set, as it would have introduced another parameter in our model.
Indication of syntactic complexity: we compute the number of nominal, verbal, prepositional, adjec-
tival and adverbial phrases.
Information concerning the heads of the arguments: we keep the lemma of negative element linked
to the head, we also get some temporal/aspectual information (number of auxiliaries dependent of the
head, tense, person, number of the auxiliaries), information about the heads dependents (if an object, a
by-object or a modifier is present ; if a preposition dependent of the head, subject or object is present ;
part-of-speech of the modifiers and prepositional dependents of the head, subject and object) and some
morphological information (tense and person of the head if verbal, gender if non verbal, number of the
head, precise part-of-speech, ?VPP?, and simplified,?V?). We also add features pairing the tenses for
verbal heads and the heads numbers.
Position: we add a feature indicating if the example is inter or intra-sentential.
Indication of thematic continuity: we compute general lemma overlap and lemma overlap for open
class words.
6 Experiments
Our main objective is to assess whether one can use the artificial data to improve the performance of a
system solely based on data manually annotated only available in small amount. We therefore test the
methods described in section 3.
We experimented with a maximum entropy classifier from the MegaM
5
package, in multiclass clas-
sification, with a maximum of 100 iterations. We did not try to optimize the regularization parameter
which is then equal to 1.
We rebalance the corpus of manually annotated data to a maximum of 70 examples per relation.
6
We
have too few annotated examples to be able to construct a separate test set sufficiently large to make
statistical significance test. Thus, we decided to make a stratified nested cross-validation. It has been
shown that this method provides an estimate of the error that is very close to that one could obtain
on an independent evaluation set ((Varma and Simon, 2006), (Scheffer, 1999)), as it prevents us from
optimizing our hyper-parameters and performing evaluation on the same data. Specifically, there are two
cross-validation loops: the inner loop is used for tuning the hyper-parameters (as described in section
6.2) and the outer loop estimates the generalization error. The data are first split into N folds. We take
the fold k (with 1 ? k ? N ) as the current evaluation set. The N ? 1 other folds are used as training
data and split into M folds used for model fitting. The best model is then evaluated on the fold k.
Finally, we report performance on the N folds. We used two 5-fold cross-validation in order to select
and evaluate the best models for the systems described in section 3.2. We have no guarantee to select the
best models at each test step, but this procedure allows to evaluate the stability of the system with respect
to the hyper-parameters (i.e. the chosen values should not be too scattered), the overfitting (i.e. inner and
5
http://www.umiacs.umd.edu/~hal/megam/version0_3/
6
Our focus is on the methodology of data combination, so we left for future work the issue of dealing with the highly
imbalanced relation distribution of the natural data. Incidentally, note that this setting prevents us from getting a system solely
performing well on highly frequent relations.
1700
outer estimations should be close) and the stability of the models (i.e. variance in the predictive capacity,
between the results on the outer folds).
As in the previous studies, we report performance using micro-averaged accuracy and F
1
score per
relation. In order to evaluate the statistical significance of our results, we use the Student?s t-test (with p-
value< 0.05) which has been proved to work with very small sample (see (de Winter, 2013)) if the effect
size (computed using the Cohen coefficient) and the correlation between the sample are large enough,
while, as noted in (de Winter, 2013), the Wilcoxon signed rank test (that we initially tried) could lead to
overestimated p-value with such small sample. The results of the most relevant systems are presented in
table 3.
Without selection With selection
NATONLY ARTONLY ADDPRED ARTINIT ADDPRED+SELEC NATW+selec
Accuracy 37.3 23.0 39.3 40.1 41.7
?
41.3
contrast 15.0 23.2 16.0 16.9 20.8 19.2
result 47.6 15.7 50.6 45.9 51.0 48.3
continuation 28.1 32.1 31.9 34.0 31.2 32.4
explanation 47.9 22.4 46.7 52.2 53.9 53.4
Table 3: Most relevant systems, with or without selection of examples, overall accuracy and F
1
score per
relation,
?
corresponds to a significant improvement over NATONLY.
6.1 Basic Models
In the first set of experiments, we trained two classifiers. The first one is trained on the natural implicit
data (NATONLY, 252 examples), and the second one on the artificial implicit data (ARTONLY, 93, 636
examples). We test both models on natural implicit data.
The overall accuracy of the NATONLY model is 37.3 with F
1
score ranging from 15.0 for contrast
to 47.9 for explanation. The performance on contrast is fairly low, probably because this relation is
the least frequent in our training set. Note that the overall accuracy obtained is quite close to the 40.3
obtained for English by (Sporleder and Lascarides, 2008).
The overall accuracy of the ARTONLY model is 47.8 when evaluated on the same type of data, that
is, artificial ones (11, 704 test examples), but only 23.0 when evaluated on natural data. This significant
drop in performance has been observed in the previous studies on English. It can be attributed to the
distribution differences described in section 3. We can observe that the use of the artificial data lowers
the F
1
score for result and explanation while, for contrast, F
1
score is raised by about 10 points.
6.2 Models with Combinations
In this section, we present the results for the systems using both natural and artificial data. We either
directly combine the data or use the data to build separate models that are then combined. Some of these
models use hyper-parameters. When weighting the natural examples, we test weights c ? [0.5, 1, 5]
and c ? [10; 2000] with an increment of 10 until 100, of 50 until 1000 and of 500 until 2000. When
adding random subsets of artificial data, we add each time k times the number of natural examples
artificial examples with k ? [0.1; 600] with an increment of 0.1 until 1, of 10 until 100 and of 50 until
600. Finally, when taking a linear interpolation of the models, we build a new model by weighting the
artificial model by ? ? [0.1; 0.9] with increments of 0.1.
In general, we observe that most of the systems lead to similar or higher accuracy than NATONLY, but
none of the improvements is statistically significant. The best system is ARTINIT (accuracy 40.1, p-value
of 0.18 and a small effect size, 0.39). Two other systems get an accuracy score better than 39, that is AD-
DPRED (39.3) and LININT (39.3), but not significantly better than NATONLY. The system ADDPROB,
similar to ADDPRED, leads to lower accuracy, showing that adding the probabilities decrease the per-
formance. For these systems, the scores on each of the outer folds are close
7
, specially for ADDPRED,
7
ARTINIT : standard deviation (sd) = 0.074, mean = 40.1 ; ADDPRED : sd = 0.037, ADDPROB sd = 0.061, mean ' 39
1701
revealing a high model stability. The other systems allow to evaluate the impact of the artificial data on
the final results.
The only method leading to lower results is when training on the union of the data sets (UNION), the
accuracy (22.6) is similar to ARTONLY. This was expected, as the natural data are about 372 times less
numerous than the artificial ones, the new model is thus more influenced by the latter. Note that Wang et
al. (2012) also experiment this setting but do not observe such a gap, maybe because their artificial data
are based on manually annotated explicit examples, which are likely to be less noisy.
When directly combining the data, either by adding random subsets of the artificial data (ARTSUB,
accuracy 34.5) or by weighting the natural examples (NATW, accuracy 38.9), we observe, on the in-
ner folds, an inverse trend. As expected, the accuracy increases as the influence of the artificial data
decreases, that is, decreasing the coefficients for ARTSUB and increasing the weights for NATW. Ob-
serving the results in the inner folds reveals a same trend about the relative importance of the two kinds
of data: natural data have to be around 2.5 times more important than the artificial ones. We also ob-
serve this effect with LININT, with the mean of the choosen ? values equals to 0.3. We also note that
the variance for the values of the hyper-parameter for ARTSUB is pretty high, probably caused by the
randomness of the subsamples selection. It is a bit lower for NATW and LININT showing that these
methods are more robust. Nevertheless, the strategy does not give an a priori good value for the hyper-
parameter but restricts the space of values (1020 plus or minus 272 for NATW and 0.3 plus or minus 0.18
for LININT).
6.3 Models with Automatic Selection of Examples
Previous experiments showed that adding artificial data mostly improves the performance but still not
significantly. We assume that a lot of the artificial data are noisy, which could hurt the systems. The
method of selection of examples thus aims at eliminating potentially noisy examples. The artificial
model is used on the training set, and we keep the examples that are predicted with a probability higher
than a threshold s ? [0.3; 0.85] with an increment of 0.1 until 0.5 and of 0.05 until 0.85. If the model
is confident enough about its prediction, the example might not correspond to noise, that is, a word
form that does not have a discourse readings and/or a segmentation error. We also check whether the
connective is redundant. For each threshold, we rebalance the data based on the least represented relation
(+SELEC systems).
The automatic selection of examples allows to improve previous results. The accuracy of the AR-
TONLY model moves from 23.0 to 25.0 with selection, and the system UNION move from 22.6 to 40.1
with selection.
The best results are obtained when we use artificial data to create new features but when we add only
the relation predicted by the artificial model (ADDPRED+SELEC). With this system, we observe a clear
tendency toward significance (accuracy 41.7 with a large effect size, 0.756, and a high correlation, 0.842).
The F
1
scores for all classes are improved : 20.8 for contrast, 51.0 for result, 31.2 for continuation and
53.9 for explanation. Two other systems get an accuracy over 40: NATW+SELEC (accuracy 41.3, with
a trend toward significance
8
) and UNION+SELEC (no significantly higher than NATONLY). We note that
ADDPRED corresponds to the best baseline in (Daum? III and Marcu, 2006), which shows the relevance
of dealing with the distributions differences in our data through domain adaptation methods.
The automatic selection step allows a more important weight on the informations provided by the
artificial data. For LININT+SELEC, the best results are obtained with an almost equal influence of the
two models. In the same way, the mean of the choosen values for the coefficient for NATW+SELEC
is much lower, and it increases a lot for ARTSUB+SELEC allowing for larger subsamples. Even if the
choosen values are widly scattered, these observations tend to prove that the selection improves the
quality of our artificial corpus. Regarding the choosen values for the thresholds, the mean over all the
systems is 0.7, with a variable standard deviation but always greater than 0.1. This deviation is pretty
high, this hyper-parameter probably needs a better optimisation, by repeating the inner loop for example,
but these experiments will allow to reduce the search space.
8
p-value = 0.077, large effect size, 0.68 and high correlation, 0.67
1702
The automatic selection of examples leads to one system, namely ADDPRED+SELEC, significantly
improving the accuracy of NATONLY. This shows that the artificial data, when rightly integrated, can
thus be used to improve a system identifying implicit relations, especially if their influence is low, the
model is driven towards the good distribution.
6.4 Effects on the Identification of the Relations
Looking at the F
1
score per relation, we observed that these systems have dissimilar behaviors. A larger
influence of the artificial model allows improvements for contrast: the best result for this relation is ob-
tained when only the artificial data are used for training (at best, 28.8 F
1
score with ARTONLY+SELEC).
The identification of the relation continuation seems to be also improved by the influence of the artificial
data. We can observe it with the linear interpolation of the models: the mean of the F
1
score increases
with the increasing of the ? coefficient for these relations. For continuation, however, the best mean F
1
is obtained with ? = 0.8, this relation needs a certain degree of influence from the natural data. Some
support for this proposition can be found in the fact that the best result for this relation is obtained with
NATW (at best, 44.7 F
1
score). For the other relations, a large weight on the artificial data clearly de-
creases the F
1
score. However, the identification of explanation is improved when we add the predictions
of the artificial model (at best, ADDPRED+SELEC, 53.9 F
1
score). Improvement is fairly low for result
(at best, 51.0 with ADDPRED+SELEC).
The relation contrast might take advantage of less noisy artificial data as most of the examples are
extracted using the connective mais (but) always in discourse readings. For explanation, predictions of
the artificial model could be quiet coherent as most of the artificial examples correspond to the pragmatic
relation explanation
?
. Moreover, if we look at the feature distribution (850 features overall), we observe
a gap of more than 30% for 2 and 5 features for result and explanation that is not observed for contrast
and continuation, the relations that make the most of the artificial data.
7 Conclusion
We have presented the first system that identifies implicit discourse relations for French. This kind
of relation is difficult to identify because of the lack of specific predictors. In the previous studies
on English, the performance on this task are fairly low despite the use of complex features, probably
because of a lack of manually annotated data. To deal with this issue, even more crucial for French,
our system also resorts to additional data, automatically annotated using discourse connectives. These
new data, however, do not generalize well to natural implicit data, because of distribution differences.
We thus test methods inspired by domain adaptation in order to combine natural and artificial data.
We add an automatic selection of examples among the artificial data to deal with noise generated by
the method of automatic annotation. We manage to get significant improvement over a system solely
trained using available data manually annotated by using automatic selection and the addition of features
corresponding to the predictions of the artificial model.
In future work, we will explore more sophisticated methods to deal with data samples that follow
different distributions. We will also explore ways to deal with imbalanced data and use our methods on
all the relations annotated in our French corpus. Finally, we will test these methods on English corpora,
in order to compare their efficiency with previous studies.
References
Stergos Afantenos, Nicholas Asher, Farah Benamara, Myriam Bras, C?cile Fabre, Lydia-Mai Ho-Dac, Anne
Le Draoulec, Philippe Muller, Marie-Paule P?ry-Woodley, Laurent Pr?vot, Josette Rebeyrolles, Ludovic Tanguy,
Marianne Vergez-Couret, and Laure Vieu. 2012. An empirical resource for discovering cognitive principles of
discourse organisation: the annodis corpus (regular paper). In Proceedings of LREC.
Nicholas Asher and Alex Lascarides. 2003. Logics of Conversation. Cambridge University Press.
Sasha Blair-Goldensohn, Kathleen R. McKeown, and Owen C. Rambow. 2007. Building and refining rhetorical-
semantic relation models. In Proceedings of NAACL HLT.
1703
Marie Candito, Joakim Nivre, Pascal Denis, and Enrique Henestroza Anguiano. 2010. Benchmarking of statistical
dependency parsers for french. In Proceedings of ICCL (posters).
Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski. 2001. Building a discourse-tagged corpus in the
framework of rhetorical structure theory. In Proceedings of the Second SIGdial Workshop on Discourse and
Dialogue.
Hal Daum? III and Daniel Marcu. 2006. Domain adaptation for statistical classifiers. Journal of Artificial Intelli-
gence Research.
Hal Daum? III. 2007. Frustratingly easy domain adaptation. In Proceedings of ACL.
Joost C.F. de Winter. 2013. Using the student?s t-test with extremely small sample sizes. Practical Assessment,
Research & Evaluation.
Pascal Denis and Beno?t Sagot. 2009. Coupling an annotated corpus and a morphosyntactic lexicon for state-of-
the-art POS tagging with less human effort. In Proceedings of PACLIC.
Anna Gastel, Sabrina Schulze, Yannick Versley, and Erhard Hinrichs. 2011. Annotation of implicit discourse
relations in the t?ba-d/z treebank. GSCL.
David J. Hand. 2006. Classifier technology and the illusion of progress. Statistical Science.
Hugo Hernault, Helmut Prendinger, David A. duVerle, and Mitsuru Ishizuka. 2010. Hilda: A discourse parser
using support vector machine classification. Dialogue and Discourse.
Jing Jiang. 2008. A literature survey on domain adaptation of statistical classifiers. Available from:
http://sifaka.cs.uiuc.edu/jiang4/domain_adaptation/survey/da_survey.html.
Shafiq R. Joty, Giuseppe Carenini, Raymond T. Ng, and Yashar Mehdad. 2013. Combining intra- and multi-
sentential rhetorical parsing for document-level discourse analysis. In Proceedings of ACL.
Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng. 2009. Recognizing implicit discourse relations in the penn discourse
treebank. In Proceedings of EMNLP.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A PDTB-styled end-to-end discourse parser. Technical
report, National University of Singapore.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. 2009. Domain adaptation: Learning bounds and
algorithms. In Proceedings of COLT.
Daniel Marcu and Abdessamad Echihabi. 2002. An unsupervised approach to recognizing discourse relations. In
Proceedings of ACL.
Ryan McDonald and Fernando Pereira. 2006. Online learning of approximate dependency parsing algorithms. In
Proceedings of EACL.
Jose G. Moreno-Torres, Troy Raeder, Roc?o Alaiz-Rodr?guez, Nitesh V. Chawla, and Francisco Herrera. 2012. A
unifying view on dataset shift in classification. Pattern Recognition.
Joonsuk Park and Claire Cardie. 2012. Improving implicit discourse relation recognition through feature set
optimization. In Proceedings of SIGDIAL.
Emily Pitler and Ani Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. In
Proceedings of the ACL-IJCNLP (Short Papers).
Emily Pitler, Annie Louis, and Ani Nenkova. 2009. Automatic sense prediction for implicit discourse relations in
text. In Proceedings of ACL-IJCNLP.
Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008. The penn discourse treebank 2.0. In Proceedings of LREC.
Charlotte Roze, Laurence Danlos, and Philippe Muller. 2012. Lexconn: A french lexicon of discourse connectives.
Discours.
Kenji Sagae. 2009. Analysis of discourse structure with syntactic dependencies and data-driven shift-reduce
parsing. Proceedings of IWPT.
1704
Tobias Scheffer. 1999. Error Estimation and Model Selection. Ph.D. thesis, Technischen Universitet Berlin,
School of Computer Science.
Anders Sogaard. 2013. Semi-supervised learning and domain adaptation in natural language processing. Morgan
& Claypool.
Caroline Sporleder and Alex Lascarides. 2008. Using automatically labelled examples to classify rhetorical
relations, an assessment. Natural Language Engineering.
Caroline Sporleder. 2008. Lexical models to identify unmarked discourse relations: Does Wordnet help? Lexical-
Semantic Resources in Automated Discourse Analysis.
Sudhir Varma and Richard Simon. 2006. Bias in error estimation when using cross-validation for model selection.
BMC bioinformatics.
Yannick Versley. 2013. Subgraph-based classification of explicit and implicit discourse relations. In Proceedings
of IWCS.
WenTing Wang, Jian Su, and Chew Lim Tan. 2010. Kernel based discourse relation recognition with temporal
ordering information. In Proceedings of ACL.
Xun Wang, Sujian Li, Jiwei Li, and Wenjie Li. 2012. Implicit discourse relation recognition by selecting typical
training examples. In Proceedings of COLING (Technical Papers).
Zhi-Min Zhou, Yu Xu, Zheng-Yu Niu, Man Lan, Jian Su, and Chew Lim Tan. 2010. Predicting discourse connec-
tives for implicit discourse relation recognition. In Proceedings of COLING (Posters).
1705
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 130?134,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
French TimeBank: An ISO-TimeML Annotated Reference Corpus
Andre? Bittar
Alpage
Univ. Paris Diderot
andre.bittar@linguist.jussieu.fr
Pascal Amsili
LLF
Univ. Paris Diderot
amsili@linguist.jussieu.fr
Pascal Denis
Alpage
INRIA
pascal.denis@inria.fr
Laurence Danlos
Alpage
Univ. Paris Diderot
danlos@linguist.jussieu.fr
Abstract
This article presents the main points in the cre-
ation of the French TimeBank (Bittar, 2010),
a reference corpus annotated according to the
ISO-TimeML standard for temporal annota-
tion. A number of improvements were made
to the markup language to deal with linguistic
phenomena not yet covered by ISO-TimeML,
including cross-language modifications and
others specific to French. An automatic pre-
annotation system was used to speed up the
annotation process. A preliminary evaluation
of the methodology adopted for this project
yields positive results in terms of data quality
and annotation time.
1 Introduction
The processing of temporal information (events,
time expressions and relations between these enti-
ties) is essential for overall comprehension of nat-
ural language discourse. Determining the temporal
structure of a text can bring added value to numer-
ous NLP applications (information extraction, Q&A
systems, summarization...). Progress has been made
in recent years in the processing of temporal data,
notably through the ISO-TimeML standard (ISO,
2008) and the creation of the TimeBank 1.2 cor-
pus (Pustejovsky et al 2006) for English. Here we
present the French TimeBank (FTiB), a corpus for
French annotated in ISO-TimeML. We also present
the methodology adopted for the creation of this re-
source, which may be generalized to other annota-
tion tasks. We evaluate the effects of our methodol-
ogy on the quality of the corpus and the time taken
in the task.
2 ISO-TimeML
ISO-TimeML (ISO, 2008) is a surface-based lan-
guage for the marking of events (<EVENT> tag) and
temporal expressions (<TIMEX3>), as well as the
realization of the temporal (<TLINK>), aspectual
(<ALINK>) and modal subordination (<SLINK>)
relations that exist among these entities. The tags?
attributes capture semantic and grammatical features
such as event class, tense, aspect and modality, and
the type and normalized interpretative value of tem-
poral expressions. The <SIGNAL> tag is used to an-
notate relation markers, such as before and after. A
set of resources for English has been developed over
the years, including an annotated corpus, TimeBank
1.2 (TB1.2)1, which has become a reference for tem-
poral annotation in English.
3 Improving ISO-TimeML
We propose a number of improvements to ISO-
TimeML to deal with as yet untreated phenom-
ena. These include both cross-language annotation
guidelines, as well as guidelines specific to French.
All these guidelines are implemented in the FTiB.
Cross-language Improvements : ISO-TimeML
currently provides for the annotation of event
modality by capturing the lemma of a modal on
a subordinated event tag in the modality at-
tribute. Inspired by the fact that in French, modal-
ity is expressed by fully inflected verbs, we pro-
pose that those verbs be tagged as modal, and we
1Annotated according to the TimeML 1.2 specification, as
opposed to the more recent ISO-TimeML.
130
provide a set of normalized values for the modal-
ity attribute, within a manual annotation context,
that reflect the classic classes of linguistic modality
(Palmer, 1986): NECESSITY and POSSIBILITY
(epistemic), OBLIGATION and PERMISSION (de-
ontic). We also provide a way of capturing the dif-
ference between support verb constructions with
a neutral aspectual value (mener une attaque (carry
out an attack)) and those with an inchoative as-
pectual value (lancer une attaque (launch an at-
tack)). ISO-TimeML encodes the relation between
the verb and its nominal argument via a <TLINK>
of type IDENTITY. We encode aspectual variants
in the FTiB by using an <ALINK>. A signifi-
cant proportion (13/36) of the annotated <ALINK>
tags in the FTiB (36%) are used in this case. A
third improvement we propose is the introduction of
the event class EVENT CONTAINER2 to distinguish
predicates that take an event nominal as subject.
In TB1.2, these predicates were sometimes marked,
but not distinguished from the OCCURRENCE class.
The distinction is appropriate as these predicates
have events as arguments, unlike OCCURRENCEs.
The relative frequency of this class (19 occurrences)
compared to the standard PERCEPTION class (10)
also justifies its use. Although not yet dealt with
in ISO-TimeML, aspectual periphrases, such as
en train de + Vinf (akin to the English progres-
sive -ing), adding an aspectual value to an event,
are captured in the FTiB in the aspect attribute
for events. We also propose a new value for as-
pect, PROSPECTIVE, encoding the value of the
construction aller + Vinf (going to + Vinf ), as in
le soleil va exploser (the sun is going to explode).
Improvements for French : a correspondence had
to be made between the ISO-TimeML schema and
the grammatical tense system of French, in particu-
lar, to account for tenses such as the passe? compose?
(PAST tense value, as opposed to the present per-
fect used in English) and imparfait (IMPERFECT,
not present in English as a morphological tense).
French modal verbs behave differently to English
modal auxiliaries as they can be conjugated in all
tenses, fall within the scope of aspectual, negative
polarity and other modal operators. Unlike in TB1.2,
2After the terminology of (Vendler, 1967)
modal verbs (and adjectives), are marked <EVENT>
in FTiB and have the class MODAL. 72 events (3.4%)
are annotated with this class in the FTiB.
4 Methodology
Text sampling : the source texts for the FTiB were
selected from the Est Re?publicain corpus of journal-
istic texts.3 The journalistic genre was chosen for
its relatively high frequency of events and temporal
expressions. Texts were sampled from 7 different
sub-genres4, the distributions of which are shown in
Table 1. Certain sub-genres appear in higher pro-
portions than others, for two main reasons. Firstly,
to favor comparison with TB1.2 (which is made up
of news articles). Secondly, because the news gen-
res are relatively diverse in style compared to the
other sub-genres, which follow a certain format (e.g.
obituaries). We present some of the correlations be-
tween sub-genre and linguistic content in Section 5.
Sub-genre Doc # Doc % Token # Token %
Annmt. 22 20.2% 1 679 10.4%
Bio. 1 0.9% 186 1.1%
Intl. news 32 29.4% 5 171 31.9%
Loc. news 19 17.5% 4 370 27.0%
Natl. news 25 22.9% 3 347 20.7%
Obituary 2 1.8% 313 1.9%
Sport 8 7.3% 1 142 7.0%
Total 109 100% 16 208 100%
Table 1: Proportions of sub-genres in the FTiB.
Automatic pre-annotation : To speed up the an-
notation process, we carried out an automatic pre-
annotation of markables (events, temporal expres-
sions and some relation markers), followed by man-
ual correction. Relations were annotated entirely by
hand, as this task remains very difficult to automate.
Below we describe the two modules developed for
pre-annotation.
The TempEx Tagger marks temporal expressions
<TIMEX3> and sets the tag?s attributes, and anno-
tates certain <SIGNAL> tags. This module con-
sists of a set of Unitex (Paumier, 2008) transduc-
ers that are applied to raw text. We adapted and
3Available at http://www.cnrtl.fr.
4These are announcement, biography, international news,
local news, national news, obituary and sport.
131
EVENT 
correction
Adjudication Adjudication
Coherence 
check
TIMEX3 
correction
SIGNAL 
correction
Pre-
annotated 
text
Annotated 
Markables
Annotated 
Markables + 
LINKs
Gold 
Standard
LINK 
annotation
Figure 1: Schema of the annotation strategy.
enriched a pre-existing set of transducers for anno-
tating temporal expressions in French (Gross, 2002)
for our purposes. Marked expressions are classified
according to their ISO-TimeML type5 and the val-
ues of certain attributes are calculated. The value
attribute is only set during normalization, carried out
after the detection phase. A script calculates normal-
ized values for marked expressions, including index-
icals, such as lundi dernier (last Monday) or l?anne?e
prochaine (next year) (with the article?s publication
date as reference point). A comparative evaluation
with the DEDO system of (Parent et al 2008) shows
very similar performance (for exact match on tag
span and for the value attribute) over the same
evaluation corpus (Table 2).
System Prec. Rec. F-sc.
Match TempEx 84.2 81.8 83.0
DEDO 83.0 79.0 81.0
Value TempEx 55.0 44.9 49.4
DEDO 56.0 45.0 50.0
Table 2: Comparative evaluation of the TempEx Tagger
for exact match on tag span and value calculation.
The Event Tagger marks up events (<EVENT> tag)
and certain relation markers through the application
of a sequence of rules acting on the local chunk con-
text. The rules eliminate unlikely candidates or tag
appropriate ones, based on detailed lexical resources
and various contextual criteria. Input is a text pre-
processed with POS tags, morphological analysis
and chunking (carried out with the Macaon process-
5DATE (e.g. 15/01/2001, le 15 janvier 1010, jeudi, demain),
TIME (ex. 15h30, midi), DURATION (ex. trois jours, un an) ou
SET (ex. tous les jours, chaque mardi)
ing pipeline (Nasr et al 2010)). A reliable com-
parison with the DEDO system, to our knowledge
the only other system for this task in French, was
unfortunately not possible. Evaluations were made
on different, yet comparable, corpora, so results are
merely indicative. For event tagging, our system
scored a precision of 62.5 (62.5 for DEDO), recall
of 89.4 (77.7) and an F-score of 75.8 (69.3). There
is room for improvement, although the system still
yields significant gains in total annotation time and
quality. An experiment to evaluate the effects of the
pre-annotation showed a near halving of annotation
time compared to manual annotation, as well as a
significant reduction of human errors (Bittar, 2010).
Unfortunately, it was not possible to reliably com-
pare the performance of the Event Tagger with the
similar module by (Parent et al 2008) (DEDO), to
our knowledge the only other system developed for
this task for French. Evaluations of each system
were carried out on different, although similar, cor-
pora. Thus, results remain merely indicative. For the
task of event recognition, our system scored a preci-
sion of 62.5 (62.5 for DEDO), recall of 89.4 (77.7)
and an F-score of 75.8 (69.3).
Manual annotation and validation : after pre-
annotation of markables, texts were corrected by 3
human annotators (2 per text), using the Callisto6
and Tango7 tools, designed for this task. Figure 1
shows the process undergone by each document.
The final step of the process is a coherence check
of the temporal graph in each document, carried out
6http://callisto.mitre.org/
7http://timeml.org/site/tango/tool.html
132
via application of Allen?s algorithm (Allen, 1983)
and graph saturation (Tannier & Muller, 2008). Us-
ing the same method, we found 18 incoherent graphs
among the 183 files of the TB1.2 corpus for English.
At this stage, the corpus contained 8 incoherencies,
which were all eliminated by hand. Manually elim-
inating incoherencies is an arduous task, and per-
forming an online coherence check during annota-
tion of relations would be extremely useful in a man-
ual annotation tool. All files were validated against
a DTD, provided with the corpus.
5 French TimeBank
Our aim for the FTiB is to provide a corpus of
comparable size to TB1.2 (approx. 61 000 to-
kens). Version 1.0 of FTiB, presented here and
made available online8 in January 2011, represents
about 14 of the target tokens. Figure 2 shows that
proportions of annotated elements for French are
mostly very similar to those in TB1.2. This sug-
gests the annotation guidelines were applied in a
similar way in both corpora and that, for the journal-
istic genre, the distributions of the various marked
elements are similar in French and English. By far
the most common relation type in the French corpus
is the <TLINK>. Among these, 1 175 are marked
between two event arguments (EVENT-EVENT),
722 between an event and a temporal expression
(EVENT-TIMEX3), and 486 between two temporal
expressions (TIMEX3-TIMEX3).
Figure 2: Annotated content of the FTiB and TB1.2.
Inter-annotator agreement was measured over the
entire FTiB corpus and compared with reported
agreement for TB1.2.9. F-scores for agreement
8Via the INRIA GForge at https://gforge.inria.
fr/projects/fr-timebank/.
9Available at http://www.timeml.org/site/
timebank/documentation-1.2.html Note that fig-
'$7( 7,0( '85$7,21 6(7
*HQUH
7
,
0
(
;


7
\
S
H


Figure 3: Distribution of <TIMEX3> types by sub-genre.
&
O
D
V
V


,B67$7(
$63(&78$/
,B$&7,21
3(5&(371
67$7(
02'$/
&$86(
2&&855(1&(
5(3257,1*
(&217$,1(5
ELRLQWOORFDO
QDWORELWVSRUW
DQQ
Figure 4: Distribution of <EVENT> classes by sub-genre.
are significantly higher for the French corpus on
<EVENT> and <TIMEX3> tag spans than for
TB1.2, and very slightly lower for <SIGNAL>. Fig-
ures for tag attributes are higher for TB1.2, as a
much looser metric10 was used for agreement, so
comparison is not yet possible. The same measure
will need to be implemented to afford an accurate
comparison.
ures were only calculated for a small subset of the entire
corpus, unlike for the FTiB, for which all data was used.
10Agreement for TB1.2 was only calculated over tags with
matching spans and wrong attributes on non-matching spans
were not penalized. For the FTiB, all tags were considered and
all attributes for non-matching tag spans were penalized.
133
Corpus
<TIMEX3> <EVENT> <SIGNAL>
Span Attr Span Attr Span
FTiB .89 .86 .86 .85 .75
TB 1.2 .83 (.95) .78 (.95) .77
Table 3: Inter-annotator agreement (F-scores).
Sub-genre and linguistic content : a preliminary
study showed correlations between the various sub-
genres chosen for the corpus and the annotations
in the texts. For example, Figure 3 shows a high
proportion of TIMEs in announcement texts (46%
of the corpus total)11, while DURATIONs are in-
frequent (2%), but appear in higher proportions in
news (21?32%) and sports (13,5%). DATEs are by
far the most frequently marked (80%), with SETs
being the least. In Figure 4, the preponderance of
the OCCURRENCE class is obvious (62.1% of all
events). REPORTING is most frequent in local and
international news. Announcements stand out yet
again, with the highest number and highest propor-
tion of the class EVENT CONTAINER. These ini-
tial observations argue in favor of text sampling to
achieve a diversity of temporal information in a cor-
pus and suggest such features may prove useful in
text classification.
6 Conclusion
Our experiences show ISO-TimeML is a stable lan-
guage and, with some modification, is applicable
to French. The FTiB is a valuable resource that
will surely stimulate development and evaluation of
French temporal processing systems, providing es-
sential data for training machine learning systems.
An initial survey of the data suggests temporal in-
formation may be useful for text classification. Our
methodology is time-efficient and ensures data qual-
ity and usability (coherence). It could be adopted to
create temporally annotated corpora for other lan-
guages as well as being adapted and generalized to
other annotation tasks.
11This is particularly significant given the low proportion of
the total corpus tokens in this sub-genre.
References
ISO 2008. ISO DIS 24617-1: 2008 Language Resource
Management - Semantic Annotation Framework - Part 1:
Time and Events. International Organization for Stan-
dardization, Geneva, Switzerland.
Andre? Bittar 2010. Building a TimeBank for French:
a Reference Corpus Annotated According to the ISO-
TimeML Standard.. PhD thesis. Universite? Paris Diderot,
Paris, France.
Andre? Bittar 2009. Annotation of Temporal Informa-
tion in French Texts.. Computational Linguistics in the
Netherlands (CLIN 19).
Se?bastien Paumier 2008. Unitex 2.0 User Manual..
Universite? Paris Est Marne-la-Valle?e, Marne-la-Valle?e,
France.
Gabriel Parent, Michel Gagnon and Philippe Muller
2008. Annotation d?expressions temporelles et
d?e?ve?nements en franc?ais. Actes de TALN 2008.
Avignon, France.
Alexis Nasr, Fre?de?ric Be?chet and Jean-Franc?ois Rey
2010. MACAON : Une cha??ne linguistique pour le trait-
meent de graphes de mots. Actes de TALN 2010. Mon-
treal, Canada.
James F. Allen. 1983. Maintaining Knowledge About
Temporal Intervals. Communications of the ACM. 26:11
832-843.
Xavier Tannier and Philippe Muller 2008. Evalua-
tion Metrics for Automatic Temporal Annotation of Texts.
Proceedings of the Sixth International Language Re-
sources and Evaluation (LREC?08) Marrakech, Mo-
rocco.
Frank Robert Palmer 1986. Mood and Modality Cam-
bridge University Press Cambridge, UK.
James Pustejovsky, Marc Verhagen, Roser Saur??, Jes-
sica Littman, Robert Gaizauskas, Graham Katz, Inderjeet
Mani, Robert Knippen and Andrea Setzer 2006. Time-
Bank 1.2 Linguistic Data Consortium
Nabil Hathout, Fiammetta Namer and Georgette Dal
2002. An Experimental Constructional Database: The
MorTAL Project Many Morphologies 178?209 Paul
Boucher ed. Somerville, Mass., USA
Zeno Vendler 1967 Linguistics and Philosophy Cornell
University Press Ithaca, NY, USA
Maurice Gross 2002 Les de?terminants nume?raux, un
exemple : les dates horaires Langages 145 Larousse
Paris, France
134
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 497?506,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Improving pairwise coreference models through
feature space hierarchy learning
Emmanuel Lassalle
Alpage Project-team
INRIA & Univ. Paris Diderot
Sorbonne Paris Cite?, F-75205 Paris
emmanuel.lassalle@ens-lyon.org
Pascal Denis
Magnet Project
INRIA Lille - Nord Europe
Avenue Helo??se, 59650 Villeneuve d?Ascq
pascal.denis@inria.fr
Abstract
This paper proposes a new method for
significantly improving the performance
of pairwise coreference models. Given a
set of indicators, our method learns how
to best separate types of mention pairs
into equivalence classes for which we con-
struct distinct classification models. In ef-
fect, our approach finds an optimal fea-
ture space (derived from a base feature set
and indicator set) for discriminating coref-
erential mention pairs. Although our ap-
proach explores a very large space of pos-
sible feature spaces, it remains tractable
by exploiting the structure of the hierar-
chies built from the indicators. Our exper-
iments on the CoNLL-2012 Shared Task
English datasets (gold mentions) indicate
that our method is robust relative to dif-
ferent clustering strategies and evaluation
metrics, showing large and consistent im-
provements over a single pairwise model
using the same base features. Our best
system obtains a competitive 67.2 of aver-
age F1 over MUC, B3, and CEAF which,
despite its simplicity, places it above the
mean score of other systems on these
datasets.
1 Introduction
Coreference resolution is the problem of partition-
ing a sequence of noun phrases (or mentions), as
they occur in a natural language text, into a set of
referential entities. A common approach to this
problem is to separate it into two modules: on
the one hand, one defines a model for evaluating
coreference links, in general a discriminative clas-
sifier that detects coreferential mention pairs. On
the other hand, one designs a method for group-
ing the detected links into a coherent global out-
put (i.e. a partition over the set of entity men-
tions). This second step is typically achieved
using greedy heuristics (McCarthy and Lehnert,
1995; Soon et al, 2001; Ng and Cardie, 2002;
Bengston and Roth, 2008), although more so-
phisticated clustering approaches have been used,
too, such as cutting graph methods (Nicolae and
Nicolae, 2006; Cai and Strube, 2010) and Integer
Linear Programming (ILP) formulations (Klenner,
2007; Denis and Baldridge, 2009). Despite its
simplicity, this two-step strategy remains competi-
tive even when compared to more complex models
utilizing a global loss (Bengston and Roth, 2008).
In this kind of architecture, the performance of
the entire coreference system strongly depends on
the quality of the local pairwise classifier.1 Con-
sequently, a lot of research effort on coreference
resolution has focused on trying to boost the per-
formance of the pairwise classifier. Numerous
studies are concerned with feature extraction, typ-
ically trying to enrich the classifier with more
linguistic knowledge and/or more world knowl-
edge (Ng and Cardie, 2002; Kehler et al, 2004;
Ponzetto and Strube, 2006; Bengston and Roth,
2008; Versley et al, 2008; Uryupina et al, 2011).
A second line of work explores the use of dis-
tinct local models for different types of mentions,
specifically for different types of anaphoric men-
tions based on their grammatical categories (such
as pronouns, proper names, definite descriptions)
(Morton, 2000; Ng, 2005; Denis and Baldridge,
2008).2 An important justification for such spe-
1There are however no theoretical guarantees that improv-
ing pair classification will always result in overall improve-
ments if the two modules are optimized independently.
2Sometimes, distinct sample selections are also adopted
497
cialized models is (psycho-)linguistic and comes
from theoretical findings based on salience or ac-
cessibility (Ariel, 1988). It is worth noting that,
from a machine learning point of view, this is re-
lated to feature extraction in that both approaches
in effect recast the pairwise classification problem
in higher dimensional feature spaces.
In this paper, we claim that mention pairs
should not be processed by a single classifier, and
instead should be handled through specific mod-
els. But we are furthermore interested in learning
how to construct and select such differential mod-
els. Our argument is therefore based on statisti-
cal considerations, rather than on purely linguis-
tic ones3. The main question we raise is, given
a set of indicators (such as grammatical types,
distance between two mentions, or named entity
types), how to best partition the pool of mention
pair examples in order to best discriminate coref-
erential pairs from non coreferential ones. In ef-
fect, we want to learn the ?best? subspaces for our
different models: that is, subspaces that are neither
too coarse (i.e., unlikely to separate the data well)
nor too specific (i.e., prone to data sparseness and
noise). We will see that this is also equivalent to
selecting a single large adequate feature space by
using the data.
Our approach generalizes earlier approaches in
important ways. For one thing, the definition
of the different models is no longer restricted to
grammatical typing (our model allows for various
other types of indicators) or to the sole typing of
the anaphoric mention (our models can also be
specific to a particular type antecedent or to the
two types of the mention pair). More importantly,
we propose an original method for learning the
best set of models that can be built from a given
set of indicators and a training set. These models
are organized in a hierarchy, wherein each leaf cor-
responds to a mutually disjoint subset of mention
pair examples and the classifier that can be trained
from it. Our models are trained using the Online
Passive-Aggressive algorithm or PA (Crammer et
al., 2006), a large margin version of the percep-
tron. Our method is exact in that it explores the full
space of hierarchies (of size at least 22n) definable
on an indicator sequence, while remaining scal-
able by exploiting the particular structure of these
during the training of the distinct local models (Ng and
Cardie, 2002; Uryupina, 2004).
3However it should be underlined that the statistical view-
point is complementary to the linguistic work.
hierarchies with dynamic programming. This ap-
proach also performs well, and it largely outper-
forms the single model. As will be shown based
on a variety of experiments on the CoNLL-2012
Shared Task English datasets, these improvements
are consistent across different evaluation metrics
and for the most part independent of the clustering
decoder that was used.
The rest of this paper is organized as follows.
Section 2 discusses the underlying statistical hy-
potheses of the standard pairwise model and de-
fines a simple alternative framework that uses a
simple separation of mention pairs based on gram-
matical types. Next, in section 3, we generalize the
method by introducing indicator hierarchies and
explain how to learn the best models associated
with them. Section 4 provides a brief system de-
scription and Section 5 evaluates the various mod-
els on CoNLL-2012 English datasets.
2 Modeling pairs
Pairwise models basically employ one local clas-
sifier to decide whether two mentions are corefer-
ential or not. When using machine learning tech-
niques, this involves certain assumptions about the
statistical behavior of mention pairs.
2.1 Statistical assumptions
Let us adopt a probabilistic point of view to de-
scribe the prototype of pairwise models. Given
a document, the number of mentions is fixed and
each pair of mentions follows a certain distribution
(that we partly observe in a feature space). The ba-
sic idea of pairwise models is to consider mention
pairs independently from each other (that is why a
decoder is necessary to enforce transitivity).
If we use a single classifier to process all
pairs, then they are supposed to be identically dis-
tributed. We claim that pairs should not be pro-
cessed by a single classifier because they are not
identically distributed (or a least the distribution is
too complex for the classifier); rather, we should
separate different ?types? on pairs and create a
specific model for each of them.
Separating different kinds of pairs and handling
them with different specific models can lead to
more accurate global models. For instance, some
coreference resolution systems process different
kinds of anaphors separately, which suggests for
example that pairs containing an anaphoric pro-
noun behave differently from pairs with non-
498
pronominal anaphors. One could rely on a rich set
of features to capture complex distributions, but
here we actually have a rather limited set of ele-
mentary features (see section 4) and, for instance,
using products of features must be done carefully
to avoid introducing noise in the model. Instead
of imposing heuristic product of features, we will
show that a clever separation of instances leads to
significant improvements of the pairwise model.
2.2 Feature spaces
2.2.1 Definitions
We first introduce the problem more formally. Ev-
ery pair of mentions mi and mj is modeled by a
random variable:
Pij : ? ? X ?Y
? 7? (xij(?), yij(?))
where ? classically represents randomness, X is
the space of objects (?mention pairs?) that is not
directly observable and yij(?) ? Y = {+1,?1}
are the labels indicating whether mi and mj are
coreferential or not. To lighten the notations, we
will not always write the index ij. Now we define
a mapping:
?F : X ? F
x 7? x
that casts pairs into a feature space F through
which we observe them. For us, F is simply a
vector space over R (in our case many features are
Boolean; they are cast into R as 0 and 1).
For technical coherence, we assume that
?F1(x(?)) and ?F2(x(?)) have the same values
when projected on the feature space F1 ? F2:
it means that common features from two feature
spaces have the same values.
From this formal point of view, the task of
coreference resolution consists in fixing ?F , ob-
serving labeled samples {(?F (x), y)t}t?TrainSet
and, given partially observed new variables
{(?F (x))t}t?TestSet, recovering the correspond-
ing values of y.
2.2.2 Formalizing the statistical assumptions
We claimed before that all mention pairs seemed
not to be identically distributed since, for exam-
ple, pronouns do not behave like nominals. We
can formulate this more rigorously: since the ob-
ject space X is not directly observable, we do not
know its complexity. In particular, when using a
mapping to a too small feature space, the classifier
cannot capture the distribution very well: the data
is too noisy.
Now if we say that pronominal anaphora do not
behave like other anaphora, we distinguish two
kinds of pair i.e. we state that the distribution of
pairs in X is a mixture of two distributions, and
we deterministically separate pairs to their specific
distribution part. In this way, we may separate
positive and negative pairs more easily if we cast
each kind of pair into a specific feature space. Let
us call these feature spaces F1 and F2. We can ei-
ther create two independent classifiers on F1 and
F2 to process each kind of pair or define a single
model on a larger feature space F = F1 ? F2. If
the model is linear (which is our case), these ap-
proaches happen to be equivalent.
So we can actually assume that the random vari-
ables Pij are identically distributed, but drawn
from a complex mixture. A new issue arises: we
need to find a mapping ?F that renders the best
view on the distribution of the data.
From a theoretical viewpoint, the higher the di-
mension of the feature space (imagine taking the
direct sum of all feature spaces), the more we get
details on the distribution of mention pairs and the
more we can expect to separate positives and neg-
atives accurately. In practice, we have to cope
with data sparsity: there will not be enough data
to properly train a linear model on such a space.
Finally, we seek a feature space situated between
the two extremes of a space that is too big (sparse-
ness) or too small (noisy data). The core of this
work is to define a general method for choosing
the most adequate space F among a huge num-
ber of possibilities when we do not know a priori
which is the best.
2.2.3 Linear models
In this work, we try to linearly separate pos-
itive and negative instances in the large space
F with the Online Passive-Aggressive (PA) algo-
rithm (Crammer et al, 2006): the model learns a
parameter vector w that defines a hyperplane that
cuts the space into two parts. The predicted class
of a pair x with feature vector ?F (x) is given by:
CF (x) := sign(wT ? ?F (x))
Linearity implies an equivalence between: (i)
separating instances of two types, t1 and t2, in two
499
independent models with respective feature spaces
F1 and F2 and parameters w1 and w2, and (ii) a
single model on F1?F2. To see why, let us define
the map:
?F1?F2(x) :=
?
??
??
(
?F1(x)T 0
)T if x typed t1(
0 ?F2(x)T
)T if x typed t2
and the parameter vector w =
(
w1
w2
)
? F1 ?
F2. Then we have:
CF1?F2(x) =
{
CF1(x) if x typed t1
CF2(x) if x typed t2
Now we check that the same property applies
when the PA fits its parameter w. For each new
instance of the training set, the weight is updated
according to the following rule4:
wt+1 = arg min
w?F
1
2 ?w ?wt?
2 s.t. l(w; (xt, yt)) = 0
where l(w; (xt, yt)) = min(0, 1?yt(w??F (xt))),
so that when F = F1 ? F2, the minimum if x is
typed t1 is wt+1 =
(
w1t+1
w2t
)
and if x is typed
t2 is wt+1 =
(
w1t
w2t+1
)
where the wit+1 corre-
spond to the updates in space Fi independently
from the rest. This result can be extended easily
to the case of n feature spaces. Thus, with a deter-
ministic separation of the data, a large model can
be learned using smaller independent models.
2.3 An example: separation by gramtype
To motivate our approach, we first introduce a
simple separation of mention pairs which cre-
ates 9 models obtained by considering all possi-
ble pairs of grammatical types {nominal, name,
pronoun} for both mentions in the pair (a simi-
lar fine-grained separation can be found in (Chen
et al, 2011)). This is equivalent to using 9 differ-
ent feature spacesF1, . . . ,F9 to capture the global
distribution of pairs. With the PA, this is also a sin-
gle model with feature space F = F1 ? ? ? ? ? F9.
We will call it the GRAMTYPE model.
As we will see in Section 5, these separated
models significantly outperform a single model
4The parameter is updated to obtain a margin of a least 1.
It does not change if the instance is already correctly classi-
fied with such margin.
that uses the same base feature set. But we would
like to define a method that adapts a feature space
to the data by choosing the most adequate separa-
tion of pairs.
3 Hierarchizing feature spaces
In this section, we have to keep in mind that sep-
arating the pairs in different models is the same
as building a large feature space in which the pa-
rameter w can be learned by parts in independent
subspaces.
3.1 Indicators on pairs
For establishing a structure on feature spaces, we
use indicators which are deterministic functions
on mention pairs with a small number of outputs.
Indicators classify pairs in predefined categories in
one-to-one correspondence with independent fea-
ture spaces. We can reuse some features of the sys-
tem as indicators, e.g. the grammatical or named
entity types. We can also employ functions that
are not used as features, e.g. the approximate po-
sition of one of the mentions in the text.
The small number of outputs of an indica-
tor is required for practical reasons: if a cate-
gory of pairs is too refined, the associated fea-
ture space will suffer from data sparsity. Accord-
ingly, distance-based indicators must be approxi-
mated by coarse histograms. In our experiments
the outputs never exceeded a dozen values. One
way to reduce the output span of an indicator is
to binarize it like binarizing a tree (many possible
binarizations). This operation produces a hierar-
chy of indicators which is exactly the structure we
exploit in what follows.
3.2 Hierarchies for separating pairs
We define hierarchies as combinations of indi-
cators creating finer categories of mention pairs:
given a finite sequence of indicators, a mention
pair is classified by applying the indicators suc-
cessively, each time refining a category into sub-
categories, just like in a decision tree (each node
having the same number of children as the number
of outputs of its indicator). We allow the classifi-
cation to stop before applying the last indicator,
but the behavior must be the same for all the in-
stances. So a hierarchy is basically a sub-tree of
the complete decision tree that contains copies of
the same indicator at each level.
If all the leaves of the decision tree have the
500
same depth, this corresponds to taking the Carte-
sian product of outputs of all indicators for in-
dexing the categories. In that case, we refer to
product-hierarchies. The GRAMTYPE model can
be seen as a two level product-hierarchy (figure 1).
Figure 1: GRAMTYPE seen as a product-hierarchy
Product-hierarchies will be the starting point of
our method to find a feature space that fits the data.
Now choosing a relevant sequence of indicators
should be achieved through linguistic intuitions
and theoretical work (gramtype separation is one
of them). The system will find by itself the best
usage of the indicators when optimizing the hier-
archy. The sequence is a parameter of the model.
3.3 Relation with feature spaces
Like we did for the GRAMTYPE model, we asso-
ciate a feature space Fi to each leaf of a hierarchy.
Likewise, the sum F = ?iFi defines a large fea-
ture space. The corresponding parameter w of the
model can be obtained by learning the wi in Fi.
Given a sequence of indicators, the number of
different hierarchies we can define is equal to the
number of sub-trees of the complete decision tree
(each non-leaf node having all its children). The
minimal case is when all indicators are Boolean.
The number of full binary trees of height at most
n can be computed by the following recursion:
T (1) = 1 and T (n + 1) = 1 + T (n)2. So
T (n) ? 22n : even with small values of n, the
number of different hierarchies (or large feature
spaces) definable with a sequence of indicators is
gigantic (e.g. T (10) ? 3.8.1090).
Among all the possibilities for a large feature
space, many are irrelevant because for them the
data is too sparse or too noisy in some subspaces.
We need a general method for finding an ade-
quate space without enumerating and testing each
of them.
3.4 Optimizing hierarchies
Let us assume now that the sequence of indicators
is fixed, and let n be its length. To find the best
feature space among a very high number of pos-
sibilities, we need a criterion we can apply with-
out too much additional computation. For that we
only evaluate the feature space locally on pairs,
i.e. without applying a decoder on the output. We
employ 3 measures on pairwise classification re-
sults: precision, recall and F1-score. Now select-
ing the best space for one of these measures can
be achieved by using dynamic programming tech-
niques. In the rest of the paper, we will optimize
the F1-score.
Training the hierarchy Starting from the
product-hierarchy, we associate a classifier and its
proper feature space to each node of the tree5. The
classifiers are then trained as follows: for each in-
stance there is a unique path from the root to a leaf
of the complete tree. Each classifier situated on
the path is updated with this instance. The number
of iterations of the Passive-Aggressive is fixed.
Computing scores After training, we test all the
classifiers on another set of pairs6. Again, a classi-
fier is tested on an instance only if it is situated on
the path from the root to the leaf associated with
the instance. We obtain TP/FP/FN numbers7 on
pair classifications that are sufficient to compute
the F1-score. As for training, the data on which a
classifier at a given node is evaluated is the same
as the union of all data used to evaluate the clas-
sifiers corresponding to the children of this node.
Thus we are able to compare the scores obtained
at a node to the ?union of the scores? obtained at
its children.
Cutting down the hierarchy For the moment
we have a complete tree with a classifier at each
node. We use a dynamic programming technique
to compute the best hierarchy by cutting this tree
and only keeping classifiers situated at the leaf.
The algorithm assembles the best local models (or
feature spaces) together to create larger models. It
goes from the leaves to the root and cuts the sub-
tree starting at a node whenever it does not pro-
5In the experiments, the classifiers use a copy of a same
feature space, but not the same data, which corresponds to
crossing the features with the categories of the decision tree.
6The training set is cut into two parts, for training and
testing the hierarchy. We used 10-fold cross-validation in our
experiments.
7True positives, false positives and false negatives.
501
vide a better score than the node itself, or on the
contrary propagates the score of the sub-tree when
there is an improvement. The details are given in
algorithm 1.
list? list of nodes given by a breadth-first1
search for node in reversed list do
if node.children 6= ? then2
if sum-score(node.children) >3
node.score then
node.TP/FP/FN?4
sum-num(node.children)
else5
node.children? ?6
end7
end8
end9
Algorithm 1: Cutting down a hierarchy
Let us briefly discuss the correctness and com-
plexity of the algorithm. Each node is seen two
times so the time complexity is linear in the num-
ber of nodes which is at least O(2n). However,
only nodes that have encountered at least one
training instance are useful and there are O(n ?
k) such nodes (where k the size of the training
set). So we can optimize the algorithm to run
in time O(n ? k)8. If we scan the list obtained
by breadth-first search backwards, we are ensured
that every node will be processed after its chil-
dren. (node.children) is the set of children of
node, and (node.score) its score. sum-num pro-
vides TP/FP/FN by simply adding those of the
children and sum-score computes the score based
on these new TP/FP/FN numbers. (line 6) cuts the
children of a node when they are not used in the
best score. The algorithm thus propagates the best
scores from the leaves to the root which finally
gives a single score corresponding to the best hi-
erarchy. Only the leaves used to compute the best
score are kept, and they define the best hierarchy.
Relation between cutting and the global feature
space We can see the operation of cutting as re-
placing a group of subspaces by a single subspace
in the sum (see figure 2). So cutting down the
product-hierarchy amounts to reducing the global
initial feature space in an optimal way.
8In our experiments, cutting down the hierarchy was
achieved very quickly, and the total training time was about
five times longer than with a single model.
Figure 2: Cutting down the hierarchy reduces the
feature space
To sum up, the whole procedure is equivalent to
training more than O(2n) perceptrons simultane-
ously and selecting the best performing.
4 System description
Our system consists in the pairwise model ob-
tained by cutting a hierarchy (the PA with selected
feature space) and using a greedy decoder to cre-
ate clusters from the output. It is parametrized by
the choice of the initial sequence of indicators.
4.1 The base features
We used classical features that can be found in
details in (Bengston and Roth, 2008) and (Rah-
man and Ng, 2011): grammatical type and sub-
type of mentions, string match and substring, ap-
position and copula, distance (number of sepa-
rating mentions/sentences/words), gender/number
match, synonymy/hypernym and animacy (using
WordNet), family name (based on lists), named
entity types, syntactic features (gold parse) and
anaphoricity detection.
4.2 Indicators
As indicators we used: left and right grammati-
cal types and subtypes, entity types, a boolean in-
dicating if the mentions are in the same sentence,
and a very coarse histogram of distance in terms of
sentences. We systematically included right gram-
type and left gramtype in the sequences and added
other indicators, producing sequences of different
lengths. The parameter was optimized by docu-
ment categories using a development set after de-
coding the output of the pairwise model.
4.3 Decoders
We tested 3 classical greedy link selection strate-
gies that form clusters from the classifier decision:
Closest-First (merge mentions with their closest
coreferent mention on the left) (Soon et al, 2001),
502
Best-first (merge mentions with the mention on
the left having the highest positive score) (Ng
and Cardie, 2002; Bengston and Roth, 2008), and
Aggressive-Merge (transitive closure on positive
pairs) (McCarthy and Lehnert, 1995). Each of
these decoders is typically (although not always)
used in tandem with a specific sampling selec-
tion at training. Thus, Closest-First for instance is
used in combination with a sample selection that
generates training instances only for the mentions
that occur between the closest antecedent and the
anaphor (Soon et al, 2001).
P R F1
SINGLE MODEL 22.28 63.50 32.99
RIGHT-TYPE 29.31 45.23 35.58
GRAMTYPE 39.12 45.83 42.21
BEST HIERARCHY 45.27 51.98 48.40
Table 1: Pairwise scores on CoNLL-2012 test.
5 Experiments
5.1 Data
We evaluated the system on the English part of the
corpus provided in the CoNLL-2012 Shared Task
(Pradhan et al, 2012), referred to as CoNLL-2012
here. The corpus contains 7 categories of doc-
uments (over 2K documents, 1.3M words). We
used the official train/dev/test data sets. We evalu-
ated our system in the closed mode which requires
that only provided data is used.
5.2 Settings
Our baselines are a SINGLE MODEL, the GRAM-
TYPE model (section 2) and a RIGHT-TYPE
model, defined as the first level of the gramtype
product hierarchy (i.e. grammatical type of the
anaphora (Morton, 2000)), with each greedy de-
coder and also the original sampling with a single
model associated with those decoders.
The hierarchies were trained with 10-fold cross-
validation on the training set (the hierarchies are
cut after cumulating the scores obtained by cross-
validation) and their parameters are optimized by
document category on the development set: the
sequence of indicators obtaining the best average
score after decoding was selected as parameter for
the category. The obtained hierarchy is referred to
as the BEST HIERARCHY in the results. We fixed
the number of iterations for the PA for all models.
In our experiments, we consider only the gold
mentions. This is a rather idealized setting but our
focus is on comparing various pairwise local mod-
els rather than on building a full coreference reso-
lution system. Also, we wanted to avoid having to
consider too many parameters in our experiments.
5.3 Evaluation metrics
We use the three metrics that are most commonly
used9, namely:
MUC (Vilain et al, 1995) computes for each
true entity cluster the number of system clusters
that are needed to cover it. Precision is this quan-
tity divided by the true cluster size minus one. Re-
call is obtained by reversing true and predicated
clusters. F1 is the harmonic mean.
B3 (Bagga and Baldwin, 1998) computes recall
and precision scores for each mention, based on
the intersection between the system/true clusters
for that mention. Precision is the ratio of the in-
tersection and the true cluster sizes, while recall is
the ratio of the intersection to the system cluster
sizes. Global recall, precision, and F1 scores are
obtained by averaging over the mention scores.
CEAF (Luo, 2005) scores are obtained by com-
puting the best one-to-one mapping between the
system/true partitions, which is equivalent to find-
ing the best optimal alignment in the bipartite
graph formed out of these partitions. We use the
?4 similarity function from (Luo, 2005).
These metrics were recently used in the CoNLL-
2011 and -2012 Shared Tasks. In addition, these
campaigns use an unweighted average over the F1
scores given by the three metrics. Following com-
mon practice, we use micro-averaging when re-
porting our scores for entire datasets.
5.4 Results
The results obtained by the system are reported in
table 2. The original sampling for the single model
associated to Closest-First and Best-First decoder
are referred to as SOON and NGCARDIE.
The P/R/F1 pairwise scores before decoding are
given in table 1. BEST HIERARCHY obtains a
strong improvement in F1 (+15), a better precision
and a less significant diminution of recall com-
pared to GRAMTYPE and RIGHT-TYPE.
9BLANC metric (Recasens and Hovy, 2011) results are
not reported since they are not used to compute the CoNLL-
2012 global score. However we can mention that in our ex-
periments, using hierarchies had a positive effect similar to
what was observed on B3 and CEAF.
503
MUC B3 CEAF
Closest-First P R F1 P R F1 P R F1 Mean
SOON 79.49 93.72 86.02 26.23 89.43 40.56 49.74 19.92 28.44 51.67
SINGLE MODEL 78.95 75.15 77.0 51.88 68.42 59.01 37.79 43.89 40.61 58.87
RIGHT-TYPE 79.36 67.57 72.99 69.43 56.78 62.47 41.17 61.66 49.37 61.61
GRAMTYPE 80.5 71.12 75.52 66.39 61.04 63.6 43.11 59.93 50.15 63.09
BEST HIERARCHY 83.23 73.72 78.19 73.5 67.09 70.15 47.3 60.89 53.24 67.19
MUC B3 CEAF
Best-First P R F1 P R F1 P R F1 Mean
NGCARDIE 81.02 93.82 86.95 23.33 93.92 37.37 40.31 18.97 25.8 50.04
SINGLE MODEL 79.22 73.75 76.39 40.93 75.48 53.08 30.52 37.59 33.69 54.39
RIGHT-TYPE 77.13 65.09 70.60 48.11 66.21 55.73 31.07 47.30 37.50 54.61
GRAMTYPE 77.21 65.89 71.1 49.77 67.19 57.18 32.08 47.83 38.41 55.56
BEST HIERARCHY 78.11 69.82 73.73 53.62 70.86 61.05 35.04 46.67 40.03 58.27
MUC B3 CEAF
Aggressive-Merge P R F1 P R F1 P R F1 Mean
SINGLE MODEL 83.15 88.65 85.81 35.67 88.18 50.79 36.3 28.27 31.78 56.13
RIGHT-TYPE 83.48 89.79 86.52 36.82 88.08 51.93 45.30 33.84 38.74 59.07
GRAMTYPE 83.12 84.27 83.69 44.73 81.58 57.78 45.02 42.94 43.95 61.81
BEST HIERARCHY 83.26 85.2 84.22 45.65 82.48 58.77 46.28 43.13 44.65 62.55
Table 2: CoNLL-2012 test (gold mentions): Closest-First, Best-First and Aggressive-Merge decoders.
Despite the use of greedy decoders, we observe
a large positive effect of pair separation in the
pairwise models on the outputs. On the mean
score, the use of distinct models versus a sin-
gle model yields F1 increases from 6.4 up to 8.3
depending on the decoder. Irrespective of the
decoder being used, GRAMTYPE always outper-
forms RIGHT-TYPE and single model and is al-
ways outperformed by BEST HIERARCHY model.
Interestingly, we see that the increment in pair-
wise and global score are not proportional: for
instance, the strong improvement of F1 between
RIGHT-TYPE and GRAMTYPE results in a small
amelioration of the global score.
Depending on the document category, we found
some variations as to which hierarchy was learned
in each setting, but we noticed that parameters
starting with right and left gramtypes often pro-
duced quite good hierarchies: for instance right
gramtype ? left gramtype ? same sentence ?
right named entity type.
We observed that product-hierarchies did not
performed well without cutting (especially when
using longer sequences of indicators, because of
data sparsity) and could obtain scores lower than
the single model. Hopefully, after cutting them the
results always became better as the resulting hier-
archy was more balanced.
Looking at the different metrics, we notice that
overall, pair separation improves B3 and CEAF
(but not always MUC) after decoding the output:
GRAMTYPE provides a better mean score than the
single model, and BEST HIERARCHY gives the
highest B3, CEAF and mean score.
The best classifier-decoder combination reaches
a score of 67.19, which would place it above the
mean score (66.41) of the systems that took part
in the CoNLL-2012 Shared Task (gold mentions
track). Except for the first at 77.22, the best
performing systems have a score around 68-69.
Considering the simple decoding strategy we em-
ployed, our current system sets up a strong base-
line.
6 Conclusion and perspectives
In this paper, we described a method for select-
ing a feature space among a very large number of
choices by using linearity and by combining indi-
cators to separate the instances. We employed dy-
namic programming on hierarchies of indicators
to compute the feature space providing the best
pairwise classifications efficiently. We applied this
504
method to optimize the pairwise model of a coref-
erence resolution system. Using different kinds
of greedy decoders, we showed a significant im-
provement of the system.
Our approach is flexible in that we can use a va-
riety of indicators. In the future we will apply the
hierarchies on finer feature spaces to make more
accurate optimizations. Observing that the gen-
eral method of cutting down hierarchies is not re-
stricted to modeling mention pairs, but can be ap-
plied to problems having Boolean aspects, we aim
at employing hierarchies to address other tasks in
computational linguistics (e.g. anaphoricity detec-
tion or discourse and temporal relation classifica-
tion wherein position information may help sepa-
rating the data).
In this work, we have only considered standard,
heuristic linking strategies like Closest-First. So,
a natural extension of this work is to combine our
method for learning pairwise models with more
sophisticated decoding strategies (like Bestcut or
using ILP). Then we can test the impact of hierar-
chies with more realistic settings.
Finally, the method for cutting hierarchies
should be compared to more general but similar
methods, for instance polynomial kernels for SVM
and tree-based methods (Hastie et al, 2001). We
also plan to extend our method by breaking the
symmetry of our hierarchies. Instead of cutting
product-hierarchies, we will employ usual tech-
niques to build decision trees10 and apply our cut-
ting method on their structure. The objective is
twofold: first, we will get rid of the sequence of
indicators as parameter. Second, we will avoid
fragmentation or overfitting (which can arise with
classification trees) by deriving an optimal large
margin linear model from the tree structure.
Acknowledgments
We thank the ACL 2013 anonymous reviewers for
their valuable comments.
References
M. Ariel. 1988. Referring and accessibility. Journal
of Linguistics, pages 65?87.
A. Bagga and B. Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of
LREC 1998, pages 563?566.
10(Bansal and Klein, 2012) show good performances of de-
cision trees on coreference resolution.
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics: Long Papers-Volume 1, pages
389?398. Association for Computational Linguis-
tics.
Eric Bengston and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of EMNLP 2008, pages 294?303, Hon-
olulu, Hawaii.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
COLING, pages 143?151.
Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim
Tan. 2011. A unified event coreference resolu-
tion by integrating multiple resolvers. In Proceed-
ings of 5th International Joint Conference on Nat-
ural Language Processing, pages 102?110, Chiang
Mai, Thailand, November. Asian Federation of Nat-
ural Language Processing.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551?585.
Pascal Denis and Jason Baldridge. 2008. Specialized
models and ranking for coreference resolution. In
Proceedings of EMNLP 2008, pages 660?669, Hon-
olulu, Hawaii.
Pascal Denis and Jason Baldridge. 2009. Global joint
models for coreference resolution and named entity
classification. Procesamiento del Lenguaje Natural,
43.
Trevor Hastie, Robert Tibshirani, and J. H. Friedman.
2001. The elements of statistical learning: data
mining, inference, and prediction: with 200 full-
color illustrations. New York: Springer-Verlag.
A. Kehler, D. Appelt, L. Taylor, and A. Simma. 2004.
The (non)utility of predicate-argument frequencies
for pronoun interpretation. In Proceedings of HLT-
NAACL 2004.
M. Klenner. 2007. Enforcing coherence on corefer-
ence sets. In Proceedings of RANLP 2007.
X. Luo. 2005. On coreference resolution performance
metrics. In Proceedings of HLT-NAACL 2005, pages
25?32.
J. F. McCarthy and W. G. Lehnert. 1995. Using de-
cision trees for coreference resolution. In IJCAI,
pages 1050?1055.
T. Morton. 2000. Coreference for NLP applications.
In Proceedings of ACL 2000, Hong Kong.
V. Ng and C. Cardie. 2002. Improving machine learn-
ing approaches to coreference resolution. In Pro-
ceedings of ACL 2002, pages 104?111.
505
V. Ng. 2005. Supervised ranking for pronoun resolu-
tion: Some recent improvements. In Proceedings of
AAAI 2005.
Cristina Nicolae and Gabriel Nicolae. 2006. Best-
cut: A graph algorithm for coreference resolution.
In EMNLP, pages 275?283.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the HLT 2006, pages 192?199, New York
City, N.Y.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1?40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Altaf Rahman and Vincent Ng. 2011. Narrowing the
modeling gap: a cluster-ranking approach to coref-
erence resolution. J. Artif. Int. Res., 40(1):469?521.
Recasens and Hovy. 2011. Blanc: Implementing the
rand index for coreference evaluation. Natural Lan-
guage Engineering, 17:485?510, 9.
W. M. Soon, H. T. Ng, and D. Lim. 2001. A
machine learning approach to coreference resolu-
tion of noun phrases. Computational Linguistics,
27(4):521?544.
Olga Uryupina, Massimo Poesio, Claudio Giuliano,
and Kateryna Tymoshenko. 2011. Disambiguation
and filtering methods in using web knowledge for
coreference resolution. In FLAIRS Conference.
O. Uryupina. 2004. Linguistically motivated sample
selection for coreference resolution. In Proceedings
of DAARC 2004, Furnas.
Yannick Versley, Alessandro Moschitti, Massimo Poe-
sio, and Xiaofeng Yang. 2008. Coreference systems
based on kernels methods. In COLING, pages 961?
968.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proceedings fo the 6th Message
Understanding Conference (MUC-6), pages 45?52,
San Mateo, CA. Morgan Kaufmann.
506
Proceedings of the SIGDIAL 2013 Conference, pages 2?11,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Expressivity and comparison of models of discourse structure
Antoine Venant1 Nicholas Asher2 Philippe Muller1 Pascal Denis3 Stergos Afantenos1
(1) IRIT, Toulouse University, France, (2) IRIT, CNRS, France (3) Mostrare, INRIA, France ?
Abstract
Several discourse annotated corpora now ex-
ist for NLP. But they use different, not eas-
ily comparable annotation schemes: are the
structures these schemes describe incompati-
ble, incomparable, or do they share interpre-
tations? In this paper, we relate three types
of discourse annotation used in corpora or dis-
course parsing: (i) RST, (ii) SDRT, and (iii)
dependency tree structures. We offer a com-
mon language in which their structures can be
defined and furnished a range of interpreta-
tions. We define translations between RST and
DT preserving these interpretations, and intro-
duce a similarity measure for discourse repre-
sentations in these frameworks. This will en-
able researchers to exploit different types of
discourse annotated data for automated tasks.
1 Introduction
Computer scientists and linguists now largely agree
that representing discourse structure as a hierarchical
relational structure over discourse units linked by dis-
course relations is appropriate to account for a variety
of interpretative tasks. There is also some agreement
over the taxonomy of discourse relations ?almost all
current theories include expressions that refer to rela-
tions like Elaboration, Explanation, Result, Narration,
Contrast, Attribution. Sanders, Spooren, and Noord-
man 1992; Bateman and Rondhuis 1997 discuss corre-
spondences between different taxonomies.
Different theories, however, assume different sets of
constraints that govern these representations; some ad-
vocate trees: RST Mann and Thompson 1987, DLTAG
Webber et al 1999; others, graphs of different sorts:
SDRT Asher and Lascarides 2003, Graphbank Wolf
and Gibson 2005. Consider:
(1) [?he was a very aggressive firefighter.]C1 [he
loved the work he was in,?]C2 [said acting fire
chief Lary Garcia.]C3 . [?He couldn?t be bested
in terms of his willingness and his ability to do
something to help you survive.?]C4 (from Egg
and Redeker 2010)
Using RST, Egg and Redeker 2010 provide the tree an-
notated with nuclearity features for this example (given
by the linear encoding in (s1)), while SDRT provides
?This research was supported by ERC grant 269427.
a different kind of structure (s2). Dependency trees
(DTs), similar to syntactic dependency trees and used
in Muller et al 2012 for automated parsing, give yet an-
other representation (s3). Elab stands for elaboration,
Attr for attribution, and Cont for continuation.
Elab1(Attr(Elab2(C1N ,C2S )N ,C3S )N ,C4S ) (s1)
Attr(pi,C3) ? pi :Elab(C1, pi1) ? pi1 :Cont(C2,C4) (s2)
Elab1(C1,C2) ? Attr(C1,C3) ? Elab(C1,C4) (s3)
Several corpora now exist annotated with such struc-
tures: RSTTB Carlson, Marcu, and Okurowski 2002,
Discor Baldridge, Asher, and Hunter 2007, Graph-
Bank1. But how exactly do these annotations compare?
In the illustrative example chosen and for the relation
types they agree on (Elaboration and Attribution), dif-
ferent annotation models and theoretical frameworks
invoke different numbers of instances of these relations
and assign the instances different arguments or differ-
ent scopes, at least on the surface. In this paper we de-
velop a method of comparing the scopes of relations in
different types of structures by developing a notion of
interpretation shared between different structures. This
interpretation specifies the set of possible scopes of re-
lations compatible with a given structure. This theoret-
ical work is important for furthering empirical research
on discourse. Discourse annotations are expensive. It
behooves researchers to use as much data as they can,
annotated in several formalisms, while pursuing pre-
diction or evaluation in their chosen theory. This paper
provides a theoretical basis to do this.
What a given structure expresses exactly is often not
clear; some discourse theories are not completely for-
malized or lack a worked out semantics. Neverthe-
less, in all of them rhetorical relations have semantic
consequences bearing on tasks like text summarization,
textual entailment, anaphora resolution, as well as the
temporal, spatial and thematic organization of a text
Hobbs, Stickel, and Martin 1993; Kehler 2002; Asher
1993; Lascarides and Asher 1993; Hobbs, Stickel, and
Martin 1993; Hitzeman, Moens, and Grover 1995, inter
alia. Theories like SDRT or Polanyi et al 2004 adopt a
conception of discourse structure as logical form. Dis-
course structures are like logical formulae and relations
1The Penn Discourse Treebank Prasad et al 2008 could
also be considered as a corpus with partial dependency struc-
tures.
2
function like logical operators on the meaning of their
arguments. Hence their exact scope has great semantic
impact on the phenomena we have mentioned, in ex-
actly the way the relative scope of quantifiers make a
great semantic difference in first order logic. By con-
centrating on exact meaning representations, however,
the syntax-semantics interface becomes quite complex:
as happens with quantifiers at the intra sentential level,
discourse relations might semantically require a scope
that is, at least a priori, not determined by syntactic
considerations alone and violates surface order (see s2).
Other theories like Polanyi?s Linguistic Discourse
Model (LDM) of Polanyi 1985; Polanyi and Scha 1984,
and DLTAG Webber et al 1999 explicitly adopt a
syntactic point of view, and RST with strongly con-
strained (tree-shaped) structures is subject to parsing
approaches duVerle and Prendinger 2009; Sagae 2009;
Subba and Di Eugenio 2009 that adhere to the syntac-
tic approach in adopting decoding strategies of syntac-
tic parsing. In such theories, discourse structure repre-
sentations, subject to syntactic constraints (e.g. domi-
nance of spans of text one over another) respect surface
order but do not always and unproblematically yield a
semantic interpretation that fits intuitions. According
to Marcu 1996, an RST tree is not by itself sufficient to
generate desired predictions; he employs the nuclearity
principle, NP, as an additional interpretation principle
on scopes of relations.
We focus on two theories: RST, which offers the
model for the annotations of the RST treebank Carl-
son, Marcu, and Okurowski 2002 and the Potsdam
commentary corpus Stede 2004, and on SDRT, which
counts several small corpora annotated with semantic
scopes, Discor Baldridge, Asher, and Hunter 2007 and
Annodis Afantenos et al 2012. We describe these the-
ories in section 2. We will also compare these two the-
ories to dependency tree representations of discourse
Muller et al 2012. Section 3 introduces a language for
describing semantics scopes of relations that is power-
ful enough to: i) compare the expressiveness (in terms
of what different scopes can be expressed) of the dif-
ferent formalisms considered; ii) give a formal target
language that will provide comparable interpretations
of the different structures at stake. Section 4 discusses
Marcu?s nuclearity principle and proposes an alterna-
tive way to interpret an RST tree as a set of different
possible scopes expressed in our language. Section 5
provides intertranslability results between the different
formalisms. Section 6 defines a measure of similarity
over discourse structures in different formalisms.
2 Discourse formalisms
These formalisms we introduce here all require the in-
put text to be segmented into elementary units (EDUs).
The definition of what an EDU is varies slightly with
the formalism, but roughly corresponds to the clause
level in RST, SDRT and other theories. We assume a
segmentation common to the different formalisms and
use examples with a non controversial and intuitive
segmentation.
Rhetorical Structure Theory (RST), the theory un-
derlying the RST-Treebank is the most used corpus for
discourse parsing, cf. duVerle and Prendinger 2009,
Subba and Di Eugenio 2009, inter alia.
In its Mann and Thompson 1987 formulation, RST
builds a descriptive tree for the discourse by the recur-
sive application of schemata in a bottom-up procedure.
Each schema application ideally reflects the most plau-
sible relation the writer intended between two contigu-
ous spans of text, as well as hierarchical information
about the arguments of the relation, distinguishing be-
tween nuclei as essential arguments of a relation and
satellites as more contingent parts. The set of RS Trees
is inductively defined as follows:
1- An EDU is a RS Tree.
2- if R is a nucleus-statellite relation symbol, s1 and
s2 are both RS Trees with contiguous spans (the left-
most leaf in s2 is textually located right after the right-
most one in s1), and ?a1, a2? ? {?N, S ?; ?S ,N?} then
R(t1 a1, t2 a2) is an RS Tree.
3- if R is a multinuclear relation symbol and
?s1, . . . , sn? are n RS Trees with contiguous spans then
R(s1 N, . . . , sn N) is an RS Tree.
Following Mann and Thompson 1987 a complete RS
tree makes explicit the content the author intended to
communicate. RS Trees are graphically represented
Marcu 1996 with intermediate nodes labelled with re-
lation names, leaves with symbols referring to EDUs,
and edges with nucleus/satellite distinctions.
Segmented Discourse Representation Theory
(SDRT), our second case-study theory, inherits a
framework from dynamic semantics and enriches
it with rhetorical relations. The set of SDRSs is
inductively defined as follows:
Assume a set of rhetorical relations R, distinguished
between coordinating and subordinating relations.
- Any EDU is an SDRS.
- Any Complex Discourse Unit (CDU) is a SDRS.
- a CDU is an acyclic labelled graph (A, E) where
every node is a discourse unit (DU) or SDRS and each
labelled edge is a discourse relation such that:
(i) every node is connected to some other node;
(ii) no two nodes are linked by subordinating and co-
ordinating relations,
(iii) given EDUs a1, . . . , an+1 in their textual order
that yield a CDU (A, E) = G, each EDU a j+1 j < n is
linked either: (a) to nodes on the right frontier of the
CDU G? a subgraph of G constructed from a1, . . . , a j;
or (b) to one or more nodes in G? = (A?,G?), a subgraph
of G, which linked to one or more nodes on the right
frontier of the graph G?, and where G? is constructed
from a subset of a j+2, . . . an.
The right frontier of a graph G consists of the nodes
a that are not the left arguments to any coordinating
relation and for which if any node b is linked to some
node dominating a, then there is a path of subordinating
3
relations from b to a.
A Segmented Discourse Representation Structure
(SDRS), is assigned a recursively computed meaning
in terms of context-change potential (relation between
pairs of ? world, assignation function ?) in the tradi-
tion of dynamic semantics. The semantics of a complex
constituent is compositionally defined from the seman-
tics of rhetorical relations and the interpretation of its
subconstituents. In the base case of an EDU, the se-
mantics is given in dynamic semantics.
We also consider dependency trees (DTs). Muller
et al 2012 derive DTs from the SDRSs of the ANN-
ODIS corpus to get a reduced search space, simplify-
ing automated discourse parsing. A DT is an SDRS
in which there are no CDUs and there is a unique arc
between any two nodes. Muller et al 2012 provide
a procedure from SDRSs to DTs, which we slightly
modify to respect the Frontier Contraint that they use.
? works in a bottom-up fashion replacing every CDU
X that is an argument of a rhetorical relation in ? by
their top-most immediate sub-constituent which do not
appear on the right of any relation in X, or distributing
the top relation when necessary to preserve projectivity.
To give a simple example: ?(R([R?(a, [R??(b, c)])], d)) =
?(R([R?(a, b) ? R??(b, c)], d)) = R(a, d) ? R?(a, b) ?
R??(b, c). (1) provides a more complicated example we
discuss in Section 6).
3 Describing the scope of relations
We provide here a language expressive and general
enough to express the structures of the 3 theories. All
our case-study theories involve structures described by
a list of rhetorical relations and their arguments. Two
things may vary: first, the nature of the arguments.
SDRT for instance, introduces complex constituents
as arguments of relations (e.g.
{
pi : Rsubord(b, c)
Rsubord(a, pi) ),
which finds a counterpart within RS Trees, where a
relation may directly appear as argument of another
(R(aN ,R(bN , cS )S )) but not within dependency trees.
Second, the set of constraints that restrict the possi-
ble lists of such relations can vary across theories (e.g.
right frontier, or requirement for a tree structure).
To deal with the first point above, we remark that
it suffices to list, for each instance of a discourse rela-
tion, the set of elementary constituents that belong to its
left and right scope in order to express the three kinds
of structures. We do this in a way that an isomorphic
structure can always be recovered. Models of our com-
mon language will be a list of relation instances and el-
ementary constituents, together with a set of predicates
stating what is in the scope of what. As for the second
point, we axiomatize each constraint in our common
language, thereby describing each of the 3 types of dis-
course structures as a theory in our language.
Our language contains only binary relations. Among
discourse formalisms, only RST makes serious (and
empirical) use of n?ary discourse relations. Neverthe-
less, such RST structures are expressible in our frame-
work, if we assume certain semantic equivalences.
RST allows for two cases of non-binary trees: (i) nu-
cleus with n satellites, each one linked to the nucleus
by some relation Rn. Such a structure is semantically
equivalent to the conjunction of n-binary relations Rn
between the nucleus and the nth satellite, which is ex-
pressible in our framework. (ii) RST also allows for n-
ary multinuclear relations such as List and Sequence. In
our understanding, multinuclear relations R(a1, . . . an),
essentially serve a purpose of expressiveness, and such
an n-ary tree is an equivalent to the split non-tree
shaped structure R(a1, a2) ? R(a2, a3) . . .R(a(n?1), an).
This seems clear for the Sequence relation, which
states that a1 . . . an are in temporal sequence and can
be equivalently formulated as ?each ai precedes ai+1?.
This might appear less obvious for the List relation.
The semantics (as it appears on the RST website http:
//www.sfu.ca/rst/) of this relation requires the ai to
be ?comparable?, and as far as this is a transitive prop-
erty, we can split the relation into a set of binary ones.
Formally, our scope language Lscopes is a fragment of
that of monadic second order logic with two sorts of in-
dividuals: relation instances (i), and elementary consti-
tuants (l). Below, we assume R is the set of all relation
names (elaboration, narration, justification, . . . ).
Definition 1 (Scoping language). Let S be the set {i, l}.
The set of primitive, disjoint types of Lscopes consists of
i, l and t (type of formulae). For each of the types in
S , we have a countable set of variable symbols Vi (Vl).
Two additional countable sets of variable symbols V?i,t?
and V?l,t? range over sets of individuals. These four sets
of variable symbols are pairwise disjoint.
The alphabet of our language is constituted by Vi, Vs,
a set of predicates, equality, connector and quantifier
symbols. The set of predicate symbols is as follows:
1) For each relation symbol r in R, LR is a unary
predicate of type ?i, t??i.e., LR : ?i, t? .
2) unary predicates, sub, coord and sub?1 : ?i, t?.
3) binary predicates ?l and ?r : ?i, l, t?.
4) two equality relations, =s : ?s, s, t? for s ? {i, l}.
Logical connectors, and quantifiers are as usual.
The sets of terms ?i,?l and ?t are recursively defined:
1. Vi ? ?i, Varl ? ?l. 2. For v ? Vs,t, v : ?s, t?. 3. For
each symbol ? of type ?u1, . . . , un? in the alphabet, for
all (t1, . . . , tn?1) ? ?u1?? ? ???un?1, ?[t1, . . . , tn?1] ? ?un .
?t is the set of well formed formulae of the scope lan-
guage.
The predicates ?l and ?r take a relation instance r of
type i and a elementary constituent x of type l as argu-
ments. Intuitively, they mean that x has to be included
in the left (for ?l) or right (for ?r) scope of r. For each
relation symbol R such as justification or elaboration,
the predicate LR takes a relation instance r has argu-
ment and states that r is an instance of the rhetorical re-
lation R. Predicates sub, coord and sub?1 apply to a re-
lation instance r, respectively specifying that r?s left ar-
gument hierarchically dominate its right argument, that
4
both are of equal hierarchical importance, or that the
left one is subordinate to the right one.
Definition 2 (Scope structure and Interpretation).
A scope structure is an Lscopes-structure M =
?Di,Dl, |.|M?. Di and Dl are disjoint sets of individu-
als for the sorts i and l respectively, and |.|M assigns to
each predicate symbol P of type ?u1, . . . , un, t? a func-
tion |.|P : Du1?? ? ??Dun 7? {0, 1}. Variables of type ?i, t?
are assigned subsets of Di and similarly for variables of
type ?l, t?, The predicates =i and =s are interpreted as
equality over Di and Dl respectively.
The interpretation ~?Mv of a formula ? ? ?S is the
standard interpretation of a monadic second order for-
mula w.r.t to a model and a valuation (interpretation of
first order quantifiers and connectors is as usual, quan-
tification over sets is over all sets of individuals). Va-
lidity |= also follows the standard definition.
These scope structures offer a common framework
for different discourse formalisms. Given one of the
three formalisms, we say that two structures S 1 and S 2
are equivalent iff there is an encoding from one struc-
ture into a scoped structure or set of scoped structures
and a decoding back from the scoped structure or set of
scoped structures into S 2
Fact 1. One can define two algorithms I and E such
that:
? from a given structure s which is a RS Tree, a
SDRS or a DT, I computes a scope structure I(s).
? given such a computed structure, E allow to re-
trieve the original structure s (E(I(s)) = s).
RST Encoding and Decoding To flesh out I and E
for RST, we need to define dominance. Set lArgs(r) =
{e ? Dl | (r, e) ? |?l|M}; rArgs(r) is defined analogously
(where ?r replaces ?l). The left and right dominance
relations vl and vr are defined as follows: r vl r? iff
(Args(r) ? lArgs(r?)).
- r vl r? ? ?z : l((z ?l r)? z ?r r))? z ?l r?) with r vr r?
defined analogously.
Dominance v is: v=vl ? vr.
- lArgs(r, X)??z : l(z ?l r) ? z ? X), with rArgs(r, X)
similar and
-Args(r, X)? ?z : l((z ?l r) ? z ?r r))? z ? X).
The NS, NN and NS schemes of RST will be re-
spectively encoded by the predicates sub, coord and
sub?1. We proceed recursively. If t is an EDU e, re-
turn Mt = ?Di = ?,Dl = {e}, ? where  is the inter-
pretation that assigns the empty set to each predicate
symbol. If the root of t is a binary node instantiating
a relation R(t1a1 , t2a2 ), let Tr ? {sub, coord, sub?1} bethe predicate that encodes the schema a1a2, let Mt1 =
?D1i ,D1l , |.|1? and Mt2 = ?D2i ,D2l , |.|2?. The algorithm re-turns Mt = ?D1i ? D2i ? {r},D1l ? D2l , |.|Mt ? where r is a?fresh? relation instance variable not in D1i or D2i , and
|.|Mt is updated in the appropriate fashion to reflect the
left and right arguments of r. Finally, if the root of t is
an n-ary node, split it into a sequence of binary relation
R1(t1, t2),R2(t2, t3), . . . , proceed to recursively compute
the scope-structures Mi for each of the relations using
2 (take care to introduce a ?fresh? relation instance in-
dividual for each relation of the sequence), then return
the union of the models Mi.
RST Decoding Given a finite scope structure M =
?Di,Dl, |.|M?, for each relation instance r compute the
left arguments of r and its right arguments. We then
identify L(r), the unique relation symbol R such that
r ? |LR|M. If that fails, the algorithm fails. Similarly
retrieve the right nuclearity schema from the adequate
predicate that applies to r. Then compute the domi-
nance relations for r. If the input structure M = I(t)
for some RS Tree t then there is at least one maximal
relation instance for the dominance relation. If t the
root node of t is a binary relation, there is exactly one
maximal element in the dominance relation. If there
is none, then we return fail. If there is exactly one,
recursively compute the two RS Trees obtained from
the models computed from the left and right arguments
and descendants of r. If there is more than one, the root
node of the encoded RS Tree was a n-ary relation and
one has to reconstruct the n-ary node if that is possi-
ble; if not the algorithm fails (but that means the input
structure was not obtained from a valid RS Tree).
SDRT Encoding and Decoding: This is similar to
the RST encoding and decoding; for the encoding al-
gorithm, we proceed recursively top down. A SDRS
s is a complex constituent that contents a graph g =
?V, E? whose edges are relations holding between sub-
constituents, simple or complex as well. First come
up with an encoding of the set E of all edges that
hold between two sub-constituents of s, i.e. a struc-
ture M = ?Di = Ei,Dl = V, {LR}, ?l, ?r?, where, for
each edge e ? Ei, LR encodes its relation type, and
?l1 and ?r1 consists of all the pairs (x, e) of left and
right nodes x of the edges e ? E. Finally, for each
complex immediate sub-constituent of s in Dl, update
M as follows: for c such a subconsituent, recursively
compute its encoding Mc, then add everything of Mc
to M, finally remove c from M but add instead for
each relation r scoping over c to the right (left), all
the pairs {(r, x) | x is a constituent in Mc}. The decod-
ing works again similarly to the one for RST, top-down
once again: one recursively retrieves immediate con-
tent of the current complex constituent at each level
then moves to inner constituents.
DT: Dependency trees are syntactically a special case
of SDRSs; there is only one CDU whose domain is
only EDUs.
The scope language allows us to axiomatize three
classes of scope structures corresponding to RS Trees,
SDRSs and DTs. Not every scope structure will yield
a RS Tree when fed to the RST decoding algorithm,
only those obtainable from encoding an RS tree. As not
all scope structures obey these axioms, our language is
5
strictly more expressive than any of these discourse for-
malisms.
As an example of an axiom, the following formula
expresses that a relation cannot have both left and right
scope over the same elementary constituent:
Strong Irreflexivity:
?r : i?x : l?(x ?l r ? x ?r r)) (A0)
Strong irreflexivity entails irreflexivity; a given relation
instance cannot have the same (complete) left and right
scopes. All discourse theories validate A0.
In the Appendix, we define left and right strong dom-
inance relations vl(r) as well as n-ary RS trees and
CDUs of SDRT. We exploit these facts in the Appendix
to express axioms (A1-A9) that axiomatize the struc-
tures corresponding to RST, SDRT and DTs. Axiom
A1 says that every discourse unit is linked via some dis-
course relation instance. Axiom A2 insures that all our
relation instances have the right number of arguments;
Axioms A3 and A4 ensure acyclicity and no crossing
dependencies. A5a and A5b restrict structures to a tree-
like dominance relation with a maximal dominating el-
ement, while A6 defines the Right Frontier constraint
for SDRT, and A7 fixes the domain for SDRT con-
straints on CDUs. A8 ensures that no coordinating and
subordinating relations have the same left and right ar-
guments, while A9 provide the restrictions needed to
define the set of DTs. We use the encoding and decod-
ing maps to show:
Fact 2.
1. The theory TRS T ={A0, A1, A2, A3, A4, A5a, A5b, A8}
characterizes RST structures in the sense that:
- E applied to any structure M such that M |= TRS T
yield an RST Tree.
- for any RST Tree t, I(t) |= TRS T .
2. The theory TS DRT ={A0, A1, A2, A3, A6, A7, A8}
similarly characterizes SDRSs.
3. The theory TDT =TS DRT ? {A9a, A9b} similarly
characterizes Dependency Trees structures.
4 Different Interpretations of Scope
The previous section defined the set of scope structures
as well as the means to import, and then retrieve, RS
trees, DTs, or SDRs into, and from, this set. Some of
these scope structures export both into RST and SDRT,
yielding a 1 ? 1 correspondence between a subset of
SDRT and RST structures. But what does this corre-
spondence actually tell us about these two structures?
In mathematics, the existence of an isomorphism relies
on a bijection that preserves structure. Our correspon-
dence preserves the immediate interpretation of the se-
mantic scopes of relations.
Immediate Interpretation Consider a scope struc-
tureM (validating A0, A1, A2). The predicates lArgs(r)
and rArgs(r) are the sets of all units in the left or right
scope of a relation instance r. Whether r, labelled by
relation name R holds of two discourse units or not
in M, depends on the semantic content of its left and
right arguments, recursively described by lArgs(r) and
all relations r? such that r? @l r, and rArgs(r) and all
relations r? such that r? @r r. Algorithm I computes
what we call the immediate interpretation of an input
structure. Intuitively, in this interpretation the semantic
scope of relations is directly read from the structures
themselves; a node R(t1, t2) in a RS Tree expresses that
R holds between contents expressed by the whole sub-
structures t1 and t2. Similarly, for SDRT and DTs, im-
mediate interpretation of an edge pi1 ?R pi2 is that R
holds between the whole content of pi1 and pi2.
While this immediate interpretation is standard in
SDRT, it is not in RST. Consider again (1) from the
introduction or:
(2) [In 1988, Kidder eked out a $ 46 mil-
lion profit,]31 [mainly because of severe cost
cutting.]32 [Its 1,400-member brokerage oper-
ation reported an estimated $ 5 million loss last
year,]33 [although Kidder expects to turn a profit
this year]34 (RST Treebank, wsj 0604).
(3) [Suzanne Sequin passed away Saturday at the
communal hospital of Bar-le-Duc,]3 [where she
had been admitted a month ago.]4 [. . . ] [Her fu-
neral will be held today at 10h30 at the church
of Saint-Etienne of Bar-le-Duc.]5 (annodis cor-
pus).
These examples involve what are called long distance
attachments. (2) involves a relation of contrast, or com-
parison between 31 and 33, but which does not involve
the contribution of 32 (the costs cutting of 1988). (3)
displays something comparable. A causal relation like
result, or at least a temporal narration holds between
3 and 5, but it should not scope over 4 if one does
not wish to make Sequin?s admission to the hospital
a month ago a consequence of her death last Saturday.
Finally in (1) C4 elaborates on C1, but not on the fact
that C1 is attributed to chief Garcia, so the correspond-
ing elaboration relation should not scope over C3.
It is impossible however, to account for long distance
attachment using the immediate interpretation of RST
trees. (2), for instance, also involves an explanation
relation between 31 and 32, which should include none
of 33 or 34 in its scope. Since 31 is in the scope of both
the explanation and the contrast relation, Axiom A5a of
the previous section entails than an RST tree involving
the two relations has to make one of the two relations
dominates the other.
Marcu?s Nuclearity Principle (NP) Marcu 1996 pro-
vides an alternative to the immediate interpretation and
captures some long distance attachments Danlos 2008;
Egg and Redeker 2010. According to the NP, a rela-
6
tion between two spans of text, expressed at a node of
a RS Tree should hold between the most salient parts
of these spans. Most salient part is recursively defined:
the most salient part of an elementary constituent is it-
self, for a multinuclear relation R(t1N , . . . , tkN) its most
salient part is the union of the most salient parts of the
ti2. Following Egg and Redeker 2010, the NP, or weak
NP is a constraint on which RST trees may correctly
characterize an input text; it is not a mechanism for
computing scopes. Given their analysis of (1) given in
the introduction, NP entails that Elab1 holds between
C1 and C4, accounting for the long distance attach-
ment, and that Attribution holds between C1 and C4
which meets intuition in this case. There is however no
requirement that Attribution do not hold between the
wider span [C1,C2] and C3, as there is no requirement
that Elab1 does not hold between [C1,C2,C3] and C4.
In order to accurately account for (1), the former must
be true and the latter false.
However, this interpretation of NP together with an
RST tree does not determine the semantic scope of all
relations. Danlos 2008 reformulates NP as a Mixed
Nuclearity Principle (MNP) that outputs determinate
scopes for a given structure. The MNP requires for a
given node, that the most salient parts of his daughters
furnish the exact semantic scope for the relation at that
node. The MNP transforms an RST tree t into a scope
structureMt, which validates A0 ? A3 but also A6.3, A7
and A8. HenceM could be exported back to SDRT and
the MNP would yield a translation from RST-trees to
SDRSs.
But when applied to the RST Treebank, the MNP
yields wrong, or at least incomplete, semantic scopes
for intuitively correct RS Trees. The mixed principle
applied to the tree of s1 gives the Attribution scope
over C1 only, but not C2, which is incorrect. Focus-
ing on the attribution relation which is the second most
frequent in the RST Treebank, we find out that, regard-
less of whether we assign Attribution?s arguments S
and N or N and S, this principle makes wrong predic-
tions 86% of the time in a random sampling of 50 cases
in which we have attributions with multi-clause second
argument spans. Consider the following example from
the RST Treebank:
(4) [Interprovincial Pipe Line Co. said]1 [it will de-
lay a proposed two-step, 830 million Canadian-
dollar [(US$705.6 million)]3 expansion of its
system]2 [because Canada?s output of crude oil
is shrinking.]4
Applied to the annotated RS Tree for this example (fig-
2Except for Sequence which only retains the most salient
part of tk
3That A6 is valid in the resulting model is not immediate.
Assume a multinuclear (coordinating) relation instance r has
scope over xn and xn+k later in the textual order. Then it is
impossible to attach with r? a later found constituent xn+k+l to
xn alone, for it would require that xn+1 escapes the scope of r?
from the MNP which it will not do by multinuclearity of r.
attribution
1
S
reason
restatement
2
N
3SN
5S
N
Figure 1: Annotated RST Tree for example (4).
ure 1), the MNP yields an incorrect scope of the attribu-
tion relation over 2 only, regardless of whether the at-
tribution is annotated N-S or S -N. The idea behind the
weak NP provides a better fit with intuitions. The prin-
ciple gives minimal semantic requirements for scoping
relations; everything beyond those requirements is left
underspecified. We formalize this as the relaxed Nu-
clearity Principle (RNP), which does not compute one
structure where each relation is given its exact scope,
but a set of such structures.
The target structures are not trees any more, but we
want them to still reflect the dominance information
present in the RS Tree. We therefore define a notion
of weak dominance over structures of the scoping lan-
guage: for two sets of constituents, X  Y iff X ? Y or
there is a subordinating relation whose left argument is
X and right one Y . Weak dominance is given by tran-
sitive closure ? of . For two relations, r ?l r? iff theleft argument of r weakly dominates both arguments
of r?. ?r is symmetrically defined. Finally, structures
computed by the RNP have to validate the weakened
version of A5: if two relations scope over the same el-
ementary constituent one has to weakly dominates the
other. Let AW5 denote this axiom.
Definition 3 (Relaxed Nuclearity Principle). One can
assign to an RS Tree t a formula of the scoping lan-
guage ?t = ?x??r??t ? ?t such that:
1? ?t is a formula specifying that all individuals
quantified in x? and r? are pairwise distinct, and that there
is no other individuals that the ones just mentioned. ?t
also specifies for each intermediate node n that the cor-
responding relation instance rn is labelled with the ad-
equate relation symbol R and relation type (subordinat-
ing if N-S . . . ).
2? ?t encodes the nuclearity principle applied to t:
for all intermediate nodes ni and n j in t such that nl is
the left (resp. right) daughter of ni, ?t specifies that ni
must scope to the left (resp. right) over the nucleus of
n j.
The interpretation ~t is defined as the set of struc-
turesM that validate ?t and A0, A1, A2, A3, AW5 (they allhave |t| individuals, as fixed by ?t). Moreover, it can
be shown that each model of this set validates TS DRT ;
so we have a interpretation of an RS-Tree into a set of
SDRSs.
5 Intertranslability between RST/DTs
DTs are a restriction of SDRSs to structures without
complex constituents. So the ? function of section 2
7
can transform distinct SDRSs transform into the same
DT with a consequent loss of information.
a?R1 pi
pi : b?R2 c | a?R1 b?R2 c |
pi?R2 b
pi : a?R1 b (1)
Each of the SDRSs above yields the same DT after sim-
plification, namely the second one a?R1 b?R2 c.
The natural interpretation of a DT g describes the
set of fully scoped SDRS structures that are compat-
ible with these minimal requirements, i.e that would
yield g by simplification. To get this set, every edge
r(x, y) in g, r, must be assigned left scope among the
descendants of x in g (and right scope among those of
y); this is a consequence of i) x and y being heads of the
left and right arguments of r and ii) the SDRSs that are
compatible with g do not admit relations with a right
argument in one constituent and a left one outside of it.
Definition 4. Assume that we map each node4 x of g
into a unique variable vx ? Vl and each edge e into a
unique variable symbol re ? Vi. Define x? and r? in an
analogous way as in definition 3.
For a given dependency tree g, we compute a for-
mula ?g = ?x??r? ?g ? ?g such that
? ?g is defined analogously as in definition 3, defin-
ing the set of relation instances and EDUs.
? ?g is the formula stating the minimal scopes for
each relation instance: for all edge in e = R(x, y)
in g, ?g entails i) re has vx in its left scope and
vy in its right scope and ii) let Des(x) be the set
of variable symbols for all the descendants of x in
g, ?g entails that if re has left scope over some vz
then vz is in Des(x) (symmetrically for y and right
scope).
The interpretation ~g of a DT is: {M | M |=
?g, A0-A3, A6, A7}. The DT a ?R1 b ?R2 c for in-
stance, is interpreted as a set of three structures iso-
morphic to the ones in (1) above.
We now relate DTs to RS Trees interpreted with the
RNP. To this aim, we focus on a restricted class of DTs,
those who involve i) coordinating chains of 3 edus or
more only if they involve a single coordinating relation:
x1 ?R1 x2 ?R2 ? ? ? ?Rn?1 xn may appear only for n >
2 if all the Ri are the same coordinating relation, and
ii) subordinating nests of 3 edus or more only if they
involve a single subordinating relation:
x
y1
R1
. . . yn
Rn
is allowed for n > 1 only if all Ri
are labelled with the same subor-
dinating relation.
This restricted class of DTs corresponds exactly with
the set of RS-Trees interpreted with the RNP, provided
that we restrict the interpretation of a DT in the fol-
lowing way: a principle called Continuing Discourse
Pattern, CDP Asher and Lascarides 2003 must apply,
4Recall that unlike RS Trees, DTs have EDUs as nodes
and relations as edges.
who states that whenever a sequence of coordinating
relation Ric originates as a node which appear to be
also in the right scope of a subordinating relation Rs,
Rs must totally include all the Ric in its right scope. A
second principle is required, who states that whenever
two subordinating relations R0s and R?s originate at the
same node in the DT, and the right argument of R?s is
located after the right argument of Rs, any structure in
the interpretation of the DT must verify R?s l Rs. The
translation needs these requirements to work, because:
i) with the NP a relation scoping over a multinuclear
one must includes all the nucleus in RST, and ii)a node
in a RS Tree cannot scope over something that is not its
descendant). Let CDP+ denote these requirements.
Using the restricted interpretation of a DT g;
~gCDP = {M | M |= A0-A3, A6, A7,CDP+}, we trans-
form an RS Tree t into a dependency graph G(t) such
that ~t = ~G(t)CDP:
Definition 5 (RS Trees to dependency graphs). The
translation G takes a RS Tree t as input and outputs
a pair ?G, n?, where G = ?Nodes, Edges? is the corre-
sponding dependency graph, and n an attachment point
used along the recursive definition of G.
? If t is an EDU x then (G)(t) = ?({x}, {}), x?.
? If t = R(t1N , t2S ) then let ?G1, n1? = G(t1) and
?G2, n2? = G(t2).
G(t) = ?(G1 ?G2 ? {Rsubord(n1, n2))}; n1?
? If t = R(t1S , t2N) then G(t) = G(R(t2N , t1S ))
? If t = R(t1N , . . . , tkN) (multinuclear), let ?Gi, ni? =
G(ti), let G be the result of adding a chain
n1 ?Rcoord ? ? ? ?Rcoord nk to the union of the Gi,
G(t) = ?G; n1?
? If t is a nuclear satellite relation with several satel-
lites R(t1S , . . . t jN , . . . tkS ), compute the Gi has inthe previous case, then add to the union of the Gi
the nest of k ? 1 subordinating relations R linking
n j to each of the ni, i , j.
Recall RS Tree (s1). Applying G to this tree yields
the dependency tree (s3): Elab1(C1,C2)?Attr(C1,C3)?
Elab2(C1,C4). ~s3 supports any reading of (s1) pro-
vided by RNP, but also an additional one where Attr
scopes over [C1,C2,C4]. This is however forbidden
by CDP+ for C4 is after C3 in the textual order but
Elab(C1,C4) l Attr(C1,C3).
6 Similarities and distances
The framework we have presented yields a notion of
similarity that applies to structures of different for-
malisms. To motivate our idea, recall example (1);
the structure in (s3) in which Attribution just scopes
over C1 differs from the intuitively correct interpreta-
tion only in that Attribution should also scope over C2
8
as in (s2), while a structure that does this but in which
C3 is in the scope of the Elaboration relation is intu-
itively further away from the correct interpretation.
Our similarity measure Sim over structures M1 and
M2 assumes a common set of elementary constituents
and a correspondence between relation types in the
structures. We measure similarity in terms of the
scopes given to the relations. The intuition, is that given
a map f from elements of relation instances inM1 re-
lation instances in M2, we achieve a similarity score
by counting for each relation instance r the number of
EDUs that are both in the left scope of one element of
r and in f (r), then divide this number by the total num-
ber of diffrents constituents in the left scope of r1 and
r2, and do the same for right scopes as well. The global
similarity is given by the correspondence which yields
the best score.
Given a relation r1 ? M1 and a relation r2?M2, let
?(r1, r2) =
{ 1 if r1 and r2 have the same label
0 otherwise . De-
fine Cl(r1, r2) = |{x : l | M1 |= x ?l r1 ?M2 |= x ?l r2}|,
the number of constituents over which r1 and r2 scope
and Dl(r1, r2) = |{x : l |M1 |= x ?l r1?M2 |= x ?l r2}|.
Define Cr and Dr analogously and assume thatM1 has
less relation instances thanM2. Let Inj(D1i ,D2i ) be theset of injections of relations instances of M1 to those
ofM2.
S im(M1,M2) = 12Max(|M1|, |M2|)?
Max
f?Inj(D1i ,D2i )
?
r:i
?(r, f (r)) ? ( Cl(r, f (r))Dl(r, f (r)) +
Cr(r, f (r))
Dr(r, f (r)) )
If M2 has more relation instances, Invert arguments
and use the definition above. If they have same number
of instances, both directions coincide.
d(M1,M2)=1 ? S im(M1,M2)
For a discourse structureM, S im(M,M) = 1; Sim
ranges between 1 and 0. d is a Jaccard-like met-
ric obeying symmetry, d(x, x) = 0 d(x, y) , 0 for
x , y, and the triangle equality. One can further define
the maximal or average similarity between any pair of
structures of two sets S 1 and S 2. This gives an idea
of the similarity between two underspecified interpre-
tations, such as the ones provided by RNP of section 4.
For example, the maximal similarity between (s2) in-
terpreted as itself (immediate interpretation) and a pos-
sible scope structure for the DT (s3), interpreted with
the underspecified ~ of section 5, is 7/12. It is pro-
vided by the interpretation of (s3) where Attr is given
left scope over C1,C2,C4, Elab1 holds between C1 and
C2, and the second Elab fails to match the continua-
tion of (s3). sim(~s2, ~?(s2) = 7/12 also, because
? must distribute [2, 4] in s2 to avoid crossing depen-
dencies; so ~?(s2)  ~s3. The maximal similarity
between the RS tree in (s1) with RNP (or equivalently,
(3) with ~CDP+) and (s2) is 19/36, achieved when both
C1 and C2 are left argument of Attr (though not C4).
With MNP, the similarity is 17/36.
Given our results in sections 4 and 5, we have:
Fact 3. (i) For any DT g without a > 3 length flat se-
quence and interpreted using CDP+, there an RS tree
t interpreted with RNP such that S im(g, t) = 1. (ii)
For any RS tree with RNP there is a DT g such that
S im(t, g) = 1.
To prove (i) construct a model using Definition 4 and
then use RST decoding. To prove (ii) construct a model
given Definition 3 and use DT encoding. Our similarity
measure provides general results for SDRSs and DTTs
(and a fortiori SDRSs and RS trees) (See Appendix).
7 Related Work
Our work shares a motivation with Blackburn, Gardent,
and Meyer-Viol 1993: Blackburn, Gardent, and Meyer-
Viol 1993 provides a modal logic framework for for-
malizing syntactic structures; we have used MSO and
our scope language to formalize discourse structures.
While many concepts of discourse structure admit of
a modal formalization, the fact that discourse relations
can have scope over multiple elementary nodes either
in their first or second argument makes an MSO treat-
ment more natural. Danlos 2008 compares RST, SDRT
and Directed Acyclic Graphs (DAGs) in terms of their
strong generative capacity in a study of structures and
examples involving 3 EDUS. We do not consider gen-
erative capacity, but we have given a generic and gen-
eral axiomatization of RST, SDRT and DT in a formal
interpreted language. We can translate any structure of
these theories into this language, independent of their
linguistic realization. We agree with Danlos that the
NP does not yield an accurate semantic representation
of some discourses. We agree with Egg and Redeker
2010 that the NP is rather a constraint on structures, and
we formalize this with the relaxed principle and show
how it furnishes a translation from RS trees to sets of
scoped structures. Danlos?s interesting correspondence
between restricted sets of RST trees, SDRSs and DAGs
assumes an already fixed scope-interpretation for each
kind of structure: SDRSs and DAGs are naturally in-
terpreted as themselves, and RS Trees are interpreted
with the mixed NP Our formalism allows us both to
describe the structures themselves and various ways of
computing alternate scopes for relations.
With regard to the discussion in Egg and Redeker
2008; Wolf and Gibson 2005 of tree vs. graph struc-
tures, we show exactly how tree based structures
like RST with or without the NP compare to graph
based formalisms like SDRT. We have not investigated
Graphbank here, but the scope language can axioma-
tize Graphbank (with A0-A3, A8).
8 Conclusions
We have investigated how to determine the semantic
scopes of discourse relations in various formalisms by
9
developing a canonical formalism that encodes scopes
of relations regardless of particular assumptions about
discourse structure. This provides a lingua franca for
comparing discourse formalisms and a way to measure
similarity between structures, which can help to com-
pare different annotations of a same text.
References
Afantenos, S. et al (2012). ?An empirical resource for
discovering cognitive principles of discourse organ-
isation: the ANNODIS corpus?. In: Proceedings of
LREC 2012. ELRA.
Asher, N. and A. Lascarides (2003). Logics of Conver-
sation. Cambridge University Press.
Asher, N. (1993). Reference to Abstract Objects in Dis-
course. Studies in Linguistics and Philosophy 50.
Dordrecht: Kluwer.
Baldridge, J., N. Asher, and J. Hunter (2007). ?Anno-
tation for and Robust Parsing of Discourse Structure
on Unrestricted Texts?. In: Zeitschrift fr Sprachwis-
senschaft 26, pp. 213?239.
Bateman, J. and K. J. Rondhuis (1997). ?Coherence re-
lations : Towards a general specification?. In: Dis-
course Processes 24.1, pp. 3?49.
Blackburn, P., C. Gardent, and W. Meyer-Viol (1993).
?Talking about Trees?. In: EACL 6, pp. 21?29.
Carlson, L., D. Marcu, and M. E. Okurowski (2002).
RST Discourse Treebank. Linguistic Data Consor-
tium, Philadelphia.
Danlos, L. (2008). ?Strong generative capacity of RST,
SDRT and discourse dependency DAGSs?. English.
In: Constraints in Discourse. Ed. by A. Benz and P.
Kuhnlein. Benjamins, pp. 69?95.
duVerle, D. and H. Prendinger (2009). ?A Novel Dis-
course Parser Based on Support Vector Machine
Classification?. In: Proceedings of ACL-IJCNLP
2009. ACL, pp. 665?673.
Egg, M. and G. Redeker (2008). ?Underspecified dis-
course representation?. In: PRAGMATICS AND BE-
YOND NEW SERIES 172, p. 117.
? (2010). ?How Complex is Discourse Structure?? In:
Proceedings of LREC?10. Ed. by N. Calzolari et al
ELRA.
Hitzeman, J., M. Moens, and C. Grover (1995). ?Algo-
rithms for Analyzing the Temporal Structure of Dis-
course?. In: Proceedings of the 7th Meeting of the
European Chapter of the Association for Computa-
tional Linguistics, pp. 253?260.
Hobbs, J. R., M. Stickel, and P. Martin (1993). ?In-
terpretation as Abduction?. In: Artificial Intelligence
63, pp. 69?142.
Kehler, A. (2002). Coherence, Reference and the The-
ory of Grammar. CSLI Publications.
Lascarides, A. and N. Asher (1993). ?Temporal In-
terpretation, Discourse Relations and Commonsense
Entailment?. In: Linguistics and Philosophy 16,
pp. 437?493.
Mann, W. C. and S. A. Thompson (1987). ?Rhetorical
Structure Theory: A Framework for the Analysis of
Texts?. In: International Pragmatics Association Pa-
pers in Pragmatics 1, pp. 79?105.
Marcu, D. (1996). ?Building up rhetorical structure
trees?. In: Proceedings of the thirteenth national
conference on Artificial intelligence - Volume 2.
AAAI?96. Portland, Oregon: AAAI Press, pp. 1069?
1074. isbn: 0-262-51091-X.
Muller, P. et al (2012). ?Constrained decoding for
text-level discourse parsing?. Anglais. In: COLING
- 24th International Conference on Computational
Linguistics. Mumbai, Inde.
Polanyi, L. (1985). ?A Theory of Discourse Structure
and Discourse Coherence?. In: Papers from the Gen-
eral Session at the 21st Regional Meeting of the
Chicago Linguistics Society. Ed. by P. D. K. W. H.
Eilfort and K. L. Peterson.
Polanyi, L. and R. Scha (1984). ?A Syntactic Ap-
proach to Discourse Semantics?. In: Proceedings of
the 10th International Conference on Computational
Linguistics (COLING84). Stanford, pp. 413?419.
Polanyi, L. et al (2004). ?A Rule Based Approach
to Discourse Parsing?. In: Proceedings of the 5th
SIGDIAL Workshop in Discourse and Dialogue,
pp. 108?117.
Prasad, R. et al (2008). ?The penn discourse tree-
bank 2.0?. In: Proceedings of the 6th International
Conference on Language Resources and Evaluation
(LREC 2008), p. 2961.
Sagae, K. (2009). ?Analysis of Discourse Structure
with Syntactic Dependencies and Data-Driven Shift-
Reduce Parsing?. In: Proceedings of IWPT?09. ACL,
pp. 81?84.
Sanders, T., W. Spooren, and L. Noordman (1992).
?Toward a taxonomy of coherence relations?. In:
Discourse processes 15.1, pp. 1?35.
Stede, M. (2004). ?The Potsdam Commentary Cor-
pus?. In: ACL 2004 Workshop on Discourse Annota-
tion. Ed. by B. Webber and D. K. Byron. Barcelona,
Spain: Association for Computational Linguistics,
pp. 96?102.
Subba, R. and B. Di Eugenio (2009). ?An effective Dis-
course Parser that uses Rich Linguistic Information?.
In: Proceedings of HLT-NAACL. ACL, pp. 566?574.
Webber, B. et al (1999). ?Discourse Relations: A
Structural and Presuppositional Account Using Lex-
icalised TAG?. In: Proceedings of the 37th ACL
Conference. College Park, Maryland, USA: Associ-
ation for Computational Linguistics, pp. 41?48. doi:
10.3115/1034678.1034695.
Wolf, F. and E. Gibson (2005). ?Representing Dis-
course Coherence: A Corpus Based Study?. In:
Computational Linguistics 31.2, pp. 249?287.
Appendix
In what follows, let @ denotes the irreflexive part of
v We assume that we have access to the textual order
10
of EDUs as a function f : EDUs ? N with an associ-
ated strict linear ordering < over EDUs. We also ap-
peal to the notion of a chain over EDUs {x1, x2, . . . xn}
with a set of relation instances r1, . . . , rn} all of which
are instances of an n-ary relation type, of the form
x1 ?r1 x2 ?r2 . . . ?rn xn which can be defined in
MSO. To handle RST relations with multiple satellites,
we define a nest: Nest(X,R) iff all r ? R have the same
left argument in X but take different right arguments in
X. Finally, we define CDUs:
cdu(X,R)? ?rArgs(r, X)?
?r? (?x x ?r r? ? x ? X)? r? ? R
Axiomatization
?x : l ?r : i (x ?l r) ? (x ?r r)
(A1:Weak Connectedness)
?r?x, y(x ?r r) ? y ?l r))
(A2 :Properness of the relation)
?X : (l, t)(X , 0? ?y?X ?n?y ?l n
(A3 :Acyclicity or Well Foundedness)
No crossing dependencies using the textual order < of
EDUs:
?x, y, z,w((x < y < z < w) ?
?m, n?(x ?l n ? z ?r n
? y ?l m ? w ?r m)) (A4)
Tree Structures. Define scopes(r, x) := x ?l r ? x ?r r.
?r, r? ((?(?X,R r, r? ? R ? chain(X,R) ? nest(X,R))
? (?x scopes(r, x) ? scopes(r?, x)))
? (r v r? ? r? v r))
(A5a)
?R : (i, t)?!r : i ?r? ? R r? v r (A5b)
Right Frontier:
?n, xn, xn+1?r ((xn+1 ?r r)? (xn ?l r) ? (?xn ?l r
? ?X,R(chain(X,R) ? ?r?(r? ? R? sub(r?))
? ?y ? X?z?k ?m, j ? R (scopes( j, y) ? acc(z, y)
? scopes(m, xn) ? z ?l k ? k ? ?xn+1)))) (A6)
(The definition of SDRS accessibility acc is easy)
CDUs or EDUs and no overlapping CDUs:
?!x : l ? ?X,R cdu(X,R)?
?X,Y,R,R? (cdu(X,R) ? cdu(Y,R?)?
(R ? R? , 0? (R ? R? ? R? ? R))
(A7)
The same arguments cannot be linked by subordinating
and coordinating relations. The formal axiom is evi-
dent.
Finally, two axioms for restricting SDRSs to depen-
dency trees:
?r?x, y((x ?l r) ? y ?l r))
? (x ?r r) ? y ?r r)))? x = y
(A9a : NoCDUs.)
?r?r??X,Y(lArgs(r, X) ? rArgs(r,Y)
? lArgs(r?, X) ? rArgs(r?,Y))
? r = r?
(A9b :unique arc)
We note that as a consequence of A5a and A5b we have
no danglers or contiguous spans:
?x, y, n (x ?l n ? y ?l n ? x , y)
? ??m?z (x ?l m ? z ?r m
? ?(z ?l n ? z ?r n))
We also note that A5a and A5b entail A7, A8 and A9b,
though not vice-versa.
Fact 4. Where ? is any SDRS and ? : S DRS ? DT as
in section 2, set R1 = {r : i : |{x : M? |= x ?l r)}| > 1},
R2 = {r : i : |{x : M? |= x ?r r)}| > 1}, and
R{x,y} = {r|?r? : i(x ?l r? ? y ?r r? ? r? , r}. Assume the
immediate interpretation of ? and ?(?):
S im(?, ?(?))=
2|I| ? |(R1 ? R2) ??x,y?D2l X{x,y}|
2|I|
+
1
2|I| {?r?R1
1
|x : M? |= x ?l r)}|
+?r?R2
1
|x : M? |= x ?r r)}| }
Explanation: We suppose that I is the number of re-
lation instances in the SDRS. ? removes CDUs in an
SDRS and attaches all incoming arcs to the CDUs to
the head of the CDU. It also removes multiple arcs
into any given node. So for any node m such that
|{r : m ?r r}| = a > 1, then the information contained
in the a ? 1 arcs will be lost. In addition ? will restrict
that one incoming arc that in the SDRS has in its scope
all the elements in the CDU to just the head. So the
scope information concerning all the other elements in
the CDU will be lost.
11
