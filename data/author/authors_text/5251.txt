Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 89?92,
New York, June 2006. c?2006 Association for Computational Linguistics
Summarizing Speech Without Text Using Hidden Markov Models
Sameer Maskey, Julia Hirschberg
Dept. of Computer Science
Columbia University
New York, NY
{smaskey, julia}@cs.columbia.edu
Abstract
We present a method for summarizing
speech documents without using any type
of transcript/text in a Hidden Markov
Model framework. The hidden variables
or states in the model represent whether
a sentence is to be included in a sum-
mary or not, and the acoustic/prosodic fea-
tures are the observation vectors. The
model predicts the optimal sequence of
segments that best summarize the docu-
ment. We evaluate our method by compar-
ing the predicted summary with one gen-
erated by a human summarizer. Our re-
sults indicate that we can generate ?good?
summaries even when using only acous-
tic/prosodic information, which points to-
ward the possibility of text-independent
summarization for spoken documents.
1 Introduction
The goal of single document text or speech sum-
marization is to identify information from a text
or spoken document that summarizes, or conveys
the essence of a document. EXTRACTIVE SUM-
MARIZATION identifies portions of the original doc-
ument and concatenates these segments to form a
summary. How these segments are selected is thus
critical to the summarization adequacy.
Many classifier-based methods have been exam-
ined for extractive summarization of text and of
speech (Maskey and Hirschberg, 2005; Christensen
et. al., 2004; Kupiec et. al., 1995). These ap-
proaches attempt to classify segments as to whether
they should or should not be included in a summary.
However, the classifiers used in these methods im-
plicitly assume that the posterior probability for the
inclusion of a sentence in the summary is only de-
pendent on the observations for that sentence, and
is not affected by previous decisions. Some of these
(Kupiec et. al., 1995; Maskey and Hirschberg, 2005)
also assume that the features themselves are inde-
pendent. Such an independence assumption simpli-
fies the training procedure of the models, but it does
not appear to model the factors human beings appear
to use in generating summaries. In particular, human
summarizers seem to take previous decisions into
account when deciding if a sentence in the source
document should be in the document?s summary.
In this paper, we examine a Hidden Markov
Model (HMM) approach to the selection of seg-
ments to be included in a summary that we believe
better models the interaction between extracted seg-
ments and their features, for the domain of Broad-
cast News (BN). In Section 2 we describe related
work on the use of HMMs in summarization. We
present our own approach in Section 3 and discuss
our results in Section 3.1. We conclude in Section 5
and discuss future research.
2 Related Work
Most speech summarization systems (Christensen
et. al., 2004; Hori et. al., 2002; Zechner, 2001) use
lexical features derived from human or Automatic
Speech Recognition (ASR) transcripts as features to
select words or sentences to be included in a sum-
mary. However, human transcripts are not gener-
ally available for spoken documents, and ASR tran-
scripts are errorful. So, lexical features have prac-
tical limits as a means of choosing important seg-
ments for summarization. Other research efforts
have focussed on text-independent approaches to ex-
tractive summarization (Ohtake et. al., 2003), which
rely upon acoustic/prosodic cues. However, none
of these efforts allow for the context-dependence of
extractive summarization, such that the inclusion of
89
one word or sentence in a summary depends upon
prior selection decisions. While HMMs are used in
many language processing tasks, they have not been
employed frequently in summarization. A signifi-
cant exception is the work of Conroy and O?Leary
(2001), which employs an HMM model with pivoted
QR decomposition for text summarization. How-
ever, the structure of their model is constrained by
identifying a fixed number of ?lead? sentences to be
extracted for a summary. In the work we present
below, we introduce a new HMM approach to ex-
tractive summarization which addresses some of the
deficiencies of work done to date.
3 Using Continuous HMM for Speech
Summarization
We define our HMM by the following parameters:
? = 1..N : The state space, representing a set of
states where N is the total number of states in the
model; O = o1k, o2k, o3k, ...oMk : The set of obser-
vation vectors, where each vector is of size k; A =
{aij} : The transition probability matrix, where aij
is the probability of transition from state i to state j;
bj(ojk) : The observation probability density func-
tion, estimated by ?Mk=1cjkN(ojk, ?jk,?jk), where
ojk denotes the feature vector; N(ojk, ?jk,?jk) de-
notes a single Gaussian density function with mean
of ?jk and covariance matrix ?jk for the state j,
with M the number of mixture components and cjk
the weight of the kth mixture component; ? = pii :
The initial state probability distribution. For conve-
nience, we define the parameters for our HMM by
a set ? that represents A, B and ?. We can use the
parameter set ? to evaluate P (O|?), i.e. to measure
the maximum likelihood performance of the output
observables O. In order to evaluate P (O|?), how-
ever, we first need to compute the probabilities in
the matrices in the parameter set ?
The Markov assumption that state durations have
a geometric distribution defined by the probability
of self transitions makes it difficult to model dura-
tions in an HMM. If we introduce an explicit du-
ration probability to replace self transition proba-
bilities, the Markov assumption no longer holds.
Yet, HMMs have been extended by defining state
duration distributions called Hidden Semi-Markov
Model (HSMM) that has been succesfully used
(Tweed et. al., 2005). Similar to (Tweed et. al.,
1
2
3
4
L-1
L
Figure 1: L state position-sensitive HMM
2005)?s use of HSMMs, we want to model the po-
sition of a sentence in the source document explic-
itly. But instead of building an HSMM, we model
this positional information by building our position-
sensitive HMM in the following way:
We first discretize the position feature into L num-
ber of bins, where the number of sentences in each
bin is proportional to the length of the document.
We build 2 states for each bin where the second
state models the probability of the sentence being
included in the document?s summary and the other
models the exclusion probability. Hence, for L bins
we have 2L states. For any bin lth where 2l and
2l ? 1 are the corresponding states, we remove all
transitions from these states to other states except
2(l+1) and 2(l+1)?1. This converts our ergodic L
state HMM to an almost Left-to-Right HMM though
l states can go back to l ? 1. This models sentence
position in that decisions at the lth state can be ar-
rived at only after decisions at the (l ? 1)th state
have been made. For example, if we discretize sen-
tence position in document into 10 bins, such that
10% of sentences in the document fall into each bin,
then states 13 and 14, corresponding to the seventh
bin (.i.e. all positions between 0.6 to 0.7 of the text)
can be reached only from states 11, 12, 13 and 14.
The topology of our HMM is shown in Figure 1.
3.1 Features and Training
We trained and tested our model on a portion of
the TDT-2 corpus previously used in (Maskey and
Hirschberg, 2005). This subset includes 216 stories
from 20 CNN shows, comprising 10 hours of audio
data and corresponding manual transcript. An an-
notator generated a summary for each story by ex-
tracting sentences. While we thus rely upon human-
90
identified sentence boundaries, automatic sentence
detection procedures have been found to perform
with reasonable accuracy compared to human per-
formance (Shriberg et. al., 2000).
For these experiments, we extracted only acous-
tic/prosodic features from the corpus. The intu-
ition behind using acoustic/prosodic features for
speech summarization is based on research in speech
prosody (Hirschberg, 2002) that humans use acous-
tic/prosodic variation ? expanded pitch range,
greater intensity, and timing variation ? to indi-
cate the importance of particular segments of their
speech. In BN, we note that a change in pitch, am-
plitude or speaking rate may signal differences in
the relative importance of the speech segments pro-
duced by anchors and reporters ? the professional
speakers in our corpus. There is also considerable
evidence that topic shift is marked by changes in
pitch, intensity, speaking rate and duration of pause
(Shriberg et. al., 2000), and new topics or stories
in BN are often introduced with content-laden sen-
tences which, in turn, often are included in story
summaries.
Our acoustic feature-set consists of 12 features,
similar to those used in (Inoue et. al., 2004; Chris-
tensen et. al., 2004; Maskey and Hirschberg, 2005).
It includes speaking rate (the ratio of voiced/total
frames); F0 minimum, maximum, and mean; F0
range and slope; minimum, maximum, and mean
RMS energy (minDB, maxDB, meanDB); RMS
slope (slopeDB); sentence duration (timeLen =
endtime - starttime). We extract these features by
automatically aligning the annotated manual tran-
scripts with the audio source. We then employ Praat
(Boersma, 2001) to extract the features from the
audio and produce normalized and raw versions of
each. Normalized features were produced by divid-
ing each feature by the average of the feature values
for each speaker, where speaker identify was deter-
mined from the Dragon speaker segmentation of the
TDT-2 corpus. In general, the normalized acoustic
features performed better than the raw values.
We used 197 stories from this labeled corpus to
train our HMM. We computed the transition proba-
bilities for the matrix ANXN by computing the rel-
ative frequency of the transitions made from each
state to the other valid states. We had to compute
four transition probabilities for each state, i.e. aij
where j = i, i + 1, i + 2, i + 3 if i is odd and
j = i ? 1, i, i + 1, i + 2 if i is even. Odd states
signify that the sentence should not be included in
the summary, while even states signify sentence in-
clusion. Observation probabilities were estimated
using a mixture of Gaussians where the number of
mixtures was 12. We computed a 12X1 matrix for
the mean ? and 12X12 matrices for the covariance
matrix ? for each state. We then computed the max-
imum likelihood estimates and found the optimal
sequence of states to predict the selection of docu-
ment summaries using the Viterbi algorithm. This
approach maximizes the probability of inclusion of
sentences at each stage incrementally.
4 Results and Evaluation
We tested our resulting model on a held-out test set
of 19 stories. For each sentence in the test set we ex-
tracted the 12 acoustic/prosodic features. We built a
12XN matrix using these features for N sentences
in the story where N was the total length of the
story. We then computed the optimal sequence of
sentences to include in the summary by decoding
our sentence state lattice using the Viterbi algorithm.
For all the even states in this sequence we extracted
the corresponding segments and concatenated them
to produce the summary.
Evaluating summarizers is a difficult problem,
since there is great disagreement between humans
over what should be included in a summary. Speech
summaries are even harder to evaluate because most
objective evaluation metrics are based on word over-
lap. The metric we will use here is the standard
information retrieval measure of Precision, Recall
and F-measure on sentences. This is a strict met-
ric, since it requires exact matching with sentences
in the human summary; we are penalized if we iden-
tify sentences similar in meaning but not identical to
the gold standard.
We first computed the F-measure of a baseline
system which randomly extracts sentences for the
summary; this method produces an F-measure of
0.24. To determine whether the positional informa-
tion captured in our position-sensitive HMM model
was useful, we first built a 2-state HMM that models
only inclusion/exclusion of sentences from a sum-
mary, without modeling sentence position in the
document. We trained this HMM on the train-
91
ing corpus described above. We then trained a
position-sensitive HMM by first discretizing posi-
tion into 4 bins, such that each bin includes one-
quarter of the sentences in the story. We built an
8-state HMM that captures this positional informa-
tion. We tested both on our held-out test set. Re-
sults are shown in Table 1. Note that recall for
the 8-state position-sensitive HMM is 16% better
than recall for the 2-state HMM, although precision
for the 2-state model is slightly (1%) better than
for the 8-state model. The F-measure for the 8-
state position-sensitive model represents a slight im-
provement over the 2-state model, of 1%. These re-
sults are encouraging, since, in skewed datasets like
documents with their summaries, only a few sen-
tences from a document are usually included in the
summary; thus, recall is generally more important
than precision in extractive summarization. And,
compared to the baseline, the position-sensitive 8-
state HMM obtains an F-measure of 0.41, which is
17% higher than the baseline.
ModelType Precision Recall F-Meas
HMM-8state 0.26 0.95 0.41
HMM-2state 0.27 0.79 0.40
Baseline 0.23 0.24 0.24
Table 1: Speech Summarization Results
5 Conclusion
We have shown a novel way of using continuous
HMMs for summarizing speech documents without
using any lexical information. Our model generates
an optimal summary by decoding the state lattice,
where states represent whether a sentence should
be included in the summary or not. This model is
able to take the context and the previous decisions
into account generating better summaries. Our re-
sults also show that speech can be summarized fairly
well using acoustic/prosodic features alone, without
lexical features, suggesting that the effect of ASR
transcription errors on summarization may be mini-
mized by techniques such as ours.
6 Acknowledgement
We would like to thank Yang Liu, Michel Galley and
Fadi Biadsy for helpful comments. This work was
funded in part by the DARPA GALE program under
a subcontract to SRI International.
References
Boersma P. Praat, a system for doing phonetics by com-
puter Glot International 5:9/10, 341-345. 2001.
Christensen H., Kolluru B., Gotoh Y., Renals S. From
text summarisation to style-specific summarisation for
broadcast news Proc. ECIR-2004, 2004
Conroy J. and Leary D.O Text Summarization via Hidden
Markov Models and Pivoted QR Matrix Decomposi-
tion Technical report, University of Maryland, March
2001
Hirschberg J Communication and Prosody: Functional
Aspects of Prosody Speech Communication, Vol 36,
pp 31-43, 2002.
Hori C., Furui S., Malkin R., Yu H., Waibel A.. Au-
tomatic Speech Summarization Applied to English
Broadcast News Speech Proc. of ICASSP 2002, pp.
9-12 .
Inoue A., Mikami T., Yamashita Y. Improvement of
Speech Summarization Using Prosodic Information
Proc. of Speech Prosody 2004, Japan
Kupiec J., Pedersen J.O., Chen F. A Trainable Document
Summarizer Proc. of SIGIR 1995
Language Data Consortium ?TDT-2 Corpus Univ. of
Pennsylvania.
Maskey S. and Hirschberg J. 2005. Comparing Lexical,
Acoustic/Prosodic, Structural and Discourse Features
Proc. of ICSLP, Lisbon, Portugal.
Ohtake K., Yamamoto K., Toma y., Sado S., Ma-
suyama S. Newscast Speech Summarization via Sen-
tence Shortening Based on Prosodic Features Proc. of
SSPR pp.167-170. 2003
Shriberg E., Stolcke A., Hakkani-Tur D., Tur G. Prosody
Based Automatic Segmentation of Speech into Sen-
tences and Topics? Speech Communication 32(1-2)
September 2000
Tweed D., Fisher R., Bins J., List T, Efficient Hidden
Semi-Markov Model Inference for Structured Video
Sequences Proc. of (VS-PETS), pp 247-254, Beijing,
Oct 2005.
Witbrock M.J. and Mittal V.O. Ultra-Summarization: A
Statistical Approach to Generating Highly Condensed
Non-Extractive Summaries Proc. of SIGIR 1999
Zechner K. Automatic Generation of Concise Sum-
maries of Spoken Dialogues in Unrestricted Domains
Research and Development in Information Retrieval,
199-207, 2001.
92
Coling 2010: Poster Volume, pages 828?836,
Beijing, August 2010
A Power Mean Based Algorithm for Combining Multiple
Alignment Tables
Sameer Maskey, Steven J. Rennie, Bowen Zhou
IBM T.J. Watson Research Center
{smaskey, sjrennie, zhou}@us.ibm.com
Abstract
Most existing techniques for combining
multiple alignment tables can combine
only two alignment tables at a time, and
are based on heuristics (Och and Ney,
2003), (Koehn et al, 2003). In this pa-
per, we propose a novel mathematical for-
mulation for combining an arbitrary num-
ber of alignment tables using their power
mean. The method frames the combi-
nation task as an optimization problem,
and finds the optimal alignment lying be-
tween the intersection and union of multi-
ple alignment tables by optimizing the pa-
rameter p: the affinely extended real num-
ber defining the order of the power mean
function. The combination approach pro-
duces better alignment tables in terms of
both F-measure and BLEU scores.
1 Introduction
Machine Translation (MT) systems are trained on
bi-text parallel corpora. One of the first steps
involved in training a MT system is obtaining
alignments between words of source and target
languages. This is typically done using some
form of Expectation Maximization (EM) algo-
rithm (Brown et al, 1993), (Och and Ney, 2003),
(Vogel et al, 1996). These unsupervised algo-
rithms provide alignment links between english
words ei and the foreign words fj for a given e?f
sentence pair. The alignment pairs are then used
to extract phrases tables (Koehn et al, 2003), hi-
erarchical rules (Chiang, 2005), or tree-to-string
mappings (Yamada and Knight, 2001). Thus, the
accuracy of these alignment links has a significant
impact in overall MT accuracy.
One of the commonly used techniques to im-
prove the alignment accuracy is combining align-
ment tables obtained for source to target (e2f ) and
target to source (f2e) directions (Och and Ney,
2003). This combining technique involves obtain-
ing two sets of alignment tables A1 and A2 for the
same sentence pair e ? f , and producing a new
set based on union A? = A1 ? A2 or intersec-
tion A? = A1 ?A2 or some optimal combination
Ao such that it is subset of A1 ? A2 but a super-
set of A1 ? A2. How to find this optimal Ao is a
key question. A? has high precision but low re-
call producing fewer alignments and A? has high
recall but low precision.
2 Related Work
Most existing methods for alignment combina-
tion (symmetrization) rely on heuristics to iden-
tify reliable links (Och and Ney, 2003), (Koehn
et al, 2003). The method proposed in (Och and
Ney, 2003), for example, interpolates the intersec-
tion and union of two asymmetric alignment ta-
bles by adding links that are adjacent to intersec-
tion links, and connect at least one previously un-
aligned word. Another example is the method in
(Koehn et al, 2003), which adds links to the inter-
section of two alignment tables that are the diago-
nal neighbors of existing links, optionally requir-
ing that any added links connect two previously
unaligned words.
Other methods try to combine the tables dur-
ing alignment training. In (Liang et al, 2006),
asymmetric models are jointly trained to maxi-
mize the similarity of their alignments, by opti-
828
mizing an EM-like objective function based on
agreement heuristics. In (Ayan et al, 2004), the
authors present a technique for combining align-
ments based on various linguistic resources such
as parts of speech, dependency parses, or bilingual
dictionaries, and use machine learning techniques
to do alignment combination. One of the main dis-
advantages of (Ayan et al, 2004)?s method, how-
ever, is that the algorithm is a supervised learning
method, and so requires human-annotated data.
Recently, (Xiang et al, 2010) proposed a method
that can handle multiple alignments with soft links
which are defined by confidence scores of align-
ment links. (Matusov et al, 2004) on the other
hand, frame symmetrization as finding a set with
minimal cost using use a graph based algorithm
where costs are associated with local alignment
probabilities.
In summary, most existing alignment combina-
tion methods try to find an optimal alignment set
Ao that lies between A? and A? using heuristics.
The main problems with methods based on heuris-
tics are:
1. they may not generalize well across language
pairs
2. they typically do not have any parameters to
optimize
3. most methods can combine only 2 align-
ments at a time
4. most approaches are ad-hoc and are not
mathematically well defined
In this paper we address these issues by propos-
ing a novel mathematical formulation for com-
bining an arbitrary number of alignment tables.
The method frames the combination task as an op-
timization problem, and finds the optimal align-
ment lying between the intersection and union of
multiple alignment tables by optimizing the pa-
rameter p of the power mean function.
3 Alignment combination using the
power mean
Given an english-foreign sentence pair (eI1, fJ1 )
the alignment problem is to determine the pres-
ence of absence of alignment links aij between
the words ei and fj , where i ? I and j ? J . In
this paper we will use the convention that when
aij = 1, words ei and fj are linked, otherwise
aij = 0. Let us define the alignment tables we ob-
tain for two translation directions as A1 and A2,
respectively. The union of these two alignment
tables A? contain all of the links in A1 and A2,
and the intersection A? contain only the common
links. Definitions 1 and 2 below define A? and
A? more formally. Our goal is to find an align-
ment set Ao such that |A?| ? |Ao| ? |A?| that
maximizes some objective function. We now de-
scribe the power mean (PM) and show how the
PM can represent both the union and intersection
of alignment tables using the same formula.
The power mean:
The power mean is defined by equation 1 below,
where p is a real number in (??,?) and an is a
positive real number.
Sp(a1, a2, ..., an) = (
1
n
n?
k=1
apk)
1
p (1)
The power mean, also known as the generalized
mean, has several interesting properties that are
relevant to our alignment combination problem.
In particular, the power mean is equivalent to the
geometric mean G when p? 0 as shown in equa-
tion 2 below:
G(a1, a2, ..., an) = (
n?
i=1
ai)
1
n
= lim
p?0
( 1n
n?
k=1
apk)
1
p (2)
The power mean, furthermore, is equivalent to the
maximum function M when p??:
M(a1, a2, ..., an) = max(a1, a2, ..., an)
= lim
p??(
1
n
n?
k=1
apk)
1
p (3)
Importantly, the PM Sp is a non-decreasing
function of p. This means that Sp is lower
bounded by G and upper-bounded by M for p ?
[0, ?]:
G < Sp < M, 0 < p <?. (4)
829
Figure 1: The power-mean is a principled way to interpolate between the extremes of union and inter-
section when combining multiple alignment tables.
They key insight underpinning our mathematical
formulation of the alignment combination prob-
lem is that the geometric mean of multiple align-
ment tables is equivalent to their intersection,
while the maximum of multiple alignment tables
is equivalent to their union.
Let Aq be an alignment with elements aqij such
that aqij = 1 if words ei and fj are linked, and
aqij = 0 otherwise. The union and intersection of
a set of n alignment tables can then be formally
defined as follows:
Definition 1: The union of alignments
A1, A2, ..., An is a set A? with a?ij = 1 if aqij = 1
for any q ? {1, 2, ..., n}.
Definition 2: The intersection of alignments
A1, A2, ..., An is a set A? with a?ij = 1 if aqij = 1
for all q ? {1, 2, ..., n}.
Figure 1 depicts a simple example of the align-
ment combination problem for the common case
of alignment symmetrization. Two alignments ta-
bles, Ae?f and Af?e (one-to-many alignments),
need to be combined. The result of taking
the union A? and intersection A? of the ta-
bles is shown. A? can be computed by taking
the element-wise maximum of Ae?f and Af?e,
which in turn is equal to the power mean Ap of
the elements of these tables in the limit as p??.
The intersection of the two tables, A?, can simi-
larly be computed by taking the geometric mean
of the elements of Ae?f and Af?e, which is
equal to the power mean Ap of the elements of
these tables in the limit as p? 0. For p ? (0,?),
equation 4 implies that Ap has elements with val-
ues between A? and A?. We now provide formal
proofs for these results when combining an arbi-
trary number of alignment tables.
3.1 The intersection of alignment tables
A1..An is equivalent to their
element-wise geometric mean
G(A1, A2, ..., An), as defined in (2).
Proof : Let A? be the intersection of all Aq
where q ? {1, 2, .., n}. As per our definition of
intersection ? between alignment tables, A? con-
tains links where aqij = 1 ? q.
Let Ag be the set that contains the elements
830
of G(A1, A2, ..., An). Then agij is the geo-
metric mean of the elements aqij where q ?
{1, 2, .., n}, as defined in equation 2, that is, agij =
(?nq=1 agij)
1
n . This product is equal to 1 iff aqij =
1 ? q and zero otherwise, since aqij ? {0, 1} ? q.
Hence Ag = A?. Q.E.D.
3.2 The union of alignment tables A1..An is
equivalent to their element-wise
maximum M(A1, A2, ..., An), as defined
in (3).
Proof : Let A? be the union of all Aq for q ?
{1, 2, .., n}. As per our definition of the union be-
tween alignments A? has links where aqij = 1 for
some q.
Let Am be the set that contain the elements of
M(A1, A2, ..., An). Let amij be the maximum of
the elements aqij where q ? {1, 2, .., n}, as defined
in equation (3). The max function is equal to 1
iff aqij = 1 for some q and zero otherwise, since
aqij ? {0, 1} ? q. Hence Am = A?. Q.E.D.
3.3 The element-wise power mean
Sp(A1, A2, ..., An) of alignment tables
A1..An has entries that are
lower-bounded by the intersection of
these tables, and upper-bounded by their
union for p ? [0, ?].
Proof : We have already shown that the union
and intersection of a set of alignment tables are
equivalent to the maximum and geometric mean
of these tables, respectively. Therefore given that
the result in equation 4 is true (we will not prove it
here), the relation holds. In this sense, the power
mean can be used to interpolate between the in-
tersection and union of multiple alignment tables.
Q.E.D.
4 Data
We evaluate the proposed method using an
English-Pashto translation task, as defined by the
DARPA TransTac program. The training data for
this task consists of slightly more than 100K par-
allel sentences. The Transtac task was designed to
evaluate speech-to-speech translation systems, so
all training sentences are conversational in nature.
The sentence length of these utterances varies
greatly, ranging from a single word to more than
Method F-measure
I 0.5979
H 0.6891
GDF 0.6712
PM 0.6984
PMn 0.7276
U 0.6589
Table 1: F-measure Based on Various Alignment
Combination Methods
50 words. 2026 sentences were randomly sampled
from this training data to prepare held out devel-
opment set. The held out Transtac test set consists
of 1019 parallel sentences.
5 Experiments and Discussion
We have shown in the previous sections that union
and intersection of alignments can be mathemat-
ically formulated using the power mean. Since
both combination operations can be represented
with the same mathematical expression, we can
search the combination space ?between? the in-
tersection and union of alignment tables by op-
timizing p w.r.t. any chosen objective function.
In these experiments, we define the optimal align-
ment as the one that maximizes the objective func-
tion f({aijt}, {a?ijt}, p), where f is standard F-
measure, {a?ijt} is the set of all estimated align-
ment entries on some dataset, {aijt} is the set of
all corresponding human-annotated alignment en-
tries, and p is the order of the power mean func-
tion. Instead of attempting to optimize the F-
measure using heuristics, we can now optimize it
by finding the appropriate power order p using any
suitable numerical optimization algorithm. In our
experiments we used the general simplex algo-
rithm of amoeba search (Nelder and Mead, 1965),
which attempts to find the optimal set of parame-
ters by evolving a simplex of evaluated points in
the direction that the F-measure is increasing.
In order to test our alignment combination for-
mulation empirically we performed experiments
on English-Pashto language with data described in
Section 4. We first trained two sets of alignments,
the e2f and f2e directions, based on GIZA++
(Och and Ney, 2003) algorithm. We then com-
bined these alignments by performing intersec-
831
tion (I) and union (U). We obtained F-measure of
0.5979 for intersection (I), 0.6589 for union (U).
For intersection the F-measure is lower presum-
ably because many alignments are not shared by
the input alignment tables so the number of links
is under-estimated. We then also re-produced the
two commonly used combination heuristic meth-
ods that are based on growing the alignment di-
agonally (GDF) (Koehn et al, 2003), and adding
links based on refined heuristics (H) (Och and
Ney, 2003), respectively. We obtained F-measure
of 0.6891 for H, and 0.6712 for GDF as shown in
Table 1.
We then used our power mean formulation for
combination to maximize the F-measure function
with the aforementioned simplex algorithm for
tuning the power parameter p, where F-measure
is computed with respect to the hand aligned de-
velopment data, which contains 150 sentences.
This hand aligned development set is different
than the development set for training MT models.
While doing so we also optimized table weights
Wq ? (0, 1),
?
q Wq = 1, which were applied to
the alignment tables before combining them using
the PM. The Wq allow the algorithm to weight the
two directions differently. We found that the F-
measure function had many local minima so the
simplex algorithm was initialized at several val-
ues of p and {Wq} to find the globally optimal
F-measure.
After obtaining power mean outputs for the
alignment entries, they need to be converted
into binary valued alignment links, that is,
Sp(a1ij , a2ij , ...anij) needs to be converted into a bi-
nary table. There are many ways to do this con-
version such as simple thresholding or keeping
best N% of the links. In our experiments we used
the following simple selection method, which ap-
pears to perform better than thresholding. First we
sorted links by PM value and then added the links
from the top of the sorted list such that ei and fj
are linked if ei?1 and ei+1 are connected to fj , or
fj?1 and fj+1 is linked to ei, or both ei and fj are
not connected. After tuning power mean parame-
ter and the alignment weights the best parameter
gave an F-measure of 0.6984 which is higher than
commonly used GDF by 2.272% and H by 0.93%
absolute respectively. We observe in Figure 2 that
even though PM has higher F-measure compared
with GDF it has significantly fewer number of
alignment links suggesting that PM has improved
precision on the finding the alignment links. The
presented PM based alignment combination can
be tuned to optimize any chosen objective, so it is
not surprising that we can improve upon previous
results based on heuristics.
One of the main advantages of the combining
alignment tables using the PM is that our state-
ments are valid for any number of input tables,
whereas most heuristic approaches can only pro-
cess two alignment tables at a time. The presented
power mean algorithm, in contrast, can be used
to combine any number of alignments in a sin-
gle step, which, importantly, makes it possible to
jointly optimize all of the parameters of the com-
bination process.
In the second set of experiments the PM ap-
proach, which we call PMn, is applied simultane-
ously to more than two alignments. We obtained
four more sets of alignments from the Berke-
ley aligner (BA) (Liang et al, 2006), the HMM
aligner (HA) (Vogel et al, 1996), the alignment
based on partial words (PA), and alignment based
on dependency based reordering (DA) (Xu et al,
2009). Alignment I was obtained by using Berke-
ley aligner as an off-the-shelf alignment tool. We
built the HMM aligner based on (Vogel et al,
1996) and use the HMM aligner for producing
Alignment II. Producing different sets of align-
ments using different algorithms could be useful
because some alignments that are pruned by one
algorithm may be kept by another giving us a big-
ger pool of possible links to chose from.
We produced Alignment III based on partial
words. Pashto is morphologically rich language
with many prefixes and suffixes. In lack of a mor-
phological segmenter it has been suggested that
keeping only first ?n? characters of a word can ef-
fectively reduce the vocabulary size and may pro-
duce better alignments. (Chiang et al, 2009) used
partial words for alignment training in English and
Urdu. We trained such alignments using using
GIZA++ on parallel data with partial words for
Pashto sentences.
The fourth type of alignment we produced,
Alignment IV, was motivated by the (Xu et al,
832
Figure 2: Number of Alignments Links for Dif-
ferent Combination Types
2009). (Xu et al, 2009) showed that transla-
tion between subject-verb-object (English) and
subject-object-verb (Pashto) languages can be im-
proved by reordering the source side of the par-
allel data. They obtained dependency tree of the
source side and used high level human gener-
ated rules to reorder source side using precedence-
based movement of dependency subtrees. The
rules were particularly useful in reordering of
verbs that moved to the end of the sentence. Mak-
ing the ordering of source and target side more
similar may produce better alignments for lan-
guage pairs which differ in verb ordering, as many
alignment algorithms penalize or fail to consider
alignments that link words that differ greatly in
sentence position. A Pashto language expert was
hired to produce similar precedence-based rules
for the English-Pashto language pair. Using the
rules and algorithm described in (Xu et al, 2009)
we reordered all of the source side and used
GIZA++ to align the sentences.
The four additional alignment sets just de-
scribed, including our baseline alignment, Align-
ment V, were combined using the presented PMn
combination algorithm, where n signifies the
number of tables being combined. As seen on
Table 1, we obtained an F-measure of 0.7276
which is 12.97% absolute better than intersection
and 6.87% better than union. Furthermore PMn,
which in these experiments utilizes 5 alignments,
is better than PM by 2.92% absolute. This is an
encouraging result because this not only shows
that we are finding better alignments than inter-
section and union, but also that combining more
than two alignments is useful. We note that PMn
performed 3.85% absolute better than H (Och and
Ney, 2003), and 5.64% better than GDF heuris-
tics.
In the above experiments the parameters of
the power mean combination method were tuned
on development data to optimize alignment F-
measure, and the performance of several align-
ment combination techniques were compared in
terms of F-measure. However, it is not clear how
correlated alignment F-measures are with BLEU
scores, as explained in (Fraser and Marcu, 2007).
While there is no mathematical problem with
optimizing the parameters of the presented PM-
based combination algorithm w.r.t. BLEU scores,
computationally it is not practical to do so because
each iteration would require a complete training
phase. To further evaluate the quality of the align-
ments methods being compared in this paper, we
built several MT models based on them and com-
pared the resulting BLEU scores.
E2F Dev Test
I 0.1064 0.0941
H 0.1028 0.0894
GDF 0.1256 0.1091
PM 0.1214 0.1094
PMn 0.1378 0.1209
U 0.1062 0.0897
Table 2: E2F BLEU: PM Alignment Combination
Based MT Model Comparision
We built a standard phrase-based translation
system (Koehn et al, 2003) that utilizes a stack-
based decoder based on an A? search. Based on
the combined alignments, we extracted phrase ta-
bles with a maximum phrase length of 6 for En-
glish and 8 for Pashto, respectively. We then
trained the lexicalized reordering model that pro-
duced distortion costs based on the number of
words that are skipped on the target side, in
a manner similar to (Al-Onaizan and Papineni,
2006). Our training sentences are a compilation
of sentences from various domains collected by
DARPA, and hence we were able to build interpo-
lated language model which weights the domains
differently. We built an interpolated LM for both
833
English and Pashto, but for English we had signif-
icantly more monolingual sentences (1.4 million
in total) compared to slightly more than 100K sen-
tences for Pashto. We tuned our MT model using
minimum error rate (Och, 2003) training.
F2E Dev Test
I 0.1145 0.1101
H 0.1262 0.1193
GDF 0.1115 0.1204
PM 0.1201 0.1155
PMn 0.1198 0.1196
U 0.1111 0.1155
Table 3: F2E BLEU : PM Alignment Combina-
tion Based MT Model Comparision
We built five different MT models based on
Intersection (I), Union (U), (Koehn et al, 2003)
Grow Diagonal Final (GDF), (Och and Ney, 2003)
H refined heuristics and Power Mean (PMn) align-
ment sets where n = 5. We obtained BLEU (Pa-
pineni et al, 2002) scores for E2F direction as
shown in Table 2. As expected MT model based
on I alignment has the low BLEU score of 0.1064
on the dev set and 0.0941 on the test set on E2F
direction. Intersection, though, has higher preci-
sion, but throws away many alignments, so the
overall number of alignments is too small to pro-
duce a good phrase translation table. Similarly
the U alignment also has low scores (0.1062 and
0.0897) on the dev and test sets, respectively. The
best scores for E2F direction for both dev and test
set is obtained using the model based on PMn al-
gorithm. We obtained BLEU scores of 0.1378 on
the dev set and 0.1209 on the test set which is bet-
ter than all heuristic based methods. It is better
by 1.22 absolute BLEU score on the dev set and
1.18 on a test compared to commonly used GDF
(Koehn et al, 2003) heuristics. The above BLEU
scores were all computed based on 1 reference.
Note that for the e2f direction PM, which com-
bines only 2 alignments, is not worse than any of
the heuristic based methods. Also note that the
difference in the BLEU score of PM and PMn is
quite large, which indicates that combining more
than two alignments using the power mean leads
to substantial gains in performance.
Although we saw significant gains on E2F di-
Type PT Size (100K)
I 182.17
H 30.73
GDF 27.65
PM 60.87
PMn 25.67
U 24.54
Table 4: E2F Phrase Table Size
rection we did not see similar gains on F2E di-
rection unfortunately. Matching our expectation
Intersection (I) produced the worse results with
BLEU scores of 0.1145 and 0.1101 on the dev
and test set respectively, as shown in Table 3. Our
PMn algorithm obtained BLEU score of 0.1198
on the dev set and 0.1196 on test set which is
better by 0.83 absolute in dev set over GDF. On
the test set though performance between PMn and
GDF is only slightly different with 0.1196 for
PMn and 0.1204 for GDF. The standard deviation
on test set BLEU scores for F2E direction is only
0.0042 which is one third of the standard devia-
tion in E2F direction at 0.013 signifying that the
alignment seems to make less difference in F2E
direction for our models. One possible explana-
tion for such results is that the Pashto LM for the
E2F direction is trained on a small set of sen-
tences available from training corpus while En-
glish LM for F2E direction was trained on 1.4 mil-
lion sentences. Therefore the English LM, which
is trained on significantly more data, is probably
more robust to translation model errors.
Type PT Size (100K)
I 139.98
H 56.76
GDF 22.96
PM 47.50
PMn 21.24
U 20.33
Table 5: F2E Phrase Table Size
Note that different alignments lead to differ-
ent phrase table (PT) sizes (Figure 2). The inter-
section (I) method has the least number of align-
ment links, and tends to produce the largest phrase
tables, because there are less restrictions on the
834
phrases to be extracted. The Union (U) method,
on the other hand, tends to produce the least num-
ber of phrases, because the phrase extraction al-
gorithm has more constraints to satisfy. We ob-
serve that PT produced by intersection is signifi-
cantly larger than others as seen in Tables 4 and
5. The PT size produced by PMn as shown in
Table 4 is between I and U and is significantly
smaller than the other heuristic based methods. It
is 7.1% smaller than GDF heuristic based phrase
table. Similarly in F2E direction as well (Table
5) we see the similar trend where PMn PT size
is smaller than GDF by 4.2%. The decrease in
phrase table size and increase in BLEU scores for
most of the dev and test sets show that our PM
based combined alignments are helping to pro-
duce better MT models.
6 Conclusion and Future Work
We have presented a mathematical formulation for
combining alignment tables based on their power
mean. The presented framework allows us to find
the optimal alignment between intersection and
union by finding the best power mean parameter
between 0 and ?, which correspond to intersec-
tion and union operations, respectively. We eval-
uated the proposed method empirically by com-
puting BLEU scores in English-Pashto transla-
tion task and also by computing an F-measure
with respect to human alignments. We showed
that the approach is more effective than intersec-
tion, union, the heuristics of (Och and Ney, 2003),
and the grow diagonal final (GDF) algorithm of
(Koehn et al, 2003). We also showed that our al-
gorithm is not limited to two tables, which makes
it possible to jointly optimize the combination of
multiple alignment tables to further increase per-
formance.
In future work we would like to address two
particular issues. First, in this work we converted
power mean outputs to binary alignment links by
simple selection process. We are currently investi-
gating ways to integrate the binary constraint into
the PM-based optimization algorithm. Second,
we do not have to limit ourselves to alignments ta-
bles that are binary. PM based algorithm can com-
bine alignments that are not binary, which makes
it easier to integrate other sources of information
such as posterior probability of word translation
into the alignment combination framework.
7 Acknowledgment
This work is partially supported by the DARPA
TRANSTAC program under the contract number
of NBCH2030007. Any opinions, findings, and
conclusions or recommendations expressed in this
material are those of the authors and do not nec-
essarily reflect the views of DARPA.
References
Al-Onaizan, Yaser and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings of ACL.
Ayan, Necip, Bonnie J. Dorr, , and Nizar Habash.
2004. Multi-align: Combining linguistic and statis-
tical techniques to improve alignments for adaptable
mt. In Proceedings of the 6th Conference of the As-
sociation for Machine Translation in the Americas.
Brown, P., V. Della Pietra, S. Della Pietra, and R. Mer-
cer. 1993. The mathematics of statistical machine
translation: parameter estimation. Computational
Linguistics, 19(2):263?311.
Chiang, David, Kevin Knight, and Samad Echihabi.
2009. In Presentation at NIST MT 2009 Workshop,
August.
Chiang, David. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of ACL.
Fraser, Alexander and Daniel Marcu. 2007. Measur-
ing word alignment quality for statistical machine
translation. Comput. Linguist., 33(3):293?303.
Koehn, Philipp, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of HLT/NAACL.
Liang, Percy, Ben Taskar, and Dan Klein. 2006.
Alignment by agreement. In Proceedings of ACL.
Matusov, Evgeny, Richard Zens, and Hermann Ney.
2004. Symmetric word alignments for statistical
machine translation. In Proceedings of COLING,
page 219, Morristown, NJ, USA.
Nelder, JA and R Mead. 1965. A simplex method for
function minimization. The Computer Journal 7:
308-313.
Och, F. J. and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19?51.
835
Och, Franz J. 2003. Minimum error rate training in
statistical machine. In Proceedings of ACL.
Papineni, Kishore, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: A method for automatic eval-
uation of machine translation. In In Proceedings of
ACL, pages 311?318.
Vogel, Stephan, Hermann Ney, and Christoph Till-
mann. 1996. Hmm-based word alignment in statis-
tical translation. In COLING 96: The 16th Int. Conf.
on Computational Linguistics, pages 836?841.
Xiang, Bing, Yonggang Deng, and Bowen Zhou. 2010.
Diversify and combine: Improving word alignment
for machine translation on low-resource languages.
In Proceedings of ACL.
Xu, Peng, Jaeho Kang, Michael Ringgaard, and Franz
Och. 2009. Using a dependency parser to improve
smt for subject-object-verb languages. In NAACL,
pages 245?253, Morristown, NJ, USA.
Yamada, Kenji and Kevin Knight. 2001. A syntax-
based statistical translation model. In Proceedings
of ACL, pages 523?530, Toulouse, France, July.
ACL.
836
Proceedings of NAACL-HLT 2013, pages 878?887,
Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational Linguistics
Finding What Matters in Questions
Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli, Sameer Maskey and Radu Florian
IBM T.J. Watson Research Center
1101 Kitchawan Road, Yorktown Heights, NY 10598
{xiaoluo,hraghav,vittorio,smaskey,raduf}@us.ibm.com
Abstract
In natural language question answering (QA)
systems, questions often contain terms and
phrases that are critically important for re-
trieving or finding answers from documents.
We present a learnable system that can ex-
tract and rank these terms and phrases (dubbed
mandatory matching phrases or MMPs), and
demonstrate their utility in a QA system on In-
ternet discussion forum data sets. The system
relies on deep syntactic and semantic analysis
of questions only and is independent of rele-
vant documents. Our proposed model can pre-
dict MMPs with high accuracy. When used in
a QA system features derived from the MMP
model improve performance significantly over
a state-of-the-art baseline. The final QA sys-
tem was the best performing system in the
DARPA BOLT-IR evaluation.
1 Introduction
In most question answering (QA) systems and
search engines term-weights are assigned in a con-
text independent fashion using simple TF-IDF like
models (Robertson and Walker, 1994; Ponte and
Croft, 1998). Even the more recent advances
in information retrieval techniques for query term
weighting (Bendersky et al, 2010; Bendersky, 2011)
typically rely on bag-of-words models and cor-
pus statistics, such as inverse-document-frequency
(IDF), to assign weights to terms in questions. While
such solutions may work for keyword queries of the
type common on search engines such as Google,
they do not exploit syntactic and semantic informa-
tion when it comes to well formed natural language
questions. In this paper we propose a new model
that identifies important terms and phrases in a natu-
ral language question, providing better query analy-
sis that ultimately leads to significant improvements
in a QA system.
To motivate the work presented here, consider the
query ?How does one apply for a New York day care
license??. A bag-of-words model would likely as-
sign a high score to ?New licenses for day care cen-
ters in York county, PA? because of high word over-
lap, but it does not answer the question, and also
the state is wrong. A matching component that uses
the phrases ?New York,? ?day care,? and ?license?
is likely to do better. However, a better matching
component will understand that in the context of this
query all three phrases ?New York,? ?day care? and
?license? are important, and that ?New York? needs
to modify ?day care.? A snippet that does not con-
tain1 these important phrases, is unlikely an answer.
We call these important phrases mandatory match-
ing phrases (MMPs).
In this paper, we explore deep syntactic and se-
mantic analyses of questions to determine and rank
MMPs. Unlike existing work (Zhao and Callan,
2010; Bendersky et al, 2010; Bendersky, 2011),
where term/concept weights are learned from a set
of questions and judged documents based on corpus-
based statistics, we annotate questions and build a
trainable system to select and score MMPs. This
model relies heavily on existing syntactic parsers
and semantic-oriented named-entity recognizers, but
does not need question answer pairs. This is espe-
1
?contain? here means semantic equivalence or entailment,
not necessarily the exact words or phrases.
878
cially attractive at the initial system-building stage
when no or little answer data is available.
The main contributions of this paper are: firstly,
we propose a framework to select and rank impor-
tant question phrases (MMPs) for question answer-
ing in Section 3. This framework seamlessly incor-
porates lexical, syntactic and semantic information,
resulting in an MMP prediction F-measure as high
as 88.6%. Secondly, we show that features derived
from identified MMPs improve significantly a rele-
vance classification model, in Section 4.2. Thirdly,
we show that using the improved relevance model
into our QA system results in a statistically signifi-
cant 5 point improvement in F-measure, in Section
5. This finding is further corroborated by the results
on the official 2012 BOLT IR (IR, 2012) task where
the combined system yielded the best performance
in the evaluation.
2 Related Work
Popular information retrieval systems like
BM25 (Robertson and Walker, 1994) and language
models (Ponte and Croft, 1998) use unsupervised
techniques based on corpus statistics for term
weighting. Many of these techniques are variants
of the one proposed by (Luhn, 1958). Recently,
several researchers have studied approaches for term
weighting using supervised learning techniques.
However, much of this research has focused on
information retrieval task rather than on question
answering problems of the nature addressed in
this paper. (Bendersky and Croft, 2008) restricted
themselves to predicting key noun phrases, which
is perhaps sufficient for a retrieval task. However,
for questions like ?Find comments about how
American hedge funds legally avoid taxes,? the verb
?avoid? is perhaps as important as the noun phrase
?American hedge funds? and ?taxes?. Works like
that of (Lease et al, 2009) and (Zhao and Callan,
2010) predict importance at the word level. While
word level importance is perhaps sufficient for
an IR task, predicting the importance of phrases,
especially those derived from a parse tree, gives
a much richer representation that might also be
useful for better question understanding and thus
generate more relevant answers. Both (Lease et al,
2009; Zhao and Callan, 2010) propose supervised
methods that learn from a large set of queries and
relevance judgments on their answers. While this is
possible in a TREC Ad-hoc-retrieval-like task, such
a large training corpus of question-answer pairs is
unavailable for most scenarios. (Monz, 2007) learns
term weights for the IR component of a question
answering task. His work unlike ours does not aim
to find the answers to the questions.
Most QA systems in the literature have dealt
with answering factoid questions, where the an-
swer is a noun phrase in response to questions of
the form ?Who,? ?Where,? ?When.? Most sys-
tems have a question analysis component that rep-
resents the question as syntactic relations in a parse
or as deep semantic relations in a handcrafted on-
tology (Hermjakob et al, 2000; Chu-carroll et al,
2003; Moldovan et al, 2003). In addition certain
systems (Bunescu and Huang, 2010) aim to find the
?focus? of the question, that is, the noun-phrases in
the question that would co-refer with answers. Ad-
ditionally, much past work has focused on finding
the lexical answer type (Pinchak, 2006; Li and Roth,
2002). Since these papers considered a small num-
ber of answer types, rules over the detected relations
and answer types could be applied to find the rel-
evant answer. However, since our system answers
non-factoid questions that can have answer of arbi-
trary types, we want to use as few rules as possible.
The MMPs therefore become a critical component
of our system, both for question analysis and for rel-
evance detection.
3 Question Data and MMP Model
To train the MMP model, we first create a set of
questions and label their MMPs. The labeled data
is then used to train a statistical model to predict
MMPs for new questions as discussed next.
3.1 Question Corpus
We use a subset of the DARPA BOLT corpus (see
Section 5.1) containing forum postings in English.
Four annotators use a search tool to explore this
document collection. They can perform keyword
searches and retrieve forum threads from which they
generate questions. The program participants de-
cided a basic set of question types that are out-of-
scope of the current research agenda. Accordingly,
879
annotators cannot generate questions (1) that require
reasoning or calculation over the data to compute
the answers; (2) that are vague or ambiguous; (3)
that can be broken into multiple disjoint questions;
(4) that are multiple choice questions; (5) that are
factoid questions?the kinds that have already been
well studied in TREC (Voorhees, 2004). Any other
kind of question is allowed. Two other annotators,
who have neither browsed the corpus nor generated
the questions, mark selected spans of the questions
into one of two categories?MMP-Must and MMP-
maybe. The annotation tool allows arbitrary spans
to be highlighted and the annotators are instructed to
select spans corresponding to the smallest semantic
units. The phrases that are very likely to appear con-
tiguously in a relevant answer are marked as MMP-
Must. Annotators can mark multiple spans per ques-
tion, but not overlapping spans. We generated 201
annotated questions using this process.
Figure 1 contains an example, where ?American,?
?hedge fund,? and ?legally avoid taxes? are required
elements to find answers and are thus marked as
MMP-Musts (signified by enclosing rectangles). We
purposely annotate MMPs at the word level and not
in the parse tree, because this requires minimal lin-
guistic knowledge. We do, however, employ an
automatic procedure to attach MMPs to parse tree
nodes when generating MMP training instances.
3.2 MMP Training
Questions annotated in Section 3.1 are first pro-
cessed by an information extraction (IE) pipeline
consisting of syntactic parsing, mention detection
and coreference resolution (Florian et al, 2004; Luo
et al, 2004; Luo and Zitouni, 2005). After IE, we
have access to the syntactic structure represented by
a parse tree and semantic information represented
by coreferenced mentions (including those of named
entities).
To take advantage of the availability of the syn-
tactic and semantic information, we first attach the
MMP annotations to parse tree nodes of a question,
and, if necessary, we augment the parse tree.
There are several reasons why we want to embed
the MMPs into a parse tree. First, many constituents
in parse trees correspond to important phrases we
want to capture, especially proper names. Second,
after an MMP is attached to a tree node, the problem
VP0
VB
Find comments
NNS
about
IN
SBAR
PP
VP
how
WRB NNP NN
legally taxes
NNS
funds
RB NNS
NP
S
avoidhedge
NP
WHADVP
VBP
 American
GPE
NP NP1
NP
2
Figure 1: MMPs are aligned with tree nodes: MMPs
are shown in rectangular boxes along with their aligned
nodes (with slanted labels); augmented parse tree nodes
(i.e., NP1, NP2) in dashed nodes. Dotted edges under
NP0 are the structure before the tree is augmented.
of predicting MMPs reduces to classifying parse tree
nodes, and syntactic information can be naturally
built into the MMP classifier. Lastly, and more im-
portantly, associating MMPs with tree nodes opens
the door to explore features derived from the syn-
tactic parse tree. For instance, it is easy to read
bilexical dependencies from a parse tree (provided
that head information is propagated); with MMPs
aligned with the parse tree, bilexical dependencies
can be ranked by examining whether or not an MMP
phrase is a head or a dependent. This way, not
only are the dependencies in a question captured, but
MMP scores or ranks can be propagated to depen-
dencies as well. We will discuss more how MMP
features are computed in Section 4.2.2.
Annotators can mark MMPs that are not perfectly
aligned with a tree node. Hence, care has to be taken
when generating MMP training instances. As an ex-
ample, In Figure 1, ?American? and ?hedge funds?
are marked as two separate MMPs, but the Penn-
Tree-style parse tree has a flat ?NP0? constituent
spanning directly on ?American hedge fund,? illus-
trated in Figure 1 as dotted edges.
To anchor MMPs in the parse tree, we augment
it by combining the IE output and the MMP anno-
tation. In the aforementioned example, ?American?
is a named mention with the entity type GPE (geo-
political entity) and there is no non-terminal node
spanning it: so, a new node ?NP1? is created; ?hedge
funds? is marked as an MMP: so, a second node
(?NP2?) is created to anchor it.
880
A training instance for building the MMP model
is defined as a span along with an MMP label. For
instance, ?hedge funds? in Figure 1 will generate a
positive training instance as ?(5,6), +1?, where
(5,6) is the span of ?hedge funds? in the question
sentence, and +1 signifies that it is a positive train-
ing instance. For the purpose of this paper we use
only binary labels, mapping all MMP-Must to +1
and MMP-Skip and MMP-Maybe to ?1.
Formally, we use the following procedure to gen-
erate training instances:
Algorithm 1 Pseudo code to generate MMP training
instances.
Input: An input question tree with detected men-
tions and marked MMPs
Output: A list of MMP training instances
1: Foreach mention m in the question
2: if no node spans m, and m does not cross bracket
3: Find lowest node N dominating m
4: Insert a child node of N that spans exactly m
5: Foreach mention p in marked MMPs
6: Find lowest non-terminal Np dominating p
7: Generate a positive training example for Np
8: Mark Np as visited
9: Recursively generate instances for Np?s children
10: Generate a negative training instance for all un-
visited nodes in Step 5-9
Steps 1 to 4 augment the question tree by creating
a node for each named mention, provided that no ex-
isting node spans exactly the mention and the men-
tion does not cross-bracket tree constituents. Steps 5
to 8 generate positive training instances for marked
MMPs; step 9 recursively generates positive training
instances 2 for tree nodes dominated by Np, where
Np is the lowest non-terminal node dominating the
marked MMP p.
After MMP training instances are generated we
design and compute features for each instance, and
use them to train a classifier.
3.3 MMP Features and Classifier
We compute four types of features that will be used
in a statistical classifier. These features are designed
to characterize a phrase from the lexical, syntactic,
2One exception to this step is that if a node spans a single
stop word, then a negative training instance is generated.
semantic and corpus-level aspect. The weights asso-
ciated with these features are automatically learned
from training data.
We will use ?(NP1 American)? in Figure 1 as the
running example below.
Lexical Features: Lexical features are motivated by
the observation that spellings in English sometimes
offer important cues about word significance. For
example, an all-capitalized word often signifies an
acronym; an all-digit word in a question is likely a
year, etc. We compute the following lexical features
for a candidate MMP:
CaseFeatures: is the first word of an MMP
upper-case? Is it all capital letters? Does it contain
numeric letters? For ?(NP American)? in Figure 1,
the upper-case feature fires.
CommonQWord: Does the MMP contain question
words, including ?What,? ?When,? ?Who,? etc.
Syntactic Features: The second group of features
are computed from syntactic parse trees after anno-
tated MMPs are aligned with question parse-trees
as described previously.
PhraseLabel: this feature returns the phrasal label
of the MMP. For ?(NP American)? in Figure 1, the
feature value is ?NP.? This captures that an NP is
more likely an MMP than, say, an ADVP.
NPUnique: this Boolean feature fires if a phrase
is the only NP in a question, indicating that this
constituent probably should be matched. For ?(NP
American),? the feature value would be false.
PosOfPTN: these features characterize the position
of the parse tree node to which an MMP is anchored.
They compute: (1) the position of the left-most
word of the node; (2) whether the left-most word is
the beginning of the question; (3) the depth of the
anchoring node, defined as the length of the path to
the root node. For ?(NP American)? in Figure 1, the
features state that it is the 5th word in the sentence;
it is not the first word of the sentence; and the depth
of the node is 6 (where root has depth 0).
PhrLenToQLenRatio: This feature computes the
number of words in an MMP, and its relative ratio to
the sentence length. This feature controls the length
of MMPs at decoding time, since most of MMPs
are short.
Semantic Features (NETypes): The third group of
features are computed from named entities and aim
to capture semantic information. The feature tests if
881
a phrase is or contains a named entity, and, if this
is the case, the value is the entity type. For ?(NP
American)? in Figure 1, the feature value would be
?GPE.?
Corpus-based Features ( AvgCorpusIDF): This
group of features computes the average of the IDFs
of the words in this phrase. From the corpus IDF,
we also compute the ratio between the number of
stop words and the total number of words in the
MMP, and use it as another feature.
3.4 MMP Classification Results
We now show that we can reliably predict MMPs of
questions. We split our set of 201 annotated ques-
tions into a training set consisting of 174 questions
and a test set with the remaining 27 questions. We
use the procedure and features described in Sec-
tion 3 to train a logistic regression binary classifier
using WEKA. Then, the trained MMP classifier is
applied to the test set question trees. Since the class
bias is quite skewed (only 16% of the phrases are
marked as MMP-Must) we also use re-sampling at
training time to balance the prior probability of the
two classes. At testing time, a parser and a men-
tion detection algorithm (Florian et al, 2004; Luo et
al., 2004; Luo and Zitouni, 2005) are run on each
question. The detected mentions are then used to
augment the question parse trees. The MMP classi-
fier achieves an 88.6% F-measure (cf. Table 1, with
91.6% precision). This is a respectable number, con-
sidering the limited amount of training data. We ex-
perimented with decision trees and bagging as well
but found logistic regression to work the best.
Feature P R F1
AvgCorpusIDF 0.849 0.634 0.725
+NPUnique 0.868 0.634 0.732
+NETypes 0.867 0.662 0.750
+PhraseLabel 0.890 0.705 0.783
+CaseFeatures 0.829 0.820 0.824
+PosOfPTN 0.911 0.852 0.880
+PhrLenToQLenRatio 0.915 0.855 0.883
+commonQWord 0.916 0.858 0.886
Table 1: The performances of the MMP classifier while
incrementally adding features.
The examples in Table 2 illustrate the top three
MMPs produced by the model on two questions.
These results are encouraging: in the first exam-
ple the word AIDS is clearly the most ?important?
word, but IDF alone is not adequate to place it in the
top since AIDS is also a common verb (words are
lower-cased before IDF look-up). Similarly, in the
third example, the phrase ?the causes? has a much
higher MMP score than the phrase ?the concerns?
(MMP score of 0.109), even though the words ?con-
cerns? has a slightly higher IDF, 2.80, than the word
?causes?(2.68). However, in this question, under-
standing that the word ?causes? is critical to the
meaning of the question is critical and is captured
by the MMP model.
We analyzed feature importance for MMP classi-
fication by incrementally adding each feature group
to the model. The result is tabulated in Table 1. Not
surprisingly, syntactical (i.e., ?NPUnique,? ?Phrase-
Label? and ?PosOfPTN?) and semantic features
(i.e., ?NETypes?) are complementary to the corpus-
based statistics features (i.e., average IDF). Lexical
features also improve recall: the addition of ?Case-
Features? boosts the F-measure by 4 points. At first
sight, it is surprising that the feature group ?PosOf-
PTN,? which characterize the position of a candi-
date MMP relative to the sentence and relative to the
parse tree, has such a large impact?it improves the
F-measure by 5.6 points. However, a cursory brows-
ing of the training questions reveals that most MMPs
are short and concentrate towards the end of the sen-
tence. So this feature group helps by directing the
model to predict MMPs at the end of the sentence
and to prefer short phrases versus long ones.
4 Relevance Model with MMPs
We now validate our second hypothesis that MMPs
are effective for open domain question answering.
We demonstrate this through the improvement in
performance on relevance prediction. More specif-
ically, given a natural language question, the task
is one of finding relevant sentences in posts on on-
line forums. The relevance prediction component
is critical for question answering as has been seen
in TREC(Ittycheriah and Roukos, 2001) and more
recently in the Jeopardy challenge(Gondek et al,
2012). The improved relevance model further im-
proves our question answering system as seen in
Section 5.
882
Question Top 3 MMPs MMP-
score
Top words
by IDF
List statistics about changes in the de-
mographics of AIDS.
1: AIDS 0.955 demographics
2: changes 0.525 AIDS
3: the demographics 0.349 statistics
What are the concerns about the
causes of autism?
1: autism 0.989 autism
2: the causes 0.422 concerns
3: the causes of autism 0.362 causes
Table 2: Example questions and the top-3 phrases ranked by the MMP model.
4.1 Data for Relevance Model
The data to train and test the relevance model is ob-
tained as follows. First, a rudimentary version (i.e.,
key word search) of a QA system using Lucene is
built. The Lucene index comprised of a large num-
ber of threads in online forums released to the par-
ticipants of the BOLT-IR task(IR, 2012) for devel-
opment of our systems. The corpus is described in
more detail in Sec. 5. Top snippets returned by the
search engine are judged for relevancy by our an-
notators. The initial (small) batch of data is used
to train a relevance model which is deployed in the
system. The new model is in turn used to create
more answers for new questions. When more data
is collected, the relevance model is retrained and re-
deployed to collect more data. The process is iter-
ated for several months, and at the end of this pro-
cess, a total of 390 training questions are created and
about 28,915 snippets are judged by human annota-
tors, out of which about 6,528 are relevant answers.
These question-answers pairs are used to train the fi-
nal relevance model used in our question-answering
system. A separate held-out test set of 59 questions
is created and its system output is also judged by hu-
mans. This data set is our test set.
4.2 Relevance Prediction
A key component in our question-answering sys-
tem is the snippet relevance model, which is used
to compute the probability that a snippet is relevant
to a question. The relevance model is a conditional
distribution P (r|q, s;D), where r is a binary ran-
dom variable indicating if the candidate snippet s is
relevant to the question q. D is the document where
the snippet s is found.
In our question answering system, MMPs ex-
tracted from questions are used to compute the fea-
tures for the relevance model. To test their effective-
ness, we conduct a controlled experiment by com-
paring the system with MMP features with 2 base-
lines: (1) a system without MMP features; (2) a
baseline with each word as an MMP and the word?s
IDF as the MMP score.
4.2.1 Baseline Features
We list the features used in our baseline system,
where no MMP feature is used. The features can
be categorized into the following types. (1) Text
Match Features: One set of features are the cosine
scores between different representations of the query
and the snippet. In one version the query and snip-
pet words are used as is; in another version the query
and snippet are stemmed using porter stemmer; in
yet another the words are morphed to their roots by
a table extracted from WordNet. We also compute
the inclusion scores (the proportion of query words
found in the snippet) and other word overlap fea-
tures. (2) Answer Type Features: The top 3 pre-
dictions of a statistical classifier trained to predict
answer categories were used as features. (3) Men-
tion Match Features compute whether a named en-
tity in the query occurs in the snippet. The matching
takes into consideration the results from within and
cross document coreference resolution components
for nominal and pronominal mentions. (4) Event
match features use several hand-crafted dictionar-
ies containing terms exclusive to various types of
events like ?violence?, ?legal?, ?election?. Accord-
ingly a set of features that take a value of ?1? if
both the query and snippet contain the same event
type were designed. (5) Snippet Statistics: Several
features based on snippet length, the position of the
snippet in the post etc were created.
883
4.2.2 Features Derived from MMP
The MMPs extracted from questions are used to
compute features in the following ways.
As MMPs are aligned with a question?s syntactic
tree, they can be used to find answers by matching
a question constituent with that of a candidate snip-
pet. The MMP model also returns a score for each
phrase, which can be used to compute the degree to
which a question matches a candidate snippet.
In this section, we use s = wn1 to denote a snip-
pet with words w1, w2, ? ? ? , wn, and m to denote
a phrase from the MMP model along with a score
M(m). The features are listed below:
HardMatch: Let I(m ? s) be a 1 or 0 function
indicating if a snippet contains the MMP m, then
the hard match score is computed as:
HM(q, s) =
?
m?q M(m)I(m ? s)
?
m?q M(m)
.
SoftLMMatch: The SoftLMMatch score is a
language-model (LM) based score, similar to that
used in (Bendersky and Croft, 2008), except that
MMPs play the role of concepts. The snippet-side
language model score LM(v|s) is computed as:
LM(v|s) =
?n
i=1 I(wi = v) + 0.05
n + 0.05|V | ,
where wi is the ith in snippet s; I(wi = v) is an
indicator function, taking value 1 if wi is v and 0
otherwise; |V | is the vocabulary size.
The soft match score between a question q and a
snippet s is then:
SM(q, s) =
?
m?q
(
M(m)?w?m LM(w|s)
)
?
m?q M(m)
,
where m ? q denotes all MMPs in question q, and
similarly, w ? m signifying words in m.
MMPInclScore: An MMP m?s inclusion score is:
IS(m, s) =
?
w?m I(l(w, s) > ?)IDF (w)
?
w?m IDF (w)
,
where w ? m are the words in m; I(?) is the in-
dicator function taking value 1 when the argument
is true and 0 otherwise; ? is a constant threshold;
IDF (w) is the IDF of word w. l(w, s) is the sim-
ilarity of word w to the snippet s as: l(w, s) =
maxv?sJW (w, v), where JW (w, v) is the Jaro
Winkler similarity score between words w and v.
The MMP weighted inclusion score between the
question q and snippet s is computed as:
IS(q, s) =
?
m?q M(m)IS(m, s)
?
m?q M(m)
MMPRankDep: This feature, RD(q, s) first tests
if there exists a matched bilexcial dependency be-
tween q and s; if yes, it further tests if the head or
dependent in the matched dependency is the head of
any MMP.
Let m(i) be the ith ranked MMP; let ?wh, wd|q?
and ?uh, ud|s? be bilexical dependencies from q and
s, respectively, where wh and uh are the heads and
wd and ud are the dependents; let EQ(w, u) be a
function testing if the question word w and snip-
pet word u are a match. In our implementation,
EQ(w, u) is true if either w and u are exactly the
same, or their morphs are the same, or they head
the same entity, or their synset in WordNet overlap.
With these notations, RD(q, s) is true if and only if
EQ(wh, uh) ? EQ(wd, ud) ?wh ? m(i) ?wd ? m(j)
is true for some ?wh, wd|q?, for some ?uh, ud|s? and
for some i and j.
EQ(wh, uh)?EQ(wd, ud) requires that the ques-
tion dependency ?wh, wd|q? and the snippet depen-
dency ?uh, ud|s? match; wh ? m(i) ?wd ? m(j) re-
quires that the head word and dependent word are in
the ith-rank and jth rank MMP, respectively. There-
fore, RD(q, s) is a dependency feature enhanced
with MMPs.
To test the effectiveness of the MMP features, we
trained 3 snippet classifiers on the data described
in Section 4.1: one baseline system without MMP
features (henceforth ?no-MMP?); a second baseline
with words as MMPs and their IDFs as the scores
in the MMP model(henceforth ?IDF-as-MMP?); the
third system uses the MMPs generated by the model
from Section 3 and all MMP features described in
this section. We used two types of classifiers: deci-
sion tree (DTree) and logistic regression (Logit).
The classification results on a set of 59 questions
disjoint from the training set are shown in Table 3.
The numbers in the table are F-measure on answer
snippets (or positive snippets). Within a machine
884
Learner
Model DTree Logit
noMMP 0.426 0.458
IDF-as-MMP 0.413 0.455
MMP 0.451 0.470
Table 3: F-measure for Relevance Prediction.
learning method, the model with MMP features is
always the best. Between the two classifiers, the lo-
gistic regression models are consistently better than
the decision tree ones. The results show that MMP
features are very helpful to the relevance model.
5 End-to-End System Results
The question-answering system is used in the 2012
BOLT IR evaluation (IR, 2012). The task is to an-
swer questions against a corpus of posts collected
from Internet discussion forums in 3 languages:
Arabic, Chinese and English. There are 499K, 449K
and 262K threads in each of these languages. The
Arabic and Chinese posts were first translated into
English before being processed. We now describe
our experiments on the set of 59 questions devel-
oped internally and demonstrate the effectiveness of
an MMP based relevance model in the end-to-end
system. In the next subsection we discuss our per-
formance in the BOLT-IR evaluation done by NIST
for DARPA.
We now briefly describe the question-answering
system we developed for the DARPA BOLT IR task,
where we applied the MMP classifier and its fea-
tures. Users submit questions to the system in natu-
ral language; the BOLT program mandates that these
questions comply with the restrictions described in
Section 3.1. Questions are analyzed by a query pre-
processing stage that includes our MMP extraction
classifier. The preprocessed queries are converted
to search queries. These are sent to an Indri-based
search engine (Strohman et al, 2005), which re-
turns candidate passages, typically spanning numer-
ous sentences. Each sentence of the retrieved pas-
sages is analyzed by a relevance detection module,
consisting of a statistical classifier that uses, among
others, features computed from the MMPs extracted
from the questions. Sentences or spans that are
deemed relevant to the question by the relevance de-
tection module are further grouped into equivalence
classes that provide different information about the
answers. The system generates a single answer for
each equivalence class, since elements of the same
class are redundant with respect to each other. The
elements of each equivalence class are converted
into citations that support the corresponding answer.
The ultimate goal of the MMP model is to im-
prove the performance of our question-answering
system. To test the effectiveness of the MMP model,
we contrast the model trained in Section3 with an
IDF baseline, where each non-stop word in a ques-
tion is an MMP and its score is the corpus IDF. The
IDF baseline is what a typical question answering
system would do in absence of deep question analy-
sis. To have a fair comparison, the two systems are
tested on the same set of 59 questions as the rele-
vance model.
The results of the IDF baseline and MMP system
are tabulated in Table 4. Note that the recalls are
less than 1.0 because (1) annotated snippets come
from both systems; (2) the annotation is done for all
snippets in a window surrounding system snippets.
As can be seen from Table 4, the MMP system is
about 5 points better than the baseline system. The
precision is notably better by 2 points, and the re-
call is far better (by 7.7%) than that of the baseline.
We also compute the question-level F-measures and
conduct a Wilcoxon signed-rank test for paired sam-
ples. The test indicates that the MMP system is bet-
ter than the baseline system at p < 0.00066. There-
fore, the MMP system has a clear advantage over the
baseline system.
System Prec Recall F1
baseline .4228 .3679 .3935
MMP .4425 .4452 .4438
Table 4: End-to-End system result on 59 questions.
5.1 BOLT Evaluation Results
The BOLT evaluation consists of 146 questions,
mostly event- or topic- related, e.g., ?What are peo-
ple saying about the ending of NASA?s space shuttle
program??. A system answer, if correct, is mapped
manually to a facet, which is one semantic unit that
answers the question. For each question, facets
are collected across all participants? submission. A
885
facet-based F-measure is computed for each partic-
ipating site. The recall from which the official F-
measure is computed is weighted by snippet cita-
tions (a citation is a reference to the original docu-
ment that supports the correct facet). In other words,
a snippet with more citations leads to a higher recall
than one with less citations. The performances of
4 participating sites are listed in Table 5. Note that
the F-measure is weighted and is not necessarily a
number between the precision and the recall.
Facet Metric
Site Precision Recall (Weighted) F
SITE 1 0.2713 0.1595 0.1713
SITE 2 0.1500 0.1316 0.1109
SITE 3 0.1935 0.2481 0.1734
Ours 0.2729 0.2195 0.2046
Table 5: Official BOLT 2012 IR evaluation results.
.
Among 4 participating sites, our system has the
highest performance. SITE 1 has about the same
level of precision, with lower recall, while SITE 3
has the best recall, but lower precision. The results
validate that the MMP question analysis technique
presented in this paper is quite effective.
6 Conclusions
We propose a framework to select and rank manda-
tory matching phrases (MMP) for question answer-
ing. The framework makes full use of the lexical,
syntactic and semantic information in a question and
does not require answer data.
The proposed MMP framework is tested at 3 lev-
els in a full QA system and is shown to be very effec-
tive to improve its performance: first, we show that
it is possible to reliably predict MMPs from ques-
tions alone: the MMP classifier can achieve an F-
measure as high as 88.6%; second, phrases proposed
by the MMP model are incorporated into a snippet
relevance model and we show that it improves its
performance; third, the MMP framework is used in
an question answering system which achieved the
best performance in the official 2012 BOLT IR (IR,
2012) evaluation.
Acknowledgments
This work was partially supported by the Defense
Advanced Research Projects Agency under contract
No. HR0011-12-C-0015. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position or policy
of the U.S. government and no official endorsement
should be inferred.
References
Michael Bendersky and W. Bruce Croft. 2008. Discov-
ering key concepts in verbose queries. Proceedings of
the 31st annual international ACM SIGIR conference
on research and development in information retrieval
- SIGIR ?08, page 491.
Michael Bendersky, Donald Metzler, and W. Bruce Croft.
2010. Learning concept importance using a weighted
dependence model. Proceedings of the third ACM in-
ternational conference on Web search and data mining
- WSDM ?10, page 31.
Michael Bendersky. 2011. Parameterized concept
weighting in verbose queries. Proceedings of the 34th
annual international ACM SIGIR conference on re-
search and development in information retrieval.
Razvan Bunescu and Yunfeng Huang. 2010. Towards a
general model of answer typing: Question focus iden-
tification. In Proceedings of the 11th International
Conference on Intelligent Text Processing and Com-
putational Linguistics (CICLing).
Jennifer Chu-carroll, John Prager, Christopher Welty,
Krzysztof Czuba, and David Ferrucci. 2003. A multi-
strategy and multi-source approach to question an-
swering. In In Proceedings of Text REtrieval Confer-
ence.
R Florian, H Hassan, A Ittycheriah, H Jing, N Kamb-
hatla, X Luo, N Nicolov, and S Roukos. 2004. A
statistical model for multilingual entity detection and
tracking. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 1?8, Boston, Massachusetts, USA, May 2
- May 7. Association for Computational Linguistics.
D. C. Gondek, A. Lally, A. Kalyanpur, J. W. Murdock,
P. A. Duboue, L. Zhang, Y. Pan, Z. M. Qiu, and
C. Welty. 2012. A framework for merging and rank-
ing of answers in DeepQA. IBM Journal of Research
and Development, 56(3.4):14:1 ?14:12, may-june.
Ulf Hermjakob, Eduard H. Hovy, and Chin yew Lin.
2000. Knowledge-based question answering. In In
Proceedings of the 6th World Multiconference on Sys-
tems, Cybernetics and Informatics (SCI-2002, pages
772?781.
886
BOLT IR. 2012. Broad operational language translation
(BOLT). www.darpa.mil/Our_Work/I2O/
Programs/Broad_Operational_Language_
Translat%ion_(BOLT).aspx. [Online; ac-
cessed 10-Dec-2012].
Abraham Ittycheriah and Salim Roukos. 2001. IBM?s
statistical question answering system - TREC-11. In
Proceedings of the Text REtrieval Conference.
Matthew Lease, James Allan, and W. Bruce Croft. 2009.
Advances in Information Retrieval, volume 5478 of
Lecture Notes in Computer Science. Springer Berlin
Heidelberg, Berlin, Heidelberg, April.
Xin Li and Dan Roth. 2002. Learning question classi-
fiers. In Proceedings of the 19th international confer-
ence on Computational linguistics - Volume 1, COL-
ING ?02, pages 1?7, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
H. P. Luhn. 1958. A business intelligence system. IBM
J. Res. Dev., 2(4):314?319, October.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proc. of Human Language Technology
(HLT)/Empirical Methods in Natural Language Pro-
cessing (EMNLP).
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proc. of ACL.
Dan Moldovan, Christine Clark, Sanda Harabagiu, and
Steve Maiorano. 2003. Cogex: a logic prover for
question answering. In Proceedings of the 2003 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology - Volume 1, NAACL ?03, pages 87?
93.
Christof Monz. 2007. Model tree learning for query
term weighting in question answering. In Proceed-
ings of the 29th European conference on IR re-
search, ECIR?07, pages 589?596, Berlin, Heidelberg.
Springer-Verlag.
Christopher Pinchak. 2006. A probabilistic answer type
model. In In EACL, pages 393?400.
Jay M. Ponte and W. Bruce Croft. 1998. A language
modeling approach to information retrieval. In Pro-
ceedings of the 21st annual international ACM SIGIR
conference on research and development in informa-
tion retrieval, SIGIR ?98, pages 275?281, New York,
NY, USA. ACM.
S. E. Robertson and S. Walker. 1994. Some simple
effective approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of
the 17th annual international ACM SIGIR conference
on research and development in information retrieval,
SIGIR ?94, pages 232?241, New York, NY, USA.
Springer-Verlag New York, Inc.
Trevor Strohman, Donald Metzler, Howard Turtle, and
W. Bruce Croft. 2005. Indri: a language-model based
search engine for complex queries. Technical report,
in Proceedings of the International Conference on In-
telligent Analysis.
Ellen M. Voorhees. 2004. Overview of the TREC 2004
question answering track. In TREC.
Le Zhao and Jamie Callan. 2010. Term necessity predic-
tion. Proceedings of the 19th ACM international con-
ference on Information and knowledge management -
CIKM ?10, page 259.
887
11
Automatic Summarization
Ani Nenkova University of Pennsylvania
Sameer Maskey IBM Research
Yang Liu University of Texas at Dallas
2
Why summarize?
23
Text summarization
News articles
Scientific Articles
Emails
Books
Websites
Social Media 
Streams
4
Speech summarization
MeetingPhone Conversation
Classroom
Radio NewsBroadcast News
Talk Shows
Lecture
Chat
35
How to 
summarize
Text & Speech?
-Algorithms
-Issues
-Challenges
-Systems
Tutorial
6
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Frequency, Lexical chains, TF*IDF,
Topic Words, Topic Models [LSA, EM, Bayesian]
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
47
Motivation: where does summarization 
help?
 Single document summarization 
 Simulate the work of intelligence analyst
 Judge if a document is relevant to a topic of interest
?Summaries as short as 17% of the full text length speed up 
decision making twice, with no significant degradation in 
accuracy.?
?Query-focused summaries enable users to find more relevant 
documents more accurately, with less need to consult the full text 
of the document.?
[Mani et al, 2002]
8
Motivation: multi-document summarization 
helps in compiling and presenting
 Reduce search time, especially when the goal of the 
user is to find as much information as possible about a 
given topic
 Writing better reports, finding more relevant information, 
quicker
 Cluster similar articles and provide a multi-document 
summary of the similarities
 Single document summary of the information unique to 
an article
[Roussinov and Chen, 2001; Mana-Lopez et al, 2004; McKeown et al, 2005 ]
59
Benefits from speech summarization
 Voicemail
 Shorter time spent on listening (call centers)
 Meetings
 Easier to find main points
 Broadcast News
 Summary of story from mulitiple channels
 Lectures
 Useful for reviewing of course materials
[He et al, 2000; Tucker and Whittaker, 2008; Murray et al, 2009]
10
Assessing summary quality: overview
 Responsiveness
 Assessor directly rate each summary on a scale
 In official evaluations but rarely reported in papers
 Pyramid
 Assessors create model summaries
 Assessors identifies semantic overlap between summary 
and models
 ROUGE
 Assessors create model summaries
 ROUGE automatically computes word overlap
611
Tasks in summarization
Content (sentence) selection
 Extractive summarization
Information ordering
 In what order to present the selected sentences, especially 
in multi-document summarization
Automatic editing, information fusion and compression
 Abstractive summaries
12
Extractive (multi-document) summarization
Input text2Input text1 Input text3
Summary
1. Selection
2. Ordering
3. Fusion
Compute Informativeness
713
Computing informativeness
 Topic models (unsupervised)
 Figure out what the topic of the input
 Frequency, Lexical chains, TF*IDF
 LSA, content models (EM, Bayesian) 
 Select informative sentences based on the topic
 Graph models (unsupervised)
 Sentence centrality
 Supervised approaches
 Ask people which sentences should be in a summary
 Use any imaginable feature to learn to predict human 
choices
14
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, Lexical chains, TF*IDF, 
Topic Words,Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
815
Frequency as document topic proxy
10 incarnations of an intuition
 Simple intuition, look only at the document(s)
 Words that repeatedly appear in the document are likely to 
be related to the topic of the document
 Sentences that repeatedly appear in different input 
documents represent themes in the input
 But what appears in other documents is also helpful 
in determining the topic
 Background corpus probabilities/weights for word 
16
What is an article about?
 Word probability/frequency
 Proposed by Luhn in 1958 [Luhn 1958]
 Frequent content words would be indicative of the 
topic of the article
 In multi-document summarization, words or 
facts repeated in the input are more likely to 
appear in human summaries [Nenkova et al, 2006]
917
Word probability/weights 
Libya
bombing
trail
Gadafhi
suspects
Libya refuses 
to surrender 
two Pan Am 
bombing 
suspects 
Pan Am
INPUT
SUMMARY
WORD PROBABILITY TABLE
Word Probability
pan 0.0798
am 0.0825
libya 0.0096
suspects 0.0341
gadafhi 0.0911
trail 0.0002
?.
usa 0.0007
HOW?
UK and 
USA
18
HOW: Main steps in sentence selection 
according to word probabilities
Step 1 Estimate word weights (probabilities)
Step 2 Estimate sentence weights
Step 3 Choose best sentence
Step 4 Update word weights
Step 5 Go to 2 if desired length not reached
)()( SentwCFSentWeight i ?=
10
19
More specific choices [Vanderwende et al, 2007; Yih et al, 
2007; Haghighi and Vanderwende, 2009]
 Select highest scoring sentence
 Update word probabilities for the selected sentence 
to reduce redundancy
 Repeat until desired summary length
?
?
=
Sw
wp
S
SScore )(||
1)(
pnew (w) = pold (w).pold (w)
20
Is this a reasonable approach: yes, people 
seem to be doing something similar
 Simple test
 Compute word probability table from the input
 Get a batch of summaries written by H(umans) and S(ystems)
 Compute the likelihood of the summaries given the word 
probability table 
 Results
 Human summaries have higher likelihood
HSSSSSSSSSSHSSSHSSHHSHHHHH
HIGH LIKELIHOODLOW
11
21
Obvious shortcomings of the pure 
frequency approaches
 Does not take account of related words
 suspects -- trail
 Gadhafi ? Libya
 Does not take into account evidence from 
other documents
 Function words: prepositions, articles, etc.
 Domain words: ?cell? in cell biology articles
 Does not take into account many other 
aspects
22
Two easy fixes
 Lexical chains [Barzilay and Elhadad, 1999, Silber and McCoy, 
2002, Gurevych and Nahnsen, 2005]
 Exploits existing lexical resources (WordNet)
 TF*IDF weights [most summarizers]
 Incorporates evidence from a background corpus
12
23
Lexical chains and WordNet relations
 Lexical chains
 Word sense disambiguation is performed 
 Then topically related words represent a topic
 Synonyms, hyponyms, hypernyms
 Importance is determined by frequency of the words in a 
topic rather than a single word
 One sentence per topic is selected 
 Concepts based on WordNet [Schiffman et al, 2002, Ye et al, 
2007]
 No word sense disambiguation is performed
 {war, campaign, warfare, effort, cause, operation}
 {concern, carrier, worry, fear, scare}
24
TF*IDF weights for words
Combining evidence for document topics from the 
input and from a background corpus
 Term Frequency (TF)
 Times a word occurs in the input 
 Inverse Document Frequency (IDF)
 Number of documents (df) from a background 
corpus of N documents that contain the word
)/log(* dfNtfIDFTF ?=
13
25
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
26
Topic words (topic signatures)
 Which words in the input are most descriptive?
 Instead of assigning probabilities or weights to all words, 
divide words into two classes: descriptive or not
 For iterative sentence selection approach, the binary 
distinction is key to the advantage over frequency and 
TF*IDF
 Systems based on topic words have proven to be the most 
successful in official summarization evaluations 
14
27
Example input and associated topic words
 Input for summarization: articles relevant to the 
following user need
Title: Human Toll of Tropical 
Storms Narrative: What has been the human toll in death or injury 
of tropical storms in recent years? Where and when have each of 
the storms caused human casualties? What are the approximate 
total number of casualties attributed to each of the storms?
ahmed, allison, andrew, bahamas, bangladesh, bn, caribbean, carolina, caused, cent, 
coast, coastal, croix, cyclone, damage, destroyed, devastated, disaster, dollars, drowned, 
flood, flooded, flooding, floods, florida, gulf, ham, hit, homeless, homes, hugo, hurricane, 
insurance, insurers, island, islands, lloyd, losses, louisiana, manila, miles, nicaragua, 
north, port, pounds, rain, rains, rebuild, rebuilding, relief, remnants, residents, roared, salt, 
st, storm, storms, supplies, tourists, trees, tropical, typhoon, virgin, volunteers, weather, 
west, winds, yesterday.
Topic Words
28
Formalizing the problem of identifying topic 
words 
 Given
 t: a word that appears in the input
 T: cluster of articles on a given topic (input)
 NT: articles not on topic T (background corpus)
 Decide if t is a topic word or not
 Words that have (almost) the same probability in T 
and NT are not topic words
15
29
Computing probabilities
 View a text as a sequence of Bernoulli trails
 A word is either our term of interest t or not
 The likelihood of observing term t which occurs with 
probability p in a text consisting of N words is given by 
 Estimate the probability of t in three ways
 Input + background corpus combines
 Input only
 Background only
t
30
Testing which hypothesis is more 
likely: log-likelihood ratio test
has a known statistical distribution: chi-square 
At a given significance level, we can decide if a word is 
descriptive of the input or not.
This feature is used in the best performing systems for 
multi-document summarization of news [Lin and Hovy, 
2000; Conroy et al, 2006]
Likelihood of the data given H1
Likelihood of the data given H2
? =
-2 log ?
16
31
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
32
The background corpus takes more 
central stage
 Learn topics from the background corpus
 topic ~ themes often discusses in the background
 topic representation ~ word probability tables
 Usually one time training step
 To summarize an input 
 Select sentences from the input that correspond 
to the most prominent topics
17
33
Latent semantic analysis (LSA) [Gong and Liu, 
2001, Hachey et al, 2006, Steinberger et al, 2007]
 Discover topics from the background corpus with n unique 
words and d documents
 Represent the background corpus as nxd matrix A
 Rows correspond to words
 Aij=number of times word I appears in document j
 Use standard change of coordinate system and dimensionality 
reduction techniques
 In the new space each row corresponds to the most important 
topics in the corpus
 Select the best sentence to cover each topic
TUPVA =
34
Notes on LSA and other approaches
 The original article that introduced LSA for 
single document summarization of news did 
not find significant difference with TF*IDF
 For multi-document summarization of news 
LSA approaches have not outperformed topic 
words or extensions of frequency approaches
 Other topic/content models have been much 
more influential
18
35
Domain dependent content models
 Get sample documents from the domain
 background corpus
 Cluster sentences from these documents 
 Implicit topics
 Obtain a word probability table for each topic
 Counts only from the cluster representing the 
topic
 Select sentences from the input with highest 
probability for main topics 
36
Text structure can be learnt
 Human-written examples from a domain
Location, time
relief efforts
magnitude
damage
19
37
Topic = cluster of similar sentences from 
the background corpus
 Sentences cluster from earthquake articles
 Topic ?earthquake location?
 The Athens seismological institute said the temblor?s epicenter 
was located 380 kilometers (238 miles) south of the capital.
 Seismologists in Pakistan?s Northwest Frontier Province said the 
temblor?s epicenter was about 250 kilometers (155 miles) north of 
the provincial capital Peshawar.
 The temblor was centered 60 kilometers (35 miles) north- west of 
the provincial capital of Kunming, about 2,200 kilometers (1,300
miles) southwest of Beijing, a bureau seismologist said.
38
Content model [Barzilay and Lee, 2004, Pascale et al, 2003] 
 Hidden Markov Model (HMM)-based 
 States - clusters of related sentences ?topics?
 Transition prob. - sentence precedence in corpus
 Emission prob. - bigram language model
location, 
magnitude casualties
relief efforts
)|()|(),|,( 11111 +++++ ?=><>< iieiitiiii hsphhphshsp
Earthquake reportsTransition from previous 
topic
Generating 
sentence in 
current topic
20
39
Learning the content model
 Many articles from the same domain
 Cluster sentences: each cluster represents a topic from 
the domain
 Word probability tables for each topic
 Transitions between clusters can be computed from 
sentence adjacencies in the original articles  
 Probabilities of going from one topic to another
 Iterate between clustering and transition probability 
estimation to obtain domain model
40
To select a summary
 Find main topics in the domain
 using a small collection of summary-input pairs
 Find the most likely topic for each sentence in 
the input 
 Select the best sentence per main topic
21
41
Historical note
 Some early approaches to multi-document 
summarization relied on clustering the 
sentences in the input alone [McKeown et al, 1999, 
Siddharthan et al, 2004]
 Clusters of similar sentences represent a theme in 
the input
 Clusters with more sentences are more important
 Select one sentence per important cluster
42
Example cluster
Choose one sentence to represent the cluster
1. PAL was devastated by a pilots' strike in June and by the 
region's currency crisis.
2. In June, PAL was embroiled in a crippling three-week 
pilots' strike.
3. Tan wants to retain the 200 pilots because they stood by 
him when the majority of PAL's pilots staged a 
devastating strike in June.
22
43
Bayesian content models
 Takes a batch of inputs for summarization
 Many word probability tables
 One for general English
 One for each of the inputs to be summarized
 One for each document in any input
To select a summary S with L words from 
document collection D given as input
The goal is to select the summary, not a 
sentence. Greedy selection vs. global will 
be discussed in detail later
S* = minS:words(S)?LKL(PD||PS)
44
KL divergence
 Distance between two probability distributions: P, Q
 P, Q: Input and summary word distributions  
KL (P || Q) = pP (w) log2 pP (w)pQ (w)w?
23
45
Intriguing side note
 In the full Bayesian topic models, word 
probabilities for all words is more important 
than binary distinctions of topic and non-topic 
word
 Haghighi and Vanderwende report that a 
system that chooses the summary with 
highest expected number of topic words 
performs as SumBasic
46
Review
 Frequency based informativeness has been 
used in building summarizers
 Topic words probably more useful
 Topic models
 Latent Semantic Analysis
 Domain dependent content model
 Bayesian content model
24
47
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
48
Using graph representations [Erkan and Radev, 
2004; Mihalcea and Tarau, 2004; Leskovec et al, 2005 ]
 Nodes
 Sentences
 Discourse entities
 Edges
 Between similar sentences
 Between syntactically related entities
 Computing sentence similarity
 Distance between their TF*IDF weighted vector 
representations
25
49
50
Sentence :
Iraqi vice president?
Sentence :
Ivanov contended?
Sim(d1s1, d3s2)
26
51
Advantages of the graph model
 Combines word frequency and sentence 
clustering
 Gives a formal model for computing 
importance: random walks
 Normalize weights of edges to sum to 1
 They now represent probabilities of transitioning 
from one node to another
52
Random walks for summarization
 Represent the input text as graph
 Start traversing from node to node 
 following the transition probabilities 
 occasionally hopping to a new node
 What is the probability that you are in any 
particular node after doing this process for a 
certain time? 
 Standard solution (stationary distribution)
 This probability is the weight of the sentence
27
53
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
54
Supervised methods 
 For extractive summarization, the task can be 
represented as binary classification
 A sentence is in the summary or not
 Use statistical classifiers to determine the score of a 
sentence: how likely it?s included in the summary
 Feature representation for each sentence
 Classification models trained from annotated data
 Select the sentences with highest scores (greedy for 
now, see other selection methods later)
28
55
Features
 Sentence length
 long sentences tend to be more important
 Sentence weight
 cosine similarity with documents
 sum of term weights for all words in a sentence
 calculate term weight after applying LSA
56
Features
 Sentence position
 beginning is often more important
 some sections are more important (e.g., in 
conclusion section)
 Cue words/phrases 
 frequent n-grams
 cue phrases (e.g., in summary, as a conclusion)
 named entities
29
57
Features
 Contextual features
 features from context sentences
 difference of a sentence and its neighboring ones 
 Speech related features (more later):
 acoustic/prosodic features
 speaker information (who said the sentence, is the 
speaker dominant?)
 speech recognition confidence measure 
58
Classifiers
 Can classify each sentence individually, or 
use sequence modeling
 Maximum entropy [Osborne, 2002]
 Condition random fields (CRF) [Galley, 2006]
 Classic Bayesian Method [Kupiec et al, 1995]
 HMM [Conroy and O'Leary, 2001; Maskey, 2006 ]
 Bayesian networks 
 SVMs [Xie and Liu, 2010]
 Regression [Murray et al, 2005]
 Others
30
59
So that is it with supervised methods? 
 It seems it is a straightforward classification 
problem
 What are the issues with this method?
 How to get good quality labeled training data
 How to improve learning
 Some recent research has explored a few 
directions
 Discriminative training, regression, sampling, co-
training, active learning
60
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
31
61
Improving supervised methods: different 
training approaches
 What are the problems with standard training 
methods?
 Classifiers learn to determine a sentence?s label 
(in summary or not)   
 Sentence-level accuracy is different from 
summarization evaluation criterion (e.g., 
summary-level ROUGE scores)
 Training criterion is not optimal
 Sentences? labels used in training may be too 
strict (binary classes)
62
Improving supervised methods: MERT 
discriminative training
 Discriminative training based on MERT [Aker et 
al., 2010]
 In training, generate multiple summary candidates 
(using A* search algorithm)
 Adjust model parameters (feature weights) 
iteratively to optimize ROUGE scores
Note: MERT has been used for machine translation discriminative training
32
63
Improving supervised methods: ranking 
approaches 
 Ranking approaches [Lin et al 2010]
 Pair-wise training
 Not classify each sentence individually
 Input to learner is a pair of sentences
 Use Rank SVM to learn the order of two sentences
 Direct optimization
 Learns how to correctly order/rank summary candidates 
(a set of sentences)
 Use AdaRank [Xu and Li 2007] to combine weak rankers
64
Improving supervised methods: regression 
model
 Use regression model [Xie and Liu, 2010]
 In training, a sentence?s label is not +1 and -1
 Each one is labeled with numerical values to 
represent their importance
 Keep +1 for summary sentence
 For non-summary sentences (-1), use their similarity to 
the summary as labels
 Train a regression model to better discriminate 
sentence candidates
33
65
Improving supervised methods: sampling
 Problems -- in binary classification setup for 
summarization, the two classes are 
imbalanced
 Summary sentences are minority class. 
 Imbalanced data can hurt classifier training
 How can we address this?
 Sampling to make distribution more balanced to 
train classifiers
 Has been studied a lot in machine learning
66
Improving supervised methods: sampling
 Upsampling: increase minority samples
 Replicate existing minority samples
 Generate synthetic examples (e.g., by some kind 
of interpolation)
 Downsampling: reduce majority samples
 Often randomly select from existing majority 
samples
34
67
Improving supervised methods: sampling
 Sampling for summarization [Xie and Liu, 2010]
 Different from traditional upsampling and downsampling
 Upsampling
 select non-summary sentences that are like summary 
sentences based on cosine similarity or ROUGE scores
 change their label to positive 
 Downsampling: 
 select those that are different from summary sentences
 These also address some human annotation disagreement
 The instances whose labels are changed are often the ones 
that humans have problems with
68
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-raining
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
35
69
Supervised methods: data issues
 Need labeled data for model training
 How do we get good quality training data? 
 Can ask human annotators to select extractive 
summary sentences
 However, human agreement is generally low
 What if data is not labeled at all? or it only 
has abstractive summary?
70
 Distributions of content units and words are similar
 Few units are expressed by everyone; many units 
are expressed by only one person
Do humans agree on summary sentence 
selection? Human agreement on word/sentence/fact selection
36
71
Supervised methods: semi-supervised 
learning
 Question ? can we use unlabeled data to 
help supervised methods? 
 A lot of research has been done on semi-
supervised learning for various tasks
 Co-training and active learning have been 
used in summarization
72
Co-training
 Use co-training to leverage unlabeled data
 Feature sets represent different views
 They are conditionally independent given the 
class label
 Each is sufficient for learning
 Select instances based on one view, to help the 
other classifier
37
73
Co-training in summarization
 In text summarization [Wong et al, 2008]
 Two classifiers (SVM, na?ve Bayes) are used on 
the same feature set
 In speech summarization [Xie et al, 2010]
 Two different views: acoustic and lexical features
 They use both sentence and document as 
selection units
74
Active learning in summarization
 Select samples for humans to label
 Typically hard samples, machines are not 
confident, informative ones
 Active learning in lecture summarization [Zhang 
et al 2009]
 Criterion: similarity scores between the extracted 
summary sentences and the sentences in the 
lecture slides are high
38
75
Supervised methods: using labeled 
abstractive summaries
 Question -- what if I only have abstractive 
summaries, but not extractive summaries? 
 No labeled sentences to use for classifier 
training in extractive summarization 
 Can use reference abstract summary to 
automatically create labels for sentences
 Use similarity of a sentence to the human written 
abstract (or ROUGE scores, other metrics)
76
Comment on supervised performance
 Easier to incorporate more information
 At the cost of requiring a large set of human 
annotated training data
 Human agreement is low, therefore labeled 
training data is noisy
 Need matched training/test conditions
 may not easily generalize to different domains
 Effective features vary for different domains
 e.g., position is important for news articles
39
77
Comments on supervised performance
 Seems supervised methods are more 
successful in speech summarization than in 
text
 Speech summarization is almost never multi-
document
 There are fewer indications about the topic of the 
input in speech domains
 Text analysis techniques used in speech 
summarization are relatively simpler 
78
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
40
79
Parameters to optimize
 In summarization methods we try to find 
1. Most significant sentences
2. Remove redundant ones
3. Keep the summary under given length
 Can we combine all 3 steps in one?
 Optimize all 3 parameters at once
80
Summarization as an optimization problem
 Knapsack Optimization Problem 
Select boxes such that amount of money is 
maximized while keeping total weight under X Kg
 Summarization Problem 
Select sentences such that summary relevance is 
maximized while keeping total length under X words
 Many other similar optimization problems  
 General Idea: Maximize a function given a set of 
constraints
41
81
Optimization methods for summarization
 Different flavors of solutions
 Greedy Algorithm
 Choose highest valued boxes
 Choose the most relevant sentence 
 Dynamic Programming algorithm
 Save intermediate computations
 Look at both relevance and length
 Integer Linear Programming
 Exact Inference
 Scaling Issues
We will now discuss these 3 types of optimization solutions
82
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
42
83
Greedy optimization algorithms
 Greedy solution is an approximate algorithm which 
may not be optimal
 Choose the most relevant + least redundant 
sentence if the total length does not exceed the 
summary length
 Maximal Marginal Relevance is one such greedy algorithm 
proposed by [Carbonell et al, 1998]
84
Maximal Marginal Relevance (MMR) 
[Carbonell et al, 1998]
 Summary: relevant and non-redundant information
 Many summaries are built based on sentences ranked by 
relevance
 E.g. Extract most relevant 30% of sentences
Relevance Redundancyvs.
 Summary should maximize relevant information as 
well as reduce redundancy
43
85
Marginal relevance
 ?Marginal Relevance? or ?Relevant Novelty?
 Measure relevance and novelty separately
 Linearly combine these two measures
 High Marginal relevance if
 Sentence is relevant to story (significant information)
 Contains minimal similarity to previously selected sentences 
(new novel information)
 Maximize Marginal Relevance to get summary that 
has significant non-redundant information
86
Relevance with query or centroid
 We can compute relevance of text snippet 
with respect to query or centroid
 Centroid as defined in [Radev, 2004]
 based on the content words of  a document 
 TF*IDF vector of all documents in corpus
 Select words above a threshold : remaining vector 
is a centroid vector
44
87
Maximal Marginal Relevance (MMR) 
[Carbonell et al, 1998]
 Q ? document centroid/user query
 D ? document collection
 R ? ranked listed
 S ? subset of documents in R already selected
 Sim ? similarity metric 
 Lambda =1 produces most significant ranked list
 Lambda = 0 produces most diverse ranked list
MMR? Argmax(Di?R?S)[?(Sim1(Di, Q))?(1??)max(Dj?S)Sim2(Di, Dj)]
88
MMR based Summarization [Zechner, 2000]
Iteratively select next sentence
Next Sentence = 
Frequency Vector 
of all content words
centroid
45
89
MMR based summarization
 Why this iterative sentence selection process 
works?
 1st Term: Find relevant sentences similar to 
centroid of the document
 2nd Term: Find redundancy ? sentences that are 
similar to already selected sentences are not 
selected
90
 MMR is an iterative sentence selection 
process
 decision made for each sentence
 Is this selected sentence globally optimal?
Sentence selection in MMR
Sentence with same level of relevance but shorter may not be 
selected if a longer relevant sentence is already selected
46
91
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
92
Global inference
D=t1, t2, , tn?1, tn
 Modify our greedy algorithm 
 add constraints for sentence length as well
 Let us define document D with tn textual 
units
47
93
Global inference
 Let us define
Relevance of ti to be in the 
summary
Redundancy between ti and tj
Length of til(i)
Red(i,j)
Rel(i)
94
Inference problem [McDonald, 2007]
 Let us define inference problem as 
Summary Score
Pairwise RedundancyMaximum Length
48
95
Greedy solution [McDonald, 2007]
Sort by Relevance
Select Sentence
 Sorted list may have longer sentences at the top
 Solve it using dynamic programming
 Create table and fill it based on length and redundancy 
requirements
No consideration of
sentence length
96
Dynamic programming solution [McDonald, 2007]
High scoring summary
of length k and i-1
text unitsHigh scoring 
summary of
length k-l(i) +
ti
Higher ?
49
97
 Better than the previously shown greedy 
algorithm
 Maximizes the space utilization by not 
inserting longer sentences
 These are still approximate algorithms: 
performance loss?
Dynamic programming algorithm [McDonald, 2007]
98
Inference algorithms comparison
[McDonald, 2007]
System 50 100 200
Baseline 26.6/5.3 33.0/6.8 39.4/9.6
Greedy 26.8/5.1 33.5/6.9 40.1/9.5
Dynamic Program 27.9/5.9 34.8/7.3 41.2/10.0
Summarization results: Rouge-1/Rouge-2
Sentence Length
50
99
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
100
Integer Linear Programming (ILP) [Gillick 
and Favre, 2009; Gillick et al, 2009; McDonald, 2007]
 Greedy algorithm is an approximate solution
 Use exact solution algorithm with ILP (scaling issues 
though)
 ILP is constrained optimization problem
 Cost and constraints are linear in a set of integer variables
 Many solvers on the web
 Define the constraints based on relevance and 
redundancy for summarization
 Sentence based ILP
 N-gram based ILP
51
101
Sentence-level ILP formulation [McDonald, 
2007]
1 if ti in summary
Constraints
Optimization Function
102
N-gram ILP formulation [Gillick and Favre, 2009; 
Gillick et al, 2009]
 Sentence-ILP constraint on redundancy is 
based on sentence pairs
 Improve by modeling n-gram-level 
redundancy
 Redundancy implicitly defined
Ci indicates presence
of n-gram i in summary 
and its weight is wi
?
i wici
52
103
N-gram ILP formulation [Gillick and Favre, 2009]
Constraints
Optimization Function n-gram level ILP has different  optimization 
function than one shown before
104
Sentence vs. n-gram ILP
System ROUGE-2 Pyramid
Baseline 0.058 0.186
Sentence ILP
[McDonald, 2007] 0.072 0.295
N-gram ILP
[Gillick and Favre, 2009] 0.110 0.345
53
105
Other optimization based summarization 
algorithms
 Submodular selection [Lin et al, 2009]
 Submodular set functions for optimization
 Modified greedy algorithm [Filatova, 2004]
 Event based features
 Stack decoding algorithm [Yih et al, 2007]
 Multiple stacks, each stack represents hypothesis of different 
length
 A* Search [Aker et al, 2010]
 Use scoring and heuristic functions
106
Submodular selection for summarization 
[Lin et al, 2009]
 Summarization Setup
 V ? set of all sentences in document
 S ? set of extraction sentences
 f(.) scores the quality of the summary
 Submodularity been used in solving many 
optimization problems in near polynomial time
 For summarization: 
Select subset S (sentences) representative of V 
given the constraint |S| =< K (budget)
54
107
Submodular selection [Lin et al, 2009]
 If V are nodes in a Graph G=(V,E) representing 
sentences
 And E represents edges (i,j) such that w(i,j) 
represents similarity between sentences i and j
 Introduce submodular set functions which measures 
?representative? S of entire set V
 [Lin et al, 2009] presented 4 submodular set functions
108
Submodular selection for summarization 
[Lin et al, 2009]
Comparison of results using different methods
55
109
Review: optimization methods
 Global optimization methods have shown to be 
superior than 2-step selection process and reduce 
redundancy
 3 parameters are optimized together
 Relevance
 Redundancy
 Length
 Various Algorithms for Global Inference
 Greedy
 Dynamic Programming 
 Integer Linear Programming
 Submodular Selection
110
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
56
111
Speech summarization
 Increasing amount of data available in 
speech form
 meetings, lectures, broadcast, youtube, voicemail
 Browsing is not as easy as for text domains
 users need to listen to the entire audio
 Summarization can help effective information 
access
 Summary output can be in the format of text 
or speech
112
Domains 
 Broadcast news
 Lectures/presentations
 Multiparty meetings
 Telephone conversations
 Voicemails
57
113
Example
Meeting transcripts and summary sentences (in red)
so it?s possible that we could do something like a 
summary node of some sort that
me003
but there is some technology you could try to applyme010
yeahme010
now I don?t know that any of these actually apply in 
this case
me010
uh so if you co- you could ima- and i-me010
mmmme003
there?re ways to uh sort of back off on the purity of 
your bayes-net-edness
me010
andme010
uh i- i slipped a paper to bhaskara and about noisy-
or?s and noisy-maxes
me010
which is there are technical ways of doing itme010
uh let me just mention something that i don?t want 
to pursue today
me010
there there are a variety of ways of doing itme010
Broadcast news transcripts and summary (in red)
try to use electrical appliances before p.m. and after p.m. and 
turn off computers, copiers and lights when they're not being 
used
set your thermostat at 68 degrees when you're home, 55 
degrees when  you're away
energy officials are offering tips to conserve electricity, they say, 
to delay holiday lighting until after at night
the area shares power across many states
meanwhile, a cold snap in the pacific northwest is putting an 
added strain on power supplies
coupled with another unit, it can provide enough power for about
2 million people
it had been shut down for maintenance
a unit at diablo canyon nuclear plant is expected to resume 
production today
california's strained power grid is getting a boost today which 
might help increasingly taxed power supplies
114
Speech vs. text summarization: similarities
 When high quality transcripts are available
 Not much different from text summarization
 Many similar approaches have been used
 Some also incorporate acoustic information
 For genres like broadcast news, style is also 
similar to text domains
58
115
Speech vs. text summarization: differences
 Challenges in speech summarization
 Speech recognition errors can be very high
 Sentences are not as well formed as in most text 
domains: disfluencies, ungrammatical
 There are not clearly defined sentences
 Information density is also low (off-topic 
discussions, chit chat, etc.)
 Multiple participants
116
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
59
117
What should be extraction units in speech 
summarization?
 Text domain
 Typically use sentences (based on punctuation 
marks)
 Speech domain
 Sentence information is not available
 Sentences are not as clearly defined
Utterance from previous example:
there there are a variety of ways of doing it uh let me just mention something 
that i don?t want to pursue today which is there are technical ways of doing it
118
Automatic sentence segmentation (side note) 
 For a word boundary, determine whether it?s a sentence 
boundary
 Different approaches: 
 Generative: HMM
 Discriminative: SVM, boosting, maxent, CRF
 Information used: word n-gram, part-of-speech, parsing 
information, acoustic info (pause, pitch, energy)
60
119
What is the effect of different 
units/segmentation on summarization?
 Research has used different units in speech 
summarization
 Human annotated sentences or dialog acts
 Automatic sentence segmentation
 Pause-based segments
 Adjacency pairs
 Intonational phrases 
 Words
120
What is the effect of different 
units/segmentation on summarization?
 Findings from previous studies
 Using intonational phrases (IP) is better than 
automatic sentence segmentation, pause-based 
segmentation [Maskey, 2008 ]
 IPs are generally smaller than sentences, also 
linguistically meaningful
 Using sentences is better than words, between 
filler segments [Furui et al, 2004]
 Using human annotated dialog acts is better than 
automatically generated ones [Liu and Xie, 2008]
61
121
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
122
Using acoustic information in 
summarization
 Acoustic/prosodic features: 
 F0 (max, min, mean, median, range)
 Energy (max, min, mean, median, range)
 Sentence duration
 Speaking rate (# of words or letters)
 Need proper normalization
 Widely used in supervised methods, in 
combination with textual features
62
123
Using acoustic information in 
summarization
 Are acoustic features useful when combining 
it with lexical information?
 Results vary depending on the tasks and 
domains 
 Often lexical features are ranked higher
 But acoustic features also contribute to overall 
system performance
 Some studies showed little impact when adding 
speech information to textual features [Penn and Zhu, 
2008]
124
Using acoustic information in 
summarization
 Can we use acoustic information only for speech 
summarization?
 Transcripts may not be available
 Another way to investigate contribution of acoustic 
information
 Studies showed using just acoustic information can 
achieve similar performance to using lexical 
information [Maskey and Hirschberg, 2005; Xie et al, 2009; Zhu et al, 
2009]
 Caveat: in some experiments, lexical information is used 
(e.g., define the summarization units)
63
125
Speech recognition errors
 ASR is not perfect, often high word error rate
 10-20% for read speech
 40% or even higher for conversational speech
 Recognition errors generally have negative 
impact on summarization performance
 Important topic indicative words are incorrectly 
recognized
 Can affect term weighting and sentence scores
126
Speech recognition errors
 Some studies evaluated effect of recognition 
errors on summarization by varying word 
error rate [Christensen et al, 2003; Penn and Zhu, 2008; Lin et al, 
2009]
 Degradation is not much when word error 
rate is not too low (similar to spoken 
document retrieval)
 Reason: better recognition accuracy in summary 
sentences than overall  
64
127
What can we do about ASR errors? 
 Deliver summary using original speech 
 Can avoid showing recognition errors in the 
delivered text summary
 But still need to correctly identify summary 
sentences/segments
 Use recognition confidence measure and 
multiple candidates to help better summarize
128
Address problems due to ASR errors
 Re-define summarization task: select 
sentences that are most informative, at the 
same time have high recognition accuracy
 Important words tend to have high recognition 
accuracy
 Use ASR confidence measure or n-gram 
language model scores in summarization
 Unsupervised methods [Zechner, 2002; Kikuchi et al, 2003; 
Maskey, 2008]
 Use as a feature in supervised methods
65
129
Address problems due to ASR errors
 Use multiple recognition candidates
 n-best lists [Liu et al, 2010]
 Lattices [Lin et al, 2010]
 Confusion network [Xie and Liu, 2010]
 Use in MMR framework
 Summarization segment/unit contains all the word 
candidates (or pruned ones based on probabilities)
 Term weights (TF, IDF) use candidate?s posteriors
 Improved performance over using 1-best recognition 
output
130
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency
66
131
Disfluencies and summarization
 Disfluencies (filler words, repetitions, revisions, 
restart, etc) are frequent in conversational speech
 Example from meeting transcript:
so so does i- just remind me of what what you were going to do with the 
what what what what's
y- you just described what you've been doing
 Existence may hurt summarization systems, also 
affect human readability of the summaries
132
Disfluencies and summarization
 Natural thought: remove disfluenices 
 Word-based selection can avoid disfluent 
words 
 Using n-gram scores tends to select fluent 
parts [Hori and Furui, 2001]
 Remove disfluencies first, then perform 
summarization 
 Does it work? not consistent results 
 Small improvement [Maskey, 2008; Zechner, 2002]
 No improvement [Liu et al, 2007]
67
133
Disfluencies and summarization
 In supervised classification, information related to 
disfluencies can be used as features for 
summarization 
 Small improvement on Switchboard data [Zhu and Penn, 2006]
 Going beyond disfluency removal, can perform 
sentence compression in conversational speech to 
remove un-necessary words [Liu and Liu, 2010]
 Help improve sentence readability
 Output is more like abstractive summaries
 Compression helps summarization
134
Review on speech summarization
 Speech summarization has been performed 
for different domains
 A lot of text-based approaches have been 
adopted
 Some speech specific issues have been 
investigated
 Segmentation 
 ASR errors
 Disfluencies
 Use acoustic information
68
135
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
136
Manual evaluations
 Task-based evaluations 
 too expensive
 Bad decisions possible, hard to fix
 Assessors rate summaries on a scale
 Responsiveness
 Assessors compare with gold-standards
 Pyramid
69
137
Automatic and fully automatic 
evaluation
 Automatically compare with gold-standard
 Precision/recall (sentence level)
 ROUGE (word level)
 No human gold-standard is used
 Automatically compare input and summary
138
Precision and recall for extractive 
summaries
 Ask a person to select the most important 
sentences
Recall: system-human choice 
overlap/sentences chosen by human
Precision: system-human choice 
overlap/sentences chosen by system
70
139
Problems?
 Different people choose different sentences
 The same summary can obtain a recall score 
that is between 25% and 50% different 
depending on which of two available human 
extracts is used for evaluation
 Recall more important/informative than 
precision?
140
More problems?
 Granularity
We need help. Fires have spread in the nearby 
forest and threaten several villages in this remote 
area.
 Semantic equivalence
 Especially in multi-document summarization
 Two sentences convey almost the same 
information: only one will be chosen in the human 
summary
71
141
Pyramid
Responsiveness
ROUGE
Fully automatic
Model 
summaries
Manual comparison/ 
ratings
Evaluation methods for content
142
Pyramid method [Nenkova and Passonneau, 2004; Nenkova et al, 
2007]
 Based on Semantic Content Units (SCU)
 Emerge from the analysis of several texts
 Link different surface realizations with the 
same meaning
72
143
SCU example
S1 Pinochet arrested in London on Oct 16 at a 
Spanish judge?s request for atrocities against 
Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has 
been arrested in London at the request of the 
Spanish government.
S3 Britain caused international controversy and 
Chilean turmoil by arresting former Chilean 
dictator Pinochet in London.
144
SCU: label, weight, contributors 
Label London was where Pinochet was 
arrested
Weight=3
S1 Pinochet arrested in London on Oct 16 at a Spanish 
judge?s request for atrocities against Spaniards in Chile.
S2 Former Chilean dictator Augusto Pinochet has been
arrested in London at the request of the Spanish
government.
S3 Britain caused international controversy and Chilean 
turmoil by arresting former Chilean dictator Pinochet in 
London.
73
145
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
146
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
74
147
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
148
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
75
149
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
150
Ideally informative summary
 Does not include an SCU from a lower tier 
unless all SCUs from higher tiers are 
included as well
76
151
Different equally good summaries
 Pinochet arrested
 Arrest in London
 Pinochet is a former 
Chilean dictator
 Accused of atrocities 
against Spaniards
152
Different equally good summaries
 Pinochet arrested
 Arrest in London
 On Spanish warrant
 Chile protests
77
153
Diagnostic ? why is a summary bad?
 Good  Less relevant 
summary
154
Importance of content 
 Can observe distribution in human 
summaries
 Assign relative importance
 Empirical rather than subjective
 The more people agree, the more important
78
155
Pyramid score for evaluation
 New summary with n content units
 Estimates the percentage of information that is 
maximally important
IdealWight
ightObservedWe
Ideal
Weight
n
i
i
n
i
i
=
?
?
=
=
1
1
156
ROUGE [Lin, 2004]
 De facto standard for evaluation in text 
summarization
 High correlation with manual evaluations in that 
domain
 More problematic for some other domains, 
particularly speech
 Not highly correlated with manual evaluations
 May fail to distinguish human and machine 
summaries
79
157
ROUGE details
 In fact a suite of evaluation metrics
 Unigram
 Bigram
 Skip bigram
 Longest common subsequence
 Many settings concerning
 Stopwords
 Stemming
 Dealing with multiple models
158
How to evaluate without human 
involvement? [Louis and Nenkova, 2009]
 A good summary should be similar to the 
input
 Multiple ways to measure similarity
 Cosine similarity
 KL divergence
 JS divergence
 Not all work!
80
159
 Distance between two distributions as 
average KL divergence from their mean 
distribution
JS divergence between input and 
summary
)]||()||([)||( 21 ASummKLAInpKLSummInpJS +=
SummaryandInputofondistributimeanSummInpA ,
2
+
=
160
Summary likelihood given the input
 Probability that summary is generated according to 
term distribution in the input
Higher likelihood ~ better summary
 Unigram Model
 Multinomial Model
ii
n
rInp
n
Inp
n
Inp
wwordofsummaryincountn
vocabularysummaryr
wpwpwp r
=
?
)()()( 21 21 K
sizesummarynN
wpwpwp
i
i
n
rInp
n
Inp
n
Inpnn
N r
r
==?
)()()( 21
1 21!!
! KK
81
161
 Fraction of summary = input?s topic words
 % of input?s topic words also appearing in summary 
 Capture variety
 Cosine similarity: input?s topic words and all summary 
words
 Fewer dimensions, more specific vectors
Topic words identified by log-likelihood 
test
162
How good are these metrics? 
48 inputs, 57 systems
JSD -0.880 -0.736
0.795 0.627
-0.763 -0.694
0.712 0.647
0.712 0.602
-0.688 -0.585
-0.188 -0.101
0.222 0.235
% input?s topic in summary
KL div summ-input
Cosine similarity
% of summary = topic words
KL div input-summ
Unigram summ prob.
Multinomial summ prob.
-0.699 0.629Topic word similarity
Pyramid Responsiveness
Spearman correlation on macro level for the query focused task.
82
163
 JSD correlations with pyramid scores even better than 
R1-recall
 R2-recall is consistently better
 Can extend features using higher order n-grams 
How good are these metrics?
0.870.90R2-recall
0.800.85R1-recall
-0.73-0.88JSD
Resp.Pyramid
164
Motivation & Definition
Topic Models
Graph Based Methods
Supervised Techniques
Global Optimization Methods
Speech Summarization
Evaluation
Frequency, TF*IDF, Topic Words
Topic Models [LSA, EM, Bayesian]
Manual (Pyramid), Automatic (Rouge, F-Measure)
Fully Automatic
Features, Discriminative Training
Sampling, Data, Co-training
Iterative, Greedy, Dynamic Programming
ILP, Sub-Modular Selection
Segmentation, ASR
Acoustic Information, Disfluency 
83
165
Current summarization research  
 Summarization for various new genres
 Scientific articles
 Biography
 Social media (blog, twitter)
 Other text and speech data 
 New task definition 
 Update summarization 
 Opinion summarization
 New summarization approaches 
 Incorporate more information (deep linguistic knowledge, information 
from the web)
 Adopt more complex machine learning techniques
 Evaluation issues
 Better automatic metrics
 Extrinsic evaluations
And more?
166
 Check out summarization papers at ACL this 
year
 Workshop at ACL-HLT 2011:
 Automatic summarization for different genres, 
media, and languages [June 23, 2011]
 http://www.summarization2011.org/
84
167
References
 Ahmet Aker, Trevor Cohn, Robert Gaizauska. 2010. Multi-document summarization using A* search and 
discriminative training. Proc. of EMNLP.
 R. Barzilay and M. Elhadad. 2009. Text summarizations with lexical chains. In: I. Mani and M. Maybury (eds.): 
Advances in Automatic Text Summarization.
 Jaime Carbonell and Jade Goldstein. 1998. The Use of MMR, Diversity-Based reranking for Reordering 
Documents and Producing Summaries. Proceedings of the 21st Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval.
 H. Christensen, Y. Gotoh, B. Killuru, and S. Renals. 2003. Are Extractive Text Summarization Techniques 
Portable to Broadcast News? Proc. of ASRU.
 John Conroy and Dianne O'Leary. 2001. Text Summarization via Hidden Markov Models. Proc. of SIGIR.
 J. M. Conroy, J. D. Schlesinger, and D. P. OLeary. 2006. Topic-Focused Multi-Document Summarization Using an 
Approximate Oracle Score. Proc. COLING/ACL 2006. pp. 152-159.
 Thomas Cormen, Charles E. Leiserson, and Ronald L. Rivest.1990. Introduction to algorithms. MIT Press.
 G. Erkan and D. R. Radev.2004. LexRank: Graph-based Centrality as Salience in Text Summarization. Journal of 
Artificial Intelligence Research (JAIR).
 Pascale Fung, Grace Ngai, and Percy Cheung. 2003. Combining optimal clustering and hidden Markov models for 
extractive summarization. Proceedings of ACL Workshop on Multilingual Summarization.Sadoki Furui, T. Kikuchi, 
Y. Shinnaka, and C. Hori. 2004. Speech-to-text and Speech-to-speech Summarization of Spontaneous Speech. 
IEEE Transactions on Audio, Speech, and Language Processing. 12(4), pages 401-408.
 Michel Galley. 2006. A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance. 
Proc. of EMNLP.
 Dan Gillick, Benoit Favre. 2009. A scalable global model for summarization. Proceedings of the Workshop on 
Integer Linear Programming for Natural Language Processing.
 Dan Gillick, Korbinian Riedhammer, Benoit Favre, Dilek Hakkani-Tur. 2009. A global optimization framework for 
meeting summarization. Proceedings of ICASSP.
 Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and Mark Kantrowitz. 2000. Multi-document summarization by 
sentence extraction. Proceedings of the 2000 NAACL-ANLP Workshop on Automatic Summarization.
168
References
 Y. Gong and X. Liu. 2001. Generic text summarization using relevance measure and latent semantic analysis. 
Proc. ACM SIGIR.
 I. Gurevych and T. Nahnsen. 2005. Adapting Lexical Chaining to Summarize Conversational Dialogues. Proc. 
RANLP.
 B. Hachey, G. Murray, and D. Reitter.2006. Dimensionality reduction aids term co-occurrence based multi-
document summarization. In: SumQA 06: Proceedings of the Workshop on Task-Focused Summarization and 
Question Answering. 
 Aria Haghighi and Lucy Vanderwende. 2009. Exploring content models for multi-document summarization. Proc. of 
NAACL-HLT.
 L. He, E. Sanocki, A. Gupta, and J. Grudin. 2000. Comparing presentation summaries: Slides vs. reading vs.
listening. Proc. of SIGCHI on Human factors in computing systems.
 C. Hori and Sadaoki Furui. 2001. Advances in Automatic Speech Summarization. Proc. of Eurospeech.
 T. Kikuchi, S. Furui, and C. Hori. 2003. Automatic Speech Summarization based on Sentence Extractive and 
Compaction. Proc. of ICSLP.  
 Julian Kupiec, Jan Pedersen, and Francine Chen. 1995. A Trainable Document Summarizer. Proc. of SIGIR.
 J. Leskovec, N. Milic-frayling, and M. Grobelnik. 2005. Impact of Linguistic Analysis on the Semantic Graph 
Coverage and Learning of Document Extracts. Proc. AAAI.
 Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries, Workshop on Text 
Summarization Branches Out. 
 C.Y. Lin and E. Hovy. 2000. The automated acquisition of topic signatures for text summarization. Proc. COLING.
 Hui Lin and Jeff Bilmes. 2010. Multi-document summarization via budgeted maximization of submodular functions. 
Proc. of NAACL.
 Hui Lin and Jeff Bilmes and Shasha Xie. 2009. Graph-based Submodular Selection for Extractive Summarization. 
Proceedings of ASRU.
 Shih-Hsiang Lin and Berlin Chen. 2009. Improved Speech Summarization with Multiple-hypothesis 
Representations and Kullback-Leibler Divergence Measures. Proc. of Interspeech.
 Shih-Hsiang Lin, Berlin Chen, and H. Min Wang. 2009. A Comparative Study of Probabilistic Ranking Models for 
Chinese Spoken Document Summarization. ACM Transactions on Asian Language Information Processing.
85
169
References
 Shih Hsiang Lin, Yu Mei Chang, Jia Wen Liu, Berlin Chen. 2010 Leveraging Evaluation Metric-related Training 
Criteria for Speech Summarization. Proc. of ICASSP.
 Fei Liu and Yang Liu. 2009. From Extractive to Abstractive Meeting Summaries: Can it be done by sentence 
compression? Proc. of ACL.
 Fei Liu and Yang Liu. 2010. Using Spoken Utterance Compression for Meeting Summarization: A pilot study. Proc. 
of IEEE SLT.
 Yang Liu and Shasha Xie. 2008. Impact of Automatic Sentence Segmentation on Meeting Summarization. Proc. of 
ICASSP.
 Yang Liu, Feifan Liu, Bin Li, and Shasha Xie. 2007. Do Disfluencies Affect Meeting Summarization: A pilot study 
on the impact of disfluencies. Poster at MLMI.
 Yang Liu, Shasha Xie, and Fei Liu. 2010. Using n-best Recognition Output for Extractive Summarization and 
Keyword Extraction in Meeting Speech. Proc. of ICASSP.
 Annie Louis and Ani Nenkova. 2009. Automatically evaluating content selection in summarization without 
human models. Proceedings of EMNLP
 H.P. Luhn. 1958. The Automatic Creation of Literature Abstracts. IBM Journal of Research and Development 2(2).
 Inderjeet Mani, Gary Klein, David House, Lynette Hirschman, Therese Firmin, and Beth Sundheim. 2002. 
SUMMAC: a text summarization evaluation. Natual Language Engineering. 8,1 (March 2002), 43-68.
 Manuel J. Mana-Lopez, Manuel De Buenaga, and Jose M. Gomez-Hidalgo. 2004. Multidocument summarization: 
An added value to clustering in interactive retrieval. ACM Trans. Inf. Systems.
 Sameer Maskey. 2008. Automatic Broadcast News Summarization. Ph.D thesis. Columbia University.
 Sameer Maskey and Julia Hirschberg. 2005. Comparing lexical, acoustic/prosodic, discourse and structural 
features for speech summarization. Proceedings of Interspeech.
 Sameer Maskey and Julia Hirschberg. 2006. Summarizing Speech Without Text Using Hidden Markov Models. 
Proc. of HLT-NAACL.
 Ryan McDonald. 2007. A Study of Global Inference Algorithms in Multi-document Summarization. Lecture Notes in 
Computer Science. Advances in Information Retrieval. 
 Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, and Julia Hirschberg. 2005. Do 
summaries help?. Proc. of SIGIR.
 K. McKeown, J. L. Klavans, V. Hatzivassiloglou, R. Barzilay, and E. Eskin.1999. Towards multidocument
summarization by reformulation: progress and prospects. Proc. AAAI 1999.
170
References
 R. Mihalcea and P. Tarau .2004. Textrank: Bringing order into texts. Proc. of EMNLP 2004. 
 G. Murray, S. Renals, J. Carletta, J. Moore. 2005. Evaluating Automatic Summaries of Meeting Recordings. Proc. 
of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation.
 G. Murray, T. Kleinbauer, P. Poller, T. Becker, S. Renals, and J. Kilgour. 2009. Extrinsic Summarization 
Evaluation: A Decision Audit Task. ACM Transactions on Speech and Language Processing.
 A. Nenkova and R. Passonneau. 2004. Evaluating Content Selection in Summarization: The Pyramid Method. 
Proc. HLT-NAACL.
 A. Nenkova, L. Vanderwende, and K. McKeown. 2006. A compositional context sensitive multi-document 
summarizer: exploring the factors that influence summarization. Proc. ACM SIGIR.
 A. Nenkova, R. Passonneau, and K. McKeown. 2007. The Pyramid Method: Incorporating human content 
selection variation in summarization evaluation. ACM Trans. Speech Lang. Processing.
 Miles Osborne. 2002. Using maximum entropy for sentence extraction. Proc. of ACL Workshop on Automatic 
Summarization.
 Gerald Penn and Xiaodan Zhu. 2008. A critical Reassessement of Evaluation Baselines for Speech 
Summarization. Proc. of ACL-HLT.
 Dmitri G. Roussinov and Hsinchun Chen. 2001. Information navigation on the web by clustering and summarizing 
query results. Inf. Process. Manage. 37, 6 (October 2001), 789-816. 
 B. Schiffman, A. Nenkova, and K. McKeown. 2002. Experiments in Multidocument Summarization. Proc. HLT.
 A. Siddharthan, A. Nenkova, and K. Mckeown.2004. Syntactic Simplification for Improving Content Selection in 
Multi-Document Summarization. Proc. COLING.
 H. Grogory Silber and Kathleen F. McCoy. 2002. Efficiently computed lexical chains as an intermediate 
representation for automatic text summarization. Computational. Linguist. 28, 4 (December 2002), 487-496.
 J. Steinberger, M. Poesio, M. A. Kabadjov, and K. Jeek. 2007. Two uses of anaphora resolution in summarization. 
Inf. Process. Manage. 43(6).
 S. Tucker and S. Whittaker. 2008. Temporal compression of speech: an evaluation. IEEE Transactions on Audio, 
Speech and Language Processing, pages 790-796.
 L. Vanderwende, H. Suzuki, C. Brockett, and A. Nenkova. 2007. Beyond SumBasic: Task-focused summarization 
with sentence simplification and lexical expansion. Information Processing and Management 43.
86
171
References
 Kam-Fai Wong, Mingli Wu, and Wenjie Li. 2008. Extractive Summarization using Supervised and Semi-supervised 
learning. Proc. of ACL.
 Shasha Xie and Yang Liu. 2010. Improving Supervised Learning for Meeting Summarization using Sampling and 
Regression. Computer Speech and Language. V24, pages 495-514.
 Shasha Xie and Yang Liu. 2010. Using Confusion Networks for Speech Summarization. Proc. of NAACL.
 Shasha Xie, Dilek Hakkani-Tur, Benoit Favre, and Yang Liu. 2009. Integrating Prosodic Features in Extractive 
Meeting Summarization. Proc. of ASRU.
 Shasha Xie, Hui Lin, and Yang Liu. 2010. Semi-supervised Extractive Speech Summarization via Co-training 
Algorithm. Proc. of Interspeech.
 S. Ye, T.-S. Chua, M.-Y. Kan, and L. Qiu. 2007. Document concept lattice for text understanding and 
summarization. Information Processing and Management 43(6).
 W. Yih, J. Goodman, L. Vanderwende, and H. Suzuki. 2007. Multi-Document Summarization by Maximizing 
Informative Content-Words. Proc. IJCAI 2007.
 Klaus Zechner. 2002. Automatic Summarization of Open-domain Multiparty Dialogues in Diverse Genres. 
Computational Linguistics. V28, pages 447-485.
 Klaus Zechner and Alex Waibel. 2000. Minimizing word error rate in textual summaries of spoken language. 
Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference.
 Justin Zhang and Pascale Fung. 2009. Extractive Speech Summarization by Active Learning. Proc. of ASRU.
 Xiaodan Zhu and Gerald Penn. 2006. Comparing the Roles of Textual, Acoustic and Spoken-language Features 
on Spontaneous Conversation Summarization. Proc. of HLT-NAACL.
 Xiaodan Zhu, Gerald Penn, and F. Rudzicz. 2009. Summarizing Multiple Spoken Documents: Finding Evidence 
from Untranscribed Audio. Proc. of ACL.
