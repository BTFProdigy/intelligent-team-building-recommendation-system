Fine-Grained Word Sense Disambiguation Based on Parallel Corpora,  
Word Alignment, Word Clustering and Aligned Wordnets 
Dan TUFI? 
Institute for Artificial 
Intelligence 
13, ?13 Septembrie? 
Bucharest, 050711 
Romania 
tufis@racai.ro 
Radu ION 
Institute for Artificial 
Intelligence  
13, ?13 Septembrie? 
Bucharest, 050711 
Romania 
radu@racai.ro 
Nancy IDE  
Department of Computer Science
Vassar College  
Poughkeepsie,  
NY 12604-0520  
USA 
ide@cs.vassar.edu 
Abstract 
The paper presents a method for word sense 
disambiguation based on parallel corpora. The 
method exploits recent advances in word 
alignment and word clustering based on 
automatic extraction of translation equivalents 
and being supported by available aligned 
wordnets for the languages in the corpus. The 
wordnets are aligned to the Princeton 
Wordnet, according to the principles 
established by EuroWordNet. The evaluation 
of the WSD system, implementing the 
method described herein showed very 
encouraging results. The same system used in 
a validation mode, can be used to check and 
spot alignment errors in multilingually 
aligned wordnets as BalkaNet and 
EuroWordNet.  
1 Introduction 
Word Sense Disambiguation (WSD) is well-
known as one of the more difficult problems in 
the field of natural language processing, as noted 
in  (Gale et al 1992; Kilgarriff, 1997; Ide and 
V?ronis, 1998), and others. The difficulties stem 
from several sources, including the lack of means 
to formalize the properties of context that 
characterize the use of an ambiguous word in a 
given sense, lack of a standard (and possibly 
exhaustive) sense inventory, and the subjectivity 
of the human evaluation of such algorithms. To 
address the last problem, (Gale et al 1992) argue 
for upper and lower bounds of precision when 
comparing automatically assigned sense labels 
with those assigned by human judges. The lower 
bound should not drop below the baseline usage 
of the algorithm (in which every word that is 
disambiguated is assigned the most frequent 
sense) whereas the upper bound should not be too 
restrictive? when the word in question is hard to 
disambiguate even for human judges (a measure 
of this difficulty is the computation of the 
agreement rates between human annotators). 
Identification and formalization of the 
determining contextual parameters for a word 
used in a given sense is the focus of WSD work 
that treats texts in a monolingual setting?that is, 
a setting where translations of the texts in other 
languages either do not exist or are not 
considered. This focus is based on the 
assumption that for a given word w and two of its 
contexts C1 and C2, if C1 ? C2 (are perfectly 
equivalent), then w is used with the same sense in 
C1 and C2. A formalized definition of context for 
a given sense would then enable a WSD system 
to accurately assign sense labels to occurrences 
of w in unseen texts. Attempts to characterize 
context for a given sense of a word have 
addressed a variety of factors: 
? Context length: what is the size of the window 
of text that should be considered to determine 
context?  Should it consist of only a few words, 
or include much larger portions of text? 
? Context content: should all context words be 
considered, or only selected words (e.g., only 
words in a certain part of speech or a certain 
grammatical relations to the target word)? Should 
they be weighted based on distance from the 
target or treated as a ?bag of words?? 
? Context formalization: how can context 
information be represented to enable definitions 
of an inter-context equivalence function? Is there 
a single representation appropriate for all words, 
or does it vary according to, for example, the 
word?s part of speech? 
The use of multi-lingual parallel texts 
provides a very different approach to the problem 
of context identification and characterization. 
?Context? now becomes the word(s) by which 
the target word (i.e., the word to be 
disambiguated) is translated in one or more other 
languages. The assumption here is that different 
senses of a word are likely to be lexicalized 
differently in different languages; therefore, the 
translation can be used to identify the correct 
sense of a word. Effectively, the translation 
captures the context as the translator conceived it. 
The use of parallel translations for sense 
disambiguation brings up a different set of issues, 
primarily because the assumption that different 
senses are lexicalized differently in different 
languages is true only to an extent. For instance, 
it is well known that many ambiguities are 
preserved across languages (e.g. the French 
int?r?t and the English interest), especially 
languages that are relatively closely related. This 
raises new questions: how many languages, and 
of which types (e.g., closely related languages, 
languages from different language families), 
provide adequate information for this purpose? 
How do we measure the degree to which 
different lexicalizations provide evidence for a 
distinct sense? 
We have addressed these questions in 
experiments involving sense clustering based on 
translation equivalents extracted from parallel 
corpora (Ide, 199; Ide et al, 2002). Tufi? and Ion 
(2003) build on this work and further describe a 
method to accomplish a ?neutral? labelling for 
the sense clusters in Romanian and English that 
is not bound to any particular sense inventory. 
Our experiments confirm that the accuracy of 
word sense clustering based on translation 
equivalents is heavily dependent on the number 
and diversity of the languages in the parallel 
corpus and the language register of the parallel 
text. For example, using six source languages 
from three language families (Romance, Slavic 
and Finno-Ugric), sense clustering of English 
words was approximately 74% accurate; when 
fewer languages and/or languages from less 
diverse families are used accuracy drops 
dramatically. This drop is obviously a result of 
the decreased chances that two or more senses of 
an ambiguous word in one language will be 
lexicalized differently in another when fewer 
languages, and languages that are more closely 
related, are considered. 
To enhance our results, we have explored the 
use of additional resources, in particular, the 
aligned wordnets in BalkaNet (Tufi? et al 
2004a). BalkaNet  is a European project that is 
developing monolingual wordnets for five Balkan 
languages (Bulgarian, Greek, Romanian Serbian, 
and Turkish) and improving the Czech wordnet 
developed in the EuroWordNet project. The 
wordnets are aligned to the Princeton Wordnet 
(PWN2.0), taken as an interlingual index, 
following the principles established by the 
EuroWordNet consortium. The underlying 
hypothesis in this experiment exploits the 
common intuition that reciprocal translations in 
parallel texts should have the same (or closely 
related) interlingual meanings (in terms of 
BalkaNet, interlingual index (ILI) codes). 
However, this hypothesis is reasonable if the 
monolingual wordnets are reliable and correctly 
linked to the interlingual index (ILI). Quality 
assurance of the wordnets is a primary concern in 
the BalkaNet project, and to this end, the 
consortium developed several methods and tools 
for validation, described in various papers 
authored by BalkaNet consortium members (see 
Proceedings of the Global WordNet Conference, 
Brno, 2004).  
We previously implemented a language-
independent disambiguation program, called 
WSDtool, which has been extended to serve as a 
multilingual wordnet checker and specialized 
editor for error-correction. In (Tufi?, et al, 2004) 
it was demonstrated that the tool detected several 
interlingual alignment errors that had escaped 
human analysis. In this paper, we describe a 
disambiguation experiment that exploits the ILI 
information in the corrected wordnets 
2 Methodology and the algorithm 
Our methodology consists of the following steps: 
1. given a bitext TL1L2 in languages L1 and L2 for 
which there are aligned wordnets, extract all pairs 
of lexical items that are reciprocal 
translations:{<WiL1 WjL2>+} 
2. for each lexical alignment <WiL1 WjL2>, extract 
the ILI codes for the synsets that contain WiL1 and 
WjL2 respectively to yield two lists of ILI codes, 
L1ILI(WiL1) and L2ILI(WjL2) 
3. identify one ILI code common to the 
intersection L1ILI(WiL1) ? L2ILI(WjL2) or a pair of 
ILI codes ILI1? L1ILI(WiL1)  and ILI2? L2ILI(WjL2), 
so that ILI1 and ILI2 are the most similar ILI 
codes (defined below) among the candidate pairs 
(L1ILI(WiL1)?L2ILI(WjL2) [? = Cartesian product]. 
The accuracy of step 1 is essential for the 
success of the validation method. A recent shared 
task evaluation) of different word aligners 
(www.cs.unt.edu/~rada/wpt, organized on the 
occasion of the Conference of the NAACL 
showed that step 1 may be solved quite reliably. 
Our system (Tufi? et al 2003) produced lexicons 
relevant for wordnets evaluation, with an 
aggregated F-measure as high as 84.26%. 
Meanwhile, the word-aligner was further 
improved so that current performance on the 
same data is about 1% better on all scores in 
word alignment and about 2% better in wordnet-
relevant dictionaries. The word alignment 
problem includes cases of null alignment, where 
words in one part of the bitext are not translated 
in the other part; and cases of expression 
alignment, where multiple words in one part of 
the bitext are translated as one or more words in 
the other part. Word alignment algorithms 
typically do not take into account the part of 
speech (POS) of the words comprising a 
translation equivalence pair, since cross-POS 
translations are rather frequent. However, for the 
aligned wordnet-based word sense 
disambiguation we discard both translation pairs 
which do not preserve the POS and null 
alignments. Multiword expressions included in a 
wordnet are dealt with by the underlying 
tokenizer. Therefore, we consider only one-to-
one, POS-preserving alignments. 
Once the translation equivalents were 
extracted, then, for any translation equivalence 
pair <WL1 WL2> and two aligned wordnets, the 
steps 2 and 3 above should ideally identify one 
ILI concept lexicalized by WL1 in language L1 
and by WL2 in language L2. However, due to 
various reasons, the wordnets alignment might 
reveal not the same ILI concept, but two concepts 
which are semantically close enough to license 
the translation equivalence of WL1 and WL2. This 
can be easily generalized to more than two 
languages. Our measure of interlingual concepts 
semantic similarity is based on PWN2.0 
structure. We compute semantic-similarity score 
by formula: 
ss(ILI1, ILI2) = 1/1+k 
where k is the number of links from ILI1 to ILI2 
or from both ILI1 and ILI2 to the nearest common 
ancestor. The semantic similarity score is 1 when 
the two concepts are identical, 0.33 for two sister 
concepts, and 0.5 for mother/daughter, 
whole/part, or concepts related by a single link. 
Based on empirical studies, we decided to set the 
significance threshold of the semantic similarity 
score to 0.33.  Other approaches to similarity 
measures are described in (Budanitsky and Hirst 
2001). 
In order to describe the algorithm for WSD 
based on aligned wordnets let us assume we have 
a parallel corpus containing texts in k+1 
languages (T, L1, L2?Lk), where T is the target 
language and L1, L2?Lk are the source languages 
and monolingual wordnets for each of the k+1 
languages interlinked via an ILI-like structure. 
For each source language and for all occurrences 
of a specific word in the target language T, we 
build a matrix of translation equivalents as shown 
in Table 1 (eqij represents the translation 
equivalent in the ith source language of the jth 
occurrence of the word in the target language).  
 Occ #1 Occ #2 ? Occ #n 
L1 eq11 eq12 ? eq1n 
L2 eq21 eq22 ? eq2n 
? ? ? ? ? 
Lk eqk1 eqk2 ? eqkn 
Table 1. The translation equivalents matrix 
(EQ matrix) 
If the target word is not translated in language Li, 
eqij is represented by the null string.  
The second step transforms the matrix in 
Table 1 to a VSA (Validation and Sense 
Assignment) matrix with the same dimensions 
(Table 2).  
 Occ #1 Occ #2 ? Occ #n 
L1 VSA11  VSA12 ? VSA 1n  
L2 VSA21 VSA22  VSA22 
? ? ? ? ? 
Lk VSAk1 VSAk2 ? VSAkn 
Table 2. The VSA matrix 
Here,  VSAij = LENILI(WEN) ? LiILI(WjLi),, where 
LENILI(WEN) represent the ILI codes of all synsets 
in which the target word WEN occurs, and 
LiILI(WjLi) is the list of ILI-codes for all synsets in 
which the translation equivalent for the jth 
occurrence of WEN occurs. 
If no translation equivalent is found in 
language Li for the jth occurrence of WEN, 
VSA(i,j) is undefined; otherwise, it is a set 
containing 0, 1, or more ILI codes. For undefined 
VSAs, the algorithm cannot determine the sense 
number for the corresponding occurrence of the 
target word. However, it is very unlikely that an 
entire column in Table 2 is undefined, i.e., that 
there is no translation equivalent for an 
occurrence of the target word in any of the source 
languages.  
When VSA(i,j) contains a single ILI code, the 
target occurrence and its translation equivalent 
are assigned the same sense. 
When VSA(i,j) is empty?i.e., when none of 
the senses of the target word corresponds to an 
ILI code to which a sense of the translation 
equivalent was linked--the algorithm selects the 
pair in LENILI(WEN) ? LiILI(WjLi) with the highest 
similarity score. If no pair in LENILI(WEN) ? 
LiILI(WjLi) has a  the semantic similarity score 
above the significance threshold, neither the 
occurrence of the target word nor its translation 
equivalent can be semantically disambiguated; 
but once again, it is extremely rare that there is 
no translation equivalent for an occurrence of the 
target word in any of the source languages. 
In case of ties, the pair corresponding to the 
most frequent sense of the target word in the 
current bitext pair is selected. If this heuristic in 
turn fails, the choice is made in favor of the pair 
corresponding to the lowest PWN2.0 sense 
number for the target word, since PWN senses 
are ordered by frequency.  
When the VSA cell contains two or more ILI-
codes, we have the case of cross-lingual 
ambiguity, i.e., two or more senses are common 
to the target word and the corresponding 
translation equivalent in the ith language.  
2.1 Agglomerative clustering   
As noted before, when VSA(i,j) is undefined, we 
may get the information from a VSA 
corresponding to the same occurrence of the 
target word in a different language. However, this 
demands that aligned wordnets are available for 
all languages in the parallel corpus, and that the 
quality of the inter-lingual linking is high for all 
languages concerned. In cases where we cannot 
fulfill these requirements, we rely on a ?back-
off? method involving sense clustering based on 
translation equivalents, as discussed in (Ide, et 
al., 2002). We apply the clustering method after 
the wordnet-based method has been applied, and 
therefore each cluster containing an 
undisambiguated occurrence of the target word 
will also typically contain several occurrences 
that have already been assigned a sense. We can 
therefore assign the most frequent sense 
assignment in the cluster to previously unlabeled 
occurrences within the same cluster. The 
combined approach has two main advantages: 
? it eliminates reliance only on high-quality, k-1 
aligned wordnets. Indeed, having k+1 languages 
in our corpus, we need only apply the WSD 
method to the aligned wordnets for the target 
language (English in our case) and one source 
language, say Li, and alignment lexicons from the 
target language to every other language in the 
corpus. The WSD procedure in the bilingual 
setting would ensure the sense assignment for 
most of the non-null translation equivalence pairs 
and the clustering algorithm would classify the 
target words which were not translated (or for 
which the word alignment algorithm didn?t find a 
correct translation) in Li based on their 
equivalents in the other k-1 source languages. 
? it can reinforce or modify the sense 
assignment decided by the tie heuristics in case 
of cross-lingual ambiguity. 
To perform the clustering, we derive a set of 
m binary vectors VECT(Lp, TWi) for each source 
language Lp and each target word i occurring m 
times in the corpus. To compute the vectors, we 
first construct a Dictionary Entry List 
DEL(Lp,TWi)={Wj | <TWi, Wj> is a translation 
equivalence pair}, comprising the ordered list of 
all the translation equivalents in the source 
language pL of the target word TW
i. In this part 
of the experiment, the translation equivalents are 
automatically extracted from the parallel corpus 
using a hypothesis testing algorithm described in 
(Tufi? 2002). VECT(Lp,TWik)  specifies which of 
the possible translations of TWi was actually 
used as an equivalent for the kth occurrence of 
TWi. All positions in VECT(Lp,TWik)  are set to 
0 except the bit at position h, which is 1 if the 
translation equivalent (Lp,TWik)=DELh(Lp,TWi). 
The vector for each target word occurrence is 
obtained by concatenating the VECT(Lp,TWik) 
for all k souce languages  and its length is 
?
=
k
1p
i
p  |)TW,DEL(L| . 
We use a Hierarchical Clustering Algorithm 
based on Stolcke?s Cluster2.9 to classify similar 
vectors into sense classes. Stolcke?s algorithm 
generates a clustering tree, the root of which 
corresponds to a baseline clustering (all the 
occurrences are clustered in one sense class) and 
the leaves are single element classes, 
corresponding to each occurrence vector of the 
target word. An interior cut in the clustering tree 
will produce a specific number (say X) of sub-
trees, the roots of which stand for X classes each 
containing the vectors of their leaves. We call an 
interior cut a pertinent cut if X is equal to the 
number of senses TWi has been used throughout 
the entire corpus. One should note that in a 
clustering tree many pertinent cuts could be 
possible. The pertinent cut which corresponds to 
the correct sense clustering of the m occurrences 
of TWi is called a perfect cut.  However, if TWi 
has Y possible senses, it is possible that only a 
subset of the Y senses will be used in an arbitrary 
text. Therefore, a perfect cut in a clustering tree 
cannot be deterministically computed. Instead of 
deriving the clustering tree and guessing at a 
perfect cut, we stop the clustering algorithm 
when Z clusters have been created, where Z is the 
number of senses in which the occurrences of 
TWi have been used in the text in question. 
However, the value of Z is specific to each word 
and depends on the type and size of the text; it 
cannot therefore be computed a priori. In our 
previous work (Tufi? and Ion, 2003), to 
approximate Z we imposed an exit condition for 
the clustering algorithm based on distance 
heuristics. In particular, the algorithm stops when 
the minimal distance between the existing classes 
increases beyond a given threshold level:  
?>+
?+
)1(
)()1(
kdist
kdistkdist                                   (1) 
where dist(k) is the minimal distance between 
two clusters at the k-th  iteration  step and ? is  an 
empirical numerical threshold. Experimentation 
revealed that reasonable results are achieved with 
a value for ? is 0.12. However, although the 
threshold is a parameter for the clustering 
algorithm irrespective of the target words, the 
number of classes the clustering algorithm 
generates (Z) is still dependent on the particular 
target word and the corpus in which it appears. 
By using sense information produced by the 
ILI-similarity approach, the algorithm and its exit 
condition have been modified as described 
below:  
- the sense label of a cluster is given by the 
majority sense of its members as assigned by the 
wordnet-based sense labelling; a cluster 
containing only non-disambiguated occurrences 
has an wild-card sense label;    
- two joinable clusters (that is the clusters with 
the minimal distance and the exit condition (1) 
not satisfied) are joint only when their sense 
labels is the same or one of them has an wild-
card sense label; in this case the wild-card sense 
label is turned into the sense label of the sense-
assigned cluster. Otherwise the next distant 
clusters are tried. 
- the algorithm stops when no clusters can be 
further joined. 
3 The Experiment 
The parallel corpus we used for our experiments 
is based on Orwell?s novel ?Ninety Eighty Four? 
(1984) which has been initially developed by the 
Multext-East consortium. Besides Orwell?s 
original text, the corpus contained professional 
translations in six languages (Bulgarian, Czech, 
Estonian, Hungarian, Romanian and Slovene). 
The Multext-East corpus (and other language 
resources) is maintained by Toma? Erjavec and a 
new release of it may be found at 
http://nl.ijs.si/ME/V3. Later, the parallel corpus 
has been extended with many other new language 
translations. The BalkaNet consortium added 
three new translations to the ?1984? corpus: 
Greek, Serbian and Turkish. Each language text 
is tokenized, tagged and sentence aligned to the 
English original. We extracted from the entire 
parallel corpus only the languages of concern in 
the BalkaNet project (English, Bulgarian, Czech, 
Greek, Romaniann, Serbian and Turkish) and 
further retained only the 1-1 sentence alignments 
between English and all the other languages. This 
way, we built a unique alignment for all the 
languages and, by exploiting the transitivity of 
sentence alignment, we are able to make 
experiments with any combination of languages. 
The BalkaNet version of the ?1984? corpus is 
encoded as a sequence of translation units (TU), 
each containing one sentences per language, so 
that they are reciprocal translations.  In order to 
evaluate both the performance of the WSDtool 
and to assess the accuracy of the interlingual 
linking of the BalkaNet wordnets we selected a 
bag of English target words (nouns and verbs) 
occurring in the corpus. The selection considered 
only polysemous words (at least two senses per 
part of speech) implemented (and ILI linked) in 
all BalkaNet wordnets. There resulted 211 words 
with 1644 occurrences in the English part of the 
parallel corpus. 
Three experts independently sense-tagged all 
the occurrences of the target words and the 
disagreements were negotiated until consensus 
was obtained. The commonly agreed annotation 
represented the Gold Standard (GS) against 
which the WSD algorithm was evaluated. 
Additionally, a number of 13 students, enrolled in 
a Computational Linguistics Master program, 
were asked to manually sense-tag overlapping 
subsets of the same word occurrences.  The 
overlapping ensured that each target word 
occurrence was seen by at least three students. 
Based on the students? annotations, using a 
majority voting, we computed another set of 
comparison data which below is referred to as 
SMAJ (Students MAJority). 
Finally, the same targeted words were 
automatically disambiguated by the WSDtool 
algorithm (ALG) which was run both with and 
without the back-off clustering algorithm.  For 
the basic wordnet-based WSD we used the 
Princeton Wordnet, the Romanian wordnet and 
the English-Romanian translation equivalence 
dictionary. For the back-off clustering we 
extracted a four1 language translation dictionary 
(EN-RO-CZ-BG) based on which we computed 
the initial clustering vectors for all occurrences of 
the target words. 
                                                     
1 Although we used only RO, CZ and BG 
translation texts, nothing prevents us from using any 
other translations, irrespective of whether their 
languages belong or not to the BalkaNet consortium.  
Out of the 211 set of targeted words, with 
1644 occurrences the system could not make a 
decision for 38 (18 %) words with 63 
occurrences (3.83%). Most of these words were 
happax legomena (21) for which neither the 
wordnet-based step not the clustering back-off 
could do anything. Others, were not translated by 
the same part of speech, were wrongly translated 
by the human translator or not translated at all 
(28). Finally, four occurrences remained 
untagged due to the incompleteness of the 
Romanian synsets linked to the relevant concepts 
(that is the four translation equivalents had their 
relevant sense missing from the Romanian 
wordnet). Applying the simple heuristics (SH) 
that says that any unlabelled target occurrence 
receives its most frequent sense, 42 out of 63 of 
them got a correct sense-tag. The table below 
summarizes the results.   
WSD annotation Precision Recall F 
AWN  74.88% 72.01% 73.41%
AWN + C 75.26% 72.38% 73.79%
AWN + C + SH 74.93% 74.93% 74.93%
SMAJ 72.99% 72.99% 72.99%
Table 4. WSD precision recall and F-measure for 
the algorithm based on aligned wordnets (AWN), 
for AWN with clustering (AWN+C) and for 
AWN+C and the simple heuristics 
(AWN+C+SH) and for the students? majority 
voting (SMAJ) 
It is interesting to note that in this experiment 
the students? majority annotation is less accurate 
than the one achieved by the automatic WSD 
annotation in all three variants. This is a very 
encouraging result since it shows that the tedious 
hand-made WSD in building word-sense 
disambiguated corpora for supervised training 
can be avoided. 
4 Conclusion 
Considering the fine granularity of the PWN2.0 
sense inventory, our disambiguation results using 
parallel resources are superior to the state of the 
art in monolingual WSD (with the same sense 
inventory). This is not surprising since the 
parallel texts contain implicit knowledge about 
the sense of an ambiguous word, which has been 
provided by human translators.  The drawback of 
our approach is that it relies on the existence of 
parallel data, which in the vast majority of cases 
is not available. On the other hand, supervised 
monolingual WSD relies on the existence of large 
samples of training data, and our method can be 
applied to produce such data to bootstrap 
monolingual applications. Given that parallel 
resources are becoming increasingly available, in 
particular on the World Wide Web (see for 
instance http://www.balkantimes.com where the 
same news is published in 10 languages), and 
aligned wordnets are being produced for more 
and more languages, it should be possible to 
apply our and similar methods to large amounts 
of parallel data in the not-too-distant future.  
One of the greatest advantages of our 
approach is that it can be used to automatically 
sense-tag corpora in several languages at once. 
That is, if we have a parallel corpus in multiple 
languages (such as the Orwell corpus), 
disambiguation performed on any one of them 
propagates to the rest via the ILI linkage. Also, 
given that the vast majority of words in any given 
language are monosemous (e.g., approximately 
82% of the words in PWN have only one sense), 
the use of parallel corpora in multiple languages 
for WSD offers the potential to significantly 
improve results and provide substantial sense-
annotated corpora for training in a range of 
languages.  
Acknowledgements 
The work reported here was carried within the 
European project BalkaNet, no. IST-2000 29388 
and support from the Romanian Ministry of 
Education and Research. 
References  
Alex. Budanitsky and Graeme Hirst 2001. 
Semantic distance in WordNet: An 
experimental, application-oriented evaluation 
of five measures. Proceedings of the Workshop 
on WordNet and Other Lexical Resources, 
Second meeting of the NAACL, Pittsburgh, 
June. 
William Gale, Ken Church and Dan Yarowsky 
1992. Estimating upper and lower bounds on 
the performance of wordsense disambiguation 
programs. Proceedings of the 30th Annual 
Meeting of ACL, 249-256. 
Adam Kilgarriff 1997. I don't believe in word 
senses. In Computers and the Humanities, 31 
(2): 91-113. 
Nancy Ide and Jean V?ronis 1998. Word Sense 
Disambiguation: The State of the Art. 
Computational Linguistics,24(1): 1-40. 
Nancy Ide, N. 1999. Parallel translations as sense 
discriminators. SIGLEX99: Standardizing 
Lexical Resources, ACL99 Workshop, College 
Park, Maryland, 52-61. 
Nancy Ide, Toma? Erjavec and Dan Tufi? 2002. 
Sense Discrimination with Parallel Corpora. In 
Proceedings of the SIGLEX Workshop on Word 
Sense Disambiguation: Recent Successes and 
Future Directions, 56-60, Philadelphia. 
Andreas Stolcke 1996. ftp.icsi.berkeley.edu/ 
pub/ai/stolcke/software/cluster-2.9.tar.Z/ 
Dan Tufi?. 2002. A cheap and fast way to build 
useful translation lexicons. In Proceedings of 
the 19th International Conference on 
Computational Linguistics, 1030-1036, Taipei.  
Dan Tufi? and Radu Ion. 2003. Word sense 
clustering based on translation equivalence in 
parallel texts; a case study in Romanian. In 
Proceedings of the International Conference on 
Speech and Dialog ? SPED, 13-26, Bucharest.  
Dan Tufi?,  Ana-Maria Barbu and Radu Ion 
2003. A word-alignment system with limited 
language resources. In Proceedings of the 
NAACL 2003 Workshop on Building and Using 
Parallel Texts; Romanian-English Shared 
Task, 36-39, Edmonton.  
Dan Tufi?, Radu Ion and Nancy Ide 2004. Word 
sense disambiguation as a wordnets validation 
method in Balkanet. In Proceedings of the 
LREC?2004, 741-744, Lisbon 
Dan Tufi?, Dan Cristea and Sofia Stamou 2004a. 
BalkaNet: Aims, Methods, Results and 
Perspectives. A General Overview. In D. Tufi? 
(ed): Special Issue on BalkaNet. Romanian 
Journal on Science and Technology of 
Information, 7(3-4):9-44 
Improved Lexical Alignment by Combining Multiple Reified
Alignments
Dan Tufi?
 Institute for Artificial 
Intelligence
 13, ?13 Septembrie?, 
 050711, Bucharest 5, 
Romania 
tufis@racai.ro
Radu Ion 
Institute for Artificial
Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania 
radu@racai.ro
Alexandru Ceau?u
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5, 
Romania 
alceausu@racai.ro
Dan ?tef?nescu
Institute for Artificial 
Intelligence
13, ?13 Septembrie?, 
050711, Bucharest 5,
 Romania 
danstef@racai.ro
Abstract
We describe a word alignment platform 
which ensures text pre-processing (to-
kenization, POS-tagging, lemmatization, 
chunking, sentence alignment) as re-
quired by an accurate word alignment. 
The platform combines two different 
methods, producing distinct alignments. 
The basic word aligners are described in 
some details and are individually evalu-
ated. The union of the individual align-
ments is subject to a filtering post-
processing phase. Two different filtering 
methods are also presented. The evalua-
tion shows that the combined word 
alignment contains 10.75% less errors 
than the best individual aligner. 
1 Introduction 
It is almost a truism that more decision makers, 
working together, are likely to find a better solu-
tion than when working alone. Dieterich (1998) 
discusses conditions under which different deci-
sions (in his case classifications) may be com-
bined for obtaining a better result. Essentially, a 
successful automatic combination method would 
require comparable performance for the decision 
makers and, additionally, that they should not 
make similar errors. This idea has been exploited 
by various NLP researchers in language model-
ling, statistical POS tagging, parsing, etc.
We developed two quite different word align-
ers, driven by two distinct objectives: the first 
one was motivated by a project aiming at the de-
velopment of an interlingually aligned set of 
wordnets while the other one was developed 
within an SMT ongoing project. The first one 
was used for validating, against a multilingual 
corpus, the interlingual synset equivalences and 
also for WSD experiments. Although, initially, it 
was concerned only with open class words re-
corded in a wordnet, turning it into an ?all 
words? aligner was not a difficult task. This 
word aligner, called YAWA is described in sec-
tion 3.
A quite different approach from the one used 
by YAWA, is implemented in our second word 
aligner, called MEBA, described in section 4. It 
is a multiple parameter and multiple step algo-
rithm using relevance thresholds specific to each 
parameter, but different from each step to the 
other. The implementation of MEBA was 
strongly influenced by the notorious five IBM 
models described in (Brown et al 1993). We 
used GIZA++ (Och and Ney 2000; Och and Ney, 
2003) to estimate different parameters of the 
MEBA aligner. 
The alignments produced by MEBA were 
compared to the ones produced by YAWA and 
evaluated against the Gold Standard (GS)1 anno-
tations used in the Word Alignment Shared 
Tasks (Romanian-English track) organized at 
HLT-NAACL2003 (Mihalcea and Pedersen 
2003).
Given that the two aligners are based on quite 
different models and that their F-measures are 
comparable, it was quite a natural idea to com-
bine their results and hope for an improved align-
ment. Moreover, by analyzing the alignment er-
rors done by each word aligner, we found that 
the number of common mistakes was small, so 
1 We noticed in the GS Alignment various errors (both sen-
tence and word alignment errors) that were corrected. The 
tokenization of the bitexts used in the GS Alignment was 
also modified, with the appropriate modification of the ref-
erence alignment. These reference data are available at 
http://www.racai.ro/res/WA-GS
153
the premises for a successful combination were 
very good (Dieterich, 1998). The Combined 
Word Aligner, COWAL-described in section 5, 
is a wrapper of the two aligners (YAWA and 
MEBA) merging the individual alignments and 
filtering the result. At the Shared Task on Word 
Alignment organized by the ACL2005 Work-
shop on ?Building and Using Parallel Corpora: 
Data-driven Machine Translation and Beyond? 
(Martin, et al 2005), we participated (on the 
Romanian-English track) with the two aligners 
and the combined one (COWAL). Out of 37 
competing systems, COWAL was rated the first, 
MEBA the 20th and TREQ-AL (Tufi? et al 
2003), the former version of YAWA, was rated 
the 21st. The usefulness of the aligner combina-
tion was convincingly demonstrated. 
Meanwhile, both the individual aligners and 
their combination were significantly improved. 
COWAL is now embedded into a larger platform 
that incorporates several tools for bitexts pre-
processing (briefly reviewed in section 2), a 
graphical interface that allows for comparing and 
editing different alignments, as well as a word 
sense disambiguation module.  
2 The bitext processing  
The two base aligners and their combination use 
the same format for the input data and provide 
the alignments in the same format. The input 
format is obtained from two raw texts that repre-
sent reciprocal translations. If not already sen-
tence aligned, the two texts are aligned by our 
sentence aligner that builds on Moore?s aligner 
(Moore, 2002) but which unlike it, is able to re-
cover the non-one-to-one sentence alignments. 
The texts in each language are then tokenized, 
tagged and lemmatized by the TTL module (Ion, 
2006). More often than not, the translation 
equivalents have the same part-of speech, but 
relying on such a restriction would seriously af-
fect the alignment recall. However, when the 
translation equivalents have different parts of 
speech, this difference is not arbitrary.  During 
the training phase, we estimated POS affinities: 
{p(POSmRO|POSnEN)} and p(POSnEN|POSmRO)}
and used them to filter out improbable translation 
equivalents candidates.
The next pre-processing step is represented by 
sentence chunking in both languages. The 
chunks are recognized by a set of regular expres-
sions defined over the tagsets and they corre-
spond to (non-recursive) noun phrases, adjectival 
phrases, prepositional phrases and verb com-
plexes (analytical realization of tense, aspect 
mood and diathesis and phrasal verbs). Finally, 
the bitext is assembled as an XML document 
(Tufi? and Ion, 2005), which is the standard input 
for most of our tools, including COWAL align-
ment platform. 
3 YAWA 
YAWA is a three stage lexical aligner that uses 
bilingual translation lexicons and phrase bounda-
ries detection to align words of a given bitext. 
The translation lexicons are generated by a dif-
ferent module, TREQ (Tufi?, 2002), which gen-
erates translation equivalence hypotheses for the 
pairs of words (one for each language in the par-
allel corpus) which have been observed occur-
ring in aligned sentences more than expected by 
chance. The hypotheses are filtered by a log-
likelihood score threshold. Several heuristics 
(string similarity-cognates, POS affinities and 
alignments locality2) are used in a competitive 
linking manner (Melamed, 2001) to extract the 
most likely translation equivalents. 
YAWA generates a bitext alignment by in-
crementally adding new links to those created at 
the end of the previous stage. The existing links 
act as contextual restrictors for the new added 
links. From one phase to the other new links are 
added without deleting anything. This monotonic 
process requires a very high precision (at the 
price of a modest recall) for the first step. The 
next two steps are responsible for significantly 
improving the recall and ensuring an increased 
F-measure.  
In the rest of this section we present the three 
stages of YAWA and evaluate the contribution 
of each of them to the final result. 
3.1 Phase 1: Content Words Alignment 
YAWA begins by taking into account only very 
probable links that represent the skeleton align-
ment used by the second phase. This alignment is 
done using outside resources such as translation 
lexicons and involves only the alignment of con-
tent words (nouns, verbs, adjective and adverbs). 
The translation equivalence pairs are ranked 
according to an association score (i.e. log-
likelihood, DICE, point-wise mutual informa-
2 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
requires that all alignment links starting from a chunk in the 
one language end in a chunk in the other language.
154
tion, etc.). We found that the best filtering of the 
translation equivalents was the one based on the
log-likelihood (LL) score with a threshold of 9.
Each translation unit (pair of aligned sen-
tences) of the target bitext is scanned for estab-
lishing the most likely links based on a competi-
tive linking strategy that takes into account the
LL association scores given by the TREQ trans-
lation lexicon. If a candidate pair of words is not 
found in the translation lexicon, we compute
their orthographic similarity (cognate score 
(Tufi?, 2002)). If this score is above a predeter-
mined threshold (for Romanian-English task we 
used the empirically found value of 0.43), the 
two words are treated as if they existed in the
translation lexicon with a high association score 
(in practice we have multiplied the cognate score
by 100 to yield association scores in the range 0
.. 100). The Figure 1 exemplifies the links cre-
ated between two tokens of a parallel sentence by
the end of the first phase.
Figure 1: Alignment after the first step 
3.2 Phase 2: Chunks Alignment 
The second phase requires that each part of the
bitext is chunked. In our Romanian-English ex-
periments, this requirement was fulfilled by us-
ing a set of regular expressions defined over the
tagsets used in the target bitext. These simple
chunkers recognize noun phrases, prepositional
phrases, verbal and adjectival or adverbial group-
ings of both languages.
In this second phase YAWA produces first
chunk-to-chunk matching and then aligns the 
words within the aligned chunks. Chunk align-
ment is done on the basis of the skeleton align-
ment produced in the first phase. The algorithm
is simple: align two chunks c(i) in source lan-
guage and c(j) in the target language if c(i) and 
c(j) have the same type (noun phrase, preposi-
tional phrase, verb phrase, adjectival/adverbial 
phrase) and if there exist a link ?w(s), w(t)? so 
that w(s) ? c(i) then w(t) ? c(j).
After alignment of the chunks, a language pair 
dependent module takes over to align the un-
aligned words belonging to the chunks. Our 
module for the Romanian-English pair of lan-
guages contains some very simple empirical
rules such as: if b is aligned to c and b is pre-
ceded by a, link a to c, unless there exist d in the 
same chunk with c and the POS category of d has 
a significant affinity with the category of a. The 
simplicity of these rules derives from the shallow
structures of the chunks. In the above example b
and c are content words while a is very likely a 
determiner or a modifier for b. The result of the 
second alignment phase, considering the same
sentence in Figure 1, is shown in Figure 2. The
new links are represented by the double lines. 
 Figure 2: Alignment after the second step 
3.3 Phase 3: Dealing with sequences of un-
aligned words
This phase identifies contiguous sequences of
words (blocks) in each part of the bitext which
remain unaligned and attempts to heuristically
match them. The main criteria used to this end
are the POS-affinities of the remaining unaligned 
words and their relative positions. Let us illus-
trate, using the same example and the result 
shown in Figure 2, how new links are added in
this last phase of the alignment. At the end of 
phase 2 the blocks of consecutive words that re-
main to be aligned are: English {en0 = (you), en1
= (that), en2 = (is, not), en3 = (and), en4 = (.)} and 
155
Romanian {ro0 = (), ro1 = (c?), ro2 = (nu, e), ro3 = 
(?i), ro4 = (.)}. The mapping of source and target
unaligned blocks depends on two conditions: that 
surrounding chunks are already aligned and that
pairs in candidate unaligned blocks have signifi-
cant POS-affinity. For instance in the figure
above, blocks en1 = (that) and ro1 = (c?) satisfy
the above conditions because they appear among
already aligned chunks (<?ll notice> ? <ve?i
observa> and <D?ncu ?s generosity> ? <gene- 
rozitatea lui D?ncu>) and they contain words 
with the same POS.
After block alignment3, given a pair of aligned
blocks, the algorithm links words with the same
POS and then the phase 2 is called again with 
these new links as the skeleton alignment. In 
Figure 3 is shown the result of phase 3 alignment
of the sentence we used as an example through-
out this section. The new links are shown (as
before) by double lines. 
Figure 3: Alignment after the third step 
The third phase is responsible for significant
improvement of the alignment recall, but it also 
generates several wrong links. The detection of
some of them is quite straightforward, and we
added an additional correction phase 3.f. By ana-
lysing the bilingual training data we noticed the trans-
lators? tendency to preserve the order of the 
phrasal groups. We used this finding (which 
might not be valid for any language pair) as a 
removal heuristics for the links that cross two or
more aligned phrase groups.  One should notice 
that the first word in the English side of the ex-
ample in Figure 3 (?you?) remained unaligned
(interpreted as not translated in the Romanian
side). According to the Gold Standard used for 
3 Only 1:1 links are generated between blocks. 
evaluation in the ACL2005 shared task, this in-
terpretation was correct, and therefore, for the
example in Figure 3, the F-measure for the 
YAWA alignment was 100%.
However, Romanian is a pro-drop language 
and although the translation of the English pro-
noun is not lexicalized in Romanian, one could 
argue that the auxiliary ?ve?i? should be aligned 
also to the pronoun ?you? as it incorporates the
grammatical information carried by the pronoun. 
Actually, MEBA (as exemplified in Figure 4)
produced this multiple token alignment (and was 
penalized for it!). 
3.4 Performance analysis
The table that follows presents the results of the
YAWA aligner at the end of each alignment
phase. Although the Precision decreases from
one phase to the next one, the Recall gains are 
significantly higher, so the F-measure is mono-
tonically increasing.
Precision Recall F-Measure
Phase 1 94.08% 34.99% 51.00%
Phase 1+2 89.90% 53.90% 67.40%
Phase 1+2+3 88.82% 73.44% 80.40%
Phase 1+2+3+3.f 88.80% 74.83% 81.22%
Table 1: YAWA evaluation 
4 MEBA 
MEBA uses an iterative algorithm that takes ad-
vantage of all pre-processing phases mentioned
in section 2. Similar to YAWA aligner, MEBA 
generates the links step by step, beginning with 
the most probable (anchor links). The links to be 
added at any later step are supported or restricted 
by the links created in the previous iterations. 
The aligner has different weights and different
significance thresholds on each feature and itera-
tion. Each of the iterations can be configured to
align different categories of tokens (named enti-
ties, dates and numbers, content words, func-
tional words, punctuation) in decreasing order of 
statistical evidence. 
The first iteration builds anchor links with a 
high level of certainty (that is cognates, numbers,
dates, pairs with high translation probability).
The next iteration tries to align content words
(open class categories) in the immediate vicinity
of the anchor links. In all steps, the candidates
are considered if and only if they meet the mini-
mal threshold restrictions.
A link between two tokens is characterized by
a set of features (with values in the [0,1] inter-
val). We differentiate between context independ-
156
ent features that refer only to the tokens of the
current link (translation equivalency, part-of-
speech affinity, cognates, etc.) and context de-
pendent features that refer to the properties of the 
current link with respect to the rest of links in a 
bi-text (locality, number of traversed links, to-
kens indexes displacement, collocation). Also, 
we distinguish between bi-directional features
(translation equivalence, part-of-speech affinity)
and non-directional features (cognates, locality,
number of traversed links, collocation, indexes 
displacement).
Precision Recall F-measure
?Anchor? links 98.50% 26.82% 42.16%
Words around 
?anchors? 96.78% 42.41% 58.97%
Funct. words 
and punctuation 94.74% 59.48% 73.08%
Probable links 92.05% 71.00% 80.17%
Table 2: MEBA evaluation 
The score of a candidate link (LS) between a 
source token i and a target token j is computed
by a linear function of several features scores
(Tiedemann, 2003).
?
 
 
n
i
ii ScoreFeatjiLS
1
*),( O ; 1
1
 ?
 
n
i
iO
Each feature has defined a specific signifi-
cance threshold, and if the feature?s value is be-
low this threshold, the contribution to the LS of 
the current link of the feature in case is nil. 
The thresholds of the features and lambdas are 
different from one iteration to the others and they
are set by the user during the training and system
fine-tuning phases. There is also a general 
threshold for the link scores and only the links 
that have the LS above this threshold are retained
in the bitext alignment. Given that this condition 
is not imposing unique source or target indexes,
the resulting alignment is inherently many-to-
many.
In the following subsections we briefly discuss
the main features we use in characterising a link.
4.1 Translation equivalence
This feature may be used for two types of pre-
processed data: lemmatized or non-lemmatized
input. Depending on the input format, MEBA
invokes GIZA++ to build translation probability
lists for either lemmas or the occurrence forms of 
the bitext4. Irrespective of the lemmatisation op-
tion, the considered token for the translation 
model build by GIZA++ is the respective lexical 
item (lemma or wordform) trailed by its POS tag 
(eg. plane_N, plane_V, plane_A). In this way we 
avoid data sparseness and filter noisy data. For 
instance, in case of highly inflectional languages 
(as Romanian is) the use of lemmas significantly
reduces the data sparseness. For languages with
weak inflectional character (as English is) the 
POS trailing contributes especially to the filter-
ing the search space. A further way of removing
the noise created by GIZA++ is to filter out all 
the translation pairs below a LL-threshold. We 
made various experiments and, based on the es-
timated ratio between the number of false nega-
tives and false positive, empirically set the value
of this threshold to 6. All the probability losses 
by this filtering were redistributed proportionally
to their initial probabilities to the surviving trans-
lation equivalence candidates. 
4.2 Translation equivalence entropy score 
The translation equivalence relation is a se-
mantic one and it directly addresses the notion of 
word sense. One of the Zipffian laws prescribes a 
skewed distribution of the senses of a word oc-
curring several times in a coherent text. We used
this conjecture as a highly informative informa-
tion source for the validity of a candidate link.
The translation equivalence entropy score is a 
favouring parameter for the words that have few 
high probability translations. Since this feature is
definitely sensitive to the order of the lexical 
items, we compute an average value for the link: 
DES(A)+EES(B). Currently we use D=E=0.5, but 
it might be interesting to see, depending on dif-
ferent language pairs, how the performance of 
the aligner would be affected by a different set-
tings of these parameters.
N
TRWpTRWp
N
i
ii
WES log
),(log*),(
11)(
?
 

 
4.3 Part-of-speech affinity
In faithful translations the translated words tend
to be translated by words of the same part-of-
speech. When this is not the case, the different 
POSes, are not arbitrary. The part of speech af-
finity, P(cat(A)|cat(B), can be easily computed
from a gold standard alignment. Obviously, this
4 Actually, this is a user-set parameter of the MEBA aligner;
if the input bitext contain lemmatization information, both 
translation probability tables may be requested. 
157
is a directional feature, so an averaging operation 
is necessary in order to ascribe this feature to a 
link: PA=DP(cat(A)|cat(B)) + EP(cat(B)|cat(A)).
Again, we used D=E=0.5 but different values of 
these weights might be worthwhile investigating. 
4.4 Cognates 
The similarity measure, COGN(TS, TT), is im-
plemented as a Levenstein metric. Using the
COGN test as a filtering device is a heuristic 
based on the cognate conjecture, which says that 
when the two tokens of a translation pair are 
orthographically similar, they are very likely to
have similar meanings (i.e. they are cognates). 
The threshold for the COGN(TS, TT) test was 
empirically set to 0.42. This value depends on 
the pair of languages in the bitext. The actual 
implementation of the COGN test includes a lan-
guage-dependent normalisation step, which strips 
some suffixes, discards the diacritics, reduces 
some consonant doubling, etc. This normalisa-
tion step was hand written, but, based on avail-
able lists of cognates, it could be automatically
induced.
4.5 Obliqueness 
Each token in both sides of a bi-text is character-
ized by a position index, computed as the ratio 
between the relative position in the sentence and 
the length of the sentence. The absolute value of 
the difference between tokens? position indexes,
subtracted from 15, gives the link?s ?oblique-
ness?.
)()(
1),(
TS
ji Sentlength
j
Sentlength
iTWSWOBL  
This feature is ?context free? as opposed to the 
locality feature described below.
4.6 Locality 
Locality is a feature that estimates the degree to 
which the links are sticking together. 
MEBA has three features to account for local-
ity: (i) weak locality, (ii) chunk-based locality
and (iii) dependency-based locality.
The value of the weak locality feature is de-
rived from the already existing alignments in a 
window of N tokens centred on the focused to-
ken. The window size is variable, proportional to 
the sentence length. If in the window there exist
k linked tokens and the relative positions of the 
5 This is to ensure that values close to 1 are ?good? ones and 
those near 0 are ?bad?. This definition takes into account the
relatively similar word order in English and Romanian.
tokens in these links are <i1 j1>, ?<ik jk> then 
the locality feature of the new link <ik+1, jk+1> is 
defined by the equation below: 
)
||
||1,1min(
1 1
1?
 


 
k
m mk
mk
jj
ii
k
LOC
If the new link starts from or ends in a token 
already linked, the index difference that would
be null in the formula above is set to 1. This way,
such candidate links would be given support by
the LOC feature (and avoid overflow error). In 
the case of chunk-based locality the window 
span is given by the indexes of the first and last 
tokens of the chunk. 
Dependency-based locality uses the set of the 
dependency links of the tokens in a candidate
link for the computation of the feature value. In
this case, the LOC feature of a candidate link
<ik+1, jk+1> is set to 1 or 0 according to the fol-
lowing rule: 
if between ik+1 and iD there is a (source lan-
guage) dependency and if between jk+1 and jE
there is also a (target language) dependency then 
LOC is 1 if iD and jE are aligned, and 0 otherwise. 
Please note that in case jk+1{ jE a trivial depend-
ency (identity) is considered and the LOC attrib-
ute of the link <ik+1, jk+1> is set to always to 1.
Figure 4: Chunk and dependency-based locality
4.7 Collocation 
Monolingual collocation is an important clue for 
word alignment. If a source collocation is trans-
lated by a multiword sequence, very often the
lexical cohesion of source words can also be
found in the corresponding translated words. In 
this case the aligner has strong evidence for 
158
many to many linking. When a source colloca-
tion is translated as a single word, this feature is
a strong indication for a many to 1 linking.
Bi-gram lists (only content words) were built 
from each monolingual part of the training cor-
pus, using the log-likelihood score (threshold of 
10) and minimal occurrence frequency (3) for
candidates filtering.
We used the bi-grams list to annotate the 
chains of lexical dependencies among the con-
tents words. Then, the value of the collocation 
feature is computed similar to the dependency-
based locality feature. The algorithm searches for
the links of the lexical dependencies around the 
candidate link. 
5 Combining the reified alignments 
From a given alignment one can compute a se-
ries of properties for each of its links (such as the 
parameters used by the MEBA aligner). A link
becomes this way a structured object that can be 
manipulated in various ways, independent of the
bitext (or even of the lexical tokens of the link)
from which it was extracted. We call this proce-
dure alignment reification. The properties of the 
links of two or more alignments are used for our 
methods of combining the alignments.
One simple, but very effective method of
alignment combination is a heuristic procedure, 
which merges the alignments produced by two or
more word aligners and filters out the links that 
are likely to be wrong. For the purpose of filter-
ing, a link is characterized by its type defined by
the pair of indexes (i,j) and the POS of the tokens
of the respective link. The likelihood of a link is 
proportional to the POS affinities of the tokens of
the link and inverse proportional to the bounded
relative positions (BRP) of the respective tokens:
  where avg is the average
displacement in a Gold Standard of the aligned 
tokens with the same POSes as the tokens of the 
current link. From the same gold standard we 
estimated a threshold below which a link is re-
moved from the final alignment.
||||1 avgjiBRP  
A more elaborated alignment combination
(with better results than the previous one) is 
modelled as a binary statistical classification 
problem (good / bad) and, as in the case of the 
previous method, the net result is the removal of 
the links which are likely to be wrong. We used
an ?off-the-shelf? solution for SVM training and 
classification - LIBSVM6 (Fan et al, 2005) with 
6 http://www.csie.ntu.edu.tw/~cjlin/libsvm/
the default parameters (C-SVC classification and
radial basis kernel function). Both context inde-
pendent and context dependent features charac-
terizing the links were used for training. The
classifier was trained with both positive and 
negative examples of links. A set of links ex-
tracted from the Gold Standard alignment was
used as positive examples set. The same number
of negative examples was extracted from the
alignments produced by COWAL and MEBA 
where they differ from the Gold Standard.
It is interesting to notice that for the example
discussed in Figures 1-4, the first combiner
didn?t eliminate the link <you ve?i> producing 
the result shown in Figure 4. This is because the 
relative positions of the two words are the same
and the POS-affinity of the English personal
pronouns and the Romanian auxiliaries is signifi-
cant. On the other hand, the SVM-based com-
biner deleted this link, producing the result
shown in Figure 3. The explanation is that, ac-
cording to the Gold Standard we used, the links 
between English pronouns and Romanian auxil-
iaries or main verbs in pro-drop constructions
were systematically dismissed (although we 
claim that they shouldn?t and that the alignment
in Figure 4 is better than the one in Figure 3).
The evaluation (according to the Gold Standard)
of the SVM-based combination (COWAL),
compared with the individual aligners, is shown
in Table 3. 
Aligner Precision Recall F-measure
YAWA 88.80% 74.83% 81.22%
MEBA 92.05% 71.00% 80.17%
COWAL 86.99% 79.91% 83.30%
Table 3: Combined alignment
6 Conclusions and further work
Neither YAWA nor MEBA needs an a priori bi-
lingual dictionary, as this will be automatically
extracted by TREQ or GIZA++. We made
evaluation of the individual alignments in both
experimental settings: without a start-up bilin-
gual lexicon and with an initial mid-sized bilin-
gual lexicon. Surprisingly enough, we found that
while the performance of YAWA increases a
little bit (approx. 1% increase of the F-measure)
MEBA is doing better without an additional lexi-
con. Therefore, in the evaluation presented in the 
previous section MEBA uses only the training
data vocabulary.
YAWA is very sensitive to the quality of the
bilingual lexicons it uses. We used automatically
translation lexicons (with or without a seed lexi-
159
con), and the noise inherently present might have 
had a bad influence on YAWA?s precision. Re-
placing the TREQ-generated bilingual lexicons 
with validated (reference bilingual lexicons) 
would further improve the overall performance 
of this aligner.  Yet, this might be a harder to 
meet condition for some pairs of languages than 
using parallel corpora. 
MEBA is more versatile as it does not require 
a-priori bilingual lexicons but, on the other hand, 
it is very sensitive to the values of the parameters 
that control its behaviour. Currently they are set 
according to the developers? intuition and after 
the analysis of the results from several trials. 
Since this activity is pretty time consuming (hu-
man analysis plus re-training might take a couple 
of hours) we plan to extend MEBA with a super-
vised learning module, which would automati-
cally determine the ?optimal? parameters 
(thresholds and weights) values. 
It is worth noticing that with the current ver-
sions of our basic aligners, significantly im-
proved since the ACL shared word alignment 
task in June 2005, YAWA is now doing better 
than MEBA, and the COWAL F-measure in-
creased with 9.4%. However, as mentioned be-
fore, these performances were measured on a 
different tokenization of the evaluation texts and 
on the partially corrected gold standard align-
ment (see footnote 1).  
References
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert J. Mercer. 1993. The mathe-
matics of statistical machine translation: Parameter 
estimation. Computational Linguistics, 19(2): 263?
311.
Thomas G. Dietterich. 1998. Approximate Statistical 
Tests for Comparing Supervised Classification 
Learning Algorithms. Neural Computation, 10 (7) 
1895-1924.
Rong-en Fan, Pai-Hsuen Chen, Chij-Jen Lin. 2005. 
Working set selection using the second order 
information for training SVM. Technical report, 
Department of Computer Science, National Taiwan 
University (www.csie.ntu.edu.tw/~cjlin/papers/
quadworkset.pdf).
William A. Gale, Kenneth W. Church. 1991. Identify-
ing word correspondences in parallel texts. In Pro-
ceedings of the Fourth DARPA Workshop on 
Speech and Natural Language. Asilomar, CA:152?
157.
Radu Ion. 2006. TTL: A portable framework for to-
kenization, tagging and lemmatization of large cor-
pora. PhD thesis progress report. Research Institute 
for Artificial Intelligence, Romanian Academy, 
Bucharest (in Romanian), 22p. 
Dan Melamed. 2001. Empirical Methods for Exploit-
ing Parallel Texts. Cambridge, MA, MIT Press. 
Rada Mihalcea, Ted Pedersen. 2003. An Evaluation 
Exercise for Word Alignment. Proceedings of the 
HLT-NAACL 2003 Workshop: Building and Using 
Parallel Texts Data Driven Machine Translation 
and Beyond. Edmonton, Canada: 1?10. 
Joel Martin, Rada Mihalcea, Ted Pedersen. 2005. 
Word Alignment for Languages with Scarce Re-
sources. In Proceeding of the ACL2005 Workshop 
on ?Building and Using Parallel Corpora: Data-
driven Machine Translation and Beyond?. June,
2005, Ann Arbor, Michigan, June, Association for 
Computational Linguistics, 65?74 
Robert Moore. 2002. Fast and Accurate Sentence 
Alignment of Bilingual Corpora in Machine Trans-
lation: From Research to Real Users. In Proceed-
ings of the 5th Conference of the Association for 
Machine Translation in the Americas, Tiburon, 
California), Springer-Verlag, Heidelberg, Ger-
many: 135-244. 
Franz J. Och, Herman Ney. 2003. A Systematic Com-
parison of Various Statistical Alignment Models, 
Computational Linguistics, 29(1):19-51. 
Franz J. Och, Herman Ney. 2000. Improved Statistical 
Alignment Models. In Proceedings of the 38th Con-
ference of ACL, Hong Kong: 440-447. 
Joerg Tiedemann. 2003. Combining clues for word 
alignment. In Proceedings of the 10th EACL, Bu-
dapest, Hungary: 339?346. 
Dan Tufi?. 2002. A cheap and fast way to build useful 
translation lexicons. In Proceedings of COL-
ING2002, Taipei, China: 1030-1036. 
Dan Tufi?, Ana-Maria Barbu, Radu Ion. 2003. TREQ-
AL: A word-alignment system with limited lan-
guage resources. In Proceedings of the NAACL 
2003 Workshop on Building and Using Parallel 
Texts; Romanian-English Shared Task, Edmonton, 
Canada: 36-39. 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan Ste-
f?nescu. 2005. Combined Aligners. In Proceeding
of the ACL2005 Workshop on ?Building and Using 
Parallel Corpora: Data-driven Machine Transla-
tion and Beyond?. June, 2005, Ann Arbor, Michi-
gan, June, Association for Computational Linguis-
tics, pp. 107-110. 
Dan Tufi?, Radu Ion. 2005. Multiple Sense Invento-
ries and Test-Bed Corpora. In C. Burileanu (ed.) 
Trends in Speech Technology, Publishing House of 
the Romanian Academy, Bucharest: 49-58.
160
TREQ-AL: A word alignment system with limited language resources 
 
Dan Tufi?, Ana-Maria Barbu, Radu Ion 
Romanian Academy Institute for Artificial Intelligence 
13, ?13 Septembrie?, 74311, Bucharest 5, Romania 
{tufis,abarbu,radu}@racai.ro 
 
 
 
Abstract
 
We provide a rather informal presentation of a 
prototype system for word alignment based on 
our previous translation equivalence approach, 
discuss the problems encountered in the 
shared-task on word-aligning of a parallel 
Romanian-English text, present the preliminary 
evaluation results and suggest further ways of 
improving the alignment accuracy. 
 
1 Introduction 
In (Tufi? and Barbu, 2002; Tufi?, 2002) we largely 
described our extractor of translation equivalents, called 
TREQ. It was aimed at building translation dictionaries 
from parallel corpora. We described in (Ide et al 2002) 
how this program is used in word clustering and in 
checking out the validity of the cross-lingual links 
between the monolingual wordnets of the multilingual 
Balkanet lexical ontology (Stamatou et al 2002). In this 
paper we describe the TREQ-AL system, which builds 
on TREQ and aims at generating a word-alignment map 
for a parallel text (a bitext). TREQ-AL was built in less 
than two weeks for the Shared Task proposed by the 
organizers of the workshop on ?Building and Using 
Parallel Texts:Data Driven Machine Translation and 
Beyond? at the HLT-NAACL 20031 conference. It can 
be improved in several ways that became conspicuous 
when we analyzed the evaluation results. TREQ-AL has 
no need for an a priori bilingual dictionary, as this will 
be automatically extracted by TREQ. However, if such 
a dictionary is available, both TREQ and TREQ-AL 
know to make best use of it. This ability allows both 
systems to work in a bootstrapping mode and to produce 
larger dictionaries and better alignments as they are 
used. 
The word alignment, as it was defined in the shared 
task is different and harder than the problem of 
translation equivalence as previously addressed. In a 
dictionary extraction task one translation pair is 
considered correct, if there is at least one context in 
which it has been rightly observed. A multiply 
occurring pair would count only once for the final 
                                                 
1 http://www.cs.unt.edu/~rada/wpt/index.html#shared 
dictionary. This is in sharp contrast with the alignment 
task where each occurrence of the same pair equally 
counts. 
Another differentiating feature between the two 
tasks is the status of functional word links. In extracting 
translation equivalents one is usually interested only in 
the major categories (open classes). In our case (because 
of the WordNet centered approach of our current 
projects) we were especially interested in POS-
preserving translation equivalents. However, since in 
EuroWordNet and Balkanet one can define cross-POS 
links, the different POS translation equivalents became 
of interest (provided these categories are major ones).  
The word alignment task requires each word 
(irrespective of its POS) or punctuation mark in both 
parts of the bitext be assigned a translation in the other 
part (or the null translation if the case).  
Finally, the evaluations of the two tasks, even if 
both use the same measures as precision or recall, have 
to be differently judged. The null alignments in a 
dictionary extraction task have no significance, while in 
a word alignment task they play an important role (in 
the Romanian-English gold standard data the null 
alignments represent 13,35% of the total number of 
links).  
 
2 The preliminary data processing  
The TREQ system requires sentence aligned parallel 
text, tokenized, tagged and lemmatized. The first 
problem we had with the training and test data was 
related to the tokenization. In the training data there 
were several occurrences of glued words (probably due 
to a problem in text export of the initial data files) plus 
an unprintable character (hexadecimal code A0) that 
generated several tagging errors due to guesser 
imperfect performance (about 70% accurate). 
To remedy these inconveniences we wrote a script 
that automatically split the glued words and eliminated 
the unprintable characters occurring in the training data. 
The set of splitting rules, learnt from the training 
data was posted on the site of the shared task. The set of 
rules is likely to be incomplete (some glued words 
might have survived in the training data) and also might 
produce wrong splitting in some cases (e.g. turnover 
being split always in turn over).  
The text tokenization, as considered by the 
evaluation protocol, was the simplest possible one, with 
white spaces and punctuation marks taken as separators. 
The hyphen (?-?) was always considered a separator and 
consequently taken to be always a token by itself. 
However, in Romanian, the hyphen is more frequently 
used as an elision marker (as in ?intr-o?= ?intru o?/in a), 
a clitics separator (as in ?da-mi-l?=?da ?mi ?l?=?da mie 
el?/give to me it/him) or as a compound marker (as in 
?terchea-berchea? /(approx.) loafer) than as a separator.  
In such cases the hyphen cannot be considered a token. 
A similar problem appeared in English with respect to 
the special quote character, which was dealt with in 
three different ways: it was sometimes split as a distinct 
token (we?ll = we + ? + ll), sometimes was adjoined to 
the string (a contracted positive form or a genitival) 
immediately following it (I?m = I + ?m, you?ve =  
you+?ve,  man?s = man + ?s etc.) and systematically left 
untouched in the negative contracted forms (couldn?t, 
wasn?t, etc).   
Since our processing tools (especially the tokeniser) 
were built with a different segmentation strategy in 
mind, we generated the alignments based on our own 
tokenization and, at the end, we ?re-tokenised? the text 
according to the test data model (and consequently re-
index) all the linking pairs.  
For tagging the Romanian side of the training bitext 
we used the tiered-tagging approach (Tufi?, 1999) but 
we had to construct a new language model since our 
standard model was created from texts containing 
diacritics. As the Romanian training data did not contain 
diacritical characters, this was by no means a trivial task 
in the short period of time at our disposal (actually it 
took most of the training time). The lack of diacritics in 
the training data and the test data induced spurious 
ambiguities that degraded the tagging accuracy with at 
least 1%. This is to say that we estimate that on a 
normal Romanian text (containing the diacritical 
characters) the performance of our system would have 
been better. The English training data was tagged by 
Eric Gaussier, warmly acknowledged here. As the 
tagsets used for the two languages in the parallel 
training corpus were quite different, we defined a tagset 
mapping and translated the tagging of the English part 
into a tagging closer to the Romanian one. This 
mapping introduced some ambiguities that were solved 
by hand. Based on the training data (both Romanian and 
English texts), tagged with similar tagsets, we built the 
language models used for the test data alignment. 
POS-preserving translation equivalence is a too 
restrictive condition for the present task and we defined 
a meta-tagset, common for both languages that 
considered frequent POS alternations. For instance, the 
verb, noun and adjective tags, in both languages were 
prefixed with a common symbol, given that verb-
adjective, noun-verb, noun-adjective and the other 
combinations are typical for Romanian-English 
translation equivalents that do not preserve the POS. 
With these prefixes, the initial algorithm for extracting 
POS-preserving translation equivalents could be used 
without any further modifications. Using the tag-
prefixes seems to be a good idea not only for legitimate 
POS-alternating translations, but also for overcoming 
some typical tagging errors, such as participles versus 
adjectives. In both languages, this is by far the most 
frequent tagging error made by our tagger. 
The last preprocessing phase is encoding the corpus 
in a XCES-Align-ana format as used in the MULTEXT-
EAST corpus (see http://nl.ijs.si/ME/V2/) which is the 
standard input for the TREQ translation equivalents 
extraction program. Since the description of TREQ is 
extensively given elsewhere, we will not go into further 
details, except of saying that the resulted translation 
dictionary extracted from the training data contains 
49283 entries (lemma-form). The filtering of the 
translation equivalents candidates (Tufi? and Barbu, 
2002) was based on the log-likelihood and the cognate 
scores with a threshold value set to 15 and 0,43 
respectively.  We roughly estimated the accuracy of this 
dictionary based on the aligned gold standard: precision 
is about 85% and recall is about 78% (remember, the 
dictionary is evaluated in terms of lemma entries, and 
the non-matching meta-category links are excluded).  
 
3 The TREQ-AL linking program  
This program takes as input the dictionary created by 
TREQ and the parallel text to be word-aligned. The 
alignment procedure is a greedy one and considers the 
aligned translation units independent of the other 
translation units in the parallel corpus.  It has 4 steps: 
1. left-to-right pre-alignment 
2. right-to-left adjustment of the pre-alignment 
3. determining alignment zones and filtering them out 
4. the word-alignment inside the  alignment zones 
 
3.1 The left-to-right pre-alignment 
For each sentence-alignment unit, this step scans the 
words from the first to the last in the source-language 
part (Romanian). The considered word is initially linked 
to all the words in the target-language part (English) of 
the current sentence-alignment unit, which are found in 
the translation dictionary as potential translations. If for 
the source word no translations are identified in the 
target part of the translation unit, the control advances to 
the next source word. The cognate score and the relative 
distance are decision criteria to choose among the 
possible links. When consecutive words in the source 
part are associated with consecutive or close to each 
other words in the target part, these are taken as forming 
an ?alignment chain? and, out of the possible links, are 
considered those that correspond to the densest 
grouping of words in each language. High cognate 
scores in an alignment chain reinforce the alignment. 
One should note that at the end of this step it is possible 
to have 1-to-many association links if multiple 
translations of one or more source words are found in 
the target part of the current translation unit (and, 
obviously, they satisfy the selection criteria). 
 
3.2 The right-to-left adjustment of the pre-alignment 
This step tries to correct the pre-alignment errors (when 
possible) and makes a 1-1 choice in case of the 1-m 
links generated before. The alignment chains (found in 
the previous step) are given the highest priority in 
alignment disambiguation. That is, if for one word in 
the source language there are several alignment 
possibilities, the one that belongs to an alignment chain 
is always selected. Then, if among the competing 
alignments one has a cognate score higher than the 
others then this is the preferred one (this heuristics is 
particularly useful in case of several proper names 
occurring in the same translation unit). Finally, the 
relative position of words in the competing links is 
taken into account to minimize the distance between the 
surrounding already aligned words. 
The first two phases result in a 1-1 word mapping. 
The next two steps use general linguistic knowledge 
trying to align the words that remain unaligned (either 
due to no translation equivalents or because of failure to 
meet the alignment criteria) after the previous steps. 
This could result in n-m word alignments, but also in 
unlinking two previously linked words since a wrong 
translation pair existing in the extracted dictionary 
might license a wrong link.  
 
3.3 Alignment zones and filtering suspicious links out 
An alignment zone (in our approach) is a piece of text 
that begins with a conjunction, a preposition, or a 
punctuation mark and ends with the token preceding the 
next conjunction, preposition, punctuation or end of 
sentence. A source-language alignment zone is mapped 
to one or more target-language alignment zones via the 
links assigned in the previous steps (based on the 
translation equivalents). One has to note that the 
mapping of the alignment zones is not symmetric. An 
alignment zone that contains no link is called a virgin 
zone. 
In most of the cases the words in the source 
alignment zone (starting zone) are linked to words in the 
target algnment zone/s (ending zone/s). The links with 
either side outside the alignment zones are suspicious 
and they are deleted. This filtering proved to be almost 
100% correct in case the outlier resides in a zone non-
adjacent to the starting or ending zones. The failures of 
this filtering were in the majority of cases due to a 
wrong use of punctuation in one or the other part of the 
translation unit (such as omitted comma, a comma 
between the subject and predicate). 
 
3.4 The word-alignment inside the alignment zones 
For each un-linked word in the starting zone the 
algorithm looks for a word in the ending zone/s of the 
same category (not meta-category). If such a mapping 
was not possible, the algorithm tries to link the source 
word to a target word of the same meta-category, thus 
resulting in a cross-POS alignment. The possible meta-
category mappings are specified by the user in an 
external mapping file. Any word in the source or target 
languages that is not assigned a link after the four 
processing steps described above is automatically 
assigned a null link. 
 
4 Post-processing  
As said in the second section, our tokenization was 
different from the tokenization in the training and test 
data. To comply with the evaluation protocol, we had to 
re-tokenize the aligned text and re-compute the indexes 
of the links. Re-tokenizing the text meant splitting 
compounds and contracted future forms and gluing 
together the previously split negative contracted forms 
(do+n?t=don?t). Although the re-tokenization was a 
post-processing phase, transparent for the task itself, it 
was a source of missing some links for the negative 
contracted forms. In our linking the English ?n?t? was 
always linked to the Romanian negation and the English 
auxiliary/modal plus the main verb were linked to the 
Romanian translation equivalent found for the main 
verb. Some multi-word expressions recognized by the 
tokenizer as one token, such as dates (25 Ianuarie, 
2001), compound prepositions (de la, pina la), 
conjunctions (pentru ca, de cind, pina cind) or adverbs 
(de jur imprejur, in fata) as well as the hyphen 
separated nominal compounds (mass-media, prim-
ministru) were split, their positions were re-indexed and 
the initial one link of a split compound was replaced 
with the set obtained by adding one link for each 
constituent of the compound to the target English word. 
If the English word was also a compound the number of 
links generated for one aligned multiword expression 
was equal to the N*M, where N represented the number 
of words in the source compound and M the number of 
words in the target compound.   
5 Evaluation 
The results of the evaluation of TREQ-AL performance 
are shown in the Table 1. In our submission file the 
sentence no. 221 was left out by (our) mistake. We used 
the official evaluation program to re-evaluate our 
submission with the omitted sentence included and the 
precision improved with 0,09%, recall with 0,45%, F-
measure and AER with 0,33%.). The figures in the first 
and second columns of the Table 1 are those considered 
by the official evaluation. The last column contains the 
evaluation of the result that was our main target. Since 
TREQ-AL produces only ?sure? links, AER (alignment 
error rate - see the Shared Task web-page for further 
details) reduces to 1 - F-measure. 
TREQ-AL uses no external bilingual-resources. A 
machine-readable bilingual dictionary would certainly 
improve the overall performance.  The present version 
of the system (which is far from being finalized) seems 
to work pretty well on the non-null assignments and this 
is not surprising, because these links are supposed to be 
relevant for a translation dictionary extraction system 
and this was the very reason we developed TREQ.  
Moreover if we consider only the content words (main 
categories: noun, verbs, adjectives and general adverbs), 
which are the most relevant with respect to our 
immediate goals (multilingual wordnets interlinking and 
word sense disambiguation), we think TREQ-AL 
performs reasonably well and is worth further 
improving it. 
 
 Non-null 
links only 
Null links 
included 
Dictionary 
entries 
Precision 81,38% 60,43% 84,42% 
Recall 60,71% 62,80% 77,72% 
F-measure 69,54% 61,59% 80,93% 
AER 30,46% 38,41% 
 Table 1. Evaluation results 
6 Conclusions and further work 
TREQ-AL was developed in a short period of time and 
is not completely tested and debugged. At the time of 
writing we already noticed two errors that were 
responsible for several wrong or missed links. There are 
also some conceptual limitations which, when removed, 
are likely to further improve the performance. For 
instance all the words in virgin alignment zones are 
automatically given null links but the algorithm could 
be modified to assign all the links in the Cartesian 
product of the words in the corresponding virgin zones. 
The typical example for such a case is represented by 
the idiomatic expressions (tanda pe manda = the list 
that sum up). A bilingual dictionary of idioms as an 
external resource certainly would significantly improve 
the results. Also, with an additional preprocessing 
phase, for collocation recognition, many missing links 
could be recovered. At present only those collocations 
that represent 1-2 or 2-1 alignments are recovered. 
A major improvement will be to make the 
algorithm symmetric. There are many cases when 
reversing the source and target languages new links can 
be established. This can be explained by different 
polysemy degrees of the translation equivalent words 
and the way we associate alignment zones. 
The word order in Romanian and English to some 
extent is similar, but in the present version of TREQ-AL 
this is not explicitly used. One obvious and easy 
improvement of TREQ-AL performance would be to 
take advantage of the similarity in word order and map 
the virgin zones and afterwards, the words in the virgin 
zones. 
Finally, we noticed in the gold standard some 
wrong alignments. One example is the following:  
?? a XI ? a ?? = ?? eleventh?? 
Our program aligned all the 4 tokens in Romanian (a, 
XI, ?, a) to the English token (eleventh), while the gold 
standard assigned only ?XI? to ?eleventh? and the other 
three Romanian tokens were given a null link.  We also 
noticed some very hard to achieve alignments 
(anaphoric links).   
 
7 References 
Tufi?, D. Barbu, A.M.: ?Revealing translators 
knowledge: statistical methods in constructing 
practical translation lexicons for language and speech 
processing?, in International Journal of Speech 
Technology. Kluwer Academic Publishers, no.5, 
pp.199-209, 2002. 
Tufi?, D. ?A cheap and fast way to build useful 
translation lexicons? in Proceedings of the 19th 
International Conference on Computational 
Linguistics, COLING2002,  Taipei, 25-30 August, 
2002, pp. 1030-1036p. 
Ide, N., Erjavec, T., Tufis, D.: ?Sense Discrimination 
with Parallel Corpora? in Proceedings of the SIGLEX 
Workshop on Word Sense Disambiguation: Recent 
Successes and Future Directions. ACL2002, July 
Philadelphia 2002, pp. 56-60. 
Stamou, S., Oflazer K., Pala  K., Christoudoulakis D., 
 Cristea D., Tufis D., Koeva  S., Totkov G., Dutoit 
 D., Grigoriadou M.. ?BALKANET A Multilingual 
Semantic Network for the Balkan Languages?, in 
Proceedings of the International Wordnet Conference, 
Mysore, India, 21-25 January 2002. 
Tufi?, D. ?Tiered Tagging and Combined 
Classifiers? In F. Jelinek, E. N?th (eds) Text, 
Speech and Dialogue, Lecture Notes in Artificial 
Intelligence 1692, Springer, 1999, pp. 28-33. 
Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 107?110,
Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005
 
Combined word alignments  
 
Dan Tufi?, Radu Ion, Alexandru Ceau?u, Dan ?tef?nescu 
Romanian Academy Institute for Artificial Intelligence 
13, ?13 Septembrie?, 74311, Bucharest 5, Romania 
{tufis, radu, alceusu, danstef}@racai.ro 
 
 
Abstract
 
We briefly describe a word alignment system 
that combines two different methods in bitext 
correspondences identification. The first one is 
a hypotheses testing approach (Gale and 
Church, 1991; Melamed, 2001; Tufi? 2002) 
while the second one is closer to a model 
estimating approach (Brown et al, 1993; Och 
and Ney, 2000). We show that combining the 
two aligners the results are significantly 
improved as compared to each individual 
aligner. 
 
Introduction 
In (Tufi?, 2002) we described a translation equivalence 
extraction program called TREQ the development of 
which was twofold motivated: to help enriching the 
synsets of the Romanian wordnet (Tufi? et al 2004a) 
with new literals based on bilingual corpora evidence 
and to check the interlingual alignment of our wordnet 
against the Princeton Wordnet. The translation 
equivalence extractor has been also incorporated into a 
WSD system (Tufi? et al, 2004b) part of a semantic 
web annotation platform. It also constituted the 
backbone of our TREQ-AL word aligner which 
successfully participated in the previous HLT-NAACL 
2003 Shared Task1 on word alignment for Romanian-
English parallel texts. A detailed description of 
TREQ&TREQ-AL is given in (Tufi? et al 2003b) and it 
will be very shortly overviewed. 
A quite different approach from our hypotheses 
testing implemented in the TREQ-AL aligner is taken 
by the model-estimating aligners, most of them relying 
on the IBM models (1 to 5) described in the (Brown et 
al. 1993) seminal paper. The first wide-spread and 
publicly available implementation of the IBM models 
was the GIZA program, which itself was part of the 
SMT toolkit EGYPT (Al-Onaizan et al, 1999). GIZA 
has been superseded by its recent extension GIZA++ 
(Och and Ney, 2000, 2003) publicly available2. We used 
the translation probabilities generated by GIZA++ for 
implementing a second aligner, MEBA, described in a 
                                                 
1 http://www.cs.unt.edu/~rada/wpt/index.html#shared  
2 http://www.fjoch.com/GIZA++.2003-09-30.tar.gz 
little more details in a subsequent section. The 
alignments produced by MEBA were compared to the 
ones produced by TREQ-AL. We used for comparison 
the Gold Standard3 annotation from the HLT-NAACL 
2003 Shared Task. In order to combine the two aligners 
we had to check whether their accuracy was comparable 
and that when they are wrong the set of mistakes made 
by one aligner is not a proper set of the errors made by 
the second one. The first check was performed by using 
McNamer?s test  (Dieterich, 1998) and for the second 
we used Brill &Wu test (Brill, Wu, 1998). Both tests 
confirmed that the conditions for combining were 
ensured so, we built the combiner.  
The Combined Word Aligner, COWAL, is a 
wrapper of the two aligners (TREQ-AL and MEBA) 
ensuring the pre- and post-processing. It is 
complemented by a graphical user interface that allows 
for the visualisation of the alignments (intermediary and 
the final ones) as well as for their editing. We should 
note that the corrections made by the user are stored by 
COWAL as positive and negative examples for word 
dependencies (in the monolingual context) and 
translation equivalencies (in the bilingual context). In 
the current version the editorial logs are used by the 
human developers but we plan to further extend 
COWAL for automatic learning from this extremely 
valuable kind of data.    
 
The bitext processing  
The two base aligners and their combination use the 
same format for the input data and provide the 
alignments in the same format. The input format is 
obtained from two raw texts which represent reciprocal 
translations. If not already sentence aligned, the two 
texts are aligned. In the shared task this step was not 
necessary since both the training data and evaluation 
data were provided in the sentence aligned format.  
The texts in each language are then tokenized with 
the MULTEXT multilingual tokenizer4. The tokenizer is 
a finite state automaton using language specific 
                                                 
3 We noticed in the Gold Standard two sentences where 
alignments were wrongly shifted by one position (due to an 
unprintable character) and we corrected them.  
4 http://aune.lpl.univ-aix.fr:16080/projects/multext/MtSeg/  
107
 resources. It recognizes several compounds (phrasal 
verbs, idioms, dates) and split contrasted or cliticized 
constructions. This tokenization considerably differs 
from the one prescribed by the Shared Task where a 
token is any character string delimited by a blank or a 
punctuation sign (which itself is considered a token).   
Since our processing tools (especially the tokeniser) 
were built with a different segmentation strategy in 
mind, we generated the alignments based on our own 
tokenization and, at the end, we ?re-tokenised? the text 
according to original evaluation data (and consequently 
re-index) all the linking pairs. After tokenization, both 
texts are tagged and lemmatized.  We used in-house 
language models and lemmatizers and the Brants?s TnT 
tagger5. For both English and Romanian we used 
MULTEXT-EAST6 compliant tagsets. With different 
tags, a tagset mapping table becomes an obligatory 
external resource. Although, more often than not, the 
translation equivalents have the same part-of speech, 
relying on such a restriction would seriously affect the 
alignment recall. However, when the translation 
equivalents have different parts of speech, this 
difference is not arbitrary.  During the training phase we 
estimated bilingual POS affinities:{p(POSmRO| POSnEN)} 
and {p(POSnEN|POSmRO)}. POS affinities were used as 
one of the information sources in dealing with 
competitive alignments.  
The next preprocessing step is represented by a 
rather primitive form of sentence chunking in both 
languages. They roughly correspond to (non-recursive) 
noun phrases, adjectival phrases, prepositional phrases 
and verb complexes (analytical realization of tense, 
aspect mood and diathesis and phrasal verbs).  The 
?chunks? are recognized by a set of regular expressions 
defined over the tagsets. Finally, the bitext is assembled 
as an XML document (XCES-Align-ana format), as 
used in the MULTEXT-EAST corpus, which is the 
standard input for most of our tools, including COWAL 
alignment platform. 
 
The three aligners  
TREQ-AL generates translation equivalence hypotheses 
for the pairs of words (one for each language in the 
parallel corpus) which have been observed occurring in 
aligned sentences more than expected by chance. The 
hypotheses are filtered by a loglikelihood score 
threshold. Several heuristics (string similarity-cognates, 
POS affinities and alignments locality7) are used in a 
                                                 
5 http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf  
6 http://nl.ijs.si/ME/V2/  
7 The alignments locality heuristics exploits the observation 
made by several researchers that adjacent words of a text in 
the source language tend to align to adjacent words in the 
target language. A more strict alignment locality constraint 
competitive linking manner (Melamed, 2001) to make 
the final decision on the most likely translation 
equivalents. Given that, initially, this program was 
designed for extracting translation equivalents for the 
alignment of the Romanian wordnet to the Princeton 
wordnet, it deals only with one to one mappings. To 
cope with the many to many mappings (especially for 
functional words alignment), the earlier version of the 
translation equivalence extractor encoded some general 
rules assumed to be valid over a large set of natural 
languages such as: auxiliaries and verbal particles 
(infinitive, subjunctive, aspectual and temporal) are 
related to the closest main verb, determiners (articles, 
pronominal adjectives, quantifiers) are related to the 
closest nominal category (noun or pronoun). Currently 
this part of the TREQ-AL code became redundant 
because the chunking module mentioned before does 
the same job in a more general and flexible way.  
MEBA is an iterative algorithm which uses the 
translation probabilities, distorsions and POS-affinities 
generated by GIZA++ and takes advantage of all 
preprocessing phases mentioned in the previous section. 
In each step are aligned different categories of tokens 
(content words, named entities, functional words) in 
decreasing order of statistical evidence. The score of a 
link is computed by a linear function of 7 parameters? 
scores: translation probability, POS affinity, string 
similarity, alignments locality (both strict and weaker 
versions) distortions and the entropy of the translation 
equivalents. For all these parameters, in each processing 
step, we empirically set minimal thresholds and various 
weights. The tokens considered for the computing 
translation probabilities are the lemmas trailed by the 
grammatical categories (eg. plane_N, plane_V 
plane_A). This way we aimed at avoiding data 
sparseness and filtering noisy data. For highly 
inflectional languages (as Romanian is) the use of 
lemmas instead of word occurrences contributes 
significantly to the data sparseness reduction. For 
languages with weak inflectional character (as English 
is) the POS trailing contributes especially to the filtering 
the search space. Each processing step is controlled by 
above mentioned parameters, the weights and thresholds 
of which vary from step to step (even the order of the 
processing steps is one of the possible parameters). 
The first alignment step builds only links with a 
high level of certainty (that is cognates, pairs of high 
translation probability and high POS affinity). The 
grammatical categories which are considered in this step 
are user controlled (usually nouns, adjectives or non-
auxiliary verbs and which have the fewest competitive 
translations). The next processing steps try to align 
                                                                             
requires that all alignment links starting from a chunk, in the 
one language end in a chunk in the other language. This 
restricted form of locality is relevant for related languages.  
108
 content words (open class categories) as confidently as 
possible, following the alignments in previous steps as 
anchor points. In all steps the candidates are considered 
if and only if they meet the minimal threshold 
restrictions. If the input bitext is chunked, the strict 
alignment locality heuristics is very effective to 
determine the correct alignment even for unseen pairs of 
words (or for which the translation equivalence 
probability is below the considered threshold). When 
the pre-chunking of the parallel texts is not available, 
MEBA uses the weaker form of the locality heuristics 
by analyzing the alignments already existing in a 
window of N tokens centered on the focused token. The 
window size is variable, proportional to the sentence 
length. For all alignments in the window, an average 
displacement is computed and, among the competing 
alignments, preference will be given to the links with 
displacement values closer to the average one.  
The functional words and punctuation are processed 
in the last step and their alignments are guided by the 
POS-affinities and alignment locality heuristics. If none 
of the alignment clues or their combination (Tiedemann, 
2003) is strong enough, the functional words are 
automatically aligned with the word(s) their governor is 
aligned to. The governor is chunk-based defined: it is 
the content word of a chunk (if there are more content 
words in a chunk, then the governor is the grammatical 
head). If the chunking is not available, the closest 
content word is selected as the governor. Proximity is 
checked to the left or to the right according to the 
frequencies of the POS-ngram containing the current 
functional word.  
We should mention that the probabilities computed 
during the training phase are not re-estimated for each 
run-time processing step. At run-time only the weights 
and thresholds change from step to step.  
COWAL, the combined aligner takes advantage of the 
alignments independently provided by TREQ-AL and 
MEBA. The simplest combination method consists in 
computing either the union (high recall, low precision), 
or the intersection (lower recall, higher precision) of the 
independent alignments. We evaluated both these 
simple methods of combination and found that the best 
F-measure was provided by the union-based 
combination. Although for the shared task we submitted 
the union-based combined alignment (Baseline 
COWAL, see Table 1), there are various ways to 
improve it. We discuss three cases where improvement 
is possible (C1, C2 and C3, see below) and which were 
evaluated after the submission deadline. The results of 
this (unofficial) evaluation are summarized in Table 1 
by the f-COWAL line. These cases refer to competing 
links that appeared after the union of the independent 
alignments. The conflicts resolution is based on the 
(weak) locality and distortion heuristics discussed 
before. The currently identified competing links are 
only those for which the following conditions apply: 
C1) if one aligner found for a word W a non-null 
alignment and the other aligner generated for the 
same word W a null link, then the baseline alignment 
contains an impossible situation: the token W is 
recorded both as translated and not-translated in the 
other language. The translation probabilities, POS 
affinity and the relative displacement of the tokens in 
the non-null candidates were the strongest decision 
criteria. We found that in about 60% of the cases the 
null alignments were mistaken. So, for the time being, 
we simply eliminated the null competing alignments 
(this should be addressed in a more principled way by 
the future version of the combiner).  
C2) long distant competing links; this case appears 
when one aligner found for the word Ws the link to 
the target word Wtm, the other aligner found for Ws 
the target Wtn, and the distance between Wtm and 
Wtn, is more than 3 words (in a future version this 
maximum distance will be a dynamic parameter, 
depending on the sentence length and the POS of 
Ws). 
C3) competing links to the same target(s) of a word 
occurring several times in the same sentence; 
consider, for example, the Romanian fragment:  
     ??la1 Neptun, la2 Orastie si la3 Afumati, ?   
     which in English is translated by the next segment: 
     ??in Neptun, Orastie and Afumati? 
In spite of the gold standard considering that all three 
occurrences of the preposition ?la? in Romanian (la1, 
la2 ,la3) are aligned to the same word in English (?in?), 
the filtering, in this case, licensed only the alignment 
?la1 <-> in?. We consider that this filtered alignment 
is correct, since omitting ?la2? and ?la3? does not alter 
the syntactic correctness of the Romanian text, and 
also because the insertion in the English fragment of 
the preposition ?in? before ?Orastie? and before 
?Afumati? wouldn?t alter the grammaticality of the 
English fragment. Since both repetitions and 
omissions are optional, we consider that only the first 
occurrence of the preposition (?la1?) is translated in 
English, while the others are omitted. 
Another possible improvement (not implemented yet) 
was revealed by observing that the final result contained 
several incomplete n-m (phrasal) alignments. It is likely 
that even an elementary n-gram analysis (both sides of 
the bitext) would bring valuable evidence for improving 
the phrasal alignments.  
 
Post-processing  
As said in the second section, our tokenization was 
different from the tokenization in the training and test 
data. To comply with the evaluation protocol, we had to 
re-tokenize the aligned text and re-compute the indexes 
109
 of the links. Some multi-word expressions recognized 
by the tokenizer as one token, such as dates (25 
ianuarie, 2001), compound prepositions (de la, p?n? 
la), conjunctions (pentru ca, de c?nd, p?n? c?nd) or 
adverbs (de jur ?mprejur, ?n fa?a) as well as the hyphen 
separated nominal compounds (mass-media, prim-
ministru) were split, their positions were re-indexed and 
the initial one link of a split compound was replaced 
with the set obtained by adding one link for each 
constituent of the compound to the target English word. 
The same hold for the other way around. Therefore if 
two multiword expressions were initially found to be 
translation equivalents (one alignment link) after the 
post-processing number of  generated links became 
N*M, where N represented the number of words in the 
first language compound and M the number of words in 
the second language compound.   
Evaluation and conclusions 
Neither TREQ-AL nor MEBA needs an a priori 
bilingual dictionary, as this will be automatically 
extracted by the TREQ or GIZA++. We made 
evaluation of the individual alignments in both 
experimental settings: without a startup bilingual 
lexicon and with an initial mid-sized bilingual lexicon. 
Surprisingly enough, we found that while the 
performance of TREQ-AL increases a little bit (approx. 
1% increase of the F-measure) MEBA is doing better 
without an additional lexicon. So, in the evaluation 
below MEBA uses only the training data vocabulary.  
 
Aligner Precision Recall F-
meas. 
AER 
TREQ-AL 81.71 60.57 69.57 30.43 
MEBA 82.85 60.41 69.87 30.13 
Baseline 
(union)COWAL 
70.84 76.67 73.64 26.36 
f-COWAL 
(H1+H2+H3) 
87.17 70.25 77.80 22.20 
         Table 1. Evaluation results against the official GS 
After the release of the official Gold Standard we 
noticed and corrected some obvious errors and also 
removed the controversial links of the type c) discussed 
in the previous section. The evaluations against this new 
?Gold Standard? showed, on average, 3.5% better 
figures (precision, recall, F-measure and AER) for the 
individual aligners, while for the combined classifiers, 
the performance scores were about 4% better. 
MEBA is very sensitive to the values of the 
parameters which control its behavior. Currently they 
are set according to the developers? intuition and after 
the analysis of the results from several trials. Since this 
activity is pretty time consuming (human analysis plus 
re-training might take a couple of hours) we plan to 
extend MEBA with a supervised learning module, 
which would automatically determine the ?optimal? 
parameters (thresholds and weights) values. 
 
References 
Al-Onaizan, Y., Curin, J., Jahr, M., Knight K., Lafferty, J., 
Melamed, D., Och, F. J., Purdy, D., Smith, N.A., 
Yarowsky, D. (1999) : Statistical Machine 
Translation, Final Report, JHU Workshop, 42 pages 
Brill, E., and Wu, J. (1998). ?Classifier Combination for 
Improved Lexical Disambiguation? In Proceedings of 
COLING-ACL?98  Montreal, Canada, 191-195 
Brown, P. F., Della Pietra, S.A.,  Della Pietra, V. J., 
Mercer, R. L.(1993) ?The mathematics of statistical 
machine translation: Parameter estimation?. 
Computational Linguistics, 19(2) pp. 263?311. 
Dietterich, T. G., (1998). ?Approximate Statistical Tests 
for Comparing Supervised Classification Learning 
Algorithms?. Neural Computation, 10 (7) 1895-1924. 
Gale, W.A. and Church, K.W. (1991). ?Identifying word 
correspondences in parallel texts?. Proceedings of the 
Fourth DARPA Workshop on Speech and Natural 
Language. Asilomar, CA, pp. 152?157. 
Melamed, D. (2001). Empirical Methods for Exploiting 
Parallel Texts. Cambridge, MA: MIT Press. 
Och, F.J., Ney, H. (2003) "A Systematic Comparison of 
Various Statistical Alignment Models", Computa-
tional Linguistics, 29(1), pp. 19-51 
Och, F.J., Ney, H.(2000) "Improved Statistical Alignment 
Models". Proceedings of the 38th ACL, Hongkong,  
pp. 440-447 
Tiedemann, J. (2003) ?Combining clues for word 
alignment?. In Proceedings of the 10th EACL, 
Budapest, pp. 339?346 
Tufi?, D.(2002) ?A cheap and fast way to build useful 
translation lexicons?. Proceedings of COLING2002, 
Taipei, pp. 1030-1036. 
Tufi?, D., Barbu, A.M., Ion R (2003).: ?TREQ-AL: A 
word-alignment system with limited language 
resources?, Proceedings of the NAACL 2003 
Workshop on Building and Using Parallel Texts; 
Romanian-English Shared Task, Edmonton, pp. 36-39 
Tufi?, D., Ion, R., Ide, N.(2004a): Fine-Grained Word 
Sense Disambiguation Based on Parallel Corpora, 
Word Alignment, Word Clustering and Aligned 
Wordnets. Proceedings of COLING2004, Geneva, pp. 
1312-1318 
Tufis, D., Barbu, E., Mititelu, V., Ion, R., Bozianu, 
L.(2004b): ?The Romanian Wordnet?.  In Romanian 
Journal on Information Science and Technology, Dan 
Tufi? (ed.) Special Issue on BalkaNet, Romanian 
Academy, 7(2-3), pp. 105-122.  
110
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 282?287,
Prague, June 2007. c?2007 Association for Computational Linguistics
RACAI: Meaning Affinity Models
Radu Ion
Institute for Artificial Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania
radu@racai.ro
Dan Tufis?
Institute for Artificial Intelligence
13, ?13 Septembrie?,
050711, Bucharest 5,
Romania
tufis@racai.ro
Abstract
This article introduces an unsupervised word
sense disambiguation algorithm that is in-
spired by the lexical attraction models of
Yuret (1998). It is based on the assump-
tion that the meanings of the words that
form a sentence can be best assigned by con-
structing an interpretation of the whole sen-
tence. This interpretation is facilitated by
a dependency-like context specification of
a content word within the sentence. Thus,
finding the context words of a target word
is a matter of finding a pseudo-syntactic de-
pendency analysis of the sentence, called a
linkage.
1 Introduction
Word Sense Disambiguation (WSD) is a difficult
Natural Language Processing task which requires
that for every content word (noun, adjective, verb
or adverb) the appropriate meaning is automatically
selected from the available sense inventory1. Tradi-
tionally, the WSD algorithms are divided into two
rough classes: supervised and unsupervised. The
supervised paradigm relies on sense annotated cor-
pora, with the assumption that neighbouring disam-
biguate words provide a strongly discriminating and
generalizable context representation for the meaning
of a target word. Obviously, this approach suffers
from the knowledge acquisition bottleneck in that
1In principle, one can select meanings for any part of speech
that is represented into the semantic lexicon (prepositions for
instance) but the content words disambiguation is the de facto
standard.
there will never be enough training data to ensure
a scalable result of such algorithms. The unsuper-
vised alternative to WSD tries to alleviate the burden
of manually sense tagging the corpora, by employ-
ing algorithms that use different knowledge sources
to determine the correct meaning in context. In fact,
the ?knowledge source usage? is another way to dis-
tinguish among the WSD methods. Such methods
call upon further processing of the text to be dis-
ambiguated such as parsing and/or use handcrafted,
semantically rich sense inventories such as Word-
Net (Fellbaum, 1998). WSD methods in this cate-
gory range from the very simple ranking based on
counting the number of words occurring in both the
target word?s context and its sense definitions in a
reference dictionary (Lesk, 1986) to the more elabo-
rated approaches using the semantic lexicon?s tax-
onomies, (shallow) parsing, collocation discovery
etc. (Stevenson and Wilks, 2001).
One of the central issues of any WSD implemen-
tation is given by the context representation. The
standard principle that is applied when trying to dis-
ambiguate the meaning of a word is that the same
word in similar contexts should have the same mean-
ing. By and large, the context of a target word is ma-
terialized by a collection of features among which
are: the collocates of the target word, the part-of-
speech (POS) of the target word, ?k words sur-
rounding the target word and/or their POSes and so
on. More often than not, the contexts similarity is es-
timated by the distance in the feature vector space.
Lin (1997) defines the local context of a target word
by the collection of syntactic dependencies in which
the word takes part. According to this notion of con-
282
text, Lin assumes that two different words are likely
to have similar meanings if they occur in identical
local contexts.
What we will attempt here is to combine the two
views of context similarity/identity versus meaning
similarity/identity by using a dependency-like repre-
sentation of the context as a lexical attraction model.
More specifically, we will not consider any feature
of the context and will try to maximize a meaning at-
traction function over all linked words of a sentence.
In section 2 we will describe SynWSD, an unsuper-
vised, knowledge-based WSD algorithm and in sec-
tions 3 and 4 we will present the application of Syn-
WSD to two of SEMEVAL-2007 ?all words? tasks:
English Coarse-Grained and English Fine-Grained.
Finally, with section 5 we will conclude the article.
2 SynWSD
The syntactic context representation is not new in
the realm of WSD algorithms. For instance, Lin
(1997) used the dependency relations of the target
word to specify its context and Stetina (1998) ex-
tracted head-modifier relations to obtain the context
pairs for each word of interest from a constituents
tree. The syntactic representation of the context of a
target word has one main advantage over the collec-
tion of features method: the target word is related
only with the relevant word(s) in its window and
not with all the words and thus, many noisy cooc-
currences are eliminated. Mel?c?uk (1988) further
strengthens the intuition of a syntactic context rep-
resentation with his Meaning Text Model in which
there is a deterministic translation from the surface
syntactic dependency realization of the sentence to
its deep syntactic one and therefore to the semantic
representation.
To use a syntactic analysis as a context representa-
tion, one needs a parser which will supply the WSD
algorithm with the required analysis. Because we
have intended to develop a language independent
WSD algorithm and because there is no available,
reliable dependency parser for Romanian, we have
backed off to a simpler, easier to obtain dependency-
like representation of a sentence: a slightly modified
version of the lexical attraction models of (Yuret,
1998).
2.1 LexPar
Lexical attraction is viewed as the likelihood of a
syntactic dependency relation between two words of
a sentence and is measured by the pointwise mutual
information between them. Yuret (1998) shows that
the search for the lowest entropy lexical attraction
model leads to the unsupervised discovery of undi-
rected dependency relations or links.
LexPar (Ion and Barbu Mititelu, 2006) is a link
analyzer (a linker) which generates a connected,
undirected, acyclic and planar graph of an input sen-
tence in which the nodes are the words of the sen-
tence and the edges are the highest lexical attracted
dependency-like relations. This program is simi-
lar to the suboptimal one presented in (Yuret, 1998)
with the following main differences:
? the policy of checking pairs of words to be re-
lated is based on the assumption that most of
the syntactic relations2 are formed between ad-
jacent words and then between adjacent groups
of linked words;
? it operates on POS-tagged and lemmatized cor-
pora and attempts to improve parameter estima-
tion by using both lemmas and POS tags. The
score of a link is defined as the weighted sum
of the pointwise mutual information of the lem-
mas and of the POS tags, thus coping even with
the unknown lemmas;
? it uses a rule filter that will deny the formation
of certain links based on the POSes of the can-
didate words. For instance, neither the relation
between a determiner and an adverb nor the re-
lation between a singular determiner and a plu-
ral noun should be permitted;
In Figure 1 we have an example of a XML en-
coded, LexPar processed sentence. The head at-
tribute of the w tag specifies the position of the head
word of the tagged word. Because LexPar considers
non-directed dependency relations, for the purposes
of XML encoding3, the first word of every sentence
2At least for our languages of interest, namely English and
Romanian.
3The encoding of the morpho-syntactic descriptors (MSD) is
MULTEXT-East compliant (http://nl.ijs.si/ME/V3/
msd/00README.txt).
283
Figure 1: The XML representation of a LexPar pro-
cessed sentence.
(position 0) is always the root of the syntactic de-
pendency tree, its dependents are its children nodes,
and so on while we recursively build the tree from
the LexPar result.
We have chosen not to give a detailed presentation
of LexPar here (the reader is directed towards (Yuret,
1998; Ion and Barbu Mititelu, 2006)) and instead,
to briefly explain how the linkage in Figure 1 was
obtained. The processor begins by inspecting a list
G of groups of linked words which initially contains
the positions of each of the words in the sentence:
G0 = {(0), (1), (2), (3), (4), (5)}
The linking policy is trying to link words in the
groups (0) and (1) or (1) and (2). The syntactic
rule filter says that auxiliary verbs (Va) can only
be linked with main verbs (Vm) and so one link is
formed and the list of groups becomes:
G1 = {(0), (?1, 2?), (3), (4), (5)}
Next, the processor must decide linking the groups
(?1, 2?) and (3) or (3) and (4) but the syntactic rule
filter is denying any link from positions 1 or 2 to 3
(no links from any kind of verb V to any kind of a
determiner D) or from 3 to 4 (no link from a nega-
tive determiner Dz3 to a qualificative adjective Af).
Continuing this way, the progress of G list is as fol-
lows:
G1 = {(0), (?1, 2?), (3), (?4, 5?)}
G2 = {(?0, 2?, ?1, 2?), (?3, 5?, ?4, 5?)}
G3 = {(?0, 2?, ?1, 2?, ?2, 5?, ?3, 5?, ?4, 5?)}
So in 3 steps G3 contains a single group of linked
words namely the linkage of the sentence.
2.2 Meaning Affinity Models
If the lexical attraction models are geared towards
the discovery of the most probable syntactic rela-
tions of a sentence, we can naturally generalize this
idea to construct a class of models that will find a
combination of meanings that maximizes a certain
meaning attraction function over a linkage of a sen-
tence. We call this class of models the meaning
affinity models.
Optimizing meaning affinity over a syntactic rep-
resentation of a sentence has been tried in (Stetina et
al., 1998; Horbovanu, 2002). SynWSD (Ion, 2007)
is an implementation with two phases of the mean-
ing affinity concept: training which takes as input
a corpus with LexPar linked sentences (of the type
shown in Figure 1) and outputs a table M of mean-
ing co-occurrence frequencies and disambiguation
of a LexPar linked sentence S, based on the counts
in table M from the previous phase.
Before continuing with the descriptions of these
phases, we will introduce the notations that we will
use throughout this section:
? A n-word sentence is represented by a vec-
tor S of n elements, each of them contain-
ing a triple ?wordform, lemma,POS?. For in-
stance, the first element from S in Figure 1 is
S[0] = ?We,we, Pp1?pn?;
? L is the LexPar linkage of S, and is also a vec-
tor containing pairs of positions ?i, j? in S that
are related, where 0 ? i < j < n;
? lem(S, i) and pos(S, i) are two functions that
give the lemma and the POS of the position i in
S, 0 ? i < n.
The training phase is responsible for collecting
meaning co-occurrence counts. It simply iterates
over each sentence S of the training corpus and for
every link L[k] of the form ?a, b? from its linkage,
does the following (K stores the total number of
recorded meaning pairs):
1. extracts the sets of meanings Ia and Ib corre-
sponding to the lemma lem(S, a) with the POS
pos(S, a) and to the lemma lem(S, b) with the
POS pos(S, b) from the sense inventory4;
4If the lemma does not appear in the sense inventory or its
284
2. increases by 1 the M table frequencies for ev-
ery pair of the cartesian product Ia ? Ib. For
every meaning m ? Ia, the frequency of the
special pair ?m, ?? is increased with |Ib|. Simi-
larly, the pair ??,m? frequency is also increased
with |Ia| for m ? Ib);
3. K ? K + |Ia ? Ib|.
We have used the Princeton WordNet (Fellbaum,
1998), version 2.0 (PWN20) as our sense inventory
and the mappings from its synsets to the SUMO
ontology concepts (Niles and Pease, 2003) and to
the IRST domains (Magnini and Cavaglia, 2000).
Thus we have tree different sense inventories each
with a different granularity. For instance, the noun
homeless has 2 senses in PWN20, its first sense
(?someone with no housing?) being mapped onto the
more general Human SUMO concept and onto the
person IRST domain. The second sense of the
same noun is ?people who are homeless? which cor-
responds to the same SUMO concept and to a differ-
ent IRST domain (factotum).
In order to reduce the number of recorded pairs
in the case of PWN20 meanings (the finest granular-
ity available) and to obtain reliable counts, we have
modified the step 1 of the training phase in the fol-
lowing manner:
? if we are dealing with nouns or verbs, for every
meaning mi of the lemma, extract the upper-
most hypernym meaning which does not sub-
sume any other meaning of the same lemma;
? if we are dealing with adjectives, for every
meaning mi of the lemma, extract the meaning
of the head adjective if mi is part of a cluster;
? if we are dealing with adverbs, for every mean-
ing mi of the lemma, return mi (no generaliza-
tion is made available by the sense inventory in
this case).
This generalization procedure will be reversed at the
time of disambiguation as will be explained shortly.
POS does not give a noun, verb, adjective or adverb, the lemma
itself is returned as the sole meaning because in the disambigua-
tion phase we need a meaning for every word of the sentence,
be it content word or otherwise.
Figure 2: The tree representation of the sentence in
Figure 1.
The disambiguation phase takes care of finding
the best interpretation of a linked sentence based on
the frequency table M . For a test sentence S, with
the linkage L, the procedure goes as follows:
1. produce a proper tree T of positions from L by
taking position 0 as the root of the tree. Then,
for every link that contains 0 make the other po-
sition in the link a child of 0 and then, in a re-
cursive manner, apply the same process for all
children of 0. For instance, the tree for Figure
1 if depicted in Figure 2;
2. construct a vector P of sentence positions vis-
ited during a depth-first traversal of the T tree.
The vector of sentence positions for Figure 2 is
P = (0, 2, 5, 3, 5, 4, 5, 2, 1, 2, 0)
3. construct a meaning vector V of the same
length as P . V [i] contains the list of mean-
ings of the lemma lem(S, P [i]) with the POS
pos(S, P [i]). If the sense inventory is PWN20,
every meaning from the list is generalized as
described above;
4. finally, apply the Viterbi algorithm ((Viterbi,
1967)) on the V vector and extract the path (se-
quence of meanings) which maximizes mean-
ing affinity.
Each state transition is scored according to a
meaning affinity function. In our experiments we
have considered three meaning affinity functions. If
K is the total number of meaning pairs and if m1
285
and m2 are two meanings from adjacent V positions
for which f(m1,m2) is the pair frequency extracted
from M , the functions are:
1. DICE:
dice(m1,m2) =
2f(m1,m2)+2f(m2,m1)
f(m1,?)+f(?,m1)+f(m2,?)+f(?,m2)
2. Pointwise mutual information:
mi(m1,m2) =
log Kf(m1,m2)+Kf(m2,m1)(f(m1,?)+f(?,m1))(f(m2,?)+f(?,m2))
3. Log-Likelihood, ll(m1,m2) which is com-
puted as in (Moore, 2004).
After the Viterbi path (best path) has been calcu-
lated, every state (meaning) from V [i] (0 ? i < |V |)
along this path is added to a finalD vector. When the
PWN20 sense inventory is used, the reverse of the
generalization procedure is applied to each meaning
recorded in D, thus coming back to the meanings
of the words of S. Please note that an entry in D
may contain more than one meaning especially in
the case of PWN20 meanings for which there was
not enough training data.
3 SEMEVAL-2007 Task #7:
Coarse-grained English All-Words
LexPar and SynWSD were trained on an 1 million
words corpus comprising the George Orwell?s 1984
novel and the SemCor corpus (Miller et al, 1993).
Both texts have been POS-tagged (with MULTEXT-
East compliant POS tags) and lemmatized and the
result was carefully checked by human judges to en-
sure a correct annotation.
SynWSD was run with all the meaning attraction
functions (dice, mi and ll) for all the sense in-
ventories (PWN20, SUMO categories and IRST do-
mains) and a combined result was submitted to the
task organizers. The combined result was prepared
in the following way:
1. for each sense inventory and for each token
identifier, get the union of the meanings for
each run (dice, mi and ll);
2. for each token identifier with its three union
sets of PWN20 meanings, SUMO categories
and IRST domains:
(a) for each PWN20 meaningmi in the union,
if there is a SUMO category that maps
onto it, increase mi?s weight by 1;
(b) for each PWN20 meaningmi in the union,
if there is a IRST domain that maps onto
it, increase mi?s weight by 1;
(c) from the set of weighted PWN20 mean-
ings, select the subset C that best over-
laps with a cluster. That is, the intersec-
tion between the subset and the cluster has
a maximal number of meanings for which
the sum of weights is also the greatest;
(d) output the lowest numbered meaning inC.
With this combination, the official F-measure of
SynWSD is 0.65712 which places it into the 11th
position out of 16 competing systems5.
Another possible combination is that of the inter-
section which is obtained with the exact same steps
as above, replacing the union operation with the in-
tersection. When the PWN20 meanings set is void,
we can make use of the most frequent sense (MFS)
backoff strategy thus selecting the MFS of the cur-
rent test word from PWN20. Working with the of-
ficial key file and scoring software, the intersection
combination with MFS backoff gives an F-measure
of 0.78713 corresponding to the 6th best result. The
same combination method but without MFS backoff
achieves a precision of 0.80559 but at the cost of a
very low F-measure (0.41492).
4 SEMEVAL-2007 Task #17: English
All-Words
For this task, LexPar and SynWSD were further
trained on a 12 million POS tagged and lemmatized
balanced corpus6. The run that was submitted was
the intersection combination with the MFS backoff
strategy which obtained an F-measure of 0.527. This
score puts our algorithm on the 8th position out of
14 competing systems. For the union combinator
5Precision = Recall = F-measure. In what follows, mention-
ing only the F-measure means that this equality holds.
6A random subset of the BNC (http://www.natcorp.
ox.ac.uk/).
286
(the MFS backoff strategy is not applicable), the F-
measure decreases to 0.445 (10th place). Finally, if
we train SynWSD only on corpora from task#7, the
union combinator leads to an F-measure of 0.344.
5 Conclusions
SynWSD is a knowledge-based, unsupervised WSD
algorithm that uses a dependency-like analysis of a
sentence as a uniform context representation. It is a
language independent algorithm that doesn?t require
any feature selection.
Our system can be improved in several ways.
First, one can modify the generalization procedure
in the case of PWN20 meanings in the sense of se-
lecting a fixed set of top level hypernyms. The size
of this set will directly affect the quality of meaning
co-occurrence frequencies. Second, one may study
the effect of a proper dependency parsing on the re-
sults of the disambiguation process including here
making use of the syntactic relations names and ori-
entation.
Even if SynWSD rankings are not the best avail-
able, we believe that the unsupervised approach to
the WSD problem combined with different knowl-
edge sources represents the future of these systems
even if, at least during the last semantic evalua-
tion exercise SENSEVAL-3, the supervised systems
achieved top rankings.
References
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. MIT Press, May.
Vladimir Horbovanu. 2002. Word Sense Disambigua-
tion using WordNet. ?Alexandru Ioan Cuza? Univer-
sity, Faculty of Computer Science, Ias?i, Romania. In
Romanian.
Radu Ion and Verginica Barbu Mititelu. 2006. Con-
strained lexical attraction models. In Proceedings of
the Nineteenth International Florida Artificial Intelli-
gence Research Society Conference, pages 297?302,
Menlo Park, Calif., USA. AAAI Press.
Radu Ion. 2007. Word Sense Disambiguation meth-
ods applied to English and Romanian. Ph.D. thesis,
Research Institute for Artificial Intelligence (RACAI),
Romanian Academy, January. In Romanian, to be de-
fended.
Michael Lesk. 1986. Automatic sense disambiguation :
How to tell a pine cone from an ice cream cone. In
Proceedings of the 1986 SIGDOC Conference, Asso-
ciation for Computing Machinery, pages 24?26, New
York.
Dekang Lin. 1997. Using syntactic dependency as lo-
cal context to resolve word sense ambiguity. In Pro-
ceedings of the 35th Annual Meeting of the Association
for Computational Linguistics, pages 64?71, Madrid,
Spain, July.
Bernardo Magnini and Gabriela Cavaglia. 2000. Inte-
grating Subject Field Codes into WordNet. In Gavrili-
dou M., Crayannis G., Markantonatu S., Piperidis S.,
and Stainhaouer G., editors, Proceedings of LREC-
2000, Second International Conference on Language
Resources and Evaluation, pages 1413?1418, Athens,
Greece, June.
Igor Mel?c?uk. 1988. Dependency Syntax: theory and
practice. State University of New York Press, Albany,
NY.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T. Bunker. 1993. A semantic concordance.
In Proceedings of the 3rd DARPA Workshop on Hu-
man Language Technology, pages 303?308, Plains-
boro, New Jersey.
Robert C. Moore. 2004. On Log-Likelihood Ratios and
the Significance of Rare Events. In Proceedings of
the 2004 Conference on Empirical Methods in Natu-
ral Language Processing, pages 333?340, Barcelona,
Spain.
Ian Niles and Adam Pease. 2003. Linking Lexicons
and Ontologies: Mapping WordNet to the Suggested
Upper Merged Ontology. In Proceedings of the 2003
International Conference on Information and Knowl-
edge Engineering (IKE 03), Las Vegas, Nevada, June.
Jiri Stetina, Sadao Kurohashi, and Makoto Nagao. 1998.
General word sense disambiguation method based on
a full sentential context. In Proceedings of the Coling-
ACL?98 Workshop ?Usage of WordNet in Natural Lan-
guage Processing Systems?, pages 1?8, Montreal.
Mark Stevenson and YorickWilks. 2001. The interaction
of knowledge sources in word sense disambiguation.
Computational Linguistics, 27(3):321?349.
Andrew J. Viterbi. 1967. Error bounds for convolu-
tional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information Theory,
IT(13):260?269, April.
Deniz Yuret. 1998. Discovery of linguistic relations
using lexical attraction. Ph.D. thesis, Department of
Computer Science and Electrical Engineering, MIT,
May.
287
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 91?96,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
ACCURAT Toolkit for Multi-Level Alignment and  
Information Extraction from Comparable Corpora 
 
M?rcis Pinnis1, Radu Ion2, Dan ?tef?nescu2, Fangzhong Su3, 
Inguna Skadi?a1, Andrejs Vasi?jevs1, Bogdan Babych3 
 1Tilde, Vien?bas gatve 75a, Riga, Latvia 
{marcis.pinnis,inguna.skadina,andrejs}@tilde.lv 
 
2Research Institute for Artificial Intelligence, Romanian Academy 
{radu,danstef}@racai.ro 
 
3Centre for Translation Studies, University of Leeds 
{f.su,b.babych}@leeds.ac.uk 
 
 
Abstract 
The lack of parallel corpora and linguistic 
resources for many languages and domains is 
one of the major obstacles for the further 
advancement of automated translation. A 
possible solution is to exploit comparable 
corpora (non-parallel bi- or multi-lingual text 
resources) which are much more widely 
available than parallel translation data. Our 
presented toolkit deals with parallel content 
extraction from comparable corpora. It consists 
of tools bundled in two workflows: (1) 
alignment of comparable documents and 
extraction of parallel sentences and (2) 
extraction and bilingual mapping of terms and 
named entities. The toolkit pairs similar 
bilingual comparable documents and extracts 
parallel sentences and bilingual terminological 
and named entity dictionaries from comparable 
corpora. This demonstration focuses on the 
English, Latvian, Lithuanian, and Romanian 
languages. 
Introduction 
In recent decades, data-driven approaches have 
significantly advanced the development of 
machine translation (MT). However, lack of 
sufficient bilingual linguistic resources for many 
languages and domains is still one of the major 
obstacles for further advancement of automated 
translation. At the same time, comparable corpora, 
i.e., non-parallel bi- or multilingual text resources 
such as daily news articles and large knowledge 
bases like Wikipedia, are much more widely 
available than parallel translation data.  
While methods for the use of parallel corpora in 
machine translation are well studied (Koehn, 
2010), similar techniques for comparable corpora 
have not been thoroughly worked out. Only the 
latest research has shown that language pairs and 
domains with little parallel data can benefit from 
the exploitation of comparable corpora (Munteanu 
and Marcu, 2005; Lu et al, 2010; Smith et al, 
2010; Abdul-Rauf and Schwenk, 2009 and 2011). 
In this paper we present the ACCURAT 
toolkit1 - a collection of tools that are capable of  
analysing comparable corpora and extracting 
parallel data which can be used to improve the 
performance of statistical and rule/example-based 
MT systems. 
Although the toolkit may be used for parallel 
data acquisition for open (broad) domain systems, 
it will be most beneficial for under-resourced 
languages or specific domains which are not 
covered by available parallel resources. 
The ACCURAT toolkit produces: 
? comparable document pairs with 
comparability scores, allowing to estimate 
the overall comparability of corpora; 
? parallel sentences which can be used as 
additional parallel data sources for 
statistical translation model learning; 
                                                          
1 http://www.accurat-project.eu/ 
91
? terminology dictionaries ? this type of 
data is expected to improve domain-
dependent translation; 
? named entity dictionaries. 
The demonstration showcases two general use 
case scenarios defined in the toolkit: ?parallel data 
mining from comparable corpora? and ?named 
entity/terminology extraction and mapping from 
comparable corpora?. 
The next section provides a general overview of 
workflows followed by descriptions of methods 
and tools integrated in the workflows. 
1 Overview of the Workflows 
The toolkit?s tools are integrated within two 
workflows (visualised in Figure 1). 
 
  
Figure 1. Workflows of the ACCURAT toolkit. 
 
The workflow for parallel data mining from 
comparable corpora aligns comparable corpora in 
the document level (section 2.1). This step is 
crucial as the further steps are computationally 
intensive. To minimise search space, documents 
are aligned with possible candidates that are likely 
to contain parallel data. Then parallel sentence 
pairs are extracted from the aligned comparable 
corpora (section 2.2). 
The workflow for named entity (NE) and 
terminology extraction and mapping from 
comparable corpora extracts data in a dictionary-
like format. Providing a list of document pairs, the 
workflow tags NEs or terms in all documents using 
language specific taggers (named entity 
recognisers (NER) or term extractors) and 
performs multi-lingual NE (section 2.3) or term 
mapping (section 2.4), thereby producing bilingual 
NE or term dictionaries. The workflow also 
accepts pre-processed documents, thus skipping 
the tagging process. 
Since all tools use command line interfaces, task 
automation and workflow specification can be 
done with simple console/terminal scripts. All 
tools can be run on the Windows operating system 
(some are also platform independent). 
2 Tools and Methods 
This section provides an overview of the main 
tools and methods in the toolkit. A full list of tools 
is described in ACCURAT D2.6. (2011). 
2.1 Comparability Metrics 
We define comparability by how useful a pair of 
documents is for parallel data extraction. The 
higher the comparability score, the more likely two 
documents contain more overlapping parallel data. 
The methods are developed to perform lightweight 
comparability estimation that minimises search 
space of relatively large corpora (e.g., 10,000 
documents in each language). There are two 
comparability metric tools in the toolkit: a 
translation based and a dictionary based metric. 
The Translation based metric (Su and Babych, 
2012a) uses MT APIs for document translation 
into English. Then four independent similarity 
feature functions are applied to a document pair: 
? Lexical feature ? both documents are pre-
processed (tokenised, lemmatised, and 
stop-words are filtered) and then 
vectorised. The lexical overlap score is 
calculated as a cosine similarity function 
over the vectors of two documents. 
? Structural feature ? the difference of 
sentence counts and content word counts 
(equally interpolated). 
? Keyword feature ? the cosine similarity 
of top 20 keywords. 
? NE feature ? the cosine similarity of NEs 
(extracted using Stanford NER). 
These similarity measures are linearly combined in 
a final comparability score. This is implemented by 
a simple weighted average strategy, in which each 
92
type of feature is associated with a weight 
indicating its relative confidence or importance. 
The comparability scores are normalised on a scale 
of 0 to 1, where a higher comparability score 
indicates a higher comparability level. 
The reliability of the proposed metric has been 
evaluated on a gold standard of comparable 
corpora for 11 language pairs (Skadi?a et al, 
2010). The gold standard consists of news articles, 
legal documents, knowledge-base articles, user 
manuals, and medical documents. Document pairs 
in the gold standard were rated by human judges as 
being parallel, strongly comparable, or weakly 
comparable. The evaluation results suggest that the 
comparability scores reliably reflect comparability 
levels. In addition, there is a strong correlation 
between human defined comparability levels and 
the confidence scores derived from the 
comparability metric, as the Pearson R correlation 
scores vary between 0.966 and 0.999, depending 
on the language pair.  
The Dictionary based metric (Su and Babych, 
2012b) is a lightweight approach, which uses 
bilingual dictionaries to lexically map documents 
from one language to another. The dictionaries are 
automatically generated via word alignment using 
GIZA++ (Och and Ney, 2000) on parallel corpora. 
For each word in the source language, the top two 
translation candidates (based on the word 
alignment probability in GIZA++) are retrieved as 
possible translations into the target language. This 
metric provides a much faster lexical translation 
process, although word-for-word lexical mapping 
produces less reliable translations than MT based 
translations. Moreover, the lower quality of text 
translation in the dictionary based metric does not 
necessarily degrade its performance in predicting 
comparability levels of comparable document 
pairs. The evaluation on the gold standard shows a 
strong correlation (between 0.883 and 0.999) 
between human defined comparability levels and 
the confidence scores of the metric. 
2.2 Parallel Sentence Extractor from 
Comparable Corpora 
Phrase-based statistical translation models are 
among the most successful translation models that 
currently exist (Callison-Burch et al, 2010). 
Usually, phrases are extracted from parallel 
corpora by means of symmetrical word alignment 
and/or by phrase generation (Koehn et al, 2003). 
Our toolkit exploits comparable corpora in order to 
find and extract comparable sentences for SMT 
training using a tool named LEXACC (?tef?nescu 
et al, 2012). 
LEXACC requires aligned document pairs (also 
m to n alignments) for sentence extraction. It also 
allows extraction from comparable corpora as a 
whole; however, precision may decrease due to 
larger search space. 
LEXACC scores sentence pairs according to five 
lexical overlap and structural matching feature 
functions. These functions are combined using 
linear interpolation with weights trained for each 
language pair and direction using logistic 
regression. The feature functions are: 
? a lexical (translation) overlap score for 
content words (nouns, verbs, adjectives, 
and adverbs) using GIZA++ (Gao and 
Vogel, 2008) format dictionaries; 
? a lexical (translation) overlap score for 
functional words (all except content 
words) constrained by the content word 
alignment from the previous feature; 
? the alignment obliqueness score, a measure 
that quantifies the degree to which the 
relative positions of source and target 
aligned words differ; 
? a score indicating whether strong content 
word translations are found at the 
beginning and the end of each sentence in 
the given pair; 
? a punctuation score which indicates 
whether the sentences have identical 
sentence ending punctuation. 
For different language pairs, the relevance of 
the individual feature functions differ. For 
instance, the locality feature is more important for 
the English-Romanian pair than for the English-
Greek pair. Therefore, the weights are trained on 
parallel corpora (in our case - 10,000 pairs). 
LEXACC does not score every sentence pair in 
the Cartesian product between source and target 
document sentences. It reduces the search space 
using two filtering steps (?tef?nescu et al, 2012). 
The first step makes use of the Cross-Language 
Information Retrieval framework and uses a search 
engine to find sentences in the target corpus that 
are the most probable translations of a given 
sentence. In the second step (which is optional), 
93
the resulting candidates are further filtered, and 
those that do not meet minimum requirements are 
eliminated.  
To work for a certain language pair, LEXACC 
needs additional resources: (i) a GIZA++-like 
translation dictionary, (ii) lists of stop-words in 
both languages, and (iii) lists of word suffixes in 
both languages (used for stemming). 
The performance of LEXACC, regarding 
precision and recall, can be controlled by a 
threshold applied to the overall interpolated 
parallelism score. The tool has been evaluated on 
news article comparable corpora. Table 1 shows 
results achieved by LEXACC with different 
parallelism thresholds on automatically crawled 
English-Latvian corpora, consisting of 41,914 
unique English sentences and 10,058 unique 
Latvian sentences. 
 
Threshold Aligned pairs Precision 
Useful 
pairs 
0.25 1036 39.19% 406 
0.3 813 48.22% 392 
0.4 553 63.47% 351 
0.5 395 76.96% 304 
0.6 272 84.19% 229 
0.7 151 88.74% 134 
0.8 27 88.89% 24 
0.9 0 - 0 
 
Table 1. English-Latvian parallel sentence extraction 
results on a comparable news corpus. 
 
Threshold Aligned pairs Precision Useful pairs
0.2 2324 10.32% 240 
0.3 1105 28.50% 315 
0.4 722 53.46% 386 
0.5 532 89.28% 475 
0.6 389 100% 389 
0.7 532 100% 532 
0.8 386 100% 386 
0.9 20 100% 20 
 
Table 2. English-Romanian parallel sentence extraction 
results on a comparable news corpus. 
Table 2 shows results for English-Romanian on 
corpora consisting of 310,740 unique English and 
81,433 unique Romanian sentences. 
Useful pairs denote the total number of parallel 
and strongly comparable sentence pairs (at least 
80% of the source sentence is a translation in the 
target sentence). The corpora size is given only as 
an indicative figure, as the amount of extracted 
parallel data greatly depends on the comparability 
of the corpora. 
2.3 Named Entity Extraction and Mapping 
The second workflow of the toolkit allows NE and 
terminology extraction and mapping. Starting with 
named entity recognition, the toolkit features the 
first NER systems for Latvian and Lithuanian 
(Pinnis, 2012). It also contains NER systems for 
English (through an OpenNLP NER2 wrapper) and 
Romanian (NERA). In order to map named entities, 
documents have to be tagged with NER systems 
that support MUC-7 format NE SGML tags.  
The toolkit contains the mapping tool NERA2. 
The mapper requires comparable corpora aligned 
in the document level as input. NERA2 compares 
each NE from the source language to each NE 
from the target language using cognate based 
methods. It also uses a GIZA++ format statistical 
dictionary to map NEs containing common nouns 
that are frequent in location names. This approach 
allows frequent NE mapping if the cognate based 
method fails, therefore, allowing increasing the 
recall of the mapper. Precision and recall can be 
tuned with a confidence score threshold. 
2.4 Terminology Mapping 
During recent years, automatic bilingual term 
mapping in comparable corpora has received 
greater attention in light of the scarcity of parallel 
data for under-resourced languages. Several 
methods have been applied to this task, e.g., 
contextual analysis (Rapp, 1995; Fung and 
McKeown, 1997) and compositional analysis 
(Daille and Morin, 2008). Symbolic, statistical, and 
hybrid techniques have been implemented for 
bilingual lexicon extraction (Morin and 
Prochasson, 2011). 
Our terminology mapper is designed to map 
terms extracted from comparable or parallel 
                                                          
2 Open NLP - http://incubator.apache.org/opennlp/. 
94
documents. The method is language independent 
and can be applied if a translation equivalents table 
exists for a language pair. As input, the application 
requires term-tagged bilingual corpora aligned in 
the document level. 
The toolkit includes term-tagging tools for 
English, Latvian, Lithuanian, and Romanian, but 
can be easily extended for other languages if a 
POS-tagger, a phrase pattern list, a stop-word list, 
and an inverse document frequency list (calculated 
on balanced corpora) are available. 
The aligner maps terms based on two criteria 
(Pinnis et al, 2012; ?tef?nescu, 2012): (i) a 
GIZA++-like translation equivalents table and (ii) 
string similarity in terms of Levenshtein distance 
between term candidates.  For evaluation, Eurovoc 
(Steinberger et al, 2002) was used. Tables 4 and 5 
show the performance figures of the mapper for 
English-Romanian and English-Latvian. 
 
Threshold P R F-measure
0.3 0.562 0.194 0.288 
0.4 0.759 0.295 0.425 
0.5 0.904 0.357 0.511 
0.6 0.964 0.298 0.456 
0.7 0.986 0.216 0.359 
0.8 0.996 0.151 0.263 
0.9 0.995 0.084 0.154 
 
Table 3. Term mapping performance for English-
Romanian. 
 
Threshold P R F-measure 
0.3 0.636 0.210 0.316 
0.4 0.833 0.285 0.425 
0.5 0.947 0.306 0.463 
0.6 0.981 0.235 0.379 
0.7 0.996 0.160 0.275 
0.8 0.996 0.099 0.181 
0.9 0.997 0.057 0.107 
 
Table 4. Term mapping performance for English-
Latvian. 
3 Conclusions and Related Information 
This demonstration paper describes the 
ACCURAT toolkit containing tools for multi-level 
alignment and information extraction from 
comparable corpora. These tools are integrated in 
predefined workflows that are ready for immediate 
use. The workflows provide functionality for the 
extraction of parallel sentences, bilingual NE 
dictionaries, and bilingual term dictionaries from 
comparable corpora. 
The methods, including comparability metrics, 
parallel sentence extraction and named entity/term 
mapping, are language independent. However, they 
may require language dependent resources, for 
instance, POS-taggers, Giza++ translation 
dictionaries, NERs, term taggers, etc.3 
 The ACCURAT toolkit is released under the 
Apache 2.0 licence and is freely available for 
download after completing a registration form4.  
Acknowledgements 
The research within the project ACCURAT 
leading to these results has received funding from 
the European Union Seventh Framework 
Programme (FP7/2007-2013), grant agreement no 
248347. 
References  
Sadaf Abdul-Rauf and Holger Schwenk. On the use of 
comparable corpora to improve SMT performance. 
EACL 2009: Proceedings of the 12th conference of 
the European Chapter of the Association for 
Computational Linguistics, Athens, Greece, 16-23. 
Sadaf Abdul-Rauf and Holger Schwenk. 2011. Parallel 
sentence generation from comparable corpora for 
improved SMT. Machine Translation, 25(4): 341-
375. 
ACCURAT D2.6 2011. Toolkit for multi-level 
alignment and information extraction from 
comparable corpora (http://www.accurat-project.eu). 
Dan Gusfield. 1997. Algorithms on strings, trees and 
sequences. Cambridge University Press. 
Chris Callison-Burch, Philipp Koehn, Christof Monz, 
Kay Peterson, Mark Przybocki and Omar Zaidan. 
2010. Findings of the 2010 Joint Workshop on 
Statistical Machine Translation and Metrics for 
Machine Translation. Proceedings of the Joint Fifth 
Workshop on Statistical Machine Translation and 
MetricsMATR, 17-53. 
B?atrice Daille and Emmanuel Morin. 2008. Effective 
compositional model for lexical alignment. 
Proceedings of the 3rd International Joint Conference 
                                                          
3 Full requirements are defined in the documentation of each 
tool (ACCURAT D2.6, 2011). 
4 http://www.accurat-project.eu/index.php?p=toolkit 
95
on Natural Language Processing, Hyderabad, India, 
95-102. 
Franz Josef Och and Hermann Ney. 2000. Improved 
statistical alignment models. Proceedings of the 38th 
Annual Meeting of the Association for 
Computational Linguistics, 440-447. 
Pascale Fung and Kathleen Mckeown. 1997. Finding 
terminology translations from non-parallel corpora. 
Proceedings of the 5th Annual Workshop on Very 
Large Corpora, 192-202. 
Qin Gao and Stephan Vogel. 2008. Parallel 
implementations of a word alignment tool. 
Proceedings of ACL-08 HLT: Software Engineering, 
Testing, and Quality Assurance for Natural Language 
Processing, June 20, 2008. The Ohio State 
University, Columbus, Ohio, USA, 49-57. 
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 
2003. Statistical phrase-based translation. 
Proceedings of the Human Language Technology and 
North American Association for Computational 
Linguistics Conference (HLT/NAACL), May 27-
June 1, Edmonton, Canada. 
Philip Koehn. 2010. Statistical machine translation, 
Cambridge University Press. 
Bin Lu, Tao Jiang, Kapo Chow and Benjamin K. Tsou. 
2010. Building a large English-Chinese parallel 
corpus from comparable patents and its experimental 
application to SMT. Proceedings of the 3rd workshop 
on building and using comparable corpora: from 
parallel to non-parallel corpora, Valletta, Malta, 42-
48. 
Drago? ?tefan Munteanu and Daniel Marcu. 2006. 
Extracting parallel sub-sentential fragments from 
nonparallel corpora. ACL-44: Proceedings of the 21st 
International Conference on Computational 
Linguistics and the 44th annual meeting of the 
Association for Computational Linguistics, 
Morristown, NJ, USA, 81-88. 
Emmanuel Morin and Emmanuel Prochasson. 2011. 
Bilingual lexicon extraction from comparable 
corpora enhanced with parallel corpora. ACL HLT 
2011, 27-34. 
M?rcis Pinnis. 2012. Latvian and Lithuanian named 
entity recognition with TildeNER. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey. 
M?rcis Pinnis, Nikola Ljube?i?, Dan ?tef?nescu, Inguna 
Skadi?a, Marko Tadi?, Tatiana Gornostay. 2012. 
Term extraction, tagging, and mapping tools for 
under-resourced languages. Proceedings of the 10th 
Conference on Terminology and Knowledge 
Engineering (TKE 2012), June 20-21, Madrid, Spain. 
Reinhard Rapp. 1995. Identifying word translations in 
non-parallel texts. Proceedings of the 33rd annual 
meeting on Association for Computational 
Linguistics, 320-322.  
Jason R. Smith, Chris Quirk, and Kristina Toutanova. 
2010.  Extracting parallel sentences from comparable 
corpora using document level alignment. Proceedings 
of NAACL 2010, Los Angeles, USA. 
Dan ?tef?nescu. 2012. Mining for term translations in 
comparable corpora. Proceedings of the 5th 
Workshop on Building and Using Comparable 
Corpora (BUCC 2012) to be held at the 8th edition of 
Language Resources and Evaluation Conference 
(LREC 2012), Istanbul, Turkey, May 23-25, 2012. 
Ralf Steinberger, Bruno Pouliquen and Johan Hagman. 
2002. Cross-lingual document similarity calculation 
using the multilingual thesaurus Eurovoc. 
Proceedings of the 3rd International Conference on 
Computational Linguistics and Intelligent Text 
Processing (CICLing '02), Springer-Verlag London, 
UK, ISBN:3-540-43219-1. 
Inguna Skadi?a, Ahmet Aker, Voula Giouli, Dan Tufis, 
Rob Gaizauskas, Madara Mieri?a and Nikos 
Mastropavlos. 2010. Collection of comparable 
corpora for under-resourced languages. In 
Proceedings of the Fourth International Conference 
Baltic HLT 2010, IOS Press, Frontiers in Artificial 
Intelligence and Applications, Vol. 219, pp. 161-168. 
Fangzhong Su and Bogdan Babych. 2012a. 
Development and application of a cross-language 
document comparability metric. Proceedings of the 
8th international conference on Language Resources 
and Evaluation (LREC 2012), Istanbul, Turkey.  
Fangzhong Su and Bogdan Babych.  2012b. Measuring 
comparability of documents in non-parallel corpora 
for efficient extraction of (semi-) parallel translation 
equivalents. Proceedings of  EACL'12 joint 
workshop on Exploiting Synergies between 
Information Retrieval and Machine Translation 
(ESIRMT) and Hybrid Approaches to Machine 
Translation (HyTra), Avignon, France.  
Dan ?tef?nescu, Radu Ion and Sabine Hunsicker. 2012. 
Hybrid parallel sentence mining from comparable 
corpora. Proceedings of the 16th Conference of the 
European Association for Machine Translation 
(EAMT 2012), Trento, Italy.  
96
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 692?700,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Large tagset labeling using Feed Forward Neural Networks. Case 
study on Romanian Language 
 
 
Tiberiu Boro Radu Ion Dan Tufi 
Research Institute for 
$UWLILFLDO,QWHOOLJHQFH?0LKDL
DrJQHVFX? 
Romanian Academy 
Research Institute for 
$UWLILFLDO,QWHOOLJHQFH?0LKDL
DrJQHVFX? 
Romanian Academy 
Research Institute for 
$UWLILFLDO,QWHOOLJHQFH?0LKDL
DrJQHVFX? 
Romanian Academy 
tibi@racai.ro radu@racai.ro tufis@racai.ro 
 
 
 
Abstract 
Standard methods for part-of-speech tagging 
suffer from data sparseness when used on 
highly inflectional languages (which require 
large lexical tagset inventories). For this 
reason, a number of alternative methods have 
been proposed over the years. One of the 
most successful methods used for this task, 
FDOOHG7LHUHG7DJJLQJ7XIL, 1999), exploits 
a reduced set of tags derived by removing 
several recoverable features from the lexicon 
morpho-syntactic descriptions. A second 
phase is aimed at recovering the full set of 
morpho-syntactic features. In this paper we 
present an alternative method to Tiered 
Tagging, based on local optimizations with 
Neural Networks and we show how, by 
properly encoding the input sequence in a 
general Neural Network architecture, we 
achieve results similar to the Tiered Tagging 
methodology, significantly faster and without 
requiring extensive linguistic knowledge as 
implied by the previously mentioned method. 
1 Introduction 
Part-of-speech tagging is a key process for 
various tasks such as `information extraction, 
text-to-speech synthesis, word sense 
disambiguation and machine translation. It is also 
known as lexical ambiguity resolution and it 
represents the process of assigning a uniquely 
interpretable label to every word inside a 
sentence. The labels are called POS tags and the 
entire inventory of POS tags is called a tagset.  
There are several approaches to part-of-speech 
tagging, such as Hidden Markov Models (HMM) 
(Brants, 2000), Maximum Entropy Classifiers 
(Berger et al, 1996; Ratnaparkhi, 1996), 
Bayesian Networks (Samuelsson, 1993), Neural 
Networks (Marques and Lopes, 1996) and 
Conditional Random Fields (CRF) (Lafferty et 
al., 2001). All these methods are primarily 
intended for English, which uses a relatively 
small tagset inventory, compared to highly 
inflectional languages. For the later mentioned 
languages, the lexicon tagsets (called morpho-
syntactic descriptions (Calzolari and Monachini, 
1995) or MSDs) may be 10-20 times or even 
larger than the best known tagsets for English. 
For instance Czech MSD tagset requires more 
than 3000 labels (Collins et al, 1999), Slovene 
more than 2000 labels (Erjavec and Krek, 2008), 
and Romanian more than 1100 labels (Tufi, 
1999). The standard tagging methods, using such 
large tagsets, face serious data sparseness 
problems due to lack of statistical evidence, 
manifested by the non-robustness of the language 
models. When tagging new texts that are not in 
the same domain as the training data, the 
accuracy decreases significantly. Even tagging 
in-domain texts may not be satisfactorily 
accurate. 
One of the most successful methods used for 
this taVN FDOOHG 7LHUHG 7DJJLQJ 7XIL, 1999), 
exploits a reduced set of tags derived by 
removing several recoverable features from the 
lexicon morpho-syntactic descriptions. 
According to the MULTEXT EAST lexical 
specifications (Erjavec and Monachini, 1997), 
the Romanian tagset consists of a number of 614 
MSD tags (by exploiting the case and gender 
regular syncretism) for wordforms and 10 
punctuation tags (Tufi et al, 1997), which is 
still significantly larger than the tagset of 
English. The MULTEX EAST version 4 
(Erjavec, 2010) contains specifications for a total 
of 16 languages: Bulgarian, Croatian, Czech, 
Estonian, English, Hungarian, Romanian, 
692
693
In the case of out-of-vocabulary (OOV) 
words, both approaches use suffix analysis to 
determine the most probable tags that can be 
assigned to the current word.  
To clarify how these two methods work, if we 
want to train the network to label the current 
word, using a context window of 1 (previous tag, 
current possible tags, and possible tags for the 
next word) and if we have, say 100 tags in the 
tagset, the input is a real valued vector of 300 
sub-unit elements and the output is a vector 
which contains 100 elements, also sub-unit real 
numbers. As mentioned earlier, each value in the 
output vector corresponds to a distinct tag from 
tagset and the tag assigned to the current word is 
chosen to correspond to the maximum value 
inside the output vector. 
The previously proposed methods still suffer 
from the same issue of data sparseness when 
applied to MSD tagging. However, in our 
approach, we overcome the problem through a 
different encoding of the input data (see section 
2.1).  
The power of neural networks results mainly 
from their ability to attain activation functions 
over different patterns via their learning 
algorithm. By properly encoding the input 
sequence, the network chooses which input 
features contribute in determining the output 
features for MSDs (e.g. patterns composed of 
part of speech, gender, case, type etc. contribute 
independently in selecting the optimal output 
sequence). This way, we removed the need for 
explicit MSD to CTAG conversion and MSD 
recovery from CTAGs.  
2.1 The MSD binary encoding scheme 
A MSD language independently encodes a part 
of speech (POS) with the associated lexical 
attribute values as a string of positional ordered 
character codes (Erjavec, 2004). The first 
character is an upper case character denoting the 
SDUWRIVSHHFKHJ?1? IRUQRXQV?9?IRUYHUEV
?$? IRU DGMHFWLYHV HWF DQG WKH IROORZLQJ
FKDUDFWHUV ORZHU OHWWHUV RU ?-? specify the 
instantiations of the characteristic lexical 
attributes of the POS. For example, the MSD 
?1FIVUQ? specifies a noun (the first character is 
?1? the type of ZKLFK LV FRPPRQ ?F? WKH
second character), feminine gender ?I?VLQJXODU 
number ?V? LQQRPLQDWLYHDFFXVDWLYHFDVH?U?
and indefinite form ?Q?If a specific attribute is 
not relevant for a language, or for a given 
combination of feature-YDOXHVWKHFKDUDFWHU?-?LV
used in the corresponding position. For a 
language which does not morphologically mark 
the gender and definiteness features, the earlier 
H[HPSOLILHG06'ZLOOEHHQFRGHGDV?1F-sr-? 
 
In order to derive a binary vector for each of 
the 614 MSDs of Romanian we proceeded to: 
1. List and sort all possible POSes of 
Romanian (16 POSes) and form a binary 
vector with 16 positions in which position k 
is equal 1 only if the respective MSD has 
the corresponding POS (i.e. the k-th POS in 
the sorted list of POSes); 
2. List and sort all possible values of all lexical 
attributes GLVUHJDUGLQJWKHZLOGFDUG?-? for 
all POSes (94 values) and form another 
binary vector with 94 positions such that the 
k-th position of this vector is 1 if the 
respective MSD has an attribute with the 
corresponding value; 
3. Concatenate the vectors from steps 1 and 2 
and obtain the binary codification of a MSD 
as a 110-position binary vector. 
2.2 The training and tagging procedure 
The tagger automatically assigns four dummy 
tokens (two at the beginning and two at the end) 
to the target utterance and the neural network is 
trained to automatically assign a MSD given the 
context (two previously assigned tags and the 
possible tags for the current and following two 
words) of the current word (see below for 
details).  
In our framework a training example consists 
of the features extracted for a single word inside 
an utterance as input and it?s MSD within that 
utterance as output. The features are extracted 
from a window of 5 words centered on the 
current word. A single word is characterized by a 
vector that encodes either its assigned MSD or its 
possible MSDs. To encode the possible MSDs 
we use equation 2, where each possible attribute 
a, has a single corresponding position inside the 
encoded vector.  
 
2:=S; L %:S?=;%:S;  (2) 
 
Note that we changed the probability 
estimates to account for attributes not tags.  
 
To be precise, for every word wk, we obtain its 
input features by concatenating a number of 5 
vectors. The first two vectors encode the MSDs 
assigned to the previous two words (wk-1 and wk-
694
2).The next three vectors are used to encode the 
possible MSDs for the current word (wk) and the 
following two words (wk+1 and wk+2).  
During training, we also compute a list of 
suffixes with associated MSDs, which is used at 
run-time to build the possible MSDs vector for 
unknown words. When such words are found 
within the test data, we approximate their 
possible MSDs vector using a variation of the 
method proposed by Brants (2000).  
When the tagger is applied to a new utterance, 
the system iteratively calculates the output MSD 
for each individual word. Once a label has been 
assigned to a word, the ZRUG?VDVVRFLDWHGYHFWRU
is edited so it will have the value of 1 for each 
attribute present in its newly assigned MSD.  
As a consequence of encoding each individual 
attribute separately for MSDs, the tagger can 
assign new tags (that were never associated with 
the current word in the training corpus). 
Although this is a nice behavior for dealing with 
unknown words it is often the case that it assigns 
attribute values that are not valid for the 
wordform. To overcome these types of errors we 
use an additional list of words with their allowed 
MSDs. For an OOV word, the list is computed as 
a union from all MSDs that appeared with the 
suffixes that apply to that word. 
When the tagger has to assign a MSD to a 
given word, it selects one from the possible 
wordform?V MSDs in its wordform/MSDs 
associated list using a simple distance function: 
 
???
???
? K? F A?
?
?@4
 
(3) 
2 - The list of all possible MSDs for the given word 
J - The length of the MSD 
encoding (110 bits) 
K - The output of the Neural Network for the current word 
A - Binary encoding for a MSD in P 
3 Network hyperparameters 
In our experiments, we used a fully connected, 
feed forward neural network with 3 layers (1 
input layer, 1 hidden layer and 1 output layer) 
and a sigmoid activation function (equation 3). 
While other network architectures such as 
recurrent neural networks may prove to be more 
suitable for this task, they are extremely hard to 
train, thus, we traded the advantages of such 
architectures for the robustness and simplicity of 
the feed-forward networks. 
 
B:P; L ssE A?? (3) 
B:P; - Neuron output 
P - 
The weighted sum of all the 
neuron outputs from the 
previous layer 
 
Based on the size of the vectors used for MSD 
encoding, the output layer has 110 neurons and 
the input layer is composed of 550 (5 x 110) 
neurons. 
In order to fully characterize our system, we 
took into account the following parameters: 
accuracy, runtime speed, training speed, hidden 
layer configuration and the number of optimal 
training iterations. These parameters have 
complex dependencies and relations among each 
other. For example, the accuracy, the optimal 
number of training iterations, the training and the 
runtime speed are all highly dependent on the 
hidden layer configuration. Small hidden layer 
give high training and runtime speeds, but often 
under-fit the data. If the hidden layer is too large, 
it can easily over-fit the data and also has a 
negative impact on the training and runtime 
speed. The number of optimal training iterations 
changes with the size of the hidden layer (larger 
layers usually require more training iterations). 
To obtain the trade-offs between the above 
mentioned parameters we devised a series of 
experiments, in all of which we used WKH??
MSD annotated corpus, which is composed of 
118,025 words. We randomly kept out 
approximately 1/10 (11,960 words) of the 
training corpus for building a cross-validation 
set. The baseline accuracy on the cross-validation 
set (i.e. returning the most probable tag) is 
93.29%. We also used an additional inflectional 
wordform/MSD lexicon composed of 
approximately 1 million hand-validated entries.  
695
The first experiment was designed to 
determine the trade-off between the run-time 
speed and the size of the hidden layer. We made 
a series of experiments disregarding the tagging 
accuracy. 
 
Hidden size Time (ms) Words/sec 
50 1530 7816 
70 1888 6334 
90 2345 5100 
110 2781 4300 
130 3518 3399 
150 5052 2367 
170 5466 2188 
190 6734 1776 
210 7096 1685 
230 8332 1435 
250 9576 1248 
270 10350 1155 
290 11080 1079 
310 12364 967 
 
Table 1 - Execution time vs. number of neurons on 
the hidden layer 
 
Because, for a given number of neurons in the 
hidden layer, the tagging speed is independent on 
the tagging accuracy, we partially trained (using 
one iteration and only 1000 training sentences) 
several network configurations. The first network 
only had 50 neurons in the hidden layer and for 
the next networks, we incremented the hidden 
layer size by 20 neurons until we reached 310 
neurons. The total number of tested networks is 
14. After this, we measured the time it took to 
tag the 1984 test corpus (11,960 words) for each 
individual network, as an average of 3 tagging 
runs in order to reduce the impact of the 
operating system load on the tagger (Table 1 
shows the figures). 
Determining the optimal size of the hidden 
layer is a very delicate subject and there are no 
perfect solutions, most of them being based on 
trial and error: small-sized hidden layers lead to 
under-fitting, while large hidden layers usually 
cause over-fitting. Also, because of the trade-off 
between runtime speed and the size of hidden 
layers, and if runtime speed is an important 
factor in a particular NLP application, then 
hidden layers with smaller number of neurons are 
preferable, as they surely do not over-fit the data 
and offer a noticeable speed boost. 
 
hidden 
layer 
Train set 
accuracy 
Cross 
validation 
accuracy 
50 99.18 97.95 
70 99.20 98.02 
90 99.27 98.03 
110 99.29 98.05 
130 99.35 98.12 
150 99.35 98.09 
170 99.41 98.07 
190 99.40 98.10 
210 99.40 98.21 
 
Table 2 - Train and test accuracy rates for different 
hidden layer configurations 
 
As shown in Table 1, the runtime speed of our 
system shows a constant decay when we increase 
the hidden layer size. The same decay can be 
seen in the training speed, only this time by an 
order of magnitude larger. Because training a 
single network takes a lot of time, this 
experiment was designed to estimate the size of 
the hidden layer which offers good performance 
in tagging. To do this, we individually trained a 
number of networks in 30 iterations, using 
various hidden layer configurations (50, 70, 90, 
0.97
0.975
0.98
0.985
0.99
0.995
1
1 5 9 13 17 21 25 29 33 37 41 45 49 53 57 61 65 69 73 77 81 85 89 93 97
Test set
Train set
Number of iterrations 
Accuracy 
 
Figure 2 - 130 hidden layer network test and train set tagging accuracy as a function of the number of iterations 
696
110, 130, 150, 170, 190, and 210 neurons) and 5 
initial random initializations of the weights. For 
each configuration, we stored the accuracy of 
reproducing the learning data (the tagging of the 
training corpus) and the accuracy on the unseen 
data (test sets). The results are shown in Table 2. 
Although a hidden layer of 210 neurons did not 
seem to over-fit the data, we stopped the 
experiment, as the training time got significantly 
longer.  
The next experiment was designed to see how 
the number of training iterations influences the 
tagging performance of networks with different 
hidden layer configurations. Intuitively, the 
training process must be stopped when the 
network begins to over-fit the data (i.e. the train 
set accuracy increases, but the test set accuracy 
drops). Our experiments indicate that this is not 
always the case, as in some situations the 
continuation of the training process leads to 
better results on the cross-validation data (as 
shown in Figure 2). So, the problem comes to 
determining which is the most stable 
configuration of the neural network (i.e. which 
hidden unit size will be most likely to return 
good results on the test set) and establish the 
number of iterations it takes for the system to be 
trained. To do this, we ran the training procedure 
for 100 iterations and for each training iteration, 
we computed the accuracy rate of every 
individual network on the cross-validation set 
(see Table 3 for the averaged values). As shown, 
the network configuration using 130 neurons on 
the hidden layer is most likely to produce better 
results on the cross-validation set regardless of 
the number of iterations.  
Although, some other configurations provided 
better figures for the maximum accuracy, their 
average accuracy is lower than that of the 130 
hidden unit network. Other good candidates are 
the 90 and 110 hidden unit networks, but not the 
larger valued ones, which display a lower 
average accuracy and also significantly slower 
tagging speeds.  
The most suitable network configuration for a 
given task depends on the language, MSD 
encoding size, speed and accuracy requirements. 
In our own daily applications we use the 130 
hidden unit network. After observing the 
behavior of the various networks on the cross-
validation set we determined that a good choice 
is to stop the training procedure after 40 
iterations. 
 
Hidden 
units Avg. acc. Max. acc. St. dev. 
50 97.94 98.31 0.127002 
70 98.03 98.31 0.12197 
50 97.94 98.37 0.139762 
70 98.03 98.43 0.124996 
90 98.07 98.39 0.134487 
110 98.08 98.45 0.127109 
130 98.14 98.44 0.136072 
150 98.01 98.36 0.143324 
170 97.94 98.36 0.122834 
 
Table 3 - Average and maximum accuracy for various 
hidden layer configuration calculated over 100 
training iterations on the test set 
 
To obtain the accuracy of the system, in our 
last experiment we used the 130 hidden unit 
network and we performed the training/testing 
procedure on the 1984 corpus, using 10-fold 
validation and 30 random initializations. The 
final accuracy was computed as an average 
between all the accuracy figures measured at the 
end of the training process (after 40 iterations). 
The first 1/10 of the 1984 corpus on which we 
tuned the hyperparameters was not included in 
the test data, but was used for training. The mean 
accuracy of the system (98.41%) was measured 
as an average of 270 values. 
4 Comparison to other methods 
,Q KLV ZRUN &HDXu (2006) presents a 
different approach to MSD tagging using the 
Maximum Entropy framework. He presents his 
results on the same corpus we used for training 
and testing (the 1984 corpus) and he compares 
his method (98.45% accuracy) with the Tiered 
Tagging methodology (97.50%) (Tufi and 
Dragomirescu, 2004). 
Our Neural Network approach obtained 
similar (slightly lower) results (98.41%), 
although it is arguable that our split/train 
procedure is not identical to the one used in his 
work (no details were given as how the 1/10 of 
the training corpus was selected). Also, our POS 
tagger detected cases where the annotation in the 
Gold Standard was erroneous. One such example 
LV LQ ?lame de ras? (QJOLVK ?UD]RU EODGHV?
ZKHUH?ODPH?English ?EODGHV?LVDQRXQ?GH?
?for?LVDSUHSRVLWLRQDQG?UDV??VKDYLQJ?) is a 
supine verb (with a past participle form) which 
was incorrectly annotated as a noun. 
697
5 Network pattern analysis 
Using feed-forward neural networks gives the 
ability to outline what input features contribute to 
the selection of various MSD attribute values in 
the output layer which might help in reducing the 
tagset and thus, redesigning the network 
topology with beneficial effects both on the 
speed and accuracy.  
To determine what input features contribute to 
the selection of certain MSD attribute values, one 
can analyze the weights inside the neural 
network and extract the input ? output links that 
are formed during training. We used the network 
with 130 units on the hidden layer, which was 
previously trained for 100 iterations. Based on 
the input encoding, we divided the features into 5 
groups (one group for each MSD inside the local 
context ? two previous MSDs, current and 
following two possible MSDs). For a target 
attribute value (noun, gender feminine, gender 
masculine, etc.) and for each input group, we 
selected the top 3 input values which support the 
decision of assigning the target value to the 
attribute (features that increase the output value) 
and the top 3 features which discourage this 
decision (features that decrease the output value). 
For clarity, we will use the following notations 
for the groups: 
x G
-2: group one ? the assigned MSD for 
the word at position i-2 
x G
-1: group two ? the assigned MSD for 
the word at position i-1 
x G0: group three ? the possible MSDs for 
the word at position i 
x G1: group four? the possible MSDs for 
the word at position i+1 
x G2: group five ? the possible MSDs for 
the word at position i+2 
where i corresponds to the position of the word 
which is currently being tagged. Also, we 
classify the attribute values into two categories 
(C): (P) want to see (support the decision) and 
(N) GRQ?WZDQWWRVHH (discourage the decision). 
 
Table 4 shows partial (G
-1 G0 G1) examples of 
two target attribute values (cat=Noun and gender 
=Feminine) and their corresponding input 
features used for discrimination. 
 
Target 
value Group C Attribute values  
Noun G
-1 P 
main (of a verb), article, 
masculine (gender of a 
noun/adjective 
N 
particle, conjunctive particle, 
auxiliary (of a verb), 
demonstrative (of a pronoun) 
G0 
P noun, common/proper (of a 
noun) 
N 
adverb, pronoun, numeral, 
interrogative/relative (of a 
pronoun) 
G1 
P 
genitive/dative (of a 
noun/adjective), particle, 
punctuation 
N 
conjunctive particle, strong (of 
a pronoun), non-definite (of a 
noun/adjective), exclamation 
mark 
Fem. 
G
-1 
P 
main (of a verb), preposition, 
feminine (of a 
noun/adjective) 
N auxiliary (of a verb), particle, demonstrative (of a pronoun) 
G0 
P 
feminine (of a 
noun/adjective), 
nominative/accusative (of a 
noun/adjective), past (of a 
verb) 
N 
masculine (of a 
noun/adjective), auxiliary (of a 
verb), interrogative/relative (of 
a pronoun), adverb 
G1 
P 
dative/genitive (of a 
noun/adjective), indicative (of 
a verb), feminine (of a 
noun/adjective) 
N 
conjunctive particle, future 
particle, nominative/accusative 
(of a noun/adjective) 
 
Table 4 ? P/N features for various attribute 
values. 
 
For instance, when deciding on whether to give a 
noun (N) label to current position (G0), we can 
see that the neural network has learned some 
interesting dependencies: at position G
-1 we find 
an article (which frequently determines a noun) 
and at the current position it is very important for 
the word being tagged to actually be a common 
or proper noun (either by lexicon lookup or by 
suffix guessing) and not be an adverb, pronoun 
or numeral (POSes that cannot be found in the 
typical ambiguity class of a noun). At the next 
position of the target (G1) we also find a noun in 
genitive or dative, corresponding to a frequent 
construction in Romanian, HJ ?PDina 
ELDWXOXL? EHLQJ D VHTXHQFH RI WZR nouns, the 
second at genitive/dative. 
If the neural network outputs the feminine 
gender to its current MSD, one may see that it 
698
has actually learned the agreement rules (at least 
locally): the feminine gender is present both 
before (G
-1) the target word as well as after it 
(G1). 
6 Conclusions and future work 
We presented a new approach for large tagset 
part-of-speech tagging using neural networks. An 
advantage of using this methodology is that it 
does not require extensive knowledge about the 
grammar of the target language. When building a 
new MSD tagger for a new language one is only 
required to provide the training data and create 
an appropriate MSD encoding system and as 
shown, the MSD encoding algorithm is fairly 
simple and our proposed version works for any 
other MSD compatible encoding, regardless of 
the language.  
Observing which features do not participate in 
any decision helps design custom topologies for 
the Neural Network, and provides enhancements 
in both speed and accuracy. The configurable 
nature of our system allows users to provide their 
own MSD encodings, which permits them to 
mask certain features that are not useful for a 
given NLP application.  
If one wants to process a large amount of text 
and is interested only in assigning grammatical 
categories to words, he can use a MSD encoding 
in which he strips off all unnecessary features. 
Thus, the number of necessary neurons would 
decrease, which assures faster training and 
tagging. This is of course possible in any other 
tagging approaches, but our framework supports 
this by masking attributes inside the MSD 
encoding configuration file, without having to 
change anything else in the training corpus. 
During testing the system only verifies if the 
MSD encodings are identical and the displayed 
accuracy directly reflects the performance of the 
system on the simplified tagging schema. 
We also proposed a methodology for selecting 
a network configurations (i.e. number of hidden 
units), which best suites the application 
requirements. In our daily applications we use a 
network with 130 hidden units, as it provides an 
optimal speed/accuracy trade-off (approx. 3400 
words per second with very good average 
accuracy).  
The tagger is implemented as part of a larger 
application that is primarily intended for text-to-
speech (TTS) synthesis. The system is free for 
non-commercial use and we provide both web 
and desktop user-interfaces. It is part of the 
METASHARE platform and available online 2 . 
Our primary goal was to keep the system 
language independent, thus all our design choices 
are based on the necessity to avoid using 
language specific knowledge, when possible. The 
application supports various NLP related tasks 
such as lexical stress prediction, syllabification, 
letter-to-sound conversion, lemmatization, 
diacritic restoration, prosody prediction from text 
and the speech synthesizer uses unit-selection. 
From the tagging perspective, our future plans 
include testing the system on other highly 
inflectional languages such as Czech and 
Slovene and investigating different methods for 
automatically determining a more suitable 
custom network topology, such as genetic 
algorithms. 
Acknowledgments 
The work reported here was funded by the 
project METANET4U by the European 
Commission under the Grant Agreement No 
270893  
                                                          
2
 http://ws.racai.ro:9191  
699
References  
Berger, A. L., Pietra, V. J. D. and Pietra, S. A. D. 
1996. A maximum entropy approach to natural 
language processing. Computational linguistics, 
22(1), 39-71. 
 
Brants, T. 2000. TnT: a statistical part-of-speech 
tagger. In Proceedings of the sixth conference on 
applied natural language processing (pp. 224-231). 
Association for Computational Linguistics. 
 
Calzolari, N. and Monachini M. (eds.). 1995. 
Common Specifications and Notation for Lexicon 
Encoding and Preliminary Proposal for the 
Tagsets. MULTEXT Report, March. 
 
&HDXu, A. 2006. Maximum entropy tiered tagging. In 
Proceedings of the 11th ESSLLI Student Session 
(pp. 173-179). 
 
CollinV05DPVKDZ/+DML?-DQG7LOOPDQQ&
1999. A statistical parser for Czech. In Proceedings 
of the 37th annual meeting of the Association for 
Computational Linguistics on Computational 
Linguistics (pp. 505-512). Association for 
Computational Linguistics. 
 
Erjavec, T. and Monachini, M. (Eds.). 1997. 
Specifications and Notation for Lexicon Encoding. 
Deliverable D1.1 F. Multext-East Project COP-
106. 
 
Erjavec, T. 2004. MULTEXT-East version 3: 
Multilingual morphosyntactic specifications, 
lexicons and corpora. In Fourth International 
Conference on Language Resources and 
Evaluation, LREC (Vol. 4, pp. 1535-1538). 
 
Erjavec, T. and Krek, S. 2008. The JOS 
morphosyntactically tagged corpus of Slovene. In 
Proceedings of the Sixth International Conference 
RQ/DQJXDJH5HVRXUFHVDQG(YDOXDWLRQ/5(&? 
 
Erjavec, T. 2010. MULTEXT-East Version 4: 
Multilingual Morphosyntactic Specifications, 
Lexicons and Corpora. In Proceedings of the 
Seventh International Conference on Language 
Resources and Evaluation (LREC'10), Valletta, 
Malta. European Language Resources Association 
(ELRA) ISBN 2-9517408-6-7. 
 
Lafferty, J., McCallum, A. and Pereira, F. C. 2001. 
Conditional random fields: Probabilistic models 
for segmenting and labeling sequence data. 
 
Marques, N. C. and Lopes, G. P. 1996. A neural 
network approach to part-of-speech tagging. In 
Proceedings of the 2nd Meeting for Computational 
Processing of Spoken and Written Portuguese (pp. 
21-22). 
 
Ratnaparkhi, A. 1996. A maximum entropy model for 
part-of-speech tagging. In Proceedings of the 
conference on empirical methods in natural 
language processing (Vol. 1, pp. 133-142). 
 
Samuelsson, C. 1993. Morphological tagging based 
entirely on Bayesian inference. In 9th Nordic 
Conference on Computational Linguistics. 
 
Schmid, H. 1994. Part-of-speech tagging with neural 
networks. In Proceedings of the 15th conference on 
Computational linguistics-Volume 1 (pp. 172-176). 
Association for Computational Linguistics. 
 
Tufi, D., Barbu A.M.,
 
3WUD?FX 9 Rotariu G. and 
Popescu C. 1997. Corpora and Corpus-Based 
Morpho-Lexical Processing. In Recent Advances 
in Romanian Language Technology, (pp. 35-56). 
Romanian Academy Publishing House, ISBN 973-
27-0626-0. 
 
7XIL, D. 1999. Tiered tagging and combined 
language models classifiers. In Text, Speech and 
Dialogue (pp. 843-843). Springer 
Berlin/Heidelberg. 
 
Tufi, D., and Dragomirescu, L. 2004. Tiered tagging 
revisited. In Proceedings of the 4th LREC 
Conference (pp. 39-42). 
 
700
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 411?416,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
RACAI: Unsupervised WSD experiments @ SemEval-2, Task #17 
 
 
Radu Ion 
Institute for AI, Romanian Academy 
13, Calea 13 Septembrie, Bucharest 
050711, Romania 
radu@racai.ro 
Dan ?tef?nescu 
Institute for AI, Romanian Academy 
13, Calea 13 Septembrie, Bucharest 
050711, Romania 
danstef@racai.ro 
 
 
Abstract 
This paper documents the participation of the 
Research Institute for Artificial Intelligence of 
the Romanian Academy (RACAI) to the Task 
17 ? All-words Word Sense Disambiguation 
on a Specific Domain, of the SemEval-2 com-
petition. We describe three unsupervised WSD 
systems that make extensive use of the Prince-
ton WordNet (WN) structure and WordNet 
Domains in order to perform the disambigua-
tion. The best of them has been ranked the 12th 
by the task organizers out of 29 judged runs. 
1 Introduction 
Referring to the last SemEval (SemEval-1, 
(Agirre et al, 2007a)) and to our recent work 
(Ion and ?tef?nescu, 2009), unsupervised Word 
Sense Disambiguation (WSD) is still at the bot-
tom of WSD systems ranking with a significant 
loss in performance when compared to super-
vised approaches. With Task #17 @ SemEval-2, 
this observation is (probably 1 ) reinforced but 
another issue is re-brought to light: the difficulty 
of supervised WSD systems to adapt to a given 
domain (Agirre et al, 2009). With general scores 
lower with at least 3% than 3 years ago in Task 
#17 @ SemEval-1 which was a supposedly hard-
er task  (general, no particular domain WSD was 
required for all words), we observe that super-
vised WSD is certainly more difficult to imple-
ment in a real world application. 
Our unsupervised WSD approach benefited 
from the specification of this year?s Task #17 
which was a domain-limited WSD, meaning that 
the disambiguation would be applied to content 
words drawn from a specific domain: the sur-
rounding environment. We worked under the 
assumption that a term of the given domain 
                                                 
1 At the time of the writing we only know the systems rank-
ing without the supervised/unsupervised distinction. 
would have the same meaning with all its occur-
rences throughout the text. This hypothesis has 
been put forth by Yarowsky (1993) as the ?one 
sense per discourse? hypothesis (OSPD for 
short). 
The task organizers offered a set of back-
ground documents with no sense annotations to 
the competitors who want to train/tune their sys-
tems using data from the same domain as the 
official test set. Working with the OSPD hypo-
thesis, we set off to construct/test domain specif-
ic WSD models from/on this corpus using the 
WordNet Domains (Bentivogli et al, 2004). For 
testing purposes, we have constructed an in-
house gold standard from this corpus that com-
prises of 1601 occurrences of 204 terms of the 
?surrounding environment? domain that have 
been automatically extracted with the highest 
confidence. We have observed that our gold 
standard (which has been independently anno-
tated by 3 annotators but on non-overlapping 
sections which led to having no inter-annotator 
agreement scores) obeys the OSPD hypothesis 
which we think that is appropriate to domain-
limited WSD. 
In what follows, we will briefly acknowledge 
the usage of WordNet Domains in WSD, we will 
then describe the construction of the corpus of 
the background documents including here the 
creation of an in-house gold standard, we will 
then briefly describe our three WSD algorithms 
and finally we will conclude with a discussion on 
the ranking of our runs among the 29 evaluated 
by the task organizers. 
2 Related Work 
WordNet Domains is a hierarchy of labels that 
have been assigned to WN synsets in a one to 
(possible) many relationship (but the frequent 
case is a single WN domain for a synset). A do-
main is the name of an area of knowledge that is 
recognized as unitary (Bentivogli et al, 2004). 
411
Thus labels such as ?architecture?, ?sport? or 
?medicine? are mapped onto synsets like 
?arch(4)-noun?, ?playing(2)-noun? or ?chron-
ic(1)-adjective? because of the fact that the re-
spective concept evokes the domain. 
WordNet Domains have been used in various 
ways to perform WSD. The main usage of this 
mapping is that the domains naturally create a 
clustering of the WN senses of a literal thus of-
fering a sense inventory that is much coarser than 
the fine sense distinctions of WN. For instance, 
senses 1 (?a flat-bottomed motor vehicle that can 
travel on land or water?) and 2 (?an airplane 
designed to take off and land on water?) of the 
noun ?amphibian? are both mapped to the do-
main ?transport? but the 3rd sense of the same 
noun is mapped onto the domains ?ani-
mals/biology? being the ?cold-blooded verte-
brate typically living on land but breeding in 
water; aquatic larvae undergo metamorphosis 
into adult form? (definitions from version 2.0 of 
the WN). 
V?zquez et al (2004) use WordNet Domains 
to derive a new resource they call the Relevant 
Domains in which, using WordNet glosses, they 
extract the most representative words for a given 
domain. Thus, for a word w and a domain d, the 
Association Ratio formula between w and d is 
)(P
)|(Plog)|(P),(AR 2 w
dwdwdw ?=  
in which, for each synset its gloss has been POS 
tagged and lemmatized. The probabilities are 
computed counting pairs dw, in glosses (each 
gloss has an associated d domain via its synset). 
Using the Relevant Domains, the WSD proce-
dure for a given word w in its context C (a 100 
words window centered in w), computes a simi-
larity measure between two vectors of AR 
scores: the first vector is the vector of AR scores 
of the sentence in which w appears and the other 
is the vector of domain scores computed for the 
gloss of a sense of w (both vectors are norma-
lized such that they contain the same domains). 
The highest similarity gives the sense of w that is 
closest to the domain vector of C. With this me-
thod, V?zquez et al obtain a precision of 0.54 
and a recall of 0.43 at the SensEval-2, English 
All-Words Task placing them in the 10th position 
out of 22 systems where the best one (a super-
vised system) achieved a 0.69 precision and an 
equal recall. 
Another approach to WSD using the WordNet 
Domains is that of Magnini et al (2002). The 
method is remarkably similar to the previous one 
in that the description of the vectors and the se-
lection of the assigned sense is the same. What 
differs, is the weights that are assigned to each 
domain in the vector. Magnini et al distinguish 
between text vectors (C vectors in the previous 
presentation) and sense vectors. Text (or context) 
vector weights are computed comparing domain 
frequency in the context with the domain fre-
quency over the entire corpus (see Magnini et al 
(2002) for details). Sense vectors are derived 
from sense-annotated data which qualifies this 
method as a supervised one. The results that have 
been reported at the same task the previous algo-
rithm participated (SensEval-2, English All-
Words Task), are: precision 0.748 and recall 
0.357 (12th place). 
Both the methods presented here are very sim-
ple and easy to adapt to different domains. One 
of our methods (RACAI-1, see below) is even 
simpler (because it makes the OSPD simplifying 
assumption) and performs with approximately 
the same accuracy as any of these methods judg-
ing by the rank of the system and the total num-
ber of participants.  
3 Using the Background Documents 
collection  
Task #17 organizers have offered a set of back-
ground documents for training/tuning/testing 
purposes. The corpus consists of 124 files from 
the ?surrounding environment? domain that have 
been collected in the framework of the Kyoto 
Project (http://www.kyoto-project.eu/). 
First, we have assembled the files into a single 
corpus in order to be able to apply some cleaning 
procedures. These procedures involved the re-
moval of the paragraphs in which the proportion 
of letters (Perl character class ?[A-Za-z_-]?) 
was less than 0.8 because the text contained a lot 
of noise in form of lines of numbers and other 
symbols which probably belonged to tables. The 
next stage was to have the corpus POS-tagged, 
lemmatized and chunked using the TTL web ser-
vice (Tufi? et al, 2008). The resulting file is an 
XML encoded corpus which contains 136456 
sentences with 2654446 tokens out of which 
348896 are punctuation tokens. 
In order to test our domain constrained WSD 
algorithms, we decided to construct a test set 
with the same dimension as the official test set of 
about 2000 occurrences of content words specific 
to the ?surrounding environment? domain. In 
doing this, we have employed a simple term ex-
412
traction algorithm which considers that terms, as 
opposed to words that are not domain specific, 
are not evenly distributed throughout the corpus. 
To formalize this, the corpus is a vector of lem-
mas [ ]Nlll ,,,C 21 K=  and for each unique lem-
ma Njl j ??1, , we compute the mean of the 
absolute differences of its indexes in C as 
mjkj
j
Nkj llkmjmll
lf
kj
?<<??=?
?
=
?
?<? ,,,
1)(
1?
where )( jlf  is the frequency of jl  in C. We 
also compute the standard deviation of these dif-
ferences from the mean as 
2)(
)(
1
2
?
??
=
?
?<?
j
Nkj
lf
kj ?
?  
in the same conditions as above. 
With the mean and standard deviation of in-
dexes differences of a content word lemma com-
puted, we construct a list of all content word 
lemmas that is sorted in descending order by the 
quantity ?? / which we take as a measure of the 
evenness of a content word lemma distribution. 
Thus, lemmas that are in the top of this list are 
likely to be terms of the domain of the corpus (in 
our case, the ?surrounding environment? do-
main). Table 1 contains the first 20 automatically 
extracted terms along with their term score. 
Having the list of terms of our domain, we 
have selected the first ambiguous 210 (which 
have more than 1 sense in WN) and constructed 
a test set in which each term has (at least) 10 oc-
currences in order to obtain a test corpus with at 
least 2000 occurrences of the terms of the ?sur-
rounding environment? domain. A large part of 
these occurrences have been independently 
sense-annotated by 3 annotators which worked 
on disjoint sets of terms (70 terms each) in order 
to finish as soon as possible. In the end we ma-
naged to annotate 1601 occurrences correspond-
ing to 204 terms. 
When the gold standard for the test set was 
ready, we checked to see if the OSPD hypothesis 
holds. In order to determine if it does, we com-
puted the average number of annotated different 
senses per term which is 1.36. In addition, consi-
dering the fact that out of 204 annotated terms, 
145 are annotated with a single sense, we may 
state that in this case, the OSPD hypothesis 
holds. 
Term Score Term Score
gibbon 15.89 Oceanica 9.41
fleet 13.91 orangutan 9.19
sub-region 13.01 laurel 9.08
Amazon 12.41 coral 9.06
roundwood 12.26 polar 9.05
biocapacity 12.23 wrasse 8.80
footprint 11.68 reef 8.78
deen 11.45 snapper 8.67
dune 10.57 biofuel 8.53
grouper 9.67 vessel 8.35
 
Table 1: The first 20 automatically extracted terms of 
the ?surrounding environment? domain 
4 The Description of the Systems 
Since we are committed to assign a unique sense 
per word in the test set, we might as well try to 
automatically induce a WSD model from the 
background corpus in which, for each lemma 
along with its POS tag that also exists in WN, a 
single sense is listed that is derived from the cor-
pus. Then, for any test set of the same domain, 
the algorithm would give the sense from the 
WSD model to any of the occurrences of the 
lemma. 
What we actually did, was to find a list of 
most frequent 2 WN domains (frequency count 
extracted from the whole corpus) for each lemma 
with its POS tag, and using these, to list all 
senses of the lemma that are mapped onto these 2 
domains (thus obtaining a reduction of the aver-
age number of senses per word). The steps of the 
algorithm for the creation of the WSD model are: 
1. in the given corpus, for each lemma l 
and its POS-tag p normalized to WN 
POS notation (?n? for nouns, ?v? for 
verbs, ?a? for adjectives and ?b? for ad-
verbs), for each of its senses from WN, 
increase by 1 each frequency of each 
mapped domain; 
2. for each lemma l with its POS-tag p, re-
tain only those senses that map onto the 
most frequent 2 domains as determined 
by the frequency list from the first step. 
Using our 2.65M words background corpus to 
build such a model (Table 2 contains a sample), 
we have obtained a decrease in average ambigui-
ty degree (the average number of senses per con-
tent word lemma) from 2.43 to 2.14. If we set a 
threshold of at least 1 for the term score of the 
lemmas to be included into the WSD model 
(which selects 12062 lemmas, meaning about 1/3 
of all unique lemmas in the corpus), we obtain 
413
the same reduction thus contradicting our hypo-
thesis that the average ambiguity degree of terms 
would be reduced more than the average ambigu-
ity degree of all words in the corpus. This result 
might be due to the fact that the ?factotum? do-
main is very frequent (much more frequent than 
any of the other domains). 
 
Lemma POS:Total no. 
of WN senses 
First 2 selected 
domains 
Selected 
senses 
fish n:2 animals,biology
  
1 
Arctic n:1 geography 1 
coral n:4 chemistry,animals 2,3,4 
 
Table 2: A sample of the WSD model built from the 
background corpus 
 
In what follows, we will present our 3 systems 
that use WSD models derived from the test sets 
(both the in-house and the official ones). In the 
Results section we will explain this choice. 
4.1 RACAI-1: WordNet Domains-driven, 
Most Frequent Sense 
The first system, as its name suggests, is very 
simple: using the WSD model, it chooses the 
most frequent sense (MFS) of the lemma l with 
POS p according to WN (that is, the lowest num-
bered sense from the list of senses the lemma has 
in the WSD model).  
Trying this method on our in-house developed 
test set, we obtain encouraging results: the over-
all accuracy (precision is equal with the recall 
because all test set occurrences are tried) is at 
least 4% over the general MFS baseline (sense 
no. 1 in all cases). The Results section gives de-
tails. 
4.2 RACAI-2: The Lexical Chains Selection 
With this system, we have tried to select only 
one sense (not necessarily the most frequent one) 
of lemma l with POS p from the WSD model. 
The selection procedure is based on lexical 
chains computation between senses of the target 
word (the word to be disambiguated) and the 
content words in its sentence in a manner that 
will be explained below. 
We have used the lexical chains description 
and computation method described in (Ion and 
?tef?nescu, 2009). To reiterate, a lexical chain is 
not simply a set of topically related words but 
becomes a path of synsets in the WordNet hie-
rarchy. The lexical chain procedure is a function 
of two WN synsets, LXC(s1, s2), that returns a 
semantic relation path that one can follow to 
reach s2 from s1. On the path from s2 to s1 there 
are k synsets (k ? 0) and between 2 adjacent syn-
sets there is a WN semantic relation. Each lexical 
chain can be assigned a certain score that we in-
terpret as a measure of the semantic similarity 
(SS) between s1 and s2 (see (Ion and ?tef?nescu, 
2009) and (Moldovan and Novischi, 2002) for 
more details). Thus, the higher the value of 
SS(s1, s2), the higher the semantic similarity be-
tween s1 and s2. 
We have observed that using RACAI-1 on our 
in-house test set but allowing it to select the most 
frequent 2 senses of lemma l with POS p from 
the WSD model, we obtain a whopping 82% 
accuracy. With this observation, we tried to pro-
gram RACAI-2 to make a binary selection from 
the first 2 most frequent senses of lemma l with 
POS p from the WSD model in order to approach 
the 82% percent accuracy limit which would 
have been a very good result. The algorithm is as 
follows: for a lemma l with POS p and a lemma 
lc with POS pc from the context (sentence) of l, 
compute the best lexical chain between any of 
the first 2 senses of l and any of the first 2 senses 
of lc according to the WSD model. If the first 2 
senses of l are a and b and the first 2 senses of lc 
are x and y and the best lexical chain score has 
been found between a and y for instance, then 
credit sense a of l with SS(a, y). Sum over all lc 
from the context of l and select that sense of l 
which has a maximum semantic similarity with 
the context. 
4.3 RACAI-3: Interpretation-based Sense 
Assignment 
This system tries to generate all the possible 
sense assignments (called interpretations) to the 
lemmas in a sentence. Thus, in principle, for 
each content word lemma, all its WN senses are 
considered thus generating an exponential explo-
sion of the sense assignments that can be attri-
buted to a sentence. If we have N content word 
lemmas which have k senses on average, we ob-
tain a search space of kN interpretations which 
have to be scored. 
Using the observation mentioned above that 
the first 2 senses of a lemma according to the 
WSD model yields a performance of 82%, brings 
the search space to 2N but for a large N, it is still 
too big. 
The solution we adopted (besides considering 
the first 2 senses from the WSD model) consists 
in segmenting the input sentence in M indepen-
dent segments of 10 content word lemmas each, 
which will be processed independently, yielding 
414
a search space of at most 102?M of smaller in-
terpretations. The best interpretation per each 
segment would thus be a part of the best interpre-
tation of the sentence. Next, we describe how we 
score an interpretation. 
For each sense s of a lemma l with POS p 
(from the first 2 senses of l listed in the WSD 
model) we compute an associated set of content 
words (lemmas) from the following sources: 
? all content word lemmas extracted from 
the sense s corresponding gloss (disre-
garding the auxiliary verbs); 
? all literals of the synset in which lemma l 
with sense s exists; 
? all literals of the synsets that are linked 
with the synset l(s) by a relation of the fol-
lowing type: hypernym, near_antonym, 
eng_derivative, hyponym, meronym, ho-
lonym, similar_to, derived; 
? all content word lemmas extracted from 
the glosses corresponding to synsets that 
are linked with the l(s) synset by a relation 
of the following type: hypernym, 
eng_derivative, similar_to, derived; 
With this feature set V of a sense s belonging to 
lemma l with POS p, for a given interpretation (a 
specific assignment of senses to each lemma in a 
segment), its score S (initially 0) is computed 
iteratively (for two adjacent position i and i + 1 
in the segment) as 
111 VVV,VVSS +++ ???+? iiiii  
where the |X| function is the cardinality function 
on the set X and ?  is the assignment operator. 
5 Results 
In order to run our WSD algorithms, we had to 
extract WSD models. We tested the accuracy of 
the disambiguation (onto the in-house developed 
gold standard) with RACAI-1 and RACAI-2 sys-
tems (RACAI-3 was not ready at that time) with 
models extracted a) from the whole background 
corpus and b) from the in-house developed test 
set (named here the RACAI test set, see section 
3). The results are reported in Table 3 along with 
RACAI-1 system returning the first 2 senses of a 
lemma from the WSD model and the general 
MFS baseline. 
As we can see, the results with the WSD mod-
el extracted from the test set are marginally bet-
ter than the other results. This was the reason for 
which we chose to extract the WSD model from 
the official test set as opposed to using the WSD 
model extracted from the background corpus. 
 
 RACAI 
Test Set 
Background 
Corpus 
RACAI-1 0.647 0.644 
RACAI-1 (2 senses) 0.825 0.811 
RACAI-2 0.591 0.582 
MFS (sense no. 1) 0.602 0.602 
 
Table 3: RACAI systems results (accuracy) on the 
RACAI test set 
 
However, we did not research the possibility of 
adding the official test set to either the RACAI 
test set or the background corpus and extract 
WSD models from there. 
The official test set (named the SEMEVAL 
test set here) contains 1398 occurrences of con-
tent words for disambiguation, out of which 366 
are occurrences of verbs and 1032 are occur-
rences of nouns. These occurrences correspond 
to 428 lemmas. Inspecting these lemmas, we 
have found that there are many of them which 
are not domain specific (in our case, specific to 
the ?surrounding environment? domain). For 
instance, the verb to ?be? is at the top of the list 
with 99 occurrences. It is followed by the noun 
?index? with 32 occurrences and by the noun 
?network? with 22 occurrences. With fewer oc-
currences follow ?use?, ?include?, ?show?, ?pro-
vide?, ?part? and so on. Of course, the SEMEV-
AL test set includes proper terms of the designat-
ed domain such as ?area? (61 occurrences), 
?species? (58 occurrences), ?nature? (31 occur-
rences), ?ocean?, ?sea?, ?water?, ?planet?, etc. 
Table 4 lists our official results on the SE-
MEVAL test set. 
 
 Precision Recall Rank 
RACAI-1 0.461 0.46 #12 
RACAI-2 0.351 0.35 #25 
RACAI-3 0.433 0.431 #18 
MFS 0.505 0.505 #6  
 
Table 4: RACAI systems results (accuracy) on the 
SEMEVAL test set 
 
Precision is not equal to recall because of the fact 
that our POS tagger found two occurrences of the 
verb to ?be? as auxiliaries which were ignored. 
The column Rank indicates the place our systems 
have in a 29 run ranking of all systems that parti-
cipated in Task 17 ? All-words Word Sense Dis-
ambiguation on a Specific Domain, of the Se-
415
mEval-2 competition which was won by a sys-
tem that achieved a precision of 0.57 and a recall 
of 0.555.  
The differences with the runs on the RACAI 
test set are significant but this can be explained 
by the fact that our WordNet Domains WSD me-
thod cannot cope with general (domain indepen-
dent) WSD requirements in which the ?one sense 
per discourse? hypothesis does not necessarily 
hold. 
6 Conclusions 
Regarding the 3 systems that we entered in the 
Task #17 @ SemEval-2, we think that the lexical 
chains algorithm (RACAI-2) is the most promis-
ing even if it scored the lowest of the three. We 
attribute its poor performances to the lexical 
chains computation, especially to the weights of 
the WN semantic relations that make up a chain. 
Also, we will extend our research regarding the 
correctness of lexical chains (the degree to which 
a human judge will appreciate as correct or evoc-
ative or as common knowledge a semantic path 
between two synsets). 
We also want to check if our three systems 
make the same mistakes or not in order to devise 
a way in which we can combine their outputs.  
RACAI is at the second participation in the 
SemEval series of WSD competitions. We are 
committed to improving the unsupervised WSD 
technology which, we think, is more easily 
adaptable and usable in real world applications. 
We hope that SemEval-3 will reveal significant 
improvements in this direction. 
 
Acknowledgments 
The work reported here was supported by the 
Romanian Ministry of Education and Research 
through the STAR project (no. 742/19.01.2009). 
References  
Eneko Agirre, Llu?s M?rquez and Richard Wicen-
towski, Eds., 2007. Proceedings of Semeval-2007 
Workshop. Prague, Czech Republic: Association 
for Computational Linguistics, 2007. 
Eneko Agirre, Oier Lopez de Lacalle, Christiane Fell-
baum, Andrea Marchetti, Antonio Toral, Piek Vos-
sen. 2009. SemEval-2010 Task 17: All-words Word 
Sense Disambiguation on a Specific Domain. In 
Proceedings of NAACL workshop on Semantic 
Evaluations (SEW-2009). Boulder,Colorado, 2009. 
Luisa Bentivogli, Pamela Forner, Bernardo Magnini 
and Emanuele Pianta. 2004. Revising WordNet 
Domains Hierarchy: Semantics, Coverage, and 
Balancing. In COLING 2004 Workshop on "Multi-
lingual Linguistic Resources", Geneva, Switzer-
land, August 28, 2004, pp. 101-108. 
Radu Ion and Dan ?tef?nescu. 2009. Unsupervised 
Word Sense Disambiguation with Lexical Chains 
and Graph-based Context Formalization. In Zyg-
munt Vetulani, editor, Proceedings of the 4th Lan-
guage and Technology Conference: Human Lan-
guage Technologies as a Challenge for Computer 
Science and Linguistics, pages 190?194, Pozna?, 
Poland, November 6?8 2009. Wydawnictwo 
Pozna?skie Sp. 
Bernardo Magnini, Carlo Strapparava, Giovanni 
Pezzulo, Alfio Gliozzo. 2002. The role of domain 
information in Word Sense Disambiguation. Natu-
ral Language Engineering, 8(4), 359?373, De-
cember 2002. 
Dan Moldovan and Adrian Novischi. 2002. Lexical 
chains for question answering. In Proceedings of 
the 19th International Conference on Computation-
al Linguistics, August 24 ? September 01, 2002, 
Taipei, Taiwan, pp. 1?7. 
Dan Tufi?, Radu Ion, Alexandru Ceau?u and Dan 
?tef?nescu. 2008. RACAI's Linguistic Web Servic-
es. In Proceedings of the 6th Language Resources 
and Evaluation Conference ? LREC 2008, Marra-
kech, Morocco, May 2008. ELRA ? European 
Language Ressources Association. ISBN 2-
9517408-4-0. 
Sonia V?zquez, Andr?s Montoyo and German Ri-
gau. 2004. Using Relevant Domains Resource for 
Word Sense Disambiguation. In Proceedings of the 
International Conference on Artificial Intelligence 
(IC-AI'04), Las Vegas, Nevada, 2004. 
David Yarowsky. 1993. One sense per collocation. In 
ARPA Human Language Technology Workshop, 
pp. 266?271, Princeton, NJ, 1993. 
 
416
An Expectation Maximization Algorithm for Textual Unit Alignment 
Radu Ion 
Research Institute for AI 
Calea 13 Septembrie nr. 13 
Bucharest 050711, Romania 
radu@racai.ro 
Alexandru Ceau?u 
Dublin City University 
Glasnevin, Dublin 9, Ireland 
[address3] 
aceausu@computing.dcu.ie 
Elena Irimia 
Research Institute for AI 
Calea 13 Septembrie nr. 13 
Bucharest 050711, Romania 
elena@racai.ro 
 
 
Abstract 
The paper presents an Expectation Maximiza-
tion (EM) algorithm for automatic generation 
of parallel and quasi-parallel data from any 
degree of comparable corpora ranging from 
parallel to weakly comparable. Specifically, 
we address the problem of extracting related 
textual units (documents, paragraphs or sen-
tences) relying on the hypothesis that, in a 
given corpus, certain pairs of translation 
equivalents are better indicators of a correct 
textual unit correspondence than other pairs of 
translation equivalents. We evaluate our 
method on mixed types of bilingual compara-
ble corpora in six language pairs, obtaining 
state of the art accuracy figures. 
1 Introduction 
Statistical Machine Translation (SMT) is in a con-
stant need of good quality training data both for 
translation models and for the language models. 
Regarding the latter, monolingual corpora is evi-
dently easier to collect than parallel corpora and 
the truth of this statement is even more obvious 
when it comes to pairs of languages other than 
those both widely spoken and computationally 
well-treated around the world such as English, 
Spanish, French or German. 
Comparable corpora came as a possible solu-
tion to the problem of scarcity of parallel corpora 
with the promise that it may serve as a seed for 
parallel data extraction. A general definition of 
comparability that we find operational is given by 
Munteanu and Marcu (2005). They say that a (bi-
lingual) comparable corpus is a set of paired doc-
uments that, while not parallel in the strict sense, 
are related and convey overlapping information.  
Current practices of automatically collecting 
domain-dependent bilingual comparable corpora 
from the Web usually begin with collecting a list 
of t terms as seed data in both the source and the 
target languages. Each term (in each language) is 
then queried on the most popular search engine and 
the first N document hits are retained. The final 
corpus will contain t ? N documents in each lan-
guage and in subsequent usage the document 
boundaries are often disregarded. 
At this point, it is important to stress out the 
importance of the pairing of documents in a com-
parable corpus. Suppose that we want to word-
align a bilingual comparable corpus consisting of 
M documents per language, each with k words, 
using the IBM-1 word alignment algorithm (Brown 
et al, 1993). This algorithm searches for each 
source word, the target words that have a maxi-
mum translation probability with the source word. 
Aligning all the words in our corpus with no regard 
to document boundaries, would yield a time com-
plexity of      operations. The alternative would 
be in finding a 1:p (with p a small positive integer, 
usually 1, 2 or 3) document assignment (a set of 
aligned document pairs) that would enforce the ?no 
search outside the document boundary? condition 
when doing word alignment with the advantage of 
reducing the time complexity to      operations. 
When M is large, the reduction may actually be 
vital to getting a result in a reasonable amount of 
time. The downside of this simplification is the 
loss of information: two documents may not be 
correctly aligned thus depriving the word-
alignment algorithm of the part of the search space 
that would have contained the right alignments. 
128
Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 128?135,
49th Annual Meeting of the Association for Computational Linguistics,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Word alignment forms the basis of the phrase 
alignment procedure which, in turn, is the basis of 
any statistical translation model. A comparable 
corpus differs essentially from a parallel corpus by 
the fact that textual units do not follow a transla-
tion order that otherwise greatly reduces the word 
alignment search space in a parallel corpus. Given 
this limitation of a comparable corpus in general 
and the sizes of the comparable corpora that we 
will have to deal with in particular,  we have de-
vised one variant of an Expectation Maximization 
(EM) algorithm (Dempster et al, 1977) that gener-
ates a 1:1 (p = 1) document assignment from a par-
allel and/or comparable corpus using only pre-
existing translation lexicons. Its generality would 
permit it to perform the same task on other textual 
units such as paragraphs or sentences. 
In what follows, we will briefly review the lit-
erature discussing document/paragraph alignment 
and then we will present the derivation of the EM 
algorithm that generates 1:1 document alignments. 
We will end the article with a thorough evaluation 
of the performances of this algorithm and the con-
clusions that arise from these evaluations. 
2 Related Work 
Document alignment and other types of textual 
unit alignment have been attempted in various sit-
uations involving extracting parallel data from 
comparable corpora. The first case study is offered 
by Munteanu and Marcu (2002). They align sen-
tences in an English-French comparable corpus of 
1.3M of words per language by comparing suffix 
trees of the sentences. Each sentence from each 
part of the corpus is encoded as a suffix tree which 
is a tree that stores each possible suffix of a string 
from the last character to the full string. Using this 
method, Munteanu and Marcu are able to detect 
correct sentence alignments with a precision of 
95% (out of 100 human-judged and randomly se-
lected sentences from the generated output). The 
running time of their algorithm is approximately 
100 hours for 50000 sentences in each of the lan-
guages. 
A popular method of aligning sentences in a 
comparable corpus is by classifying pairs of sen-
tences as parallel or not parallel. Munteanu and 
Marcu (2005) use a Maximum Entropy classifier 
for the job trained with the following features: sen-
tence lengths and their differences and ratios, per-
centage of the words in a source sentence that have 
translations in a target sentence (translations are 
taken from pre-existing translation lexicons), the 
top three largest fertilities, length of the longest 
sequence of words that have translations, etc. The 
training data consisted of a small parallel corpus of 
5000 sentences per language. Since the number of 
negative instances (50002 ? 5000) is far more large 
than the number of positive ones (5000), the nega-
tive training instances were selected randomly out 
of instances that passed a certain word overlap fil-
ter (see the paper for details). The classifier preci-
sion is around 97% with a recall of 40% at the 
Chinese-English task and around 95% with a recall 
of 41% for the Arabic-English task. 
Another case study of sentence alignment that 
we will present here is that of Chen (1993). He 
employs an EM algorithm that will find a sentence 
alignment in a parallel corpus which maximizes 
the translation probability for each sentence bead 
in the alignment. The translation probability to be 
maximized by the EM procedure considering each 
possible alignment  is given by 
 
 (     )   ( )? ([  
    
 ])
 
   
 
 
The following notations were used:   is the 
English corpus (a sequence of English sentences), 
  is the French corpus, [  
    
 ] is a sentence bead 
(a pairing of m sentences in English with n 
sentences in French),  ([  
    
 ]   [  
    
 ]) 
is the sentence alignment (a sequence of sentence 
beads) and p(L) is the probability that an alignment 
contains L beads. The obtained accuracy is around 
96% and was computed indirectly by checking 
disagreement with the Brown sentence aligner 
(Brown et al, 1991) on randomly selected 500 
disagreement cases. 
The last case study of document and sentence 
alignment from ?very-non-parallel corpora? is the 
work from Fung and Cheung (2004). Their contri-
bution to the problem of textual unit alignment 
resides in devising a bootstrapping mechanism in 
which, after an initial document pairing and conse-
quent sentence alignment using a lexical overlap-
ping similarity measure, IBM-4 model (Brown et 
al., 1993) is employed to enrich the bilingual dic-
tionary that is used by the similarity measure. The 
129
process is repeated until the set of identified 
aligned sentences does not grow anymore. The 
precision of this method on English-Chinese sen-
tence alignment is 65.7% (out of the top 2500 iden-
tified pairs). 
3 EMACC 
We propose a specific instantiation of the well-
known general EM algorithm for aligning different 
types of textual units: documents, paragraphs, and 
sentences which we will name EMACC (an acro-
nym for ?Expectation Maximization Alignment for 
Comparable Corpora?). We draw our inspiration 
from the famous IBM models (specifically from 
the IBM-1 model) for word alignment (Brown et 
al., 1993) where the translation probability (eq. (5)) 
is modeled through an EM algorithm where the 
hidden variable a models the assignment (1:1 word 
alignments) from the French sequence of words (? 
indexes) to the English one. 
By analogy, we imagined that between two sets 
of documents (from now on, we will refer to doc-
uments as our textual units but what we present 
here is equally applicable ? but with different per-
formance penalties ? to paragraphs and/or sentenc-
es) ? let?s call them   and  , there is an assignment 
(a sequence of 1:1 document correspondences1), 
the distribution of which can be modeled by a hid-
den variable   taking values in the set {true, false}. 
This assignment will be largely determined by the 
existence of word translations between a pair of 
documents, translations that can differentiate be-
tween one another in their ability to indicate a cor-
rect document alignment versus an incorrect one. 
In other words, we hypothesize that there are cer-
tain pairs of translation equivalents that are better 
indicators of a correct document correspondence 
than other translation equivalents pairs. 
We take the general formulation and derivation 
of the EM optimization problem from (Borman, 
2009). The general goal is to optimize  (   ), that 
is to find the parameter(s)   for which  (   ) is 
maximum. In a sequence of derivations that we are 
not going to repeat here, the general EM equation 
is given by: 
                                                          
1 Or ?alignments? or ?pairs?. These terms will be used with 
the same meaning throughout the presentation. 
    
       
 
? (      )    (     )
 
 (1) 
where  ?  (      )   . At step n+1, we try to 
obtain a new parameter      that is going to max-
imize (the maximization step) the sum over z (the 
expectation step) that in its turn depends on the 
best parameter    obtained at step n. Thus, in 
principle, the algorithm should iterate over the set 
of all possible   parameters, compute the expecta-
tion expression for each of these parameters and 
choose the parameter(s) for which the expression 
has the largest value. But as we will see, in prac-
tice, the set of all possible parameters has a dimen-
sion that is exponential in terms of the number of 
parameters. This renders the problem intractable 
and one should back off to heuristic searches in 
order to find a near-optimal solution. 
We now introduce a few notations that we will 
operate with from this point forward. We suggest 
to the reader to frequently refer to this section in 
order to properly understand the next equations: 
?   is the set of source documents,     is the 
cardinal of this set; 
?   is the set of target documents with     its 
cardinal; 
?     is a pair of documents,      and 
    ; 
?    is a pair of translation equivalents 
?     ? such that    is a lexical item that 
belongs to    and    is a lexical item that 
belongs to   ; 
?   is the set of all existing translation 
equivalents pairs ?     ?.   is the transla-
tion probability score (as the one given for 
instance by GIZA++ (Gao and Vogel, 
2008)). We assume that GIZA++ transla-
tion lexicons already exist for the pair of 
languages of interest. 
In order to tie equation 1 to our problem, we de-
fine its variables as follows: 
?   is the sequence of 1:1 document align-
ments of the form              ,     
{              }. We call   an assign-
ment which is basically a sequence of 1:1 
document alignments. If there are     1:1 
document alignments in   and if        , 
then the set of all possible assignments has 
130
the cardinal equal to     (
   
   
) where n! is 
the factorial function of the integer n and 
.
 
 
/ is the binomial coefficient. It is clear 
now that with this kind of dimension of the 
set of all possible assignments (or   pa-
rameters), we cannot simply iterate over it 
in order to choose the assignment that 
maximizes the expectation; 
?   *          + is the hidden variable that 
signals if a pair of documents     repre-
sents a correct alignment (true) or not 
(false); 
?   is the sequence of translation equivalents 
pairs    from T in the order they appear 
in each document pair from  . 
Having defined the variables in equation 1 this 
way, we aim at maximizing the translation equiva-
lents probability over a given assignment,  (   ). 
In doing so, through the use of the hidden variable 
z, we are also able to find the 1:1 document align-
ments that attest for this maximization. 
We proceed by reducing equation 1 to a form 
that is readily amenable to software coding. That 
is, we aim at obtaining some distinct probability 
tables that are going to be (re-)estimated by the 
EM procedure. Due to the lack of space, we omit 
the full derivation and directly give the general 
form of the derived EM equation 
           
 
,   (   )     (      )- (2) 
Equation 2 suggests a method of updating the as-
signment probability  (      )  with the lexical 
alignment probability  (   ) in an effort to pro-
vide the alignment clues that will ?guide? the as-
signment probability towards the correct 
assignment. All it remains to do now is to define 
the two probabilities. 
The lexical document alignment probability 
 (   ) is defined as follows: 
 (   )  ?
?  (   |   )     
      
     
 (3) 
where  (       )  is the simplified lexical docu-
ment alignment probability which is initially equal 
to  (   ) from the set  . This probability is to be 
read as ?the contribution    makes to the correct-
ness of the     alignment?. We want that the 
alignment contribution of one translation equiva-
lents pair    to distribute over the set of all possi-
ble document pairs thus enforcing that 
?  (   |   )   
    {              }
 (4) 
The summation over   in equation 3 is actually 
over all translation equivalents pairs that are to be 
found only in the current     document pair and 
the presence of the product        ensures that we 
still have a probability value. 
The assignment probability  (      ) is also 
defined in the following way: 
 
 (      )  ?  (        )
     
 (5) 
for which we enforce the condition: 
?  (        )   
    {             }
 
(6) 
Using equations 2, 3 and 5 we deduce the final, 
computation-ready EM equation 
     
       
 
[  ?
?  (       )     
      
     
   ?  (        )
     
]
       
 
? [  
?  (       )     
      
     
    (        )] 
(7) 
As it is, equation 7 suggests an exhaustive search 
in the set of all possible   parameters, in order to 
find the parameter(s) for which the expression that 
is the argument of ?argmax? is maximum. But, as 
we know from section 3, the size of this this set is 
prohibitive to the attempt of enumerating each   
assignment and computing the expectation expres-
sion. Our quick solution to this problem was to 
directly construct the ?best?   assignment2 using a 
                                                          
2 We did not attempt to find the mathematical maximum of the 
expression from equation 7 and we realize that the conse-
131
greedy algorithm: simply iterate over all possible 
1:1 document pairs and for each document pair 
    {              }  compute the align-
ment count (it?s not a probability so we call it a 
?count? following IBM-1 model?s terminology) 
 
  
?  (   |   )     
      
    (        ) 
 
Then, construct the best 1:1 assignment      by 
choosing those pairs     for which we have counts 
with the maximum values. Before this cycle 
(which is the basic EM cycle) is resumed, we per-
form the following updates: 
 (        )   (        )
 
?  (   |   )     
      
 
(7a) 
 
 (   |   )  ?  (   |   )
        
 (7b) 
and normalize the two probability tables with 
equations 6 and 4. The first update is to be inter-
preted as the contribution the lexical document 
alignment probability makes to the alignment 
probability. The second update equation aims at 
boosting the probability of a translation equivalent 
if and only if it is found in a pair of documents be-
longing to the best assignment so far. In this way, 
we hope that the updated translation equivalent 
will make a better contribution to the discovery of 
a correct document alignment that has not yet been 
discovered at step n + 1. 
Before we start the EM iterations, we need to 
initialize the probability tables  (        ) and 
 (   |   ) . For the second table we used the 
GIZA++ scores that we have for the     translation 
equivalents pairs and normalized the table with 
equation 4. For the first probability table we have 
(and tried) two choices: 
? (D1) a uniform distribution: 
 
      
; 
? (D2) a lexical document alignment meas-
ure  (   ) (values between 0 and 1) that is 
computed directly from a pair of docu-
                                                                                           
quence of this choice and of the greedy search procedure is not 
finding the true optimum. 
ments     using the    translation equiva-
lents pairs from the dictionary  : 
 (   )  
 
?    (  )        ?    (  )        
        
 
(8) 
where      is the number of words in document    
and    (  ) is the frequency of word    in docu-
ment    (please note that, according to section 3, 
    is not a random pair of words, but a pair of 
translation equivalents). If every word in the 
source document has at least one translation (of a 
given threshold probability score) in the target 
document, then this measure is 1. We normalize 
the table initialized using this measure with equa-
tion 6. 
EMACC finds only 1:1 textual units alignments 
in its present form but a document pair     can be 
easily extended to a document bead following the 
example from (Chen, 1993). The main difference 
between the algorithm described by Chen and ours 
is that the search procedure reported there is inva-
lid for comparable corpora in which no pruning is 
available due to the nature of the corpus. A second 
very important difference is that Chen only relies 
on lexical alignment information, on the parallel 
nature of the corpus and on sentence lengths corre-
lations while we add the probability of the whole 
assignment which, when initially set to the D2 dis-
tribution, produces a significant boost of the preci-
sion of the alignment. 
4 Experiments and Evaluations 
The test data for document alignment was com-
piled from the corpora that was previously collect-
ed in the ACCURAT project3 and that is known to 
the project members as the ?Initial Comparable 
Corpora? or ICC for short. It is important to know 
the fact that ICC contains all types of comparable 
corpora from parallel to weakly comparable docu-
ments but we classified document pairs in three 
classes: parallel (class name: p), strongly compa-
rable (cs) and weakly comparable (cw). We have 
considered the following pairs of languages: Eng-
lish-Romanian (en-ro), English-Latvian (en-lv), 
English-Lithuanian (en-lt), English-Estonian (en-
et), English-Slovene (en-sl) and English-Greek 
                                                          
3 http://www.accurat-project.eu/ 
132
(en-el). For each pair of languages, ICC also con-
tains a Gold Standard list of document alignments 
that were compiled by hand for testing purposes. 
We trained GIZA++ translation lexicons for 
every language pair using the DGT-TM4 corpus. 
The input texts were converted from their Unicode 
encoding to UTF-8 and were tokenized using a 
tokenizer web service described by Ceau?u (2009). 
Then, we applied a parallel version of GIZA++ 
(Gao and Vogel, 2008) that gave us the translation 
dictionaries of content words only (nouns, verbs, 
adjective and adverbs) at wordform level. For Ro-
manian, Lithuanian, Latvian, Greek and English, 
we had lists of inflectional suffixes which we used 
to stem entries in respective dictionaries and pro-
cessed documents. Slovene remained the only lan-
guage which involved wordform level processing. 
The accuracy of EMACC is influenced by three 
parameters whose values have been experimentally 
set: 
? the threshold over which we use transla-
tion equivalents from the dictionary   for 
textual unit alignment; values for this 
threshold (let?s name it ThrGiza) are 
from the ordered set *             +; 
? the threshold over which we decide to up-
date the probabilities of translation equiva-
lents with equation 7b; values for this 
threshold (named ThrUpdate) are from 
the same ordered set *             +; 
? the top ThrOut% alignments from the 
best assignment found by EMACC. This 
parameter will introduce precision and re-
call with the ?perfect? value for recall 
equal to ThrOut%. Values for this pa-
rameter are from the set *         +. 
We ran EMACC (10 EM steps) on every possible 
combination of these parameters for the pairs of 
languages in question on both initial distributions 
D1 and D2. For comparison, we also performed a 
baseline document alignment using the greedy al-
gorithm of EMACC with the equation 8 supplying 
the document similarity measure. The following 4 
tables report a synthesis of the results we have ob-
tained which, because of the lack of space, we 
cannot give in full. We omit the results of EMACC 
with D1 initial distribution because the accuracy 
                                                          
4 http://langtech.jrc.it/DGT-TM.html 
figures (both precision and recall) are always lower 
(10-20%) than those of EMACC with D2. 
cs P/R Prms. P/R Prms. # 
en-
ro 
1/ 
0.69047 
0.4 
0.4 
0.7 
0.85714/ 
0.85714 
0.4 
0.4 
1 
42 
en-
sl 
0.96666/ 
0.28807 
0.4 
0.4 
0.3 
0.83112/ 
0.83112 
0.4 
0.4 
1 
302 
en-
el 
0.97540/ 
0.29238 
0.001 
0.8 
0.3 
0.80098/ 
0.80098 
0.001 
0.4 
1 
407 
en-
lt 
0.97368/ 
0.29191 
0.4 
0.8 
0.3 
0.72978/ 
0.72978 
0.4 
0.4 
1 
507 
en-
lv 
0.95757/ 
0.28675 
0.4 
0.4 
0.3 
0.79854/ 
0.79854 
0.001 
0.8 
1 
560 
en-
et 
0.88135/ 
0.26442 
0.4 
0.8 
0.3 
0.55182/ 
0.55182 
0.4 
0.4 
1 
987 
Table 1: EMACC with D2 initial distribution on strong-
ly comparable corpora 
 
cs P/R Prms. P/R Prms. # 
en-
ro 
1/ 
0.69047 
0.4 
0.7 
0.85714/ 
0.85714 
0.4 
1 
42 
en-
sl 
0.97777/ 
0.29139 
0.001 
0.3 
0.81456/ 
0.81456 
0.4 
0.1 
302 
en-
el 
0.94124/ 
0.28148 
0.001 
0.3 
0.71851/ 
0.71851 
0.001 
1 
407 
en-
lt 
0.95364/ 
0.28514 
0.001 
0.3 
0.72673/ 
0.72673 
0.001 
1 
507 
en-
lv 
0.91463/ 
0.27322 
0.001 
0.3 
0.80692/ 
0.80692 
0.001 
1 
560 
en-
et 
0.87030/ 
0.26100 
0.4 
0.3 
0.57727/ 
0.57727 
0.4 
1 
987 
Table 2: D2 baseline algorithm on strongly comparable 
corpora 
 
cw P/R Prms. P/R Prms. # 
en-
ro 
1/ 
0.29411 
0.4 
0.001 
0.3 
0.66176/ 
0.66176 
0.4 
0.001 
1 
68 
en-
sl 
0.73958/ 
0.22164 
0.4 
0.4 
0.3 
0.42767/ 
0.42767 
0.4 
0.4 
1 
961 
en-
el 
0.15238/ 
0.04545 
0.001 
0.8 
0.3 
0.07670/ 
0.07670 
0.001 
0.8 
1 
352 
en-
lt 
0.55670/ 
0.16615 
0.4 
0.8 
0.3 
0.28307/ 
0.28307 
0.4 
0.8 
1 
325 
en-
lv 
0.23529/ 
0.07045 
0.4 
0.4 
0.3 
0.10176/ 
0.10176 
0.4 
0.4 
1 
511 
en-
et 
0.59027/ 
0.17634 
0.4 
0.8 
0.3 
0.27800/ 
0.27800 
0.4 
0.8 
1 
483 
Table 3: EMACC with D2 initial distribution on weakly 
comparable corpora 
133
cw P/R Prms. P/R Prms. # 
en-
ro 
0.85/ 
0.25 
0.4 
0.3 
0.61764/ 
0.61764 
0.4 
1 
68 
en-
sl 
0.65505/ 
0.19624 
0.4 
0.3 
0.39874/ 
0.39874 
0.4 
1 
961 
en-
el 
0.11428/ 
0.03428 
0.4 
0.3 
0.06285/ 
0.06285 
0.4 
1 
352 
en-
lt 
0.60416/ 
0.18012 
0.4 
0.3 
0.24844/ 
0.24844 
0.4 
1 
325 
en-
lv 
0.13071/ 
0.03921 
0.4 
0.3 
0.09803/ 
0.09803 
0.4 
1 
511 
en-
et 
0.48611/ 
0.14522 
0.001 
0.3 
0.25678/ 
0.25678 
0.4 
1 
483 
Table 4: D2 baseline algorithm on weakly comparable 
corpora 
 
In every table above, the P/R column gives the 
maximum precision and the associated recall 
EMACC was able to obtain for the corresponding 
pair of languages using the parameters (Prms.) 
from the next column. The P/R column gives the 
maximum recall with the associated precision that 
we obtained for that pair of languages.  
The Prms. columns contain parameter settings 
for EMACC (see Tables 1 and 3) and for the D2 
baseline algorithm (Tables 2 and 4): in Tables 1 
and 3 values for ThrGiza, ThrUpdate and 
ThrOut are given from the top (of the cell) to the 
bottom and in Tables 2 and 4 values of ThrGiza 
and ThrOut are also given from top to bottom 
(the ThrUpdate parameter is missing because the 
D2 baseline algorithm does not do re-estimation). 
The # column contains the size of the test set: the 
number of documents in each language that have to 
be paired. The search space is # * # and the gold 
standard contains # pairs of human aligned docu-
ment pairs.  
To ease comparison between EMACC and the 
D2 baseline for each type of corpora (strongly and 
weakly comparable), we grayed maximal values 
between the two: either the precision in the P/R 
column or the recall in the P/R column. 
In the case of strongly comparable corpora (Ta-
bles 1 and 2), we see that the benefits of re-
estimating the probabilities of the translation 
equivalents (based on which we judge document 
alignments) begin to emerge with precisions for all 
pairs of languages (except en-sl) being better than 
those obtained with the D2 baseline. But the real 
benefit of re-estimating the probabilities of transla-
tion equivalents along the EM procedure is visible 
from the comparison between Tables 3 and 4. Thus, 
in the case of weakly comparable corpora, in 
which EMACC with the D2 distribution is clearly 
better than the baseline (with the only exception of 
en-lt precision), due to the significant decrease in 
the lexical overlap, the EM procedure is able to 
produce important alignment clues in the form of 
re-estimated (bigger) probabilities of translation 
equivalents that, otherwise, would have been ig-
nored. 
It is important to mention the fact that the re-
sults we obtained varied a lot with values of the 
parameters ThrGiza and ThrUpdate. We ob-
served, for the majority of studied language pairs, 
that lowering the value for ThrGiza and/or 
ThrUpdate (0.1, 0.01, 0.001?), would negative-
ly impact the performance of EMACC due to the 
fact of introducing noise in the initial computation 
of the D2 distribution and also on re-estimating 
(increasing) probabilities for irrelevant translation 
equivalents. At the other end, increasing the 
threshold for these parameters (0.8, 0.85, 0.9?) 
would also result in performance decreasing due to 
the fact that too few translation equivalents (be 
they all correct) are not enough to pinpoint correct 
document alignments since there are great chances 
for them to actually appear in all document pairs. 
So, we have experimentally found that there is a 
certain balance between the degree of correctness 
of translation equivalents and their ability to pin-
point correct document alignments. In other words, 
the paradox resides in the fact that if a certain pair 
of translation equivalents is not correct but the re-
spective words appear only in documents which 
correctly align to one another, that pair is very im-
portant to the alignment process. Conversely, if a 
pair of translation equivalents has a very high 
probability score (thus being correct) but appears 
in almost every possible pair of documents, that 
pair is not informative to the alignment process and 
must be excluded. We see now that the EMACC 
aims at finding the set of translation equivalents 
that is maximally informative with respect to the 
set of document alignments. 
We have introduced the ThrOut parameter in 
order to have better precision. This parameter actu-
ally instructs EMACC to output only the top (ac-
cording to the alignment score probability 
 (        )) ThrOut% of the document align-
ments it has found. This means that, if all are cor-
rect, the maximum recall can only be ThrOut%. 
134
But another important function of ThrOut is to 
restrict the translation equivalents re-estimation 
(equation 7b) for only the top ThrOut% align-
ments. In other words, only the probabilities of 
translation equivalents that are to be found in top 
ThrOut% best alignments in the current EM step 
are re-estimated. We introduced this restriction in 
order to confine translation equivalents probability 
re-estimation to correct document alignments 
found so far. 
Regarding the running time of EMACC, we can 
report that on a cluster with a total of 32 CPU 
cores (4 nodes) with 6-8 GB of RAM per node, the 
total running time is between 12h and 48h per lan-
guage pair (about 2000 documents per language) 
depending on the setting of the various parameters. 
5 Conclusions 
The whole point in developing textual unit align-
ment algorithms for comparable corpora is to be 
able to provide good quality quasi-aligned data to 
programs that are specialized in extracting parallel 
data from these alignments. In the context of this 
paper, the most important result to note is that 
translation probability re-estimation is a good tool 
in discovering new correct textual unit alignments 
in the case of weakly related documents. We also 
tested EMACC at the alignment of 200 parallel 
paragraphs (small texts of no more than 50 words) 
for all pairs of languages that we have considered 
here. We can briefly report that the results are bet-
ter than the strongly comparable document align-
ments from Tables 1 and 2 which is a promising 
result because one would think that a significant 
reduction in textual unit size would negatively im-
pact the alignment accuracy. 
Acknowledgements 
This work has been supported by the ACCURAT 
project (http://www.accurat-project.eu/) funded by 
the European Community?s Seventh Framework 
Program (FP7/2007-2013) under the Grant Agree-
ment n? 248347. It has also been partially support-
ed by the Romanian Ministry of Education and 
Research through the STAR project (no. 
742/19.01.2009). 
 
 
References 
 
Borman, S. 2009. The Expectation Maximization Algo-
rithm. A short tutorial. Online at: 
http://www.isi.edu/natural-
language/teaching/cs562/2009/readings/B06.pdf 
Brown, P. F., Lai, J. C., and Mercer, R. L. 1991. Align-
ing sentences in parallel corpora. In Proceedings of 
the 29th Annual Meeting of the Association for 
Computational Linguistics, pp. 169?176, June 8-21, 
1991, University of California, Berkeley, California, 
USA. 
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mer-
cer, R. L. 1993. The Mathematics of Statistical Ma-
chine Translation: Parameter Estimation. 
Computational Linguistics, 19(2): 263?311. 
Ceau?u, A. 2009. Statistical Machine Translation for 
Romanian. PhD Thesis, Romanian Academy (in Ro-
manian). 
Chen, S. F. 1993. Aligning Sentences in Bilingual Cor-
pora Using Lexical Information. In Proceedings of 
the 31st Annual Meeting on Association for Compu-
tational Linguistics, pp. 9?16, Columbus, Ohio, 
USA. 
Dempster, A. P., Laird, N. M., and Rubin, D. B. 1977. 
Maximum likelihood from incomplete data via the 
EM algorithm. Journal of the Royal Statistical Socie-
ty, 39(B):1?38. 
Fung, P., and Cheung, P. 2004. Mining Very-Non-
Parallel Corpora: Parallel Sentence and Lexicon Ex-
traction via Bootstrapping and EM. In Proceedings of 
EMNLP 2004, Barcelona, Spain: July 2004. 
Gao, Q., and Vogel, S. 2008.  Parallel implementations 
of word alignment tool.  ACL-08 HLT: Software En-
gineering, Testing, and Quality Assurance for Natu-
ral Language Processing, pp. 49?57, June 20, 2008, 
The Ohio State University, Columbus, Ohio, USA.  
Munteanu, D. S., and Marcu, D. 2002. Processing com-
parable corpora with bilingual suffix trees. In Pro-
ceedings of the 2002 Conference on Empirical 
Methods in Natural Language Processing (EMNLP 
2002), pp. 289?295, July 6-7, 2002, University of 
Pennsylvania, Philadelphia, USA. 
Munteanu, D. S., and Marcu, D. 2005. Improving ma-
chine translation performance by exploiting non-
parallel corpora. Computational Linguistics, 
31(4):477?504. 
135
