Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 280?287,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Learning with Annotation Noise
Eyal Beigman
Olin Business School
Washington University in St. Louis
beigman@wustl.edu
Beata Beigman Klebanov
Kellogg School of Management
Northwestern University
beata@northwestern.edu
Abstract
It is usually assumed that the kind of noise
existing in annotated data is random clas-
sification noise. Yet there is evidence
that differences between annotators are not
always random attention slips but could
result from different biases towards the
classification categories, at least for the
harder-to-decide cases. Under an annota-
tion generation model that takes this into
account, there is a hazard that some of the
training instances are actually hard cases
with unreliable annotations. We show
that these are relatively unproblematic for
an algorithm operating under the 0-1 loss
model, whereas for the commonly used
voted perceptron algorithm, hard training
cases could result in incorrect prediction
on the uncontroversial cases at test time.
1 Introduction
It is assumed, often tacitly, that the kind of
noise existing in human-annotated datasets used in
computational linguistics is random classification
noise (Kearns, 1993; Angluin and Laird, 1988),
resulting from annotator attention slips randomly
distributed across instances. For example, Os-
borne (2002) evaluates noise tolerance of shallow
parsers, with random classification noise taken to
be ?crudely approximating annotation errors.? It
has been shown, both theoretically and empiri-
cally, that this type of noise is tolerated well by
the commonly used machine learning algorithms
(Cohen, 1997; Blum et al, 1996; Osborne, 2002;
Reidsma and Carletta, 2008).
Yet this might be overly optimistic. Reidsma
and op den Akker (2008) show that apparent dif-
ferences between annotators are not random slips
of attention but rather result from different biases
annotators might have towards the classification
categories. When training data comes from one
annotator and test data from another, the first an-
notator?s biases are sometimes systematic enough
for a machine learner to pick them up, with detri-
mental results for the algorithm?s performance on
the test data. A small subset of doubly anno-
tated data (for inter-annotator agreement check)
and large chunks of singly annotated data (for
training algorithms) is not uncommon in compu-
tational linguistics datasets; such a setup is prone
to problems if annotators are differently biased.1
Annotator bias is consistent with a number of
noise models. For example, it could be that an
annotator?s bias is exercised on each and every in-
stance, making his preferred category likelier for
any instance than in another person?s annotations.
Another possibility, recently explored by Beigman
Klebanov and Beigman (2009), is that some items
are really quite clear-cut for an annotator with any
bias, belonging squarely within one particular ca-
tegory. However, some instances ? termed hard
cases therein ? are harder to decide upon, and this
is where various preferences and biases come into
play. In a metaphor annotation study reported by
Beigman Klebanov et al (2008), certain markups
received overwhelming annotator support when
people were asked to validate annotations after a
certain time delay. Other instances saw opinions
split; moreover, Beigman Klebanov et al (2008)
observed cases where people retracted their own
earlier annotations.
To start accounting for such annotator behavior,
Beigman Klebanov and Beigman (2009) proposed
a model where instances are either easy, and then
all annotators agree on them, or hard, and then
each annotator flips his or her own coin to de-
1The different biases might not amount to much in the
small doubly annotated subset, resulting in acceptable inter-
annotator agreement; yet when enacted throughout a large
number of instances they can be detrimental from a machine
learner?s perspective.
280
cide on a label (each annotator can have a different
?coin? reflecting his or her biases). For annota-
tions generated under such a model, there is a dan-
ger of hard instances posing as easy ? an observed
agreement between annotators being a result of all
coins coming up heads by chance. They therefore
define the expected proportion of hard instances in
agreed items as annotation noise. They provide
an example from the literature where an annota-
tion noise rate of about 15% is likely.
The question addressed in this article is: How
problematic is learning from training data with an-
notation noise? Specifically, we are interested in
estimating the degree to which performance on
easy instances at test time can be hurt by the pre-
sence of hard instances in training data.
Definition 1 The hard case bias, ? , is the portion
of easy instances in the test data that are misclas-
sified as a result of hard instances in the training
data.
This article proceeds as follows. First, we show
that a machine learner operating under a 0-1 loss
minimization principle could sustain a hard case
bias of ?( 1?
N
) in the worst case. Thus, while an-
notation noise is hazardous for small datasets, it is
better tolerated in larger ones. However, 0-1 loss
minimization is computationally intractable for
large datasets (Feldman et al, 2006; Guruswami
and Raghavendra, 2006); substitute loss functions
are often used in practice. While their tolerance to
random classification noise is as good as for 0-1
loss, their tolerance to annotation noise is worse.
For example, the perceptron family of algorithms
handle random classification noise well (Cohen,
1997). We show in section 3.4 that the widely
used Freund and Schapire (1999) voted percep-
tron algorithm could face a constant hard case bias
when confronted with annotation noise in training
data, irrespective of the size of the dataset. Finally,
we discuss the implications of our findings for the
practice of annotation studies and for data utiliza-
tion in machine learning.
2 0-1 Loss
Let a sample be a sequence x1, . . . , xN drawn uni-
formly from the d-dimensional discrete cube Id =
{?1, 1}d with corresponding labels y1, . . . , yN ?
{?1, 1}. Suppose further that the learning al-
gorithm operates by finding a hyperplane (w,?),
w ? Rd, ? ? R, that minimizes the empirical er-
rorL(w,?) =
?
j=1...N [yj?sgn(
?
i=1...d x
i
jw
i?
?)]2. Let there be H hard cases, such that the an-
notation noise is ? = HN .
2
Theorem 1 In the worst case configuration of in-
stances a hard case bias of ? = ?( 1?
N
) cannot be
ruled out with constant confidence.
Idea of the proof : We prove by explicit con-
struction of an adversarial case. Suppose there is
a plane that perfectly separates the easy instances.
The ?(N) hard instances will be concentrated in
a band parallel to the separating plane, that is
near enough to the plane so as to trap only about
?(
?
N) easy instances between the plane and the
band (see figure 1 for an illustration). For a ran-
dom labeling of the hard instances, the central
limit theorem shows there is positive probability
that there would be an imbalance between +1 and
?1 labels in favor of ?1s on the scale of
?
N ,
which, with appropriate constants, would lead to
the movement of the empirically minimal separa-
tion plane to the right of the hard case band, mis-
classifying the trapped easy cases.
Proof : Let v = v(x) =
?
i=1...d x
i denote the
sum of the coordinates of an instance in Id and
take ?e =
?
d ? F?1(
?
? ? 2?
d
2 + 12) and ?h =?
d ? F?1(? +
?
? ? 2?
d
2 + 12), where F (t) is the
cumulative distribution function of the normal dis-
tribution. Suppose further that instances xj such
that ?e < vj < ?h are all and only hard instances;
their labels are coinflips. All other instances are
easy, and labeled y = y(x) = sgn(v). In this case,
the hyperplane 1?
d
(1 . . . 1) is the true separation
plane for the easy instances, with ? = 0. Figure 1
shows this configuration.
According to the central limit theorem, for d,N
large, the distribution of v is well approximated by
N (0,
?
d). If N = c1 ? 2d, for some 0 < c1 < 4,
the second application of the central limit the-
orem ensures that, with high probability, about
?N = c1?2d items would fall between ?e and ?h
(all hard), and
?
? ? 2?
d
2N = c1
?
?2d would fall
between 0 and ?e (all easy, all labeled +1).
Let Z be the sum of labels of the hard cases,
Z =
?
i=1...H yi. Applying the central limit the-
orem a third time, for large N , Z will, with a
high probability, be distributed approximately as
2In Beigman Klebanov and Beigman (2009), annotation
noise is defined as percentage of hard instances in the agreed
annotations; this implies noise measurement on multiply an-
notated material. When there is just one annotator, no dis-
tinction between easy vs hard instances can be made; in this
sense, all hard instances are posing as easy.
281
0 ?e ?h
Figure 1: The adversarial case for 0-1 loss.
Squares correspond to easy instances, circles ? to
hard ones. Filled squares and circles are labeled
?1, empty ones are labeled +1.
N (0,
?
?N). This implies that a value as low as
?2? cannot be ruled out with high (say 95%) con-
fidence. Thus, an imbalance of up to 2
?
?N , or of
2
?
c1?2d, in favor of ?1s is possible.
There are between 0 and ?h about 2
?
c1
?
?2d
more?1 hard instances than +1 hard instances, as
opposed to c1
?
?2d easy instances that are all +1.
As long as c1 < 2
?
c1, i.e. c1 < 4, the empirically
minimal threshold would move to ?h, resulting in
a hard case bias of ? =
?
?
?
c12d
(1??)?c12d
= ?( 1?
N
).
To see that this is the worst case scenario, we
note that 0-1 loss sustained on ?(N) hard cases
is the order of magnitude of the possible imba-
lance between ?1 and +1 random labels, which
is ?(
?
N). For hard case loss to outweigh the loss
on the misclassified easy instances, there cannot
be more than ?(
?
N) of the latter 2
Note that the proof requires that N = ?(2d)
namely, that asymptotically the sample includes
a fixed portion of the instances. If the sample is
asymptotically smaller, then ?e will have to be ad-
justed such that ?e =
?
d ? F?1(?( 1?
N
) + 12).
According to theorem 1, for a 10K dataset with
15% hard case rate, a hard case bias of about 1%
cannot be ruled out with 95% confidence.
Theorem 1 suggests that annotation noise as
defined here is qualitatively different from more
malicious types of noise analyzed in the agnostic
learning framework (Kearns and Li, 1988; Haus-
sler, 1992; Kearns et al, 1994), where an adver-
sary can not only choose the placement of the hard
cases, but also their labels. In worst case, the 0-1
loss model would sustain a constant rate of error
due to malicious noise, whereas annotation noise
is tolerated quite well in large datasets.
3 Voted Perceptron
Freund and Schapire (1999) describe the voted
perceptron. This algorithm and its many vari-
ants are widely used in the computational lin-
guistics community (Collins, 2002a; Collins and
Duffy, 2002; Collins, 2002b; Collins and Roark,
2004; Henderson and Titov, 2005; Viola and
Narasimhan, 2005; Cohen et al, 2004; Carreras
et al, 2005; Shen and Joshi, 2005; Ciaramita and
Johnson, 2003). In this section, we show that the
voted perceptron can be vulnerable to annotation
noise. The algorithm is shown below.
Algorithm 1 Voted Perceptron
Training
Input: a labeled training set (x1, y1), . . . , (xN , yN )
Output: a list of perceptrons w1, . . . , wN
Initialize: t? 0; w1 ? 0; ?1 ? 0
for t = 1 . . . N do
y?t ? sign(?wt, xt?+ ?t)
wt+1 ? wt + yt?y?t2 ? xt
?t+1 ? ?t + yt?y?t2 ? ?wt, xt?
end for
Forecasting
Input: a list of perceptrons w1, . . . , wN
an unlabeled instance x
Output: A forecasted label y
y? ?
PN
t=1 sign(?wt, xt?+ ?t)
y ? sign(y?)
The voted perceptron algorithm is a refinement
of the perceptron algorithm (Rosenblatt, 1962;
Minsky and Papert, 1969). Perceptron is a dy-
namic algorithm; starting with an initial hyper-
plane w0, it passes repeatedly through the labeled
sample. Whenever an instance is misclassified
by wt, the hyperplane is modified to adapt to the
instance. The algorithm terminates once it has
passed through the sample without making any
classification mistakes. The algorithm terminates
iff the sample can be separated by a hyperplane,
and in this case the algorithm finds a separating
hyperplane. Novikoff (1962) gives a bound on the
number of iterations the algorithm goes through
before termination, when the sample is separable
by a margin.
282
The perceptron algorithm is vulnerable to noise,
as even a little noise could make the sample in-
separable. In this case the algorithm would cycle
indefinitely never meeting termination conditions,
wt would obtain values within a certain dynamic
range but would not converge. In such setting,
imposing a stopping time would be equivalent to
drawing a random vector from the dynamic range.
Freund and Schapire (1999) extend the percep-
tron to inseparable samples with their voted per-
ceptron algorithm and give theoretical generaliza-
tion bounds for its performance. The basic idea
underlying the algorithm is that if the dynamic
range of the perceptron is not too large then wt
would classify most instances correctly most of
the time (for most values of t). Thus, for a sample
x1, . . . , xN the new algorithm would keep track
of w0, . . . , wN , and for an unlabeled instance x it
would forecast the classification most prominent
amongst these hyperplanes.
The bounds given by Freund and Schapire
(1999) depend on the hinge loss of the dataset. In
section 3.2 we construct a difficult setting for this
algorithm. To prove that voted perceptron would
suffer from a constant hard case bias in this set-
ting using the exact dynamics of the perceptron is
beyond the scope of this article. Instead, in sec-
tion 3.3 we provide a lower bound on the hinge
loss for a simplified model of the perceptron algo-
rithm dynamics, which we argue would be a good
approximation to the true dynamics in the setting
we constructed. For this simplified model, we
show that the hinge loss is large, and the bounds
in Freund and Schapire (1999) cannot rule out a
constant level of error regardless of the size of the
dataset. In section 3.4 we study the dynamics of
the model and prove that ? = ?(1) for the adver-
sarial setting.
3.1 Hinge Loss
Definition 2 The hinge loss of a labeled instance
(x, y) with respect to hyperplane (w,?) and mar-
gin ? > 0 is given by ? = ?(?, ?) = max(0, ? ?
y ? (?w, x? ? ?)).
? measures the distance of an instance from
being classified correctly with a ?margin. Figure 2
shows examples of hinge loss for various data
points.
Theorem 2 (Freund and Schapire (1999))
After one pass on the sample, the probability
that the voted perceptron algorithm does not
? ?? ??? ?
Figure 2: Hinge loss ? for various data points in-
curred by the separator with margin ?.
predict correctly the label of a test instance
xN+1 is bounded by 2N+1EN+1
[
d+D
?
]2
where
D = D(w,?, ?) =
?
?N
i=1 ?
2
i .
This result is used to explain the convergence of
weighted or voted perceptron algorithms (Collins,
2002a). It is useful as long as the expected value of
D is not too large. We show that in an adversarial
setting of the annotation noise D is large, hence
these bounds are trivial.
3.2 Adversarial Annotation Noise
Let a sample be a sequence x1, . . . , xN drawn uni-
formly from Id with y1, . . . , yN ? {?1, 1}. Easy
cases are labeled y = y(x) = sgn(v) as before,
with v = v(x) =
?
i=1...d x
i. The true separation
plane for the easy instances is w? = 1?
d
(1 . . . 1),
?? = 0. Suppose hard cases are those where
v(x) > c1
?
d, where c1 is chosen so that the
hard instances account for ?N of all instances.3
Figure 3 shows this setting.
3.3 Lower Bound on Hinge Loss
In the simplified case, we assume that the algo-
rithm starts training with the hyperplane w0 =
w? = 1?
d
(1 . . . 1), and keeps it throughout the
training, only updating ?. In reality, each hard in-
stance can be decomposed into a component that is
parallel to w?, and a component that is orthogonal
to it. The expected contribution of the orthogonal
3See the proof of 0-1 case for a similar construction using
the central limit theorem.
283
0 c1?d
Figure 3: An adversarial case of annotation noise
for the voted perceptron algorithm.
component to the algorithm?s update will be posi-
tive due to the systematic positioning of the hard
cases, while the contributions of the parallel com-
ponents are expected to cancel out due to the sym-
metry of the hard cases around the main diagonal
that is orthogonal to w?. Thus, while wt will not
necessarily parallel w?, it will be close to parallel
for most t > 0. The simplified case is thus a good
approximation of the real case, and the bound we
obtain is expected to hold for the real case as well.
For any initial value ?0 < 0 all misclassified in-
stances are labeled ?1 and classified as +1, hence
the update will increase ?0, and reach 0 soon
enough. We can therefore assume that ?t ? 0
for any t > t0 where t0  N .
Lemma 3 For any t > t0, there exist ? =
?(?, T ) > 0 such that E(?2) ? ? ? ?.
Proof : For ? ? 0 there are two main sources
of hinge loss: easy +1 instances that are clas-
sified as ?1, and hard -1 instances classified as
+1. These correspond to the two components of
the following sum (the inequality is due to disre-
garding the loss incurred by a correct classification
with too wide a margin):
E(?2) ?
[?]?
l=0
1
2d
(
d
l
)
(
?
?
d
?
l
?
d
+ ?)2
+
1
2
d?
l=c1
?
d
1
2d
(
d
l
)
(
l
?
d
?
?
?
d
+ ?)2
Let 0 < T < c1 be a parameter. For ? > T
?
d,
misclassified easy instances dominate the loss:
E(?2) ?
[?]?
l=0
1
2d
(
d
l
)
(
?
?
d
?
l
?
d
+ ?)2
?
[T
?
d]?
l=0
1
2d
(
d
l
)
(
T
?
d
?
d
?
l
?
d
+ ?)2
?
T
?
d?
l=0
1
2d
(
d
l
)
(T ?
l
?
d
+ ?)2
?
1
?
2pi
? T
0
(T + ? ? t)2e?t
2/2dt = HT (?)
The last inequality follows from a normal ap-
proximation of the binomial distribution (see, for
example, Feller (1968)).
For 0 ? ? ? T
?
d, misclassified hard cases
dominate:
E(?2) ?
1
2
d?
l=c1
?
d
1
2d
(
d
l
)
(
l
?
d
?
?
?
d
+ ?)2
?
1
2
d?
l=c1
?
d
1
2d
(
d
l
)
(
l
?
d
?
T
?
d
?
d
+ ?)2
?
1
2
?
1
?
2pi
? ?
??1(?)
(t? T + ?)2e?t
2/2dt
= H?(?)
where ??1(?) is the inverse of the normal distri-
bution density.
Thus E(?2) ? min{HT (?),H?(?)}, and
there exists ? = ?(?, T ) > 0 such that
min{HT (?),H?(?)} ? ? ? ? 2
Corollary 4 The bound in theorem 2 does not
converge to zero for large N .
We recall that Freund and Schapire (1999) bound
is proportional to D2 =
?N
i=1 ?
2
i . It follows from
lemma 3 that D2 = ?(N), hence the bound is in-
effective.
3.4 Lower Bound on ? for Voted Perceptron
Under Simplified Dynamics
Corollary 4 does not give an estimate on the hard
case bias. Indeed, it could be that wt = w? for
almost every t. There would still be significant
hinge in this case, but the hard case bias for the
voted forecast would be zero. To assess the hard
case bias we need a model of perceptron dyna-
mics that would account for the history of hyper-
planesw0, . . . , wN the perceptron goes through on
284
a sample x1, . . . , xN . The key simplification in
our model is assuming that wt parallels w? for all
t, hence the next hyperplane depends only on the
offset ?t. This is a one dimensional Markov ran-
dom walk governed by the distribution
P(?t+1??t = r|?t) = P(x|
yt ? y?t
2
??w?, x? = r)
In general ?d ? ?t ? d but as mentioned before
lemma 3, we may assume ?t > 0.
Lemma 5 There exists c > 0 such that with a high
probability ?t > c ?
?
d for most 0 ? t ? N .
Proof : Let c0 = F?1(
?
2 +
1
2); c1 = F
?1(1??).
We designate the intervals I0 = [0, c0 ?
?
d]; I1 =
[c0 ?
?
d, c1 ?
?
d] and I2 = [c1 ?
?
d, d] and define
Ai = {x : v(x) ? Ii} for i = 0, 1, 2. Note that the
constants c0 and c1 are chosen so that P(A0) =
?
2
and P(A2) = ?. It follows from the construction
in section 3.2 that A0 and A1 are easy instances
and A2 are hard. Given a sample x1, . . . , xN , a
misclassification of xt ? A0 by ?t could only hap-
pen when an easy +1 instance is classified as ?1.
Thus the algorithm would shift ?t to the left by
no more than |vt ? ?t| since vt = ?w?, xt?. This
shows that ?t ? I0 implies ?t+1 ? I0. In the
same manner, it is easy to verify that if ?t ? Ij
and xt ? Ak then ?t+1 ? Ik, unless j = 0 and
k = 1, in which case ?t+1 ? I0 because xt ? A1
would be classified correctly by ?t ? I0.
We construct a Markov chain with three states
a0 = 0, a1 = c0 ?
?
d and a2 = c1 ?
?
d governed
by the following transition distribution:
?
?
?
?
1? ?2 0
?
2
?
2 1? ?
?
2
?
2
1
2 ?
3?
2
1
2 + ?
?
?
?
?
Let Xt be the state at time t. The principal eigen-
vector of the transition matrix (13 ,
1
3 ,
1
3) gives the
stationary probability distribution of Xt. Thus
Xt ? {a1, a2} with probability 23 . Since the tran-
sition distribution of Xt mirrors that of ?t, and
since aj are at the leftmost borders of Ij , respec-
tively, it follows that Xt ? ?t for all t, thus
Xt ? {a1, a2} implies ?t ? I1?I2. It follows that
?t > c0 ?
?
d with probability 23 , and the lemma
follows from the law of large numbers 2
Corollary 6 With high probability ? = ?(1).
Proof : Lemma 5 shows that for a sample
x1, . . . , xN with high probability ?t is most of
the time to the right of c ?
?
d. Consequently
for any x in the band 0 ? v ? c ?
?
d we get
sign(?w?, x?+?t) = ?1 for most t hence by defi-
nition, the voted perceptron would classify such
an instance as ?1, although it is in fact a +1 easy
instance. Since there are ?(N) misclassified easy
instances, ? = ?(1) 2
4 Discussion
In this article we show that training with annota-
tion noise can be detrimental for test-time results
on easy, uncontroversial instances; we termed this
phenomenon hard case bias. Although under
the 0-1 loss model annotation noise can be tole-
rated for larger datasets (theorem 1), minimizing
such loss becomes intractable for larger datasets.
Freund and Schapire (1999) voted perceptron al-
gorithm and its variants are widely used in compu-
tational linguistics practice; our results show that
it could suffer a constant rate of hard case bias ir-
respective of the size of the dataset (section 3.4).
How can hard case bias be reduced? One pos-
sibility is removing as many hard cases as one
can not only from the test data, as suggested in
Beigman Klebanov and Beigman (2009), but from
the training data as well. Adding the second an-
notator is expected to detect about half the hard
cases, as they would surface as disagreements be-
tween the annotators. Subsequently, a machine
learner can be told to ignore those cases during
training, reducing the risk of hard case bias. While
this is certainly a daunting task, it is possible that
for annotation studies that do not require expert
annotators and extensive annotator training, the
newly available access to a large pool of inexpen-
sive annotators, such as the Amazon Mechanical
Turk scheme (Snow et al, 2008),4 or embedding
the task in an online game played by volunteers
(Poesio et al, 2008; von Ahn, 2006) could provide
some solutions.
Reidsma and op den Akker (2008) suggest a
different option. When non-overlapping parts of
the dataset are annotated by different annotators,
each classifier can be trained to reflect the opinion
(albeit biased) of a specific annotator, using dif-
ferent parts of the datasets. Such ?subjective ma-
chines? can be applied to a new set of data; an
item that causes disagreement between classifiers
is then extrapolated to be a case of potential dis-
agreement between the humans they replicate, i.e.
4http://aws.amazon.com/mturk/
285
a hard case. Our results suggest that, regardless
of the success of such an extrapolation scheme in
detecting hard cases, it could erroneously invali-
date easy cases: Each classifier would presumably
suffer from a certain hard case bias, i.e. classify
incorrectly things that are in fact uncontroversial
for any human annotator. If each such classifier
has a different hard case bias, some inter-classifier
disagreements would occur on easy cases. De-
pending on the distribution of those easy cases in
the feature space, this could invalidate valuable
cases. If the situation depicted in figure 1 corre-
sponds to the pattern learned by one of the clas-
sifiers, it would lead to marking the easy cases
closest to the real separation boundary (those be-
tween 0 and ?e) as hard, and hence unsuitable for
learning, eliminating the most informative mate-
rial from the training data.
Reidsma and Carletta (2008) recently showed
by simulation that different types of annotator
behavior have different impact on the outcomes of
machine learning from the annotated data. Our re-
sults provide a theoretical analysis that points in
the same direction: While random classification
noise is tolerable, other types of noise ? such as
annotation noise handled here ? are more proble-
matic. It is therefore important to develop models
of annotator behavior and of the resulting imper-
fections of the annotated datasets, in order to di-
agnose the potential learning problem and suggest
mitigation strategies.
References
Dana Angluin and Philip Laird. 1988. Learning from
Noisy Examples. Machine Learning, 2(4):343?370.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Com-
putational Linguistics, accepted for publication.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing Disagreements. In
COLING 2008 Workshop on Human Judgments in
Computational Linguistics, pages 2?7, Manchester,
UK.
Avrim Blum, Alan Frieze, Ravi Kannan, and Santosh
Vempala. 1996. A Polynomial-Time Algorithm for
Learning Noisy Linear Threshold Functions. In Pro-
ceedings of the 37th Annual IEEE Symposium on
Foundations of Computer Science, pages 330?338,
Burlington, Vermont, USA.
Xavier Carreras, Llu?is Ma`rquez, and Jorge Castro.
2005. Filtering-Ranking Perceptron Learning for
Partial Parsing. Machine Learning, 60(1):41?71.
Massimiliano Ciaramita and Mark Johnson. 2003. Su-
persense Tagging of Unknown Nouns in WordNet.
In Proceedings of the Empirical Methods in Natural
Language Processing Conference, pages 168?175,
Sapporo, Japan.
William Cohen, Vitor Carvalho, and Tom Mitchell.
2004. Learning to Classify Email into ?Speech
Acts?. In Proceedings of the Empirical Methods
in Natural Language Processing Conference, pages
309?316, Barcelona, Spain.
Edith Cohen. 1997. Learning Noisy Perceptrons by
a Perceptron in Polynomial Time. In Proceedings
of the 38th Annual Symposium on Foundations of
Computer Science, pages 514?523, Miami Beach,
Florida, USA.
Michael Collins and Nigel Duffy. 2002. New Ranking
Algorithms for Parsing and Tagging: Kernels over
Discrete Structures, and the Voted Perceptron. In
Proceedings of the 40th Annual Meeting on Associa-
tion for Computational Linguistics, pages 263?370,
Philadelphia, USA.
Michael Collins and Brian Roark. 2004. Incremen-
tal Parsing with the Perceptron Algorithm. In Pro-
ceedings of the 42nd Annual Meeting on Associa-
tion for Computational Linguistics, pages 111?118,
Barcelona, Spain.
Michael Collins. 2002a. Discriminative Training
Methods for Hidden Markov Hodels: Theory and
Experiments with Perceptron Algorithms. In Pro-
ceedings of the Empirical Methods in Natural Lan-
guage Processing Conference, pages 1?8, Philadel-
phia, USA.
Michael Collins. 2002b. Ranking Algorithms for
Named Entity Extraction: Boosting and the Voted
Perceptron. In Proceedings of the 40th Annual
Meeting on Association for Computational Linguis-
tics, pages 489?496, Philadelphia, USA.
Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and
Ashok Ponnuswami. 2006. New Results for Learn-
ing Noisy Parities and Halfspaces. In Proceedings
of the 47th Annual IEEE Symposium on Foundations
of Computer Science, pages 563?574, Los Alamitos,
CA, USA.
William Feller. 1968. An Introduction to Probability
Theory and Its Application, volume 1. Wiley, New
York, 3rd edition.
Yoav Freund and Robert Schapire. 1999. Large Mar-
gin Classification Using the Perceptron Algorithm.
Machine Learning, 37(3):277?296.
Venkatesan Guruswami and Prasad Raghavendra.
2006. Hardness of Learning Halfspaces with Noise.
In Proceedings of the 47th Annual IEEE Symposium
on Foundations of Computer Science, pages 543?
552, Los Alamitos, CA, USA.
286
David Haussler. 1992. Decision Theoretic General-
izations of the PAC Model for Neural Net and other
Learning Applications. Information and Computa-
tion, 100(1):78?150.
James Henderson and Ivan Titov. 2005. Data-Defined
Kernels for Parse Reranking Derived from Proba-
bilistic Models. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguis-
tics, pages 181?188, Ann Arbor, Michigan, USA.
Michael Kearns and Ming Li. 1988. Learning in the
Presence of Malicious Errors. In Proceedings of the
20th Annual ACM symposium on Theory of Comput-
ing, pages 267?280, Chicago, USA.
Michael Kearns, Robert Schapire, and Linda Sellie.
1994. Toward Efficient Agnostic Learning. Ma-
chine Learning, 17(2):115?141.
Michael Kearns. 1993. Efficient Noise-Tolerant
Learning from Statistical Queries. In Proceedings
of the 25th Annual ACM Symposium on Theory of
Computing, pages 392?401, San Diego, CA, USA.
Marvin Minsky and Seymour Papert. 1969. Percep-
trons: An Introduction to Computational Geometry.
MIT Press, Cambridge, Mass.
A. B. Novikoff. 1962. On convergence proofs on per-
ceptrons. Symposium on the Mathematical Theory
of Automata, 12:615?622.
Miles Osborne. 2002. Shallow Parsing Using Noisy
and Non-Stationary Training Material. Journal of
Machine Learning Research, 2:695?719.
Massimo Poesio, Udo Kruschwitz, and Chamberlain
Jon. 2008. ANAWIKI: Creating Anaphorically An-
notated Resources through Web Cooperation. In
Proceedings of the 6th International Language Re-
sources and Evaluation Conference, Marrakech,
Morocco.
Dennis Reidsma and Jean Carletta. 2008. Reliability
measurement without limit. Computational Linguis-
tics, 34(3):319?326.
Dennis Reidsma and Rieks op den Akker. 2008. Ex-
ploiting Subjective Annotations. In COLING 2008
Workshop on Human Judgments in Computational
Linguistics, pages 8?16, Manchester, UK.
Frank Rosenblatt. 1962. Principles of Neurodynamics:
Perceptrons and the Theory of Brain Mechanisms.
Spartan Books, Washington, D.C.
Libin Shen and Aravind Joshi. 2005. Incremen-
tal LTAG Parsing. In Proceedings of the Human
Language Technology Conference and Empirical
Methods in Natural Language Processing Confer-
ence, pages 811?818, Vancouver, British Columbia,
Canada.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and Fast ? But is it
Good? Evaluating Non-Expert Annotations for Nat-
ural Language Tasks. In Proceedings of the Empir-
ical Methods in Natural Language Processing Con-
ference, pages 254?263, Honolulu, Hawaii.
Paul Viola and Mukund Narasimhan. 2005. Learning
to Extract Information from Semi-Structured Text
Using a Discriminative Context Free Grammar. In
Proceedings of the 28th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 330?337, Salvador,
Brazil.
Luis von Ahn. 2006. Games with a purpose. Com-
puter, 39(6):92?94.
287
Proceedings of the NAACL HLT Workshop on Computational Approaches to Linguistic Creativity, pages 1?8,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Discourse Topics and Metaphors
Beata Beigman Klebanov
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Daniel Diermeier
Northwestern University
d-diermeier@northwestern.edu
Abstract
Using metaphor-annotated material that is
sufficiently representative of the topical
composition of a similar-length document in
a large background corpus, we show that
words expressing a discourse-wide topic of
discussion are less likely to be metaphorical
than other words in a document. Our
results suggest that to harvest metaphors more
effectively, one is advised to consider words
that do not represent a discourse topic.
Traditionally, metaphor detectors use the
observation that a metaphorically used item creates
a local incongruity because there is a violation
of a selectional restriction, such as providing a
non-vehicle object to the verb derail in Protesters
derailed the conference. Current state of art
in metaphor detection therefore tends to be
?localistic? ? the distributional profile of the target
word in its immediate grammatical or collocational
context in a background corpus or a database
like WordNet is used to determine metaphoricity
(Mason, 2004; Krishnakumaran and Zhu, 2007;
Birke and Sarkar, 2006; Gedigian et al, 2006; Fass,
1991).
However, some theories of metaphor postulate
certain features of metaphors that connect it to the
surrounding text beyond the small grammatical or
proximal locality. For example, for Kittay (1987)
metaphor is a discourse phenomenon; although
the minimal metaphoric unit is a clause, often
much larger chunks of text constitute a metaphor.
Consider, for example, the TRAIN metaphor in the
following excerpt from a Sunday Times article on
20 September 1992:
Thatcher warned EC leaders to stop their
endless round of summits and take notice
of their own people. ?There is a fear that
the European train will thunder forward,
laden with its customary cargo of gravy,
towards a destination neither wished for
nor understood by electorates. But the
train can be stopped,? she said.
In the example above, the quotation is not in itself
a metaphor, as there is no indication that something
other than the actual train is being discussed (and
so no local incongruities exist). Only when situated
in the context prepared by the first sentence (and
indeed the rest of the article), the train imagery
becomes a metaphor.
According to Kittay, a metaphor occurs when
a semantic field is used to discuss a different
content domain. The theory therefore predicts that a
metaphorically used semantic domain would be off-
topic in the given document.
Although a single document can have singular,
idiosyncratic topics, it is likelier to discuss a mix of
topics that are typical of the discourse of which it is
part. We therefore derive the following hypothesis:
Words in a given document that represent a common
topic of discussion in a corpus of relevant documents
would be predominantly non-metaphorical. That is,
a smaller share of metaphorically used words in a
document would fall in such topical words than the
share of topical words in the document.
We test this hypothesis in the current article.
1
Using a large background corpus, we estimate
the topical composition of the target documents
(section 1) that were annotated for metaphors
(section 2). We then report the results of the
experiment (section 3) that strongly support the
hypothesis, and discuss the findings (section 4). The
concluding section provides a summary and outlines
the significance of the results for the practice of
metaphor detection.
1 Topic identification
1.1 EUI corpus
Our aimwas to create a large corpus of British media
discourse regarding the emerging European Union
institutions, with both Euro-phile and Euro-sceptic
camps represented. Our corpus consists of 12,814
articles drawn from three British newspapers: The
Guardian (34%), The Times (38%), and The
Independent (28%), dating from 1990 to 2000.
We used LexisNexis Academic1 to search for the
Subject index term European Union Institutions
(henceforth, EUI).2 After results are retrieved, we
further narrow them down to only documents on the
subject European Union Institutions in the detailed
subject index of the retrieved results.3,4
1.2 Identification of discourse topics
We converted all 12,858 documents5 (henceforth,
EUI+M corpus) into plain text format and removed
1http://academic.lexisnexis.com/online-services/academic-
features.aspx
2In LexisNexis subject index hierarchy: Government
and Public Administration/International Organization and
Bodies/International Governmental Organizations/European
Union Institutions.
3In the initial search, an article that scores 72% on the
subject would be retrieved, but it would not be classified as
being on this subject, and so would not be included in the final
dataset. Articles in the final dataset tend to score about 90% on
the subject, according to LexisNexis index.
4There is a gap in LexisNexis? index coverage of The
Times during 1996-7 and of The Independent during 2000. To
avoid under-representation of the newspaper and of the relevant
years in the sample, we added articles returned for the search
SECTION(Home news) AND (European Union OR Brussels)
on The Times 01/1996 through 04/1998, and SECTION(News
AND NOT Foreign) AND (European Union OR Brussels) on
The Independent throughout 2000.
512,814 EUI corpus plus 44 documents annotated for
metaphors, to be described in section 2.
words from a list of 153 common function words.
We then constructed an indexing vocabulary V that
included all and only words that (a) contained only
letters; and (b) appeared at least 6 times in the
collection. All documents were indexed using this
21,046 word vocabulary. We will designate all the
indexed words in document i as Di.
To identify the main discourse topics in
the EUI+M corpus, we submitted the indexed
documents to an unsupervised clustering method
Latent Dirichlet Allocation (Blei et al, 2003)
(henceforth, LDA).6 The designation of the clusters
as topics is supported by findings reported in Blei
et al (2003) that the clusters contain information
relevant for topic discrimination. Additionally,
Chanen and Patrick (2007) show that LDA achieves
significant correlations with humans on a topic
characterization task, where humans produced not
just a topic classification but also identified phrases
they believed were indicative of each class.
Using the default settings of LDA
implementation,7 we analyzed the corpus into
100 topics. Table 1 exemplifies some of the
emergent topics.
1.3 Topical words in a text
LDA is a generative model of text. According to its
outlook, every text is about a small (typically 5-7)
number of topics, and each indexed word in the text
belongs to one of these topics. However, in many
cases, the relationship between the word and the
topic is quite tentative, as the word is not particularly
likely given the topic. We therefore use parameter k
to control topic assignments ? we only take LDA?s
assignment of word to topic if the word is in the
top k most likely words for that topic. For k=25,
about 15% of in-vocabulary words in a document
are assigned to a topic; for k=400, about half the
in-vocabulary words are assigned to some topic. We
designate byTki all indexed words in document i that
are assigned to some topic for the given value of k.
The ratio |Tki ||Di| describes the proportion of discoursetopical words in the indexed words for the given
document.
6No stemming was performed.
7downloaded from http://www.cs.princeton.edu/?blei/lda-c/
2
Table 1: Examples of topics identified by LDA in the
EUI+M corpus. All words are taken from top 25 most
likely words given the topic. We boldface one word per
cluster, that could provide, in our view, an appropriate
label for the cluster.
foreign nato military war russian defence soviet
piece un kosovo sanctions bosnia moscow
rail tunnel transport train pounds channel eurostar
ferry trains passengers services paris eurotunnel
countries europe enlargement new membership
members eastern conference reform voting summit
commission foreign join poland negotiations
parliament mep party socialist strasbourg christian
vote leader labour conservative right political green
democrat elections epp
television commission satellite tv broadcasting
tickets film broadcasters bbc programmes media
industry channel public directive
court article justice member directive treaty
question provisions case law regulation judgment
interpretation rules order proceedings
social workers employment working hours
jobs week employers legislation unions
employees chapter rights health minimum
bank central euro monetary rates currency
interest bundesbank markets economic exchange
finance inflation dollar german
players football clubs uefa league fifa game cup
fishing fish fishermen fisheries quota vessels
boats waters sea fleet
racism racist ethnic xenophobia black minorities
jury discrimination white relations
drugs patent research human companies genetic
scientists health medical biotechnology disease
children parents punishment school rights family
childcare corporal education law father mother
controls immigration border asylum checks
passport police citizens crime europol
energy nuclear emissions oil electricity gas
environment carbon tax pollution fuel global cut
commission fraud commissioners brussels report
allegations officials inquiry meps corruption
mismanagement staff santer
2 Metaphor annotation
Ideally, we should have sampled a small sub-corpus
from the EUI corpus for metaphor annotation;
however, the choice of the data for annotation
predated the construction of the EUI corpus.
Our interest being in the way metaphors used
in public discourse help shape attitudes towards
a complex, ongoing and fateful political reality,
we came across Musolff?s (2000) work on the
British discourse on the European integration
process throughout the 1990s. Working in the
corpus linguistics tradition, Musolff (2000) studied
a number of metaphors recurrent in this discourse,
making available a selection of materials he used,
marked with the metaphors.8
One caveat to directly using the database is the
lack of clarity regarding the metaphor annotation
procedure. In particular, the author does not
report how many people participated, or any inter-
annotator agreement figures. We therefore chose
4 out of Musolff?s list of source domains, took
all articles corresponding to them (128 documents),
along with 23 articles from other source domains,
and submitted them to a group of 8 undergraduate
annotators, on top of Musolff?s original markup that
is treated as another annotator.
Annotators received the following instructions,
reflecting our focus on the persuasive use of
metaphor, as part of an argument:
Generally speaking, a metaphor is a
linguistic expression whereby something
is compared to something else that it is
clearly literally not, in order to make a
point. Thus, in Tony Blair?s famous ?I
haven?t got a reverse gear?, Tony Blair
is compared to a car in order to stress
his unwillingness/inability to retract his
statements or actions. We would say in
this case that a metaphor from a VEHICLE
domain is used. In this study we will
consider metaphors from 4 domains.
For the 4 chosen domains we provided the
following descriptions, along with 2 examples for
each:
8available from http://www.dur.ac.uk/andreas.musolff/Arcindex.htm
3
AUTHORITY Metaphors that have to do with
discipline and authority, like school, religion,
royalty, asylum, prison, etc.
LOVE Metaphors from love/romance and family.
BUILD Metaphors that have to do with building
(the process) and houses and other buildings or
constructions, their parts and uses.
VEHICLE Metaphors that have to do with land-
borne vehicles, their parts, operation and
maintenance.
People were instructed to mark every paragraph
where a metaphor from a given domain occurs. They
were also asked to provide a comment that briefly
summarizes the ground for their decision, saying
what is being compared to what.9
Table 2 shows the inter-annotator agreement
figures.
Table 2: Inter-annotator agreement, measured on 2364
paragraphs (151 documents).11
Source Domain of Metaphor ?
LOVE 0.66
VEHICLE 0.66
AUTHORITY 0.39
BUILD 0.43
LOVE and VEHICLE are close to acceptable
reliability, with the other two types scoring low.
In order to understand the nature of disagreements,
we submitted the annotated materials plus some
random annotations to 7 out of the original 8 people
for validation, 4-8 weeks after they completed
the annotations, asking them to accept or reject
9In the topics vs metaphors experiment, we test the
hypothesis on words rather than paragraphs. For metaphors
from a pre-specified domain, such as VEHICLE or LOVE, it
was usually clear which words in the paragraph belong to the
domain and are used metaphorically. People?s comments often
explicitly used words from the paragraph, or made it otherwise
clear through their description. For OpenMeta phase (please see
below), where people were asked to mark metaphors from any
source domain, they were also asked to single out the words in
the paragraph that witness the metaphor, and these are the words
used in the current experiment.
11These are results for binary classification for each metaphor
type rather than a multiclass classification, since some articles
have more than one type and some have none.
metaphor markups. We found that metaphors
initially marked by at least 4 people (out of 9) were
accepted as valid by people who did not initially
mark them in 91% of the cases, on average across
the metaphor types. These are thus uncontroversial
cases, with the missing annotations likely due to
attention slips rather than to genuine differences of
opinion. Metaphors initially marked by 1-3 people
were more controversial, with the average validation
rate of 41% (Beigman Klebanov et al, 2008).
Evidently, some of the metaphors are clearer-
cut than others, yet even the more difficult cases
got non-negligible support at validation time from
people who did not initially mark them. We
therefore decided to regard the whole of the
annotated data as valid for the purpose of the current
research. Our focus is on finding metaphors (recall),
and less on making sure all candidate metaphors are
acceptable to all annotators; it suffices to know that
even the minority opinion often finds support.
In the second stage of the research, we expanded
the repertoire of the metaphor types to include
additional source domains, mainly from Musolff?s
list. The dataset has so far been subjected to
non-expert annotations by a group of the total of
15 undergraduate students. Metaphors from the
source domains of VEHICLE, LOVE, BUILDING,
AUTHORITY, WAR, SHOW, SCHOOL, RELIGION,
MEDICINE were annotated by different subsets of
the students.
The outcome of the second stage of the project is
not sufficient for addressing the issue of discourse
topics vs metaphors, however, as there are instances
of metaphors in the text that do not fall into any
of the source domains singled out by Musolff as
recurrent ones in the discourse under consideration.
We are now at an early stage of the third phrase
we call OpenMeta, where annotators are asked to
mark all metaphors they can detect, not confining
themselves to a given list of source domains.
Only annotators who participated in the previous,
type-constrained, version of the task participate in
OpenMeta project. So far, we have 44 documents
annotated by 3 people for open-domain metaphors.
This subset features as full a coverage of all
metaphors used in the documents as we were able
to obtain so far, and it is going to serve as test data
for the topics vs metaphors hypothesis.
4
Our test set is thus biased towards recurrent
metaphorical domains (those named by Musolff),
and towards metaphors that are relatively salient
to a naive reader, from recurrent or other source
domains. Metaphors marked in the test data are
those afforded a high degree of rhetorical presence
in the discourse ? either quantitatively, because
they are repeated and elaborated, or qualitatively,
because they are striking enough to arrest the
naive reader?s attention. According to the Presence
Theory in rhetoric (Perelman and Olbrechts-Tyteca,
1969; Gross and Dearin, 2003; Atkinson et al,
2008), elements afforded high presence are key to
the rhetorical design of the argument. These are
not so much metaphors we live by without even
noticing, such as those often studied in Conceptual
Metaphor literature, like VALUE AS SIZE or TIME
AS SPACE; these are metaphors that are clearly a
matter of the author?s conscious choice, closest in
the current theorizing to Steen?s (2008) notion of
deliberate metaphors.
2.1 Pseudo sampling
The annotated data is not really a sample of the
corpus. In fact, it is not known to us exactly how the
documents were chosen; although all 44 metaphor
annotated documents are from the newspapers and
dates participating in the EUI corpus, only 20% are
actually in the EUI corpus. How can we establish
that there is a fit between the EUI collection and
the annotated texts? We check how well discourse
topics cover the documents, in the corpus and in
the annotated material. Specifically, for a fixed
k, is there a difference in the |Tki ||Di| for annotateddocuments as opposed to the corpus at large? Using
a random sample of 50 documents from EUI corpus,
a 2-tailed t-test yielded p < 0.05, for all k, the
trend being towards a better coverage of the EUI
documents than of the metaphor annotated ones.
We hypothesized that this was due to the large
discrepancy in the lengths of the texts: An average
text in the EUI sample is 432 words long, whereas
the metaphor annotated texts are 775 words long on
average, with the shortest having 343 words. Shorter
texts tend to be less elaborate and more ?to the
point?, with a higher percentage of topical words.
To neutralize the effect of length on topical
coverage, we chose from the EUI sample only
documents that were at least 343 words long,
resulting in 31 documents. Comparing those to the
44 metaphor annotated documents, we found p >
0.37 for every k, i.e. the annotated documents are
indistinguishable in topical coverage from similar-
length documents in the EUI corpus.
3 Experiment
3.1 Summary of notation
V All and only non-stop words containing only
letters that appeared in at least 6 documents in
the collection.
Di All words in document i that are in V.
Tki All words in document i that are in V and are
in the top k words for some topic active in
document i according to LDA output.
Mi All words in document i that are in V and are
marked as metaphors in this document.
3.2 Hypothesis
We hypothesize that words in a given document
that are high-ranking representatives of a common
topic of discussion in a relevant corpus are less
likely to be metaphorical than other words in the
document. That is, such words would contain a
smaller proportion of metaphors than their share in
text. Using the definitions above: For an average
document i and any k, |Tki ||Di| >
|Mi?Tki |
|Mi| .
3.3 Results
As we hypothesized, metaphors are under-
represented in topically used words. Thus, for
k=25, about 15% of the indexed words in the
document are deemed topical, containing about
3% of the metaphorically used indexed words
in that document. For k=400, about 53% of the
indexed words are topical, capturing only 22% of
the metaphors.
4 Discussion
4.1 Metaphors from salient domains
A number of domains singled out by Musolff (2000)
as being recurrent metaphors in the corpus, such
5
0.000.100.20
0.300.400.50
0.60
25 50 100 150 200 250 300 350 400k
Figure 1: As hypothesized, |Tki ||Di| , shown in circles, is
larger than |Mi?Tki ||Mi| , shown in squares, for various k.
as VEHICLE or LOVE, are also things people care
about politically, hence they also correspond to
recurrent topics of discussion (see clusters titled
transport and childcare in table 1). It has been
shown experimentally that the subject?s in-depth
familiarity with the source domain is necessary
for the metaphor to work as intended ? see for
example Gentner and Gentner (1983) work on using
water flow metaphors for electricity. Our results
suggest that participants in political discourse draw
on domains not only familiar in general, but indeed
highly salient in the specific discourse itself.
As a consequence, an extended metaphor from a
discourse-topical domain can be easily mistaken by
the topic detection software for a topical use of the
relevant items. Consider, for example, an extract
from a 19 December 1991 article in Times:
Denis Healey, former Labour Chancellor
of the Exchequer, urged the primeminister
to stop playing Tory party politics with
the negotiations over Europe and drew an
image of Mr Major as a driver. He said:
?I understand that if you are driving a car
and sitting behind you is a lady with a
handbag and a man with fangs, you may
feel it wiser to drive in the slow lane. My
own advice is that he should pull into a
lay-by, turf the others out and then hand
the wheel over to firmer and safer hands.?
LDA considered {drive driving} to belong to
the topic that deals with safety and road accidents,
including in its 200 most likely words {crash
died accidents pedestrians traffic safety cars maps
motorists}, although additional metaphorically used
items from the same semantic domain, such as
lane and wheel, were not among the top 200
representatives of this topic.
It is an intriguing direction for future research
to compare the topical and metaphorical uses of
such domains, in order to determine which aspects
loom large indeed, being both matters of literal
concern and prolific generators of metaphors, and
how these are manipulated for persuasive effects.
The example above suggests that in the British EU-
related discourse in 1990s safety of driving is both
a topic-of-discussion (?Cyclists and pedestrians are
more vulnerable on British roads than anywhere else
in the European Union?, proclaims The Times on 18
February 2000) and a metaphorical axis, stressing
the importance of care and control, the hallmark
of the Euro-sceptic stance towards the European
integration process.
4.2 Topical metaphors
Putting aside topic detector?s mistakes on extended
metaphors from certain domains such as discussed
in the previous section, what do metaphors in the
topical vocabulary look like? The last topic shown
in table 1 has to do with criticism towards EU
bureaucracy, reflecting extensive discussions in the
British media in the late 1990s of alleged corruption
and mismanagement in the European Commission.
Together with the words cited in the table, this topic
lists root as one of its 300 most likely words.
This word shows up as a metaphor in 3 of our test
documents. In two of them it is used precisely in the
context projected by the topic:
In limpid language, whose meaning no
bureaucrat can twist, these four wise
men and one wise woman delivered, to
their great credit, a coruscating indictment
not just of individual commissioners, but
of the entire management and corporate
culture of the European Commission.
They have made an incontestable case, in
Tony Blair?s words, for ?root and branch
reform?.
6
Here, root is used in the root and branch idiom
suggesting a complete change, a reform, which
comes as part of a bundle with severe criticism.
Yet the figurative nature of this expression as a
metaphor from PLANT domain is apparent to naive
readers, making it an instance of imagery routinely
going together with criticism in this corpus. A
related metaphorical sense of root is attested in
similar contexts in the corpus, further explaining its
connection to the topic:
Not unless they insist on credible systems
to hold commissioners and bureaucrats to
account. And not unless they appoint
a new team with a brief not just to
root out malpractices but to shut down
entire programmes, such as tourism and
humanitarian aid, which the Commission
is incompetent to manage and which
should never have been added to its ever-
expanding empire.
A bloodied European Commission looks
likely to cling on to power today after
an eleventh-hour threat to quit by its
President, Jacques Santer, called the bluff
of the European Parliament ... All
week MEPs had been talking up the
?nuclear option? of sacking the full
Commission body over a burgeoning
fraud and nepotism scandal that dates
from 1995 ... Early 1997: Finnish
Commissioner Erkki Liikanen announces
plan to root out nepotism in Commission
and improve financial controls.
In the third document with root metaphor, root
is used in a different environment, and is not
considered topical by LDA:
For at the root of this conflict lies the
German denial that unemployment has
anything to do with cyclical fluctuations
in the economy.
Our quantitative results show that cases such
as root are more an exception than a rule. Yet,
from the perspective of the argumentative use of
metaphors, such cases are instructive of the way
certain metaphors get ?attached? to certain topics of
discussion. In this case, the majority of mentions
of root in this critical context come from Tony
Blair?s expression that was cited and referenced
widely enough to acquire a statistical association
with the discussion of the Commission?s failings
in the corpus. Indeed, the political significance of
Blair?s successful appropriation of the issue was not
lost on the media:
Tony Blair has swiftly positioned himself
as the champion of ?root and branch?
reform. Not to be outdone, William Hague
unveiled a ?10-point plan? for reform
of the Commission, no doubt drawing
on his extensive McKinsey management
expertise.
In future work, we plan to look closely at the
topical metaphors, as they potentially represent
outcomes of leadership battles fought in the media,
and can thus have political consequences.
5 Conclusion
Using metaphor-annotated material that is
sufficiently representative of the topical composition
of a similar-length document in a large background
corpus, we showed that words expressing a
discourse-wide topic of discussion are less likely to
be metaphorical than other words in a document.
This is, to our knowledge, the first quantitative
demonstration of the connection between
metaphoricity of a given word and its role in the
relevant background discourse. It complements the
traditionally ?localistic? outlook on metaphors that
is based on the observation that a metaphorically
used item creates a local incongruity because there
is a violation of a selectional restrictions between
verbs and their arguments (Fass, 1991; Mason,
2004; Gedigian et al, 2006; Birke and Sarkar, 2006)
or in the adjective-noun pairs (Krishnakumaran and
Zhu, 2007). Global discourse-level information
can potentially be used to focus metaphor detectors
operating at the local level on items with higher
metaphoric potential.
Reining and Lo?nneker-Rodman (2007) use
minimal topical information to focus their search
for metaphors. Working with a French-language
7
corpus discussing European politics, Reining and
Lo?nneker-Rodman (2007) proposed harvesting
salient collocates of the lemma Europe, that
represents the main topic of discussion and is
thus hypothesized to be the main target domain
of metaphors in this corpus. Indeed, numerous
instances of metaphors were collected using a
4-word window around the lemma in their corpus.
Our work can be understood as developing a
more nuanced approach to finding the likely target
domains in the corpus ? those words that represent
a topic of discussion rather than the means to
discuss a topic. Thus, it is not just Europe per se
that is the target, but, more specifically, aspects
such as monetary integration, employment, energy,
immigration, transportation, and defense, among
others. Our results suggest that to harvest deliberate
metaphors more effectively, one is advised to
consider words that do not represent a discourse
topic.
References
Nathan Atkinson, David Kaufer, and Suguru Ishizaki.
2008. Presence and Global Presence in Genres of Self-
Presentation: A Framework for Comparative Analysis.
Rhetoric Society Quarterly, 38(3):1?27.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing Disagreements. In COL-
ING 2008 Workshop on Human Judgments in Compu-
tational Linguistics, pages 2?7, Manchester, UK.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of nonlit-
eral language. In Proceedings of EACL, pages 329?
336.
David Blei, Andrew Ng, and Michael Jordan. 2003. La-
tent Dirichlet Allocation. Journal of Machine Learn-
ing Resarch, 3:993?1022.
Ari Chanen and Jon Patrick. 2007. Measuring correla-
tion between linguists judgments and Latent Dirichlet
Allocation topics. In Proceedings of the Australasian
Language Technology workshop, pages 13?20, Mel-
bourne, Australia.
Dan Fass. 1991. Met*: A method for discriminating
metonymy and metaphor by computer. Computational
Linguistics, 17(1):49?90.
Matt Gedigian, John Bryant, Srinivas Narayanan, and
Branimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of NAACL Workshop on Scalable Natural
Language Understanding, pages 41?48.
Deidre Gentner and Donald Gentner. 1983. Flowing wa-
ters or teeming crowds: Mental models of electricity.
In D. Gentner and A. Stevens, editors, Mental models.
Hillsdale, NJ: Lawrence Erlbaum.
Alan Gross and Ray Dearin. 2003. Chaim Perelman.
Albany: SUNY Press.
Eva Feder Kittay. 1987. Metaphor: Its cognitive force
and linguistic structure. Oxford: Calderon Press.
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of the Workshop on Computational
Approaches to Figurative Language, pages 13?20,
Rochester, New York.
Zachary J. Mason. 2004. CorMet: A computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
Andreas Musolff. 2000. Mirror images of Eu-
rope: Metaphors in the public debate about
Europe in Britain and Germany. Mu?nchen:
Iudicium. Annotated data is available at
http://www.dur.ac.uk/andreas.musolff/Arcindex.htm.
Chaim Perelman and Lucie Olbrechts-Tyteca. 1969. The
New Rhetoric: A Treatise on Argumentation. Wilkin-
son, J. and Weaver, P. (trans). Notre Dame, IN: Uni-
versity of Notre Dame Press.
Astrid Reining and Birte Lo?nneker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings of
the Workshop on Computational Approaches to Figu-
rative Language, pages 5?12, Rochester, New York.
Gerard Steen. 2008. The Paradox of Metaphor: Why
We Need a Three-Dimensional Model of Metaphor.
Metaphor and Symbol, 23(4):213?241.
8
Squibs
From Annotator Agreement to Noise Models
Beata Beigman Klebanov?
Northwestern University
Eyal Beigman??
Northwestern University
This article discusses the transition from annotated data to a gold standard, that is, a subset
that is sufficiently noise-free with high confidence. Unless appropriately reinterpreted, agreement
coefficients do not indicate the quality of the data set as a benchmarking resource: High overall
agreement is neither sufficient nor necessary to distill some amount of highly reliable data from
the annotated material. A mathematical framework is developed that allows estimation of the
noise level of the agreed subset of annotated data, which helps promote cautious benchmarking.
1. Introduction
By and large, the reason a computational linguist engages in an annotation project is to
build a reliable data set for the eventual testing, and possibly training, of an algorithm
performing the task. Hence, the crucial question regarding the annotated data set is
whether it is good for benchmarking.
For classification tasks, the current practice is to infer this information from the
value of an inter-annotator agreement coefficient such as the ? statistic (Cohen 1960;
Siegel and Castellan 1988; Carletta 1996). If agreement is high, the whole of the data set
is good for training and testing; the remaining disagreements are typically adjudicated
by an expert (Snyder and Palmer 2004; Palmer, Kingsbury, and Gildea 2005; Girju,
Badulescu, andMoldovan 2006) or through discussion (Litman, Hirschberg, and Swerts
2006), or, in case of more than two annotators, the majority label is chosen (Vieira and
Poesio 2000).1 There are some studies where cases of disagreement were removed from
test data (Markert and Nissim 2002; Dagan, Glickman, and Magnini 2006). If agreement
is low, the whole data set is discarded as unreliable. The threshold of acceptability seems
to have stabilized around ? = 0.67 (Carletta 1996; Di Eugenio and Glass 2004).
There is little understanding, however, of exactly how and how well the value of ?
reflects the quality of the data for benchmarking purposes. We develop a model of an-
notation generation that allows estimation of the level of noise in a specially constructed
gold standard. A gold standard with a noise figure supports cautious benchmarking,
? Kellogg School of Management, Northwestern University, Evanston, IL, beata@northwestern.edu.
?? Kellogg School of Management, Northwestern University, Evanston, IL, e-beigman@northwestern.edu.
1 In many studies, the procedure for handling disagreements is not clearly specified. For example, Gildea
and Jurafsky (2002) mention a ?consistency check?; in Lapata (2002), two annotators attained ? = 0.78 on
200 test instances, but it is not clear how cases of disagreements were settled.
Submission received: 30 June 2008; revised submission received: 3 December 2008; accepted for publication:
26 January 2009.
? 2009 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 4
by requiring that the performance of an algorithm be better than baseline by more than
that which can be attributed to noise. Articulating an annotation generation model also
allows us to shed light on the information ? can contribute to benchmarking.
2. Annotation Noise
We are interested in finding out which parts of the annotated data are sufficiently
reliable. This question presupposes a division of instances into two types: reliable
and unreliable, or, as we shall call them, easy and hard, under the assumption that
items that are easy are reliably annotated, whereas items that are hard display con-
fusion and disagreement. The plausibility of separation into easy and hard instances
is supported by researchers conducting annotation projects: ?With many judgments
that characterize natural language, one would expect that there are clear cases as well
as borderline cases that are more difficult to judge? (Wiebe, Wilson, and Cardie 2005,
page 200).
This suggests a model of annotation generation with latent variables for types, thus,
for every instance i, there is a variable li with values E (easy) and H (hard). Let n be the
number of instances, k the number of annotators, and Xij the classification of the ith in-
stance by the jth annotator. An annotation generationmodel assigns a functional form to
the joint distribution conditioned on the latent variable P(Xi1, . . . ,Xik|li). Similar models
have been studied in biometrics (Aickin 1990; Hui and Zhou 1998; Albert, McShane,
and Shih 2001; Albert and Dodd 2004). The main assumption is that, conditioned on the
type, annotators agree on easy instances and independently flip a coin on hard ones.
The joint distribution satisfies:
P(Xi1= ...=Xik|li=E)=1; P(Xi1=b1, ...,Xik=bk|li=H)=
?k
j=1
P(Xij=bj|li=H)
We want to take only easy instances into the gold standard, so that it contains
only settled, trustworthy judgments.2 The problem is that the fact of being easy or
hard is not directly observable, but has to be inferred from the observed annotations.
In particular, some of the observed agreements will in fact be hard instances, since
coin-flips could occasionally come out all-heads or all-tails. Our objective is to estimate,
with a given degree of confidence (?), the proportion ? of hard instances in the agreed
annotations, based on the number of observed disagreements. The value of ? is the level
of annotation noise in the gold standard comprising agreed annotations.
Let p be the probability that the annotators agree on a hard instance in a binary
classification task:
p=P(Xi1= ...=Xik|li=H)=
?k
j=1
P(Xij=0|li=H)+
?k
j=1
P(Xij=1|li=H)
Denote by Ad the event that there are d disagreed instances; these are hard, and are
assumed to be labeled by coin-flips. Let Bh be the event that there are overall h hard
2 On the status of hard instances, see Section 5.1.
496
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
instances; some of these may be unobserved as they surface as random agreements. We
note that P(Ad|Bh) =
(
h
d
)
? (1? p)d ? ph?d for d ? h, hence:
P(Bh|Ad) =
P(Ad ? Bh)
P(Ad)
=
P(Ad|Bh) ? P(Bh)
?n
i=d P(Ad|Bi) ? P(Bi)
=
(
h
d
)
? ph?d ? P(Bh)
?n
i=d
(
i
d
)
? pi?d ? P(Bi)
Let X be a random variable designating the number of coin-flips. It follows that
P(X > t|Ad) =
?n
i=t+1
(
i
d
)
? pi?d ? P(Bi)
?n
i=d
(
i
d
)
? pi?d ? P(Bi)
(1)
Let t0 be the smallest integer for which P(X > t0|Ad) < 1? ?. Given d observed dis-
agreements, we estimate the noise level of the agreed subset of the annotations as at
most ? = t0?dn?d , with confidence ?.
3. Relation to ? Statistic
3.1 The Case of High ? with Two Annotators
Suppose 1,000 instances have been annotated by two people, such that 900 are instances
of agreement. Both in the 900 agreed instances and in the 100 disagreed ones, the
categories were estimated to be equiprobable for both annotators.3 In this case p = 0.5,
? = 0.8,4 which is usually taken to be an indicator of sufficiently agreeable guidelines,
and, by implication, of a high quality data set. Our candidate gold standard is the 900
instances of agreement. What is its 95% confidence noise rate?We find, using ourmodel,
that with more than 5% probability up to 125 agreements are due to coin-flipping, hence
? = 13.8%.5 This scenario is not hypothetical. In Poesio and Vieira (1998) Experiment 1,
the classification of definite descriptions into Anaphoric-or-Associative versus Unfa-
miliar has n = 992, d = 121, p = 0.47, which, with 95% confidence, yields ? = 15%.
Let us reverse the question: For a two-annotator project with 1,000 instances, how
many disagreements could we tolerate, so that the agreed part is 95% noise-free with
95% confidence? Only 33 disagreements, corresponding to ? = 0.93. In practice, this
means that a two-annotator project of this size is unlikely to produce a high-quality
gold standard, the high ? notwithstanding.
3.2 The Case of Low ? with Five Annotators
Suppose now 1,000 instances are annotated by five people, with 660 agreements. With
categories equiprobable in both hard and easy instances, p = 0.0625. The exact value
of ? depends on the distribution of votes in the 340 disagreed cases, from ? = 0.73
when all disagreements are split 4-to-1, to ? = 0.52 when all disagreements are split
3-to-2. Assuming disagreements are coin-flips, the most likely measurement would be
about ? = 0.637, where the 340 observed coin-flips yielded the most likely pattern.6 This
value of ? is considered low, yet the 660 agreed items make a gold standard within the
3 We estimate P(Xij=1|li=H) by the proportion of disagreed instances that annotator j put in category 1.
4 For calculating ?, we use the version shown in Equation (2).
5 In all our calculations P(B1) = ...=P(Bn ), that is, a priori, any number of hard instances is equiprobable.
6 That is, there are twice as many 3-to-2 cases than 4-to-1, corresponding to
(
5
3
)
as opposed to
(
5
4
)
.
497
Computational Linguistics Volume 35, Number 4
noise rate of ? = 5% with 95% confidence, according to our model. Hence it is possible
for the overall annotation to have low-ish ?, but the agreement of all five annotators,
if observed sufficiently frequently, is reliable, and can be used to build a clean gold
standard.
3.3 Interpreting the ? Statistic in the Annotation Generation Model
The ? statistic is defined as ? = PA?PE1?PE where PA is the observed agreement and PE is the
agreement expected by chance, calculated from the marginals. We use the Siegel and
Castellan (1988) version, referred to as K in Artstein and Poesio (2008):
PE =
m
?
j=1
p2j ; pj =
?n
i=1 aij
nk
; PA =
1
n
n
?
i=1
PAi ; PAi =
?m
j=1
(aij
2
)
(
k
2
) (2)
where n is the number of items; m is the number of categories; k is the number of anno-
tators; and aij is the number of annotators who assigned the ith item to the jth category.
Suppose there are h hard instances and e easy ones, and m = 2. Suppose further
that all annotators flip the same coin on hard instances, and that the distribution of the
categories in easy and hard instances is the same and is given by q1, . . . , qm. Then the
probability for chance agreement between two annotators is q =
?m
j=1 q
2
j , of which PE is
an estimator. Agreement on a particular instance PAi is measured by the proportion
of agreeing pairs of annotators out of all such pairs, and PA is an estimator of the
expected agreement across all instances. Our model assumes perfect agreement on easy
instances and agreement with probability q on hard ones, so we expect to see e+q?h
agreed instances, hence PA is an estimator of
e+qh
e+h . Putting these together, ?=
PA?PE
1?PE
is an estimator of
e+qh
e+h
?q
1?q =
e
e+h , the proportion of easy instances.
7 In fact, Aickin (1990)
shows that ? is very close to this ratio when themarginal distribution over the categories
is uniform, with a more substantial divergence for skewed category distributions.8
The correspondence between ? and the proportion of easy instances makes it clear
why ? is not a sufficient indicator of data quality for benchmarking. For when ? = 0.8,
20% of the data are hard cases. Using all data, especially for testing, is thus potentially
hazardous, and the crucial question is: Can we zero in on the easy instances effectively,
without admitting much noise? This is exactly the question answered by the model.
When the distribution of categories is the same in easy and hard instances and
uniform, ? can be used to address this question as well. Recall that in the two-annotator
case in Section 3.1, ? = 0.8, that is, 80% of instances are estimated to be easy. Because
easy cases are a subset of agreed ones in our model, 800 of the agreed 900 instances are
easy, giving an estimate of 11% noise in the gold standard. Requiring 95% confidence in
noise estimation, we found ? = 13.8%, using our model. Similarly, in the five-annotator
7 The proportion of easy cases is positive, whereas the estimator ? can be negative with non-negligible
probability when e = O(
?
h).
8 In Aickin (1990), category distribution on easy cases is derived from that in the hard cases. The closer the
categories are to uniform distribution in the hard cases, the closer their distribution in hard cases is to
that in easy cases. For example, if the categories are distributed uniformly in hard cases, they are also so
distributed in the easy ones. If the categories are distributed ( 13 ,
2
3 ) in the hard cases, they are distributed
( 15 ,
4
5 ) in the easy cases. For this reason, in Aickin?s model, it is not possible to distinguish between
category imbalance (many more 0s than 1s) and differences in category distributions in easy and hard
cases. His simulations show that in cases of category imbalance (which imply, in his model, differences in
category distributions in easy and hard cases), ? tends to underestimate the proportion of easy instances.
498
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
scenario in Section 3.2, ? = 0.637 tells us that about 637 out of 1,000 instances are easy;
they are captured quite precisely by the 660 agreements, yielding a noise estimate of
3.5%, again somewhat lower than the high confidence one we gave using the model.
4. Training and Testing in the Presence of Annotation Noise
We discuss two uses of a gold standard within the benchmarking enterprise. The data
could be used for testing, and, if there is enough of it and after an appropriate partition,
for training as well. We consider each case separately in the following sections.
4.1 Testing with Annotation Noise
The two questions one wants to answer using the data are: Howwell does an algorithm
capture the phenomenon? For any two algorithms, which one is better? Consider the
algorithm comparison situation. Suppose we have a gold standard with L items of
which up to R are noise (?=RL ). Two algorithms might differ in performance on the easy
cases, the hard ones, or both. Because we cannot distinguish between easy and hard
instances in the gold standard, we are unable to attribute the difference in performance
correctly. Moreover, as the annotations of the hard instances are random coin-flips, there
is an expected difference in performance that is a result of pure chance.
Suppose two algorithms perform equally well on easy instances; their performance
on the hard ones is as good as agreement-by-coin-flipping would allow. Thus, the
difference in the number of ?correct? answers on hard instances for algorithms A and
B is a random variable S satisfying S =
?R
i=1 Xi where X1, . . . ,XR are independent and
identically distributed random variables which obtain values?1 (A ?right?, B ?wrong?)
and 1 (A ?wrong?, B ?right?) with probability 14 and 0 with probability
1
2 , thus ?S = 0;
?S =
?
R
2 . By Chebyshev?s inequality Pr(|S| > k?) ?
1
k2
: that is, the chance difference
between the algorithms will be within 4.5? with 95% probability.9 In our example, L =
900 and R = 125, hence a difference of up to 35 ?correct? answers (3.9% of the gold
standard) can be attributed to chance.10
This example shows that even if getting a clean data set is not feasible, it is impor-
tant to report the noise rate of the data set that has been produced. This would allow
calibrating the benchmarking procedure by requiring the difference between the two
competing algorithms to be larger than the chance difference scale.
Some perils of testing on noisy data were discussed in a recent article in this journal
by Reidsma and Carletta (2008). They showed that a machine-learning classifier is
sensitive to the type of noise in the data. Specifically, if the noise is in the form of
category over-use (an annotator disproportionately favors a certain category), when
algorithm performance is measured against the noisy data, accuracy estimates are often
inflated relative to performance on the real data, uncorrupted by noise (see Figure 3(b)
therein). This is because ?when the observed data is used to test performance, some of
9 For large R, normal approximation can be used with the tighter 2? bound for 95% confidence.
10 We note that because the difference attributable to coin-flipping is O(
?
?
L ), and assuming noise rate
is constant, the scale of chance difference diminishes with larger data sets (see also footnote 9).
The issue is more important when dealing with small-to-moderate data sets. However, even for
a 130K test set (Sections 22?24 of the Wall Street Journal corpus, standardly used as a test set in
POS-tagging benchmarks), it is useful to know the estimated noise rate, as it is not clear that all
reported improvements in performance would come out significant. For example, Shen, Satta, and
Joshi (2007) summarize performance of five previously published and three newly reported algorithms,
all between 97.10% and 97.33%.
499
Computational Linguistics Volume 35, Number 4
the samples match not because the classifier gets the label right, but because it overuses
the same label as the human coder? (Reidsma and Carletta 2008, page 232). On the
other hand, if disagreements are random classification noise (the label of any instance
can be flipped with a certain probability), a performance estimate based on observed
data would often be lower than performance on the real data, because the noise that
corrupted it was ignored by the classifier (see Figure 2(d) therein).
Reidsma and Carletta (2008) suggest that the community develops methods to
investigate the patterns of disagreements between annotators to gain insight into the po-
tential of incorrect performance estimation. Althoughwe agree on the general point that
human agreements and disagreements should bear directly on the practice of estimating
the performance of an algorithm, we focus on improving the quality of performance
estimation. We suggest (1) mitigating the effect of annotation noise on performance
estimation by using the least noisy part of the data set for testing, that is, a gold standard
with agreed items; (2) providing an estimate of the level of noise in the gold standard,
which can be used to gauge the divergence between the estimate of performance using
the gold standard from the real performance figure on the easy instances (i.e., on noise-
free data), similarly to the algorithm comparison scenario provided herein.
4.2 Learning with Annotation Noise
The problem with noise in the training data is the potential for misclassification of easy
instances in the test data as a result of hard instances in the training data, the problem
we call hard case bias.
Learning in the presence of noise is an active research area in machine learning.
However, annotation noise is different from existing well-understood noise models.
Specifically, random classification noise, where each instance has the same probability of
having its label flipped, is known to be tolerable in supervised learning (Blum et al 1996;
Cohen 1997; Reidsma and Carletta 2008). In annotation noise, coin-flipping is confined
to hard instances, which should not be assumed to be uniformly distributed across the
feature space. Indeed, there is reason to believe that they form clusters; certain feature
combinations tend to give rise to hard instances. The finding reported by Reidsma and
op den Akker (2008) that a classifier trained on data from one annotator tended to agree
much better with test data from the same annotator than with that of another annotator
exemplifies a situation where observed hard cases (i.e., cases where the annotators
disagree) constitute a pattern in the feature space that a classifier picks up.
In a separate article, we establish a number of properties of learning under anno-
tation noise (Beigman and Beigman Klebanov 2009). We show that the 0-1 loss model
may be vulnerable to annotation noise for small data sets, but becomes increasingly
robust the larger the data set, with worst-case hard case bias of ?( 1?
n
). We also show
that learning with the popular voted-perceptron algorithm (Freund and Schapire 1999)
could suffer a constant rate of hard case bias irrespective of the size of the data set.
5. Discussion
5.1 The Status of Hard Instances
We suggested that only the easy instances should be taken into the gold standard. This
is not to say that hard cases should be eliminated from the researcher?s attention; we
merely argue that they should not be used for testing algorithms for benchmarking
500
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
purposes. Hard cases are interesting for theory development, because this is where the
theory might have a difficulty, but they do not allow for a fair comparison, as their
correct label cannot be determined under the current theory. The agreed data embodies
the well-articulated parts of the theory, which are ready for deployment as a gold
standard for machine learning. Once the theory is improved to a stage where some of
the previously hard cases receive an unproblematic treatment, those items can be added
to the data set, which can make the task more challenging for the machine. Linguistic
theories-in-the-making can have limited coverage; they do not immediately attain the
status of medical conditions, for example, where there presumably exists a true label
even for the hardest-to-diagnose cases.11
5.2 Plausibility of the Model
Beyond the separation into easy and hard instances, our model prescribes certain an-
notator behavior for each type. In our work on metaphor, we observed that certain
metaphor markups were retracted by their authors, when asked after 4?8 weeks to
revisit the annotations (Beigman Klebanov, Beigman, and Diermeier 2008). These were
apparently hard cases, with people resolving their doubts inconsistently on the two
occasions; coin-flipping is a reasonable first-cut model for such cases. The model also
accommodates category over-use bias (Di Eugenio and Glass 2004; Artstein and Poesio
2008; Reidsma and Carletta 2008), as P(Xij=bj|li=H) may vary across annotators.
Still, this model is clearly a simplification. For example, it is possible that there
is more than one degree of hardness, and annotator behavior changes accordingly.
Another extension is modeling imperfect annotators, allowed to commit random errors
on easy cases; this extension would be needed if a large number of annotators is used.
Such extensions, as well as methods for estimating these more complex models,
should clearly be put on the community?s research agenda. The main contribution
of the simple model is in outlining the trajectory from agreement to gold standard
with a noise estimate, and indicating the potential benefit of the latter to data uti-
lization (low overall agreement does not preclude the existence of a reliable subset)
and to prudent benchmarking. Furthermore, the simple model helps us improve the
understanding of the information provided by the ? statistic, and to appreciate its
limitations. It also allows us to see the benefit of adding annotators, as discussed in the
next section.
5.3 Adding Annotators
If we want the test data to be able to detect small advances in machines? handling of
the task, we need to produce gold standards with low noise levels. The level of noise
in agreed data depends on two parameters: (a) the number of agreed items, and (b) the
probability of chance agreement between annotators. Although the first is not under
the researcher?s control once the data set is chosen, the second is, by changing the
number of annotators. Obviously, the more annotators are required to agree, the lower
p will be, and the smaller the number of agreements that can be attributed to coin-
flipping. If indeed 800 out of 1,000 items are easy, agreement between two annotators
can only detect them with up to 13.8% noise. Adding a third annotator means p = 0.25.
11 As one of the anonymous reviewers pointed out, some medical conditions, such as autism, are also only
partially understood.
501
Computational Linguistics Volume 35, Number 4
We are most likely to observe 850 agreed instances, which would not contain more
than 7.7% noise, with 95% confidence. Effectively, we got rid of about half the random
agreements.
Acknowledgments
We thank Eli Shamir and Bei Yu for reading
earlier drafts of this article, as well as the
editor and the anonymous reviewers for
comments that helped us improve the
article significantly.
References
Aickin, Mikel. 1990. Maximum likelihood
estimation of agreement in the constant
predictive probability model, and its
relation to Cohen?s kappa. Biometrics,
46(2):293?302.
Albert, Paul and Lori Dodd. 2004. A
cautionary note on the robustness of latent
class models for estimating diagnostic
error without a gold standard. Biometrics,
60(2):427?435.
Albert, Paul, Lisa McShane, and Joanna Shih.
2001. Latent class modeling approaches
for assessing diagnostic error without a
gold standard: With applications to p53
immunohistochemical assays in bladder
tumors. Biometrics, 57(2):610?619.
Artstein, Ron and Massimo Poesio. 2008.
Inter-coder agreement for computational
linguistics. Computational Linguistics,
34(4):555?596.
Beigman, Eyal and Beata Beigman Klebanov.
2009. Learning with annotation noise. In
Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics,
Singapore.
Beigman Klebanov, Beata, Eyal Beigman,
and Daniel Diermeier. 2008. Analyzing
disagreements. In COLING 2008 Workshop
on Human Judgments in Computational
Linguistics, pages 2?7, Manchester.
Blum, Avrim, Alan Frieze, Ravi Kannan,
and Santosh Vempala. 1996. A
polynomial-time algorithm for learning
noisy linear threshold functions. In
Proceedings of the 37th Annual IEEE
Symposium on Foundations of Computer
Science, pages 330?338, Burlington, VT.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Cohen, Edith. 1997. Learning noisy
perceptrons by a perceptron in polynomial
time. In Proceedings of the 38th Annual
Symposium on Foundations of Computer
Science, pages 514?523, Miami Beach, FL.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Dagan, Ido, Oren Glickman, and Bernardo
Magnini. 2006. The PASCAL recognising
textual entailment challenge. In The
PASCAL Recognising Textual Entailment
Challenge, Springer, Berlin, pages 177?190.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
Freund, Y. and R. E. Schapire. 1999. Large
margin classification using the perceptron
algorithm.Machine Learning, 37(3):277?296.
Gildea, Daniel and Daniel Jurafsky. 2002.
Automatic labeling of semantic roles.
Computational Linguistics, 28(3):245?288.
Girju, Roxana, Adriana Badulescu, and Dan
Moldovan. 2006. Automatic discovery
of part-whole relations. Computational
Linguistics, 32(1):83?135.
Hui, Siu and Xiao Zhou. 1998. Evaluation of
diagnostic tests without gold standards.
Statistical Methods in Medical Research,
7(4):354?370.
Lapata, Maria. 2002. The disambiguation of
nominalizations. Computational Linguistics,
28(3):357?388.
Litman, Diane, Julia Hirschberg, and Marc
Swerts. 2006. Characterizing and
predicting corrections in spoken dialogue
systems. Computational Linguistics,
32(3):417?438.
Markert, Katja and Malvina Nissim. 2002.
Metonymy resolution as a classification
task. In Proceedings of the Empirical Methods
in Natural Language Processing Conference,
pages 204?213, Philadelphia, PA.
Palmer, Martha, Paul Kingsbury, and Daniel
Gildea. 2005. The proposition bank: An
annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?106.
Poesio, Massimo and Renata Vieira. 1998.
A corpus-based investigation of definite
description use. Computational Linguistics,
24(2):183?216.
Reidsma, Dennis and Jean Carletta. 2008.
Reliability measurement without limit.
Computational Linguistics, 34(3):319?326.
Reidsma, Dennis and Rieks op den Akker.
2008. Exploiting subjective annotations.
In COLING 2008 Workshop on Human
Judgments in Computational Linguistics,
pages 8?16, Manchester.
502
Beigman Klebanov and Beigman From Annotator Agreement to Noise Models
Shen, Libin, Giorgio Satta, and Aravind
Joshi. 2007. Guided learning for
bidirectional sequence classification. In
Proceedings of the 45th Annual Meeting
of the Association of Computational
Linguistics, pages 760?767, Prague.
Siegel, Sidney and N. John Castellan Jr.
1988. Nonparametric Statistics for the
Behavioral Sciences. McGraw-Hill,
2nd edition.
Snyder, Benjamin and Martha Palmer. 2004.
The English all-words task. In Senseval-3:
3rd International Workshop on the Evaluation
of Systems for the Semantic Analysis of Text,
pages 41?43, Barcelona.
Vieira, Renata and Massimo Poesio. 2000. An
empirically based system for processing
definite descriptions. Computational
Linguistics, 26(4):539?593.
Wiebe, Janyce, Teresa Wilson, and Claire
Cardie. 2005. Annotating expressions of
opinions and emotions in language.
Language Resources and Evaluation,
39(2):165?210.
503

Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 438?446,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Some Empirical Evidence for Annotation Noise in a Benchmarked Dataset
Beata Beigman Klebanov
Kellogg School of Management
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Abstract
A number of recent articles in computational
linguistics venues called for a closer exami-
nation of the type of noise present in anno-
tated datasets used for benchmarking (Rei-
dsma and Carletta, 2008; Beigman Klebanov
and Beigman, 2009). In particular, Beigman
Klebanov and Beigman articulated a type of
noise they call annotation noise and showed
that in worst case such noise can severely
degrade the generalization ability of a linear
classifier (Beigman and Beigman Klebanov,
2009). In this paper, we provide quantita-
tive empirical evidence for the existence of
this type of noise in a recently benchmarked
dataset. The proposed methodology can be
used to zero in on unreliable instances, facili-
tating generation of cleaner gold standards for
benchmarking.
1 Introduction
Traditionally, studies in computational linguistics
use few trained annotators. Lately this might be
changing, as inexpensive annotators are available in
large numbers through projects like Amazon Me-
chanical Turk or through online games where an-
notations are produced as a by-product (Poesio et
al., 2008; von Ahn, 2006), and, at least for certain
tasks, the quality of multiple non-expert annotations
is close to that of a small number of experts (Snow
et al, 2008; Callison-Burch, 2009).
Apart from the reduced costs, mass annotation is
a promising way to get detailed information about
the dataset, such as the level of difficulty of the dif-
ference instances. Such information is important
both from the linguistic and from the machine learn-
ing perspective, as the existence of a group of in-
stances difficult enough to look like they have been
labeled by random guesses can in the worst case
induce the machine learner training on the dataset
to misclassify a constant proportion of easy, non-
controversial instances, as well as produce incor-
rect comparative results in a benchmarking setting
(Beigman Klebanov and Beigman, 2009; Beigman
and Beigman Klebanov, 2009) .
In this article, we employ annotation generation
models to estimate the types of instances in a multi-
ply annotated dataset for a binary classification task.
We provide the first quantitative empirical demon-
stration, to our knowledge, of the existence of what
Beigman Klebanov and Beigman (2009) call ?anno-
tation noise? in a benchmarked dataset, that is, for
a case where instances cannot be plausibly assigned
to just two classes, and where instances in the third
class can be plausibly described as having been an-
notated by flips of a nearly fair coin. The ability to
identify such instances helps improve the gold stan-
dard by eliminating them, and allows further empiri-
cal investigation of their impact on machine learning
for the task in question.
2 Generative models of annotation
We present a graphical model for the generation of
annotations. The basic idea is that there are different
types of instances that induce different responses
from annotators. Each instance may have a true la-
bel of ?0? or ?1?, however, the researcher?s access
to it is mediated by annotators who are guessing the
true label by flipping a coin, where the bias of the
coin depends on the type of the instance. The bias
of the coin essentially models the difficulty of label-
438
ing the instance; coins biased close to 0 and 1 cor-
respond to instances that are easy to classify; a fair
coin represents instances that are very difficult if not
impossible to classify correctly with the given pool
of annotators. The model presented in Beigman Kle-
banov and Beigman (2009) is a special case with 3
types (A,B,C) where pA=0, pC=1 (easy cases), and
0<pB<1 represents the hard cases, the harder the
closer pB is to 0.5. Models used here are a type of la-
tent class models (McCutcheon, 1987) widely used
in the Biometrics community (Espeland and Handel-
man, 1989; Yang and Becker, 1997; Albert et al,
2001; Albert and Dodd, 2004).
The goal of modeling is to determine whether
more than two types of instances need to be postu-
lated, to estimate how difficult each type is, and to
identify the troublemaking instances.
The graphical model is presented in figure 1. We
assume the dataset of size N is a mixture of k dif-
ferent types of instances. The proportion of types is
given by ? = (?1, . . . , ?k), and coin biases for each
type are given by p = (p1, . . . , pk). Each instance is
annotated by n i.i.d coinflips, and random variable
x ? {0, . . . , n} counts the number of ?1?s in the n
annotations given to an instance. Each instance be-
longs to a type t ? {1, ..., k}, characterized by a coin
with the probability pt of annotating with the label
?1?. Conditioned on t, the number of ?1?s in n an-
notations has a binomial distribution with parameter
pt: Pr(x = j|t) =
(n
j
)
pjt (1? pt)
n?j .
x t ? N 
x t ? 
N a ? 
p 
p 
x a ? N 
p 
Figure 1: A graphical model of annotation generation.
The probability of observing j ?1?s out of n an-
notations for an instance given ? and p is therefore
Pr(x = j|?, p) =
?k
t=1 Pr(t|?) ? Pr(x = j|t) =
=
(n
j
)?k
t=1 ?tp
j
t (1 ? pt)
n?j . The annotations are
thus generated by a superposition of k binomials.
3 Data
3.1 Recognizing Textual Entailment -1
For the experiments reported here we use the 800
item test data of the first Recognizing Textual Entail-
ment benchmark (RTE-1) from Dagan et al (2006).
This task drew a lot of attention in the community,
with a series of benchmarks in 2005-2007.
The task is defined as follows: ?... textual entail-
ment is defined as a directional relationship between
pairs of text expressions, denoted by T - the entail-
ing ?Text?, and H - the entailed ?Hypothesis?. We
say that T entails H if the meaning of H can be in-
ferred from the meaning of T, as would typically be
interpreted by people. This somewhat informal defi-
nition is based on (and assumes) common human
understanding of language as well as common back-
ground knowledge? (Dagan et al, 2006). Further
guidelines included an instruction to disregard tense
differences, to accept cases where the inference is
?very probable (but not completely certain)? and to
avoid cases where the inference ?has some positive
probability that is not clearly very high.? An exam-
ple of a true entailment is the pair T-H: (T) Cavern
Club sessions paid the Beatles ?15 evenings and ?5
lunchtime. (H) The Beatles perform at Cavern.
Although annotated by a small number of experts
for the benchmark, the RTE-1 dataset has been later
transferred to a mass annotation framework by Snow
et al (2008), who submitted simplified guidelines
to the Amazon Mechanical Turk workplace (hence-
forth, AMT), collected 10 annotations per item from
the total of 164 annotators, and showed that major-
ity vote by Turkers agreed with expert annotation in
89.7% of the cases. We call the Snow et al (2008)
Turker annotations SRTE dataset, and use it in sec-
tion 6. The instructions, followed by two examples,
read: ?Please state whether the second sentence (the
Hypothesis) is implied by the information in the first
sentence (the Text), i.e., please state whether the Hy-
pothesis can be determined to be true given that the
Text is true. Assume that you do not know anything
about the situation except what the Text itself says.
Also, note that every part of the Hypothesis must be
implied by the Text in order for it to be true.? The
guidelines for Turkers are somewhat different from
the original, not mentioning the issue of highly prob-
able though not certain inference or a special treat-
439
ment of tense mismatch between H and T, as well as
discouraging reliance on background knowledge.
Using Snow et al (2008) instructions, we col-
lected 20 annotations for each of the 800 items
through AMT from the total of 441 annotators. Each
annotator did the minimum of 2 items, and was
paid $0.01 for 2 items, for the total annotator cost
of $80. We used only annotators with prior AMT
approval rate of at least 95%, that is, only people
whose performance in previous tasks on AMT was
almost always approved by the requester of the task.
Our design is thus somewhat different from Snow et
al. (2008), as we paid more and selected annotators
with a stake in their AMT reputation.
3.2 Preparing the data for model fitting
We collected the annotations in two separate batches
of 10 annotations per item, using the same set of in-
structions, incentives, and examples. We hypothe-
sized that controlling for these elements, we would
get two random samples from the same distribution
of Turkers, and hence will have two samples to make
sure a model fitted on one sample generalized to
the other. It turned out, however, that a 3-Binomial
model with a good fit on one of the samples was re-
jected with high probability for the other.1 Thus, on
the one hand, the variations between annotators in
each sample were not as high as to preclude a model
that captures only instance variability from fitting
well; on the other hand, evidently, the two samples
did not come from the same annotator distribution,
but differed systematically due to factors we did not
control for.2 In order for our models not to inherit a
systematic bias of any of the two samples, we mixed
the two samples, and constructed two sets, BRTEa
and BRTEb, each with 10 annotations per item, by
randomly splitting the 20 answers per item into two
groups, allowing the same annotator to contribute
to different groups on different instances. Indeed,
after the randomization, a model fitted for BRTEa
produced excellent generalization on BRTEb, as we
will see in section 4.2.
1For details of the model fitting procedure, see section 4.
2Such factors could be the hour and day of assignment, as
the composition of AMT?s global 24/7 workforce could differ
systematically by day and hour.
4 Fitting a model to BRTE data
Using the model template presented in section 2, we
successively attempt to fit a model with k = 2, 3, . . .
until a model with a good fit is found or no degrees
of freedom are left. For a given k, we fit the pa-
rameters ? and p using non-linear least squares trust-
region method as implemented in the default version
of MATLAB?s lsqnonlin function. We then use ?2 to
measure goodness of fit; a model that cannot be re-
jected with 95% confidence (p>0.05) would be con-
sidered a good fit. In all cases N=800, n=10, as we
use 10 annotations for each instance.
4.1 Mixture of 2 Binomials
Suppose k=2, with types t0 and t1. The best fit yields
p0=0.237, p1=0.867, ?0=431800 , ?1=1-?0. The model
(shown in figure 2) is a poor fit, with ?2=73.66 well
above the critical value of 14.07 for df=7, p=0.05.3
02040
6080100
120140160
0 1 2 3 4 5 6 7 8 9 10Number of label "1" annotations
Number of instan
ces ObservedPredicted
Figure 2: Fitting the model B1+B2 to BRTEa data. B1?
B(10,0.237) on 431 instances, B2? B(10,0.867) on 369
instances. The point (x,y) means that there are y in-
stances given label ?1? in exactly x out of 10 annotations.
4.2 Model M: Mixture of 3 Binomials
Suppose now k=3. The best fitting model
M=B1+B2+B3 is specified in figure 3; M fits the
data very well. Assuming B1 and B3 reflect items
3For degrees of freedom, we take the number of datapoints
being fitted (11), take one degree of freedom off for knowing in
advance the total number of instances, and take off additional 3
degrees of freedom for estimating p0, p1, and ?0 from the data.
We are therefore left with 7 degrees of freedom in this case.
440
with uncontroversial labels ?0? and ?1?, respec-
tively, the model suggests that detecting ?0? (no tex-
tual entailment) is somewhat more difficult for non-
experts than detecting ?1? (there is textual entail-
ment) in this dataset, with the rate of incorrect pre-
dictions of about 20% and 10%, respectively.4 The
model also predicts that 159800 ? 20% of the data are
difficult cases, with annotators flipping a close-to-a-
fair coin (p=0.5487).
02040
6080100
120140
0 1 2 3 4 5 6 7 8 9 10
ObservedB1B2B3Predicted
Figure 3: Fitting the model M=B1+B2+B3 to BRTEa
data. B1? B(10,0.1978) on 343 instances, B2?
B(10,0.5487) on 159 instances, B3? B(10,0.8942) on
298 instances. The binomials are shown in grey lines.
The model M fits with ?2=5.091; for df=5, this corre-
sponds to p=0.4.
We use the dataset BRTEb to test the model de-
veloped on BRTEa. The model fits with ?2=13.13,
which, for df=10,5 corresponds to p=0.2154.
We therefore conclude that, after eliminating sys-
tematic differences between annotators, we were un-
able to fit a model with two types of instances,
whereas a model with three types of instances pro-
vides a good fit both for the dataset on which it is
estimated and for a new dataset. This constitutes
empirical evidence for the existence of a group of
instances with near-random labels in this recently
4We note that any conclusions from the model hold for the
particular 800 item dataset in question, and not for the task of
recognizing textual entailment in general, as the dataset is not
necessarily a representative sample. In fact, we know from Da-
gan et al (2006) that these 800 items are not a random sam-
ple, but rather what remained after some 400 instances were re-
moved due to disagreements between expert annotators or due
to the judgment of one of organizers of the RTE-1 challenge.
5No parameters are fitted using the BRTEb data.
benchmarked dataset, at least for our pool of more
than 400 non-expert annotators.
5 Could annotator heterogeneity provide
an alternative explanation?
In the previous section, we established that instance
heterogeneity can explain the observations. We
might however ask whether a different model could
provide a similarly fitting explanation. Specifically,
heterogeneity among annotators has been seen as a
major source of noise in the aggregate data and there
are several works attempting to separate high qual-
ity annotators from low quality ones (Raykar et al,
2009; Donmez et al, 2009; Sheng et al, 2008; Car-
penter, 2008). Could we explain the observed beha-
vior with a model with only two types of instances
that allows for annotator heterogeneity?
In this section we construct such a model. We
show that this model entails an instance distribu-
tion that is a superposition of two normal distribu-
tions. We subsequently show that the best fitting
two-Gaussian model does not provide a good fit.
We use a generation model similar to those in
(Raykar et al, 2009; Carpenter, 2008) but with
weaker parametric assumptions. The graphical
model is given in figure 4.
x t ? N 
x t ? 
N a ? 
p 
p 
x 
? 
N 
p 
t ? 
Figure 4: Annotation generation model with annotator
heterogeneity.
We assume there are two types of instances t ?
{0, 1} with the proportions ? = (?0, ?1). The 2n
probabilities p = (pt1, . . . , ptn) for t = 0, 1 cor-
respond to coins drawn independently from some
distribution with parameter ? = (?1, . . . , ?n). We
make no assumption on the functional form apart
from a positive probability to draw a value between
0 and 1, this in particular is true for the beta distribu-
tion used in (Raykar et al, 2009; Carpenter, 2008).
As before, the number of ?1?s attributed to an in-
stance of type t is a random variable x, determined
441
by independent flips of the n coins that correspond
to the value of t. The marginal distribution of x is:
Pr(x = j|?, ?) =
=
?
t=0,1
Pr(t|?)
?
[0,1]n
Pr(pt|?)?Pr(x = j|pt, t, ?)dpt
=
?
t=0,1
?t
?
[0,1]n
Pr(pt|?)
?
?
?
|S|=j
?
i?S
pti
?
i6?S
(1? pti)
?
? dpt
Let x1, . . . , xN be the random variables correspond-
ing to the number of ?1?s attributed to instances
1, . . . , N . W.l.g we assume instances 1, . . . , N ? are
all of type t0 (N ? = ?0 ? N ) and the rest of type t1.
Since 0 ? xj ? n it follows that E(xj),Var(xj) <
? for j = 1, . . . , N . If for each instance the coin-
flips are independent, we can think of this as a two
step process where we first draw the coins and then
flip them. Thus, x1, . . . , xN ? are i.i.d and the cen-
tral limit theorem implies that the average number
of ?1?s on t0 instances, namely the random variable
y0 = 1N ?
?N ?
j=1 xj has an approximately normal dis-
tribution.6 Making the same argument for the distri-
bution of y1 for instances of type t1, it follows that
the number of ?1?s attributed to an instance of any
type y = y0 + y1 would have a distribution that is a
superposition of two Gaussians.
The best least-squares fit of all two-Gaussian
models to BRTEa data is produced by G=N1+N2,
N1? N (2.22, 1.73) on 418 instances, N2?
N (9.07,1.41) on 382 instances; G is shown in
figure 5. G fits with ?2=36.77, much above the crit-
ical value ?2=11.07 for df=5, p=0.05. We can thus
rule out annotator heterogeneity as the only expla-
nation of the observed pattern of responses.
6 Testing M on SRTE data
We further test M on the annotations collected by
Snow et al (2008) for the same 800 item dataset.
While the instructions and the task were identical in
BRTEa, BRTEb, and BRTE datasets, and in all cases
6It can be shown that y0 ? N (?, ?) for ? = n ? EDist(?)(p)
and ? =
p
VarDist(?)(p) ? n, using the expectation and variance
of the coin parameter for type t0 instances. For example, for a
beta distribution with parameters ? and ? these would be ? =
?
?+? n and ? =
q
??
?+? n.
02040
6080100
120
0 1 2 3 4 5 6 7 8 9 10
ObservedPredicted
Figure 5: Model G?s fit to BRTEa data, G= N1+N2, a
mixture of two Gaussians.
each item was given 10 annotations, the incentive
design was different (see section 3).
Figure 6 shows that model M=B1+B2+B3 does
not fit well, as SRTE dataset exhibits a rather diffe-
rent distribution from both BRTE datasets. In par-
ticular, it is clear that had a model been fitted on
SRTE data, the coin flipping probabilities for the
clear types, B1 and B3, would have to be moved
towards 0.5; that is, an average annotator in SRTE
dataset had worse ability to detect clear 0s and clear
1s than an average BRTE annotator. We note that
BRTEa and BRTEb agreed with expert annotation
in 92.5% and 90.8% of the instances, respectively,
both better than 89.7% in SRTE.7 Since we offered
somewhat better incentives in BRTE, it is tempting
to attribute the observed better quality of BRTE an-
notations to the improved incentives, although it is
possible that some other uncontrolled AMT-related
factor is responsible for the difference between the
datasets, just as we found for our original two col-
lected samples (see section 3.2).
Supposing the main source of misfit is difference
in incentives, we conjecture that the difference be-
tween the 441 BRTE annotators and the 164 SRTE
ones is due to the existence in SRTE of unmotivated,
or ?lazy? annotators, that is, people who flipped the
same coin on every instance, no matter what type.
Our hypothesis is that once an annotator is diligent
(and motivated) enough to pay attention to the data,
her annotations can be described by model M, but
some annotators are not sufficiently diligent.
7Turker annotations were aggregated using majority vote, as
in Snow et al (2008) section 4.3.
442
0204060
80100120140
0 1 2 3 4 5 6 7 8 9 10
BRTEaBRTEbObserved (SRTE)Predicted by M
Figure 6: Model M?s fit to SRTE data. BRTEa and
BRTEb are shown in grey lines.
In this model we assume there are three types
of instances as before, and two types of annotators
a ? {D,L}, for Diligent and Lazy, with their pro-
portions in the population ? = (?D, ?L). The corre-
sponding graphical model is shown in figure 7.
x t ? N 
x 
t ? N 
a ? 
p 
p 
x 
? 
N 
p 
t ? 
x 
t ? N 
a ? p 
c 
Figure 7: Annotation generation with diligent and lazy
annotators.
We assume that diligent annotators flip coins cor-
responding to the types of instances, whereas lazy
annotators always flip the same coin pL.
Let nD and nL=n?nD be the number of diligent
and lazy annotations given to a certain instance, thus
Pr(nD=r|?)=
(n
r
)
?rD?
n?r
L , and the probability of ob-
serving j label ?1? annotations for an instance of
type t is given by:
Pr(x = j|t, ?, p) =
n?
r=1
[(
n
r
)
?rD?
n?r
L ?
?
[
?
(j1,j2)?S
(
r
j1
)
pj1t (1? pt)
r?j1 ?
?
(
n? r
j2
)
pj2L (1? pL)
n?r?j2
]]
where S={(j1, j2):j1+j2=j; j1?r;j2?n-r}. Finally,
Pr(x=j|?, ?, p)=
?k
t=1 ?t Pr(x=j|t, ?, p).
We assume that model M provides the values for
? and p for all diligent annotators, and estimate ?
and pL, the proportion of the lazy annotators and
the coin they flip. The best fitting model yields
?=(0.79,0.21), and pL=0.74, predicting that about
one-fifth of SRTE annotators are lazy.8 This model
fits with ?2=14.63, which is below the critical level
of ?2=15.51 for df=8,p=0.05, hence a hypothesis
that model M behavior for the diligent annotators
and flipping a coin with bias 0.74 for the lazy ones
generated the SRTE data cannot be rejected with
high confidence. We note that Carpenter (2008) ar-
rived at a similar conclusion ? that there are quite
a few annotators making random guesses in SRTE
dataset ? by means of jointly estimating annotator
accuracies.
7 Discussion
To summarize our findings: With systematic dif-
ferences between annotators smoothed out, there
is evidence that non-expert annotators performing
RTE task on RTE-1 test data tend to flip a close-
to-fair coin on about 20% of instances, according
to the best fitting model.9 This constitutes, to our
knowledge, the first empirical evidence for the ex-
istence of the kind of noise termed annotation noise
in Beigman Klebanov and Beigman (2009). Given
Beigman Klebanov and Beigman (2009) warning
against annotation noise in test data and their find-
ing in Beigman and Beigman Klebanov (2009) that
annotation noise in training data can potentially dev-
astate a linear classifier learning from the data, the
immediate usefulness of our result is that instances
of this difficult type can be identified, removed from
the dataset before further benchmarking, and pos-
8A more precise statement is that there are about one-fifth
lazy potential annotators in the SRTE pool for any given item.
It is possible that the length of stay of an annotator in the pool is
not independent of her diligence; for example, Callison-Burch
(2009) found in his AMT experiments with tasks related to ma-
chine translation that lazy annotators tended to stay longer and
do more annotations.
9Beigman Klebanov and Beigman (2009) discuss the con-
nection between noise models and inter-annotator agreement.
443
sibly used in a controlled fashion for subsequent
studies of the impact of annotation noise on specific
learning algorithms and feature spaces for this task.
The current literature on generating benchmark-
ing data from AMT annotations overwhelmingly
considers annotator heterogeneity as the source of
observed discrepancies, with instances falling into
two classes only. Our results suggest that, at least in
RTE data, instance heterogeneity cannot be ignored.
It also transpired that small variations in incen-
tives (as between SRTE and BRTE), and even un-
known factors possibly related to differences in the
composition of AMT?s workforce can lead to sys-
tematic differences in the resulting annotator pools,
which results in annotations that are described by
models with somewhat different parameter values.
This can potentially limit the usefulness of our main
finding, because it is not clear how reliable the iden-
tification of hard cases is using any particular group
of Turkers. While this is a valid concern in general,
we show in section 7.1 that many items consistently
found to be hard by different groups of Turkers war-
rant at least an additional examination, as they often
represent borderline cases of highly or not-so-highly
probable inferences, corruption of meaning by un-
grammaticality, or difficulties related to the treat-
ment of time references and background knowledge.
Finally, our findings seem to be at odds with the
fact that the 800 items analyzed here were left af-
ter all items on which two experts disagreed and all
items that looked controversial to the arbiter were
removed (see section 3). One potential explanation
is that things that are hard for Turkers are not nec-
essarily hard for experts. Yet it is possible that two
or three annotators, graduate students or faculty in
computational linguistics, are an especially homoge-
nous and small pool of people to base gold standard
annotations of the way things are ?typically inter-
preted by people? upon. Furthermore, there is some
evidence from additional expert re-annotations of
this dataset that some controversies remain; we dis-
cuss relation to expert annotations in section 7.2.
7.1 Hard cases
We examine some of the instances that in all likeli-
hood belong to the difficult type, according to Turk-
ers. We focus on items that received between 4 and
7 class ?1? annotations in SRTE and in each of our
two datasets (before randomization).
(1) T: Saudi Arabia, the biggest oil producer in
the world, was once a supporter of Osama bin
Laden and his associates who led attacks against
the United States. H: Saudi Arabia is the
world?s biggest oil exporter.
(2) T: Seiler was reported missing March 27 and
was found four days later in a marsh near her
campus apartment. H: Abducted Audrey Seiler
found four days after missing.
(3) T: The spokesman for the rescue authorities,
Linart Ohlin, said that the accident took place
between 01:00 and dawn today, Friday (00:00
GMT) in a disco behind the theatre, where ?hun-
dreds? of young people were present. H: The
fire happened in the early hours of Friday morn-
ing, and hundreds of young people were present.
(4) T: William Leonard Jennings sobbed loudly as
was charged with killing his 3-year-old son,
Stephen, who was last seen alive on Dec.12,
1962. H: William Leonard Jennings killed his
3-year-old son, Stephen.
Labeling of examples 1-4 seems to hinge on the
assessment of the likelihood of an alternative expla-
nation. Thus, it is possible that the biggest producer
of oil is not the biggest exporter, because, for ex-
ample, its internal consumption is much higher than
in the second-biggest producer. In 2, abduction is
a possible cause for being missing, but how rela-
tively probable is it? Similarly, fire is a kind of ac-
cident, but can we infer that there was fire from a
report about an accident? In 4, could the man have
sobbed because on top of loosing his son he was
also being falsely accused of having killed him? Ex-
perts marked all five as true entailments, while many
Turkers had reservations.
(5) T: Bush returned to the White House late Satur-
day while his running mate was off campaigning
in the West. H: Bush left the White House.
(6) T: De la Cruz?s family said he had gone to Saudi
Arabia a year ago to work as a driver after a long
period of unemployment. H: De la Cruz was
unemployed.
(7) T: Measurements by ground-based instruments
around the world have shown a decrease of up
to 10 percent in sunlight from the late 1950s to
the early 1990s. H: The world is about 10 per
cent darker than half a century ago.
444
In examples 5-7 time seems to be an issue. If Bush
returned to White House, he must have left it before-
hand, but does this count as entailment, or is the hy-
pothesis referencing a time concurrent with the text,
in which case T and H are in contradiction? In 6,
can H be seen as referring to some time more than a
year ago? In 7, if the hypothesis is taken to be stated
in mid- or late-2000s, the time of annotation, half
a century ago would reach to late 1950s, but it is
possible that further substantial reduction occurred
between early 1990s mentioned in the text and mid
2000s, amounting to much more than 10%. Experts
labeled example 5 as false, 6 and 7 as true.
(8) T: On 2 February 1990, at the opening of Parlia-
ment, he declared that apartheid had failed and
that the bans on political parties, including the
ANC, were to be lifted. H: Apartheid in South
Africa was abolished in 1990.
(9) T: Kennedy had just won California?s Demo-
cratic presidential primary when Sirhan shot
him in Los Angeles on June 5, 1968. H: Sirhan
killed Kennedy.
Labeling examples 8 and 9 (both true according to
the experts) requires knowledge about South African
and American politics, respectively. Was the ban on
ANC the only or the most important manifestation
of apartheid? Was abolishing apartheid merely an
issue of declaring that it failed? In 9, killing is a po-
tential but not necessary outcome of shooting, so de-
tails of Robert Kennedy?s case need to be known to
the annotator to render the case-specific judgment.
(10) T: The version for the PC has essentially the
same packaging as those for the big game con-
soles, but players have been complaining that
it offers significantly less versatility when it
comes to swinging through New York. H: Play-
ers have been complaining that it sells signifi-
cantly less versatility when it comes to swinging
through New York.
(11) T: During his trip to the Middle East that took
three days, Clinton made the first visit by an
American president to the Palestinian Territories
and participated in a three-way meeting with Is-
raeli Prime Minister Benjamin Netanyahu and
Palestinian President Yasser Arafat. H: During
his trip to the east of the Middle which lasted
three days, the Clinton to first visit to Ameri-
can President to the occupied Palestinian terri-
tories and participated in meeting tripartite co-
operation with Israeli Prime Minister Benjamin
Netanyahu and Palestinian President, Yasser
Arafat.
(12) T: The ISM non-manufacturing index rose to
64.8 in July from 59.9 in June. H: The non-
manufacturing index of the ISM raised 64.8 in
July from 59.9 in June.
(13) T: Henryk Wieniawski, a Polish-born musician,
was known for his special preference for resur-
recting neglected or lost works for the violin. H:
Henryk Wieniawski was born in Polish.
Examples 10-13 were labeled as false by experts,
possibly betraying over-sensitivity to the failings of
language technology. Sells is not an ideal substitu-
tion for offers, but in a certain sense versatility is
sold as part of a product. In 11-13, some Turkers
felt the hypothesis is not too bad a rendition of the
text or of its part, while experts seemed to hold MT
to a higher standard.
7.2 Turkers vs experts
Model M puts 159 items in the difficult type B2.
While M is the best fitting model, it is possible to
find a model that still fits with p>0.05 but places
a smaller number of items in B2, in order to ob-
tain a conservative estimate on the number of dif-
ficult cases. The model with B1? B(10, 0.21) on
373 items, B2? B(10,0.563) on 110 items, B3?
B(10,0.89) on 327 items still produces a fit with
p>0.05, but going down to 100 instances in B2
makes it impossible to find a good fit with a 3 type
model. There are therefore about 110 difficult cases
by a conservative estimate. Assuming there remain
110 hard cases in the 800 item dataset for which
even experts flip a fair coin, we expect about 55
disagreements between the 800 item gold standard
from RTE-1 and a replication by a new expert, or
an agreement of 745800=93% on average. This estimate
is consistent with reports of 91% to 96% replication
accuracy for the expert annotations on various sub-
sets of the data by different groups of experts (see
section 2.3 in Dagan et al (2006)).
Acknowledgments
We would like to thank the anonymous reviewers of
this and the previous draft for helping us improve the
paper significantly. We also thank Amar Cheema for
his advice on AMT.
445
References
Paul Albert and Lori Dodd. 2004. A Cautionary Note on
the Robustness of Latent Class Models for Estimating
Diagnostic Error without a Gold Standard. Biometrics,
60(2):427?435.
Paul Albert, Lisa McShane, Joanna Shih, and The U.S.
National Cancer Institute Bladder Tumor Marker Net-
work. 2001. Latent Class Modeling Approaches for
Assessing Diagnostic Error without a Gold Standard:
With Applications to p53 Immunohistochemical As-
says in Bladder Tumors. Biometrics, 57(2):610?619.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with Annotation Noise. In Proceedings of
the 47th Annual Meeting of the Association for Com-
putational Linguistics, pages 280?287, Singapore.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Ac-
cepted to Computational Linguistics.
Chris Callison-Burch. 2009. Fast, Cheap, and Cre-
ative: Evaluating Translation Quality Using Amazon?s
Mechanical Turk. In Proceedings of the Empirical
Methods in Natural Language Processing Conference,
pages 286?295, Singapore.
Bob Carpenter. 2008. Multilevel Bayesian Mod-
els of Categorical Data Annotation. Unpub-
lished manuscript, last accessed 28 July 2009
at lingpipe.files.wordpress.com/2009/01/anno-bayes-
entities-09.pdf.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entail-
ment Challenge. In J. Quin?onero Candela, I. Dagan,
B. Magnini, and F. d?Alche?-Buc, editors, Machine
Learning Challenges, pages 177?190. Springer.
Pinar Donmez, Jaime Carbonell, and Jeff Schneider.
2009. Efficiently Learning and Accuracy of Labeling
Sources for Selective Sampling. In Proceedings of the
15th International Conference on Knowledge Discov-
ery and Data Mining, pages 259?268, Paris, France.
Mark Espeland and Stanley Handelman. 1989. Using
Class Models to Characterize and Assess Relative Er-
ror in Discrete Measurements. Biometrics, 45(2):587?
599.
Allan McCutcheon. 1987. Latent Class Analysis. New-
bury Park, CA, USA: Sage.
Massimo Poesio, Udo Kruschwitz, and Jon Chamberlain.
2008. ANAWIKI: Creating Anaphorically Annotated
Resources through Web Cooperation. In Proceedings
of the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco.
Vikas Raykar, Shipeng Yu, Linda Zhao, Anna Jerebko,
Charles Florin, Gerardo Hermosillo Valadez, Luca Bo-
goni, and Linda Moy. 2009. Supervised Learning
from Multiple Experts: Whom to Trust when Every-
one Lies a Bit. In Proceedings of the 26th Annual In-
ternational Conference on Machine Learning, pages
889?896, Montreal, Canada.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Linguis-
tics, 34(3):319?326.
Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.
2008. Get Another Label? Improving Data Quality
and Data Mining Using Multiple, Noisy Labelers. In
Proceedings of the 14th International Conference on
Knowledge Discovery and Data Mining, pages 614?
622, Las Vegas, Nevada, USA.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Ng. 2008. Cheap and Fast - But is it Good?
Evaluating Non-Expert Annotations for Natural Lan-
guage Tasks. In Proceedings of the Empirical Methods
in Natural Language Processing Conference, pages
254?263, Honolulu, Hawaii.
Luis von Ahn. 2006. Games with a Purpose. Computer,
39(6):92?94.
Ilsoon Yang and Mark Becker. 1997. Latent Vari-
able Modeling of Diagnostic Accuracy. Biometrics,
53(3):948?958.
446
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 698?709,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A Game-Theoretic Model of Metaphorical Bargaining
Beata Beigman Klebanov
Kellogg School of Management
Northwestern University
beata@northwestern.edu
Eyal Beigman
Washington University in St. Louis
beigman@wustl.edu
Abstract
We present a game-theoretic model of bar-
gaining over a metaphor in the context of
political communication, find its equilib-
rium, and use it to rationalize observed
linguistic behavior. We argue that game
theory is well suited for modeling dis-
course as a dynamic resulting from a num-
ber of conflicting pressures, and suggest
applications of interest to computational
linguists.
1 Introduction
A 13 Dec 1992 article in The Times starts thus:
The European train chugged out of the station
last night; for most of the day it looked as if it
might be stalled there for some time. It managed
to pull away at around 10:30 pm only after the
Spanish prime minister, Felipe Gonzalez, forced
the passengers in the first class carriages into a
last minute whip round to sweeten the trip for the
European Community?s poor four: Spain, Portu-
gal, Greece and Ireland.
The fat controller, Helmut Kohl, beamed with
satisfaction as the deal was done. The elegantly-
suited Francois Mitterrand was equally satisfied.
But nobody was as pleased as John Major, sta-
tionmaster for the UK presidency, for whom the
agreement marked a scarce high point in a bat-
tered premiership.
The departure had actually been delayed by
seven months by Danes on the line. Just when
that problem was solved, there was the volu-
ble outbreak, orchestrated by Spain, from the
poor four passengers demanding that they should
travel free and be given spending money, too.
The coupling of the carriages may not be reli-
ably secure but the pan-European express is in
motion. That few seem to agree the destination
suggests that future arguments are inevitable at
every set of points. Next stop: Copenhagen.
Apart from an entertaining read, the extended
metaphor provides an elaborate conceptual cor-
respondence between a familiar domain of train
journeys and the unfolding process of European
integration. Carriages are likened to nation states;
passengers to their peoples; treaties to stations;
politicians to responsible rail company employees.
In a compact form, the metaphor gives expres-
sion to both the small and the large scale of the
process. It provides for the recent history: Den-
mark?s failure to ratify the 1992 Maastricht treaty
until opt-outs were negotiated later that year is
compared to dissenters sabotaging the journey by
laying on the tracks (Danes on the line); nego-
tiations over the Cohesion Fund that would pro-
vide less developed regions with financial aid to
help them comply with convergence criteria are
likened to second class carriages with poor pas-
sengers for whom the journey had to be subsi-
dized. At a more general level, the European in-
tegration is a purposeful movement towards some
destination according to a worked out plan, get-
ting safely through negotiation and implementa-
tion from one treaty to another, as a train moving
on its rails through subsequent stations, with each
nation being separate yet tied with everyone else.
Numerous inferences regarding speed, timetables,
stations, passengers, different classes of tickets,
temporary obstacles on the tracks, and so on can
be made by the reader based on the knowledge of
train journeys, giving him or her a feeling of an en-
hanced understanding1 of the highly complex pro-
cess of European integration.
So apt was the metaphor that political fights
were waged over its details (Musolff, 2000). Wor-
ries about destination were given an eloquent ex-
pression by Margaret Thatcher (Sunday Times, 20
Sept 1992):
She warned EC leaders to stop their endless
round of summits and take notice of their own
people. ?There is a fear that the European train
will thunder forward, laden with its customary
cargo of gravy, towards a destination neither
wished for nor understood by electorates. But
the train can be stopped,? she said.
1More on enhanced understanding in sections 3.2 and 4.2.
698
The metaphor proved flexible enough for fur-
ther elaboration. John Major, a Conservative PM
of Britain, spoke on June 1st, 1994 about his vi-
sion of the decision making at the EU level, say-
ing that he had never believed that Europe must
act as one on every issue, and advocating ?a sensi-
ble new approach, varying when it needs to, multi-
track, multi-speed, multi-layered.? He attempted
to turn a largely negative Conservative take on the
European train (see Thatcher above) into a tenable
positive vision ? each nation-carriage is now pre-
sumably a rather autonomous entity, waiting on a
side track for the right locomotive, in a huge yet
smoothly operating railroad system.
Major?s political opponents offered their
counter-frames. In both cases, the imagery of
a large transportation system was taken up, yet
turned around to suggest that ?multi, for every-
one? amounts to Britain being in ?the slow lane,?
and a different image was suggested that makes
the negative evaluation of Britain?s opt-outs
more poignant ? a football metaphor, where
relegation to the second division is a sign of a
weak performance, and a school metaphor, where
Britain is portrayed as an under-achiever:
John Cunningham, Labour He has admitted that his Go-
vernment would let Britain fall behind in Europe. He
is apparently willing to offer voluntary relegation to the
second division in Europe, and he isn?t even prepared to
put up a fight. I believe that in any two-speed Europe,
Britain must be up with those in the fast lane. Clearly
Mr Major does not.
Paddy Ashdown, Liberal Democrat Are you really saying
that the best that Britain can hope for under your leader-
ship is ... the slow lane of a two-speed Europe? Most
people in this country will want to aim higher, and will
reject your view of a ?drop-out? Britain.
The pro-European camp rallied around the
?Britain in the slow lane? version as a critical
stance towards the government?s European policy.
Of the alternative metaphors, the school metaphor
has some traction in the Euro discourse, where the
European (mainly German) financial officers are
compared to school authorities, and governments
struggling to meet the strict convergence criteria to
enter the Euro are compared to pupils that barely
make the grade with Britain as a ?drop-out? who
gave up even trying (Musolff, 2000).
The fact that European policy is being commu-
nicated and negotiated via a metaphor is not sur-
prising; after all, ?there is always someone willing
to help us think by providing us with a metaphor
that accords with HIS views.?2 From the point of
view of the dynamics of political discourse, the
puzzle is rather the apparent tendency of politi-
cians to be compelled by the rival?s metaphori-
cal framework. Thatcher tries to turn the train
metaphor used by the pro-EU camp around. Yet,
assuming metaphors are matters of choice, why
should Thatcher feel constrained by her rival?s
choice, why doesn?t she ignore it and merely sug-
gest a new metaphor of her own design? As the
evidence above suggests, this is not Thatcher?s
idiosyncrasy, as Major and his rivals acted simi-
larly. Can this dynamic be explained?
In this article, we use the explanatory frame-
work of game theory, seeking to rationalize the ob-
served behavior by designing a game that would
produce, at equilibrium, the observed dynamics.
Specifically, we formalize the notion that the price
of ?locking? the public into a metaphorical frame
of reference is that a politician is coerced into stay-
ing within the metaphor as well, even if he or she
is at the receiving end of a rival?s rhetorical move.
Since the use of game theory is not common in
computational linguistics, we first explain its main
attributes, justify our decision to make use of it,
and draw connections to research questions that
can benefit from its application (section 2). Next,
we design the game of bargaining over a metaphor,
and find its equilibrium (section 3), followed by a
discussion (section 4).
2 Game-Theoretic models
The basic construct is that of a game, that is,
a model of participants in an interaction (called
?players?), their goals (or ?utilities?) and allow-
able moves. Different moves yield different util-
ities for a player; it is assumed that each player
would pick a strategy that maximizes her utility.
The observable is the actual sequence of moves;
importantly, these are assumed to be the optimal
outcome (an equilibrium) of the relevant game. A
popular notion of equilibrium is Nash equilibrium
(Nash, 1950). For extensive form games (the type
employed in this paper), the notion of subgame
perfect equilibirum is typically used, denoting a
Nash equilibrium that would remain such if the
players start from any stage of the evolving game
(Selten (1975; 1965)).
The task of a game theorist is to reverse-
engineer the model for which the observed se-
2Capitalization in the original, Bolinger (1980, p. 146).
699
quence of actions is an equilibrium. The resulting
model is thereby able to rationalize the observed
behavior as a naturally emerging dynamics be-
tween agents maximizing certain utility functions.
In economics, game-theoretic models are used to
explain price change, organization of production,
and market failures (Mas-Colell et al, 1995; von
Neumann and Morgenstern, 1944); in biology ?
the operation of natural selection processes (Ax-
elrod and Hamilton, 1981; Maynard Smith and
Price, 1973); in social sciences ? political institu-
tions, collective action, and conflict (Greif, 2006;
Schelling, 1997; North, 1990). In recent appli-
cations in linguistics, pragmatic phenoma such as
implicatures are rendered as an equilibrium out-
come of a communication game (Ja?ger and Ebert,
2008; van Rooij, 2008; Ross, 2007; van Rooij and
Schulz, 2004; Parikh, 2001; Glazer and Rubin-
stein, 2001; Dekker and van Rooy, 2000).
Computing equilibria is simple for some games
and quite evolved for others. For example, com-
puting the equilibrium of a zero-sum game is equi-
valent to LP optimization (Luce and Raiffa, 1957);
an equilibrium of general bimatrix games can be
found using a pivoting algorithm (von Stengel,
2007; Lemke and Howson, 1964). Interesting
connections have been pointed out between game
theory and machine learning: Freund and Schapire
(1996) present both online learning and boosting
as a repeated zero-sum game; Shalev-Shwartz and
Singer (2006) show similarly that loss minimiza-
tion in online learning is akin to an equilibrium
path in a repeated game.
While game theoretic models are not much uti-
lized in computational linguistics, they are quite
attractive to tackle some of the problems com-
putational linguists are interested in. For exam-
ple, generation of referring expressions (Paraboni
et al, 2007; Gardent et al, 2004; Siddharthan
and Copestake, 2004; Dale and Reiter, 1995) can
be rendered as a communication game with util-
ity functions that reflect pressures to use shorter
expressions while avoiding excessive ambiguity
(Clark and Parikh, 2007), with corpora anno-
tated for entity mentions informing the design
of a model. Generally, computational linguis-
tics research produces algorithms to detect enti-
ties of various kinds, be it topics, named entities,
metaphors, moves in a multi-party conversations,
or syntactic constructions in large corpora; such
primary data can be used to trace developments
not only in chronological terms (Gruhl et al, 2004;
Allan, 2002), but in strategic terms, i.e. in terms
that reflect agendas of the actors, such as political
agendas in legislatures (Quinn et al, 2006) or ac-
tivist forums (Greene and Resnik, 2009), research
agendas in group meetings (Morgan et al, 2001),
or social agendas in speed-dates (Jurafsky et al,
2009). Game theoretical models are well suited
for modeling dynamics that emerge under multi-
ple, possibly conflicting constraints, as we exem-
plify in this article.
3 The model
We extend Rubinstein (1982) model of negotia-
tion through offers and counter-offers between two
players with a public benefit constraint.
The model consists of (1) two players repre-
senting the opposing sides, (2) a set of frames
X?Rn compact and convex, (3) preference re-
lations described by continuous utility func-
tions U1, U2:X?R+, (4) a sequence of frames
X0?X1 . . .?2X that can be suggested to the pub-
lic, and (5) a sequence of public preferences over
frames inXt for t=0, 1, 2, . . . described by a public
utility function Upt .
The game proceeds as follows. Initially the
frame is F0=X . In odd rounds player 1 appeals to
the public with a frame A1t?Xt|Ft , Xt|Ft={A?Xt :
A?Ft}, player 2 counters with a frame A2t?Xt|Ft .
The public chooses one of the frames based on
Upt (A
i
t) with ties broken in 1?s favor. The ac-
cepted frame becomes the current frame for the
next round Ft+1. In even rounds the parts of play-
ers 1 and 2 are reversed.
A finite sequence F0, . . . , Ft?1 gives the his-
tory of the bargaining process up to t. A
strategy ?i of player i is a function specify-
ing for any history h={F0, . . . , Ft?1} the move
player i makes at time t, namely the frame Ait
she chooses to address the public. A sequence
F0, F1, F2, F3, . . . describes a path the bargaining
process can take, leading to an outcome ??t=0Ft.
The players? utility for an outcome is given by
Ui=limt??
?
Ft
Ui(x)d?Ft for i=1, 2 where ?Ft is
a probability measure on Ft. If ??t=0Ft={x} the
utility is the point utility of x otherwise it is the
expected utility on the intersection set.
3.1 Player utility
For a given issue under discussion, such as Eu-
ropean integration process, we order the possible
700
states of the world along a single dimension that
spans the policy variations proposed by the diffe-
rent players (politicians). Politics of a single issue
are routinely modeled as lying on a single dimen-
sion.3 In the British context, various configura-
tions of the unfolding European reality are situated
along the line between high degree of integration
and complete separatism; Liberal Democrats are
the most pro-European party, while United King-
dom Independence Party are at the far-right end of
the scale, preferring British withdrawal from the
EU. The two major parties, Labour and Conserva-
tives (Tories), prefer intermediate left-leaning and
right-leaning positions, respectively. A schematic
description is shown in figure 1.
??????
???
???
??????
???
???
LibDem? Labour? Tories? UKIP?
????????????????????
? that is unfolding too fast 
? but it is possible to regulate the speed 
? in which case we?ll go slower than others 
??? ???
??????
Figure 1: Preferences on pro-anti Europe axis.
The utilities of the different players can in this
case be described as continuous single-peaked
functions over an interval.4 Thus X=[0, 1], and
the utility functions Ui(x)=?(||x? vi||) for vi?X
where ? is a monotonically strictly decreasing
function and || || is Euclidean distance.
3.2 Public utility
We note the difference between two types of util-
ities: The utility of the players is over outcomes,
the utility of the public is over sets of outcomes
(frames). The latter does not represent a utility the
public has for one outcome or another, but rather a
utility it has for an enhanced understanding. Thus,
the public?s utility from a frame is a function of
the information content of the proposed frame re-
lative to the current frame, i.e. the relative en-
tropy of the two sets.5 Formally, if the accepted
3Indeed, Poole and Rosenthal (1997) argue that no more
than two dimensions are needed to account for voting patterns
on all issues in the US Congress.
4Single-peakedness is a common assumption in position
modeling in political science (Downs, 1957).
5The notion that new beliefs are refinements of existing
ones is current in contemporary theorizing about formation
and change of beliefs, evaluations, and preferences. An up-
date based on the latest available information is consistent
with memory-based theories; in our model, in the equilib-
rium, the current frame contains information about the path-
so-far, thus early stages of the bargaining processes are in
some sense integrated into the current frame, compatible with
the rival, online model of belief formation. See Druckman
and Luria (2000) for a review of the relevant literature.
frame at time t is Ft then for any Borel set A?Ft
the public utility for A is Upt (A)=?(Entt(A))
where Entt(A)=??t(A) log ?t(A) for a continu-
ous probability measure ?t on Ft and ? is a con-
tinuous, monotone ascending function; for A 6?Ft,
Upt (A)=0. We take ?t to be the relative length of
the segment ?t(A)=
|A|
|Ft|
, hence the entropy maxi-
mizing subsegments are of length |Ft|2 .
3.3 Game dynamics
At every point in the game, a certain set of the
states-of-affairs is being deemed sufficiently pro-
bable by the public to require consideration. Sup-
pose that initially any state of affairs within the in-
terval [0, 1] is assigned a uniform probability and
thus merits public attention. Each in her turn, the
players propose to the public to concentrate on
a subset of the currently considered states of af-
fairs, arguing that those are the likelier ones to ob-
tain, hence merit further attention. The metaphor
used to deliver the proposal describes the newly
proposed subset in a way that makes those states-
of-affairs that are in it aligned with the metaphor,
whereas all other states are left out of the proposed
metaphorical frame. As the game proceeds, the
pub ic attention is concentrated on successively
smaller sets of eventualities, and these are given
a more and more detailed metaphoric description,
providing the educational gratification of increa-
singly knowing better and better what is going on.
At each step, each player strives to provide maxi-
mum public gratification while leading the public
to focus on the frame (i.e. subset of states of af-
fairs) that best meets the player?s preferences.6
Figure 2 sketches the frame negotiation through
train metaphor, from some point in time when the
general train metaphor got established, through
Thatcher?s flashing out the issue of excessive
speed and unclear direction, Major?s multi-track
corrective, and reply of his opponents on the left.
The final frame has all those states of affairs that
fit the extended metaphor ? everyone is acting
within the same broad system of rules, with Britain
and perhaps others sometimes wanting to negoti-
ate special, more gradual procedures, which would
leave Britain less tightly integrated into the com-
6We note that in our model every utterance has an impact
on the public for which the player bears the consequences and
is therefore a (costly) strategic move in the game. This is dif-
ferent from models of cheap talk such as Aumann (1990),
Lewis (1969) where communication is devoid of strategic
moves and is used primarily as a coordination device.
701
munity than some other European partners.Integration is likea train journey?? that isunfolding too fast? but it is possible toregulate the speed? in which case we?ll goslower than others
Figure 2: Bargaining over train metaphor.
3.4 The equilibrium
A pair of strategies (?1, ?2) is a Nash equilibrium
if there is no deviation strategy ? such that (?, ?2)
leads to an outcome with higher utility for player 1
than outcome of (?1, ?2) and the same for player
2. A subgame are all the possible moves following
a history h={F0, . . . , Ft}, in our case it is equi-
valent to a game with an initial frame Ft and the
corresponding utilities. A sub-strategy is that part
of the original strategy that is a strategy on the
subgame. A pair of strategies is a subgame per-
fect equilibrium if, for any subgame, their sub-
strategies are a Nash equilibrium.
Theorem 1 In the frame bargaining game with
single-peaked preferences
1. There exists a canonical subgame perfect
equilibrium path F0, F1, F2, . . . such that
??t=0Ft={x}.
2. For any subgame perfect equilibrium path
F ?0, F
?
1, F
?
2, . . . there exists T such that
??t=0F
?
t=?
T
t=0Ft.
The theorem states that the outcome of the bar-
gaining will always be a frame on the canoni-
cal path. The rivals would suggest more specific
frames either until convergence or until a situation
where any further specification would produce a
frame that ?misses their point,? so-to-speak, by re-
moving too much of the favorable outcome space
for both players. Figure 3 shows a situation where
parties could decide to stall on the current frame:
If player 1 has to choose between retaining F0, or
playing F1 which would result in the rival?s play-
ing F2, player 1 might choose to remain in F0 if
the utility of any outcome of the subgame starting
from F2 is lower than that of F0, as long as player
1 believes that player 2 would reason similarly.
F0 
F2 
F1 
Player 1 Player 2 
??? ???
Figure 3: Stalled bargaining.
The idea of the proof is to construct a pair of
strategies where each side attempts to pull the pub-
licly accepted frame in the direction of its peak
utility point. We show, assuming the peak of the
first mover is to the left of peak of the second, that
any deviation of the first mover would enable the
second to shift the public frame more to the right,
to an outcome of lower utility to the first mover.
The full details of the proof of part 1 are given in
the appendix; part 2 is proved in an accompanying
technical report.
The equilibrium exhibits the following prop-
erties: (a) a first mover?s advantage ? for any
player, the outcome would be closer to her peak
point if she moves first than if she moves second;
(b) a centrist?s advantage ? if a player moves first
and her peak is closer to the middle of the initial
frame, she can derive a higher utility from the out-
come than if her peak were further from the mid-
dle. Please see appendix for justifications.
4 Discussion
4.1 Political communication
This article studies some properties of frame bar-
gaining through metaphor in political communi-
cation, where rival politicians choose how to ela-
borate the current metaphor to educate the pub-
lic about the ongoing situation in a way most con-
sistent with their political preferences. Modeling
the public preferences as highest relative entropy
subset of possible states-of-affairs, we show that
strategic choices by the politicians lead to a sub-
game perfect equilibrium where the less politically
extreme player who moves first is at an advantage.
In a democracy, such player would typically be
the government, as the bulk of voters do not by
definition vote for extreme views, and since the
government is the agent that brings about changes
in the current states of affairs, and is thus the first
and most prepared to explain them to the public.
Indeed, Entman?s model of frame activation in po-
litical discourse is hierarchical, with the govern-
702
ment (administration) being the topmost frame-
activator, and opposition and media elites typi-
cally reacting to the administration?s frame (Ent-
man, 2003).
4.2 Metaphor in political communication
The role of metaphor in communication has long
been a subject of interest, with views ranging from
an ornament that beautifies the argument in the
ancient rhetorical traditions, to the contemporary
views of conceptual metaphor as permeating every
aspect of life (Lakoff and Johnson, 1980).
In political communication specifically,
metaphor has long been known as a framing
device. Framing can be defined as ?selecting
and highlighting some facets of events or issues,
and making connections among them in order to
promote a particular interpretation, evaluation,
or solution? (Entman, 2003). Metaphors are
notorious for allowing subliminal framing, where
the metaphor seems so natural that the aspects
of the phenomenon in question that do not align
with the metaphor are seamlessly concealed.
For example, WAR AS A COMPETITIVE GAME
metaphor emphasizes the glory of winning and the
shame of defeat, but hides the death-and-suffering
aspect of the war, which makes sports metaphors
a strategic choice when wishing to arouse a
pro-war sentiment in the audience (Lakoff, 1991).
Such subliminal framing can often be effectively
contested by merely exposing the frame.
Our examples show a different use of metaphor.
Far from being subliminal or covert, the details of
the metaphor, its implications, and the evaluation
promoted by any given version are an important
tool in the public discussion of a complex politi-
cal issue. The function of metaphorical framing
here resembles a pedagogical one, where render-
ing an abstract theory in physics (such as electri-
city) in concrete commonsensical terms (such as
water flow) is an effective strategy to enhance the
students? understanding of the former (Gentner
and Gentner, 1983). The measure of success for a
given version of the frame is its ability to sway the
public in the evaluative direction envisioned by the
author by providing sufficient educational benefit,
so-to-speak, that is, convincingly rendering a good
portion of a complex reality in accessible terms.
Once a frame is found that provides extensive
education benefit, such as the EUROPEAN INTE-
GRATION AS TRAIN JOURNEY above, a politi-
cian?s attempt to debunk a metaphor as inappropri-
ate risk public antagonism, as this would be akin
to taking the benefit of enhanced understanding
away. Thus, rather than contesting the validity of
the metaphoric frame, politicians strive to find a
way to turn the metaphor around, i.e. accept the
general framework, but focus on a previously un-
explored aspect that would lead to a different eva-
luative tilt. Our results show that being the first
to use an effective metaphor that manages to lock
the public in its framework is a strategic advantage
as the need to communicate with the same public
would compel the rival to take up the metaphor
of your choice. To our knowledge, this is the first
explanation of the use of extended metaphor in po-
litical communication on a complex issue in terms
of the agendas of the rival parties and the chang-
ing disposition of the public being addressed. It
is an open question whether similar ?locking in?
of the public can be attained by non-metaphorical
means, and whether the ensuing dynamics would
be similar.
4.3 Social dynamics
This article contributes to the growing literature on
modeling social linguistic behavior, like debates
(Somasundaran and Wiebe, 2009), dating (Juraf-
sky et al, 2009; Ranganath et al, 2009), colla-
borative authoring and editing in wikis (Leuf and
Cunningham, 2001) such as Wikipedia (Vuong et
al., 2008; Kittur et al, 2007; Vie?gas et al, 2004).
The latter literature in particular sees the social ac-
tivity as an unfolding process, for example, detec-
ting the onset and resolution of a controversy over
the content of a Wikipedia article through track-
ing article talk7 and deletion-and-reversion pat-
terns. Somewhat similarly to the metaphor debate
discussed in this article, Vie?gas et al (2004) note
first-mover advantage in Wikipedia authoring, that
is, the first version gives the tone for the subse-
quent edits and has its parts survive for relatively
many editing cycles. Finding out how the ini-
tial contribution constrains and guides subsequent
edits of the content of aWikipedia article and what
kind of argumentative strategies are employed in
persuading others to retain one?s contribution is an
interesting direction for future research.
A number of recent studies of the linguistic as-
pects of social processes are construed as if the
7a page separate from the main article that is devoted to
the discussion of the edits
703
events are taking place all-at-once ? there is no
differentiation between early and later stages of a
debate in Somasundaran and Wiebe (2009) or ini-
tial and subsequent speed-dates for the same sub-
ject in Jurafsky et al (2009). Yet adopting a dy-
namic perspective stands to reason in such cases.
For example, Somasundaran and Wiebe (2009)
built a system for recognizing stance in an online
debate (such as pro-iPhone or pro-Blackberry on
http://www.covinceme.net). They noticed that the
task was complicated by concessions ? acknow-
ledgments of some virtues of the competitor be-
fore stating own preference. This is quite possi-
bly an instance of debate dynamics whereby as the
debate evolves certain common ground emerges
between the sides and the focus of the debate
changes from the initial stage of elucidating which
features are better in which product to a stage
where the ?facts? are settled and acknowledged by
both sides and the debate moves to evaluation of
the relative importance of those features.
As another example, consider the construction
of statistical models of various emotional and per-
sonality traits based on a corpus of speed dates
such as Jurafsky et al (2009). Take the trait of
intelligence. In their experiment with speed-dates,
Fisman et al (2006) found that males tend to dis-
prefer females they perceive as more intelligent or
ambitious than themselves. Consequently, an in-
telligent female might choose to act less intelligent
in later rounds of speed dating if she has not so far
met a sufficiently intelligent male, assuming she
prefers a less-intelligent male to no match at all.
Better sensitivity to the dynamics of social pro-
cesses underlying the observed linguistic commu-
nication will we believe result in increased inte-
rest in game-theoretic models, as these are espe-
cially well suited to handle cases where the sides
have certain goals and adapt their moves based on
the current situations, the other side?s move, and
possibly other considerations, such as the need to
address effectively a wider audience, beyond the
specific interlocutors. A game theoretic explana-
tion advances the understanding of the process be-
ing modeled, and hence of the applicability, and
the potential adaptation, of statistical models de-
veloped on a certain dataset to situations that dif-
fer somewhat from the original data: For exam-
ple, a corpus with more rounds of speed-dates
per participant might suddenly make females seem
smarter, or a debate with a longer history would
feature more, and perhaps more elaborate, conces-
sions.
5 Empirical challenges
We suggested that models of dynamics such as
the one presented in this article be built over data
where entities of interest are clearly identified.
This article is based on chapters 1 and 2 of the
book by Musolff (2000) which itself is informed
by a corpus-linguistic analysis of metaphor in me-
dia discourse in Britain and Germany. We now
discuss the state of affairs in empirical approaches
to detecting metaphors.
5.1 Metaphors in NLP
Metaphors received increasing attention from
computational linguistics community in the last
two decades. The tasks that have been ad-
dressed are explication of the reasoning behind
the metaphor (Barnden et al, 2002; Narayanan,
1999; Hobbs, 1992); detection of conventional
metaphors between two specific domains (Mason,
2004); classification of words, phrases or sen-
tences as metaphoric or non-metaphoric (Krishna-
kumaran and Zhu, 2007; Birke and Sarkar, 2006;
Gedigian et al, 2006; Fass, 1991).
We are not aware of research on automatic
methods specifically geared to recognition of ex-
tended metaphors. Indeed, most computational
work cited above concentrates on the detection of
a local incongruity due to a violation of selectional
restrictions when the verb or one of its arguments
is used metaphorically (as in Protesters derailed
the conference). Extended metaphors are expected
to be difficult for such approaches, since many of
the clauses are completely situated in the source
domain and hence no local incongruities exist (see
examples on the first page of this article).
5.2 Data collection
Supervised approaches to metaphor detection need
to rely on annotated data. While metaphors are
ubiquitous in language, an annotation project that
seeks to narrow the scope of relevant metaphors
down to metaphors from a particular source do-
main (such as train journeys) that describe a par-
ticular target domain (such as European integra-
tion) and are uttered by certain entities (such as
senior UK politicians) face the problem of spar-
sity of the relevant data in the larger discourse: A
random sample of the size amenable to human an-
704
notation is unlikely to capture in sufficient detail
material pertaining to the one metaphor of interest.
To increase the likelihood of finding mentions
of the source domain, a lexicon of words from
the source domain can be used to select docu-
ments (Hardie et al, 2007; Gedigian et al, 2006).
Another approach is metaphor ?harvesting? ?
hypothesizing that metaphors of interest would oc-
cur in close proximity to lexical items representing
the target domain of the metaphor, such as the 4
word window around the lemma Europe used in
Reining and Lo?nneker-Rodman (2007).
5.3 Data annotation
A further challenge is producing reliable anno-
tations. Pragglejaz (2007) propose a methodo-
logy for testing metaphoricity of a word in dis-
course and report ?=0.56-0.70 agreement for a
group of six highly expert annotators. Beigman
Klebanov et al (2008) report ?=0.66 for detec-
ting paragraphs containing metaphors from the
source domains LOVE and VEHICLE with mul-
tiple non-expert annotators, though other source
domains that often feature highly conventiona-
lized metaphors (like structure or foundation from
BUILDLING domain) or are more abstract and dif-
ficult to delimit (such as AUTHORITY) present a
more challenging annotation task.
5.4 Measuring metaphors
A fully empirical basis for the kind of model pre-
sented in this paper would also involve defining
a metric on metaphors that would allow measu-
ring the frame chosen by the given version of the
metaphor relatively to other such frames ? that is,
quantifying which part of the ?integration is a train
journey? metaphor is covered by those states of af-
fairs that also fit Thatcher?s critical rendition.
6 Conclusion
This article addressed a specific communicative
setting (rival politicians trying to ?sell? to the pub-
lic their versions of the unfolding realities and ne-
cessary policies) and a specific linguistic tool (an
extended metaphor), showing that the particular
use made of metaphor in such setting can be ratio-
nalized based on the characteristics of the setting.
Various questions now arise. Given the cen-
tral role played by the public gratification con-
straint in our model, would conversational situa-
tions without the need to persuade the public, such
as meetings of small groups of peers or phone con-
versations between friends, tend less to the use of
extended metaphor? Conversely, does the use of
extended metaphor in other settings testify to the
existence of presumed onlookers who need to be
?captured? in a particular version of reality ? as
in pedagogic or poetic context?
Considerations of the participants? agendas and
their impact on the ensuing dynamics of the ex-
change would we believe lead to further interest in
game theoretic models when addressing complex
social dynamics in situations like collaborative
authoring, debates, or dating, and will augment
the existing mostly statistical approaches with a
broader picture of the relevant communication.
A Proof of Existence of a Subgame
Perfect Equilibrium
For a segment [a, b] and a?v1<v2?b let
U1(x)=?(||x ? v1||) and U2(x)=?(||x ? v2||)
be utility functions with peaks v1 and v2, re-
spectively. For a history h={F0, . . . , Ft} where
Ft=[lt, rt], let ??1(h), player 1?s move, be de-
fined as choosing Ft+1=[lt+1, rt+1] such that
|Ft+1|=
|Ft|
2 , and rt+1 is as close as possible to
v1. ??2 sets lt+1 with respect to v2 in a symmet-
ric fashion. Since Ft shrinks by half every round,
limt?? lt=limt?? rt=x?, converging to a point.
We now show (??1, ?
?
2) is an equilibrium by show-
ing that neither player has a profitable deviation.
Notice that after the first round the subgame is
identical to the initial game with F1 replacing F0,
and the roles of players reversed. Player 2 had no
influence on the choice of F1, hence she has a pro-
fitable deviation iff she has a profitable deviation
on the continuation subgame where she is the first
mover. It thus suffices to show that the first mover
(player 1) has no profitable deviations to establish
that (??1, ?
?
2) is an equilibrium.
Since by definition ??2 always chooses an en-
tropy maximizing segment, for player 1 to choose
a non-entropy maximizing segment (more or less
than half the length) amounts to yielding the round
to player 2, which is equivalent in terms of the re-
sulting accepted frame to a situation where player
1 chooses an entropy maximizing segment ? the
same one chosen by player 2. Thus we need to
consider only deviations with entropy maximizing
frames.
Step 1: Suppose ??1 is a strategy of player 1 and
let F ?0, F
?
1, F
?
2, . . . be the sequence of frames on
705
the path corresponding to the pair (??1, ?
?
2). Let
t0 be the first move deviating from the equilibrium
path, namely Ft0 6=F
?
t0 . We first show that Ft0?1
could not be (a) completely to the left of v1 or (b)
completely to the right of v2. Suppose (a) holds.
Then by definition rt0?2=rt0?1<v1, and, induc-
tively, r0=rt0?1<v1; this contradicts r0=1 that fol-
lows from F0=[0, 1]. Possibility (b) is similarly
refuted. Therefore, the only two cases for Ft0?1
with respect to v1 are depicted in figure 4. Note
that this implies v1?x??v2.
??? ???
Case 2: 
Case 1: Ft0?1
Ft0?1
rt0
Figure 4: Two cases of current frame location.
Step 2: In case 1, ??1 will choose frames of type
[lt, v1] for any t?t0, and ??2 will do the same on
any history in the continuation game, hence the
outcome will eventually be v1. As this is player 1?s
peak utility point, she has no profitable deviation.
Step 3: In case 2, Ft0 is the leftmost entropy
maximizing subsegment of Ft0?1 and the devia-
tion F ?t0 can only be a shift to the right namely
r?t0?rt0 . If player 2 could choose [v2, rt0+1] given
rt0 , she can still choose the same frame given r
?
t0 ,
so the outcome would be v2 and F ?t0 was not pro-
fitable. If player 2 could not choose [v2, rt0+1]
given rt0 , implying that x
?<v2, but as a result of
the deviation can now choose [v2, r?t0+1], imply-
ing that the outcome would be v2, clearly player
1 has not benefited from the deviation since U1
is descending right of v1. If player 2 still cannot
choose [v2, r?t0+1] after the deviation, she would
choose the rightmost entropy maximizing segment
with l?t0+1?lt0+1. If this still allows player 1 to
do [l?t0+2, v1] and hence to lead to v1 as the out-
come, it was possible in [lt0+2, v1] as well, so no
profit is gained by having deviated. Otherwise,
r?t0+2?rt0+2.
Step 3 can be repeated ad infinitum to show
that r?t?rt unless for some history h the de-
viation enables ?2(h)=[v2, r?t]. In the former
case we get limt?? r?t=x
??x?=limt?? rt where
??t=1F
?
t={x
?}. Since r?t and rt are to the right
of v1 and U1 is descending right of v1 it fol-
lows that U1(x?)?U1(x?). In the latter case
x??v2. Since Ft is never strictly to the right of v2,
x?=limt?? lt?v2?x?, therefore U1(x?)?U1(x?).
In either case the deviation ??1 cannot result in a
better outcome for player 1. This finishes the proof
that (??1, ?
?
2) is a Nash equilibrium.
Notice that (??1, ?
?
2) prescribe sub-strategies on
any subgame that are themselves Nash equilibria
for the subgames, hence (??1, ?
?
2) is a subgame per-
fect equilibrium 2
First Mover?s Advantage: The proof of step
3 shows that having the left boundary of the cur-
rent frame further to the right cannot yield a bet-
ter outcome for player 1. Yet, if player 1?s first
turn comes after that of player 2, she will start
with a current frame with the left boundary further
to the right than the initial frame before player 2
moved, since moving the left boundary is player
2?s equilibrium strategy. Hence a player would
never achieve a better outcome starting second if
both players are playing the canonical strategy.
Centrist?s Advantage: Let M be the middle of
F0. Consider a more extreme version of player 1
? player 1#. Suppose w.l.g. v#1 <v1?M . In case
v#1 <v1<v2, for all utilities u of the outcome of
dynamics vs player 2, if player 1# could attain u,
player 1 could attain u or more; the reverse is not
true, for example when |v#1 ? lt|<
|Ft|
2 ?|v1 ? lt|
and player 1 (or 1#) is moving first. In case
v2<v
#
1 <v1, if player 1 (or 1
#) moves first, she
is able to force her peak point as the outcome. If
v#1 <v2<v1, player 1 can force v1 as the outcome,
whereas player 1# would not necessarily be able
to force v#1 , as player 2 would pull the outcome
towards v2. Hence a first moving centrist is never
worse off, and often better off, than a first moving
extremist.
References
James Allan, editor. 2002. Topic Detection and Track-
ing: Event-Based Information Organization. Nor-
well, MA:Kluwer Academic Publishers.
Robert Aumann. 1990. Nash Equilibria are not Self-
Enforcing. In Jean J. Gabszewicz, Jean-Francois
Richard, and Laurence A. Wolsey, editors, Eco-
nomic Decision-Making: Games, Econometrics and
Optimisation, pages 201?206. Amsterdam: Elsevier.
Robert Axelrod and William D. Hamilton. 1981. The
evolution of cooperation. Science, 211(4489):1390?
1396.
John A. Barnden, Sheila R. Glasbey, Mark G. Lee, and
Alan M. Wallington. 2002. Reasoning in metaphor
706
understanding: The ATT-Meta approach and sys-
tem. In Proceedings of COLING, pages 121?128.
Beata Beigman Klebanov, Eyal Beigman, and Daniel
Diermeier. 2008. Analyzing disagreements. In
Ron Artstein, Gemma Boleda, Frank Keller, and
Sabine Schulte im Walde, editors, Proceedings of
COLING Workshop on Human Judgments in Com-
putational Linguistics, pages 2?7, Manchester, UK,
August. International Committee on Computational
Linguistics.
Julia Birke and Anoop Sarkar. 2006. A clustering ap-
proach for nearly unsupervised recognition of non-
literal language. In Proceedings of EACL, pages
329?336.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet Allocation. Journal of Ma-
chine Learning Resarch, 3:993?1022.
Dwight Bolinger. 1980. Language ? The Loaded
Weapon. London: Longman.
Robin Clark and Prashant Parikh. 2007. Game Theory
and Discourse Anaphora. Journal of Logic, Lan-
guage and Information, 16:265?282.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
18:233?263.
Paul Dekker and Robert van Rooy. 2000. Bi-
directional optimality theory: An application of
game theory. Journal of Semantics, 17(3):217?242.
Anthony Downs. 1957. An economic theory of politi-
cal action in a democracy. The Journal of Political
Economy, 65(2):135?150.
James Druckman and Arthur Luria. 2000. Preference
formation. Annual Review of Political Science, 2:1?
24.
Robert M. Entman. 2003. Cascading activation: Con-
testing the White House?s frame after 9/11. Political
Communication, 20:415?432.
Dan Fass. 1991. Met*: a method for discriminating
metonymy and metaphor by computer. Computa-
tional Linguistics, 17(1):49?90.
Raymond Fisman, Sheena Iyengar, Emir Kamenica,
and Itamar Simonson. 2006. Gender Differences
in Mate Selection: Evidence from a Speed Dat-
ing Experiment. Quarterly Journal of Economics,
121(2):673?697.
Yoav Freund and Robert E. Schapire. 1996. Game
theory, on-line prediction, and boosting. In Pro-
ceedings of the annual conference on Computational
Learning Theory, pages 325?332, Desenzano del
Garda, Italy, June -July.
Claire Gardent, Hlne Manue?lian, Kristina Striegnitz,
and Marilisa Amoia. 2004. Generating Definite De-
scriptions: Non-Incrementality, Inference and Data.
In Thomas Pechmann and Christopher Habel, ed-
itors, Multidisciplinary Approaches to Language
Production. Mouton de Gruyter.
Matt Gedigian, John Bryant, Srini Narayanan, and Bra-
nimir Ciric. 2006. Catching metaphors. In Pro-
ceedings of NAACL Workshop on Scalable Natural
Language Understanding, pages 41?48.
Deidre Gentner and Donald Gentner. 1983. Flowing
waters or teeming crowds: Mental models of electri-
city. In D. Gentner and A. Stevens, editors, Mental
models. Hillsdale, NJ: Lawrence Erlbaum.
Jacob Glazer and Ariel Rubinstein. 2001. Debates and
decisions: On a rationale of argumentation rules.
Games and Economic Behavior, 36(2):158?173.
Stephan Greene and Philip Resnik. 2009. More than
Words: Syntactic Packaging and Implicit Sentiment.
In Proceedings of NAACL, pages 503?511, Boulder,
CO, June.
Avner Greif. 2006. Institutions and the path to the
modern economy: Lessons from medieval trade.
Cambridge University Press.
Daniel Gruhl, R. Guha, David Liben-Nowell, and An-
drew Tomkins. 2004. Information diffusion through
blogspace. In Proceedings of the 13th international
conference on World Wide Web, pages 491?501.
Andrew Hardie, Veronika Koller, Paul Rayson, and
Elena Semino. 2007. Exploiting a semantic anno-
tation tool for metaphor analysis. In Proceedings
of the Corpus Linguistics Conference, Birmingham,
UK, Julyt.
Jerry Hobbs. 1992. Metaphor and abduction. In An-
drew Ortony, Jon Slack, and Oliviero Stock, editors,
Communication from an Artificial Intelligence Per-
spective: Theoretical and Applied Issues, pages 35?
58. Springer Verlag.
Gerhard Ja?ger and Christian Ebert. 2008. Prag-
matic Rationalizability. In Proceedings of the 13th
annual meeting of Gesellschaft fur Semantik, Sinn
und Bedeutung, pages 1?15, Stuttgart, Germany,
September-October.
Dan Jurafsky, Rajesh Ranganath, and Dan McFarland.
2009. Extracting social meaning: Identifying inter-
actional style in spoken conversation. In Proceed-
ings of NAACL, pages 638?646, Boulder, CO, June.
Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, and
Ed H. Chi. 2007. He says, she says: Conflict and co-
ordination in Wikipedia. In CHI-07: Proceedings of
the SIGCHI conference on Human Factors in Com-
puting Systems, pages 453?462, San Jose, CA, USA.
707
Saisuresh Krishnakumaran and Xiaojin Zhu. 2007.
Hunting elusive metaphors using lexical resources.
In Proceedings of NAACL Workshop on Computa-
tional Approaches to Figurative Language, pages
13?20.
George Lakoff and Mark Johnson. 1980. Metaphors
We Live By. Chicago University Press.
George Lakoff. 1991. Metaphor and war: The
metaphor system used to justify war in the Gulf.
Peace Research, 23:25?32.
Carlton E. Lemke and Joseph T. Howson. 1964.
Equilibrium Points of Bimatrix Games. Journal of
the Society for Industrial and Applied Mathematics,
12(2):413?423.
Bo Leuf and Ward Cunningham. 2001. The Wiki way:
quick collaboration on the Web. Boston: Addison-
Wesley.
David Lewis. 1969. Convention. Cambridge, MA:
Harvard University Press.
Robert D. Luce and Howard Raiffa. 1957. Games and
decisions. New York: John Wiley and Sons.
AndreuMas-Colell, Michael D.Whinston, and Jerry R.
Green. 1995. Microeconomic theory. Oxford Uni-
versity Press.
Zachary J. Mason. 2004. CorMet: a computational,
corpus-based conventional metaphor extraction sys-
tem. Computational Linguistics, 30(1):23?44.
John Maynard Smith and George R. Price. 1973. The
logic of animal conflict. Nature, 246(5427):15?18.
Nelson Morgan, Don Baron, Jane Edwards, Dan Ellis,
David Gelbart, Adam Janin, Thilo Pfau, Elizabeth
Shriberg, and Andreas Stolcke. 2001. The Meeting
Project at ICSI. In Proceedings of the HLT, pages
246?252, San Diego, CA.
Andreas Musolff. 2000. Mirror images of Europe:
Metaphors in the public debate about Europe in
Britain and Germany. Mu?nchen: Iudicium.
Srini Narayanan. 1999. Moving right along: A
computational model of metaphoric reasoning about
events. In Proceedings of AAAI, pages 121?128.
John F. Nash. 1950. Equilibrium points in n-person
games. Proceedings of the National Academy of Sci-
ences, 36(1):48?49.
Douglass C. North. 1990. Institutions, institutional
change, and economic performance. Cambridge
University Press.
Ivandr Paraboni, Kees van Deemter, and Judith Mas-
thoff. 2007. Generating Referring Expressions:
Making Referents Easy to Identify. Computational
Lingusitics, 33(2):229?254.
Prashant Parikh. 2001. The Use of Language. Stan-
ford: CSLI Publications.
Keith T. Poole and Howard Rosenthal. 1997.
Congress: A Political-Economic History of Roll Call
Voting. Oxford University Press.
Group Pragglejaz. 2007. MIP: A Method for Iden-
tifying Metaphorically Used Words in Discourse.
Metaphor and Symbol, 22(1):1?39.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th-108th
U.S. Senate. Unpublished Manuscript.
Rajesh Ranganath, Dan Jurafsky, and Dan McFarland.
2009. It?s not you, it?s me: Detecting flirting and
its misperception in speed-dates. In Proceedings of
EMNLP, pages 334?342, Singapore, August.
Astrid Reining and Birte Lo?nneker-Rodman. 2007.
Corpus-driven metaphor harvesting. In Proceedings
of the Workshop on Computational Approaches to
Figurative Language, pages 5?12, Rochester, New
York.
Ian Ross. 2007. Situations and Solution Concepts
in Game-Theoretic Approaches to Pragmatics. In
Ahti-Veikko Pietarinen, editor, Game Theory and
Linguistic Meaning, pages 135?147. Oxford, UK:
Elsevier Ltd.
Ariel Rubinstein. 1982. Perfect equilibrium in a bar-
gaining model. Econometrica, 50(1):97?109.
Thomas C. Schelling. 1997. The strategy of conflict.
Harvard University Press.
Reinhard Selten. 1965. Spieltheoretische behand-
lung eines oligopolmodells mit nachfragetra?gheit.
Zeitschrift fu?r die Gesamte Staatswissenschaft,
12:301?324.
Reinhard Selten. 1975. Re-examination of the Per-
fectness Concept for Equilibrium Points in Exten-
sive Form Games. International Journal of Game
Theory, 4:25?55.
Shai Shalev-Shwartz and Yoram Singer. 2006. Convex
Repeated Games and Fenchel Duality. In Proceed-
ings of NIPS, pages 1265?1272.
Advaith Siddharthan and Ann Copestake. 2004. Gen-
erating referring expressions in open domains. In
Proceedings of the ACL, pages 407?414, Barcelona,
Spain, July.
Swapna Somasundaran and Janyce Wiebe. 2009. Rec-
ognizing Stances in Online Debates. In Proceedings
of the ACL, pages 226?234.
Robert van Rooij and Katrin Schulz. 2004. Exhaustive
Interpretation of Complex Sentences. Journal of
Logic, Language and Information, 13(4):491?519.
708
Robert van Rooij. 2008. Games and Quantity
implicatures. Journal of Economic Methodology,
15(3):261?274.
Fernanda B. Vie?gas, Martin Wattenberg, and Kushal
Dave. 2004. Studying cooperation and conflict be-
tween authors with history flow visualizations. In
CHI-04: Proceedings of the SIGCHI conference on
Human Factors in Computing Systems, pages 575?
582, Vienna, Austria.
John von Neumann and Oskar Morgenstern. 1944.
Theory of games and economic behavior. Princeton
University Press.
Bernhard von Stengel. 2007. Equilibrium computa-
tion for two-player games in strategic and extensive
form. In Noam Nisan, Tim Roughgarden, Eva Tar-
dos, and Vijay Vazirani, editors, Algorithmic Game
Theory, pages 53?78. Cambridge University Press.
Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-Tam
Le, and Hady Wirawan Lauw. 2008. On ranking
controversies in Wikipedia: models and evaluation.
In Proceedings of the international conference on
Web Search and Web Data Mining, pages 171?182,
Palo Alto, CA, USA.
709
Proceedings of the ACL 2010 Conference Short Papers, pages 253?257,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Vocabulary Choice as an Indicator of Perspective
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
Northwestern University and Washington University in St. Louis
beata,d-diermeier@northwestern.edu, beigman@wustl.edu
Abstract
We establish the following characteris-
tics of the task of perspective classifi-
cation: (a) using term frequencies in a
document does not improve classification
achieved with absence/presence features;
(b) for datasets allowing the relevant com-
parisons, a small number of top features is
found to be as effective as the full feature
set and indispensable for the best achieved
performance, testifying to the existence
of perspective-specific keywords. We re-
late our findings to research on word fre-
quency distributions and to discourse ana-
lytic studies of perspective.
1 Introduction
We address the task of perspective classification.
Apart from the spatial sense not considered here,
perspective can refer to an agent?s role (doctor vs
patient in a dialogue), or understood as ?a par-
ticular way of thinking about something, espe-
cially one that is influenced by one?s beliefs or
experiences,? stressing the manifestation of one?s
broader perspective in some specific issue, or ?the
state of one?s ideas, the facts known to one, etc.,
in having a meaningful interrelationship,? stress-
ing the meaningful connectedness of one?s stances
and pronouncements on possibly different issues.1
Accordingly, one can talk about, say, opinion
on a particular proposed legislation on abortion
within pro-choice or pro-life perspectives; in this
case, perspective essentially boils down to opi-
nion in a particular debate. Holding the issue con-
stant but relaxing the requirement of a debate on a
specific document, we can consider writings from
pro- and con- perspective, in, for example, the
death penalty controversy over a course of a period
of time. Relaxing the issue specificity somewhat,
1Google English Dictionary, Dictionary.com
one can talk about perspectives of people on two
sides of a conflict; this is not opposition or sup-
port for any particular proposal, but ideas about
a highly related cluster of issues, such as Israeli
and Palestinian perspectives on the conflict in all
its manifestations. Zooming out even further, one
can talk about perspectives due to certain life con-
tingencies, such as being born and raised in a par-
ticular culture, region, religion, or political tradi-
tion, such perspectives manifesting themselves in
certain patterns of discourse on a wide variety of
issues, for example, views on political issues in the
Middle East from Arab vs Western observers.
In this article, we consider perspective at all
the four levels of abstraction. We apply the same
types of models to all, in order to discover any
common properties of perspective classification.
We contrast it with text categorization and with
opinion classification by employing models rou-
tinely used for such tasks. Specifically, we con-
sider models that use term frequencies as features
(usually found to be superior for text categoriza-
tion) and models that use term absence/presence
(usually found to be superior for opinion classi-
fication). We motivate our hypothesis that pre-
sence/absence features would be as good as or
better than frequencies, and test it experimentally.
Secondly, we investigate the question of feature
redundancy often observed in text categorization.
2 Vocabulary Selection
A line of inquiry going back at least to Zipf strives
to characterize word frequency distributions in
texts and corpora; see Baayen (2001) for a sur-
vey. One of the findings in this literature is that
a multinomial (called ?urn model? by Baayen)
is not a good model for word frequency distri-
butions. Among the many proposed remedies
(Baayen, 2001; Jansche, 2003; Baroni and Evert,
2007; Bhat and Sproat, 2009), we would like to
draw attention to the following insight articulated
253
most clearly in Jansche (2003). Estimation is im-
proved if texts are construed as being generated by
two processes, one choosing which words would
appear at all in the text, and then, for words that
have been chosen to appear, how many times they
would in fact appear. Jansche (2003) describes a
two-stage generation process: (1) Toss a z-biased
coin; if it comes up heads, generate 0; if it comes
up tails, (2) generate according to F (?), where
F (?) is a negative binomial distribution and z is a
parameter controlling the extent of zero-inflation.
The postulation of two separate processes is
effective for predicting word frequencies, but is
there any meaning to the two processes? The first
process of deciding on the vocabulary, or word
types, for the text ? what is its function? Jansche
(2003) suggests that the zero-inflation component
takes care of the multitude of vocabulary words
that are not ?on topic? for the given text, including
taboo words, technical jargon, proper names. This
implies that words that are chosen to appear are
all ?on topic?. Indeed, text segmentation studies
show that tracing recurrence of words in a text
permits topical segmentation (Hearst, 1997; Hoey,
1991). Yet, if a person compares abortion to infan-
ticide ? are we content with describing this word
as being merely ?on topic,? that is, having a certain
probability of occurrence once the topic of abor-
tion comes up? In fact, it is only likely to occur
if the speaker holds a pro-life perspective, while a
pro-choicer would avoid this term.
We therefore hypothesize that the choice of vo-
cabulary is not only a matter of topic but also
of perspective, while word recurrence has mainly
to do with the topical composition of the text.
Therefore, tracing word frequencies is not going to
be effective for perspective classification beyond
noting the mere presence/absence of words, dif-
ferently from the findings in text categorization,
where frequency-based features usually do better
than boolean features for sufficiently large voca-
bulary sizes (McCallum and Nigam, 1998).
3 Data
Partial Birth Abortion (PBA) debates: We use
transcripts of the debates on Partial Birth Abor-
tion Ban Act on the floors of the US House and
Senate in 104-108 Congresses (1995-2003). Simi-
lar legislation was proposed multiple times, passed
the legislatures, and, after having initially been ve-
toed by President Clinton, was signed into law
by President Bush in 2003. We use data from
278 legislators, with 669 speeches in all. We
take only one speech per speaker per year; since
many serve multiple years, each speaker is repre-
sented with 1 to 5 speeches. We perform 10-fold
cross-validation splitting by speakers, so that all
speeches by the same speaker are assigned to the
same fold and testing is always inter-speaker.
When deriving the label for perspective, it is im-
portant to differentiate between a particular leg-
islation and a pro-choice / pro-life perspective.
A pro-choice person might still support the bill:
?I am pro-choice, but believe late-term abortions
are wrong. Abortion is a very personal decision
and a woman?s right to choose whether to ter-
minate a pregnancy subject to the restrictions of
Roe v. Wade must be protected. In my judgment,
however, the use of this particular procedure can-
not be justified.? (Rep. Shays, R-CT, 2003). To
avoid inconsistency between vote and perspective,
we use data from pro-choice and pro-life non-
governmental organizations, NARAL and NRLC,
that track legislators? votes on abortion-related
bills, showing the percentage of times a legislator
supported the side the organization deems consis-
tent with its perspective. We removed 22 legisla-
tors with a mixed record, that is, those who gave
20-60% support to one of the positions.2
Death Penalty (DP) blogs: We use University
of Maryland Death Penalty Corpus (Greene and
Resnik, 2009) of 1085 texts from a number of pro-
and anti-death penalty websites. We report 4-fold
cross-validation (DP-4) using the folds in Greene
and Resnik (2009), where training and testing data
come from different websites for each of the sides,
as well as 10-fold cross-validation performance on
the entire corpus, irrespective of the site.3
Bitter Lemons (BL): We use the GUEST part
of the BitterLemons corpus (Lin et al, 2006), con-
taining 296 articles published in 2001-2005 on
http://www.bitterlemons.org by more than 200 dif-
ferent Israeli and Palestinian writers on issues re-
lated to the conflict.
Bitter Lemons International (BL-I): We col-
lected 150 documents each by a different per-
2Ratings are from: http://www.OnTheIssues.org/. We fur-
ther excluded data from Rep. James Moran, D-VA, as he
changed his vote over the years. For legislators rated by nei-
ther NRLC nor NARAL, we assumed the vote aligns with the
perspective.
3The 10-fold setting yields almost perfect performance
likely due to site-specific features beyond perspective per se,
hence we do not use this setting in subsequent experiments.
254
son from either Arab or Western perspectives
on Middle Eastern affairs in 2003-2009 from
http://www.bitterlemons-international.org/. The
writers and interviewees on this site are usually
former diplomats or government officials, aca-
demics, journalists, media and political analysts.4
The specific issues cover a broad spectrum, includ-
ing public life, politics, wars and conflicts, educa-
tion, trade relations in and between countries like
Lebanon, Jordan, Iraq, Egypt, Yemen, Morocco,
Saudi Arabia, as well as their relations with the
US and members of the European Union.
3.1 Pre-processing
We are interested in perspective manifestations
using common English vocabulary. To avoid the
possibility that artifacts such as names of senators
or states drive the classification, we use as features
words that contain only lowercase letters, possibly
hyphenated. No stemming is performed, and no
stopwords are excluded.5
Table 1: Summary of corpora
Data #Docs #Features # CV folds
PBA 669 9.8 K 10
BL 296 10 K 10
BL-I 150 9 K 10
DP 1085 25 K 4
4 Models
For generative models, we use two versions
of Naive Bayes models termed multi-variate
Bernoulli (here, NB-BOOL) and multinomial (here,
NB-COUNT), respectively, in McCallum and
Nigam (1998) study of event models for text cate-
gorization. The first records presence/absence of a
word in a text, while the second records the num-
ber of occurrences. McCallum and Nigam (1998)
found NB-COUNT to do better than NB-BOOL for
sufficiently large vocabulary sizes for text catego-
rization by topic. For discriminative models, we
use linear SVM, with presence-absence, norma-
lized frequency, and tfidf feature weighting. Both
types of models are commonly used for text clas-
sification tasks. For example, Lin et al (2006) use
4We excluded Israeli, Turkish, Iranian, Pakistani writers
as not clearly representing either perspective.
5We additionally removed words containing support, op-
pos, sustain, overrid from the PBA data, in order not to in-
flate the performance on perspective classification due to the
explicit reference to the upcoming vote.
NB-COUNT and SVM-NORMF for perspective clas-
sification; Pang et al (2002) consider most and
Yu et al (2008) all of the above for related tasks
of movie review and political party classification.
We use SVMlight (Joachims, 1999) for SVM and
WEKA toolkit (Witten and Frank, 2005; Hall et
al., 2009) for both version of Naive Bayes. Param-
eter optimization for all SVMmodels is performed
using grid search on the training data separately
for each partition into train and test data.6
5 Results
Table 2 summarizes the cross-validation results for
the four datasets discussed above. Notably, the
SVM-BOOL model is either the best or not signif-
icantly different from the best performing model,
although the competitors use more detailed textual
information, namely, the count of each word?s ap-
pearance in the text, either raw (NB-COUNT), nor-
malized (SVM-NORMF), or combined with docu-
ment frequency (SVM-TFIDF).
Table 2: Classification accuracy. Scores sig-
nificantly different from the best performance
(p2t<0.05 on paired t-test) are given an asterisk.
Data NB SVM
BOOL COUNT BOOL NORMF TFIDF
PBA *0.93 0.96 0.96 0.96 0.97
DP-4 0.82 0.82 0.83 0.82 0.727
DP-10 *0.88 *0.93 0.98 *0.97 *0.97
BL 0.89 0.88 0.89 0.86 0.84
BL-I 0.68 0.66 0.73 0.65 0.65
We conclude that there is no evidence for the
relevance of the frequency composition of the
text for perspective classification, for all levels of
venue- and topic-control, from the tightest (PBA
debates) to the loosest (Western vs Arab authors
on Middle Eastern affairs). This result is a clear
indication that perspective classification is quite
different from text categorization by topic, where
count-based features usually perform better than
boolean features. On the other hand, we have not
6Parameter c controlling the trade-off between errors
on training data and margin is optimized for all datasets,
with the grid c = {10?6, 10?5, . . . , 105}. On the DP
data parameter j controlling penalties for misclassification
of positive and negative cases is optimized as well (j =
{10?2, 10?1, . . . , 102}), since datasets are unbalanced (for
example, there is a fold with 27%-73% split).
7Here SVM-TFIDF is doing somewhat better than SVM-
BOOL on one of the folds and much worse on two other folds;
paired t-test with just 4 pairs of observations does not detect
a significant difference.
255
observed that boolean features are reliably better
than count-based features, as reported for the sen-
timent classification task in the movie review do-
main (Pang et al, 2002).
We note the low performance on BL-I, which
could testify to a low degree of lexical consolida-
tion in the Arab vs Western perspectives (more on
this below). It is also possible that the small size of
BL-I leads to overfitting and low accuracies. How-
ever, PBA subset with only 151 items (only 2002
and 2003 speeches) is still 96% classifiable, so size
alone does not explain low BL-I performance.
6 Consolidation of perspective
We explore feature redundancy in perspective
classification.We first investigate retention of only
N best features, then elimination thereof. As a
proxy of feature quality, we use the weight as-
signed to the feature by the SVM-BOOL model
based on the training data. Thus, to get the per-
formance with N best features, we take the N2
highest and lowest weight features, for the posi-
tive and negative classes, respectively, and retrain
SVM-BOOL with these features only.8
Table 3: Consolidation of perspective. Nbest
shows the smallest N and its proportion out of
all features for which the performance of SVM-
BOOL with only the best N features is not sig-
nificantly inferior (p1t>0.1) to that of the full
feature set. No-Nbest shows the largest num-
ber N for which a model without N best fea-
tures is not significantly inferior to the full model.
N={50, 100, 150, . . . , 1000}; for DP and BL-I, ad-
ditionally N={1050, 1100, ..., 1500}; for PBA, ad-
ditionally N={10, 20, 30, 40}.
Data Nbest No-Nbest
N % N %
PBA 250 2.6% 10 <1%
BL 500 4.9% 100 <1%
DP 100 <1% 1250 5.2%
BL-I 200 2.2% 950 11%
We observe that it is generally sufficient to use
a small percentage of the available words to ob-
tain the same classification accuracy as with the
full feature set, even in high-accuracy cases such
as PBA and BL. The effectiveness of a small
subset of features is consistent with the observa-
tion in the discourse analysis studies that rivals
8We experimented with the mutual information based fea-
ture selection as well, with generally worse results.
in long-lasting controversies tend to consolidate
their vocabulary and signal their perspective with
certain stigma words and banner words, that is,
specific keywords used by a discourse commu-
nity to implicate adversaries and to create sym-
pathy with own perspective, respectively (Teubert,
2001). Thus, in abortion debates, using infanti-
cide as a synonym for abortion is a pro-life stigma.
Note that this does not mean the rest of the fea-
tures are not informative for classification, only
that they are redundant with respect to a small per-
centage of top weight features.
When N best features are eliminated, perfor-
mance goes down significantly with even smaller
N for PBA and BL datasets. Thus, top features
are not only effective, they are also crucial for ac-
curate classification, as their discrimination capa-
city is not replicated by any of the other vocabu-
lary words. This finding is consistent with Lin
and Hauptmann (2006) study of perspective vs
topic classification: While topical differences be-
tween two corpora are manifested in difference in
distributions of great many words, they observed
little perspective-based variation in distributions
of most words, apart from certain words that are
preferentially used by adherents of one or the other
perspective on the given topic.
For DP and BL-I datasets, the results seem
to suggest perspectives with more diffused key-
word distribution (No-NBest figures are higher).
We note, however, that feature redundancy exper-
iments are confounded in these cases by either a
low power of the paired t-test with only 4 pairs
(DP) or by a high variance in performance among
the 10 folds (BL-I), both of which lead to nume-
rically large discrepancy in performance that is not
deemed significant, making it easy to ?match? the
full set performance with small-N best features as
well as without large-N best features. Better com-
parisons are needed in order to verify the hypo-
thesis of low consolidation.
In future work, we plan to experiment with ad-
ditional features. For example, Greene and Resnik
(2009) reported higher classification accuracies
for the DP-4 data using syntactic frames in which
a selected group of words appeared, rather than
mere presence/absence of the words. Another di-
rection is exploring words as members of seman-
tic fields ? while word use might be insufficiently
consistent within a perspective, selection of a se-
mantic domain might show better consistency.
256
References
Herald Baayen. 2001. Word frequency distributions.
Dordrecht: Kluwer.
Marco Baroni and Stefan Evert. 2007. Words
and Echoes: Assessing and Mitigating the Non-
Randomness Problem in Word Frequency Distribu-
tion Modeling. In Proceedings of the ACL, pages
904?911, Prague, Czech Republic.
Suma Bhat and Richard Sproat. 2009. Knowing the
Unseen: Estimating Vocabulary Size over Unseen
Samples. In Proceedings of the ACL, pages 109?
117, Suntec, Singapore, August.
Stephan Greene and Philip Resnik. 2009. More
than Words: Syntactic Packaging and Implicit Sen-
timent. In Proceedings of HLT-NAACL, pages 503?
511, Boulder, CO, June.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringe, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An up-
date. SIGKDD Explorations, 11(1).
Marti Hearst. 1997. TextTiling: Segmenting Text into
Multi-Paragraph Subtopic Passages. Computational
Linguistics, 23(1):33?64.
Michael Hoey. 1991. Patterns of Lexis in Text. Oxford
University Press.
Martin Jansche. 2003. Parametric Models of Linguis-
tic Count Data. In Proceedings of the ACL, pages
288?295, Sapporo, Japan, July.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Schlkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods -
Support Vector Learning. MIT Press.
Wei-Hao Lin and Alexander Hauptmann. 2006. Are
these documents written from different perspec-
tives? A test of different perspectives based on sta-
tistical distribution divergence. In Proceedings of
the ACL, pages 1057?1064, Morristown, NJ, USA.
Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and
Alexander Hauptmann. 2006. Which side are you
on? Identifying perspectives at the document and
sentence levels. In Proceedings of CoNLL, pages
109?116, Morristown, NJ, USA.
Andrew McCallum and Kamal Nigam. 1998. A com-
parison of event models for Naive Bayes text clas-
sification. In Proceedings of AAAI-98 Workshop
on Learning for Text Categorization, pages 41?48,
Madison, WI, July.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment Classification using
Machine Learning Techniques. In Proceedings of
EMNLP, Philadelphia, PA, July.
Wolfgang Teubert. 2001. A Province of a Federal
Superstate, Ruled by an Unelected Bureaucracy ?
Keywords of the Euro-Sceptic Discourse in Britain.
In Andreas Musolff, Colin Good, Petra Points, and
Ruth Wittlinger, editors, Attitudes towards Europe:
Language in the unification process, pages 45?86.
Ashgate Publishing Ltd, Hants, England.
Ian H. Witten and Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques.
Morgan Kaufmann, 2 edition.
Bei Yu, Stefan Kaufmann, and Daniel Diermeier.
2008. Classifying party affiliation from political
speech. Journal of Information Technology and Pol-
itics, 5(1):33?48.
257
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 390?396,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Difficult Cases: From Data to Learning, and Back
Beata Beigman Klebanov
?
Educational Testing Service
660 Rosedale Road
Princeton, NJ 08541
bbeigmanklebanov@ets.org
Eyal Beigman
?
Liquidnet Holdings Inc.
498 Seventh Avenue
New York, NY 10018
e.beigman@gmail.com
Abstract
This article contributes to the ongoing dis-
cussion in the computational linguistics
community regarding instances that are
difficult to annotate reliably. Is it worth-
while to identify those? What informa-
tion can be inferred from them regarding
the nature of the task? What should be
done with them when building supervised
machine learning systems? We address
these questions in the context of a sub-
jective semantic task. In this setting, we
show that the presence of such instances
in training data misleads a machine learner
into misclassifying clear-cut cases. We
also show that considering machine lear-
ning outcomes with and without the diffi-
cult cases, it is possible to identify specific
weaknesses of the problem representation.
1 Introduction
The problem of cases that are difficult for anno-
tation received recent attention from both the the-
oretical and the applied perspectives. Such items
might receive contradictory labels, without a clear
way of settling the disagreement. Beigman and
Beigman Klebanov (2009) showed theoretically
that hard cases ? items with unreliable annota-
tions ? can lead to unfair benchmarking results
when found in test data, and, in worst case, to a
degradation in a machi74ne learner?s performance
on easy, uncontroversial instances if found in the
training data. Schwartz et al (2011) provided an
empirical demonstration that the presence of such
difficult cases in dependency parsing evaluations
1
The work presented in this paper was done when the first
author was a post-doctoral fellow at Northwestern University,
Evanston, IL and the second author was a visiting assistant
professor at Washington University, St. Louis, MO.
leads to unstable benchmarking results, as diffe-
rent gold standards might provide conflicting an-
notations for such items. Reidsma and Carletta
(2008) demonstrated by simulation that systema-
tic disagreements between annotators negatively
impact generalization ability of classifiers built
using data from different annotators. Oosten et
al. (2011) showed that judgments of readability
of the same texts by different groups of experts
are sufficiently systematically different to hamper
cross-expert generalization of readability classi-
fiers trained on annotations from different groups.
Rehbein and Ruppenhofer (2011) discuss the ne-
gative impact of systematic simulated annotation
inconsistencies on active learning performance on
a word-sense disambiguation task.
In this paper, we address the task of classify-
ing words in a text as semantically new or old.
Using multiple annotators, we empirically identify
instances that show substantial disagreement be-
tween annotators. We then discuss those both from
the linguistic perspective, identifying some char-
acteristics of such cases, and from the perspec-
tive of machine learning, showing that the pres-
ence of difficult cases in the training data misleads
the machine learner on easy, clear-cut cases ? a
phenomenon termed hard case bias in Beigman
and Beigman Klebanov (2009). The main con-
tribution of this paper is in providing additional
empricial evidence in support of the argument put
forward in the literature regarding the need to pay
attention to problematic, disagreeable instances in
annotated data ? not only from the linguistic per-
spective, but also from a machine learning one.
2 Data
The task considered here is that of classifying first
occurrences of words in a text as semantically old
or new. One of goals of the project is to inves-
tigate the relationship between various kinds of
non-novelty in text, and, in particular, the rela-
390
tionship between semantic non-novelty (conceptu-
alized as semantic association with some preced-
ing word in the text), the information structure in
terms of given and new information, and the cog-
nitive status of discourse entities (Postolache et al,
2005; Birner and Ward, 1998; Gundel et al, 1993;
Prince, 1981). If an annotator identified an asso-
ciative tie from the target word back to some other
word in the text, the target word is thereby classi-
fied as semantically old (class 1, or positive); if no
ties were identified, it is classified as new (class 0,
or negative).
For the project, annotations were collected for
10 texts of various genres, where annotators were
asked, for every first appearance of a word in a
text, to point out previous words in the text that
are semantically or associatively related to it. All
data was annotated by 22 undergraduate and grad-
uate students in various disciplines who were re-
cruited for the task. During outlier analysis, data
from two annotators was excluded from considera-
tion, while 20 annotations were retained. This task
is fairly subjective, with inter-annotator agreement
?=0.45 (Beigman Klebanov and Shamir, 2006).
Table 1 shows the number and proportion of in-
stances that received the ?semantically old? (1) la-
bel from i annotators, for 0? i ? 20. The first col-
umn shows the number of annotators who gave the
label ?semantically old? (1). Column 2 shows the
number and proportion of instances that received
the label 1 from the number of annotators shown in
column 1. Column 3 shows the split into item dif-
ficulty groups. We note that while about 20% of
the instances received a unanimous 0 annotation
and about 12% of the instances received just one 1
label out of 20 annotators, the remaining instances
are spread out across various values of i. Reasons
for this spread include intrinsic difficulty of some
of the items, as well as attention slips. Since anno-
tators need to consider the whole of the preceding
text when annotating a given word, maintaining
focus is a challenge, especially for words that first
appear late in the text.
Our interest being in difficult, disagreeable
cases, we group the instances into 5 bands accor-
ding to the observed level of disagreement and
the tendency in the majority of the annotations.
Thus, items with at most two label 1 annotations
are clearly semantically new, while those with at
least 17 (out of 20) are clearly semantically old.
The groups Hard 0 and Hard 1 contain instances
# 1s # instances group
(proportion)
0 476 (.20) Easy 0
1 271 (.12) (.40)
2 191 (.08)
3 131 (.06) Hard 0
4 106 (.05) (.25)
5 76 (.03)
6 95 (.04)
7 85 (.04)
8 78 (.03)
9 60 (.03) Very
10 70 (.03) Hard
11 60 (.03) (.08)
12 57 (.02) Hard 1
13 63 (.03) (.13)
14 68 (.03)
15 49 (.02)
16 65 (.03)
17 60 (.03) Easy 1
18 72 (.03) (.14)
19 94 (.04)
20 99 (.04)
Table 1: Sizes of subsets by levels of agreement.
with at least a 60% majority classification, while
the middle class ? Very Hard ? contains instances
for which it does not appear possible to even iden-
tify the overall tendency.
In what follows, we investigate the learnabi-
lity of the classification of semantic novelty from
various combinations of easy, hard, and very hard
data.
3 Experimental Setup
3.1 Training Partitions
The objective of the study is to determine the use-
fulness of instances of various types in the training
data for semantic novelty classification. In parti-
cular, in light of Beigman and Beigman Klebanov
(2009), we want to check whether the presence of
less reliable data (hard cases) in the training set
adversely impacts performance on the highly reli-
able data (easy cases). We therefore test separately
on easy and hard cases.
We ran 25 rounds of the following experiment.
All easy cases are randomly split 80% (train) and
20% (test), all hard cases are split into train and
test sets in the same proportions. Then various
391
parts of the training data are used to train the 5 sys-
tems described in Table 2. We build models using
easy data; hard data; easy and hard data; easy,
hard, and very hard data; easy data and a weighted
sample of the hard data. The labels for very hard
data were assigned by flipping a fair coin.
System Easy Hard Very Hard
E +
H +
E+H + +
E+H+VH + + +
E+H
100
w
+ sample
1
Table 2: The 5 training regimes used in the experi-
ment, according to the parts of the data utilized for
training.
3.2 Machine Learning
We use linear Support Vector Machines classifier
as implemented in SVMLight (Joachims, 1999).
Apart from being a popular and powerful ma-
chine learning method, linear SVM is one of the
family of classifiers analyzed in Beigman and
Beigman Klebanov (2009), where they are theo-
retically shown to be vulnerable to hard case bias
in the worst case.
To represent the instances, we use two features
that capture semantic relatedness between words.
One feature uses Latent Semantic Analysis (Deer-
wester et al, 1990) trained on the Wall Street Jour-
nal articles to quantify the distributional similarity
of two words, the other uses an algorithm based
on WordNet (Miller, 1990) to calculate seman-
tic relatedness, combining information from both
the hierarchy and the glosses (Beigman Klebanov,
2006). For each word, we calculate LSA (Word-
Net) relatedness score for this word with each pre-
ceding word in the text, and report the highest pair-
wise score as the LSA (WordNet) feature value for
the given word. The values of the features can
be thought of as quantifying the strength of the
evidence for semantic non-newness that could be
obtained via a distributional or a dictionary-based
method.
1
The weight corresponds to the number of people who
marked the item as 1, for hard cases. We take a weighted
sample of 100 hard cases.
4 Results
We calculate the accuracy of every system sepa-
rately on the easy and hard test data. Table 3 shows
the results.
Train Test-E Test-H
Acc Rank Acc Rank
E 0.781 1 0.643 2
E+H 0.764 2 0.654 1
E+H+VH 0.761 2 0.650 1,2
H 0.620 3 0.626 3
E+H
100
w
0.779 1 0.645 2
Table 3: Accuracy and ranking for semantic no-
velty classification for systems built using various
training data and tested on easy (Test-E) and hard
(Test-H) cases. Systems with insignificant differ-
ences in performance (paired t-test, n=25, p>0.05)
are given the same rank.
We observe first the performance of the system
trained solely on hard cases (H in Table 3). This
system shows the worst performance, both on the
easy test and on the hard test. In fact, this system
failed to learn anything about the positive class in
24 out of the 25 runs, classifying all cases as nega-
tive. It is thus safe to conclude that in the feature
space used here the supervision signal in the hard
cases is too weak to guide learning.
The system trained solely on easy cases (E in
Table 3) significantly outperforms H both on the
easy and on the hard test. That is, easy cases are
more informative about the classification of hard
cases than the hard cases themselves. This shows
that at least some hard cases pattern similarly to
the easy ones in the feature space; SVM failed to
single them out when trained on hard cases alone,
but they are learnable from the easy data.
The system that trained on all cases ? both easy
and hard ? attains the best performance on hard
cases but yields to E on the easy test (Test-E). This
demonstrates what Beigman and Beigman Kle-
banov (2009) called hard case bias ? degradation
in test performance on easy cases due to hard cases
in the training data. The negative effect of using
hard cases in training data can be mitigated if we
only use a small sample of them (system E+H
100
w
);
yet neither this nor other schemes we tried of
selectively incorporating hard cases into training
data produced an improvement over E when tested
on easy cases (Test-E).
392
5 Discussion
5.1 Beyond worst case
Beigman and Beigman Klebanov (2009) per-
formed a theoretical analysis showing that hard
cases could lead to hard case bias where hard cases
have completely un-informative labels, with pro-
bability of p=0.5 for either label. These corre-
spond to very hard cases in our setting. According
to Table 3, it is indeed the case that adding the
very hard cases hurts performance, but not signif-
icantly so ? compare results for E+H vs E+H+VH
systems.
Our results suggest that un-informative labels
are not necessary for the hard case bias to sur-
face. The instances grouped under Hard 1 have
the probability of p=0.66 for class 1 and the in-
stances grouped under Hard 0 have the probabi-
lity of p=0.71 for class 0. Thus, while the labels
are somewhat informative, it is apparently the case
that the hard instances are distributed sufficiently
differently in the feature space from the easy cases
with the same label to produce a hard case bias.
Inspecting the distribution of hard cases (Fig-
ure 1), we note that hard cases do not follow
the worst case pattern analyzed in Beigman and
Beigman Klebanov (2009), where they were con-
centrated in an area of the feature space that was
removed far from the separation plane, a malig-
nant but arguably unlikely scenario (Dligach et al,
2010). Here, hard cases are spread both close and
far from the plane, yet their distribution is suffi-
ciently different from that of the easy cases to pro-
duce hard case bias during learning.Hard cases
00.10.2
0.30.40.5
0.60.70.8
0 0.2 0.4 0.6 0.8 1LSA score
WordNet score  
                
Easy Separator Hard "-"Easy+Hard Separator Hard "+"
Figure 1: Hard cases with separators learned from
easy and easy+hard training data.
5.2 The nature of hard cases
Figure 1 plots the hard instances in the two-
dimensional feature space: Latent Semantic Anal-
ysis score is shown on x-axis, and WordNet-based
score is shown on the y-axis. The red lines show
the linear separator induced when the system is
trained on easy cases only (system E in Table 3),
whereas the green line shows the separator in-
duced when the system is trained on both easy and
hard cases (system E+H).
It is apparent from the figure that the difference
in the distributions of the easy and the hard cases
lead to a lower threshold for LSA score when
WordNet score is zero and a higher threshold of
WordNet score when LSA score is zero in hard
vs easy cases. That is, the system exposed to hard
cases learned to trust LSA more and to trust Word-
Net less when determining that an instance is se-
mantically old than a system that saw only easy
cases at train time.
The tendency to trust WordNet less yields an
improvement in precision (92.1% for system E+H
on Test-E class 1 data vs 84% for system E on
Test-E class 1 data), which comes at a cost of a
drop in recall (42.2% vs 53.3%) on easy positive
cases. This suggests that high WordNet scores that
are not supported by distributional evidence are a
source of Hard 0 cases that made the system more
cautious when relying on WordNet scores.
The pattern of low LSA score and high Word-
Net score often obtains for rare senses of words:
Distributional evidence typically points away from
these senses, but they can be recovered through
dictionary definitions (glosses) in WordNet.
An example of hard 0 case involves a homony-
mous rare sense. Deck is used in the observation
deck sense in one of the texts. However, it was
found to be highly related to buy by WordNet-
based measure through the notion of illegal ? buy
in the sense of bribe and deck in the sense of a
packet of illegal drugs. This is clearly a spuri-
ous connection that makes deck appear semanti-
cally associated with preceding material, whereas
annotators largely perceived it as new.
Exposure to such cases at training time leads the
system to forgo handling rare senses that lack dis-
tributional evidence, thus leading to misclassifica-
tion of easy positive cases that exhibit a similar
pattern. Thus, stall and market are both used in the
sales outlet sense in one of the text. They come out
highly related by WordNet measure; yet in the 68
393
instances of stall in the training data for LSA the
homonymous verbal usage predominates. Simi-
larly, partner is overwhelmingly used in the busi-
ness partner sense in the WSJ data, hence wife and
partner come out distributionally unrelated, while
the WordNet based measure successfully recovers
these connections.
Our features, while rich enough to diagnose
a rare sense (low LSA score and high WordNet
score), do not provide information regarding the
appropriateness of the rare sense in context. Short
of full scale word sense disambiguation, we expe-
rimented with the idea of taking the second highest
pairwise score as the value of the WordNet fea-
ture, under the assumption that an appropriate rare
sense is likely to be related to multiple words in
the preceding text, while a spurious rare sense is
less likely to be accidentally related to more than
one preceding word. We failed to improve per-
formance, however; it is thus left for future work
to enrich the representation of the problem so that
cases with inappropriate rare senses can be differ-
entiated from the appropriate ones. In the context
of the current article, the identification of a parti-
cular weakness in the representation is an added
value of the analysis of the machine learning per-
formance with and without the difficult cases.
6 Related Work
Reliability of annotation is a concern widely
discussed in the computational linguistics litera-
ture (Bayerl and Paul, 2011; Beigman Klebanov
and Beigman, 2009; Artstein and Poesio, 2008;
Craggs and McGee Wood, 2005; Di Eugenio and
Glass, 2004; Carletta, 1996). Ensuring high re-
liability is not always feasible, however; the ad-
vent of crowdsourcing brought about interest in
algorithms for recovering from noisy annotations:
Snow et al (2008), Passonneau and Carpenter
(2013) and Raykar et al (2010) discuss methods
for improving over annotator majority vote when
estimating the ground truth from multiple noisy
annotations.
A situation where learning from a small num-
ber of carefully chosen examples leads to a better
performance in classifiers is discussed in the ac-
tive learning literature (Schohn and Cohn, 2000;
Cebron and Berthold, 2009; Nguyen and Smeul-
ders, 2004; Tong and Koller, 2001). Recent work
in the proactive active learning and multi-expert
active learning paradigms incorporates considera-
tions of item difficulty and annotator expertise into
an active learning scheme (Wallace et al, 2011;
Donmez and Carbonell, 2008).
In information retrieval, one line of work con-
cerns the design of evaluation schemes that reflect
different levels of document relevance to a given
query (Kanoulas and Aslam, 2009; Sakai, 2007;
Kek?al?ainen, 2005; Sormunen, 2002; Voorhees,
2001; J?arvelin and Kek?al?ainen, 2000; Voorhees,
2000). J?arvelin and Kek?al?ainen (2000) consider,
for example, a tiered evaluation scheme, where
precision and recall are reported separately for ev-
ery level of relevance, which is quite analogous
to the idea of testing separately on easy and hard
cases as employed here. The graded notion of
relevance addressed in the information retrieval
research assumes a coding scheme where people
assign documents into one of the relevance tiers
(Kek?al?ainen, 2005; Sormunen, 2002). In our case,
the graded notion of semantic novelty is a possible
explanation for the observed pattern of annotator
responses.
7 Conclusion
This article contributes to the ongoing discussion
in the computational linguistics community re-
garding instances that are difficult to annotate re-
liably ? how to identify those, and what to do
with them once identified. We addressed this is-
sue in the context of a subjective semantic task.
In this setting, we showed that the presence of
difficult instances in training data misleads a ma-
chine learner into misclassifying clear-cut, easy
cases. We also showed that considering machine
learning outcomes with and without the difficult
cases, it is possible to identify specific weaknesses
of the problem representation. Our results align
with the literature suggesting that difficult cases
in training data can be disruptive (Beigman and
Beigman Klebanov, 2009; Schwartz et al, 2011;
Rehbein and Ruppenhofer, 2011; Reidsma and
Carletta, 2008); yet we also show that investigat-
ing their impact on the learning outcomes in some
detail can provide insight about the task at hand.
The main contribution of this paper is there-
fore in providing additional empirical evidence in
support of the argument put forward in the litera-
ture regarding the need to pay attention to prob-
lematic, disagreeable instances in annotated data
? both from the linguistic and from the machine
learning perspectives.
394
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555?596.
Petra Saskia Bayerl and Karsten Ingmar Paul. 2011.
What determines inter-coder agreement in manual
annotations? a meta-analytic investigation. Comput.
Linguist., 37(4):699?725, December.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with Annotation Noise. In Proceedings
of the 47th Annual Meeting of the Association for
Computational Linguistics, pages 280?287, Singa-
pore, August.
Beata Beigman Klebanov and Eyal Beigman. 2009.
From Annotator Agreement to Noise Models. Com-
putational Linguistics, 35(4):493?503.
Beata Beigman Klebanov and Eli Shamir. 2006.
Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109?126.
Beata Beigman Klebanov. 2006. Measuring Seman-
tic Relatedness Using People and WordNet. In Pro-
ceedings of the Human Language Technology Con-
ference of the NAACL, Companion Volume: Short
Papers, pages 13?16, New York City, USA, June.
Association for Computational Linguistics.
Betty Birner and Gregory Ward. 1998. Information
Status and Non-canonical Word Order in English.
Amsterdam/Philadelphia: John Benjamins.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Nicolas Cebron and Michael Berthold. 2009. Active
learning for object classification: From exploration
to exploitation. Data Mining and Knowledge Dis-
covery, 18:283?299.
Richard. Craggs and Mary McGee Wood. 2005. Eval-
uating Discourse and Dialogue Coding Schemes.
Computational Linguistics, 31(3):289?296.
Scott Deerwester, Susan Dumais, George Furnas,
Thomas Landauer, and Richard Harshman. 1990.
Indexing by Latent Semantic Analysis. Journal
of the American Society For Information Science,
41:391?407.
Barbara Di Eugenio and Michael Glass. 2004. The
kappa statistic: a second look. Computational Lin-
guistics, 30(1):95?101.
Dmitriy Dligach, Rodney Nielsen, and Martha Palmer.
2010. To Annotate More Accurately or to Annotate
More. In Proceedings of the 4th Linguistic Annota-
tion Workshop, pages 64?72, Uppsala, Sweden, July.
Pinar Donmez and Jaime G. Carbonell. 2008. Proac-
tive learning: Cost-sensitive active learning with
multiple imperfect oracles. In Proceedings of the
17th ACM Conference on Information and Knowl-
edge Management, CIKM ?08, pages 619?628, New
York, NY, USA. ACM.
Jeanette Gundel, Nancy Hedberg, and Ron Zacharski.
1993. Cognitive status and the form of referring ex-
pressions in discourse. Language, 69:274?307.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2000. IR
Evaluation Methods for Retrieving Highly Relevant
Documents. In Proceedings of the 23th Annual In-
ternational Conference on Research and Develop-
ment in Information Retrieval, pages 41?48, Athens,
Greece, July.
Thorsten Joachims. 1999. Advances in Kernel
Methods - Support Vector Learning. In Bern-
hard Schlkopf, Christopher Burges, and Alexander
Smola, editors, Making large-scale SVM learning
practical, pages 169?184. MIT Press.
Evangelos Kanoulas and Javed Aslam. 2009. Empir-
ical Justification of the Gain and Discount Function
for nDCG . In Proceedings of the 19th ACM Confer-
ence on Information and Knowledge Management,
pages 611?620, Hong Kong, November.
Jaana Kek?al?ainen. 2005. Binary and graded relevance
in IR evaluations ? Comparison of the effects on
ranking of IR systems. Information Processing and
Management, 41:1019?1033.
George Miller. 1990. WordNet: An on-line lexical
database. International Journal of Lexicography,
3(4):235?312.
Hieu Nguyen and Arnold Smeulders. 2004. Ac-
tive Learning Using Pre-clustering. In Proceedings
of 21st International Conference on Machine Lear-
ning, pages 623?630, Banff, Canada, July.
Philip Oosten, Vronique Hoste, and Dries Tanghe.
2011. A posteriori agreement as a quality mea-
sure for readability prediction systems. In Alexan-
der Gelbukh, editor, Computational Linguistics and
Intelligent Text Processing, volume 6609 of Lec-
ture Notes in Computer Science, pages 424?435.
Springer Berlin Heidelberg.
Rebecca J. Passonneau and Bob Carpenter. 2013. The
benefits of a model of annotation. In Proceedings of
the 7th Linguistic Annotation Workshop and Interop-
erability with Discourse, pages 187?195, Sofia, Bul-
garia, August. Association for Computational Lin-
guistics.
Oana Postolache, Ivana Kruijff-Korbayova, and Geert-
Jan Kruijff. 2005. Data-driven approaches for in-
formation structure identification. In Proceedings
of Human Language Technology Conference and
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 9?16, Vancouver, British
Columbia, Canada, October.
395
Ellen Prince. 1981. Toward a taxonomy of given-new
information. In Peter Cole, editor, Radical Prag-
matics, pages 223?255. Academic Press.
Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-
ardo Hermosillo Valadez, Charles Florin, Luca Bo-
goni, and Linda Moy. 2010. Learning from crowds.
J. Mach. Learn. Res., 11:1297?1322, August.
Ines Rehbein and Josef Ruppenhofer. 2011. Evaluat-
ing the impact of coder errors on active learning. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies - Volume 1, HLT ?11, pages 43?
51, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Dennis Reidsma and Jean Carletta. 2008. Reliability
Measurement without Limits. Computational Lin-
guistics, 34(3):319?326.
Tetsuya Sakai. 2007. On the reliability of information
retrieval metrics based on graded relevance. Infor-
mation Processing and Management, 43:531?548.
Greg Schohn and David Cohn. 2000. Less is more:
Active Learning with Support Vector Mfachines. In
Proceedings of 17th International Conference on
Machine Learning, pages 839?846, San Francisco,
July.
Roy Schwartz, Omri Abend, Roi Reichart, and Ari
Rappoport. 2011. Neutralizing linguistically prob-
lematic annotations in unsupervised dependency
parsing evaluation. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 663?672, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast?but is it
good?: Evaluating non-expert annotations for nat-
ural language tasks. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 254?263, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Eero Sormunen. 2002. Liberal relevance criteria of
TREC ? Counting on negligible documents? In
Proceedings of the 25th Annual International ACM
SIGIR Conference on Research and Development
in Information Retrieval, pages 324?330, Tampere,
Finland, August.
Simon Tong and Daphne Koller. 2001. Support Vec-
tor Machine active learning with applications to text
classification. Journal of Machine Learning Re-
search, 2:45?66.
Ellen Voorhees. 2000. Variations in relevance judge-
ments and the measurement of retrieval effective-
ness. Information Processing and Management,
36:697?716.
Ellen Voorhees. 2001. Evaluation by highly relevant
documents. In Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 74?82,
New Orleans, LA, USA, September.
B. Wallace, K. Small, C. Brodley, and T. Trikalinos,
2011. Who Should Label What? Instance Alloca-
tion in Multiple Expert Active Learning, chapter 16,
pages 176?187.
396
Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 2?7
Manchester, August 2008
Analyzing Disagreements
Beata Beigman Klebanov, Eyal Beigman, Daniel Diermeier
Kellogg School of Business
Northwestern University
{beata,e-beigman,d-diermeier}@northwestern.edu
Abstract
We address the problem of distinguishing
between two sources of disagreement in
annotations: genuine subjectivity and slip
of attention. The latter is especially likely
when the classification task has a default
class, as in tasks where annotators need to
find instances of the phenomenon of inter-
est, such as in a metaphor detection task
discussed here. We apply and extend a data
analysis technique proposed by Beigman
Klebanov and Shamir (2006) to first dis-
till reliably deliberate (non-chance) anno-
tations and then to estimate the amount of
attention slips vs genuine disagreement in
the reliably deliberate annotations.
1 Introduction
Classification tasks fall into two broad categories.
Those in the first category proceed by requiring
that every item is explicitly assigned a tag out of
a given set of tags; part-of-speech tagging is an
example (Santorini, 1990).
In the second group of tasks, the annotator is
asked to identify a phenomenon of interest, thus
implicitly classifying items as belonging to the
phenomenon (marked) and not belonging to it (left
unmarked). When the studied phenomenon is ex-
pected to have low incidence, this is a time-saving
strategy, as annotators do not need to bother with
explicitly marking (almost) everything as a non-
phenomenon. A recent example of such a task is
Beigman Klebanov and Shamir (2006), where an-
notators were asked to provide anchors for words
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported
license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
deemed anchored in the text (i.e. associatively
connected to a previous item in the text), thus leav-
ing words that did not receive an anchor implic-
itly marked as un-anchored. Psychological exper-
iments where people are asked to respond to the
occurrence of a given phenomenon can also be
viewed as implicit classifications; for example, see
Spiro?s (2007) work on identification of bound-
aries of musical phrases by listeners. The task
of metaphor detection discussed in this paper also
falls under the implicit classification category.
While such a strategy uses annotators? time effi-
ciently, some of the observed disagreements could
be due to an annotator missing an occurrence of
the relevant phenomenon, rather than genuinely
disagreeing on the matter of occurrence.
We show in section 2 that our metaphor
identification task features less-than-perfect inter-
annotator agreement. Section 3 uses Beigman Kle-
banov and Shamir?s (2006) methodology to find
annotations that can be reliably attributed to a de-
liberate decision by at least some of the annotators.
We then discuss the use of validation experiment to
distinguish between slips of attention and genuine
disagreements (sections 4,5).
2 Metaphor Detection Study
For a project studying the use of metaphors in pub-
lic discourse, a dataset of 151 articles from the
British press was subjected to annotation.1 Partic-
ipants were asked to mark paragraphs that contain
occurrences of metaphors from LOVE, VEHICLE,
AUTHORITY and BUILDING domains (hence-
forth, metaphor types).
For example, the following paragraph in 20
September 1992 issue of Sunday Times contains an
1This is part of the data discussed in (Musolff, 2000).
2
extended metaphor from the VEHICLE domain:
Thatcher warned EC leaders to stop their
endless round of summits and take no-
tice of their own people. ?There is a
fear that the European train will thunder
forward, laden with its customary cargo
of gravy, towards a destination nei-
ther wished for nor understood by elec-
torates. But the train can be stopped,?
she said.
The title2 of one of the articles in the 19 Octo-
ber 1999 issue of The Guardian contains a LOVE
metaphor:
Euro-flirting is not only a matter of de-
sire.
The discussion in this paper is based on the out-
put of 9 annotators who performed metaphor iden-
tification (henceforth, production task), and of 7
annotators (out of 9) who took part in the sub-
sequent validation study (henceforth, validation
task). Subjects were not told about validation until
after they finished production on the whole of the
dataset. A time gap of 2 weeks existed between
the end of the production study and the start of
the validation, each of the tasks taking 6 weeks,
in weekly installments of 25 articles each.
For the production task, the annotators were
instructed to mark every paragraph where a
metaphor from the given metaphor type appeared;
the 151-article dataset yields 2364 paragraphs.
This paradigm corresponds to the implicit clas-
sification task discussed earlier, in that only the
positive (metaphor-containing) cases are given an
explicit markup. The incidence of positive cases
is quite low ? VEHICLE, the most ubiquitous
type, featured in 4% of the paragraphs, on average
across annotators.
We note that the appearances of the different
metaphor types are not mutually exclusive, and,
indeed, there is no a-priori reason to suppose
any relationship between them. For example, the
following paragraph from the leading article in
15 November 1995 issue of The Guardian was
marked by some annotators as containing both
LOVE and VEHICLE metaphors:
The first European bank notes - proba-
bly to be called ?euros? - will not be in
2A title is treated as a paragraph in our annotations.
circulation until 2002 judging by yester-
day?s report from the European Mone-
tary Institute. But this doesn?t mean that
monetary union has been delayed be-
yond 1999 because the printing of Euro-
pean bank notes will have been preceded
by a period of three years when na-
tional currencies will have been locked
together in indissoluble monetary matri-
mony [...] Although France looks as if it
might buckle under the strain of meet-
ing the fiscal criteria and in Germany
the SDP is having doubts (though only
about whether the new currency will be
strong enough) the Maastricht train is
still theoretically on the rails. Nobody
has changed the timetable.
We therefore treat the detection of metaphors
from each metaphor type as a separate binary
classification task. Table 1 shows the inter-
annotator agreement for the production task using
the ? statistic (Carletta, 1996; Krippendorff, 1980;
Siegel and Castellan, 1988).
Table 1: Metaphor annotation data (production),
by metaphor type. The third column shows the
percentage of paragraphs (out of 2364) marked as
having a metaphor of the given type, on average
across 9 annotators.
Type ? marked
VEHICLE 0.66 4.0%
LOVE 0.66 2.5%
AUTHORITY 0.39 2.7%
BUILD 0.43 1.7%
Clearly, it is not the case that the whole of the
dataset was reliably annotated, even for the better-
agreed-upon metaphor types like VEHICLE and
LOVE. Hence, additional procedures are needed
to distill reliable annotations. We apply Beigman
Klebanov and Shamir?s (2006) statistical tech-
nique to find a subset of the data that is sufficiently
reliable, and later corroborate the statistical analy-
sis through the validation task.
3 Reliably Deliberate Annotations
In Beigman Klebanov and Shamir (2006), 22
subjects performed the anchoring annotation; the
overall inter-annotator agreement was ?=0.45.
3
Thus, some of the data was clearly unreliable, as
in our metaphor detection task, but the possibility
existed that some other part was in fact annotated
sufficiently reliably.
Beigman Klebanov and Shamir?s (2006) analy-
sis proceeded thus: Suppose each of the 20 anno-
tators3 (i = 1...20) was flipping a coin with the
probability of heads p
i
equal to the proportion of
?anchored? markups in annotator i?s data. What
is the level of agreement for which this scenario is
sufficiently improbable? For their data, the random
anchoring hypothesis could be rejected with 99%
confidence for cases marked by at least 13 people.
Items featuring at least this level of agreement can
be considered, with high probability, as deliber-
ately annotated as ?anchored?, as at least some of
those who marked them were not flipping a coin.
Following the procedure in Beigman Klebanov
and Shamir (2006), we wish to determine a re-
liably deliberate subset of our metaphor annota-
tions. We induce 9 random pseudo-annotators
from the 9 actual ones, each marking paragraphs
at random as containing a metaphor of a given
type or not. Pseudo-annotator i flips a coin
with p(heads) = p
i
, which is the proportion of
metaphor markups by the i?th annotator for the
most common metaphor type (VEHICLE).
Assuming each annotator flips her coin, we cal-
culate the probability of 3 or more coins coming up
heads simultaneously;4 this probability is 0.0045.
Thus, with 99.5% confidence, a metaphor markup
by at least 3 people is not a result of coinflip, at
least for some of the annotators. We note, how-
ever, that 99.5% confidence is insufficient for our
case: It allows for random highly agreed markup
in 0.5% of the instances. Given that only up to
4% of the instances have positive markups, this
would yield a high percentage of random items
in the positive instances. The probability of 4 or
more pseudo-annotators having their coins come
up heads simultaneously is below 0.0003; we con-
sider this sufficient confidence for our case, and
regard metaphor markups produced by at least 4
people as reliably deliberate.
Note that we cannot find a similar threshold for
no-metaphor annotations, as a lack of metaphor
3Two people were excluded as outliers.
4In Beigman Klebanov and Shamir (2006), a normal ap-
proximation is used to handle collective decision making by
20 pseudo-annotators. In the current case, 9 annotators is a
sufficiently small number to allow an exact probability calcu-
lation over the 512 possibilities.
annotation could happen by chance with a high
probability (p = 0.69). In view of the potential use
of the dataset for evaluating metaphor detection
algorithms, a putative metaphor suggested by the
algorithm cannot be rejected based on the lack of
metaphor annotation in the data. A complementary
procedure would be needed, for example, collect-
ing human judgments for the putative metaphors
separately.
4 Attention Slips vs Genuine
Disagreements
Deliberate annotation does not guarantee agree-
ment. It remained the case that some of the reliably
deliberate data in Beigman Klebanov and Shamir
(2006) was actually produced by only some of the
original subjects. Indeed, some of the deliberately
marked metaphors were annotated by only 4 out
of the 9 participants. For cases where the posi-
tive annotations were produced deliberately, what
is the status of negative annotations accorded to
the same items? Were these mere attention slips,
or genuine differences of opinion? Note that this
question cannot be meaningfully posed regarding
the parts of annotations for which the hypothesis
of random positive marking could not be rejected
with sufficiently high probability, since, obviously,
apparent disagreements there could be simply a re-
sult of different coinflip outcomes.
Beigman Klebanov and Shamir (2006) hypoth-
esized that dissenting annotations of the reliable
pairs would be cases of attention slips, rather than
genuine differences of opinion. In other words,
while there was no initial agreement, these items
were potentially agreeable. To test the hypothe-
sis, they devised a validation experiment, where
subjects were presented with all pairs marked by
at least one annotator, plus some random pairs,
and were asked to cross out things they disagree
with. The reasoning was as follows: If attention
slip was the cause for a dissenting negative anno-
tation, when the subject is asked about the relevant
item, i.e. it is explicitly brought to her attention,
she would accept it, whereas if a case is that of
a genuine disagreement, she would reject it. To
control for the possibility that people just accept
everything so that not to be dissonant with others,
some random annotations were also included.
The results reported by Beigman Klebanov and
Shamir (2006) largely bore out the hypothesis.
First, people did not tend to accept everything,
4
as only 15% of judgments of random annota-
tions and only 62% of judgments on all human-
generated annotations were ?accept? judgments.
However, 94% of judgments of the reliable anno-
tations were ?accept? judgments. Hence, the rate
of genuine disagreement on the reliably deliberate
part of Beigman Klebanov and Shamir?s (2006)
data turned out to be quite low.
We are interested in estimating the degree of
genuine disagreements in metaphor production.
Using Beigman Klebanov and Shamir?s method-
ology, we collected all paragraphs marked as con-
taining a metaphor of a given type by at least one
of the 9 annotators, plus added random markups.
This data was submitted to 7 subjects for valida-
tion.
Table 2: Percentage of ?Accept? validations for
random (Rand) and human (Hum) metaphor pro-
duction data, as well as for the partition of the
human data into reliably deliberate (Rel) and unre-
liable (URel) subsets. For each subset, the number
of data instances covered by the subset is shown.
Subscripts indicate metaphor type: (V)EHICLE,
(L)OVE, (A)UTHORITY, (B)UILD. The bottom
line shows the average over metaphor types.
Subset # Acc Subset # Acc
Rand
V
94 5% Hum
V
194 73%
Rand
L
56 6% Hum
L
137 64%
Rand
A
62 12% Hum
A
258 51%
Rand
B
40 1% Hum
B
126 68%
Rand 252 6% Hum 715 62%
Subset # Acc Subset # Acc
URel
V
92 49% Rel
V
102 94%
URel
L
81 43% Rel
L
56 95%
URel
A
218 42% Rel
A
40 96%
URel
B
86 55% Rel
B
40 96%
URel 477 46% Rel 238 95%
Table 2 reports the percentage of ?accept? votes
for random and human metaphor production data,
as well as for reliably deliberate and unreliable
subsets of the human data. As in Beigman Kle-
banov and Shamir?s case, the validation experi-
ment clearly distinguishes between random, hu-
man in general, and reliably deliberate subsets, and
puts the estimated degree of genuine disagreement
in metaphor identification at 5% on average, with
little variation across the metaphor types. That
is, given that, with high probability, at least some
humans deliberately identified a paragraph as con-
taining a metaphor, the chance for its rejection is
about 5%. The rest of observed production dis-
agreements, for the reliably deliberate subset, are
remedied at validation time, thus probably consti-
tuting attention slips during production. The reli-
ably deliberate subset contains 33% (238/715) of
all human-generated data.
5 Separating self and others
One potential confounder in the above analysis
is conflation of self-consistency with affirmation
of someone else?s annotations. It is possible that
many of the validation-time ?accept? votes are
cases of people accepting their own earlier annota-
tion; the proportion of such cases is expected to in-
crease the more people marked the metaphor dur-
ing production. Therefore, to get a more precise
estimate of the degree of genuine disagreement,
we control for self-affirmation, and calculate the
proportion of ?accept? validations in cases where
the person did not mark the metaphor during pro-
duction. Specifically, if X of the 7 people who par-
ticipated in both production and validation marked
the metaphor at production,5 we check the split of
the remaining 7-X votes during validation. Table 3
presents average other-affirmation rates for the re-
liably deliberate and unreliable human produced
data. Note that only 184 out of the 238 deliberately
reliable cases can be used, as the remaining 54 are
cases where all 7 annotators produced the markup,
so there is no disagreement.
Table 3: Percentage of ?Accept? validations for re-
liably deliberate (Rel) and unreliable (URel) sub-
sets of the metaphor production data, given that the
subject himself did NOT produce the metaphor.
Subset # Acc Subset # Acc
URel
V
92 44% Rel
V
78 90%
URel
L
81 39% Rel
L
38 92%
URel
A
218 35% Rel
A
30 91%
URel
B
86 53% Rel
B
38 91%
URel 477 41% Rel 184 91%
5The actual total of the production annotations could be up
to X+2, as there were 2 more annotators in production than
in validation.
5
According to the table, 91% cases of disagree-
ments in the reliably deliberate data are remedied
at validation time. That is, given that, with high
probability, at least some human deliberately iden-
tified a paragraph as containing a metaphor, the
chance for its rejection by a human who initially
apparently disagreed with the annotation is only
about 9%.
Finally, validation data allows an investigation
of the stability of people?s judgments by calculat-
ing self-rejection rates, i.e. estimating the prob-
ability of rejecting during validation an instance
that the same annotator marked as containing a
metaphor during production. Table 4 shows the
results.
Table 4: Percentage of ?Reject? validations for re-
liably deliberate (Rel) and unreliable (URel) sub-
sets of the metaphor production data, given that the
subject himself produced the annotation.
Subset # Rej Subset # Rej
URel
V
72 25% Rel
V
102 4%
URel
L
55 26% Rel
L
56 5%
URel
A
198 22% Rel
A
40 2%
URel
B
60 23% Rel
B
40 2%
URel 3856 23% Rel 238 4%
For the reliably deliberate data, i.e. cases where
at least 4 people produced the markup, the average
self-rejection rate is 4%. This low figure further
supports the designation of the reliably deliberate
subset as such, i.e. containing stable annotations,
as in 96% of the cases a person who produced the
markup is likely to re-affirm it when asked again,
even after a substantial time delay.7
For the ?unreliable? data, i.e. cases where only
one or two people marked the metaphor during
production, the average self-rejection rate is 23%.
Self-rejection means either that the initial positive
markup was a mistake, or that it is difficult for the
annotator to make up his mind about the annota-
tion of the item. In any case, high self-rejection
6Note that only 385 of the 477 items in the unreliable data
could be used for the calculation. The remaining items were
not produced by any of the 7 people who participated in both
production and validation, but only by one or both of the 2
additional production-task annotators.
7The time difference between production and validation
per article ranged between 4 and 8 weeks, due to differences
in the order in which the different subjects were given the
articles.
rate means that the relevant production annotations
cannot be trusted to contain a settled judgment that
could be then agreed or disagreed with by other an-
notators, or indeed replicated by a computational
model.
We consider self-rejected cases potential indica-
tors of a difficulty on the annotator?s part to de-
cide on the correct markup. We plan a more de-
tailed investigation of the materials to see whether
these cases exhibit any interesting common prop-
erties that could help characterize the difficulties in
metaphor identification task.
6 Conclusion
In this article, we showed an application of
Beigman Klebanov and Shamir?s (2006) method-
ology for analyzing annotation data to metaphor
identification annotations. The approach allowed
establishing an agreement threshold beyond which
the annotations are reliably deliberate, in the sense
that, with high probability, at least some of the
annotators who detected a metaphor were not flip-
ping a coin. This threshold is agreement of 4 out
of 9 annotators, for 99.9% reliability.
To investigate the nature of disagreements in the
reliably deliberate subset, we followed Beigman
Klebanov and Shamir (2006) in conducting a val-
idation study, where subjects were asked to ac-
cept or reject markups produced during the ini-
tial annotation study, as well as some random
annotations. Sharpening the methodology some-
what, we showed that in 91% of reliably deliber-
ate cases where an annotator did not produce the
markup himself, he accepted it during validation.
Hence, the bulk of the initial disagreements were
amended during validation, with the residual 9%
being likely locations for genuine difference of
opinion.
Further analysis of validation data revealed that
the reliably deliberate subset features low self-
rejection rates, meaning that people are consis-
tent with their own production. This was not the
case for the subset deemed unreliable during sta-
tistical analysis, where a 23% self-rejection rate
was observed. We hypothesize that some of these
would be hard-to-decide cases with respect to the
metaphor identification task, and hence warrant a
closer look in order to characterize annotator diffi-
culties with the task.
6
7 Acknowledgment
We would like to thank an anonymous reviewer
for a thorough and insightful review that helped as
improve this article significantly.
References
Beigman Klebanov, Beata and Eli Shamir. 2006.Reader-based exploration of lexical cohesion. Lan-
guage Resources and Evaluation, 40(2):109?126.
Carletta, Jean. 1996. Assessing agreement on clas-
sification tasks: the kappa statistic. Computational
Linguistics, 22(2):249?254.
Krippendorff, Klaus. 1980. Content Analysis. SagePublications.
Musolff, Andreas. 2000. Mirror images of Europe:
Metaphors in the public debate about Europe in
Britain and Germany. Mu?nchen: Iudicium.
Santorini, Beatrice. 1990. Part-of-speechtagging guidelines for the Penn Tree-
bank project (3rd revision, 2nd printing).ftp://ftp.cis.upenn.edu/pub/treebank/doc/tagguide.ps.gz.
Siegel, Sidney and John Castellan. 1988. Nonparamet-
ric Statistics for the Behavioral Sciences. McGraw-
Hill Book Company.
Spiro, Neta. 2007. What contributes to the percep-
tion of musical phrases in Western classical music?Ph.D. thesis, University of Amsterdam, The Nether-
lands.
7
