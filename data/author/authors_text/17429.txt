Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392?402,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Towards Situated Dialogue: Revisiting Referring Expression Generation
Rui Fang, Changsong Liu, Lanbo She, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University, East Lansing, MI, 48824, USA
{fangrui,cliu,shelanbo,jchai}@cse.msu.edu
Abstract
In situated dialogue, humans and agents have
mismatched capabilities of perceiving the
shared environment. Their representations
of the shared world are misaligned. Thus
referring expression generation (REG) will
need to take this discrepancy into consider-
ation. To address this issue, we developed
a hypergraph-based approach to account for
group-based spatial relations and uncertain-
ties in perceiving the environment. Our em-
pirical results have shown that this approach
outperforms a previous graph-based approach
with an absolute gain of 9%. However, while
these graph-based approaches perform effec-
tively when the agent has perfect knowledge
or perception of the environment (e.g., 84%),
they perform rather poorly when the agent has
imperfect perception of the environment (e.g.,
45%). This big performance gap calls for new
solutions to REG that can mediate a shared
perceptual basis in situated dialogue.
1 Introduction
Situated human robot dialogue has received increas-
ing attention in recent years. In situated dialogue,
robots/artificial agents and their human partners are
co-present in a shared physical world. Robots need
to automatically perceive and make inference of the
shared environment. Due to its limited perceptual
and reasoning capabilities, the robot?s representation
of the shared world is often incomplete, error-prone,
and significantly mismatched from that of its human
partner?s. Although physically co-present, a joint
perceptual basis between the human and the robot
cannot be established (Clark and Brennan, 1991).
Thus, referential communication between the hu-
man and the robot becomes difficult.
How this mismatched perceptual basis affects ref-
erential communication in situated dialogue was in-
vestigated in our previous work (Liu et al, 2012).
In that work, the main focus is on reference resolu-
tion: given referential descriptions from human part-
ners, how to identify referents in the environment
even though the robot only has imperfect percep-
tion of the environment. Since robots need to col-
laborate with human partners to establish a joint per-
ceptual basis, referring expression generation (REG)
becomes an equally important problem in situated
dialogue. Robots have much lower perceptual capa-
bilities of the environment than humans. How can
a robot effectively generate referential descriptions
about the environment so that its human partner can
understand which objects are being referred to?
There has been a tremendous amount of work
on referring expression generation in the last two
decades (Dale, 1995; Krahmer and Deemter, 2012).
However, most existing REG algorithms were devel-
oped and evaluated under the assumption that agents
and humans have access to the same kind of domain
information. For example, many experimental se-
tups (Gatt et al, 2007; Viethen and Dale, 2008;
Golland et al, 2010; Striegnitz et al, 2012) were
developed based on a visual world for which the in-
ternal representation is assumed to be known and
can be represented symbolically. However, this as-
sumption no longer holds in situated dialogue with
robots. There are two important distinctions in situ-
ated dialogue. First, the perfect knowledge of the en-
vironment is not available to the agent ahead of time.
The agent needs to automatically make inferences to
connect recognized lower-level visual features with
392
symbolic labels or descriptors. Both recognition and
inference are error-prone and full of uncertainties.
Second, in situated dialogue the agent and the hu-
man have mismatched representations of the envi-
ronment. The agent needs to take this difference into
consideration to identify the most reliable features
for REG. Given these two distinctions, it is not clear
whether state-of-the-art REG approaches are appli-
cable under mismatched perceptual basis in situated
dialogue.
To address this issue, this paper revisits the prob-
lem of REG in the context of mismatched percep-
tual basis. We extended a well known graph-based
approach (Krahmer et al, 2003) that has shown
to be effective in previous work (Gatt and Belz,
2008; Gatt et al, 2009). We incorporated uncer-
tainties in perception into cost functions. We fur-
ther extended regular graph representation into hy-
pergraph representation to account for group-based
spatial relations that are important for visual descrip-
tions (Dhande, 2003; Tenbrink and Moratz, 2003;
Funakoshi et al, 2006; Liu et al, 2012). Our em-
pirical results demonstrate that both enhancements
lead to about a 9% absolute performance gain com-
pared to the original approach. However, while
our approache performs effectively when the agent
has perfect knowledge or perception of the environ-
ment (e.g., 84%), it performs poorly under the mis-
matched perceptual basis (e.g., 45%). This perfor-
mance gap calls for new solutions for REG that are
capable of mediating mismatched perceptual basis.
In the following sections, we first describe our
hypergraph-based representations and illustrate how
uncertainties from automated perception can be in-
corporated. We then describe an empirical study us-
ing Amazon Mechanical Turks for evaluating gener-
ated referring expressions. Finally we present evalu-
ation results and discuss potential future directions.
2 Related Work
Since the Full Brevity algorithm (Dale, 1989), many
approaches have been developed and evaluated for
REG (Dale, 1995; Krahmer and Deemter, 2012),
such as the incremental algorithm (Dale, 1995),
the locative algorithm (Kelleher and Kruijff, 2006),
and graph-based approaches (Krahmer et al, 2003;
Croitoru and Van Deemter, 2007). Most of these ap-
proaches assume the agent has access to a complete
symbolic representation of the domain. While these
approaches work well for many applications involv-
ing user interfaces, the question is whether they can
be extended to the situation where the agent has in-
complete or incorrect knowledge and needs to make
inference about the domain or the world.
Recently, there has been increasing interest in
REG for visual objects (Roy, 2002; Golland et al,
2010; Mitchell et al, 2013). Some work (Golland
et al, 2010) uses visual scenes that are generated by
computer graphics and thus the internal representa-
tion of the scene is known. Some other work focuses
on the connection between lower-level visual fea-
tures and symbolic descriptors for REG (Roy, 2002;
Mitchell et al, 2013). However, most work assumes
no vision recognition errors. It is well established
that automated recognition of visual scenes is ex-
tremely challenging. This process is error-prone
and full of uncertainties. It is not clear whether
the existing approaches can be extended to the sit-
uation where the agent has imperfect perception of
the shared environment.
An earlier work by Horacek (Horacek, 2005)
has looked into the problem of mismatched knowl-
edge between conversation partners for REG. The
approach is a direct extension of the incremental al-
gorithm (Dale, 1995). However, this work only pro-
vides a proof of concept example to illustrate the
idea. No empirical evaluation was given.
All these previous works have motivated our
present investigation. We are interested in REG un-
der mismatched perceptual basis between conversa-
tion partners, where the agent has imperfect percep-
tion and knowledge of the shared environment. In
particular, we took a well-studied graph-based ap-
proach (Krahmer et al, 2003) and extended it to in-
corporate group spatial relations and uncertainties
associated with automated perception of the envi-
ronment. The reason we chose a graph-based ap-
proach is that graph representations are widely used
in the fields of computer vision (CV) and pattern
recognition to represent spatially rich scenes. Never-
theless, the findings from this investigation provide
insight to other approaches.
393
(a) an original scene (b) the corresponding impoverished
scene
Figure 1: An original scene and its impoverished scene processed by CV algorithm
3 Hypergraph-based REG
Towards mediating a shared perceptual basis in sit-
uated dialogue, our previous work (Liu et al, 2012)
has conducted experiments to study referential com-
munication between partners with mismatched per-
ceptual capabilities. We simulated mismatched ca-
pabilities by making an original scene (Figure 1(a))
available to a director (simulating higher perceptual
calibre) and a corresponding impoverished scene
(Figure 1(b)) available to a matcher (simulating low-
ered perceptual calibre). The impoverished scene
is created by re-rendering automated recognition re-
sults of the original scene by a CV algorithm. An
example of the original scene and an impoverished
scene is shown in Figure 1. Using this setup, the di-
rector and the matcher were instructed to collaborate
with each other on some naming games. Through
these games, they collected data on how partners
with mismatched perceptual capabilities collaborate
to ground their referential communication.
The setup in (Liu et al, 2012) is intended to sim-
ulate situated dialogue between a human (like the
director) and a robot (like the matcher). The robot
has a significantly lowered ability in perception and
reasoning. The robot?s internal representation of the
shared world will be much like the impoverished
scene which contains many recognition errors. The
data from (Liu et al, 2012; Liu et al, 2013) shows
that different strategies were used by conversation
partners to produce referential descriptions. Besides
directly describing attributes or binary relations with
a relatum, they often use group-based descriptions
(e.g., a cluster of four objects on the right). This is
mainly due to the fact that some objects are simply
not recognizable to the matcher. Binary spatial rela-
tionships sometimes are difficult to describe the tar-
get object, so the matcher must resort to group infor-
mation to distinguish the target object from the rest
of the objects. For example, suppose the matcher
needs to describe the target object 5 in Figure 1(b),
he/she may have to start by indicating the group of
three objects at the bottom and then specify the re-
lationship (i.e., top) of the target object within this
group.
The importance of group descriptions has been
shown not only here, but also in previous works
on REG (Funakoshi et al, 2004; Funakoshi et al,
2006; Weijers, 2011). While the original graph-
based approach can effectively represent attributes
and binary relations between objects (Krahmer et al,
2003), it is insufficient to capture within-group or
between-group relations. Therefore, to address the
low perceptual capabilities of artificial agents, we in-
troduce hypergraphs to represent the shared environ-
ment. Our approach has two unique characteristics
compared to previous graph-based approaches: (1)
A hypergraph representation is more general than
a regular graph. Besides attributes and binary re-
lations, it can also represent group-based relations.
(2) Unlike previous work, here the generation of hy-
pergraphs are completely driven by automated per-
ception of the environment. This is done by incor-
porating uncertainties in perception and reasoning
into cost functions associated with graphs. Next we
394
give a detailed account on hypergraph representa-
tion, cost functions incorporating uncertainties, and
the search algorithm for REG.
3.1 Hypergraph Representation
A directed hypergraph G (Gallo et al, 1993) is a
tuple of the form: G = ?X, A?, in which
X = {xm}
A = {ai = (ti, hi) | ti ? X, hi ? X}
Similar to regular graphs, a hypergraph consists
of a set of nodes X and a set of arcs A. However,
different from regular graphs, each arc in A is con-
sidered as a hyperarc in the sense that it can capture
relations between any two subsets of nodes: a tail
(ti) and a head (hi). Therefore, a hypergraph is a
generalization of a regular graph. It becomes a reg-
ular graph if the cardinalities of both the tail and the
head are restricted to one for all hyperarcs. While
regular graphs are commonly used to represent bi-
nary relations between two nodes, hypergraphs pro-
vide a more general representation for n-ary rela-
tions among multiple nodes.
We use hypergraphs to represent the agent?s per-
ceived physical environment (also called scene hy-
pergraphs). More specifically, each perceived ob-
ject is represented by a node in the graph. Each per-
ceived visual attribute of an object (e.g., color, size,
type information) or a group of objects (e.g., num-
ber of objects in the group, location) is captured by
a self-looping hyperarc. Hyperarcs are also used to
capture the spatial relations between any two subsets
of nodes, whether it is a relation between two ob-
jects, or between two groups of objects, or between
one or more objects within a group of objects.
For example, Figure 2 shows a hypergraph cre-
ated for part of the impoverished scene shown in
Figure 1(b) (i.e., the upper right corner including
objects 7, 8, 9, 11, and 13). One important char-
acteristic is that, because the graph is created based
on an automated vision recognition system, the val-
ues of an attribute or a relation in the hypergraph
are numeric (except for the type attribute). For ex-
ample, the value of the color attribute is the RGB
distribution extracted from the corresponding visual
object, the value of the size attribute is the width and
height of the bounding box and the value of the lo-
cation attribute is a function of spatial coordinates.
These numerical features will be further converted to
symbolic labels with certain confidence scores (de-
scribed later in Section 3.3.2).
3.2 Hypergraph Pruning
The perceived visual scene can be represented as a
complete hypergraph, in which any pair of two sub-
sets of nodes are connected by a hyperarc. However,
such a complete hypergraph is not only inefficient
but also unnecessary. Instead of keeping all possible
n-ary relations (i.e., hyperarcs), we only retain those
relations that are likely used by humans to produce
referring expressions, based on two heuristics.
The first heuristic is based on perceptual prin-
ciples, also called the Gestalt Laws of perception
(Sternberg, 2003), which describe how people group
visually similar objects into entities or groups. Two
well known principles of perceptual grouping are
proximity and similarity (Wertheimer, 1938): ob-
jects that lie close together are often perceived as
groups; objects of similar shape, size or color are
more likely to form groups than objects differing
along these dimensions. Based on these two prin-
ciples, previous works have developed different al-
gorithms for perceptual grouping (Thrisson, 1994;
Gatt, 2006). In our investigation, we adopted Gatt?s
algorithm (Gatt, 2006), which has shown to be more
accurate for spatial grouping. Given the results
from spatial grouping, we only retain hyperarcs that
represent spatial relations between two objects, be-
tween two perceived groups, between one object and
a perceived group, or between one object and the
group it belongs to.
The second heuristic is based on the observation
that, given a certain orientation, people tend to use a
relatum that is closer to the referent than more dis-
tant relata. In other words, it is less likely to refer to
an object relative to a distant relatum when there is
a closer relatum. For example, when referring to the
stapler (object 9 in Figure 1(a) ), it is more likely to
use ?the stapler above the battery? than ?the stapler
above the cellphone?. Based on this observation, we
prune the hypergraphs by only retaining hyperarcs
between an object and their closest relata for each
possible orientation.
Figure 2 shows the resulting hypergraph for rep-
resenting a subset of objects (7, 8, 9, 11, and 13) in
Figure 1(a).
395
Figure 2: An example of hypergraph representing the per-
ceived scene (a partial scene only including object 7, 8,
9, 11, 13 for Figure 1(a)).
3.3 Symbolic Descriptors for Attributes
As mentioned earlier, the values of attributes of ob-
jects and their relations are numerical in nature. In
order for the agent to generate natural language de-
scriptions, the first step is to assign symbolic labels
or descriptors to those attributes and relations. Next
we describe how we use a lexicon with grounded se-
mantics in this process.
3.3.1 Lexicon with Grounded Semantics
Grounded semantics provides a bridge to connect
symbolic labels or words with lower level visual fea-
tures (Harnad, 1990). Previous work has developed
various approaches for grounded semantics mainly
for the reference resolution task, i.e., identifying vi-
sual objects in the environment given language de-
scriptions (Dhande, 2003; Gorniak and Roy, 2004;
Tenbrink and Moratz, 2003; Siebert and Schlangen,
2008; Liu et al, 2012). For the referring expression
generation task here, we also need a lexicon with
grounded semantics.
In our lexicon, the semantics of each category
of words is defined by a set of semantic grounding
functions that are parameterized on visual features.
For example, for the color category it is defined as a
multivariate Gaussian distribution based on the RGB
distribution. Specific words such as green, red, or
blue have different means and co-variances as the
following:
color : red = fr(~vcolor) = N(~vcolor | ?1,
?
1)
color : green = fg(~vcolor) = N(~vcolor | ?2,
?
2)
color : blue = fb(~vcolor) = N(~vcolor | ?3,
?
3)
The above functions define how likely a set of rec-
ognized visual features (i.e., ~vcolor) describing the
color dimensions (i.e., RGB distribution) is to match
the color terms red, green, and blue.
For the spatial relation terms such as above, be-
low, left, right, the semantic grounding functions
take both vertical and horizontal coordinates of two
objects, as follows 1:
spatialRel : above(a, b) = fabove(~valoc, ~vbloc)
=
{
1? |xa?xb|400 if ya < yb;
0 otherwise.
Using the above convention, we have defined se-
mantic grounding functions for size category words
(e.g., small and big) and absolute position words
(e.g., top, below, left, and right). In addition, we use
object recognition models (Zhang and Lu, 2002) to
define class type category words such as apple and
orange used in our domain.
3.3.2 Attribute Descriptors and Cost Functions
Given the lexicon with grounded semantics as de-
scribed above, the numerical attributes captured in
the scene hypergraph can be converted to symbolic
descriptors. For each attribute (e.g., color) or re-
lation, the corresponding visual feature vector (i.e.,
~vcolor) is plugged into the semantic grounding func-
tions for the corresponding category of words. The
word that best describes the attribute is chosen as the
descriptor for that attribute. For example, given an
RGB color distribution ~vcolor, we can find the color
descriptor as follows:
color : w? = argmax
red,green,blue
fw(~vcolor),
For each attribute or relation, we can find a best
descriptor in this manner. In addition, we also ob-
tain a numerical value (returned from the semantic
1The size of the overall scene is 800x800.
396
grounding functions) that measures how well this
descriptor describes the corresponding visual fea-
tures. Intuitively, one would choose a descriptor that
closely matches the visual features. Based on this
intuition, we define the cost for each attribute A as
the following:
cost(A) = 1? fw?( ~vA)
where w? is the best descriptor for the attribute.
Given an attribute, the better the descriptor
matches the extracted visual features, the lower the
cost of the corresponding hyperarc.
3.4 Graph Matching for REG
Now the hypergraph representing the perceived en-
vironment has symbolic descriptors for its attributes
and relations together with corresponding costs.
Given this representation, REG can be formulated as
a graph matching algorithm similar to that described
in (Krahmer et al, 2003). We use the same Branch
and Bound algorithm described in (Krahmer et al,
2003). In this approach, a hypothesis hypergraph
(starting with one node representing the target ob-
ject) is gradually expanded by adding in a least cost
hyperarc from the scene hypergraph. At each ex-
pansion, the hypothesis graph is matched against the
scene hypergraph to decide whether it matches any
nodes other than the target node in the scene hyper-
graph. The expansion stops if the hypothesis graph
does not cover any other nodes except for the target
node. At this point, the hypothesis graph captures all
the content (e.g., attributes and relations) required to
uniquely describe the target object. We then apply
a set of simple generation templates to generate the
surface form of referring expressions based on the
hypothesis graph.
4 Empirical Evaluations
4.1 Evaluation Setup
To evaluate the performance of this hypergraph-
based approach to REG, we conducted a compara-
tive study using crowd-sourcing. More specifically,
we created 48 different scenes similar to that in Fig-
ure 1(a). Each scene has 13 objects on average and
there are 621 objects in total. For each of these
scenes, we applied a CV algorithm (Zhang and Lu,
2002) and generated scene hypergraphs as described
in Section 3.1. We then use different generation
strategies (varied in terms of graph representations
and cost functions, to be explained in Section 4.2) to
automatically generate referring expressions to refer
to each object.
To evaluate the quality of these generated refer-
ring expressions, we applied Amazon Mechanical
Turk to solicit feedback from the crowd 2. Through
an interface, we displayed an original scene and gen-
erated referring expressions (from different genera-
tion strategies) in a random order. We asked each
turk to select the object in the scene that he/she be-
lieved was the one referred to by the shown refer-
ring expression (i.e., reference identification task).
Each referring expression received three votes from
the crowd. In total, 217 turks participated in our ex-
periment.
4.2 Generation Strategies
We applied a set of different strategies to generate
referring expressions for each object. The variations
lie in two dimensions: (1) different graph repre-
sentations: using a hypergraph to represent the per-
ceived scene as described in Section 3.1 versus us-
ing a regular graph as introduced in (Krahmer et al,
2003); and (2) different cost functions for attributes
and relations: cost functions that have been used in
previous works (Theune et al, 2007; Krahmer et al,
2008) and cost functions that incorporate uncertain-
ties of perception as described in Section 3.3.2.
Cost functions play an important role in graph-
based approaches (Krahmer et al, 2003). Previous
works have examined different types of cost func-
tions (Theune et al, 2007; Krahmer et al, 2008;
Theune et al, 2011). We adopted some commonly
used cost functions from previous work together
with the cost functions defined here. In particular,
we experimented with the following different cost
functions:
Simple Cost: The costs for all hyperarcs are set to
1. With this cost function, the graph-based algorithm
resembles the Full Brevity algorithm of Dale (Dale,
2To control the quality of crowdsourcing, we recruited par-
ticipants based on the following criteria: Participants? locations
are limited to the United States. Approval rate for each partic-
ipant?s previous work is greater than or equal to 95%, and the
number of each participant?s previous approved work is greater
than or equal to 1000.
397
1992) in that a shortest distinguishing description is
preferred.
Absolute Preferred: The costs for hyperarcs rep-
resenting absolute attributes (e.g., type, color, and
position) are set to 1. The costs for relative at-
tributes (e.g., size) and relations are set to 2. This
cost function mimics human?s preference for abso-
lute attributes over relative ones (Dale, 1995).
Relative Preferred: The costs for hyperarcs repre-
senting absolute attributes are set to 2 and for rela-
tive attributes and relations are set to 1. This cost
function has been applied previously to emphasize
the importance of spatial relations in REG (Viethen
and Dale, 2008).
Uncertainty Based: The costs for all hyperarcs are
defined by incorporating uncertainties from percep-
tion as described in Section 3.3.2.
Uncertainty Relative Preferred: To emphasize the
importance of spatial relations as demonstrated in
situated interaction (Tenbrink and Moratz, 2003;
Kelleher and Kruijff, 2006), the costs for hyperarcs
representing relative attributes and relations are di-
vided by 3. This cost function will allow the algo-
rithm to prefer spatial relations through the reduced
cost.
Note that we only tested a few (not all) com-
monly used cost functions proposed by previous
work (Krahmer et al, 2003; Theune et al, 2007;
Krahmer et al, 2008; Theune et al, 2011). For ex-
ample, we did not include the stochastic cost func-
tion which is defined based on the frequencies of at-
tribute selection from the training data (Krahmer et
al., 2003). On the one hand, we did not have a large
set of human descriptions of the impoverished scene
to learn the stochastic cost. On the other hand, it
is not clear whether human strategies of describing
the impoverished scene should be used to represent
optimal strategies for the robot. Nevertheless, the
above different cost functions will allow us to eval-
uate whether incorporating perceptual uncertainties
will make a difference in the REG performance.
4.3 Evaluation Results
As mentioned earlier, each generated referring ex-
pression received three independent votes regarding
its referent from the crowd. The referent with the
most votes is taken as the predicted referent and is
used for evaluation. If all three votes are differ-
Cost Function Regular Graph Hypergraph
Simple Costs 33.2% 33.3%
Absolute Preferred 30.1% 30.3%
Relative Preferred 31.1% 35.4%
Uncertainty Based 35.7% 37.5%
Uncertainty Rel. Prefer. 36.7% 45.2%
Table 1: Results with different cost functions
ent, then by default, it is deemed that the referent
is not correctly identified for that expression. We
use the accuracy of the referential identification task
(i.e., the percentage of generated referring expres-
sions where the referents are correctly identified) as
the metric to evaluate different generation strategies
illustrated in Section 4.2.
4.3.1 The Role of Cost Functions
Table 1 shows the results based on different cost
functions and different graph representations. There
are several observations.
First, when the agent does not have perfect knowl-
edge of the environment and has to automatically
infer the environment as in our setting here, cost
functions based on uncertainties of perception lead
to better results. This occurs for both regular graphs
and hypergraphs. This result is not surprising and
indicates that cost functions should be tied to the
agent?s ability to perceive and infer the environment.
The uncertainty based cost functions allow the agent
to prefer reliable attributes or relations.
Second, consistent with previous work (Viethen
and Dale, 2008), we observed the importance of spa-
tial relations. Especially when the perceived world
is full of uncertainties, spatial relations tend to be
more reliable. In particular, as shown in Table 1,
using hypergraphs enables generating group-based
relations and results in significantly better perfor-
mance (45.2%) compared to regular graphs (36.7%)
(p = 0.002).
Note that our current cost function only includes
uncertainties of the agent?s own perception in a sim-
plistic form. When humans and agents have mis-
matched perceptual basis, the human?s model of
comprehension and tolerance of inaccurate descrip-
tion could play a role in REG. Incorporating human
models in the cost function will require in-depth em-
pirical studies and we will leave that to our future
398
work.
4.3.2 The Role of Imperfect Perception
To further understand the role of hypergraphs in
mediating mismatched perceptions between humans
and agents, we created a perfect scene regular graph
and a perfect scene hypergraph (representing the
agent?s perfect knowledge of the environment) for
each of the 48 scenes used in the experiments. In
each of these scene graphs, the attribute and rela-
tion descriptors are manually provided. We fur-
ther applied the Absolute Preferred cost function
(which has shown competitive performance in previ-
ous work) to generate referring expressions for each
object. Again, each referring expression received
three votes from the crowd.
Table 2 shows the results comparing two con-
ditions: (1) REs generated (by the Absolute Pre-
ferred cost function) based on the perfect graphs
which represent the agent?s perfect knowledge and
perception of the environment; and (2) REs gener-
ated based on automatically created graphs (by the
Uncertainty Relative Preferred cost function) which
represent the agent?s imperfect knowledge of the
environment as a result of automated recognition
and inference. The result shows that given perfect
knowledge of the environment, hypergraphs only
perform marginally better than the regular graphs
(p = 0.07). Given imperfect knowledge of the envi-
ronment, hypergraphs significantly outperforms the
regular graphs by taking advantage of spatial group-
ing information (p = 0.002). It is worthwhile to
mention that currently we use spatial proximity to
identify groups. However, the hypergraph based ap-
proach is not restricted to spatial grouping. In the-
ory, it can represent any type of group based on dif-
ferent similarity criteria.
Furthermore, our result shows that the graph-
based approaches perform quite competitively under
the condition of perfect knowledge and perception.
Although evaluated on different data sets, this result
is consistent with results from previous work (Gatt
and Belz, 2008; Gatt et al, 2009). However, what is
more interesting here is that while graph-based ap-
proaches perform well when the agent has perfect
knowledge of the environment, as its human part-
ner, these approaches literally fall apart with close
to 40% performance degradation when applied to
Environment Regular Graph Hypergraph
Pefect Perception 80.4% 84.2%
Imperfect Perception 36.7% 45.2%
Table 2: Results of comparing perfect perception and im-
perfect perception of the shared world.
the situation where the agent?s representation of the
shared world is problematic and full of mistakes.
These results indicate that REG for automati-
cally perceived scenes can be extremely challeng-
ing. Many errors result from automated perception
and reasoning that will affect the internal representa-
tion of the world and thus the generated REs. In our
experiments here, we applied a very basic CV algo-
rithm which resulted in rather poor performance in
our data: overall, 60.3% of objects in the original
scene are mis-recognized, and 10.5% of objects are
mis-segmented. We think this poor CV performance
represents a more challenging problem.
Some errors such as recognition errors can be by-
passed using our current approach based on hyper-
graphs. For example, in Figure 1 target object 9 (a
stapler) and 13 (a key) are mis-recognized as a cup
and a pen. Using our hypergraph-based approach,
for the target object 9, instead of generating ?a small
cup? (as in the case of using regular graphs), ?a gray
object on the top within a cluster of four objects?
is generated. For the target object 13, instead of ?a
pen? as generated by regular graphs, ?a small object
on the right within a cluster of 4? is generated. Even
with recognition errors, these group-based descrip-
tions will allow the listener to identify target objects
in their representation correctly. Nevertheless, many
processing errors cannot be handled by our current
approach. For example, an object can be mistak-
enly segmented into multiple parts or several objects
can be mistakenly grouped into one object. In addi-
tion, our current semantic grounding functions are
simple. Sometimes they do not provide correct de-
scriptors for the extracted visual features. More so-
phisticated functions that better reflect human?s vi-
sual perception (Regier, 1996; Mojsilovic, 2005;
Mitchell et al, 2011) should be pursued in the fu-
ture.
399
Minimum Effort Extra Effort
Pefect Perception 84.2% 88.1%
Imperfect Perception 45.2% 51.5%
Table 3: Results of comparing minimum effort and extra
effort using hypergraphs
4.3.3 The Role of Extra Effort
While REG systems have a tendency to produce
minimal descriptions, recent psycholinguistic stud-
ies have shown that speakers do not necessarily fol-
low the Grice?s maxim of quantity, and they tend
to provide redundant properties in their descrip-
tions (Jordan and Walker, 2000; Belke and Meyer,
2002; Arts et al, 2011). With this in mind, we
conducted a very simple evaluation on the role of
extra effort. Once a set of descriptors are selected
based on the minimum cost, one additional descrip-
tor (with the least cost among the remaining at-
tributes or relations) is added to the referential de-
scription. We once again solicited the crowd feed-
back to this set of expressions generated by extra
effort. Each expression again received three votes
from the crowd.
Table 3 shows the results by comparing minimum
effort with extra effort when using hypergraphs to
generate REs. As indicated here, extra effort (by
adding one additional descriptor) leads to more com-
prehensible REs with 3.9% improvement under per-
fect perception and 6.3% improvement under imper-
fect perception (both are significant, p < 0.05). The
improvement is larger under imperfect perception.
This seems to indicate that exploring extra effort in
REG could help mediate mismatched perceptions in
situated dialogue. However, more understanding on
how to engage in such extra effort will be required
in the future.
5 Conclusion
In situated dialogue, humans and agents have mis-
matched perceptions of the shared environment. To
facilitate successful referential communication be-
tween a human and an agent, the agent needs to take
such discrepancies into consideration and generate
referential descriptions that can be understood by
its human partner. With this in mind, we re-visited
the problem of referring expression generation in the
context of mismatched perceptions between humans
and agents. In particular, we applied and extended
the state of the art graph-based approach (Krahmer
et al, 2003) in this new setting. Our empirical re-
sults have shown that, to address the agent?s limited
perceptual capability, REG algorithms will need to
take into account the uncertainties in perception and
reasoning. Group-based information appears more
reliable and thus should be modeled by an approach
that deals with automated perception of spatially
rich scenes.
While graph-based approaches have shown effec-
tive for the situation where the agent has complete
knowledge of the environment, as its human part-
ner, these approaches are often inadequate when hu-
mans and agents have mismatched representations
of the shared world. Our empirical results here call
for new solutions to address the mismatched per-
ceptual basis. Previous work indicated that referen-
tial communication is a collaborative process (Clark
and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995).
Conversation partners make extra effort to collab-
orate with each other. For the situation with mis-
matched perceptual basis, a potential solution thus
should go beyond the objective of generating a mini-
mum description, and towards a collaborative model
which incorporates immediate feedback from the
conversation partner (Edmonds, 1994).
6 Acknowledgments
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-1208390
from the National Science Foundation.
References
Anja Arts, Alfons Maes, Leo Noordman, and Carel
Jansen. 2011. Overspecification facilitates object
identification. Journal of Pragmatics, 43(1):361?374.
E. Belke and A. S. Meyer. 2002. Tracking the time
course of multidimensional stimulus discrimination:
Analyses of viewing patterns and processing times
during ?same?-?different? decisions. European Jour-
nal of Cognitive Psychology, 14(2):237?266.
H.H. Clark and S.E. Brennan. 1991. Grounding in com-
munication. Perspectives on socially shared cognition,
13:127?149.
H. H Clark and D Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1?39.
400
Madalina Croitoru and Kees Van Deemter. 2007. A con-
ceptual graph approach to the generation of referring
expressions. In Proceedings of the 20th international
joint conference on Artifical intelligence, IJCAI?07,
pages 2456?2461.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th annual meeting on Associ-
ation for Computational Linguistics, ACL ?89, pages
68?75, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. The MIT Press,Cambridge, Massachusetts.
Robert Dale. 1995. Computational interpretations of the
gricean maxims in the generation of referring expres-
sions. Cognitive Science, 19:233?263.
Sheel Sanjay Dhande. 2003. A computational model to
connect gestalt perception and natural language. In
Masters thesis, Massachusetts Institure of Technology.
Philip G. Edmonds. 1994. Collaboration on reference to
objects that are not mutually known. In Proceedings
of the 15th conference on Computational linguistics -
Volume 2, COLING ?94, pages 1118?1122, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generation of relative
referring expressions based on perceptual grouping. In
COLING.
Kotaro Funakoshi, Satoru Watanabe, and Takenobu
Tokunaga. 2006. Group-based generation of referring
expressions. In INLG, pages 73?80.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and ap-
plications. Discrete applied mathematics, 42(2):177?
201.
Albert Gatt and Anja Belz. 2008. Attribute selection for
referring expression generation: new algorithms and
evaluation methods. In Proceedings of the Fifth In-
ternational Natural Language Generation Conference,
INLG ?08, pages 50?58, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the Eleventh European Workshop on Nat-
ural Language Generation, ENLG ?07, pages 49?56,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The tuna-
reg challenge 2009: overview and evaluation results.
In Proceedings of the 12th European Workshop on
Natural Language Generation, ENLG ?09, pages 174?
182, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Albert Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Associa-
tion for Computational Linguistics, pages 321?328.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 410?419, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. Journal of Artificial
Intelligence Research, 21:429?470.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D, 42:335?346.
Peter A. Heeman and Graeme Hirst. 1995. Collaborating
on referring expressions. Computational Linguistics,
21:351?382.
Helmut Horacek. 2005. Generating referential descrip-
tions under conditions of uncertainty. In Proceedings
of the 10th European Workshop on Natural Language
Generation (ENLG) pages 58-67, Aberdeen, UK.
Pamela W Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 181-190.
John D. Kelleher and Geert-Jan M. Kruijff. 2006. In-
cremental generation of spatial referring expressions
in situated dialog. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 1041?1048,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
computational linguistics, 38(1):173?218.
Emiel Krahmer Krahmer, Sebastiaan van Erk, and Andre?
Verleg. 2003. Graph-based generation of referring
expressions. Computational Linguistics, 29(1):53?72,
March.
Emiel Krahmer, Mariet Theune, Jette Viethen, and Iris
Hendrickx. 2008. Graph: The costs of redundancy
in referring expressions. In In Proceedings of the 5th
International Conference on Natural Language Gen-
eration, Salt Fork OH, USA.
Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. To-
wards mediating shared perceptual basis in situated di-
alogue. In Proceedings of the 13th Annual Meeting of
401
the Special Interest Group on Discourse and Dialogue,
SIGDIAL ?12, pages 140?149, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Y. Chai.
2013. Modeling collaborative referring for situated
referential grounding. In The 14th Annual SIGdial
Meeting on Discourse and Dialogue.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
In Proceedings of the 13th European Workshop on
Natural Language Generation, ENLG ?11, pages 63?
70, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013. Generating expressions that refer to visible ob-
jects. In Proceedings of NAACL-HLT 2013, pages
1174-1184.
Aleksandra Mojsilovic. 2005. A computational model
for color naming and describing color composition
of images. IEEE Transactions on Image Processing,
14:690 ? 699.
Terry Regier. 1996. The human semantic potential. The
MIT Press,Cambridge, Massachusetts.
Deb Roy. 2002. Learning visually grounded words and
syntax of natural spoken language. Evolution of Com-
munication, 4.
Alexander Siebert and David Schlangen. 2008. A sim-
ple method for resolution of definite reference in a
shared visual context. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, SIGdial
?08, pages 84?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert Sternberg. 2003. Cognitive Psychology,Third
Edition. Thomson Wadsworth.
Kristina Striegnitz, Hendrik Buschmeier, and Stefan
Kopp. 2012. Referring in installments: a corpus
study of spoken object references in an interactive vir-
tual environment. In Proceedings of the Seventh In-
ternational Natural Language Generation Conference,
INLG ?12, pages 12?16, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thora Tenbrink and Reinhard Moratz. 2003. Group-
based spatial reference in linguistic human-robot in-
teraction. Spatial Cognition and Computation, 6:63?
106.
Marie?t Theune, Pascal Touset, Jette Viethen, and Emiel
Krahmer. 2007. Cost-based attribute selection for gre
(graph-sc/graph-fp). In Proceedings of the MT Summit
XI Workshop on Using Corpora for NLG: Language
Generation and Machine Translation (UCNLG+MT).
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter ? how much
data is required to train a reg algorithm? In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 660?664, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Kristinn R. Thrisson. 1994. Simulated perceptual group-
ing: An application to human-computer interaction. In
Proceedings of the Sixteenth Annual Conference of the
Cognitive Science Society, pages 876?881.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expression generation. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, INLG ?08, pages 59?
67, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
S. Weijers. 2011. Referring expressions with groups as
landmarks. volume 15. University of Twente.
Max Wertheimer. 1938. Laws of organization in per-
ceptual forms. A Source Book of Gestalt Psychology.
Routledge and Kegan Paul, London.
Dengsheng Zhang and Guojun Lu. 2002. An integrated
approach to shape based image retrieval. In Proc.
of 5th Asian Conference on Computer Vision (ACCV,
pages 652?657.
402
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 13?18,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Labeling for Efficient Referential Grounding based on
Collaborative Discourse
Changsong Liu, Lanbo She, Rui Fang, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{cliu, shelanbo, fangrui, jchai}@cse.msu.edu
Abstract
When humans and artificial agents (e.g.
robots) have mismatched perceptions of
the shared environment, referential com-
munication between them becomes diffi-
cult. To mediate perceptual differences,
this paper presents a new approach us-
ing probabilistic labeling for referential
grounding. This approach aims to inte-
grate different types of evidence from the
collaborative referential discourse into a
unified scheme. Its probabilistic labeling
procedure can generate multiple ground-
ing hypotheses to facilitate follow-up dia-
logue. Our empirical results have shown
the probabilistic labeling approach sig-
nificantly outperforms a previous graph-
matching approach for referential ground-
ing.
1 Introduction
In situated human-robot dialogue, humans and
robots have mismatched capabilities of perceiving
the shared environment. Thus referential commu-
nication between them becomes extremely chal-
lenging. To address this problem, our previous
work has conducted a simulation-based study to
collect a set of human-human conversation data
that explain how partners with mismatched per-
ceptions strive to succeed in referential commu-
nication (Liu et al, 2012; Liu et al, 2013). Our
data have shown that, when conversation partners
have mismatched perceptions, they tend to make
extra collaborative effort in referential commu-
nication. For example, the speaker often refers
to the intended object iteratively: first issuing an
initial installment, and then refashioning till the
hearer identifies the referent correctly. The hearer,
on the other hand, often provides useful feedback
based on which further refashioning can be made.
This data has demonstrated the importance of in-
corporating collaborative discourse for referential
grounding.
Based on this data, as a first step we developed
a graph-matching approach for referential ground-
ing (Liu et al, 2012; Liu et al, 2013). This ap-
proach uses Attributed Relational Graph to cap-
ture collaborative discourse and employs a state-
space search algorithm to find proper ground-
ing results. Although it has made meaning-
ful progress in addressing collaborative referen-
tial grounding under mismatched perceptions, the
state-space search based approach has two ma-
jor limitations. First, it is neither flexible to ob-
tain multiple grounding hypotheses, nor flexible
to incorporate different hypotheses incrementally
for follow-up grounding. Second, the search al-
gorithm tends to have a high time complexity for
optimal solutions. Thus, the previous approach
is not ideal for collaborative and incremental di-
alogue systems that interact with human users in
real time.
To address these limitations, this paper de-
scribes a new approach to referential grounding
based on probabilistic labeling. This approach
aims to integrate different types of evidence from
the collaborative referential discourse into a uni-
fied probabilistic scheme. It is formulated un-
der the Bayesian reasoning framework to easily
support generation and incorporation of multi-
ple grounding hypotheses for follow-up processes.
Our empirical results have shown that the prob-
abilistic labeling approach significantly outper-
forms the state-space search approach in both
grounding accuracy and efficiency. This new ap-
proach provides a good basis for processing col-
laborative discourse and enabling collaborative di-
alogue system in situated referential communica-
tion.
13
2 Related Work
Previous works on situated referential grounding
have mainly focused on computational models that
connect linguistic referring expressions to the per-
ceived environment (Gorniak and Roy, 2004; Gor-
niak and Roy, 2007; Siebert and Schlangen, 2008;
Matuszek et al, 2012; Jayant and Thomas, 2013).
These works have provided valuable insights on
how to manually and/or automatically build key
components (e.g., semantic parsing, grounding
functions between visual features and words, map-
ping procedures) for a situated referential ground-
ing system. However, most of these works only
dealt with the interpretation of single referring ex-
pressions, rather than interrelated expressions in
collaborative dialogue.
Some earlier work (Edmonds, 1994; Heeman
and Hirst, 1995) proposed a symbolic reasoning
(i.e. planning) based approach to incorporate col-
laborative dialogue. However, in situated settings
pure symbolic approaches will not be sufficient
and new approaches that are robust to uncertain-
ties need to be pursued. DeVault and Stone (2009)
proposed a hybrid approach which combined sym-
bolic reasoning and machine learning for inter-
preting referential grounding dialogue. But their
?environment? was a simplistic block world and
the issue of mismatched perceptions was not ad-
dressed.
3 Data
Previously, we have collected a set of human-
human dialogues on an object-naming task (Liu
et al, 2012). To simulate mismatched perceptions
between a human and an artificial agent, two par-
ticipants were shown different versions of an im-
age: the director was shown the original image
containing some randomly placed objects (e.g.,
fruits), and the matcher was shown an impov-
erished version of the image generated by com-
puter vision. They were instructed to communi-
cate with each other to figure out the identities of
some ?named? objects (only known to the direc-
tor), such that the matcher could also know which
object has what name.
Here is an example excerpt from this dataset:
D
1
: there is basically a cluster of four objects in the upper
left, do you see that (1)
M: yes (2)
D: ok, so the one in the corner is a blue cup (3)
1
D stands for the director; M stands for the matcher.
M: I see there is a square, but fine, it is blue (4)
D: alright, I will just go with that, so and then right under
that is a yellow pepper (5)
M: ok, I see apple but orangish yellow (6)
D: ok, so that yellow pepper is named Brittany (7)
M: uh, the bottom left of those four? Because I do see a
yellow pepper in the upper right (8)
D: the upper right of the four of them? (9)
M: yes (10)
D: ok, so that is basically the one to the right of the blue
cup (11)
M: yeah (12)
D: that is actually an apple (13)
As we can see from this example, both the direc-
tor and the matcher make extra efforts to overcome
the mismatched perceptions through collaborative
dialogue. Our ultimate goal is to develop com-
putational approaches that can ground interrelated
referring expressions to the physical world, and
enable collaborative actions of the dialogue agent
(similar to the active role that the matcher played
in the human-human dialogue). For the time be-
ing, we use this data to evaluate our computa-
tional approach for referential grounding, namely,
replacing the matcher by our automatic system to
ground the director?s referring expressions.
4 Probabilistic Labeling for Reference
Grounding
4.1 System Overview
Our system first processes the data using auto-
matic semantic parsing and coreference resolu-
tion. For semantic parsing, we use a rule-based
CCG parser (Bozsahin et al, 2005) to parse each
utterance into a formal semantic representation.
For example, the utterance ?a pear is to the right
of the apple? is parsed as
[a
1
, a
2
] , [Pear(a
1
), Apple(a
2
), RightOf(a
1
, a
2
)]
which consists of a list of discourse entities (e.g.,
a
1
and a
2
) and a list of first-order-logic predicates
that specify the unary attributes of these entities
and the binary relations between them.
We then perform pairwise coreference resolu-
tion on the discourse entities to find out the dis-
course relations between entities from different ut-
terances. Formally, let a
i
be a discourse entity ex-
tracted from the current utterance, and a
j
a dis-
course entity from a previous utterance. We train a
maximum entropy classifier
2
(Manning and Klein,
2
The features we use for the classification include the dis-
tance between a
i
and a
j
, the determiners associated with
them, the associated pronouns, the syntactic roles, the ex-
tracted unary properties, etc.
14
2003) to predict whether a
i
and a
j
should refer to
the same object (i.e. positive) or to different ob-
jects (i.e. negative).
Based on the semantic parsing and pairwise
coreference resolution results, our system fur-
ther builds a graph representation to capture the
collaborative discourse and formulate referential
grounding as a probabilistic labeling problem, as
described next.
4.2 Graph Representation
We use an Attributed Relational Graph (Tsai and
Fu, 1979) to represent the referential grounding
discourse (which we call the ?dialogue graph?). It
is constructed based on the semantic parsing and
coreference resolution results. The dialogue graph
contains a set A of N nodes:
A = {a
1
, a
2
, . . . , a
N
}
in which each node a
i
represents a discourse en-
tity from the parsing results. And for each pair
of nodes a
i
and a
j
there can be an edge a
i
a
j
that
represents the physical or discourse relation (i.e.
coreference) between the two nodes.
Furthermore, each node a
i
can be assigned a set
of ?attributes?:
x
i
=
{
x
(1)
i
, x
(2)
i
, . . . , x
(K)
i
}
which are used to specify information about the
unary properties of the corresponding discourse
entity. Similarly, each edge a
i
a
j
can also be as-
signed a set of attributes x
ij
to specify informa-
tion about the binary relations between two dis-
course entities. The node attributes are from the
semantic parsing results, i.e., the unary proper-
ties associated to a discourse entity. The edge at-
tributes can be either from parsing results, such
as a spatial relation between two entities (e.g.,
RightOf(a
1
, a
2
)); Or from pairwise coreference
resolution results, i.e., two entities are coreferen-
tial (coref = +) or not (coref = ?).
Besides the dialogue graph that represents the
linguistic discourse, we build another graph to rep-
resent the perceived environment. This graph is
called the ?vision graph? (since this graph is built
based on computer vision?s outputs). It has a set ?
of M nodes:
? = {?
1
, ?
2
, . . . , ?
M
}
in which each node ?
?
represents a physical ob-
ject in the scene. Similar to the dialogue graph,
the vision graph also has edges (e.g., ?
?
?
?
), node
attributes (e.g.,
?
x
?
) and edge attributes (e.g.,
?
x
??
).
Note that the attributes in the vision graph mostly
have numeric values extracted by computer vision
algorithms, whereas the attributes in the dialogue
graph have symbolic values extracted from the lin-
guistic discourse. A set of ?symbol grounding
functions? are used to bridge between the hetero-
geneous attributes (described later).
Given these two graph representations, referen-
tial grounding then can be formulated as a ?node
labeling? process, that is to assign a label ?
i
to
each node a
i
. The value of ?
i
can be any of the
M node labels from the set ?.
4.3 Probabilistic Labeling Algorithm
The probabilistic labeling algorithm (Christmas et
al., 1995) is formulated in the Bayesian frame-
work. It provides a unified evidence-combining
scheme to integrate unary attributes, binary rela-
tions and prior knowledge for updating the label-
ing probabilities (i.e. P (?
i
= ?
?
)). The algo-
rithm finds proper labelings in an iterative manner:
it first initiates the labeling probabilities by consid-
ering only the unary attributes of each node, and
then updates the labeling probability of each node
based on the labeling of its neighbors and the rela-
tions with them.
Initialization:
Compute the initial labeling probabilities:
P
(0)
(?
i
= ?
?
) =
P (a
i
| ?
i
= ?
?
)
?
P (?
i
= ?
?
)
?
?
???
P (a
i
| ?
i
= ?
?
)
?
P (?
i
= ?
?
)
in which
?
P (?
i
= ?
?
) is the prior probability of
labeling a
i
with ?
?
. The prior probability can be
used to encode any prior knowledge about possi-
ble labelings. Especially in incremental process-
ing of the dialogue, the prior can encode previ-
ous grounding hypotheses, and other information
from the collaborative dialogue such as confirma-
tion, rejection, or replacement.
P (a
i
| ?
i
= ?
?
) is called the ?compatibility co-
efficient? between a
i
and ?
?
, which is computed
based on the attributes of a
i
and ?
?
:
P (a
i
| ?
i
= ?
?
) = P (x
i
| ?
i
= ?
?
)
?
?
k
P
(
x
(k)
i
| ?
i
= ?
?
)
and we further define
15
P(
x
(k)
i
| ?
i
= ?
?
)
= p
(
x
(k)
i
| x?
(k)
?
)
=
p
(
x?
(k)
?
|x
(k)
i
)
p
(
x
(k)
i
)
?
x
(k)
j
?L
(k)
p
(
x?
(k)
?
|x
(k)
j
)
p
(
x
(k)
j
)
where L
(k)
is the ?lexicon? for the k-th attribute of
a dialogue graph node, e.g., for the color attribute:
L
(k)
= {red, green, blue, . . .}
and p
(
x?
(k)
?
| x
(k)
i
)
is what we call a ?symbol
grounding function?, i.e., the probability of ob-
serving x?
(k)
?
given the word x
(k)
i
. It judges the
compatibilities between the symbolic attribute val-
ues from the dialogue graph and the numeric at-
tribute values from the vision graph. These sym-
bol grounding functions can be either manually
defined or automatically learned. In our current
work, we use a set of manually defined ground-
ing functions motivated by previous work (Gor-
niak and Roy, 2004).
Iteration:
Once the initial probabilities are calculated, the
labeling procedure iterates till all the labeling
probabilities have converged or the number of it-
erations has reached a specified limit. At each it-
eration and for each possible labeling, it computes
a ?support function? as:
Q
(n)
(?
i
= ?
?
) =
?
j?N
i
?
?
?
??
P
(n)
(?
j
= ?
?
)
P (a
i
a
j
| ?
i
= ?
?
, ?
j
= ?
?
)
and updates the probability of each possible label-
ing as:
P
(n+1)
(?
i
= ?
?
) =
P
(n)
(?
i
=?
?
)Q
(n)
(?
i
=?
?
)
?
?
???
P
(n)
(?
i
=?
?
)Q
(n)
(?
i
=?
?
)
The support function Q
(n)
(?
i
= ?
?
) expresses
how the labeling ?
i
= ?
?
at the n-th itera-
tion is supported by the labeling of a
i
?s neigh-
bors
3
, taking into consideration the binary rela-
tions that exist between a
i
and them. Similar to
the node compatibility coefficient, the edge com-
patibility coefficient between a
i
a
j
and ?
?
?
?
,
3
The set of indices N
i
is defined as:
N
i
= {1, 2, . . . , i? 1, i+ 1, . . . , N}
Top-1 Top-2 Top-3
Random
7.7% 15.4% 23.1%
Guess
a
S.S.S. 19.1% 19.7% 21.3%
P.L. 24.9% 36.1% 45.0%
Gain
b
5.8% 16.4% 23.7%
(p < 0.01) (p < 0.001) (p < 0.001)
P.L. using
66.4% 74.8% 81.9%annotated
coreference
a
Each image contains an average of 13 objects.
b
p-value is based on the Wilcoxon signed-rank
test (Wilcoxon et al, 1970) on the 62 dialogues.
Table 1: Comparison of the reference grounding
performances of a random guess baseline, Prob-
abilistic Labeling (P.L.) and State-Space Search
(S.S.S.), and P.L. using manually annotated coref-
erence.
namely the P (a
i
a
j
| ?
i
= ?
?
, ?
j
= ?
?
) for com-
puting Q
(n)
(?
i
= ?
?
), is also based on the at-
tributes of the two edges and their corresponding
symbol grounding functions. So we also man-
ually defined a set of grounding functions for
edge attributes such as the spatial relation (e.g.,
RightOf , Above). If an edge is used to encode
the discourse relation between two entities (i.e.,
the pairwise coreference results), the compatibility
coefficient can be defined as (suppose edge a
i
a
j
encodes a positive coreference relation between
entities a
i
and a
j
):
P (a
i
a
j
= + | ?
i
= ?
?
, ?
j
= ?
?
)
=
P
(
?
i
=?
?
,?
j
=?
?
|a
i
a
j
=+
)
P (a
i
a
j
=+)
P
(
?
i
=?
?
,?
j
=?
?
)
which can be calculated based on the results from
the coreference classifier (Section 4.1).
5 Evaluation and Discussion
Our dataset has 62 dialogues, each of which con-
tains an average of 25 valid utterances from the
director. We first applied the semantic parser and
coreference classifier as described in Section 4.1
to process each dialogue, and then built a graph
representation based on the automatic processing
results at the end of the dialogue. On average, a di-
alogue graph consists of 33 discourse entities from
the director?s utterances that need to be grounded.
We then applied both the probabilistic label-
ing algorithm and the state-space search algorithm
to ground each of the director?s discourse entities
onto an object perceived from the image. The av-
eraged grounding accuracies of the two algorithms
16
are shown in the middle part of Table 1. The first
column of Table 1 shows the grounding accura-
cies of the algorithm?s top-1 grounding hypothesis
(i.e., ?
i
= argmax
?
?
P (?
i
= ?
?
) for each i). The
second and third column then show the ?accura-
cies? of the top-2 and top-3 hypotheses
4
, respec-
tively.
As shown in Table 1, probabilistic labeling
(i.e. P.L.) significantly outperforms state-space
search (S.S.S.), especially with regard to produc-
ing meaningful multiple grounding hypotheses.
The state-space search algorithm actually only re-
sults in multiple hypotheses for the overall match-
ing, and it fails to produce multiple hypotheses
for many individual discourse entities. Multiple
grounding hypotheses can be very useful to gen-
erate responses such as clarification questions or
nonverbal feedback (e.g. pointing, gazing). For
example, if there are two competing hypotheses,
the dialogue manager can utilize them to gener-
ate a response like ?I see two objects there, are
you talking about this one (pointing to) or that one
(pointing to the other)??. Such proactive feedback
is often an effective way in referential communi-
cation (Clark and Wilkes-Gibbs, 1986; Liu et al,
2013).
The probabilistic labeling algorithm not only
produces better grounding results, it also runs
much faster (with a running-time complexity of
O
(
MN
2
)
,
5
comparing to O
(
N
4
)
of the state-
space search algorithm
6
). Figure 1 shows the av-
eraged running time of the state-space search al-
gorithm on a Intel Core i7 1.60GHz CPU with
16G RAM computer (the running time of the prob-
abilistic labeling algorithm is not shown in Fig-
ure 1 since it always takes less than 1 second to
run). As we can see, when the size of the dialogue
graph becomes greater than 15, state-space search
takes more than 1 minute to run. The efficiency of
the probabilistic labeling algorithm thus makes it
more appealing for real-time interaction applica-
tions.
Although probabilistic labeling significantly
outperforms the state-space search, the grounding
performance is still rather poor (less than 50%)
4
The accuracy of the top-2/top-3 grounding hypotheses is
measured by whether the ground-truth reference is included
in the top-2/top-3 hypotheses.
5
M is the number of nodes in the vision graph and N is
the number of nodes in the dialogue graph.
6
Beam search algorithm is applied to reduce the exponen-
tial O
(
M
N
)
to O
(
N
4
)
.
Figure 1: Average running time of the state-space
search algorithm with respect to the number of
nodes to be grounded in a dialogue graph.
even for the top-3 hypotheses. With no surprise,
the coreference resolution performance plays an
important role in the final grounding performance
(see the grounding performance of using manually
annotated coreference in the bottom part of Ta-
ble 1). Due to the simplicity of our current coref-
erence classifier and the flexibility of the human-
human dialogue in the data, the pairwise coref-
erence resolution only achieves 0.74 in precision
and 0.43 in recall. The low recall of coreference
resolution makes it difficult to link interrelated re-
ferring expressions and resolve them jointly. So it
is important to develop more sophisticated coref-
erence resolution and dialogue management com-
ponents to reliably track the discourse relations
and other dynamics in the dialogue to facilitate ref-
erential grounding.
6 Conclusion
In this paper, we have presented a probabilistic la-
beling based approach for referential grounding in
situated dialogue. This approach provides a uni-
fied scheme for incorporating different sources of
information. Its probabilistic scheme allows each
information source to present multiple hypotheses
to better handle uncertainties. Based on the in-
tegrated information, the labeling procedure then
efficiently generates probabilistic grounding hy-
potheses, which can serve as important guidance
for the dialogue manager?s decision making. In
future work, we will utilize probabilistic labeling
to incorporate information from verbal and non-
verbal communication incrementally as the dia-
logue unfolds, and to enable collaborative dia-
logue agents in the physical world.
Acknowledgments
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-
1208390 from the National Science Foundation.
17
References
Cem Bozsahin, Geert-Jan M Kruijff, and Michael
White. 2005. Specifying grammars for openccg: A
rough guide. Included in the OpenCCG distribution.
William J. Christmas, Josef Kittler, and Maria Petrou.
1995. Structural matching in computer vision
using probabilistic relaxation. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
17(8):749?764.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1?39.
David DeVault and Matthew Stone. 2009. Learning to
interpret utterances using dialogue history. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 184?192. Association for Computa-
tional Linguistics.
Philip G Edmonds. 1994. Collaboration on reference
to objects that are not mutually known. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 2, pages 1118?1122. Association
for Computational Linguistics.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. J. Artif. Intell.
Res.(JAIR), 21:429?470.
Peter Gorniak and Deb Roy. 2007. Situated lan-
guage understanding as filtering perceived affor-
dances. Cognitive Science, 31(2):197?231.
Peter A Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21(3):351?382.
Krishnamurthy Jayant and Kollar Thomas. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transac-
tions of the Association of Computational Linguis-
tics, 1:193?206.
Changsong Liu, Rui Fang, and Joyce Chai. 2012. To-
wards mediating shared perceptual basis in situated
dialogue. In Proceedings of the 13th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 140?149, Seoul, South Korea, July.
Association for Computational Linguistics.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai.
2013. Modeling collaborative referring for situated
referential grounding. In Proceedings of the SIG-
DIAL 2013 Conference, pages 78?86, Metz, France,
August. Association for Computational Linguistics.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology: Tutorials - Volume 5,
NAACL-Tutorials ?03, pages 8?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 1671?1678, New York, NY, USA,
July. Omnipress.
Alexander Siebert and David Schlangen. 2008. A
simple method for resolution of definite reference
in a shared visual context. In Proceedings of the
9th SIGdial Workshop on Discourse and Dialogue,
pages 84?87. Association for Computational Lin-
guistics.
Wen-Hsiang Tsai and King-Sun Fu. 1979. Error-
correcting isomorphisms of attributed relational
graphs for pattern analysis. Systems, Man and Cy-
bernetics, IEEE Transactions on, 9(12):757?768.
Frank Wilcoxon, SK Katti, and Roberta A Wilcox.
1970. Critical values and probability levels for the
wilcoxon rank sum test and the wilcoxon signed
rank test. Selected tables in mathematical statistics,
1:171?259.
18
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 140?149,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Towards Mediating Shared Perceptual Basis in Situated Dialogue
Changsong Liu, Rui Fang, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI, 48864
{cliu,fangrui,jchai}@cse.msu.edu
Abstract
To enable effective referential grounding in
situated human robot dialogue, we have con-
ducted an empirical study to investigate how
conversation partners collaborate and medi-
ate shared basis when they have mismatched
visual perceptual capabilities. In particu-
lar, we have developed a graph-based repre-
sentation to capture linguistic discourse and
visual discourse, and applied inexact graph
matching to ground references. Our empiri-
cal results have shown that, even when com-
puter vision algorithms produce many errors
(e.g. 84.7% of the objects in the environment
are mis-recognized), our approach can still
achieve 66% accuracy in referential ground-
ing. These results demonstrate that, due to its
error-tolerance nature, inexact graph matching
provides a potential solution to mediate shared
perceptual basis for referential grounding in
situated interaction.
1 Introduction
To support natural interaction between a human and
a robot, technology enabling human robot dialogue
has become increasingly important. Human robot
dialogue often involves objects and their identities
in the environment. One critical problem is inter-
pretation and grounding of references - a process
to establish mutual understanding between conver-
sation partners about intended references (Clark and
Wilkes-Gibbs, 1986). The robot needs to identify
referents in the environment that are specified by its
human partner and the partner needs to recognize
that the intended referents are correctly understood.
It is critical for the robot and its partner to quickly
and reliably reach the mutual acceptance of refer-
ences before conversation can move forward.
Despite recent progress (Scheutz et al, 2007b;
Foster et al, 2008; Skubic et al, 2004; Kruijff et al,
2007; Fransen et al, 2007), interpreting and ground-
ing references remains a very challenging problem.
In situated interaction, although a robot and its hu-
man partner are co-present in a shared environment,
they have significantly mismatched perceptual capa-
bilities (e.g., recognizing objects in the surround-
ings). Their knowledge and representation of the
shared world are significantly different. When a
shared perceptual basis is missing, grounding ref-
erences to the environment will be difficult (Clark,
1996). Therefore, a foremost question is to under-
stand how partners with mismatched perceptual ca-
pabilities mediate shared basis to achieve referential
grounding.
To address this problem, we have conducted an
empirical study to investigate how conversation part-
ners collaborate and mediate shared basis when they
have mismatched visual perceptual capabilities. In
particular, we have developed a graph-based rep-
resentation to capture linguistic discourse and vi-
sual discourse, and applied inexact graph matching
to ground references. Our empirical results have
shown that, even when the perception of the envi-
ronment by computer vision algorithms has a high
error rate (84.7% of the objects are mis-recognized),
our approach can still correctly ground those mis-
recognized objects with 66% accuracy. The results
demonstrate that, due to its error-tolerance nature,
inexact graph matching provides a potential solu-
140
tion to mediate shared perceptual basis for referen-
tial grounding in situated interaction.
In the following sections, we first describe an em-
pirical study based on a virtual environment to ex-
amine how partners mediate their mismatched visual
perceptual basis. We then provide details about our
graph matching based approach and its evaluation.
2 Related Work
There has been an increasing number of published
works on situated language understanding(Scheutz
et al, 2007a; Foster et al, 2008; Skubic et al,
2004; Huwel and Wrede, 2006), focusing on inter-
pretation of referents in a shared environment. Dif-
ferent approaches have been developed to resolve
visual referents. Gorniak and Roy present an ap-
proach that grounds referring expressions to visual
objects through semantic decomposition, using con-
text free grammar that connect linguistic structures
with underlying visual properties (Gorniak and Roy,
2004a). Recently, they have extended this work
by including action-affordances (Gorniak and Roy,
2007). This line of work has mainly focused on
grounding words to low-level visual properties. To
incorporate situational awareness, incremental ap-
proaches have been developed to prune interpreta-
tions which do not have corresponding visual ref-
erents in the environment (Scheutz et al, 2007a;
Scheutz et al, 2007b; Brick and Scheutz, 2007).
A recent work applies a bidirectional approach to
connect bottom-up incremental language processing
to top-down constrains on possible interpretation of
referents given situation awareness (Kruijff et al,
2007). Most of these previous works address utter-
ance level processing. Here, we are interested in ex-
ploring how the mismatched perceptual capabilities
influences the collaborative discourse, and develop-
ing a graph-based framework for referential ground-
ing with mismatched perceptions.
3 Empirical Study
It is very difficult to study the collaborative pro-
cess between partners with mismatched perceptual
capabilities. Subjects with truly mismatched per-
ceptual capabilities are difficult to recruit, and the
discrepancy between capabilities is difficult to mea-
sure and control. The wizard-of-oz studies with
Figure 1: Our experimental system. Two partners collab-
orate on an object naming task using this system. The
director on the left side is shown an (synthesized) origi-
nal image, while the matcher on the right side is shown
an impoverished version of the original image.
physical robots (e.g., as in (Green and Severin-
son Eklundh, 2001; Shiomi et al, 2007; Kahn et al,
2008)) are also insufficient since it is not clear what
should be the underlying principles to guide the wiz-
ard?s decisions and thus the perceived robot?s behav-
iors (Steinfeld et al, 2009). To address these prob-
lems, motivated by the Map Task (Anderson et al,
1991) and the recent encouraging results from vir-
tual simulation in Human Robot Interaction (HRI)
studies (Carpin et al, 2007; Chernova et al, 2010),
we conducted an empirical study based on virtual
simulations of mismatched perceptual capabilities.
3.1 Experimental System and Task
The setup of our experimental system is shown in
Figure 1. In the experiment, two human partners
(a director and a matcher) collaborate on an object
naming task. The mismatched perceptual capabili-
ties between partners are simulated by different ver-
sions of an image shown to them: the director looks
at an original image, while the matcher looks at an
impoverished version of the original image.
The original image (the one on the left in Fig-
ure 1) was created by randomly selecting images of
daily-life items (office supplies, fruits, etc.) from
an image database and randomly positioning them
onto a background. To create the impoverished im-
141
age (the one on the right in Figure 1), we applied
standard Computer Vision (CV) algorithms to pro-
cess the original image and then create an abstract
representation based on the outputs from the CV al-
gorithms.
More specifically, the original image was fed
into a segmentation ? feature extraction ?
recognition pipeline of CV algorithms. First, the
OTSU algorithm (Otsu, 1975) was used for image
segmentation. Then visual features such as color
and shape were extracted from the segmented re-
gions (Zhang and Lu, 2002). Finally, object recogni-
tion was done by searching the nearest neighbor (in
the shape-feature vector space) from a knowledge
base of ?known? objects. The impoverished image
was then created based on the CV algorithms? out-
puts. For example, if an object in the original image
was recognized as a pear, an abstract illustration of
pear would be displayed in the impoverished image
at the same position. Other features such like color
and size of the object were also extracted from the
original image and assigned to the illustration in the
impoverished image.
In the naming task, the director?s goal is to com-
municate the ?secret names? of some randomly se-
lected objects (i.e., target objects) in his/her image to
the matcher, so that the matcher would know which
object has what name. As shown in Figure 1, those
secret names are displayed only on the director?s
screen but not the matcher?s. Once the matcher be-
lieves that he/she correctly acquires the name of an
target object, he/she will record the name by mouse-
clicking on the target and repeating the name. A
task is considered complete when the matcher has
recorded the names of all the target objects.
3.2 Examples
Consistent with previous findings (Liu et al, 2011),
our empirical study shows that human partners tend
to combine object properties and spatial relations to
construct their referring expressions. In addition,
our empirical study has further demonstrated how
partners manage to mediate their perceptual basis
through collaborative discourse. Here are two ex-
amples from our data:
Example 1.
D1: the very top right hand corner, there is a red apple
M: ok
D: and then to the left of that red apple on the top of the
screen is a red or black cherry
M: ok
D: and then to the left of that is a brown kiwi fruit
M: ok
D: and the, the red cherry is called Richard
? ? ? ? ? ?
Example 2.
D: ok, um, so can we start in the top right
M: alright, um, the top right there are two rows of items,
they are all circular or apple shaped
D: ok, um, the item in the very top right corner does not
have a name
M: um, no name
M: um, to the left of that
D: yes, to the left of that is Richard
M: ok, are there only three items in that row
D: yes, there are only three
M: ok, this is Richard
? ? ? ? ? ?
As shown in Example 1, the most commonly used
object properties include object class, color, spatial
location, and others such as size, length and shape.
For the relations, the most common one is the pro-
jective spatial relations (Liu et al, 2010), such as
right, left, above, below. Besides, as illustrated by
Example 2, descriptions based on grouping of mul-
tiple objects are also commonly used. To mediate
their shared basis, both the director and the matcher
make extra effort to collaborate with each other. For
instance, in Example 1, the director applies install-
ment (Clark and Wilkes-Gibbs, 1986) where he ut-
ters noun phrases in episodes and the matcher ex-
plicitly accepts each installment before the director
moves forward. In Example 2, the matcher intends
to assist the grounding process by proactively pro-
viding what he perceives about the environment.
The data collected from our empirical study have
indicated that, to mediate a shared perceptual basis
and ground references, a successful method should
consider the following issues: (1) It needs to capture
the dynamics of the linguistic discourse and iden-
tify various relations among different referring ex-
pressions throughout discourse. (2) it needs to rep-
resent the perceived visual features and topological
relations between visual objects in the visual dis-
course. (3) Because the perceived visual world by
1D stands for Director and M for Matcher.
142
the matcher (who represents the lower-calibre arti-
ficial agent) very often differs from the perceived
visual world by the director (who represents the
higher-calibre human partner), reference resolution
will need some approximation without enforcing a
complete satisfaction of constraints. Based on these
considerations, we have developed a graph-based
approach for referential grounding. Next we give
a detailed account on this approach.
4 A Graph-based Approach to Referential
Grounding
In the field of image analysis and pattern recogni-
tion, Attributed Relational Graph (ARG) is a very
useful data structure to represent an image (Tsai and
Fu, 1979; Sanfeliu and Fu, 1983). In an ARG, the
underlying unlabeled graph represents the topologi-
cal structure of the scene. Then each node and edge
are labeled with a vector of attributes that represents
local features of a single node or the topological fea-
tures between two nodes. Based on the ARG rep-
resentations, an inexact graph matching is to find
a graph or a subgraph whose error-transformation
cost with the already given graph is minimum (Es-
hera and Fu, 1984).
Motivated by the representation power of ARG
and the error-correcting capability of inexact graph
matching, we developed a graph-based approach to
address the referential grounding problem. ARG
and probabilistic graph matching have been pre-
viously applied in multimodal reference resolu-
tion (Chai et al, 2004a; Chai et al, 2004b) by in-
tegrating speech and gestures. Here, although we
use similar ARG representations, our algorithm is
based on inexact graph matching and our focus is on
mediating shared perceptual basis.
4.1 Graph Representations
Figure 2 illustrates the key elements and the process
of our graph-based method. The key elements of our
method are two ARG representations, one of which
is called the discourse graph and the other called the
vision graph.
The discourse graph captures the information ex-
tracted from the linguistic discourse.2 To create the
discourse graph, the linguistic discourse first needs
2Currently we only focus on the utterances from the director.
Figure 2: An illustration of graph representations in our
method. The discourse graph is created from formal se-
mantic representations of the linguistic discourse; The vi-
sion graph is created by applying CV algorithms on the
corresponding scene. Given the two graphs, referential
grounding is to construct a node-to-node mapping from
the discourse graph to the vision graph.
to be processed by NLP components, such as the se-
mantic composition and discourse coreference res-
olution components. The output of the NLP com-
ponents are usually in the form of some formal se-
mantic representations, e.g. in the form of first-order
logic representations. The discourse graph is then
created based on the formal semantics, i.e. each
new discourse entity corresponds to a node in the
graph, one-arity predicates correspond to node at-
tributes and two-arity predicates correspond to edge
attributes. The vision graph, on the other hand, is a
representation of the visual features extracted from
the scene. Each object detected by CV algorithms
is represented as a node in the vision graph, and the
attributes of the node correspond to visual features,
such as the color, size and position of the object. The
edges between nodes represent their relations in the
physical space.
Given the discourse graph and the vision graph,
now we can formulate referential grounding as con-
structing a node-to-node mapping from the dis-
course graph to the vision graph, or in other words,
a matching between the two graphs. Note that, the
143
matching we encounter here is different from the
original graph matching problem that is often used
in the image analysis field. The original version only
considers matching between two graphs that have
the same type of values for each attribute. But in
the case of referential grounding, all the attributes in
the discourse graph possess symbolic values since
they come from formal semantic representations,
whereas the attributes in the vision graph are often
numeric values produced by CV algorithms. Our so-
lution is to introduce a set of symbol grounding func-
tions, which bridges the heterogeneous attributes of
the two graphs and makes general graph matching
algorithms applicable to referential grounding.
4.2 Inexact Graph Matching
We formulate referential grounding as a graph
matching problem, which has extended the origi-
nal graph matching approach used in image process-
ing and pattern recognition filed (Tsai and Fu, 1979;
Tsai and Fu, 1983; Eshera and Fu, 1984).
First, we give the formal definition of an ARG,
which is a doublet of the form
G = (N,E)
where
N The set of attributed-nodes of graph G,
defined as
N = {(i, a) |1 ? i ? |N | } .
E The set of directed attributed-edges of
graph G, defined as
E = {(i, j, e) |1 ? i, j ? |N | } .
(i, a) ? N Node i with a as its attribute vector,
where a = [v1, v2, ? ? ? , vK ] is a vector
of K attributes. To simplify the nota-
tion, We will denote a node as ai.
(i, j, e) ? E The directed edge from node i to node
j with e as its attribute vector, where
e = [u1, u2, ? ? ? , uL] is a vector of L
attributes. We will denote an edge as
eij .
In an ARG, the value of a node/edge attribute
vk/ul can be symbolic, numeric, or as a vector of
numeric values. For example, if v1 is used to rep-
resent the color feature of an object, then a possible
assignment could be v1 = [255, 0, 0], which is the
rgb color vector.
Suppose we represent referring expressions from
the linguistic discourse as a discourse graph G and
objects perceived from the environment as a vi-
sion graph G?, referential grounding then becomes
a graph matching problem: given G = (N,E) and
G? = (N ?, E?), in which
N = {ai |1 ? i ? I } , E = {ei1i2 |1 ? i1, i2 ? I }
N ? = {aj ? |1 ? j ? J } , E? = {e?j1j2 |1 ? j1, j2 ? J }
A matching between G and G? is to find a one-to-
one mapping between the nodes in N and the nodes
in N ?.
Note that it is not necessary for every node in
N or N ? to be mapped to a corresponding node in
the other graph. If a node is not to be mapped to
any node in the other graph, we describe it as be-
ing mapped to ?, which denotes an abstract ?null?
node. To represent the matching result, we re-order
N and N ? such that the first I ?/J ? nodes in N /N ? are
those which have been mapped to their correspond-
ing nodes in the other graph, and the nodes after
them are the unmatched nodes, i.e. those matched
with ?. Then the matching result is
M = M1 ?M2 ?M3
= {(i, j) |1 ? i ? I ?, 1 ? j ? J ? }
? {i |I ? < i ? I }
? {j |J ? < j ? J }
Here M1 is a set of I ? pairs of indices of matched
nodes. M2 and M3 are the sets of indices of all the
unmatched nodes in N and N ?, respectively. Then
M is what we call a matching between G and G?.
It is an inexact matching in the sense that we allow
bothG andG? to have a subset of nodes, i.e. M2 and
M3, that are not matched with any node in the other
graph (Conte et al, 2004). The cost of a matching
M is then defined as
C (M) = C (M1) + C (M2) + C (M3)
To complete the definition of C (M), we use M11
to denote the set of all the first indices of the matched
pairs in M1, i.e. M11 = {i |1 ? i ? I ? }, and H =(
NH , EH
) is the subgraph of G that is induced by
the subset of nodes NH = {ai |i ?M11 }, then we
have
C (M1) =
?
ai?NH
CN (ai, a?j) +
?
ei1i2?EH
CE (ei1i2 , e?j1j2)
C (M2) =
?
ai?(N?NH)
CN (ai,?) +
?
ei1i2?(E?EH)
CE (ei1i2 ,?)
144
in which CN (ai, a?j) is the cost of mapping ai
to a?j , CE (ei1i2 , e?j1j2) is the cost of mapping
ei1i2 to e?j1j2 , and CN (ai,?)/CE (ei1i2 ,?) is the
cost of mapping ai/ei1i2 to the null node/edge.
They are also called node/edge substitution cost and
node/edge insertion cost, respectively (Eshera and
Fu, 1984). Note that, in our case we let C (M3) = 0
since we have assumed that the size of G? is bigger
than the size of G.
Finally, the optimal matching between G and G?
is the one with the minimum matching cost
M? = arg min
M
C (M)
which gives us the most feasible result of grounding
the entities in the discourse graph with the objects in
the vision graph.
Given our formulation of referential grounding as
a graph matching problem, the next question is how
to find the optimal matching between two graphs.
Unfortunately, such a problem belongs to the class
of NP-complete (Conte et al, 2004). In practice,
techniques such as A? search are commonly used to
improve the efficiency, e.g. in (Tsai and Fu, 1979;
Tsai and Fu, 1983). But the memory requirement
can still be considerably large if the heuristic does
not provide a close estimate of the future matching
cost (Conte et al, 2004). In our current approach, we
use a simple beam search algorithm (Zhang, 1999)
to retain the tractability. Following the assumption
in (Eshera and Fu, 1984), we set the beam size as
hJ2, where h is the current level of the search tree
and J is the size of the bigger graph (in our caseG?).
4.3 Symbol Grounding Functions
As mentioned in Section 4.1, in referential ground-
ing the discourse graph and the vision graph pos-
sess different types of attribute values, therefore we
introduce a set of ?symbol grounding functions?,
based on which node/edge substitution and insertion
costs can be formally defined.
We start with node substitution cost to give a for-
mal definition of symbol grounding functions. As
defined in the previous section, the node substitu-
tion cost of mapping (substituting) node a with node
a? is3
CN (a, a?)
3For the ease of notation we have dropped the subscript of a
node.
Recall that in our definition of ARG, each node
is represented by a vector of attributes, i.e. a =
[v1, v2, ? ? ? , vK ] and a? = [v?1, v?2, ? ? ? , v?K ]. Thus,
we define the node substitution cost as
CN (a, a?) =
K?
k=1
? ln fk (vk, v?k)
in which fk (vk, v?k) = p (p ? [0, 1]) is what we call
the symbol grounding function for the k-th attribute.
More specifically, a symbol grounding function
for the k-th attribute takes two input arguments,
namely vk and v?k, which are the values of the k-th
attribute from node a and a? respectively. The out-
put of the function is a real number p in the range
of [0, 1], which can be interpreted as a measurement
of the compatibility between a symbol (or word) vk
and a visual feature value v?k.
Let L = {w1, w2, ? ? ? , wZ ,UNK} be the set of all
possible symbolic values of vk, then fk (vk, v?k) can
be further decomposed as
fk (vk, v?k) =
?
??????
??????
fk1 (v?k) if vk = w1;
fk2 (v?k) if vk = w2;... ...
fkZ (v?k) if vk = wZ ;
?k if vk = UNK.
Here the idea is that each value of vk may specify an
unique function that determines the compatibility of
a visual feature value v?k. For example, suppose that
we are defining a symbol grounding function for the
attribute of ?spatial location?, i.e. where is an ob-
ject located in the environment. The symbolic value
v can be in the set of {Top,Bottom, ? ? ? ,UNK}, and
the visual feature value v? is the x and y coordinates
(in pixels) of the object?s center of mass in the im-
age. A grounding function for the symbol Top can
be defined as4
fTop (v?) = fTop (x, y) =
{
1? y800 if y < 400;
0 otherwise.
Note that we have added a special symbol UNK
to represent the ?unknown? (or ?unspecified?) value
of vk. When the value of an attribute in the dis-
course graph is unknown, i.e. the speaker did not
mention anything about a particular property, the
grounding function will simply return a predefined
4Assume that the size of the image is 800? 800 pixels and
the left-top corner is the origin (0, 0)
145
Type of Error Numberof Objects
No Error 9 (5.1%)
Recognition Error 150 (84.7%)
Segmentation Error 18 (10.2%)
Total 177
Table 1: Types of errors among all the target (named)
objects. Recognition error: an object is incorrectly rec-
ognized as another type of object, or an unknown type.
Segmentation error: an object is missing, or merged with
another object.
constant, which we denote as ?. The node insertion
cost CN (a,?) is now defined as5
CN (a,?) =
K?
k=1
? ln ?k
Currently we set al the symbol grounding func-
tions? outputs for the unknown value (i.e. the ?s) to
?, which is an arbitrarily small real number (? > 0).
5 Empirical Results
Three pairs of subjects participated in our experi-
ment. Each pair (one acted as the director and the
other as the matcher) completed the naming task on
8 randomly created images. In total we collected 24
dialogues with 177 target objects to be named. Table
1 summarizes the errors made by the CV algorithms
when the 177 named objects from the original im-
ages were processed and represented in the impover-
ished images, as described in Section 3.1. As shown
in the table, only 5% of the objects were correctly
represented in the impoverished images. The other
95% of objects were either mis-recognized (about
85%) or mis-segmented (10%).
The evaluation of our approach is based on
whether the target objects are correctly grounded
by the graph matching method. To focus our cur-
rent effort on the referential grounding aspect, we
ignored all the matchers? contributions to the dia-
logues. Thus the discourse graphs were built based
on only the director?s utterances. The formal se-
mantics of each of the director?s valid utterances
was manually annotated using the DRS (Discourse
Representation Structure) representation (Bird et al,
2009). The discourse graphs were then generated
5The edge substitution/insertion cost is defined in the same
way as the node substitution/insertion cost.
Accuracy/Detection Rate
Type of Error Object-properties Object-properties
Only and Relations
No Error 66.7% (6/9) 77.8% (7/9)
Recognition Error 38.7% (58/150) 66% (99/150)
Segmentation Error 33.3% (6/18) 44.4% (8/18)
Overall 39.5% (70/177) 64.4% (114/177)
Table 2: Referential grounding performance of our
method. The accuracy/detection rates in the table were
obtained by comparing the results with annotated ground
truths.
from the annotated formal semantics. The vision
graphs were generated from the outputs of the CV
algorithms. The graph matching method was then
applied to return a (sub-) optimal matching between
the two graphs.
Table 2 shows the referential grounding perfor-
mance of our method. To better understand the ad-
vantages of the graph-based approach, we have com-
pared two settings. In the first setting, only the
object-specific properties are considered for com-
puting the comparability between a linguistic ex-
pression and a visual object, and the relations be-
tween objects are ignored. This setting is similar
to the baseline approach used in (Prasov and Chai,
2008; Prasov and Chai, 2010). In the second set-
ting, the complete graph-based approach is applied,
i.e. both the object?s properties and the relations be-
tween objects are considered. As shown in Table 2,
although the improvements of performance for the
no-error objects and mis-segmented objects are not
significant due to the small sample sizes, the perfor-
mance for the mis-recognized objects is significantly
improved by 27.3% (p < .001). The improvement
for the overall performance is also significant (by
24.9%, p < .001). The comparison between two
settings have demonstrated the importance of rep-
resenting and reasoning on relations between ob-
jects in referential grounding, and the graph-based
approach provides an ideal solution to capture rela-
tions.
In particular, even CV error rate is high (due to the
simple CV algorithms we used), our method is still
able to achieve 66% accuracy of grounding the mis-
recognized objects. Furthermore, when a referred
object is completely ?missing? in the vision graph
146
due to segmentation error6, our method is capable
to detect such discrepancy between linguistic input
and visual perception. The results have shown that
44.4% of those cases have been correctly detected.
This is also a very important aspect since informa-
tion about failures of grounding will allow the di-
alogue manager and/or the vision system to adapt
better strategies.
6 Discussions
The work presented here only represents an initial
step in our on-going investigation towards mediat-
ing shared perceptual basis in situated dialogue. It
consists of several simplifications which will be ad-
dressed in our future work.
First, the discourse graph is created only based
on contributions from the director, using manual an-
notations of formal semantics of the discourse. As
shown in the examples (Section 3.2), the collabora-
tive discourse has rich dynamics reflecting partici-
pants? collaborative behaviors. So our future work
is to model these different discourse dynamics and
take them into account in the creation of the dis-
course graph. The discourse graph will be created
after each contribution as the conversation unfolds.
When utterances are automatically processed, se-
mantics of these utterances often will not be ex-
tracted correctly or completely as in their manual
annotations. Therefore, our future work will also
explore how to efficiently match hypothesized dis-
course graphs (from automated semantic process-
ing) with vision graphs.
Second, our current symbol grounding functions
are very simple and intuitive. Our future work will
explore more sophisticated models that have theoret-
ical motivations (e.g., grounding spatial terms based
on the Attentional Vector Sum (AVS) model (Regier
and Carlson, 2001)) and enable automated acquisi-
tion of these functions (Roy, 2002; Gorniak and Roy,
2004b). In addition, we will explore context-based
symbol grounding functions where context will be
explicitly modeled. Grounding a linguistic term to a
visual feature will be influenced by contextual fac-
tors such as surroundings of the environment, the
6For example, if the director refers to ?a white ball? but
CV algorithm fails to detect that object from the environment,
then the node in the discourse graph representing ?a white ball?
should not be mapped to anything in the vision graph.
discourse history, the speaker?s individual prefer-
ence, and so on.
Lastly, as shown in our examples, the matcher
also contributes significantly to ground references.
This appears to suggest that, in situated dialogue,
lower-calibre partners (i.e., robot, and here the
matcher) also make extra effort to ground refer-
ences. The underlying motivation could be their
urge to match what they perceive from the environ-
ment to what they are told by their higher-calibre
partners (i.e., human). This motivation can be poten-
tially modeled as graph-matching and can be used
to guide the design of system responses. We will
explore this idea in the future.
7 Conclusion
In situated human robot dialogue, a robot and its
human partners have significantly mismatched capa-
bilities in perceiving the environment, which makes
grounding of references in the environment espe-
cially difficult. To address this challenge, this paper
describes an empirical study investigating how hu-
man partners mediate the mismatched perceptual ba-
sis. Based on this data, we developed a graph-based
approach and formulate referential grounding as in-
exact graph matching. Although our current investi-
gation has several simplifications, our initial empiri-
cal results have shown the potential of this approach
in mediating shared perceptual basis in situated dia-
logue.
Acknowledgments
This work was supported by Award #1050004 and
Award #0957039 from National Science Founda-
tion and Award #N00014-11-1-0410 from Office of
Naval Research.
References
A.H. Anderson, M. Bader, E.G. Bard, E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, et al 1991. The hcrc map task corpus. Lan-
guage and speech, 34(4):351?366.
S. Bird, E. Klein, and E. Loper. 2009. Natural language
processing with Python. O?Reilly Media.
T. Brick and M. Scheutz. 2007. Incremental natural
language processing for hri. In Proceeding of the
147
ACM/IEEE international conference on Human-Robot
Interaction (HRI-07), pages 263?270.
S. Carpin, M. Lewis, J. Wang, S. Balakirsky, and
C. Scrapper. 2007. USARSim: a robot simulator for
research and education. In Proceedings of the 2007
IEEE Conference on Robotics and Automation.
J.Y. Chai, P. Hong, and M.X. Zhou. 2004a. A proba-
bilistic approach to reference resolution in multimodal
user interfaces. In Proceedings of the 9th international
conference on Intelligent user interfaces, pages 70?77.
ACM.
J.Y. Chai, P. Hong, M.X. Zhou, and Z. Prasov. 2004b.
Optimization in multimodal interpretation. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 1. Association for
Computational Linguistics.
S. Chernova, J. Orkin, and C. Breazeal. 2010. Crowd-
sourcing hri through online multiplayer games. AAAI
Symposium on Dialogue with Robots.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a
collaborative process. In Cognition, number 22, pages
1?39.
H. H. Clark. 1996. Using language. Cambridge Univer-
sity Press, Cambridge, UK.
D. Conte, P. Foggia, C. Sansone, and M. Vento. 2004.
Thirty years of graph matching in pattern recognition.
International journal of pattern recognition and artifi-
cial intelligence, 18(3):265?298.
M. A. Eshera and K. S. Fu. 1984. A graph distance mea-
sure for image analysis. IEEE transactions on systems,
man, and cybernetics, 14(3):398?410.
M.E. Foster, E.G. Bard, R.L. Hill, M. Guhe, J. Oberlan-
der, and A. Knoll. 2008. Generating haptic- osten-
sive referring expressions in cooperative, task-based
human-robot dialogue. Proceedings of ACM/IEEE
Human-Robot Interaction.
B. Fransen, V. Morariu, E. Martinson, S. Blis-
ard, M. Marge, S. Thomas, A. Schultz, and
D. Perzanowski. 2007. Using vision, acoustics, and
natural language for disambiguation. In Proceedings
of HRI07, pages 73?80.
P. Gorniak and D. Roy. 2004a. Grounded semantic com-
position for visual scenes. In Journal of Artificial In-
telligence Research, volume 21, pages 429?470.
P. Gorniak and D. Roy. 2004b. Grounded semantic com-
position for visual scenes. J. Artif. Intell. Res. (JAIR),
21:429?470.
P. Gorniak and D. Roy. 2007. Situated language under-
standing as filtering perceived affordances. In Cogni-
tive Science, volume 31(2), pages 197?231.
A. Green and K. Severinson Eklundh. 2001. Task-
oriented dialogue for CERO: a user centered ap-
proach. In Proceedings of 10th IEEE international
workshop on robot and human interactive communi-
cation, September.
Sonja Huwel and Britta Wrede. 2006. Situated
speech understanding for robust multi-modal human-
robot communication. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING/ACL).
P. Kahn, N. Greier, T. Kanda, H. Ishiguro, J. Ruckert,
R. Severson, and S. Kane. 2008. Design patterns for
sociality in human-robot interaction. In Proceedings
of HRI, pages 97?104.
Geert-Jan M. Kruijff, Pierre Lison, Trevor Benjamin,
Henrik Jacobsson, and Nick Hawes. 2007. Incremen-
tal, multi-level processing for comprehending situated
dialogue in human-robot interaction. In Symposium on
Language and Robots.
C. Liu, J. Walker, and J.Y. Chai. 2010. Disambiguating
frames of reference for spatial language understanding
in situated dialogue. In AAAI Fall Symposium on Dia-
logue with Robots.
C. Liu, D. Kay, and J.Y. Chai. 2011. Awareness of part-
ners eye gaze in situated referential grounding: An em-
pirical study. In 2nd Workshop on Eye Gaze in Intelli-
gent Human Machine Interaction.
N. Otsu. 1975. A threshold selection method from gray-
level histograms. Automatica, 11:285?296.
Z. Prasov and J.Y. Chai. 2008. What?s in a gaze?: the
role of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of the 13th
international conference on Intelligent user interfaces,
pages 20?29. ACM.
Z. Prasov and J.Y. Chai. 2010. Fusing eye gaze with
speech recognition hypotheses to resolve exophoric
references in situated dialogue. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 471?481. Association
for Computational Linguistics.
T. Regier and L.A. Carlson. 2001. Grounding spatial lan-
guage in perception: an empirical and computational
investigation. Journal of Experimental Psychology:
General, 130(2):273.
D.K. Roy. 2002. Learning visually grounded words and
syntax for a scene description task. Computer Speech
& Language, 16(3):353?385.
A. Sanfeliu and K. S. Fu. 1983. A distance measure
between attributed relational graphs for pattern recog-
nition. IEEE transactions on systems, man, and cyber-
netics, 13(3):353?362.
M. Scheutz, P. Schermerhorn, J. Kramer, and D. Ander-
son. 2007a. First steps toward natural human-like
HRI. In Autonomous Robots, volume 22.
M. Scheutz, P. Schermerhorn, J. Kramer, and D. Ander-
son. 2007b. Incremental natural language processing
for hri. In Proceedings of HRI.
148
M. Shiomi, T. Kanda, S. Koizumi, H. Ishiguro, and
N. hagita. 2007. Group attention control for commu-
nication robots with wizard of OZ approach. In Pro-
ceedings of HRI, pages 121?128.
M. Skubic, D. Perzanowski, S. Blisard, A. Schultz,
W. Adams, M. Bugajska, and D. Brock. 2004. Spatial
language for human-robot dialogs. IEEE Transactions
on Systems, Man and Cybernetics, Part C, 34(2):154?
167.
A. Steinfeld, O. C. Jenkins, and B. Scassellati. 2009. The
oz of wizard: Simulating the human for interaction re-
search. In Proceedings of HRI, pages 101?107.
W.H. Tsai and K.S. Fu. 1979. Error-correcting isomor-
phisms of attributed relational graphs for pattern anal-
ysis. Systems, Man and Cybernetics, IEEE Transac-
tions on, 9(12):757?768.
W.H. Tsai and K.S. Fu. 1983. Subgraph error-correcting
isomorphisms for syntactic pattern. year: 1983,
13:48?62.
D. Zhang and G. Lu. 2002. An integrated approach to
shape based image retrieval. In Proc. of 5th Asian con-
ference on computer vision (ACCV), pages 652?657.
W. Zhang. 1999. State-space search: Algorithms, com-
plexity, extensions, and applications. Springer-Verlag
New York Inc.
149
Proceedings of the SIGDIAL 2013 Conference, pages 78?86,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Modeling Collaborative Referring for Situated Referential Grounding
Changsong Liu, Rui Fang, Lanbo She, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{cliu, fangrui, shelanbo, jchai}@cse.msu.edu
Abstract
In situated dialogue, because humans and
agents have mismatched capabilities of
perceiving the shared physical world, ref-
erential grounding becomes difficult. Hu-
mans and agents will need to make ex-
tra efforts by collaborating with each other
to mediate a shared perceptual basis and
to come to a mutual understanding of in-
tended referents in the environment. In
this paper, we have extended our previous
graph-matching based approach to explic-
itly incorporate collaborative referring be-
haviors into the referential grounding al-
gorithm. In addition, hypergraph-based
representations have been used to account
for group descriptions that are likely to oc-
cur in spatial communications. Our empir-
ical results have shown that incorporating
the most prevalent pattern of collaboration
with our hypergraph-based approach sig-
nificantly improves reference resolution in
situated dialogue by an absolute gain of
over 18%.
1 Introduction
As more and more applications require humans
to interact with robots, techniques to support sit-
uated dialogue have become increasingly impor-
tant. In situated dialogue, humans and artificial
agents (e.g., robots) are co-present in a shared
environment to achieve joint tasks. Their dia-
logues often involve making references to the en-
vironment. To ensure the conversation proceeds
smoothly, it is important to establish a mutual un-
derstanding of these references, a process called
referential grounding (Clark and Brennan, 1991):
the agent needs to identify what the human refers
to in the environment and the human needs to
know whether the agent?s understanding is correct;
and vice versa.
Although reference resolution (Heeman and
Hirst, 1995; Gorniak and Roy, 2004; Siebert
and Schlangen, 2008) and referential ground-
ing (Traum, 1994; DeVault et al, 2005) have been
studied in previous work, the unique characteris-
tics of situated dialogue post bigger challenges to
this problem. In situated dialogue, although hu-
mans and agents are co-present in a shared world,
they have different capabilities in perceiving the
environment (a human can perceive and reason
about the environment much better than an agent).
The shared perceptual basis, which plays an im-
portant role in facilitating referential grounding
between the human and the agent, thus is miss-
ing. Communication between the human and the
agent then becomes difficult, and they will need
to make extra efforts to jointly mediate a shared
basis and reach a mutual understanding (Clark,
1996). The goal of this paper is to investigate what
kinds of collaborative efforts may happen under
mismatched perceptual capabilities and how such
collaborations can be incorporated into our refer-
ential grounding algorithm.
Previous psycholinguistic studies have indi-
cated that grounding references is a collaborative
process (i.e., collaborative referring) (Clark and
Wilkes-Gibbs, 1986; Clark and Brennan, 1991):
The process begins with one participant present-
ing an initial referring expression. The other par-
ticipant would then either accept it, reject it, or
postpone the decision. If a presentation is not
accepted, then either one participant or the other
needs to refashion it. This new presentation (i.e.,
the refashioned expression) is then judged again,
and the process continues until the current pre-
sentation is accepted. To understand the implica-
tion of collaborative referring under the situation
of mismatched perceptual capabilities, we have
conducted experiments on human-human conver-
sation using a novel experimental setup. Our col-
lected data demonstrate an overwhelming use of
78
collaborative referring to mediate a shared percep-
tual basis.
Motivated by these observations, we have de-
veloped an approach that explicitly incorporates
collaborative referring into a graph-matching al-
gorithm for referential grounding. As the conver-
sation unfolds, our approach incrementally builds
a dialogue graph by keeping track of the contri-
butions (i.e., presentation and acceptance) from
both the human and the robot. This dialogue
graph is then matched against the perceived en-
vironment (i.e., a vision graph representing what
are perceived by the robot from the environment)
in order to resolve referring expressions from the
human. In addition, in contrast to our previous
graph-based approach (Liu et al, 2012), the new
approach applies hypergraphs: a more general
and flexible representation that can capture group-
based (n-ary) relations (whereas a regular graph
can only model binary relations between two enti-
ties). Our empirical results have shown that, incor-
porating the most prevalent pattern of collabora-
tion (i.e., agent-present-human-accept, discussed
later) with the hypergraph-based approach signif-
icantly improves reference resolution in situated
dialogue by an absolute gain of over 18%.
In the following sections, we first give a brief
discussion about the related work. We then de-
scribe our experiment setting and the patterns of
collaboration observed in the collected data. We
then illustrate how to build a dialogue graph as
the conversation unfolds, followed by the formal
definition of the hypergraph representation and
the referential grounding procedure. Finally we
demonstrate the advantage of using hypergraphs
and incorporating a prevalent collaborative behav-
ior into the graph-matching approach for reference
resolution.
2 Related Work
In an early work, Mellish (Mellish, 1985) used a
constraint satisfaction approach to identify refer-
ents that could be only partially specified. This
work illustrated the theoretical idea of how to re-
solve referring expressions based on an internal
model of a world. Heeman and Hirst (Heeman
and Hirst, 1995) presented a planning-based ap-
proach to cast Clark?s collaborative referring idea
into a computational model. They used plan con-
struction and plan inference to capture the pro-
cesses of building referring expressions and identi-
fying their referents. Previous work in situated set-
tings (Dhande, 2003; Gorniak and Roy, 2004; Fu-
nakoshi et al, 2005; Siebert and Schlangen, 2008)
mainly focused on developing/learning computa-
tional models that map words to visual features of
objects in the environment. These ?visual seman-
tics? of words were then integrated into seman-
tic composition procedures to resolve referring ex-
pressions.
These previous work has provided valuable in-
sights in computational approaches for reference
resolution. However, they mostly dealt with a sin-
gle expression or a single referent. In this pa-
per, our goal is to resolve complex referring di-
alogues that involve multiple objects in a shared
environment. In our previous work (Liu et al,
2012), we developed a graph-matching based ap-
proach to address this problem. However, the pre-
vious approach can not handle group-based rela-
tions among multiple objects. Furthermore, it did
not look into incorporating collaborative behav-
iors, which is a particularly important characteris-
tic in situated dialogue. This paper aims to address
these limitations.
3 Experiments and Observations
To investigate collaborative referring under mis-
matched perceptual capabilities, we conducted ex-
periments on human-human interaction (details of
the experimental setup can be found in (Liu et al,
2012)). In these experiments, we have two human
subjects play a set of naming games. One sub-
ject (referred to as the human-player) is provided
with an original image containing over ten objects
(Figure 1(a)). Several of these objects have se-
cret names. The other subject (referred to as the
robot-player) only has access to an impoverished
image of the same scene (Figure 1(b)) to mimic
the lower perceptual capability of a robot. The
human-player?s goal is to communicate the names
of target objects to the robot-player so that the
robot-player knows which object in his view has
what name. The impoverished image was auto-
matically created by applying standard computer
vision algorithms and thus may contain different
types of processing errors (e.g., mis-segmentation
and/or mis-recognition).
Using this setup, we have collected a set of dia-
logues. The following shows an example dialogue
segment (collected using the images shown in Fig-
ure 1):
79
Figure 1: An example of different images used in
our experiments.
H1: there is basically a cluster of four objects in the upper
left, do you see that
R2 : yes
H: ok, so the one in the corner is a blue cup
R : I see there is a square, but fine, it is blue
H: alright, I will just go with that, so and then right under
that is a yellow pepper
R : ok, I see apple but orangish yellow
H: ok, so that yellow pepper is named Brittany
R : uh, the bottom left of those four? Because I do see a
yellow pepper in the upper right
H: the upper right of the four of them?
R : yes
H: ok, so that is basically the one to the right of the blue cup
R : yeah
H: that is actually an apple, it is green, I guess it has some
amount of yellow on it, but that is a green apple and it
is named Ashley
. . . . . .
This example demonstrates two important char-
acteristics regarding referential communication
under mismatched perceptual capabilities. First,
conversation partners rely on both object-specific
properties (e.g., object class, color) and spatial
relations to describe objects in the environment.
Spatial expressions include not only the binary re-
lations (e.g., ?the one to the left of the blue cup?),
but also the group-based references (Tenbrink and
Moratz, 2003; Funakoshi et al, 2005) (e.g., ?the
upper right of the four of them?).
Second, because the shared perceptual basis
is missing here, the partners make extra efforts
to refer and ground references. For example,
the human-player go through step-by-step install-
ments (Clark and Wilkes-Gibbs, 1986) to come
to the targeted object. The robot-player often
proactively provides what he perceives from the
environment. The human-player and the robot-
player collaborate with each other through itera-
tive presentation-acceptance phases as described
in the Contribution Model proposed in (Clark and
Schaefer, 1989; Clark and Brennan, 1991).
1H stands for the human-player.
2R stands for the robot-player.
These observations indicate that, the approach
to referential grounding in situated dialogue
should capture not only binary relations but also
group-based relations. Furthermore, it should go
beyond traditional approaches that purely rely on
semantic constraints from single utterances. It
should incorporate the step-by-step collaborative
dynamics from the discourse as the conversation
proceeds.
4 Modeling Collaboration
In this section, we first give a brief description of
collaboration patterns observed in our data, and
then discuss one prevalent pattern and illustrate
how it may be taken into consideration by our
computational approach for referential grounding.
4.1 Patterns of Collaboration
Consistent with Clark?s Contribution Model, the
interactions between the human-player and the
robot-player in general fall into two phases: a pre-
sentation phase and an acceptance phase. In our
data, a presentation phase mainly consists of the
following three forms:
? A complete description: the speaker issues a
complete description in a single turn. For ex-
ample, ?there is a red apple on the top right?.
? An installment: a description is dividedinto several parts/installments, each of whichneeds to be confirmed before continuing tothe rest. For example,
A: under the big green cup we just talked about,
B: yes
A: there are two apples,
B: OK
A: one is red and one is yellow.
? A trial: a description (either completed or in-
complete) with a try marker. For example, ?Is
there a red apple on the top right??
In an acceptance phase, the addressee can either
accept or reject the current presentation. Two ma-
jor forms of accepting a presentation are observed
in our data:
? Acknowledgement: the addressee explicitly
shows his/her understanding, using assertions
(e.g., ?Yes?,?Right?, ?I see?) or continuers
(e.g., ?uh huh?, ?ok?).
? Relevant next turn: the addressee proceeds
to the next contribution that is relevant to the
current presentation. For example: A says ?I
see a red apple? and directly following that B
says ?there is also a green apple to the right
of that red one?.
80
In addition, there are also two forms of rejecting
a presentation:
? Rejection: the addressee explicitly rejects the
current presentation, for example, ?I don?t
see any apple?.
? Alternative description: the addressee
presents an alternative description. For
example, A says ?there is a red apple on the
top left,? and immediately following that B
says ?I only see a red apple on the right?.
In general, referential grounding dialogues in
our data emerge as hierarchical structures of re-
cursive presentation-acceptance phases. The ac-
ceptance to a previous presentation often repre-
sents a new presentation itself, which triggers fur-
ther acceptance. In particular, our data shows
that when mediating their shared perceptual ba-
sis, the human-player often takes into considera-
tion what the robot-player sees and uses that to
gradually lead to his intended referents. This is
demonstrated in the following example3, where
the human-player accepts (Turn 3) the robot-
player?s presentation (Turn 2) through a relevant
next turn.
(Turn 1) H: There is a kiwi fruit.
(Turn 2) R: I don?t see any kiwi fruit. I see an apple.
(Turn 3) H: Do you see a mug to the right of that apple?
(Turn 4) R: Yes.
(Turn 5) H: OK, then the kiwi fruit is to the left of that apple.
As shown later in Section 5, this is one promi-
nent collaborative strategy observed in our data.
We give this pattern a special name: agent-
present-human-accept collaboration. Next we
continue to use this example to show how the
agent-present-human-accept pattern can be incor-
porated to potentially improve reference resolu-
tion.
4.2 An Illustrating Example
In this example, the human and the robot face
a shared physical environment. The robot per-
ceives the environment through computer vision
(CV) algorithms and generates a graph represen-
tation (i.e., a vision graph), which captures the
perceived objects and their spatial relations4. As
shown in Figure 2(a), the kiwi is represented as
an unknown object in the vision graph due to in-
sufficient object recognition. Besides the vision
3This is a clean-up version of the original example to
demonstrate the key ideas.
4The spatial relations between objects are represented as
their relative coordinates in the vision graph.
graph, the robot also maintains a dialogue graph
that captures the linguistic discourse between the
human and the robot.
At Turn 1 in Figure 2(b), the human says ?there
is a kiwi fruit?. Upon receiving this utterance,
through semantic processing, a node representing
?a kiwi? is generated (i.e., x1). The dialogue graph
at this point only contains this single node. Iden-
tifying the referent of the expression ?a kiwi fruit?
is essentially a process that matches the dialogue
graph to the vision graph. Because the vision
graph does not have a node representing a kiwi ob-
ject, no high confidence match is returned at this
point. Therefore, the robot responds with a rejec-
tion as in Turn 2 (Figure 2(c)) ?I don?t see any
kiwi fruit? 5. In addition, the robot takes an extra
effort to proactively describe what is being con-
fidently perceived (i.e., ?I see an apple?). Now
an additional node y1 is added to the dialogue
graph to represent the term ?an apple? 6. Note that
when the robot generates the term ?an apple?, it
knows precisely which object in the vision graph
this term refers to. Therefore, as shown in Fig-
ure 2(c), y1 is mapped to v2 in the vision graph.
In Turn 3 (Figure 2(d)), through semantic pro-
cessing on the human?s utterance ?a mug to the
right of that apple?, two new nodes (x2 and x3)
and their relation (RightOf ) are added to the di-
alogue graph. In addition, since ?that apple?(i.e.,
x2) corefers with ?an apple? (i.e., y1) presented by
the robot in the previous turn, a coreference link
is created from x2 to y1. Importantly, in this turn
human displays his acceptance of the robot?s pre-
vious presentation (?an apple?) by coreferring to it
and building further reference based on it. This is
exactly the agent-present-human-accept strategy
described earlier. Since y1 maps to object v2 and
x2 now links to y1, it becomes equivalent to con-
sider x2 also maps to v2. We name a node such
as x2 a grounded node, since from the robot?s
point of view this node has been ?grounded? to a
perceived object (i.e., a vision graph node) via the
agent-present-human-accept pattern.
At this point, the robot matches the updated di-
alogue graph with the vision graph again and can
5Note that, since in this paper we are working with a
dataset of human-human (i.e., the human-player and the
robot-player) dialogues, decisions from the robot-player are
assumed known. We leave robot?s decision making (i.e., re-
sponse generation) into our future work.
6We use xi to denote nodes that represent expressions
from the human?s utterances and yi to represent nodes from
the robot?s utterances.
81
Figure 2: An example of incorporating collaborative efforts in an unfolding dialogue into graph representations.
successfully match x3 to v3. Note that, the match-
ing occurs here is considered constrained graph-
matching in the sense that some nodes in the dia-
logue graph (i.e., x2) are already grounded, and
the only node needs to be matched against the
vision graph is x3. Different from previous ap-
proaches that do not take dialogue dynamics into
consideration, the constrained matching utilizes
additional constraints from the collaboration pat-
terns in a dialogue and thus can improve both the
efficiency and accuracy of the matching algorithm.
This is one innovation of our approach here.
Based on such matching result, the robot re-
sponds with a confirmation as in Turn 4 Fig-
ure 2(e)). The human further elaborates in Turn
5 ?the kiwi is to the left of the apple?. Again se-
mantic processing and linguistic coreference reso-
lution will allow the robot to update the dialogue
graph as shown in Figure 2(f). Given this dialogue
graph, based on the context of the larger dialogue
graph and through constrained matching, it will
be possible to match x1 to v1 although the object
class of v1 is unknown.
This example demonstrates how the dialogue
graph can be created to incorporate the collabo-
rative referring behaviors as the conversation un-
folds and how such accumulated dialogue graph
can help referential resolution through constrained
matching. Next, we give a detailed account on
how to create a dialogue graph and briefly discuss
graph-matching for reference resolution.
4.3 Dialogue Graph
To account for different types of referring expres-
sions (i.e., object-properties, binary relations and
group-based relations), we use hypergraphs to rep-
resent dialogue graphs.
4.3.1 Hypergraph Representation
A directed hypergraph (Gallo et al, 1993) is a 2-
tuple in the form of G = (X, A), in which
X = {xm}
A = {ai = (ti, hi) | ti ? X, hi ? X}
82
(a) Dialogue Graph (b) Vision Graph
Figure 3: Example hypergraph representations
X is a set of nodes, and A is a set of ?hyperarcs?.
Similar to an arc in a regular directed graph, each
hyperarc ai in a hypergraph also has two ?ends?,
i.e., a tail (ti) and a head (hi). The tail and head
of a hyperarc are both subsets of X , thus they can
contain any number of nodes in X . Hypergraph is
a more general representation than regular graph.
It can represent not only binary relations between
two nodes, but also group-based relations among
multiple nodes.
For example, suppose the language input issued
by the human includes the following utterances:
1. There is a cluster of four objects in the upper left.
2. The one in the corner is a blue cup.
3. Under the blue cup is a yellow pepper.
4. To the right of the blue cup, which is also in the upper
right of the four objects, is a green apple.
The corresponding dialogue graph Gd =
(Xd, Ad) is shown in Figure 3(a), where Xd =
{x1, x2, x3, x4} and Ad = {a1, a2, a3}. In Ad,
for example, a1 = ({x1}, {x3}) represents the
relation ?right of? between the tail {x3} and the
head {x1}, and a3 = ({x3}, {x1, x2, x3, x4}) rep-
resents the group-based relation ?upper right? be-
tween one node and a group of nodes.
As also illustrated in Figure 3(a), we can at-
tach a set of labels (or attributes) {attrk} to a
node/hyperarc, and use them to store specific in-
formation about this node/hyperarc. The per-
ceived visual world can be represented by a
hypergraph in a similar way (i.e., a vision graph),
as shown in Figure 3(b) 7.
4.3.2 Building Dialogue Graphs
Given the hypergraph representation, a set of op-
erations can be applied to build a dialogue graph
as the conversation unfolds. It mainly consists of
three components:
7Hyperarcs of the vision graph are not shown in the figure.
A hyperarc may exist between any two subsets of objects.
Semantic Constraints. Apply a semantic parser to
extract information from human utterances. For
example, the utterance ?The kiwi is to the left of
the apple? can be parsed into a formal meaning
representation as
[x1, x2] , [Kiwi(x1), Apple(x2), LeftOf(x1, x2)]
This representation contains a list of discourse
entities introduced by the utterance, and a list of
FOL predicates specifying the properties and rela-
tions of these entities. For each discourse entity, a
node is added to the graph. Unary predicates be-
come the labels for nodes, and binary predicates
become arcs in the graph. Group-based relations
are incorporated into the graphs as hyperarcs.
Discourse Coreference. For each discourse entity
in a referring expression, identify whether it is a
new discourse entity or it corefers to a discourse
entity mentioned earlier. In our previous example
in Figure 2(d), x2 corefers with y1, thus a coref-
erence link is added to link the coreferring nodes.
Coreferring nodes are merged before matching.
Dialogue Dynamics. Different types of dialogue
dynamics can be modeled. In this paper, we only
focus on a particularly prevalent type of dynamics
as observed from our data, i.e. the agent-present-
human-accept pattern as we described in Section
4.1. When such a pattern is identified, the associ-
ated nodes (e.g., x2 in the previous example) will
be marked as grounded nodes and the mappings
to their grounded visual entities (i.e., vision graph
nodes) will be added into the dialogue graph.
Based on the above three types of operations,
the dialogue graph is updated at each turn of the
conversation.
4.3.3 Constrained Matching
Given a dialogue graph G = (X, A) and a vi-
sion graph G? = (X ?, A?), reference resolution
becomes a graph matching problem which is to
83
find a one-to-one mapping between the nodes in
X and in X ?. Due to the insufficiencies of the
NLP and the CV components, both the dialogue
graph and the vision graph are likely to contain er-
rors. Therefore, we do not require every node in
the dialog graph to be mapped to a node in the vi-
sion graph, but follow the inexact graph matching
criterion (Conte et al, 2004) to find the best match
even if they are only partial.
The matching algorithm is similar to the one
used in our previous work for regular graphs (Liu
et al, 2012), which uses a state-space search ap-
proach (Zhang, 1999). The key difference here
is to incorporate the agent-present-human-accept
collaboration pattern. The search procedure can
now start from the state that already represents
the known matching of grounded nodes (as il-
lustrated in Section 4.2), instead of starting from
the root. Thus it is constrained in a smaller and
more promising subspace to improve both effi-
ciency and accuracy.
5 Evaluation
A total of 32 dialogues collected from our ex-
periments (as described in Section 3) are used in
the evaluation. For each of these dialogues, we
have manually annotated (turn-by-turn) the formal
semantics, discourse coreferences and grounded
nodes as described in Section 4.3.2. Since the fo-
cus of this paper is on incorporating collaboration
into graph matching for referential grounding, we
use these annotations to build the dialogue graphs
in our evaluation. Vision graphs are automatically
generated by CV algorithms from the original im-
ages used in the experiments. The CV algorithms?
object recognition performance is rather low: only
5% of the objects in those images are correctly rec-
ognized. Thus reference resolution will need to
rely on relations and collaborative strategies.
The 32 dialogue graphs have a total of 384
nodes8 that are generated from human-players? ut-
terances (12 per dialogue on average), and a to-
tal of 307 nodes generated from robot-players? ut-
terances (10 per dialogue on average). Among
the 307 robot-player generated nodes, 187 (61%)
are initially presented by the robot-player and
then coreferred by human-players? following ut-
terances (i.e., relevant next turns). This indicates
8As mentioned in Section 4.3.2, multiple expressions that
are coreferential with each other and describing the same en-
tity are merged into a single node.
that the agent-present-human-accept strategy is a
prevalent way to collaborate in our experiment. As
mentioned earlier, those human-player generated
nodes which corefer to nodes initiated by robot-
players are marked as grounded nodes. In total,
187 out of the 384 human-player generated nodes
are in fact grounded nodes.
To evaluate our approach, we apply the graph-
matching algorithm on each pair of dialogue graph
and vision graph. The matching results are com-
pared with the annotated ground-truth to calcu-
late the accuracy of our approach in ground-
ing human-players? referring descriptions to vi-
sual objects. For each dialogue, we have pro-
duced matching results under four different set-
tings: with/without modeling collaborative re-
ferring (i.e., the agent-present-human-accept col-
laboration) and with/without using hypergraphs.
When collaborative referring is modeled, the
graph-matching algorithm uses the grounded
nodes to constrain its search space to match the
remaining ungrounded nodes. When collabora-
tive referring is not modeled, all the human-player
generated nodes need to be matched.
The results of four different settings (averaged
accuracies on the 32 dialogues) are shown in Ta-
ble 1. Modeling collaborative referring improves
the matching accuracies for both regular graphs
and hypergraphs. When regular graphs are used,
it improves overall matching accuracy by 11.6%
(p = 0.05, paired Wilcoxon T-test). The improve-
ment is even higher as 18.3% when hypergraphs
are used (p = 0.012, paired Wilcoxon T-test). The
results indicate that proactively describing what
the robot sees to the human to facilitate com-
munication is an important collaborative strategy
in referential grounding dialogues. Humans can
often ground the robot presented object via the
agent-present-human-accept strategy and use the
grounded object as a reference point to further
describe other intended object(s), and our graph-
matching approach is able to capture and utilize
such collaboration pattern to improve the referen-
tial grounding accuracy.
The improvement is more significant when
hypergraphs are used. A potential explanation
is that those group-based relations captured by
hypergraphs always involve multiple (more than
2) objects (nodes). If one node in a group-based
relation is grounded, all other involved nodes can
have a better chance to be correctly matched.
84
Regular graph Hypergraph
Not modeling 44.1% 47.9%collaborative referring
Modeling 55.7% 66.2%collaborative referring
Improvement 11.6% 18.3%
Table 1: Averaged matching accuracies under four
different settings.
Group 1 Group 2 Group 3
Number of dialogues 9 11 12
% of grounded nodes <30% 30%?60% >60%
Average number of 20 21 12object properties a
Average number of 11 13 8relations b
Not modeling 49.7% 49.4% 45.3%collaborative referring
Modeling 57.0% 76.6% 63.6%collaborative referring
Improvement 7.3% 27.2% 18.3%
aSpecified by human-players.
bSpecified by human-players. The number includes both
binary and group-based relations.
Table 2: Matching accuracies of three groups of
dialogues (all the matching results here are pro-
duced using hypergraphs).
Whereas in regular graphs one grounded node can
only improve the chance of one other node, since
only one-to-one (binary) relations are captured by
regular graphs.
To further investigate the effect of modeling
collaborative referring, we divide the 32 dia-
logues into three groups according to how often
the agent-present-human-accept collaboration pat-
tern happens (measured by the percentage of the
grounded nodes among all the human-player gen-
erated nodes in a dialogue). As shown at the top
part of Table 2, the agent-present-human-accept
pattern happened less often in the dialogues in
group 1 (i.e., less than 30% of human-player gen-
erated nodes are grounded nodes). In the dia-
logues in group 2, robot-players more frequently
provided proactive descriptions which led to more
grounded nodes. Robot-players were the most
proactive in the dialogues in group 3, thus this
group contains the highest percentage of grounded
nodes. Note that, although the dialogues in group
3 contain more proactive contributions from robot-
players, human-players tend to specify less num-
ber of properties and relations describing intended
objects (as shown in the middle part of Table 2).
The matching accuracies for each of the three
groups are shown at the bottom part of Table 2.
Since the agent-present-human-accept pattern ap-
pears less often in group 1, modeling collabora-
tive referring only improves matching accuracy
by 7.3%. The improvements for group 2 and
group 3 are more significant compared to group
1. However, group 3?s improvement is less than
group 2, although the dialogues in group 3 contain
more proactive contributions from robot-players.
This indicates that in some cases even with mod-
eling collaborative referring, underspecified in-
formation from human speakers (human-players
in our case) may still be insufficient to identify
the intended referents. Therefore, incorporating a
broader range of dialogue strategies to elicit ade-
quate information from humans is also important
for successful human-robot communication.
6 Conclusion
In situated dialogue, conversation partners make
extra collaborative efforts to mediate a shared per-
ceptual basis for referential grounding. It is impor-
tant to model such collaborations in order to build
situated conversational agents. As a first step, we
developed an approach for referential grounding
that takes a particular type of collaborative refer-
ring behavior, i.e. agent-present-human-accept,
into account. By incorporating this pattern into the
graph-matching process, our approach has shown
an absolute gain of over 18% in subsequent refer-
ence resolution. Extending the results in this pa-
per, our future work will address explicitly model-
ing the collaborative dynamics with a richer repre-
sentation. The dialogue graph presented in this pa-
per represents all the mentioned entities and their
relations that are currently available at any given
dialogue status. But we have not modeled the col-
laborative dynamics at the illocutionary level. Our
next step is to explicitly represent those dynam-
ics, not only for grounding human references to
the physical world, but also generating the collab-
orative behaviors for the agent.
Acknowledgments
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-
1208390 from the National Science Foundation.
References
Herbert H Clark and Susan E Brennan. 1991. Ground-
ing in communication. Perspectives on socially
shared cognition, 13(1991):127?149.
85
Herbert H Clark and Edward F Schaefer. 1989.
Contributing to discourse. Cognitive science,
13(2):259?294.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1?39.
Herbert H Clark. 1996. Using language, volume 4.
Cambridge University Press Cambridge.
Donatello Conte, Pasquale Foggia, Carlo Sansone, and
Mario Vento. 2004. Thirty years of graph match-
ing in pattern recognition. International journal
of pattern recognition and artificial intelligence,
18(03):265?298.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Pro-
ceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 1?4. Association for
Computational Linguistics.
Sheel Sanjay Dhande. 2003. A computational model
to connect gestalt perception and natural language.
Master?s thesis, Massachusetts Institute of Technol-
ogy.
Kotaro Funakoshi, Satoru Watanabe, Takenobu Toku-
naga, and Naoko Kuriyama. 2005. Understanding
referring expressions involving perceptual grouping.
In 4th International Conference on Cyberworlds,
pages 413?420.
Giorgio Gallo, Giustino Longo, Stefano Pallottino,
and Sang Nguyen. 1993. Directed hypergraphs
and applications. Discrete applied mathematics,
42(2):177?201.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. J. Artif. Intell.
Res.(JAIR), 21:429?470.
Peter A Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21(3):351?382.
Changsong Liu, Rui Fang, and Joyce Y Chai. 2012.
Towards mediating shared perceptual basis in situ-
ated dialogue. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 140?149. Association for Com-
putational Linguistics.
Christopher S Mellish. 1985. Computer interpretation
of natural language descriptions. John Wiley and
Sons, New York, NY.
Alexander Siebert and David Schlangen. 2008. A
simple method for resolution of definite reference
in a shared visual context. In Proceedings of the
9th SIGdial Workshop on Discourse and Dialogue,
pages 84?87. Association for Computational Lin-
guistics.
Thora Tenbrink and Reinhard Moratz. 2003. Group-
based spatial reference in linguistic human-robot in-
teraction. In Proceedings of EuroCogSci, volume 3,
pages 325?330.
David R Traum. 1994. A Computational Theory
of Grounding in Natural Language Conversation.
Ph.D. thesis, University of Rochester.
Weixiong Zhang. 1999. State Space Search: Algo-
rithms, Complexity, Extensions, and Applications.
Springer.
86
