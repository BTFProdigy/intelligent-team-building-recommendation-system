LFG Generat ion  Produces  Context - f ree  Languages  
Rona ld  M.  Kap lan  
Xerox Pale Alto Research Center 
3333 Coyote Hill Road 
Pale Alto, California 94304 USA 
kaplan(@parc.xerox.com 
J / i rgen  Wedek ind  
Center for Language Technology 
Njalsgade 80 
2300 Copenhagen S, Den inark  
juerge l l~cst .ku .dk  
Abst rac t  
This pat}er examines the generation prol}lem for a 
ce\]:tain linguisti{:ally relevant sul0class of LFG gram- 
mars. Our main result; is that the set of strings that 
such a grammar relates to a particular f-structure 
is a context-free language. This result obviously ex- 
l;en{ls to other {:ontext-free base{l grammatical  f(}r- 
malisIns, such its PATll,, and also to formalisms {,hal; 
1)ermit a context-free skeleton to 1}e extracted (1)er- 
haps some variants {}f HPSG). The l)\]:(}{)f is c{mstru{:- 
l;ive: from the given f-sl;ru{:ture a l)art;i{:ular c{}ntext- 
free grannnar is create, d whose, yM{l is the (lesire, d 
sel; Of S|;l'illgS. ~4aily existing generat ion sl;ral;e, gies 
(top-{lown, l}ottom-ul} , head-driven) can be under- 
stood as all:ernative ways of avoiding the creation of 
useless context-Dee productions. Our result can t}e 
estat)lished for the m{}re general {:lass of LFG gram- 
mars, but that is beyond the scope of the present 
paper. 
1 I n t roduct ion  and  Pre l iminar ies  
This 1}al)er exat\]liltes the generation t)\]'{}t)leln for 
a {:erl;ain linguistically mol;ivate, d subclass {}f LFG 
grammars. Our luaill result is thai; the se, l. (}f 
st;rings thai; su{:h a grammar elates to a 1)articular 
f-stru{:l;ure is a context-fl'ee language. This result ex- 
tends easily to other context-fl:ee t)ased gramnmtical 
formalisms, su{-h as PATR (Shiel}er et; al. 1988), al)xt 
I)erhal)s also to tbrinalisms that 1}ermit a {:onl;exl;- 
fi'ee skeleton to l)e e.xtracted from richer ret)resenl;a- 
tions. 
We begin with some ba{:kgroun{1 and formal de- 
filfitions s{} that we can make the 1}roblem and its 
solution explicit. An LFG granmmr G assigns to ev- 
ery string in its language at least one c -s t ructure / f  
structure pair that are set in correst}ondence, by a 
piecewise flmetion (~ (Kaplan 1995). The situation 
can be characterized in terms of a derivation relation 
A(;, defined as follows: 
(1) Aa(s ,  c, (/), f )  ill" G assigns to the string s a 
{:-structure c that pie(:ewise,-corresponds to 
fstru{:ture f via the function (). 
The 'lfiecewise-{:orrest}onds' notion means thai, (/; 
maps individual nodes of a {:-structure tree to m\]il;s 
of the f-structure. The arrangement of tile four com- 
i)onents of an LFG rel)resentation is il lustrated in 
the diagram of Figure 1. This representation be- 
hmgs to the Aa  relation for a grammar that includes 
the almotated (nonterminal) rules in (2) and lexical 
rules in (3). 
(2) a. S -+ NP VP 
(1" suB,\]) =$ ?=$ 
(4. (:AS~,:) ---- NOM (? 'PI,:NS,,:) 
b. NP -+ I)ET N 
t-=4 I-=4 
{:. VP --~ V 
?=4 
(3) a. DET -+ a 
(1" sPEc) = ,N,},,:v 
(1" NUM) = sG 
t). N --+ st;u{lent; 
(? Pm,;D) = 'STUm,:Nq" 
(1" Nt:M) = s(~ 
(I" sl,l.:c) 
(:. V -~ M1 
(J" PlIFI))~-'I"ALL((SUB.\]))" 
(1" 'H.:NSt.:) = PAS'I' 
The (:-stru(:ture, in Figure 1 is derived by applying 
a sequence of rules from (2) to rewt'ite the symbol 
S, the grmnmar's tart symbol, and then rewriting 
the preterminal categories according to the lexical 
rules. I~exical rules are just notational variants of 
traditional LFG lexical entries. 
The () correspondence and the f-structure in Fig- 
ure 1 are ass{}ciated with thai; c-structure 1)e{:ause 
the f-slru{:ture satisfies the (/Mnstautiated escrip- 
tion cousl;rucl;ed fl'oIn I;11o a,motated c-structure 
derivatiolq and fllrthermore, it is a minimal model 
for the set of instantiated escriptions collected from 
all the nodes of the ani\]otated c-structure. The ()- 
instmd;iated escril}tio,l for a local mother-daughters 
configuration justified by a rule is created in the fol- 
lowing way. First, all o(:currences of the symbol J" in 
the functional mmotations of the daughters are re- 
placed t)y a variable standing fl)r the f-structure unit 
that r/) assigns t{} the moth{n" node,. Then for each of 
the daughter categories, all occurrences of the sym- 
1}ol $ in its annotations are replaced 1}y a variat)le 
425 
DET j N -~ V - - -1  
a s tudent  fel l  
~t~ \[PRED ISTUDENTr 
NUM SG 
SUB.I /SF'EC IN1)I'3F 
LCASE NOM 
em~D 'rALI,< (SUl~.0 >' 
TENSE PAST 
Figure 1: Piecewise c- and f-structure correspondence. 
standing for the ? assignment of the daughter node. 
Observe that all variables denote f-structure units in 
the range of 4), and that the $ on a category and the ? 
on the daughters that fllrther expand that category 
are always instantiated with the same variable. 
We now turn to the generation problem. A gener- 
ator for G provides for any given fstructure F the 
set; of strings that are related to it; by the grmn- 
111 ar: 
(4) Gcna(F) = {s \ [~c,? s.t. (s,c,?,F) E At,,}. 
Our main result is that for a certain subclass of LFG 
grmnmars the set: Gcna(F) is a context-free lan- 
guage. In the next section we prove that this is the 
case by constructing a context-free grammar that ac- 
cepts exactly this set of strings. Our proof dei)ends 
on the fact that the int)ut F - -and  hence the range 
of q5-is fully specified; Dymetman (1991), van No- 
ord (1993), and Wedekind (1.999) have shown that 
the general probleln of generating froln an under- 
specified input is unsolvable. We return to this issue 
at the end of the I)aper and observe that tbr cer- 
tain linfited tbrms of underspecification the context- 
fl'ee result can still be established. Our proof also 
det)ends on the fact that, with minor except;ions, 
the instantiated escriptions are ideml)otent: if p 
is a particular instantiated proposition, then a de- 
scription containing two occurrences of 1) is logically 
equivalent to one containing just a single occurrence. 
This means that descriptions can be collected by 
the union oi)erator for ordinary sets rather than by 
multi-set ration. 
The standard LFG tbrmalism includes a number 
of notational conveniences that make it easy to ex- 
press linguistic generalizations but which would add 
comI)lexity to our mathematical nalysis. We make 
a number of siml)lifying transformations, without 
loss of generality. The LFG c-structure notation 
allows the right-hand sides of rules to denote arbi- 
trary regular languages, expressed by Boolean com- 
binations of regular predicates (Kaplan 1995, Ka- 
plan and Maxwell 1996). We assume that these 
languages are normalized to standard regular ex- 
pressions involving only concatenation, disjunction, 
and Kleene-star, and then transform the grmnmar so 
that the right sides of the productions denote only 
finite sequences of aImotated categories. First, the 
effects of any Kleene-stars are removed in the usual 
way by the introduction of additional nonterminal 
categories and the rules necessary to expand them 
at)propriately. Second, every category X with dis- 
junctive annotations i  replaced by a disjunction of 
X's each associated with one of the alternatives of 
the original disjunction. Finally, rules with disjunc- 
tive right sides are replaced by sets of rules each 
of which expands to one of the alternative right- 
side category sequences. The result of these trans- 
formations is a set of productions all of which are 
in conventional context-free format and have no in- 
ternal disjunctions and which together define the 
stone st r ing/ f  structure nmpping as a grammar en- 
coded in the original, linguistically more expressive, 
notation. The Kleene-star conversions produce c- 
structures from which the original ones can be sys- 
tematically recovered. 
The full LFG fommlism allows for grammars that 
assign cyclic and otherwise linguistically unmoti- 
vated structures to sentences. The context-free re- 
sult can be established for these granmmrs, but the 
argument would require a longer and more techni- 
cal presentatiou than we can provide in this pal)er. 
Thus, without loss of linguistic relevance, we concen- 
trate here on a restricted class of LFG grammars, 
those that assign acyclic f-structures to sentences. 
For our tmrposes, then, ml LFG grammar G is a 
4-tuple (N, T, S, R} where N is the set of nontermi- 
nal categories, T is the set of terminal symbols (the 
lexical items), S E N is the root category, and 1~, is 
the set; of annotated productions. The context-fl'ee 
skeletons of the rules are of the form X0 -+ X1 ..Xn 
or X -+a,  with X1..Xn EN*  and aET .  If thean-  
notations of a nonterminal daughter establish a rela- 
tionship between $ and T, then $ is either identified 
with j', the value of an attri lmte in $ ((~ or) =$), or 
the member of a set in 1" ($E (T a)), where a is a 
possibly empty sequence of attributes. 
2 A Context- f ree Grammar  
for Gena(F) 
An inl)ut structure F for generation is t)resented as 
a hierarchical attribute-value matrix such as the oue 
in Figul"e 1, repeated here in (5). 
426 
\[ l)l{ 1~',1) t STUI)I,\]NTt- NUM SG SUIL\] SI)EC INI)EI" 
\[CASI.: NOM 
PR.,.:,) ' VA,,I,<(SU,/.0 >' 
TENSE PAST 
An fs t ructure  is an attr i lmte-valut sl;ructure where 
the values a.re either subsidiary atl;rilxlte-vahm rim- 
trices, symliols, semantic forms, or sei;s of subsidiary 
structures (not shown in this example). 
(6) A structure 9 is contained in a structm'e J' if 
and only if: 
.q= f,  
f is a set and g is eonl;aintd in an  dement  of 
f ,  or 
f is an f-structm'e and 9 is contained in (fa) 
for some attr ibute a. 
in tssence, 9 is conl;ained in f if 9 can 11o located 
ill f by ignoring sonm enclosing SUl)erstructure. For 
any f-structure f ,  the sel; of all units contained in f 
is then defined as in (7). 
(7) Units(f) - {g lo  is contained in f}  
Note t;hat Units(f) is a tinit;e set for any f ,  and 
U'nits(f) is the range of any ? that A(; associai;es 
with a parl;icular intmC F. 
The. (:-strucl;m'es and (/) corresliondences tbr F are 
the unknowns i;hai; nmsI; be discovered in the process 
of generation so l;hat the 1)rol)er instantiatcd escrip- 
Lions can \])e constructed and cvahtal;e(l, llowever, 
since thtre ix only a tinite mlml)er of l)ossible terms 
thai: can be used i;o designate the ltnil;s of t ?, we can  
produce a (Iinite) SUl)Cxsct of the, 1)r(/t)er instantiaW, d 
descriptions without knowing in advance the details 
of either the (;-sl;rucl;ure or ;4 1)articular (/). 
Let l;' be an f-structure tlmt has m (m > 0) set 
elements. We introduce m + 1 distinct variables 
v0,..,v,~, which denote biuniquely the root refit of 
F (v0) and each net element of F (vi, i > 0). 1 We 
consider the set of all designators of the tbrm (vi c,) 
which art  defined in F, where a is a (possibly empty) 
sequence of attributes. The set of designators for a 
particular unit corresponds, of course, to the set of 
all possible fs t rueture  paths fl'om one of the vi roots 
to that unit. Thus, the set of designa.t;ors for all units 
of F in finitt, since the number of units of F is tinite 
and there art  no cycles in F.  
The set of variables that we will use to construct 
the instantiated escriptions is the set 1/- consisl;ing 
of all vt where t in a designator of the set just de- 
fined. If l is the maximal arity of the rules in G, 
we will conskltr for the instantiation the set Z con- 
sisting of all sequences <vto, vt,,.., vt; ) of variables of 
V of length 1., . . ,n + 1, not containing any set tit;- 
1 Mul t i - rooted sl, ructures would require ~ whole set of ree l  
wu'iables, similm" I,o set elements.  
merit w~rial)le v,,~ (i = O, .., m) more dlan once. On 
the basis of this (finite) set of sequences, we define 
a (partial) fmwtion 1D which assigns to eat:h rule 
7' E 1{ and each sequence I E 27 that is apl)ropriate 
for r an instantiated escription. 
Let r be an n-ary LFG rule 
X0 -~ Xj ..X~ 
,5% S,, 
with annotated flmctional schemata S I . . .S , z .  A se-  
quence  of variables I G 27 is appr'opr'iatc for r if 
I = @t0,vt, , . - ,vl .)  is of length n + 1 and 
('Oi O-t(7) i f  ~,(1 ~--- (V i (Y') a l l ( l  (j" (7) = ,Le  S j  
tj = a set element varial)lt vj if SE (j" o-) E Sj 
for all j = 1, .., n ((7' and ? are (possibly t lnpty) se- 
quences of attributes). (Note that (? (7) =$ reduces 
to J '=$ if a is empty.) If I is al)prot)riatt for r, then 
ID(r, I), the instantiated escription for r and l, is 
defined as follows: 
N 
(s) m(,., n = U l ,<sj, V,o, %), 
j= l  
where l:nsl.(,gj, vto, vtj) in the instantiated escrip- 
tion produced by substituting vt0 for all occurrences 
of 1" in ,5'j and substituting vtq for all occurences of 
$ in Sj. 
If r is a lexical rule with a context-free skeleton of 
the fl)rm X ~ a every sequence I = (v,0> of length \] 
is ~@propriate for r mMID is detined by: 
(9) m(,., \]) - I',,..~.(S,, ',,,,,). 
The instantiation using a.pprotn'iate sequences of 
variables, all;hough tinite, permits an elfectivt dis- 
crinfinal;ion of l;he fst, ructure variables, since it pro- 
rides diflbXeld; varial)les for the $% associated with 
diti'erent daughters i;hat have different flmction as- 
sigmnents (i.e., mmotations of the form (1" c,) =$ 
and (t (7') =$ with (7 ? J ) ,  but identifies variables 
where fstructure variables are identified explicitly 
(j'=$) or where the identity tbllows by ratification, 
as in cases where the annotations of two diflbrent 
(laughters contain the same function-assigning equa- 
l;loll (J" (7) =$. Hence, we in fact have enough vari- 
ables to make all the distinctions that could arise 
from any c-si;rueturt and ? correspondence for the 
given f-structurt. 
The set of all possible instantiated escriptions is 
large but finite, since R. and Y are finite. Thus, the 
set IP(F) of all possible instantiated propositions 
for G and F is also large but finite. 
(10) re (F )  = U Ra'~w(*rD)  
For the construction of the eonttxt-fi'ee grmmnar we 
have to consider those subsets of IP(F) which have 
F as their minimal model. This is the set D(F) ,  
again finite. 
427 
(11) D(F) is tim set of all D C_ IP(F) such that 
F is a minimal model for D. 
We are now prepared to establish the main result of 
tiffs paper: 
(12) Lct G be an LFG grammar conforming to thc 
restrictions we have described. Then for" any 
f-structure F, the set GenG(F) is a context- 
free languaf\]e. 
Pro@ If F is incomt)lete or incoherent, tlseu 
Genc(F) is the empty context-free language. Let 
G = (N, T, S, R) be an LFG grammar. If D(F) is 
empty, then Gena(F) is again the empty context- 
free language. If D(F) is not empty, we construct a 
context-free grammar Gr  = (ARE, Tr ,  SF, RE} its the 
following way. 
The collection of nonterlninals ~rj,, is the (finite) 
set {SF} U N x V x I)ow(IP(F)), wtsere SF is a new 
root; category. Categories in NI; other than SF are 
written X:v:D, where X is a category in N, v is con- 
tained in 17, and D is an instantiated escription in 
Pow(IP(F)). 2),, is the set T x {(/)} x {0}. The rules 
RF are constructed from the annotated rules R of 
G. We include all rules of the form: 
(i) S,,, ~ S:v~o:D, for every D d D(F) 
(ii) X0:vto:D0--+ Xl:Vtl:Dl..Xn:vt:Dn s.t. 
(a) there is an r E R expanding X0 to X1..X,~, 
(b) Do = m(,. ,  . . ,v ,o) )u UD, ,  
i=1 
(c) if vv~ 6 (vtj c~) belongs to Dj then 
v,,, ? vt,, (k = 1, .., v,) and 
(1,, ? j) s.t. v,,, c (v,,,, o-') c 
(iii) X:vl:D -~ a:(/):~ s.t. 
(a) there is an r E R expanding X to a, 
(b) D = ZD(r, (vt)). 
We define the projection Cat(a::?\]:z)= a: for ev-  
ery  category in NF U Tl,, and extend this function in 
the natural way to strings of categories and sets of 
strings of categories. Note that the set 
Cat(L(G~)) = {s I Bw E L(GF) s.t. Cat(w) = s} 
is context-free, since the set of context-free languages 
is closed under homolnorphisms such as Cat. We 
show that the language Cat(L(GF)) = Gena(F). 
We prove first that Gcna(F) C Cat(L(aA). Let 
c be an annotated c-structure of a string s with f- 
structure F in G. On the basis of c and F we con- 
struct a derivation tree of a string s' in G j,, with 
Cat(s') = s in two steps. In the first step we rela- 
bel each terminal node with label a by a:(~, the rook 
by S:vv0, each node introducing a set element with 
label X biuniquely by X:v~, and each other node 
~This condition captures LFG's special interpretation of
membership statements. The proper treatment of LFG's se- 
mantic forms requires a similar condition. 
labelled X by X:vt where * is a designator that is 
constructal)le from the function-assigning equations 
of the mmotations along the path from the unique 
root or set element o that node. On the basis of 
this relabelled c-structure we construct a derivation 
tree of s' in Gt,' bottom-up. We relabel each ter- 
urinal node with label a:(/) by a:(/):~) and each preter- 
minal node with label X:vt by X:vt:D where D is 
defined as in (iiib) with r expanding X in c to a. Sup- 
pose we have constructed the subtrees dominated by 
X1 :Vtl:D1..X,z:vt.:D,, the corresponding subtrees in 
c are derived with r expanding X0 to X1..X m and 
the nlother node is relabelled by X0:vt0. We then 
relabel this mother node by Xo:vto:Do where Do is 
determined according to (iib). By induction on the 
depth of the subtrees it is then easy to verify that 
the instantiated escription D of a subtree donfi- 
nated by X:vt:D is equivalent to the f-description of 
the corresponding annotated subtree its c. Thus, F 
must be a minimal model of the instantiated escrip- 
tion of the root label S:v~0:D~, , Sl,. derives S:v~o:DF 
in GI, ~ and Cat(J) = s. 
We now show that Cat(L(GI~)) C Geno(F). Let 
c" be a derivation tree of s' in Gr  with Uat(s') = s 
and supl)ose that the root (with label SF) expands 
to S:vv0:DF. We construct a new derivation tree c' 
that results from c" by eliminating the root. We 
then define a fimction ?' such that for each nonter- 
minal node /t of c': ?'(IL) = vt if # is labelled by 
X:vt:D in c'. According to our rule construction it
can easily be seen by induction on the depth of the 
subtrees that  the, re nmst be an annotated c-structure 
c of G with the same underlying tree structure as c' 
such that for each node tt labelled by z:~/:D in c': 
(i) t* is labelled by a: in c, 
(ii) D is identical with the description that results 
from Dr, , the f-description of the sub-c-structure 
dominated by tt in c, by replacing each occurrence 
of an f-structure variable 'qS0/)' (usually abbreviated 
by f , )  in D,~ by 4/(,,). Since (/'(It) = qS(,,) follows for 
two f-structure designators if (b'(#) = 4/(u), tim f 
description of the whole c-structure must be equiva- 
lent to DE mid thus Ac,,(s, c, ?, F) where ~ = ~b' o Ov 
and Cv is the unique flmction ttmt maps each ut to 
the unit of F that is denoted by t. QEI) 
3 An  Example  
As a simple illustration, we produce the context- 
fl'ee gramnmr GF for the input (5) and the grmnmar 
in (2,3) above. The only designator variables that 
will yield useful rules are v~ 0 mid v(~ o sui33), in tim 
tbllowing abbreviated by v aim Vs. Consider first the 
context-fl'ee rules that correspond to the rules that 
generate NP's. If we choose the sequence I = (vs), 
the instantiated escription for the determiner rule 
in (33)is (13). 
(13) {(v, spp~c) = IN1)EF, (Vs NUM) = so} 
428 
Rule (14) is tlms a production of GI,'. 
;('Os S')EC) : INI)I'H'~ 
(14) DET:,,.:/. i". NUM)=S ; j -+ 
Rule (15) is obtained from the N rule using the same 
seqnence. 
(15) N:'vs:{t?'s I'ILE,)) = 'S'FUI)ENT') 
(v~ NUM) = s(~ ~ ~ student:~:0 
(l's SPEC) ) 
For the NP rule and the sequence {vs,vs,vs}, both 
daughter annotatiolls instantiate to the trivial de- 
scription vs = vs, and this can combine, with many 
daughter descriptions. Two of these are the basis for 
the rules (16) and (17). The (laughter categories of 
rule (1.6) match the mother categories of rules (14) 
a,nd (15), all(1 the tlll"ee rllles together can derive the 
stting a:(~:0 student:(/}:{~. Rule (17), Oll the other 
hand, is a legi(;iinate rule but does not combine with 
any others to l)roduce a terminal string. 1\]; is a use- 
less, albeit harmless, production; if desired, it tan 
be removed froln the set of productions 1) 3, standard 
algorithnts tbr COll(;exl;-\['l.ee gramnmrs. 
llf we contimm along in this rammer, we find that 
the rules in (18,1.9,20) are the only other useful rules 
that belong to G1,'. 
The grammar GI~' also includes (;he following sl;arl;- 
ing rule: 
0, s . . . ) ) -  ,,,, "1 
= NOM / 
1) ~- '/) | 
(.,, ',',.:Ns,.:) \[ 
s,,,.:(,) l 
('V I'll.H))= 'FAIA,((SUB,I)}" / 
(',, r,:Ns\].:) = l,as'r ) 
This grammar provides one derivation for a sin- 
gle string, a:(/):(/) student:(/):(/) Dll:(/):{/}. Applying Cat 
to this string gives 'a stlldent Dll', tim only sen- 
tence that this grammar associates with the inlmt 
f'd;ructure. 
4 Consequences  and  Observat ions  
Our main result oflb.rs a new way to con(:et)tualize 
the problenl of generation lbr HPG and other lfigher- 
order context-free-based grainmatical tbr, nalisms. 
The proof of the theorem is constructive: it indicates 
precisely how to lmild 1;111.' grmnmar GI; whose lan- 
guage is the desired set; of strings. Thus, the 1)rol~lem 
of LFG generation is divided into two phases, con- 
structing the context-Dee grammar G/,,, an(t then 
using a standard context-free generation algorithm 
to produce strings fl'om it. 
\Ve can regard the first t)hase of LFG generation 
as specializing the original LFG gl'allilllal to Oi11~ that 
only produces the given input fstructure. This spe- 
cialization refines the context-fiee backbone of 1;11(; 
original grannnar, but our theoreln indica.tes that 
the inl)ut t'-si;ru(:ture l)rovides enough infornmtion so 
tlmt, in effect, tlm metaw~riables in the functional 
annotat ions can all be replaced by variables con- 
tained in a tixed tinite set. Thus, in the LFG gen- 
eration case the st)e(:ialized grammar turns out to 
be in a less l)owerful tbrmal class than the original. 
\Ve (:an mlderstand ifferent aspects of generation 
as I)ertaining either to the way the grammar is con- 
strutted or to well-known properties of (;Oll(;exl;-free 
grammars and (~olltoxl;-\]'l'ee g neration. 
It follows as an immediate corollary, tbr exam- 
pie, that it is (lecidalfle whether the set GcnG,(F) is 
emt)ty , contains a tinite mmfl)er of strings, or con- 
tains all infinite number of strings. This C}lll lie de- 
ternfined by inspecting GF with standard context- 
free tools, once it has l)een constructed. If the lan- 
guage is infinite, we (:an make use of tim context-Dee 
pumping lemma to identify a tlnite number of short 
strings Dora which all other strings ('an be produced 
1)y rel)el,ition of sul)(lcrivations. Wedekin(1 (19{)5) 
tirs( estal)lished the de(:idability of I,FG generation 
and t)roved a lmmping lemma ti)1 the generated 
string set; our tlwx)r(nn l)rovides alternative ;ul(l very 
direct 1)root's of the.st previously known results. 
\?e also \]lave gtll exl)lanation for another ob- 
servation of Wedekind (1995). Kaplan and Bre.s- 
nan (1982) showed that the Nonbranclfing I)omi- 
nance Condition (sometinms called ()flline Parsabil- 
ity) is a sufficient (:on(liti(m to guarantee (le(:idal)il- 
ity of lhe meml)ership l)rol)lenL Wedekind noted, 
how(~ver, (;bat (;hi~ condition is not nex:essary to de- 
lermine \v\]mlht~r a given tkstrlletlll'e corresponds 1;o 
any strings. We now see more clearly why this is the 
case: if there is a (:olltext-Dee derivation for a given 
string that involves a nonl)ranching dominance cy- 
(:le, we know (fronl the pumi)ing hmnna) that there 
is another derivation for tlmt saint string that has 
no such cycle. Thus, the generated language is the 
same whether or not derivations with nonbranching 
dominance (:y(:h;s are allowed. 
There is a practical consequence to the two phases 
of LFG generation. Tim gralllllHtl' GI,' eaIt t)e pro- 
vided to a client as a finite representation f the set 
of 1)crhal)s infinitely many strings that corresl)ond 
to the given fstrueture, and the client can then ('o11- 
trol the process of enumerating individual strings. 
The client ntay choose simply to produce the short- 
est ones jl lst 1) 3, avoiding recursive category expan- 
sions. O1 the client may apply the technology of 
stochastic ontext-free grammars to choose the most 
probable, senI;ence, f1'o111 the set of possibilities. The 
client may also be ilW;erested in strings that meet 
further conditions that the shortest or most proba- 
ble strings fail to satist~y; in this case the client may 
429 
Us ~ Us ~ 
/ (v,, SI'EC) = INI)EF } f(vs SPEC) = INDEF'~ 
(16) NP:,,~:~ (,,~ ~M)= s~; - .  D\]n':,,~: ~ (v~ NUM) ( SO J I(v~ PR,,~,,)= 's~u,.,:~'r'  
t, (,,~ sPl~C) , 
(17) NP:v~:{v~ = v~, (v~ NUIVl) = SG} -+ DET:v~:{(v~ NUM) = SO} N:v~:(a 
(18) W:v: {(V F'IIED) = 'FALL((SUP, J))"'~ 
(v Tt,:NSI,:) = PAST J --> fell:{,'}:0 
(191 
(20) 
V----I) 
vP:~:  (~ Hum)  = 'I,al,r,((suB.O)'? 
(,, T~s~)  = \[}AS',' J 
i" (,, s,,,,.,) = ,,,~ 
/ (,,~ casl.:) = NOM 
| V ~ V 
/ (,~ TI,:NSI~) 
l)s ~ Us 
S:v:~ ('Os SPEC) = INI)EF / (Vs NUM) = SG 
/ ("~ Pro,:.)= 's'PuI.~NT' 
/ ' (Vs SPEC) 
I(v PRED) = q~aLL((SUBa))' 
\[, (1) TENSE) = PAST 
---} M:'U'~(V I}REI)) = tFALL((SUBJ))t~ 
? \[ (~ TI~NSE) = PAST J 
= l( 'Os ~ 'Us 
(Us SPEC) = INDEF 
-+ NP:v~: (v~ NUM) = SC 
Us PLIED) --~ ISTUDENTI 
(Us SI'EC) 
N:vs: 
VP:v: 
{ ('Os I ' I IH ) )= 'STUDI,:NT'~ 
(~ TE~S~) = PAS~r J 
apply the pumt)ing lemma to systematically produce 
longer strings for exmnination. 
Our recipe tbr constructing GF may produce 
many categories and expansion rules that ca.ili, ot 
play a role in any derivation, either because they 
are inaccessible from the root symbol, they do not 
lead to a terminal string, or because they involve in- 
dividual descriptions that F does not sat, is\[y. Hav- 
ing constructed the grammar, we ea.n again api)ly 
standard context-free methods, this time to trot the 
grammar in a more ot)timal forln by reinoving use- 
less categories and productions. We can view sev- 
eral difl!erent generation algorithms as strategies tbr 
avoiding the creation of useless categories in the first 
place. 
The most obvious optimization, of course, is to in- 
cretnentally evaluate all the instantiated escriptions 
and remove froin consideration categories and rules 
involving descriptions for which F is not a model. 
A second strategy is to construct he grammar in 
bottom-up fashion. We begin by comparing the ter- 
minal rules of the LFG grannnar with the features 
of the input f-structure, and construct only the cor- 
responding categories and rules that meet the crite- 
ria in (iii) above. We then construct rules that can 
derive the mother categories of those rules, and so 
oil. With this strategy we insure that every cate- 
gory we construct can derive a terminal string, but 
we have no guarantee that every bottom-up sequence 
will reach the root symbol. 
It is also at)pealing to construct he grmnmar by 
means of a top-down process. If we start with an 
agenda containiug the root symbol, create rules only 
to expand categories on the agenda, and place cate- 
gories on the agenda whenever they appear for the 
first time oi1 the right side of a new rule, we get the 
effect of a top-dowu exploration of the gratnmar. We 
will only create categories and rules that are acces- 
sible fronl the root symbol, but we may still 1)roduce 
categories that derive no terminal string. 
The toi)-down strategy may not provide ett'ective 
gui(tance, however, if the set D(F)  contains many 
alternative descriptions of F. But suppose we can 
associate with every instantiated escription D a 
unique canonical description that has the stone f- 
structure as its minimal model, and suppose that we 
then reformulate tlm grammar construction in terms 
of such canonical descriptions. This can shari)ly re- 
duce the size of the grammar we produce according 
to any enumeration strategy, since it avoids rules 
and categories that express only uuinforlnative vari- 
ation. It can particularly benefit a top-down era> 
meration because the set D(F) will have at most 
one canonical member. Presumably any practical 
generation scheme will define and operate on canon- 
ical descriptions of some sort, but our context-Dee 
result does not depend on whether or how such de- 
scriptions inight be specified and maifipulated. 
Just as for context-free parsing, there are a num- 
ber of mixed strategies that take top-down and 
bottom-up inibrmation into account at the stone 
time. We can use a precomputed reachability ta- 
ble to guide the process of top-down exploration, 
for iilstance. Or we can simulate a left-corner enu- 
meration of tile sem'ch space, considering categories 
that are reachable froin a current goal category and 
430 
nmtch the left; corner of a possible rule. In general, 
ahnost any of the traditional algorithms tbr process- 
\[llg (;()iltext-frec gt'atil l l lars call be reforl l l l l latctl  as 
a strategy tbr tn,oiding the creation of useless cat- 
egories and rules. Other enmneration strategies fo- 
cus on the characteristics of the input f-structure. A 
head-driven strategy (e.g. van Noord 1993) identi- 
ties the lexical heads first, finds the rules that ex- 
l)and them, and then uses information associated 
with those heads, such as their grmmnatical flmetion 
assigmnents, to pick other categories to exlmnd. 
Our proof depends on the assmnl~tion that the in- 
put \],' is flllly specified so that the set of i)ossible 
instantiations ix finite, l )ymetman (1991), van No- 
ord (1993), and Wedekind (1999) have shown that 
it ix ill generM undecidable whether or not there are 
any strings associated with an f-structure that has 
units ill addition to those in the input. Indeed, our 
proof of context-freeness does not go through if we 
allow new units to be hypothesized arbitrarily, l/e- 
yond the ones that appear in F; if this ix permitted, 
we cannot establish a finite. 1)ound on the munbcr of 
l/ossil)le categories. This is unfortmmte, since there 
may be interesting practical situations ill which it is 
convenient o leave UnSlmCified tile value of a liar- 
titular feature. However, if there can be, only a ii- 
nil, e nlunb(',i' of possible wflues for an underspecitied 
feature, the (:ontext-free resull: can still be esi;al)- 
lished. We create from F a set of alternative struc- 
tures F~..F, by filling ill all possible values of the 
UllSl)eeified features, a.ml we l)roduce the context- 
Dee grammar corresponding to o, ach of thcln. Since 
a finite ration of eontext-flee languages is context- 
Dee, the set of strings generated fl'om any of t, hese 
structures renmins ill that class. 
A tinal COilllllellt a\])ollt ;he generation l/rolflem for 
other high-order granmmtical t'ornmlisnis. ()llr proof 
dcl)ends on se, veral tb, aturcs of LFG: the (:Oll\[:exl;-ti'(?e 
1)ase, the pieeewise correspondence of 1)hrase struc- 
ture, and f-structure units, and the ideml)Otency of 
the flumtional description language. PATR shares 
these properties, although the correspondence is iln- 
plicit in the mechanisnl and not reified as a linglfisti- 
cally significant concept. So, our proof can be used 
to establish the context-free result for PATR. On 
the other hand, it is not clear whether the string 
set corresponding to an underlying I{PSG structure 
is context-flee. HPSG (Pollard and Sag 1994) does 
trot Iltake direct use of a context-free skeleton, and 
olmrations other than concatenation may be used 
to assenfl)le a collection of substrings into an entire 
Selltetlce. \~e canllot extend ore" proof to ttPSG m> 
less the etli~ct of these mechanisms can be reduced 
to an equivalent characterization with a context-free 
base. However, grammars written for the ALE sys- 
tem's logic of typed feature structures (Carl)enter 
and Penn 1.994) do have a context-free COlll\])Ollelll; 
and therefore' are, ainell~fl)\]e to the, treatnlent we have 
outlined. 
Acknowledgments  
We arc imlcbted to John Maxwell, t ladar Slmmtov, 
Martin Kay, and Paula Newman for many fl'uit- 
fill and insightflfl discussions of the LFG genera- 
tion 1)roblem, and for criticisms and suggestions that 
have, helped to clarit~y many of tile mathenlatieal nd 
conllmtatiolml issues. 
References 
Carpenter, B. aim G. Petal. 1994. ALE 2.0 User's 
Guide. Technical report, Carnegie Mellon Univer- 
sity, l~ittslmrgh, PA. 
Dymetman, M. 1991. Inherently Reversible Gram- 
mars, Logic Programming and Computability. In 
P~vcecdings of th, c ACL Workshop: Reversible 
Gram, mar i'n Natural Language PTwccssing. Berke- 
ley, CA, pages 20 -30. 
Kat)lan, I{. M. 1995. The Fornml Architecture of 
Lexical-IPunctio\]ml Grannnar. In M. Dahyml)le , 
11. M. Kaplan, .1. Maxwell, and A. Zaen('al, edi- 
tors, \]?orbital \]s.s"ltcs i'n Lczical-l,;uu, cl, ional Gram- 
mar. CSLI l?ublications, Stanford, CA, pages 7- 
27. 
Kapbm, \]7/. M. and J. Bresnan. 1982. Lexical- 
Functional Grammar: A li'orntal System for 
(~ranunatical lq.epreseill;ation. In J. Bresnan, (!(l- 
iter, The Mental l~,cprc,s'c'ntat{o~t of G~'(t'llt~rtitti- 
cal Rclatio'ns. MIT Press, Carol)ridge, MA, 1)ages 
173 281. 
Kaplan, l/. M. mM J. Maxwell. 1996. LIeG 
Crwm,'mar Writer's Workbe,nch,. Technical re- 
port, Xerox Pale Alto Research Center. At 
http: / /ftp.par,:.xero:~.co~n/p,,1, /lfg/ltk,tlai,,~l.ps. 
Pollard, C. mM 1. Sag. 1994. \]toad-Driven IW, rasc 
Str'uct'urc, Gra'm'mar. The University of Chicago 
Press, Chicago, IL. 
Shieber, S., It. Uszkoreit, F. Pereira, .1. llol)inson, 
and M. Tyson. 1983. The Formalistn and hnl)le- 
mentation of PATR-II. In B. Grosz and M. Stickel, 
editors, I~cscarch on Interactive Acquisition and 
Use of K'nowlcdgc. SRI Final Report 1894. SRI 
hlternational, Menlo Park, CA, pages 39--79. 
wm Noord, G. 1993. ll,eversibility in Natural Lan- 
guage Processing. Ph.D. thesis, Rijksuniversiteit 
Utrecht. 
Wedekind, J. 1995. Sonm Remarks on the l)ecidalfil- 
ity of the Generation Problem ill LFG- mM PATI{- 
style Unification Grmmnars. lit Procccdings of th, c 
71,h Co'nferc,nc(: of tit(, E'uropcan Chapter of th, c As- 
sociation for Comp~ttational Linq'uistics. l)ublin, 
pages 45- 52. 
Wedekind, J. 1999. Semantic-drivell Generation 
with LFG- and PATR-style Grammars. Cornp,u- 
rational Ling,uistics , 25(2): 277 281. 
431 
Speed and Accuracy in Shallow and Deep Stochastic Parsing
Ronald M. Kaplan , Stefan Riezler , Tracy Holloway King
John T. Maxwell III, Alexander Vasserman and Richard Crouch
Palo Alto Research Center, 3333 Coyote Hill Rd., Palo Alto, CA 94304
{kaplan|riezler|king|maxwell|avasserm|crouch}@parc.com
Abstract
This paper reports some experiments that com-
pare the accuracy and performance of two
stochastic parsing systems. The currently pop-
ular Collins parser is a shallow parser whose
output contains more detailed semantically-
relevant information than other such parsers.
The XLE parser is a deep-parsing system that
couples a Lexical Functional Grammar to a log-
linear disambiguation component and provides
much richer representations theory. We mea-
sured the accuracy of both systems against a
gold standard of the PARC 700 dependency
bank, and also measured their processing times.
We found the deep-parsing system to be more
accurate than the Collins parser with only a
slight reduction in parsing speed.1
1 Introduction
In applications that are sensitive to the meanings ex-
pressed by natural language sentences, it has become
common in recent years simply to incorporate publicly
available statistical parsers. A state-of-the-art statistical
parsing system that enjoys great popularity in research
systems is the parser described in Collins (1999) (hence-
forth ?the Collins parser?). This system not only is fre-
quently used for off-line data preprocessing, but also
is included as a black-box component for applications
such as document summarization (Daume and Marcu,
2002), information extraction (Miller et al, 2000), ma-
chine translation (Yamada and Knight, 2001), and ques-
tion answering (Harabagiu et al, 2001). This is be-
1This research has been funded in part by contract #
MDA904-03-C-0404 awarded from the Advanced Research and
Development Activity, Novel Intelligence from Massive Data
program. We would like to thank Chris Culy whose original ex-
periments inspired this research.
cause the Collins parser shares the property of robustness
with other statistical parsers, but more than other such
parsers, the categories of its parse-trees make grammati-
cal distinctions that presumably are useful for meaning-
sensitive applications. For example, the categories of
the Model 3 Collins parser distinguish between heads,
arguments, and adjuncts and they mark some long-
distance dependency paths; these distinctions can guide
application-specific postprocessors in extracting impor-
tant semantic relations.
In contrast, state-of-the-art parsing systems based on
deep grammars mark explicitly and in much more de-
tail a wider variety of syntactic and semantic dependen-
cies and should therefore provide even better support for
meaning-sensitive applications. But common wisdom has
it that parsing systems based on deep linguistic grammars
are too difficult to produce, lack coverage and robustness,
and also have poor run-time performance. The Collins
parser is thought to be accurate and fast and thus to repre-
sent a reasonable trade-off between ?good-enough? out-
put, speed, and robustness.
This paper reports on some experiments that put this
conventional wisdom to an empirical test. We investi-
gated the accuracy of recovering semantically-relevant
grammatical dependencies from the tree-structures pro-
duced by the Collins parser, comparing these dependen-
cies to gold-standard dependencies which are available
for a subset of 700 sentences randomly drawn from sec-
tion 23 of the Wall Street Journal (see King et al (2003)).
We compared the output of the XLE system, a
deep-grammar-based parsing system using the English
Lexical-Functional Grammar previously constructed as
part of the Pargram project (Butt et al, 2002), to the
same gold standard. This system incorporates sophisti-
cated ambiguity-management technology so that all pos-
sible syntactic analyses of a sentence are computed in
an efficient, packed representation (Maxwell and Ka-
plan, 1993). In accordance with LFG theory, the output
includes not only standard context-free phrase-structure
trees but also attribute-value matrices (LFG?s f(unctional)
structures) that explicitly encode predicate-argument re-
lations and other meaningful properties. XLE selects the
most probable analysis from the potentially large candi-
date set by means of a stochastic disambiguation com-
ponent based on a log-linear (a.k.a. maximum-entropy)
probability model (Riezler et al, 2002). The stochas-
tic component is also ?ambiguity-enabled? in the sense
that the computations for statistical estimation and selec-
tion of the most probable analyses are done efficiently
by dynamic programming, avoiding the need to unpack
the parse forests and enumerate individual analyses. The
underlying parsing system also has built-in robustness
mechanisms that allow it to parse strings that are outside
the scope of the grammar as a shortest sequence of well-
formed ?fragments?. Furthermore, performance parame-
ters that bound parsing and disambiguation work can be
tuned for efficient but accurate operation.
As part of our assessment, we also measured the pars-
ing speed of the two systems, taking into account all
stages of processing that each system requires to produce
its output. For example, since the Collins parser depends
on a prior part-of-speech tagger (Ratnaparkhi, 1996), we
included the time for POS tagging in our Collins mea-
surements. XLE incorporates a sophisticated finite-state
morphology and dictionary lookup component, and its
time is part of the measure of XLE performance.
Performance parameters of both the Collins parser and
the XLE system were adjusted on a heldout set consist-
ing of a random selection of 1/5 of the PARC 700 depen-
dency bank; experimental results were then based on the
other 560 sentences. For Model 3 of the Collins parser, a
beam size of 1000, and not the recommended beam size
of 10000, was found to optimize parsing speed at little
loss in accuracy. On the same heldout set, parameters of
the stochastic disambiguation system and parameters for
parsing performance were adjusted for a Core and a Com-
plete version of the XLE system, differing in the size of
the constraint-set of the underlying grammar.
For both XLE and the Collins parser we wrote con-
version programs to transform the normal (tree or f-
structure) output into the corresponding relations of
the dependency bank. This conversion was relatively
straightforward for LFG structures (King et al, 2003).
However, a certain amount of skill and intuition was
required to provide a fair conversion of the Collins
trees: we did not want to penalize configurations in the
Collins trees that encoded alternative but equally legit-
imate representations of the same linguistic properties
(e.g. whether auxiliaries are encoded as main verbs or
aspect features), but we also did not want to build into
the conversion program transformations that compensate
for information that Collins cannot provide without ap-
pealing to additional linguistic resources (such as identi-
fying the subjects of infinitival complements). We did not
include the time for dependency conversion in our mea-
sures of performance.
The experimental results show that stochastic parsing
with the Core LFG grammar achieves a better F-score
than the Collins parser at a roughly comparable parsing
speed. The XLE system achieves 12% reduction in error
rate over the Collins parser, that is 77.6% F-score for the
XLE system versus 74.6% for the Collins parser, at a cost
in parsing time of a factor of 1.49.
2 Stochastic Parsing with LFG
2.1 Parsing with Lexical-Functional Grammar
The grammar used for this experiment was developed in
the ParGram project (Butt et al, 2002). It uses LFG as a
formalism, producing c(onstituent)-structures (trees) and
f(unctional)-structures (attribute value matrices) as out-
put. The c-structures encode constituency and linear or-
der. F-structures encode predicate-argument relations and
other grammatical information, e.g., number, tense, state-
ment type. The XLE parser was used to produce packed
representations, specifying all possible grammar analyses
of the input.
In our system, tokenization and morphological analy-
sis are performed by finite-state transductions arranged in
a compositional cascade. Both the tokenizer and the mor-
phological analyzer can produce multiple outputs. For ex-
ample, the tokenizer will optionaly lowercase sentence
initial words, and the morphological analyzer will pro-
duce walk +Verb +Pres +3sg and walk +Noun +Pl for
the input form walks. The resulting tokenized and mor-
phologically analyzed strings are presented to the sym-
bolic LFG grammar.
The grammar can parse input that has XML de-
limited named entity markup: <company>Columbia
Savings</company> is a major holder of so-called junk
bonds. To allow the grammar to parse this markup,
the tokenizer includes an additional tokenization of the
strings whereby the material between the XML markup
is treated as a single token with a special morphologi-
cal tag (+NamedEntity). As a fall back, the tokenization
that the string would have received without that markup
is also produced. The named entities have a single mul-
tiword predicate. This helps in parsing both because it
means that no internal structure has to be built for the
predicate and because predicates that would otherwise be
unrecognized by the grammar can be parsed (e.g., Cie.
Financiere de Paribas). As described in section 5, it was
also important to use named entity markup in these ex-
periments to more fairly match the analyses in the PARC
700 dependency bank.
To increase robustness, the standard grammar is aug-
mented with a FRAGMENT grammar. This allows sen-
tences to be parsed as well-formed chunks specified by
the grammar, in particular as Ss, NPs, PPs, and VPs, with
unparsable tokens possibly interspersed. These chunks
have both c-structures and f-structures corresponding to
them. The grammar has a fewest-chunk method for de-
termining the correct parse.
The grammar incorporates a version of Optimality
Theory that allows certain (sub)rules in the grammar to be
prefered or disprefered based on OT marks triggered by
the (sub)rule (Frank et al, 1998). The Complete version
of the grammar uses all of the (sub)rules in a multi-pass
system that depends on the ranking of the OT marks in
the rules. For example, topicalization is disprefered, but
the topicalization rule will be triggered if no other parse
can be built. A one-line rewrite of the Complete grammar
creates a Core version of the grammar that moves the ma-
jority of the OT marks into the NOGOOD space. This ef-
fectively removes the (sub)rules that they mark from the
grammar. So, for example, in the Core grammar there is
no topicalization rule, and sentences with topics will re-
ceive a FRAGMENT parse. This single-pass Core grammar
is smaller than the Complete grammar and hence is faster.
The XLE parser also allows the user to adjust per-
formance parameters bounding the amount of work that
is done in parsing for efficient but accurate operation.
XLE?s ambiguity management technology takes advan-
tage of the fact that relatively few f-structure constraints
apply to constituents that are far apart in the c-structure,
so that sentences are typically parsed in polynomial time
even though LFG parsing is known to be an NP-complete
problem. But the worst-case exponential behavior does
begin to appear for some constructions in some sentences,
and the computational effort is limited by a SKIMMING
mode whose onset is controlled by a user-specified pa-
rameter. When skimming, XLE will stop processing the
subtree of a constituent whenever the amount of work ex-
ceeds that user-specified limit. The subtree is discarded,
and the parser will move on to another subtree. This guar-
antees that parsing will be finished within reasonable lim-
its of time and memory but at a cost of possibly lower
accuracy if it causes the best analysis of a constituent
to be discarded. As a separate parameter, XLE also lets
the user limit the length of medial constituents, i.e., con-
stituents that do not appear at the beginning or the end
of a sentence (ignoring punctuation). The rationale be-
hind this heuristic is to limit the weight of constituents in
the middle of the sentence but still to allow sentence-final
heavy constituents. This discards constituents in a some-
what more principled way as it tries to capture the psy-
cholinguistic tendency to avoid deep center-embedding.
When limiting the length of medial constituents, cubic-
time parsing is possible for sentences up to that length,
even with a deep, non-context-free grammar, and linear
parsing time is possible for sentences beyond that length.
The Complete grammar achieved 100% coverage of
section 23 as unseen unlabeled data: 79% as full parses,
21% FRAGMENT and/or SKIMMED parses.
2.2 Dynamic Programming for Estimation and
Stochastic Disambiguation
The stochastic disambiguation model we employ defines
an exponential (a.k.a. log-linear or maximum-entropy)
probability model over the parses of the LFG grammar.
The advantage of this family of probability distributions
is that it allows the user to encode arbitrary properties
of the parse trees as feature-functions of the probability
model, without the feature-functions needing to be inde-
pendent and non-overlapping. The general form of con-
ditional exponential models is as follows:
p?(x|y) = Z?(y)
?1e??f(x)
where Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant over the set X(y) of parses for sentence y, ? is
a vector of log-parameters, f is a vector of feature-
values, and ? ? f(x) is a vector dot product denoting the
(log-)weight of parse x.
Dynamic-programming algorithms that allow the ef-
ficient estimation and searching of log-linear mod-
els from a packed parse representation without enu-
merating an exponential number of parses have
been recently presented by Miyao and Tsujii (2002)
and Geman and Johnson (2002). These algorithms can
be readily applied to the packed and/or-forests of
Maxwell and Kaplan (1993), provided that each conjunc-
tive node is annotated with feature-values of the log-
linear model. In the notation of Miyao and Tsujii (2002),
such a feature forest ? is defined as a tuple ?C,D, r, ?, ??
where C is a set of conjunctive nodes, D is a set of dis-
junctive nodes, r ? C is the root node, ? : D ? 2C is
a conjunctive daughter function, and ? : C ? 2D is a
disjunctive daughter function.
A dynamic-programming solution to the problem of
finding most probable parses is to compute the weight
?d of each disjunctive node as the maximum weight of
its conjunctive daugher nodes, i.e.,
?d = max
c??(d)
?c (1)
and to recursively define the weight ?c of a conjunctive
node as the product of the weights of all its descendant
disjunctive nodes and of its own weight:
?c =
?
d??(c)
?d e
??f(c) (2)
Keeping a trace of the maximally weighted choices in a
computaton of the weight ?r of the root conjunctive node
r allows us to efficiently recover the most probable parse
of a sentence from the packed representation of its parses.
The same formulae can be employed for an effi-
cient calculation of probabilistic expectations of feature-
functions for the statistical estimation of the parameters
?. Replacing the maximization in equation 1 by a sum-
mation defines the inside weight of disjunctive node. Cor-
respondingly, equation 2 denotes the inside weight of a
conjunctive node. The outside weight ?c of a conjunctive
node is defined as the outside weight of its disjunctive
mother node(s):
?c =
?
{d|c??(d)}
?d (3)
The outside weight of a disjunctive node is the sum of
the product of the outside weight(s) of its conjunctive
mother(s), the weight(s) of its mother(s), and the inside
weight(s) of its disjunctive sister(s):
?d =
?
{c|d??(c)}
{?c e
??f(c)
?
{d?|d???(c),d? 6=d}
?d?} (4)
From these formulae, the conditional expectation of a
feature-function fi can be computed from a chart with
root node r for a sentence y in the following way:
?
x?X(y)
e??f(x)fi(x)
Z?(y)
=
?
c?C
?c?cfi(c)
?r
(5)
Formula 5 is used in our system to compute expectations
for discriminative Bayesian estimation from partially la-
beled data using a first-order conjugate-gradient routine.
For a more detailed description of the optimization prob-
lem and the feature-functions we use for stochastic LFG
parsing see Riezler et al (2002). We also employed a
combined `1 regularization and feature selection tech-
nique described in Riezler and Vasserman (2004) that
considerably speeds up estimation and guarantees small
feature sets for stochastic disambiguation. In the experi-
ments reported in this paper, however, dynamic program-
ming is crucial for efficient stochastic disambiguation,
i.e. to efficiently find the most probable parse from a
packed parse forest that is annotated with feature-values.
There are two operations involved in stochastic disam-
biguation, namely calculating feature-values from a parse
forest and calculating node weights from a feature forest.
Clearly, the first one is more expensive, especially for
the extraction of values for non-local feature-functions
over large charts. To control the cost of this compu-
tation, our stochastic disambiguation system includes
a user-specified parameter for bounding the amount of
work that is done in calculating feature-values. When the
user-specified threshold for feature-value calculation is
reached, this computation is discontinued, and the dy-
namic programming calculation for most-probable-parse
search is computed from the current feature-value anno-
tation of the parse forest. Since feature-value computa-
tion proceeds incrementally over the feature forest, i.e.
for each node that is visited all feature-functions that ap-
ply to it are evaluated, a complete feature annotation can
be guaranteed for the part of the and/or-forest that is vis-
ited until discontinuation. As discussed below, these pa-
rameters were set on a held-out portion of the PARC700
which was also used to set the Collins parameters.
In the experiments reported in this paper, we used a
threshold on feature-extraction that allowed us to cut off
feature-extraction in 3% of the cases at no loss in accu-
racy. Overall, feature extraction and weight calculation
accounted for 5% of the computation time in combined
parsing and stochastic selection.
3 The Gold-Standard Dependency Bank
We used the PARC 700 Dependency Bank (DEPBANK)
as the gold standard in our experiments. The DEPBANK
consists of dependency annotations for 700 sentences that
were randomly extracted from section 23 of the UPenn
Wall Street Journal (WSJ) treebank. As described by
(King et al, 2003), the annotations were boot-strapped
by parsing the sentences with a LFG grammar and trans-
forming the resulting f-structures to a collection of depen-
dency triples in the DEPBANK format. To prepare a true
gold standard of dependencies, the tentative set of depen-
dencies produced by the robust parser was then corrected
and extended by human validators2. In this format each
triple specifies that a particular relation holds between a
head and either another head or a feature value, for ex-
ample, that the SUBJ relation holds between the heads
run and dog in the sentence The dog ran. Average sen-
tence length of sentences in DEPBANK is 19.8 words, and
the average number of dependencies per sentence is 65.4.
The corpus is freely available for research and evaluation,
as are documentation and tools for displaying and prun-
ing structures.3
In our experiments we used a Reduced version of the
DEPBANK, including just the minimum set of dependen-
cies necessary for reading out the central semantic rela-
tions and properties of a sentence. We tested against this
Reduced gold standard to establish accuracy on a lower
bound of the information that a meaning-sensitive appli-
cation would require. The Reduced version contained all
the argument and adjunct dependencies shown in Fig.
1, and a few selected semantically-relevant features, as
shown in Fig. 2. The features in Fig. 2 were chosen be-
2The resulting test set is thus unseen to the grammar and
stochastic disambiguation system used in our experiments. This
is indicated by the fact that the upperbound of F-score for the
best matching parses for the experiment grammar is in the range
of 85%, not 100%.
3http://www2.parc.com/istl/groups/nltt/fsbank/
Function Meaning
adjunct adjuncts
aquant adjectival quantifiers (many, etc.)
comp complement clauses (that, whether)
conj conjuncts in coordinate structures
focus int fronted element in interrogatives
mod noun-noun modifiers
number numbers modifying nouns
obj objects
obj theta secondary objects
obl oblique
obl ag demoted subject of a passive
obl compar comparative than/as clauses
poss possessives (John?s book)
pron int interrogative pronouns
pron rel relative pronouns
quant quantifiers (all, etc.)
subj subjects
topic rel fronted element in relative clauses
xcomp non-finite complements
verbal and small clauses
Figure 1: Grammatical functions in DEPBANK.
cause it was felt that they were fundamental to the mean-
ing of the sentences, and in fact they are required by the
semantic interpreter we have used in a knowledge-based
application (Crouch et al, 2002).
Feature Meaning
adegree degree of adjectives and adverbs
(positive, comparative, superlative)
coord form form of a coordinating
conjunction (e.g., and, or)
det form form of a determiner (e.g., the, a)
num number of nouns (sg, pl)
number type cardinals vs. ordinals
passive passive verb (e.g., It was eaten.)
perf perfective verb (e.g., have eaten)
precoord form either, neither
prog progressive verb (e.g., were eating)
pron form form of a pronoun (he, she, etc.)
prt form particle in a particle verb
(e.g., They threw it out.)
stmt type statement type (declarative,
interrogative, etc.)
subord form subordinating conjunction (e.g. that)
tense tense of the verb (past, present, etc.)
Figure 2: Selected features for Reduced DEPBANK
.
As a concrete example, the dependency list in Fig. 3 is
the Reduced set corresponding to the following sentence:
He reiterated his opposition to such funding,
but expressed hope of a compromise.
An additional feature of the DEPBANK that is relevant
to our comparisons is that dependency heads are rep-
resented by their standard citation forms (e.g. the verb
swam in a sentence appears as swim in its dependencies).
We believe that most applications will require a conver-
sion to canonical citation forms so that semantic relations
can be mapped into application-specific databases or on-
tologies. The predicates of LFG f-structures are already
represented as citation forms; for a fair comparison we
ran the leaves of the Collins tree through the same stem-
mer modules as part of the tree-to-dependency transla-
tion. We also note that proper names appear in the DEP-
BANK as single multi-word expressions without any in-
ternal structure. That is, there are no dependencies hold-
ing among the parts of people names (A. Boyd Simpson),
company names (Goldman, Sachs & Co), and organiza-
tion names (Federal Reserve). This multiword analysis
was chosen because many applications do not require
the internal structure of names, and the identification of
named entities is now typically carried out by a separate
non-syntactic pre-processing module. This was captured
for the LFG parser by using named entity markup and for
the Collins parser by creating complex word forms with
a single POS tag (section 5).
conj(coord?0, express?3)
conj(coord?0, reiterate?1)
coord form(coord?0, but)
stmt type(coord?0, declarative)
obj(reiterate?1, opposition?6)
subj(reiterate?1, pro?7)
tense(reiterate?1, past)
obj(express?3, hope?15)
subj(express?3, pro?7)
tense(express?3, past)
adjunct(opposition?6, to?11)
num(opposition?6, sg)
poss(opposition?6, pro?19)
num(pro?7, sg)
pron form(pro?7, he)
obj(to?11, funding?13)
adjunct(funding?13, such?45)
num(funding?13, sg)
adjunct(hope?15, of?46)
num(hope?15, sg)
num(pro?19, sg)
pron form(pro?19, he)
adegree(such?45, positive)
obj(of?46, compromise?54)
det form(compromise?54, a)
num(compromise?54, sg)
Figure 3: Reduced dependency relations for He reiterated
his opposition to such funding, but expressed hope of a
compromise.
4 Conversion to Dependency Bank Format
A conversion routine was required for each system to
transform its output so that it could be compared to the
DEPBANK dependencies. While it is relatively straightfor-
ward to convert LFG f-structures to the dependency bank
format because the f-structure is effectively a dependency
format, it is more difficult to transform the output trees of
the Model 3 Collins parser in a way that fairly allocates
both credits and penalties.
LFG Conversion We discarded the LFG tree structures
and used a general rewriting system previously developed
for machine translation to rewrite the relevant f-structure
attributes as dependencies (see King et al (2003)). The
rewritings involved some deletions of irrelevant features,
some systematic manipulations of the analyses, and some
trivial respellings. The deletions involved features pro-
duced by the grammar but not included in the PARC 700
such as negative values of PASS, PERF, and PROG and
the feature MEASURE used to mark measure phrases. The
manipulations are more interesting and are necessary to
map systematic differences between the analyses in the
grammar and those in the dependency bank. For example,
coordination is treated as a set by the LFG grammar but as
a single COORD dependency with several CONJ relations
in the dependency bank. Finally, the trivial rewritings
were used to, for example, change STMT-TYPE decl in
the grammar to STMT-TYPE declarative in the de-
pendency bank. For the Reduced version of the PARC
700 substantially more features were deleted.
Collins Model 3 Conversion An abbreviated represen-
tation of the Collins tree for the example above is shown
in Fig. 4. In this display we have eliminated the head lex-
ical items that appear redundantly at all the nonterminals
in a head chain, instead indicating by a single number
which daughter is the head. Thus, S?2 indicates that the
head of the main clause is its second daughter, the VP,
and its head is its first VP daughter. Indirectly, then, the
lexical head of the S is the first verb reiterated.
(TOP?1
(S?2 (NP-A?1 (NPB?1 He/PRP))
(VP?1 (VP?1 reiterated/VBD
(NP-A?1 (NPB?2 his/PRP$
opposition/NN)
(PP?1 to/TO
(NPB?2 such/JJ
funding/NN))))
but/CC
(VP?1 expressed/VBD
(NP-A?1 (NPB?1 hope/NN)
(PP?1 of/IN
(NP-A?1 (NPB?2 a/DT
compromise/NN))))))))
Figure 4: Collins Model 3 tree for He reiterated his op-
position to such funding, but expressed hope of a compro-
mise.
The Model 3 output in this example includes standard
phrase structure categories, indications of the heads, and
the additional -A marker to distinguish arguments from
adjuncts. The terminal nodes of this tree are inflected
forms, and the first phase of our conversion replaces them
with their citation forms (the verbs reiterate and express,
and the decapitalized and standardized he for He and his).
We also adjust for systematic differences in the choice of
heads. The first conjunct tends to be marked as the head
of a coordination in Model 3 output, whereas the depen-
dency bank has a more symmetric representation: it in-
troduces a new COORD head and connects that up to the
conjunction, and it uses a separate CONJ relation for each
of the coordinated items. Similarly, Model 3 identifies
the syntactic markers to and that as the heads of com-
plements, whereas the dependency bank treats these as
selectional features and marks the main predicate of the
complements as the head. These adjustments are carried
out without penalty. We also compensate for the differ-
ences in the representation of auxiliaries: Model 3 treats
these as main verbs with embedded complements instead
of the PERF, PROG, and PASSIVE features of the DEP-
BANK, and our conversion flattens the trees so that the
features can be read off.
The dependencies are read off after these and a few
other adjustments are made. NPs under VPs are read off
either as objects or adjuncts, depending on whether or
not the NP is annotated with the argument indicator (-A)
as in this example; the -A presumably would be miss-
ing in a sentence like John arrived Friday, and Friday
would be treated as an ADJUNCT. Similarly, NP-As un-
der S are read off as subject. In this example, however,
this principle of conversion does not lead to a match with
the dependency bank: in the DEPBANK grammatical rela-
tions that are factored out of conjoined structures are dis-
tributed back into those structures, to establish the correct
semantic dependencies (in this case, that he is the subject
of both reiterate and express and not of the introduced
coord). We avoided the temptation of building coordinate
distribution into the conversion routine because, first, it is
not always obvious from the Model 3 output when dis-
tribution should take place, and second, that would be
a first step towards building into the conversion routine
the deep lexical and syntactic knowledge (essentially the
functional component of our LFG grammar) that the shal-
low approach explicitly discounts4.
For the same reasons our conversion routine does not
identify the subjects of infinitival complements with par-
ticular arguments of matrix verbs. The Model 3 trees pro-
vide no indication of how this is to be done, and in many
cases the proper assignment depends on lexical informa-
tion about specific predicates (to capture, for example, the
well-known contrast between promise and persuade).
Model 3 trees also provide information about certain
4However, we did explore a few of these additional transfor-
mations and found only marginal F-score increases.
long-distance dependencies, by marking with -g annota-
tions the path between a filler and a gap and marking the
gap by an explicit TRACE in the terminal string. The filler
itself is not clearly identified, but our conversion treats
all WH categories under SBAR as potential fillers and
attempts to propagate them down the gap-chain to link
them up to appropriate traces.
In sum, it is not a trivial matter to convert a Model 3
tree to an appropriate set of dependency relations, and the
process requires a certain amount of intuition and skill.
For our experiments we tried to define a conversion that
gives appropriate credit to the dependencies that can be
read from the trees without relying on an undue amount
of sophisticated linguistic knowledge5.
5 Experiments
We conducted our experiments by preparing versions of
the test sentences in the form appropriate to each sys-
tem. We used a configuration of the XLE parser that ex-
pects sentences conforming to ordinary text conventions
to appear in a file separated by double line-feeds. A cer-
tain amount of effort was required to remove the part-of-
speech tags and labeled brackets of the WSJ corpus in a
way that restored the sentences to a standard English for-
mat (for example, to remove the space between wo and n?t
that remains when the POS tags are removed). Since the
PARC 700 treats proper names as multiword expressions,
we then augmented the input strings with XML markup
of the named entities. These are parsed by the grammar
as described in section 2. We used manual named entity
markup for this experiment because our intent is to mea-
sure parsing technology independent of either the time
or errors of an automatic named-entity extractor. How-
ever, in other experiments with an automatic finite-state
extractor, we have found that the time for named-entity
recognition is negligible (on the order of seconds across
the entire corpus) and makes relatively few errors, so that
the results reported here are good approximations of what
might be expected in more realistic situations.
As input to the Collins parser, we used the part-of-
speech tagged version of section 23 that was provided
with the parser. From this we extracted the 700 sentences
in the PARC 700. We then modified them to produce
named entity input so that the parses would match the
PARC 700. This was done by putting underscores be-
tween the parts of the named entity and changing the final
part of speech tag to the appropriate one (usually NNP)
if necessary. (The number of words indicated at the be-
ginning of the input string was also reduced accordingly.)
An example is shown in (1).
5The results of this conversion are available at
http://www2.parc.com/istl/groups/nltt/fsbank/
(1) Sen. NNP Christopher NNP Dodd NNP ??
Sen. Christopher Dodd NNP
After parsing, the underscores were converted to spaces
to match the PARC 700 predicates.
Before the final evaluation, 1/5 of the PARC 700 de-
pendency bank was randomly extracted as a heldout set.
This set was used to adjust the performance parameters of
the XLE system and the Collins parser so as to optimize
parsing speed without losing accuracy. For example, the
limit on the length of medial phrases was set to 20 words
for the XLE system (see Sec. 2), and a regularizer penalty
of 10 was found optimal for the `1 prior used in stochas-
tic disambiguation. For the Collins parser, a beam size
of 1000 was found to improve speed considerably at lit-
tle cost in accuracy. Furthermore, the np-bracketing flag
(npbflag) was set to 0 to produce an extended set of NP
levels for improved argument/adjunct distinction6. The fi-
nal evaluation was done on the remaining 560 examples.
Timing results are reported in seconds of CPU time7. POS
tagging of the input to the Collins parser took 6 seconds
and this was added to the timing result of the Collins
parser. Time spent for finite-state morphology and dictio-
nary lookup for XLE is part of the measure of its timing
performance. We did not include the time for dependency
extraction or stemming the Collins output.
Table 1 shows timing and accuracy results for the Re-
duced dependency set. The parser settings compared are
Model 3 of the Collins parser adjusted to beam size 1000,
and the Core and Complete versions of the XLE sys-
tem, differing in the size of the grammar?s constraint-
set. Clearly, both versions of the XLE system achieve a
significant reduction in error rate over the Collins parser
(12% for the core XLE system and 20% for the complete
system) at an increase in parsing time of a factor of only
1.49 for the core XLE system. The complete version gives
an overall improvement in F-score of 5% over the Collins
parser at a cost of a factor of 5 in parsing time.
Table 1: Timing and accuracy results for Collins parser
and Complete and Core versions of XLE system on Re-
duced version of PARC 700 dependency bank.
time prec. rec. F-score
LFG core 298.88 79.1 76.2 77.6
LFG complete 985.3 79.4 79.8 79.6
Collins 1000 199.6 78.3 71.2 74.6
6A beam size of 10000 as used in Collins (1999) improved
the F-score on the heldout set only by .1% at an increase of pars-
ing time by a factor of 3. Beam sizes lower than 1000 decreased
the heldout F-score significantly.
7All experiments were run on one CPU of a dual proces-
sor AMD Opteron 244 with 1.8 GHz and 4GB main memory.
Loading times are included in CPU times.
6 Conclusion
We presented some experiments that compare the accu-
racy and performance of two stochastic parsing systems,
the shallow Collins parser and the deep-grammar-based
XLE system. We measured the accuracy of both systems
against a gold standard derived from the PARC 700 de-
pendency bank, and also measured their processing times.
Contrary to conventional wisdom, we found that the shal-
low system was not substantially faster than the deep
parser operating on a core grammar, while the deep sys-
tem was significantly more accurate. Furthermore, ex-
tending the grammar base of the deep system results in
much better accuracy at a cost of a factor of 5 in speed.
Our experiment is comparable to recent work on read-
ing off Propbank-style (Kingsbury and Palmer, 2002)
predicate-argument relations from gold-standard tree-
bank trees and automatic parses of the Collins parser.
Gildea and Palmer (2002) report F-score results in the
55% range for argument and boundary recognition based
on automatic parses. From this perspective, the nearly
75% F-score that is achieved for our deterministic rewrit-
ing of Collins? trees into dependencies is remarkable,
even if the results are not directly comparable. Our scores
and Gildea and Palmer?s are both substantially lower than
the 90% typically cited for evaluations based on labeled
or unlabeled bracketing, suggesting that extracting se-
mantically relevant dependencies is a more difficult, but
we think more valuable, task.
References
Miriam Butt, Helge Dyvik, Tracy Holloway King, Hi-
roshi Masuichi, and Christian Rohrer. 2002. The
parallel grammar project. In Proceedings of COL-
ING2002, Workshop on Grammar Engineering and
Evaluation, pages 1?7.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
D. Crouch, C. Condoravdi, R. Stolle, T.H. King,
V. de Paiva, J. Everett, and D. Bobrow. 2002. Scal-
ability of redundancy detection in focused document
collections. In Proceedings of Scalable Natural Lan-
guage Understanding, Heidelberg.
Hal Daume and Daniel Marcu. 2002. A noisy-channel
model for document compression. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL?02), Philadelphia, PA.
Anette Frank, Tracy H. King, Jonas Kuhn, and John
Maxwell. 1998. Optimality theory style constraint
ranking in large-scale LFG grammars. In Proceedings
of the Third LFG Conference.
Stuart Geman and Mark Johnson. 2002. Dynamic
programming for parsing and estimation of stochatic
unification-based grammars. In Proceedings of the
40th Annual Meeting of the Association for Compu-
tational Linguistics (ACL?02), Philadelphia, PA.
Dan Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL?02), Philadelphia.
Sanda Harabagiu, Dan Moldovan, Marius Pas?ca, Rada
Mihalcea, Mihai Surdeanu, Ra?zvan Bunescu, Roxana
G??rju, Vasile Rus, and Paul Mora?rescu. 2001. The
role of lexico-semantic feedback in open-domain tex-
tual question-answering. In Proceedings of the 39th
Annual Meeting and 10th Conference of the European
Chapter of the Asssociation for Computational Lin-
guistics (ACL?01), Toulouse, France.
Tracy H. King, Richard Crouch, Stefan Riezler, Mary
Dalrymple, and Ronald M. Kaplan. 2003. The PARC
700 dependency bank. In Proceedings of the Work-
shop on ?Linguistically Interpreted Corpora? at the
10th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics (LINC?03), Bu-
dapest, Hungary.
Paul Kingsbury and Martha Palmer. 2002. From tree-
bank to propbank. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Evalu-
ation (LREC?02), Las Palmas, Spain.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Scott Miller, Heidi Fox, Lance Ramshaw, and Ralph
Weischedel. 2000. A novel use of statistical parsing
to extract information from text. In Proceedings of
the 1st Conference of the North American Chapter of
the Association for Computational Linguistics (ANLP-
NAACL 2000), Seattle, WA.
Yusuke Miyao and Jun?ichi Tsujii. 2002. Maximum
entropy estimation for feature forests. In Proceed-
ings of the Human Language Technology Conference
(HLT?02), San Diego, CA.
Adwait Ratnaparkhi. 1996. A maximum entropy model
for part-of-speech tagging. In Proceedings of EMNLP-
1.
Stefan Riezler and Alexander Vasserman. 2004. Gradi-
ent feature testing and `1 regularization for maximum
entropy parsing. Submitted for publication.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. Maxwell, and Mark John-
son. 2002. Parsing the Wall Street Journal using a
Lexical-Functional Grammar and discriminative esti-
mation techniques. In Proceedings of the 40th Annual
Meeting of the Association for Computational Linguis-
tics (ACL?02), Philadelphia, PA.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting and 10th Conference of the Eu-
ropean Chapter of the Asssociation for Computational
Linguistics (ACL?01), Toulouse, France.
Parsing the Wall Street Journal using a Lexical-Functional Grammar and
Discriminative Estimation Techniques
Stefan Riezler Tracy H. King Ronald M. Kaplan
Palo Alto Research Center Palo Alto Research Center Palo Alto Research Center
Palo Alto, CA 94304 Palo Alto, CA 94304 Palo Alto, CA 94304
riezler@parc.com thking@parc.com kaplan@parc.com
Richard Crouch John T. Maxwell III Mark Johnson
Palo Alto Research Center Palo Alto Research Center Brown University
Palo Alto, CA 94304 Palo Alto, CA 94304 Providence, RI 02912
crouch@parc.com maxwell@parc.com mj@cs.brown.edu
Abstract
We present a stochastic parsing system
consisting of a Lexical-Functional Gram-
mar (LFG), a constraint-based parser and
a stochastic disambiguation model. We re-
port on the results of applying this sys-
tem to parsing the UPenn Wall Street
Journal (WSJ) treebank. The model com-
bines full and partial parsing techniques
to reach full grammar coverage on unseen
data. The treebank annotations are used
to provide partially labeled data for dis-
criminative statistical estimation using ex-
ponential models. Disambiguation perfor-
mance is evaluated by measuring matches
of predicate-argument relations on two
distinct test sets. On a gold standard of
manually annotated f-structures for a sub-
set of the WSJ treebank, this evaluation
reaches 79% F-score. An evaluation on a
gold standard of dependency relations for
Brown corpus data achieves 76% F-score.
1 Introduction
Statistical parsing using combined systems of hand-
coded linguistically fine-grained grammars and
stochastic disambiguation components has seen con-
siderable progress in recent years. However, such at-
tempts have so far been confined to a relatively small
scale for various reasons. Firstly, the rudimentary
character of functional annotations in standard tree-
banks has hindered the direct use of such data for
statistical estimation of linguistically fine-grained
statistical parsing systems. Rather, parameter esti-
mation for such models had to resort to unsupervised
techniques (Bouma et al, 2000; Riezler et al, 2000),
or training corpora tailored to the specific grammars
had to be created by parsing and manual disam-
biguation, resulting in relatively small training sets
of around 1,000 sentences (Johnson et al, 1999).
Furthermore, the effort involved in coding broad-
coverage grammars by hand has often led to the spe-
cialization of grammars to relatively small domains,
thus sacrificing grammar coverage (i.e. the percent-
age of sentences for which at least one analysis is
found) on free text. The approach presented in this
paper is a first attempt to scale up stochastic parsing
systems based on linguistically fine-grained hand-
coded grammars to the UPenn Wall Street Journal
(henceforth WSJ) treebank (Marcus et al, 1994).
The problem of grammar coverage, i.e. the fact
that not all sentences receive an analysis, is tack-
led in our approach by an extension of a full-
fledged Lexical-Functional Grammar (LFG) and a
constraint-based parser with partial parsing tech-
niques. In the absence of a complete parse, a so-
called ?FRAGMENT grammar? allows the input to be
analyzed as a sequence of well-formed chunks. The
set of fragment parses is then chosen on the basis
of a fewest-chunk method. With this combination of
full and partial parsing techniques we achieve 100%
grammar coverage on unseen data.
Another goal of this work is the best possible ex-
ploitation of the WSJ treebank for discriminative es-
timation of an exponential model on LFG parses. We
define discriminative or conditional criteria with re-
                Computational Linguistics (ACL), Philadelphia, July 2002, pp. 271-278.
                         Proceedings of the 40th Annual Meeting of the Association for
CS 1: FRAGMENTS
Sadj[fin]
S[fin]
NP
D 
the
NPadj
AP[attr]
A
golden
NPzero
N
share
VPall[fin]
VP[pass,fin]
AUX[pass,fin]
was
VPv[pass]
V[pass]
scheduled
VPinf
VPinf?pos
PARTinf
to
VPall[base]
VPv[base]
V[base]
expire
PPcl
PP
P
at
NP
D
the
NPadj
NPzero
N
beginning
FRAGMENTS
TOKEN
of
"The golden share was scheduled to expire at the beginning of"
?schedule<NULL, [132:expire]>[11:share]?PRED
?share?PRED 
?golden<[11:share]>?PRED  [11:share]SUBJADEGREE positive , ADJUNCT?TYPE nominal, ATYPE attributive23ADJUNCT
unspecifiedGRAINNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE nom , NUM  sg, PERS   311
SUBJ
?expire<[11:share]>?PRED  [11:share]SUBJ
?at<[170:beginning]>?PRED
?beginning ?PRED 
GERUND +, GRAIN unspecifiedNTYPE
DET?FORM  the _, DET?TYPE  defDETSPEC
CASE acc, NUM  sg, PCASE   at, PERS   3170
OBJ
ADV?TYPE	  vpadv
 , PSEM   locative, PTYPE   sem164
ADJUNCT	
INF?FORM to , PASSIVE   ?, VTYPE  main132
XCOMP
MOOD indicative, TENSE pastTNS?ASP
PASSIVE +, STMT?TYPE decl, VTYPE main67
FIRST
ofTOKEN229FIRST3218REST3188
Figure 1: FRAGMENT c-/f-structure for The golden share was scheduled to expire at the beginning of
spect to the set of grammar parses consistent with
the treebank annotations. Such data can be gathered
by applying labels and brackets taken from the tree-
bank annotation to the parser input. The rudimen-
tary treebank annotations are thus used to provide
partially labeled data for discriminative estimation
of a probability model on linguistically fine-grained
parses.
Concerning empirical evaluation of disambigua-
tion performance, we feel that an evaluation measur-
ing matches of predicate-argument relations is more
appropriate for assessing the quality of our LFG-
based system than the standard measure of match-
ing labeled bracketing on section 23 of the WSJ
treebank. The first evaluation we present measures
matches of predicate-argument relations in LFG f-
structures (henceforth the LFG annotation scheme)
to a gold standard of manually annotated f-structures
for a representative subset of the WSJ treebank. The
evaluation measure counts the number of predicate-
argument relations in the f-structure of the parse
selected by the stochastic model that match those
in the gold standard annotation. Our parser plus
stochastic disambiguator achieves 79% F-score un-
der this evaluation regime.
Furthermore, we employ another metric which
maps predicate-argument relations in LFG f-
structures to the dependency relations (henceforth
the DR annotation scheme) proposed by Carroll et
al. (1999). Evaluation with this metric measures the
matches of dependency relations to Carroll et al?s
gold standard corpus. For a direct comparison of our
results with Carroll et al?s system, we computed an
F-score that does not distinguish different types of
dependency relations. Under this measure we obtain
76% F-score.
This paper is organized as follows. Section 2
describes the Lexical-Functional Grammar, the
constraint-based parser, and the robustness tech-
niques employed in this work. In section 3 we
present the details of the exponential model on LFG
parses and the discriminative statistical estimation
technique. Experimental results are reported in sec-
tion 4. A discussion of results is in section 5.
2 Robust Parsing using LFG
2.1 A Broad-Coverage LFG
The grammar used for this project was developed in
the ParGram project (Butt et al, 1999). It uses LFG
as a formalism, producing c(onstituent)-structures
(trees) and f(unctional)-structures (attribute value
matrices) as output. The c-structures encode con-
stituency. F-structures encode predicate-argument
relations and other grammatical information, e.g.,
number, tense. The XLE parser (Maxwell and Ka-
plan, 1993) was used to produce packed represen-
tations, specifying all possible grammar analyses of
the input.
The grammar has 314 rules with regular expres-
sion right-hand sides which compile into a collec-
tion of finite-state machines with a total of 8,759
states and 19,695 arcs. The grammar uses several
lexicons and two guessers: one guesser for words
recognized by the morphological analyzer but not
in the lexicons and one for those not recognized.
As such, most nouns, adjectives, and adverbs have
no explicit lexical entry. The main verb lexicon con-
tains 9,652 verb stems and 23,525 subcategorization
frame-verb stem entries; there are also lexicons for
adjectives and nouns with subcategorization frames
and for closed class items.
For estimation purposes using the WSJ treebank,
the grammar was modified to parse part of speech
tags and labeled bracketing. A stripped down ver-
sion of the WSJ treebank was created that used
only those POS tags and labeled brackets relevant
for determining grammatical relations. The WSJ la-
beled brackets are given LFG lexical entries which
constrain both the c-structure and the f-structure of
the parse. For example, the WSJ?s ADJP-PRD la-
bel must correspond to an AP in the c-structure and
an XCOMP in the f-structure. In this version of the
corpus, all WSJ labels with -SBJ are retained and
are restricted to phrases corresponding to SUBJ in
the LFG grammar; in addition, it contains NP under
VP (OBJ and OBJth in the LFG grammar), all -LGS
tags (OBL-AG), all -PRD tags (XCOMP), VP under
VP (XCOMP), SBAR- (COMP), and verb POS tags
under VP (V in the c-structure). For example, our
labeled bracketing of wsj 1305.mrg is [NP-SBJ His
credibility] is/VBZ also [PP-PRD on the line] in the
investment community.
Some mismatches between the WSJ labeled
bracketing and the LFG grammar remain. These
often arise when a given constituent fills a gram-
matical role in more than one clause. For exam-
ple, in wsj 1303.mrg Japan?s Daiwa Securities Co.
named Masahiro Dozen president., the noun phrase
Masahiro Dozen is labeled as an NP-SBJ. However,
the LFG grammar treats it as the OBJ of the ma-
trix clause. As a result, the labeled bracketed version
of this sentence does not receive a full parse, even
though its unlabeled, string-only counterpart is well-
formed. Some other bracketing mismatches remain,
usually the result of adjunct attachment. Such mis-
matches occur in part because, besides minor mod-
ifications to match the bracketing for special con-
structions, e.g., negated infinitives, the grammar was
not altered to mirror the idiosyncrasies of the WSJ
bracketing.
2.2 Robustness Techniques
To increase robustness, the standard grammar has
been augmented with a FRAGMENT grammar. This
grammar parses the sentence as well-formed chunks
specified by the grammar, in particular as Ss, NPs,
PPs, and VPs. These chunks have both c-structures
and f-structures corresponding to them. Any token
that cannot be parsed as one of these chunks is
parsed as a TOKEN chunk. The TOKENs are also
recorded in the c- and f-structures. The grammar has
a fewest-chunk method for determining the correct
parse. For example, if a string can be parsed as two
NPs and a VP or as one NP and an S, the NP-S
option is chosen. A sample FRAGMENT c-structure
and f-structure are shown in Fig. 1 for wsj 0231.mrg
(The golden share was scheduled to expire at the
beginning of), an incomplete sentence; the parser
builds one S chunk and then one TOKEN for the
stranded preposition.
A final capability of XLE that increases cov-
erage of the standard-plus-fragment grammar is a
SKIMMING technique. Skimming is used to avoid
timeouts and memory problems. When the amount
of time or memory spent on a sentence exceeds
a threshhold, XLE goes into skimming mode for
the constituents whose processing has not been
completed. When XLE skims these remaining con-
stituents, it does a bounded amount of work per sub-
tree. This guarantees that XLE finishes processing
a sentence in a polynomial amount of time. In pars-
ing section 23, 7.2% of the sentences were skimmed;
26.1% of these resulted in full parses, while 73.9%
were FRAGMENT parses.
The grammar coverage achieved 100% of section
23 as unseen unlabeled data: 74.7% as full parses,
25.3% FRAGMENT and/or SKIMMED parses.
3 Discriminative Statistical Estimation
from Partially Labeled Data
3.1 Exponential Models on LFG Parses
We employed the well-known family of exponential
models for stochastic disambiguation. In this paper
we are concerned with conditional exponential mod-
els of the form:
p?(x|y) = Z?(y)
?1e??f(x)
where X(y) is the set of parses for sentence y,
Z?(y) =
?
x?X(y) e
??f(x) is a normalizing con-
stant, ? = (?1, . . . , ?n) ? IRn is a vector of
log-parameters, f = (f1, . . . , fn) is a vector of
property-functions fi : X ? IR for i = 1, . . . , n
on the set of parses X , and ? ? f(x) is the vector dot
product
?n
i=1 ?ifi(x).
In our experiments, we used around 1000
complex property-functions comprising information
about c-structure, f-structure, and lexical elements
in parses, similar to the properties used in Johnson
et al (1999). For example, there are property func-
tions for c-structure nodes and c-structure subtrees,
indicating attachment preferences. High versus low
attachment is indicated by property functions count-
ing the number of recursively embedded phrases.
Other property functions are designed to refer to
f-structure attributes, which correspond to gram-
matical functions in LFG, or to atomic attribute-
value pairs in f-structures. More complex property
functions are designed to indicate, for example, the
branching behaviour of c-structures and the (non)-
parallelism of coordinations on both c-structure and
f-structure levels. Furthermore, properties refering
to lexical elements based on an auxiliary distribution
approach as presented in Riezler et al (2000) are
included in the model. Here tuples of head words,
argument words, and grammatical relations are ex-
tracted from the training sections of the WSJ, and
fed into a finite mixture model for clustering gram-
matical relations. The clustering model itself is then
used to yield smoothed probabilities as values for
property functions on head-argument-relation tuples
of LFG parses.
3.2 Discriminative Estimation
Discriminative estimation techniques have recently
received great attention in the statistical machine
learning community and have already been applied
to statistical parsing (Johnson et al, 1999; Collins,
2000; Collins and Duffy, 2001). In discriminative es-
timation, only the conditional relation of an analysis
given an example is considered relevant, whereas in
maximum likelihood estimation the joint probability
of the training data to best describe observations is
maximized. Since the discriminative task is kept in
mind during estimation, discriminative methods can
yield improved performance. In our case, discrimi-
native criteria cannot be defined directly with respect
to ?correct labels? or ?gold standard? parses since
the WSJ annotations are not sufficient to disam-
biguate the more complex LFG parses. However, in-
stead of retreating to unsupervised estimation tech-
niques or creating small LFG treebanks by hand, we
use the labeled bracketing of the WSJ training sec-
tions to guide discriminative estimation. That is, dis-
criminative criteria are defined with respect to the set
of parses consistent with the WSJ annotations.1
The objective function in our approach, denoted
by P (?), is the joint of the negative log-likelihood
?L(?) and a Gaussian regularization term ?G(?)
on the parameters ?. Let {(yj , zj)}mj=1 be a set of
training data, consisting of pairs of sentences y and
partial annotations z, let X(y, z) be the set of parses
for sentence y consistent with annotation z, and let
X(y) be the set of all parses produced by the gram-
mar for sentence y. Furthermore, let p[f ] denote the
expectation of function f under distribution p. Then
P (?) can be defined for a conditional exponential
model p?(z|y) as:
P (?) = ?L(?)?G(?)
= ? log
m?
j=1
p?(zj |yj) +
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
?
X(yj)
e??f(x)
+
n?
i=1
?2i
2?2i
= ?
m?
j=1
log
?
X(yj ,zj)
e??f(x)
+
m?
j=1
log
?
X(yj)
e??f(x) +
n?
i=1
?2i
2?2i
.
Intuitively, the goal of estimation is to find model pa-
1An earlier approach using partially labeled data for estimat-
ing stochastics parsers is Pereira and Schabes?s (1992) work on
training PCFG from partially bracketed data. Their approach
differs from the one we use here in that Pereira and Schabes
take an EM-based approach maximizing the joint likelihood of
the parses and strings of their training data, while we maximize
the conditional likelihood of the sets of parses given the corre-
sponding strings in a discriminative estimation setting.
rameters which make the two expectations in the last
equation equal, i.e. which adjust the model param-
eters to put all the weight on the parses consistent
with the annotations, modulo a penalty term from
the Gaussian prior for too large or too small weights.
Since a closed form solution for such parame-
ters is not available, numerical optimization meth-
ods have to be used. In our experiments, we applied
a conjugate gradient routine, yielding a fast converg-
ing optimization algorithm where at each iteration
the negative log-likelihood P (?) and the gradient
vector have to be evaluated.2 For our task the gra-
dient takes the form:
?P (?) =
?
?P (?)
??1
,
?P (?)
??2
, . . . ,
?P (?)
??n
?
, and
?P (?)
??i
= ?
m?
j=1
(
?
x?X(yj ,zj)
e??f(x)fi(x)
?
x?X(yj ,zj)
e??f(x)
?
?
x?X(yj)
e??f(x)fi(x)
?
x?X(yj)
e??f(x)
) +
?i
?2i
.
The derivatives in the gradient vector intuitively are
again just a difference of two expectations
?
m?
j=1
p?[fi|yj , zj ] +
m?
j=1
p?[fi|yj ] +
?i
?2i
.
Note also that this expression shares many common
terms with the likelihood function, suggesting an ef-
ficient implementation of the optimization routine.
4 Experimental Evaluation
4.1 Training
The basic training data for our experiments are sec-
tions 02-21 of the WSJ treebank. As a first step, all
sections were parsed, and the packed parse forests
unpacked and stored. For discriminative estimation,
this data set was restricted to sentences which re-
ceive a full parse (in contrast to a FRAGMENT or
SKIMMED parse) for both its partially labeled and
its unlabeled variant. Furthermore, only sentences
2An alternative numerical method would be a combination
of iterative scaling techniques with a conditional EM algorithm
(Jebara and Pentland, 1998). However, it has been shown exper-
imentally that conjugate gradient techniques can outperform it-
erative scaling techniques by far in running time (Minka, 2001).
which received at most 1,000 parses were used.
From this set, sentences of which a discriminative
learner cannot possibly take advantage, i.e. sen-
tences where the set of parses assigned to the par-
tially labeled string was not a proper subset of the
parses assigned the unlabeled string, were removed.
These successive selection steps resulted in a fi-
nal training set consisting of 10,000 sentences, each
with parses for partially labeled and unlabeled ver-
sions. Altogether there were 150,000 parses for par-
tially labeled input and 500,000 for unlabeled input.
For estimation, a simple property selection pro-
cedure was applied to the full set of around 1000
properties. This procedure is based on a frequency
cutoff on instantiations of properties for the parses
in the labeled training set. The result of this proce-
dure is a reduction of the property vector to about
half its size. Furthermore, a held-out data set was
created from section 24 of the WSJ treebank for ex-
perimental selection of the variance parameter of the
prior distribution. This set consists of 120 sentences
which received only full parses, out of which the
most plausible one was selected manually.
4.2 Testing
Two different sets of test data were used: (i) 700 sen-
tences randomly extracted from section 23 of the
WSJ treebank and given gold-standard f-structure
annotations according to our LFG scheme, and (ii)
500 sentences from the Brown corpus given gold
standard annotations by Carroll et al (1999) accord-
ing to their dependency relations (DR) scheme.3
Annotating the WSJ test set was bootstrapped
by parsing the test sentences using the LFG gram-
mar and also checking for consistency with the
Penn Treebank annotation. Starting from the (some-
times fragmentary) parser analyses and the Tree-
bank annotations, gold standard parses were created
by manual corrections and extensions of the LFG
parses. Manual corrections were necessary in about
half of the cases. The average sentence length of
the WSJ f-structure bank is 19.8 words; the average
number of predicate-argument relations in the gold-
standard f-structures is 31.2.
Performance on the LFG-annotated WSJ test set
3Both corpora are available online. The WSJ f-structure
bank at www.parc.com/istl/groups/nltt/fsbank/, and Carroll et
al.?s corpus at www.cogs.susx.ac.uk/lab/nlp/carroll/greval.html.
was measured using both the LFG and DR metrics,
thanks to an f-structure-to-DR annotation mapping.
Performance on the DR-annotated Brown test set
was only measured using the DR metric.
The LFG evaluation metric is based on the com-
parison of full f-structures, represented as triples
relation(predicate, argument). The predicate-
argument relations of the f-structure for one parse of
the sentence Meridian will pay a premium of $30.5
million to assume $2 billion in deposits. are shown
in Fig. 2.
number($:9, billion:17) number($:24, million:4)
detform(premium:3, a) mood(pay:0, indicative)
tense(pay:0, fut) adjunct(million:4, ?30.5?:28)
adjunct(premium:3, of:23) adjunct(billion:17, ?2?:19)
adjunct($:9, in:11) adjunct(pay:0, assume:7)
obj(pay:0, premium:3) stmttype(pay:0, decl)
subj(pay:0, ?Meridian?:5) obj(assume:7, $:9)
obj(of:23, $:24) subj(assume:7, pro:8)
obj(in:11, deposit:12) prontype(pro:8, null)
stmttype(assume:7, purpose)
Figure 2: LFG predicate-argument relation represen-
tation
The DR annotation for our example sentence, ob-
tained via a mapping from f-structures to Carroll et
al?s annotation scheme, is shown in Fig. 3.
(aux pay will) (subj pay Meridian )
(detmod premium a) (mod million 30.5)
(mod $ million) (mod of premium $)
(dobj pay premium ) (mod billion 2)
(mod $ billion) (mod in $ deposit)
(dobj assume $ ) (mod to pay assume)
Figure 3: Mapping to Carroll et al?s dependency-
relation representation
Superficially, the LFG and DR representations are
very similar. One difference between the annotation
schemes is that the LFG representation in general
specifies more relation tuples than the DR represen-
tation. Also, multiple occurences of the same lex-
ical item are indicated explicitly in the LFG rep-
resentation but not in the DR representation. The
main conceptual difference between the two an-
notation schemes is the fact that the DR scheme
crucially refers to phrase-structure properties and
word order as well as to grammatical relations in
the definition of dependency relations, whereas the
LFG scheme abstracts away from serialization and
phrase-structure. Facts like this can make a correct
mapping of LFG f-structures to DR relations prob-
lematic. Indeed, we believe that we still underesti-
mate by a few points because of DR mapping diffi-
culties. 4
4.3 Results
In our evaluation, we report F-scores for both types
of annotation, LFG and DR, and for three types
of parse selection, (i) lower bound: random choice
of a parse from the set of analyses (averaged over
10 runs), (ii) upper bound: selection of the parse
with the best F-score according to the annotation
scheme used, and (iii) stochastic: the parse selected
by the stochastic disambiguator. The error reduc-
tion row lists the reduction in error rate relative to
the upper and lower bounds obtained by the stochas-
tic disambiguation model. F-score is defined as 2 ?
precision? recall/(precision+ recall).
Table 1 gives results for 700 examples randomly
selected from section 23 of the WSJ treebank, using
both LFG and DR measures.
Table 1: Disambiguation results for 700 randomly
selected examples from section 23 of the WSJ tree-
bank using LFG and DR measures.
LFG DR
upper bound 84.1 80.7
stochastic 78.6 73.0
lower bound 75.5 68.8
error reduction 36 35
The effect of the quality of the parses on disam-
biguation performance can be illustrated by break-
ing down the F-scores according to whether the
parser yields full parses, FRAGMENT, SKIMMED, or
SKIMMED+FRAGMENT parses for the test sentences.
The percentages of test examples which belong to
the respective classes of quality are listed in the first
row of Table 2. F-scores broken down according to
classes of parse quality are recorded in the follow-
4See Carroll et al (1999) for more detail on the DR an-
notation scheme, and see Crouch et al (2002) for more de-
tail on the differences between the DR and the LFG annotation
schemes, as well as on the difficulties of the mapping from LFG
f-structures to DR annotations.
ing rows. The first column shows F-scores for all
parses in the test set, as in Table 1. The second col-
umn shows the best F-scores when restricting atten-
tion to examples which receive only full parses. The
third column reports F-scores for examples which
receive only non-full parses, i.e. FRAGMENT or
SKIMMED parses or SKIMMED+FRAGMENT parses.
Columns 4-6 break down non-full parses according
to examples which receive only FRAGMENT, only
SKIMMED, or only SKIMMED+FRAGMENT parses.
Results of the evaluation on Carroll et al?s Brown
test set are given in Table 3. Evaluation results for
the DR measure applied to the Brown corpus test set
broken down according to parse-quality are shown
in Table 2.
In Table 3 we show the DR measure along with an
evaluation measure which facilitates a direct com-
parison of our results to those of Carroll et al
(1999). Following Carroll et al (1999), we count
a dependency relation as correct if the gold stan-
dard has a relation with the same governor and de-
pendent but perhaps with a different relation-type.
This dependency-only (DO) measure thus does not
reflect mismatches between arguments and modi-
fiers in a small number of cases. Note that since
for the evaluation on the Brown corpus, no heldout
data were available to adjust the variance parame-
ter of a Bayesian model, we used a plain maximum-
likelihood model for disambiguation on this test set.
Table 3: Disambiguation results on 500 Brown cor-
pus examples using DO measure and DR measures.
DO DR
Carroll et al (1999) 75.1 -
upper bound 82.0 80.0
stochastic 76.1 74.0
lower bound 73.3 71.7
error reduction 32 33
5 Discussion
We have presented a first attempt at scaling up a
stochastic parsing system combining a hand-coded
linguistically fine-grained grammar and a stochas-
tic disambiguation model to the WSJ treebank.
Full grammar coverage is achieved by combining
specialized constraint-based parsing techniques for
LFG grammars with partial parsing techniques. Fur-
thermore, a maximal exploitation of treebank anno-
tations for estimating a distribution on fine-grained
LFG parses is achieved by letting grammar analyses
which are consistent with the WSJ labeled bracket-
ing define a gold standard set for discriminative es-
timation. The combined system trained on WSJ data
achieves full grammar coverage and disambiguation
performance of 79% F-score on WSJ data, and 76%
F-score on the Brown corpus test set.
While disambiguation performance of around
79% F-score on WSJ data seems promising, from
one perspective it only offers a 3% absolute im-
provement over a lower bound random baseline.
We think that the high lower bound measure high-
lights an important aspect of symbolic constraint-
based grammars (in contrast to treebank gram-
mars): the symbolic grammar already significantly
restricts/disambiguates the range of possible analy-
ses, giving the disambiguator a much narrower win-
dow in which to operate. As such, it is more appro-
priate to assess the disambiguator in terms of reduc-
tion in error rate (36% relative to the upper bound)
than in terms of absolute F-score. Both the DR and
LFG annotations broadly agree in their measure of
error reduction.
The lower reduction in error rate relative to the
upper bound for DR evaluation on the Brown corpus
can be attributed to a corpus effect that has also been
observed by Gildea (2001) for training and testing
PCFGs on the WSJ and Brown corpora.5
Breaking down results according to parse quality
shows that irrespective of evaluation measure and
corpus, around 4% overall performance is lost due
to non-full parses, i.e. FRAGMENT, or SKIMMED, or
SKIMMED+FRAGMENT parses.
Due to the lack of standard evaluation measures
and gold standards for predicate-argument match-
ing, a comparison of our results to other stochastic
parsing systems is difficult. To our knowledge, so
far the only direct point of comparison is the parser
of Carroll et al (1999) which is also evaluated on
Carroll et al?s test corpus. They report an F-score
5Gildea reports a decrease from 86.1%/86.6% re-
call/precision on labeled bracketing to 80.3%/81% when
going from training and testing on the WSJ to training on the
WSJ and testing on the Brown corpus.
Table 2: LFG F-scores for the 700 WSJ test examples and DR F-scores for the 500 Brown test examples
broken down according to parse quality.
WSJ-LFG all full non-full fragments skimmed skimmed+fragments
% of test set 100 74.7 25.3 20.4 1.4 3.4
upper bound 84.1 88.5 73.4 76.7 70.3 61.3
stochastic 78.6 82.5 69.0 72.4 66.6 56.2
lower bound 75.5 78.4 67.7 71.0 63.0 55.9
Brown-DR all full non-full fragments skimmed skimmed+fragments
% of test set 100 79.6 20.4 20.0 2.0 1.6
upper bound 80.0 84.5 65.4 65.4 56.0 53.5
stochastic 74.0 77.9 61.5 61.5 52.8 50.0
lower bound 71.1 74.8 59.2 59.1 51.2 48.9
of 75.1% for a DO evaluation that ignores predicate
labels, counting only dependencies. Under this mea-
sure, our system achieves 76.1% F-score.
References
Gosse Bouma, Gertjan von Noord, and Robert Malouf.
2000. Alpino: Wide-coverage computational analysis
of Dutch. In Proceedings of Computational Linguis-
tics in the Netherlands, Amsterdam, Netherlands.
Miriam Butt, Tracy King, Maria-Eugenia Nin?o, and
Fre?de?rique Segond. 1999. A Grammar Writer?s Cook-
book. Number 95 in CSLI Lecture Notes. CSLI Publi-
cations, Stanford, CA.
John Carroll, Guido Minnen, and Ted Briscoe. 1999.
Corpus annotation for parser evaluation. In Proceed-
ings of the EACL workshop on Linguistically Inter-
preted Corpora (LINC), Bergen, Norway.
Michael Collins and Nigel Duffy. 2001. Convolution
kernels for natural language. In Advances in Neural
Information Processing Systems 14(NIPS?01), Van-
couver.
Michael Collins. 2000. Discriminative reranking for nat-
ural language processing. In Proceedings of the Seven-
teenth International Conference on Machine Learning
(ICML?00), Stanford, CA.
Richard Crouch, Ronald M. Kaplan, Tracy H. King, and
Stefan Riezler. 2002. A comparison of evaluation
metrics for a broad-coverage stochastic parser. In Pro-
ceedings of the ?Beyond PARSEVAL? Workshop at the
3rd International Conference on Language Resources
and Evaluation (LREC?02), Las Palmas, Spain.
Dan Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of 2001 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), Pittsburgh, PA.
Tony Jebara and Alex Pentland. 1998. Maximum con-
ditional likelihood via bound maximization and the
CEM algorithm. In Advances in Neural Information
Processing Systems 11 (NIPS?98).
Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi,
and Stefan Riezler. 1999. Estimators for stochastic
?unification-based? grammars. In Proceedings of the
37th Annual Meeting of the Association for Computa-
tional Linguistics (ACL?99), College Park, MD.
Mitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz,
Robert MacIntyre, Ann Bies, Mark Ferguson, Karen
Katz, and Britta Schasberger. 1994. The Penn tree-
bank: Annotating predicate argument structure. In
ARPA Human Language Technology Workshop.
John Maxwell and Ron Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Computa-
tional Linguistics, 19(4):571?589.
Thomas Minka. 2001. Algorithms for maximum-
likelihood logistic regression. Department of Statis-
tics, Carnegie Mellon University.
Fernando Pereira and Yves Schabes. 1992. Inside-
outside reestimation from partially bracketed corpora.
In Proceedings of the 30th Annual Meeting of the
Association for Computational Linguistics (ACL?92),
Newark, Delaware.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized Stochastic Modeling of
Constraint-Based Grammars using Log-Linear Mea-
sures and EM Training. In Proceedings of the 38th
Annual Meeting of the Association for Computational
Linguistics (ACL?00), Hong Kong.
Adapting Existing Grammars: The XLE Experience
Ronald M. Kaplan and Tracy Holloway King and John T. Maxwell III
Palo Alto Research Center
Palo Alto, CA 94304 USA
kaplan, thking, maxwell @parc.com
Abstract
We report on the XLE parser and grammar develop-
ment platform (Maxwell and Kaplan, 1993) and de-
scribe how a basic Lexical Functional Grammar for
English has been adapted to two different corpora
(newspaper text and copier repair tips).
1 Introduction
Large-scale grammar development platforms should
be able to be used to develop grammars for a wide
variety of purposes. In this paper, we report on the
the XLE system (Maxwell and Kaplan, 1993), a
parser and grammar development platform for Lex-
ical Functional Grammars. We describe some of the
strategies and notational devices that enable the ba-
sic English grammar developed for the ParGram
project (Butt et al, 1999; Butt et al, 2002) to be
adapted to two corpora with different properties.
1.1 The Corpora
The STANDARD Pargram English grammar covers
the core phenomena of English (e.g., main and sub-
ordinate clauses, noun phrases, adjectives and ad-
verbs, prepositional phrases, coordination; see (Butt
et al, 1999)). We have built two different specialized
grammars on top of this: the EUREKA grammar and
the WSJ grammar.
The EUREKA grammar parses the Eureka cor-
pus of copier repair tips, a collection of documents
offering suggestions for how to diagnose and fix
particular copier malfunctions. These informal and
unedited documents were contributed by copier re-
pair technicians, and the corpus is characterized by
a significant amount of ungrammatical input (e.g.,
typos, incorrect punctuation, telegraphic sentences)
and much technical terminology (1). The goal of
parsing this corpus is to provide input to a semantics
and world-knowledge reasoning application (Ev-
erett et al, 2001).
(1) a. (SOLUTION 27032 70) If exhibiting 10-
132 faults replace the pre-fuser transport
sensor (Q10-130).
b. (SOLUTION 27240 80) 4. Enter into the
machine log, the changes that have been
made.
The WSJ grammar covers the UPenn Wall Street
Journal (WSJ) treebank sentences (Marcus et al,
1994). This corpus is characterized by long sen-
tences with many direct quotes and proper names,
(2a). In addition, for evaluation and training pur-
poses we also parsed a version of this corpus marked
up with labeled brackets and part-of-speech tags, as
in (2b). Riezler et al (2002) report on our WSJ pars-
ing experiments.
(2) a. But since 1981, Kirk Horse Insurance Inc.
of Lexington, Ky. has grabbed a 20% stake
of the market.
b. But since 1981, [NP-SBJ Kirk Horse In-
surance Inc. of Lexington, Ky.] has/VBZ
grabbed/VBN [NP a 20% stake of the mar-
ket].
2 Priority-based Grammar Specialization
The XLE system is designed so that the grammar
writer can build specialized grammars by both ex-
tending and restricting another grammar (in our case
the base grammar is the STANDARD Pargram En-
glish grammar). An LFG grammar is presented to
the XLE system in a priority-ordered sequence of
files containing phrase-structure rules, lexical en-
tries, abbreviatory macros and templates, feature
declarations, and finite-state transducers for tok-
enization and morphological analysis. XLE is ap-
plied to a single root file holding a CONFIGURA-
TION that identifies all the other files containing rel-
evant linguistic specifications, that indicates how
those components are to be assembled into a com-
plete grammar, and that specifies certain parameters
that control how that grammar is to be interpreted.
A key idea is that there can be only one definition
of an item of a given type with a particular name
(e.g., there can be only one NP rule although that sin-
gle rule can have many alternative expansions), and
items in a higher priority file override lower priority
items of the same type with the same name. This set
up is similar to the priority-override scheme of the
earlier LFG Grammar Writer?s Workbench (Kaplan
and Maxwell, 1996).
This arrangement makes it relatively easy to con-
struct a specialized grammar from a pre-existing
standard. The specialized grammar is defined by
a CONFIGURATION in its own root file that speci-
fies the relevant STANDARD grammar files as well
as the new files for the specialized grammar. The
files for the specialized grammar can also contain
items of different types (phrase-structure rules, lex-
ical entries, templates, etc.), and they are ordered
with higher priority than the STANDARD files.
Consider the configuration for the EUREKA gram-
mar. It specifies all of the STANDARD grammar files
as well as its own rule, template, lexicon, and mor-
phology files. A part of this configuration is shown
in (3) (the notationtemplates.lfg are shared by all the
languages? grammars, not just English).
(3) FILES ../standard/english-lexicons.lfg
../standard/english-rules.lfg
../standard/english-templates.lfg
../../common/notationtemplates.lfg
english-eureka-morphconfig
eureka-lexicons.lfg
eureka-rules.lfg
eureka-templates.lfg
This configuration specifies that the EUREKA rules,
templates, and lexical entries are given priority
over the STANDARD items by putting the spe-
cial EUREKA files at the end of the list. Thus, if
the ../standard/english-rules.lfg and eureka-rules.lfg
files both contain a rule expanding the NP category,
the one from the STANDARD file will be discarded in
favor of the EUREKA rule.
In the following subsections, we provide several
illustrations of how simple overriding has been used
for the EUREKA and WSJ grammar extensions.
2.1 Rules
The override convention makes it possible to: add
rules (e.g., for new or idiosyncratic constructions);
delete rules (e.g., to block constructions not found in
the new corpus); and modify rules to allow different
daughter sequences.
Rules may need to be added to allow for corpus-
specific constructions. This is illustrated in the EU-
REKA corpus by the identifier information that pre-
cedes each sentence, as in (1). In order to parse this
substring, a new category (FIELD) was defined with
an expansion that covers the identifier information
followed by the usual ROOT category of the STAN-
DARD grammar. The top-level category is one of
the parameters of a configuration, and the EUREKA
CONFIGURATION specifies that FIELD instead of the
STANDARD ROOT is the start-symbol of the gram-
mar. Thus the EUREKA grammar produces the tree
in (4) and functional-structure in (5) for (1a).
(4) FIELD
LP EURHEAD ID SUB-ID RP ROOT
( SOLUTION 27032 70 )
(5) PRED replace SUBJ, OBJ
SUBJ [ ]
OBJ [ ]
FIELD solution
TIP-ID 27032
SUB-TIP-ID 70
It is unusual in practice to need to delete a rule,
i.e., to eliminate completely the possibility of ex-
panding a given category of the STANDARD gram-
mar. This is generally only motivated when the spe-
cialized grammar applies to a domain where certain
constructions are rarely encountered, if at all. Al-
though there has been no need to delete rules for the
EUREKA and WSJ corpora, the override convention
also provides a natural way of achieving this effect.
For example, topicalization is extremely rare in the
the Eureka corpus and the STANDARD topicalization
rule sometimes introduces parsing inefficiency. This
can be avoided by having the high priority EUREKA
file replace the STANDARD rule with the one in (6).
(6) CPtop .
This vacuous rule expands the CPtop category to the
empty language, the language containing no strings;
so, this category is effectively removed from the
grammar.
Perhaps the most common change is to make
modifications to the behavior of existing rules. The
most direct way of doing this is simply to define a
new, higher priority expansion of the same left-hand
category. Since XLE only allows a single rule for a
given category, the old rule is discarded and the new
one comes into play. The new rule can be arbitrar-
ily different from the STANDARD one, but this is not
typically the case. It is much more common that the
specialized version incorporates most of the behav-
ior of the original, with minor extensions or restric-
tions. One way of producing the modified behavior
is to create a new rule that includes a copy of some
or all of the STANDARD rule?s right side along with
new material, and to give the new definition higher
priority than the old. For example, plurals in the Eu-
reka corpus can be formed by the addition of ?s in-
stead of the usual s, as in (7).
(7) (CAUSE 27416 10) A 7mfd inverter motor ca-
pacitor was installed on an unknown number of
UDH?s.
In order to allow for this, the N rule was rewritten to
allow a PL marker to optionally occur after any N,
as in (8).
(8) N copy of STANDARD N rule
(PL)
As a result of this rule modification, UDH?s in (7)
will have the tree and functional-structure in (9).
(9) a. N
PART PL
UDH ?s
b.
PRED UDH
NUM pl
Copying material from one version to another is
perhaps reasonable for relatively stable and simple
rules, like the N rule, but this can cause maintainabil-
ity problems with complicated rules in the STAN-
DARD grammar that are updated frequently. An al-
ternative strategy is to move the body of the STAN-
DARD N rule to a different rule, e.g., Nbody, which
in turn is called by the N rule in both the STANDARD
and EUREKA grammars. The Nbody category can be
supressed in the tree structure by invoking this rule
as a macro (notationally indicated as @Nbody).
(10) N @Nbody (PL).
Often the necessary modification can be made
simply by redefining a macro that existing rules al-
ready invoke. Consider the ROOT rule, in (11).
(11) ROOT @DECL-BODY @DECL-PUNCT
@INT-BODY @INT-PUNCT
@HEADER .
In the STANDARD grammar, the DECL-PUNCT
macro is defined as in (12a). However, this must
be modified in the EUREKA grammar because the
punctuation is much sloppier and often does not
occur at all; the EUREKA version is shown in (12b).
(12) a. DECL-PUNCT = PERIOD
EXCL-POINT .
b. DECL-PUNCT = ( PERIOD
EXCL-POINT
COLON
SEMI-COLON ).
The modular specifications that macros and tem-
plates provide allow rule behavior to be modified
without having to copy the parts of the rule that do
not change.
XLE also has a mechanism for systemati-
cally modifying the behavior of all rules: the
METARULEMACRO. For example, in order to
parse labeled bracketed input, as in (2b), the WSJ
grammar was altered so that constituents could
optionally be surrounded by the appropriately
labeled brackets. The METARULEMACRO is applied
to each rule in the grammar and produces as output
a modified version of that rule. This is used in
the STANDARD grammar for coordination and to
allow quote marks to surround any constituent. The
METARULEMACRO is redefined for the WSJ to add
the labeled bracketing possibilities for each rule, as
shown in (13).
(13) METARULEMACRO( CAT BASECAT RHS) =
LSB LABEL[ BASECAT] CAT RSB
copy of STANDARD coordination
copy of STANDARD surrounding quote .
The CAT, BASECAT, and RHS are arguments to
the METARULEMACRO that are instantiated to dif-
ferent values for each rule. RHS is instantiated to
the right-hand side of the rule, i.e., the rule expan-
sion. CAT and BASECAT are two ways of repre-
senting the left-hand side of the rule. For simple cat-
egories the CAT and BASECAT are the same (e.g.
NP for the NP rule). XLE also allows for complex
category symbols to specialize the expansion of par-
ticular categories in particular contexts. For exam-
ple, the VP rule is parameterized for the form of its
complement and its own form, so that VP[perf,fin]
is one of the complex VP categories. When the
METARULEMACRO applies to rules with complex
left-side categories, CAT refers to the category in-
cluding the parameters and the BASECAT refers to
the category without the parameters. For the VP ex-
ample, CAT is VP[perf,fin] and BASECAT is VP.
In the definition in (13), LSB and RSB parse the
brackets themselves, while the LABEL[ BASECAT]
parses the label in the bracketing and matches it to
the label in the tree (NP in (2b)); the consituent itself
is the CAT. Thus, a label-bracketed NP is assigned
the structure in (14).
(14) NP
LSB LABEL[NP] NP RSB
[ NP-SBJ Kirk Horse ]
These examples illustrate how the prioritized re-
definition of rules and macros has enabled us to in-
corporate the STANDARD rules in grammars that are
tuned to the special properties of the EUREKA and
WSJ corpora.
2.2 Lexical Entries
Just as for rules, XLE?s override conventions make
it possible to: add new lexical items or new part-of-
speech subentries for existing lexical items; delete
lexical items; and modify lexical items. In addition
to the basic priority overrides, XLE provides for
?edit lexical entries? (Kaplan and Newman, 1997)
that give finer control over the construction of the
lexicon. Edit entries were introduced as a way of rec-
onciling information from lexical databases of vary-
ing degrees of quality, but they are also helpful in
tailoring a STANDARD lexicon to a specialized cor-
pus. When working on specialized corpora, such as
the Eureka corpus, modifications to the lexicon are
extremely important for correctly handling techni-
cal terminology and eliminating word senses that are
not appropriate for the domain.
Higher-priority edit lexical entries provide for op-
erators that modify the definitions found in lower-
priority entries. The operators can: add a subentry
(+); delete a subentry ( ); replace a subentry (!);
or retain existing subentries (=). For example, the
STANDARD grammar might have an entry for button
as in (15).
(15) button !V @(V-SUBJ-OBJ %stem);
!N @(NOUN %stem);
ETC.
However, the EUREKA grammar might not need the
V entry but might require a special partname N en-
try. Assuming that the EUREKA lexicons are given
priority over the STANDARD lexicons, the entry in
(16) would accomplish this.
(16) button V ;
+N @(PARTNAME %stem);
ETC.
Note that the lexical entries in (15) and (16) end with
ETC. This is also part of the edit lexical entry sys-
tem. It indicates that other lower-priority definitions
of that lexical item will be retained in addition to
the new entries. For example, if in another EUREKA
lexicon there was an adjective entry for button with
ETC, the V, N, and A entries would all be used. The
alternative to ETC is ONLY which indicates that only
the new entry is to be used. In our button example, if
an adjective entry was added with ONLY, the V and
N entries would be removed, assuming that the ad-
jective entry occurred in the highest priority lexicon.
This machinery provides a powerful tool for build-
ing specialized lexicons without having to alter the
STANDARD lexicons.
The EUREKA corpus contains a large number of
names of copier parts. Due to their particular syn-
tax and to post-syntactic processing requirements, a
special lexical entry is added for each part name. In
addition, the regular noun parse of these entries is
deleted because whenever they occur in the corpus
they are part names. A sample lexical is shown in
(17); the ? is the escape character for the space.
(17) separator? finger
!PART-NAME @(PART-NAME %stem);
N;
ETC.
The first line in (17) states that separator finger can
be a PART NAME and when it is, it calls a template
PART-NAME that provides relevant information for
the functional-structure. The second line removes
the N entry, if any, as signalled by the before the
category name.
Because of the non-context free nature of Lexical
Functional Grammar, it sometimes happens that ex-
tensions in one part of the grammar require a cor-
responding adjustment in other rules or lexical en-
tries. Consider again the EUREKA ?s plurals. The
part-name UDH is singular when it appears with-
out the ?s and thus the morphological tag +Sg is ap-
pended to it. In the STANDARD grammar, the tag +Sg
has a lexical entry as in (18a) which states that +Sg is
of category NNUM and assigns sg to its NUM. How-
ever, if this is used in the EUREKA grammar, the sg
NUM specification will clash with the pl NUM spec-
ification when UDH appears with ?s, as seen in (7).
Thus, a new entry for +Sg is needed which has sg
as a default value, as in (18b). The first line of (18b)
states that NUM must exist but does not specify a
value, while the second line optionally supplies a sg
value to NUM; when the ?s is used, this option does
not apply since the form already has a pl NUM value.
(18) a. +Sg NNUM ( NUM)=sg
b. +Sg NNUM ( NUM)
(( NUM)=sg)
3 Tokenizing and Morphological Analysis
Tokenization and morphological analysis in XLE
are carried out by means of finite state transductions.
The STANDARD tokenizing transducer encodes the
punctuation conventions of normal English text,
which is adequate for many applications. However,
the Eureka and WSJ corpora include strings that must
be tokenized in non-standard ways. The Eureka part
identifiers have internal punctuation that would nor-
mally cause a string to be broken up (e.g. the hyphen
in PL1-B7), and the WSJ corpus is marked up with
labeled brackets and part-of-speech tags that must
also receive special treatment. An example of the
WSJ mark-up is seen in (19).
(19) [NP-SBJ Lloyd?s, once a pillar of the world
insurance market,] is/VBZ being/VBG
shaken/VBN to its very foundation.
Part-of-speech tags appear in a distinctive format,
beginning with a / and ending with a , with the in-
tervening material indicating the content of the tag
(VBZ for finite 3rd singular verb, VBG for a progres-
sive, VBN for a passive, etc.). The tokenizing trans-
ducer must recognize this pattern and split the tags
off as separate tokens. The tag-tokens must be avail-
able to filter the output of the morphological ana-
lyzer so that only verbal forms are compatible with
the tags in this example and the adjectival reading of
shaken is therefore blocked.
XLE tokenizing transducers are compiled from
specifications expressed in the sophisticated Xerox
finite state calculus (Beesley and Karttunen, 2002).
The Xerox calculus includes the composition, ig-
nore, and substitution operator discussed by Kaplan
and Kay (1994) and the priority-union operator of
Kaplan and Newman (1997). The specialized tok-
enizers are constructed by using these operators to
combine the STANDARD specification with expres-
sions that extend or restrict the standard behavior.
For example, the ignore operator is applied to allow
the part-of-speech information to be passed through
to the morphology without interrupting the standard
patterns of English punctuation.
XLE also allows separately compiled transduc-
ers to be combined at run-time by the operations
of priority-union, composition, and union. Priority-
union was used to supplement the standard morphol-
ogy with specialized ?guessing? transducers that ap-
ply only to tokens that would otherwise be unrec-
ognized. Thus, a finite-state guesser was added to
identify Eureka fault numbers (09-425), adjustment
numbers (12-23), part numbers (606K2100), part list
numbers (PL1-B7), repair numbers (2.4), tag num-
bers (P-102), and diagnostic code numbers (dC131).
Composition was used to apply the part-of-speech
filtering transducer to the output of the morpholog-
ical analyzer, and union provided an easy way of
adding new, corpus-specific terminology.
4 Optimality Marks
XLE supports a version of Optimality Theory (OT)
(Prince and Smolensky, 1993) which is used to rank
an analysis relative to other possible analyses (Frank
et al, 2001). In general, this is used within a specific
grammar to prefer or disprefer a construction. How-
ever, it can also be used in grammar extensions to
delete or include rules or parts of rules.
The XLE implementation of OT works as fol-
lows.1 OT marks are placed in the grammar and are
associated with particular rules, parts of rules, or
lexical entries. These marks are then ranked in the
grammar CONFIGURATION. In addition to a simple
ranking of constraints which states that a construc-
tion with a given OT mark is (dis)prefered to one
1The actual XLE OT implementation is more complicated
than this, allowing for UNGRAMMATICAL and STOPPOINT
marks as well. Only OT marks that are associated with NO-
GOOD are of interest here. For a full description, see (Frank et
al., 2001).
without it, XLE allows the marks to be specified as
NOGOOD. A rule or rule disjunct which has a NO-
GOOD OT mark associated with it will be ignored
by XLE. This can be used for grammar extensions
in that it allows a standard grammar to anticipate the
variations required by special corpora without using
them in normal circumstances.
Consider the example of the EUREKA ?s plurals
discussed in section 2.1. Instead of rewriting the N
rule in the EUREKA grammar, it would be possible
to modify it in the STANDARD grammar and include
an OT mark, as in (20).
(20) N original STANDARD N rules
(PL: @(OT-MARK EUR-PLURAL)).
The CONFIGURATION files of the STANDARD and
EUREKA grammars would differ in that the STAN-
DARD grammar would rank the EUR-PLURAL OT
mark as NOGOOD, as in (21a), while the EUREKA
grammar would simply not rank the mark, as in
(21b).
(21) a. STANDARD optimality order:
EUR-PLURAL NOGOOD
b. EUREKA optimality order:
NOGOOD
Given the OT marks, it would be possible to have
one large grammar that is specialized by different
OT rankings to produce the STANDARD, EUREKA,
and WSJ variants. However, from a grammar writ-
ing perspective this is not a desirable solution be-
cause it becomes difficult to keep track of which
constructions belong to standard English and are
shared among all the specializations and which are
corpus-specific. In addition, it does not distinguish a
core set of slowly changing linguistic specifications
for the basic patterns of the language, and thus does
not provide a stable foundation that the writers of
more specialized grammars can rely on.
5 Maintenance with Grammar Extensions
Maintenance is a serious issue for any large-scale
grammar development activity, and the maintenance
problems are compounded when multiple versions
are being created perhaps by several different gram-
mar writers. Our STANDARD grammar is now quite
mature and covers all the linguistically significant
constructions and most other constructions that we
have encountered in previous corpus analysis. How-
ever, every now and then, a new corpus, even a spe-
cialized one, will evidence a standard construction
that has not previously been accounted for. If spe-
cialized grammars were written by copying all the
STANDARD files and then modifying them, the im-
plementation of new standard constructions would
tend to appear only in the specialized grammar. Our
techniques for minimizing the amount of copying
encourages us to implement new constructions in the
STANDARD grammar and this makes them available
to all other specializations.
If a new version of a rule for a specialized gram-
mar is created by copying the corresponding STAN-
DARD rule, changes later made to the special rule
will not automatically be reflected in the STANDARD
grammar, and vice versa. This is the desired behav-
ior when adding unusual, corpus-specific construc-
tions. However, if the non-corpus specific parts of
the new rule are modified, these modifications will
not migrate to the STANDARD grammar. To avoid
this problem, the smallest rule possible should be
modified in the specialized grammar, e.g., modify-
ing the N head rule instead of the entire NP. For
this reason, having highly modularized rules and us-
ing macros and templates helps in grammar mainte-
nance both within a grammar and across specialized
grammar extensions.
As seen above, the XLE grammar development
platform provides a number of mechanisms to allow
for grammar extensions without altering the core
(STANDARD) grammar. However, there are still ar-
eas that could use improvement. For example, as
mentioned in section 2, the CONFIGURATION file
states which other files the grammar includes and
how they are prioritized. The CONFIGURATION con-
tains other information such as declarations of the
governable grammatical functions, the distributive
features, etc. As this information rarely changes
with grammar extensions, it would be helpful for
an extension configuration to incorporate by refer-
ence such additional parameters of the STANDARD
configuration. Currently these declarations must be
copied into each CONFIGURATION.
6 Discussion and Conclusion
As a result of the strategies and notational devices
outlined above, our specialized grammars share
substantial portions of the pre-existing STANDARD
grammar. The statistics in table (22) give an indica-
tion of the size of the STANDARD grammar and of
the additional material required for the EUREKA and
WSJ specializations. As can be seen from this table,
the specialized grammars require a relatively small
number of rules compared to the rules in the STAN-
DARD grammar. The number of lines that the rules
and lexical entries take up also provides a measure of
the relative size of the specifications. The WSJ lexi-
cons include many titles and proper nouns that may
ultimately be moved to the STANDARD files. The ta-
ble also shows the number of files called by the CON-
FIGURATION, as another indication of the size of the
specifications. This number is somewhat arbitrary as
separate files can be combined into a single multi-
sectioned file, although this is likely to reduce main-
tainability and readability.
(22)
STANDARD EUREKA WSJ
rules 310 32 14
lines:
rules 6,539 425 894
lexicons 44,879 5,565 15,135
files 14 5 8
The grammars compile into a collection of finite-
state machines with the number of states and arcs
listed in table (23). The WSJ grammar compiles into
the largest data structures, mainly because of its abil-
ity to parse labeled bracketed strings and part-of-
speech tags, (2b). This size increase is the result of
adding one disjunct in the METARULEMACRO and
hence reflects only a minor grammar change.
(23)
STANDARD EUREKA WSJ
states 4,935 5,132 8,759
arcs 13,268 13,639 19,695
In sum, the grammar specialization system used
in XLE has been quite sucessful in developing cor-
pus specific grammars using the STANDARD English
grammar as a basis. A significant benefit comes from
being able to distinguish truly unusual constructions
that exist only in the specialized grammar from those
that are (or should be) in the STANDARD grammar.
This allows idiosyncratic information to remain in a
specialized grammar while all the specialized gram-
mars benefit from and contribute to the continuing
development of the STANDARD grammar.
References
K. Beesley and L. Karttunen. 2002. Finite-State
Morphology: Xerox Tools and Techniques. Cam-
bridge University Press. To Appear.
M. Butt, T.H. King, M.-E. Nin?o, and F. Segond.
1999. A Grammar Writer?s Cookbook. CSLI
Publications, Stanford, CA.
M. Butt, H. Dyvik, T.H. King, H. Masuichi, and
C. Rohrer. 2002. The parallel grammar project.
In Proceedings of COLING 2002. Workshop on
Grammar Engineering and Evaluation.
J. Everett, D. Bobrow, R. Stolle, R. Crouch,
V. de Paiva, C. Condoravdi, M. van den Berg,
and L. Polanyi. 2001. Making ontologies work
for resolving redundancies across documents.
Communications of the ACM, 45:55?60.
A. Frank, T. H. King, J. Kuhn, and J. T. Maxwell III.
2001. Optimality theory style constraint rank-
ing in large-scale LFG grammars. In Peter Sells,
editor, Formal and Empirical Issues in Optimal-
ity Theoretic Syntax. CSLI Publications, Stanford,
CA.
R. Kaplan and M. Kay. 1994. Regular models of
phonological rule systems. Computational Lin-
guistics, 20:331?378.
R. Kaplan and J. Maxwell. 1996. LFG Gram-
mar Writer?s Workbench. System documentation
manual; available on-line at PARC.
R. Kaplan and P. Newman. 1997. Lexical resource
conciliation in the Xerox Linguistic Environment.
In Proceedings of the ACL Workshop on Com-
putational Environments for Grammar Develop-
ment and Engineering.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. Mac-
Intyre, A. Bies, M. Ferguson, K. Katz, and
B. Schasberger. 1994. The Penn treebank: An-
notative predicate argument structure. In ARPA
Human Language Technology Workshop.
J. Maxwell and R. Kaplan. 1993. The interface be-
tween phrasal and functional constraints. Compu-
tational Lingusitics, 19:571?589.
A. Prince and P. Smolensky. 1993. Optimality the-
ory: Constraint interaction in generative gram-
mar. RuCCS Technical Report #2, Rutgers Uni-
versity.
S. Riezler, T.H. King, R. Kaplan, D. Crouch, J. T.
Maxwell, III, and M. Johnson. 2002. Parsing
the Wall Street Journal using a lexical-functional
grammar and discriminative estimation tech-
niques. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics,
University of Pennsylvania.
LFG Generation by Grammar Specialization
Ju?rgen Wedekind?
University of Copenhagen
Ronald M. Kaplan??
Nuance Communications, Inc.
This article describes an approach to Lexical-Functional Grammar (LFG) generation that is
based on the fact that the set of strings that an LFG grammar relates to a particular acyclic
f-structure is a context-free language. We present an algorithm that produces for an arbitrary
LFG grammar and an arbitrary acyclic input f-structure a context-free grammar describing
exactly the set of strings that the given LFG grammar associates with that f-structure. The
individual sentences are then available through a standard context-free generator operating
on that grammar. The context-free grammar is constructed by specializing the context-free
backbone of the LFG grammar for the given f-structure and serves as a compact representation
of all generation results that the LFG grammar assigns to the input. This approach extends
to other grammatical formalisms with explicit context-free backbones, such as PATR, and also
to formalisms that permit a context-free skeleton to be extracted from richer specifications. It
provides a general mathematical framework for understanding and improving the operation of a
family of chart-based generation algorithms.
1. Introduction
Algorithms providing compact representations of alternative syntactic analyses have
been the state-of-the-art in parsing for many years. For context-free grammars, for
example, the well-known chart parsing algorithms have been used for more than
four decades. These assign to a sentence not just one possible analysis but a chart
that compactly represents all possible syntactic analyses. Algorithms have also been
developed that extend packing to the functional specifications of unification grammars
by producing compact representations of feature-structure ambiguities as well. One
that is pertinent to (but not restricted to) Lexical-Functional Grammar (LFG) is the con-
texted constraint satisfaction method developed by Maxwell and Kaplan (1991). These
algorithms lead to better average time performance because they carefully manage the
ambiguities that are rampant in natural language. They work by dividing the parsing
problem into two phases, a recognition or satisfiability phase that creates the compact
representation and determines whether there is at least one parse, and an enumeration
phase in which the alternative parses are produced one by one. Parsing performance
? Center for Language Technology, University of Copenhagen, Njalsgade 140, 2300 Copenhagen S,
Denmark. E-mail: jwedekind@hum.ku.dk.
?? Nuance Communications, Inc., 1198 East Arques Avenue, Sunnyvale, CA 94085, USA.
E-mail: Ronald.Kaplan@nuance.com.
Submission received: 24 March 2011; revised submission received: 31 October 2011; accepted for publication:
28 December 2011.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
is typically identified with the complexity of the first phase (e.g., the cubic bound for
context-free parsing), because the collection of all parses can be delivered to a client ap-
plication merely by presenting the compact representation. A client may be able to select
a limited number of particularly desirable parses, perhaps the smallest or the most prob-
able, without doing a full enumeration (Johnson and Riezler 2002; Kaplan et al 2004).
Lang (1994) gives a clear formal characterization of the first phase of context-
free chart parsing.1 He observes that the recognition problem consists of finding the
intersection of the language of the grammar with the input string, and then testing to see
whether that intersection is empty. Many language classes are closed under intersection
with a regular set, and the result of the intersection of a language L(G) with a regular
language ? is describable as a specialization G? of G that assigns to all and only the
strings in ? effectively the same parse trees as G would assign. Lang argues that a chart
for an input string s (a trivial regular language) and a context-free grammar G can be
regarded as a specialization Gs of G that derives either the empty language (if s does
not belong to L(G)) or a language consisting of just that input. In this view a parsing
chart/grammar is a representation that makes it possible to enumerate all the derivation
trees of the string, guaranteeing that each tree can be produced in a backtrack-free way
in time proportional to its size. This guarantee holds even for an infinitely ambiguous
string: It would take forever to enumerate all valid derivations, but any particular one
can be read out in linear time. The procedure for tree enumeration follows directly
from the standard context-free generation algorithm applied to the grammar Gs.
The generation problem for LFG and other description-based grammatical for-
malisms can also be viewed from this perspective. Several algorithms have been pro-
posed for generation that avoid redundant recomputation by storing intermediate
processing results in a chart-like auxiliary data structure (e.g., Shieber 1988; Kay 1996;
Shemtov 1997; Neumann 1998; Carroll et al 1999; Moore 2002; Carroll and Oepen 2005;
Cahill and van Genabith 2006; White 2006; de Kok and van Noord 2010). Most of them
can be construed as having a first phase that provides a compact representation for
alternative results, in this case for the strings that the grammar provides for a given
functional or semantic input. The individual generated strings are then produced by an
enumeration procedure operating on this compact representation.
In this article we observe that the edges of a generation chart can be interpreted as
rules of a specialized context-free grammar, just as in Lang?s (1994) characterization of
parsing. We present a generation algorithm that specializes the context-free backbone of
a given LFG grammar to a grammar that describes exactly the strings that the LFG gram-
mar relates to a given acyclic f-structure. Derivations of the resulting grammar simulate
all and only those derivations of the LFG grammar whose derived strings are assigned
to that input.2 Thus the generated string set is a context-free language compactly repre-
sented by the specialized grammar, and the individual members of that language can be
enumerated, just as for parsing, by using standard context-free generation algorithms.
Our approach can be seen as a generalization and formalization of other chart-based
generation algorithms, producing all and only correct outputs for larger combinations
of grammars and inputs. It extends to unification grammars with explicit context-free
backbones, such as PATR (Shieber et al 1983), and also to formalisms that permit a
context-free skeleton to be extracted from richer specifications. But it does not extend
1 Dymetman (1997) extends this characterization to unification grammars.
2 The word ?derivation? here and in the following is used only to characterize the notion of
well-formedness in LFG and is not meant to undermine the contrast between LFG and
conventional transformational approaches to syntax.
868
Wedekind and Kaplan LFG Generation by Grammar Specialization
Figure 1
The components of an LFG representation: string, constituent structure, functional structure.
to cyclic input structures because, as we will show by example, an LFG grammar
might relate to a cyclic structure a set of strings that is not context-free. Because acyclic
structures are normally assumed to be the only f-structures that are motivated for
linguistic analysis (Kaplan and Bresnan 1982), this restriction does not seem to limit
the applicability of our algorithm for natural language generation.
We begin with some background so that we can make the problem and its solution
more explicit. Along with many other description-based grammar formalisms, an LFG
grammar G assigns to every string s in its language at least one f-structure. This situation
can be characterized in terms of a derivation relation ?G, defined as follows:
(1) ?G(s,F) iff G assigns to the string s the f-structure F
In the LFG approach a sentence s and its f-structure F are not directly related. Their
relation is mediated by a valid c-structure for s (Kaplan 1995). The arrangement of the
three components of an LFG representation is illustrated in Figure 1. This representation
is derivable by a grammar that includes the annotated (nonterminal) rules in (2a?c)
and lexical expansions in (2d?f). Annotated lexical c-structure rules are just notational
variants of traditional LFG lexical entries.
(2) a. S ? NP VP
(? SUBJ) = ? ? = ?
b. NP ? DET N
? = ? ? = ?
c. VP ? V
? = ?
d. DET ? a
(? SPEC) = INDEF
(? NUM) = SG
e. N ? student
(? PRED) = 'STUDENT'
(? NUM) = SG
f. V ? fell
(? PRED) = 'FALL?(SUBJ)?'
(? TENSE) = PAST
In accordance with the basic architecture of LFG, an LFG grammar provides a set of
licensing conditions that determine grammatical representations by descriptive, model-
based rather than procedural methods. The well-formedness of the representation in
Figure 1 with respect to the grammar in (2) is thus characterized as follows.
The c-structure is valid or well-formed because we can assign to each nonterminal
node a grammar rule that licenses or justifies the local mother?daughters configuration
constituted by the node and its immediate daughters. If we assume that the c-structure
of Figure 1 consists of the nodes root,n1, ..,n8 and these nodes are related and labeled
as depicted in Figure 2, then the rule-mapping ? that justifies the c-structure is given
by (3).
(3) ?root = (2a), ?n1 = (2b), ?n2 = (2c), ?n3 = (2d), ?n4 = (2e), ?n5 = (2f )
869
Computational Linguistics Volume 38, Number 4
Sroot
NPn1 VPn2
DETn3 Nn4 Vn5
an6 studentn7 felln8
Figure 2
The c-structure of Figure 1 with explicitly specified nodes.
A description of the f-structure (called the f-description) for this tree and rule-
mapping is constructed by instantiating the annotations of all justifying rules in the
following way. For each rule justifying a local mother?daughters configuration, all
occurrences of the ? symbol (called a metavariable) in the functional annotations of
the daughters are replaced by the mother node, and for each of the daughter cat-
egories, all occurrences of the ? metavariable in its annotations are replaced by the
corresponding daughter node.3 Thus, the ? of the annotations on a daughter category
of a rule and the ? of the annotations of the rule that further expands that category
are always instantiated with the same node. The complete f-description is the union
of the instantiated descriptions of all the justifying rules. The f-description obtained
from the c-structure in Figure 1 and the rules of the justifying mapping in (3) is given
in (4).
(4) ?
?
?
?
?
?
?
?
?
?
?
(root SUBJ) = n1, root = n2,
n1 = n3,n1 = n4,
n2 = n5,
(n3 SPEC) = INDEF, (n3 NUM) = SG,
(n4 PRED) = 'STUDENT', (n4 NUM) = SG,
(n5 PRED) = 'FALL?(SUBJ)?', (n5 TENSE) = PAST
?
?
?
?
?
?
?
?
?
?
?
The f-structure in Figure 1 is associated with the given c-structure because it satisfies
the f-description in (4), and furthermore, it is the unique minimal solution for this
description. In general, LFG requires the f-description of a grammatical sentence to be
satisfiable and thus to have at least one model. Each such satisfying model consists of
a universe and an interpretation function that assigns unary (partial) functions to the
attributes (SUBJ, NUM, SPEC, etc.) and elements of the universe to the atomic feature
values (SG, INDEF, etc.), as well as to the nodes in the description. Among the models
satisfying a given f-description there is an (up to isomorphism unique) minimal model,
one that is not properly subsumed by other satisfying models.4 This minimal model
represents the f-description?s minimal solution, the one from which the f-structure
for the sentence is obtained. Conventional attribute?value matrices where the nodes
(or node numbers) are attached to the left brackets are LFG-typical representations of
3 Our instantiation procedure uses the nodes themselves instead of the related f-structure variables of
Kaplan and Bresnan (1982) or the more complex ?-terms of Kaplan (1995). This is mathematically
equivalent to the other representations but simplifies our illustrations.
4 For some sentences and some grammars (e.g., those involving functional uncertainty) there are
f-descriptions with several non-isomorphic minimal models. As we discuss in a subsequent article
(Wedekind and Kaplan forthcoming), these also lie within the bounds of our context-free construction.
870
Wedekind and Kaplan LFG Generation by Grammar Specialization
exactly such minimal models. The attribute?value matrix representation of the minimal
model of the f-description (4) is given in (5).
(5) root
n2
n5
?
?
?
?
?
?
?
SUBJ
n1
n3
n4
?
?
PRED 'STUDENT'
NUM SG
SPEC INDEF
?
?
PRED 'FALL?(SUBJ)?'
TENSE PAST
?
?
?
?
?
?
?
The solution in (5) is converted to the f-structure representation in Figure 1 by
removing the node labels that record the relation of the f-structure to the c-structure.
From a formal point of view, an f-structure is obtained from the minimal model of an
f-description by restricting its interpretation function to the attributes and atomic fea-
ture values of the grammar, thus disregarding the nodes and their interpretation.5
We now turn to the generation problem. A generator for G provides for any given
f-structure F the set of strings that are related to it by the grammar:
(6) GenG(F) = {s | ?G(s,F)}
The algorithm presented in this article accomplishes the generation task by pro-
ducing a context-free grammar for GenG(F), for a given LFG grammar G and any acyclic
input f-structure F.
The abstract generator characterization in (6) is of course dual to the one for a parser
for G, since a parser produces for any given terminal string s the set of f-structures that
are assigned to it by G:
(7) ParG(s) = {F | ?G(s,F)}
For parsing, the Kaplan and Bresnan (1982) proscription of nonbranching dominance
chains guarantees that ParG(s) will contain only a finite number of f-structures, but this
condition does not ensure the finiteness of the set of strings GenG(F) that are related to
an f-structure F. This is illustrated by the simple grammar in (8):
(8) S ? a S b
? = ?
S ? c
(? H?) = V?
S ? a b
(? H) = V
This generates for the input (9)
(9) [H V]
the infinite context-free language {an bn | 1 ? n}.
From a cognitive point of view it seems unrealistic that the number of sentences
that a natural language grammar relates to an f-structure is infinite. As a minimum,
there should be some relationship that bounds the size of the c-structure of a sentence
5 Note that the interpretation of the node constants that we restrict out of the minimal models can be seen
to represent the structural correspondence function that maps individual nodes of the c-structure tree
into elements of the f-structure (cf. Kaplan 1995).
871
Computational Linguistics Volume 38, Number 4
by the size of the f-structures associated with it. Such a structural relationship would
then force the related sentences to form a finite set. Studies to determine intuitively
plausible restrictions are rather scarce, however, and proposals for such restrictions are
not yet generally accepted. It is thus still an open question whether grammars of actual
natural languages satisfy the particular resource-boundedness restrictions on which
termination of some existing chart-based generators depends.
Even if only finite sets of sentences are related to the f-structures, these sets might
still be very large. Experiments with a broad-coverage German LFG grammar (Dipper
2003; Rohrer and Forst 2006) have shown that, because of the scrambling that German
allows, a given f-structure might be related to a huge set of long sentences.6 This has
consequences at least for those approaches that assume the output of generation to be
a word lattice (Langkilde and Knight 1998) or a finite-state machine representing the
(finite) set of all generated sentences. A lattice can represent a large collection of strings
compactly only if they are characterized by independent sets of alternative substrings.
Scrambling languages, however, have alternative substrings that reappear in different
positions with complex cooccurrence dependencies and therefore cannot be shared in a
lattice representation (see Langkilde [2000] for discussion). Our context-free grammars
(and also Langkilde?s [2000] and Knight and Langkilde?s [2000] forest representations)
offer a much more compact encoding under these circumstances, and their structure
and formal properties are as well understood as lattices and finite-state machines.
Our approach might also be more appropriate than existing chart-based approaches
for optimality-theoretic generation (Kuhn 2001, 2002, 2003). An optimality-theoretic
LFG system consists of two components: a universal LFG grammar and a language-
specifically ordered set of violable constraints (Bresnan 2000). The universal LFG gram-
mar is used to produce the candidate space of possible analyses (consisting of the
c-structure/f-structure pairs that are derivable by the grammar). The optimal and thus
grammatical analyses are those candidates that violate the fewest constraints. A tech-
nical problem comes from the fact that the universal grammar by design may assign
an infinite number of c-structures and string realizations to a given f-structure, and the
optimal outputs can be identified only by evaluating all of these against the collection
of constraints. Our context-free characterization provides a finite evaluation procedure
even for an infinite candidate space. By virtue of the pumping lemma for context-free
languages (Bar-Hillel, Perles, and Shamir 1961; see also Hopcroft and Ullman 1979) we
can enumerate the c-structure trees assigned to an input f-structure one by one in order
of increasing depth. Because the number of constraint violations increases beyond a
certain number of recursive category expansions, the optimal results from the infinite
space can be chosen after examining only a finite number of relatively small structures
(see Kuhn [2003] for details).
Similar to Lang?s approach to parsing (see also Billot and Lang 1989), we provide
a general framework encompassing all forms of chart generation in a single formalism.
This is because existing chart-based generators can be understood as concrete but some-
how restricted algorithm/datastructure implementations of our context-free grammar
construction. These restrictions may lead them to produce incorrect outputs in some
situations. Because we show the correctness of the output grammar for unrestricted
6 The German grammar was developed as part of the Parallel Grammar project (ParGram), a research
and development consortium that has produced large-scale LFG grammars for several languages (Butt
et al 1996, 2002). These grammars are developed on the XLE system, a high-performance platform for
LFG parsing and generation. More information on the ParGram project and XLE can be found at:
http://pargram.b.uib.no/.
872
Wedekind and Kaplan LFG Generation by Grammar Specialization
LFG grammars, our framework allows us to examine, compare, and improve on existing
chart-based generation techniques.
The organization of this article is as follows. In the next section we define the
fundamental formal objects of LFG theory and the relevant relationships among them.
Section 3 is the technical core of the article. There we present and prove the correctness
of the context-free grammar-construction algorithm for LFG grammars with arbitrary
equational constraints and acyclic input f-structures. The grammar construction ab-
stracts away from specific details of data structure and computational strategy not es-
sential to the mathematical argument. Performance and computational strategy are then
briefly considered in Section 4, and Section 5 compares our approach to other generation
algorithms. In Section 6 we identify a fundamental limitation of our approach, demon-
strating that the context-free property does not hold for elementary equational con-
straints if the input f-structure contains cycles. On the other hand, if the input is acyclic,
the basic context-free construction can be extended beyond simple equations to the
additional descriptive devices proposed by Kaplan and Bresnan (1982) and still in
common use. This is shown in Section 7. The last section highlights some additional
consequences of this approach.
The present article elaborates on ideas that we first presented in Kaplan and
Wedekind (2000). In that paper we outlined a context-free grammar construction for
a subclass of LFG grammars with restricted functional annotations and single-rooted
input structures. Here we consider a more general class of grammars and inputs that
requires a more rigorous mathematical analysis.
2. Preliminaries
We start with a formal characterization of LFG grammars with equational statements.
Let V? denote the set of all finite strings over V. An LFG grammar G over a set ? of
attribute and value symbols is defined as follows:
Definition 1
An LFG grammar G (over attribute?value set ?) is a 4-tuple (N,T, S,R) where N is a
finite set of nonterminal categories, T is a finite set of terminal symbols, S ? N is the
root category, and R is a finite set of annotated productions of the form
A ? X1 .. Xm
D1 Dm
with A ? N and X1..Xm ? (N ? T)?. (Note that R might contain -productions, al-
though these do not appear in most current linguistic descriptions.) Each annotated
description Dj (j = 1, ..,m) is a (possibly empty) finite set of equalities between expres-
sions of the form (? ?), (? ?), or v where v is a value of ? and ? is a possibly empty
sequence of attributes of ?. When ? is empty, (? ?), (? ?) are equivalent to ? and ?,
respectively.7
7 Note that this definition permits equations containing terms of the form (? ?), with ? nonempty, and that
it does not require ? and ? to occur in the annotation of each category. It thus allows for grammars that
assign to sentences multiply rooted f-structures or f-structures consisting of totally unconnected parts.
We take the ?f-structure of a sentence? to be the collection of all elements that correspond to c-structure
nodes, even those that are not accessible from the root node?s f-structure.
873
Computational Linguistics Volume 38, Number 4
We next define how instantiated descriptions are obtained from the rules by sub-
stituting for the ? and ? metavariables elements drawn from a collection of terms.
C-structure nodes are included among the terms, but later on we also make use of addi-
tional elements. We define a function Inst that assigns to each m-ary rule r, term t, and
term sequence t1..tm the instantiated description that is obtained from the annotations
of r and the terms by substituting t for ? and tj for ? in the annotations of all j = 1, ..,m
daughters. In the following definition we use the (more compact) linear rule notation
A ? (X1,D1)..(Xm,Dm) that we prefer in more formal specifications.
Definition 2
Let r be an m-ary LFG rule A ? (X1,D1)..(Xm,Dm) (m ? 0) and ? = (t, t1..tm) be a pair
of a term and a sequence of terms of length m. Then the instantiated description that
results from r and ? is given by
Inst(r, ?) =
m
?
j=1
Inst(Dj, t, tj)
where Inst(Dj, t, tj) is the instantiated description produced by substituting t for all
occurrences of ? in Dj and substituting tj for all occurrences of ? in Dj.
The derivation relation for LFG grammars (?G) is defined as already described
informally in the previous section. This is based on context-free derivation trees. Let
us assume that root is the root node of any c-structure c, and that dts is a function that
assigns to each nonterminal node n of c the sequence of its immediate daughters (dts(n)).
Context-free derivations are then defined as follows:
Definition 3
A labeled tree c and a rule-mapping ? from the nonterminal nodes of c into the rules of
context-free grammar G is a context-free derivation of string s from nonterminal B in
G iff
(i) the label (category) of root is B,
(ii) the yield is s,
(iii) for each nonterminal node n with label A and dts(n) = n1..nm with
labels X1, ..,Xm, respectively, ?n = A ? X1..Xm.
When we informally described LFG derivations, we pointed out that we obtain the
f-structure from the (up to isomorphism) unique minimal model of the f-description
by restricting it to the attribute?value set ?. This is formalized in the following def-
inition by requiring the f-structure to be isomorphic (?=) to M|?, the restriction to ?
of a minimal model M of the derived f-description. The effect of the isomorphism
is to abstract away from the particular properties of different f-structure models that
have no linguistic significance. Moreover, because we operate on an arbitrary mem-
ber of the class of isomorphic structures without regard to any of its accidental or
nonsignificant properties, we know that our analysis applies to all members of the
class.
874
Wedekind and Kaplan LFG Generation by Grammar Specialization
Definition 4
A labeled tree c and a mapping ? from the nonterminal nodes of c into R is an LFG deri-
vation of string s with functional description FD and f-structure F in LFG grammar G iff
(i) the label of root is S,
(ii) the yield is s,
(iii) for each nonterminal node n with label A and dts(n) = n1..nm with
labels X1, ..,Xm, respectively, ?n = A ? (X1,D1)..(Xm,Dm),
(iv) FD =
?
n?Dom(?)
Inst(?n, (n, dts(n))),
(v) FD is satisfiable,
(vi) FD  a = v if v is an atomic feature value and a is any other constant
(atomic feature value or node) occurring in FD,
(vii) FD  (v ?) = (v ?) if v is an atomic feature value and ? is a nonempty
sequence of attributes,
(viii) M|? ?= F where M is a minimal model of FD.
Conditions (vi) and (vii) are syntactic versions of the constant/constant and con-
stant/complex clash conditions that together capture LFG?s functional uniqueness
condition (the denotations of an atomic feature value and any other distinct atomic
feature value or node constant have to be distinct (vi); atomic feature values have no
attributes (vii)).8 A model of an f-description, like the restricted one in (viii), is a pair
(U , I) consisting of a universe U and an interpretation function I. The interpretation
function assigns to each constant occurring in the f-description an element of U and
to each attribute a unary partial function on U .
Note that we create the f-description by instantiating the ??s and ??s by the nodes of
a given c-structure. Thus, we conceive of these terms as constants and will refer to them
on the f-description level sometimes as node constants rather than nodes. Because the
instantiating nodes are uniquely determined if we have a mapping ? licensing a given
c-structure, in the following we abbreviate Inst(?n, (n, dts(n))) by Inst(?n).
In general, two descriptions D and D? are said to be equivalent (D ? D?) iff the
restrictions of their minimal models to ? are isomorphic.
Definition 5
Let D and D? be two descriptions with minimal models M and M?. Then D ? D? iff
M|? ?= M?|?.
From Definition 4 we obtain the derivability relation ? as follows.
Definition 6
A terminal string s is derivable with f-structure F in G (?G(s,F)) iff there is a derivation
of s with F (with some f-description FD) in G.
8 Usually, conditions (vi) and (vii) are taken to be additional nonlogical axiom schemata of some traditional
equational logic expressive enough to axiomatize LFG?s underlying feature logic. Because we are not
primarily interested in completely axiomatizing LFG?s formal devices within some appropriate
meta-theory, we enforce the special properties of LFG?s atomic feature values by definition and assume
that standard first-order logic with equality is used to determine satisfiability.
875
Computational Linguistics Volume 38, Number 4
In this context we repeat the definition of the set of stringsGenG(F) that an LFG grammar
G relates to a given f-structure F:
Definition 7
For any LFG grammar G and any f-structure F
GenG(F) = {s ? T? | ?G(s,F)}.
In the next section we establish the basic result of this article: We present an al-
gorithm to construct for an arbitrary LFG grammar G and any acyclic f-structure F a
particular context-free grammar that provides a formal representation for the language
GenG(F).
3. Constructing the Specialized Grammar for GenG (F)
In the process of generation, the c-structures and the f-descriptions for an input
f-structure F are the unknowns that must be discovered to confirm that a given string
belongs to the set GenG(F). The set of valid c-structures that G provides for F is clearly
a subset of the trees that are generated by the context-free backbone of G. But this
subset might be infinite, as we have already seen with the input (9) and the grammar
in (8), because there is in general no fixed finite upper bound on the length of the
strings related to F or the size of their c-structures. Whether or not a given tree is a
valid c-structure for F then depends on the properties of the f-description that arises
by instantiating with the proper node constants the annotations on the individual rules
that license the derivation of that tree. The valid c-structures are just those trees for
which F is the f-structure of the resulting f-description.
Because of the possibly unbounded size of the c-structures, there is also no fixed
upper bound on the number of node constants that may occur in an f-description for
F. However, because the number of f-structure elements to which the node constants
actually refer is bounded by the size of F, it must be possible to obtain for any derived
f-description FD an equivalent description whose constants are drawn from a fixed
finite set. For instance, if we introduce a distinct canonical constant for each element
of F, we can create an equivalent description by substituting for each node constant
in FD the canonical constant associated with the functional element corresponding to
that node. This substitution typically reduces the number of distinct terms needed for
instantiation, and its usual effect is to replace several different node constants with a
single canonical term. But these replacements will provide an equivalent description
because we substitute a given term for two node constants if and only if it logically
follows from FD that those two nodes map to the same element of F. Thus, if FD
discriminates between two elements of F, so will the description that results from such
a reducing substitution.
Our context-free grammar construction crucially depends on the ability to find
for every f-description of F (from every possible c-structure) an equivalent descrip-
tion that involves only a finite number of distinct instantiation terms. This is what
enables us to simulate all the conditions for correct LFG generation with a finite set of
context-free category labels and a finite set of context-free productions, and thus to
rely on the finite control of the rule-by-rule category matching process of context-free
generation to produce the strings in GenG(F).
876
Wedekind and Kaplan LFG Generation by Grammar Specialization
The set of terms that correspond directly to the elements of F is large enough
to enforce all functional discriminations for an f-description associated with a com-
plete c-structure, as we have suggested. But unfortunately that set may not be large
enough to keep track of all necessary distinctions as an f-description is created in an
incremental context-free derivation process. For some f-structures and some grammars
it may not follow from the description associated with one portion of a derivation
tree that two nodes map to the same functional element, even though that identity
does follow when equations in the f-description for the entire tree are taken into
account.
Suppose that a three-daughter LFG start rule provides the instantiated annotations
(root F) = n1, (root G) = n2, and root = n3. Based only on this information we cannot
tell whether n1 and n2 can map to the same element of the f-structure and therefore
whether it is correct to substitute the same canonical constant for both of them. It
depends on whether the larger description that incorporates the expansion of the third
daughter implies the identity of the n1 and n2 structures. The same-constant substitution
would preserve equivalence only if the larger description implies that (n3 F) = (n3 G).
We must have two distinct constants available until that implication is deduced in the
course of the derivation, even if the input f-structure does not contain separate ele-
ments for those constants to correspond to.
Thus the set of constants needed to correctly reduce an arbitrary description as a
derivation proceeds incrementally may be larger than the number of elements in the
input f-structure and larger than what is required for an equivalent description for a
complete derivation. However, for each acyclic F we show that there is always a finite
set of canonical terms that can maintain all necessary functional discriminations as a
derivation unfolds. We use this set to construct a reducing substitution that permits
generation to be carried out under finite control. In contrast, we observe in Section 6
that the partial descriptions of cyclic structures cannot safely be reduced without an
unbounded number of canonical terms.
For the derivations that the simple LFG grammar in (8) provides for the input
[H V], we can accomplish the reduction of the f-description space with only two terms,
the canonical constant root and a separate canonical constant? that serves as a value for
all nodes that do not occur in an f-description. Let us start with the shortest derivation
for the given input. This consists of the c-structure in (10), which is licensed by the
rule-mapping ?root =
S? a b
(? H) = V .
(10) Sroot
an1 bn2
If we pair the licensing rule with its instantiating nodes (root,n1 n2), we arrive at the
instantiated rule
(
S? a b
(? H) = V , (root,n1 n2 )
)
. The reduction can be accomplished by apply-
ing to the instantiating nodes a substitution that replaces root by root and both n1 and
n2 by ?. This produces the instantiation
(
S? a b
(? H) = V , (root,??)
)
from which we obtain
the description {(root H) = V}. This is identical to the description that arises from the
original node-instantiated rule, because the ? does not occur in the annotations. The
reduction for all other derivations of [H V] is illustrated in Figure 3. The substitutions
of the node constants of the schematically represented derivations by canonical terms
are indicated by assigning the canonical term values to the nodes. If we consider the
resulting instantiation of the applied rules at the bottom of column (b), we observe that
877
Computational Linguistics Volume 38, Number 4
Figure 3
Schematic representation of the derivations of length > 1 with f-structure [H V] admitted
by the grammar in (8). The right-hand side shows the instantiated descriptions produced by
appropriately instantiating metavariables by node constants in column (a) and by particular
canonical canonical terms in column (b).
the f-description of each derivation of the input in G reduces to the description (11a)
and that this is the description that results for any derivation with the set of instantiated
rules depicted in (11b).
(11) a.
{
root = root,
(root H) = V
}
b.
?
?
?
?
?
(
S?a S b
? = ? , (root,? root?)
)
,
(
S? a b
(? H) = V , (root,??)
)
?
?
?
?
?
The reducibility of the f-description space for F provides the key insight for our
context-free grammar construction. The construction is accomplished in three steps. In
the first step we identify (as illustrated earlier) a finite set of canonical terms that can
serve in reducing the f-description space that G provides for F.
In the second step we use these terms to construct a set of instantiated rules
of G. These instantiations are ?appropriate? in the sense (to be made precise later)
that they maintain all necessary distinctions. They are formed by associating with
the metavariables canonical terms that can legitimately be used to reduce the corre-
sponding nodes of a local tree of a potential derivation of F. For the grammar (8)
and the terms root and ?, for example, there are only three appropriately instantiated
rules, the two rules contained in (11b) and
(
S? c
(? H? ) = V? , (root,?)
)
. We then determine
all collections of appropriately instantiated rules that together provide descriptions
of F without mistakenly collapsing a functional discrimination. For our particular ex-
ample there are just two collections of instantiated rules that provide a description of
[H V], namely,
{(
S? a b
(? H) = V , (root,??)
)}
and the set in (11b). These collections are drawn
878
Wedekind and Kaplan LFG Generation by Grammar Specialization
from the power set of the appropriately instantiated rules, so there is only a finite
number of them and each contains a finite number of instantiated rules. This ensures
that we can determine the f-description space that G provides for F without knowing
the details of the derivations for F and their reductions.
In the third and final step we create the context-free grammar that simulates exactly
those derivations in G whose strings are assigned the f-structure F. The categories of
this new grammar consist of refinements of the categories of the context-free backbone
of G together with a distinct root category SF. The original categories are augmented
with two additional components, a canonical instantiation term as used in the first step,
and a subset of one of the instantiated-rule collections determined in the second step.
The term component is used to encode the reducing substitution for the f-description
of a simulated derivation, and the rule component is used to record the reduced in-
stantiations of the licensing LFG rules whose application must still be simulated in
order to complete that derivation. The productions of the new grammar are created
from the rules contained in the instantiated-rule collections by replacing the original
categories by a certain number of their refinements, and then adding a particular set of
start rules. The start rules expand the root category of the new grammar to the original
start symbol augmented by root and one of the instantiated-rule collections determined
in the second step.
The context-free grammar thus constructed has a much larger set of categories and
many more rules than G. It is organized so that the normal matching of categories in a
context-free derivation globally ensures that the refined rules simulate all derivations
of F in G whose f-description is reducible to a description provided by one of the
instantiated-rule collections determined in the second step. Because we have already
indicated that every f-description of F must be reducible to a description provided by
one of these instantiated-rule collections, the constructed grammar simulates exactly
the set of derivations that G provides for F. The strings of GenG(F) are obtained by
removing the additional components from the categories of the terminal strings. With
r1 abbreviating
(
S?a S b
? = ? , (root,? root?)
)
and r2 abbreviating
(
S? a b
(? H) = V , (root,??)
)
our
construction produces for the LFG grammar in (8) and the input [H V] a context-free
grammar that contains the rules in (12).
(12) a. SF ? S:root:{r1, r2} b. SF ? S:root:{r2}
c. S:root:{r1, r2} ? a:?:? S:root:{r1, r2} b:?:?
d. S:root:{r1, r2} ? a:?:? S:root:{r2} b:?:? e. S:root:{r2} ? a:?:? b:?:?
These derive the set of terminal strings {a:?:?n b:?:?n | 1 ? n}. By removing the terminal
refinements we obtain {an bn | 1 ? n} and thus exactly the set of strings that the gram-
mar in (8) relates to [H V]. The rules (12b,e) simulate the derivation with the c-structure
in (10) and the rules (12a,c?e) simulate the derivations of length> 1. Rule (12c) simulates
recursions of the S rule of (8) and an application of rule (12d) terminates a recursion
because it consumes r1. An application of rule (12e) consumes r2 and terminates the
derivations.
In the remainder of this section we first identify the finite set of canonical terms
that can be used to reduce the f-description space for a given f-structure F derivable
with an LFG grammar G. We then investigate in Section 3.2 the problem of reducing the
f-description space for F and G. In Section 3.3 we give a precise recipe for constructing
the context-free grammar for F and G, and in Section 3.4 we illustrate this with a few
examples.
879
Computational Linguistics Volume 38, Number 4
3.1 Identifying the Reducing Terms
Our reduction of the f-description space makes use of the fact that we can eliminate cer-
tain node constants from an f-description FDwithout risk of producing a description not
equivalent to the original. This is because some node constants can be defined in terms
of others. We proceed rule-wise top?down based on the following definability relation.
Definition 8
Let r be an m-ary LFG rule, t be a term, and a1..am be a sequence of constants of length m,
each of them not occurring in t. A constant aj is m(other)-definable in Inst(r, (t, a1..am))
iff there is a (possibly empty) ? such that Inst(r, (t, a1..am))  aj = (t ?).
If the constant aj that instantiates the ? for a particular daughter is m-definable in terms
of (t ?) in Inst(r, (t, a1..am)), then all functional discriminations will be preserved if aj is
eliminated in favor of the term (t ?) from any description containing this instantiated
description of r.
To illustrate the elimination process, let us assume that our grammar includes
among its rules the ones in (13).
(13) a. S ? NP VP
(? SUBJ) = ? ? = ?
b. VP ? V ADVP
? = ? ? = ?
c. ADVP ? ADV ADVP
(? ADJ) = (? ELE) ? = ?
d. ADVP ? ADV
(? ADJ) = (? ELE)
e. NP ? John
(? PRED) = 'JOHN'
f. V ? fell
(? PRED) = 'FALL?(SUBJ)?'
(? TENSE) = PAST
g. ADV ? today
(? PRED) = 'TODAY'
h. ADV ? quickly
(? PRED) = 'QUICKLY'
In this grammar fragment we use equational annotations in the adverbial phrase
rules (13c) and (13d) instead of the more traditional set-membership statements (Kaplan
and Bresnan 1982). This is another way of allowing for multi-valued attributes that
was the original motivation for introducing set representations into LFG theory. We
use the equational treatment here to illustrate the fact that undefinable and hence
ineliminable constants can arise even when equality is the only formal device in an
f-description. The adverbial rules also illustrate that undefinable constants can figure in
the description of multiply rooted f-structures. The rules in (13) provide, for example,
the derivation depicted in Figure 4. The figure shows the f-structure on the left-hand
side and the instantiated descriptions of the licensing rules associated with the nodes
of the c-structure on the right-hand side. This more conventional way of depicting
derivations (Kaplan and Bresnan 1982) makes it easy to see the mother-definability
relation and at the same time permits the licensing rule-mapping to be read from the
annotated c-structure. The complete f-description is shown as a set in (14).
(14)
?
?
?
?
?
?
?
?
?
(root SUBJ) = n1, (n1 PRED) = 'JOHN',
root = n2, n2 = n4,n2 = n5,n5 = n8,
(n4 PRED) = 'FALL?(SUBJ)?', (n4 TENSE) = PAST,
(n5 ADJ) = (n7 ELE), (n7 PRED) = 'TODAY',
(n8 ADJ) = (n10 ELE), (n10 PRED) = 'QUICKLY'
?
?
?
?
?
?
?
?
?
880
Wedekind and Kaplan LFG Generation by Grammar Specialization
Sroot
NPn1
(root SUBJ) = n1
VPn2
root = n2
Johnn3
(n1 PRED) = 'JOHN'
Vn4
n2 = n4
ADVPn5
n2 = n5
felln6
(n4 PRED) = 'FALL?(SUBJ)?'
(n4 TENSE) = PAST
ADVn7
(n5 ADJ) = (n7 ELE)
ADVPn8
n5 = n8
todayn9
(n7 PRED) = 'TODAY'
ADVn10
(n8 ADJ) = (n10 ELE)
quicklyn11
(n10 PRED) = 'QUICKLY'
?
?
?
?
?
SUBJ
[
PRED 'JOHN'
]
PRED 'FALL?(SUBJ)?'
TENSE PAST
ADJ
?
?
?
?
?
[
ELE
PRED 'TODAY'
]
[
ELE
PRED 'QUICKLY'
]
Figure 4
The derivation of John fell today quickly with the rules in (13).
In this derivation, the node constants n1,n2,n4,n5, and n8 are m-definable whereas
root, the adverbial nodes n7 and n10, and the terminal nodes are not. For all m-definable
constants, we can construct definitions rule-wise top?down in the following way. We
begin with the start rule and derive from its instantiated description {(root SUBJ)= n1,
root= n2} the definitions n1 = (root SUBJ) and n2 = root for the m-definable daughters n1
and n2. We then continue with the rules that expand n1 and n2. Let us consider the VP
rule that expands n2. For the m-definable n2 we use the already constructed definition
to replace n2 by its defining term root in the instantiated description of the VP rule.
From Inst(VP? (V,{? = ?})(ADVP, {? = ?}), (root,n4 n5)) we then derive the definitions
n4 = root and n5 = root for its m-definable daughters, and so forth. If we run like this
through the whole derivation, for all m-definable daughters we obtain defining terms
that do not contain mother-definable node constants. For our example these are the
ones in (15).
(15) n1 = (root SUBJ)
n2 = root
n4 = root
n5 = root
n8 = root
By substituting all mother-definable node constants by their defining terms we can
then produce from the original f-description the equivalent description in (16).
(16)
?
?
?
?
?
?
?
?
?
(root SUBJ) = (root SUBJ), (root SUBJ PRED) = 'JOHN',
root = root,
(root PRED) = 'FALL?(SUBJ)?', (root TENSE) = PAST,
(root ADJ) = (n7 ELE), (n7 PRED) = 'TODAY',
(root ADJ) = (n10 ELE), (n10 PRED) = 'QUICKLY'
?
?
?
?
?
?
?
?
?
Notice that for each acyclic f-structure F the maximal length of the ? in the defining
terms is bounded by the depth of F.
Thus we see that a mother-definable constant can be eliminated in favor of a
constant corresponding to a higher node and a sequence of attributes leading down
881
Computational Linguistics Volume 38, Number 4
through the f-structure. The constants that are not eliminable are the root constant root (if
it occurs in FD) and all daughter constants that occur in FD but are not mother-definable.
At least for acyclic f-structures, however, we can show that there is an upper bound
on the number of these remaining constants. This is because the remaining constants
must each denote one of the elements of the given f-structure, but no two of them can
denote the same element. This is a consequence of LFG?s instantiation procedure and
functional uniqueness condition, and the acyclicity of the f-structure.
Given LFG?s instantiation procedure, as formalized in Definition 2, two distinct
node constants can be related in a single equation only if the nodes stand in a mother?
daughter relationship. Thus a daughter and a node external to the mother cannot be
related directly by instantiation but only as a consequence of a deduction involving
at least one instantiated annotation of some other licensing rule. Because of LFG?s
functional uniqueness condition the equations involved in such a deduction cannot
contain atomic feature values. The constant/complex clash condition (vii) of Defini-
tion 4 prevents atomic values from being substituted for proper subterms and the
constant/constant clash condition (vi) prevents them from being equated to nodes. Thus
such a deduction can only involve equations relating a daughter to its mother (or a node
to itself).
If an undefinable daughter corefers with a node external to the mother, then the
deduction that relates them must involve an instantiated annotation of that daugh-
ter that is (up to symmetric permutation) of the form (? ?) = (? ??) with |??| > 0
(such as the adverbial annotations previously mentioned), and there must be deduc-
tions from other equations that induce a cycle. This is demonstrated in the following
lemma.
Lemma 1
Let c and ? be a derivation with f-description FD for an acyclic f-structure in G. If nj is a daughter
of n and nj is not m-definable in Inst(?n) then FD  nj = (n? ?) for all n? not dominated by nj
and all (possibly empty) sequences of attributes ?.
Proof
Let nj be a daughter of n that is not m-definable in Inst(?n) and suppose that nj = (n
? ?)
would follow from FD for node n? not dominated by nj. Assume further that FD and
the instantiated descriptions of the licensing rules are closed under symmetry. Now
recall that the rule of substituting equals for equals has the form
e t = t?
e?
where e is an equation containing subterm t and e? is obtained from e by replacing one
occurrence of t in e by t?. Then we know that there is an equation t = t? that is either
in FD (or follows from FD by partial reflexivity9) such that nj = (n
? ?) is derivable from
t = t? by a left-branching substitution proof of the form
9 It may be the case that such a left-branching proof must start with a reflexive equation t = t that is not in
FD but can be inferred by partial reflexivity from an equation (t ?) = t?? in FD. Partial reflexivity is the
restriction of reflexivity to well-defined (object denoting) terms. It is a sound inference rule for the theory
of partial functions for which full reflexivity does not hold.
882
Wedekind and Kaplan LFG Generation by Grammar Specialization
?
?
?
?
t= t?
(n??)
(n??)
(nj?
??)
?
?
?
?
(n??)
(n??)
(nj?
??) t= t?
(i) (ii)
Figure 5
Possible dominance relations between nj and the node occurring in t
?. (i) The node occurring in
t? is dominated by nj. Note that n might occur in t. (ii) The node occurring in t
? is not dominated
by nj. As a special case, t might be (nj ?
?). The dashed and dotted arrows indicate the tree
traversal performed in the rewriting proofs of nj = (n
? ?) from t = t?, that is, the sequences of
nodes that must appear in the equations used to rewrite t to nj and t
? to (n? ?), respectively.
t = t? t1 = t
?
1
e1 . . .
em?1 tm = t
?
m
nj = (n
? ?)
where t is rewritten to nj and t
? to (n? ?) by a sequence of substitutions all justified by
equations ti = t
?
i ? FD, i = 1, ..,m (cf. Statman 1977; Wedekind 1994, Section 4). Because
of LFG?s instantiation procedure and the constant/constant and constant/complex
clash conditions, each premise of this proof must have the form (n? ??) = (n? ??) where
either n? and n? are in a mother?daughter relation or n? = n?. Depending on the dominance
relation between nj and the node occurring in t
? there are two possible cases. These
are illustrated in Figure 5. (i) If the node occurring in t? is dominated by nj then there
must be a premise (nj ?
?) = (n ?) from Inst(?n) such that t
? is rewritten to (nj ?
??) and
(nj ?
??) to (n ??). Because FD  t = nj and hence FD  nj = (nj ???), |??| = 0 due to
acyclicity. Thus nj = (n ?) ? Inst(?n), contradicting the undefinability assumption. (ii) If
the node occurring in t? is not dominated by nj there must be a premise (n ?) = (nj ?
?)
from Inst(?n) such that either t is rewritten to (n ??) and then to (nj ?
??), or t = (nj ?
?).
Since FD  t = nj, we get in both cases |??| = 0 because of acyclicity and thus the same
contradiction as in (i).
The following corollary follows directly from Lemma 1.
Corollary 1
Let c and ? be a derivation with f-description FD for an acyclic f-structure in G. If nj is a daughter
of n and nj is not m-definable in Inst(?n) then
(i) FD  nj = root, and
(ii) FD  nj = n?i for any distinct node n
?
i that is not definable in terms of its
mother n? in Inst(?n? ).
883
Computational Linguistics Volume 38, Number 4
From Corollary 1 it immediately follows that the denotations of FD?s undefinable
node constants are biunique. So, their number must be less than or equal to the size of
the universe of a minimal model M of FD. Suppose F is the f-structure for a derivation
with f-description FD. Because F is isomorphic to M|? for any minimal model M of
FD, and M|? and M share the same universe, we can use F?s (finite) universe to define
the constants that we require. Thus for each element a of the universe of F that is not
denoted by an atomic value we introduce a constant aa.
10
Definition 9
Let F be an f-structure with F = (U , I). We define the set of constants CF by
CF = {aa | a ? U and there is no atomic feature value v with I(v) = a}.
The set CF provides a sufficient number of constants to produce an equivalent reduced
description by a biunique renaming of the node constants that remain after the m-
definable ones are eliminated.
The renaming of the remaining undefinable constants can be accomplished, for
example, if we map in the natural way each mother-undefinable daughter n correspond-
ing to a in the isomorphic image F of M|? to the constant aa. Because of Corollary 1,
such a mapping must be biunique. Hence it can be used to rename all undefinable
daughters occurring in FD and will thus produce an equivalent description where all
nodes except root are replaced by constants drawn from F.
As an illustration we pick for the f-structure depicted in Figure 4 the structure
with the universe in (17a) and the interpretation function whose directed acyclic graph
representation is given in (17b).11
(17) a. {a, b, c, d, e, f, g, h, i, j}
b. a
PRED
TENSE ADJ
SUBJ
b c d e
'FALL?(SUBJ)?' PAST
PRED
ELE ELE
f g h
PRED PRED
'JOHN'
i j
'TODAY' 'QUICKLY'
The constants we obtain from the structure (17) by Definition 9 are the ones in (18).
(18) {aa, ad, ae, af, ag}
10 From a computational point of view a constant aa can be regarded as the address of a or a pointer to a.
11 According to our formalization, attribute symbols are interpreted by unary partial functions over the
universe and atomic value symbols by elements of the universe. Thus the graph indicates, for example,
that the interpretation function assigns to the attribute symbol PRED the (unary) partial function
{(a, b), (e, h), (f, i), (g, j)} and to the attribute symbol ELE the partial function {(f, d), (g, d)}. Furthermore,
it interprets the atomic value symbol 'FALL?(SUBJ)?' as denoting b and PAST as denoting c.
884
Wedekind and Kaplan LFG Generation by Grammar Specialization
Now, let M be a minimal model of our original f-description. Because an isomor-
phism between M|? and our structure (17) must map the denotation of n7 in M to f and
the denotation of n10 to g, we can rename n7 by af and n10 by ag and obtain from (16) the
equivalent description (19).
(19)
?
?
?
?
?
?
?
?
?
(root SUBJ) = (root SUBJ), (root SUBJ PRED) = 'JOHN',
root = root,
(root PRED) = 'FALL?(SUBJ)?', (root TENSE) = PAST,
(root ADJ) = (af ELE), (af PRED) = 'TODAY',
(root ADJ) = (ag ELE), (ag PRED) = 'QUICKLY'
?
?
?
?
?
?
?
?
?
We next compose the substitution that is induced by the definitions of the definable
daughters and the substitution that we used to rename the undefinable daughters.
This provides a substitution that allows us to produce from the original f-description
an equivalent description in a single transformation. If we compose the two sub-
stitutions of our example, that is, the one induced by the definitions in (15) and
the renaming substitution {(n7, af), (n10, ag)}, we arrive at the reducing substitution
in (20).
(20) {(n1, (root SUBJ)), (n2, root), (n4, root), (n5, root), (n7, af), (n8, root), (n10, ag)}
We now give a precise specification of a (finite) set of terms that can serve as the
range of the reducing substitutions for all derivations of an f-structure F. This set is
obtained from the constants in CF and the attributes of F in the following way. We
first provide the constants CF with their intended interpretation by expanding F in the
natural way to the canonical structure F? for ? ? CF.
Definition 10
Let F be an f-structure with F = (U , I). We define the canonical expansion F? of F to
? ? CF by
F? = (U , I?) with I? = I ? {(aa, a) | aa ? CF}.
In the canonical expansion F?, each element of the universe is denoted by exactly one
constant. Each new constant aa is interpreted by a and each atomic feature value by its
original denotation.
A set TF of canonical terms that includes the ranges of the reducing substitutions
for all possible derivations of F is a set that contains all terms of the form (aa ?) that
are defined in F? but do not denote an element already designated by an atomic feature
value. It also contains all terms that we obtain from those by substituting root for their
constant symbols. This set includes all constants of CF (because ? can be empty) and thus
all possible constant values for the mother-undefinable daughters of a derivation for F.
Because each element in the universe of F? is denoted by a constant, TF also contains
all possible defining terms for the mother-definable nodes of that derivation. Terms
referring to the denotation of an atomic feature value are not required, since there are
(because of the constant/constant clash condition) no node constants with the same
denotation as any atomic feature value.
The node constant root is substituted for the CF constants in every term to account
for the fact that different derivations may associate different functional elements with
the root of the c-structure. That would be the case, for example, if our grammar contains
885
Computational Linguistics Volume 38, Number 4
in addition the S and VP rules (21a,b) and alternatively derives the adverbials with the
rules (21c,d).
(21) a. S ? S ADVP
? = (? ADJ) ? = ?
b. VP ? V
? = ?
c. ADVP ? ADV ADVP
? = (? ELE) ? = ?
d. ADVP ? ADV
? = (? ELE)
With such a grammar we can derive the f-structure of Figure 4 also with an f-description
where root denotes the ADJ value and where the top of the f-structure is denoted by the
mother-undefinable S node that is expanded by the original start rule. As this example
indicates, a grammar might produce several f-descriptions for the same f-structure
by anchoring the description at different f-structure elements and then moving along
different paths through the structure. This is why the term set must contain the entire set
of constants CF (and the terms containing them) and not just the ones for the f-structure
roots.
Thus TF contains sufficiently many constant symbols and defining terms for the
reducing substitutions to make all the distinctions that could arise from any c-structure
and f-description for the given F. It is defined formally in the following way.
Definition 11
Let F = (U , I) be an f-structure. On the basis of the canonical expansion F? = (U , I?) of F to
? ? CF we first define the set of terms TF
TF = {(aa ?) ? Dom(I?) | aa ? CF and there is no value v ? ? s.t. I?(v) = I?(aa ?)}.
The set of terms TF that we will use for the grammar construction is then defined by
TF = TF ? {(root ?) | there is a term (aa ?) ? TF} ? {?}.
For mathematical convenience we add the dummy constant? as a value for those nodes
of the c-structure that are not interpreted in a minimal model of the f-description. These
are just the ones that do not occur in the f-description. The complete set of terms for the
structure in (17) is given in (22).
(22) { aa, ad, ae, af, ag,
(aa ADJ), (aa SUBJ),
(af ELE), (ag ELE)
}
?
{
root,
(root ADJ), (root SUBJ),
(root ELE)
}
? {?}
We have illustrated that we can reduce the f-description of every derivation of an acyclic
f-structure F to an equivalent description if we replace the node constants by terms
of TF. But this assumes that the c-structure and the f-description are already known.
Our grammar construction requires us to simulate this reduction without knowing
in advance the details of either the c-structure or a particular f-description. And that
means only on the basis of the possible values of the reducing substitutions, namely
TF, and the rules of G.
3.2 Reducing the f-Description Space
We now shift our attention to the rules of G and their instantiating terms, that is, to the
arguments of the Inst function. These are pairs consisting of an m-ary rule r of G and its
886
Wedekind and Kaplan LFG Generation by Grammar Specialization
instantiating terms (t, t1..tm). Let us call such a pair an instantiation of r, or sometimes
simply an instantiated rule. Let us further extend the reducing substitutions that we
constructed for the derivations of F to total functions by assigning root to root and ?
to each non-denoting node constant. Now recall that the f-description of a particular
derivation for F consists of the union of the instantiated descriptions of the rules that
together license that derivation. If we consider these licensing rules together with
their node instantiation, that is, pairs of the form (r, (n,n1..nm)), and use a reducing
substitution for that derivation to replace the node constants in the instantiations by
canonical terms, then we obtain a collection of instantiated rules of the form (r, (t, t1..tm))
all of which are instantiated by terms of TF. The union of the instantiated descriptions of
these rules is identical to the description that the reducing substitution produces from
the original f-description. Because R and TF are finite, the set of all instantiated-rule
collections that we obtain from the (possibly infinite) set of derivations of F by reducing
their node-instantiated licensing rules must be finite too. This fact is crucial for our
grammar construction.
We further observe that the instantiated rules that result from this substitution are
also appropriate in the following sense.
Definition 12
Let r be an m-ary LFG rule in R of G (m ? 0), F be an f-structure, (t, t1..tm) ? TF ? T mF ,
and a1..am be a sequence of length m of pair-wise distinct constants not in TF. Then
the instantiated rule (r, (t, t1..tm)) is appropriately instantiated (by terms of TF) iff the
following conditions are satisfied:
(i) if tj = ? then aj is not interpreted in a minimal model of Inst(r, (t, a1..am)),
(ii) if aj is m-definable in Inst(r, (t, a1..am)) then Inst(r, (t, a1..am))  aj = tj,
(iii) otherwise tj ? CF, tj = t and tj = ti for all i = 1, ..,m with i = j.
In the following the set of all appropriately instantiated rules is denoted by IRF (IRF =
{(r, ?) ? R? (TF ? T ?F ) | (r, ?) is appropriately instantiated}).
The constants a1..am in this definition provide the same discriminations as the daughter
nodes of any local tree licensed by the rule. This definition is satisfied by rules that result
from eliminating node constants in favor of terms in the way that we have described.
Such term-instantiated rules satisfy condition (ii), because whenever the mother is
instantiated by t and an m-definable daughter nj is reduced to a term (t ?) ? TF then
also Inst(r, (t, a1..am))  aj = tj (= (t ?)). Condition (iii) is satisfied, because of the pair-
wise distinctness of the values for the mother-undefinable nodes, due to Corollary 1.
And condition (i) holds, because non-denoting node constants are mapped to ?.12 The
set IRF of all possible appropriately instantiated rules is large but finite, because R and
TF are finite.
For our start rule (13a) S ? (NP, {(? SUBJ) = ?})(VP, {? = ?}), only the two instan-
tiations in (23) are appropriate.
12 Note that we cannot establish the converse of (i). This is because a daughter node constant that is not
interpreted in a minimal model of the instantiated description of a rule might occur in a statement
introduced by a rule expanding that daughter. In a minimal model corresponding to a larger
derivational context such a daughter constant might thus belong to the interpreted symbols.
887
Computational Linguistics Volume 38, Number 4
(23) a.
(
S? NP VP
(? SUBJ) = ?? = ?, (root, (root SUBJ) root)
)
b.
(
S? NP VP
(? SUBJ) = ?? = ?, (aa, (aa SUBJ) aa )
)
The rule ADVP ? (ADV, {(? ADJ) = (? ELE)})(ADVP, {? = ?}), on the other hand, has
many appropriate instantiations, among them the ones in (24).
(24) a.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, af root)
)
b.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, ag root)
)
c.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (aa, ag aa )
)
d.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, ((aa ADJ), ag (aa ADJ))
)
Instantiations that are not appropriate for this rule are, for example, the ones in (25).
(25) a.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, root root)
)
b.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (aa, aa aa )
)
They are not properly discriminating, because the ADV node of any derived f-
description must denote an entity distinct from the denotation of the mother and the
other daughter node.
The instantiations in (23a) and (24a) are the ones obtained from the derivation in
Figure 4 and the reducing substitution (20). Note that the appropriately instantiated
rule (23b) that does not associate the S node with the root constant might result from
derivations where the top of the f-structure is not denoted by the root node of the
c-structure, as illustrated with the rules in (21).
So far we have considered only the individual instantiated rules that we obtain from
the licensing rules of a derivation for F by replacing the node constants as described
by terms of TF. As a consequence of Corollary 1, we also observe that our reducing
substitutions never replace undefinable daughters of two distinct node-instantiated
licensing rules by one and the same constant. That is, the term-instantiated rules that
result from two distinct node-instantiated licensing rules always satisfy the following
compatibility relation.
Definition 13
Two appropriately instantiated rules (r, (t, t1..tm)) and (r
?, (t?, t?1..t
?
l )) of IRF are compati-
ble iff
ti = t?j for all ti, t
?
j ? CF with ti = t and t
?
j = t
? (1 ? i ? m, 1 ? j ? l).
Given appropriateness, the conditions ti ? CF and ti = t imply that ti is not definable
in terms of t in the instantiated description of r. In essence, two instantiated rules are
compatible only if there are no repetitions of daughter constants instantiating mother-
undefinable daughters: All shared daughter constants instantiate mother-definable
daughters. Incompatible instantiations do not respect the biuniqueness property given
by Corollary 1 and therefore cannot appear together in the set of TF-instantiated rules
for any derivation of F. Note that this compatibility relation is symmetric, but reflexive
only for those instantiated rules (r, (t, t1..tm)) where each daughter that is instantiated
888
Wedekind and Kaplan LFG Generation by Grammar Specialization
by a constant from CF is mother-definable. As a consequence of Corollary 1, only an in-
stantiated rule that is compatible with itself can emerge from two separate applications
of r in a derivation of F.
The instantiated rules in (26a?c), for example, are compatible while the ones in (26d)
are not. The latter rules mistakenly introduce an identity that, because of Corollary 1,
can never be derived by the grammar. The rules in (26a) result from reducing the
licensing rules of the derivation in Figure 4 with the reducing substitution (20).
(26) a.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, af root)
) (
ADVP? ADV
(? ADJ) = (? ELE), (root, ag )
)
b.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, ag root)
) (
ADVP? ADV
(? ADJ) = (? ELE), (root, af )
)
c.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, aa root)
) (
ADVP? ADV
(? ADJ) = (? ELE), (root, af )
)
d.
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, af root)
) (
ADVP? ADV
(? ADJ) = (? ELE), (root, af )
)
Our observations lead to a definition that characterizes reducing substitutions en-
tirely in terms of the identified properties of the TF-instantiated rules and thus in a
way that will permit us to simulate their construction by a refinement of the context-
free backbone of G. In the following definition we use Nc to denote the nodes of a c-
structure c and ?[?] to indicate the expression that is obtained from an expression ?
(term, sequence of terms, formula, set of formulas, etc.) and a substitution ? (mapping
from constants to terms) by replacing all occurrences of constants a in ? simultaneously
by ?(a).
Definition 14
Let c and ? be a derivation of f-structure F in G and ? be a mapping from Nc into TF.
Then ? is a reducing substitution for the given derivation iff ?(root) = root, and for all
n,n? ? Dom(?) with n = n?
(i) (?n, (n, dts(n))[?]) is appropriately instantiated, and
(ii) (?n, (n, dts(n))[?]) is compatible with (?n? , (n
?, dts(n?))[?]).
That reducing substitutions in fact preserve equivalence is then established by the
following lemma.
Lemma 2
Let c and ? be a derivation with f-description FD and f-structure F in G. If ? is a reducing
substitution for c and ?, then FD ? FD[?].
Proof
We prove the lemma by induction on the number of nodes, according to a left-to-right,
top?down traversal of the c-structure. Let c and ? be a derivation with f-description FD
and f-structure F in G, M = (U , I) a minimal model of FD, and ? a reducing substitution
for c and ?. We first define for each node n of c the set Nn consisting of all nodes higher
than n, all nodes of the same depth as n but preceding (on the left), and n. Now for
each Nn with |Nn| = i let the function ?i be the restriction of ? to Nn (?|Nn). Then we
can show by induction for each i = 1, .., |Nc| that FD ? FD[?i], that is, left-to-right, top?
down. The equivalence is established by constructing a minimal model Mi also on the
universe U of M. Thus the isomorphism between M|? and Mi|? is the identity function.
889
Computational Linguistics Volume 38, Number 4
The basis, i = 1, is trivial, because ?1 = {(root, root)} by definition. Thus FD[?1] = FD
and M1 = M is a minimal model of FD[?1]. Hence FD ? FD[?1]. For the induction step,
let i > 1. Then FD ? FD[?i?1] by hypothesis. Let Mi?1 = (U , Ii?1) be a minimal model
of FD[?i?1], and suppose that node nj with mother n is the next node in the sequence
(i.e., |Nnj | = i).
If nj is not interpreted in M, it does not occur in FD and hence not in FD[?i?1]. Thus
FD[?i] = FD[?i?1], Mi = Mi?1 is a minimal model of FD[?i], and FD ? FD[?i].
If nj is interpreted in M, there are two cases to consider.
(a) If nj is m-definable in Inst(?n, (?i?1(n), dts(n))) and ?(nj)= tj then FD[?i?1]  nj = tj.
Because nj does not occur in tj and hence not in FD[?i], FD[?i?1] is logically equivalent
to the definitional extension FD[?i] ? {nj = tj} of FD[?i]. Because tj occurs in FD[?i],
Mi = Mi?1|(Dom(Ii?1)\{nj}) is a minimal model of FD[?i]. Hence Mi?1|? ?= Mi|? and
FD ? FD[?i].
(b) If nj is not m-definable in Inst(?n, (?i?1(n), dts(n))) then ?(nj) ? CF. Let ?(nj) = aa.
Then aa cannot occur in FD[?i?1], because the instantiation is appropriate and pair-
wise compatible and aa = root (= ?(root)).13 So the model Mi that results from Mi?1
by renaming nj by aa must be a minimal model of FD[?i]. Thus Mi?1|? ?= Mi|? and
FD ? FD[?i].
Hence, FD ? FD[?|Nc|] = FD[?].
Appropriateness and compatibility do not ensure that undefinable daughter con-
stants are distinct from the root. This case is covered, however, because we kept root
for the root.
We indicated earlier that for an arbitrary derivation of an acyclic f-structure F we
can?dependent on a minimal model of its f-description?construct a substitution with
range TF that satisfies the conditions of Definition 14. We now provide a rigorous proof
of this assertion.
Lemma 3
For every derivation of an acyclic f-structure F in G there exists a reducing substitution.
Proof
Suppose there is a derivation c and ? with f-description FD and f-structure F in G, that
FD has minimal model M = (U , I), and that h is an isomorphism between M|? and F.
Suppose furthermore that c has depth k. For each i = 0, .., k we define by induction a
function ?i : Nc ? TF as follows. For the root (i = 0) we set ?0(root) = root. Suppose we
have defined ?i?1, 0 < i ? k. We then set ?i(n) = ?i?1(n) for all n ? Dom(?i?1). Now,
let nj be a node of depth i with mother n. If nj is not interpreted in M we set ?i(nj) = ?.
If nj is interpreted in M we set
?i(nj) =
{
(?i?1(n) ?) s.t. Inst(?n) (n ?) = nj if nj is m-definable in Inst(?n)
ah(I(nj )) otherwise.
13 Of course, the constant aa cannot occur as a proper subterm of any other ?i?1 value. If ?i?1 were to
map a node n? to (aa ?) then n
? must be m-definable and there must be a node dominating n? that is
not m-definable and mapped to aa. Because aa = root, aa must instantiate a daughter of another rule,
contradicting compatibility.
890
Wedekind and Kaplan LFG Generation by Grammar Specialization
Now, let ? = ?k. Then ? trivially satisfies the appropriateness conditions (i) and (ii)
by definition. Appropriateness condition (iii) and compatibility follow by Corollary 1.
Thus ? is a reducing substitution for c and ?.
Lemma 3 ensures that there exists a reducing substitution for every derivation of
an acyclic f-structure F in G. By Lemma 2 we know that such a substitution preserves
equivalence.14 Thus, the collections of instantiated rules that result from the derivations
for F and their reducing substitutions must belong to the set consisting of all possible
collections of appropriately instantiated and pair-wise compatible rules that together
provide descriptions of F. If we extend the Inst function in the obvious way to sets of
instantiated rules IR
Inst(IR) =
?
(r,?)?IR
Inst(r, ?)
then this set is defined as follows.
Definition 15
Let F be an f-structure. Then IRDF is the set of all sets IR ? IRF such that
(i) for all (r, ?), (r?, ??) ? IR with (r, ?) = (r?, ??), (r, ?) is compatible with (r?, ??),
(ii) M|? ?= F, for a minimal model M of Inst(IR).
This is a finite set whose size is bounded by a function of the sizes of R and TF.
Lemma 2 also shows that we can produce an equivalent description for any derived
f-description of F, not only with the model-dependent substitutions used in the proof
of Lemma 3, but in general with any mapping that satisfies the definition of a reducing
substitution. This is important for our grammar construction, because it provides the
conditions that we have to control to make sure that we simulate the derivations of
f-descriptions for F together with equivalence-preserving substitutions. Under these
conditions we can reduce the sets of node-instantiated licensing rules of the simulated
derivations to collections that are also included in IRDF. IRDF can be determined with-
out knowing the details of the valid derivations for F, just on the basis of F and the LFG
grammar G alone.
3.3 Producing the Context-free GrammarGF
The context-free grammar GF that simulates all valid derivations for F in G is specified
in the following definition. From this we can produce all strings in GenG(F) by conven-
tional context-free generation algorithms.
Definition 16
Let G = (N,T, S,R) be an LFG grammar and F be an acyclic f-structure. For G and F
we construct a context-free grammar GF = (NF,TF, SF,RF) in the following way. The
collection of nonterminals NF is the (finite) set
{SF} ? (N ? TF ?
?
{Pow(IR) | IR ? IRDF})
14 Note that the particular substitution that we construct in the proof of Lemma 3 reduces FD to an
equivalent description that is satisfied in an expansion of F? by an interpretation for root in U .
891
Computational Linguistics Volume 38, Number 4
where SF is a new root category. Categories in NF other than SF are written A:t:IR, where
A is a category in N, t is a term in TF, and IR is a subset of a set of instantiated rules in
IRDF. TF is the set T ? TF ? {?}.15 The rules RF are constructed from the annotated rules
R of G. We include all and only rules of the form:
(i) SF ? S:root:IRroot, where IRroot is any element of IRDF,
(ii) A:t:IR ? X1:t1:IR1..Xm:tm:IRm such that
(a) there is an r ? R expanding A to X1..Xm,
(b) IR = {(r, (t, t1..tm))} ?
m
?
j=1
IRj,
(c) if (r, (t, t1..tm)) ? IRj (j = 1, ..,m), or (r?, ??) ? IRi ? IRj and i = j
(i, j = 1, ..,m), then (r, (t, t1..tm)), respectively (r
?, ??), is compatible
with itself.
We define the projection Cat(X:t:IR) = X for every category in NF ? TF except SF and
extend this function in the natural way to strings of categories and sets of strings of
categories. Note that the set
Cat(L(GF)) = {s | ?s? ? L(GF) such that Cat(s?) = s}
is context-free, because the set of context-free languages is closed under homomor-
phisms such as Cat.16
Before presenting our main theorem and its proof let us sketch how the derivations
for F in G are simulated by the context-free grammar GF.
The grammar GF expands the root symbol SF to complex categories of the form
S:root:IRroot containing the root category S of G as their first component. A derivation
from S:root:IRroot in GF then consists of a phrase structure tree whose nodes are labeled
with refinements of the categories of the original LFG grammar. By taking the Cat
projection of every category, we obtain the c-structure of at least one derivation for F in
G that is simulated by the derivation from S:root:IRroot in GF. The term component of the
augmented categories encodes a reducing substitution ? for the simulated derivation
with the given c-structure. That is, if a node n in the GF derivation is labeled by X:t:IR,
then ?(n) = t for the corresponding LFG c-structure tree.
The component IR contains all instantiated rules of G that are required to license
the subderivation in G that corresponds (under the Cat projection) to the subderivation
from n in GF, except that the licensed nodes are replaced in the instantiated rules by
their ? values.17 Thus, the additional components of the root label S:root:IRroot record
15 The set of terminals TF is constructed from the full term set TF instead of just ? to allow for the possibility
of ? appearing in lexical entries (e.g., Zaenen and Kaplan 1995).
16 Cf. Hopcroft and Ullman (1979).
17 The rule component IR is a refinement of the third component of the categories defined in Kaplan and
Wedekind (2000). The categories there were distinguished by Inst(IR), the descriptions produced by
collecting the instantiated annotations from our third-component rules, and thus give a more compact
representation whenever different subderivations provide the same instantiated description. As we
demonstrated, such a simpler representation is sufficient to control the generation process for grammars
with a conventional set of descriptive devices. Instantiated descriptions, however, do not provide
enough information for grammars with devices whose evaluation requires the c-structure to be taken
into account, as, for example, functional precedence (Bresnan 1995; Zaenen and Kaplan 1995). We show
in Wedekind and Kaplan (forthcoming) that these devices can be modeled with our more elaborate rule
representation.
892
Wedekind and Kaplan LFG Generation by Grammar Specialization
that ?(root) is set to root (the initial condition for reducing substitutions) and that the
node-instantiated licensing rules of the simulated derivation are reduced to IRroot by ?.
Each application of a rule A:t:IR ? X1:t1:IR1..Xm:tm:IRm that expands a nonterminal node
n of the derivation in GF simulates the application of an LFG rule with context-free back-
boneA ? X1..Xm whose instantiation with (t, t1..tm) combines with the instantiated-rule
components of all daughters to form the rule component IR of the mother.18 Now, ?
must be a reducing substitution for the simulated derivation, because all instantiated
rules in IRroot are appropriately instantiated and pair-wise compatible and because
condition (iic) of Definition 16 ensures that rules that are not self-compatible can only
be used once for licensing the Cat projection. Thus, because of Lemma 2, the derivation
in GF simulates a derivation of an f-description in G that ? reduces to the equivalent
description provided by IRroot.
We can also see that every derivation for F in G is simulated by a derivation in
GF. We know from Lemmas 2 and 3 that we can construct for every derivation of an
f-description for F in G a reducing substitution ? that produces a description equiv-
alent to the original one. Based on ? we can then augment the category labels of the
c-structure of a derivation for F in G by term and rule components that record ? and
the licensing rules (with the node constants replaced by their ? values). We thus obtain
a derivation from S:root:IRroot where the instantiated description provided by IRroot is
equivalent to the original f-description. Because GF contains a start rule for every set
of appropriately instantiated and pair-wise compatible rules that provides a description
of F, there must also be a rule that expands SF to S:root:IRroot and the terminal string of
the derivation for F in G must be the Cat projection of a derivable string in GF.
We are now prepared to prove our main theorem.
Theorem
For any LFG grammar G and any acyclic f-structure F, GenG(F) = Cat(L(GF)).
Proof
We prove first that GenG(F) ? Cat(L(GF)). Suppose there is a derivation c and ? of a
terminal string s with f-description FD and f-structure F in G. By Lemma 3, there exists a
reducing substitution ? for c and ?. Thus FD ? FD[?] by Lemma 2. We construct a deri-
vation c? and ?? of s? from S:root:IRroot with Cat(s
?) = s. We obtain c? by relabeling each
node n with label X by X:?(n):{(?n?, (n?, dts(n?))[?]) | n dominates nonterminal node n?}.
That means that the c-structures of both derivations share the same tree skeleton. We de-
fine ?? for each nonterminal node n with label A:?(n):IR and dts(n) = n1..nm with labels
X1:?(n1):IR1, ..,Xm:?(nm):IRm by ?
?
n = A:?(n):IR ? X1:?(n1):IR1..Xm:?(nm):IRm. Because
FD[?] = Inst(IRroot) by construction of IRroot and because IRroot ? IRF and condition (i)
of Definition 15 hold by the properties of ?, IRroot must be an element of IRDF. Thus
SF ? S:root:IRroot is in RF. Moreover, Ran(??) ? RF, because by construction the rule
components are subsets of IRroot, the rule components of the terminals are empty, and
the rules satisfy (iia,b) of Definition 16 by construction and (iic) because ? is a reducing
substitution. Thus s ? Cat(L(GF)).
We now prove that Cat(L(GF)) ? GenG(F). Suppose there is a GF derivation c? and
?? of s? from S:root:IRroot with Cat(s
?) = s and IRroot ? IRDF. We first construct a new
c-structure c with the same tree skeleton as c? by relabeling each node n with label X:t:IR
18 Note that the licensing LFG rule might not be uniquely determined if the derivation in GF simulates
recursions.
893
Computational Linguistics Volume 38, Number 4
by X. We define a substitution ? by setting ?(n) = t for each node n with label X:t:IR.
We then show that there is a mapping ? into R licensing c with FD ? Inst(IRroot). By
induction on the depth of the subtrees we first define for each nonterminal n a function
?n from all nonterminal nodes dominated by n into R such that
(a) ?n licenses the subtree of c with root n,
(b) IR = {(?nn?, (n?, dts(n?))[?]) | n dominates nonterminal node n?} if n has label
A:t:IR in c?,
and, for all n?, n? ? Dom(?n) with n? = n?
(c) (?nn?, (n?, dts(n?))[?]) is appropriately instantiated and
(d) (?nn?, (n?, dts(n?))[?]) is compatible with (?
n
n?, (n?, dts(n?))[?]).
Suppose n with dts(n) = n1..nm is expanded by ?
?
n = A:t:IR ? X1:t1:IR1..Xm:tm:IRm in
c?, then there is a rule r ? R satisfying the conditions of Definition 16(ii). Thus, r ex-
pands A to X1..Xm, IR = {(r, (t, t1..tm))} ?
m
?
j=1
IRj, and (r, (n, dts(n))[?]) = (r, (t, t1..tm)) by
definition of ?. If n is a preterminal node then IRj = ? (for each j = 1, ..,m). We then
set ?n = {(n, r)} and (a)?(d) hold trivially. If ?nj has been defined for all nonterminal
daughters nj we set ?
n = {(n, r)} ?
?
{?nj | nj is a nonterminal daughter of n}. Then (a)?
(c) by construction of ??n and by the inductive hypothesis, and (d) by Definition 16(iic)
and because IR ? IRroot by Definition 15(i). So, ? = ?root licenses c, ? is a reducing
substitution for c and ?, and FD ? FD[?] by Lemma 2. Then FD[?] = Inst(IRroot) by (b)
and thus s is derivable in G with F.
The following corollary is an immediate consequence of this theorem.
Corollary 2
For any LFG grammar G and any acyclic f-structure F, GenG(F) is a context-free language.
3.4 A Few Examples
In the preceding sections we have shown how to construct a context-free grammar
that generates exactly the set of strings that an LFG grammar assigns to a given
f-structure. Those strings can be produced by running a context-free generator with
that grammar. In this section we provide examples to illustrate the derivation space of
the constructed context-free grammar and the correspondence between the derivations
of the constructed grammar and the derivations of the original LFG grammar.
As one illustration of the correspondences between the derivations, let us consider
the f-structure F in (27) and the LFG grammar with the rules (13) and the VP rule in (2).
(27)
?
?
?
SUBJ
[
PRED 'JOHN'
]
PRED 'FALL?(SUBJ)?'
TENSE PAST
?
?
?
For this grammar there is only one derivation of a string with the given f-structure,
the one that is depicted in the upper part of Figure 6. The figure shows the derivation
with all its components, that is, the c-structure, the rule-mapping ? together with the
node instantiation of the licensing rules, and the f-description. This LFG derivation is
894
W
e
d
e
k
in
d
a
n
d
K
a
p
la
n
L
F
G
G
e
n
e
ra
tio
n
b
y
G
ra
m
m
a
r
S
p
e
cia
liz
a
tio
n
LFG derivation
Sroot
?
??? S? NP VP
(? SUBJ)= ?? = ? (root,n1n2 )
NPn1
?
??? NP? John
(? PRED)= 'JOHN' (n1,n3 )
VPn2
?
??? VP? V
? = ? (n2,n4 )
Johnn3 Vn4
?
??? V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
(n4,n5 )
felln5
f-description
?
?
?
?
?
?
?
?
?
?
?
(root SUBJ) = n1,
root = n2,
(n1 PRED) = 'JOHN',
n2 = n4,
(n4 PRED) = 'FALL?(SUBJ)?',
(n4 TENSE) = PAST
?
?
?
?
?
?
?
?
?
?
?
reducing substitution
?(root) = root
?(n1) = (root SUBJ)
?(n2) = root
?(n3) = ?
?(n4) = root
?(n5) = ?
context-free derivation
SF
S:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
S? NP VP
(? SUBJ)= ?? = ?, (root, (root SUBJ) root)
)
,
(
NP? John
(? PRED)= 'JOHN', ((root SUBJ),?)
)
,
(
VP? V
? = ?, (root, root)
)
,
(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
root
NP:(root SUBJ):
{(
NP? John
(? PRED)= 'JOHN', ((root SUBJ),?)
)}
n1
VP:root:
?
?
?
?
?
?
?
(
VP? V
? = ?, (root, root)
)
,
(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)
?
?
?
?
?
?
?
n2
John:?:?n3
V:root:
{(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)}
n4
fell:?:?n5
instantiated description of the
start rule?s rule component
?
?
?
?
?
?
?
?
?
(root SUBJ) = (root SUBJ),
root = root,
(root SUBJ PRED) = 'JOHN',
(root PRED) = 'FALL?(SUBJ)?',
(root TENSE) = PAST
?
?
?
?
?
?
?
?
?
Figure 6
The LFG derivation for (27) and the rules in (2) and (13) with the corresponding context-free derivation of the
constructed grammar.
8
9
5
Computational Linguistics Volume 38, Number 4
simulated in the constructed context-free grammar by the derivation that is shown in
the lower part of the figure.
Both the depicted substitution ? and the subtree to which SF expands are related
to the original LFG derivation by the construction of the first half of our proof. That
is, ? is a reducing substitution and the context-free derivation specializes the category
label of each node n of the original c-structure. The term component is n?s ? value.
The rule component is the set of all instantiated rules that result from the licensing
rules of the corresponding n-dominated LFG subderivation. These are instantiated by
replacing the instantiating nodes of the LFG derivation by their ? values. Thus, the
instantiated description provided by the rule component of the start rule is equivalent
to the original f-description and hence the context-free derivation tree at the bottom of
Figure 6 is licensed completely by the rules of the constructed grammar. Note that the
Cat projection of the terminal string of the context-free derivation is the terminal string
of the c-structure, the sentence John fell.
On the other hand, the depicted LFG derivation and the context-free derivation
are also related by the construction of the second half of the proof. The c-structure is
the Cat projection of the constituent structure that SF?s daughter derives. The reducing
substitution maps each node of this c-structure to the term of its complex label in the
corresponding context-free derivation. And the LFG rule that the licensing mapping
maps to each node is the rule of the node label?s rule component that licenses the node
and its daughters in the Cat projection. This is instantiated by the term components of
the applied context-free rule and combines with the rule components of the daughters
to form the rule component of the mother. These licensing LFG rules for the immediate
daughters are shown in gray in the rule component of the node labels in the context-free
derivation.
As a more complicated illustration, we sketch the derivations of the context-free
grammar GF produced for the f-structure F given in (17) and the grammar compris-
ing the rules in (13). This LFG grammar produces two terminal strings for the given
input, John fell today quickly and John fell quickly today. A set of pair-wise compatible
appropriately instantiated rules that yields a description of the input f-structure is,
for example, the one contained in the start rule (28). This set arises from reducing
the node-instantiated licensing rules of the derivation in Figure 4 with the reducing
substitution (20) extended by mapping non-denoting nodes to ?.
(28)
SF ? S:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
S? NP VP
(? SUBJ) = ?? = ?, (root, (root SUBJ) root)
)
,
(
NP? John
(? PRED) = 'JOHN', ((root SUBJ),?)
)
,
(
VP? V ADVP
? = ?? = ?, (root, root root)
)
,
(V? fell
(? PRED) = 'FALL?(SUBJ)?'
(? TENSE) = PAST
, (root,?)
)
,
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, af root)
)
,
(
ADV? today
(? PRED) = 'TODAY', (af,?)
)
,
(
ADVP? ADV
(? ADJ) = (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
The only useful rule of GF for expanding the daughter of rule (28) is the rule (29).
All other admissible distributions of the members of the mother?s rule component
also result in rules of GF. But these other rules cannot be used to produce a terminal
896
W
e
d
e
k
in
d
a
n
d
K
a
p
la
n
L
F
G
G
e
n
e
ra
tio
n
b
y
G
ra
m
m
a
r
S
p
e
cia
liz
a
tio
n
(29)
S:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
S? NP VP
(? SUBJ)= ?? = ?, (root, (root SUBJ) root)
)
,
(
NP? John
(? PRED)= 'JOHN', ((root SUBJ),?)
)
,
(
VP? V ADVP
? = ?? = ?, (root, root root)
)
,
(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)
,
(
ADVP? ADV ADVP
(? ADJ)= (? ELE)? = ?, (root, af root)
)
,
(
ADV? today
(? PRED)= 'TODAY', (af,?)
)
,
(
ADVP? ADV
(? ADJ)= (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED)= 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
NP:(root SUBJ):
{(
NP? John
(? PRED)= 'JOHN', ((root SUBJ),?)
)}
VP:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
VP? V ADVP
? = ?? = ?, (root, root root)
)
,
(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)
,
(
ADVP? ADV ADVP
(? ADJ)= (? ELE)? = ?, (root, af root)
)
,
(
ADV? today
(? PRED)= 'TODAY', (af,?)
)
,
(
ADVP? ADV
(? ADJ)= (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED)= 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(30)
VP:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
VP? V ADVP
? = ?? = ?, (root, root root)
)
,
(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)
,
(
ADVP? ADV ADVP
(? ADJ)= (? ELE)? = ?, (root, af root)
)
,
(
ADV? today
(? PRED)= 'TODAY', (af,?)
)
,
(
ADVP? ADV
(? ADJ)= (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED)= 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
V:root:
{(V? fell
(? PRED)= 'FALL?(SUBJ)?'
(? TENSE)= PAST
, (root,?)
)}
ADVP:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
ADVP? ADV ADVP
(? ADJ)= (? ELE)? = ?, (root, af root)
)
,
(
ADV? today
(? PRED)= 'TODAY', (af,?)
)
,
(
ADVP? ADV
(? ADJ)= (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED)= 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?8
9
7
Computational Linguistics Volume 38, Number 4
string. For instance, when the instantiated S rule
(
S? NP VP
(? SUBJ) = ?? = ?, (root, (root SUBJ) root)
)
is
distributed over the daughters, the derivation will not produce a terminal string because
S is not reachable from either NP or VP in the context-free skeleton of the grammar
in (13). Similarly, the verbal and adverbial categories are not reachable from NP and NP
is not reachable from VP.
We see then that the left daughter of (29) matches the mother of (31) that derives the
terminal symbol ?John:?:??.
(31) NP:(root SUBJ):
{(
NP? John
(? PRED) = 'JOHN', ((root SUBJ),?)
)}
? John:?:?
Now, for the right daughter of (29), only the expansion with (30) gives rise to a terminal
string. By applying (32) to the left daughter of (30) we first derive the terminal symbol
?fell:?:??.
(32)
V:root:
{(V? fell
(? PRED) = 'FALL?(SUBJ)?'
(? TENSE) = PAST
, (root,?)
)}
? fell:?:?
Rule (33) then is the only possible rule of GF whose application (to the right daughter
of (30)) will lead to a terminal string.
(33)
ADVP:root:
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, af root)
)
,
(
ADV? today
(? PRED) = 'TODAY', (af,?)
)
,
(
ADVP? ADV
(? ADJ) = (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
ADV:af:
{(
ADV? today
(? PRED) = 'TODAY'
, (af,?)
)}
ADVP:root:
?
?
?
?
?
?
?
(
ADVP? ADV
(? ADJ) = (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)
?
?
?
?
?
?
?
All other legitimate distributions of the instantiated rules of the mother over the
daughters also produce categories that fail to derive terminal strings. Some of these
distributions will figure in derivations that fail, as we observed earlier, because the LFG
rules predicted in the rule component of our categories collectively do not derive a
terminal string (ADVP is not reachable from ADV in this particular case). This example
illustrates that derivations can also fail to produce any sentence because of mismatches
of the term component of an augmented daughter category and the terms instantiating
the left-hand categories of the rules in the rule component that expand that daughter
category. That is why the alternative rule in GF in which the adverbial daughter has
the triple category ADV:af:
{(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)}
does not produce a terminal
string. Note moreover that condition (iic) of Definition 16 blocks recursions of ADVP
producible by the context-free backbone of the original grammar, because the instan-
tiated recursive ADVP rule
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, af root)
)
cannot be distributed
over the daughters.
For the same reasons, (34) is the only useful rule that matches the right daughter
of (33).
(34)
ADVP:root:
?
?
?
?
?
(
ADVP? ADV
(? ADJ) = (? ELE), (root, ag )
)
,
(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)
?
?
?
?
?
? ADV:ag:
{(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)}
898
Wedekind and Kaplan LFG Generation by Grammar Specialization
With rules (35) and (36) then we obtain the terminal string ?John:?:? fell:?:?
today:?:? quickly:?:??.
(35) ADV:af:
{(
ADV? today
(? PRED) = 'TODAY', (af,?)
)}
? today:?:?
(36) ADV:ag:
{(
ADV? quickly
(? PRED) = 'QUICKLY', (ag,?)
)}
? quickly:?:?
The Cat projection of this string is John fell today quickly, the only sentence whose
derivation GF simulates by starting with rule (28).
The only other derivation of a string with f-structure F is simulated if we use a rule
like (28) except that the ADVP rules are instantiated as in (37), that is, exactly the other
way around.
(37)
(
ADVP? ADV ADVP
(? ADJ) = (? ELE)? = ?, (root, ag root)
)
(
ADVP? ADV
(? ADJ) = (? ELE), (root, af )
)
If we begin with this alternative starting rule, we can derive the string ?John:?:?
fell:?:? quickly:?:? today:?:?? with the corresponding sentence John fell quickly today.
There are alternative derivations in GF that also simulate these two LFG derivations,
and in that sense the grammar GF allows for spurious ambiguities. These derivations
differ from the given ones in that the instantiating constants of CF are biuniquely
renamed (e.g., af by aa and ag by ad) or some of the terminal daughters with no ? in
their annotation are biuniquely instantiated by otherwise unused constants of CF. In the
next section we consider some computational strategies for eliminating rules that fail
to produce terminal strings or give rise to spurious ambiguities.
4. Computational Considerations
So far we imposed only loose restrictions on the ingredients of the generation grammar
GF, and a faithful implementation of the grammar definition may create categories and
rules that are either useless or redundant. Useless rules cannot participate in the simu-
lation of any LFG derivation while redundant ones simulate only the same derivations
as other rules and categories in the grammar. There are a number of techniques for
avoiding the construction of these unnecessary and undesirable grammar elements.
If the equations in an LFG rule provide alternative definitions for one and the same
daughter, a naive implementation would produce distinct but equivalent daughter in-
stantiations. Rule and category instantiations that express only uninformative variation
can be eliminated by normalizing the rule annotations in advance of generation so that
there is exactly one canonical function-assigning equation for each mother-definable
daughter and by using that equation to construct its defining term. Normalization
can be accomplished by exploiting symmetry and substitutivity to reduce the anno-
tations of the rules to some normal form according to an appropriate complexity norm,
as suggested by Johnson (1988). Another off-line computation can identify terminal
daughters that are introduced with rules that do not contain ? and so will never be
interpreted. Without loss of generality we can disregard other instantiating constants
that might be drawn from CF and systematically instantiate all of those terminals with
the distinguished constant ?.
We can remove another major source of redundancy by ignoring derivations that
differ only by renaming of the instantiating constants of CF. This can arise if IRDF
899
Computational Linguistics Volume 38, Number 4
contains rule sets that are identical up to renaming of the instantiating canonical con-
stants, as indicated in Section 3.4. We observed in conjunction with Lemma 3 that the
f-description of every derivation for F can be reduced to an equivalent description that
is satisfied in the canonical model F? expanded by some interpretation of root. Thus the
generation grammar can be constructed by considering only the set IRDF? containing
those elements of IRDF whose instantiated descriptions are modeled by some root
expansion of F?.
Even with these refinements, the last example in Section 3.4 illustrates the fact that
our recipe for constructing GF may produce other useless categories and expansion
rules. These cannot play a role in any derivation either because they are unreachable
from the root symbol SF or because they do not lead to a terminal string. We can borrow
strategies from conventional context-free grammar processing to control the production
of these useless items.
A top?down approach to grammar construction is the simplest way of avoid-
ing categories and rules that are unreachable from the root symbol. It corresponds
most directly to the specification of Definition 16. The algorithm maintains three data-
structures, an agendaA of categories whose expansion rules have yet to be constructed,
a set V of terminal categories and nonterminal categories that have already been consid-
ered for expansion, and a setR of constructed context-free rules. All three structures are
empty at the outset. The first step of the algorithm is to add the root category SF to A.
Then at each subsequent step a category ? is selected from A and moved to V , all rules
?? ?1..?m satisfying conditions (i) (with IRDF? instead of IRDF) and (ii) of Definition 16
are added to the rule setR, and each of the nonterminals ?j not already in V is added to
the agenda. Because Definition 16 provides for a finite number of categories, the agenda
eventually will become empty. At that point the algorithm terminates withR containing
a subset of RF sufficient to simulate all and only the LFG derivations for F. As indicated,
this algorithm has the desirable property of creating just those categories and rules of
GF that are accessible from the root symbol. It is guided incrementally by the c-structure
skeleton of the LFG grammar. It is also guided by properties of the input f-structure as
the rule component for each new category is a subset of some element IRroot of IRDF?.
But this procedure has the disadvantage of typically producing many categories that
derive no terminal string.
An alternative strategy is to construct the categories and rules in bottom?up fashion.
The bottom?up algorithm uses the same three sets, all empty at the outset. Here the first
step is to add to the agenda A all of the elements in the set TF of terminal categories.
In each subsequent step a category is selected from A and moved to V , as in the top?
down approach. In this case, however, we add to R all rules ?? ?1..?m that satisfy
conditions (i) and (ii) of Definition 16 and where the selected category is at least one of
the daughters ?j and all other daughter categories already exist in V . If ? is not SF, we
further require ??s rule component to be a subset of some IRroot so that this process is
also constrained at each step by the input f-structure. The category ? is added to the
agenda if it is not already present in V . This algorithm also terminates when the agenda
is empty. It ensures that every category we construct can derive a terminal string, but it
does not guarantee that every bottom?up sequence will reach the root symbol.
A more serious shortcoming of both strategies is that they presuppose the prior
computation of all elements of IRDF?, but neither specifies how to instantiate those rule
sets in an efficient manner. A straightforward modification of the bottom?up algorithm
can sidestep this difficulty. We can replace the subset test on the rule component of
each ? with a check to see whether the instantiated description of that component is
satisfied in F? expanded by some interpretation of root. This test makes reference just
900
Wedekind and Kaplan LFG Generation by Grammar Specialization
to the canonical model of the input, examining only those features that are relevant
to each potential new category. We reject a category if it fails this test, knowing that
its rule component cannot be a subset of any element of IRDF?. This is similar in spirit
to the step-by-step subsumption test of other bottom?up generation algorithms (e.g.,
Shieber 1988 and Kay 1996). A further restriction is needed to filter the creation of start
rules. Rules of the form SF ? S:root:IR are included inR only when some root expansion
of F? is not only a model for Inst(IR) but a minimal one at that. We know in that case that
we have arrived at one of the elements of IRDF?. The minimality condition is an ana-
logue of the completeness requirement of other algorithms.
The incremental satisfiability test of this modified algorithm depends on the
interpretation of the node constant root, and we saw in Section 3.2 that root may denote
different elements of the universe in different derivations of F. Although its eventual
denotation cannot be uniquely predicted at intermediate steps of the bottom?up
process, we can avoid reconsideration of root denotations already determined to
be unsatisfactory by carrying along the satisfying denotations in an auxiliary data
structure associated with each category in A and V . For an LFG rule r that expands
A with the c-structure categories X1..Xm, a rule A:t:IR ? X1:t1:IR1..Xm:tm:IRm is only
added to R if there is at least one root expansion of F? that satisfies Inst(r, (t, t1..tm))
and whose root denotation is shared across all daughters. The root denotations of
all such F? expansions are then associated with A:t:IR. The complexity of this test is
proportional to the complexity of the instantiated description of the LFG rule and
not of the instantiated description of the entire rule component IR, because the rule
components of the daughter categories do not need to be reevaluated.
For further optimizations we can make use of context-free strategies that take top?
down and bottom?up information into account at the same time. For instance, we can
simulate a left-corner enumeration of the search space, considering categories that are
reachable from a current goal category and match the left corner of a possible rule. As
another option, we can precompute a reachability table for the context-free backbone
of G and use it as an additional filter on rule construction. In general, almost any of
the traditional algorithms for parsing context-free grammars can be reformulated as
a strategy for avoiding the creation of useless categories and rules. We can also use
enumeration strategies that focus on the characteristics of the input f-structure. A head-
driven strategy (cf., e.g., Shieber et al 1990; van Noord 1993) identifies the lexical heads
first, finds the rules that expand to them, and then uses information associated with
those heads, such as their grammatical function assignments, to pick other categories
to expand.
5. Other Chart-based Approaches
A bottom?up strategy for grammar construction comes closest to the algorithms of
previous chart-based generation proposals. There is a correspondence between the
edges that are added incrementally to a generation chart and the context-free rules that
we add to the grammar. But chart edges in these proposals typically collapse some of
the distinctions that we have in our rules and categories, and therefore these algorithms
cannot faithfully interpret the full set of grammatical dependencies. For some grammars
and inputs they may produce strings that should not belong to the generated language.
In an attempt to guarantee termination these algorithms may also include grammar
restrictions or processing limits that unduly narrow the set of legitimate results. We
will illustrate some correspondences and differences with the modified (F?-guided)
901
Computational Linguistics Volume 38, Number 4
algorithm sketched in the previous section by comparing its first few steps with the
operations of Kay?s (1996) chart-generation algorithm.
To facilitate the comparison, we have adapted the grammar for one of Kay?s exam-
ples to an equivalent grammar in the LFG formalism. The LFG grammar is given in (38).
(38) a. S ? NP VP
(? ARG1) = ? ? = ?
b. NP ? DET N
? = ? ? = ?
c. VP ? V NP
? = ? (? ARG2) = ?
d. DET ? the
(? SPEC) = DEF
e. N ? cat
(? PRED) = 'CAT'
f. N ? dog
(? PRED) = 'DOG'
g. V ? saw
(? PRED) = 'SEE?(ARG1)(ARG2)?'
(? TENSE) = PAST
This grammar with its particular lexical rules has the sentence The dog saw the cat in its
language, and that sentence is assigned the f-structure in (39), a direct encoding of Kay?s
semantic specification.19 This shows the f-structure elements that are used to define the
constants in CF.
(39) s
PRED
TENSE ARG1
ARG2
'SEE?(ARG1)(ARG2)?' PAST d c
PRED SPEC SPEC PRED
'DOG' DEF 'CAT'
Taking this f-structure as input, the first step of our bottom?up algorithm is to
initialize the agenda with the terminal categories TF = {the:?:?, dog:?:?, ..}. Those cate-
gories are sufficient to complete the right sides of the given lexical rules, and so in the
next steps the terminal categories are moved to V and rules including those in (40) are
constructed. These are the ones that can potentially contribute to the generation of the
noun phrase the dog: The instantiated descriptions produced with these terms pass our
satisfiability test on F?.
(40) a. DET:ad:
{(
DET? the
(? SPEC) = DEF, (ad,?)
)}
? the:?:?
b. N:ad:
{(
N? dog
(? PRED) = 'DOG', (ad,?)
)}
? dog:?:?
c. DET:(root ARG1):
{(
DET? the
(? SPEC) = DEF, ((root ARG1),?)
)}
? the:?:?
d. N:(root ARG1):
{(
N? dog
(? PRED) = 'DOG', ((root ARG1),?)
)}
? dog:?:?
19 Kay provides a flat, unordered collection of separate propositions as input to the generation process,
but the difference between a flat and hierarchical arrangement is not material to our discussion. We
have translated his constants s, d, c into the elements of our f-structure input, and we have mapped his
propositions (dog(d), arg1(s, d)..) into equivalent attribute?value relationships. By the same token,
because here we are focusing on the organization of data structures, we note without further comment
that his active-passive computational schema is but one way of specializing our general bottom?up
algorithm.
902
Wedekind and Kaplan LFG Generation by Grammar Specialization
Rules (40a,b) correspond directly to the lexical edges that are added to the chart in the
initialization step of Kay?s algorithm. A lexical edge includes the word (the Cat projec-
tion of our right-hand complex category), a syntactic category (a left-hand c-structure
category) paired with an instantiation term, and instantiated semantic propositions (an
instantiated description collected from our rule annotations).20 The chart edges that
parallel the first two rules are shown in (41).
(41) Words Category Semantics
the DET:ad (ad SPEC) = DEF
dog N:ad (ad PRED) = 'DOG'
Note that the instantiating term that corresponds to Kay?s semantic index d is the
canonical constant ad drawn from CF. It is a significant limitation that ground-level
terms like these are the only ones available for instantiation. We observed at the be-
ginning of Section 3 that the set CF is in general not large enough to equivalently
reproduce the discriminations that are required for grammars that allow for undefinable
daughters and path equations and for inputs that contain reentrancies. Thus, as origi-
nally presented, Kay?s algorithm is correct only for a very restricted set of unification
grammars.
In contrast, we draw from the larger term set TF that includes in addition the
collection of path-terms that combine constants with sequences of attributes. Rules
(40c,d) make use of the path-term (root ARG1), and it is not unreasonable to extend
Kay?s approach to create the corresponding edges shown in (42). This would allow his
algorithm to be applied to a broader set of grammars and inputs.
(42) the DET:(root ARG1) (root ARG1 SPEC) = DEF
dog N:(root ARG1) (root ARG1 PRED) = 'DOG'
Continuing with the bottom?up strategy, the categories above will be moved from
the agenda to V , the rule in (43) will be created from the right-side categories of (40c,d),
another NP rule will be created from the constant-instantiated rules in (40a,b), and both
new categories will be placed on the agenda.21
(43)
NP:(root ARG1):
?
?
?
?
?
?
?
?
?
?
?
(
NP? DET N
? = ?? = ?, ((root ARG1), (root ARG1) (root ARG1))
)
,
(
N? dog
(? PRED) = 'DOG', ((root ARG1),?)
)
,
(
DET? the
(? SPEC) = THE, ((root ARG1),?)
)
?
?
?
?
?
?
?
?
?
?
?
?
DET:(root ARG1):
{(
DET? the
(? SPEC) = THE, ((root ARG1),?)
)}
N:(root ARG1):
{(
N? dog
(? PRED) = 'DOG', ((root ARG1),?)
)}
20 Kay?s instantiated semantics corresponds more directly to the third components of the categories of the
less sophisticated grammar construction of Kaplan and Wedekind (2000). These instantiated descriptions
collapse some of the distinctions of our third-component rules that are not needed for the limited range
of dependencies that Kay is considering.
21 The NP based on the rules (40a,b) will not survive into a larger derivation in our framework. This is
because all NP daughters are mother-definable in this grammar, and therefore the ad instantiation is not
appropriate for the ARG1 daughter of S.
903
Computational Linguistics Volume 38, Number 4
Our extended version of Kay?s algorithm also combines determiner and noun edges to
make up the NP edges in (44).
(44) the dog NP:(root ARG1) (root ARG1) = (root ARG1),
(root ARG1 SPEC) = DEF, (root ARG1 PRED) = 'DOG'
the dog NP:ad ad = ad,
(ad SPEC) = DEF, (ad PRED) = 'DOG'
These edges reveal another significant difference between Kay?s algorithm and our
approach. The Words fields now consist of sequences of words, the (Cat projections
of the) terminal strings for the full noun phrases. These strings are constructed by
concatenating the Words from the two component edges in the order specified by the
grammar rule that justifies the combination. That is, an edge does not incorporate the
justifying rule but instead records a single member of the yield of the subtree beneath
the category of the edge. The effect is that the incremental construction of the chart is
intermixed with the process of recursively assembling the terminal strings of longer and
longer phrases. The advantage of Kay?s strategy is that after termination the generated
strings can be read out as the Words of all the edges whose Category is the start category
paired with the top-level index and whose Semantics exactly matches the original input:
There is no need for a separate context-free generation phase.
The disadvantage is that an additional condition must be imposed to guarantee
that only a finite number of edges will be created so that the chart-construction process
does in fact terminate. Kay proposes a use-once restriction that bounds the size of the
derivable constituents by the number of predicates in the input. For some grammars
and inputs his algorithm will only produce a proper subset of the full set of generable
strings. Another disadvantage in comparison to our approach and other approaches in
the chart-based family is that Kay?s chart edges do not record intermediate generation
results in a compact form that allows operations on the generated string set to be carried
out in advance of enumerating the individual strings.22
Kay?s algorithm is one of a family of chart-based approaches that differ in detail
but have similar characteristics at an abstract level. A common thread is that each
edge contains a semantic or feature-structure representation aggregated from all of the
edges in the subtree that it dominates, and edge creation is filtered by testing whether
these representations subsume the generation input. Each algorithm in the family also
imposes one or more additional restrictions in an attempt to guarantee termination of
the string generation process. Kay appeals to a use-once processing condition, as noted
earlier, that ensures termination but may only produce a proper subset of the complete
output set.
Shieber?s (1988) algorithm and its refinements are closer to our approach in that
they do not associate individual terminal strings with the edges of the chart. Each edge
contains a semantic or feature-structure representation and a sequence of immediate
daughter edges from which that representation can be assembled. The individual sub-
strings consistent with that representation are obtained by a recursive traversal reaching
down to the terminal edges. The chart-construction phase of these algorithms (and our
grammar construction) will not terminate if the number of distinct edges is not bounded
by the size of the input. This may be the case for cyclic inputs, because they have
22 Maxwell (2006) describes a variant of Kay?s algorithm that provides a more compact representation for
the generated string sets and also deals efficiently with disjunctive input structures.
904
Wedekind and Kaplan LFG Generation by Grammar Specialization
infinitely many distinct unfoldings all of which subsume the input. A separate question,
even with a bounded chart, is whether the string-production traversal is guaranteed
to terminate with a finite set of strings. A grammar may give rise to infinitely many
strings if it has recursive or iterative rules whose feature structures subsume the same
portion of the input. Any finite set of output strings for such a grammar and input will
necessarily be incomplete.
Shieber suggests that the end-to-end generation process will terminate and produce
a finite but complete set of output strings for a restricted class of semantically mono-
tonic grammars. Shieber?s condition requires that the semantic representation of every
mother phrase is subsumed by the semantic structure of each of its daughter phrases.
In LFG terms this condition amounts to the requirement that each daughter is mother-
definable (with an annotation of the form (? ?) = ? for |?| ? 0) and, as a consequence,
that strings can be generated only for single-rooted inputs. On deeper analysis, how-
ever, we see that this restriction is not sufficient to ensure that the generation process
will terminate with a finite output set. It does not by itself preclude grammars that
assign cyclic feature structures and therefore the chart-construction process may be
unbounded. And with an acyclic input and a finite chart the complete set of output
strings may still be unbounded since several daughters in a recursive rule may subsume
exactly the same portion of the mother?s semantic representation. A formal example
of this is the monotonic grammar in (8) that produces the string set {an bn | 1 ? n}. A
stronger restriction on the form of the annotations, namely, that ? is never empty, will
guarantee a finite chart and a finite and complete output set, but monotonic grammars
in this sense cannot naturally identify the functional or semantic head-daughters that
figure prominently in so many linguistic descriptions. It seems that monotonicity is not
a particularly helpful restriction and that some other constraint, either on grammars or
processing steps, is needed to guarantee an output set containing only a finite number
of syntactic variants (cf., e.g., Neumann 1994; Moore 2002).
If we translate Shieber?s and other similar algorithms to our framework, we see that
their instantiations need only terms involving root and none of the constants in CF or the
terms containing those constants.23 This is because these algorithms are not set up to
control subsumption accurately for multi-rooted inputs and grammars with mother-
undefinable daughters, and in fact their result set may be incorrect in those cases. As
we have demonstrated, maintaining all of the proper discriminations requires the larger
term set and a mechanism with the same effect as our appropriateness and compatibility
conditions.
Comparing other chart-based generation proposals to our bottom?up strategy for
creating a generation grammar has brought out some similarities but also highlighted
some important differences. Chart edges contain information that summarizes the syn-
tactic and semantic contribution of their subtrees and also allows for the correlated
terminal strings to be read out by a straightforward traversal. These algorithms cannot
attain correctness, completeness, and termination without imposing limits on the kinds
of grammatical dependencies that the generator can faithfully interpret, the range of
structures that can be provided as input, or the size and number of output strings that
can be produced. Our approach operates correctly on a larger class of grammars and
inputs because we have more instantiating terms and therefore are able to maintain
23 Moore (2002) observed that the basic properties of the algorithm do not change if semantically vacuous
constituents are allowed. In this case the translation would require the additional term ? to reduce
nodes that are not interpreted in a model of the f-description.
905
Computational Linguistics Volume 38, Number 4
appropriate discriminations without special restrictions. The resulting grammar gives
a finite encoding of the complete set of generated outputs in a well-understood formal
system. These can be enumerated on demand in our separate context-free generation
phase.
6. Cycles
We have established the context-free result only for acyclic f-structures; the result does
not hold for cyclic inputs. This is because the f-structures that correspond to subderiva-
tions of a derivation of a cyclic structure are not necessarily bounded by the size of the
input. So we might need an infinite number of terms in order to reproduce correctly
any discrimination made in the f-description for some subderivation of a cyclic input
structure. The following example demonstrates that the set of strings that a grammar
relates to a particular cyclic input might not be context-free.24
Consider the LFG grammar G = ({S,A,C}, {a, b, c}, S,R) with the annotated rules
R given in (45).
(45) a. S ? A C
(? F) = ? (? G) = ?
(? G) = ?
(? F) = (? F)
b. A ? a A b
(? G) = ?
(? F) = (? F)
c. C ? c C
(? G) = ?
d. A ? a b
(? F) = (? H)
e. C ? c
(? H F) = (? H G)
Now, let F be the following input f-structure.
(46) F H
G
The set of terminal strings that are derivable with F is {an bn cn | 1 ? n}, a language that
is not context-free. Each top?down derivation for a terminal string that gets assigned the
given input f-structure F starts with the S rule. Suppose ai bi C is derived from S by i? 1
(i > 0) applications of (45b) and one application of (45d). Such a string gets assigned a
c-structure and an f-structure of the form depicted in Figure 7 where the C node is
mapped to the leftmost G value. The f-structure corresponding to the subderivation
24 Wedekind (2006) provides another example of such a grammar. This runs counter to an assertion in
Kaplan and Wedekind (2000) that cyclic structures lie within the scope of our context-free analysis. This
was based on reasoning that we now understand to be incorrect.
906
Wedekind and Kaplan LFG Generation by Grammar Specialization
i times
?
?
?
?
?
S
A C
A
a a b b
? ?? ? ? ?? ?
ai bi
F
i times
? ?? ?
G G
F
F
F
H
Figure 7
The derivation of ai bi C in grammar (45).
up to this point is arbitrarily larger than the original input, but the rest of the derivation
forces the distinguished F, G, and H attributes to collapse into the simple cycles. Because
the rightmost G value is the only position where this structure can be folded up to
F using the annotations of (45e), (45c) has to be applied exactly i? 1 times yielding
ai bi ci?1 C. With one application of (45e) we obtain F and the sentence ai bi ci. Thus
GenG(F) = {an bn cn | 1 ? n}.
In general our grammar construction will produce correct outputs for the term
set drawn from any finite unfolding of a cyclic input structure, but a complete char-
acterization of the output strings would require an infinite term set. We have not yet
investigated the formal properties of the languages that are related to cyclic structures.
It is an open research question whether a more expressive system (e.g., indexed gram-
mars or other forms of controlled grammars) can give a finite characterization of the
complete string set and whether our context-free grammar construction can be extended
to produce such a formal encoding.
7. Other Descriptive Devices
We have shown that the context-free grammar of Definition 16 produces the strings in
GenG(F) for an LFG grammar G that characterizes f-structures by means of equality and
function application, the most primitive descriptive devices of the LFG formalism. In
this section we extend the grammar-construction procedure so that it produces context-
free generation grammars that simulate the other formal devices that were originally
proposed by Kaplan and Bresnan (1982).25
Completeness and Coherence. The result holds trivially when we also take into
account LFG?s devices for enforcing the subcategorization requirements of individual
predicates, the completeness and coherence conditions. Both conditions are concerned
with the semantic-form PREDicate values that consist of a predicate and a list of gov-
ernable grammatical functions, as for example, 'FALL?(SUBJ)?' with the list ?(SUBJ)? and
'JOHN' with the empty list. An f-structure is complete if each substructure (including the
entire structure) that contains a PRED also contains all governable grammatical functions
its semantic form subcategorizes for. And an f-structure is coherent if all its governable
functions are subcategorized by a local semantic form. If an input f-structure F is not
complete and coherent, the LFG derivation relation ?G does not associate it with any
25 Because LFG theory has evolved away from the original c-structural encoding of long-distance
dependencies, we will not consider it here. In Wedekind and Kaplan (forthcoming) we describe the
construction for grammars that use functional uncertainty, the device that superseded the initial
mechanism for characterizing long-distance dependencies.
907
Computational Linguistics Volume 38, Number 4
strings, and the set GenG(F) is empty. Thus, when we determine by inspection that an
input f-structure fails to satisfy these conditions, we maintain the context-free result by
assigning it a trivial grammar that generates the empty context-free language.
C-Structure Regular Predicates and Disjunctive Functional Constraints. The con-
struction in Section 3.3 produces context-free generation grammars for LFG grammars
whose c-structure rules are of an elementary form: Their right-hand sides consist of
concatenated sequences of annotated categories, and the equations in the annotation
sets are interpreted as simple conjunctions of f-structure requirements. The full LFG
notation is more expressive, allowing functional requirements to be stated as arbi-
trary Boolean combinations of basic assertions. It also allows the right-hand sides of
c-structure rules to denote arbitrary regular languages over annotated categories. Rules
with the richer notation can be normalized to rules of the necessary elementary form
by simple transformations. First, in the regular right-side of each rule every category
X with a Boolean combination of primitive annotations is replaced by a disjunction
of X?s each associated with one of the alternatives of the disjunctive normal form of
the original annotation. Then the augmented regular right-sides are converted to a
collection of right-linear rewriting rules by systematically introducing new nontermi-
nals and their expansions, as described by Chomsky (1959) (see also Hopcroft and
Ullman 1979). The new nonterminals are annotated with ? = ? equations as needed
to ensure that f-structure requirements are properly maintained. The result of these
transformations is a set of productions all of which are in conventional context-free
format and have no internal disjunctions and which together define the same string/
f-structure mapping as a grammar encoded in the original, linguistically more expres-
sive, notation.
Constraining Statements and Negation. The statements in an LFG f-description
are divided into two classes: defining and constraining statements. The constraining
statements are evaluated once all defining statements have been processed and a mini-
mal model (of the defining statements) has been constructed. The constraining devices
introduced by Kaplan and Bresnan (1982) are constraining equations and inequali-
ties, and existential and negative existential constraints. If a constraining statement is
contained in an f-description FD, it is evaluated against a minimal model M of the
defining statements of FD in the obvious way: M |= t =c t? iff M |= t = t? (constraining
equation), M |= t iff ?t?(M |= t = t?) (existential constraint), M |= ?? iff M |= ? (negation
of a constraining or defining statement).
We can extend our grammar construction to descriptions with constraining state-
ments by adjusting the definition of IRDF. We modify condition (ii) of Definition 15
so that M|? ?= F for a minimal model M of just the defining statements of Inst(IR)
and additionally require M |= ? for all constraints ? of Inst(IR). Then a context-free
grammar based on this revised definition will properly reflect the defining/constraining
distinction.
The proof of this depends on one further technicality, however. Recall that the con-
structions that we used in the proof of our main theorem yield in both proof directions
FD[?] = Inst(IRroot). As a consequence, the constraining statements in Inst(IRroot) are
exactly the ones that result from those in FD by substitution with ?. Suppose that M
and Mroot are minimal models of the defining part of FD and Inst(IRroot), respectively.
In order to establish also that M satisfies all constraints in FD iff Mroot satisfies the ones
contained in Inst(IRroot), it is sufficient to show that M |= t = t? iff Mroot |= t[?] = t?[?]
holds for all denoting terms. This follows (with M? as Mroot) from the isomorphic
mapping of term denotations provided by Lemma 2?, a slightly stronger version of
Lemma 2.
908
Wedekind and Kaplan LFG Generation by Grammar Specialization
Lemma 2?
Let c and ? be a derivation with f-description FD and f-structure F in G. If ? is a reducing
substitution for c and ? and M = (U , I) and M? = (U ?, I?) are minimal models of the defining
parts of FD and FD[?], respectively, then there is an isomorphism h between M|? and M?|?
such that h(I(t)) = I?(t[?]) for each interpreted term t or t[?].26
Membership Statements. Membership statements are formulas of the form t? ? t.
Membership in LFG is interpreted just as a binary relation between functional ele-
ments, and a model satisfies a membership statement t? ? t iff the membership relation
holds between the denotation of t? and the denotation of t. Membership statements
may introduce daughters that are undefinable in terms of their mother and therefore
may be instantiated by CF constants as we illustrated earlier in our treatment of the
(? ADJ) = (? ELE) annotation. Then, if we expand the isomorphism-based determina-
tion of the equivalence of feature structures and feature descriptions in the usual way
to sets and set descriptions, membership statements can be handled by our original
construction without further modification.27
Semantic Form Instantiation. As described earlier, semantic forms are the single-
quoted values of PRED attributes in terms of which the completeness and coherence
conditions are defined. They are also instantiated, in the sense that for each occurrence
of a semantic form in a derivation a new and distinct indexed form is chosen. Because
of this special property, semantic forms occurring in annotated rules may be regarded
as metavariables that are substituted by the instantiation procedure similar to the
familiar ? and ? symbols. The distinguishing indices on semantic forms are usually
only displayed in a graphical representation of an f-structure if this is necessary for
clarity, but distinctively indexed semantic forms are always available for appropriately
instantiating the LFG rules, just like the other constants that we draw from the input
structure. We can extend the mechanism for controlling the correct instantiation of
undefinable daughters to ensure that the semantic forms of all simulated derivations
are correctly instantiated. As part of an appropriate instantiation of an LFG rule we also
substitute for the prototypical semantic forms in the rule distinct indexed forms, drawn
from F, and we expand the compatibility condition to this larger set of instantiations.
8. Consequences and Observations
We have shown that a given LFG grammar can be specialized to a context-free grammar
that characterizes all and only the strings that correspond to a given (acyclic) f-structure.
We can now understand different aspects of generation as pertaining either to the way
the specialized grammar GF is constructed or to well-known properties of context-free
grammars and context-free generation.
It follows as an immediate corollary, for example, that it is decidable whether the set
GenG(F) is empty, contains a finite number of strings, or contains an infinite number of
strings. This can be determined by inspecting GF with standard context-free tools, once
26 The proof requires an elaboration of the argument used in the proof of Lemma 2. Following the inductive
construction of that proof, it is easy to see that I(t) = Ii(t[?i]) holds for all terms t and t[?i] that are
interpreted in M = (U , I) or Mi = (U , Ii ). Because there must be an isomorphism h between M|Nc| and
any other minimal model M? = (U ?, I? ) of the defining part of FD[?], h(I|Nc|(t[?])) = I
?(t[?]) and thus
h(I(t)) = I?(t[?]) for each interpreted term t or t[?].
27 Rounds (1988) proposes a bisimulation-based characterization of sets and set membership. This would
require a more sophisticated analysis, but it is more of mathematical than linguistic interest.
909
Computational Linguistics Volume 38, Number 4
it has been constructed. If the language is infinite, we can make use of the context-free
pumping lemma to identify a finite number of short strings from which all other strings
can be produced by repetition of subderivations. Wedekind (1995) first established the
decidability of LFG generation and proved a pumping lemma for the generated string
set; our theorem provides alternative and very direct proofs of these previously known
results.
We also have an explanation for another observation of Wedekind (1995). Kaplan
and Bresnan (1982) showed that the Nonbranching Dominance Condition (sometimes
called Off-line Parsability) is a sufficient condition to guarantee decidability of the
membership problem. Wedekind noted, however, that this condition is not necessary
to determine whether a given f-structure corresponds to any strings. We now see more
clearly why this is the case: If there is a context-free derivation for a given string that
involves a nonbranching dominance cycle, we know that there is another derivation
for that same string that has no such cycle. Thus, the generated language is the same
whether or not derivations with nonbranching dominance cycles are allowed.
There are practical consequences to the two phases of LFG generation. The grammar
GF can be provided to a client as a finite representation of the set of perhaps infinitely
many strings that correspond to the given f-structure, and the client can then control
the process of enumerating individual strings. The client may choose to produce the
shortest ones just by avoiding recursive category expansions. Or the client may apply
an n-gram model (Langkilde 2000), a stochastic context-free grammar model (Cahill
and van Genabith 2006) or a more sophisticated statistical language model trained
on a collection of derivations to identify the most probable derivation and thus the
presumably most fluent sentence from the set of possibilities (Velldal and Oepen 2006;
de Kok, Plank, and van Noord 2011; Zarrie?, Cahill, and Kuhn 2011).
We have assumed in our construction that terminals are morphologically un-
analyzed, full-form words. A more modular arrangement is to factor morphological
generalizations into a separate formal specification with less expressive power than
LFG rules can provide, namely, a regular relation (Karttunen, Kaplan, and Zaenen 1992;
Kaplan and Kay 1994). The analysis of a sentence then consists of mapping the string
of words into a string of morphemes to which the LFG grammar is then applied. The
full relation between strings of words and associated f-structures is then the compo-
sition of the regular morphology with an LFG language over morpheme strings. To
generate with such a combined system, we can produce the context-free morpheme
strings corresponding to the input f-structure, and then pass those results through the
morphology. Because the class of context-free languages is closed under composition
with regular relations and regular relations are closed under inversion, the resulting set
of word strings will remain context-free.
Our proof also depends on the assumption that the input F is fully specified so
that the set of possible instantiations is finite. Dymetman (1991), van Noord (1993),
and Wedekind (1999, 2006) have shown that it is in general undecidable whether or
not there are any strings associated with a structure that is an arbitrary extension of
the f-structure provided as the input. Indeed, our proof of context-freeness does not
go through if we allow new elements to be hypothesized arbitrarily, beyond the ones
that appear in F; if this is permitted, we cannot establish a finite bound on the number
of possible categories. This is unfortunate, because there may be interesting practical
situations in which it is convenient to leave unspecified the value of a particular feature.
If we know in advance that there can be only a finite number of possible values for
an underspecified feature, however, the context-free result can still be established. We
create from F a set of alternative structures {F1, ..,Fn} by filling in all possible values of
910
Wedekind and Kaplan LFG Generation by Grammar Specialization
the unspecified features, and for each of them we produce the corresponding context-
free grammar. Because a finite union of context-free languages is context-free, the set
of strings generated from any of these structures must again remain in that class.
Of course, this is not a particularly efficient technique: It introduces and propagates
features that the grammar may never actually interrogate, and it needlessly repeats the
construction of common subgrammars that do not make reference to the alternative fea-
ture specifications. The amount of computation may be reduced by adapting methods
from the parsing literature that operate on conjunctive equivalents of disjunctive feature
constraints (e.g., Karttunen 1984; Maxwell and Kaplan 1991).
Our theorem helps us to understand better the problem of ambiguity-preserving
generation. We showed previously that the problem is undecidable in the general case
(Wedekind and Kaplan 1996). But our generation result does enable us to make that
decision under certain recognizable circumstances, namely, if the intersection of the
sentence sets assigned to the different f-structures is computable. This is true if the
sentences belong to some formally restricted subsets of the context-free languages, for
example, finite sets or regular languages; this is the unstated presupposition of Knight
and Langkilde?s (2000) parse-forest technique. For a set of f-structures {F1, ..,Fn} we
construct the context-free grammars GFi and inspect them with standard context-free
tools to determine whether L(GFi ) belongs to an intersectable subclass (i = 1, ..,n). If
each of them meets this condition, we can compute the intersection
n
?
i=1
L(GFi ) to find any
sentences that are derived ambiguously with f-structures F1, ..,Fn.
We have shown in this article that the context-free property also holds for other
descriptive devices as originally proposed by Kaplan and Bresnan (1982). In Wedekind
and Kaplan (forthcoming) we broaden the grammar-construction procedure so that
it produces context-free generation grammars that simulate the more sophisticated
mechanisms that were introduced and adopted into later versions of the LFG formalism.
Among these are devices for the f-structure characterization of long-distance depen-
dencies and coordination: functional uncertainty (Kaplan and Maxwell 1988a; Kaplan
and Zaenen 1989), set distribution for coordination, and the interaction of uncertainty
and set distribution (Kaplan and Maxwell 1988b). We also extend to devices whose
evaluation depends on properties of the c-structure to f-structure correspondence,
namely, functional categories and extended heads (Zaenen and Kaplan 1995; Kaplan
and Maxwell 1996) and functional precedence (Bresnan 1995; Zaenen and Kaplan 1995).
Of course, the context-free result trivially holds for purely abbreviatory notations
such as templates, lexical rules, and complex categories (Butt et al 1996; Kaplan and
Maxwell 1996; Dalrymple, Kaplan, and Holloway King 2004; Crouch et al 2008); these
clearly help in expressing linguistic generalizations but can be formally treated in the
obvious way by translating their occurrences into the more basic descriptions that they
abbreviate. In contrast, the restriction operator (Kaplan and Wedekind 1993) requires
more careful consideration. Restriction can cause the functional information associated
with intermediate c-structure nodes not to be included in the f-structures of higher
nodes. This is formally quite tractable if the restricted information is provided to the
generator as a separately rooted f-structure. Otherwise, the f-structure input is essen-
tially underspecified, and thus, as discussed earlier, a context-free generation grammar
can be produced just in case restriction can eliminate only a finite amount of information
(see also Wedekind 2006).
A final comment concerns the generation problem for other high-order grammatical
formalisms. The PATR formalism also augments a context-free backbone with a set of
feature-structure constraints, but it differs from LFG in that its metavariables allow
911
Computational Linguistics Volume 38, Number 4
constraints on one daughter to refer directly to sister feature structures that may not
be mother-definable. It is relatively straightforward to extend our lemmas and theorem
so that they apply to a more general notion of definability that encompasses sisters
as well as mothers. We can thus establish the context-free result for a broader family
of formalisms that share the property of being endowed with a context-free base. On
the other hand, it is not clear whether the string set corresponding to an underlying
Head-driven Phrase Structure Grammar (HPSG) feature structure is context-free. HPSG
(Pollard and Sag 1994) does not make direct use of a context-free skeleton, and op-
erations other than concatenation may be used to assemble a collection of substrings
into an entire sentence. We cannot extend our proof to HPSG unless the effect of these
mechanisms can be reduced to an equivalent characterization with a context-free base.
Grammars written for the ALE system?s logic of typed feature structures (Carpenter and
Penn 1994), however, do have a context-free component and therefore are amenable to
the treatment we have outlined.
In sum, this article offers a new way to conceptualize the generation problem
for LFG and other higher-order grammatical formalisms with context-free backbones.
Distinguishing the grammar-specialization phase from a string-enumeration phase
provides a mathematical framework for understanding the formal properties of the
generated string sets. It also provides a framework for analyzing and understanding the
computational behavior of existing approaches to generation. Existing algorithms oper-
ate properly on restricted grammars and inputs and thus only approximate a complete
solution to the problem. They typically implement particular techniques for optimizing
the size of the search space and bounding the amount of computation required by the
generation process. Our formulation can allow a larger and perhaps more attractive set
of candidates to be safely considered, and it also makes available a collection of familiar
tools that may suggest new ways of improving algorithmic performance.
From a more general perspective, there has been no deep tradition for the formal
analysis of higher-order generation akin to the richness of our mathematical and com-
putational understanding of parsing. The approach outlined in this article, we hope,
will serve as a major step in redressing that imbalance.
Acknowledgments
We are indebted to John Maxwell for many
fruitful and insightful discussions of the LFG
generation problem. Hadar Shemtov, Martin
Kay, and Paula Newman have also offered
criticisms and suggestions that have helped
to clarify many of the mathematical and
computational issues. We also thank the
anonymous reviewers for their valuable
comments on an earlier draft. This work was
carried out while R. M. K. was at the Palo
Alto Research Center and at Microsoft
Corporation.
References
Bar-Hillel, Yehoshua, Micha A. Perles, and
Eliahu Shamir. 1961. On formal properties
of simple phrase structure grammars.
Zeitschrift fu?r Phonetik, Sprachwissenschaft,
und Kommunikationsforschung, 14:143?172.
Billot, Sylvie and Bernard Lang. 1989. The
structure of shared forests in ambiguous
parsing. In Proceedings of the 27th Annual
Meeting of the Association for Computational
Linguistics, pages 143?151, Vancouver.
Bresnan, Joan. 1995. Linear order, syntactic
rank, and empty categories: On weak
crossover. In Mary Dalrymple, Ronald M.
Kaplan, John T. Maxwell III, and Annie
Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, CSLI
Publications, Stanford, CA, pages 241?274.
Bresnan, Joan. 2000. Optimal syntax.
In Joost Dekkers, Frank van der Leeuw,
and Jeroen van de Weijer, editors,
Optimality Theory: Phonology, Syntax and
Acquisition. Oxford University Press,
Oxford, pages 334?385.
Butt, Miriam, Helge Dyvik, Tracy Holloway
King, Hiroshi Masuichi, and Christian
Rohrer. 2002. The parallel grammar
project. In Proceedings of the Workshop on
912
Wedekind and Kaplan LFG Generation by Grammar Specialization
Grammar Engineering and Evaluation,
pages 1?7, Taipei.
Butt, Miriam, Tracy Holloway King,
Maria-Eugenia Nin?o, and Fre?de?rique
Segond. 1996. A Grammar Writer?s
Cookbook. CSLI Publications, Stanford, CA.
Cahill, Aoife and Josef van Genabith.
2006. Robust PCFG-based generation
using automatically acquired LFG
approximations. In Proceedings of the 21st
International Conference on Computational
Linguistics and the 44th Annual Meeting of
the Association for Computational Linguistics,
pages 1033?1040, Sydney.
Carpenter, Bob and Gerald Penn. 1994.
ALE 2.0 user?s guide. Technical
report, Carnegie Mellon University,
Pittsburgh, PA.
Carroll, John A., Ann Copestake, Dan
Flickinger, and Victor Poznan?ski.
1999. An efficient chart generator for
(semi-)lexicalist grammars. In Proceedings
of the 7th European Workshop on Natural
Language Generation, pages 86-95,
Toulouse.
Carroll, John A. and Stephan Oepen.
2005. High efficiency realization for a
wide-coverage unification grammar. In
Proceedings of the 2nd International Joint
Conference on Natural Language Processing,
pages 165?176, Jeju Island.
Chomsky, Noam. 1959. On certain formal
properties of grammars. Information and
Control, 2:137?167.
Crouch, Richard, Mary Dalrymple, Ronald
M. Kaplan, Tracy Holloway King, John T.
Maxwell III, and Paula Newman. 2008.
XLE documentation. Technical report,
Palo Alto Research Center, Palo Alto, CA.
Available at http://www2.parc.com/isl/
groups/nltt/xle/doc/xle toc.html.
Dalrymple, Mary, Ronald M. Kaplan,
and Tracy Holloway King. 2004.
Linguistic generalizations over
descriptions. In Proceedings of the
International Lexical-Functional
Grammar Conference 2004,
pages 199?208, Stanford, CA.
de Kok, Danie?l, Barbara Plank, and
Gertjan van Noord. 2011. Reversible
stochastic attribute?value grammars.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 194?199, Portland, OR.
de Kok, Danie?l and Gertjan van Noord. 2010.
A sentence generator for Dutch. In Eline
Westerhout, Thomas Markus, and Paola
Monachesi, editors, Computational
Linguistics in the Netherlands 2010: Selected
Papers from the 20th CLIN Meeting.
Netherlands Graduate School of
Linguistics, Utrecht, pages 75?90.
Dipper, Stefanie. 2003. Implementing and
Documenting Large-scale Grammars?
German LFG. Ph.D. thesis, University
of Stuttgart.
Dymetman, Marc. 1991. Inherently reversible
grammars, logic programming and
computability. In Proceedings of the ACL
Workshop: Reversible Grammar in Natural
Language Processing, pages 20?30,
Berkeley, CA.
Dymetman, Marc. 1997. Charts,
interaction-free grammars, and the
compact representation of ambiguity.
In Proceedings of the 15th International
Joint Conference on Artificial Intelligence,
pages 1002?1009, Nagoya.
Hopcroft, John E. and Jeffrey D. Ullman.
1979. Introduction to Automata Theory,
Languages, and Computation.
Addison-Wesley, Reading, MA.
Johnson, Mark. 1988. Attribute?Value
Logic and the Theory of Grammar. CSLI
Publications, Stanford, CA.
Johnson, Mark and Stefan Riezler. 2002.
Statistical models of syntax learning and
use. Cognitive Science, 26(3):239?253.
Kaplan, Ronald M. 1995. The formal
architecture of Lexical-Functional
Grammar. In Mary Dalrymple, Ronald M.
Kaplan, John T. Maxwell III, and Annie
Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, Stanford, CA,
pages 7?27.
Kaplan, Ronald M. and Joan Bresnan.
1982. Lexical-Functional Grammar:
A formal system for grammatical
representation. In Joan Bresnan, editor,
The Mental Representation of Grammatical
Relations. MIT Press, Cambridge, MA,
pages 173?281.
Kaplan, Ronald M. and Martin Kay. 1994.
Regular models of phonological rule
systems. Computational Linguistics,
20(3):331?378.
Kaplan, Ronald M. and John T. Maxwell III.
1988a. An algorithm for functional
uncertainty. In Proceedings of the 12th
International Conference on Computational
Linguistics, pages 297?302, Budapest.
Kaplan, Ronald M. and John T. Maxwell III.
1988b. Constituent coordination in
Lexical-Functional Grammar. In
Proceedings of the 12th International
Conference on Computational Linguistics,
pages 303?305, Budapest.
913
Computational Linguistics Volume 38, Number 4
Kaplan, Ronald M. and John T. Maxwell III.
1996. LFG grammar writer?s workbench.
Technical report, Xerox Palo Alto Research
Center, Palo Alto, CA. Available at
ftp://ftp.parc.xerox.com/pub/lfg/
lfgmanual.pdf.
Kaplan, Ronald M., Stefan Riezler, Tracy
Holloway King, John T. Maxwell III,
Alexander Vasserman, and Richard
Crouch. 2004. Speed and accuracy in
shallow and deep stochastic parsing.
In Proceedings of the Human Language
Technology Conference and the 4th Annual
Meeting of the North American Chapter of the
Association for Computational Linguistics,
pages 97?104, Boston, MA.
Kaplan, Ronald M. and Ju?rgen Wedekind.
1993. Restriction and correspondence-
based translation. In Proceedings of the
6th Conference of the European Chapter
of the Association for Computational
Linguistics, pages 193?202, Utrecht.
Kaplan, Ronald M. and Ju?rgen Wedekind.
2000. LFG generation produces
context-free languages. In Proceedings
of the 18th International Conference on
Computational Linguistics, pages 425?431,
Saarbru?cken.
Kaplan, Ronald M. and Annie Zaenen. 1989.
Long-distance dependencies, constituent
structure, and functional uncertainty. In
Mark Baltin and Anthony Kroch, editors,
Alternative Conceptions of Phrase Structure.
Chicago University Press, Chicago, IL,
pages 17?42.
Karttunen, Lauri. 1984. Features and values.
In Proceedings of the 10th International
Conference on Computational Linguistics and
22nd Annual Meeting of the Association for
Computational Linguistics, pages 28?33,
Stanford, CA.
Karttunen, Lauri, Ronald M. Kaplan,
and Annie Zaenen. 1992. Two-level
morphology with composition. In
Proceedings of the 15th International
Conference on Computational Linguistics,
pages 141?148, Nantes.
Kay, Martin. 1996. Chart generation.
In Proceedings of the 34th Annual Meeting of
the Association for Computational Linguistics,
pages 200?204, Santa Cruz, CA.
Knight, Kevin and Irene Langkilde. 2000.
Preserving ambiguities in generation via
automata intersection. In Henry A. Kautz
and Bruce W. Porter, editors, Proceedings of
the 17th National Conference on Artificial
Intelligence and 12th Conference on Innovative
Applications of Artificial Intelligence,
pages 697?702, Austin, TX.
Kuhn, Jonas. 2001. Formal and Computational
Aspects of Optimality-Theoretic Syntax.
Ph.D. thesis, University of Stuttgart.
Kuhn, Jonas. 2002. OT syntax: Decidability
of generation-based optimization.
In Proceedings of the 40th Annual
Meeting of the Association for
Computational Linguistics, pages 48?55,
Philadelphia, PA.
Kuhn, Jonas. 2003. Optimality-Theoretic
Syntax: a Declarative Approach. CSLI
Publications, Stanford, CA.
Lang, Bernard. 1994. Recognition can be
harder than parsing. Computational
Intelligence, 10(4):486?494.
Langkilde, Irene. 2000. Forest-based
statistical sentence generation. In
Proceedings of the 6th Applied Natural
Language Processing Conference,
pages 170?177, Seattle, WA.
Langkilde, Irene and Kevin Knight. 1998.
Generation that exploits corpus-based
statistical knowledge. In Proceedings
of the 36th Annual Meeting of the
Association for Computational Linguistics
and the 17th International Conference on
Computational Linguistics, pages 704?710,
Montreal.
Maxwell III, John T. 2006. Efficient
generation from packed input.
In Miriam Butt, Mary Dalrymple,
and Tracy Holloway King, editors,
Intelligent Linguistics Architectures:
Variations on Themes by Ronald M.
Kaplan. CSLI Publications, Stanford,
CA, pages 19?34.
Maxwell III, John T. and Ronald M. Kaplan.
1991. A method for disjunctive constraint
satisfaction. In Masaru Tomita, editor,
Current Issues in Parsing Technology.
Kluwer, Dordrecht, pages 173?190.
Moore, Robert C. 2002. A complete,
efficient sentence-realization algorithm
for unification grammar. In Proceedings of
the 2nd International Natural Language
Generation Conference, pages 41?48,
New York, NY.
Neumann, Gu?nter. 1994. A Uniform
Computational Model for Natural Language
Parsing and Generation. Ph.D. thesis,
Saarland University.
Neumann, Gu?nter. 1998. Interleaving natural
language parsing and generation through
uniform processing. Artificial Intelligence,
99:121?163.
Pollard, Carl and Ivan Sag. 1994.
Head-Driven Phrase Structure Grammar.
The University of Chicago Press,
Chicago, IL.
914
Wedekind and Kaplan LFG Generation by Grammar Specialization
Rohrer, Christian and Martin Forst. 2006.
Improving coverage and parsing quality
of a large-scale LFG for German. In
Proceedings of the 5th Conference on
Language Resources and Evaluation
(LREC-2006), pages 2206?2211, Genoa.
Rounds, William C. 1988. Set values for
unification-based grammar formalisms
and logic programming. Research report
CSLI-88-129, CSLI, Stanford University,
Stanford, CA.
Shemtov, Hadar. 1997. Ambiguity
Management in Natural Language Generation.
Ph.D. thesis, Stanford University.
Shieber, Stuart M. 1988. A uniform
architecture for parsing and generation.
In Proceedings of the 12th International
Conference on Computational Linguistics,
pages 614?619, Budapest.
Shieber, Stuart M., Hans Uszkoreit,
Fernando C. N. Pereira, Jane Robinson,
and Mabry Tyson. 1983. The formalism
and implementation of PATR-II.
In Barbara J. Grosz and Mark E. Stickel,
editors, Research on Interactive Acquisition
and Use of Knowledge. SRI Final Report
1894, SRI International, Menlo Park, CA,
pages 39?79.
Shieber, Stuart M., Gertjan van Noord,
Fernando C. N. Pereira, and Robert C.
Moore. 1990. Semantic-head-driven
generation. Computational Linguistics,
16(1):30?42.
Statman, Richard. 1977. Herbrand?s theorem
and Gentzen?s notion of a direct proof.
In Jon Barwise, editor, Handbook of
Mathematical Logic. North-Holland,
Amsterdam, pages 897?912.
van Noord, Gertjan. 1993. Reversibility in
Natural Language Processing. Ph.D. thesis,
Utrecht University.
Velldal, Erik and Stephan Oepen. 2006.
Statistical ranking in tactical generation.
In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language
Processing, pages 517?525, Sydney.
Wedekind, Ju?rgen. 1994. Some remarks
on the logic of unification grammars.
In Christopher J. Rupp, Michael Rosner,
and Roderick Johnson, editors, Constraints,
Language and Computation. Academic Press,
London, pages 29?76.
Wedekind, Ju?rgen. 1995. Some remarks
on the decidability of the generation
problem in LFG- and PATR-style
unification grammars. In Proceedings
of the 7th Conference of the European
Chapter of the Association for Computational
Linguistics, pages 45?52, Dublin.
Wedekind, Ju?rgen. 1999. Semantic-driven
generation with LFG- and PATR-style
grammars. Computational Linguistics,
25(2):277?281.
Wedekind, Ju?rgen. 2006. On some formal
properties of LFG generation. In
Miriam Butt, Mary Dalrymple, and
Tracy Holloway King, editors, Intelligent
Linguistic Architectures: Variations on
Themes by Ronald M. Kaplan. CSLI
Publications, Stanford, CA, pages 53?72.
Wedekind, Ju?rgen and Ronald M. Kaplan.
1996. Ambiguity-preserving generation
with LFG- and PATR-style grammars.
Computational Linguistics, 22(4):555?558.
Wedekind, Ju?rgen and Ronald M. Kaplan.
Forthcoming. LFG generation with
rich descriptive devices. Working paper,
University of Copenhagen and Nuance
Communications, Inc.
White, Michael. 2006. CCG chart realization
from disjunctive inputs. In Proceedings
of the 4th International Natural Language
Generation Conference, pages 12?19, Sydney.
Zaenen, Annie and Ronald M. Kaplan.
1995. Formal devices for linguistic
generalizations: West Germanic word
order in LFG. In Mary Dalrymple,
Ronald M. Kaplan, John T. Maxwell III,
and Annie Zaenen, editors, Formal Issues in
Lexical-Functional Grammar, CSLI
Publications, Stanford, CA, pages 215?239.
Zarrie?, Sina, Aoife Cahill, and Jonas Kuhn.
2011. Underspecifying and predicting
voice for surface realisation ranking.
In Proceedings of the 49th Annual Meeting
of the Association for Computational
Linguistics: Human Language Technologies,
pages 1007?1017, Portland, OR.
915

