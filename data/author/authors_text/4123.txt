Two-Phase Biomedical Named Entity
Recognition Using A Hybrid Method
Seonho Kim1, Juntae Yoon2, Kyung-Mi Park1, and Hae-Chang Rim1
1 Dept. of Computer Science and Engineering,
Korea University, Seoul, Korea
2 NLP Lab. Daumsoft Inc. Seoul, Korea
Abstract. Biomedical named entity recognition (NER) is a difficult
problem in biomedical information processing due to the widespread am-
biguity of terms out of context and extensive lexical variations. This pa-
per presents a two-phase biomedical NER consisting of term boundary
detection and semantic labeling. By dividing the problem, we can adopt
an effective model for each process. In our study, we use two exponential
models, conditional random fields and maximum entropy, at each phase.
Moreover, results by this machine learning based model are refined by
rule-based postprocessing implemented using a finite state method. Ex-
periments show it achieves the performance of F-score 71.19% on the
JNLPBA 2004 shared task of identifying 5 classes of biomedical NEs.
1 Introduction
Due to dynamic progress in biomedical literature, a vast amount of new infor-
mation and research results have been published and many of them are available
in the electronic form - for example, like the PubMed MedLine database. Thus,
automatic knowledge discovery and efficient information access are strongly de-
manded to curate domain databases, to find out relevant information, and to
integrate/update new information across an increasingly large body of scien-
tific articles. In particular, since most biomedical texts introduce specific no-
tations, acronyms, and innovative names to represent new concepts, relations,
processes, functions, locations, and events, automatic extraction of biomedical
terminologies and mining of their diverse usage are major challenges in biomed-
ical information processing system. In these processes, biomedical named entity
recognition (NER) is the core step to access the higher level of information.
In fact, there has been a wide range of research on NER like the NER task on
the standard newswire domain in the Message Understanding Conference (MUC-
6). In this task, the best system reported 95% accuracy in identifying seven types
of named entities (person, organization, location, time, date, money, and per-
cent). While the performance in the standard domain turned out to be quite good
as shown in the papers, that in the biomedical domain is not still satisfactory,
which is mainly due to the following characteristics of biomedical terminologies:
First, NEs have various naming conventions. For instance, some entities have
descriptive and expanded forms such as ?activated B cell lines, 47 kDa sterol
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 646?657, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Two-Phase Biomedical Named Entity Recognition 647
regulatory element binding factor?, whereas some entities appear in shortened
or abbreviated forms like ?EGFR? and ?EGF receptor? representing epidermal
growth factor receptor. Second, biomedical NEs have the widespread ambiguity
out of context. For instance, ?IL-2? can be doubly classified as ?protein? and
?DNA? according to its context. Third, biomedical NEs often comprise a nested
structure, for example ??DNA??protein?TNF alpha?/protein?gene?/DNA??. Ac-
cording to [13], 16.57% of biomedical terms in GENIA have cascaded construc-
tions. In the case, recognition of the longest terms is the main target in general.
However, in our evaluation task, when the embedded part of a term is regarded
as the meaningful or important class in the context, the term is labeled only with
the class of embedded one. Thus, identification of internal structures of NEs is
helpful to recognize correct NEs. In addition, more than one NE often share the
same head noun with a conjunction/disjunction or enumeration structure, for
instance, ?IFN-gamma and GM-CSF mRNA?, ?CD33+, CD56+, CD16- acute
leukemia?or ?antigen- or cAMP-activated Th2 cell?. Last, there is a lot of inter-
annotator disagreement. [7] reported that the inter-annotator agreement rate of
human experts was just 77.6% when performing gene/protein/mRNA classifica-
tion task manually.
Thus, a lot of term occurrences in real text would not be identified with sim-
ple dictionary look-up, despite the availability of many terminological databases,
as claimed in [12]. That is one of the reasons why machine learning approaches
are more dominant in biomedical NER than rule-based or dictionary-based ap-
proaches [5], even though existence of reliable training resources is very critical.
Accordingly, much work has been done on biomedical NER, based on ma-
chine learning techniques. [3] and [13] have used hidden Markov Model (HMM)
for biomedical NER where state transitions are made by semantic trigger fea-
tures. [4] and [11] have applied maximum entropy plus Markovian sequence based
models such as maximum entropy markov model (MEMM) and conditional ran-
dom fields (CRFs), which present a way for integrating different features such
as internal word spellings and morphological clues within an NE string and con-
textual clues surrounding the string in the sentence.
These works took an one-phase based approach where boundary detection
of named entities and semantic labeling come together. On the other hand, [9]
proposed a two-phase model in which the biomedical named entity recognition
process is divided into two processes of distinguishing biomedical named entities
from general terms and labeling the named entities with semantic classes that
they belong to. They use support vector machines (SVM) for each phase. How-
ever, the SVM does not provide an easy way for labeling Markov sequence data
like B following O and I following B in named entities. Furthermore, since this
system is tested on the GENIA corpus rather than JNLPBA 2004 shared task, we
cannot confirm the effectiveness of this approach on the ground of experiments
for common resources.
In this paper, we present a two-phase named entity recognition model: (1)
boundary detection for NEs and (2) term classification by semantic labeling.
The advantage of dividing the recognition process into two phase is that we can
648 S. Kim et al
select separately a discriminative feature set for each subtask, and moreover can
measure effectiveness of models at each phase. We use two exponential models
for this work, namely conditional random fields for boundary detection having
Markov sequence, and the maximum entropy model for semantic labeling. In ad-
dition, results from the machine learning based model are refined by a rule-based
postprocessing, which is implemented using a finite state transducer (FST). The
FST is constructed with the GENIA 3.02 corpus. We here focus on identification
of five classes of NEs, i.e. ?protein?, ?RNA?, ?DNA?, ?cell line?, and ?cell type?
and experiments are conducted on the training and evaluation set provided by
the shared task in COLING 2004 JNLPBA.
2 Training
2.1 Maximum Entropy and Conditional Random Fields
Before we describe the features used in our model, we briefly introduce the ME
and CRF model which we make use of. In the ME framework, the conditional
probability of predicting an outcome o given a history h is defined as follows:
p?(o|h) =
1
Z?(h)
exp
(
k
?
i=1
?ifi(h, o)
)
(1)
where fi(h, o) is a binary-valued feature function, ?i is the weighting parameter
of fi(h, o), k is the number of features, and Z?(h) is a normalization factor
for ?op?(o|h)=1. That is, the probability p?(o|h) is calculated by the weighted
sum of active features. Given an exponential model with k features and a set
of training data, empirical distribution, weights of the k features are trained to
maximize the model?s log-likelihood:
L(p) =
?
o,h
p?(h, o)log(o|h) (2)
Although the maximum entropy model above provides a powerful tool for
classification by integrating different features, it is not easy to model the Markov
sequence data. In this case, the CRF is used for a task of assigning label sequences
to a set of observation sequences. Based on the principle of maximum entropy,
a CRF has a single exponential model for the joint probability of the entire
sequence of labels given the observation sequence. The CRF is a special case of
the linear chain that corresponds to conditionally trained finite-state machine
and define conditional probability distributions of a particular label sequence s
given observation sequence o
p?(s|o) = 1Z(o)exp(
?k
j=1 ?jFj(s,o))
Fj(s,o) =
?n
i=1 fj(si?1, si,o, i)
(3)
Two-Phase Biomedical Named Entity Recognition 649
where s = s1 . . . sn, and o = o1 . . . on, Z(o) is a normalization factor, and each
feature is a transition function [8]. For example, we can think of the following
feature function.
fj(si?1, si,o, i) =
?
?
?
1 if si?1=B and si=I,
and the observation word at position i is ?gene??
0 otherwise
(4)
Our CRFs for term boundary detection have a first-order Markov dependency
between output tags. The label at position i, si is one of B, I and O. In contrast
to the ME model, since B is the beginning of a term, the transition from O to I
is not possible. CRFs constrain results to consider only reasonable paths. Thus,
total 8 combinations are possible for (si?1,si) and the most likely s can be found
with the Viterbi algorithm. The weights are set to maximize the conditional log
likelihood of labeled sequences in the training set using a quasi-Newton method
called L-BFGS [2].
2.2 Features for Term Boundary Detection
Table 1 shows features for the step of finding the boundary of biomedical terms.
Here, we give a supplementary description of a part of the features.
Table 1. Feature set for boundary detection (+:conjunction)
Model Feature Description
CRF, MEmarkov Word wi?1, wi?2, wi, wi+1, wi+2
CRF, MEmarkov Word Normalization normalization forms of the 5 words
CRF, MEmarkov POS POSwi?1 , POSwi , POSwi+1
CRF, MEmarkov Word Construction form WFwi
CRF, MEmarkov Word Characteristics WCwi?1 , WCwi , WCwi+1
CRF, MEmarkov Contextual Bigrams wi?1 + wi
wi + wi+1
wi+1 + wi+2
CRF, MEmarkov Contextual Trigrams wi?1 + wi + wi+1
CRF, MEmarkov Bigram POS POSwi?1 + POSwi
POSwi + POSwi+1
CRF, MEmarkov Trigram POS POSwi?1 + POSwi + POSwi+1
CRF, MEmarkov Modifier MODI(wi)
CRF, MEmarkov Header HEAD(wi)
CRF, MEmarkov SUFFIX SUFFIX(wi)
CRF, MEmarkov Chunk Type CTypewi
CRF, MEmarkov Chunk Type + Pre POS CTypewi + POSwi?1
MEmarkov Pre label labelwi?1
MEmarkov Pre label + Cur Word labelwi?1 + wi
? word and POS: 5 words(target word(wi), left two words, and right two
words) and three POS(POSwi?1 , POSwi , POSwi+1) are considered.
650 S. Kim et al
? word normalization: This feature contributes to word normalization. We
attempt to reduce a word to its stem or root form with a simple algorithm
which has rules for words containing plural, hyphen, and alphanumeric let-
ters. Specifically, the following patterns are considered.
(1) ?lymphocytes?, ?cells? ? ?lymphocyte?, ?cell?
(2) ?il-2?, ?il-2a?, ?il2a? ? ?il?
(3) ?5-lipoxygenase?, ?v-Abl? ? ?lipoxygenase?, ?abl?
(4) ?peri-kappa?or ?t-cell? has two normalization forms of ?peri?and?kappa?
and ?t? and ?cell? respectively.
(5) ?Ca2+-independent? has two roots of ?ca? and ?independent?.
(6) The root of digits is ?D?.
? informative suffix: This feature appears if a target word has a salient suffix
for boundary detection. The list of salient suffixes is obtained by relative
entropy [10].
? word construction form: This feature indicates how a target word is or-
thographically constructed. Word shapes refer to a mapping of each word
on equivalence classes that encodes with dashes, numerals, capitalizations,
lower letters, symbols, and so on. All spellings are represented with combina-
tions of the attributes1. For instance, the word construction form of ?IL-2?
would become ?IDASH-ALPNUM?.
? word characteristics: This feature appears if a word represents a DNA
sequence of ?A?,?C?,?G?,?T? or Greek letter such as beta or alpha, ordinal
index such as I, II or unit such as BU/ml, micron/mL. It is encoded with
?ACGT?, ?GREEK?, ?INDEX?, ?UNIT?.
? head/modifying information: If a word prefers the rightmost position
of terminologies, we regard it has the property of a head noun. On the
other hand, if a word frequently occurs in other positions, we regard it has
the property of a modifying noun. It can help to establish the beginning
and ending point of multi-word entities. We automatically extract 4,382
head nouns and 7,072 modifying nouns from the training data as shown in
Table 2.
? chunk-type information: This feature is also effective in determining the
position of a word in NEs, ?B?, ?I?, ?O? which means ?begin chunk?, ?in
chunk? and ?others?, respectively. We consider the chunk type of a target
word and the conjunction of the current chunk type and the POS of the
previous word to represent the structure of an NE.
We also tested an ME-based model for boundary detection. For this, we add
two special features : previous state (label) and conjunction of previous label
1 ?IDASH? (inter dash), ?EDASH? (end dash), ?SDASH? (start dash),
?CAP?(capitalization), ?LOW?(lowercase), ?MIX?(lowercase and capitaliza-
tion letters), ?NUM?(digit), ?ALPNUM?(alpha-numeric), ?SYM?(symbol),
?PUNC?(punctuation),and ?COMMA?(comma)
Two-Phase Biomedical Named Entity Recognition 651
Table 2. Examples of Head/Modifying Nouns
Modifying Nouns Head Nouns
nf-kappa cytokines
nuclear elements
activated assays
normal complexes
phorbol macrophages
viral molecules
inflammatory pathways
murine extracts
electrophoretic glucocorticoids
acute levels
intracellular responses
epstein-barr clones
cytoplasmic motifs
and current word to consider state transition. That is, a previous label can be
represented as a feature function in our model as follows:
fi(h, o) =
{
1 if pre label+tw=B+gene,o=I
0 otherwise
(5)
It means that the target word is likely to be inside a term (I), when the word
is ?gene? and the previous label is ?B?. In our model, the current label is de-
terministically assigned to the target word with considering the previous state
with the highest probability.
2.3 Features for Semantic Labeling
Table 3 shows features for semantic labeling with respect to recognized NEs.
? word contextual feature: We make use of three kinds of internal and ex-
ternal contextual features: words within identified NEs, their word normal-
ization forms, and words surrounding the NEs. In Table 3, NEw0 denotes
the rightmost word in an identified NE region. Moreover, the presence of
specific head nouns acting as functional words takes precedence when de-
termining the term class, even though many terms do not contain explicit
term category information. For example, functional words, such as ?factor?,
?receptor?, and ?protein? are very useful in determining protein class, and
?gene?, ?promoter?, and ?motif ? are clues for classifying DNA [5]. In gen-
eral, such functional words are often the last word of an entity. This is the
reason we consider the position where a word occurs in NEs along with the
word. For inside context features, we use non-positional word features as
well. As non-positional features, all words inside NEs are used.
? internal bigrams and trigrams: We consider the rightmost bigrams/
trigrams inside identified NEs and the normalized bigrams/trigrams.
652 S. Kim et al
Table 3. Feature Set for Semantic Classification
Feature description
Word Features (positional) NEwothers , NEw?3 , NEw?2 , NEw?1 , NEw0
Word Features (non-positional) AllNEw
Word Normalization (positional) WFNEw?3 , WFNEw?2 , WFNEw?1 , WFNEw0
Left Context(Words Surrounding an NE) LCW?2, LCW?1
Right Context RCW+1, RCW+2
Internal Bigrams NEw?1 + NEw0
Internal Trigrams NEw?2 + NEw?1 + NEw0
Normalized Internal Bigrams WFNEw?1 + WFNEw0
Normalized Internal Trigrams NEw?2 + NEw?1 + NEw0
IDASH-word related Bigrams/Trigrams
Keyword KEYWORD(NEi)
? IDASH-word related bigrams/trigrams: This feature appears if NEw0
or NEw?1 contains dash characters. In this case, the bigram/trigram are
additionally formed by removing all dashes from the spelling. It is useful to
deal with lexical variants.
? keywords: This feature appears if the identified NE is informative key-
word with respect to a specific class. The keywords set comprises terms
obtained by the relative entropy between general and biomedical domain
corpora.
3 Rule-Based Postprocessing
A rule-based method can be used to correct errors by NER based on machine
learning. For example, the CRFs tag ?IL-2 receptor expression? as ?B I I?,
since the NEs ended with ?receptor expression? in training data almost belong
to ?other name? class even if the NEs ended with ?receptor? belong to ?pro-
tein? class. It should be actually tagged as ?B I O?. That kind of errors is
caused mainly by the cascaded phenomenon in biomedical names. Since our sys-
tem considers all NEs belonging to other classes in the recognition phase, it
tends to recognize the longest ones. That is, in the term classification phase,
such NEs are classified as ?other? class and are ignored. Thus, the system
losts embedded NEs although the training and evaluation set in fact tends to
consider only the embedded NE when the embedded one is more meaningful
or important.
This error correction is conducted by the rule-based method, i.e. If condi-
tion THEN action. For example, the rule ?IF wi?2=IL-2, wi?1=receptor and
wi=expression THEN replace the tag of wi with O? can be applied for the above
case. We use a finite state transducer for this rule-based transformation, which
is easy to understand with given lexical rules, and very efficient. Rules used for
the FST are acquired from the GENIA corpus. We first retrieved all NEs in-
cluding embedded NEs and longest NEs from GENIA 3.02 corpus and change
Two-Phase Biomedical Named Entity Recognition 653
IL-2/B gene/I
IL-2/O gene/O expression/O
Fig. 1. Non-Deterministic FST
IL-2/? gene/ ?
expression/OOO
? /BI
Fig. 2. Deterministic FST
the outputs of all other classes except the target 5 classes to O. That is, the
input of FST is a sequence of words in a sentence and the output is categories
corresponding to the words.
Then, we removed the rules in conflict with NE information from the training
corpus. These rules are non-deterministic (Figure 1), and we can change it to
the deterministic FST (Figure 2) since the lengths of NEs are finite. The deter-
ministic FST is made by defining the final output function for the deterministic
behavior of the transducer, delaying the output. The deterministic FST is de-
fined as follows: (?1, ?2, Q, i, F, ?, ?, ?), where ?1 is a finite input alphabet; ?2
is a finite output alphabet; Q is a finite set of states or vertices; i ? Q is the
initial state; F ? Q is the set of final states; ? is the deterministic state transi-
tion function that maps Q ? ?1 on Q; ? is the deterministic emission function
that maps Q ? ?1 on ??2 and ? : F ? ??2 is the final output function for the
deterministic behavior of the transducer.
4 Evaluation
4.1 Experimental Environments
In the shared task, only biomedical named entities which belong to 5 specific
classes are annotated in the given training data. That is, terms belonging to
other classes in GENIA are excluded from the recognition target. However, we
consider all NEs in the boundary detection step since we separate the NER
task into two phases. Thus, in order to utilize other class terms, we additionally
annotated ?O? class words in the training data where they corresponds to other
classes such as other organic compound, lipid, and multi cell in GENIA 3.02p
version corpus. During the annotation, we only consider the longest NEs on
654 S. Kim et al
Table 4. Number of training examples
RNA DNA cell line cell type protein other
472 5,370 2,236 2,084 16,042 11,475
GENIA. As a consequence, we find all biomedical named entities in text at the
term detection phase. Then, biomedical NEs classified as other class are changed
to O at the semantic labeling phase. The total words that belong to other class
turned out to be 25,987. Table 4 shows the number of NEs with respect to each
class on the training data. In our experiments, a quasi-Newton method called the
L-BFGS with Gaussian Prior smoothing is applied for parameter estimation [2].
4.2 Experimental Results
Table 5 shows the overall performance on the evaluation data. Our system
achieves an F-score of 71.19%. As shown in the table, the performance of NER
for cell line class was not good, because its boundary recognition is not so good
as other classes. Also, Table 6 shows the results of semantic classification. In par-
ticular, the system often confuses protein with DNA, and cell line with cell type.
Among the correctly identified 7,093 terms, 790 terms were misclassified.
Table 7 shows the performance of each phase. Our system obtains 76.88%
F-score in the boundary detection task and, using 100% correctly recognized
terms from annotated test data, 90.54% F-score in the semantic classification
task. Currently, since we cannot directly assess the accuracy of the term detection
process on the evaluation set because of other class words, the 75% of the training
data were used for training and the rest for testing.
Table 5. Overall performance on the evaluation data
Fully Correct Left Correct Right Correct
Class Recall Precision F-score F-score F-score
protein 76.30 69.71 72.85 77.60 79.15
DNA 67.80 64.91 66.33 68.36 74.57
RNA 73.73 63.04 67.97 71.09 74.22
cell line 57.40 54.88 56.11 59.04 65.69
cell type 70.12 77.64 73.69 74.89 81.51
overall 72.77 69.68 71.19 74.75 78.23
Table 6. Confusion matrix over evaluation data
gold/sys protein DNA RNA cell line cell type other
protein 0 72 3 1 4 267
DNA 97 0 0 0 0 49
RNA 11 0 0 0 0 0
cell line 10 1 0 0 63 37
cell type 21 0 0 92 0 57
Two-Phase Biomedical Named Entity Recognition 655
Table 7. Performance of term detection and semantic classification
Recall Precision F-score
term detection (MEMarkov) 74.03 75.31 74.67
term detection (CRF) 76.14 77.64 76.88
semantic classification 87.50 93.81 90.54
overall NER 72.77 69.68 71.19
Table 8. Performance of NE recognition methods (one-phase vs. two-phase)
method Recall Precision F-score
one-phase 64.23 63.13 63.68
two-phase(baseline2) 66.24 64.54 65.38
(only 5 classes)
two-phase(baseline2) 68.51 67.58 68.04
(5 classes+other class)
Also, we compared our model with the one-phase model. The detailed results
are presented in Table 8. Both of them have pros and cons. The best-reported
system presented by [13] uses one-phase strategy. In our evaluation, the two-
phase method shows a better result than the one-phase method, although direct
comparison is not possible since we tested with a maximum entropy based expo-
nential models in all cases. The features for one-phase method are identical with
the recognition features except that the local context of a word is extended as
previous 4 words and next 4 words. In addition, we investigate whether the con-
sideration of ?other? class words is helpful in the recognition performance. Table
8 shows explicit annotations of other NE classes much improve the performance
of existing entity types.
In the next experiment, we test how individual methods have an effect on the
performance in the term detection step. Table 9 shows the results obtained by com-
bining different methods in the NER process. At the semantic labeling phase, all
methods employed the ME model using the features described in 2.3. Baseline1
is the two-phase ME model which restrict the inspection of NE candidates to the
NPs which include at least one biomedical salient word. Baseline2 is the two-phase
ME model considering all words. In order to retrieve domain salient words, we
utilized a relative frequency ratio of word distribution in the domain corpus and
that in the general corpus [10]. We used the Penn II raw corpus as out-of-domain
corpus. Both models do not use the features related to previous labels. As a re-
sult, usage of salient words decrease the performance and it only speeds up the
training process. Baseline2+FST indicates boundary extension/contraction using
FST are applied as postprocessing step in baseline2 recognition. In addition, we
compared use of CRFs and ME with Markov process features. For this, we added
features of previous labels to the feature set for ME. Baseline2+MEMarkov is the
two-phase ME model considering all features including previous label related fea-
tures. Baseline2+CRF is a model exploiting CRFs and baseline2+CRF+FST is a
model using CRFand FST as postprocessing.As shown in Table 9, the CRFs based
656 S. Kim et al
Table 9. F-score for different methods
Method Recall Precision F-score
baseline1(salientNP ) 66.21 66.34 66.27
baseline2(all) 68.51 67.58 68.04
baseline2 + FST 68.89 68.53 68.71
baseline2 + MEMarkov 70.30 67.65 68.95
baseline2 + MEMarkov + FST 70.61 68.40 69.49
baseline2 + CRF 72.44 68.77 70.56
baseline2 + CRF + FST 72.77 69.68 71.19
Table 10. Comparisons with other systems
System Precision Recall F-score
Zhou et. al (2004) 69.42 75.99 72.55
Our system 72.77 69.68 71.19
Finkel et. al (2004) 71.62 68.56 70.06
Settles (2004) 70.0 69.0 69.5
model outperforms the ME based model. Our system reached F-score 71.19% on
the baseline2 + CRF + FST model.
Table 10 shows the comparison with top-ranked systems in JNLPBA 2004
shared task. The top-ranked systems made use of external knowledge from
gazetteers and abbreviation handling routines, which were reported to be ef-
fective. Zhou et. al reported the usage of gazetteers and abbreviation handling
improves the performance of the NER system by 4.8% in F-score [13]. Finkel
et. al made use of a number of external resources, including gazetteers, web-
querying, use of the surrounding abstract, abbreviation handling, and frequency
counts from BNC corpus [4]. Settles utilized semantic domain knowledge of 17
kinds of lexicons [11]. Although the performance of our system is a bit lower than
the best system, the results are very promising since most systems use external
gazetteers, and abbreviation and conjunction/disjunction handling scheme. This
suggests areas for further work.
5 Conclusion and Discussion
We presented a two-phase biomedical NE recognition model, term boundary
detection and semantic labeling. We proposed two exponential models for each
phase. That is, CRFs are used for term detection phase including Markov process
and ME is used for semantic labeling. The benefit of dividing the whole process
into two processes is that, by separating the processes with different characteris-
tics, we can select separately the discriminative feature set for each subtask, and
moreover measure effectiveness of models at each phase. Furthermore, we use
the rule-based method as postprocessing to refine the result. The rules are ex-
tracted from the GENIA corpus, which is represented by the deterministic FST.
The rule-based approach is effective to correct errors by cascading structures
Two-Phase Biomedical Named Entity Recognition 657
of biomedical NEs. The experimental results are quite promising. The system
achieved 71.19% F-score without Gazetteers or abbreviation handling process.
The performance could be improved by utilizing lexical database and testing
various classification models.
Acknowledgements
This work was supported by Korea Research Foundation Grant, KRF-2004-037-
D00017.
References
1. Thorten Brants. TnT A Statistical Part-of-Speech Tagger. In Proceedings of the
6th Applied Natural Language Processing.; 2000.
2. Stanley F. Chen and Ronald Rosenfeld. A Gaussian prior for smoothing maximum
entropy models. Technical Report CMUCS-99-108, Carnegie Mellon University.
3. Nigel Collier, Chikashi Nobata and Jun-ichi Tsujii. Extracting the Names of Genes
and Gene Products with a Hidden Markov Model. In Proceedings of COLING 2000;
201-207.
4. Jenny Finkel, Shipra Dingare, and Huy Nguyen. Exploiting Context for Biomedical
Entity Recognition From Syntax to thw Web. In Proceedings of JNLPBA/BioNLP
2004; 88-91.
5. K. Fukuda, T. Tsunoda, A. Tamura, and T. Takagi. Toward information extrac-
tion: identifying protein names from biological papers. In Proceedins of the Pacific
Symposium on Biocomputing 98; 707-718.
6. Junichi Kazama, Takaki Makino, Yoshihiro Ohta and Junichi Tsujii. Tuning Sup-
port Vector Machines for Biomedical Named Entity Recognition, Proceedings of the
ACL Workshop on Natural Language Processing in the Biomedical Domain 2002;
1-8.
7. Michael Krauthammer and Goran Nenadic. Term Identification in the Biomedical
literature. Journal of Biomedical Informatics. 2004; 37(6):512-526.
8. John Lafferty, Andrew McCallum, and Fernando Pereira. Conditional Random
Fields: probabilistic models for segmenting and labeling sequence data. In Proceed-
ings of ICML-01; 282-289.
9. Ki-Joong Lee, Young-Sook Hwang, Seonho Kim, Hae-Chang Rim. Biomedical
named entity recognition using two-phase model based on SVMs. Journal of
Biomedical Informatics 2004; 37(6):436-447.
10. Kyung-Mi Park, Seonho Kim, Ki-Joong Lee, Do-Gil Lee, and Hae-Chang Rim.
Incorportating Lexical Knowledge into Biomedical NE Recognition. In Proceedings
of Natural Language Processing in Biomedicine and its Applications Post-COLING
Workshop 2004; 76-79.
11. Burr Settles. Biomedical Named Entity Recognition Using Conditional Random
Fields and Rich Feature Sets. In Proceedings of JNLPBA/BioNLP 2004; 104-107.
12. Olivia Tuason, Lifeng Chen, Hongfang Liu, Judith A. Blake, Carol Friedman. Bi-
ological Nomenclatures: A Source of Lexical Knowledge and Ambiguity. In Pacific
Symposium on Biocomputing 2004; 238-249.
13. GuoDong Zhou, Jie Zhang, Jian Su, Chew-Lim Tan. Exploring Deep Knowledge
Resources in Biomedical Name Recognition. In Proceedings of JNLPBA/BioNLP
2004; 99-102.
76
77
78
79
St ructura l  Feature  Se lect ion  For Eng l i sh -Korean  Stat is t ica l  
Mach ine  Trans la t ion  
Seonho Kim, Juntae Yoon, Mansuk Song 
{ pobi, j tyoon, mssong} @decemb er.yonsei, ac.kr 
\ ] )ept .  of  Computer  Science, 
Yonsci  Univers i ty ,  Seoul,  Korea  
Abstract 
When aligning texts in very different languages such 
as Korean and English, structural features beyond 
word or phrase give useful intbrmation. In this pa- 
per, we present a method for selecting struetm'al 
features of two languages, from which we construct 
a model that assigns the conditional probabilities 
to corresponding tag sequences in bilingual English- 
Korean corpora. For tag sequence mapl)ing 1)etween 
two langauges, we first, define a structural feature 
fllnction which represents tatistical prol)erties of 
elnpirical distribution of a set of training samples. 
The system, based on maximmn entrol)y coneet)t, se- 
le(:ts only ti;atures that pro(luee high increases in log- 
likelihood of training salnl)les. These structurally 
mat)ped features are more informative knowledge for 
statistical machine translation t)etween English and 
Korean. Also, the inforum.tion can help to reduce the 
1)arameter sl)ace of statisti('al alignment 1)y eliminat- 
ing synta(:tically uiflikely alignmenls. 
1 Introduction 
Aligned texts have been used for derivation of 1)ilin- 
gual dictioimries and terminoh)gy databases which 
are useflfl for nlachine translation and cross lan- 
guages infornmtion retriewfl. Thus, a lot of align- 
ment techniques have been suggested at; the sen- 
tence (Gale et al, 1993), phrase (Shin et al, 1996), 
nomt t)hrase (Kupiec, 1993), word (Brown et al, 
1993; Berger et al, 1996; Melamed, 1997), collo- 
cation (Smadja et al, 1996) and terminology level. 
Seine work has used lexical association measures 
for word alignments. However, the association mea- 
sures could be misled since a word in a source lan- 
guage frequently co-occurs with more titan one word 
in a target language. In other work, iterative re- 
estimation techniques have beets emt)loyed. They 
were usually incorporated with the EM algorithm 
mid dynmnic progranmfing. In that case, the prob- 
al)ilities of aligmnents usually served as 1)arameters 
in a model of statistical machine translation. 
In statistical machine translation, IBM 1~5 mod- 
els (Brown et al, 1993) based on the source-chmmel 
model have been widely used and revised for many 
language donmins and applications. It has also 
shortconfing that it needs much iteration time for 
parameter estimation and high decoding complex- 
ity, however. 
Much work has been done to overcome the prob- 
lem. Wu (1996) adopted chammls that eliminate 
syntactically unlikely alignments and Wang et al 
(1998) presented a model based on structures of two 
la.nguages. Tilhnann et al (1997) suggested the 
dynanfie programming lmsed search to select the 
best alignment and preprocessed bilingual texts to 
remove word order differences. Sate et al (1998) 
and Och et al (1998) proposed a model for learn- 
ing translation rules with morphological information 
mid word category in order to improve statistical 
translation. 
Furthemlore, llla,lly researches assullle(t Olle-to- 
one correspondence due to the coml)lexity and com- 
Imtati(m time of statistical aliglunents. Although 
this assumption Ire'ned out t;o 1)e useful for align- 
ment of close lallguages uch as English and French, 
it is not, applicabh~ to very different languages, in 
particular, Korean and English where there is rarely 
(:lose corresl)ondence in order at the word level. For 
such languages, even phrase level alignment, not to 
mei~tion word aligmnent, does not gives good trans- 
lation due to structural diflbrence. Itence, structural 
features beyond word or t)hrase should t)e consid- 
ered to get t)etter translation 1)etween English and 
Koreml. In addition, the construction of structural 
bilingual texts would be more informative for ex- 
tracting linguistic knowledge. 
In this paper, we suggest a method for structural 
mat)t)ing of bilingual language on the basis of the 
maximum entorl)y and feature induction fl'alnework. 
Our model based on POS tag sequence mapl)ing has 
two advantages: First;, it can reduce a lot of 1)armne- 
ters in statistical machilm translation by eliminating 
syntactically unlikely aligmnents. Second, it: can be 
used as a t)reprocessor for lexical alignments of bilin- 
gual corpora although it; (:an be also exl)loited 1)y it- 
self tbr alignment. In this case, it would serve as the 
first stet) of alignment for reducing the 1)arameter 
sI)ace. 
439 
2 Mot ivat ion  
In order to devise parameters for statistical model- 
ing of translation, we started our research from the 
IBM model which has bee:: widely used by :nany 
researches. The IBM model is represented with the 
formula shown in (1) 
l 17t 
v(f, al ) = I I  I-I t(fJ l%)d(jlaj,m, l) 
i=1 j= l  
(1) 
Here, n is the fertility probability that an English 
word generates n h'end:  words, t is tim aligmnent 
probability that the English word c generates the 
French word f ,  and d is the distortion probability 
that an English word in a certain t)osition will gener- 
ate a lh'ench word in a certain 1)osition. This formula 
is Olm of many ways in which p(f, ale ) can tie writtm. 
as the product of a series of conditional prot)at)ilities. 
In above model, the distortion probability is re-- 
lated with positional preference(word order). Since 
Korean is a free order language, the probability is 
not t~asible in English-Korean translation. 
Furthermore, the difference between two lan- 
guages leads to the discordance between words that 
the one-to-one correst)ondence b tween words gen- 
erally does not keel). The n:odel (1), however, as-- 
sumed that an English word cat: be connected with 
multiple French words, but that each French word 
is connected to exactly one English word inch:ding 
the empty word. hl conclusion, many-to-:nany :nap-- 
pings are not allowed in this model. 
According to our ext)eri:nent, inany-to-nmny 
mappings exceed 40% in English and Korean lexical 
aligninents. Only 25.1% of then: can be explained 
by word for word correspondences. It means that we 
need a statistical model which can lmndle phrasal 
mat) pings. 
In the case of the phrasal mappings, a lot of pa- 
rameters hould be searched eve:: if we restrict the 
length of word strings. Moreover, in order to prop-- 
erly estimate t)arameters we need much larger voI-- 
ume of bilingual aligned text than it in word-for- 
word modeling. Even though such a large corpora 
exist sometimes, they do not come up with the lex-- 
ical alignments. 
For this problem, we here consider syntactic fea- 
tures which are importmlt in determining structures. 
A structural feature means here a mapt)ing between 
tag sequences in bilingual parallel sentences. 
If we are concerned with tag sequence alignments, 
it is possible to estimate statistical t)armneters in 
a relatively small size of corpora. As a result, we 
can remarkably reduce the problem space for possi- 
ble lexical alignments, a sort of t probability in (1), 
which improve the complexity of a statistical ma- 
chine translation model. 
If there are similarities between corresponding tag 
sequences in two language, tile structural features 
would be easily computed or recognized. However, 
a tag sequence in English can be often translated 
into a completely different tag sequence in Korean 
as follows. 
can/MD -+ ~-, cul/ENTR1 su/NNDE1 'iss/ AJMA 
da/ENTE 
It nmans that similarities of tag features between two 
languages are not; kept all the time and it is neces- 
saw to get the most likely tag sequence mappings 
that reflect structural correspondences between two 
languages. 
In this paper, the tag sequence mappings are ob- 
taind by automatic feature selection based on the 
maximum entropy model. 
3 Prob lem Set t ing  
In tiffs ctlat)ter, we describe how the features are 
related to the training data. Let tc be an English 
tag sequence and tk be a Korean tag sequence. Let 
Ts be the set of all possible tag sequence niapI)ings in 
a aligned sentence, S. We define a feature function 
(or a feature) as follows: 
1 pair(t~,tk) C "\]-s 
f(t~,tk) = 0 othcrwi.s'c 
It indicates co-occurrence information l)etween 
tags appeared in Ts. f(t?,tk) expresses the infor- 
mation for predicting that te maps into ta.. A fea- 
ture means a sort of inforination for predicting some- 
thing. In our model, co-occurrence information on 
the same aligned sentence is used for a feature, while 
context is used as a feature in Inost of systems using 
maximum entropy. It can be less informative than 
context. Hence, we considered an initial supervision 
and feature selection. 
Our model starts with initial seed(active) features 
for mapI)ing extracted by SUl)ervision. In the next 
step, thature pool is constructed from training sam- 
ples fro:n filtering and oifly features with a large gain 
to the model are added into active feature set. The 
final outputs of our model are the set of active t'ea- 
tures, their gain values, and conditional probabilities 
of features which maximize the model. Tim results 
can be embedded in parameters of statistical ma- 
chine translation and hell) to construct structural 
bilingual text. 
Most alignment algorithm consists of two steps: 
(1) estimate translation probabilities. 
(2) use these probabilities to search for most t)roba- 
ble alignment path. 
Our study is focused on (1), especially the part of 
tag string alignments. 
Next, we will explain the concept of the model. 
We are concerned with an ot)timal statistical inodel 
which can generate the traiifing samples. Nmnely, 
our task is to construct a stochastic model that pro- 
440 
(1) duces outl)ut tag sequenc0, "~k, given a tag sequence ~+~,-~. 
To The l)roblem of interest is to use Salnt/les of ? --J~\,What .... 
tagged sentences to observe the/)charier of the ran- u~, ,~ 
(loin t)roeess. 'rile model p estinmtes tile conditional tt'2,Y 
probability that tile process will outlmt t,~, given t~.. ~ ,~/~ ,o~!! 
It is chosen out of a set of all allowed probability o~,~ ~ 
e}~..?0,, me (tistributions . . . .  
The fbllowing steps are emt)loyed for ()tit" model, v~ / 
Input: a set L of POS-labeled bilingual aligned 
sentences. 
I. Make a set ~: of corresl)ondence pairs of tag 
sequences, (t~, tk) from a small portion of L by 
supervision. 
2. Set 2F into a set of active features, A. 
3. Maximization of 1)arameters, A of at:tire fea- 
tures 1)y I IS(hnproved Iterative Sealing) algo- 
rithm. 
4. Create a feature pool set ?9 of all possible align- 
nmnts a(t(,, tk) from tag seqllellces of samples. 
5. Filter 7 ) using frequency and sintilarity with M. 
6. Coml)ute the atit)roximate gains of tkmtm:es in 
"p. 
7. Select new features(A/') with a large gain vahle, 
and add A. 
Outt)ut: p(tklt~,)whcrc(t(,, t~.) C M and their Ai. 
We I)egan with training samples comi)osed of 
English-Korean aligned sentence t)airs, (e,k). Since 
they included long sentences, w(', 1)roke them into 
shorter ones. The length of training senl;en(:es was 
limited to ml(h',r 14 on the basis of English. It is 
reasona,bh; \])(',(:&llSe we are interested in not lexical 
alignments lint tag sequence aliglmients. The sam- 
ples were tagged using brill's tagger and qVIorany' 
that we iml)lenmnted as a Korean tagger. Figure \] 
shows the POS tags we considered. For simplicity, 
we adjusted some part of Brill's tag set. 
In the, sut)ervision step, 700 aligned sentences were 
used to construct he tag sequences mal)I)ings wlfich 
are referred to as an active feature set A. As Fig- 
ure 2 shows, there are several ways in constructing 
the corresl)ondem;es. We chose the third mapping 
although (1) can be more useflll to explain Korean 
with I)redieate-argunmnt structure. Since a subject 
of a English sentence is always used for a subject 
tbrln in Korean, we exlcuded a subject case fi'onl ar- 
gulnents of a l/redicate. For examl)le, 'they' is only 
used for a subject form, whereas 'me' is used for a 
object form and a dative form. 
II1 tile next step, training events, (t,:, It.) are con- 
structed to make a feature 1)eel froln training sam- 
pies. The event consists of a tag string t,, of a English 
(2) (31 
? ~ lm-  
~" - -  + ~ '~Whatever  
Figure 2: Tag sequence corresl)ondences at the 
phrase level 
1)OS-tagged sentence and a tag string tL~ of the cor- 
responding Korean POS-tagged sentence and it Call 
be represented with indicator functions fi(t~, tk). 
For a given sequence, the features were drawn 
fl'om all adjacent i)ossible I)airs and sonic interrupted 
pairs. Only features (tci, tfii ) out of the feature pool 
that meet the following conditions are extracted. 
? #(l, ei,t~:i) _> 3, # is count 
? there exist H:.~,, where (t(,i,tt.~.) in A and the 
similarity(sanle tag; colin|;) of lki an(1 tkx _> 0.6 
Table \] shows possible tL'atures, for a given aligned 
sentence , 'take her out - g'mdCOrcul baggcuro 
dcrfleoflara'. 
Since the set of the structural ti;atm'es for align- 
ment modeling is vast, we constructed a maximum 
entrol)y model for p(tkltc) by the iterative model 
growing method. 
4 Maximum Ent ropy  
To explain our method, we l)riefly des(:ribe the con- 
(:ept of maximum entrol)y. Recently, many al)- 
lnoaches l)ased on the maximum entroi)y lnodel have 
t)een applied to natural anguage processing (Berger 
eL al., \]994; Berger et al, 1996; Pietra et al, 1997). 
Suppose a model p which assigns a probability to 
a random variable. If we don't have rely knowledge, 
a reasonal)le solution for p is the most unifbrnl dis- 
tribution. As some knowledge to estilnate the model 
p are added, tile solution st)ace of p are more con- 
strained and the model would lie close to the ol\]timal 
probability model. 
For the t)url/ose of getting tile optimal 1)robability 
model, we need to maxi\]nize the unifl)rnlity under 
some constraints we have. ltere, the constraints are 
related with features. A feature, fi is usually rel/re - 
sented with a binary indicator funct, ion. The inlpor- 
tahoe of a feature, fi can be identified by requiring 
that the model accords with it. 
As a (:onstraint, the expected vahle of fi with re- 
spect to tile model P(fi) is supposed to be the same 
as tile exl)ected value of fi with respect o empiri(:al 
distril)ution in training saml)les, P(fi). 
441 
TAG 
cb 
DT 
PW 
JJ 
JJS 
MD 
NNP 
PDT 
PRP 
RB 
RBS 
SYM 
UH 
VBD 
VBN 
WP$ 
NOT 
BED 
BEG 
HVD 
DOD 
DESCRIPTION 
comma 
conjunction,coordinating 
determiner 
foreign word 
adjective, ordinal 
adjective, superlative 
modal auxiliary 
noun, proper, singular 
pre-determiner 
pronoun, personal 
adverb 
adverb, superlative 
symbol 
interjection 
verb, past tense 
verb, past participle 
WH-pronoun, possessive 
not 
be verb, past tense 
be verb, present participle 
have verb, past participle 
do verb, past tense 
TAG 
CD 
EX 
IN 
J JR 
LS 
NN 
NNPS 
POS 
PRP$ 
RBR 
RP 
TO 
VBP 
VBG 
WDT 
WR8 
BEP 
BEN 
HVP 
DOP 
DON 
DESCRIPTION 
sentence terminator TAG 
numeral, cardinal 
existential there NNIN1 
preposition, subordinating NNIN2 
adjective, comparative NNDE1 NNDE2 list item marker PN 
noun, common NU 
noun, proper, plural VBMA 
genitive marker AJMA 
pronoun, possessive CO 
adverb, comparative AX 
particle ADCO 
to or infinitive marker APSE 
verb, present tense CJ 
verb, present participle ANCO 
WH-determiner ANDE 
WH-adverb ANNU 
be verb. present tense EX 
be verb, past participle LQ 
have verb, present tense RQ 
do verb, present tense SY 
do verb, past participle 
POS 
proper noun 
common noun 
common-dependent noun 
unit-dependent noun 
pronoun 
number 
verb 
adjective 
copula 
auxiliary verb 
constituent adverb 
sentential adverb 
conjunctive adverb 
configurative adnominal 
demonstrative adnominal 
numeral adnominal 
exclamination 
left quotation mark 
right quotation mark 
symbols 
TAG 
PPCA1 
PPCA2 
PPCA3 
PPCA4 
PPAD 
PPCJ 
PPAU 
ENTE 
ENCO1 
ENCO2 
ENCO3 
ENTRt 
ENTR2 
ENTR3 
ENCM 
PE 
SF 
PF 
CM 
SO 
Figure 1: English Tags (left) and Korean Tags (right) 
POS 
nominative postposition 
accusative postposition 
possessive postposition 
vocative postposition 
adverbial postposition 
conjunctive postposition 
auxiliary postposition 
final ending 
coordinate ending 
subordinate ending 
auxiliary ending 
adnominal ending 
nominal ending 
adverbial ending 
ending+postposition 
pre-ending 
suffix 
prefix 
comma 
termination 
~P~II'~ l lq  l l l l  l l~l '~l~t I l l i i LU i I f ( i |  I I  IL'II|II~ ;'~ ;~ l i l | ' t l l l  I I I l l  I LI~M 
\[VBI'+IN\] [take+out\] \[1+3\] 
\[wp\] \[tak(q \[t\] 
\[VBP+PI~P\] \[take+her\] \[1+2\] 
\[W3P+PRP+IN\] \[take+her+out\] \[1+2+3\] 
\[PRP\] [ho,-\] [2\] 
\[IN\] \[out\] \[3\] 
\[I'PCA2+P1)AD-FVBMA\] \[rcul+euro+deryeoga\] \[2+4+5\] 
\[PN\] b.... :/~o\] i l l  
\[PI)AI)+VBIvlA-FENTE\] \[reu/+cure-I- dcrycoga+ra\] \[4+5+6\] 
\[NNIN2\] [bagg \] \[3\] 
\[NNIN2+PPAD\] \[bagg+euro\] \[3+41 
\[ENTE\] \[ra\] [6\] 
\[P1)AD+VBMA\] \[curo+deryeoga\] \[4+5\] 
\[PPAD+VBMA+ENTE\] [euro+deryeoga+ra\] \[4+5+6\] 
\[PPCA2+NNIN2+PPAD+VBMA\] \[reul+bagg+curo+ deryeoga \] \[2+3+4+5\] 
\[PPCA2+NNIN2+PPAI)+VBMA+ENTE\] \[reul+bagg+euro+dcryeoga+ra \] \[2+3+4+5+6\] 
\[P1)CA2+NNIN2+PPAD+VBMA\] \[renl+deryeoga \] \[2+3+4+5\] 
\[PPCA2+NNIN2+Pt)AD+VBMA+ENTE\] \[reul+deryeoga+ra\] \[2+3+4+5+6\] 
Table 1: possible tag sequences 
In sun1, the maxilnunl entropy fralnework finds 
the model which has highest entropy(most uniform)~ 
given constraints. It is related to the constrained 
optimization. To select a model from a constrained 
set, C of allowed l)rol)ability distributions, the model 
p, C C with maximum entropy H(p) is chosen. 
In general, for the constrained optimization prob- 
lem, Lagrange inultipliers of the number of features 
can be used. However, it was proved that the model 
with maximum entropy is equivalent o the model 
that maximizes the log likelihood of the training 
samples like (2) if we can assume it as an exponential 
model. 
hi (2), the left side is Lagrangian of the condi-. 
tional entropy and the right side is inaxilnlHn log-. 
likelihood. We use the right side equation of (2) to 
select I. for the best model p,. 
~,g,,~..~,(- ~.,,. ~(~)v(yl~)logv(vlx)+~,,(v(f,)-~(/,))) (2) 
:a,'9,,,ax~, .,,. ~(x,v)lo.~n,(ylx) 
Since t ,  cannot be tbund analytically, we use 
the tbllowing improved iterative scaling algorithm to 
colnpute I ,  of n active features in .4 in total sam-- 
ples. 
1. Start with l i  = 0 for all i 6 {1 ,2 , . . . ,n}  
2. Do for ca.oh i ~ { \ ] ,2 , . . . ,n}  : 
(a) Let AAi be the solution to the log likeli- 
hood 
(b) Update the value of Ai into l i  + A,h, 
~. ..... ~(.,,v)A(:~,v) 
where AAi = log ~, : ,  ~i.~)v~(?11.~)/~(.~,v) 
px(yl:r) = ~-A--e(~ x'f'("':')) zx (:~.) ' , 
z (x) = E:, c(E ,  
3. Stop if not all the Ai have converged, otherwise 
go to step 2 
Tile exponential model is represented as(3). Here, 
l i  is the weight of feature f i .  In ore" model, since 
only one feature is applied to each pair of x and y, 
it can be represented as (4) and fi is the feature 
related with x and y. 
~(ylx) = ~ i  C'f'(x'Y) (3) 
cAifi(x,Y) 
= (4) 
442 
5 Feature  select ion 
Only a small subset of features will 1)e emph)yed in 
a model by sele(:ting useflfl feal;m'es from (;tie flmture 
1)ool 7 ). Let 1).,4 lie (;tie optimal mo(lel constrained 
by a set of active features M and A U J'i 1)e ,/lfi. Le(; 
PAf~ be the ot)timal model in the space of l)rol)abil- 
ity distribution C(Af i ) .  The optimal model can be 
tel)resented as (5). Here, the optimal model means 
a maxilmnn entropy nlodd. 
1 
v~ :, = z,,.(:,;) p'~ (:11,)::' ("'") 
zo,(: .)  = ~ v.~(::l:,,)c"S'(*'"> (5) 
Y 
The imi)rovement of l;he model regarding the ad- 
dition of a single feature f i  can be estiumted by mea- 
suring the difference of maximmn log-likelihood be- 
tween L(pAf~) and L(pA). We denote the gain of 
t~ature f i t i y  A(~lfi) an(l it can be r(!t/resented in
(G). 
A(A . I 'd  - .,..,:,;,,cAI~(.) 
('A:,(,,,) = J~(>t:,)-- L(v,O 
= _ ~(:~)~, .~( : , / I , . ) :  :'('''') 
x y 
?'~P(.fi) IS) 
Note that a model PA has a, set of t)arameters A 
which means weights of teatures. The m(idel P.Afl 
contains the l )a ra . lnetc . rs  an( I  the  new \[)a.l'a, lllCi;('~r (11 
with l'eSl)ect () the t'eal;ure fi. W'hen adding a new 
feature to A, the optimal values (if all parame(ers of 
probability (listril)u(,ion change. To make th(; (:om- 
i)utation of feature selection tractal)le, we al)l)roxi- 
mate that the addition of a feature f i  affec(;s only 
the single 1)aranxeter a, as shown in (5). 
qShe following a.lgoritlnn is used for (;omputing the 
gain of the model with rest)ect o fi. We referred 
to the studies of (Berger et al, 1996; Pietra e.t al., 
1997). We skip tile detailed contents and 1)root~. 
1. Let 
1 i f  P(fi) <_ PA(J;) 
r = -1  oth, erwise 
2. Set a0 = 0 
3. Repeat the following until GAf f (%, )  has con- 
verged : 
Co i l l l ) l l te  0@1,+ i frOll l  og n l lS i l lg  
a log (1 ! -~6t:' ( ' "~)  ~ ctn+l = (xn + 7" ,. (;~:~(~,,): 
Compute GaV~ (a,~+l) using 
GAA (a)  = - Ea,/3(a,') log Z,,(:,:) + ctf)(fi) , 
c'A:, (,~) = ~(k)  - Ex  ~(~0M( : , . ) ,  
G"  :ct~ A:,, , = -- E.~ P(")V2i:, ((fi -- M(;,;))" la;) 
set description # of disjoint total 
features cvtults 
A active feat m'es 1483 4113 
P feature Calldidat, es 3172 63773 
N new f'eaLures 97 5503 
Table 2: Summery of Features Selected 
where  (:~ ~ (l~n+ 1
A f~ = A u f~,  
M(z)  - p~f~ (fila-) , 
PP4S, (fi l ':) --- E .  ~'~s, (:,?l:r)k(:., ~J) 
d. Set ~ AL(Af i )  <-- GAS,(ct.) 
This algorittun is iteratively comtmted using Net- 
Wel l 'S  method. \?e cmt recognize the iml)ortance of a 
fl;ature with the gain value. As mentioned above, it 
means how much the feature accords with the model. 
We viewed the feature as tile information that Q. and 
t, occur together. 
6 Exper imenta l  resu l t s  
The total saml)les consists of 3,000 aligned Sellteiice 
pairs of English-Korean, which were extracted from 
news on the web site of 'Korea Times' and a. maga- 
zine fl)r English learning. 
In the initial step, we manually constucted (;tie 
correspondences of tag sequences with 700 POS- 
tagged sentence I)airs. hi the SUl)ervision step, 
we extracte(t l.,d83 correct tag sequence corresl)on- 
it(miles its shown in Table 2, and it work as active 
features. As a feature I)OOI, 3,172 (lisjoint %a(;ures 
of tag sequence ma.I)pings were retrieved. 1% is very 
important o make atomic thatures. 
We maxinfized A of active features with resl)ect 
to total smnples using improved the iterative scal- 
ing algoritlun. Figure 3 shows Ai of each feature 
.f(Q31,:P+.m,ttO C A. There a.re nlany corresl)on- 
dence 1)atterns with resl)ect o the Englsh tag string, 
'BEP+J J ' .  
Note that p(tt~lQ) is comtmted by the exponential 
model of (4) mid the conditional probability is the 
saine with empirical probal)ility in (7). Since the 
wflue of p(ylx)  shows the maxinmm likelihood, it is 
proved that each A was converged correctly.  
# of  (.% y) occurs in sam, pie 
P(ylx) - n, um, ber o f  t imes  o f  a: (7) 
hi feature selection step, we chose useflll fea- 
tures with the gain threshold of 0.008. Figure 
4 shows some feaures with a large gain. Anion\ 
then1, tag sequences mapping including 'RB'  are er- 
roneous.  It means that position of adverb in Ko- 
rean is very compl icated  to handle. Also, proper 
noun in English aligned coInmon nouns in Korean 
443 
English 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
BEP+JJ 
Feature(x,y) 
Korean 
VBMA+ENCO3+AX+ENTE 
VBMA 
AJ MA 
AJMA+ENTE 
VBMA+ENTE 
NNIN2+CO 
NNIN2+CO+VBMA 
NNIN2+PPCA1 +VBMA+ENTE 
NNIN2+CO+ENTE 
NNIN2+PPCA2+AX+ENTE 
NNIN2+PPCA1 +VBMA 
1 
10.1369 
8.8520 
8.6787 
8.2628 
7.2379 
7.1372 
6.9909 
8.8402 
6.8308 
6.4256 
6.4250 
Figure 3: A 
\[PRP\] you ~/'\[PN\] cJ~ (dangsin) 
"-\[PPAU\] ~-(eun) 
\[RB\] usually \[ADCO\] EttX41_~(dachero) 
HVP\] have, /\[NNIN2\] ~uf~(ilbanseok) 
TO\] to /~ \[PPAD\] 0tl(e) 
\[VBP\] take ; / \ [VBMA\ ]  N(anj) 
,\[ENCO3\] O~OIE~(ayaman) 
\[J J\] regular . . . . . .  ':-:- \[AX\] "$F(ha) 
\[NN\] seating/' \[ENTE\] L E}(nda) 
Figure 5: Best Lexical alignment 
because of tagging errors. Note that in the case of 
'PN+PPCA2+PPAD+VBMA' ,  it is not an adjacent 
string but an interrupted string. It means ttlat a 
verb in English generally map to a verb taking as 
argument the accusative and adverbial postposition 
in Korean. 
One way of testing usefulness of our method is 
to construct structured aligned bil ingual sentences. 
Table 3 shows lexical al ignments using tag sequence 
al ignments drawn from our algorithm for a given 
sentence, 'you usually have to take regular seating 
- dangsineun dachcw ilbanscokc anjayaman handa' 
and Figure 5 shows the best lexical alignment of the 
sentence. 
We conducted the exi)eriment on 100 sentences 
composed of words in length 14 or less and siln- 
lilY chose the most likely paths. As tim result, the 
accuray was about 71.1~. It shows that we can 
partly use the tag sequence alignments for lexical 
alignments. We will extend tlle structural mapping 
model with consideration to the lexical information. 
The parmneters, the conditional probabilities about 
stuctural mappings will be embedded in a statisti- 
cal model. Table 4 shows conditional probabilities 
of seine features according to 'DT+NN'.  In general, 
determiner is translated into NULL or adnominal 
word in Korean. 
7 Conclus ion 
When aligning Englist>Korean sentences, the differ- 
ences of word order and word unit require structural 
information. For tiffs reason, we tried structural tag 
c(x,y) 
162.0C 
45.00 
39.00 
25.00 
9.00 
8.00 
7.00 
6.00 
6.00 
4.00 
4.00 
P(Ylx) 
0.4247 
0.1180 
0.0996 
0.0655 
0.0236 
0.0210 
0.0183 
0.0157 
0.0157 
0.0105 
0.0105 
Example 
English 
are+prepared 
are+careful 
am+healthy 
is+new 
am+sure 
am+rich 
is+selfish 
is+patriotic 
is+reasonable 
is+reprehensible 
is+helpful 
Korean 
~kllKl+0~+?~+~ LI El- 
0-94 8i 
~ J  8t+ ~ LI C\[ 
~X\[+01 
0191~+01+~LIEt 
011~;Xt+9\[+~+EF 
'NPJ ~ +01 +g~ 
.~N+01 +.El 
of active features in A 
t~ 
I)T-t-NN 
I)Tq-NN 
DT-FNN 
DT+NN 
DT-t-NN 
DT-FNN 
I)T-FNN 
etc 
t~ 
NNIN2 
ANI)E+NNIN2 
ANNUWNNDE2 
NNIN2+PPCA1 
NNIN2+NNIN2 
NNIN2-FPPAU 
ADCO 
eI;c 
p(t~l*~) 
0.524131 
0.15161 
0.091036 
0.063515 
0.058322 
0.05768 
0.049622 
Table 4: Conditional Probability 
string mapping using maximum entropy modeling 
and feature selection concept. We devised a nlodel 
that generates a English tag string given a Korean 
tag string. From initial active structural features, 
useful features are extended by feature selection. 
Tile retrieved features and parameters can be em- 
bedded in statistical maclfine translation and reduce 
the complexity of searching. We showed that they 
can helpful to construct structured aligned bilingual 
sentences. 
References  
Adam L. Berger, Peter F. Brown, Stephen A. 
Della Pietra, Vincent J. Della Pietra, John R. 
Gillett, John D. Lafferty, Robert L. Mercer, Harry 
Printz, and Lubos Ures. 1994. The Calldie sys- 
tem for machine translation, hi Proceedings of the 
ARPA Conference on Human Language Technol- 
ogy, Plainsborough, New Jersey. 
Adam L. Berger, Stephen A. Della Pietra, and Vin- 
cent J. Della Pietra. 1996. A maximum entropy 
approacll to natural anguage processing. Compu- 
tational Linguistics, 22(1):39-73. 
Peter F. Brown, John Cocke, Stephen A. Della 
Pietra, Vincent J. Della Pietra, Fredrick Jelinek, 
John D. Lafferty, Robert L. Mercer, and Paul S. 
Roossin. 1990. A statistical approach to nlachine 
translation. Computational Linguistics, 16(2):79- 
85 
Peter F. Brown, Stephen A. Della Pietra, Vincent J. 
Della Pietra, Robert L. Mercer. 1993. The math- 
enlatics of statistical machine translation: pa- 
444 
Feature(x,y) 
X 
VBP+PRP+TO 
BEP+RBR+IN 
DT4-CD 
JJ+IN 
VBG+TO 
BEP 
BEP 
NNP 
NNP 
TO+PRP 
TO+PRP 
MD+FIB 
MD+RB 
MD+BEP 
NNP+NNS 
VBP+TO+VBP 
BED+VSN+IN 
BED+VSN+IN 
Y 
PNYPPCA2+PPAD+VBMA 
PPAD+AJMA 
NU+NNDE2 
PPAD+AJMA+ENTR1 
PPAD+PPCA2+VBMA 
PPCA1 +AJMA 
PPCA1 +AJMA+ENTE 
NNIN2+PPAU 
NNIN2+NNIN2 
PN+PPAD 
PN+PPCA1 
ENTRI+NNDEI+CO 
ENTRI+NNDEI+CO+ENTE 
CO+ENCO2+VBMA 
NNIN2+SF 
PPCA2+VBMA+ENCO2+VBM~ 
PPAD+VBMA+PE+ENTE 
PPAD+VBMA+PE 
r,~; P(ylx) /~ L(Afi) 
9.8687 0.1722 0.0194 
9.6780 0.3265 0.0192 
9.2799 0.2449 0.0190 
9.6343 0.2450 0.0190 
9.9542 0,3269 0.0189 
9.6720 0.2941 0.0188 
9.2724 0.1961 0.0188 
8.7481 0.1225 0.0182 
9.1397 0.1337 0.0180 
9.5634 0.2307 0.0180 
9.2604 0.1730 0.0180 
9.2445 0.1548 0.0177 
9.2564 0.1548 0.0177 
8.5435 0.0934 0.0177 
9.1597 0.1470 0,0176 
8.9928 0.1278 0.0174 
9.1511 0.1704 0.0173 
9.1636 0.1706 0,0173 
English 
send+Nm+to - - 
is+more+than 
the+two 
smarter+than 
serving+to 
is 
are 
IBM 
Harvard 
to+him 
to+her 
? 
? 
should+be 
English+books 
request+to+send 
was+thrown+to 
were+sent+to 
Example 
Korean 
21+ N+-0tF+-~ L\]I 
-h20+~ 
?+N 
~H+~PA+N+L 
~OII3tI+-~+U~ 
-0I+OA 
-01+~+H 
IBM+~- 
3+OIl)II 
~LI +)F 
? 
? 
Ol+OiOI+N 
-N+~Ll l+Kf~+5/N bF 
~011?t1+c3 ~1XI+~+FA 
Figure d: Some fbatm'es with a large gain 
Tag a l ignment  (km( l i t io lml  Lex ica l  aliglulw, nt 
l ) l{P : PN+I )PAU 0.150109 you : dangs in+cun 
lt\]3 : A I )CO 0.142193 usmt l ly  : dachero  
II, B : NNIN2+PI 'A I )  0.038105 usua l ly  : i lbanseol?-l-e 
I IVP+TO : I~N( JO3-bAX+I"NTE 0,982839 have+to  : ayaman+handa 
VBP  : P1)A I )+VBMA 0.05022,l take  : e+an j  
VBP  : VBMA+F,  NCO3+AX+I , ;NTE  0.011110 take  : an jay+aman-Fha+nda 
V I3P  : P1)A I )+VI~MA+ENCO3-}-AX- I -ENT\ ] ,~  0,001851 take  : e -Fan jaya imm+handa 
V I3P -F J J  : NNIN2+PI )A I ) - \ ] -VBMA 0.057657 take- t - regu lar  : i l bansenk+e+,a l l j  
. I . J+NN : NNIN2 0.581791 regu lar+seat ing  : i l l )anseok 
an(, 3: l,exi(:al aligmnents using tag alignments 
rameter estimation. Computational Linguistics, 
19(2):263-311. 
Stanley F. Chert. 1993. Aligning sentences in bilin- 
gual corpora using lexical information. In l'rocccd- 
ings of ACL ,71, 9-16. 
A. P. Dempster, N. M. Laird and 1). 13. l{ubin. 
1976. Maximum likelihood fi'om incomplet,e data 
via the EM algorithm. The Royal ,S'tatistics Soci- 
ety, 39(B) 205-237. 
Williain A. Gale, Kenneth W. Church. 1993. A pro- 
gram fbr aligning senten(:es in bilingual (-orl)ora. 
Coml)utational Linguistics, \]9:75-102. 
Frederick Jelinek. \]997. Statistical Methods for 
Speech Recognition MIT Press. 
Marin Kay, Martin Roscheisen. 1993. Text- 
translation alignment. Computational Linguis- 
tics, 19:121-142. 
Julian Kupiec. 1993. An algorithm tbr finding noun 
phrase corresl)ondenccs in bilingual corl)ola. In 
Proceedings of ACL 31, 17-22. 
Yuji Matsmno~o, Hiroyuki Ishimoto, Takehito Ut- 
sure. 1993. Structural inatching of para.llel texts. 
In Proceedings of ACL 3I, 23-30. 
I. Dan Melame(l. 1997. A word-to-word model of 
translation equivalence. In PTvcccdings of ACL 
35/EACL 8, 1.6-23. 
Frmlz Josef Och mid Ilans Wel)cr. 1.998. hnt)rov- 
ing Statistical Natural Language Translation with 
Categories and Rules. In Procccdings of ACL 
36/COLING, 985-989. 
Stephen A. Della Pietra, Vincent J. Della Pietra, 
John D. La.tl'erty. 1997. llnducing features of ran- 
dora fields. IEEE ~IYansactions on Pattern Anal- 
ysis and Machine Intelligence, 19(4):380-393. 
Frank Smadja, Kathleen R. McKeown, and Vasileios 
Hatziw~ssiloglou. 1996. Translating collocations 
fi)r bilingual lexicons: A statistical approa(:h. 
Computational Linguistics, 22 (1) :1-38. 
Kengo Sate 1998. Maximum Entrol)y Model Learn- 
ing of the Translation Rules. In Procccdinfls of 
ACL 35/COLING, 1171-1175. 
Jung H. Shin, Y(mng S. Han, and Key-Sun 
Choi. 1996. Bilingual knowledge acquisition from 
Korean-English paralM cort)us using aligmnent 
method. In Proceedings of COLING 96. 
C. Tilhnann, S. Vogel, H. Ney, and A. Zubiaga. 
1997. A I)P t)ased sea.rch using monotone a.lign- 
ments in statistical translation. In Procccdings of 
ACL 35/EACL 8, 289-296. 
Ye-Yi Wa.ng and Alex Waibel. 1997. Decoding algo- 
rithm in statistical machine translation. In Pro- 
cccdinfls of ACL 35/EACL 8, 366-372. 
Ye-Yi Wang and Alex Waibel. 1998. Modeling with 
structures in machine translation. In Procccdings 
of ACL 36/COLING 
Dekai Wu 1996. A t)olynonlial-time algorithm for 
statistical machine translation. In Proceeding of 
A CL 34. 
445 
 
	ff