Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 593?600
Manchester, August 2008
Robust Similarity Measures for Named Entities Matching
Erwan Moreau1
Institut Te?le?com ParisTech
& LTCI CNRS
erwan.moreau@enst.fr
Franc?ois Yvon
Univ. Paris Sud
& LIMSI CNRS
yvon@limsi.fr
Olivier Cappe?
Institut Te?le?com ParisTech
& LTCI CNRS
cappe@enst.fr
Abstract
Matching coreferent named entities with-
out prior knowledge requires good similar-
ity measures. Soft-TFIDF is a fine-grained
measure which performs well in this task.
We propose to enhance this kind of met-
rics, through a generic model in which
measures may be mixed, and show experi-
mentally the relevance of this approach.
1 Introduction
In this paper, we study the problem of matching
coreferent named entities (NE in short) in text col-
lections, focusing primarily on orthographic vari-
ations in nominal groups (we do not handle the
case of pronominal references). Identifying textual
variations in entities is useful in many text min-
ing and/or information retrieval tasks (see for ex-
ample (Pouliquen et al, 2006)). As described in
the literature (e.g. (Christen, 2006)), textual dif-
ferences between entities are due to various rea-
sons: typographical errors, names written in dif-
ferent ways (with/without first name/title, etc.),
abbreviations, lack of precision in organization
names, transliterations, etc. For example, one
wants ?Mr. Rumyantsev? to match with ?Alexan-
der Rumyanstev? but not with ?Mr. Ryabev?.
Here we do not address the related problem of dis-
ambiguation2 (e.g. knowing whether a given oc-
currence of ?George Bush? refers to the 41st or
43rd president of the USA), because it is techni-
cally very different from the matching problem.
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1Now at LIPN - Univ. Paris 13 & UMR CNRS 7030.
2Which is essential in the Web People Search task.
There are different ways to tackle the problem
of NE matching: the first and certainly most reli-
able one consists in studying the specific features
of the data, and then use any available tool to de-
sign a specialized method for the matching task.
This approach will generally take advantage of
language-specific (e.g. in (Freeman et al, 2006))
and domain-specific knowledge, of any external
resources (e.g. database, names dictionaries, etc.),
and of any information about the entities to pro-
cess, e.g. their type (person name, organization,
etc.), or internal structure (e.g. in (Prager et al,
2007)). In such an in-depth approach, supervised
learning is helpful: it has been used for example
in a database context3 in (Bilenko et al, 2003), but
this approach requires labeled data which is usu-
ally costly. All those data specific appproaches
would necessitate some sort of human expertise.
The second approach is the robust one: we
propose here to try to match any kind of NE,
extracted from ?real world? (potentially noisy)
sources, without any kind of prior knowledge4.
One looks for coreferent NE, whatever their type,
source, language5 or quality6. Such robust simi-
larity methods may be useful for a lot of generic
tasks, in which maximum accuracy is not the main
criterion, or simply where the required resources
are not available.
The literature on string comparison metrics is
abundant, containing both general techniques and
3The matching task is quite different in this framework,
because one observes records (structured information).
4In this kind of knowledge are included the need for hand-
tuning parameters or defining specific thresholds.
5Actually we have only studied English and French (our
approach is neither ?multilingual?, in the sense that it is not
specific to multilingual documents).
6In particular, this task clearly depends on the NE recog-
nition step, which may introduce errors.
593
more linguistically motivated measures, see e.g.
(Cohen et al, 2003) for a review. From a bird?s eye
view, these measures can be sorted in two classes:
?Sequential character-based methods? and ?Bag-
of-words methods?7. Both classes show relevant
results, but do not capture the same kind of simi-
larity. In a robust approach for NE matching, one
needs a more fine-grained method, which performs
at least as well as bag-of-words methods, without
ignoring coreferent pairs that such methods miss.
A first attempt in this direction was introduced
in (Cohen et al, 2003), in the form of a measure
called Soft-TFIDF. We will show that this measure
has theoretical pitfalls and a few practical draw-
backs. Nevertheless, Soft-TFIDF outperforms the
better standard string similarity measures in the
NE matching task. That is why we propose to gen-
eralize and improve its principle, and show exper-
imentally that this approach is relevant.
In section 2 we introduce standard similar-
ity measures and enhance the definition of Soft-
TFIDF. Then we define a generic model in which
similarity measures may be combined (section 3).
Finally, section 4 shows that experiments with two
different corpora validate our approach.
2 Approximate matching methods
We present below some of the main string similar-
ity measures used to match named entities (Chris-
ten, 2006; Cohen et al, 2003; Bilenko et al, 2003).
2.1 Classical metrics
2.1.1 Sequential character based methods
Levenshtein edit distance. This well-known dis-
tance metric d represents the minimum number
of insertions, deletions or substitutions needed to
transform a string x into another string y. For ex-
ample, d(kitten, sitting) = 3 (k 7? s, e 7? i,
? 7? g). The corresponding normalized similarity
measure is defined as s = 1 ? d/max(|x|, |y|). A
lot of variants and/or improvements exist (Navarro,
2001), among which:
? Damerau. One basic edit operation is added:
a transposition consists in swapping two
characters;
? Needleman-Wunch. Basic edit operation
costs are parameterized: G is the cost of a gap
7We omit measures based on phonetic similarity such
as Soundex, because they are language-specific and/or type-
specific (person names).
(insertion or deletion), and there is a function
cost(c, c
?
) which gives the cost of substituting
c with c? for any pair of characters (c, c?).
Jaro metric (Winkler, 1999). This measure is
based on the number and the order of common
characters. Given two strings x = a
1
. . . a
n
and
y = b
1
. . . b
m
, let H = min(n,m)/2: a
i
is in com-
mon with y if there exists b
j
in y such that a
i
= b
j
and i ? H ? j ? i + H . Let x? = a?
1
. . . a
?
n
?
(resp. y? = b?
1
. . . b
?
m
?
) be the sequence of charac-
ters from x (resp. y) in common with y (resp. x),
in the order they appear in x (resp. y). Any posi-
tion i such that a?
i
6= b
?
i
is called a transposition.
Let T be the number of transpositions between x?
and y? divided by 2:
Jaro(x, y) =
1
3
?
(
|x
?
|
|x|
+
|y
?
|
|y|
+
|y
?
|?T
|y
?
|
)
2.1.2 Bag-of-words methods
With these methods, each NE is represented as
a set of features (generally words or characters n-
grams8). Let X = {x
i
}
1?i?n
and Y = {y
i
}
1?i?m
be the sets representing the entities x, y. Simplest
measures only count the number of elements in
common9, e.g:
Overlap(x, y) =
|X ? Y |
min(|X|, |Y |)
Some more subtle techniques are based on a
vector representation of entities x and y, which
may take into account parameters that are are
not included in the sets themselves. Let A =
(a
1
, . . . , a
|?|
) and B = (b
1
, . . . , b
|?|
) be such vec-
tors10, the widely used cosine similarity is:
cos(A,B) =
?
|?|
i=1
a
i
b
i
?
?
|?|
i=1
a
2
i
?
?
|?|
i=1
b
2
i
Traditionally, TF-IDF weights are used in
vectors (Term Frequency-Inverse Document Fre-
quency). In the NE case, this value represents the
importance each feature w (e.g. word) has for an
entity x belonging to the set E of entities:
tf(w, x) =
n
w,x
?
w
?
??
n
w
?
,x
, idf(w) = log
|E|
|{x ? E|w ? x}|
,
tfidf(w, x) = tf(w, x) ? idf(w).
with n
w,x
the number of times w appears in x.
Thus the similarity score is CosTFIDF(x, y) =
Cos(A,B), where each a
i
(resp. b
i
) in A (resp. in
B) is tfidf(w
i
, x) (resp. tfidf(w
i
, y)).
8In the remaining the term n-grams is always used for
characters n-grams.
9
|E| denotes the number of elements in E.
10
? is the vocabulary, containing all possible features.
594
2.2 Special measures for NE matching
Experiments show that sequential character-based
measures catch mainly coreferent pairs of long NE
that differ only by a few characters. Bag-of-words
methods suit better to the NE matching problem,
since they are more flexible about word order and
position. But a lot of coreferent pairs can not be
identified by such measures, because of small dif-
ferences between words: for example, ?Director
ElBaradei? and ?Director-General ElBareidi? is
out of reach for such methods. That is why ?sec-
ond level? measures are relevant: their principle is
to apply a sub-measure sim? to all pairs of words
between the two NE and to compute a final score
based on these values. This approach is possible
because NE generally contain only a few words.
Monge-Elkan measure belongs to this category:
it simply computes the average of the better pairs
of words according to the sub-measure:
sim(x, y) =
1
n
n
?
i=1
m
max
j=1
(sim
?
(x
i
, y
j
)).
But experiments show that Monge-Elkan does
not perform well. Actually, its very simple behav-
ior favors too much short entities, because averag-
ing penalizes a lot every non-matching word.
A more elaborated measure is proposed in (Co-
hen et al, 2003): Soft-TFIDF is intended precisely
to take advantage of the good results obtained with
Cosine/TFIDF, without automatically discarding
words which are not strictly identical. The original
definition is the following: let CLOSE(?,X, Y )
be the set of words w ? X such that there ex-
ists a word v ? Y such that sim?(w, v) > ?. Let
N(w, Y ) = max({sim
?
(w, v)|v ? Y }). For any
w ? CLOSE(?,X, Y ), let
S
w,X,Y
= weight(w,X) ? weight(w, Y ) ?N(w, Y ),
where weight(w,Z) = tfidf(w,Z)?
?
w?Z
tfidf(w,Z)
2
.
Finally,
SoftTFIDF(X,Y ) =
?
w?CLOSE(?,X,Y )
S
w,X,Y
.
This definition is not entirely correct, be-
cause weight(w, Y ) = 0 if w /? Y (in other
words, w must appear in both X and Y , thus
SoftTFIDF(X,Y ) would always be equal to
CosTFIDF(X,Y )). We propose instead the fol-
lowing corrected definition, which corresponds to
the implementation the authors provided in the
package SecondString11:
11http://secondstring.sourceforge.net
Let CLOSEST(?,w,Z) = {v ? Z | ?v? ? Z :
sim
?
(w, v) ? sim
?
(w, v
?
) ? sim
?
(w, v) > ?}.
SoftTFIDF(X,Y ) =
?
w?X
weight(w,X) ? ?
w,Y
,
where ?
w,Z
= 0 if CLOSEST(?,w,Z) = ?, and
?
w,Z
= weight(w
?
, Z) ? sim
?
(w,w
?
) otherwise,
with12 w? ? CLOSEST(?,w,Z).
As one may see, SoftTFIDF relies on the same
principle than Monge-Elkan: for each word x
i
in the first entity, find a word y
j
in the second
one that maximizes sim?(x
i
, y
j
). Therefore, these
measures have both the drawback not to be sym-
metric. Furthermore, there is another theoretical
pitfall with SoftTFIDF: in Monge-Elkan, the fi-
nal score is simply normalized in [0, 1] using the
average among words of the first entity. Accord-
ing to the principle of the Cosine angle of TF-
IDF-weighted vectors, SoftTFIDF uses both vec-
tors norms. However the way words are ?approx-
imately matched? does not forbid the matching of
a given word in the second entity twice: in this
case, normalization is wrong because this word is
counted only once in the norm of the second vec-
tor. Consequently there is a potential overflow: ac-
tually it is not hard to find simple examples where
the final score is greater than 1, even if this case is
unlikely with real NE and a high threshold ?.
3 Generalizing Soft-TFIDF
3.1 A unifying framework for similarity
measures
We propose to formalize similarity measures in the
generic model below. This model is intended to
define, compare and possibly mix different kinds
of measures. The underlying idea is simply that
most measures may be viewed as a process follow-
ing different steps: representation as a sequence of
features13 (e.g. tokenization), alignment and a way
to compute the final score. We propose to define a
similarity measure sim through these three steps,
each of them is modeled as a function14:
Representation. Given a set F of features, let
features(e) = ?a
1
, . . . , a
n
? be a function that as-
12If |CLOSEST(?, w, Z)| > 1, pick any such w? in the
set. In the case of matching words between NE, this should
almost never happen.
13We use the word feature for the sake of generality.
14Of course, alternative definitions may be relevant. In par-
ticular one may wish to allow the alignment function to return
a set of graphs instead of only one. In the same way, one may
wish to add a special vertex ? to the graph, in order to repre-
sent the fact that a feature is not matched by adding an edge
between this feature and ?.
595
signs an (ordered) sequence of features to any en-
tity e (a
i
? F for any i). Features may be of any
kind (e.g. characters, words, n-grams, or even con-
textual elements of the entity) ;
Alignment. Given a function simF : F 2 7? R
which defines similarity between any pair of fea-
tures, let algn(?a
1
, . . . , a
n
?, ?a
?
1
, . . . , a
?
n
?
?) = G
be a function which assigns a graph G to any pair
of features sequences. G = (V,E) is a bipartite
weighted graph where:
? The set of vertices is V = A ? A?, where
A and A? are the partitions defined as A =
{v
1
, . . . , v
n
} and A? = {v?
1
, . . . , v
?
n
?
}. Each
v
i
(resp. v?
i
) represents (the position of) the
corresponding feature a
i
(resp. a?
i
) ;
? The set of weighted edges is E =
{(v
i
j
, v
?
i
?
j
, s
j
)}
1?j?|E|
, where v
i
j
? A,
v
?
i
?
j
? A
?
. Weights s
j
generally depend on
sim
F
(a
i
j
, a
?
i
?
j
).
Scoring. Finally sim = score(G), where score
assigns a real value (possibly normalized in [0, 1])
to the alignment G.
The representation step is not particularly origi-
nal, since different kinds of representation have al-
ready been used both with sequential methods and
?bag-of-features? methods. However our model
also entails an alignment step, which does not exist
with bag-of-features methods. Actually, the align-
ment is implicit with such methods, and we will
show that making it visible is essential in the case
of NE matching.
In the remaining of this paper we will only con-
sider normalized metrics (scores belong to [0, 1]).
3.2 Revisiting classical similarity measures
Measures presented in section 2 may be defined
within the model presented above. This mod-
elization is only intended to provide a theoretical
viewpoint on the measures: for all practical pur-
poses, standard implementations are clearly more
efficient. Below we do not detail the represen-
tation step, because there is no difficulty with it,
and also because it is interesting to consider that
any measure may be used with different kinds
of features, as we will show in the next section.
Let S = ?a
1
, . . . , a
n
? = features(e) and S? =
?a
?
1
, . . . , a
?
n
?
? = features(e
?
) for any pair of enti-
ties (e, e?).
3.2.1 Levenshtein-like similarity
The function align
lev
(S, S
?
) is defined in the
following way: let G
lev
be the set of all graphs
G = (V,E) such that any pair of edges
(v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E satisfies (i
j
<
i
k
? i
?
j
< i
?
k
) ? (i
j
> i
k
? i
?
j
> i
?
k
). This
constraint ensures that the sequential order of fea-
tures is respected15 , and that no feature may be
matched twice. In the simplest form of Leven-
shtein16, simF (a, b) = 1 if a = b and 0 otherwise:
for any (v
i
j
, v
?
i
?
j
, s
j
) ? E, s
j
= sim
F
(a
i
j
, a
?
i
?
j
).
Let
sim(G) = M ?n
g
?cost
g
?|E|+
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
,
where M = max(n, n?) and n
g
is the number of
vertices that are not connected (i.e. the number of
inserted or deleted words). cost
g
= 1 in the simple
Levenshtein form, but may be a parameter in the
Needleman-Wunch variant (gap cost). In brief, the
principle in this definition is to count the positions
where no edit operation is needed: thus maximiz-
ing sim(G) is equivalent to minimizing the cost of
an alignment:
align
lev
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?)|G? ? G
lev
}).
Finally, the function score
lev
is simply defined
as score
lev
(G) = sim(G)/max(n, n
?
). It is not
hard to see that this definition is equivalent to the
usual one (see section 2): basically, the graph rep-
resents the concept called trace in (Wagner and
Fischer, 1974), except that the cost function is ?re-
versed? to become a similarity function.
Figure 1: Example of Levenshtein alignment
k
i
t
t
s
i
t
t
i
n
g
e
n
0
1
1
1
0
1
Suppose cost
g
= 1:
sim(G) = M ?n
g
?|E|+
?
e
j
?E
s
j
sim(G) = 7 ? 1 ? 6 + 4
sim(G) = 4
score
lev
(G) = 4/7.
3.2.2 Bag of features
For all simple measures using only sets of fea-
tures, the function align
bag
(S, S
?
) is defined in
the following way: let G be the set of all graphs
15Constraints are a bit more complex for Damerau.
16In the Needleman-Wunch variant, simF should depend
on the cost function, e.g.: simF (a, b) = 1? cost(a, b).
596
G = (V,E) such that if (v
i
j
, v
?
i
?
j
, s
j
) ? E then
a
i
j
= a
?
i
?
j
(equivalently simF (a
i
j
, a
?
i
?
j
) = 1). Now
let once(G) be the set of all G ? G such that
any pair of edges (v
i
j
, v
?
i
?
j
, s
j
), (v
i
k
, v
?
i
?
k
, s
k
) ? E
satisfies i
j
6= i
k
? i
?
j
6= i
?
k
(at most one match
for each feature), and a
i
j
6= a
i
k
(a feature oc-
curring several times is matched only once). Let
sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
for any G = (V,E).
align
bag
(S, S
?
) = G, where G is any graph such
that sim(G) = max({sim(G?) |G? ? once(G)}).
Since all weights are equal to 1, one may show
that sim(G) = |S ? S?| for any G ? once(G).
Thus the score function is simply used for nor-
malization, depending on the given measure: for
example, score
overlap
(G) =
sim(G)
min(n, n
?
)
.
3.2.3 Soft-TFIDF
The case of Cosine measure with TFIDF
weighted vectors is a bit different. Here we define
the SoftTFIDF version: let algn
soft
(S, S
?
) be the
graph G = (V,E) defined as17 (v
i
j
, v
?
i
?
j
, s
j
) ? E if
and only if a?
i
?
j
= select(CLOSEST(?, a
i
j
, S
?
)),
where CLOSEST is the function defined in sec-
tion 2 and select(E) is a function returning the
first element in E if |E| > 0, and is undefined
otherwise18. For any such edge, the weight s
j
is
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Once again, let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
.
score
soft
(G) = sim(G)/(?S|| ? ?S
?
?), where
??a
1
, . . . , a
n
?|| =
?
?
?
?
n
?
i=1
(
idf(a
i
)
n
)
2
.
Although it is not explicitly used in this defini-
tion, term frequency is taken into account through
the number of edges: suppose a given term t ap-
pears m times in S and m? times in S?, all m ver-
tices corresponding to t in A (the partition repre-
senting S) will be connected to all m? vertices cor-
responding to t in A?. Thus there will be m ? m?
edges, which is exactly the unnormalized product
17In the simple case of CosTFIDF, the condition would be:
(v
i
j
, v
?
i
?
j
, s
j
) ? E if and only if a
i
j
= a
?
i
?
j
. In other words,
all identical features (and only they) are connected.
18
?the first element? means that select(E) may return any
e ? E, provided the same element is always returned for the
same set.
of term frequencies tf(t, S) ? tf(t, S?) ?n ?n?. Thus
summing m ? m? times idf(t)/n ? idf(t)/n? in
sim(G) is equal to tfidf(t, S) ? tfidf(t, S?) (nor-
malization is computed in the same way).
3.3 Meta-Levenshtein: Soft-TFIDF with
Levenshtein alignment
We have shown in part 2.2 that there are some
pitfalls in Soft-TFIDF, especially in the way the
alignment is computed: no symmetry, possible
score overflow. But experiments show that tak-
ing words IDF into account increases performance,
and that Soft-TFIDF, i.e. the possible matching
of words that are not strictly identical, increases
performance (see section 4). That is why improv-
ing this kind of measure is interesting. Follow-
ing the model we proposed above, we propose to
mix the cosine-like similarity used in Soft-TFIDF
with a Levenshtein-like alignment. The following
measure, called Meta-Levenshtein (ML for short),
takes IDFs into account but is not a bag-of-features
metrics.
Let us define align
ML
in the following way: let
G
ML
be defined exactly as the set of graphs G
lev
(see part 3.2.1), except that weights are defined as
in the case of Soft-TFIDF: for any G = (V,E) ?
G
lev
and for any edge (v
i
j
, v
?
i
?
j
, s
j
) ? E, let
s
j
= sim
F
(a
i
j
, a
?
i
?
j
) ?
idf(a
i
j
)
n
?
idf(a
?
i
?
j
)
n
?
.
Let sim(G) =
?
(v
i
j
,v
?
i
?
j
,s
j
)?E
s
j
, and
align
ML
(S, S
?
) = G, where G is such that
sim(G) = max({sim(G
?
) |G
?
? G
ML
}). Finally,
score
ML
(G) = sim(G)/(?S|| ? ?S
?
?).
Compared to Soft-TFIDF, ML solves the prob-
lem of symmetry (ML(S, S?) = ML(S?, S)), and
also the potential overflow, because no feature may
be matched twice (see fig. 2). Of course, the align-
ment is less flexible in ML, since it must satisfy the
sequential order of features. Practically, this mea-
sure may be efficiently implemented in the same
way as Levenshtein similarity, including option-
ally the Damerau extension for transpositions. We
have also tested a simple variant with possible ex-
tended transpositions, i.e. cases like ABC com-
pared to CA, where both C and A are matched.
3.4 Recursive combinations for NE matching
One of the points we want to emphasize through
the generic framework presented above is the mod-
597
Figure 2: Soft-TFIDF vs. ML alignment
With sim(A,D) ? ?, and sim(C,E) ? sim(B,E) ? ?:
A
C
B
A
D
E
F
Soft-TFIDF
A
C
B
A
D
E
F
ML
ularity of similarity measures. Our viewpoint is
that traditional measures may be seen not only in
their original context, but also as modular param-
eterized functions. The first application of such a
definition is already in use in the form of measures
like Monge-Elkan or Soft-TFIDF, which rely on
some sub-measure to compare words inside NEs.
But we will show that modularity is also useful
at a lower level: measures concerning words may
rely on similarity between (for example) n-grams,
and even at this restricted level numerous possible
kinds of similarity may be used.
Moreover, from the viewpoint of applications it
is not very costly to compute similarities between
n-grams and even between words. The number
of n-grams is clearly bounded, and the number of
words is not so high because there are only about 2
words by entity in average, and overall some words
appear very often in entities19.
4 Experiments
4.1 Data
Two corpora were used. Both contain mainly news
and press articles, collected from various interna-
tional sources. The first one, called ?Iran Nu-
clear Threat? (INT in short), is in English and
was extracted from the NTI (Nuclear Threat Ini-
tiative) web site20. It is 236,000 words long. Our
second corpus, called ?French Speaking Medias?
(FSM in short), is 856,000 words long. It was ex-
tracted from a regular crawling of a set of French-
speaking international newspapers web sites dur-
ing a short time-frame (in July 2007). GATE21
was used as the named entities recognizer for INT,
whereas Arisem22 performed the tagging of NEs
19In the corpora we studied, 1172 NE (resp. 2533) contain
1107 distinct words (resp. 2785).
20http://www.nti.org
21http://gate.ac.uk
22http://www.arisem.com
for FSM. Recognition errors23 appear in both cor-
pora, but significantly less in FSM. We restricted
the sets of NEs to those recognized as locations,
organizations and persons, and decided to work
only on entities appearing at least twice. Finally
for INT (resp. FSM) we obtain 1,588 distinct
NE (resp. 3,278) accounting altogether for 33,147
(resp. 23,725) occurrences.
Of course, it would be too costly to manually
label as match (positive) or non-match (negative)
the whole set containing n ? (n ? 1)/2 pairs, for
the observed values of n. The approach consist-
ing in labeling only a randomly chosen subset of
pairs is ineffective, because of the disproportion
between the number of negative and positive pairs
(less than 0.1%). Therefore we tried to find all pos-
itive pairs, assuming the remaining lot are nega-
tive. Practically, the labeling step was based only
on the best pairs as identified by a large set of
measures24. The guidelines we used for labeling
are the following: any incomplete, over-tagged or
simply wrongly recognized NE is discarded. Then
remaining pairs are classified as positive (corefer-
ent), negative (non-coreferent), or ?don?t know?25.
Corpus Discarded Pos. Neg. Don?t know
INT 416 / 1,588 764 2,821 302
FSM 745 / 3,278 741 32,348 419
According to our initial hypotheses, all non-
tagged pairs are considered as negative in the ex-
periments below. ?Don?t know? pairs are ignored.
As a further note, about 20% of the pairs are not
orthographically similar (e.g. acronyms and their
expansion): these pairs are out of reach of our tech-
niques, and would require additional knowledge.
4.2 Observations
4.2.1 Taking IDF into account
To evaluate the contribution of IDF26 in scor-
ing the coreference degree between NE, let us ob-
23Mainly truncated entities, over-tagged entities, and com-
mon nouns beginning with a capital letter.
24This is a potential methodological bias, but we hope to
have kept its effect as low as possible: the measures we used
are quite diverse and do not assign good scores to the same
pairs; therefore, for each measure, we expect that the poten-
tial misses (false negatives) will be matched by some other
measure, thus allowing a fair evaluation of its performance.
A few positive pairs are manually added (mainly acronyms).
25All ambiguous cases, mainly due to some missing preci-
sion (e.g. ?Ministry of Foreign Affairs? and ?Russian Min-
istry of Foreign Affairs?), and more rarely homonymy (e.g.
?Lebedev? and ?[Valery|Oleg] Lebedev?)
26It may be noticed that the Term Frequency in TFIDF is
rarely important, since a given word appear almost always
only once in a NE.
598
serve the differences among best scored pairs for
measures Bag-of-words Cosine and Cosine over
TFIDF weighted vectors. For example, the for-
mer will assign 0.5 to pair ?Prime Minister Tony
Blair?/?Blair? (from corpus INT), whereas the
latter gives 0.61. As expected, IDF weights lighten
the effect of non-informative words and strengthen
important words. In both corpora, The F1-measure
for TFIDF Cosine is about 10 points (in average)
better than for Bag-of-words Cosine (see fig. 3).
4.2.2 Soft-TFIDF problems: normalization,
threshold and sub-measure
As we have explained in section 2.2, the Soft-
TFIDF measure (Cohen et al, 2003) may suffer
from normalization problems. This is probably
the reason why the authors seem to use it parsi-
moniously, i.e. only in the case words are very
close (which is verified using a high threshold
?). Indeed, problems occur when the sub-measure
and/or the threshold are not carefully chosen, caus-
ing performances drop: using Jaro measure with
a very low threshold (0.2 here), performances
are even worst than Bag-of-words cosine (see fig.
3). This is due to the double matching problem:
for example, pair ?Tehran Times (Tehran)?/?Inter
Press Service? (from INT) is scored more than 1.0
because ?Tehran? matches ?Inter? twice: even
with a low score as a coefficient, ?Inter? has a
high IDF compared to ?Press? and ?Service?, so
counting it twice makes normalization wrong.
However, this problem may be solved by choos-
ing a more adequate sub-measure: experiments
show that using the CosTFIDF measure with bi-
grams or trigrams outperforms standard CosT-
FIDF. Of course, there are some positive pairs
that are found ?later? by Soft-TFIDF, since it may
only increase score. But the ?soft? comparison
brings back to the top ranked pairs a lot of positive
ones. In both corpora, the best sub-measure found
is CosTFIDF with trigrams. ?Mohamed ElBa-
radei?/?Director Mohammad ElBaradei? (INT)
or ?Chine?/?China? (FSM) are typical positive
pairs found by this measure but not by standard
CosTFIDF. Here no threshold is needed anymore
because the sub-measure has been chosen with
care, depending on the data, in order to avoid the
normalization problem. This is clearly a drawback
for Soft-TFIDF: it may perform well, but only with
hand-tuning sub-measure and/or threshold.
4.2.3 Beyond Soft-TFIDF: (recursive) ML
In the FSM corpus, replacing Soft-TFIDF with
(simple) Meta-Levenshtein at the word level does
not decrease performance, even though the align-
ment is more constrained in the latter case. Us-
ing the same sub-measure to compare words (tri-
grams CosTFIDF), it does neither increase perfor-
mance. A few positive pairs are missed in the INT
corpus, due to the more flexible word order in En-
glish: ?U.S. State Department?/?US Department
of State? is such an example (12 among 764 are
concerned). This problem is easily solved with the
ML variant with extended transposition (see part
3.3): in both corpora, there are no positive pairs
requiring more than a gap of one word in the align-
ment. Thus this measure is not only performant but
also robust, since it does not need any hand-tuning.
As a second step, we want to improve results
by selecting a more fine-grained sub-measure. We
have tried several ideas, such as using different
kinds of n-grams similarity inside the words sim-
ilarity measure. Firstly, trigrams performed bet-
ter than bigrams or simple characters. Secondly,
the best trigrams similarity method found is actu-
ally very simple: it consists in using CosTFIDF
computed on the trigrams contexts, i.e. the set of
closest27 trigrams of all occurrences of the given
trigram. Unsurprisingly, good scores are generally
obtained for pairs of trigrams that have common
characters. But it seems that this approach also
enhances robustness, because it finds similarities
between ?close characters?: in the French corpus,
one observes quite good scores between trigrams
containing an accentuated version and the non ac-
centuated version of the same character. Further-
more, some character encoding errors are some-
how corrected this way28. This is possibly the rea-
son why the improvement of results is better in
FSM than in INT (see table 1).
Finally, using also ML to compute similarity
between words29 yields the best results. This
means that compared to the simple CosTFIDF sub-
measure, one does not compare bags of trigrams
but ordered sequences of trigrams30.
27We have tried different window sizes for such contexts,
from 2 to 10 trigrams long: performances were approximately
the same. We only consider trigrams found in the entities.
28For example, the ?? in the name ?Lugovo??? appears also in
FSM as i, as y, as a`, and is sometimes deleted.
29i.e. not only between sequences of words: in this case
ML is run between trigrams at the word level, and then an-
other time between words at the NE level.
30It is hard to tell whether it is the sequential alignment or
599
Figure 3: F1-Measures for FSM (percentages)
 0
 20
 40
 60
 80
 100
 0  500  1000  1500  2000  2500
F1
-M
ea
su
re
n best scored pairs (considered as positive)
Bag of words Cosine
Cosine TFIDF (words)
Soft-TFIDF (Jaro)
Soft-TFIDF (TFIDF 3g)
ML (ML/contexts 3g)
Example: for Cosine TFIDF with words, if the threshold is
set in such a way that (only) the 1000 top ranked pairs are
classified as positive, then the F1-measure is around 60%.
Table 1: Best F1-measures (percentages)
INT FSM
Measure F1 P R F1 P R
Cosine 51.6 63.2 43.6 59.5 76.2 48.7
CosTFIDF 62.6 71.7 55.6 69.9 84.2 59.8
Soft TFIDF/3g 68.6 74.2 63.9 73.1 79.8 67.6
ML/ML-context 70.6 72.6 68.7 77.0 82.5 72.2
P/R: Corresponding Precision/Recall.
4.3 Global results
Results are synthesized in table 1, which is based
on the maximum F1-measure for each measure.
One observes that F1-measure is 3 to 6 points bet-
ter for Soft-TFIDF than for standard TF-IDF, and
that our measure still increases F1-measure by 2
(INT) to 4 points (FSM). Results show that its
contribution consists mainly in improving the re-
call, which means that our measure is able to catch
more positive pairs than Soft-TFIDF: for exam-
ple, the pair ?Fatah Al Islam?/ ?Fateh el-Islam?
(FSM) is scored 0.54 by SoftTFIDF and 0.70 by
ML. Our measure remains the best for all values of
n in fig. 3, and results are similar for F0.5-measure
and F2-measure: thus, irrespective of specific ap-
plication needs which may favor precision or re-
call, ML seems preferable.
5 Conclusion
In conclusion, we have proposed a generic model
to show that similarity measures may be combined
in numerous ways. We have tested such a combi-
nation, based on Soft-TFIDF, which performs bet-
the ?right? use of the trigrams sub-measure which is responsi-
ble for the improvement, since the only possible comparison
at this level is Soft-TFIDF.
ter than all existing similarity metrics on two cor-
pora. Our measure is robust, since it does not rely
on any kind of prior knowledge. Thus it may be
easily used, in particular in applications where NE
matching is useful but is not the essential task.
Acknowledgements
This work has been funded by the National Project
Cap Digital - Infom@gic. We thank Lo??s Rigouste
(Pertimm) and Nicolas Dessaigne and Aure?lie Mi-
geotte (Arisem) for providing us with the anno-
tated French corpus.
References
Bilenko, Mikhail, Raymond J. Mooney, William W.
Cohen, Pradeep Ravikumar, and Stephen E. Fien-
berg. 2003. Adaptive name matching in information
integration. IEEE Intelligent Systems, 18(5):16?23.
Christen, Peter. 2006. A comparison of personal name
matching: Techniques and practical issues. Techni-
cal Report TR-CS-06-02, Department of Computer
Science, The Australian National University, Can-
berra 0200 ACT, Australia, September.
Cohen, William W., Pradeep Ravikumar, and
Stephen E. Fienberg. 2003. A comparison of
string distance metrics for name-matching tasks. In
Kambhampati, Subbarao and Craig A. Knoblock,
editors, Proceedings of IJCAI-03 Workshop on
Information Integration on the Web (IIWeb-03),
August 9-10, 2003, Acapulco, Mexico, pages 73?78.
Freeman, Andrew, Sherri L. Condon, and Christopher
Ackerman. 2006. Cross linguistic name matching
in English and Arabic. In Moore, Robert C., Jeff A.
Bilmes, Jennifer Chu-Carroll, and Mark Sanderson,
editors, Proc. HLT-NAACL.
Navarro, Gonzalo. 2001. A guided tour to approximate
string matching. ACM Comput. Surv., 33(1):31?88.
Pouliquen, Bruno, Ralf Steinberger, Camelia Ignat,
Irina Temnikova, Anna Widiger, Wajdi Zaghouani,
and Jan Zizka. 2006. Multilingual person name
recognition and transliteration. CORELA - Cogni-
tion, Representation, Langage.
Prager, John, Sarah Luger, and Jennifer Chu-Carroll.
2007. Type nanotheories: a framework for term
comparison. In Proceedings of CIKM ?07, pages
701?710, New York, NY, USA. ACM.
Wagner, R. and M. Fischer. 1974. The string-to-string
correction problem. JACM, 21(1):168?173.
Winkler, W. E. 1999. The state of record linkage
and current research problems. Technical Report
RR99/04, US Bureau of the Census.
600
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 504?513,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Practical very large scale CRFs
Thomas Lavergne
LIMSI ? CNRS
lavergne@limsi.fr
Olivier Cappe?
Te?le?com ParisTech
LTCI ? CNRS
cappe@enst.fr
Franc?ois Yvon
Universite? Paris-Sud 11
LIMSI ? CNRS
yvon@limsi.fr
Abstract
Conditional Random Fields (CRFs) are
a widely-used approach for supervised
sequence labelling, notably due to their
ability to handle large description spaces
and to integrate structural dependency be-
tween labels. Even for the simple linear-
chain model, taking structure into account
implies a number of parameters and a
computational effort that grows quadrati-
cally with the cardinality of the label set.
In this paper, we address the issue of train-
ing very large CRFs, containing up to hun-
dreds output labels and several billion fea-
tures. Efficiency stems here from the spar-
sity induced by the use of a `1 penalty
term. Based on our own implementa-
tion, we compare three recent proposals
for implementing this regularization strat-
egy. Our experiments demonstrate that
very large CRFs can be trained efficiently
and that very large models are able to im-
prove the accuracy, while delivering com-
pact parameter sets.
1 Introduction
Conditional Random Fields (CRFs) (Lafferty et
al., 2001; Sutton and McCallum, 2006) constitute
a widely-used and effective approach for super-
vised structure learning tasks involving the map-
ping between complex objects such as strings and
trees. An important property of CRFs is their abil-
ity to handle large and redundant feature sets and
to integrate structural dependency between out-
put labels. However, even for simple linear chain
CRFs, the complexity of learning and inference
This work was partly supported by ANR projects CroTaL
(ANR-07-MDCO-003) and MGA (ANR-07-BLAN-0311-
02).
grows quadratically with respect to the number of
output labels and so does the number of structural
features, ie. features testing adjacent pairs of la-
bels. Most empirical studies on CRFs thus ei-
ther consider tasks with a restricted output space
(typically in the order of few dozens of output la-
bels), heuristically reduce the use of features, es-
pecially of features that test pairs of adjacent la-
bels1, and/or propose heuristics to simulate con-
textual dependencies, via extended tests on the ob-
servations (see discussions in, eg., (Punyakanok
et al, 2005; Liang et al, 2008)). Limitating the
feature set or the number of output labels is how-
ever frustrating for many NLP tasks, where the
type and number of potentially relevant features
are very large. A number of studies have tried to
alleviate this problem. Pal et al (2006) propose
to use a ?sparse? version of the forward-backward
algorithm during training, where sparsity is en-
forced through beam pruning. Related ideas are
discussed by Dietterich et al (2004); by Cohn
(2006), who considers ?generalized? feature func-
tions; and by Jeong et al (2009), who use approx-
imations to simplify the forward-backward recur-
sions. In this paper, we show that the sparsity that
is induced by `1-penalized estimation of CRFs can
be used to reduce the total training time, while
yielding extremely compact models. The benefits
of sparsity are even greater during inference: less
features need to be extracted and included in the
potential functions, speeding up decoding with a
lesser memory footprint. We study and compare
three different ways to implement `1 penalty for
CRFs that have been introduced recently: orthant-
wise Quasi Newton (Andrew and Gao, 2007),
stochastic gradient descent (Tsuruoka et al, 2009)
and coordinate descent (Sokolovska et al, 2010),
concluding that these methods have complemen-
1In CRFsuite (Okazaki, 2007), it is even impossible to
jointly test a pair of labels and a test on the observation, bi-
grams feature are only of the form f(yt?1, yt).
504
tary strengths and weaknesses. Based on an effi-
cient implementation of these algorithms, we were
able to train very large CRFs containing more than
a hundred of output labels and up to several billion
features, yielding results that are as good or better
than the best reported results for two NLP bench-
marks, text phonetization and part-of-speech tag-
ging.
Our contribution is therefore twofold: firstly a
detailed analysis of these three algorithms, dis-
cussing implementation, convergence and com-
paring the effect of various speed-ups. This
comparison is made fair and reliable thanks to
the reimplementation of these techniques in the
same software package. Second, the experimen-
tal demonstration that using large output label sets
is doable and that very large feature sets actually
help improve prediction accuracy. In addition, we
show how sparsity in structured feature sets can
be used in incremental training regimes, where
long-range features are progressively incorporated
in the model insofar as the shorter range features
have proven useful.
The rest of the paper is organized as follows: we
first recall the basics of CRFs in Section 2, and dis-
cuss three ways to train CRFs with a `1 penalty in
Section 3. We then detail several implementation
issues that need to be addressed when dealing with
massive feature sets in Section 4. Our experiments
are reported in Section 5. The main conclusions of
this study are drawn in Section 6.
2 Conditional Random Fields
In this section, we recall the basics of Conditional
Random Fields (CRFs) (Lafferty et al, 2001; Sut-
ton and McCallum, 2006) and introduce the nota-
tions that will be used throughout.
2.1 Basics
CRFs are based on the following model
p?(y|x) =
1
Z?(x)
exp
{
K?
k=1
?kFk(x,y)
}
(1)
where x = (x1, . . . , xT ) and y = (y1, . . . , yT )
are, respectively, the input and output sequences2,
and Fk(x,y) is equal to
?T
t=1 fk(yt?1, yt, xt),
where {fk}1?k?K is an arbitrary set of feature
2Our implementation also includes a special label y0, that
is always observed and marks the beginning of a sequence.
functions and {?k}1?k?K are the associated pa-
rameter values. We denote by Y and X , respec-
tively, the sets in which yt and xt take their values.
The normalization factor in (1) is defined by
Z?(x) =
?
y?Y T
exp
{
K?
k=1
?kFk(x,y)
}
. (2)
The most common choice of feature functions is to
use binary tests. In the sequel, we distinguish be-
tween two types of feature functions: unigram fea-
tures fy,x, associated with parameters ?y,x, and bi-
gram features fy?,y,x, associated with parameters
?y?,y,x. These are defined as
fy,x(yt?1, yt, xt) = 1(yt = y, xt = x)
fy?,y,x(yt?1, yt, xt) = 1(yt?1 = y
?, yt = y, xt = x)
where 1(cond.) is equal to 1 when the condition
is verified and to 0 otherwise. In this setting, the
number of parametersK is equal to |Y |2?|X|train,
where | ? | denotes the cardinal and |X|train refers to
the number of configurations of xt observed dur-
ing training. Thus, even in moderate size applica-
tions, the number of parameters can be very large,
mostly due to the introduction of sequential de-
pendencies in the model. This also explains why it
is hard to train CRFs with dependencies spanning
more than two adjacent labels. Using only uni-
gram features {fy,x}(y,x)?Y?X results in a model
equivalent to a simple bag-of-tokens position-
by-position logistic regression model. On the
other hand, bigram features {fy?,y,x}(y,x)?Y 2?X
are helpful in modelling dependencies between
successive labels. The motivations for using si-
multaneously both types of feature functions are
evaluated experimentally in Section 5.
2.2 Parameter Estimation
Given N independent sequences {x(i),y(i)}Ni=1,
where x(i) and y(i) contain T (i) symbols, condi-
tional maximum likelihood estimation is based on
the minimization, with respect to ?, of the negated
conditional log-likelihood of the observations
l(?) = ?
N?
i=1
log p?(y
(i)|x(i)) (3)
=
N?
i=1
{
logZ?(x
(i))?
K?
k=1
?kFk(x
(i),y(i))
}
This term is usually complemented with an addi-
tional regularization term so as to avoid overfitting
505
(see Section 3.1 below). The gradient of l(?) is
?l(?)
??k
=
N?
i=1
T (i)?
t=1
Ep?(y|x(i)) fk(yt?1, yt, x
(i)
t )
?
N?
i=1
T (i)?
t=1
fk(y
(i)
t?1, y
(i)
t , x
(i)
t ) (4)
where Ep?(y|x) denotes the conditional expecta-
tion given the observation sequence, i.e.
Ep?(y|x) fk(yt?1, yt, x
(i)
t ) =
?
(y?,y)?Y 2
fk(y, y
?, xt) P?(yt?1 = y
?, yt = y|x) (5)
Although l(?) is a smooth convex function, its op-
timum cannot be computed in closed form, and
l(?) has to be optimized numerically. The com-
putation of its gradient implies to repeatedly com-
pute the conditional expectation in (5) for all in-
put sequences x(i) and all positions t. The stan-
dard approach for computing these expectations
is inspired by the forward-backward algorithm for
hidden Markov models: using the notations intro-
duced above, the algorithm implies the computa-
tion of the forward
{
?1(y) = exp(?y,x1 + ?y0,y,x1)
?t+1(y) =
?
y? ?t(y
?) exp(?y,xt+1 + ?y?,y,xt+1)
and backward recursions
{
?Ti(y) = 1
?t(y?) =
?
y ?t+1(y) exp(?y,xt+1 + ?y?,y,xt+1),
for all indices 1 ? t ? T and all labels y ? Y .
Then, Z?(x) =
?
y ?T (y) and the pairwise prob-
abilities P?(yt = y?, yt+1 = y|x) are given by
?t(y
?) exp(?y,xt+1 + ?y?,y,xt+1)?t+1(y)/Z?(x)
These recursions require a number of operations
that grows quadratically with |Y |.
3 `1 Regularization in CRFs
3.1 Regularization
The standard approach for parameter estimation in
CRFs consists in minimizing the logarithmic loss
l(?) defined by (3) with an additional `2 penalty
term ?22 ???
2
2, where ?2 is a regularization parame-
ter. The objective function is then a smooth convex
function to be minimized over an unconstrained
parameter space. Hence, any numerical optimiza-
tion strategy may be used and practical solutions
include limited memory BFGS (L-BFGS) (Liu
and Nocedal, 1989), which is used in the popu-
lar CRF++ (Kudo, 2005) and CRFsuite (Okazaki,
2007) packages; conjugate gradient (Nocedal and
Wright, 2006) and Stochastic Gradient Descent
(SGD) (Bottou, 2004; Vishwanathan et al, 2006),
used in CRFsgd (Bottou, 2007). The only caveat
is to avoid numerical optimizers that require the
full Hessian matrix (e.g., Newton?s algorithm) due
to the size of the parameter vector in usual appli-
cations of CRFs.
The most significant alternative to `2 regulariza-
tion is to use a `1 penalty term ?1???1: such regu-
larizers are able to yield sparse parameter vectors
in which many component have been zeroed (Tib-
shirani, 1996). Using a `1 penalty term thus im-
plicitly performs feature selection, where ?1 con-
trols the amount of regularization and the number
of extracted features. In the following, we will
jointly use both penalty terms, yielding the so-
called elastic net penalty (Zhou and Hastie, 2005)
which corresponds to the objective function
l(?) + ?1???1 +
?2
2
???22 (6)
The use of both penalty terms makes it possible
to control the number of non zero coefficients and
to avoid the numerical problems that might occur
in large dimensional parameter settings (see also
(Chen, 2009)). However, the introduction of a `1
penalty term makes the optimization of (6) more
problematic, as the objective function is no longer
differentiable in 0. Various strategies have been
proposed to handle this difficulty. We will only
consider here exact approaches and will not dis-
cuss heuristic strategies such as grafting (Perkins
et al, 2003; Riezler and Vasserman, 2004).
3.2 Quasi Newton Methods
To deal with `1 penalties, a simple idea is that of
(Kazama and Tsujii, 2003), originally introduced
for maxent models. It amounts to reparameteriz-
ing ?k as ?k = ?
+
k ??
?
k , where ?
+
k and ?
?
k are pos-
itive. The `1 penalty thus becomes ?1(?+ ? ??).
In this formulation, the objective function recovers
its smoothness and can be optimized with conven-
tional algorithms, subject to domain constraints.
Optimization is straightforward, but the number
of parameters is doubled and convergence is slow
506
(Andrew and Gao, 2007): the procedure lacks a
mechanism for zeroing out useless parameters.
A more efficient strategy is the orthant-wise
quasi-Newton (OWL-QN) algorithm introduced in
(Andrew and Gao, 2007). The method is based on
the observation that the `1 norm is differentiable
when restricted to a set of points in which each
coordinate never changes its sign (an ?orthant?),
and that its second derivative is then zero, mean-
ing that the `1 penalty does not change the Hessian
of the objective on each orthant. An OWL-QN
update then simply consists in (i) computing the
Newton update in a well-chosen orthant; (ii) per-
forming the update, which might cause some com-
ponent of the parameter vector to change sign; and
(iii) projecting back the parameter value onto the
initial orthant, thereby zeroing out those compo-
nents. In (Gao et al, 2007), the authors show that
OWL-QN is faster than the algorithm proposed by
Kazama and Tsujii (2003) and can perform model
selection even in very high-dimensional problems,
with no loss of performance compared to the use
of `2 penalty terms.
3.3 Stochastic Gradient Descent
Stochastic gradient (SGD) approaches update the
parameter vector based on an crude approximation
of the gradient (4), where the computation of ex-
pectations only includes a small batch of observa-
tions. SGD updates have the following form
?k ? ?k + ?
?l(?)
??k
, (7)
where ? is the learning rate. In (Tsuruoka et al,
2009), various ways of adapting this update to `1-
penalized likelihood functions are discussed. Two
effective ideas are proposed: (i) only update pa-
rameters that correspond to active features in the
current observation, (ii) keep track of the cumu-
lated penalty zk that ?k should have received, had
the gradient been computed exactly, and use this
value to ?clip? the parameter value. This is imple-
mented by patching the update (7) as follows
{
if (?k > 0) ?k ? max(0, ?k ? zk)
else if (?k < 0) ?k ? min(0, ?k ? zk)
(8)
Based on a study of three NLP benchmarks, the
authors of (Tsuruoka et al, 2009) claim this ap-
proach to be much faster than the orthant-wise ap-
proach and yet to yield very comparable perfor-
mance, while selecting slightly larger feature sets.
3.4 Block Coordinate Descent
The coordinate descent approach of Dud??k et
al. (2004) and Friedman et al (2008) uses the
fact that optimizing a mono-dimensional quadratic
function augmented with a `1 penalty can be per-
formed analytically. For arbitrary functions, this
idea can be adapted by considering quadratic ap-
proximations of the objective around the current
value ??
lk,??(?k) =
?l(??)
??k
(?k ? ??k) +
1
2
?2l(??)
??2k
(?k ? ??k)
2
+ ?1|?k|+
?2
2
?2k + C
st (9)
The minimizer of the approximation (9) is simply
?k =
s
{
?2l(??)
??2k
??k ?
?l(??)
??k
, ?1
}
?2l(??)
??2k
+ ?2
(10)
where s is the soft-thresholding function
s(z, ?) =
?
??
??
z ? ? if z > ?
z + ? if z < ??
0 otherwise
(11)
Coordinate descent is ported to CRFs in
(Sokolovska et al, 2010). Making this scheme
practical requires a number of adaptations,
including (i) approximating the second order
term in (10), (ii) performing updates in block,
where a block contains the |Y | ? |Y + 1| fea-
tures ?y?,y,x and ?y,x for a fixed test x on the
observation sequence and (iii) approximating the
Hessian for a block by its diagonal terms. (ii)
is specially critical, as repeatedly cycling over
individual features to perform the update (10)
is only possible with restricted sets of features.
The block update schemes uses the fact that
all features within a block appear in the same
set of sequences, which means that most of the
computations needed to perform theses updates
can be shared within the block. One advantage
of the resulting algorithm, termed BCD in the
following, is that the update of ?k only involves
carrying out the forward-backward recursions for
the set of sequences that contain symbols x such
that at least one {fk(y?, y, x)}(y,y?)?Y 2 is non
null, which can be much smaller than the whole
training set.
507
4 Implementation Issues
Efficiently processing very-large feature and ob-
servation sets requires to pay attention to many
implementation details. In this section, we present
several optimizations devised to speed up training.
4.1 Sparse Forward-Backward Recursions
For all algorithms, the computation time is domi-
nated by the evaluations of the gradient: our im-
plementation takes advantage of the sparsity to ac-
celerate these computations. Assume the set of bi-
gram features {?y?,y,xt+1}(y?,y)?Y 2 is sparse with
only r(xt+1)  |Y |2 non null values and define
the |Y | ? |Y | sparse matrix
Mt(y
?, y) = exp(?y?,y,xt)? 1.
Using M , the forward-backward recursions are
?t(y) =
?
y?
ut?1(y
?) +
?
y?
ut?1(y
?)Mt(y
?, y)
?t(y
?) =
?
y
vt+1(y) +
?
y
Mt+1(y
?, y)vt+1(y)
with ut?1(y) = exp(?y,xt)?t?1(y) and
vt+1(y) = exp(?y,xt+1)?t+1(y). (Sokolovska et
al., 2010) explains how computational savings can
be obtained using the fact that the vector/matrix
products in the recursions above only involve
the sparse matrix Mt+1(y?, y). They can thus be
computed with exactly r(xt+1) multiplications
instead of |Y |2. The same idea can be used
when the set {?y,xt+1}y?Y of unigram features is
sparse. Using this implementation, the complexity
of the forward-backward procedure for x(i) can be
made proportional to the average number of active
features per position, which can be much smaller
than the number of potentially active features.
For BCD, forward-backward can even be made
slightly faster. When computing the gradient wrt.
features ?y,x and ?y?,y,x (for all the values of y
and y?) for sequence x(i), assuming that x only
occurs once in x(i) at position t, all that is needed
is ??t(y), ?t
? ? t and ??t(y),?t
? ? t. Z?(x) is then
recovered as
?
y ?t(y)?t(y). Forward-backward
recursions can thus be truncated: in our experi-
ments, this divided the computational cost by 1,8
on average.
Note finally that forward-backward is per-
formed on a per-observation basis and is easily
parallelized (see also (Mann et al, 2009) for more
powerful ways to distribute the computation when
dealing with very large datasets). In our imple-
mentation, it is distributed on all available cores,
resulting in significant speed-ups for OWL-QN
and L-BFGS; for BCD the gain is less acute, as
parallelization only helps when updating the pa-
rameters for a block of features that are occur in
many sequences; for SGD, with batches of size
one, this parallelization policy is useless.
4.2 Scaling
Most existing implementations of CRFs, eg.
CRF++ and CRFsgd perform the forward-
backward recursions in the log-domain, which
guarantees that numerical over/underflows are
avoided no matter the length T (i) of the sequence.
It is however very inefficient from an implementa-
tion point of view, due to the repeated calls to the
exp() and log() functions. As an alternative way
of avoiding numerical problems, our implementa-
tion, like crfSuite?s, resorts to ?scaling?, a solution
commonly used for HMMs. Scaling amounts to
normalizing the values of ?t and ?t to one, making
sure to keep track of the cumulated normalization
factors so as to compute Z?(x) and the conditional
expectations Ep?(y|x). Also note that in our imple-
mentation, all the computations of exp(x) are vec-
torized, which provides an additional speed up of
about 20%.
4.3 Optimization in Large Parameter Spaces
Processing very large feature vectors, up to bil-
lions of components, is problematic in many ways.
Sparsity has been used here to speed up forward-
backward, but we have made no attempt to accel-
erate the computation of the OWL-QN updates,
which are linear in the size of the parameter vector.
Of the three algorithms, BCD is the most affected
by increases in the number of features, or more
precisely, in the number of features blocks, where
one block correspond to a specific test of the ob-
servation. In the worst case scenario, each block
may require to visit all the training instances,
yielding terrible computational wastes. In prac-
tice though, most blocks only require to process
a small fraction of the training set, and the ac-
tual complexity depends on the average number of
blocks per observations. Various strategies have
been tried to further accelerate BCD, such as pro-
cessing blocks that only visit one observation in
parallel and updating simultaneously all the blocks
that visit all the training instances, leading to a
small speed-up on the POS-tagging task.
508
Working with billions of features finally re-
quires to worry also about memory usage. In this
respect, BCD is the most efficient, as it only re-
quires to store one K-dimensional vector for the
parameter itself. SGD requires two such vectors,
one for the parameter and one for storing the zk
(see Eq. (8)). In comparison, OWL-QN requires
much more memory, due to the internals of the
update routines, which require several histories of
the parameter vector and of its gradient. Typi-
cally, our implementation necessitates in the order
of a dozen K-dimensional vectors. Parallelization
only makes things worse, as each core will also
need to maintain its own copy of the gradient.
5 Experiments
Our experiments use two standard NLP tasks,
phonetization and part-of-speech tagging, chosen
here to illustrate two very different situations, and
to allow for comparison with results reported else-
where in the literature. Unless otherwise men-
tioned, the experiments use the same protocol: 10
fold cross validation, where eight folds are used
for training, one for development, and one for test-
ing. Results are reported in terms of phoneme er-
ror rates or tag error rates on the test set.
Comparing run-times can be a tricky matter, es-
pecially when different software packages are in-
volved. As discussed above, the observed run-
times depend on many small implementation de-
tails. As the three algorithms share as much code
as possible, we believe the comparison reported
hereafter to be fair and reliable. All experiments
were performed on a server with 64G of memory
and two Xeon processors with 4 cores at 2.27 Ghz.
For comparison, all measures of run-times include
the cumulated activity of all cores and give very
pessimistic estimates of the wall time, which can
be up to 7 times smaller. For OWL-QN, we use 5
past values of the gradient to approximate the in-
verse of the Hessian matrix: increasing this value
had no effect on accuracy or convergence and was
detrimental to speed; for SGD, the learning rate
parameter was tuned manually.
Note that we have not spent much time optimiz-
ing the values of ?1 and ?2. Based on a pilot study
on Nettalk, we found that taking ?1 = .5 and ?2 in
the order of 10?5 to yield nearly optimal perfor-
mance, and have used these values throughout.
5.1 Tasks and Settings
5.1.1 Nettalk
Our first benchmark is the word phonetization
task, using the Nettalk dictionary (Sejnowski and
Rosenberg, 1987). This dataset contains approxi-
mately 20,000 English word forms, their pronun-
ciation, plus some prosodic information (stress
markers for vowels, syllabic parsing for con-
sonants). Grapheme and phoneme strings are
aligned at the character level, thanks to the use of
a ?null sound? in the latter string when it is shorter
than the former; likewise, each prosodic mark is
aligned with the corresponding letter. We have de-
rived two test conditions from this database. The
first one is standard and aims at predicting the pro-
nunciation information only. In this setting, the set
of observations (X) contains 26 graphemes, and
the output label set contains |Y | = 51 phonemes.
The second condition aims at jointly predict-
ing phonemic and prosodic information3. The rea-
sons for designing this new condition are twofold:
firstly, it yields a large set of composite labels
(|Y | = 114) and makes the problem computation-
ally challenging. Second, it allows to quantify how
much the information provided by the prosodic
marks help predict the phonemic labels. Both in-
formation are quite correlated, as the stress mark
and the syllable openness, for instance, greatly in-
fluence the realization of some archi-phonemes.
The features used in Nettalk experiments take
the form fy,w (unigram) and fy?,y,w (bigram),
where w is a n-gram of letters. The n-grm feature
sets (n = {1, 3, 5, 7}) includes all features testing
embedded windows of k letters, for all 0 ? k ? n;
the n-grm- setting is similar, but only includes
the window of length n; in the n-grm+ setting,
we add features for odd-size windows; in the n-
grm++ setting, we add all sequences of letters up
to size n occurring in current window. For in-
stance, the active bigram features at position t = 2
in the sequence x=?lemma? are as follows: the 3-
grm feature set contains fy,y? , fy,y?,e and fy?,y,lem;
only the latter appears in the 3-grm- setting. In
the 3-grm+ feature set, we also have fy?,y,le and
fy?,y,em. The 3-grm++ feature set additionally in-
cludes fy?,y,l and fy?,y,m. The number of features
ranges from 360 thousands (1-grm setting) to 1.6
billion (7-grm).
3Given the design of the Nettalk dictionary, this experi-
ment required to modify the original database so as to reas-
sign prosodic marks to phonemes, rather than to letters.
509
Features With Without
Nettalk
3-grm 10.74% 14.3M 14.59% 0.3M
5-grm 8.48% 132.5M 11.54% 2.5M
POS tagging
base 2.91% 436.7M 3.47% 70.2M
Table 1: Features jointly testing label pairs and
the observation are useful (error rates and features
counts.)
`2 `1-sparse `1 % zero
1-grm 84min 41min 57min 44.6%
3-grm- 65min 16min 44min 99.6%
3-grm 72min 48min 58min 19.9%
Table 2: Sparse vs standard forward-backward
(training times and percentages of sparsity of M )
5.1.2 Part-of-Speech Tagging
Our second benchmark is a part-of-speech (POS)
tagging task using the PennTreeBank corpus
(Marcus et al, 1993), which provides us with a
quite different condition. For this task, the number
of labels is smaller (|Y | = 45) than for Nettalk,
and the set of observations is much larger (|X| =
43207). This benchmark, which has been used in
many studies, allows for direct comparisons with
other published work. We thus use a standard ex-
perimental set-up, where sections 0-18 of the Wall
Street Journal are used for training, sections 19-21
for development, and sections 22-24 for testing.
Features are also standard and follow the design
of (Suzuki and Isozaki, 2008) and test the current
words (as written and lowercased), prefixes and
suffixes up to length 4, and typographical charac-
teristics (case, etc.) of the words. Our baseline
feature set alo contains tests on individual and
pairs of words in a window of 5 words.
5.2 Using Large Feature Sets
The first important issue is to assess the benefits
of using large feature sets, notably including fea-
tures testing both a bigram of labels and an obser-
vation. Table 1 compares the results obtained with
and without these features for various setting (us-
ing OWL-QN to perform the optimization), sug-
gesting that for the tasks at hand, these features
are actually helping.
`2 `1 Elastic-net
1-grm 17.81% 17.86% 17.79%
3-grm 10.62% 10.74% 10.70%
5-grm 8.50% 8.45% 8.48%
Table 3: Error rates of the three regularizers on the
Nettalk task.
5.3 Speed, Sparsity, Convergence
The training speed depends of two main factors:
the number of iterations needed to achieve conver-
gence and the computational cost of one iteration.
In this section, we analyze and compare the run-
time efficiency of the three optimizers.
5.3.1 Convergence
As far as convergence is concerned, the two forms
of regularization (`2 and `1) yield the same per-
formance (see Table 3), and the three algorithms
exhibit more or less the same behavior. They
quickly reach an acceptable set of active param-
eters, which is often several orders of magnitude
smaller than the whole parameter set (see results
below in Table 4 and 5). Full convergence, re-
flected by a stabilization of the objective function,
is however not so easily achieved. We have of-
ten observed a slow, yet steady, decrease of the
log-loss, accompanied with a diminution of the
number of active features as the number of iter-
ations increases. Based on this observation, we
have chosen to stop all algorithms based on their
performance on an independent development set,
allowing a fair comparison of the overall training
time; for OWL-QN, it allowed to divide the total
training time by almost 2.
It has finally often been found useful to fine
tune the non-zero parameters by running a final
handful of L-BFGS iterations using only a small
`2 penalty; at this stage, all the other features are
removed from the model. This had a small impact
BCD and SGD?s performance and allowed them to
catch up with OWL-QN?s performance.
5.3.2 Sparsity and the Forward-Backward
As explained in section 4.1, the forward-backward
algorithm can be written so as to use the sparsity
of the matrix My,y?,x. To evaluate the resulting
speed-up, we ran a series of experiments using
Nettalk (see Table 2). In this table, the 3-grm- set-
ting corresponds to maximum sparsity for M , and
training with the sparse algorithm is three times
faster than with the non-sparse version. Throwing
510
Method Iter. # Feat. Error Time
O
W
L
-Q
N 1-grm 63.4 4684 17.79% 11min
7-grm 140.2 38214 8.12% 1h02min
5-grm+ 141.0 43429 7.89% 1h37min
S
G
D 1-grm 21.4 3540 18.21% 9min
5-grm+ 28.5 34319 8.01% 45min
B
C
D
1-grm 28.2 5017 18.27% 27min
7-grm 9.2 3692 8.21% 1h22min
5-grm+ 8.7 47675 7.91% 2h18min
Table 4: Performance on Nettalk
in more features has the effect of making M much
more dense, mitigating the benefits of the sparse
recursions. Nevertheless, even for very large fea-
ture sets, the percentage of zeros in M averages
20% to 30%, and the sparse version remains 10 to
20% faster than the non-sparse one. Note that the
non-sparse version is faster with a `1 penalty term
than with only the `2 term: this is because exp(0)
is faster to evaluate than exp(x) when x 6= 0.
5.3.3 Training Speed and Test Accuracy
Table 4 displays the results achieved on the Nettalk
task. The three algorithms yield very compara-
ble accuracy results, and deliver compact models:
for the 5-gram+ setting, only 50,000 out of 250
million features are selected. SGD is the fastest
of the three, up to twice as fast as OWL-QN and
BCD depending on the feature set. The perfor-
mance it achieves are consistently slightly worst
than the other optimizers, and only catch up when
the parameters are fine-tuned (see above). There
are not so many comparisons for Nettalk with
CRFs, due to the size of the label set. Our results
compare favorably with those reported in (Pal et
al., 2006), where the accuracy attains 91.7% us-
ing 19075 examples for training and 934 for test-
ing, and with those in (Jeong et al, 2009) (88.4%
accuracy with 18,000 (2,000) training (test) in-
stances). Table 5 gives the results obtained for
the larger Nettalk+prosody task. Here, we only
report the results obtained with SGD and BCD.
For OWL-QN, the largest model we could han-
dle was the 3-grm model, which contained 69 mil-
lion features, and took 48min to train. Here again,
performance steadily increase with the number of
features, showing the benefits of large-scale mod-
els. We lack comparisons for this task, which
seems considerably harder than the sole phone-
tization task, and all systems seem to plateau
around 13.5% accuracy. Interestingly, simulta-
Method Error Time
S
G
D 5-grm 14.71% / 8.11% 55min
5-grm+ 13.91% / 7.51% 2h45min
B
C
D
5-grm 14.57% / 8.06% 2h46min
7-grm 14.12% / 7.86% 3h02min
5-grm+ 13.85% / 7.47% 7h14min
5-grm++ 13.69% / 7.36% 16h03min
Table 5: Performance on Nettalk+prosody. Error
is given for both joint labels and phonemic labels.
neously predicting the phoneme and its prosodic
markers allows to improve the accuracy on the pre-
diction of phonemes, which improves of almost a
half point as compared to the best Nettalk system.
For the POS tagging task, BCD appears to be
unpractically slower to train than the others ap-
proaches (SGD takes about 40min to train, OWL-
QN about 1 hour) due the simultaneous increase
in the sequence length and in the number of ob-
servations. As a result, one iteration of BCD typi-
cally requires to repeatedly process over and over
the same sequences: on average, each sequence is
visited 380 times when we use the baseline fea-
ture set. This technique should reserved for tasks
where the number of blocks is small, or, as below,
when memory usage is an issue.
5.4 Structured Feature Sets
In many tasks, the ambiguity of tokens can be re-
duced by looking up increasingly large windows
of local context. This strategy however quickly
runs into a combinatorial increase of the number
of features. A side note of the Nettalk experiments
is that when using embedded features, the active
feature set tends to reflect this hierarchical organi-
zation. This means that when a feature testing a
n-gram is active, in most cases, the features for all
embedded k-grams are also selected.
Based on this observation, we have designed
an incremental training strategy for the POS tag-
ging task, where more specific features are pro-
gressively incorporated into the model if the cor-
responding less specific feature is active. This ex-
periment used BCD, which is the most memory ef-
ficient algorithm. The first iteration only includes
tests on the current word. During the second it-
eration, we add tests on bigram of words, on suf-
fixes and prefixes up to length 4. After four itera-
tions, we throw in features testing word trigrams,
subject to the corresponding unigram block being
active. After 6 iterations, we finally augment the
511
model with windows of length 5, subject to the
corresponding trigram being active. After 10 iter-
ations, the model contains about 4 billion features,
out of which 400,000 are active. It achieves an
error rate of 2.63% (resp. 2.78%) on the develop-
ment (resp. test) data, which compares favorably
with some of the best results for this task (for in-
stance (Toutanova et al, 2003; Shen et al, 2007;
Suzuki and Isozaki, 2008)).
6 Conclusion and Perspectives
In this paper, we have discussed various ways to
train extremely large CRFs with a `1 penalty term
and compared experimentally the results obtained,
both in terms of training speed and of accuracy.
The algorithms studied in this paper have com-
plementary strength and weaknesses: OWL-QN is
probably the method of choice in small or moder-
ate size applications while BCD is most efficient
when using very large feature sets combined with
limited-size observation alphabets; SGD comple-
mented with fine tuning appears to be the preferred
choice in most large-scale applications. Our anal-
ysis demonstrate that training large-scale sparse
models can be done efficiently and allows to im-
prove over the performance of smaller models.
The CRF package developed in the course of this
study implements many algorithmic optimizations
and allows to design innovative training strategies,
such as the one presented in section 5.4. This
package is released as open-source software and
is available at http://wapiti.limsi.fr.
In the future, we intend to study how spar-
sity can be used to speed-up training in the face
of more complex dependency patterns (such as
higher-order CRFs or hierarchical dependency
structures (Rozenknop, 2002; Finkel et al, 2008).
From a performance point of view, it might also
be interesting to combine the use of large-scale
feature sets with other recent improvements such
as the use of semi-supervised learning techniques
(Suzuki and Isozaki, 2008) or variable-length de-
pendencies (Qian et al, 2009).
References
Galen Andrew and Jianfeng Gao. 2007. Scalable train-
ing of l1-regularized log-linear models. In Proceed-
ings of the International Conference on Machine
Learning, pages 33?40, Corvalis, Oregon.
Le?on Bottou. 2004. Stochastic learning. In Olivier
Bousquet and Ulrike von Luxburg, editors, Ad-
vanced Lectures on Machine Learning, Lecture
Notes in Artificial Intelligence, LNAI 3176, pages
146?168. Springer Verlag, Berlin.
Le?on Bottou. 2007. Stochastic gradient descent (sgd)
implementation. http://leon.bottou.org/projects/sgd.
Stanley Chen. 2009. Performance prediction for ex-
ponential language models. In Proceedings of the
Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 450?458, Boulder, Colorado, June.
Trevor Cohn. 2006. Efficient inference in large con-
ditional random fields. In Proceedings of the 17th
European Conference on Machine Learning, pages
606?613, Berlin, September.
Thomas G. Dietterich, Adam Ashenfelter, and Yaroslav
Bulatov. 2004. Training conditional random fields
via gradient tree boosting. In Proceedings of
the International Conference on Machine Learning,
Banff, Canada.
Miroslav Dud??k, Steven J. Phillips, and Robert E.
Schapire. 2004. Performance guarantees for reg-
ularized maximum entropy density estimation. In
John Shawe-Taylor and Yoram Singer, editors, Pro-
ceedings of the 17th annual Conference on Learning
Theory, volume 3120 of Lecture Notes in Computer
Science, pages 472?486. Springer.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, condi-
tional random field parsing. In Proceedings of the
Annual Meeting of the Association for Computa-
tional Linguistics, pages 959?967, Columbus, Ohio.
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
2008. Regularization paths for generalized linear
models via coordinate descent. Technical report,
Department of Statistics, Stanford University.
Jianfeng Gao, Galen Andrew, Mark Johnson, and
Kristina Toutanova. 2007. A comparative study of
parameter estimation methods for statistical natural
language processing. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics, pages 824?831, Prague, Czech republic.
Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.
2009. Efficient inference of crfs for large-scale nat-
ural language data. In Proceedings of the Joint Con-
ference of the Annual Meeting of the Association
for Computational Linguistics and the International
Joint Conference on Natural Language Processing,
pages 281?284, Suntec, Singapore.
Jun?ichi Kazama and Jun?ichi Tsujii. 2003. Evalua-
tion and extension of maximum entropy models with
inequality constraints. In Proceedings of the 2003
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 137?144.
Taku Kudo. 2005. CRF++: Yet another CRF toolkit.
http://crfpp.sourceforge.net/.
512
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of the International
Conference on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.
Percy Liang, Hal Daume?, III, and Dan Klein. 2008.
Structure compilation: trading structure for features.
In Proceedings of the 25th international conference
on Machine learning, pages 592?599.
Dong C. Liu and Jorge Nocedal. 1989. On the limited
memory BFGS method for large scale optimization.
Mathematical Programming, 45:503?528.
Gideon Mann, Ryan McDonald, Mehryar Mohri,
Nathan Silberman, and Dan Walker. 2009. Efficient
large-scale distributed training of conditional maxi-
mum entropy models. In Y. Bengio, D. Schuurmans,
J. Lafferty, C. K. I. Williams, and A.Culotta, editors,
Advances in Neural Information Processing Systems
22, pages 1231?1239.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of English: The Penn treebank. Com-
putational Linguistics, 19(2):313?330.
Jorge Nocedal and Stephen Wright. 2006. Numerical
Optimization. Springer.
Naoaki Okazaki. 2007. CRFsuite: A fast im-
plementation of conditional random fields (CRFs).
http://www.chokkan.org/software/crfsuite/.
Chris Pal, Charles Sutton, and Andrew McCallum.
2006. Sparse forward-backward using minimum di-
vergence beams for fast training of conditional ran-
dom fields. In Proceedings of the International Con-
ference on Acoustics, Speech, and Signal Process-
ing, Toulouse, France.
Simon Perkins, Kevin Lacker, and James Theiler.
2003. Grafting: Fast, incremental feature selection
by gradient descent in function space. Journal of
Machine Learning Research, 3:1333?1356.
Vasin Punyakanok, Dan Roth, Wen tau Yih, and Dav
Zimak. 2005. Learning and inference over con-
strained output. In Proceedings of the International
Joint Conference on Artificial Intelligence, pages
1124?1129.
Xian Qian, Xiaoqian Jiang, Qi Zhang, Xuanjing
Huang, and Lide Wu. 2009. Sparse higher order
conditional random fields for improved sequence la-
beling. In Proceedings of the Annual International
Conference on Machine Learning, pages 849?856.
Stefan Riezler and Alexander Vasserman. 2004. Incre-
mental feature selection and l1 regularization for re-
laxed maximum-entropy modeling. In Dekang Lin
and Dekai Wu, editors, Proceedings of the confer-
ence on Empirical Methods in Natural Language
Processing, pages 174?181, Barcelona, Spain, July.
Antoine Rozenknop. 2002. Mode`les syntaxiques
probabilistes non-ge?ne?ratifs. Ph.D. thesis, Dpt.
d?informatique, E?cole Polytechnique Fe?de?rale de
Lausanne.
Terrence J. Sejnowski and Charles R. Rosenberg.
1987. Parallel networks that learn to pronounce en-
glish text. Complex Systems, 1.
Libin Shen, Giorgio Satta, and Aravind Joshi. 2007.
Guided learning for bidirectional sequence classi-
fication. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 760?767, Prague, Czech Republic.
Nataliya Sokolovska, Thomas Lavergne, Olivier
Cappe?, and Franc?ois Yvon. 2010. Efficient learning
of sparse conditional random fields for supervised
sequence labelling. IEEE Selected Topics in Signal
Processing.
Charles Sutton and Andrew McCallum. 2006. An in-
troduction to conditional random fields for relational
learning. In Lise Getoor and Ben Taskar, editors, In-
troduction to Statistical Relational Learning, Cam-
bridge, MA. The MIT Press.
Jun Suzuki and Hideki Isozaki. 2008. Semi-supervised
sequential labeling and segmentation using giga-
word scale unlabeled data. In Proceedings of the
Conference of the Association for Computational
Linguistics on Human Language Technology, pages
665?673, Columbus, Ohio.
Robert Tibshirani. 1996. Regression shrinkage and
selection via the lasso. J.R.Statist.Soc.B, 58(1):267?
288.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology, pages
173?180.
Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-
niadou. 2009. Stochastic gradient descent training
for l1-regularized log-linear models with cumula-
tive penalty. In Proceedings of the Joint Conference
of the Annual Meeting of the Association for Com-
putational Linguistics and the International Joint
Conference on Natural Language Processing, pages
477?485, Suntec, Singapore.
S. V. N. Vishwanathan, Nicol N. Schraudolph, Mark
Schmidt, and Kevin Murphy. 2006. Accelerated
training of conditional random fields with stochas-
tic gradient methods. In Proceedings of the 23th In-
ternational Conference on Machine Learning, pages
969?976. ACM Press, New York, NY, USA.
Hui Zhou and Trevor Hastie. 2005. Regularization and
variable selection via the elastic net. J. Royal. Stat.
Soc. B., 67(2):301?320.
513
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 41?48
Manchester, August 2008
Using LDA to detect semantically incoherent documents
Hemant Misra and Olivier Cappe?
LTCI/CNRS and TELECOM ParisTech
{misra,cappe}@enst.fr
Franc?ois Yvon
Univ Paris-Sud 11 and LMISI-CNRS
yvon@limsi.fr
Abstract
Detecting the semantic coherence of a doc-
ument is a challenging task and has sev-
eral applications such as in text segmenta-
tion and categorization. This paper is an
attempt to distinguish between a ?semanti-
cally coherent? true document and a ?ran-
domly generated? false document through
topic detection in the framework of latent
Dirichlet analysis. Based on the premise
that a true document contains only a few
topics and a false document is made up of
many topics, it is asserted that the entropy
of the topic distribution will be lower for
a true document than that for a false docu-
ment. This hypothesis is tested on several
false document sets generated by various
methods and is found to be useful for fake
content detection applications.
1 Introduction
The ?Internet revolution? has dramatically in-
creased the monetary value of higher ranking on
the web search engines index, fostering the ex-
pansion of techniques, collectively known as ?Web
Spam?, that fraudulently help to do so. Internet is
indeed ?polluted? with fake Web sites whose only
purpose is to deceive the search engines by arti-
ficially pushing up the popularity of commercial
sites, or sites promoting illegal content 1. These
fake sites are often forged using very crude content
generation techniques, ranging from web scrap-
ping (blending of chunks of actual contents) to
simple-minded text generation techniques based
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
1The annual AirWeb challenge http://airweb.
cse.lehigh.edu gives a state-of-the art on current Web
Spam detection techniques.
on random sampling of words (?word salads?),
or randomly replacing words in actual documents
(?word stuffing?) 2. Among these, the latter two
are easy to detect using simple statistical models
of natural texts, but the former is more challeng-
ing, it being made up of actual sentences: recog-
nizing these texts as forged requires either to resort
to plagiarism detection techniques, or to automati-
cally identify their lack of semantic consistency.
Detecting the consistency of texts or of text
chunks has many applications in Natural Language
Processing. So far, it has been used mainly in the
context of automatic text segmentation, where a
change in vocabulary is often the mark of topic
change (Hearst, 1997), and, to a lesser extent, in
discourse studies (see, e.g., (Foltz et al, 1998)).
It could also serve to devise automatic metrics for
text summarization or machine translation tasks.
This paper is an attempt to address the issue
of differentiating between ?true? and ?false? doc-
uments on the basis of their consistency through
topic modeling approach. We have used La-
tent Dirichlet alocation (LDA) (Blei et al, 2002)
model as our main topic modeling tool. One of the
aims of LDA and similar methods, including prob-
abilistic latent semantic analysis (PLSA) (Hof-
mann, 2001), is to produce low dimensionality rep-
resentations of texts in a ?semantic space? such
that most of their inherent statistical characteristics
are preserved. A reduction in dimensionality facil-
itates storage as well as faster retrieval. Modeling
discrete data has many applications in classifica-
tion, categorization, topic detection, data mining,
information retrieval (IR), summarization and col-
laborative filtering (Buntine and Jakulin, 2004).
The aim of this paper is to test LDA for es-
tablishing the semantic coherence of a document
based on the premise that a real (coherent) docu-
ment should discuss only a few number of topics,
2The same techniques are commonly used in mail spams
also.
41
a property hardly granted for forged documents
which are often made up of random assemblage
of words or sentences. As a consequence, the co-
herence of a document may reflect in the entropy
of its posterior topic distribution or in its perplex-
ity for the model. The entropy of the estimated
topic distribution of a true document is expected to
be lower than that of a fake document. Moreover,
the length normalized log-likelihood of a true and
coherent document may be higher as compared to
that of a false and incoherent document.
In this paper, we compare two methods to esti-
mate the posterior topic distribution of test docu-
ments, and this study is also an attempt to inves-
tigate the role of different parameters on the effi-
ciency of these methods.
This paper is organized as follows: In Section 2,
the basics of the LDA model are set. We then dis-
cuss and contrast several approaches to the prob-
lem of inferring the topic distribution of a new
document in Section 3. In Section 4, we describe
the corpus and experimental set-up that are used
to produce the results presented in Section 5. We
summarize our main findings and draw perspec-
tives for future research in Section 6.
2 Latent Dirichlet Allocation
2.1 Basics
LDA is a probabilistic model of text data which
provides a generative analog of PLSA (Blei et al,
2002), and is primarily meant to reveal hidden top-
ics in text documents. In (Griffiths and Steyvers,
2004), the authors used LDA for identifying ?hot
topics? by analyzing the temporal dynamics of top-
ics over a period of time. More recently LDA has
also been used for unsupervised language model
(LM) adaptation in the context of automatic speech
recognition (ASR) (Hsu and Glass, 2006; Tam
and Schultz, 2007; Heidel et al, 2007). Several
extensions of the LDA model, such as hierarchi-
cal LDA (Blei et al, 2004), HMM-LDA (Grif-
fiths et al, 2005), correlated topic models (Blei
and Lafferty, 2005) and hidden topic Markov mod-
els (Gruber et al, 2007), have been proposed, that
introduce more complex dependency patterns in
the model.
Like most of the text mining techniques, LDA
assumes that documents are made up of words and
the ordering of the words within a document is
unimportant (?bag-of-words? assumption). Con-
trary to the simpler Multinomial Mixture Model
(see, e.g., (Nigam et al, 2000) and Section 2.4),
LDA assumes that every document is represented
by a topic distribution and that each topic defines
an underlying distribution on words.
The generative history of a document (a bag-
of-words) collection is the following: Assuming
a fixed and known number of topics n
T
, for each
topic t, a distribution ?
t
over the indexing vocab-
ulary (w = 1 . . . n
W
) is drawn from a Dirichlet
distribution. Then, for each document d, a distri-
bution ?
d
over the topics (t = 1 . . . n
T
) is drawn
from a Dirichlet distribution. For a document d,
the document length l
d
being an exogenous vari-
able, the next step consists of drawing a topic t
i
from ?
d
for each position i = 1...l
d
. Finally, a
word is selected from the chosen topic t
i
. Given
the topic distribution, each word is thus drawn in-
dependently from every other word using a docu-
ment specific mixture model. The probability of
i
th word token is thus:
P (w
i
|?
d
, ?) =
n
T
?
t=1
P (t
i
= t|?
d
)P (w
i
|t
i
, ?) (1)
=
n
T
?
t=1
?
dt
?
tw
(2)
Conditioned on ? and ?
d
, the likelihood of doc-
ument d is a mere product of terms such as (2),
which can be rewritten as:
P (C
d
|?
d
, ?) =
n
W
?
w=1
[
n
T
?
t=1
(?
dt
?
tw
)
]
C
dw
(3)
where C
dw
is the count of word w in d.
2.2 LDA: Training
LDA training consists of estimating the following
two parameter vectors from a text collection: the
topic distribution in each document d (?
dt
, t =
1...n
T
, d = 1...n
D
) and the word distribution in
each topic (?
tw
, t = 1...n
T
, w = 1...n
W
). Both
?
d
and ?
t
define discrete distributions, respectively
over the set of topics and over the set of words.
Various methods have been proposed to estimate
LDA parameters, such as variational method (Blei
et al, 2002), expectation propagation (Minka and
Lafferty, 2002) and Gibbs sampling (Griffiths and
Steyvers, 2004). In this paper, we have used
the latter approach, which boils down to repeat-
edly going through the training data and sampling
the topic assigned to each word token conditioned
42
on the topic assigned to all the other word to-
kens. Given a particular Gibbs sample, the pos-
teriors for ? and ? are 3: Dirichlet with parameters
(K
t1
+?, . . . ,K
tn
W
+?) and Dirichlet with param-
eters (J
d1
+ ?, . . . , J
dn
T
+ ?), respectively, where
K
tw
is the number of times word w is assigned to
topic t and J
dt
is the number of times topic t is as-
signed to some word token in document d. Hence,
?
tw
=
K
tw
+ ?
?
n
W
k=1
K
tk
+ n
W
?
(4)
?
dt
=
J
dt
+ ?
?
n
T
k=1
J
dk
+ n
T
?
(5)
During the Gibbs sampling phase, ?
t
and ?
d
are
sampled from the above posteriors while the final
estimates for these parameters are obtained by av-
eraging the posterior means over the complete set
of Gibbs iteration.
2.3 LDA: Testing
Training LDA model on a text collection already
provides interesting insights regarding the the-
matic structure of the collection. This has been the
primary application of LDA in (Blei et al, 2002;
Griffiths and Steyvers, 2004). Even better, being
a generative model, LDA can be used to make
prediction regarding novel documents (assuming
they use the same vocabulary as the training cor-
pus). In a typical IR setting, where the main fo-
cus is on computing the similarity between a doc-
ument d and a query d?, a natural similarity mea-
sure is given by P (C
d
? |?
d
, ?), computed according
to (3) (Buntine et al, 2004).
An alternative would be to compute the KL di-
vergence between the topic distribution in d and d?,
which however requires to infer the latter quantity.
As the topic distribution of a (new) document gives
its representation along the latent semantic dimen-
sions, computing this value is helpful for many
applications, including text segmentation and text
classification. Methods for efficiently and accu-
rately estimating topic distribution for text docu-
ments are presented and evaluated in Section 3.
2.4 Baseline: Multinomial Mixture Model
The performance of LDA model is compared
with that of the simpler multinomial mixture
model (Nigam et al, 2000; Rigouste et al, 2007).
3assuming non-informative priors with hyper-parameters
? and ? for the Dirichlet distribution over topics and the
Dirichlet distribution over words respectively
In this model, every word in a document belongs
to the same topic, as if the document specific topic
distribution ?
d
in LDA were bound to lie on one
vertex of the [0, 1]nT simplex. Using the same no-
tations as before (except for ?
t
, which now denotes
the position independent probability of topic t in
the collection), the probability of a document is:
P (C
d
|?
t
, ?) =
n
T
?
t=1
?
t
n
W
?
w=1
?
C
dw
tw
(6)
This model can be trained through expectation
maximization (EM), using the following reestima-
tion formulas, where (7) defines the E-step; (8) and
(9) define the M-step.
P (t|C
d
, ?, ?) =
?
t
?
n
W
w=1
(?
?
tw
)
C
dw
?
n
T
t=1
?
t
?
n
W
w=1
(?
tw
)
C
dw
(7)
?
?
t
? ? +
n
D
?
d=1
P (t|C
d
, ?, ?) (8)
?
?
tw
? ? +
n
D
?
d=1
C
dw
P (t|C
d
, ?, ?) (9)
As suggested in (Rigouste et al, 2007), we ini-
tialize the EM algorithm by drawing initial topic
distributions from a prior Dirichlet distribution
with hyper-parameter ? = 1. ? = 0.1 in all the
experiments.
During testing, the parameters of the multino-
mial models are used to estimate the posterior topic
distribution in each document using (7). The like-
lihood of a test document is given by (6).
3 Inferring the Topic Distribution of Test
Documents
P (C
d
|?
d
), the conditional probability of a docu-
ment d given ?
d
is obtained using (3) 4. Computing
the likelihood of a test document requires to inte-
grate this quantity over ?; likewise for the compu-
tation of the posterior distribution of ?. This inte-
gral has no close form solution, but can be approx-
imated using Monte-Carlo sampling techniques as:
P (C
d
) ? 1
M
M
?
m=1
P (C
d
|?(m)) (10)
where ?(m) denotes the mth sample from the
Dirichlet prior, and M is the number of Monte
4The dependence on ? is dropped for simplicity. ? is
learned during training and kept fixed during testing.
43
Carlo samples. Given the typical length of doc-
uments and the large vocabulary size, small scale
experiments convinced us that a cruder approxima-
tion was in order, as the sum in (10) is dominated
by the maximum value. We thus contend ourselves
to solve:
?
?
= argmax
?,
P
t
?
t
=1
P (C
d
|?) (11)
and use this value to approximate P (C
d
) using (3).
The maximization program (11) has no close
form solution. However, the objective function is
differentiable and log-concave, and can be opti-
mized in a number of ways. We considered two
different algorithms: an EM-like approach, ini-
tially introduced in (Heidel et al, 2007), and an ex-
ponentiated gradient approach (Kivinen and War-
muth, 1997; Globerson et al, 2007).
The first approach implements an iterative pro-
cedure based on the following update rule:
?
dt
? 1
l
d
n
W
?
w=1
C
dw
?
dt
?
tw
?
n
T
t
?
=1
?
dt
?
?
t
?
w
(12)
Although no justification was given in (Hei-
del et al, 2007), it can be shown that this
update rule converges towards a global opti-
mum of the likelihood. Let ? and ?? be two
topic distributions in the n
T
-dimensional simplex,
L(?) = log P (C
d
|?), and ?
t
(w, ?) =
?
t
?
tw
P
t
?
?
t
?
?
t
?
w
.
We define an auxiliary function Q(?, ??) =
?
w
C
w
(
?
t
?
t
(w, ?) log(?
?
t
)). Q(?, ?
?
) is concave
in ??, and performs the role played by the auxil-
iary function in the EM algorithm. Simple cal-
culus suffices to prove that (i) the update (12)
maximizes in ?? the function Q(?, ??), and (ii)
Q(?, ?
?
) ? Q(?, ?) ? L(??) ? L(?), which stems
from the concavity of the log. At an optimum of
Q(?, ?
?
) the positivity of the first term implies the
positivity of the second. Maximizing Q using the
update rule (12) thus increases the likelihood and
repeating this update converges towards the opti-
mum value. We experimented both with an un-
smoothed (12) and with a smoothed version of this
update rule. The unsmoothed version yielded a
slightly better result than the smoothed one.
Exponentiated gradient (Kivinen and Warmuth,
1997; Globerson et al, 2007) yields an alternative
update rule:
?
dt
? ?
dt
exp
(
?
n
W
?
w=1
C
dw
?
tw
?
n
T
t
?
=1
?
dt
?
?
t
?
w
)
(13)
where ? defines the convergence rate. In this form,
the update rule does not preserve the normaliza-
tion of ?, which needs to be performed after every
iteration.
A systematic comparison of these rules was car-
ried out, yielding the following conclusions:
? the convergence of the EM-like method is
very fast. Typically, it requires less than half
a dozen iterations to converge. After conver-
gence, the topic distribution estimated by this
method for a subset of train documents was
always very close (as measured by the KL-
divergence) to the respective topic distribu-
tion of the same documents observed at the
end of the LDA training. Taking n
T
= 50,
the average KL divergence for a set of 4,500
documents was found to be less than 0.5.
? exponentiated gradient has a more erratic be-
haviour, and requires a careful tuning of ? on
a per document basis. For large values of ?,
the update rule (13) sometimes fails to con-
verge; smaller values of ? allowed to consis-
tently reach convergence, but required more
iterations (typically 20-30). On a positive
side, on an average, the topic distributions
estimated by this method are better than the
ones obtained with the EM-like algorithm.
Based on these findings, we decided to use the
EM-like algorithm in all our subsequent experi-
ments.
4 Experimental protocol
4.1 Training and test corpora
The Reuters Corpus Volume 1 (RCV1) (Lewis et
al., 2004) is a collection of over 800,000 news
items in English from August 1996 to August
1997. Out of the entire RCV1 dataset, we se-
lected 27,672 documents (news items) for training
(TrainReuters) and 23,326 documents for testing
(TestReuters). The first 4000 documents from the
TestReuters dataset were used as true documents
(TrueReuters) in the experiments reported in this
paper. The vocabulary size in the train set, after
removing the function words, is 93, 214.
Along with these datasets of ?true? documents,
three datasets of fake documents were also cre-
ated. Document generation techniques are many:
here we consider documents made by mixing short
passages from various texts and documents made
44
by assembling randomly chosen words (sometimes
called as ?word salads?). In addition, we also
consider the case of documents generated with a
stochastic language model (LM). Our ?fake? test
documents are thus composed of:
? (SentenceSalad) obtained by randomly pick-
ing sentences from TestReuters.
? (WordSalad) created by generating random
sentences from a conventional unigram LM
trained on TrainReuters.
? (Markovian) created by generating random
sentences from a conventional 3-gram LM
trained on TrainReuters.
Each of these forged document set contains 4,000
documents.
To assess the performance on out-of-domain
data, we replicated the same tests using 2,000
Medline abstracts (Ohta et al, 2002). 1,500 doc-
uments were used either to generate fake docu-
ments by picking sentences randomly or to train an
LM and then using the LM to generate fake docu-
ments. The remaining 500 abstracts were set aside
as ?true? documents (TrueMedline).
4.2 Performance Measurements : EER
The entropy of the topic distribution is computed
as H = ?
?
T
j=1
?
?
dj
log
?
?
dj
. The other measure
of interest is the average ?log-likelihood per word?
(LLPW) 5.
While evaluating the performance of our sys-
tem, two types of errors are encountered: false ac-
ceptance (FA) when a false document is accepted
as a true document and false rejection (FR) when a
true document is rejected as a false document. The
rate of FA and FR is dependent on the threshold
used for taking the decision, and usually the per-
formance of a system is shown by its receiver op-
erating characteristic (ROC) curve which is a plot
between FA and FR rates for different values of
threshold. Instead of reporting the performance of
a system based on two error rates (FA and FR),
the general practice is to report the performance in
terms of equal-error-rate (EER). The EER is the
error rate at the threshold where FA rate = FR rate.
In our system, a threshold on entropy (or
LLPW) is used for taking the decision, and all the
5This measure is directly related to the text per-
plexity in the model, according to perplexity =
2
?average log-likelihood per word
documents having their entropy (or LLPW) below
(or above) the threshold are accepted as true doc-
uments. The EER is obtained on the test set by
changing the threshold on the test set itself, and
the best results thus obtained are reported.
5 Detecting semantic inconsistency
5.1 Detecting fake documents with LDA and
Multinomial mixtures
In the first set of experiments, the LLPW and en-
tropy of the topic distribution (the two measures)
of the Multinomial mixture and LDA models were
compared to check the ability of these two mea-
sures and models in discriminating between true
and false documents. These results are summa-
rized in Table 1.
TrueReuters vs. MultinomialLLPW Entropy
SentenceSalad 15.3% 48.8%
WordSalad 9.3% 35.8%
Markovian 17.6% 38.9%
TrueReuters vs. LDALLPW Entropy
SentenceSalad 18.9% 0.88%
WordSalad 9.9% 0.13%
Markovian 25.0% 0.28%
Table 1: Performance of the Multinomial Mixture
and LDA
For the multinomial mixture model, the LLPW
measure is able to discriminate between true and
false documents to a certain extent. As expected
(not shown here), the LLPW of the true documents
is usually higher than that of the false documents.
In contrast, the entropy of the posterior topic dis-
tribution does not help much in discriminating be-
tween true and false documents. In fact it remains
close to zero (meaning that only one topic is ?ac-
tive?) both for true and false documents.
The behaviour of the LDA scores is entirely dif-
ferent. The perplexity scores (LLPW) of true and
fake texts are comparable, and do not make useful
predictors. In contrast, the entropy of the topic dis-
tribution allows to sort true documents from fake
ones with a very high accuracy for all kinds of fake
texts considered in this paper. Both results stem
from the ability of LDA to assign a different topic
to each word occurrence.
Similar pattern is observed for our three false
test sets (against the TrueReuters set) with small
45
variations The texts generated with a Markov
model, no matter the order, have the highest en-
tropy, reflecting the absence of long range corre-
lation in the generation model. Though the texts
generated by mixing sentences are more confus-
ing with the true documents, the performance is
still less than 1% EER. Texts mixing a high num-
ber of topics (e.g., Sentence Salads) are almost as
likely as natural texts that address only a few top-
ics. However, the former has much higher entropy
of the topic distribution due to a large number of
topics being active in such texts (see also Figure 1).
0 1 2 3 4 5 6
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
Entropy
N
or
m
al
iz
ed
 fr
eq
ue
nc
y 
of
 o
cc
ur
en
ce
 
 
True: TrueReuters
False: SentenceSalad
False: WordSalad
False: Markovian
Figure 1: Histogram of entropy of ? for different
true and false document sets.
It is noteworthy that both the predictors (LLPW
and Entropy) give complementary clues regarding
a text category. A linear combination of these two
scores (the weight to the LLPW score is 0.1) al-
lows to substantially improve over these baseline
results, yielding a relative improvement (in EER)
of +20.0% for the sentence salads, +20.8% for the
word salads, and +27.3% for the Markov Models.
5.2 Effect of the number of topics
In this part, we investigate the performance of
LDA in detecting false documents when the num-
ber of topics is changed. Increasing the number
of topics means higher memory requirements both
during training and testing. Though the results are
shown only for SentenceSalad, similar trend is ob-
served for WordSalad and Markovian.
The numbers in Table 2 show that the perfor-
mance obtained with the LLPW score consistently
improve with an increase in the number of top-
ics, though the % improvement obtained when the
number of topics exceeds 200 is marginal. In con-
trast, the best performance in case of entropy is
achieved at 50 topics and slowly degrades when a
more complex model is used.
Number of Topics LLPW Entropy
10 27.9 1.88
50 18.9 0.88
100 16.0 0.93
200 14.8 0.90
300 13.8 1.05
400 13.6 1.10
Table 2: EER from LLPW and Entropy distribution
for TrueReuters against SentenceSalad.
5.3 Detecting ?noisy? documents
In this section, we study fake documents produced
by randomly changing words in true documents
(the TrueReuters dataset). In each document, a
fixed percentage of content words is randomly re-
placed by any other word from the training vocab-
ulary 6. This percentage was varied from 5 to 100
and EER for these corrupted document sets is com-
puted at each % corruption level (Figure 2). As
0 5 10 15 20 25 30 35 40 45 50 60 70 80 90 100
0
5
10
15
20
25
30
35
40
45
50
Noise level (% words changed)
EE
R
: L
LP
W
 a
nd
 E
nt
ro
py
 
 
LLPW
Entropy
Figure 2: EER at various noise levels
expected, the EER is very high at low noise levels,
and as the noise level is increased, EER gets lower.
When only a few words are changed in a true doc-
ument, it retrains the properties of a true document
(high LLPW and low entropy). However, as more
number of words are changed in a true document,
6When the replacement words are chosen from a small set
of very specific words, the fake document generation strategy
is termed as ?word stuffing?.
46
it starts showing the characteristics of a false docu-
ment (low LLPW and high entropy). These results
suggest that our semantic consistency tests are too
crude a measure to detect a small number of in-
consistencies, such as the ones found in the state-
of-the-art OCR or ASR systems? outputs. On the
other hand, it confirms the numerous studies that
have shown that topic detection (and topic adapta-
tion) or text categorization tasks can be performed
with the same accuracy for moderately noisy texts
and clean texts, a finding which warrants the topic-
based LM adaptation strategies deployed in (Hei-
del et al, 2007; Tam and Schultz, 2007).
The difference in the behavior of our two pre-
dictors is striking. The EER obtained using LLPW
drops more quickly than the one obtained with en-
tropy of the topic distribution. It suggests that the
influence of ?corrupting? content words (mostly
with low ?
tw
) is heavy on the LLPW, but the topic
information is not lost till a majority of the ?uncor-
rupted? content words belong to the same topic.
5.4 Effect of the document length
In this section, we study the robustness of our
two predictors with respect to the document length
by progressively increasing the number of content
words in a document (true or fake). As can be seen
from Figure 3, the entropy of the posterior topic
distribution starts to provide a reasonable discrim-
ination (5% EER) when the test documents contain
about 80 to 100 content words, and attains results
comparable to those reported earlier in this paper
when this number doubles. This definitely rules
out this method as a predictor of the semantic con-
sistency of a sentence: we need to consider at least
a paragraph to get acceptable results.
5.5 Testing with out-of-domain data
In this section, we study the robustness of our pre-
dictors on out-of-domain data using a small ex-
cerpt of abstracts from the Medline database. Both
true and fake documents are from this dataset.
The results are summarized in Table 3. The per-
TrueMedline vs. LLPW Entropy
SentenceSalad 31.23% 22.13%
WordSalad 30.03% 19.46%
Markovian 36.51% 23.63%
Table 3: Performance of LDA on PubMed ab-
stracts
formance on out-of-domain documents is poor,
10 20 30 40 50 60 70 80 90100 125 150 175 200 225
0
5
10
15
20
25
30
35
40
45
50
Number of content words
EE
R
: L
LP
W
 a
nd
 E
nt
ro
py
TrueReuters against False sets
 
 
LLPW: SentenceSalad
LLPW: WordSalad
LLPW: Markovian
Entropy: SentenceSalad
Entropy: WordSalad
Entropy: Markovian
Figure 3: EER with change in number of con-
tent words used for LDA analysis. EER based
on: LLPW of TrueReuters and false document sets
(solid line) and Entropy of topic distribution of
TrueReuters and false document sets (dashed line).
though the entropy of the topic distribution is still
the best predictor. The reasons for this failure are
obvious: a majority of the words occurring in these
documents (true or fake) are, from the perspective
of the model, characteristic of one single Reuters
topic (health and medicine). They cannot be dis-
tinguished either in terms of perplexity or in terms
of topic distribution (the entropy is low for all the
documents). It is interesting to note that all the
out-of-domain Medline data can be separated from
the in-domain TrueReuters data with good accu-
racy on the basis of the lower LLPW of the former
as compared to the higher LLPW of the latter.
6 Conclusion
In the LDA framework, this paper investigated two
methods to infer the topic distribution in a test
document. Further, the paper suggested that the
coherence of a document can be evaluated based
on its topic distribution and average LLPW, and
these measures can help to discriminate between
true and false documents. Indeed, through exper-
imental results, it was shown that entropy of the
topic distribution is lower and average LLPW of
true documents is higher for true documents and
the former measure was found to be more effective.
However, the poor performance of this method on
out-of-domain data suggests that we need to use a
much larger training corpus to build a robust fake
document detector. This raises the issue of train-
47
ing LDA model with very large collections. In fu-
ture we would like to explore the potential of this
method for text segmentation tasks.
Acknowledgment
This research was supported by the European
Commission under the contract FP6-027026-K-
Space. The views expressed in this paper are those
of the authors and do not necessarily represent the
views of the commission.
References
Blei, David and John Lafferty. 2005. Correlated topic
models. In Advances in Neural Information Process-
ing Systems (NIPS?18), Vancouver, Canada.
Blei, David M., Andrew Y. Ng, and Michael I. Jordan.
2002. Latent Dirichlet alocation. In Dietterich,
Thomas G., Suzanna Becker, and Zoubin Ghahra-
mani, editors, Advances in Neural Information Pro-
cessing Systems (NIPS), volume 14, pages 601?608,
Cambridge, MA. MIT Press.
Blei, David M., Thomas L. Griffiths, Michael I. Jordan,
and Joshua B. Tenenbaum. 2004. Hierarchical topic
models and the nested Chinese restaurant process. In
Advances in Neural Information Processing Systems
(NIPS), volume 16, Vancouver, Canada.
Buntine, Wray and Aleks Jakulin. 2004. Apply-
ing discrete PCA in data analysis. In Chickering,
M. and J. Halpern, editors, Proceedings of the 20th
Conference on Uncertainty in Artificial Intelligence
(UAI?04), pages 59?66. AUAI Press 2004.
Buntine, Wray, Jaakko Lo?fstro?m, Jukka Perkio?, Sami
Perttu, Vladimir Poroshin, Tomi Silander, Henry
Tirri, Antti Tuominen, and Ville Tuulos. 2004.
A scalable topic-based open source search engine.
In Proceedings of the IEEE/WIC/ACM International
Conference on Web Intelligence, pages 228?234,
Beijing, China.
Foltz, P.W., W. Kintsch, and T.K. Landauer. 1998. The
measurement of textual coherence with Latent Se-
mantic Analysis. Discourse Processes, 25(2-3):285?
307.
Globerson, Amir, Terry Y. Koo, Xavier Carreras, and
Michael Collins. 2007. Exponentiated gradient al-
gorithms for log-linear structured prediction. In In-
ternational Conference on Machine Learning, Cor-
vallis, Oregon.
Griffiths, Thomas L. and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences, 101 (supl 1):5228?5235.
Griffiths, Thomas L., Mark Steyvers, David M. Blei,
and Joshua Tenenbaum. 2005. Integrating topics
and syntax. In Proceedings of NIPS, 17, Vancouver,
CA.
Gruber, Amit, Michal Rosen-Zvi, and Yair Weiss.
2007. Hidden topic Markov models. In Proceedings
of International Conference on Artificial Intelligence
and Statistics, San Juan, Puerto Rico, March.
Hearst, Marti. 1997. TextTiling: Segmenting texts into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Heidel, Aaron, Hung an Chang, and Lin shan Lee.
2007. Language model adaptation using latent
Dirichlet alocation and an efficient topic inference
algorithm. In Proceedings of European Confer-
ence on Speech Communication and Technology,
Antwerp, Belgium.
Hofmann, Thomas. 2001. Unsupervised learning
by probabilistic latent semantic analysis. Machine
Learning Journal, 42(1):177?196.
Hsu, Bo-June (Paul) and Jim Glass. 2006. Style
& topic language model adaptation using HMM-
LDA. In Proceedings of the Conference on Empiri-
cal Methods in Natural Language Processing, Syd-
ney, Australia.
Kivinen, Jyrki and Manfrud K. Warmuth. 1997. Expo-
nentiated gradient versus gradient descent for linear
predictors. Information and Computation, 132:1?
63.
Lewis, David D., Yiming Yang, Tony Rose, and Fan
Li. 2004. RCV1: A new benchmark collection for
text categorization research. Machine Learning Re-
search, 5:361?397.
Minka, Thomas and John Lafferty. 2002. Expectation-
propagation for the generative aspect model. In Pro-
ceedings of the conference on Uncertainty in Artifi-
cial Intelligence (UAI).
Nigam, K., A. K. McCallum, S. Thrun, and T. M.
Mitchell. 2000. Text classification from labeled and
unlabeled documents using EM. Machine Learning,
39(2/3):103?134.
Ohta, Tomoko, Yuka Tateisi, Hideki Mima, Jun ichi
Tsujii, and Jin-Dong Kim. 2002. The GENIA cor-
pus: an annotated research abstract corpus in molec-
ular biology domain. In Proceedings of Human Lan-
guage Technology Conference, pages 73?77.
Rigouste, Lo??s, Olivier Cappe?, and Franc?ois Yvon.
2007. Inference and evaluation of the multino-
mial mixture model for text clustering. Informa-
tion Processing and Management, 43(5):1260?1280,
September.
Tam, Yik-Cheung and Tanja Schultz. 2007. Correlated
latent semantic model for unsupervised LM adapta-
tion. In Proceedings of the International Conference
on Acoustics, Speech and Signal Processing, Hon-
olulu, Hawaii, U.S.A.
48
