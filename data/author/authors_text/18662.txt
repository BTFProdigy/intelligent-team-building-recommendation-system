Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 57?60,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Graphical Interface for Automatic Error Mining in Corpora
Gregor Thiele Wolfgang Seeker Markus G
?
artner Anders Bj
?
orkelund Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present an error mining tool that is de-
signed to help human annotators to find
errors and inconsistencies in their anno-
tation. The output of the underlying al-
gorithm is accessible via a graphical user
interface, which provides two aggregate
views: a list of potential errors in con-
text and a distribution over labels. The
user can always directly access the ac-
tual sentence containing the potential er-
ror, thus enabling annotators to quickly
judge whether the found candidate is in-
deed incorrectly labeled.
1 Introduction
Manually annotated corpora and treebanks are the
primary tools that we have for developing and
evaluating models and theories for natural lan-
guage processing. Given their importance for test-
ing our hypotheses, it is imperative that they are
of the best quality possible. However, manual an-
notation is tedious and error-prone, especially if
many annotators are involved. It is therefore desir-
able to have automatic means for detecting errors
and inconsistencies in the annotation.
Automatic methods for error detection in tree-
banks have been developed in the DECCA
project
1
for several different annotation types, for
example part-of-speech (Dickinson and Meurers,
2003a), constituency syntax (Dickinson and Meur-
ers, 2003b), and dependency syntax (Boyd et al.,
2008). These algorithms work on the assumption
that two data points that appear in identical con-
texts should be labeled in the same way. While
the data points in question, or nuclei, can be single
tokens, spans of tokens, or edges between two to-
kens, context is usually modeled as n-grams over
the surrounding tokens. A nucleus that occurs
1
http://www.decca.osu.edu
multiple times in identical contexts but is labeled
differently shows variation and is considered a po-
tential error.
Natural language is ambiguous and variation
found by an algorithm may be a genuine ambigu-
ity rather than an annotation error. Although we
can support an annotator in finding inconsisten-
cies in a treebank, these inconsistencies still need
to be judged by humans. In this paper, we present
a tool that allows a user to run automatic error de-
tection on a corpus annotated with part-of-speech
or dependency syntax.
2
The tool provides the user
with a graphical interface to browse the variation
nuclei found by the algorithm and inspect their la-
bel distribution. The user can always switch be-
tween high-level aggregate views and the actual
sentences containing the potential error in order to
decide if that particular annotation is incorrect or
not. The interface thus brings together the output
of the error detection algorithm with a direct ac-
cess to the corpus data. This speeds up the pro-
cess of tracking down inconsistencies and errors
in the annotation considerably compared to work-
ing with the raw output of the original DECCA
tools. Several options allow the user to fine-tune
the behavior of the algorithm. The tool is part of
ICARUS (G?artner et al., 2013), a general search
and exploration tool.
3
2 The Error Detection Algorithm
The algorithm, described in Dickinson and Meur-
ers (2003a) for POS tags, works by starting from
individual tokens (the nuclei) by recording their
assigned part-of-speech over an entire treebank.
From there, it iteratively increases the context for
each instance by extending the string to both sides
to include adjacent tokens. It thus successively
builds larger n-grams by adding tokens to the left
2
Generalizing the tool to support any kind of positional
annotation is planned.
3
http://www.ims.uni-stuttgart.de/data/icarus.html
57
Figure 1: The variation n-gram view.
or to the right. Instances are grouped together if
their context is identical, i. e. if their token n-
grams match. Groups where all instances have
the same label do not show variation and are dis-
carded. The algorithm stops when either no vari-
ation nuclei are left or when none of them can be
further extended. All remaining groups that show
variation are considered potential errors. Erro-
neous annotations that do not show variation in the
data cannot be found by the algorithm. This limits
the usefulness of the method for very small data
sets. Also, given the inherent ambiguity of nat-
ural language, the algorithm is not guaranteed to
exclusively output errors, but it achieves very high
precision in experiments on several languages.
The algorithm has been extended to find errors
in constituency and dependency structures (Dick-
inson and Meurers, 2003b; Boyd et al., 2008),
where the definition of a nucleus is changed to
capture phrases and dependency edges. Context
is always modeled using n-grams over surround-
ing tokens, but see, e. g., Boyd et al. (2007) for
extensions.
3 Graphical Error Mining
To start the error mining, a treebank and an error
mining algorithm (part-of-speech or dependency)
must be selected. The algorithm is then executed
on the data to create the variation n-grams. The
user can choose between two views for browsing
the potential errors in the treebank: (1) a view
showing the list of variation n-grams found by the
error detection algorithm and (2) a view showing
label distributions over word forms.
3.1 The Variation N-Gram View
Figure 1 shows a screenshot of the view where the
user is presented with the list of variation n-grams
output by the error detection algorithm. The main
window shows the list of n-grams. When the user
selects one of the n-grams, information about the
nucleus is displayed below the main window. The
user can inspect the distribution over labels (here
part-of-speech tags) with their absolute frequen-
cies. Above the main window, the user can adjust
the length of the presented n-grams, sort them, or
search for specific strings.
For example, Figure 1 shows a part of the vari-
ation n-grams found in the German TiGer corpus
(Brants et al., 2002). The minimum and maximum
length was restricted to four, thus the list contains
only 4-grams. The 4-gram so hoch wie in was se-
lected, which contains wie as its nucleus. In the
lower part, the user can see that wie occurs with
four different part-of-speech tags in the treebank,
namely KOKOM, PWAV, KON, and KOUS. Note
that the combination with KOUS occurs only once
in the entire treebank.
Double clicking on the selected 4-gram in the
list will open up a new tab that displays all sen-
tences that contain this n-gram, with the nucleus
being highlighted. The user can then go through
each of the sentences and decide whether the an-
notated part-of-speech tag is correct. Each time
the user clicks on an n-gram, a new tab will be
created, so that the user can jump back to previous
results without having to recreate them.
A double click on one of the lines in the lower
part of the window will bring up all sentences that
contain that particular combination of word form
58
Figure 2: The label distribution view.
and part-of-speech tag. The fourth line will, for
example, show the one sentence where wie has
been tagged as KOUS, making it easy to quickly
judge whether the tag is correct. In this case, the
annotation is incorrect (it should have been PWAV)
and should thus be marked for correction.
3.2 The Label Distribution View
In addition to the output of the algorithm by Dick-
inson and Meurers (2003a), the tool also provides
a second view, which displays tag distributions of
word forms to the user (see Figure 2). To the left,
a list of unique label combinations is shown. Se-
lecting one of them displays a list of word forms
that occur with exactly these tags in the corpus.
This list is shown below the list of label combina-
tions. To the right, the frequencies of the differ-
ent labels are shown in a bar chart. The leftmost
bar for each label always shows the total frequency
summed over all word forms in the set. Selecting
one or more in the list of word forms adds addi-
tional bars to the chart that show the frequencies
for each selected word form.
As an example, Figure 2 shows the tag combi-
nation [VVINF][VVIZU], which are used to tag in-
finitives with and without incorporated zu in Ger-
man. There are three word forms in the cor-
pus that occur with these two part-of-speech tags:
hinzukommen, aufzul?osen, and anzun?ahern. The
chart on the right shows the frequencies for each
word form and part-of-speech tag, revealing that
hinzukommen is mostly tagged as VVINF but once
as VVIZU, whereas for the other two word forms it
is the other way around. This example is interest-
ing if one is looking for annotation errors in the
TiGer treebank, because the two part-of-speech
tags should have a complementary distribution (a
German verb either incorporates zu or it does not).
Double clicking on the word forms in the list in
the lower left corner will again open up a tab that
shows all sentences containing this word form, re-
gardless of their part-of-speech tag. The user may
then inspect the sentences and decide whether the
annotations are erroneous or not. If the user wants
to see a specific combination, which is more use-
ful if the total number of sentences is large, she
can also click on one of the bars in the chart to get
all sentences matching that combination. In the
example, the one instance of hinzukommen being
tagged as VVIZU is incorrect,
4
and the instances of
the two other verbs tagged as VVINF are as well.
3.3 Dependency Annotation Errors
As mentioned before, the tool also allows the user
to search for errors in dependency structures. The
error mining algorithm for dependency structures
(Boyd et al., 2008) is very similar to the one for
part-of-speech tags, and so is the interface to the
n-gram list or the distribution view. Dependency
edges are therein displayed as triples: the head,
the dependent, and the edge label with the edge?s
direction. As with the part-of-speech tags, the user
can always jump directly to the sentences that con-
tain a particular n-gram or dependency relation.
4
Actually, the word form hinzukommen can belong to two
different verbs, hinzu-kommen and hin-kommen. However,
the latter, which incorporates zu, does not occur in TiGer.
59
4 Error Detection on TiGer
We ran the error mining algorithm for part-of-
speech on the German TiGer Treebank (the de-
pendency version by Seeker and Kuhn (2012)) and
manually evaluated a small sample of n-grams in
order to get an idea of how useful the output is.
We manually checked 115 out of the 207 vari-
ation 6-grams found by the tool, which amounts
to 119 different nuclei. For 99.16% of these nu-
clei, we found erroneous annotations in the asso-
ciated sentences. 95.6% of these are errors where
we are able to decide what the right tag should
be, the remaining ones are more difficult to disam-
biguate because the annotation guidelines do not
cover them.
These results are in line with findings by Dick-
inson and Meurers (2003a) for the Penn Treebank.
They show that even manually annotated corpora
contain errors and an automatic error mining tool
can be a big help in finding them. Furthermore,
it can help annotators to improve their annotation
guidelines by pointing out phenomena that are not
covered by the guidelines, because these phenom-
ena will be more likely to show variation.
5 Related Work
We are aware of only one other graphical tool that
was developed to help with error detection in tree-
banks: Ambati et al. (2010) and Agarwal et al.
(2012) describe a graphical tool that was used in
the annotation of the Hindi Dependency Treebank.
To find errors, it uses a statistical and a rule-based
component. The statistical component is recall-
oriented and learns a MaxEnt model, which is used
to flag dependency edges as errors if their proba-
bility falls below a predefined threshold. In or-
der to increase the precision, the output is post-
processed by the rule-based component, which is
tailored to the treebank?s annotation guidelines.
Errors are presented to the annotators in tables,
also with the option to go to the sentences di-
rectly from there. Unlike the algorithm we im-
plemented, this approach needs annotated training
data for training the classifier and tuning the re-
spective thresholds.
6 Conclusion
High-quality annotations for linguistic corpora are
important for testing hypotheses in NLP and lin-
guistic research. Automatically marking potential
annotation errors and inconsistencies are one way
of supporting annotators in their work. We pre-
sented a tool that provides a graphical interface for
annotators to find and evaluate annotation errors
in treebanks. It implements the error detection al-
gorithms by Dickinson and Meurers (2003a) and
Boyd et al. (2008). The user can view errors from
two perspectives that aggregate error information
found by the algorithm, and it is always easy to
go directly to the actual sentences for manual in-
spection. The tool is currently extended such that
annotators can make changes to the data directly
in the interface when they find an error.
Acknowledgements
We thank Markus Dickinson for his comments.
Funded by BMBF via project No. 01UG1120F,
CLARIN-D, and by DFG via SFB 732, project D8.
References
Rahul Agarwal, Bharat Ram Ambati, and Anil Kumar
Singh. 2012. A GUI to Detect and Correct Errors in
Hindi Dependency Treebank. In LREC 2012, pages
1907?1911.
Bharat Ram Ambati, Mridul Gupta, Samar Husain, and
Dipti Misra Sharma. 2010. A High Recall Error
Identification Tool for Hindi Treebank Validation.
In LREC 2010.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2007. Increasing the Recall of Corpus Annota-
tion Error Detection. In TLT 2007, pages 19?30.
Adriane Boyd, Markus Dickinson, and Detmar Meur-
ers. 2008. On Detecting Errors in Dependency
Treebanks. Research on Language and Computa-
tion, 6(2):113?137.
Sabine Brants, Stefanie Dipper, Silvia Hansen-Shirra,
Wolfgang Lezius, and George Smith. 2002. The
TIGER treebank. In TLT 2002, pages 24?41.
Markus Dickinson and W. Detmar Meurers. 2003a.
Detecting Errors in Part-of-Speech Annotation. In
EACL 2003, pages 107?114.
Markus Dickinson and W. Detmar Meurers. 2003b.
Detecting Inconsistencies in Treebanks. In TLT
2003, pages 45?56.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60.
Wolfgang Seeker and Jonas Kuhn. 2012. Making El-
lipses Explicit in Dependency Conversion for a Ger-
man Treebank. In LREC 2012, pages 3132?3139.
60
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 47?57,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Learning Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features
Anders Bj
?
orkelund and Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{anders,jonas}@ims.uni-stuttgart.de
Abstract
We investigate different ways of learning
structured perceptron models for coref-
erence resolution when using non-local
features and beam search. Our experi-
mental results indicate that standard tech-
niques such as early updates or Learning
as Search Optimization (LaSO) perform
worse than a greedy baseline that only uses
local features. By modifying LaSO to de-
lay updates until the end of each instance
we obtain significant improvements over
the baseline. Our model obtains the best
results to date on recent shared task data
for Arabic, Chinese, and English.
1 Introduction
This paper studies and extends previous work us-
ing the structured perceptron (Collins, 2002) for
complex NLP tasks. We show that for the task of
coreference resolution the straightforward combi-
nation of beam search and early update (Collins
and Roark, 2004) falls short of more limited fea-
ture sets that allow for exact search. This contrasts
with previous work on, e.g., syntactic parsing
(Collins and Roark, 2004; Huang, 2008; Zhang
and Clark, 2008) and linearization (Bohnet et
al., 2011), and even simpler structured prediction
problems, where early updates are not even nec-
essary, such as part-of-speech tagging (Collins,
2002) and named entity recognition (Ratinov and
Roth, 2009).
The main reason why early updates underper-
form in our setting is that the task is too difficult
and that the learning algorithm is not able to profit
from all training data. Put another way, early up-
dates happen too early, and the learning algorithm
rarely reaches the end of the instances as it halts,
updates, and moves on to the next instance.
An alternative would be to continue decod-
ing the same instance after the early updates,
which is equivalent to Learning as Search Opti-
mization (LaSO; Daum?e III and Marcu (2005b)).
The learning task we are tackling is however
further complicated since the target structure is
under-determined by the gold standard annotation.
Coreferent mentions in a document are usually an-
notated as sets of mentions, where all mentions in
a set are coreferent. We adopt the recently pop-
ularized approach of inducing a latent structure
within these sets (Fernandes et al, 2012; Chang et
al., 2013; Durrett and Klein, 2013). This approach
provides a powerful boost to the performance of
coreference resolvers, but we find that it does not
combine well with the LaSO learning strategy. We
therefore propose a modification to LaSO, which
delays updates until after each instance. The com-
bination of this modification with non-local fea-
tures leads to further improvements in the cluster-
ing accuracy, as we show in evaluation results on
all languages from the CoNLL 2012 Shared Task ?
Arabic, Chinese, and English. We obtain the best
results to date on these data sets.
1
2 Background
Coreference resolution is the task of grouping re-
ferring expressions (or mentions) in a text into dis-
joint clusters such that all mentions in a cluster
refer to the same entity. An example is given in
Figure 1 below, where mentions from two clusters
are marked with brackets:
[Drug Emporium Inc.]
a
1
said [Gary Wilber]
b
1
was
named CEO of [this drugstore chain]
a
2
. [He]
b
2
suc-
ceeds his father, Philip T. Wilber, who founded [the
company]
a
3
and remains chairman. Robert E. Lyons
III, who headed the [company]
a
4
?s Philadelphia re-
gion, was appointed president and chief operating offi-
cer, succeeding [Gary Wilber]
b
3
.
Figure 1: An excerpt of a document with the men-
tions from two clusters marked.
1
Our system is available at http://www.ims.
uni-stuttgart.de/
?
anders/coref.html
47
In recent years much work on coreference res-
olution has been devoted to increasing the ex-
pressivity of the classical mention-pair model, in
which each coreference classification decision is
limited to information about two mentions that
make up a pair. This shortcoming has been ad-
dressed by entity-mention models, which relate a
candidate mention to the full cluster of mentions
predicted to be coreferent so far (for more discus-
sion on the model types, see, e.g., (Ng, 2010)).
Nevertheless, the two best systems in the lat-
est CoNLL Shared Task on coreference resolu-
tion (Pradhan et al, 2012) were both variants of
the mention-pair model. While the second best
system (Bj?orkelund and Farkas, 2012) followed
the widely used baseline of Soon et al (2001), the
winning system (Fernandes et al, 2012) proposed
the use of a tree representation.
The tree-based model of Fernandes et al (2012)
construes the representation of coreference clus-
ters as a rooted tree. Figure 2 displays an example
tree over the clusters from Figure 1. Every men-
tion corresponds to a node in the tree, and arcs be-
tween mentions indicate that they are coreferent.
The tree additionally has a dummy root node. Ev-
ery subtree under the root node corresponds to a
cluster of coreferent mentions.
Since coreference training data is typically not
annotated with trees, Fernandes et al (2012) pro-
posed the use of latent trees that are induced dur-
ing the training phase of a coreference resolver.
The latent tree provides more meaningful an-
tecedents for training.
2
For instance, the popular
pair-wise instance creation method suggested by
Soon et al (2001) assumes non-branching trees,
where the antecedent of every mention is its lin-
ear predecessor (i.e., he
b
2
is the antecedent of
Gary Wilber
b
3
). Comparing the two alternative
antecedents of Gary Wilber
b
3
, the tree in Fig-
ure 2 provides a more reliable basis for training a
coreference resolver, as the two mentions of Gary
Wilber are both proper names and have an exact
string match.
3 Representation and Learning
LetM = {m
0
,m
1
, ...,m
n
} denote the set of men-
tions in a document, including the artificial root
mention (denoted by m
0
). We assume that the
2
We follow standard practice and overload the terms
anaphor and antecedent to be any type of mention, i.e., names
as well as pronouns. An antecedent is simply the mention to
the left of the anaphor.
Drug Emporium Inc.
a
1
the company
a
3
this drugstore chain
a
2
Gary Wilber
b
1
He
b
2
Gary Wilber
b
3
root
company
a
4
Figure 2: A tree representation of Figure 1.
mentions are ordered ascendingly with respect to
the linear order of the document, where the docu-
ment root precedes all other mentions.
3
For each
mention m
j
, let A
j
denote the set of potential an-
tecedents. That is, the set of all mentions that
precede m
j
according to the linear order includ-
ing the root node, or, A
j
= {m
i
| i < j}. Fi-
nally, let A denote the set of all antecedent sets
{A
0
, A
1
, ..., A
n
}.
In the tree model, each mention corresponds to
a node, and an antecedent-anaphor pair ?a
i
,m
i
?,
where a
i
? A
i
, corresponds to a directed edge (or
arc) pointing from antecedent to anaphor.
The score of an arc ?a
i
,m
i
? is defined as
the scalar product between a weight vector w
and a feature vector ?(?a
i
,m
i
?), where ? is
a feature extraction function over an arc (thus
extracting features from the antecedent and the
anaphor). The score of a coreference tree y =
{?a
1
,m
1
?, ?a
2
,m
2
?, ..., ?a
n
,m
n
?} is defined as
the sum of the scores of all the mention pairs:
score(?a
i
,m
i
?) = w ? ?(?a
i
,m
i
?) (1)
score(y) =
?
?a
i
,m
i
??y
score(?a
i
,m
i
?)
The objective is to find the output y? that maxi-
mizes the scoring function:
y? = arg max
y?Y(A)
score(y) (2)
where Y(A) denotes the set of possible trees given
the antecedent sets A. By treating the mentions as
nodes in a directed graph and assigning scores to
the arcs according to (1), Fernandes et al (2012)
solved the search problem using the Chu-Liu-
Edmonds (CLE) algorithm (Chu and Liu, 1965;
3
We impose a total order on mentions. In case of nested
mentions, the mention that begins first is assumed to precede
the embedded one. If two mentions begin at the same token,
the longer one is taken to precede the shorter one.
48
Edmonds, 1967), which is a maximum spanning
tree algorithm that finds the optimal tree over a
connected directed graph. CLE, however, has the
drawback that the scores of the arcs must remain
fixed and can not change depending on other arcs
and it is not clear how to include non-local features
in a CLE decoder.
3.1 Online learning
We find the weight vector w by online learning us-
ing a variant of the structured perceptron (Collins,
2002). Specifically, we use the passive-aggressive
(PA) algorithm (Crammer et al, 2006), since we
found that this performed slightly better in prelim-
inary experiments.
4
The structured perceptron iterates over train-
ing instances ?x
i
, y
i
?, where x
i
are inputs and y
i
are outputs. For each instance it uses the current
weight vector w to make a prediction y?
i
given the
input x
i
. If the prediction is incorrect, the weight
vector is updated in favor of the correct structure.
Otherwise the weight vector is left untouched. In
our setting inputs x
i
correspond to documents and
outputs y
i
are trees over mentions in a document.
The training data is, however, not annotated with
trees, but only with clusters of mentions. That is,
the y
i
?s are not defined a priori.
3.2 Latent antecedents
In order to have a tree structure to update against,
we use the current weight vector and apply the
decoder to a constrained antecedent set and ob-
tain a latent tree over the mentions in a docu-
ment, where each mention is assigned a single cor-
rect antecedent (Fernandes et al, 2012). We con-
strain the antecedent sets such that only trees that
correspond to the correct clustering can be built.
Specifically, let
?
A
j
denote the set of correct an-
tecedents for a mention m
j
, or
?
A
j
=
{
{m
0
} if m
j
has no correct antecedent
{a
i
| COREF(a
i
,m
j
), a
i
? A
j
} otherwise
that is, if mention m
j
is non-referential or the first
mention of its cluster,
?
A
j
contains only the docu-
ment root. Otherwise it is the set of all mentions
to the left that belong to the same cluster as m
j
.
Analogously to A, let
?
A denote the set of con-
strained antecedent sets. The latent tree y? needed
4
We also implement the feature mapping function ? as
a hash kernel (Bohnet, 2010) and apply averaging (Collins,
2002), though for brevity we omit this from the pseudocode.
for updates is then defined to be the optimal tree
over Y(
?
A), subject to the current weight vector:
y? = arg max
y?Y(
?
A)
score(y)
The intuition behind the latent tree is that during
online learning, the weight vector will start favor-
ing latent trees that are easier to learn (such as the
one in Figure 2).
Algorithm 1 PA algorithm with latent trees
Input: Training data D, number of iterations T
Output: Weight vector w
1: w =
??
0
2: for t ? 1..T do
3: for ?M
i
,A
i
,
?
A
i
? ? D do
4: y?
i
= arg max
Y(A)
score(y) . Predict
5: if ?CORRECT(y?
i
) then
6: y?
i
= arg max
Y(
?
A)
score(y) . Latent tree
7: ? = ?(y?
i
)? ?(y?
i
)
8: ? =
??w+LOSS(y?
i
)
???
2
. PA weight
9: w = w + ?? . PA update
10: return w
Algorithm 1 shows pseudocode for the learn-
ing algorithm, which we will refer to as the base-
line learning algorithm. Instead of looping over
pairs ?x, y? of documents and trees, it loops over
triples ?M,A,
?
A? that comprise the set of men-
tions M and the two sets of antecedent candidates
(line 3). Moreover, rather than checking that the
tree is identical to the latent tree, it only requires
the tree to correctly encode the gold clustering
(line 5). The update that occurs in lines 7-9 is the
passive-aggressive update. A loss function LOSS
that quantifies the error in the prediction is used
to compute a scalar ? that controls how much the
weights are moved in each update. If ? is set to 1,
the update reduces to the standard structured per-
ceptron update. The loss function can be an arbi-
trarily complex function that returns a numerical
value of how bad the prediction is. In the sim-
plest case, Hamming loss can be used, i.e., for
each incorrect arc add 1. We follow Fernandes
et al (2012) and penalize erroneous root attach-
ments, i.e., mentions that erroneously get the root
node as their antecedent, with a loss of 1.5. For all
other arcs we use Hamming loss.
4 Incremental Search
We now show that the search problem in (2) can
equivalently be solved by the more intuitive best-
first decoder (Ng and Cardie, 2002), rather than
using the CLE decoder. The best-first decoder
49
works incrementally by making a left-to-right pass
over the mentions, selecting for each mention the
highest scoring antecedent.
The key aspect that makes the best-first decoder
equivalent to the CLE decoder is that all arcs point
from left to right, both in this paper and in the work
of Fernandes et al (2012). We sketch a proof that
this decoder also returns the highest scoring tree.
First, note that this algorithm indeed returns a
tree. This can be shown by assuming the opposite,
in which case the tree has to have a cycle. Then
there must be a mention that has its antecedent to
the right. Though this is not possible since all arcs
point from left to right.
Second, this tree is the highest scoring tree.
Again, assume the contrary, i.e., that there is a
higher scoring tree in Y(A). This implies that for
some mention there is a higher scoring antecedent
than the one selected by the decoder. This contra-
dicts the fact that the best-first decoder selects the
highest scoring antecedent for each mention.
5
5 Introducing Non-local Features
Since the best-first decoder makes a left-to-right
pass, it is possible to extract features on the partial
structure on the left. Such non-local features are
able to capture information beyond that of a men-
tion and its potential antecedent, e.g., the size of
a partially built cluster, or features extracted from
the antecedent of the antecedent.
When only local features are used, greedy
search (either with CLE or the best-first decoder)
suffices to find the highest scoring tree. That is,
greedy search provides an exact solution to equa-
tion 2. Non-local features, however, render the ex-
act search problem intractable. This is because
with non-local features, locally suboptimal (i.e.,
non-greedy) antecedents for some mentions may
lead to a higher total score over a whole document.
In order to keep some options around during
search, we extend the best-first decoder with beam
search. Beam search works incrementally by
keeping an agenda of state items. At each step,
all items on the agenda are expanded. The subset
of size k (the beam size) of the highest scoring ex-
pansions are retained and put back into the agenda
for the next step. The feature extraction function ?
5
In case there are multiple maximum spanning trees, the
best-first decoder will return one of them. This also holds for
the CLE algorithm. With proper definitions, the proof can be
constructed to show that both search algorithms return trees
belonging to the set of maximum spanning trees over a graph.
is also extended such that it also receives the cur-
rent state s as an argument: ?(?m
i
,m
j
?, s). The
state encodes the previous decisions and enables ?
to extract features from the partial tree on the left.
We now outline three different ways of learning
the weight vector w with non-local features.
5.1 Early updates
The beam search decoder can be plugged into the
training algorithm, replacing the calls to arg max.
Since state items leading to the best tree may
be pruned from the agenda before the decoder
reaches the end of the document, the introduc-
tion of non-local features may cause the decoder
to return a non-optimal tree. This is problem-
atic as it might cause updates although the correct
tree has a higher score than the predicted one. It
has previously been observed (Huang et al, 2012)
that substantial gains can be made by applying an
early update strategy (Collins and Roark, 2004):
if the correct item is pruned before reaching the
end of the document, then stop and update.
While beam search and early updates have been
successfully applied to other NLP applications,
our task differs in two important aspects: First,
coreference resolution is a much more difficult
task, which relies on more (world) knowledge than
what is available in the training data. In other
words, it is unlikely that we can devise a feature
set that is informative enough to allow the weight
vector to converge towards a solution that lets the
learning algorithm see the entire documents dur-
ing training, at least in the situation when no ex-
ternal knowledge sources are used.
Second, our gold structure is not known but
is induced latently, and may vary from iteration
to iteration. With non-local features this is trou-
blesome since the best latent tree of a complete
document may not necessarily coincide with the
best partial tree at some intermediate mentionm
j
,
j < n, i.e., a mention before the last in a docu-
ment. We therefore also apply beam search to find
the latent tree to have a partial gold structure for
every mention in a document.
Algorithm 2 shows pseudocode for the beam
search and early update training procedure. The
algorithm maintains two parallel agendas, one for
gold items and one for predicted items. At ev-
ery mention, both agendas are expanded and thus
cover the same set of mentions. Then the predicted
agenda is checked to see if it contains any correct
50
Algorithm 2 Beam search and early update
Input: Data set D, epochs T , beam size k
Output: weight vector w
1: w =
??
0
2: for t ? 1..T do
3: for ?M
i
,A
i
,
?
A
i
? ? D do
4: Agenda
G
= {}
5: Agenda
P
= {}
6: for j ? 1..n do
7: Agenda
G
= EXPAND(Agenda
G
,
?
A
j
,m
j
, k)
8: Agenda
P
= EXPAND(Agenda
P
, A
j
,m
j
, k)
9: if ?CONTAINSCORRECT(Agenda
P
) then
10: y? = EXTRACTBEST(Agenda
G
)
11: y? = EXTRACTBEST(Agenda
P
)
12: update . PA update
13: GOTO 3 . Skip and move to next instance
14: y? = EXTRACTBEST(Agenda
P
)
15: if ?CORRECT(y?) then
16: y? = EXTRACTBEST(Agenda
G
)
17: update . PA update
item. If there is no correct item in the predicted
agenda, search is halted and an update is made
against the best item from the gold agenda. The
algorithm then moves on to the next document. If
the end of a document is reached, the top scoring
predicted item is checked for correctness. If it is
not, an update is made against the best gold item.
A drawback of early updates is that the remain-
der of the document is skipped when an early up-
date is applied, effectively discarding some train-
ing data.
6
An alternative strategy that makes bet-
ter use of the training data is to apply the max-
violation procedure suggested by Huang et al
(2012). However, since our gold trees change from
iteration to iteration, and even inside of a single
document, it is not entirely clear with respect to
what gold tree the maximum violation should be
computed. Initial experiments with max-violation
updates indicated that they did not improve much
over early updates, and also had a tendency to only
consider a smaller portion of the training data.
5.2 LaSO
To make full use of the training data we imple-
mented Learning as Search Optimization (LaSO;
Daum?e III and Marcu, 2005b). It is very similar
to early updates, but differs in one crucial respect:
When an early update is made, search is continued
rather than aborted. Thus the learning algorithm
always reaches the end of a document, avoiding
the problem that early updates discard parts of the
training data.
6
In fact, after 50 iterations about 70% of the mentions in
the training data are still being ignored due to early updates.
Correct items are computed the same way as
with early updates, where an agenda of gold items
is maintained in parallel. When search is resumed
after an intermediate LaSO update, the prediction
agenda is re-seeded with gold items (i.e., items
that are all correct). This is necessary since the
update influences what the partial gold structure
looks like, and the gold agenda therefore needs to
be recreated from the beginning of the document.
Specifically, after each intermediate LaSO update,
the gold agenda is expanded repeatedly from the
beginning of the document to the point where the
update was made, and is then copied over to seed
the prediction agenda. In terms of pseudocode,
this is accomplished by replacing lines 12 and 13
in Algorithm 2 with the following:
12: update . PA update
13: Agenda
G
= {}
14: for m
i
? {m
1
, ...,m
j
} . Recreate gold agenda
15: Agenda
G
= EXPAND(Agenda
G
,
?
A
i
,m
i
, k)
16: Agenda
P
= COPY(Agenda
G
)
17: GOTO 6 . Continue
5.3 Delayed LaSO updates
When we applied LaSO, we noticed that it per-
formed worse than the baseline learning algorithm
when only using local features. We believe that the
reason is that updates are made in the middle of
documents which means that lexical forms of an-
tecedents are ?fresh in memory? of the weight vec-
tor. This results in fewer mistakes during training
and leads to fewer updates. While this feedback
makes it easier during training, such feedback is
not available during test time, and the LaSO learn-
ing setting therefore mimics the testing setting to
a lesser extent.
We also found that LaSO updates change the
shape of the latent tree and that the average dis-
tance between mentions connected by an arc in-
creased. This problem can also be attributed to
how lexical items are fresh in memory. Such trees
tend to deviate from the intuition that the latent
trees are easier to learn. They also render distance-
based features (which are standard practice and
generally rather useful) less powerful, as distance
in sentences or mentions becomes less of a reliable
indicator for coreference.
To cope with this problem, we devised the
delayed LaSO update, which differs from LaSO
only in the respect that it postpones the actual up-
dates until the end of a document. This is accom-
plished by summing the distance vectors ? at ev-
ery point where LaSO would make an update. At
51
Algorithm 3 Delayed LaSO update
Input: Data set D, iterations T , beam size k
Output: weight vector w
1: w =
??
0
2: for t ? 1..T do
3: for ?M
i
,A
i
,
?
A
i
? ? D do
4: Agenda
G
= {}
5: Agenda
P
= {}
6: ?
acc
=
??
0
7: loss
acc
= 0
8: for j ? 1..n do
9: Agenda
G
= EXPAND(Agenda
G
,
?
A
j
,m
j
, k)
10: Agenda
P
= EXPAND(Agenda
P
, A
j
,m
j
, k)
11: if ?CONTAINSCORRECT(Agenda
P
) then
12: y? = EXTRACTBEST(Agenda
G
)
13: y? = EXTRACTBEST(Agenda
P
)
14: ?
acc
= ?
acc
+ ?(y?)? ?(y?)
15: loss
acc
= loss
acc
+ LOSS(y?)
16: Agenda
P
= Agenda
G
17: y? = EXTRACTBEST(Agenda
P
)
18: if ?CORRECT(y?) then
19: y? = EXTRACTBEST(Agenda
G
)
20: ?
acc
= ?
acc
+ ?(y?)? ?(y?)
21: loss
acc
= loss
acc
+ LOSS(y?)
22: if ?
acc
6=
??
0 then
23: update w.r.t. ?
acc
and loss
acc
the end of a document, an update is made with re-
spect to the sum of all ??s. Similarly, a running
sum of the partial loss is maintained within a doc-
ument. Since the PA update only depends on the
distance vector ? and the loss, it can be applied
with respect to these sums at the end of the doc-
ument. When only local features are used, this
update is equivalent to the updates in the baseline
learning algorithm. This follows because greedy
search finds the optimal tree when only local fea-
tures are used. Similarly, using only local features,
the beam-based best-first decoder will also return
the optimal tree. Algorithm 3 shows the pseu-
docode for the delayed LaSO learning algorithm.
6 Features
In this section we briefly outline the type of fea-
tures we use. The feature sets are customized for
each language. As a baseline we use the features
from Bj?orkelund and Farkas (2012), who ranked
second in the 2012 CoNLL shared task and is pub-
licly available. The exact definitions and feature
sets that we use are available as part of the down-
load package of our system.
6.1 Local features
Basic features that can be extracted on one or
both mentions in a pair include (among oth-
ers): Mention type, which is either root, pro-
noun, name, or common; Distance features, e.g.,
the distance in sentences or mentions; Rule-based
features, e.g., StringMatch or SubStringMatch;
Syntax-based features, e.g., category labels or
paths in the syntax tree; Lexical features, e.g., the
head word of a mention or the last word of a men-
tion.
In order to have a strong local baseline, we ap-
plied greedy forward/backward feature selection
on the training data using a large set of local fea-
ture templates. Specifically, the training set of
each language was split into two parts where 75%
was used for training, and 25% for testing. Feature
templates were incrementally added or removed
in order to optimize the mean of MUC, B
3
, and
CEAF
e
(i.e., the CoNLL average).
6.2 Non-local Features
We experimented with non-local features drawn
from previous work on entity-mention mod-
els (Luo et al, 2004; Rahman and Ng, 2009), how-
ever they did not improve performance in prelimi-
nary experiments. The one exception is the size of
a cluster (Culotta et al, 2007). Additional features
we use are
Shape encodes the linear ?shape? of a cluster in
terms of mention type. For instance, the clusters
representing Gary Wilber and Drug Emporium
Inc. from the example in Figure 1, would be repre-
sented as RNPN and RNCCC, respectively. Where
R, N, P, and C denote the root node, names, pro-
nouns, and common noun phrases, respectively.
Local syntactic context is inspired by the Entity
Grid (Barzilay and Lapata, 2008), where the ba-
sic assumption is that references to an entity fol-
low particular syntactic patterns. For instance, an
entity may be introduced as an object in one sen-
tence, whereas in subsequent sentences it is re-
ferred to in subject position. Grammatical func-
tions are approximated by the path in the syntax
tree from a mention to its closest S node. The par-
tial paths of a mention and its linear predecessor,
given the cluster of the current antecedent, informs
the model about the local syntactic context.
Cluster start distance denotes the distance in
mentions from the beginning of the document
where the cluster of the antecedent in considera-
tion begins.
Additionally, the non-local model also has ac-
cess to the basic properties of other mentions in
the partial tree structure, such as head words. The
52
non-local features were selected with the same
greedy forward strategy as the local features, start-
ing from the optimized local feature sets.
7 Experimental Setup
We apply our model to the CoNLL 2012 Shared
Task data, which includes a training, develop-
ment, and test set split for three languages: Ara-
bic, Chinese and English. We follow the closed
track setting where systems may only be trained
on the provided training data, with the exception
of the English gender and number data compiled
by Bergsma and Lin (2006). We use automatically
extracted mentions using the same mention extrac-
tion procedure as Bj?orkelund and Farkas (2012).
We evaluate our system using the CoNLL 2012
scorer, which computes several coreference met-
rics: MUC (Vilain et al, 1995), B
3
(Bagga and
Baldwin, 1998), and CEAF
e
and CEAF
m
(Luo,
2005). We also report the CoNLL average (also
known as MELA; Denis and Baldridge (2009)),
i.e., the arithmetic mean of MUC, B
3
, and CEAF
e
.
It should be noted that for B
3
and the CEAF met-
rics, multiple ways of handling twinless mentions
7
have been proposed (Rahman and Ng, 2009; Stoy-
anov et al, 2009). We use the most recent ver-
sion of the CoNLL scorer (version 7), which im-
plements the original definitions of these metrics.
8
Our system is evaluated on the version of the
data with automatic preprocessing information
(e.g., predicted parse trees). Unless otherwise
stated we use 25 iterations of perceptron training
and a beam size of 20. We did not attempt to tune
either of these parameters. We experiment with
two feature sets for each language: the optimized
local feature sets (denoted local), and the opti-
mized local feature sets extended with non-local
features (denoted non-local).
8 Results
Learning strategies. We begin by looking at the
different learning strategies. Since early updates
do not always make use of the complete docu-
ments during training, it can be expected that it
will require either a very wide beam or more iter-
ations to get up to par with the baseline learning
algorithm. Figure 3 shows the CoNLL average on
7
i.e., mentions that appear in the prediction but not in
gold, or the other way around
8
Available at http://conll.cemantix.org/
2012/software.html
54
56
58
60
62
64
0 10 20 30 40 50
CoN
LL 
avg
.
Iterations
BaselineEarly (local), k=20Early (local), k=100Early (non-local), k=20Early (non-local), k=100
Figure 3: Comparing early update training with
the baseline training algorithm.
the English development set as a function of num-
ber of training iterations with two different beam
sizes, 20 and 100, over the local and non-local fea-
ture sets. The figure shows that even after 50 itera-
tions, early update falls short of the baseline, even
when the early update system has access to more
informative non-local features.
9
In Figure 4 we compare early update with LaSO
and delayed LaSO on the English development set.
The left half uses the local feature set, and the right
the extended non-local feature set. Recall that with
only local features, delayed LaSO is equivalent to
the baseline learning algorithm. As before, early
update is considerably worse than other learning
strategies. We also see that delayed LaSO out-
performs LaSO, both with and without non-local
features. Note that plain LaSO with non-local fea-
tures only barely outperforms the delayed LaSO
with only local features (i.e., the baseline), which
indicates that only delayed LaSO is able to fully
leverage non-local features. From these results we
conclude that we are better off when the learning
algorithm handles one document at a time, instead
of getting feedback within documents.
Local vs. Non-local feature sets. Table 1 dis-
plays the differences in F-measures and CoNLL
average between the local and non-local systems
when applied to the development sets for each lan-
guage. All metrics improve when more informa-
tive non-local features are added to the local fea-
ture set. Arabic and English show considerable
improvements, and the CoNLL average increases
9
Although the Early systems still seem to show slight in-
creases after 50 iterations, it needs a considerable number of
iterations to catch up with the baseline ? after 100 iterations
the best early system is still more than half a point behind the
baseline.
53
58
59
60
61
62
63
64
65
Local Non-local
CoN
LL 
avg
.
EarlyLaSODelayed LaSO
Figure 4: Comparison of learning algorithms eval-
uated on the English development set.
MUC B
3
CEAF
m
CEAF
e
CoNLL
Arabic
local 47.33 42.51 49.71 46.49 45.44
non-local 49.31 43.52 50.96 47.18 46.67
Chinese
local 65.84 57.94 62.23 57.05 60.27
non-local 66.4 57.99 62.37 57.12 60.5
English
local 69.95 58.7 62.91 56.03 61.56
non-local 70.74 60.03 65.01 56.8 62.52
Table 1: Comparison of local and non-local fea-
ture sets on the development sets.
about one point. For Chinese the gains are gen-
erally not as pronounced, though the MUC metric
goes up by more than half a point.
Final results. In Table 2 we compare the re-
sults of the non-local system (This paper) to the
best results from the CoNLL 2012 Shared Task.
10
Specifically, this includes Fernandes et al?s (2012)
system for Arabic and English (denoted Fernan-
des), and Chen and Ng?s (2012) system for Chi-
nese (denoted C&N). For English we also com-
pare it to the Berkeley system (Durrett and Klein,
2013), which, to our knowledge, is the best pub-
licly available system for English coreference res-
olution (denoted D&K). As a general baseline, we
also include Bj?orkelund and Farkas? (2012) sys-
tem (denoted B&F), which was the second best
system in the shared task. For almost all met-
rics our system is significantly better than the best
competitor. For a few metrics the best competitor
outperforms our results for either precision or re-
call, but in terms of F-measures and the CoNLL
average our system is the best for all languages.
10
Thanks to Sameer Pradhan for providing us with the out-
puts of the other systems for significance testing.
9 Related Work
On the machine learning side Collins and Roark?s
(2004) work on the early update constitutes our
starting point. The LaSO framework was intro-
duced by Daum?e III and Marcu (2005b), but has,
to our knowledge, only been applied to the related
task of entity detection and tracking (Daum?e III
and Marcu, 2005a). The theoretical motivation for
early updates was only recently explained rigor-
ously (Huang et al, 2012). The delayed LaSO
update that we propose decomposes the predic-
tion task of a complex structure into a number of
subproblems, each of which guarantee violation,
using Huang et al?s (2012) terminology. We be-
lieve this is an interesting novelty, as it leverages
the complete structures for every training instance
during every iteration, and expect it to be applica-
ble also to other structured prediction tasks.
Our approach also resembles imitation learning
techniques such as SEARN (Daum?e III et al, 2009)
and DAGGER (Ross et al, 2011), where the search
problem is reduced to a sequence of classification
steps that guide the search algorithm through the
search space. These frameworks, however, rely on
the notion of an expert policy which provides an
optimal decision at each point during search. In
our context that would require antecedents for ev-
ery mention to be given a priori, rather than using
latent antecedents as we do.
Perceptrons for coreference. The perceptron
has previously been used to train coreference re-
solvers either by casting the problem as a binary
classification problem that considers pairs of men-
tions in isolation (Bengtson and Roth, 2008; Stoy-
anov et al, 2009; Chang et al, 2012, inter alia) or
in the structured manner, where a clustering for an
entire document is predicted in one go (Fernandes
et al, 2012). However, none of these works use
non-local features. Stoyanov and Eisner (2012)
train an Easy-First coreference system with the
perceptron to learn a sequence of join operations
between arbitrary mentions in a document and ac-
cesses non-local features through previous merge
operations in later stages. Culotta et al (2007) also
apply online learning in a first-order logic frame-
work that enables non-local features, though using
a greedy search algorithm.
Latent antecedents. The use of latent an-
tecedents goes back to the work of Yu and
Joachims (2009), although the idea of determining
54
MUC B
3
CEAF
m
CEAF
e
CoNLL
Rec Prec F
1
Rec Prec F
1
Rec Prec F
1
Rec Prec F
1
avg.
Arabic
B&F 43.9 52.51 47.82 35.7 49.77 41.58 43.8 50.03 46.71 40.45 41.86 41.15 43.51
Fernandes 43.63 49.69 46.46 38.39 47.7 42.54 47.6 50.85 49.17 48.16 45.03 46.54 45.18
This paper 47.53 53.3 50.25 44.14 49.34 46.6 50.94 55.19 52.98 49.2 49.45 49.33 48.72
Chinese
B&F 58.72 58.49 58.61 49.17 53.2 51.11 56.68 51.86 54.14 55.36 41.8 47.63 52.45
C&N 59.92 64.69 62.21 51.76 60.26 55.69 59.58 60.45 60.02 58.84 51.61 54.99 57.63
This paper 62.57 69.39 65.8 53.87 61.64 57.49 58.75 64.76 61.61 54.65 59.33 56.89 60.06
English
B&F 65.23 70.1 67.58 49.51 60.69 54.47 56.93 59.51 58.19 51.34 49.14 59.21 57.42
Fernandes 65.83 75.91 70.51 51.55 65.19 57.58 57.48 65.93 61.42 50.82 57.28 53.86 60.65
D&K 66.58 74.94 70.51 53.2 64.56 58.33 59.19 66.23 62.51 52.9 58.06 55.36 61.4
This paper 67.46 74.3 70.72 54.96 62.71 58.58 60.33 66.92 63.45 52.27 59.4 55.61 61.63
Table 2: Comparison with other systems on the test sets. Bold numbers indicate significance at the
p < 0.05 level between the best and the second best systems (according to the CoNLL average) using
a Wilcoxon signed rank sum test. We refrain from significance tests on the CoNLL average, as it is an
average over other F-measures.
meaningful antecedents for mentions can be traced
back to Ng and Cardie (2002) who used a rule-
based approach. Latent antecedents have recently
gained popularity and were used by two systems in
the CoNLL 2012 Shared Task, including the win-
ning system (Fernandes et al, 2012; Chang et al,
2012). Durrett and Klein (2013) present a corefer-
ence resolver with latent antecedents that predicts
clusterings over entire documents and fit a log-
linear model with a custom task-specific loss func-
tion using AdaGrad (Duchi et al, 2011). Chang
et al (2013) use a max-margin approach to learn
a pairwise model and rely on stochastic gradient
descent to circumvent the costly operation of de-
coding the entire training set in order to compute
the gradients and the latent antecedents. None of
the aforementioned works use non-local features
in their models, however.
Entity-mention models. Entity-mention mod-
els that compare a single mention to a (partial)
cluster have been studied extensively and several
works have evaluated non-local entity-level fea-
tures (Luo et al, 2004; Yang et al, 2008; Rah-
man and Ng, 2009). Luo et al (2004) also apply
beam search at test time, but use a static assign-
ment of antecedents and learns log-linear model
using batch learning. Moreover, these works al-
ter the basic feature definitions from their pair-
wise models when introducing entity-level fea-
tures. This contrasts with our work, as our
mention-pair model simply constitutes a special
case of the non-local system.
10 Conclusion
We presented experiments with a coreference re-
solver that leverages non-local features to improve
its performance. The application of non-local fea-
tures requires the use of an approximate search al-
gorithm to keep the problem tractable. We eval-
uated standard perceptron learning techniques for
this setting both using early updates and LaSO. We
found that the early update strategy is considerably
worse than a local baseline, as it is unable to ex-
ploit all training data. LaSO resolves this issue by
giving feedback within documents, but still under-
performs compared to the baseline as it distorts the
choice of latent antecedents.
We introduced a modification to LaSO, where
updates are delayed until each document is pro-
cessed. In the special case where only local fea-
tures are used, this method coincides with stan-
dard structured perceptron learning that uses exact
search. Moreover, it is also able to profit from non-
local features resulting in improved performance.
We evaluated our system on all three languages
from the CoNLL 2012 Shared Task and present
the best results to date on these data sets.
Acknowledgments
We are grateful to the anonymous reviewers as
well as Christian Scheible and Wolfgang Seeker
for comments on earlier versions of this paper.
This research has been funded by the DFG via
SFB 732, project D8.
55
References
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In In The First Interna-
tional Conference on Language Resources and Eval-
uation Workshop on Linguistics Coreference, pages
563?566.
Regina Barzilay and Mirella Lapata. 2008. Modeling
local coherence: An entity-based approach. Compu-
tational Linguistics, 34(1):1?34.
Eric Bengtson and Dan Roth. 2008. Understand-
ing the value of features for coreference resolution.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
294?303, Honolulu, Hawaii, October. Association
for Computational Linguistics.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of
the 21st International Conference on Computational
Linguistics and 44th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 33?40,
Sydney, Australia, July. Association for Computa-
tional Linguistics.
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Joint Conference on EMNLP and
CoNLL - Shared Task, pages 49?55, Jeju Island, Ko-
rea, July. Association for Computational Linguistics.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo
Wanner. 2011. <stumaba >: From deep represen-
tation to surface. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 232?235,
Nancy, France, September. Association for Compu-
tational Linguistics.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional Linguistics (Coling 2010), pages 89?97, Bei-
jing, China, August.
Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,
Mark Sammons, and Dan Roth. 2012. Illinois-
coref: The ui system in the conll-2012 shared task.
In Joint Conference on EMNLP and CoNLL - Shared
Task, pages 113?117, Jeju Island, Korea, July. Asso-
ciation for Computational Linguistics.
Kai-Wei Chang, Rajhans Samdani, and Dan Roth.
2013. A constrained latent variable model for coref-
erence resolution. In Proceedings of the 2013 Con-
ference on Empirical Methods in Natural Language
Processing, pages 601?612, Seattle, Washington,
USA, October. Association for Computational Lin-
guistics.
Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Joint Conference on
EMNLP and CoNLL - Shared Task, pages 56?63,
Jeju Island, Korea, July. Association for Computa-
tional Linguistics.
Yoeng-jin Chu and Tseng-hong Liu. 1965. On the
shortest aborescence of a directed graph. Science
Sinica, 14:1396?1400.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Pro-
ceedings of the 42nd Meeting of the Association for
Computational Linguistics (ACL?04), Main Volume,
pages 111?118, Barcelona, Spain, July.
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of the 2002 Conference on Empirical Methods in
Natural Language Processing, pages 1?8. Associ-
ation for Computational Linguistics, July.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive?aggressive algorithms. Journal of Machine
Learning Reseach, 7:551?585, March.
Aron Culotta, Michael Wick, and Andrew McCallum.
2007. First-order probabilistic models for corefer-
ence resolution. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguistics;
Proceedings of the Main Conference, pages 81?88,
Rochester, New York, April. Association for Com-
putational Linguistics.
Hal Daum?e III and Daniel Marcu. 2005a. A large-
scale exploration of effective global features for a
joint entity detection and tracking model. In Pro-
ceedings of Human Language Technology Confer-
ence and Conference on Empirical Methods in Natu-
ral Language Processing, pages 97?104, Vancouver,
British Columbia, Canada, October. Association for
Computational Linguistics.
Hal Daum?e III and Daniel Marcu. 2005b. Learning
as search optimization: approximate large margin
methods for structured prediction. In ICML, pages
169?176.
Hal Daum?e III, John Langford, and Daniel Marcu.
2009. Search-based structured prediction. Machine
Learning, 75(3):297?325.
Pascal Denis and Jason Baldridge. 2009. Global Joint
Models for Coreference Resolution and Named En-
tity Classification. In Procesamiento del Lenguaje
Natural 42, pages 87?96, Barcelona: SEPLN.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. J. Mach. Learn. Res.,
12:2121?2159, July.
Greg Durrett and Dan Klein. 2013. Easy victories and
uphill battles in coreference resolution. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1971?1982,
56
Seattle, Washington, USA, October. Association for
Computational Linguistics.
Jack Edmonds. 1967. Optimum branchings. Jour-
nal of Research of the National Bureau of Standards,
71(B):233?240.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Joint Conference on EMNLP and CoNLL - Shared
Task, pages 41?48, Jeju Island, Korea, July. Associ-
ation for Computational Linguistics.
Liang Huang, Suphan Fayong, and Yang Guo. 2012.
Structured perceptron with inexact search. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
142?151, Montr?eal, Canada, June. Association for
Computational Linguistics.
Liang Huang. 2008. Forest reranking: Discrimina-
tive parsing with non-local features. In Proceedings
of ACL-08: HLT, pages 586?594, Columbus, Ohio,
June. Association for Computational Linguistics.
Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, Nanda
Kambhatla, and Salim Roukos. 2004. A mention-
synchronous coreference resolution algorithm based
on the bell tree. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguis-
tics, pages 135?142, Barcelona, Spain, July.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing, pages 25?32, Vancouver, British Columbia,
Canada, October. Association for Computational
Linguistics.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of 40th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 104?
111, Philadelphia, Pennsylvania, USA, July. Asso-
ciation for Computational Linguistics.
Vincent Ng. 2010. Supervised noun phrase coref-
erence research: The first fifteen years. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 1396?
1411, Uppsala, Sweden, July. Association for Com-
putational Linguistics.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. Conll-
2012 shared task: Modeling multilingual unre-
stricted coreference in ontonotes. In Joint Confer-
ence on EMNLP and CoNLL - Shared Task, pages
1?40, Jeju Island, Korea, July. Association for Com-
putational Linguistics.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, pages 968?977, Singapore,
August. Association for Computational Linguistics.
Lev Ratinov and Dan Roth. 2009. Design chal-
lenges and misconceptions in named entity recog-
nition. In Proceedings of the Thirteenth Confer-
ence on Computational Natural Language Learning
(CoNLL-2009), pages 147?155, Boulder, Colorado,
June. Association for Computational Linguistics.
St?ephane Ross, Geoffrey J. Gordon, and J. Andrew
Bagnell. 2011. A reduction of imitation learning
and structured prediction to no-regret online learn-
ing. In AISTATS, pages 627?635.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Veselin Stoyanov and Jason Eisner. 2012. Easy-first
coreference resolution. In Proceedings of COLING
2012, pages 2519?2534, Mumbai, India, December.
Veselin Stoyanov, Nathan Gilbert, Claire Cardie, and
Ellen Riloff. 2009. Conundrums in noun phrase
coreference resolution: Making sense of the state-
of-the-art. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th In-
ternational Joint Conference on Natural Language
Processing of the AFNLP, pages 656?664, Suntec,
Singapore, August. Association for Computational
Linguistics.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model the-
oretic coreference scoring scheme. In Proceedings
MUC-6, pages 45?52, Columbia, Maryland.
Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan,
Ting Liu, and Sheng Li. 2008. An entity-
mention model for coreference resolution with in-
ductive logic programming. In Proceedings of ACL-
08: HLT, pages 843?851, Columbus, Ohio, June.
Association for Computational Linguistics.
Chun-Nam Yu and T. Joachims. 2009. Learning struc-
tural svms with latent variables. In International
Conference on Machine Learning (ICML).
Yue Zhang and Stephen Clark. 2008. A tale of two
parsers: Investigating and combining graph-based
and transition-based dependency parsing. In Pro-
ceedings of the 2008 Conference on Empirical Meth-
ods in Natural Language Processing, pages 562?
571, Honolulu, Hawaii, October. Association for
Computational Linguistics.
57
Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 7?12,
Baltimore, Maryland USA, June 23-24, 2014.
c
?2014 Association for Computational Linguistics
Visualization, Search, and Error Analysis for Coreference Annotations
Markus G
?
artner Anders Bj
?
orkelund Gregor Thiele Wolfgang Seeker Jonas Kuhn
Institute for Natural Language Processing
University of Stuttgart
{thielegr,seeker,gaertnms,anders,kuhn}@ims.uni-stuttgart.de
Abstract
We present the ICARUS Coreference Ex-
plorer, an interactive tool to browse and
search coreference-annotated data. It can
display coreference annotations as a tree,
as an entity grid, or in a standard text-
based display mode, and lets the user
switch freely between the different modes.
The tool can compare two different an-
notations on the same document, allow-
ing system developers to evaluate errors in
automatic system predictions. It features
a flexible search engine, which enables
the user to graphically construct search
queries over sets of documents annotated
with coreference.
1 Introduction
Coreference resolution is the task of automatically
grouping references to the same real-world entity
in a document into a set. It is an active topic in cur-
rent NLP research and has received considerable
attention in recent years, including the 2011 and
2012 CoNLL shared tasks (Pradhan et al., 2011;
Pradhan et al., 2012).
Coreference relations are commonly repre-
sented by sets of mentions, where all mentions
in one set (or coreference cluster) are considered
coreferent. This type of representation does not
support any internal structure within the clusters.
However, many automatic coreference resolvers
establish links between pairs of mentions which
are subsequently transformed to a cluster by tak-
ing the transitive closure over all links, i.e., placing
all mentions that are directly or transitively classi-
fied as coreferent in one cluster. This is particu-
larly the case for several state-of-the-art resolvers
(Fernandes et al., 2012; Durrett and Klein, 2013;
Bj?orkelund and Kuhn, 2014). These pairwise de-
cisions, which give rise to a clustering, can be ex-
ploited for detailed error analysis and more fine-
grained search queries on data automatically an-
notated for coreference.
We present the ICARUS Coreference Explorer
(ICE), an interactive tool to browse and search
coreference-annotated data. In addition to stan-
dard text-based display modes, ICE features two
other display modes: an entity-grid (Barzilay and
Lapata, 2008) and a tree view, which makes use
of the internal pairwise links within the clusters.
ICE builds on ICARUS (G?artner et al., 2013), a
platform for search and exploration of dependency
treebanks.
1
ICE is geared towards two (typically) distinct
users: The NLP developer who designs corefer-
ence resolution systems can inspect the predic-
tions of his system using the three different dis-
play modes. Moreover, ICE can compare the pre-
dictions of a system to a gold standard annotation,
enabling the developer to inspect system errors in-
teractively. The second potential user is the cor-
pus linguist, who might be interested in brows-
ing or searching a document, or a (large) set of
documents for certain coreference relations. The
built-in search engine of ICARUS now also allows
search queries over sets of documents in order to
meet the needs of this type of user.
2 Data Representation
ICE reads the formats used in the 2011 and 2012
CoNLL shared tasks as well as the SemEval 2010
format (Recasens et al., 2010).
2
Since these for-
mats cannot accommodate pairwise links, an aux-
iliary file with standoff annotation can be pro-
vided, which we call allocation. An allocation is a
list of pairwise links between mentions. Multiple
1
ICE is written in Java and is therefore platform indepen-
dent. It is open source (under GNU GPL) and we provide
both sources and binaries for download on http://www.
ims.uni-stuttgart.de/data/icarus.html
2
These two formats are very similar tabular formats, but
differ slightly in the column representations.
7
allocations can be associated with a single docu-
ment and the user can select one of these for dis-
play or search queries. An allocation can also in-
clude properties on mentions and links. The set
of possible properties is not constrained, and the
user can freely specify properties as a list of key-
value pairs. Properties on mentions may include,
e.g., grammatical gender or number, or informa-
tion status labels. Additionally, a special property
that indicates the head word of a mention can be
provided in an allocation. The head property en-
ables the user to access head words of mentions
for display or search queries.
The motivation for keeping the allocation file
separate from the CoNLL or SemEval files is two-
fold: First, it allows ICE to work without hav-
ing to provide an allocation file, thereby making it
easy to use with the established formats for coref-
erence. The user is still able to introduce addi-
tional structure by the use of the allocation file.
Second, multiple allocation files allow the user to
switch between different allocations while explor-
ing a set of documents. Moreover, as we will see
in Section 3.3, ICE can also compare two different
allocations in order to highlight the differences.
In addition to user-specified allocations, ICE
will always by default provide an internal structure
for the clusters, in which the correct antecedent
of every mention is the closest coreferent mention
with respect to the linear order of the document
(this is equivalent to the training instance creation
heuristic proposed by Soon et al. (2001)). There-
fore, the user is not required to define an allocation
on their own.
3 Display Modes
In this section we describe the entity grid and tree
display modes by means of screenshots. ICE addi-
tionally includes a standard text-based view, sim-
ilar to other coreference visualization tools. The
example document is taken from the CoNLL 2012
development set (Pradhan et al., 2012) and we
use two allocations: (1) the predictions output by
Bj?orkelund and Kuhn (2014) system (predicted)
and (2) a gold allocation that was obtained by
running the same system in a restricted setting,
where only links between coreferent mentions are
allowed (gold). The complete document can be
seen in the lower half of Figure 1.
3.1 Entity grid
Barzilay and Lapata (2008) introduce the entity
grid, a tabular view of entities in a document.
Specifically, rows of the grid correspond to sen-
tences, and columns to entities. The cells of the ta-
ble are used to indicate that an entity is mentioned
in the corresponding sentence. Entity grids pro-
vide a compact view on the distribution of men-
tions in a document and allow the user to see how
the description of an entity changes from mention
to mention.
Figure 1 shows ICE?s entity-grid view for the
example document using the predicted allocation.
When clicking on a cell in the entity grid the im-
mediate textual context of the cell is shown in the
lower pane. In Figure 1, the cell with the blue
background has been clicked, which corresponds
to the two mentions firms from Taiwan and they.
These mentions are thus highlighted in the lower
pane. The user can also right-click on a cell and
jump straight to the tree view, centered around the
same mentions.
3.2 Label Patterns
The information that is displayed in the cells of
the entity grid (and also on the nodes in the tree
view, see Section 3.3) can be fully customized by
the user. The customization is achieved by defin-
ing label patterns. A label pattern is a string that
specifies the format according to which a mention
will be displayed. The pattern can extract infor-
mation on a mention according to three axes: (1)
at the token- level for the full mention, extracting,
e.g., the sequence of surface forms or the part-of-
speech tags of a mention; (2) at the mention- level,
extracting an arbitrary property of a mention as de-
fined in an allocation; (3) token-level information
from the head word of a mention.
Label patterns can be defined interactively
while displaying a document and the three axes are
referenced by dedicated operators. For instance,
the label pattern $form$ extracts the full surface
form of a mention, whereas #form# only extracts
the surface form of the head word of a mention.
All properties defined by the user in the allocation
(see Section 2) are accessible via label patterns.
For example, the allocations we use for Fig-
ure 1 include a number of properties on the
mentions, most of which are internally com-
puted by the coreference system: The TYPE of
a mention, which can take any of the values
8
Figure 1: Entity grid over the predicted clustering in the example document.
{Name, Common, Pronoun} and is inferred from
the part- of-speech tags in the CoNLL file; The
grammatical NUMBER of a mention, which is as-
signed based on the number and gender data com-
piled by Bergsma and Lin (2006) and can take
the values {Sin, Plu, Unknown}. The label pat-
tern for displaying the number property associated
with a mention would be %Number%.
The label pattern used in Figure 1 is defined
as ("$form$" - %Type% - %Number%). This pat-
tern accesses the full surface form of the mentions
($form$), as well as the TYPE (%Type%) and gram-
matical NUMBER (%Number%) properties defined
in the allocation file.
Custom properties and label patterns can be
used for example to display the entity grid in the
form proposed by Barzilay and Lapata (2008): In
the allocation, we assign a coarse-grained gram-
matical function property (denoted GF) to every
mention, where each mention is tagged as either
subject, object, or other (denoted S, O, X, respec-
tively).
3
The label pattern %GF% then displays the
grammatical function of each mention in the entity
grid, as shown in Figure 2.
3.3 Tree view
Pairwise links output by an automatic coreference
system can be treated as arcs in a directed graph.
Linking the first mention of each cluster to an ar-
tificial root node creates a tree structure that en-
codes the entire clustering in a document. This
representation has been used in coreference re-
3
The grammatical function was assigned by converting
the phrase-structure trees in the CoNLL file (which lack
grammatical function information) to Stanford dependencies
(de Marneffe and Manning, 2008), and then extracting the
grammatical function from the head word in each mention.
Figure 2: Example entity grid, using the labels by
Barzilay and Lapata (2008).
solvers (Fernandes et al., 2012; Bj?orkelund and
Kuhn, 2014), but ICE uses it to display links be-
tween mentions introduced by an automatic (pair-
wise) resolver.
Figure 3 shows three examples of the tree view
of the same document as before: The gold allo-
cation (3a), the predicted allocation (3b), as well
as the differential view, where the two allocations
are compared (3c). Each mention corresponds to
a node in the trees and all mentions are directly or
transitively dominated by the artificial root node.
Every subtree under the root constitutes its own
cluster and a solid arc between two mentions de-
notes that the two mentions are coreferent accord-
ing to a coreference allocation. The information
displayed in the nodes of the tree can be cus-
tomized using label patterns.
In the differential view (Figure 3c), solid arcs
correspond to the predicted allocation. Dashed
nodes and arcs are present in the gold allocation,
but not in the prediction. Discrepancies between
the predicted and the gold allocations are marked
9
(a) Tree representing the gold allocation. (b) Tree representing the predicted allocation.
(c) Differential view displaying the difference between the gold and predicted allocations.
Figure 3: Tree view over the example document (gold, predicted, differential).
with different colors denoting different types of er-
rors. The example in Figure 3c contains two errors
made by the system:
1. A false negative mention, denoted by the
dashed red node Shangtou. In the gold
standard (Figure 3a) this mention is clus-
tered with other mentions such as Shantou ?s,
Shantou City, etc. The dashed arc between
Shantou ?s and Shangtou is taken from the
gold allocation, and indicates what the sys-
tem prediction should have been like.
4
2. A foreign antecedent, denoted by the solid
orange arc between Shantou ?s new high level
technology development zone and Shantou.
In this case, the coreference system erro-
neously clustered these two mentions. The
correct antecedent is indicated by the dashed
arc that originates from the document root.
4
This error likely stems from the fact that Shantou is
spelled two different ways within the same document which
causes the resolver?s string-matching feature to fail.
This error is particularly interesting since the
system effectively merges the two clusters
corresponding to Shantou and Shantou? s new
high level technology development zone. The
tree view, however, shows that the error stems
from a single link between these two men-
tions, and that the developer needs to address
this.
Since the tree-based view makes pairwise de-
cisions explicit, the differential view shown in
Figure 3c is more informative to NLP develop-
ers when inspecting errors by automatic system
than comparing a gold standard clustering to a pre-
dicted one. The problem with analyzing the error
on clusterings instead of trees is that the clusters
would be merged, i.e., it is not clear where the ac-
tual mistake was made.
Additional error types not illustrated by Fig-
ure 3c include false positive mentions, where
the system invents a mention that is not part
of the gold allocation. When a false positive
mention is assigned as an antecedent of another
10
mention, the corresponding link is marked as an
invented antecedent. Links that erroneously start
a new cluster when it is coreferent with other men-
tions to the left is marked as false new.
4 Searching
The search engine in ICE makes the annotations
in the documents searchable for, e. g., a corpus lin-
guist who is interested in specific coreference phe-
nomena. It allows the user to express queries over
mentions related through the tree. Queries can ac-
cess the different layers of annotation, both from
the allocation file and the underlying document,
using various constructs such as, e.g., transitivity,
regular expressions, and/or disjunctions. The user
can construct queries either textually (through a
query language) or graphically (by creating nodes
and configuring constraints in dialogues). For a
further discussion of the search engine we refer to
the original ICARUS paper (G?artner et al., 2013).
Figure 4 shows a query that matches cataphoric
pronouns, i.e., pronouns that precede their an-
tecedents. The figure shows the query expressed
as a subgraph (on the left) and the corresponding
results (right) obtained on the development set of
the English CoNLL 2012 data using the manual
annotation represented in the gold allocation.
The query matches two mentions that are di-
rectly or transitively connected through the graph.
The first mention (red node) matches mentions of
the type Pronoun that have to be attached to the
document root node. In the tree formalism we
adopt, this implies that it must be the first men-
tion of its cluster. The second mention (green
node) matches any mention that is not of the type
Pronoun.
(a)
(b)
Figure 4: Example search query and correspond-
ing results.
The search results are grouped along two axes:
the surface form of the head word of the first (red)
node, and the type property of the second mention
(green node), indicated by the special grouping
operator <
*
> inside the boxes. The correspond-
ing results are shown in the right half of Figure 4,
where the first group (surface form) runs verti-
cally, and the second group (mention type) runs
horizontally. The number of hits for each configu-
ration is shown in the corresponding cell. For ex-
ample, the case that the first mention of a chain is
the pronoun I and the closest following coreferent
mention that is not a pronoun is of type Common,
occurs 6 times. By clicking on a cell, the user can
jump straight to a list of the matches, and browse
them using any of the three display modes.
5 Related Work
Two popular annotation and visualization tools
for coreference are PAlinkA (Or?asan, 2003) and
MMAX2 (M?uller and Strube, 2006), which fo-
cus on a (customizable) textual visualization with
highlighting of clusters. The TrED (Pajas and
?
St?ep?anek, 2009) project is a very flexible multi-
level annotation tool centered around tree-based
annotations that can be used to annotate and vi-
sualize coreference. It also features a powerful
search engine. Recent annotation tools include the
web-based BRAT (Stenetorp et al., 2012) and its
extension WebAnno (Yimam et al., 2013). A ded-
icated query and exploration tool for multi-level
annotations is ANNIS (Zeldes et al., 2009).
The aforementioned tools are primarily meant
as annotation tools. They have a tendency of lock-
ing the user into one type of visualization (tree- or
text-based), while often lacking advanced search
functionality. In contrast to them, ICE is not meant
to be yet another annotation tool, but was designed
as a dedicated coreference exploration tool, which
enables the user to swiftly switch between differ-
ent views. Moreover, none of the existing tools
provide an entity-grid view.
ICE is also the only tool that can graphically
compare predictions of a system to a gold standard
with a fine-grained distinction on the types of dif-
ferences. Kummerfeld and Klein (2013) present
an algorithm that transforms a predicted corefer-
ence clustering into a gold clustering and records
the necessary transformations, thereby quantify-
ing different types of errors. However, their algo-
rithm only works on clusterings (sets of mentions),
not pairwise links, and is therefore not able to pin-
point some of the mistakes that ICE can (such as
the foreign antecedent described in Section 3).
11
6 Conclusion
We presented ICE, a flexible coreference visual-
ization and search tool. The tool complements
standard text-based display modes with entity-grid
and tree visualizations. It is also able to dis-
play discrepancies between two different corefer-
ence annotations on the same document, allow-
ing NLP developers to debug coreference sys-
tems in a graphical way. The built-in search en-
gine allows corpus linguists to construct complex
search queries and provide aggregate result views
over large sets of documents. Being based on the
ICARUS platform?s plugin-engine, ICE is extensi-
ble and can easily be extended to cover additional
data formats.
Acknowledgments
This work was funded by the German Federal
Ministry of Education and Research (BMBF) via
CLARIN-D, No. 01UG1120F and the German
Research Foundation (DFG) via the SFB 732,
project D8.
References
Regina Barzilay and Mirella Lapata. 2008. Model-
ing Local Coherence: An Entity-Based Approach.
Computational Linguistics, 34(1):1?34.
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In COLING-ACL,
pages 33?40, Sydney, Australia, July.
Anders Bj?orkelund and Jonas Kuhn. 2014. Learning
Structured Perceptrons for Coreference Resolution
with Latent Antecedents and Non-local Features. In
ACL, Baltimore, MD, USA, June.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies
representation. In COLING Workshop on Cross-
framework and Cross-domain Parser Evaluation.
Greg Durrett and Dan Klein. 2013. Easy Victo-
ries and Uphill Battles in Coreference Resolution.
In EMNLP, pages 1971?1982, Seattle, Washington,
USA, October.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent Structure Perceptron with Feature In-
duction for Unrestricted Coreference Resolution. In
EMNLP-CoNLL: Shared Task, pages 41?48, Jeju Is-
land, Korea, July.
Markus G?artner, Gregor Thiele, Wolfgang Seeker, An-
ders Bj?orkelund, and Jonas Kuhn. 2013. ICARUS
? An Extensible Graphical Search Tool for Depen-
dency Treebanks. In ACL: System Demonstrations,
pages 55?60, Sofia, Bulgaria, August.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
Driven Analysis of Challenges in Coreference Res-
olution. In EMNLP, pages 265?277, Seattle, Wash-
ington, USA, October.
Christoph M?uller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Corpus Technology and Language Pedagogy: New
Resources, New Tools, New Methods, pages 197?
214. Peter Lang.
Constantin Or?asan. 2003. PALinkA: A highly cus-
tomisable tool for discourse annotation. In Akira
Kurematsu, Alexander Rudnicky, and Syun Tutiya,
editors, Proceedings of the Fourth SIGdial Work-
shop on Discourse and Dialogue, pages 39?43.
Petr Pajas and Jan
?
St?ep?anek. 2009. System for
Querying Syntactically Annotated Corpora. In ACL-
IJCNLP: Software Demonstrations, pages 33?36,
Suntec, Singapore.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
Unrestricted Coreference in OntoNotes. In CoNLL:
Shared Task, pages 1?27, Portland, Oregon, USA,
June.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling Multilingual Unre-
stricted Coreference in OntoNotes. In EMNLP-
CoNLL: Shared Task, pages 1?40, Jeju Island, Ko-
rea, July.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M. Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Inter-
national Workshop on Semantic Evaluation, pages
1?8, Uppsala, Sweden, July.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. brat: a Web-based Tool for NLP-Assisted
Text Annotation. In EACL: Demonstrations, pages
102?107, April.
Seid Muhie Yimam, Iryna Gurevych, Richard
Eckart de Castilho, and Chris Biemann. 2013.
WebAnno: A Flexible, Web-based and Visually
Supported System for Distributed Annotations. In
ACL: System Demonstrations, pages 1?6, August.
Amir Zeldes, Julia Ritz, Anke L?udeling, and Christian
Chiarcos. 2009. ANNIS: a search tool for multi-
layer annotated corpora. In Proceedings of Corpus
Linguistics.
12
First Joint Workshop on Statistical Parsing of Morphologically Rich Languages
and Syntactic Analysis of Non-Canonical Languages, pages 97?102 Dublin, Ireland, August 23-29 2014.
Introducing the IMS-Wroc?aw-Szeged-CIS Entry at the SPMRL 2014
Shared Task: Reranking and Morphosyntax Meet Unlabeled Data?
Anders Bjo?rkelund? and O?zlem C?etinog?lu? and Agnieszka Falen?ska,?
Richa?rd Farkas? and Thomas Mu?ller? and Wolfgang Seeker? and Zsolt Sza?nto??
?Institute for Natural Language Processing University of Stuttgart, Germany
Institute of Computer Science, University of Wroc?aw, Poland
?Department of Informatics University of Szeged, Hungary
?Center for Information and Language Processing University of Munich, Germany
{anders,ozlem,muellets,seeker}@ims.uni-stuttgart.de
agnieszka.falenska@cs.uni.wroc.pl
{rfarkas,szantozs}@inf.u-szeged.hu
Abstract
We summarize our approach taken in the SPMRL 2014 Shared Task on parsing morphologically
rich languages. Our approach builds upon our contribution from last year, with a number of
modifications and extensions. Though this paper summarizes our contribution, a more detailed
description and evaluation will be presented in the accompanying volume containing notes from
the SPMRL 2014 Shared Task.
1 Introduction
This paper summarizes the approach of IMS-Wroc?aw-Szeged-CIS taken for the SPMRL 2014 Shared
Task on parsing morphologically rich languages (Seddah et al., 2014). Since this paper is a rough sum-
mary that is written before submission of test runs we refer the reader to the full description paper which
will be published after the shared task (Bjo?rkelund et al., 2014).1
The SPMRL 2014 Shared Task is a direct extension of the SPMRL 2013 Shared Task (Seddah et al.,
2013) which targeted parsing morphologically rich languages. The task involves parsing both depen-
dency and phrase-structure representations of 9 languages: Arabic, Basque, French, German, Hebrew,
Hungarian, Korean, Polish, and Swedish. The only difference between the two tasks is that large amounts
of unlabeled data are additionally available to participants for the 2014 task.
Our contribution builds upon our system from last year (Bjo?rkelund et al., 2013), with additional
features and components that try to exploit the unlabeled data. Given the limited window of time to
participate in this year?s shared task, we only contribute to the setting with predicted preprocessing,
using the largest available training data set for each language.2 We also do not participate in the Arabic
track since the shared task organizers did not provide any unlabeled data at a reasonable time.
2 Review of Last Year?s System
Our current system is based on the system we participated with in the SPMRL 2013 Shared Task. We
summarize the architecture of this system as three different components.
?Authors in alphabetical order
1Due to logistical constraints this paper had to be written before the deadlines for the actual shared task and do thus not contain
a full description of the system, nor the experimental evaluation of the same.
2In other words, no gold preprocessing or smaller training sets.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
97
2.1 Preprocessing
As the initial step of preprocessing we converted the Shared Task data from the CoNLL06 format to
CoNLL09, which required a decision on using coarse or fine grained POS tags. After a set of preliminary
experiments we picked fine POS tags where possible, except Basque and Korean.
We used MarMoT3 (Mu?ller et al., 2013) to predict POS tags and morphological features jointly. We in-
tegrated the output from external morphological analyzers as features to MarMoT. We also experimented
with the integration of predicted tags provided by the organizers and observed that these stacked models
help improve Basque, Polish, and Swedish preprocessing. The stacked models provided additional infor-
mation to our tagger since the provided predictions were coming from models trained on larger training
sets than the shared task training sets.
2.2 Dependency Parsing
The dependency parsing architecture of our SPMRL 2013 Shared Task contribution is summarized in
Figure 1. The first step combines the n-best trees of two parsers, namely the mate parser4 (Bohnet, 2010)
and a variant of the EasyFirst parser (Goldberg and Elhadad, 2010), which we call best-first parser. We
merged the 50-best analyses from these parsers into one n-best list of 50 to 100 trees. We then added
parsing scores to the n-best trees from the two parsers, and additionally from the turboparser5 (Martins
et al., 2010).
mate parser
best-first
parser
turboparser
merged list
of 50-100 best
trees/sentence
merged list
scored by
all parsers
ranker
ptb trees
Parsing Ranking
IN OUT
scores
scores
scores
features
Figure 1: Architecture of the dependency ranking system from (Bjo?rkelund et al., 2013).
The scored trees are fed into the ranking system. The ranker utilizes the parsing scores and fea-
tures coming from both constituency and dependency parses. We specified a default feature set and
experimented with additional features for each language for optimal results. We achieved over 1% LAS
improvement on all languages except a 0.3% improvement on Hungarian.
2.3 Constituency Parsing
The constituency parsing architecture advances in three steps. For all setups we removed the morphologi-
cal annotation of POS tags and the function labels of non-terminals and apply the Berkeley Parser (Petrov
et al., 2006) as our baseline. As the first setup, we replaced words with a frequency < 20 with their pre-
dicted part-of-speech and morphology tags and improved the PARSEVAL scores across languages. The
second setup employed a product grammar (Petrov, 2010), where we combined 8 different grammars
trained on the same data but with different initialization setups. As a result, the scores substantially
improved on all languages.
Finally, we conducted ranking experiments on the 50-best outputs of the product grammars. We used
a slightly modified version of the Mallet toolkit (McCallum, 2002), where the reranker is trained for the
3https://code.google.com/p/cistern/
4https://code.google.com/p/mate-tools
5http://www.ark.cs.cmu.edu/TurboParser/
98
maximum entropy objective function of Charniak and Johnson (2005) and uses the standard feature set
from Charniak and Johnson (2005) and Collins (2000). Hebrew and Polish scores remained almost the
same, whereas Basque, French, and Hungarian highly benefited from reranking.
3 Planned Additions to Last Year?s System
This year we extend our systems for both the constituency and dependency tracks to add additional
information and try to profit from unlabeled data.
3.1 Preprocessing
We use the mate-tools? lemmatizer and MarMoT to preprocess all labeled and unlabeled data. From the
SPMRL 2013 Shared Task, we learned that getting as good preprocessing as possible is an important
part of the overall improvements. Preprocessing consists of predicting lemmas, part-of-speech, and
morphological features. Preprocessing for the training data is done via 5-fold jackknifing to produce
realistic input features for the parsers. This year we do not do stacking on top of provided morphological
analyses since the annotations on the labeled and unlabeled data were inconsistent for some languages.6
3.2 Dependency Parsing
We pursue two different ways of integrating additional information into our system from the SPMRL
2013 Shared Task (Bjo?rkelund et al., 2013): supertags and co-training.
Supertags (Bangalore and Joshi, 1999) are tags that encode more syntactic information than standard
part-of-speech tags. Supertags have been used in deep grammar formalisms like CCG or HPSG to prune
the search space for the parser. The idea has been applied to dependency parsing by Foth et al. (2006)
and recently to statistical dependency parsing (Ouchi et al., 2014; Ambati et al., 2014), where supertags
are used as features rather than to prune the search space. Since the supertag set is dynamically derived
from the gold-standard syntactic structures, we can encode different kinds of information into a supertag,
in particular also morphological information. Supertags are predicted before parsing using MarMoT and
are then used as features in the mate parser and the turboparser.
We will use a variant of co-training (Blum and Mitchell, 1998) by applying two different parsers to
select additional training material from unlabeled data. We use the mate parser and the turboparser to
parse the unlabeled data provided by the organizers. We then select sentences where both parsers agree
on the structure as additional training examples following Sagae and Tsujii (2007). We then train two
more models: one on the labeled training data and the unlabeled data selected by the two parsers, and
one only on the unlabeled data. These two models are then integrated into our parsing system from 2013
as additional scorers to score the n-best list. Their scores are used as features in the ranker.
Before we parse the unlabeled data to obtain the training sentences, we filter it in order to arrive
at a cleaner corpus. Most importantly, we only keep sentences up to length 50, and which contain at
maximum two unknown words (compared to the labeled training data).
3.3 Constituency Parsing
We experiment with two approaches for improving constituency parsing:
Preterminal labelsets play an important role in constituency parsing of morphologically rich lan-
guages (Dehdari et al., 2011). Instead of removing the morphological annotation of POS tags, we use a
preterminal set which carries more linguistic information while still keeping it compact. We follow the
merge procedure for morphological feature values of Sza?nto? and Farkas (2014). This procedure outputs a
clustering of full morphological descriptions and we use the cluster IDs as preterminal labels for training
the Berkeley Parser.
Reranking at the constituency parsing side is enriched by novel features. We define feature tem-
plates exploiting co-occurrence statistics from the unlabeled datasets; automatic dependency parses of
the sentence in question (Farkas and Bohnet, 2012); Brown clusters (Brown et al., 1992); and atomic
morphological feature values (Sza?nto? and Farkas, 2014).
6The organizers later resolved this issue by patching the data, although time constraints prevented us from using the patched
data.
99
4 Conclusion
This paper describes our plans for the SPMRL 2014 Shared Task, most of which are yet to be imple-
mented. For the actual system description and our results, we refer the interested reader to (Bjo?rkelund
et al., 2014) and (Seddah et al., 2014).
Acknowledgements
Agnieszka Falen?ska is funded through the Project International computer science and applied mathemat-
ics for business study programme at the University of Wroc?aw co-financed with European Union funds
within the European Social Fund No. POKL.04.01.01-00-005/13. Richa?rd Farkas and Zsolt Sza?nto? are
funded by the European Union and the European Social Fund through the project FuturICT.hu (grant no.:
TA?MOP-4.2.2.C-11/1/KONV-2012-0013). Thomas Mu?ller is supported by a Google Europe Fellowship
in Natural Language Processing. The remaining authors are funded by the Deutsche Forschungsgemein-
schaft (DFG) via the SFB 732, projects D2 and D8 (PI: Jonas Kuhn).
We also express our gratitude to the treebank providers for each language: Arabic (Maamouri et al.,
2004; Habash and Roth, 2009; Habash et al., 2009; Green and Manning, 2010), Basque (Aduriz et al.,
2003), French (Abeille? et al., 2003), Hebrew (Sima?an et al., 2001; Tsarfaty, 2010; Goldberg, 2011;
Tsarfaty, 2013), German (Brants et al., 2002; Seeker and Kuhn, 2012), Hungarian (Csendes et al., 2005;
Vincze et al., 2010), Korean (Choi et al., 1994; Choi, 2013), Polish (S?widzin?ski and Wolin?ski, 2010),
and Swedish (Nivre et al., 2006).
References
Anne Abeille?, Lionel Cle?ment, and Franc?ois Toussenel. 2003. Building a treebank for french. In Anne Abeille?,
editor, Treebanks. Kluwer, Dordrecht.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa, A. D??az de Ilarraza, A. Garmendia, and M. Oronoz. 2003.
Construction of a Basque dependency treebank. In TLT-03, pages 201?204.
Bharat Ram Ambati, Tejaswini Deoskar, and Mark Steedman. 2014. Improving dependency parsers using combi-
natory categorial grammar. In Proceedings of the 14th Conference of the European Chapter of the Association
for Computational Linguistics, volume 2: Short Papers, pages 159?163, Gothenburg, Sweden, April. Associa-
tion for Computational Linguistics.
Srinivas Bangalore and Aravind K. Joshi. 1999. Supertagging: An approach to almost parsing. Computational
Linguistics, 25(2):237?265.
Anders Bjo?rkelund, O?zlem C?etinog?lu, Richa?rd Farkas, Thomas Mu?ller, and Wolfgang Seeker. 2013. (re)ranking
meets morphosyntax: State-of-the-art results from the SPMRL 2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 135?145, Seattle, Washington,
USA, October. Association for Computational Linguistics.
Anders Bjo?rkelund, O?zlem C?etinog?lu, Agnieszka Falen?ska, Richa?rd Farkas, Thomas Mu?ller, Wolfgang Seeker,
and Zsolt Sza?nto?. 2014. The IMS-Wroc?aw-Szeged-CIS entry at the SPMRL 2014 Shared Task: Reranking and
Morphosyntax meet Unlabeled Data. In Notes of the SPMRL 2014 Shared Task on Parsing Morphologically-
Rich Languages, Dublin, Ireland, August.
Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In Proceedings of
the Eleventh Annual Conference on Computational Learning Theory, COLT? 98, pages 92?100, New York, NY,
USA. ACM.
Bernd Bohnet. 2010. Top Accuracy and Fast Dependency Parsing is not a Contradiction. In Proceedings of
the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97, Beijing, China,
August. Coling 2010 Organizing Committee.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lezius, and George Smith. 2002. The TIGER treebank.
In Erhard Hinrichs and Kiril Simov, editors, Proceedings of the First Workshop on Treebanks and Linguistic
Theories (TLT 2002), pages 24?41, Sozopol, Bulgaria.
Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jenifer C. Lai, and Robert L. Mercer. 1992. Class-based
n-gram models of natural language. Computational Linguistics, 18(4):467?479.
100
Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ?05, pages
173?180.
Key-Sun Choi, Young S Han, Young G Han, and Oh W Kwon. 1994. Kaist tree bank project for korean: Present
and future development. In Proceedings of the International Workshop on Sharable Natural Language Re-
sources, pages 7?14. Citeseer.
Jinho D. Choi. 2013. Preparing korean data for the shared task on parsing morphologically rich languages. CoRR,
abs/1309.1649.
Michael Collins. 2000. Discriminative Reranking for Natural Language Parsing. In Proceedings of the Seven-
teenth International Conference on Machine Learning, ICML ?00, pages 175?182.
Do?ra Csendes, Jano?s Csirik, Tibor Gyimo?thy, and Andra?s Kocsor. 2005. The Szeged treebank. In Va?clav Ma-
tous?ek, Pavel Mautner, and Toma?s? Pavelka, editors, Text, Speech and Dialogue: Proceedings of TSD 2005.
Springer.
Jon Dehdari, Lamia Tounsi, and Josef van Genabith. 2011. Morphological features for parsing morphologically-
rich languages: A case of arabic. In Proceedings of the Second Workshop on Statistical Parsing of Morphologi-
cally Rich Languages, pages 12?21, Dublin, Ireland, October. Association for Computational Linguistics.
Richa?rd Farkas and Bernd Bohnet. 2012. Stacking of dependency and phrase structure parsers. In Proceedings of
COLING 2012, pages 849?866, Mumbai, India, December. The COLING 2012 Organizing Committee.
Kilian A. Foth, Tomas By, and Wolfgang Menzel. 2006. Guiding a constraint dependency parser with supertags.
In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of
the Association for Computational Linguistics, pages 289?296, Sydney, Australia, July. Association for Com-
putational Linguistics.
Yoav Goldberg and Michael Elhadad. 2010. An Efficient Algorithm for Easy-First Non-Directional Dependency
Parsing. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 742?750, Los Angeles, California, June. Association for
Computational Linguistics.
Yoav Goldberg. 2011. Automatic syntactic processing of Modern Hebrew. Ph.D. thesis, Ben Gurion University of
the Negev.
Spence Green and Christopher D. Manning. 2010. Better arabic parsing: Baselines, evaluations, and analysis. In
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 394?402,
Beijing, China, August. Coling 2010 Organizing Committee.
Nizar Habash and Ryan Roth. 2009. Catib: The columbia arabic treebank. In Proceedings of the ACL-IJCNLP
2009 Conference Short Papers, pages 221?224, Suntec, Singapore, August. Association for Computational
Linguistics.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009. Syntactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on Arabic Language Resources and Tools, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and Wigdan Mekki. 2004. The Penn Arabic Treebank: Building
a Large-Scale Annotated Arabic Corpus. In NEMLAR Conference on Arabic Language Resources and Tools.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar, and Mario Figueiredo. 2010. Turbo Parsers: Dependency
Parsing by Approximate Variational Inference. In Proceedings of the 2010 Conference on Empirical Meth-
ods in Natural Language Processing, pages 34?44, Cambridge, MA, October. Association for Computational
Linguistics.
Andrew Kachites McCallum. 2002. ?mallet: A machine learning for language toolkit?.
http://mallet.cs.umass.edu.
Thomas Mu?ller, Helmut Schmid, and Hinrich Schu?tze. 2013. Efficient Higher-Order CRFs for Morphological
Tagging. In In Proceedings of EMNLP.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Talbanken05: A Swedish treebank with phrase structure and
dependency annotation. In Proceedings of LREC, pages 1392?1395, Genoa, Italy.
101
Hiroki Ouchi, Kevin Duh, and Yuji Matsumoto. 2014. Improving dependency parsers with supertags. In Proceed-
ings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, volume
2: Short Papers, pages 154?158, Gothenburg, Sweden, April. Association for Computational Linguistics.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable
tree annotation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th
annual meeting of the Association for Computational Linguistics, pages 433?440. Association for Computa-
tional Linguistics.
Slav Petrov. 2010. Products of Random Latent Variable Grammars. In Human Language Technologies: The 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages
19?27, Los Angeles, California, June. Association for Computational Linguistics.
Kenji Sagae and Jun?ichi Tsujii. 2007. Dependency parsing and domain adaptation with LR models and parser
ensembles. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1044?1050,
Prague, Czech Republic, June. Association for Computational Linguistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho D. Choi, Richa?rd Farkas, Jennifer Foster, Iakes
Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolf-
gang Maier, Joakim Nivre, Adam Przepio?rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika
Vincze, Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Clergerie. 2013. Overview of the
SPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Pro-
ceedings of the Fourth Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 146?182,
Seattle, Washington, USA, October. Association for Computational Linguistics.
Djame? Seddah, Reut Tsarfaty, Sandra Ku?bler, Marie Candito, Jinho Choi, Matthieu Constant, Richa?rd Farkas,
Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang
Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze,
Marcin Wolin?ski, Alina Wro?blewska, and Eric Villemonte de la Cle?rgerie. 2014. Overview of the SPMRL 2014
shared task on parsing morphologically rich languages. In Notes of the SPMRL 2014 Shared Task on Parsing
Morphologically-Rich Languages, Dublin, Ireland.
Wolfgang Seeker and Jonas Kuhn. 2012. Making Ellipses Explicit in Dependency Conversion for a German
Treebank. In Proceedings of the 8th International Conference on Language Resources and Evaluation, pages
3132?3139, Istanbul, Turkey. European Language Resources Association (ELRA).
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman, and Noa Nativ. 2001. Building a Tree-Bank for Modern
Hebrew Text. In Traitement Automatique des Langues.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards a bank of constituent parse trees for Polish. In Text,
Speech and Dialogue: 13th International Conference (TSD), Lecture Notes in Artificial Intelligence, pages
197?204, Brno, Czech Republic. Springer.
Zsolt Sza?nto? and Richa?rd Farkas. 2014. Special techniques for constituent parsing of morphologically rich lan-
guages. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational
Linguistics, pages 135?144, Gothenburg, Sweden, April. Association for Computational Linguistics.
Reut Tsarfaty. 2010. Relational-Realizational Parsing. Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A Unified Morpho-Syntactic Scheme of Stanford Dependencies. Proceedings of ACL.
Veronika Vincze, Do?ra Szauter, Attila Alma?si, Gyo?rgy Mo?ra, Zolta?n Alexin, and Ja?nos Csirik. 2010. Hungarian
dependency treebank. In LREC.
102
