Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 89?94,
Prague, June 2007. c?2007 Association for Computational Linguistics
Experiments of UNED at the Third
Recognising Textual Entailment Challenge
A?lvaro Rodrigo, Anselmo Pen?as, Jesu?s Herrera, Felisa Verdejo
Departmento de Lenguajes y Sistemas Informa?ticos
Universidad Nacional de Educacio?n a Distancia
Madrid, Spain
{alvarory, anselmo, jesus.herrera, felisa}@lsi.uned.es
Abstract
This paper describes the experiments devel-
oped and the results obtained in the partic-
ipation of UNED in the Third Recognising
Textual Entailment (RTE) Challenge. The
experiments are focused on the study of the
effect of named entities in the recognition
of textual entailment. While Named Entity
Recognition (NER) provides remarkable re-
sults (accuracy over 70%) for RTE on QA
task, IE task requires more sophisticated
treatment of named entities such as the iden-
tification of relations between them.
1 Introduction
The systems presented to the Third Recognizing
Textual Entailment Challenge are based on the one
presented to the Second RTE Challenge (Herrera
et al, 2006b) and the ones presented to the An-
swer Validation Exercise (AVE) 2006 (Rodrigo et
al., 2007).
Since a high quantity of pairs of RTE-3 collec-
tions contain named entities (82.6% of the hypothe-
ses in the test collection contain at least one named
entity), the objective of this work is to study the ef-
fect of named entity recognition on textual entail-
ment in the framework of the Third RTE Challenge.
In short, the techniques involved in the experi-
ments in order to reach these objectives are:
? Lexical overlapping between ngrams of text
and hypothesis.
? Entailment between named entities.
? Branch overlapping between dependency trees
of text and hypothesis.
In section 2, the main components of the systems
are described in detail. Section 3 describes the infor-
mation our systems use for the entailment decision.
The description of the two runs submitted are given
in Section 4. The results obtained and its analysis are
described in Section 5. Section 6 shows a discussion
of the results. Finally, some conclusions and future
work are given.
2 Systems Description
The proposed systems are based on surface tech-
niques of lexical and syntactic analysis considering
each task (Information Extraction, Information Re-
trieval, Question Answering and Text Summariza-
tion) of the RTE Challenge independently.
The systems accept pairs of text snippets (text and
hypothesis) at the input and give a boolean value at
the output: YES if the text entails the hypothesis and
NO otherwise. This value is obtained by the appli-
cation of the learned model by a SVM classifier.
The main components of the systems are the fol-
lowing:
2.1 Linguistic processing
Firstly, each text-hypothesis pair is preprocessed in
order to obtain the following information for the en-
tailment decision:
? POS: a Part of Speech Tagging is performed in
order to obtain lemmas for both text and hy-
pothesis using the Freeling POS tagger (Car-
reras et al, 2004).
89
<t>...Iraq invaded Kuwait on <TIMEX>August 2 1990</TIMEX>...</t>
<h>Iraq invaded Kuwait in <NUMEX>1990</NUMEX></h>
Figure 1: Example of an error when disambiguating the named entity type.
<t>...Chernobyl accident began on
<ENTITY>Saturday April 26 1986</ENTITY>...</t>
<h>The Chernobyl disaster was in <ENTITY>1986</ENTITY></h>
Figure 2: Example of a pair that justifies the process of entailment.
<pair id=??5?? entailment=??NO?? task=??IE?? length=??short??>
<t>The Communist Party USA was a small Maoist political party
which was founded in 1965 by members of the Communist Party around
Michael Laski who took the side of China in the Sino-Soviet split.
</t>
<h>Michael Laski was an opponent of China.</h>
</pair>
<pair id=??7?? entailment=??NO?? task=??IE?? length=??short??>
<t>Sandra Goudie was first elected to Parliament in the 2002
elections, narrowly winning the seat of Coromandel by defeating
Labour candidate Max Purnell and pushing incumbent Green MP
Jeanette Fitzsimons into third place.</t>
<h>Sandra Goudie was defeated by Max Purnell.</h>
</pair>
<pair id=??8?? entailment=??NO?? task=??IE?? length=??short??>
<t>Ms. Minton left Australia in 1961 to pursue her studies in
London.</t>
<h>Ms. Minton was born in Australia.</h>
</pair>
Figure 3: IE pairs with entailment between named entities but not between named entities relations.
90
? NER: the Freeling Named Entity Recogniser is
also applied to recover the information needed
by the named entity entailment module that is
described in the following section. Numeric ex-
pressions, proper nouns and temporal expres-
sions of each text and hypothesis are tagged.
? Dependency analysis: a dependency tree of
each text and hypothesis is obtained using Lin?s
Minipar (Lin, 1998).
2.2 Entailment between named entities
Once the named entities of the hypothesis and the
text are detected, the next step is to determine the
entailment relations between the named entities in
the text and the named entities in the hypothesis. In
(Rodrigo et al, 2007) the following entailment rela-
tions between named entities were defined:
1. A Proper Noun E1 entails a Proper Noun E2 if
the text string of E1 contains the text string of
E2.
2. A Time Expression T1 entails a Time Expres-
sion T2 if the time range of T1 is included in
the time range of T2.
3. A numeric expression N1 entails a numeric ex-
pression N2 if the range associated to N2 en-
closes the range of N1.
Some characters change in different expressions
of the same named entity as, for example, in a proper
noun with different wordings (e.g. Yasser, Yaser,
Yasir). To detect the entailment in these situations,
when the previous process fails, we implemented a
modified entailment decision process taking into ac-
count the edit distance of Levenshtein (Levensthein,
1966). Thus, if two named entities differ in less than
20%, then we assume that exists an entailment rela-
tion between these named entities.
However, this definition of named entities entail-
ment does not support errors due to wrong named
entities classification as we can see in Figure 1. The
expression 1990 represents a year but it is recog-
nised as a numeric expression in the hypothesis.
However the same expression is recognised as a tem-
poral expression in the text and, therefore, the ex-
pression in the hypothesis cannot be entailed by it
according to the named entities entailment definition
above.
We quantified the effect of these errors in recog-
nising textual entailment. For this purpose, we de-
veloped the following two settings:
1. A system based in dependency analysis and
WordNet (Herrera et al, 2006b) that uses the
categorization given by the NER tool, where
the entailment relations between named entities
are the previously ones defined.
2. The same system based on dependency analysis
and WordNet but not using the categorization
given by the NER tool. All named entities de-
tected receive the same tag and a named entity
E1 entails a named entity E2 if the text string
of E1 contains the text string of E2 (see Figure
2).
We checked the performance of these two settings
over the test corpus set of the Second RTE Chal-
lenge. The results obtained, using the accuracy mea-
sure that is the fraction of correct responses accord-
ing to (Dagan et al, 2006), are shown in table 1. The
table shows that with an easier and a more robust
processing (NER without classification) the perfor-
mance is not only maintained, but it is even slightly
higher.
This fact led us to ignore the named entity catego-
rization given by the tool and assume that text and
hypothesis are related and close texts where same
expressions must receive same categories, without
the need of classification. Thus, all detected named
entities receive the same tag and we consider that a
named entity E1 entails a named entity E2 if the text
string of E1 contains the text string of E2.
Table 1: Entailment between numeric expressions.
Accuracy
Setting 1 0.610
Setting 2 0.614
2.3 Sentence level matching
A tree matching module, which searches for match-
ing branches into the hypotheses? dependency trees,
is used. There is a potential matching branch per
leaf. A branch from the hypothesis is considered
91
a ?matching branch? only if all its nodes from the
root to the leaf are involved in a lexical entailment
(Herrera et al, 2006a). In this way, the subtree con-
formed by all the matching branches from a hypoth-
esis? dependency tree is included in the respective
text?s dependency tree, giving an idea of tree inclu-
sion.
We assumed that the larger is the included sub-
tree of the hypothesis? dependency tree, the more
semantically similar are the text and the hypothesis.
Thus, the existence or absence of an entailment rela-
tion from a text to its respective hypothesis considers
the portion of the hypothesis? tree that is included in
the text?s tree.
3 Entailment decision
A SVM classifier was applied in order to train a
model from the development corpus. The model was
trained with a set of features obtained from the pro-
cessing described above. The features we have used
and the training strategies were the following:
3.1 Features
We prepared the following features to feed the SVM
model:
1. Percentage of nodes of the hypothesis? de-
pendency tree pertaining to matching branches
according to section 2.3 considering, respec-
tively:
? Lexical entailment between the words of
the snippets involved.
? Lexical entailment between the lemmas of
the snippets involved.
2. Percentage of words of the hypothesis in the
text (treated as bags of words).
3. Percentage of unigrams (lemmas) of the hy-
pothesis in the text (treated as bags of lemmas).
4. Percentage of bigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
5. Percentage of trigrams (lemmas) of the hypoth-
esis in the text (treated as bags of lemmas).
6. A boolean value indicating if there is or not any
named entity in the hypothesis that is not en-
tailed by one or more named entities in the text
according to the named entity entailment deci-
sion described in section 2.2.
Table 2: Experiments with separate training over the
development corpus using cross validation.
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.64 0.67
Setting 2 0.62 0.66
Table 3: Experiments with separate training over the
test corpus.
Accuracy with Accuracy with
the same model a different model
for all tasks for each task
Setting 1 0.59 0.62
Setting 2 0.60 0.64
Table 4: Results for run 1 and run 2.
Accuracy
run 1 run 2
IE 52.50% 53.50%
IR 67% 67%
QA 72% 72%
SUM 58% 60%
Overall 62.38% 63.12%
3.2 Training
About the decision of how to perform the training
in our SVM models, we wanted to study the effect
of training a unique model compared to training one
different model per task.
For this purpose we used the following two set-
tings:
1. A SVM model that uses features 2, 3, 4 and 5
from section 3.1.
2. A SVM model that uses features 2, 3, 4, 5 and
6 from section 3.1.
Each setting was training using cross validation
over the development set of the Third RTE Chal-
lenge in two different ways:
1. Training a unique model for all pairs.
92
2. Training one model for each task. Each model
is trained with only pairs from the same task
that the model will predict.
The results obtained in the experiments are shown
in table 2. As we can see in the table, with the train-
ing of one model for each task results are slightly
better, increasing performance of both settings. Tak-
ing into account these results, we took the decision
of using a different training for each task in the runs
submitted.
Our decision was confirmed after the runs submis-
sion to RTE-3 Challenge with new experiments over
the RTE-3 test corpus, using the RTE-3 development
corpus as training (see table 3 for results).
4 Runs Submitted
Two different runs were submitted to the Third RTE
Challenge. Each run was trained using the method
described in section 3.2 with the following subset of
the features described in section 3.1:
? Run 1 was obtained using the features 2, 3,
4 and 5 from section 3.1. These features ob-
tained good results for pairs from the QA task,
as we can see in (Rodrigo et al, 2007), and
we wanted to check their performance in other
tasks.
? Run 2 was obtained using the following fea-
tures for each task:
? IE: features 2, 3, 4, 5 and 6 from section
3.1. These ones were the features that ob-
tained the best results for IE pairs in our
experiments over the development set.
? IR: features 2, 3, 4 and 5 from section 3.1.
These ones were the features that obtained
best results for IR pairs in our experiments
over the development set.
? QA: feature 6 from section 3.1. We chose
this feature, which had obtained an ac-
curacy over 70% in previous experiments
over the development set in QA pairs, to
study the effect of named entities in QA
pairs.
? SUM: features 1, 2 and 3 from section 3.1.
We selected these features to show the im-
portance of dependency analysis in SUM
pairs as it is shown in section 6.
5 Results
Accuracy was applied as the main measure to the
participating systems.
The results obtained over the test corpus for the
two runs submitted are shown in table 4.
As we can see in both runs, different accuracy val-
ues are obtained depending on the task. The best re-
sult is obtained in pairs from QA with a 72% accu-
racy in the two runs, although two different systems
are applied. This result pushes us to use this system
for Answer Validation (Pen?as et al, 2007). Results
in run 2, which uses a different setting for each task,
are slightly better than results in run 1, but only in
IE and SUM. However, results are too close to ac-
cept a confirmation of our initial intuition that pairs
from different tasks could need not only a different
training, but also the use of different approaches for
the entailment decision.
6 Discussion
In run 2 we used NER for IE and QA, the two tasks
with the higher percentage of pairs with at least one
named entity in the hypothesis (98.5% in IE and
97% in QA).
Our previous work about the use of named enti-
ties in textual entailment (Rodrigo et al, 2007) sug-
gested that NER permitted to obtain good results.
However, after the RTE-3 experience, we found that
the use of NER does not improve results in all tasks,
but only in QA in a solid way with the previous
work.
We performed a qualitative study over the IE pairs
showing that, as it can be expected, in pairs from IE
the relations between named entities are more im-
portant that named entities themselves.
Figure 3 shows some examples where all named
entities are entailed but not the relation between
them. In pair 5 both Michael Laski and China are
entailed but the relation between them is took the
side of in the text, and was an opponent of in the
hypothesis. The same problem appears in the other
pairs with the relation left instead was born in (pair
8) or passive voice instead active voice (pair 7).
Comparing run 1 and run 2, dependency analysis
shows its usefulness in SUM pairs, where texts and
hypotheses have a higher syntactic parallelism than
in pairs from other tasks. This statement is shown
93
Table 5: Percentage of hypothesis nodes in matching
branches.
Percentage
SUM 75,505%
IE 7,353%
IR 6,422%
QA 8,496%
in table 5 where the percentage of hypothesis nodes
pertaining to matching branches in the dependency
tree is much higher in SUM pairs than in the rest of
tasks.
This syntactic parallelism seems to be the respon-
sible for the 2% increasing between the first and the
second run in SUM pairs.
7 Conclusions and future work
The experiments have been focused on the study of
the importance of considering entailment between
named entities in the recognition of textual entail-
ment, and the use of a separate training for each task.
As we have seen, both approaches increase slightly
the accuracy of the proposed systems. As we have
also shown, different approaches for each task could
also increase the system performance.
Future work is focused on improving the perfor-
mance in IE pairs taking into account relations be-
tween named entities.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Technology within the
project R2D2-SyEMBRA (TIC-2003-07158-C04-
02), the Regional Government of Madrid under the
Research Network MAVIR (S-0505/TIC-0267), the
Education Council of the Regional Government of
Madrid and the European Social Fund.
References
X. Carreras, I. Chao, L. Padro?, and M. Padro?. 2004.
FreeLing: An Open-Source Suite of Language An-
alyzers.. In Proceedings of the 4th International
Conference on Language Resources and Evaluation
(LREC04). Lisbon, Portugal, 2004.
I. Dagan, O. Glickman and B. Magnini. 2006. The
PASCAL Recognising Textual Entailment Challenge.
In Quin?onero-Candela et al, editors, MLCW 2005,
LNAI Volume 3944, Jan 2006, Pages 177 - 190.
J. Herrera, A. Pen?as and F. Verdejo. 2006a. Textual En-
tailment Recognition Based on Dependency Analysis
and WordNet. In Quin?onero-Candela et al, editors,
MLCW 2005, LNAI Volume 3944, Jan 2006, Pages
231-239.
J. Herrera, A. Pen?as, A?. Rodrigo and F. Verdejo. 2006b.
UNED at PASCAL RTE-2 Challenge. In Proceed-
ings of the Second PASCAL Challenges Workshop on
Recognising Textual Entailment, Venice, Italy.
V. I. Levensthein. 1966. Binary Codes Capable of Cor-
recting Deletions, Insertions and Reversals. In Soviet
Physics - Doklady, volume 10, pages 707710, 1966.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. Workshop on the Evaluation of Parsing Systems,
Granada, Spain, May, 1998.
A. Pen?as, A?. Rodrigo, V. Sama and F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2006. In
Lecture Notes in Computer Science. In press.
A?. Rodrigo, A. Pen?as, J. Herrera and F. Verdejo. 2007.
The Effect of Entity Recognition on Answer Validation.
In Lecture Notes in Computer Science. In press.
94
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1415?1424,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Simple Measure to Assess Non-response
Anselmo Pen?as and Alvaro Rodrigo
UNED NLP & IR Group
Juan del Rosal, 16
28040 Madrid, Spain
{anselmo,alvarory@lsi.uned.es}
Abstract
There are several tasks where is preferable not
responding than responding incorrectly. This
idea is not new, but despite several previous at-
tempts there isn?t a commonly accepted mea-
sure to assess non-response. We study here an
extension of accuracy measure with this fea-
ture and a very easy to understand interpreta-
tion. The measure proposed (c@1) has a good
balance of discrimination power, stability and
sensitivity properties. We show also how this
measure is able to reward systems that main-
tain the same number of correct answers and
at the same time decrease the number of in-
correct ones, by leaving some questions unan-
swered. This measure is well suited for tasks
such as Reading Comprehension tests, where
multiple choices per question are given, but
only one is correct.
1 Introduction
There is some tendency to consider that an incorrect
result is simply the absence of a correct one. This is
particularly true in the evaluation of Information Re-
trieval systems where, in fact, the absence of results
sometimes is the worse output.
However, there are scenarios where we should
consider the possibility of not responding, because
this behavior has more value than responding incor-
rectly. For example, during the process of introduc-
ing new features in a search engine it is important
to preserve users? confidence in the system. Thus,
a system must decide whether it should give or not
a result in the new fashion or keep on with the old
kind of output. A similar example is the decision
about showing or not ads related to the query. Show-
ing wrong ads harms the business model more than
showing nothing. A third example more related to
Natural Language Processing is the Machine Read-
ing evaluation through reading comprehension tests.
In this case, where multiple choices for a question
are offered, choosing a wrong option should be pun-
ished against leaving the question unanswered.
In the latter case, the use of utility functions is
a very common option. However, utility functions
give arbitrary value to not responding and ignore
the system?s behavior showed when it responds (see
Section 2). To avoid this, we present c@1 measure
(Section 2.2), as an extension of accuracy (the pro-
portion of correctly answered questions). In Sec-
tion 3 we show that no other extension produces a
sensible measure. In Section 4 we evaluate c@1 in
terms of stability, discrimination power and sensibil-
ity, and some real examples of its behavior are given
in the context of Question Answering. Related work
is discussed in Section 5.
2 Looking for the Value of Not Responding
Lets take the scenario of Reading Comprehension
tests to argue about the development of the measure.
Our scenario assumes the following:
? There are several questions.
? Each question has several options.
? One option is correct (and only one).
The first step is to consider the possibility of not
responding. If the system responds, then the assess-
ment will be one of two: correct or wrong. But if
1415
the system doesn?t respond there is no assessment.
Since every question has a correct answer, non re-
sponse is not correct but it is not incorrect either.
This is represented in contingency Table 1, where:
? nac: number of questions for which the answer
is correct
? naw: number of questions for which the answer
is incorrect
? nu: number of questions not answered
? n: number of questions (n = nac + naw + nu)
Correct (C) Incorrect (?C)
Answered (A) nac naw
Unanswered (?A) nu
Table 1: Contingency table for our scenario
Let?s start studying a simple utility function able
to establish the preference order we want:
? -1 if question receives an incorrect response
? 0 if question is left unanswered
? 1 if question receives a correct response
Let U(i) be the utility function that returns one of
the above values for a given question i. Thus, if we
want to consider n questions in the evaluation, the
measure would be:
UF = 1
n
n
?
i=1
U(i) = nac ? naw
n
(1)
The rationale of this utility function is intuitive:
not answering adds no value and wrong answers add
negative values. Positive values of UF indicate more
correct answers than incorrect ones, while negative
values indicate the opposite. However, the utility
function is giving an arbitrary value to the prefer-
ences (-1, 0, 1).
Now we want to interpret in some way the value
that Formula (1) assigns to unanswered questions.
For this purpose, we need to transform Formula (1)
into a more meaningful measure with a parameter
for the number of unanswered questions (nu). A
monotonic transformation of (1) permit us to pre-
serve the ranking produced by the measure. Let
f(x)=0.5x+0.5 be the monotonic function to be used
for the transformation. Applying this function to
Formula (1) results in Formula (2):
0.5nac ? naw
n
+ 0.5 = 0.5
n
[nac ? naw + n] =
= 0.5
n
[nac ? naw + nac + naw + nu]
= 0.5
n
[2nac + nu] =
nac
n
+ 0.5nu
n
(2)
Measure (2) provides the same ranking of sys-
tems than measure (1). The first summand of For-
mula (2) corresponds to accuracy, while the second
is adding an arbitrary constant weight of 0.5 to the
proportion of unanswered questions. In other words,
unanswered questions are receiving the same value
as if half of them had been answered correctly.
This does not seem correct given that not answer-
ing is being rewarded in the same proportion to all
the systems, without taking into account the per-
formance they have shown with the answered ques-
tions. We need to propose a more sensible estima-
tion for the weight of unanswered questions.
2.1 A rationale for the Value of Unanswered
Questions
According to the utility function suggested, unan-
swered questions would have value as if half of them
had been answered correctly. Why half and not other
value? Even more, Why a constant value? Let?s gen-
eralize this idea and estate more clearly our hypoth-
esis:
Unanswered questions have the same value as if a
proportion of them would have been answered cor-
rectly.
We can express this idea according to contingency
Table 1 in the following way:
P (C) = P (C ?A) + P (C ? ?A) =
= P (C ?A) + P (C/?A) ? P (?A)
(3)
P (C ? A) can be estimated by nac/n, P (?A)
can be estimated by nu/n, and we have to estimate
P (C/?A). Our hypothesis is saying that P (C/?A)
1416
is different from 0. The utility measure (2) corre-
sponds to P(C) in Formula (3) where P (C/?A) re-
ceives a constant value of 0.5. It is assuming arbi-
trarily that P (C/?A) = P (C/A).
Following this, our measure must consist of two
parts: The overall accuracy and a better estimation
of correctness over the unanswered questions.
2.2 The Measure Proposed: c@1
From the answered questions we have already ob-
served the proportion of questions that received a
correct answer (P (C ?A) = nac/n). We can use this
observation as our estimation for P (C/?A) instead
of the arbitrary value of 0.5.
Thus, the measure we propose is c@1 (correct-
ness at one) and is formally represented as follows:
c@1 = nac
n
+ nac
n
nu
n
= 1
n
(nac +
nac
n
nu) (4)
The most important features of c@1 are:
1. A system that answers all the questions will re-
ceive a score equal to the traditional accuracy
measure: nu=0 and therefore c@1=nac/n.
2. Unanswered questions will add value to c@1
as if they were answered with the accuracy al-
ready shown.
3. A system that does not return any answer would
receive a score equal to 0 due to nac=0 in both
summands.
According to the reasoning above, we can inter-
pret c@1 in terms of probability as P (C) where
P (C/?A) has been estimated with P (C ? A). In
the following section we will show that there is no
other estimation for P (C/?A) able to provide a rea-
sonable evaluation measure.
3 Other Estimations for P (C/?A)
In this section we study whether other estimations
of P (C/?A) can provide a sensible measure for QA
when unanswered questions are taken into account.
They are:
1. P (C/?A) ? 0
2. P (C/?A) ? 1
3. P (C/?A) ? P (?C/?A) ? 0.5
4. P (C/?A) ? P (C/A)
5. P (C/?A) ? P (?C/A)
3.1 P (C/?A) ? 0
This estimation considers the absence of response as
incorrect response and we have the traditional accu-
racy (nac/n).
Obviously, this is against our purposes.
3.2 P (C/?A) ? 1
This estimation considers all unanswered questions
as correctly answered. This option is not reasonable
and is given for completeness: systems giving no
answer would get maximum score.
3.3 P (C/?A) ? P (?C/?A) ? 0.5
It could be argued that since we cannot have obser-
vations of correctness for unanswered questions, we
should assume equiprobability between P (C/?A)
and P (?C/?A). In this case, P(C) corresponds
to the expression (2) already discussed. As previ-
ously explained, in this case we are giving an arbi-
trary constant value to unanswered questions inde-
pendently of the system?s performance shown with
answered ones. This seems unfair. We should be
aiming at rewarding those systems not responding
instead of giving wrong answers, not reward the sole
fact that the system is not responding.
3.4 P (C/?A) ? P (C/A)
An alternative is to estimate the probability of cor-
rectness for the unanswered questions as the pre-
cision observed over the answered ones: P(C/A)=
nac/(nac+ naw). In this case, our measure would be
like the one shown in Formula (5):
P (C) = P (C ?A) + P (C/?A) ? P (?A) =
= P (C/A) ? P (A) + P (C/A) ? P (?A) =
= P (C/A) = nac
nac + naw
(5)
The resulting measure is again the observed pre-
cision over the answered ones. This is not a sensible
measure, as it would reward a cheating system that
decides to leave all questions unanswered except one
for which it is sure to have a correct answer.
1417
Furthermore, from the idea that P (C/?A) is
equal to P (C/A) the underlying assumption is that
systems choose to answer or not to answer ran-
domly, whereas we want to reward the systems that
choose not responding because they are able to de-
cide that their candidate options are wrong or be-
cause they are unable to decide which candidate is
correct.
3.5 P (C/?A) ? P (?C/A)
The last option to be considered explores the idea
that systems fail not responding in the same propor-
tion that they fail when they give an answer (i.e. pro-
portion of incorrect answers).
Estimating P (C/?A) as naw / (nac+ naw), the
measure would be:
P (C) = P (C ?A) + P (C/?A) ? P (?A) =
= P (C ?A) ? P (?C/A) ? P (?A) =
= nac
n
+ naw
nac + naw
? nu
n
(6)
This measure is very easy to cheat. It is possible
to obtain almost a perfect score just by answering in-
correctly only one question and leaving unanswered
the rest of the questions.
4 Evaluation of c@1
When a new measure is proposed, it is important
to study the reliability of the results obtained us-
ing that measure. For this purpose, we have cho-
sen the method described by Buckley and Voorhees
(2000) for assessing the stability and discrimination
power, as well as the method described by Voorhees
and Buckley (2002) for examining the sensitivity of
our measure. These methods have been used for
studying IR metrics (showing similar results with
the methods based on statistics (Sakai, 2006)), as
well as for evaluating the reliability of other QA
measures different to the ones studied here (Sakai,
2007a; Voorhees, 2002; Voorhees, 2003).
We have compared the results over c@1 with the
ones obtained using both accuracy and the utility
function (UF) defined in Formula (1). This compari-
son is useful to show how confident can a researcher
be with the results obtained using each evaluation
measure.
In the following subsections we will first show the
data used for our study. Then, the experiments about
stability and sensitivity will be described.
4.1 Data sets
We used the test collections and runs from the Ques-
tion Answering track at the Cross Language Evalu-
ation Forum 2009 (CLEF) (Pen?as et al, 2010). The
collection has a set of 500 questions with their an-
swers. The 44 runs in different languages contain
the human assessments for the answers given by ac-
tual participants. Systems could chose not to answer
a question. In this case, they had the chance to sub-
mit their best candidate in order to assess the perfor-
mance of their validation module (the one that de-
cides whether to give or not the answer).
This data collection allows us to compare c@1
and accuracy over the same runs.
4.2 Stability vs. Discrimination Power
The more stable a measure is, the lower the probabil-
ity of errors associated with the conclusion ?system
A is better than system B? is. Measures with a high
error must be used more carefully performing more
experiments than in the case of using a measure with
lower error.
In order to study the stability of c@1 and to com-
pare it with accuracy we used the method described
by Buckley and Voorhees (2000). This method al-
lows also to study the number of times systems are
deemed to be equivalent with respect to a certain
measure, which reflects the discrimination power of
that measure. The less discriminative the measure
is, the more ties between systems there will be. This
means that longer difference in scores will be needed
for concluding which system is better (Buckley and
Voorhees, 2000).
The method works as follows: let S denote a set
of runs. Let x and y denote a pair of runs from S.
Let Q denote the entire evaluation collection. Let f
represents the fuzziness value, which is the percent
difference between scores such that if the difference
is smaller than f then the two scores are deemed to
be equivalent. We apply the algorithm of Figure 1
to obtain the information needed for computing the
error rate (Formula (7)). Stability is inverse to this
value, the lower the error rate is, the more stable
the measure is. The same algorithm gives us the
1418
proportion of ties (Formula (8)), which we use for
measuring discrimination power, that is the lower
the proportion of ties is, the more discriminative the
measure is.
for each pair of runs x,y ? S
for each trial from 1 to 100
Qi = select at random subcol of size c from Q;
margin = f * max (M(x,Qi),M(y,Qi));
if(|M(x,Qi) - M(y,Qi)| < |margin|)
EQM (x,y)++;
else if(|M(x,Qi) > M(y,Qi)|)
GTM (x,y)++;
else
GTM (y,x)++;
Figure 1: Algorithm for computing EQM (x,y),
GTM (x,y) and GTM (y,x) in the stability method
We assume that for each measure the correct de-
cision about whether run x is better than run y hap-
pens when there are more cases where the value of
x is better than the value of y. Then, the number of
times y is better than x is considered as the number
of times the test is misleading, while the number of
times the values of x and y are equivalent is consid-
ered the number of ties.
On the other hand, it is clear that larger fuzziness
values decrease the error rate but also decrease the
discrimination power of a measure. Since a fixed
fuzziness value might imply different trade-offs for
different metrics, we decided to vary the fuzziness
value from 0.01 to 0.10 (following the work by Sakai
(2007b)) and to draw for each measure a proportion-
of-ties / error-rate curve. Figure 2 shows these
curves for the c@1, accuracy and UF measures. In
the Figure we can see how there is a consistent de-
crease of the error rate of all measures when the
proportion of ties increases (this corresponds to the
increase in the fuzziness value). Figure 2 shows
that the curves of accuracy and c@1 are quite simi-
lar (slightly better behavior of c@1) , which means
that they have a similar stability and discrimination
power.
The results suggest that the three measures are
quite stable, having c@1 and accuracy a lower er-
ror rate than UF when the proportion of ties grows.
These curves are similar to the ones obtained for
Figure 2: Error-rate / Proportion of ties curves for accu-
racy, c@1 and UF with c = 250
other QA evaluation measures (Sakai, 2007a).
4.3 Sensitivity
The swap-rate (Voorhees and Buckley, 2002) repre-
sents the chance of obtaining a discrepancy between
two question sets (of the same size) as to whether
a system is better than another given a certain dif-
ference bin. Looking at the swap-rates of all the
difference performance bins, the performance dif-
ference required in order to conclude that a run is
better than another for a given confidence value can
be estimated. For example, if we want to know the
required difference for concluding that system A is
better than system B with a confidence of 95%, then
we select the difference that represents the first bin
where the swap-rate is lower or equal than 0.05.
The sensitivity of the measure is the number of
times among all the comparisons in the experi-
ment where this performance difference is obtained
(Sakai, 2007b). That is, the more comparisons ac-
complish the estimated performance difference, the
more sensitive is the measure. The more sensitive
the measure, the more useful it is for system dis-
crimination.
The swap method works as follows: let S denote
a set of runs, let x and y denote a pair of runs from S.
Let Q denote the entire evaluation collection. And
let d denote a performance difference between two
runs. Then, we first define 21 performance differ-
ence bins: the first bin represents performance dif-
ferences between systems such that 0 ? d < 0.01;
the second bin represents differences such that 0.01
? d < 0.02; and the limits for the remaining bins in-
crease by increments of 0.01, with the last bin con-
taining all the differences equal or higher than 0.2.
1419
Error rateM =
?
x,y?S min(GTM (x, y), GTM (y, x))
?
x,y?S(GTM (x, y) + GTM (y, x) + EQM (x, y))
(7)
Prop T iesM =
?
x,y?S EQM (x, y)
?
x,y?S(GTM (x, y) + GTM (y, x) + EQM (x, y))
(8)
Let BIN(d) denote a mapping from a difference d to
one of the 21 bins where it belongs. Thus, algorithm
in Figure 3 is applied for calculating the swap-rate
of each bin.
for each pair of runs x,y ? S
for each trial from 1 to 100
select Qi , Q
?
i ? Q, where
Qi ? Q
?
i == ? and |Qi| == |Q
?
i| == c;
dM (Qi) = M(x,Qi)?M(y,Qi);
dM (Q
?
i) = M(x,Q
?
i)?M(y,Q
?
i);
counter(BIN(|dM (Qi)|))++;
if(dM (Qi) * dM (Q
?
i) < 0)
swap counter(BIN(|dM (Qi)|))++;
for each bin b
swap rate(b) = swap counter(b)/counter(b);
Figure 3: Algorithm for computing swap-rates
(i) (ii) (iii) (iv)
UF 0.17 0.48 35.12% 59.30%
c@1 0.09 0.77 11.69% 58.40%
accuracy 0.09 0.68 13.24% 55.00%
Table 2: Results obtained applying the swap method to
accuracy, c@1 and UF at 95% of confidence, with c =
250: (i) Absolute difference required; (ii) Highest value
obtained; (iii) Relative difference required ((i)/(ii)); (iv)
percentage of comparisons that accomplish the required
difference (sensitivity)
Given that Qi and Q
?
i must be disjoint, their size
can only be up to half of the size of the original col-
lection. Thus, we use the value c=250 for our exper-
iment1. Table 2 shows the results obtained by apply-
ing the swap method to accuracy, c@1 and UF, with
c = 250, swap-rate ? 5, and sensitivity given a con-
fidence of 95% (Column (iv)). The range of values
1We use the same size for experiments in Section 4.2 for
homogeneity reasons.
are similar to the ones obtained for other measures
according to (Sakai, 2007a).
According to Column (i), a higher absolute dif-
ference is required for concluding that a system is
better than another using UF. However, the relative
difference is similar to the one required by c@1.
Thus, similar percentage of comparisons using c@1
and UF accomplish the required difference (Column
(iv)). These results show that their sensitivity values
are similar, and higher than the value for accuracy.
4.4 Qualitative evaluation
In addition to the theoretical study, we undertook a
study to interpret the results obtained by real sys-
tems in a real scenario. The aim is to compare the
results of the proposed c@1 measure with accuracy
in order to compare their behavior. For this purpose
we inspected the real systems runs in the data set.
System c@1 accuracy (i) (ii) (iii)
icia091ro 0.58 0.47 237 156 107
uaic092ro 0.47 0.47 236 264 0
loga092de 0.44 0.37 187 230 83
base092de 0.38 0.38 189 311 0
Table 3: Example of system results in QA@CLEF 2009.
(i) number of questions correctly answered; (ii) number
of questions incorrectly answered; (iii) number of unan-
swered questions.
Table 3 shows a couple of examples where two
systems have answered correctly a similar num-
ber of questions. For example, this is the case of
icia091ro and uaic092ro that, therefore, obtain al-
most the same accuracy value. However, icia091ro
has returned less incorrect answers by not respond-
ing some questions. This is the kind of behavior we
want to measure and reward. Table 3 shows how
accuracy is sensitive only to the number of correct
answers whereas c@1 is able to distinguish when
1420
systems keep the number of correct answers but re-
duce the number of incorrect ones by not respond-
ing to some. The same reasoning is applicable to
loga092de compared to base092de for German.
5 Related Work
The decision of leaving a query without response is
related to the system ability to measure accurately its
self-confidence about the correctness of their candi-
date answers. Although there have been one attempt
to make the self-confidence score explicit and use
it (Herrera et al, 2005), rankings are, usually, the
implicit way to evaluate this self-confidence. Mean
Reciprocal Rank (MRR) has traditionally been used
to evaluate Question Answering systems when sev-
eral answers per question were allowed and given
in order (Fukumoto et al, 2002; Voorhees and Tice,
1999). However, as it occurs with Accuracy (propor-
tion of questions correctly answered), the risk of giv-
ing a wrong answer is always preferred better than
not responding.
The QA track at TREC 2001 was the first eval-
uation campaign in which systems were allowed
to leave a question unanswered (Voorhees, 2001).
The main evaluation measure was MRR, but perfor-
mance was also measured by means of the percent-
age of answered questions and the portion of them
that were correctly answered. However, no combi-
nation of these two values into a unique measure was
proposed.
TREC 2002 discarded the idea of including unan-
swered questions in the evaluation. Only one answer
by question was allowed and all answers had to be
ranked according to the system?s self-confidence in
the correctness of the answer. Systems were evalu-
ated by means ofConfidence Weighted Score (CWS),
rewarding those systems able to provide more cor-
rect answers at the top of the ranking (Voorhees,
2002). The formulation of CWS is the following:
CWS = 1
n
n
?
i=1
C(i)
i
(9)
Where n is the number of questions, and C(i) is
the number of correct answers up to the position i in
the ranking. Formally:
C(i) =
i
?
j=1
I(j) (10)
where I(j) is a function that returns 1 if answer j
is correct and 0 if it is not. The formulation of CWS
is inspired by the Average Precision (AP) over the
ranking for one question:
AP = 1
R
?
r
I(r)C(r)
r
(11)
where R is the number of known relevant results
for a topic, and r is a position in the ranking. Since
only one answer per question is requested, R equals
to n (the number of questions) in CWS. However,
in AP formula the summands belong to the posi-
tions of the ranking where there is a relevant result
(product of I(r)), whereas in CWS every position of
the ranking add value to the measure regardless of
whether there is a relevant result or not in that po-
sition. Therefore, CWS gives much more value to
some questions over others: questions whose an-
swers are at the top of the ranking are giving almost
the complete value to CWS, whereas those questions
whose answers are at the bottom of the ranking are
almost not counting in the evaluation.
Although CWS was aimed at promoting the de-
velopment of better self-confidence scores, it was
discussed as a measure for evaluating QA systems
performance. CWS was discarded in the following
campaigns of TREC in favor of accuracy (Voorhees,
2003). Subsequently, accuracy was adopted by the
QA track at the Cross-Language Evaluation Forum
from the beginning (Magnini et al, 2005).
There was an attempt to consider explicitly sys-
tems confidence self-score (Herrera et al, 2005): the
use of the Pearson?s correlation coefficient and the
proposal of measures K and K1 (see Formula 12).
These measures are based in a utility function that
returns -1 if the answer is incorrect and 1 if it is
correct. This positive or negative value is weighted
with the normalized confidence self-score given by
the system to each answer. K is a variation of K1
for being used in evaluations where more than an
answer per question is allowed.
If the self-score is 0, then the answer is ignored
and thus, this measure is permitting to leave a ques-
tion unanswered. A system that always returns a
1421
K1 =
?
i?{correctanswers}
self score(i)?
?
i?{incorrectanswers}
self score(i)
n
? [?1, 1] (12)
self-score equals to 0 (no answer) obtains a K1 value
of 0. However, the final value of K1 is difficult to
interpret: a positive value does not indicate neces-
sarily more correct answers than incorrect ones, but
that the sum of scores of correct answers is higher
than the sum resulting from the scores of incorrect
answers. This could explain the little success of this
measure for evaluating QA systems in favor, again,
of accuracy measure.
Accuracy is the simplest and most intuitive evalu-
ation measure. At the same time is able to reward
those systems showing good performance. How-
ever, together with MRR belongs to the set of mea-
sures that pushes in favor of giving always a re-
sponse, even wrong, since there is no punishment for
it. Thus, the development of better validation tech-
nologies (systems able to decide whether the can-
didate answers are correct or not) is not promoted,
despite new QA architectures require them.
In effect, most QA systems during TREC and
CLEF campaigns had an upper bound of accuracy
around 60%. An explanation for this was the effect
of error propagation in the most extended pipeline
architecture: Passage Retrieval, Answer Extraction,
Answer Ranking. Even with performances higher
than 80% in each step, the overall performance
drops dramatically just because of the product of
partial performances. Thus, a way to break the
pipeline architecture is the development of a mod-
ule able to decide whether the QA system must con-
tinue or not its searching for new candidate answers:
the Answer Validation module. This idea is behind
the architecture of IBM?s Watson (DeepQA project)
that successfully participated at Jeopardy (Ferrucci
et al, 2010).
In 2006, the first Answer Validation Exercise
(AVE) proposed an evaluation task to advance the
state of the art in Answer Validation technologies
(Pen?as et al, 2007). The starting point was the re-
formulation of Answer Validation as a Recognizing
Textual Entailment problem, under the assumption
that hypotheses can be automatically generated by
combining the question with the candidate answer
(Pen?as et al, 2008a). Thus, validation was seen as a
binary classification problem whose evaluation must
deal with unbalanced collections (different propor-
tion of positive and negative examples, correct and
incorrect answers). For this reason, AVE 2006 used
F-measure based on precision and recall for correct
answers selection (Pen?as et al, 2007). Other op-
tion is an evaluation based on the analysis of Re-
ceiver Operating Characteristic (ROC) space, some-
times preferred for classification tasks with unbal-
anced collections. A comparison of both approaches
for Answer Validation evaluation is provided in (Ro-
drigo et al, 2011).
AVE 2007 changed its evaluation methodology
with two objectives: the first one was to bring sys-
tems based on Textual Entailment to the Automatic
Hypothesis Generation problem which is not part it-
self of the Recognising Textual Entailment (RTE)
task but an Answer Validation need. The second
one was an attempt to quantify the gain in QA per-
formance when more sophisticated validation mod-
ules are introduced (Pen?as et al, 2008b). With this
aim, several measures were proposed to assess: the
correct selection of candidate answers, the correct
rejection of wrong answer and finally estimate the
potential gain (in terms of accuracy) that Answer
Validation modules can provide to QA (Rodrigo et
al., 2008). The idea was to give value to the cor-
rectly rejected answers as if they could be correctly
answered with the accuracy shown selecting the cor-
rect answers. This extension of accuracy in the An-
swer Validation scenario inspired the initial develop-
ment of c@1 considering non-response.
6 Conclusions
The central idea of this work is that not respond-
ing has more value than responding incorrectly. This
idea is not new, but despite several attempts in TREC
and CLEF there wasn?t a commonly accepted mea-
1422
sure to assess non-response. We have studied here
an extension of accuracy measure with this feature,
and with a very easy to understand rationale: Unan-
swered questions have the same value as if a pro-
portion of them had been answered correctly, and
the value they add is related to the performance (ac-
curacy) observed over the answered questions. We
have shown that no other estimation of this value
produce a sensible measure.
We have shown also that the proposed measure
c@1 has a good balance of discrimination power,
stability and sensitivity properties. Finally, we have
shown how this measure rewards systems able to
maintain the same number of correct answers and at
the same time reduce the number of incorrect ones,
by leaving some questions unanswered.
Among other tasks, measure c@1 is well suited
for evaluating Reading Comprehension tests, where
multiple choices per question are given, but only one
is correct. Non-response must be assessed if we
want to measure effective reading and not just the
ability to rank options. This is clearly not enough
for the development of reading technologies.
Acknowledgments
This work has been partially supported by the
Research Network MA2VICMR (S2009/TIC-1542)
and Holopedia project (TIN2010-21128-C02).
References
Chris Buckley and Ellen M. Voorhees. 2000. Evalu-
ating evaluation measure stability. In Proceedings of
the 23rd annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 33?40. ACM.
David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James
Fan, David Gondek, Aditya A. Kalyanpur, Adam
Lally, J. William Murdock, Eric Nyberg, John Prager,
Nico Schlaefer, and Chris Welty. 2010. Building Wat-
son: An Overview of the DeepQA Project. AI Maga-
zine, 31(3).
Junichi Fukumoto, Tsuneaki Kato, and Fumito Masui.
2002. Question and Answering Challenge (QAC-
1): Question Answering Evaluation at NTCIR Work-
shop 3. In Working Notes of the Third NTCIR Work-
shop Meeting Part IV: Question Answering Challenge
(QAC-1), pages 1-10.
Jesu?s Herrera, Anselmo Pen?as, and Felisa Verdejo. 2005.
Question Answering Pilot Task at CLEF 2004. InMul-
tilingual Information Access for Text, Speech and Im-
ages, CLEF 2004, Revised Selected Papers., volume
3491 of Lecture Notes in Computer Science, Springer,
pages 581?590.
Bernardo Magnini, Alessandro Vallin, Christelle Ayache,
Gregor Erbach, Anselmo Pen?as, Maarten de Rijke,
Paulo Rocha, Kiril Ivanov Simov, and Richard F. E.
Sutcliffe. 2005. Overview of the CLEF 2004 Multi-
lingual Question Answering Track. InMultilingual In-
formation Access for Text, Speech and Images, CLEF
2004, Revised Selected Papers., volume 3491 of Lec-
ture Notes in Computer Science, Springer, pages 371?
391.
Anselmo Pen?as, A?lvaro Rodrigo, Valent??n Sama, and Fe-
lisa Verdejo. 2007. Overview of the Answer Valida-
tion Exercise 2006. In Evaluation of Multilingual and
Multi-modal Information Retrieval, CLEF 2006, Re-
vised Selected Papers, volume 4730 of Lecture Notes
in Computer Science, Springer, pages 257?264.
Anselmo Pen?as, A?lvaro Rodrigo, Valent??n Sama, and Fe-
lisa Verdejo. 2008a. Testing the Reasoning for Ques-
tion Answering Validation. In Journal of Logic and
Computation. 18(3), pages 459?474.
Anselmo Pen?as, A?lvaro Rodrigo, and Felisa Verdejo.
2008b. Overview of the Answer Validation Exercise
2007. In Advances in Multilingual and Multimodal
Information Retrieval, CLEF 2007, Revised Selected
Papers, volume 5152 of Lecture Notes in Computer
Science, Springer, pages 237?248.
Anselmo Pen?as, Pamela Forner, Richard Sutcliffe, A?lvaro
Rodrigo, Corina Forascu, In?aki Alegria, Danilo Gi-
ampiccolo, Nicolas Moreau, and Petya Osenova.
2010. Overview of ResPubliQA 2009: Question An-
swering Evaluation over European Legislation. In
Multilingual Information Access Evaluation I. Text Re-
trieval Experiments, CLEF 2009, Revised Selected Pa-
pers, volume 6241 of Lecture Notes in Computer Sci-
ence, Springer.
Alvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.
2008. Evaluating Answer Validation in Multi-stream
Question Answering. In Proceedings of the Second In-
ternational Workshop on Evaluating Information Ac-
cess (EVIA 2008).
Alvaro Rodrigo, Anselmo Pen?as, and Felisa Verdejo.
2011. Evaluating Question Answering Validation as a
classification problem. Language Resources and Eval-
uation, Springer Netherlands (In Press).
Tetsuya Sakai. 2006. Evaluating Evaluation Metrics
based on the Bootstrap. In SIGIR 2006: Proceedings
of the 29th Annual International ACM SIGIR Confer-
ence on Research and Development in Information Re-
trieval, Seattle, Washington, USA, August 6-11, 2006,
pages 525?532.
1423
Tetsuya Sakai. 2007a. On the Reliability of Factoid
Question Answering Evaluation. ACM Trans. Asian
Lang. Inf. Process., 6(1).
Tetsuya Sakai. 2007b. On the reliability of information
retrieval metrics based on graded relevance. Inf. Pro-
cess. Manage., 43(2):531?548.
Ellen M. Voorhees and Chris Buckley. 2002. The effect
of Topic Set Size on Retrieval Experiment Error. In SI-
GIR ?02: Proceedings of the 25th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 316?323.
Ellen M. Voorhees and DawnM. Tice. 1999. The TREC-
8 Question Answering Track Evaluation. In Text Re-
trieval Conference TREC-8, pages 83?105.
Ellen M. Voorhees. 2001. Overview of the TREC 2001
Question Answering Track. In E. M. voorhees, D. K.
Harman, editors: Proceedings of the Tenth Text RE-
trieval Conference (TREC 2001). NIST Special Publi-
cation 500-250.
Ellen M. Voorhees. 2002. Overview of TREC 2002
Question Answering Track. In E.M. Voorhees, L. P.
Buckland, editors: Proceedings of the Eleventh Text
REtrieval Conference (TREC 2002). NIST Publication
500-251.
Ellen M. Voorhees. 2003. Overview of the TREC 2003
Question Answering Track. In Proceedings of the
Twelfth Text REtrieval Conference (TREC 2003).
1424
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 107?116,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Temporally Anchored Relation Extraction
Guillermo Garrido, Anselmo Pen?as, Bernardo Cabaleiro, and A?lvaro Rodrigo
NLP & IR Group at UNED
Madrid, Spain
{ggarrido,anselmo,bcabaleiro,alvarory}@lsi.uned.es
Abstract
Although much work on relation extraction
has aimed at obtaining static facts, many of
the target relations are actually fluents, as their
validity is naturally anchored to a certain time
period. This paper proposes a methodologi-
cal approach to temporally anchored relation
extraction. Our proposal performs distant su-
pervised learning to extract a set of relations
from a natural language corpus, and anchors
each of them to an interval of temporal va-
lidity, aggregating evidence from documents
supporting the relation. We use a rich graph-
based document-level representation to gener-
ate novel features for this task. Results show
that our implementation for temporal anchor-
ing is able to achieve a 69% of the upper
bound performance imposed by the relation
extraction step. Compared to the state of the
art, the overall system achieves the highest
precision reported.
1 Introduction
A question that arises when extracting a relation is
how to capture its temporal validity: Can we assign a
period of time when the obtained relation held? As
pointed out in (Ling and Weld, 2010), while much
research in automatic relation extraction has focused
on distilling static facts from text, many of the tar-
get relations are in fact fluents, dynamic relations
whose truth value is dependent on time (Russell and
Norvig, 2010).
The Temporally anchored relation extraction
problem consists in, given a natural language text
document corpus, C, a target entity, e, and a target
relation, r, extracting from the corpus the value of
that relation for the entity, and a temporal interval
for which the relation was valid.
In this paper, we introduce a methodological ap-
proach to temporal anchoring of relations automat-
ically extracted from unrestricted text. Our system
(see Figure 1) extracts relational facts from text us-
ing distant supervision (Mintz et al, 2009) and then
anchors the relation to an interval of temporal va-
lidity. The intuition is that a distant supervised sys-
tem can effectively extract relations from the source
text collection, and a straightforward date aggrega-
tion can then be applied to anchor them. We pro-
pose a four step process for temporal anchoring:
(1) represent temporal evidence; (2) select tempo-
ral information relevant to the relation; (3) decide
how a relational fact and its relevant temporal in-
formation are themselves related; and (4) aggregate
imprecise temporal intervals across multiple docu-
ments. In contrast with previous approaches that
aim at intra-document temporal information extrac-
tion (Ling and Weld, 2010), we focus on mining
a corpus aggregating temporal evidences across the
supporting documents.
We address the following research questions:
(1) Validate whether distant supervised learning is
suitable for the task, and evaluate its shortcomings.
(2) Explore whether the use of features extracted
from a document-level rich representation could im-
prove distant supervised learning. (3) Compare the
use of document metadata against temporal expres-
sions within the document for relation temporal an-
choring. (4) Analyze how, in a pipeline architecture,
the propagation of errors limits the overall system?s
107
Training
(1) IR candidate document 
retrieval
(3) Distant supervised
      learning
(5) Relation Extraction
(6) Temporal Anchoring
Document
Collection
Document 
Index
(2) Document 
Representation
(4) Classifiers
Knowledge
Base
Training seeds
< entity, relation name, value >
Training
examples
+ / -
relation 
instances
unlabelled
candidate
Training
Application
Date Extraction
output:
temporally 
anchored 
relations
Date 
Aggregation
Input: Query 
entity
Figure 1: System overview diagram.
performance.
The representation we use for temporal informa-
tion is detailed in section 2; the rich document-level
representation we exploit is described in section 3.
For a query entity and target relation, the system first
performs relation extraction (section 4); then, we
find and aggregate time constraint evidence for the
same relation across different documents, to estab-
lish a temporal validity anchor interval (section 5).
Empirical comparative evaluation of our approach is
introduced in section 6; while some related work is
shown in section 7 and conclusions in section 8.
2 Temporal Anchors
We will denominate relation instance a triple
?entity, relation name, value?. We aim at anchor-
ing relation instances to their temporal validity. We
need a representation flexible enough to capture the
imprecise temporal information available in text,
but expressed in a structured style. Allen?s (1983)
interval-based algebra for temporal representation
and reasoning, underlies much research, such as the
Tempeval challenges (Verhagen et al, 2007; Puste-
jovsky and Verhagen, 2009). Our task is different,
as we focus on obtaining the temporal interval as-
sociated to a fact, rather than reasoning about the
temporal relations among the events appearing in a
single text.
Let us assume that each relation instance is valid
during a certain temporal interval, I = [t0, tf ]. This
sharp temporal interval fails to capture the impreci-
sion of temporal boundaries conveyed in natural lan-
guage text. The Temporal Slot Filling task at TAC-
KBP 2011 (Ji et al, 2011) proposed a 4-tuple rep-
resentation that we will refer to as imprecise anchor
intervals. An imprecise temporal interval is defined
as an ordered 4-tuple of time points: (t1, t2, t3, t4),
with the following semantics: the relation is true for
a period which starts at some point between t1 and
t2 and ends between t3 and t4. It should hold that:
t1 ? t2, t3 ? t4, and t1 ? t4. Any of the four
endpoints can be left unconstrained (t1 or t3 would
be ??, and t2 or t4 would be +?). This represen-
tation is flexible and expressive, although it cannot
capture certain types of information (Ji et al, 2011).
3 Document Representation
We use a rich document representation that employs
a graph structure obtained by augmenting the syn-
tactic dependency analysis of the document with se-
mantic information.
A document D is represented as a document
graph GD; with node set VD and edge set, ED. Each
node v ? VD represents a chunk of text, which is a
sequence of words1. Each node is labeled with a
dictionary of attributes, some of which are common
for every node: the words it contains, their part-of-
speech annotations (POS) and lemmas. Also, a rep-
resentative descriptor, which is a normalized string
value, is generated from the chunks in the node. Cer-
tain nodes are also annotated with one or more types.
There are three families of types: Events (verbs
that describe an action, annotated with tense, polar-
ity and aspect); standardized Time Expressions; and
Named Entities, with additional annotations such as
gender or age.
Edges in the document graph, e ? ED, represent
four kinds of relations between the nodes:
? Syntactic: a dependency relation.
? Coreference: indicates that two chunks refer to
1Most chunks consist in one word; we join words into a
chunk (and a node) in two cases: a multi-word named entity
and a verb and its auxiliaries.
108
David[NNP,David]
NER: PERSON
DESCRIPTOR: 
David
POS: N 
Julia[NNP,Julia]
CLASS:WIFE
NER: PERSON
DESCRIPTOR: 
Julia
POS: N
GENDER:FEMALE 
September[NNP,September] 1979[CD,1979]
NER:DATE
TIMEVALUE:197909
DESCRIPTOR: September 1979
POS: NNP 
wife[NN,wife]
DESCRIPTOR: 
wife
POS: NN 
is[VBZ,be] celebrating[VBG,celebrate]
ASPECT:PROGRESSIVE
TENSE:PRESENT
POLARITY:POS
DESCRIPTOR: celebrate
POS: V 
birthday[NN,birthday]
DESCRIPTOR: 
birthday
POS: NN 
was[VBD,be] born[VBN,bear]
ASPECT:NONE
TENSE:PAST
POLARITY:POS
DESCRIPTOR: bear
POS: V 
arg0
hasClass
prep_in
arg1
arg1
has
INCLUDES
has_wife
Figure 2: Collapsed document graph representation, GC ,
for the sample text document ?David?s wife, Julia, is cel-
ebrating her birthday. She was born in September 1979?.
the same discourse referent.
? Semantic relations between two nodes, such as
hasClass, hasProperty and hasAge.
? Temporal relations between events and time ex-
pressions.
The processing includes dependency parsing,
named entity recognition and coreference reso-
lution, done with the Stanford CoreNLP soft-
ware (Klein and Manning, 2003); and events and
temporal information extraction, via the TARSQI
Toolkit (Verhagen et al, 2005).
The document graph GD is then further trans-
formed into a collapsed document graph, GC . Each
node of GC clusters together coreferent nodes, rep-
resenting a discourse referent. Thus, a node u in GC
is a cluster of nodes u1, . . . , uk of GD. There is an
edge (u, v) in GC if there was an edge between any
of the nodes clustered into u and any of the nodes
v1, . . . , vk? . The coreference edges do not appear in
this representation. Additional semantic information
is also blended into this representation: normaliza-
tion of genitives, semantic class indicators inferred
from appositions and genitives, and gender annota-
tion inferred from pronouns. A final graph example
can be seen in Figure 2.
4 Distant Supervised Relation Extraction
To perform relation extraction, our proposal fol-
lows a distant supervision approach (Mintz et al,
2009), which has also inspired other slot filling sys-
tems (Agirre et al, 2009; Surdeanu et al, 2010).
We capture long distance relations by introducing
a document-level representation and deriving novel
features from deep syntactic and semantic analysis.
Seed harvesting. From a reference Knowledge
Base (KB), we extract a set of relation triples
or seeds: ?entity, relation, value?, where the
relation is one of the target relations. Our
document-level distant supervision assumption is
that if entity and value are found in a document
graph (see section 3), and there is a path connect-
ing them, then the document expresses the relation.
Relation candidates gathering. From a seed triple,
we retrieve candidate documents that contain both
the entity and value, within a span of 20 tokens,
using a standard IR approach. Then, entity and
value are matched to the document graph represen-
tation. We first use approximate string comparison
to find nodes matching the seed entity. After an en-
tity node has been found we use local breadth-first-
search (BFS) to find a matching value and the short-
est connecting path between them. We enforce the
Named Entity type of entity and value to match a
expected type, predefined for the relation.
Our procedure traverses the document graph look-
ing for entity and value nodes meeting those condi-
tions; when found, we generate features for a pos-
itive example for the relation2. If we encounter a
node that matches the expected NE type of the rela-
tion, but does not match the seed value, we generate
a negative example for that relation.
Training. From positive and negative examples, we
generate binary features; some of them are inspired
by previous work (Surdeanu and Ciaramita, 2007;
Mintz et al, 2009; Riedel et al, 2010; Surdeanu et
al., 2010), and others are novel, taking advantage of
our graph representation. Table 1 summarizes our
choice of features. Features appearing in less than 5
training examples were discarded.
Relation instance extraction. Given an input entity
and a target relation, we aim at finding a filler value
for a relation instance. This task is known as Slot
Filling. From the set of retrieved documents relevant
to the query entity, represented as document graphs,
2From the collapsed document graph representation we ob-
tained an average of 9213 positive training examples per slot;
from the uncollapsed document graph, a slightly lower average
of 8178.5 positive examples per slot.
109
Feature name Description
path dependency path between ENTITY and
VALUE in the sentence
X-annotation NE annotations for X
X-pos Part-of-speech annotations for X
X-gov Governor of X in the dependency path
X-mod Modifiers of X in the dependency path
X-has age X is a NE, with an age attribute
X-has class-C X is a NE, with a class C
X-property-P X is a NE, and it has a property P
X-has-Y X is a NE, with a possessive relation with
another NE, Y
X-is-Y X is a NE, in a copula with another NE, Y
X-gender-G X is a NE, and it has gender G
V -tense Tense of the verb V in the path
V -aspect Aspect of the verb V in the path
V -polarity Polarity (positive or negative) of the verb V
Table 1: Features included in the model. X stands for
ENTITY and VALUE. Verb features are generated from
the verbs, V , identified in the path between ENTITY and
VALUE.
we locate matching entities and start a local BFS of
candidate values, generating for them an unlabelled
example. For each of the relations to extract, a bi-
nary classifier (extractor) decides whether the exam-
ple is a valid relation instance. For each particular
relation classifier, only candidates with the expected
entity and value types for the relation were used in
the application phase. Each extractor was a SVM
classifier with linear kernel (Joachims, 2002). All
learning parameters were set to their default values.
The classification process yields a predicted class
label, plus a real number indicating the margin. We
performed an aggregation phase to sum the mar-
gins over distinct occurrences of the same extracted
value. The rationale is that when the same value is
extracted from more than one document, we should
accumulate that evidence.
The output of this phase is the set of extracted re-
lations (positive for each of the classifiers), plus the
documents where the same fact was detected (sup-
porting documents).
5 Temporal Anchoring of Relations
In this section, we propose and discuss a unified
methodological approach for temporal anchoring of
relations. We assume the input is a relation instance
and a set of supporting documents. The task is es-
tablishing a imprecise temporal anchor interval for
the relation.
We present a four-step methodological approach:
(1) representation of intra-document temporal infor-
mation; (2) selection of relevant temporal informa-
tion for the relation; (3) mapping of the link between
relational fact and temporal information into an in-
terval; and (4) aggregation of imprecise intervals.
Temporal representation. The first methodologi-
cal step is to obtain and represent the available intra-
document temporal information; the input is a doc-
ument, and the task is to identify temporal signals
and possible links among them. We use the term link
for a relation between a temporal expression (a date)
and an event; we want to avoid confusion with the
term relation (a relational fact extracted from text).
In our particular implementation:
? We use TARSQI to extract temporal expressions
and link them to events. In particular, TARSQI
uses the following temporal links: included, si-
multaneous, after, before, begun by or ended.
? We focus also on the syntactic pattern [Event-
preposition-Time] within the lexical context of the
candidate entity and value.
? Both are normalized into one from a set of prede-
fined temporal links: within, throughout, begin-
ning, ending, after and before.
Selection of temporal evidence. For each docu-
ment and relational instance, we have to select those
temporal expressions that are relevant.
a. Document-level metadata. The default value
we use is the document creation time (DCT),
if available. The underlying assumption is that
there is a within link from each fact expressed in
the text and the document creation time.
b. Temporal expressions. Temporal evidence
comes also from the temporal expressions
present in the context of a relation. In our par-
ticular implementation, we followed a straight-
forward approach, looking for the time expres-
sion closest in the document graph to the short-
est path between the entity and value nodes. This
search is performed via a limited depth BFS,
starting from the nodes in the path, in order from
value to entity.
Mapping of temporal links into intervals. The
third step is deciding how a relational fact and its rel-
evant temporal information are themselves related.
We have to map this information, expressed in text,
110
Temporal link Constraints mapping
Before t4 = first
After t1 = last
Within and Throughout t2 = first and t3 = last
Beginning t1 = first and t2 = last
Ending t3 = first and t4 = last
Table 2: Mapping from time expression and temporal re-
lation to temporal constraints.
to a temporal representation. We will use the impre-
cise anchor intervals described is section 2.
Let T be a temporal expression identified in the
document or its metadata. Now, the mapping of tem-
poral constraints depends on the temporal link to the
time expression identified; also, the semantics of the
event have to be considered in order to decide the
time period associated to a relation instance. This
step is important because the event could refer just to
the beginning of the relation, its ending, or both. For
instance, it is obvious that having the event marry
is different to having the event divorce, when decid-
ing the temporal constraints associated to the spouse
relation.
Table 2 shows our particular mapping between
temporal links and constraints. In particular, for the
default document creation time, we suppose that a
relation which appears in a document with creation
time d held true at least in that date; that is, we are
assuming a within link, and we map t2 = d, t3 = d.
Inter-document temporal evidence aggregation.
The last step is aggregating all the time constraints
found for the same relation and value across differ-
ent documents. If we found that a relation started af-
ter two dates d and d?, where d? > d, the closest con-
straint to the real start of the relation is d?. Mapped to
temporal constraints, it means that we would choose
the biggest t1 possible. Following the same reason-
ing, we would want to maximize t3. On the other
side, when a relation started before two dates d2 and
d?2, where d
?
2 > d2, the closest constraint is d2 and
we would choose the smallest t2. In summary, we
will maximize t1 and t3 and minimize t2 and t4, so
we will narrow the margins.
6 Evaluation
We have used for our evaluation the dataset com-
piled within the TAC-KBP 2011 Temporal Slot Fill-
ing Task (Ji et al, 2011). We employed as initial
KB the one distributed to participants in the task,
which has been compiled from Wikipedia infoboxes.
It contains 898 triples ?entity, slot type, value? for
100 different entities and up to 8 different slots (re-
lations) per entity3. This gold standard contains the
correct responses pooled from the participant sys-
tems plus a set of responses manually found by
annotators. Each triple has associated a temporal
anchor. The relations had to be extracted from a
domain-general collection of 1.7 million documents.
Our system was one of the five that took part in
the task.We have evaluated the overall system and
the two main components of the architecture: Rela-
tion Extraction, and Temporal Anchoring of the re-
lations. Due to space limitations, the description of
our implementation is very concise; refer to (Garrido
et al, 2011) for further details.
6.1 Evaluation of Relation Extraction
System response in the relation extraction step con-
sists in a set of triples ?entity, slot type, value?.
Performance is measured using precision, recall and
F-measure (harmonic mean) with respect to the 898
triples in the key. Target relations (slots) are poten-
tially list-valued, that is, more than one value can
be valid for a relation (possibly at different points
in time). Only correct values yield any score, and
redundant triples are ignored.
Experiments. We run two different system settings
for the relation extraction step. They differ in the
document representation used (detailed in section3),
in order to empirically assess whether clustering of
discourse referents into single nodes benefits the ex-
traction. In SETTING 1, each document is repre-
sented as a document graph, GD, while in SETTING
2 collapsed document graph representation, GC , is
employed.
Results. Results are shown in Table 3 in the col-
umn Relation Extraction. Both settings have a sim-
ilar performance with a slight increase in the case
of graphs with clustered referents. Although preci-
sion is close to 0.5, recall is lower than 0.1. We have
studied the limits of the assumptions our approach
3There are 7 person relations: cities of residence, state-
orprovinces of residence, countries of residence, employee of,
member of, title, spouse, and an organization relation:
top members/employees.
111
is based on. First, our standard retrieval component
performance limits the overall system?s. As a matter
of example, if we retrieve the first 100 documents
per entity, we find relevant documents only for 62%
of the triples in the key. This number means that no
matter how good relation extraction method is, 38%
of relations will not be found.
Second, the distant supervision assumption un-
derlying our approach is that for a seed relation in-
stance ?entity, relation, value?, any textual men-
tion of entity and value expresses the relation. It
has been shown that this assumption is more often
violated when training knowledge base and docu-
ment collection are of different type, e.g. Wikipedia
and news-wire (Riedel et al, 2010). We have real-
ized that a more determinant factor is the relation
itself and the type of arguments it takes. We ran-
domly sampled 100 training examples per relation,
and manually inspected them to assess if they were
indeed mentions of the relation. While for the re-
lation cities of residence only 30% of the training
examples are expressing the relation, for spouse the
number goes up to 59%. For title, up to 90% of the
examples are correct. This fact explains, at least par-
tially, the zeros we obtain for some relations.
6.2 Evaluation of Temporal Anchoring
Under the evaluation metrics proposed by TAC-KBP
2011, if the value of the relation instance is judged
as correct, the score for temporal anchoring depends
on how well the returned interval matches the one
provided in the key. More precisely, let the correct
imprecise anchor interval in the gold standard key
be Sk = (k1, k2, k3, k4) and the system response be
S = (r1, r2, r3, r4). The absence of a constraint in
t1 or t3 is treated as a value of ??; the absence of
a constraint in t2 or t4 is treated as a value of +?.
Then, let di = |ki ? ri|, for i ? 1, . . . , 4, be the
difference, a real number measured in years. The
score for the system response is:
Q(S) =
1
4
4?
i=1
1
1 + di
The score for a target relation Q(r) is computed
by summing Q(S) over all unique instances of the
relation whose value is correct. If the gold standard
contains N responses, and the system output M re-
sponses, then precision is: P = Q(r)/M , and recall:
R = Q(r)/N ; F1 is the harmonic mean of P and R.
Experiments. We evaluated two different set-
tings for the temporal anchoring step; both use
the collapsed document graph representation, GC
(SETTING 2). The goal of the experiment is two-
fold. First, test the strength of the document creation
time as evidence for temporal anchoring. Second,
test how hard this metadata-level baseline is to beat
using contextual temporal expressions.
The SETTING 2-I assumes a within temporal link
between the document creation time and any relation
expressed inside the document, and aggregates this
information across the documents that we have iden-
tified as supporting the relation. The SETTING 2-II
considers documents content in order to extract tem-
poral links from the context of the text that expresses
the relation. If no temporal expression is found, the
date of the document is used as default. Temporal
links from all supporting documents are mapped into
intervals and aggregated as detailed in section 5.
The performance on relation extraction is an up-
per bound for temporal anchoring, attainable if tem-
poral anchoring is perfect. Thus, we also evaluate
the temporal anchoring performance as the percent-
age the final system achieves with respect to the re-
lation extraction upper bound.
Results. Results are shown in Table 3 under column
Temporal Anchoring. They are low, due to the upper
bound that error propagation in candidate retrieval
and relation extraction imposes upon this step: tem-
porally anchoring alone achives 69% of its upper
bound. This value corresponds to the baseline SET-
TING 2-I, showing its strength. The difference with
SETTING 2-II shows that this baseline is difficult
to beat by considering temporal evidence inside the
document content. There is a reason for this. The
temporal link mapping into time intervals does not
depend only on the type of link, but also on the se-
mantics of the text that expresses the relation as we
pointed out above. We have to decide how to trans-
form the link between relation and temporal expres-
sion into a temporal interval. Learning a model for
this is a hard open research problem that has a strong
adversary in the baseline proposed.
112
Relation Extraction Temporal Anchoring
SETTING 1 SETTING 2 SETTING 2-I SETTING 2-II
P R F P R F P R F % P R F %
(1) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(2) 0 0 0 0 0 0 0 0 0 0 0 0 0 0
(3) 0.33 0.02 0.03 0 0 0 0 0 0 0 0 0 0 0
(4) 0.22 0.09 0.13 0.29 0.11 0.16 0.23 0.09 0.13 79 0.21 0.08 0.11 72
(5) 0.53 0.13 0.20 0.54 0.12 0.19 0.34 0.07 0.12 63 0.30 0.06 0.11 56
(6) 0.70 0.12 0.20 0.75 0.13 0.22 0.57 0.10 0.16 76 0.50 0.08 0.14 67
(7) 0.50 0.06 0.10 0.50 0.07 0.12 0.29 0.04 0.07 58 0.25 0.04 0.06 50
(8) 0.25 0.04 0.07 0.20 0.04 0.07 0.15 0.03 0.05 75 0.06 0.01 0.02 30
(9) 0.42 0.08 0.14 0.45 0.08 0.14 0.31 0.06 0.10 69 0.27 0.05 0.09 60
Table 3: Results of experiments for each relation: (1) per:stateorprovinces of residence; (2) per:employee of; (3)
per:countries of residence; (4) per:member of; (5) per:title; (6) org:top members/employees; (7) per:spouse; (8)
per:cities of residence; (9) overall results (calculated as a micro-average).
System # Filled Precision Recall F1
BLENDER2 1206 0.1789 0.3030 0.2250
BLENDER1 1116 0.1796 0.2942 0.2231
BLENDER3 1215 0.1744 0.2976 0.2199
IIRG1 346 0.2457 0.1194 0.1607
Setting 2-1 167 0.2996 0.0703 0.1139
Setting 2-2 167 0.2596 0.0609 0.0986
Stanford 12 5140 0.0233 0.1680 0.0409
Stanford 11 4353 0.0238 0.1453 0.0408
USFD20112 328 0.0152 0.0070 0.0096
USFD20113 127 0.0079 0.0014 0.0024
Table 4: System ID, number of filled responses of the
system, precision, recall and F measure.
6.3 Comparative Evaluation
Our approach was compared with the other four
participants at the KBP Temporal Slot Filling Task
2011. Table 4 shows results sorted by F-measure in
comparison to our two settings (described above).
These official results correspond to a previous
dataset containing 712 triples4.
As shown in column Filled our approach returns
less triples than other systems, explaining low recall.
However, our system achieves the highest precision
for the complete task of temporally anchored rela-
tion extraction. Despite low recall, our system ob-
tains the third best F1 value. This is a very promis-
ing result, since several directions can be explored
to consider more candidates and increase recall.
7 Related Work
Compiling a Knowledge Base of temporally an-
chored facts is an open research challenge (Weikum
et al, 2011). Despite the vast amount of research fo-
cusing on understanding temporal expressions and
4Slot-fillers from human assessors were not considered
their relation to events in natural language, the com-
plete problem of temporally anchored relation ex-
traction remains relatively unexplored. Also, while
much research has focused on single-document ex-
traction, it seems clear that extracting temporally an-
chored relations needs the aggregation of evidences
across multiple documents.
There have been attempts to extend an existing
knowledge base. Wang et al (2010) use regular
expressions to mine Wikipedia infoboxes and cat-
egories and it is not suited for unrestricted text. An
earlier attempt (Zhang et al, 2008), is specific for
business and difficult to generalize to other relations.
Two recent promising works are more related to our
research. Wang et al (2011) uses manually defined
patterns to collect candidate facts and explicit dates,
and re-rank them using a graph label propagation al-
gorithm; their approach is complementary to ours,
as our aim is not to harvest temporal facts but to
extract the relations in which a query entity takes
part; unlike us, they require entity, value, and a ex-
plicit date to appear in the same sentence. Talukdar
et al (2012) focus on the partial task of temporally
anchoring already known facts, showing the useful-
ness of the document creation time as temporal sig-
nal, aggregated across documents.
Earlier work has dealt mainly with partial aspects
of the problem. The TempEval community focused
on the classification of the temporal links between
pairs of events, or an event and a temporal expres-
sion; using shallow features (Mani et al, 2003; La-
pata and Lascarides, 2004; Chambers et al, 2007),
or syntactic-based structured features (Bethard and
Martin, 2007; Pus?cas?u, 2007; Cheng et al, 2007).
Aggregating evidence across different documents
113
to temporally anchor facts has been explored in set-
tings different to Information Extraction, such as
answering of definition questions (Pas?ca, 2008) or
extracting possible dates of well-known historical
events (Schockaert et al, 2010).
Temporal inference or reasoning to solve con-
flicting temporal expressions and induce temporal
order of events has been used in TempEval (Tatu
and Srikanth, 2008; Yoshikawa et al, 2009) and
ACE (Gupta and Ji, 2009) tasks, but focused on
single-document extraction. Ling et al (2010), use
cross-event joint inference to extract temporal facts,
but only inside a single document.
Evaluation campaigns, such as ACE and TAC-
KBP 2011 have had an important role in promoting
this research. While ACE required only to identify
time expressions and classify their relation to events,
KBP requires to infer explicitly the start/end time of
relations, which is a realistic approach in the context
of building time-aware knowledge bases. KBP rep-
resents an important step for the evaluation of tem-
poral information extraction systems. In general, the
participant systems adapted existing slot filling sys-
tems, adding a temporal classification component:
distant supervised (Chen et al, 2010; Surdeanu et
al., 2010) on manually-defined patterns (Byrne and
Dunnion, 2010).
8 Conclusions
This paper introduces the problem of extracting,
from unrestricted natural language text, relational
knowledge anchored to a temporal span, aggregat-
ing temporal evidence from a collection of docu-
ments. Although compiling time-aware knowledge
bases is an important open challenge (Weikum et
al., 2011), it has remained unexplored until very re-
cently (Wang et al, 2011; Talukdar et al, 2012).
We have elucidated the two challenges of the task,
namely relation extraction and temporal anchoring
of the extracted relations.
We have studied how, in a pipeline architecture,
the propagation of errors limits the overall system?s
performance. The performance attainable in the full
task is limited by the quality of the output of the
three main phases: retrieval of candidate passages/
documents, extraction of relations and temporal an-
choring of those.
We have also studied the limits of the distant su-
pervision approach to relation extraction, showing
empirically that its performance depends not only
on the nature of reference knowledge base and doc-
ument corpus (Riedel et al, 2010), but also on the
relation to be extracted. Given a relation between
two arguments, if it is not dominant among textual
expressions of those arguments, the distant supervi-
sion assumption will be more often violated.
We have introduced a novel graph-based docu-
ment level representation, that has allowed us to gen-
erate new features for the task of relation extraction,
capturing long distance structured contexts. Our re-
sults show how, in a document level syntactic repre-
sentation, it yields better results to collapse corefer-
ent nodes.
We have presented a methodological approach
to temporal anchoring composed of: (1) intra-
document temporal information representation; (2)
selection of relation-dependent relevant temporal in-
formation; (3) mapping of temporal links to an inter-
val representation; and (4) aggregation of imprecise
intervals.
Our proposal has been evaluated within a frame-
work that allows for comparability. It has been able
to extract temporally anchored relational informa-
tion with the highest precision among the partici-
pant systems taking part in the competitive evalu-
ation TAC-KBP 2011.
For the temporal anchoring sub-problem, we have
demonstrated the strength of the document creation
time as a temporal signal. It is possible to achieve
a performance of 69% of the upper-bound imposed
by relation extraction by assuming that any relation
mentioned in a document held at the document cre-
ation time (there is a within link between the rela-
tional fact and the document creation time). This
baseline has proved stronger than extracting and an-
alyzing the temporal expressions present in the doc-
ument content.
Acknowledgments
This work has been partially supported by the Span-
ish Ministry of Science and Innovation, through
the project Holopedia (TIN2010-21128-C02), and
the Regional Government of Madrid, through the
project MA2VICMR (S2009/TIC1542).
114
References
Eneko Agirre, Angel X. Chang, Daniel S. Jurafsky,
Christopher D. Manning, Valentin I. Spitkovsky, and
Eric Yeh. 2009. Stanford-UBC at TAC-KBP. In TAC
2009, November.
James F. Allen. 1983. Maintaining knowledge about
temporal intervals. Commun. ACM, 26:832?843,
November.
Steven Bethard and James H. Martin. 2007. Cu-tmp:
temporal relation classification using syntactic and se-
mantic features. In Proceedings of the 4th Interna-
tional Workshop on Semantic Evaluations, SemEval
?07, pages 129?132, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Lorna Byrne and John Dunnion. 2010. UCD IIRG at
TAC 2010 KBP Slot Filling Task. In Proceedings of
the Third Text Analysis Conference (TAC 2010). NIST,
November.
Nathanael Chambers, Shan Wang, and Dan Jurafsky.
2007. Classifying temporal relations between events.
In Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 173?176, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li,
Wen-Pin Lin, Matthew Snover, Javier Artiles, Marissa
Passantino, and Heng Ji. 2010. CUNY-BLENDER
TAC-KBP2010: Entity linking and slot filling system
description. In Proceedings of the Third Text Analysis
Conference (TAC 2010). NIST, November.
Yuchang Cheng, Masayuki Asahara, and Yuji Mat-
sumoto. 2007. Naist.japan: temporal relation identifi-
cation using dependency parsed tree. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations, SemEval ?07, pages 245?248, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Guillermo Garrido, Bernardo Cabaleiro, Anselmo Peas,
varo Rodrigo, and Damiano Spina. 2011. A distant
supervised learning system for the TAC-KBP Slot Fill-
ing and Temporal Slot Filling Tasks. In Text Analysis
Conference, TAC 2011 Proceedings Papers.
Prashant Gupta and Heng Ji. 2009. Predicting un-
known time arguments based on cross-event propaga-
tion. In Proceedings of the ACL-IJCNLP 2009 Con-
ference Short Papers, ACLShort ?09, pages 369?372,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Heng Ji, Ralph Grishman, and Hoa Trang Dang. 2011.
Overview of the tac2011 knowledge base population
track. In Text Analysis Conference, TAC 2011 Work-
shop, Notebook Papers.
T. Joachims. 2002. Learning to Classify Text Us-
ing Support Vector Machines ? Methods, Theory, and
Algorithms. Kluwer/Springer. We used Joachim?s
SVMLight implementation available at http://
svmlight.joachims.org/.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In ACL 2003, pages 423?430.
Mirella Lapata and Alex Lascarides. 2004. Inferring
sentence-internal temporal relations. In HLT 2004.
Xiao Ling and Daniel S. Weld. 2010. Temporal informa-
tion extraction. In Proceedings of the Twenty-Fourth
AAAI Conference on Artificial Intelligence (AAAI-10).
Inderjeet Mani, Barry Schiffman, and Jianping Zhang.
2003. Inferring temporal ordering of events in news.
In NAACL-Short?03.
Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-
sky. 2009. Distant supervision for relation extraction
without labeled data. In ACL 2009, pages 1003?1011,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
M Pas?ca. 2008. Answering Definition Questions via
Temporally-Anchored Text Snippets. Proc. of IJC-
NLP2008.
Georgiana Pus?cas?u. 2007. Wvali: temporal relation
identification by syntactico-semantic analysis. In Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations, SemEval ?07, pages 484?487,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
James Pustejovsky and Marc Verhagen. 2009. SemEval-
2010 task 13: evaluating events, time expressions,
and temporal relations (TempEval-2). In Proceed-
ings of the Workshop on Semantic Evaluations: Re-
cent Achievements and Future Directions, DEW ?09,
pages 112?116, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010. Modeling relations and their mentions with-
out labeled text. In Jose? Balca?zar, Francesco Bonchi,
Aristides Gionis, and Miche`le Sebag, editors, Machine
Learning and Knowledge Discovery in Databases,
volume 6323 of LNCS, pages 148?163. Springer
Berlin / Heidelberg.
Stuart J. Russell and Peter Norvig. 2010. Artificial Intel-
ligence - A Modern Approach (3. internat. ed.). Pear-
son Education.
Steven Schockaert, Martine De Cock, and Etienne Kerre.
2010. Reasoning about fuzzy temporal information
from the web: towards retrieval of historical events.
Soft Computing - A Fusion of Foundations, Method-
ologies and Applications, 14:869?886.
Mihai Surdeanu and Massimiliano Ciaramita. 2007.
Robust information extraction with perceptrons. In
ACE07, March.
115
Mihai Surdeanu, David McClosky, Julie Tibshirani, John
Bauer, Angel X. Chang, Valentin I. Spitkovsky, and
Christopher D. Manning. 2010. A simple distant
supervision approach for the tac-kbp slot filling task.
In Proceedings of the Third Text Analysis Conference
(TAC 2010), Gaithersburg, Maryland, USA, Novem-
ber. NIST.
Partha Pratim Talukdar, Derry Wijaya, and Tom Mitchell.
2012. Coupled temporal scoping of relational facts. In
Proceedings of the Fifth ACM International Confer-
ence on Web Search and Data Mining (WSDM), Seat-
tle, Washington, USA, February. Association for Com-
puting Machinery.
Marta Tatu and Munirathnam Srikanth. 2008. Experi-
ments with reasoning for temporal relations between
events. In COLING?08.
Marc Verhagen, Inderjeet Mani, Roser Sauri, Robert
Knippen, Seok Bae Jang, Jessica Littman, Anna
Rumshisky, John Phillips, and James Pustejovsky.
2005. Automating temporal annotation with TARSQI.
In ACLdemo?05.
Marc Verhagen, Robert Gaizauskas, Frank Schilder,
Mark Hepple, Graham Katz, and James Pustejovsky.
2007. SemEval-2007 task 15: TempEval temporal re-
lation identification. In SemEval?07.
Yafang Wang, Mingjie Zhu, Lizhen Qu, Marc Spaniol,
and Gerhard Weikum. 2010. Timely YAGO: har-
vesting, querying, and visualizing temporal knowledge
from Wikipedia. In Proceedings of the 13th Inter-
national Conference on Extending Database Technol-
ogy, EDBT ?10, pages 697?700, New York, NY, USA.
ACM.
Yafang Wang, Bin Yang, Lizhen Qu, Marc Spaniol, and
Gerhard Weikum. 2011. Harvesting facts from textual
web sources by constrained label propagation. In Pro-
ceedings of the 20th ACM international conference on
Information and knowledge management, CIKM ?11,
pages 837?846, New York, NY, USA. ACM.
Gerhard Weikum, Srikanta Bedathur, and Ralf Schenkel.
2011. Temporal knowledge for timely intelligence.
In Malu Castellanos, Umeshwar Dayal, Volker Markl,
Wil Aalst, John Mylopoulos, Michael Rosemann,
Michael J. Shaw, and Clemens Szyperski, editors, En-
abling Real-Time Business Intelligence, volume 84
of Lecture Notes in Business Information Processing,
pages 1?6. Springer Berlin Heidelberg.
Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-
hara, and Yuji Matsumoto. 2009. Jointly identifying
temporal relations with Markov Logic. In Proceedings
of the Joint Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint Conference on
Natural Language Processing of the AFNLP: Volume
1 - Volume 1, ACL ?09, pages 405?413, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Qi Zhang, Fabian M. Suchanek, Lihua Yue, and Gerhard
Weikum. 2008. TOB: Timely ontologies for business
relations. In 11th International Workshop on the Web
and Databases, WebDB.
116
