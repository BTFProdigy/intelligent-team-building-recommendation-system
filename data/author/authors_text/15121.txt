Event-based Information Extraction for the biomedical domain: the Caderige project 
 
Erick Alphonse**, Sophie Aubin*, Philippe Bessi?res**, Gilles Bisson****, Thierry Hamon*, 
Sandrine Lagarrigue***, Adeline Nazarenko*, Alain-Pierre Manine**, Claire N?dellec**, 
Mohamed Ould Abdel Vetah**, Thierry Poibeau*, Davy Weissenbacher* 
 
*Laboratoire d?Informatique de Paris-Nord  
CNRS UMR 7030 
Av. J.B. Cl?ment 93430 F-Villetaneuse 
{firstname.lastname}@lipn.univ-paris13.fr 
**Laboratoire Math?matique, Informatique et G?nome (MIG), 
INRA,  
Domaine de Vilvert, 78352 F-Jouy-en-Josas 
{firstname.lastname}@jouy.inra.fr 
***Laboratoire de G?n?tique Animale,  
INRA-ENSAR 
Route de Saint Brieuc, 35042 Rennes Cedex 
lagarrig@roazhon.inra.fr 
****Laboratoire Leibniz ? UMR CNRS 5522  
46 Avenue F?lix Viallet - 38031 F-Grenoble Cedex 
Gilles.Bisson@imag.fr 
 
 Abstract  
This paper gives an overview of the 
Caderige project. This project involves 
teams from different areas (biology, 
machine learning, natural language 
processing) in order to develop high-
level analysis tools for extracting 
structured information from biological 
bibliographical databases, especially 
Medline. The paper gives an overview 
of the approach and compares it to the 
state of the art.  
1 Introduction 
Developments in biology and biomedicine are 
reported in large bibliographical databases 
either focused on a specific species (e.g. 
Flybase, specialized on Drosophilia 
Menogaster) or not (e.g. Medline). This type 
of  information sources is crucial for biologists 
but there is a lack of tools to explore them and 
extract relevant information. While recent 
named entity recognition tools have gained a 
certain success on these domains, event-based 
Information Extraction (IE) is still a challenge.  
The Caderige project aims at designing and 
integrating Natural Language Processing 
(NLP) and Machine Learning (ML) techniques 
to explore, analyze and extract targeted 
information in biological textual databases. We 
promote a corpus-based approach focusing on 
text pre-analysis and normalization: it is 
intended to drain out the linguistic variation 
dimension, as most as possible. Actually, the 
MUC (1995) conferences have demonstrated 
that extraction is more efficient when 
performed on normalized texts. The extraction 
patterns are thus easier to acquire or learn, 
more abstract and easier to maintain 
Beyond extraction patterns, it is also possible 
to acquire from the corpus, via ML methods, a 
part of the knowledge necessary for text 
normalization as shown here.  
This paper gives an overview of current 
research activities and achievements of the 
Caderige project. The paper first presents our 
approach and compares it with the one 
developed in the framework of a similar 
project called Genia (Collier et al 1999). We 
then propose an account of Caderige 
techniques on various filtering and 
normalization tasks, namely, sentence filtering, 
resolution of named entity synonymy, 
syntactic parsing, and ontology learning. 
Finally, we show how extraction patterns can 
be learned from normalized and annotated 
documents, all applied to biological texts.  
2 Description of our approach 
In this section, we give some details about the 
motivations and choices of implementation. 
We then briefly compare our approach with the 
one of the Genia project. 
43
2.1 Project organization 
The Caderige project is a multi disciplinary 
French research project on the automatic 
mining of textual data from the biomedical 
domain and is mainly exploratory orientated. It 
involved biology teams (INRA), computer 
science teams (LIPN, INRA and Leibniz-
IMAG) and NLP teams (LIPN) as major 
partners, plus LRI and INRIA from 2000 to 
2003. 
2.2 Project motivations 
Biologists can search bibliographic databases 
via the Internet, using keyword queries that 
retrieve a large superset of relevant papers. 
Alternatively, they can navigate through 
hyperlinks between genome databanks and 
referenced papers. To extract the requisite 
knowledge from the retrieved papers, they 
must identify the relevant abstracts or 
paragraphs. Such manual processing is time 
consuming and repetitive, because of the 
bibliography size, the relevant data sparseness, 
and the database continuous updating. From 
the Medline database, the focused query 
?Bacillus subtilis and transcription? which 
returned 2,209 abstracts in 2002, retrieves 
2,693 of them today. We chose this example 
because Bacillus subtilis is a model bacterium 
and transcription is a central phenomenon in 
functional genomics involved in genic 
interaction, a popular IE problem. 
GerE stimulates cotD transcription and 
inhibits cotA transcription in vitro by 
sigma K RNA polymerase, as expected from 
in vivo studies, and, unexpectedly, 
profoundly inhibits in vitro 
transcription of the gene (sigK) that 
encode sigma K. 
Figure 1: A sentence describing a genic interaction 
Once relevant abstracts have been retrieved, 
templates should be filled by hand since there 
is no available IE tool operational in genomics  
Type: positive 
Agent: GerE 
 
Interaction 
Target: transcription of the 
gene sigK 
Figure 2: A template describing a genic 
interaction. 
Still, applying IE ? la MUC to genomics and 
more generally to biology is not an easy task 
because IE systems require deep analysis 
methods to locate relevant fragments. As 
shown in the example in Figures 1 and 2, 
retrieving that GerE is the agent of the 
inhibition of the transcription of the gene sigK 
requires at least syntactic dependency analysis 
and coordination processing. In most of the 
genomics IE tasks (function, localization, 
homology) the methods should then combine 
the semantic-conceptual analysis of text 
understanding methods with IE through pattern 
matching. 
2.3 Comparison with the Genia project 
Our approach is very close to the one of the 
Genia project (Collier et al, 1999). Both 
projects rely on precise high-level linguistic 
analysis to be able to perform IE. The kind of 
information being searched is similar, 
concerning mainly gene and protein interaction 
as most of the research in this domain. The 
Genia corpus (Ohtae et al 2001) is not 
specialized on a specific species whereas ours 
is based on Bacillus Subtilis.  
Both projects develop annotation tools and 
Document Type Definition (DTD), which are, 
for the most part, compatible. The aim here is 
to build training corpus to which various 
techniques of NLP and ML are applied in 
order to acquire efficient event-based 
extraction patterns. The choice of ML and 
NLP methods differs but their aim is similar to 
our: normalizing text with predicate-arguments 
structures for learning better patterns. For 
example, Genia uses a combination of parsers 
to finally perform an HPSG-like analysis. The 
Caderige syntactic analysis is based on the 
specialization of the Link Parser (Sleator and 
Temperley, 1993 see section 4) to the 
biological domain.  
 In the following two sections, we detail our 
text filtering and normalization methods. 
Filtering aims at pruning the irrelevant part of 
the corpus while normalization aims at 
building an abstract representation of the 
relevant text. Section 4 is devoted to the 
acquisition of extraction patterns from the 
filtered and normalized text. 
3 Text filtering 
IR and text filtering are a prerequisite step to 
IE, as IE methods (including normalization and 
learning) cannot be applied to large and 
irrelevant corpora (they are not robust enough 
and they are computationally expensive). IR 
here is done through Medline interface by 
keyword queries for filtering the appropriate 
44
document subset. Then, text filtering, reduces 
the variability of textual data with the 
following assumptions: 
? desired information is local to sentences ; 
? relevant sentences contain at least two gene 
names. 
These hypotheses may lead to miss some genic 
interactions, but we assume that information 
redundancy is such that at least one instance of 
each interaction is contained into a single 
sentence in the corpus. The documents 
retrieved are thus segmented into sentences 
and the sentences with at least two gene names 
are selected. 
To identify the only relevant sentences among 
thoses,  classical supervised ML methods have 
been applied to a Bacillus Subtilis corpus in 
which relevant and irrelevant sentences had 
been annotated by a biological expert. Among 
SVMs, Na?ve Bayes (NB) methods, Neural 
Networks, decision trees (Marcotte et al, 
2001;  Nedellec et al, 2001), (Nedellec et al 
2001) demonstrates that  simple NB methods 
coupled with feature selection seem to perform 
well by yielding around 85 % precision and 
recall. Moreover, our first experiments show 
that the linguistic-based representation changes 
such as the use of lemmatization, terminology 
and named entities, do not lead to significant 
improvements. The relevant sentences filtered 
at this step are then used as input of the next 
tasks, normalization and IE. 
4 Normalization 
This section briefly presents three text 
normalization tasks: normalization of entity 
names, normalization of relations between text 
elements through syntactic dependency parsing 
and semantic labeling. The normalization 
process, by providing an abstract 
representation of the sentences, allows the 
identification of regularities that simplify the 
acquisition or learning of pattern rules. 
4.1 Entity names normalization 
Named Entity recognition is a critical point in 
biological text analysis, and a lot of work was 
previously done to detect gene names in text 
(Proux and al., 1998), (Fukuda and al., 1998). 
So, in Caderige, we do not develop any 
original NE extraction tool. We focus on a less 
studied problem that is synonyms recognition.  
Beyond typographical variations and 
abbreviations, biological entities often have 
several different names. Synonymy of gene 
names is a well-known problem, partly due to 
the huge amount of data manipulated (43.238 
references registered in Flybase for 
Drosophilia Melanogaster for example). Genes 
are often given a temporary name by a 
biologist. This name is then changed according 
to information on the concerned gene: for 
example SYGP-ORF50 is a gene name 
temporarily attributed by a sequencing project 
to the PMD1 yeast gene. We have shown that, 
in addition to available data in genomic 
database (GenBank, SwissProt,?), it is 
possible to acquire many synonymy relations 
with good precision through text analysis. By 
focusing on synonymy trigger phrases such as 
"also called" or "formerly", we can extract text 
fragments of that type :  gene trigger gene. 
However, the triggers themselves are subject to 
variation and the arguments of the synonymy 
relation must be precisely identified. We have 
shown that it is possible to define patterns to 
recognize synonymy expressions. These 
patterns have been trained on a representative 
set of sentences from Medline and then tested 
on a new corpus made of 106 sentences 
containing the keyword formerly. Results on 
the test corpus are the following: 97.5% 
precision, 75% recall. We chose to have a high 
precision since the acquired information must 
be valid for further acquisition steps 
(Weissenbacher, 2004).  
The approach that has been developed is very 
modular since abstract patterns like gene 
trigger gene (the trigger being a linguistic 
marker or a simple punctuation) can be 
instantiated by various linguistic items. A 
score can be computed for each instantiation of 
the pattern, during a learning phase on a large 
representative corpus. The use of a reduced 
tagged corpus and of a large untagged corpus 
justify the use of semi-supervised learning 
techniques.  
4.2  Sentence parsing 
The extraction of structured information from 
texts requires precise sentence parsing tools 
that exhibit relevant relation between domain 
entities. Contrary to (Akane et al 2001), we 
chose a partial parsing approach: the analysis 
is focused on relevant parts of texts and, from 
these chunks, on specific relations. Several 
reasons motivate this choice: among others, the 
fact that relevant information generally appears 
in predefined syntactic patterns and, moreover, 
45
the fact that we want to learn domain 
knowledge ontologies from specific syntactic 
relations (Faure and Nedellec, 2000 ; Bisson et 
al. 2000). 
First experiments have been done on several 
shallow parsers. It appeared that constituent 
based parsers are efficient to segment the text 
in syntactic phrases but fail to extract relevant 
functional relationships betweens phrases. 
Dependency grammars are more adequate 
since they try to establish links between heads 
of syntactic phrases. In addition, as described 
in Schneider (1998), dependency grammars are 
looser on word order, which is an advantage 
when working on  a domain specific language.  
Two dependency-based syntactic parsers have 
been tested (Aubin 2003): a hybrid commercial 
parser (henceforth HCP) that combines 
constituent and dependency analysis, and a 
pure dependency analyzer: the Link Parser.   
Prasad and Sarkar (2000) promote a twofold 
evaluation for parsers: on the one hand the use 
of a representative corpus and, on the other 
hand, the use of specific manually elaborated 
sentences. The idea is to evaluate analyzers on 
real data (corpus evaluation) and then to check 
the performance on specific syntactic 
phenomena. In this experiment, we chose to 
have only one corpus, made of sentences 
selected from the Medline corpus depending 
on their syntactic particularity. This strategy 
ensures representative results on real data. 
A set of syntactic relations was then selected 
and manually evaluated. This led to the results 
presented for major relations only in table 1. 
For each analyzer and relation, we compute a 
recall and precision score (recall = # relevant 
found relations / # relations to be found; 
precision = # relevant found relations / # 
relations found by the system).  
The Link Parser generally obtains better results 
than HCP. One reason is that a major 
particularity of our corpus (Medline abstracts) 
is that sentences are often (very) long (27 
words on average) and contain several clauses. 
The dependency analyzer is more accurate to 
identify relevant relationships between 
headwords whereas the constituent parser is 
lost in the sentence complexity. We finally 
opted for the Link Parser. Another advantage 
of the Link Parser is the possibility to modify 
its set of rules (see next subsection). The Link 
parser is currently used in INRA to extract 
syntactic relationships from texts in order to 
learn domain ontologies on the basis of a 
distributional analysis (Harris 1951, Faure and 
N?dellec, 1999).  
4.3 Recycling a general parser for biology 
During the evaluation tests, we noticed that 
some changes had to be applied either to the 
parser or to the text itself to improve the 
syntactic analysis of our biomedical corpus. 
The corpus needs to be preprocessed: sentence 
segmentation, named entities and terms 
recognition are thus performed using generic 
modules tuned for the biology domain
1
. Term 
recognition allows the removing of numerous 
structure ambiguities, which clearly benefits 
the parsing quality and execution time.  
                                                     
1
 A term analyser is currently being built at LIPN 
using existing term resources like Gene Ontology 
(see Hamon and Aubin, 2004). 
  Link Parser HCP 
Rel nbRel relOK R. RelTot P. RelOK R RelTot P. 
Subject 
18 13 0.72 19 0.68 14 0.78 20 0.65 
Object 
18 16 0.89 17 0.94 9 0.5 13 0.69 
Prep 
48 25 0.52 55 0.45 20 0.42 49 0.41 
V-GP1 
14 13 0.93 15 0.87 9 0.64 23 0.39 
O-GP 
16 7 0.43 12 0.58 12 0.75 28 0.43 
NofN 
16 13 0.81 15 0.87 14 0.87 26 0.54 
VtoV 
10 9 0.9 9 1 7 0.7 7 1 
VcooV 
10 8 0.8 9 0.89 6 0.6 6 1 
NcooN 
10 8 0.7 10 0.8 4 0.4 6 0.67 
nV-Adj 
10 8 0.8 9 0.89 0 0 0 1 
PaSim 
18 17 0.94 18 0.94 17 0.94 22 0.77 
PaRel 
12 11 0.92 11 1 8 0.67 11 0.73 
Table 1: Evaluation of two parsers on various syntactic relations 
Relations meaning: subject = subject-verb, Object = verb-object, Prep = prepositional phrase, V-GP = verb-prep. 
phrase, O-GP = Object- prep. phrase, NofN = Noun of noun, VtoV = Verb to Verb, VcooV = Verb coord. Verb, 
NcooN = Noun coord. Noun, nV-Adj = not + Verb or adjective, PaSim = passive form, PaRel = passive relative 
46
Concerning the Link Parser, we have manually 
introduced new rules and lexicon to allow the 
parsing of syntactic structures specific to the 
domain. For instance, the Latin-derived Noun 
Adjective phrase "Bacillus subtilis" has a 
structure inverse to the canonical English noun 
phrase (Adjective Noun). Another major task 
was to loosen the rules constraints because 
Medline abstracts are written by biologists 
who express themselves in sometimes broken 
English. A typical error is the omission of the 
determinant before some nouns that require 
one. We finally added words unknown to the 
original parser. 
4.4 Semantic labelling 
Asium software is used to semi-automatically 
acquire relevant semantic categories by 
distributional semantic analysis of parsed 
corpus. These categories contribute to text 
normalization at two levels, disambiguating 
syntactic parsing and typing entities and 
actions for IE. Asium is based on an original 
ascendant hierarchical clustering method that 
builds a hierarchy of semantic classes from the 
syntactic dependencies parsed in the training 
corpus. Manual validation is required in order 
to distinguish between different meanings 
expressed by identical syntactic structures. 
5 Extraction pattern learning 
Extraction pattern learning requires a training 
corpus from which the relevant and 
discriminant regularities can be automatically 
identified. This relies on two processes: text 
normalization that is domain-oriented but not 
task-oriented (as described in previous 
sections), and task-oriented annotation by the 
expert of the task.  
5.1 Annotation procedure 
The Caderige annotation language is based on 
XML and a specific DTD (Document Type 
Definition that can be used to annotate both 
prokaryote and eukaryote organisms by 50 
tags with up to 8 attributes. Such a precision is 
required for learning feasibility and extraction 
efficiency. Practically, each annotation aims at 
highlighting the set of words in the sentence 
describing: 
? Agents (A): the entities activating or 
controlling the interaction 
? Targets (T): the entities that are produced 
or controlled 
? Interaction (I): the kind of control 
performed during the interaction 
? Confidence (C): the confidence level in this 
interaction. 
The annotation of ?A low level of GerE 
activated transcription of CotD by GerE RNA 
polymerase in vitro ...? is given below. The 
attributes associated to the tag <GENIC-
INTERACTION> express the fact that the 
interaction is a transcriptional activation and 
that it is certain. The other tags (<IF>, 
<AF1>, ?) mark  the agent (AF1 and AF2), the 
target (TF1) and the interaction (IF). 
 
<GENIC-INTERACTION 
 id=?1?  
 type=?transcriptional?  
 assertion=?exist?  
 regulation=?activate?  
 uncertainty=?certain?  
 self-contained=?yes?  
 text-clarity=?good?> 
  <IF>A<I> low level </I>of</IF>     
  <AF1><A1  
     type=protein  
        role=modulate  
        direct=yes> GerE 
  </A1></AF1>,  
  <IF><I>activated</I> transcription  
      of</IF>    
     <TF1><T1 type=protein> CotD </T1>           
         </TF1> by   
     <AF2><A2  
           type=protein  
         role=required> 
       GerE RNA polymerase 
   </A2></AF2>,  
   <CF>but<C>in vitro</C></CF> 
</GENIC-INTERACTION> 
5.2 The annotation editor2 
Annotations cannot be processed in text form 
by biologists. The annotation framework 
developed by Caderige provide a general XML 
editor with a graphic interface for creating, 
checking and revising annotated documents. 
For instance, it displays the text with graphic 
attributes as defined in the editor XML style 
sheet, it allows to add the tags without strong 
constraint on the insertion order and it 
automatically performs some checking. 
The editor interface is composed of four main 
parts (see Figure 3). The editable text zone for 
annotation, the list of XML tags that can be 
used at a given time, the attributes zone to edit 
the values of the selected tag, and the XML 
                                                     
2
 Contact one of the authors if you are interested to 
use this annotation tool in a research project 
47
code currently generated. In the text zone, the 
above sentence is displayed as follows: 
A low level of GerE activated 
transcription of CotD by GerE RNA 
polymerase but in vitro 
This editor is currently used by some of the 
Caderige project partners and at SIB (Swiss 
Institute of BioInformatics) with another DTD, 
in the framework of the European BioMint 
project. Several corpora on various species 
have been annotated using this tool, mainly by 
biologists from INRA.  
5.3 Learning 
The vast majority of approaches relies on 
hand-written pattern rules that are based on 
shallow representations of the sentences (e.g. 
Ono et al, 2001). In Caderige, the deep 
analysis methods increase the complexity of 
the sentence representation, and thus of the IE 
patterns. ML techniques appear therefore very 
appealing to automate the process of rule 
acquisition (Freitag, 1998; Califf et al, 1998; 
Craven et al, 1999).  
Learning IE rules is seen as a discrimination  
task, where the concept to learn is a n-ary 
relation between arguments which correspond 
to the template fields. For example, the 
template in figure 2 can be filled by learning a 
ternary relation genic-interaction(X,Y,Z), 
where X,Y and Z are the type, the agent and 
the target of the interaction. The learning 
algorithm is provided with a set of positive and 
negative examples built from the sentences 
annotated and normalized. We use the 
relational learning algorithm, Propal (Alphonse 
et al, 2000). The appeal of using a relational 
method for this task is that it can naturally 
represent the relational structure of the 
syntactic dependencies in the normalized 
sentences and the background knowledge if 
needed, such as for instance semantic relations.  
For instance, the IE rules learned by Propal 
extract, from the following sentence :"In this 
mutant, expression of the spoIIG gene, whose 
transcription depends on both sigA and the 
phosphorylated Spo0A protein, Spo0AP, a 
major transcription factor during early stages 
of sporulation, was greatly reduced at 43 
degrees C.", successfully extract the two 
relations genic-interaction(positive, sigA, 
spoIIG) and genic-interaction(positive, 
Spo0AP, spoIIG). As preliminary experiments, 
we selected a subset of sentences as learning 
dataset, similar to this one. The performance of 
the learner evaluated by ten-fold cross-
validation is 69?6.5% of recall and 86?3.2% 
of precision. This result is encouraging, 
showing that the normalization process 
provides a good representation for learning IE 
rules with both high recall and high precision. 
6 Conclusion 
We have presented in this paper some results 
from the Caderige project. Two major issues 
are the development of a specific annotation 
editor for domain specialists and a set of 
machine learning and linguistic processing 
tools tuned for the biomedical domain.  
Current developments focus on the use of 
learning methods in the extraction process. 
These methods are introduced at different 
levels in the system architecture. A first use is 
Figure 3: the Caderige annotation editor 
48
the acquisition of domain knowledge to 
enhance the extraction phase. A second use 
concerns a dynamic adaptation of existing 
modules during the analysis according to 
specific features in a text or to specific text 
genres.  
7 References 
E. Agichtein and H. Yu (2003). Extracting 
synonymous gene and protein terms from 
biological literature. Bioinformatics, vol. 19 
Suppl.1, Oxford Press. 
E. Alphonse and C. Rouveirol (2000). Lazy 
propositionalisation for Relational  
Learning. In 14th European Conference on 
Artificial Intelligence (ECAI?00, W. Horn ed.), 
Berlin, pp. 256-260.  
S. Aubin (2003). ?valuation comparative de deux 
analyseurs produisant des relations syntaxiques. 
In workshop TALN and multilinguism. Batz-sur-
Mer. 
Y. Akane, Y. Tateisi, Y. Miyao and J. Tsujii. 
(2001). Event extraction from biomedical papers 
using a full parser. In Proceedings of the sixth 
Pacific Symposium on Biocomputing (PSB 2001). 
Hawaii, U.S.A.. pp. 408-419.  
G. Bisson, C. Nedellec, L. Ca?amero 2000. 
Designing clustering methods for ontology 
building: The Mo?K workbench. In Proceedings 
of Ontology Learning workshop (ECAI 2000), 
Berlin, 22 ao?t 2000.  
M. E. Califf, 1998. Relational Learning Techniques 
for Natural Language Extraction. Ph.D. 
Disseration, Computer Science Department, 
University of Texas, Austin, TX. AI Technical 
Report 98-276. 
N. Collier, Hyun Seok Park, Norihiro Ogata, Yuka 
Tateisi, Chikashi Nobata, Takeshi Sekimizu, 
Hisao Imai and Jun'ichi Tsujii. (1999). The 
GENIA project: corpus-based knowledge 
acquisition and information extraction from 
genome research papers. In Proceedings of the 
European Association for Computational 
Linguistics (EACL 1999). 
M. Craven et al, 1999. Constructing Biological 
Knowledge Bases by Extracting Information 
from Text Sources. ISMB 1999: 77-86 
D. Faure and C. Nedellec (1999). Knowledge 
acquisition of predicate argument structures from 
technical texts using Machine Learning: the 
system ASIUM. In EKAW'99, pp. 329-334, 
Springer-Verlag.  
D. Freitag, 1998, Multistrategy learning for 
information extraction. In Proceedings of the 
Fifteenth International Conference on Machine 
Learning, 161-169. Madison, WI: Morgan 
Kaufmann 
T. Hamon and S. Aubin (2004). Evaluating 
terminological resource coverage for relevant 
sentence selection and semantic class building. 
LIPN internal report. 
K. Fukuda, T. Tsunoda, A. Tamura, T. Takagi 
(1998). Toward information extraction : 
identifying protein names from biological papers. 
Proceedings of the Pacific Symposium of 
Biocomputing, pp. 707-718. 
Z. Harris (1951). Methods in Structural Linguistics. 
Chicago. University of Chicago Press.  
E.M. Marcotte, I. Xenarios I., and D. Eisenberg 
(2001). Mining litterature for protein-protein 
interactions. In Bioinformatics, vo. 17 n? 4, 
pp. 359-363. 
MUC (1995) Proceeding of the 6
th
 Message 
understanding Conference. Morgan Kaufmann. 
Palo Alto.  
C. N?dellec, M. Ould Abdel Vetah and P. Bessi?res 
(2001). Sentence Filtering for Information 
Extraction in Genomics: A Classification 
Problem. In Proceedings of the International 
Conference on Practical Knowledge Discovery in 
Databases (PKDD?2001), pp. 326?338. Springer 
Verlag, LNAI 2167, Freiburg. 
T. Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Mima 
and Jun'ichi Tsujii. (2001). Ontology Based 
Corpus Annotation and Tools. In Proceedings of 
the 12th Genome Informatics 2001. pp. 469--470. 
T. Ono, H. Hishigaki, A. Tanigami and T. Takagi 
(2001). Automated extraction of information on 
protein-protein interactions from the biological  
literature. Bioinformatics. vol 17, n? 2, pp. 155-
161, Oxford Press. 
B. Prasad and A. Sarkar (2000) Comparing Test-
suite based evaluation and Corpus-based 
evaluation of a wide-coverage grammar for 
English. In Using Evaluation within Human 
Language Technology. LREC. Athens.  
D. Proux, F. Rechenmann, L. Julliard, V. Pillet, B. 
Jacq (1998). Detecting gene symbols and names 
in biological texts : a first step toward pertinent 
information extraction. In Genome Informatics, 
vol. 9, pp. 72-80. 
G. Schneider (1998). A Linguistic Comparison of 
Constituency, Dependency and Link Grammar. 
PhD thesis, Institut f?r Informatik der Universit?t 
Z?rich, Switzerland. 
D. Sleator and D. Temperley (1993). Parsing 
English with a Link Grammar. In Third 
International Workshop on Parsing 
Technologies. Tilburg. Netherlands. 
D. Weissenbacher (2004). La relation de 
synonymie en g?nomique. In Recital conference. 
Fes. 
49
Proceedings of BioNLP Shared Task 2011 Workshop, pages 56?64,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP shared Task 2011 - Bacteria Biotope 
Robert Bossy1, Julien Jourde1, Philippe Bessi?res1, Maarten van de Guchte2,  
Claire N?dellec1 
 
1MIG UR1077 2Micalis UMR 1319  
INRA, Domaine de Vilvert 
78352 Jouy-en-Josas, France 
forename.name@jouy.inra.fr 
 
 
Abstract 
This paper presents the Bacteria Biotope 
task as part of the BioNLP Shared Tasks 
2011. The Bacteria Biotope task aims at 
extracting the location of bacteria from 
scientific Web pages. Bacteria location is a 
crucial knowledge in biology for phenotype 
studies. The paper details the corpus 
specification, the evaluation metrics, 
summarizes and discusses the participant 
results.  
1 Introduction 
The Bacteria Biotope (BB) task is one of the five 
main tasks of the BioNLP Shared Tasks 2011. The 
BB task consists of extracting bacteria location 
events from Web pages, in other words, citations 
of places where a given species lives. Bacteria 
locations range from plant or animal hosts for 
pathogenic or symbiotic bacteria, to natural 
environments like soil or water. Challenges for 
Information Extraction (IE) of relations in Biology 
are mostly devoted to the identification of bio-
molecular events in scientific papers where the 
events are described by relations between named 
entities, e.g. genic interactions (N?dellec, 2005), 
protein-protein interactions (Pyysalo et al, 2008), 
and more complex molecular events (Kim et al, 
2011). However, this far from reflects the diversity 
of the potential applications of text mining to 
biology. The objective of previous challenges has 
mostly been focused on modeling biological 
functions and processes using the information on 
elementary molecular events extracted from text. 
The BB task is the first step towards linking 
information on bacteria at the molecular level to 
ecological information. The information on 
bacterial habitats and properties of these habitats is 
very abundant in literature, in particular in 
Systematics literature (e.g. International Journal of 
Systematic and Evolutionary Microbiology), 
however it is rarely available in a structured way 
(Hirschman et al, 2008; Tamames and de Lorenzo, 
2009). The NCBI GenBank nucleotide isolation 
source field (GenBank) and the JGI Genome 
OnLine Database (GOLD) isolation site field are 
incomplete with respect to the microbial diversity 
and are expressed in natural language. The two 
critical missing steps in terms of biotope 
knowledge modeling are (1) the automatic 
population of databases with organism/location 
pairs that are extracted from text, and (2) the 
normalization of the habitat name with respect to 
biotope ontologies. The BB task mainly aims at 
solving the first information extraction issue. The 
second classification issue is handled through the 
categorization of locations into eight types. 
2 Context 
According to NCBI statistics there are nearly 900 
bacteria with complete genomes, which account 
for more than 87% of total complete genomes. 
Consequently, molecular studies in bacteriology 
are shifting from species-centered to full diversity 
investigation. The current trend in high-throughput 
experiments targets diversity related fields, 
typically phylogeny or ecology. In this context, 
adaptation properties, biotopes and biotope 
properties become critical information. Illustrative 
questions are: 
56
? Is there a phylogenetic correlation between 
species that share the same biotope? 
? What are common metabolic pathways of 
species that live in given conditions, especially 
species that survive in extreme conditions? 
? What are the molecular signaling patterns in 
host relationships or population relationships 
(e.g. in biofilms)? 
Recent metagenomic experiments produce 
molecular data associated with a habitat rather than 
a single species. This raises new challenges in 
computational biology and data integration, such 
as identifying known and new species that belong 
to a metagenome. 
Not only will these studies require 
comprehensive databases that associate bacterial 
species to their habitat, but they also require a 
formal description of habitats for property 
inference. The bacteria biotope description is 
potentially very rich since any physical object, 
from a cell to a continent, can be a bacterial 
habitat. However these relations are much simpler 
to model than with general formal spatial 
ontologies. A given place is a bacterial habitat if 
the bacteria and the habitat are physically in 
contact, while the relative position of the bacteria 
and its dissemination are not part of the BB task 
model.  
The BB Task requires the locations to be 
assigned different types (e.g. soil, water). We view 
location typing as a preliminary step of more fine-
grained modeling in location ontologies. Some 
classifications for bacteria biotopes have been 
proposed by some groups (Floyd et al, 2005; 
Hirschman et al, 2008; Field et al, 2008; 
Pignatelli et al, 2009). The Environment Ontology 
project (EnvO) is developing an ambitious detailed 
environment ontology for supporting standard 
manual annotation of environments of all types of 
organisms and biological samples (Field et al, 
2008). In a similar way, the GOLD group at JGI 
defined a standard classification for bacteria 
population metagenome projects. Developing 
methods for the association of such biotope classes 
to organisms remains an open question. EnvDB 
(Pignatelli et al, 2009) is an attempt to inventory 
isolation sources of bacteria as recorded in 
GenBank and to map them to a three level 
hierarchy of 71 biotope classes. The assignment of 
bacterial samples in one of the EnvDB classes is 
supported by a text-mining tool based on a Na?ve 
Bayes (NB) classifier applied to a bag of words 
representing the associated reference title and 
abstract. Unfortunately, the low number of paper 
references associated with the isolation source field 
(46 %) limits the scope of the method. 
The BB task has a similar goal, but directly 
applies to natural language texts thus avoiding the 
issue of database incompleteness. As opposed to 
database-based approaches, biotope information 
density is higher but the task has to include 
bacteria and location identification, as well as 
information extraction to relate them.  
The eight types of locations in the BB task 
capture high-level information for further ontology 
mappings.  The location types are Host, HostPart, 
Geographical and Environmental. Environmental 
is broadly defined to qualify locations that are not 
associated to hosts, in a similar way to what was 
described by Floyd et al (Floyd et al, 2005). In 
addition, the BB task types exclude artificially 
constructed biotopes (e.g. bacteria growing in labs 
on a specific medium) and laboratory mutant 
bacteria. The Environmental class is divided into 
Food, Medical, Soil and Water. Locations that are 
none of these subtypes are classified as 
Environmental. 
The exact geographical location (e.g. latitude 
and longitude coordinates) has less importance 
here than in eukaryote ecology because most of the 
biotope properties vary along distances smaller 
than the precision of the current positioning 
technologies. Geographical names are only useful 
in bacteria biotope studies when the physico-
chemical properties of the location can be inferred. 
For the sake of simplicity, the locations of bacteria 
host (e.g. the stall of the infected cow) are not 
taken into account despite their richness (Floyd et 
al., 2005). 
The important information conveyed by the 
locations, especially of Environment type, is the 
function of the bacterium in its ecosystem rather 
than the substance of the habitat. Indeed the final 
goal is to extract habitat properties and bacteria 
phenotypes. Beyond the identification of locations, 
their properties (e.g. temperature, pH, salinity, 
oxygen) are of high interest for phenotypes (e.g. 
thermophily, acidophily, halophily) and trophism 
studies. This information is difficult to extract, and 
is often incomplete or even not available in papers 
(Tamames and de Lorenzo., 2009). Hopefully, 
some properties can be automatically retrieved 
57
with the help of specialized databases, which give 
the physico-chemical properties of locations, such 
as hosts (plant, animal, human organs), soils (see 
WebSoilSurvey, Corine Land Cover), water, or 
chemical pollutants. 
From a linguistic point of view, the BB task 
differs from other IE molecular biology tasks while 
it raises some issues common to biomedicine and 
more general IE tasks. The documents are 
scientific Web pages intended for non-experts such 
as encyclopedia notices. The information is dense 
compared to scientific papers. Documents are 
structured as encyclopedia pages, with the main 
focus on a single species or a few species of the 
same genus or family. The frequency of anaphora 
and coreferences is unusually high. The location 
entities are denoted by complex expressions with 
semantic boundaries instead of rigid designators.  
3 Task description 
The goal of the BB task is illustrated in Figure 1.  
 
Bifidobacterium longum . This organism is found in 
adult humans  and formula fed infants  as a normal 
component of gut  flora. 
Figure 1. Example of information to be extracted 
in the BB Task. 
 
The entities to be extracted are of two main 
types: bacteria and locations. They are text-bound 
and their position has to be predicted. Relations are 
of type Localization between bacteria and 
locations, and PartOf between hosts and host parts. 
In the example in Figure 1, Bifidobacterium 
longum is a bacterium. adult humans and formula 
fed infants denote host locations for the bacteria. 
gut is also a bacteria location, part of the two hosts 
and thus of type host part.  
Coreference relations between entities denoting 
the same information represent valid alternatives 
for the relation arguments. For example, the three 
taxon names in Figure 2 are equivalent. 
 
 
 
The green sulfur bacteria  (GSB ; Phylum Chlorobi ) 
are commonly found in aquatic environments . 
Figure 2. Coreference example. 
 
The coreference relation between pairs of 
entities is binary, symmetric and transitive. 
Coreference sets are equivalence sets defined as 
the transitive closure of the binary coreference 
relation. Their annotation is provided in the 
training and development sets, but it does not have 
to be predicted in the test set. 
4 Corpus description 
The corpus sources are the following bacteria 
sequencing project Web pages: 
? Genome Projects referenced at NCBI; 
? Microbial Genomics Program at JGI; 
? Bacteria Genomes at EBI; 
? Microorganisms sequenced at Genoscope; 
? Encyclopedia pages from MicrobeWiki. 
The documents are publicly available and quite 
easy to understand by non-experts compared to 
scientific papers on similar topics. From the 2,086 
downloaded documents, 105 were randomly 
selected for the BB task. A quarter of the corpus 
was retained for test evaluation. The rest was split 
into train and development sets. Table 1 gives the 
distribution of the entities and relations per corpus. 
The distribution of the five document sources in 
the test corpus reflects the distribution of the 
training set and no other criteria. Food is therefore 
underrepresented.  
 
 Training+Dev Test 
Document 78 (65 + 13) 27 (26 %) 
Bacteria 538 121 (18 %) 
Environment 62 16 (21 %) 
Host 486 101 (17 %) 
HostPart 217 84 (28 %) 
Geographical 111 25 (18 %) 
Water 70 21 (23 %) 
Food 46 0 (0 %) 
Medical 24 2 (8 %) 
Soil 26 20 (43 %) 
Coreference 484 100 (17 %) 
Total entities 1,580 390 
58
 Training+Dev Test 
Localization 998 250 (20 %) 
Part of Host 204 78 (28 %) 
Total relations 1,202 328 
Table 1. Corpus Figures. 
5 Annotation methodology 
HTML tags and irrelevant metadata were stripped 
from the corpus. The Alvis pipeline (N?dellec et 
al., 2009) pre-annotated the species names that are 
potential bacteria and host names. A team of 7 
scientists manually annotated the entities, 
coreferences and relations using the Cadixe XML 
editor (Cadixe). Each document was processed by 
two independent annotators in a double-blind 
manner. Conflicts were automatically detected, 
resolved by annotator negotiation and irrelevant 
documents (e.g. without bacterial location) were 
removed. The remaining inconsistencies among 
documents were resolved by the two annotators 
assisted by a third person acting as an arbitrator. 
The annotator group designed the detailed 
annotation guidelines in two phases. First, they 
annotated a set of 10 documents, discussed the 
options and wrote detailed guidelines with 
representative and illustrative examples. During 
the annotation of the rest of the documents, new 
cases were discussed by email and the guidelines 
amended accordingly. 
Location types. The main issues under debate 
were the definition of location types, boundaries of 
annotations and coreferences. Additional 
annotation specifications concerned the exclusion 
of overly general locations (e.g. environment, 
zone), artificially constructed biotopes and indirect 
effects of bacteria on distant places. For instance, a 
disease symptom occurring in a given host part 
does not imply the presence of the bacteria in this 
place, whereas infection does. Boundaries of types 
were also an important point of discussion since 
the definite formalization of habitat categories was 
at stake. For instance we decided to exclude land 
environment citations (fields, deserts, savannah, 
etc.) from the type Soil, and thus enforced a strict 
definition of soil bacteria. The most controversial 
type was host parts. We decided to include fluids, 
secretions and excretions (which are not strictly 
organs). Therefore, the host parts category required 
specifications to determine at which point of 
dissociation from the original host is a habitat not a 
host part anymore (e.g. mother?s milk vs. industrial 
milk, rhizosphere as host part instead of soil). 
Boundaries. The bacteria name boundaries do 
not include any external modifiers (e.g. two A. 
baumannii strains). Irrelevant modifiers of 
locations are considered outside the annotation 
boundaries (e.g. responsible for a hospital 
epidemic). All annotations are contiguous and span 
on a single fragment in the same way as the other 
BioNLP Shared Tasks. This constraint led us to 
consider cases where several annotations occur 
side by side. The preferred approach was to have 
one distinct annotation for each different location 
(e.g. contact with infected animal products or 
through the air). In the case of head or modifier 
factorization, the annotation depends on the 
information conveyed by the factorized part. If the 
head is not relevant to determine the location type, 
then each term is annotated separately (e.g. 
tropical and temperate zones). Conversely, if the 
head is the most informative with regards to the 
location type, a single annotation spans the whole 
fragment (fresh and salt water). 
Coreferences. Two expressions are considered 
as coreferential and thus valid solution alternatives, 
if they convey the same information. For instance, 
complete taxon names and non-ambiguous 
abbreviations are valid alternatives (e.g. Borrelia 
garinii vs. B. garinii), while ambiguous anaphora 
ellipses are not (e.g. as in ?[..] infected with 
Borrelia duttonii. Borrelia then multiplies [..]?). 
The ellipsis of the omitted specific name 
(dutotonii) leaves the ambiguous generic name 
(Borrelia). 
The full guidelines document is available for 
download on the BioNLP Shared Task Bacteria 
Biotope page1. 
6 Evaluation procedure 
6.1 Campaign organization 
The training and development corpora with the 
reference annotations were made available to the 
participants by December 1st 2010 on the BioNLP 
Shared Tasks pages together with the evaluation 
software. The test corpus, which does not contain 
                                                   
1 https://sites.google.com/site/bionlpst/home/bacteria-biotopes/ 
BioNLP-ST_2011_Bacteria_Biotopes_Guidelines.pdf 
59
any annotation, was made available by March, 1st 
2011. The participants sent the predicted 
annotations to the BioNLP Shared Task organizers 
by March 10th. Each participant submitted a single 
final prediction set. The detailed evaluation results 
were computed, provided to the participants and 
published on the BioNLP website by March, 11th.  
6.2 Evaluation metrics 
The evaluation metrics are based on precision, 
recall and the F-measure. In the following section, 
the PartOf and Localization relations will both be 
referred to as events. The metrics measure the 
accuracy of the participant prediction of events 
with respect to the reference annotation of the test 
corpus. Predicted entities that are not event 
arguments are ignored and they do not penalize the 
score. Each event Er in the reference set is matched 
to the predicted event Ep that maximizes the event 
similarity function S. The recall is the sum of the S 
results divided by the number of events in the 
reference set. Each event Ep in the predicted set is 
matched to the reference event Er that maximizes 
S. The precision is the sum of the S results divided 
by the number of events in the predicted set. 
Participants were ranked by the F-score defined as 
the harmonic mean between precision and recall. 
Eab, the event similarity between a reference 
Localization event a and a predicted Localization 
event b, is defined as: 
Eab = Bab . Tab . Jab 
? Bab is the bacteria boundary component defined 
as: if the Bacterium arguments of both the 
predicted and reference events have exactly the 
same boundaries, then Bab = 1, otherwise Bab = 
0. Bacteria name boundary matching is strict 
since boundary mistakes usually yield a 
different taxon. 
? Tab is the location type prediction component 
defined as: if the Location arguments of both 
the predicted and reference events are of the 
same type, then Tab = 1, otherwise Tab = 0.5. 
Thus type errors divide the score by two. 
? Jab is the location boundary component defined 
as: if the Location arguments of the predicted 
and reference events overlap, then 
1?+=
ab
ba
ab OV
LENLENJ  
where LENa and LENb are the length of the 
Localization arguments of predicted and 
reference events, and OVab is the length of the 
overlapping segment between the Localization 
arguments of the predicted and reference 
events. If the arguments do not overlap, then Jab 
is 0. This formula is a Jaccard index applied to 
overlapping segments. Location boundary 
matching is relaxed, though the Jaccard index 
rewards predictions that approach the reference. 
For PartOf events between Hosts and HostParts, 
the matching score Pab is defined as: if the Host 
arguments of the reference and predicted events 
overlap and the Part arguments of the reference 
and predicted events overlap, then Pab = 1, 
otherwise Pab = 0. Boundary matching of PartOf 
arguments is relaxed, since boundary mistakes are 
already penalized in Eab. 
Arguments belonging to the same coreference 
set are strictly equivalent. In other words, the 
argument in the predicted event is correct if it is 
equal to the reference entity or to any item in the 
reference entity coreference set. 
7 Results  
7.1 Participating systems 
Three teams submitted predictions to the BB task. 
The first team is from the University of Turku 
(UTurku); their system is generic and produced 
predictions for every BioNLP Shared Task. This 
system uses ML intensely, especially SVMs, for 
entity recognition, entity typing and event 
extraction. UTurku adapted their system for the BB 
task by using specific NER patterns and external 
resources (Bj?rne and Salakoski, 2011). 
The second team is from the Japan Advanced 
Institute of Science and Technology (JAIST); their 
system was specifically designed for this task. 
They used CRF for entity recognition and typing, 
and classifiers for coreference resolution and event 
extraction (Nguyen and Tsuruoka, 2011). 
The third team is from Bibliome INRA; their 
system was specifically designed for this task 
(Ratkovik et al, 2011). This team has the same 
affiliation as the BB Task authors, however great 
care was taken to prevent communication on the 
subject between task participants and the test set 
annotators. 
60
The results of the three submissions according to 
the official metrics are shown in Table 2. The 
scores are micro-averaged: Localization and 
PartOf relations have the same weight. Given the 
novelty and the complexity of the task, these first 
results are quite encouraging. Almost half of the 
relations are correctly predicted. The Bibliome 
team achieved the highest F-measure with a 
balanced recall and precision (45%). 
 
 Recall Precision F-score 
Bibliome 45 45 45 
JAIST 27 42 33 
UTurku 17 52 26 
 
Table 2. Bacteria Biotope Task results. 
7.2 Systems description and result analysis 
All three systems perform the same distinct sub-
tasks: bacteria name detection, detection and 
typing of locations, coreference resolution and 
event extraction. The following description of the 
approaches used by the three systems in each 
subtask will be supported by intermediate results. 
Bacteria name detection. Interestingly the three 
participants used three different resources for the 
detection of bacteria names: the List of Prokaryotic 
Names with Standing in Nomenclature (LPNSN) 
by UTurku, names in the genomic BLAST page of 
NCBI by JAIST and the NCBI Taxonomy by 
Bibliome. 
 
Bibliome 84 
JAIST 55 
UTurku 16 
Table 3. Bacteria entity recall. 
 
Table 3 shows a disparity in the bacteria entity 
recall of participants. The merits of each resource 
cannot be deduced directly from these figures since 
they have been exploited in different manners. 
UTurku and JAIST systems injected the resource 
as features in a ML algorithm, whereas Bibliome 
directly projected the resource on the corpus with 
additional rule-based abbreviation detection. 
However there is some evidence that the 
resources have a major impact on the result. 
According to Sneath and Brenner (1992) LPNSN 
is necessarily incomplete. NCBI BLAST only 
contains names of species for which a complete 
genome has been published. The NCBI Taxonomy 
used by INRA only contains names of taxa for 
which some sequence was published. It appears 
that all the lists are incomplete. However, the 
bacteria referenced by the sequencing projects, 
which are mentioned in the corpus should all be 
recorded by the NCBI Taxonomy. 
Location detection and typing. As stated before, 
locations are not necessarily denoted by rigid 
designators. This was an interesting challenge that 
called for the use of external resources and 
linguistic analysis with a broad scope. 
UTurku and JAIST both used WordNet, a 
sensible choice since it encompasses a wide 
vocabulary and  is also structured with synsets and 
hyperonymy relations. The WordNet entries were 
injected as features in the participant ML-based 
entity recognition and typing subsystems. 
It is worth noting that JAIST also used word 
clustering based on MEMM for entity detection. 
This method has things in common with 
distributional semantics. JAIST experiments 
demonstrated a slight improvement using word 
clustering, but further exploration of this idea may 
prove to be valuable. 
Alternatively, the Bibliome system extracted 
terms from the corpus using linguistic criteria 
classified them as locations and predicted their 
type, by comparing them to classes in a habitat-
specific ontology. This prediction uses both 
linguistic analysis of terms and the hierarchical 
structure of the ontology. Bibliome also used 
additional resources for specific types: the NCBI 
Taxonomy for type Host and Agrovoc countries 
for type Geographical. 
 Bibliome JAIST UTurku 
Host 82 49 28 
Host part 72 36 28 
Geo. 29 60 53 
Environment 53 10 11 
Water 83 32 2 
Soil 86 37 34 
Table 4. Location entity recall by type. The 
number of entities of type Food and Medical in the 
test set is too low to be significant. The scores are 
computed using Tab and Jab. 
61
 
The location entity recall in Table 4 shows that 
Bibliome consistently outperformed the other 
groups for all types except for Geographical. This 
demonstrates the strength of exploiting a resource 
with strong semantics (ontology vs. lexicon) and 
with mixed semantic and linguistic rules. 
In order to evaluate the impact of Location entity 
boundaries and types, we computed the final score 
by relaxing Tab and Jab measures. We re-defined Tab 
as always equal to 1, in other words the type of the 
localization was not evaluated. We also re-defined 
Jab as: if the Location arguments overlap, then Jab = 
1, otherwise Jab = 0. This means that boundaries 
were relaxed. The relaxed scores are shown in 
Table 5. While the difference is not significant for 
JAIST and UTurku, the Bibliome results exhibit a 
9 point increase. This demonstrates that the 
Bibliome system is efficient at predicting which 
entities are locations, while the other participants 
predict more accurately the boundaries and types. 
 Recall Prec. F-score Diff. 
Bibliome 54 54 54 +9 
JAIST 29 45 35 +2 
UTurku 19 56 28 +2 
Table 5. Participants score using relaxed location 
boundaries and types. 
Coreference resolution. The corpus exhibits an 
unusual number of anaphora, especially bacteria 
coreferences since a single bacterium species is 
generally the central topic of a document. The 
Bibliome submission is the only one that 
performed bacteria coreference resolution. Their 
system is rule-based and dealt with referential ?it?, 
bi-antecedent anaphora and more importantly 
sortal anaphora. The JAIST system has a bacteria 
coreference module based on ML. However the 
submission was done without coreference 
resolution since their experiments did not show 
any performance improvement. 
 
Event extraction. Both UTurku and JAIST 
approached the event extraction as a classification 
task using ML (SVM). Bibliome exploited the co-
occurrence of arguments and the presence of 
trigger words from a predefined list. Both UTurku 
and Bibliome generate events in the scope of a 
sentence, whereas JAIST generates events in the 
scope of a paragraph. 
As shown in Table 6, UTurku achieved the best 
score for PartOf events. For all participants, the 
prediction is often correct (between 60 and 80%) 
while the recall is rather low (20 to 32%). 
 
  Recall Precis. F-score 
 Host 61 48 53 
 Host part 53 42 47 
 Geo. 13 38 19 
B. Env. 29 24 26 
 Water 60 55 57 
 Soil 69 59 63 
 Part-of 23 79 36 
 Host 30 43 36 
 Host part 18 68 28 
 Geo. 52 35 42 
J. Env. 5 0 0 
 Water 19 27 23 
 Soil 21 42 28 
 Part-of 31 61 41 
 Host 15 51 23 
 Host part 9 40 15 
 Geo. 32 40 36 
U. Env. 6 50 11 
 Water 1 7 2 
 Soil 12 21 15 
 Part-of 32 83 46 
Table 6. Event extraction results per type. 
 
Conversely, the score of the Localization relation 
by UTurku has been penalized by its low 
recognition of bacteria names (16%). This strongly 
affects the score of Localizations since the 
bacterium is the only expected agent argument. 
The good results of Bibliome are partly explained 
by its high bacteria name recall of 84%. 
The lack of coreference resolution might penalize 
the event extraction recall. To test this hypothesis, 
we computed the recall by taking only into account 
events where both arguments occur in the same 
sentence. The goal of this selection is to remove 
most events denoted through a coreference. The 
recall difference was not significant for Bibliome 
and JAIST, however UTurku recall raised by 12 
points (29%). That experiment confirms that 
UTurku low recall is explained by coreferences 
62
rather than the quality of event extraction. The 
paragraph scope chosen by JAIST probably 
compensates the lack of coreference resolution. 
As opposed to Bibliome, the precision of the 
Localization relation prediction by JAIST and 
UTurku, is high compared to the recall, with a 
noticeable exception of geographical locations. 
The difference between participants seems to be 
caused by the geographical entity recognition step 
more than the relation itself. This is shown by the 
difference between the entity and the event recall 
(Table 4 and 6 respectively).. The worst predicted 
type is Environment, which includes diverse 
locations, such as agricultural, natural and 
industrial sites and residues. This reveals 
significant room for improvement for Water, Soil 
and Environment entity recognition. 
8 Discussion 
The participant papers describe complementary 
methods for tackling BB Task?s new goals. The 
novelty of the task prevents participants from 
deeply investing in all of the issues together. 
Depending on the participants, the effort was 
focused on different issues with various 
approaches: entity recognition and anaphora 
resolution based on extensive use of background 
knowledge, and relation prediction based on 
linguistic analysis of syntactic dependencies. 
Moreover, these different approaches revealed to 
be complementary with distinct strengths and 
limitations. In the future, one may expect that the 
integration of these promising approaches will 
improve the current score. 
The corpus of BioNLP BB Task 2011 consists 
of a set of Web pages that were selected for their 
readability. However, some corpus traits make the 
IE task more difficult compared to scientific 
papers. For example, the relaxed style of some 
pages tolerates some typographic errors (e.g. 
morrow instead of marrow) and ambiguous 
anaphora. The genome sequencing project 
documents aim at justifying the sequencing of 
bacteria. This results in abundant descriptions of 
potential uses and locations that should not be 
predicted as actual locations. Their correct 
prediction requires complex analysis of modalities 
(possibility, probability, negation). Some pages 
describe the action of hosted bacteria at the 
molecular level, such as cellular infection. Terms 
related to the cell are ambiguous locations because 
they may refer to either bacteria or host cells. 
Scientific papers form a much richer source of 
bacterial location information that is exempt from 
such flaws. However, as opposed to Web pages, 
most of them are not publicly available and they 
are in PDF format. 
The typology of locations was designed 
according to the BB Task corpus with a strong bias 
towards natural environments since bioremediation 
and plant growth factor are important motivations 
for bacteria sequencing. It could be necessary to 
revise it according to a broader view of bacterial 
studies where pathogenicity and more generally 
human and animal health are central issues. 
9 Conclusion 
The Bacteria Biotope Task corpus and objectives 
differ from molecular biology text-mining of 
scientific papers. The annotation strategy and the 
analysis of the participant results contributed to the 
construction of a preliminary review of the nature 
and the richness of its linguistic specificities. The 
participant results are encouraging for the future of 
the Bacteria Biotope issue. The degree of 
sophistication of participating systems shows that 
the community has technologies, which are mature 
enough to address this crucial biology question. 
However, the results leave a large room for 
improvement. 
The Bacteria Biotope Task was an opportunity 
to extend molecular biology text-mining goals 
towards the support of bacteria biodiversity studies 
such as metagenomics, ecology and phylogeny. 
The prediction of bacterial location information is 
the very first step in this direction. The abundance 
of scientific papers dealing with this issue and 
describing location properties form a potentially 
rich source for further extensions. 
Acknowledgments 
The authors thank Valentin Loux for his valuable 
contribution to the definition of the Bacteria 
Biotope task. This work was partially supported by 
the French Quaero project. 
63
References 
Jari Bj?rne and Taio Salakoski. 2011. Generalizing 
Biomedical Event Extraction. Proceedings of the 
BioNLP 2011 Workshop Companion Volume for 
Shared Task. 
Cadixe. http://caderige.imag.fr/Articles/CADIXE-XML-
Annotation.pdf 
Corine Land Cover. 
http://www.eea.europa.eu/themes/landuse/interactive/
clc-download 
EnvDB database. http://metagenomics.uv.es/envDB/ 
EnvO Project. 
http://gensc.org/gc_wiki/index.php/EnvO_Project  
Dawn Field [et al. 2008. Towards a richer description 
of our complete collection of genomes and 
metagenomes: the ?Minimum Information about a 
Genome Sequence? (MIGS) specification. Nature 
Biotechnology. 26: 541-547. 
Melissa M. Floyd, Jane Tang, Matthew Kane and David 
Emerson. 2005. Captured Diversity in a Culture 
Collection: Case Study of the Geographic and 
Habitat Distributions of Environmental Isolates Held 
at the American Type Culture Collection. Applied 
and Environmental Microbiology. 71(6):2813-23. 
GenBank. http://www.ncbi.nlm.nih.gov/  
GOLD. http://www.genomesonline.org/cgi-
bin/GOLD/bin/gold.cgi 
Lynette Hirschman, Cheryl Clark, K. Bretonnel Cohen, 
Scott Mardis, Joanne Luciano, Renzo Kottmann, 
James Cole, Victor Markowitz, Nikos Kyrpides, 
Norman Morrison, Lynn M. Schriml, Dawn Field. 
2008. Habitat-Lite: a GSC case study based on free 
text terms for environmental metadata. Omics. 
12(2):129-136. 
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, 
Yoshinobu Kano, Jun?ichi Tsujii. 2010. Extracting 
bio-molecular events from literature - the BioNLP?09 
shared task. Special issue of the International 
Journal of Computational Intelligence. 
MicrobeWiki. 
http://microbewiki.kenyon.edu/index.php/MicrobeWi
ki  
Microbial Genomics Program at JGI. http://genome.jgi-
psf.org/programs/bacteria-archaea/index.jsf 
Microorganisms sequenced at Genoscope. 
http://www.genoscope.cns.fr/spip/Microorganisms-
sequenced-at.html 
Claire N?dellec. 2005. Learning Language in Logic - 
Genic Interaction Extraction Challenge" in 
Proceedings of the Learning Language in Logic 
(LLL05) workshop joint to ICML'05. Cussens J. and 
N?dellec C. (eds). Bonn. 
Claire N?dellec, Adeline Nazarenko, Robert Bossy. 
2008..Information Extraction. Ontology Handbook. 
S. Staab, R. Studer (eds.), Springer Verlag, 2008. 
Nhung T. H. Nguyen and Yoshimasa Tsuruoka. 2011. 
Extracting Bacteria Biotopes with Semi-supervised 
Named Entity Recognition and Coreference 
Resolution. Proceedings of the BioNLP 2011 
Workshop Companion Volume for Shared Task. 
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  
(2009). EnvDB, a database for describing the 
environmental distribution of prokaryotic taxa. 
Environmental Microbiology Reports. 1:198-207. 
Prokaryote Genome Projects at NCBI. 
http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  
Sampo Pyysalo, Antti Airola, Juho Heimonen, Jari 
Bj?rne, Filip Ginter and Tapio Salakoski. 2008. 
Comparative analysis of five protein-protein 
interaction corpora. BMC Bioinformatics. vol 9. 
Suppl 3. S6. 
Zorana Ratkovic, Wiktoria Golik, Pierre Warnier, 
Philippe Veber, Claire N?dellec. 2011. BioNLP 2011 
Task Bacteria Biotope ? The Alvis System. 
Proceedings of the BioNLP 2011 Workshop 
Companion Volume for Shared Task. 
Peter H. A. Sneath and Don J. Brenner. 1992. ?Official? 
Nomenclature Lists. American Society for 
Microbioloy News. 58, 175. 
Javier Tamames and Victor de Lorenzo. 2010. 
EnvMine: A text-mining system for the automatic 
extraction of contextual information. BMC 
Bioinformatics. 11:294. 
Web Soil Survey. http://websoilsurvey.nrcs.usda.gov/ 
64
Proceedings of BioNLP Shared Task 2011 Workshop, pages 102?111,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
BioNLP 2011 Task Bacteria Biotope ? The Alvis system 
Zorana Ratkovic1,2   Wiktoria Golik1    Pierre Warnier1   Philippe Veber1   Claire N?dellec1 
1 MIG INRA UR1077, Domaine de Vilvert F-850 Jouy-en-Josas, France forename.name@jouy.inra.fr   
2 LaTTiCe UMR 8094 CNRS Univ. Paris 3 1 rue Maurice Arnoux F-92120 MONTROUGE   Abstract 
This paper describes the system of the INRA Bibliome research group applied to the Bacteria Biotope (BB) task of the Bi-oNLP 2011 shared tasks. Bacteria, geo-graphical locations and host entities were processed by a pattern-based approach and domain lexical resources. For the extraction of environment locations, we propose a framework based on semantic analysis sup-ported by an ontology of the biotope do-main. Domain-specific rules were devel-oped for dealing with Bacteria anaphora. Official results show that our Alvis system achieves the best performance of participat-ing systems. 
1 Introduction Given a set of Web pages, the information extrac-tion goal of the Bacteria Biotope (BB) task is to precisely identify bacteria and their locations and to relate them. The type of the predicted locations has to be selected among eight types. Among them the host and host-part locations have to be related by the part-of relation. Three teams participated in the challenge.  BB task example Ureaplasma parvum is a mycoplasma and a pathogenic 
ureolytic mollicute which colonises 
 the urogenital tracts of humans.  One of the specificities of the BB task is that the bacteria location vocabulary is very large and vari-ous as opposed to protein subcellular locations in 
biology challenges (Kim et al, 2010) and geo-graphical locations (Zhou et al, 2005). Locations include natural environments and hosts as well as food and medical locations. In order to deal with this heterogeneity, we propose a framework based on a term analysis of the test corpus and a shallow mapping of these terms to a bacteria biotope (BB) termino-ontology. This mapping derives the type of location terms and filters out non-location terms. Large external dictionaries of host names (i.e. NCBI taxonomy) and geographical names (i.e. Agrovoc thesaurus) complete the lexical resources. The high frequency of bacteria anaphora and ambiguous antecedent candidates in the corpus was also a difficulty. Our Alvis system implements an anaphora resolution algorithm that takes into con-sideration the anaphoric distance and the position of the antecedent in the sentence. Alvis predicts the bacteria names and their relation to the locations with the help of hand-made patterns based on lin-guistic analysis and lexical resources.  The methods for predicting and typing locations (section 2) and bacteria (section 3) are first de-scribed. Section 4 details the method for relating them. Section 5 comments the experimental results. 2 Location  Our system handles separately the recognition of host and geographical names by dictionary map-pings, while the recognition of locations of the en-vironment and host part types is based on linguistic analysis and ontology inference.  Host names and geographical names appeared to be easier to predict by using a named-entity recog-nition strategy than the other types of location. They are less subject to variation than environ-mental locations, which can include any physical feature. For host name extraction, we used the NCBI taxonomy as the major source. Only the eu-karyote subtree was considered for host detection. 
Localization 
Part-of 
102
Our system filters out the ambiguous names such as Indicator (honeyguides) or Dialysis (xylophage insect) by comparing them to a list of common words in English. The host name list was enriched with additional common names including non-taxonomic host groups (e.g. herbivores), progeny names (e.g. calf) and human categories (e.g. pa-tient). The resulting host name list contains more than 1,800,000 scientific names and 60,000 com-mon names. The geographical name recognition component uses a small dictionary of all geo-graphic terms from the Agrovoc thesaurus sub-vocabularies. At first, we considered using the very rich resource GeoNames. However, it contains too many ambiguous names to be directly usable by short-term development. 2.1 Location of Environment type The identification of environment locations is done in two steps. First, the automatic extraction of all candidate terms from the test corpus, then the as-signment of a location type to these terms with the help of the Bacteria Biotope (BB) termino-ontology. The type assigned to a given term is the type of the closest concept label in the ontology. Since the BB termino-ontology was originally not structured according to the eight types, in order to be usable it first had to be enriched by the new concepts and then mapped to this topology.  Corpus term extraction. The corpus terms were automatically extracted by the AlvisNLP/ML pipe-line (Nedellec et al, 2008) with BioYatea (Nedel-lec et al, 2010). BioYatea is the version of Yatea (Hamon & Aubin, 2006) adapted to the biology domain. We modified BioYatea setting according to the training dataset study. We observed that most of the location terms in the training dataset are noun phrases with adjective modifiers (e.g. ro-dent nests) while prepositional phrases are rather rare (e.g. breaks in the skin). We set the term boundaries of BioYatea to include all prepositions except the of preposition. Considering other prepo-sitions such as with may yield syntactic attachment errors, thus we prefer the risk of incomplete terms to incorrect prepositional attachments. Bacteria Biotope ontology. We used the Bacte-ria Biotope (BB) termino-ontology for typing the extracted terms. It is under development for the study of bacteria phenotypes and habitats. The high level of the habitat part is structured in a manner similar to that proposed by the one level classifica-
tion by Floyd (Floyd et al, 2005). It has a fine-grained structure with the same goal as the general-ist EnvO habitat ontology (Field et al, 2008), but it focuses on bacteria phenotype and biotope model-ing. It includes a terminological level that records lexical forms of the concepts including terms, synonyms and variations. For the purpose of the challenge, the initial on-tology was manually completed using location concepts. The training corpus, as well as the habitat and isolation site fields of the GOLD database on sequenced prokaryotes (Liolios et al, 2009) are the main sources of location terms and synonyms. The analysis of the training corpus mainly led to the addition of adjectival forms of host parts (e.g. lym-phatic, intracellular) and human references (e.g. patient, infant, progeny).  The GOLD database isolation site field is a very rich source of bacteria location terms. It is filled by natural language descriptions of matters, natural habitats, hosts and geographical locations. For in-stance, the isolation site of Anoxybacillus flavi-thermus bacterium is waste water drain at the Wairakei geothermal power station in New Zea-land. The term analysis of GOLD isolation site en-tries yielded 3,415 location terms including 1,050 geographical names. Hundreds of these terms were manually added to the BB termino-ontology. The lack of time as well as the full sentence structure of the GOLD resource prevented us from correctly handling them in a fully automatic way. We are currently developing a method for the automatic alignment of the terms extracted from GOLD to the BB termino-ontology. Additionally, the GOLD habitat field provided around a hundred different terms that have been directly integrated into the BB termino-ontology. The current version of the habitat subpart of the BB termino-ontology contains 1,247 concepts and 266 synonyms.  Location types in Bacteria Biotope ontology. The BB termino-ontology has been developed pre-vious to the BB task and the structure of its habitat subpart does not reflect the eight location types of the task. In order to reuse the ontology for the BB task, we assigned types to each location concept. We manually associated the high level nodes of the location hierarchies to the eight BB task types. The types of the lower level concepts were then auto-matically inferred. For instance, the concept aquatic environment is tagged Water in the ontol-
103
ogy and all of its descendants lake, sea, ocean are of type Water as well. Local type exceptions were manually tagged. For instance, the waste tree in-cludes water-carried wastes of type Water and solid industrial residues of type Environment. This way all concepts in the resulting typed ontology were assigned a unique type. The concept types are then propagated to their associated term classes at the terminological level. For instance, underground water and its synonym subterranean water are both typed as Water. The resulting typed BB termino-ontology is then usable for deriving the types of the terms extracted from the test corpus. Derivation of location type. The BB termino-ontology scope is too limited for the correct predic-tion of all candidate term types by Boolean and exact comparison. From the 2,290 candidate terms of the test corpus, only 152 belong as such to the BB termino-ontology. We propose a method based on the head comparison of the candidate and BB terms for the derivation of the candidate term type.  The quality of the ontology-based annotation depends to a large extent on an accurate match be-tween the resource and the terms extracted from the corpus. Our method targets the syntactic structure of terms (candidate and BB terms) in order to gath-er the most of semantically similar terms. This approach differs from the ontology alignment and population methods that also use the information from the ontology structure in order to infer seman-tic relationships (e.g. hyponyms, meronyms) (Eu-zenat, 2007). It also differs from semantic annota-tion supported by context analysis such as distribu-tional semantics (Grefenstette, 1994) or Hearst pat-terns (Hearst, 1992). It belongs to the class of methods that focus on the morphology of the cor-pus terms, which use string-based (Levensthein, 1966, Jaro, 1989) or linguistic-based methods (Jac-quemin & Tzoukermann, 1999).  Even though the context-based approach should produce very good results, we chose a less time-consuming method that is easier and faster to set up, which is based on morphosyntactic analysis.  In our case, string similarity measures turn out to be irrelevant (laboratory rat does not mean rat labo-ratory). We observed that in candidate and BB terms, the head is very often the most informative element. Thus, the linguistic-based analysis of terms, in particular the head-similarity analysis (Hamon & Nazarenko, 2001), represents a promis-ing alternative. Our method is inspired by 
MetaMap (Aronson, 2001). MetaMap tags bio-medical corpora with the UMLS Metathesaurus by syntactic analysis that takes into account lexical heads of terms. The similarity scores computed by linguistically-based metrics are higher for terms whose heads have previously been analyzed.  The MetaMap method includes a variant compu-tation that maps acronyms, abbreviations, syno-nyms as well as derivational, inflectional and spell-ing variants. Our term typing method is less sophis-ticated and uses a few lexical variants due to the lack of a complete resource. Some ontology en-richment applications also use head-supported term matching, as in Desmontils (Desmontils et al, 2003). In Desmontils, new concepts belonging to WordNet (Fellbaum, 1998) are automatically added to the ontology in order to improve the indexing process. However, the analysis of the results shows that a great number of concepts found in the texts are not considered because they do not exist in WordNet. Our typing task uses a similar head-based method, but only for type derivation.  Our system derives the location type of candi-date terms in several steps. First, if there is a term in the BB termino-ontology that is strictly equal to the candidate term, it is assigned the same type. Then, the other candidate terms are assigned types according to the comparison of their heads to the BB term heads. We assume that in most of the cases the term head conveys the information about the type and is non-ambiguous. A given head H is non-ambiguous if all BB terms with head H are of the same type. The location term head set is the set of all habitat term heads found in the BB termino-ontology. The current version contains 693 differ-ent heads. Let Te denote the extracted term to be typed. If the head of Te does not belong to the BB term head set, then the type of Te is simply not Lo-cation (e.g. high metabolic diversity). If Te head does belong to the BB term head set and the head is non-ambiguous, then Te is assigned the associated type. For instance, the head of the extracted term stratified lake is lake. The type of all the BB terms with lake head is Water (e.g. meromictic lake). Stratified lake is therefore typed as Water.  Specific processing is applied to terms with am-biguous heads. The associative set of BB term heads and types exhibits some cases of ambiguous heads with multiple types that we analyzed in de-tail. There are two kinds of ambiguities that were 
104
processed in different ways. In the first, multiple types reflect different roles of the same object. In the second, the head is non-informative with re-spect to the type.  In the latter case the type is con-veyed by the subterm (term after head removal). We qualify non-informative BB term heads as neu-tral. They mainly denote habitats (habitat, envi-ronment, medium, zone) and extracts (sample, sur-face, isolate, material, content). In this case, the type is derived from the subterm. For instance, the head isolate of the extracted term marine isolate is neutral. After head removal, it is assigned the type Water since marine is of type Water. Freshwater has the same type as freshwater medium or fresh-water environment since medium and environment are neutral heads.  Some heads have more than one type although they denote specific locations. Their multiple types reflect different uses or states. For instance, the head bottle has two types: Food and Medical. The type Food is derived from the BB concept water bottle and the type Medical is derived from bedside water bottles in a hospital environment. The correct type for the extracted terms is then selected by a set of patterns based on the context of the term in the document. For instance, many vegetables and meats could be either of type Host or Food. The type is Host by default. One pattern states that if a term includes or is preceded by a food processing-related word (e.g. cooked, grilled, fermented), then the term is reassigned the type Food. Another pat-tern states that if a host is preceded by a death-related adjective (dead, decaying), then its type should be revised as Environment.  Our system currently includes nine disambigua-tion/retyping patterns. The first version of the type derivation method was automatically applied to the 1,263 GOLD terms after head analysis. Manual examination of the results yielded an extension of the two lists of neutral heads and heads with am-biguous types. There are 20 neutral heads and 21 ambiguous heads in the current version of the BB termino-ontology. The head-matching algorithm appears to be quite productive for the biotope terms. The procedure applied to the test corpus yielded the following figures: BioYatea extracted 2,290 terms. 416 terms matching the post-processing filters were discarded. This includes terms which are too general (i.e. approach, diver-sity), terms containing irrelevant or non desirable adjectives (i.e. numerous deficiencies, known spe-
cies) and terms containing forbidden words accord-ing to the annotation location rules (i.e. bacteria, pathogen, contaminated, parasite). Finally, 1,873 candidate terms were kept. Among these figures:  - 152  terms belong to the BB termino-ontology  - 90  terms were typed using the ontology heads - 6 terms with several types were handled by disambiguation patterns. We plan to extend the list of neutral heads and dis-criminate adjectives for type disambiguation by machine learning classification applied to the BB termino-ontology modifiers. Location entity boundary. The analysis of term extraction result from the training corpus shows that the predicted boundaries of locations were not fully consistent with the task annotation guidelines. Post-processing adjusts incorrect boundaries by filtering irrelevant words, packing and merging terms. Irrelevant words (e.g. contaminated, in-fected, host species, disease, inflammation) were removed from the location candidate terms inde-pendently of their types (e.g. contaminated Bach-man Road site vs. Bachman Road ; host plant vs. plant). Note that BioYatea extracts not only the maximum terms (e.g. contaminated Bachman Road site), but also their constituents (Bachman Road site, Bachman Road and site). Boundary adjust-ment often consists in selecting the relevant alter-native among the subterms.  Other boundary issues are handled by several patterns, which are applied after the typing stage. These patterns are type-dependent: each pattern only applies to one type or a subset of location types. When necessary, they shift the boundaries in order to include relevant modifiers.  They also split location terms or join adjacent location terms. BioYatea may have missed relevant modifiers be-cause of POS-tagging errors. For instance, if a na-tionality name precedes a location, then it is in-cluded (e.g. German oil field). Also, it frequently happens that hosts are modifiers of host parts (e.g. insect gut). BioYatea extracts the whole term and its constituents. The term is correctly typed as Host-part and the host modifier as Host. In order to avoid embedded locations, a specific pattern is de-voted to the splitting of these terms. In this way insect gut (Host-part) becomes insect (Host) and gut (Host-part). Most of these patterns involve several specific lexicons, including cardinal directions, relevant and 
105
irrelevant modifiers for each type of location, as well as types, which can be merged and split. The current resources were manually built by examin-ing the location terms of the training set and GOLD isolation fields. The acquisition of relevant and irrelevant modifiers could be automated by ma-chine learning. Some linguistic phenomena could be better handled by the customization of BioYa-tea. For instance BioYatea considers the preposi-tion with as a term boundary so it cannot extract terms containing with, like areas with high sulfur and salt concentrations.  3 Extraction of Bacteria names  We observed in the training corpus that not only were bacteria names tagged, but also higher level taxa (families) and lower level taxa (strains). We used the NCBI taxonomy as the main bacteria tax-on resource since it includes all organism levels and is kept up-to-date. This bacteria dictionary was enriched by taxa from the training corpus, in par-ticular by non standard abbreviations (e.g. Chl. = Chlorobium, ssp. = subsp) and plurals, (Vibrios as the plural for Vibrio) that were hopefully rather rare. Determining the boundaries of the bacteria names was one of the main issues because corpus strain names do not always follow conventional nomenclature rules.  Also, the recognition of bacte-ria name is evaluated using a strict exact match. Patterns were developed to account for such cases. They handle inversion (LB400 of Burkholderia xe-novorans instead of Burkholderia xenovorans LB400) and parenthesis (Tropheryma whipplei (the Twist strain) instead of Tropheryma whipplei strain Twist).  The corpus also mentions names of bacte-ria that contain modifiers not found in the NCBI dictionary, such as antimicrobial-resistant C. coli or L. pneumophila serogroup 1. Such cases, as well as abbreviations (e.g. GSB for green sulfur bacte-ria) and partial strain names (e.g. strain DSMZ 245 T for Chlorobium limicola strain DSMZ 245 T) were also specifically handled. The main source of error in bacteria name pre-diction is due to the mixture of family names and strain name abbreviations in the same text. It fre-quently happens that the strain name is abbreviated into the first word of the name. For instance Bar-tonella henselae is abbreviated as Bartonella. Un-fortunately, Bartonella is a genus mentioned in the 
same text, thus yielding ambiguities between the anaphora and the family name, which are identical. 3.1 Bacteria anaphora resolution Anaphors are frequent in the text, especially for bacteria reference and to a smaller extent for host reference. Our effort focused on bacteria anaphora resolution ignoring host anaphora. The extraction method of location relations (section 4) assumes that the relation arguments, location and bacterium (or anaphora of the bacterium) occur in the same sentence. From a total of 2,296 sentences in the training corpus, only 363 sentences contain both the location and the explicit bacterium, while 574 mention only the location. Two thirds of the loca-tions do not co-occur with bacteria. This demon-strates the importance of recovering the bacteria for these cases, which is potentially referred to by an explicit anaphora.  The manual examination of the training corpus showed that the most frequent anaphora of bacteria are not pronouns but higher level taxa, often pre-ceded by a demonstrative determinant, (i.e. This bacteria, This Clostridium) and sortal anaphora (i.e. genus, organism, species and strain), both of which are commonly found in biological texts (Torri & Vijay-Shanker, 2007). The style of some of the documents is rather relaxed and the antece-dent may be ambiguous even for a human reader. We observed three types of anaphora in the corpus. First, the standard anaphora which includes both pronouns and sortal anaphora, which requires a unique bacterial antecedent. Second, bi-anaphora or an anaphora that requires two bacteria antece-dents. This happens when the properties of two strains are compared in the document. Finally, the case of a higher taxon being used to refer to a lower taxon, which we named name taxon anaph-ora.  Anaphora with a unique antecedent C. coli is pathogenic in animals and humans. Peo-ple usually get infected by eating poultry that con-tained the bacteria, eating raw food, drinking raw milk, and drinking bottle water [?].  Anaphora with two antecedents C. coli is usually found hand in hand with its bac-teria relative, C. jejuni. These two organisms are recognized as the two most leading causes of acute inflammation of intestine in the United States and other nations. 
106
 Name taxon anaphora Ticks become infected with Borrelia duttonii while feeding on an infected rodent. Borrelia then multi-plies rapidly, causing a generalized infection throughout the tick.  For anaphora detection and resolution a pattern-based approach was preferred to machine learning because the constraints for relating anaphora to antecedent candidates of the same taxonomy level were mainly semantic and domain-dependent and the annotation of anaphora was not provided in the training corpus.  Anaphora detection consists of identifying po-tential anaphora in the corpus, given a list of pro-nouns, sortal anaphora and taxa and then filtering out irrelevant cases (Segura-Bedmar et al, 2010, Lin & Lian, 2004) before anaphora resolution. Not all the pronouns, sortal anaphora terms and higher taxon bacteria are anaphoric. For example, if a higher taxon is preceded or followed by the word genus, this signals that it is not anaphoric but that the text is actually about the higher taxon.   Non-anaphoric higher taxon Burkholderia cenocepacia HI2424[?] The genus Burkholderia consists of some 35 bacte-rial species, most of which are soil saprophytes and phytopathogens that occupy a wide range of environmental niches.  The anaphora resolution algorithm takes into ac-count two features: the distance to the antecedent candidate and its position in the sentence. The an-tecedent is usually found in proximity to the ana-phora, in order to maintain the coherence of the text. Therefore, our method ranks the antecedent candidates according to the anaphoric distance counted in sentences.  If more than one bacterium is found in a given sentence, their position is discriminate. Centering theory states that in a sentence the most prominent entities and therefore the most probable antecedent candidates are in the order: subject > object > other position (Grosz et al, 1995). In English, due to the SVO order of the language the subject is most of-ten found at the beginning of the sentence, fol-lowed by the object and the others. Therefore, the method retains the leftmost bacterium in the sen-tence when searching for the best antecedent can-didate. 
More precisely, the method selects the first ante-cedent that it finds according to the following pre-cedence list: - First bacterium in the current sentence (s) - First bacterium in the previous sentence    (s-1) - First bacterium in sentence s-2 - First bacterium in sentence s-3 - First bacterium in the current paragraph - Last bacterium in the previous paragraph - First bacterium in the first sentence of the document - The first bacterium ever mentioned. -  The method only relates anaphora to antecedents that are found before. It does not handle cataphors since they are rarely found in the corpus. For ana-phors that require two antecedents we use the same criteria but search for two bacteria in each sentence or paragraph, instead of one. For taxon anaphora we look for the presence of a lower taxon in the document found before the anaphora that is com-patible according to the species taxonomy. The counts of anaphora detected by the patterns are given in Table 1.   Corpus Single ante Bi ante Taxon ante Train 933 4 129 Dev 204 3 22 Test 240 0 18 Total 1,377 7 169  Table 1. The count of the types of anaphora per corpus.  The anaphora resolution algorithm allowed us to retrieve more sentences that contain both a bacte-rium and a location.  Out of the 574 sentences that contain only a location, 436 were found to contain an anaphora related to at least one bacterium. The remaining 138 sentences are cases where there is no bacterial anaphora or the bacterium name is im-plicit. It frequently happens that the bacterium is referred to through its action. For example in the sentence below, the bacterium name could be de-rived from the name of the disease that it causes.   In the 1600s anthrax was known as the "Black bane" and killed over 60,000 cows.  One of the questions we had about the resolution of anaphora is whether anaphora that are found in the same sentence together with a bacterium (there-fore potentially its antecedent) should be consid-
107
ered or not.  We tested this on the development set. We found that removing such anaphora from con-sideration improved the overall score. It yielded an F-score of 53.22% (precision: 46.17%, recall: 62.81%), compared to the original F-score of 50.15% (precision: 41.06%, recall: 64.44%). This improvement in F-score is solely due to an increase in precision, which shows that while resolving anaphora is important and required, the incorrect recognition of terms as anaphora and incorrect anaphora resolution can introduce noise. 4 Relation extraction In this work we concentrated most of our effort on the prediction of entities. For the prediction of events we used a strategy based on the co-occurrence of  arguments and trigger words within a sentence: - If a bacteria name, a location and a trigger word are present in a sentence, then the system pre-dicts a Localization event between the bacte-rium and the location. - If a bacteria anaphora, a location and a trigger word are present in a sentence, then the system predicts a Localization event between each ana-phora antecedent and the location. - If a host, a host part, a bacterium and at least one trigger word are present in a sentence, then the system predicts a PartOf event between the host and the host part.  The list of trigger words contains 20 verbs (e.g. inhabit, colonize, but also discover, isolate), 16 disease markers (e.g. chronic, pathogen) and 19 other relevant words (e.g. ingest, environment, niche). This list was designed by ranking words in the sentences of the training corpus containing both a bacteria name and a location. The ranking crite-rion used was the information gain with respect to whether the sentence contained an event or not. The ranked list was adjusted by removing spurious words and adding domain knowledge words. By removing the constraint of the occurrence of a trigger word in the sentence, we can determine that the maximum recall the method can achieve with this strategy is 47% (precision: 41%, F-score: 44%). The selected trigger word list yielded a re-call close to the maximum, thus it seems that the trigger words do not affect the recall and are suit-able for the task.  
5 Results Table 2 summarizes the official scores that the Bib-liome Alvis system achieved for the Bacteria Biotope Task. It ranked first among three partici-pants. The first column gives the recall of entity prediction. The prediction of hosts and bacteria named-entities achieved a good recall of 84 and 82, respectively.    Entity recall Event recall Event Precis. F-score Bacteria 84 - - - Host 82 61 48 53 Host part 72 53 42 47 Env. 53 29 24 26 Geo. 29 13 38 19 Food - - 29 41 Medical 100 50 33 40 Water 83 60 55 57 Soil 86 69 59 63 Total  45 45 45  Table 2. Bibliome system scores at Bacteria Biotope Task in BioNLP shared tasks 2011.  However, geographical locations based on a similar strategy were poorly predicted (29%). Our system predicted only 15 countries. A more appropriate resource of geographical names than the Agrovoc thesaurus would certainly increase the recall of geographical locations.  The host parts, medical, water and soil locations predicted with the same ontology-based method were surprisingly good with a recall of 72, 100, 83 and 86, respectively. The small size of the ontology and the small number of different term heads (i.e. 51 different heads) initially appeared as a limitation factor for reuse on new corpora. The good recall shows that the location vocabulary of the test set has similarities with the training set compared to potential space of location names.  The potential space is reflected by the richness of the GOLD iso-lation site field. This demonstrates the robustness of the type derivation approach based on term heads. The correctness of the derivation type can-not be calculated without a corpus where all the locations and not only bacteria ones are annotated. The recall of the environment location prediction is a little bit lower, 53%. The environment type in-
108
cludes many different types that cannot all be an-ticipated. Therefore the coverage of the BB ter-mino-ontology environment part is limited except for water and soil, which are more focused topics.  The localization event recall (column 2) is on average 20% lower for all types than the location entity recall. The regularity of the difference may suggest that once the argument is identified, the localization relation is equally harder to find by our method independently of the type. The localization event precision (column 3) is more difficult to ana-lyze because many sources of error may be in-volved, such as an incorrect arguments, incorrect anaphora resolution, relation to the wrong bacte-rium among several or the absence of a relation.  The prediction precision of localization events involving soil, water and host is better than envi-ronment and food. The manual analysis of the test corpus shows that in some cases environmental locations were mentioned as potential sources of industrial applications without actually being bac-teria isolation places. For instance, in Other fields of application for thermostable enzymes are starch-processing, organic synthesis, diagnostics, waste treatment, pulp and paper manufacture, and ani-mal feed and human food, the Alvis system errone-ously predicted waste treatment, paper manufac-ture, animal feed and human food.  This is due to the fact that the system does not handle modalities. Such hypotheses are specific to the BB task text genre, i.e. Bacteria sequencing projects. Such pro-jects contain details for potential industrial applica-tions, which are absent from academic literature. Ambiguous types are also a source of error. De-spite the host dictionary cleaning, some ambigui-ties remained. For example, the head canal in tooth root canal is erroneously typed as water and should be disambiguated with its tooth host-part modifier.  After test publication we measured the gain of anaphora resolution by using the on-line service. The anaphora resolution algorithm was found to have a strong impact on the final result.  Running the test set using all of the modules except for the anaphora resolution algorithm yielded a decrease in the F-score by almost 13% (F-score: 32.5%, preci-sion: 48.5%, 24.4%).  This shows that the addition of an anaphora resolution algorithm significantly increases the precision and that a resolution algo-rithm adapted to the Bacteria domain is necessary for the Biotope corpus. 
The part-of event prediction relies on the strict co-occurrence of a bacterium, trigger word, host and host part within a sentence. An additional run with the more relaxed constraint where the bacte-rium can be denoted by an anaphora as well yielded a gain of 6 recall points, a loss of 5 preci-sion points with a net benefit of 1 F-measure point. 6 Discussion The use of trigger words for the selection of sen-tences for relation extraction does not take into ac-count the structure or syntax of the sentence for the prediction of relation arguments. The system pre-dicts all combinations of bacteria and locations as localization events and all combination of host and host parts as part-of event. This has a negative ef-fect on the precision measure since some pairs are irrelevant as in the sentence below.  Baumannia cicadellinicola. This newly discovered or-ganism is an obligate endosymbiont of the leafhopper insect Homalodisca coagulata (Say), also known as the Glassy-Winged Sharpshooter, which feeds on the xylem of plants.  It has been shown that the use of syntactic de-pendencies to extract biological events (such as protein-protein interactions) improves the results of such systems (Erkan et al, 2007, Manine et al, 2008, Airola et al 2008). The use of syntactic de-pendencies could offer a more in depth examina-tion of the syntax and the semantics and therefore allow for a more refined extraction of bacteria-localization and host-host part relations.   Term extraction appears to be a good method for predicting locations including unseen terms, but it is limited by the typing strategy that filters out all terms with unknown heads (with respect to the BB termino-ontology). In the future, we will study the effect of linguistic markers such as enumeration and exemplification structures for recovering addi-tional location terms. For instance, in heated or-ganic materials such as compost heaps, rotting hay, manure piles or mushroom growth medium, our system has correctly typed heated organic ma-terials as environment but not the other examples because of their unknown heads. The promising performance of the Alvis system on the BB task shows that a combination of semantic analysis and domain-adapted resources is a good strategy for information extraction in the biology domain. 
109
References  Agrovoc: http://aims.fao.org/website/AGROVOC-Thesaurus Antti Airola, Sampo Pyysalo, Jari Bj?rne, Tapio Pah-nikkala, Filip Ginter, and Tapio Salakoski. 2008. A Graph Kernel for Protein-Protein Interaction Extrac-tion. BioNLP2008: Current Trends in Biomedical Natural Language Processing, pages 1-9. Alan R. Aronson. 2001. Effective mapping of biomedi-cal  text to the UMLS Metathesaurus: The MetaMap program. Proceedings of AMIA Symposium 2001, pages 17-21. Emmanuel Desmontils, Christine Jacquin and  Laurent Simon. 2003. Ontology enrichment and indexing process. Research report RR-IRIN-03.05, Institut de Recherche en Informatique de Nantes, Nantes, Fran-ce. J?r?me Euzenat and Pavel Shvaiko. 2007. Ontology matching, Springer Verlag, Heidelberg (DE),page 333. Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metage-nomes: the Minimum Information about a Genome Sequence (MIGS) specification. Nature Biotechnol-ogy 26, pages 541-547. GeoNames: http://www.geonames.org/  Gregory Grefenstette. 1994. Explorations in Automatic Thesaurus Discovery.  Natural Language Processing and Machine Translation. London: Kluwer Academic Publishers. Barbara J. Grosz, Araving K. Joshi and Scott Weinstein. 1995. Centering: A Framework for Modelling the Lo-cal Coherence of Discourse.  University of Pennsyl-vania Institute for Research in Cognitive Science Technical Reports Series. G?ne? Erkan, Arzucan ?zg?r and Dragomir R. Radev. 2007. Semi-Supervised Classification for Extracting Protein Interaction Sentences using Dependency Parsing. Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pag-es 228-237. Thierry Hamon and Sophie Aubin. 2006. Improving term extraction with terminological resources. In Salakoski, T. et al, editors, Advances in Natural Lan-guage Processing 5th International Conference on NLP (Fin- TAL?06), pages 380?387. Springer. Thierry Hamon and Adeline Nazarenko. 2001. Detection of synonymy links between terms: experiment and re-
sults, Recent Advances in Computational Terminol-ogy. Pages 185-208. John Benjamins. Marti A. Hearst. 1992. Automatic acquisition of hypo-nyms from large text corpora. In Zampolli, A.(ed.), Proceedings of the 14 th COLING, pages 539?545, Nantes, France. Christian Jacquemin and Evelyne Tzoukermann. 1999. NLP for term variant extraction: A synergy of mor-phology, lexicon, and syntax. In Strzalkowski, T. (ed.), Natural language information retrieval, volume 7 of Text, speech and language technology, chapter 2, pages  25?74. Dordrecht & Boston: Kluwer Aca-demic Publishers. Matthew A. Jaro. 1989. Advances in record linkage me-thodology as applied to matching the 1985 census of Tampa, Florida. Journal of the American Statistical Association 84(406), pages 414-20. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano and Jun?ichi Tsujii. (to appear). Extract-ing bio-molecular events from literature - the Bi-oNLP?09 shared task. Special issue of the Interna-tional Journal of Computational Intelligence. Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions and reversals. Dok-lady akademii nauk SSSR, 163(4):845-848, 1965. In Russian. English translation in Soviet Physics Dok-lady, 10(8), pages 707-710. Yu-Hsiang Lin and Tyne Liang. 2004. Pronomial and Sortal Anaphora Resolution for Biomedical Litera-ture. In Proceedings of ROCLING XVI: Conference on Computational Linguistics and Speech Processing.  Konstantinos Liolios, I-Min A. Chen., Konstantinos Mavromatis, Nektarios Tavernarakis, Philip Hugen-holtz, Victor M. Markowitz and Nikos C. Kyrpides. 2009. The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. NAR Epub. Alain-Pierre Manine, Erick Alphonse and Philippe Bes-si?res. 2008. Information extraction as an ontology population task and its application to genic interac-tions, 20th IEEE Intl. Conf. Tools with Artificial In-telligence, ICTAI'08., vol. II, pp. 74-81. NCBI taxonomy: http://www.ncbi.nlm.nih.gov/Taxonomy/  Claire N?dellec, Wiktoria Golik, Sophie Aubin and Robert Bossy. 2010. Building Large Lexicalized On-tologies from Text: a Use Case in Indexing Biotech-nology Patents, International Conference on Knowl-edge Engineering and Knowledge Management (EKAW 2010), Lisbon, Portugal. 
110
Isabel Segura-Bedmar, Mario Crespo, C?sar de De Pa-blo-S?nchez and Paloma Mart?nez. 2010. Resolving anaphoras for the extraction of drug-drug interactions in pharmacological documents. BMC Bioinformatics 11(Supl 2):S1. Manabu Torii and K. Vijay-Shanker. 2007. Sortal Anaphora Resolution in Medline Abstracts. Computa-tional Intelligence 23, pages 15-27. Zhou GuoDong, Su Jian, Zhang Jie and Zhang Min. 2005. Exploring Various Knowledge in Relation Ex-traction. In Proceedings of the 43rd Annual Meeting of the ACL, pages 427-434, Ann Arbor. Association for Computational Linguistics.  
111
Proceedings of BioNLP Shared Task 2011 Workshop, pages 121?129,
Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational Linguistics
Sentence Filtering for BioNLP: Searching for Renaming Acts
Pierre Warnier1,2 Claire Ne?dellec1
1MIG INRA UR 1077, F78352 Jouy-en-Josas, France
2LIG Universite? de Grenoble, France
forename.lastname@jouy.inra.fr
Abstract
The Bacteria Gene Renaming (RENAME)
task is a supporting task in the BioNLP Shared
Task 2011 (BioNLP-ST?11). The task con-
sists in extracting gene renaming acts and gene
synonymy reminders in scientific texts about
bacteria. In this paper, we present in details
our method in three main steps: 1) the doc-
ument segmentation into sentences, 2) the re-
moval of the sentences exempt of renaming act
(false positives) using both a gene nomencla-
ture and supervised machine learning (feature
selection and SVM), 3) the linking of gene
names by the target renaming relation in each
sentence. Our system ranked third at the of-
ficial test with 64.4% of F-measure. We also
present here an effective post-competition im-
provement: the representation as SVM fea-
tures of regular expressions that detect com-
binations of trigger words. This increases the
F-measure to 73.1%.
1 Introduction
The Bacteria Gene Renaming (Rename) supporting
task consists in extracting gene renaming acts and
gene synonymy reminders in scientific texts about
bacteria. The history of bacterial gene naming has
led to drastic amounts of homonyms and synonyms
that are often missing in gene databases or even
worse, erroneous (Nelson et al, 2000). The auto-
matic extraction of gene renaming proposals from
scientific papers is an efficient way to maintain gene
databases up-to-date and accurate. The present work
focuses on the recognition of renaming acts in the
literature between gene synonyms that are recorded
in the Bacillus subtilis gene databases. We assume
that renaming acts do not involve unknown gene
names. Instead, our system verifies the accuracy of
synonymy relations as reported in gene databases by
insuring that the literature attests these synonymy re-
lations.
1.1 Example
This positive example of the training corpus is rep-
resentative of the IE task:
?Thus, a separate spoVJ gene as defined by the 517
mutation does not exist and is instead identical with
spoVK.?
There are 2 genes in this sentence:
ID Start End Name
T1 17 22 spoVJ
T2 104 109 spoVK
Table 1: Example of provided data.
There is also a renaming act: R1 Renaming For-
mer:T1 New:T2
Given all gene positions and identifications (Tn),
the Rename task consists in predicting all renaming
acts (Rn) between Bacillus subtilis genes in multi-
sentence documents. The gene names involved are
all acronyms or short names. Gene and protein
names often have both a short and a long form. Link-
ing short to long names is a relatively well-known
task but linking short names together remains lit-
tle explored (Yu et al, 2002). Moreover, specifying
some of these synonymy relations as renaming ap-
pears quite rare (Weissenbacher, 2004). This task
relates to the more general search of relations of
121
synonymous nicknames, aliases or pseudonyms of
proper nouns from definitory contexts in encyclope-
dia or dictionaries. For instance, in Alexander III
of Macedonia commonly known as Alexander the
Great the synonymy relation is supported by com-
monly known as between the proper noun Alexan-
der III of Macedonia and the nickname Alexander
the Great. Renaming act extraction differs from the
search of coreferences or acronyms by the linguistic
markers involved.
1.2 Datasets
The renaming corpus is a set of 1,648 PubMed refer-
ences of bacterial genetics and genome studies. The
references include the title and the abstract. The
annotations provided are: the position and name of
genes (see Table 1) for all sets and the renaming acts
in the training and the development sets only.
Train Dev. Test
Documents 1146 246 252
Genes 14372 3331 3375
Unique Genes 3415 1017 1126
New genes 0 480 73
Relations 308 65 88
Words / Doc 209 212 213
Genes / Doc 12.5 12.7 13.4
Unique Genes / Doc 3.0 4.1 4.5
Relations / Doc 0.27 0.26 0.35
Table 2: Datasets of the Rename task corpus.
2 Methods
An early finding is that renaming acts very seldom
span several sentences (i.e. former and new are in
the same sentence). For the training set, 95.4% of
the relations verify this claim and in the develop-
ment set, 96.1%. Thus, it is decided to first segment
the documents into sentences and then to look for re-
naming acts inside independent sentences. Thus the
maximum expected recall is then 96.1% on the de-
velopment set. This is done by automatically filter-
ing all the sentences out that do not contain evidence
of a renaming act and then to relate the gene names
occurring in the renaming sentences. The AlvisNLP
pipeline (Ne?dellec et al, 2009) is used throughout
this process (see Fig. 1).
List based filtering
Machine learning based filtering
Attribute selection on lemmas
(AnnotationClassifierAttributeSelection)
Classification: grid search
(AnnotationClassifierTrain)
Selection of best parameters
Bacteria 
Nomenclature
Tagging
(AnnotationClassifierTag)
Lemmatization
(TreeTagger)
Gene search
.a2 files
Fix forms
(SimpleContentProjector)
Word segmentation
(WoSMIG)
Sentence segmentation
(SeSMIG)
Genes
Species
Molecules
Acronyms (imp)
Abreviations (imp)
Bacteria (imp)
Stop words
Bacteria
Regular 
expressions (imp)
Figure 1: Flowchart: Notes represent the resources used
and (imp) represent later improvements not used for the
official submission.
2.1 Word and sentence segmentation
Word and sentence segmentation is achieved by the
Alvis NLP pipeline. Named entity recognition sup-
plements general segmentation rules.
2.1.1 Derivation of boundaries from named
entities
Named entities often contains periods that should
not be confused with sentence ends. Species abbre-
viations with periods are specially frequent in the
task corpus. First, dictionaries of relevant named
entities from the molecular biology domain (e.g.
122
genes, species and molecules) are projected onto
the documents before sentence segmentation, so
that periods that are part of named entities are dis-
ambiguated and not interpreted as sentence ends.
Moreover, named enties are frequently multi-word.
Named entity recognition prior to segmentation pre-
vents irrelevant word segmentation. For example,
the projection of named entity dictionaries on the ex-
cerpt below reveals the framed multi-word entities:
?Antraformin, a new inhibitor of Bacillus subtilis
transformation. [...] During this screening program,
Streptomyces sp. 7725-CC1 was found to produce
a specific inhibitor of B. subtilis transformation.?
2.1.2 Word segmenter
The word segmenter (WosMIG in Fig. 1) has the
following properties: 1) primary separator: space,
2) punctuation isolation: customized list, 3) custom
rules for balanced punctuation, 4) fixed words: not
splittable segments The following list of terms is
obtained from the example:
[?Antraformin? , ?,?, ?a?, ?new?, ?inhibitor?, ?of?,
? Bacillus subtilis ?, ?transformation?, ?.?, [...],
?During?, ?this?, ?screening?, ?program?, ?,?,
? Streptomyces sp. ?, ? 7725-CC1 ?, ?was?, ?found?,
?to?, ?produce?, ?a?, ?specific?, ?inhibitor?, ?of?,
? B. subtilis ?, ?transformation?, ?.?]
2.1.3 Sentence segmenter
The sentence segmenter (SeSMIG in Fig. 1) has
the following properties: 1) strong punctuation:
customized list; 2) tokens forcing the end of a
sentence (e.g. etc...); 3) an upper case letter must
follow the end of a sentence. The system works
very well but could be improved with supervised
machine learning to improve the detection of
multi-word named entities. Finally, the list of words
is split into sentences:
[[?Antraformin? , ?,?, ?a?, ?new?, ?inhibitor?, ?of?,
? Bacillus subtilis ?, ?transformation?, ?.?],
[...],
[?During?, ?this?, ?screening?, ?program?, ?,?,
? Streptomyces sp. ?, ? 7725-CC1 ?, ?was?, ?found?,
?to?, ?produce?, ?a?, ?specific?, ?inhibitor?, ?of?,
? B. subtilis ?, ?transformation?, ?.?]]
2.2 Sentence filtering
Once the corpus is segmented into sentences, the
system filters out the numerous sentences that most
likely do not contain any renaming act. This way,
the further relation identification step focuses on rel-
evant sentences and increases the precision of the
results (Nedellec et al, 2001). Before the filtering,
the recall is maximum (not 100% due to few renam-
ing acts spanning two sentences), but the precision
is very low. The sentence filters aim at keeping the
recall as high as possible while gradually increasing
the precision. It is composed of two filters. The first
filter makes use of an a priori knowledge in the form
of a nomenclature of known synonyms while the
second filter uses machine learning to filter the re-
maining sentences. In the following, the term Bacil-
lus subtilis gene nomenclature is used in the sense of
an exhaustive inventory of names for Bacillus sub-
tilis genes.
2.2.1 Filtering with a gene nomenclature
We developed a tool for automatically building
a nomenclature of Bacillus subtilis gene and pro-
tein names. It aggregates the data from various
gene databases with the aim of producing the most
exhaustive nomenclature. The result is then used
to search for pairs of synonyms in the documents.
Among various information on biological sequences
or functions, the entries of gene databases record
the identifiers of the genes and proteins as asserted
by the biologist community of the species. Bacil-
lus subtilis community as opposed to other species
has no nomenclature committee. Each database cu-
rator records unilateral naming decisions that may
not reported elsewhere. The design of an exhaus-
tive nomenclature require the aggregation of multi-
ple sources.
Databases Our sources for the Bacillus subtilis
nomenclature are six publicly available databases
plus an in-house database. The public databases
are generalist (1 to 3) or devoted to Bacillus subtilis
genome (4 to 6) (see Table 3):
GenBank The genetic sequence database managed
by the National Center for Biotechnology In-
formation (NCBI) (Benson et al, 2008). It con-
tains the three official versions of the annotated
123
genome of B. subtilis with all gene canonical
names;
UniProt the protein sequence database managed by
the Swiss Institute of Bioinformatics (SIB),
the European Bioinformatics Institute (EBI)
and the Protein Information Resource (PIR)
(Bairoch et al, 2005). It contains man-
ual annotated protein sequences (Swiss-Prot)
and automatically annotated protein sequences
(TrEMBL (Bairoch and Apweiler, 1996)). Its
policy is to conserve a history of all informa-
tion relative to these sequences and in particu-
lar all names of the genes that code for these
sequences.
Genome Reviews The genome database managed
by the European Bioinformatics Institute (EBI)
(Sterk et al, 2006). It contains the re-annotated
versions of the two first official versions of the
annotated genome of B. subtilis;
BSORF The Japanese Bacillus subtilis genome
database (Ogiwara et al, 1996);
Genetic map the original genetic map of Bacillus
subtilis;
GenoList A multi-genome database managed by
the Institut Pasteur (Lechat et al, 2008). It con-
tains an updated version of the last official ver-
sion of the annotated genome of B. subtilis;
SubtiWiki A wiki managed by the Institute for Mi-
crobiology and Genetics in Go?ttingen (Flo?rez
et al, 2009) for Bacillus subtilis reannotation.
It is a free collaborative resource for the Bacil-
lus community;
EA List a local lexicon manually designed from
papers curation by Anne Goelzer and E?lodie
Marchadier (MIG/INRA) for Systems Biology
modeling (Goelzer et al, 2008).
Nomenclature merging We developed a tool for
periodically dumping the content of the seven source
databases through Web access. With respect to gene
naming the entries of all the databases contain the
same type of data per gene:
? a unique identifier (required);
? a canonical name, which is the currently rec-
ommended name (required);
? a list of synonyms considered as deprecated
names (optional).
The seven databases are handled one after the
other. The merging process follows the rules:
? the dump of the first database (SubtiWiki, see
Table 3 for order) in the list is considered the
most up-to-date and is used as the reference
for the integration of the dumps of the other
databases;
? for all next dumps, if the unique gene identifier
is new, the whole entry is considered as new
and the naming data of the entry is added to the
current merge;
? else, if the unique identifier is already present
into the merge, the associated gene names are
compared to the names of the merge. If the
name does not exist in the merge, it is added to
the merge as a new name for this identifier and
synonym of the current names. The synonym
class is not ordered.
Order Databases AE AN
1 SubtiWiki 4 261 5920
2 GenoList 0 264
3 EA List 33 378
4 BSORF 0 42
5 UniProt 0 74
6 Genome 0 0
Reviews
7 GenBank 0 7
8 Genetic Map 0 978
Total 4 294 7 663
Table 3: Database figures. AE: number of added entries,
AN: number of added names.
Synonym pair dictionary: The aggregated
nomenclature is used to produce a dictionary of all
combinations of pairs in the synonym classes.
124
Sentence filtering by gene cooccurrence: For
each sentence in the corpus, if a pair of gene syn-
onyms according to the lexicon is found inside then
the sentence is kept for the next stage. Other-
wise, it is definitively discarded. The comparison
is a case-insensitive exact match preserving non al-
phanumeric symbols. The recall at this step is re-
spectively 90.9% and 90.2% on the train and devel-
opment sets. The recall loss is due to typographic
errors in gene names in the nomenclature. The pre-
cision at this stage is respectively 38.9% and 38.1%
on the train and development sets. There are still
many false positives due to gene homologies or re-
naming acts concerning other species than Bacillus
subtilis for instance.
2.2.2 Sentence filtering by SVM
Feature selection The second filtering step aims
at improving the precision by machine learning clas-
sification of the remaining sentences after the first
filtering step. Feature selection is applied to enhance
the performances of the SVM as it is shown to suffer
from high dimensionality (Weston et al, 2001). Fea-
ture selection is applied to a bag-of-word representa-
tion using the Information Gain metrics of the Weka
library (Hall et al, 2009). Words are lemmatized by
TreeTagger (Schmid, 1994). A manual inspection
of the resulting sorting highly ranks words such as
formerly or rename and parentheses while ranking
other words such as cold or encode surprisingly cer-
tainly due to over-fitting. Although the feature se-
lection is indeed not particularly efficient compared
to the manual selection of relevant features but does
help filtering out unhelpful words and then drasti-
cally reducing the space dimension from 1919 to
141 for the best run.
Sentence classification and grid search: A SVM
algorithm (LibSVM) with a RBF kernel is applied
to the sentences encoded as bag of words. The two
classes are: ?contains a renaming act? (True) or not
(False). There are 4 parameters to tune: 1) the num-
ber of features to use (N ? 1, 5, 10, ..., 150) mean-
ing the N first words according to the feature selec-
tion, 2) the weight of the classes: True is fixed to 1
and False is tuned (W ? 0.2, 0.4, ..., 5.0), 3) the er-
rors weight (C ? 2?5,?7,...,9), 4) the variance of the
Gaussian kernel (G ? 2?11,?9,...,1). Thus, to find
the best combination of parameters for this problem,
#N ?#W ?#C ?#G = 31 ? 25 ? 8 ? 7 = 43, 400
models are trained using 10-fold cross-validation on
the training and development sets together (given
the relatively small size of the training set) and
ranked by F-measure. This step is mandatory be-
cause the tuning of C and G alone yield variations
of F-measure from 0 to the maximum. The grid
search is run on a cluster of 165 processors and takes
around 30 minutes. The best model is the model
with the highest F-measure found by the grid search.
Test sentence filtering: Finally the test set is sub-
mitted to word and sentence segmentation, feature
filtering and tagged by the best SVM model (Anno-
tationClassifierTag in Fig. 1). The sentences that are
assumed to contain a renaming act are kept and the
others are discarded (see Fig. 2).
2.3 Gene position searching
At this step, all remaining sentences are assumed to
be true positives. They all contain at least one pair
of genes that are synonymous according to our gene
nomenclature. The other gene names are not con-
sidered. The method for relating gene candidates by
a renaming relation, relies on the assumption that
all gene names are involved in at least one relation.
Most of the time, sentences contain only two genes.
We assume in this case that they are related by a re-
naming act. When there are more than two genes
in a sentence, the following algorithm is applied: 1)
compute all combinations of couples of genes; 2)
look-up the lexicon for those couples and discard
those that are not present; 3) if a given gene in a
couple has multiple occurrences, take the nearest in-
stance from the other gene involved in the renaming
act.
3 Discussion
The system ranks 3rd/3 among three participants
in the Rename task official evaluation with a F-
measure of 64.4% (see Fig. 4), five points behind the
second. The general approach we used for this task
is pragmatic: 1) simplify the problem by focusing on
sentences instead of whole documents for a minimal
loss, 2) then use a series of filters to improve the pre-
cision of the sentence classification while keeping
the recall to its maximum, 3) and finally relate gene
125
names known to be synonymous inside sentences for
a minimal loss (around 2% of measure). As opposed
to what is observed in Gene Normalization tasks
(Hirschman et al, 2005), the Rename task is char-
acterised by the lack of morphological resemblance
of gene synonyms. The gene synonyms are not ty-
pographic variants and the recognition of renaming
act requires gene context analysis. The clear bottle-
neck of our system is the sentence filtering part and
in particular the feature selection that brings a lot
of noise by ranking statistically spurious terms. On
the plus side, the whole system is fully automated
to the exception of the resources used for the word
segmentation that were designed manually for other
tasks. Moreover, our strategy does not assume that
the gene pairs from the nomenclature may be men-
tioned for other reasons than renaming, it then tends
to overgeneralize. However, many occurrences of
the predicted gene pairs are not involved in renaming
acts because the reasons for mentioning synonyms
may be different than renaming. In particular, equiv-
alent genes of other species (orthologues) with high
sequence similarities may have the same name as in
Bacillus subtilis. An obvious improvement of our
method would consists in first relating the genes to
their actual species before relating the only Bacillus
subtilis gene synonyms by the renaming relation.
Team Pre. Rec. F-M.
U. of Turku 95.9 79.6 87.0
Concordia U. 74.4 65.9 69.9
INRA 57.0 73.9 64.4
Table 4: Official scores in percentage on the test set.
3.1 Method improvement by IE patterns
After the official submission and given the result of
our system compared to competitors, a simple mod-
ification of the feature selection was tested with sig-
nificant benefits: the addition of regular expressions
as additional features. Intuitively there are words or
patterns that strongly appeal to the reader as impor-
tant markers of renaming acts. For example, vari-
ations of rename or adverbs such originally or for-
merly would certainly be reasonable candidates. Fif-
teen such shallow patterns were designed (see Table
5) supplemented by six more complex ones, orig-
inally designed to single out gene names. In ap-
pendix A, one of them is presented, the precision
of which is 95.3% and recall 27.5%. That is, more
than a quarter of renaming acts in the training and
development sets together. Interestingly, in table
5 the word formerly (3rd in feature selection rank-
ing) alone recalls 10.7% of the renaming acts with
a precision of 96.9%. In contrast, the words origi-
nally and reannotated although having 100% preci-
sion are respectively ranked 33rd and 777th. In total,
21 patterns are represented as boolean features of
the classification step in addition to the ones selected
by feature selection. Unsurprisingly, the best classi-
fiers, according to the cross-validation F-measure af-
ter the grid search, only used the regular expressions
as features neglecting the terms chosen by feature
selection. A significant improvement is achieved:
+8.7% of F-measure on the test set (see Fig. 2).
Pattern Pre. Rec. F-M.
(reannotated) 100.0 0.4 0.7
(also called) 100.0 0.4 0.7
(formerly) 96.9 10.7 19.2
(originally) 100.0 1.4 2.8
((also)? known as) 100.0 1.8 3.4
(were termed) 100.0 0.4 0.7
(identity of) 100.0 0.7 1.4
(be referred (to|as)?) 100.0 0.4 0.7
(new designation) 100.0 0.4 0.7
( allel\w+) 80.0 2.8 5.4
(split into) 100.0 0.4 0.7
( rename ) 83.4 1.8 3.4
( renamed ) 88.5 8.0 14.6
( renaming ) 100.0 0.4 0.7
(E(\.|scherichia) coli) 11.3 4.5 6.4
Table 5: Handwritten patterns. Scores are in percentage
on the training and development sets together after the
gene nomenclature filtering step. A very low precision
means the pattern could be used to filter out rather than
in.
3.2 Error analysis
The false positive errors of the sentence filter-
ing step, using hand-written patterns can be clas-
sified as follows: 1) omission: Characteriza-
tion of abn2 (yxiA), encoding a Bacillus subtilis
GH43 arabinanase, Abn2, and its role in arabino-
126
before filtering after nomenclature after SVM final evaluation0
10
20
30
40
50
60
70
80
90
100
F-m
eas
ure
 (%
)
4.1
53.5
71.7 69.2
4.1
53.5
80.8 78.3
64.4
73.1
DevDev with patternsTestTest with patterns
Figure 2: Evolution of F-measure at different measure
points for the Rename task. Dev: training on train set
and testing on dev set. Test: training on train + dev sets
and testing on test set (no intermediary measure). 64.4%
is the official submitted score. 73.1% is the best score
achieved by the system on the test set.
polysaccharide degradation. (PMID 18408032). In
this case the sentence has been filtered out by the
SVM and then the couple abn2/yxia was not an-
notated as a renaming act, 2) incorrect informa-
tion in the nomenclature: These results substanti-
ate the view that sigE is the distal member of a
2-gene operon and demonstrate that the upstream
gene (spoIIGA) is necessary for sigma E forma-
tion. (PMID 2448286). Here, the integration of
the Genetic Map to the nomenclature has introduced
a wrong synonymy relation between spoIIGA and
sigE, 3) homology with another species: We report
the cloning of the wild-type allele of divIVB1 and
show that the mutation lies within a stretch of DNA
containing two open reading frames whose pre-
dicted products are in part homologous to the prod-
ucts of the Escherichia coli minicell genes minC and
minD. (PMID 1400224). The name pair actually
exists in the nomenclature but here, divIVB1 is a
gene of B. subtilis and minC is a gene of E. Coli,
4) another problem linked to the lexicon is the fact
the synonym classes are not disjoint. Some depre-
cated names of given genes are reused as canoni-
cal names of other genes. For example, purF and
purB referred to two different genes of B. subtilis
but purB was also formerly known as purF: The
following gene order has been established: pbuG-
purB-purF-purM-purH-purD-tre (PMID 3125411).
Hence, purF and purB are uncorrectly recognized
as synonyms while they refer to two different genes
in this context. Possible solutions for improving the
system could be: 1) the inclusion of species names
as SVM features, 2) the removal of some couples
from the nomenclature (PurF/purB for instance),
3) evaluate the benefits of each resource part of the
nomenclature.
4 Conclusion
Our system detects renaming acts of Bacillus sub-
tilis genes with a final F-measure of 64.4%. Af-
ter sentence segmentation, the emphasis is on sen-
tence filtering using an exhaustive nomenclature and
a SVM. An amelioration of this method using pat-
terns as features of the machine learning algorithm
was shown to improve significantly (+8.7%) the fi-
nal performance. It was also shown that the bag of
words representation is sub-optimal for text classi-
fication experiments (Fagan, 1987; Caropreso and
Matwin, 2006) With the use of such patterns, the fil-
tering step is now very efficient. The examination
of the remaining errors showed the limits of the cur-
rent shallow system. A deeper linguistic approach
using syntactic parsing seems indicated to improve
the filtering step further.
Acknowledgments
The authors would like to thank Julien Jourde for
granting them the permission to use the Bacteria
subtilis synonym nomenclature that he is currently
building and Philippe Veber for his insightful ad-
vices on text classification. This research is partly
founded by the French Oseo QUAERO project.
References
A. Bairoch and R. Apweiler. 1996. The SWISS-PROT
protein sequence data bank and its new supplement
TREMBL. Nucleic Acids Research, 24(1):21.
A. Bairoch, R. Apweiler, C.H. Wu, W.C. Barker,
B. Boeckmann, S. Ferro, E. Gasteiger, H. Huang,
R. Lopez, M. Magrane, and Others. 2005. The uni-
versal protein resource (UniProt). Nucleic Acids Re-
search, 33(suppl 1):D154.
127
D.A. Benson, I. Karsch-Mizrachi, D.J. Lipman, J. Ostell,
and D.L. Wheeler. 2008. GenBank. Nucleic acids
research, 36(suppl 1):D25.
M. Caropreso and S. Matwin. 2006. Beyond the Bag of
Words: A Text Representation for Sentence Selection.
Advances in Artificial Intelligence, pages 324?335.
J.L. Fagan. 1987. Experiments in automatic phrase in-
dexing for document retrieval: a comparison of syn-
tactic and non-syntactic methods.
LA Flo?rez, SF Roppel, A.G. Schmeisky, C.R. Lammers,
and J. Stu?lke. 2009. A community-curated consensual
annotation that is continuously updated: the Bacillus
subtilis centred wiki SubtiWiki. Database: The Jour-
nal of Biological Databases and Curation, 2009.
A Goelzer, B Brikci, I Martin-Verstraete, P Noirot,
P Bessie`res, S Aymerich, and V Fromion. 2008. Re-
construction and analysis of the genetic and metabolic
regulatory networks of the central metabolism of
Bacillus subtilis. BMC systems biology, 2(1):20.
M Hall, E Frank, G Holmes, B Pfahringer, P Reute-
mann, and I H Witten. 2009. The WEKA data min-
ing software: an update. ACM SIGKDD Explorations
Newsletter, 11(1):10?18.
Lynette Hirschman, Alexander Yeh, Christian Blaschke,
and Alfonso Valencia. 2005. Overview of BioCre-
AtIvE: critical assessment of information extraction
for biology. BMC bioinformatics, 6 Suppl 1:S1, Jan-
uary.
P. Lechat, L. Hummel, S. Rousseau, and I. Moszer. 2008.
GenoList: an integrated environment for comparative
analysis of microbial genomes. Nucleic Acids Re-
search, 36(suppl 1):D469.
C. Nedellec, M. Abdel Vetah, and Philippe Bessie`res.
2001. Sentence filtering for information extraction in
genomics, a classification problem. Principles of Data
Mining and Knowledge Discovery, pages 326?337.
C Ne?dellec, A Nazarenko, and R Bossy. 2009. Infor-
mation Extraction. Handbook on Ontologies, pages
663?685.
K E Nelson, I T Paulsen, J F Heidelberg, and C M
Fraser. 2000. Status of genome projects for non-
pathogenic bacteria and archaea. Nature biotechnol-
ogy, 18(10):1049?54, October.
A. Ogiwara, N. Ogasawara, M. Watanabe, and T. Tak-
agi. 1996. Construction of the Bacillus subtilis ORF
database (BSORF DB). Genome Informatics, pages
228?229.
H Schmid. 1994. Probabilistic part-of-speech tagging
using decision trees.
P. Sterk, P.J. Kersey, and R. Apweiler. 2006. Genome
reviews: standardizing content and representation of
information about complete genomes. Omics: a jour-
nal of integrative biology, 10(2):114?118.
Davy Weissenbacher. 2004. La relation de synonymie en
Ge?nomique. RECITAL.
J. Weston, S. Mukherjee, O Chapelle, M. Pontil, T. Pog-
gio, and V. Vapnik. 2001. Feature selection for SVMs.
Advances in neural information processing systems,
pages 668?674.
Hong Yu, Vasileios Hatzivassiloglou, Carol Friedman,
Andrey Rzhetsky, and W.J. Wilbur. 2002. Automatic
extraction of gene and protein synonyms from MED-
LINE and journal articles. In Proceedings of the AMIA
Symposium, page 919. American Medical Informatics
Association.
128
A Gene or operon couple matching pattern
Pattern that uses bacteria gene naming rules (3 lower
case + 1 upper case letters), short genes (3 lower
case letters), long gene names, factorized operons
(3 lower case + several upper case letters), gene
names including special and/or numerical characters
in presence or not of signal words such as named,
renamed, formerly, formally, here, herein, here-
after, now, previously, as, designated, termed and/or
called, only if the pattern does not begin with and
or orf. Although this pattern could be used to di-
rectly filter in sentences containing a renaming act,
its recall is too low thus it is used as a feature of the
classifier instead.
and|orf\
GENE|OPERON-fact\
[|((now|as|previously|formerly|formally|here(in|after))\
((re)named|called|designated|termed) (now|as|previously|formerly|formally|here(in|after))\
GENE|OPERON-fact)|]
Table 6: Long pattern used for gene pair matching.
Terms matched Pattern PMID
short-GENE (short-GENE) cotA (formerly pig) 8759849
long-GENE (long-GENE) cotSA (ytxN) 10234840
fact-OPERON (fact-OPERON) ntdABC (formally yhjLKJ) 14612444
spe-GENE (spe-GENE) lpa-8 (sfp) 10471562
GENE (GENE) cwlB [lytC] 8759849
GENE (now designated GENE) yfiA (now designated glvR) 11489864
GENE (previously GENE) nhaC (previously yheL) 11274110
GENE (formerly called GENE) bkdR (formerly called yqiR) 10094682
GENE (now termed GENE) yqgR (now termed glcK) 9620975
GENE (GENE) other forms fosB(yndN) 11244082
GENE (hereafter renamed GENE) yhdQ (hereafter renamed cueR) 14663075
GENE (herein renamed GENE) yqhN (herein renamed mntR) 10760146
GENE (formally GENE) ntdR (formally yhjM) 14612444
GENE (formerly GENE) mtnK (formerly ykrT) 11545674
GENE (renamed GENE) yfjS (renamed pdaA) 12374835
GENE (named GENE) yvcE (named cwlO) 16233686
GENE (GENE) pdaA (yfjS) 14679227
Table 7: Examples matched with the long pattern.
129
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 1?7,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Overview of BioNLP Shared Task 2013 
 Claire N?dellec MIG INRA UR1077  F-78352 Jouy-en-Josas cedex claire.nedellec@jouy.inra.fr  
Robert Bossy MIG INRA UR1077  F-78352 Jouy-en-Josas cedex robert.bossy@jouy.inra.fr   Jin-Dong Kim Database Center for Life Science  2-11-16 Yayoi, Bunkyo-ku, Tokyo  jdkim@dbcls.rois.ac.jp  
Jung-jae Kim Nanyang Technological University Singapore  jungjae.kim@ntu.edu.sg  Tomoko Ohta National Centre for Text Mining and School of Computer Science University of Manchester tomoko.ohta@manchester.ac.uk  
Sampo Pyysalo National Centre for Text Mining and School of Computer Science University of Manchester sampo.pyysalo@gmail.com    Pierre Zweigenbaum LIMSI-CNRS F-91403 Orsay  pz@limsi.fr 
 
    Abstract The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST hereafter) series is a community-wide effort toward fine-grained biomolecular event extraction, from scientific documents. BioNLP-ST 2013 follows the general outline and goals of the previous tasks, namely BioNLP-ST?09 (Kim  et al, 2009) and BioNLP-ST?11 (Kim et al, 
2011). BioNLP-ST aims to provide a common framework for the comparative evaluation of information extraction (IE) methods in the biomedical domain. It shares this common goal with other tasks, namely BioCreative (Critical Assessment of Information Extraction in Biology) (Arighi  et al, 2011), DDIExtraction (Extraction of Drug-Drug Interactions from biomedical texts) (Segura-Bedmar et al, 2011) and i2b2 (Informatics for Integrating Biology and the Bedside) Shared-Tasks (Sun et al, 2013).  The biological questions addressed by the BioNLP-ST series belong to the molecular biology domain and its related fields. With the three editions, the series gathers several groups that prepared various tasks and resources, which represent diverse themes in biology. As the two previous editions, this one measures the progress accomplished by the community on complex text-bound event extraction. Compared to the other initiatives, the BioNLP-ST series proposes a linguistically motivated approach to event representation that enables the evaluation of the participating methods in a unifying computer science framework. Each edition has attracted an 
1
increasing number of teams with 22 teams submitting 38 final results this year. The task setup and the data serve as a basis for numerous further studies, released event extraction systems, and published datasets.  The first event in 2009 triggered active research in the community on a specific fine-grained IE task called Genia event extraction  task. Expanding on this, the second BioNLP-ST was organized under the theme Generalization, where the participants introduced numerous systems that could be straightforwardly applied to different tasks. This time, the BioNLP-ST goes a step further and pursues the grand theme of Knowledge base construction. There were five tasks in 2011, and this year there are 6.  - [GE] Genia Event Extraction for NFkB knowledge base  - [CG] Cancer Genetics  - [PC] Pathway Curation  - [GRO] Corpus Annotation with Gene Regulation Ontology  - [GRN] Gene Regulation Network in Bacteria  - [BB] Bacteria Biotopes  The grand theme of Knowledge base construction is addressed in various ways: semantic web (GE, GRO), pathway (PC), molecular mechanism of cancer (CG), regulation network (GRN) and ontology population (GRO, BB).  In the biology domain, BioNLP-ST 2013 covers many new hot topics that reflect the evolving needs of biologists. BioNLP-ST 2013 broadens the scope of the text-mining application domains in biology by introducing new issues on cancer genetics and pathway curation. It also builds on the well-known previous datasets GENIA, LLL/BI and BB to propose tasks closer to the actual needs of biological data integration.  As in previous events, manually annotated data are provided to the participants for training, development and evaluation of the information extraction methods. According to their relevance for biological studies, the annotations are either bound to specific expressions in the text or represented as structured knowledge. Linguistic processing support was provided to the participants in the form of analyses of the dataset texts produced by state-of-the art tools. This paper summarizes the BioNLP-ST 2013 organization, the task characteristics and their relationships. It gives synthetic figures on the participants and discusses the participating system advances. 
2 Tasks The BioNLP-ST?13 includes six tasks from four groups: DBCLS, NaCTeM, NTU and INRA. As opposed to the last edition, all tasks were main extraction tasks. There were no supporting tasks designed to assist the extraction tasks. All tasks share the same event-based representation and file format, which is similar to the previous editions. This makes it easier to reuse the systems across tasks. Five kinds of annotation types are defined: ? T: text-bound annotation (entity/event trigger) ? Equiv: entity aliases ? E: event ? M: event modification ? R: relation ? N: normalization (external reference) The normalization type has been introduced this year to represent the references to external resources such as dictionaries for GRN or ontologies for GRO and BB. The annotations are stand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST?13 web site. 2.1     Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al, 2008) that represents domain knowledge of NF?B proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al, 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al, 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more ?real? task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the co-reference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the event extraction. 
2
2.2     Cancer Genetics (CG) The CG task concerns the extraction of events relevant to cancer, covering molecular foundations, cellular, tissue, and organ-level effects, and organism-level outcomes. In addition to the domain, the task is novel in particular in extending event extraction to upper levels of biological organization. The CG task involves the extraction of 40 event types involving 18 types of entities, defined with respect to community-standard ontologies (Pyysalo et al, 2011a; Ohta et al, 2012). The newly introduced CG task corpus, prepared as an extension of a previously introduced corpus of 250 abstracts (Pyysalo et al, 2012), consists of 600 PubMed abstracts annotated for over 17,000 events. 2.3     Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al, 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al, 2011b; Ohta et al, 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities.  2.4     Gene Regulation Ontology (GRO) The GRO task aims to populate the Gene Regulation Ontology (GRO) (Beisswanger et al, 2008) with events and relations identified from text. The large size and the complex semantic representation of the underlying ontology are the main challenges of the task. Those issues, to a greater extent, should be addressed to support full-fledged semantic search over the biomedical literature, which is the ultimate goal of this work.   The corpus consists of 300 MEDLINE abstracts, prepared as an extension of (Kim et al, 2011c). The analysis of the inter-annotator agreement between the two annotators shows 
Kappa values of 43%-56%, which might indicate the difficulty of the task.  2.5     Gene Regulation Network in Bacteria           (GRN) The Gene Regulation Network task consists of the extraction of the regulatory network of a set of genes involved in the sporulation phenomenon of the model organism Bacillus subtilis. Participant system predictions are evaluated with respect to the target regulation network, rather than the text-bound relations. The aim is to assess the IE methods with regards to the needs of systems biology and predictive biology studies. The GRN corpus is a set of sentences from PubMed abstracts that extends the BioNLP-ST 2011 BI (Jourde et al, 2011) and LLL (Nedellec, 2005) corpora. The additional sentences cover a wider range of publication dates and complement the regulation network of the sporulation phenomenon. It has been thoroughly annotated with different levels of biological abstraction: entities, biochemical events, genic interactions and the corresponding regulation network. The network prediction submissions have been evaluated against the reference network using an original metric, the Slot Error Rate (Makhoul et al, 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures.  2.6     Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats with concepts from OntoBiotope,1 a large ontology of 1,700 concepts and 2,000 synonyms. The association between bacteria and their habitats is essential information for environmental biology studies, metagenomics and phylogeny. In the previous edition of the BB task, participants had to recognize bacteria and habitat entities, to categorize habitat entities among eight broad types and to extract localization relations between bacteria and their habitats (Bossy et al, 2011). The BioNLP-ST 2013 edition has been split into 3 sub-tasks in order to better assess the performance of the predictive systems for each step. The novelty of this task is mainly the more comprehensive and fine-grained categorization. It addresses the critical problem of habitat normalization necessary for the                                                            1 http://bibliome.jouy.inra.fr/MEM-OntoBiotope 
3
automatic exploitation of bacteria-habitat databases.  2.7     Task characteristics Task features are given in Table 1. Three different types of text were considered: the abstracts of scientific papers taken from PubMed (CG, PC, GRO and GRN), full-text scientific papers (GE) and scientific web pages (BB).   Task Documents # types # events GE 34 Full papers  2 13 CG 600 Abstracts 18 40 PC 525 Abstracts 4 23 GRO 300 Abstracts 174  126 GRN 201 Abstracts 6 12 BB 124 Web pages 563 2 Table 1. Characteristics of the BioNLP-ST 2013 tasks. The number of relations or events targeted greatly varies with the tasks as shown in column 3. The high number of types and events reflect the increasing complexity of the biological knowledge to be extracted. The grand theme of Knowledge base construction in this edition has been translated into rich knowledge representations with the goal of integrating textual data with data from sources other than text. These figures illustrate the shared ambition of the organizers to promote fine-grained information extraction together with an increasing biological plausibility. Beyond gene and protein interactions, they include many complex biological phenomena and environmental factors. 3 BioNLP-ST?13 organization  BioNLP-ST?13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were 
communicated shortly after and published on the ST site. The descriptions of the tasks and representative sample data have been available since October 2012 so that the participants could become acquainted with the task goals and data formats in advance. Table 2 shows the task schedule.  Date Event 23 Oct. 2012 Release of sample data sets 17 Jan 2013 Release of the training data sets 06 Apr. 2013 Release of the test data sets 16 Apr. 2013 Result submission 17 Apr. 2013 Notification of the evaluation results Table 2: Schedule of BioNLP-ST 2013. The BioNLP-ST?13 web site and a dedicated mailing-list have kept the participant informed about the whole process.  4 Participation  GE 1-2-3 CG PC GRO GRN BB 1 - 2-3 EVEX ? ? ?    ?    TEES-2.1 ? ? ? ? ? ? ?  ? ? BioSEM ?          NCBI ?          DlutNLP ?          HDS 4NLP ?          NICTA  ?  ?        USheff ?          UZH  ?          HCMUS ?          NaCTeM     ? ?      NCBI     ?       RelAgent     ?       UET-NII     ?       ISI    ?       OSEE      ?     U. of Ljubljana       ?    K.U. Leuven       ?    IRISA-TexMex       ? ? ?  Boun        ? ?  LIPN        ?   LIMSI        ? ? ? Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTurku, submitted final results with TEES-2.1 to 
4
all the tasks except one ? entity categorization. This broad participation resulted from the growing capability of the systems to be applied to various tasks without manual tuning. The remaining 15 teams participated in one single task. 5 Results  Table 4 summarizes the best results and the participating systems for each task and sub-task. They are all measured using F-scores, except when it is not relevant, in which case SER is used instead. It is noticeable that the TEES-2.1 system that participated in 9 of the 10 tasks and sub-tasks achieved the best result in 6 cases. Most of the participating systems applied a combination of machine learning algorithms and linguistic features, mainly syntactic parses, with some noticeable exceptions.   Tasks Evaluation results  GE Core event extraction TEES-2.1, EVEX, BioSEM:  0.51 GE 2 Event enrichment TEES2.1:  0.32 GE 3 Negation/Speculation TEES-2.1, EVEX:   0.25 CG TEES-2.1:  0.55 PC NaCTeM:  0.53 
GRO TEES-2.1: 0.22 (events),   0.63 (relations) 
GRN U. of Ljubljana:   0.73 (SER) BB 1 Entity detection and categorization IRISA: 0.46 (SER) BB 2 Relation extraction IRISA:  0.40 BB 3 Full event extraction TEES-2.1:  0.14 Table 4. Best results and team per task  (F-score, except when SER). Twelve teams submitted final results to the GE task. The performance of highly ranked systems shows that the event extraction technology is applicable to the most recent full papers without drop of performance. Six teams submitted final results to the CG task. The highest-performing systems achieved 
results comparable to those for established molecular level extraction tasks (Kim et al, 2011). The results indicate that event extraction methods generalize well to higher levels of biological organization and are applicable to the construction of knowledge bases on cancer. Two teams successfully completed the PC task, and the highest F-score reached 52.8%, indicating that event extraction is a promising approach to support pathway curation efforts. The GRN task attracted five participants. The best SER score was 0,73 (the higher, the worse), which shows their capability of designing regulatory network, but handling modalities remains an issue. Five teams participated to the 3 BB subtasks with 10 final submissions. Not surprisingly, the systems achieved better results in relation extraction than habitat categorization, which remains a major challenge in IE. One team participated in the GRO task, and their results were compared with those of a preliminary system prepared by the task organizers. An analysis of the evaluation results leads us to study issues such as the need to consider the ontology structure and the need for semantic analysis, which are not seriously dealt with by current approaches to event extraction. 6 Organization of the workshop The BioNLP Shared Task 2013 (BioNLP-ST) workshop was organized as part of the ACL BioNLP 2013 workshop. After submission of their system results, participants were invited to submit a paper on their systems to the workshop.  Task organizers were also invited to present overviews of each task, with analyses of the participant system features and results. The workshop was held in August 2013 in Sofia (Bulgaria). It included overview presentations on tasks, as well as oral and poster presentations by Shared Task participants.  7 Discussion and Conclusion This year, the tasks has significantly gained in complexity to face the increasing need for Systems Biology knowledge from various textual sources. The high level of participation and the quality of the results show that the maturity of the field is such that it can meet this challenge. The innovative and various solutions applied this year will without doubt be extended in the future. As for previous editions of BioNLP-ST, all tasks maintain an online evaluation service that is 
5
publicly available. This on-going challenge will contribute to the assessment of the evolving information extraction field in the biomedical domain. References  Auhors. 2013. Title. In Proceedings of the BioNLP 2013 Workshop Companion Volume for Shared Task, Sofia, Bulgaria. Association for Computational Linguistics. Arighi, C., Lu, Z., Krallinger, M., Cohen, K., Wilbur, W., Valencia, A., Hirschman, L. and Wu, C. 2011. Overview of the BioCreative III Workshop. BMC Bioinformatics, 12, S1. E Beisswanger, V Lee, JJ Kim, D Rebholz-Schuhmann, A Splendiani, O Dameron, S Schulz, U Hahn. Gene Regulation Ontology (GRO): Design principles and use cases. Studies in Health Technology and Informatics, 136:9-14, 2008. BioNLP-ST?13 web site: https://2013.bionlp-st.org Robert Bossy, Julien Jourde, Philippe Bessi?res, Maarten van de Guchte, Claire N?dellec. 2011. BioNLP shared Tasks 2011 - Bacteria Biotope. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland, USA, 2011. Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 173?180. Association for Computational Linguistics. Julien Jourde, Alain-Pierre Manine, Philippe Veber, Karen Fort, Robert Bossy, Erick Alphonse, Philippe Bessi?res. 2011. BioNLP Shared Task 2011 - Bacteria Gene Interactions and Renaming. In Proceedings of BioNLP 2011 Workshop, pages 65-73. Association for Computational Linguistics, Portland. Jin-Dong Kim, Tomoko Ohta and Jun'ichi Tsujii, 2008, Corpus annotation for mining biomedical events from literature, BMC Bioinformatics, 9(1): 10. Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano and Jun'ichi Tsujii. 2009. Overview of BioNLP'09 Shared Task on Event Extraction. In Proceedings of the BioNLP 2009 Workshop Companion Volume for Shared Task, pages 1-9. Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, Robert Bossy, Ngan Nguyen and Jun?ichi Tsujii. 2011. Overview of BioNLP Shared Task 2011. In Proceedings of BioNLP 2011 Workshop, pages 1-6. Association for Computational Linguistics. 
Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Akinori Yonezawa. 2011b. Overview of the Genia Event task in BioNLP Shared Task 2011. In Proceedings of the BioNLP 2011 Workshop Companion Volume for Shared Task, Portland, Oregon, June. Association for Computational Linguistics. Jung-Jae Kim, Xu Han and Watson Wei Khong Chua. 2011c. Annotation of biomedical text with Gene Regulation Ontology: Towards Semantic Web for biomedical literature. Proceedings of LBM 2011, pp. 63-70. Dan Klein and Christopher D Manning. 2002. Fast ex act inference with a factored model for natural language parsing. Advances in neural information processing systems, 15(2003):3?10. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Yusuke Miyao and Jun?ichi Tsujii. 2008. Feature forest models for probabilistic HPSG parsing. Computational Linguistics, 34(1):35?80. Huaiyu Mi and Paul Thomas. 2009. PANTHER path- way: an ontology-based pathway database coupled with data analysis tools. In Protein Networks and Pathway Analysis, pages 123?140. Springer. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge. In Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and Nedellec C. (eds). Bonn, August. Tomoko Ohta, Sampo Pyysalo, Sophia Ananiadou, and Jun'ichi Tsujii. 2011b. Pathway curation support as an information extraction task. Proceedings of LBM 2011. Tomoko Ohta, Sampo Pyysalo, and Jun?ichi Tsujii. 2011c. From pathways to biomolecular events: opportunities and challenges. In Proceedings of BioNLP 2011 Workshop, pages 105?113. Association for Computational Linguistics. Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Open-domain anatomical entity mention detection. In Proceedings of DSSD 2012, pages 27?36. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol Cho, Jun?ichi Tsujii, and Sophia Ananiadou. 2012. Event extraction across multiple levels of biological organization. Bioinformatics, 28(18):i575-i581. Sampo Pyysalo, Tomoko Ohta, Makoto Miwa, and Jun'ichi Tsujii. 2011b. Towards exhaustive event extraction for protein modifications. In Proceedings of the BioNLP 2011 Workshop, 
6
pp.114-123, Association for Computational Linguistics. Sampo Pyysalo, Tomoko Ohta, Jun'ichi Tsujii and Sophia Ananiadou. 2011a. Anatomical Entity Recognition with Open Biomedical Ontologies. In proceedings of LBM 2011. Isabel Segura-Bedmar, Paloma Martinez, and Daniel Sanchez-Cisneros. 2011. The 1st DDIExtraction-2011 challenge task: Extraction of Drug-Drug Interactions from biomedical texts. In Proceedings of the 1st Challenge Task on Drug-Drug Interaction Extraction 2011, SEPLN 2011 satellite workshop. Huelva, Spain, September 7. Weiyi Sun, Anna Rumshisky, Ozlem Uzuner. 2013. Evaluating temporal relations in clinical text: 2012 i2b2 Challenge. J Am Med Inform Assoc.
7
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 153?160,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP Shared Task 2013 ? An overview of the Genic Regulation Network Task 
Robert Bossy, Philippe Bessi?res, Claire N?dellec  Unit? Math?matique, Informatique et G?nome Institut National de la Recherche Agronomique UR1077, F78352 Jouy-en-Josas, France forename.name@jouy.inra.fr  Abstract 
The goal of the Genic Regulation Network task (GRN) is to extract a regulation network that links and integrates a variety of molecular interactions between genes and proteins of the well-studied model bacterium Bacillus subtilis. It is an extension of the BI task of BioNLP-ST?11. The corpus is composed of sentences selected from publicly available PubMed scientific abstracts. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results. 1 Introduction  The Genic Regulation Network (GRN) task consists of (1) extracting information on molecular interactions between genes and proteins that are described in scientific literature, and (2) using this information to reconstruct a regulation network between molecular partners in a formal way. Several other types of biological networks can be defined at the molecular level, such as metabolisms, gene expressions, protein-protein interactions or signaling pathways.  All these networks are closely interconnected. For example, a gene codes for a protein that catalyzes the transformation of small molecules (metabolites), while the expression of the gene and its related regulation is controlled by other proteins. The concept of biological networks is not new. However, the development of new methods in molecular biology in the past twenty years has made them accessible at the level of an organism as a whole. These new methods allow for the design of large-scale experimental approaches with high throughput rates of data. They are then used to build static and dynamic models that represent the behavior of a cell in the field of Systems Biology (Kitano, 2002; de Jong, 2002). In this context, there has recently been a 
considerable focus on ?biological network inference?, that is to say the process of making inferences and predictions about these networks (D'haeseleer, et al, 2000). Therefore, it is expected that Information Extraction (IE) from scientific literature may play an important role in the domain, contributing to the construction of networks (Blaschke et al, 1999). IE also plays a role in the design and the validation of large-scale experiments, on the basis of detailed knowledge that has already been published. 2 Context Extracting molecular interactions from scientific literature is one of the most popular tasks in IE challenges applied to biology. The GRN task adds a supplementary level that is closer to the biological needs: the participant systems have to extract a regulation network from the text that links and integrates basic molecular interactions.  The GRN task is based on a series of previous challenges in IE that started with the LLL challenge in 2005 (N?dellec, 2005). The LLL corpus is a set of sentences of PubMed abstracts about molecular interactions of the model bacterium Bacillus subtilis. Originally, the LLL task defined a unique binary genic interaction relation between proteins and genes. Since then, it has evolved to include the description of interaction events in a fine-grained representation that includes the distinction between transcription, different types of regulations and binding events, as proposed by (Manine et al, 2009). This new schema better captures the complexity of regulations at the molecular level. Entities other than genes and proteins were introduced, such as DNA sites (e.g. transcription promoter sites, transcriptional regulator binding sites). We proposed the Genic Interaction task (Bossy et al, 2012) in the BioNLP?11 Shared Task with a full re-annotation of the LLL corpus that follows this schema. The GRN task in 
153
BioNLP-ST?13 builds on this corpus and includes annotation improvements and extensions that are detailed below. 3 Task description The BioNLP-ST 2013 GRN task consists of the automatic construction of the regulation network that can be derived from a set of sentences. As usual in relation extraction tasks, the GRN corpus includes text-bound annotations. However the extraction target is the network, which is a structure with a higher level of abstraction. GRN thus also provides an explicit procedure to derive a network from a set of text-bound annotations. The GRN annotation is stacked in four successive levels of annotation: 1. Text-bound entities represent genes, proteins and aggregates (families, complexes). Some entities directly relate to a gene and are given a unique gene identifier corresponding to a node of the network. These entities are hereby called genic named entities. 2. Biochemical events and relations are molecular-level events (e.g. transcription, binding) and detailed knowledge on relationships between entities (e.g. promoter of gene, regulon membership). 3. Interactions denote relations between entities and events and relations. Interactions are the first abstract annotations; they are the key to the construction of the network arcs. 4. Finally, the Genic Regulation Network is derived from the Interactions and from the identifiers of the named genic entities.  GerE is a DNA-binding protein that adheres    to the promoter of cotB and cotC    Figure 1. Example of annotated sentence.  Levels 1, 2 and 3 were obtained by a manual annotation of the GRN corpus sentences by a domain expert. Level 4 was automatically computed from the lower level annotations. The training corpus was provided to the participants with level 1, 2 and 3 annotations. The algorithm 
to compute the next level was described and implemented as a script and made available to the participants during the training stage of the challenge. The test corpus was provided with only  level 1 annotations (entities). The participants submitted their prediction either as a set of Interactions (level 3) or directly as a network (level 4). This setting allows the participants to train systems that work at different levels of abstraction. Submissions in the form of Interactions are translated into a Genic Regulation Network using the algorithm provided during the training stage. The evaluation of each submission is carried out by comparing the predicted network with the reference network. The reference network is itself computed from the gold level 1, 2 and 3 annotations of the test sentences. The following subsections describe the four annotation levels. The full annotation schema that specifies the constraints on event and relation arguments can be found on the task web page1. 3.1 Text-bound entity types Text-bound entities come in three kinds: event trigger words, genic entities and entity aggregates. Trigger words are of type Action, they serve as anchors for events. Genic entities represent mentions of biochemical objects of the bacteria cell. Genic entity types include Gene, mRNA, Promoter, Protein and Site. Finally aggregates denote composite objects of the bacteria cell. Aggregate types are: - GeneFamily: homologous gene families. - Operon: operons sensu prokaryotes. - PolymeraseComplex: RNA polymerase complexes, either the core complex alone, or bound to a sigma factor. - ProteinComplex: protein complexes formed by several proteins that bind together. - ProteinFamily: homologous protein families. - Regulon: regulons, sensu prokaryotes. 3.2 Biochemical events and relation types Biochemical events and relations represent the knowledge of cellular mechanisms at the molecular level. There are three types of events: - Transcription_by represents the transcription event by a specific RNA 
                                                       1 https://sites.google.com/site/bionlpst2013/tasks/gene-regulation-network 
Master of  Promoter Interaction 
Promoter of 
154
polymerase. Its agent is usually a PolymeraseComplex. - Transcription_from represents the transcription from a specific site or promoter. - Action_Target is a generic bio-molecular event. The relation types represent three major genetic regulation patterns in bacteria: promoter activation, regulons and binding to specific DNA sites. Two types of relations specifically denote mechanisms that involve promoters: - Promoter_of is a relation between a gene (or operon) and its promoter. - Master_of_Promoter relation represents the control of the transcription from a specific promoter by a proteic entity (Protein, ProteinComplex or ProteinFamily). Two other relation types represent the function of regulons: - Member_of_Regulon relation denotes the membership of a genic entity to a regulon. - Master_of_Regulon relation represents the control of the activity of an entire regulon by a protein. Finally two types are used to represent relations that are common to different regulation mechanisms: - Bind_to relation represents the binding of a proteic entity to a site on the chromosome. - Site_of relation denotes the belonging of a chromosomal site to a genic entity such as a gene or a promoter. 3.3 Interaction types Interaction relations are labeled with one of six types grouped into a small hierarchy following two axes: mechanism and effect. The hierarchical levels are figured here by the text indentations. Regulation Binding Transcription Activation Requirement Inhibition Figure 2. Types of Interaction relations 
The Binding and Transcription types specify the mechanism through which the agent regulates the target. In a Binding Interaction, the agent binds to the target; this includes Protein-DNA binding and excludes Protein-Protein binding mechanisms. In a Transcription Interaction, the agent affects the transcription of the target. The Activation, Requirement and Inhibition types specify the effect of the agent on the target. In an Activation Interaction, the agent increases the expression of the target. In a Requirement Interaction, the agent is necessary for the expression of the target. In an Inhibition Interaction, the agent reduces the expression of the target. The Regulation type is the default type: in such interactions, neither the mechanism nor the effect is specified. 3.4 Genic Regulation Network inference algorithm The genic regulation network corresponding to a corpus is inferred from the set of Interaction relations. The network presents itself as a directed labeled graph where nodes represent gene identifiers and edges represent gene interactions. The inference is done in two steps: the resolution of Interaction relations and the removal of redundant arcs. Step 1: Resolution of Interaction relations The agent and the target of an Interaction relation are not necessarily genic named entities. They can be secondary events or relations, another Interaction, or auxiliary entities (e.g. Promoter). The resolution of an Interaction aims to look for the genic named entity in order to infer the node concerned by the network edge. The resolution of Interaction arguments is performed using the rules specified below. These rules express well-known molecular mechanisms in a logical manner: 1. If the agent (or target) is a genic named entity, then the agent (or target) node is the gene identifier of the entity. If the entity does not have a gene identifier, then it is not a genic named entity and there is no node (and thus no edge). 2. If the agent (or target) is an event, then the agent (or target) node is the entity referenced by the event. 3. If the agent (or target) is a relation, then the agent (or target) of both arguments of the relation are nodes. 
Mechanism 
Effect 
155
4. If the target is a Promoter and this promoter is the argument of a Promoter_of relation, then the target node is the other argument of the Promoter_of relation. i.e. if A interacts with P, and P is a promoter of B, then A interacts with B. 5. If the agent is a Promoter and this promoter is the argument of a Master_of_Promoter relation, the agent is the other argument of the Master_of_Promoter relation. i.e. if A is the master of promoter P, and P interacts with B, then A interacts with B. The resolution of Interaction arguments consists of a traversal of the graph of annotations where these rules are applied iteratively. Event and relation arguments are walked through. Promoter entities are handled according to rules 4 and 5. If the resolution of the agent or the target yields more than one node, then the Interaction resolves to as many edges as the Cartesian product of the resolved nodes. For instance, if both the agent and the target resolve to two nodes, the Interaction relation resolves into four edges. Edges are labeled with the same set of types as the Interactions. Each edge inherits the type of the Interaction relation from which it has been inferred. Step 2: Removal of redundant arcs In this step, edges with the same agent, target and type are simplified into a single edge. This means that if the same Interaction is annotated several times in the corpus, then it will resolve into a single edge. This means that the prediction of only one of the interactions in the corpus is enough to reconstruct the edge. Moreover, Interaction types are ordered according to the hierarchy defined in the preceding section. Since the sentences are extracted from PubMed abstracts published during different periods, they may mention the same Interaction with different levels of detail, depending on the current state of knowledge. For a given edge, if there is another edge for the same node pair with a more specialized type, then it is removed. For instance, the edges (A, Regulation, B) and (A, Transcription, B) are simplified into (A, Transcription, B). Indeed the former edge conveys no additional information in comparison with the latter.  4 Corpus description The GRN corpus is a set of 201 sentences selected from PubMed abstracts, which are 
mainly about the sporulation phenomenon in Bacillus subtilis. This corpus is an extended version of the LLL and BI (BioNLP-ST?11) corpora. The additional sentences ensure a better coverage of the description of the sporulation. An expert of this phenomenon examined the regulation network derived from the annotation of the original sentences, and then manually listed the important interactions that were missing. We selected sentences from PubMed abstracts that contain occurrences of the missing pairs of genes. In this way, the genic interaction network is more complete with respect to the sporulation. Moreover, the publications from which the sentences are extracted cover a wider period, from 1996 to 2012. They represent a diverse range of writing styles and experimental methods. 42 sentences have been added, but 4 sentences were removed from the BI sentences because they described genic interactions in bacteria other than Bacillus subtilis. The distribution of the sentences among the training, development and test sets has been done in the following way: - Legacy sentences belong to the same set as in previous evaluation campaigns (LLL and BI). - Additional sentences have been randomly distributed to training, development and test sets. The random sampling has been constrained so that the proportion of different types of interactions is as much as possible the same as in the three sets. The GRN task does not include the automatic selection by the participant methods of the relevant sentences, which are provided. With regards to a real-world application, this selection step can be achieved with good performance by sentence filtering, as demonstrated by N?dellec et al (2001), by using a Naive Bayesian classifier. Moreover, the corpus contains sentences with no interaction. Tables 1 to 3 detail the distribution of the entities, relations and events in the corpus. They are balanced between the training and test sets: the test represents between a quarter and a third of the annotations. Table 1 details the entity frequency and their distributions by type. Column 5 contains the contribution of each entity type to the total. Genes and proteins represent two thirds of the entities, since they are the main actors in genic interactions. It is worth noting that the high number of promoters and polymerase complexes is specific to bacteria 
156
where the biological mechanisms are detailed at a molecular level.  Entity # Train+Dev Test Gene 199 70% 30% GeneFamily 2 50% 50% mRNA 1 100%  0% Operon 33 67% 33% PolymeraseComplex 62 71% 29% Promoter 63 73% 27% Protein 486 65% 35% ProteinComplex 7 100%  0% ProteinFamily 18 78% 22% Regulon 14 79% 21% Site 32 78% 22% Total 917 68% 32% 
Table 1. Entity distribution in the GRN corpus. Table 2 details the distribution of the biochemical events and relations (level 2). The most frequent event is Action Target. Action Target links, for instance, Transcription by and Transcription from events to the target gene.  Event/Relation # Train+dev Test Action target 226 68% 32% Bind to 9 78% 22% Master of Promoter 60 80% 20% Master of Regulon 13 85% 15% Member of Regulon 12 92% 8% Promoter of 47 72% 28% Site of 24 75% 25% Transcription by 86 71% 29% Transcription from 18 78% 22% 
Total 495 72% 28% Table 2. Distribution of the biochemical events and relations in the GRN corpus. Finally, Table 3 details the distribution of the Interaction relations (level 3). The distribution 
among Interaction relations is more uniform than among entities and molecular events. The frequency of the Transcription relation is much higher than Binding, which is not surprising since transcription is the major mechanism of regulation in bacteria, while binding is rare. Conversely, the relative frequency of relations among Effect types of relations is balanced.  Interaction  # Train+dev Test Regulation 80 65% 35% Inhibition 50 66% 34% Activation 49 67% 33% Requirement 35 66% 34% Binding 12 75% 25% Transcription 108 74% 26% Total 334 69% 31% Table 3. Distribution of the Interaction relations in the GRN corpus. 5 Annotation methodology A senior biologist, who is a specialist of Bacillus subtilis and a bioinformatician, a specialist of semantic annotation, defined the annotation schema. The biologist annotated the whole corpus, using the BI annotations as a starting point. The bioinformatician carefully checked each annotation. They both used the AlvisAE Annotation Editor (Papazian et al, 2012) that supported their productivity due to its intuitive visualization of dense semantic annotations. Subtiwiki provided the identifiers of genes and proteins (Fl?rez et al, 2009). Subtiwiki is a community effort that has become the reference resource for the gene nomenclature normalization of Bacillus subtilis. Other genic named entities, like operons, families or protein complexes, were given an identifier similar to their surface form. Several annotation iterations and regular cross-validations allowed the annotators to refine and normalize these identifiers. The consistency of the annotations was checked by applying the rules of the network inference procedure that revealed contradictions or dangling events. The biologist double-checked the inferred network against his deep expertise of sporulation in Bacillus subtilis. 
157
6 Evaluation procedure 6.1 Campaign organization The same rules and schedule were applied to GRN as the other BioNLP-ST tasks. The training and development data were provided eleven weeks before the test set. The submissions were gathered through an on-line service, which was active for ten days. We took into account the final run of each participant to compute the official scores. They were published on the BioNLP-ST web site together with the detailed scores. 6.2 Evaluation metrics The predictions of the participating teams were evaluated by comparing the reference network to the predicted network that was either submitted directly, or derived from the predicted Interactions. Since the genic named entity annotations are provided with their identifier, the network nodes are fixed. Therefore, the evaluation consists of comparing the edges of the two networks. Their discrepancy is measured using the Slot Error Rate (SER) defined by (Makhoul et al, 1999) as: SER = (S + D + I) / N where: - S is the number of substitutions (i.e. edges predicted with the wrong type) - D is the number of deletions (false negatives) - I is the number of insertions (false positives) - N is the number of arcs in the reference network. The SER has the advantage over F1, namely it uses an explicit characterization of the substitutions. (Makhoul et al, 1999) demonstrates that the implicit comprehension of substitutions in both recall and precision scores leads to the underestimation of deletions and insertions in the F score. However, we compute the Recall, Precision and F1 in order to make the interpretation of results easier: Recall = M / N Precision = M / P where: - M is the number of matches (true positives). - P is the number of edges in the predicted network. Matches, substitutions, deletions and insertions are counted for each pair of nodes. The genic regulation network is an oriented graph, thus the 
node pairs (A,B) and (B,A) are handled independently. For a given node pair (A,B), the number of exact matches (M) is the number of edges with the same type in the prediction as in the reference. The number of substitutions, deletions and insertions depends on the number of remaining edges. We name q and r, the number of remaining edges between two nodes A and B in the prediction and the reference respectively: - S = min(q, r) - if q > r, then I = q ? r, D = 0 - if q < r, then I = 0, D = r ? q In other words, edges from the prediction and the reference are paired, first by counting matches, then by maximizing substitutions. The remaining edges are counted either as insertions or deletions depending if the extra edges are in the prediction or reference, respectively. The values of S, D, I and M for the whole network are the sum of S, D, I and M on all the node pairs. 7 Results 7.1 Participating systems Five systems participated in GRN: - University of Ljubljana (Slovenia) (?itnik et al, 2013),  - K.U.Leuven (Belgium) (Provoost and Moens, 2013),  - IRISA-TexMex (INRIA, France) (Claveau, 2013), - EVEX (U. of Turku / TUCS, Finland and VIB / U. of Ghent, Belgium) (Hakala et al, 2013),  - TEES-2.1 (TUCS, Finland) (Bj?rne and Salakoski, 2013). 
 Participant SER Recall Precision U. of Ljubljana  0.73 34% 68% K.U.Leuven  0.83 23% 50% TEES-2.1  0.86 23% 54% IRISA-TexMex  0.91 41% 40% EVEX  0.92 13% 44% Table 4. Final evaluation of the GRN task. Teams are ranked by SER. S: Substitutions, D: Deletions, I: Insertions, M: Matches. 
158
Table 4 summarizes the scores by decreasing order. The scores are distributed between the best SER, 0.73 achieved by the University of Ljubljana, 20 points more than the lowest at 0.92. For all systems, the number of insertions is much lower than the number of deletions, except for IRISA-TexMex. The substitutions correspond to the edges that were predicted with the wrong type. In order to reveal the quality of the predictions with regards to the edge types, we calculated two alternate SERs. The results are displayed in Table 5.The SER Network Shape is obtained by erasing the type of all of the edges in the reference and predicted networks, as if all edges were of the Regulation type. The SER Network Shape measures the capacity of the systems to reconstruct the unlabeled shape of the regulation network. The SER Effect is obtained by erasing the mechanism types of all edges only, as if Binding and Transcription edges were of type Regulation. The Effect edges are kept unchanged. The SER Effect measures the quality of the predictions for valued networks that only contain Effect edges.  Participant SER SER Shape SER Effect U. of Ljubljana 0.73 0.60 0.74 
K.U. Leuven 0.83 0.64 0.83 
TEES-2.1 0.86 0.74 0.84 
IRISA-TexMex 0.91 0.51 0.87 
EVEX 0.92 0.79 0.91 
Table 5. Scores obtained by erasing edge types (Network Shape) or mechanism types (Effect). The SER Network Shape is significantly better for all systems, but the impact is dramatic for IRISA-TexMex and K.U. Leuven, showing that the typing of relations may be the major source of error. The SER Effect does not differ significantly from the original score. We deduce from the comparison of the three scores that the types that are the hardest to discriminate are effect types. This result is interesting because Effect labels are in fact the most valuable for systems biology and network inference studies. U. of Ljubljana and TEES-2.1 submissions contained level 2 and 3 predictions (interactions and biochemical events). IRISA provided only 
predictions at level 3 (interactions only). K.U. Leuven and EVEX directly submitted a network. The performance of the systems that use annotations of level 2 confirms our hypothesis that a significant part of the interactions can be deduced from low-level events. 7.2     Systems description and result analysis All systems applied machine-learning algorithms with linguistic features that were stems or lemmas, POS-tags and parses, most of them being provided by the BioNLP supporting resources. With the exception of K.U. Leuven, all systems used dependency paths between candidate arguments. However different ML algorithms were used, as shown in Table 6.  Participant ML algorithm U. Ljubljana Linear-chain CRF K.U.Leuven SVM (Gaussian RBF) TEES-2.1 SVMmulticlass (linear) IRISA-TexMex kNN (language model) EVEX SVM (TEES-2.1) Table 6. ML algorithms used by the participants. Beyond syntactic parses and ML algorithms, the participant systems combined many different sources of information and processing, so that no definitive conclusion on the respective potential of the methods can be drawn here. 8 Conclusion The GRN task has a strong legacy since the corpus is derived from LLL. Moreover, the GRN task has advanced a novel IE setting. We proposed to extract a formal data structure from successive abstract layers. Five different teams participated in the task with distinct strategies. In particular, we received submissions that work on all proposed abstraction levels. This shows that Information Extraction implementations have reached a state of maturity, which allow for new problems to be addressed quickly. The performances are promising, yet some specific problems have to be addressed, like the labeling of edges. Acknowledgments This work was partially supported by the Quaero programme funded by OSEO (the French agency for innovation). 
159
References Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Christian Blaschke, Miguel A. Andrade, Christos Ouzounis, Alfonso Valencia. 1999. Automatic Extraction of Biological Information From Scientific Text: Protein-Protein Interactions. Proceedings of the International Conference on Intelligent Systems for Molecular Biology (ISMB 1999), 60-67.  Robert Bossy, Julien Jourde, Alain-Pierre Manine, Philippe Veber, Erick Alphonse, Marteen van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics. 13(Suppl 11):S3. Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Patrik D'haeseleer, Shoudan Liang, Roland Somogyi. 2000. Genetic network inference: from co-expression clustering to reverse engineering. Bioinformatics. 16(8):707-726. Lope A. Fl?rez, Sebastian F Roppel, Arne G Schmeisky, Christoph R Lammers, J?rg St?lke. 2009. A community-curated consensual annotation that is continuously updated: the Bacillus subtilis centred wiki SubtiWiki. Database (Oxford), 2009:bap012. Kai Hakala, Sofie Van Landeghem, Tapio Salakoski, Yves Van de Peer and Filip Ginter. 2013. EVEX in ST?13: Application of a large-scale text mining resource to event extraction and network construction. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. 
GenBank. http://www.ncbi.nlm.nih.gov/  Hidde de Jong. 2002. Modeling and simulation of genetic regulatory systems: a literature review. J. Computational Biology, 9(1):67-103. Hiroaki Kitano. 2002. Computational systems biology. Nature, 420(6912):206-210. John Makhoul, Francis Kubala, Richard Schwartz and Ralph Weischedel. 1999.  Performance measures for information extraction. In Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February. Alain-Pierre Manine, Erick Alphonse, Philippe Bessi?res. 2009. Learning ontological rules to extract multiple relations of genic interactions from text. Int. J. Medical Informatics, 78(12):31?38. Claire N?dellec, Mohamed Ould Abdel Vetah, Philippe Bessi?res. 2001. Sentence filtering for information extraction in genomics, a classification problem. Practice of Knowledge Discovery in Databases (PKDD 2001), 326-337. Claire N?dellec. 2005. Learning Language in Logic - Genic Interaction Extraction Challenge" in Proceedings of the Learning Language in Logic (LLL05) workshop joint to ICML'05. Cussens J. and N?dellec C. (eds). Bonn. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Thomas Provoost, Marie-Francine Moens. 2013. Detecting Relations in the Gene Regulation Network. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics. Slavko ?itnik, Marinka ?itnik, Bla? Zupan, Marko Bajec. 2013. Extracting Gene Regulation Networks Using Linear-Chain Conditional Random Fields and Rules. In Proceedings of the BioNLP 2013 Workshop, Association for Computational Linguistics.  
160
Proceedings of the BioNLP Shared Task 2013 Workshop, pages 161?169,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
BioNLP shared Task 2013 ? An Overview of the  Bacteria Biotope Task 
Robert Bossy1, Wiktoria Golik1, Zorana Ratkovic1,2, Philippe Bessi?res1, Claire N?dellec1  1Unit? Math?matique, Informatique et G?nome MIG INRA UR1077 ? F-78352 Jouy-en-Josas ? France 2LaTTiCe UMR 8094 CNRS, 1 rue Maurice Arnoux, F-92120 Montrouge ? France forename.name@jouy.inra.fr  Abstract 
This paper presents the Bacteria Biotope task of the BioNLP Shared Task 2013, which follows BioNLP-ST-11. The Bacteria Biotope task aims to extract the location of bacteria from scientific web pages and to characterize these locations with respect to the OntoBiotope ontology. Bacteria locations are crucial knowledge in biology for phenotype studies. The paper details the corpus specifications, the evaluation metrics, and it summarizes and discusses the participant results.  1 Introduction The Bacteria Biotope (BB) task extends the BioNLP 2013 Shared Task molecular biology scope. It consists of extracting bacteria and their locations from web pages, and categorizing the locations with respect to the OntoBiotope1 ontology of microbe habitats. The locations denote the places where given species live. The bacteria habitat information is critical for the study of the interaction between the species and their environment, and for a better understanding of the underlying biological mechanisms at a molecular level. The information on bacteria biotopes and their properties is very abundant in scientific literature and in genomic databases and BRC (Biology Resource Center) catalogues. However, the information is highly diverse and expressed in natural language (Bossy et al, 2012). The two critical missing steps for population of biology databases and biotope knowledge modeling are (1) the automatic extraction of organism/location pairs and (2) the normalization of the habitat names with respect to biotope ontologies.                                                         1http://bibliome.jouy.inra.fr/MEM-OntoBiotope/OntoBiotope_BioNLP-ST13.obo 
The aim of the previous edition of the BB task (BioNLP-ST?11) was to solve the first information extraction step. The results obtained by the participant systems reached 45 percent F-measure. These results showed both the feasibility of the task, as well as a large room for improvement (Bossy et al, 2012).  The 2013 edition of the BB task maintains the primary objective of event extraction, and introduces the second issue of biotope normalization. It is handled through the categorization of the locations into a large set of types defined in the OntoBiotope ontology. Bacteria locations range from hosts, plant and animals, to natural environments (e.g. water, soil), including industrial environments.  BB?11 set of categories contained 7 types. This year, entity categorization has been enriched to better answer the biological needs, as well as to contribute to the general problem of automatic semantic annotation by ontologies. BB task is divided into three sub-tasks. Entity detection and event extraction are tackled by two distinct sub-tasks, so that the contribution of each method could be assessed. A third sub-task conjugates the two in order to measure the impact of the method interactions. 2 Context Biological motivation. Today, new sequencing methods allow biologists to study complex environments such as microbial ecosystems. Therefore, the sequence annotation process is facing radical changes with respect to the volume of data and the nature of the annotations to be considered. Not only do biochemical functions still need to be assigned to newly identified genes, but biologists have to take into account the conditions and the properties of the ecosystems in which microorganisms are living and are identified, as well as the interactions and relationships developed with their environment and other 
161
living organisms (Korbel et al, 2005). Metagenomic studies of ecosystems yield important information on the phylogenetic composition of the microbiota. The availability of bacteria biotope information represented in a formal language would then pave the way for many new environment-aware bioinformatic services. The development of methods that are able to extract and normalize natural language information at a large scale would allow us to rapidly obtain and summarize information that the bacterial species or genera are associated with in the literature. In turn, this will allow for the formulation of hypotheses regarding properties of the bacteria, the ecosystem, and the links between them.  The pioneering work on EnvDB (Pignatelli et al, 2009) aimed to link GenBank sequences of microbes to biotope mentions in scientific papers. However, EnvDB was affected by the incompleteness of the GenBank isolation source field, the low number of related bibliographic references, the bag-of-words extraction method and the small size of its habitat classification. Habitat categories. The most developed classifications of habitats are EnvO, the Metagenome classification supported by the Genomics Standards Consortium (GSC), and the OntoBiotope ontology developed by our group. EnvO (Environment Ontology project) targets a Minimum Information about a Genome Sequence (MIGS) specification (Field et al, 2008) of mainly Eukaryotes. This ambitious detailed environment ontology aims to support standard manual annotations  of all types of organism environments and biological samples. However, it suffers from some limitations for bacterial biotope descriptions. A large part of EnvO is devoted to environmental biotopes and extreme habitats, whilst it fails to finely account for the main trends in bacteria studies, such as their technological use for food transformation and bioremediation, and their pathogenic or symbiotic properties. Moreover, EnvO terms are often poorly suited for bacteria literature analysis (Ratkovic et al, 2012). The Metagenome Classification  from JGI of DOE (Joint Genome Institute, US Department Of Energy) is intended to classify metagenome projects and samples according to a mixed typology of habitats (e.g. environmental, host) and their physico-chemical properties (e.g. pH, salinity) (Ivanova et al, 2010). It is a valuable 
source of vocabulary for the analysis of bacteria literature, but its structure and scope are strongly biased by the indexing of metagenome projects. The OntoBiotope ontology is appropriate for the categorization of bacteria biotopes in the BB task because its scope and its organization reflect the scientific subject division and the microbial diversity. Its size (1,756 concepts) and its deep hierarchical structure are suitable for a fine-grained normalization of the habitats. Its vocabulary has been selected after a thorough terminological analysis of relevant scientific documents, papers, GOLD (Chen et al, 2010) and GenBank, which was partly automated by term extraction. Related terms are attached to the OntoBiotope concept labels (i.e. 383 synonyms), improving OntoBiotope coverage of natural language documents.  Its structure and a part of its vocabulary have been inspired by EnvO, the Metagenome classification and the small ATCC (American Type Collection Culture) classification for microbial collections (Floyd et al, 2005). Explicit references to 34 EnvO terms are given in the OntoBiotope file. Its main topics are: - ? Artificial ? environments (industrial and domestic), Agricultural habitats, Aquaculture habitats, Processed food; - Medical environments, Living organisms, Parts of living organisms, Bacteria-associated habitats; - ? Natural ? environment habitats, Habitats wrt physico-chemical property (including extreme ones); - Experimental medium (i.e. experimental biotopes designed for studying bacteria). The structure, the comprehensiveness and the detail of the habitat classification are critical factors for research in biology. Biological investigations involving the habitats of bacteria are very diverse and still unanticipated. Thus, shallow and light classifications are insufficient to tackle the full extent of the biological questions. Indexing genomic data with a hierarchical fine-grained ontology such as OntoBiotope allows us to obtain aggregated and adjusted information by selecting the right level or axis of abstraction. Bacteria Biotope Task.  The corpus is the same as BB?11. The documents are scientific web pages intended for a general audience in the form of encyclopedia notices. They focus on a single organism or a family. The habitat mentions are dense and more diverse than 
162
in PubMed abstracts. These features make the task both useful and feasible with a reduced investment in biology. Its linguistic characteristics, high frequency of anaphora, entities denoted by complex nominal expressions raised interesting question for BioNLP that have been treated for a long time in the general and the biomedical domains. 3 Task description The BB Task is split into two secondary goals: 1. The detection of entities and their categorization(s) (Sub-task 1). 2. The extraction of Localization relations given the entities (sub-task 2) Sub-task 1 involves the prediction of habitat entities and their position in the text. The participant also has to assign each entity to one or more concepts of the OntoBiotope ontology: the categorization task. For instance, in the excerpt Isolated from the water of abalone farm, the entity abalone farm should be assigned the OntoBiotope category fish farm. Sub-task 2 is a relation extraction task. The schema of this task contains three types of entities: - The Habitat type is the same as in sub-task 1. - Geographical entities represent location and organization named entities. - Bacteria entities are bacterial taxa. Additionally, there are two types of relations illustrated by Figure 1. - Localization relations link Bacteria to the place where they live (either a Habitat or a Geographical). - PartOf relations relate couples of Habitat entities, a living organism, which is a host (e.g. adult human), and a part of this living organism (e.g. gut).  Bifidobacterium longum. This organism is 
found in adult humans and formula fed infants 
as a normal component of gut flora. 
Figure 1. Example of a localization event in the BB Task. Sub-task 2 participants are provided with document texts and entities, and should predict the relations between the candidate entities. 
Sub-task 3 is the combination of these two sub-tasks. It consists of predicting both the entity positions and the relations between entities. Compared to sub-task 1, the systems have to predict Habitat entities, but also Geographical and Bacteria entities. It is similar to the BB task of BioNLP-ST?11, except that no categorization of the entities is required. 4 Corpus description The BB corpus document sources are web pages from bacteria sequencing projects, (EBI, NCBI, JGI, Genoscope) and encyclopedia pages from MicrobeWiki. The documents are publicly available. Table 1 gives the distribution of the entities and relations in the corpora per sub-task.   Training + Dev Test 1 & 3 Test 2 Document 78 27 26 Word 25,828 7,670 10,353     Bacteria 1,347 332 541 Geographical 168 38 82 Habitat 1,545 507 623 OntoBiotope cat. 1,575 522 NA Total entities 3,060 877 1,246     Localization 1,030 269 538 Part of Host 235 111 129 Total relations 1,265 328 667 Table 1. BB?13 corpus figures. The categorization of entities by a large ontology (sub-task 1) offers a novel task to the BioNLP-ST community; a close examination of the annotated corpus allowed us to anticipate the challenges for participating teams. A total of 2,052 entities have been manually annotated for sub-task 1 (training, development and test sets together). These entities have 1,036 distinct surface forms, which means that an entity surface form is repeated a little less than twice, on average. However, only a quarter of the surface forms are actually repeated; three quarters are unique in the corpus. Moreover, 60% of habitat entities have a surface form that does not match one of the synonyms of their ontology concept. This configuration suggests that methods that simply propagate surface forms and concept attributions from ontology synonyms and from training entities would be inefficient. We have developed a baseline prediction that projects the ontology synonyms and the training corpus 
Localization Localization 
Part of Part of 
163
habitat surface forms onto the test. This prediction scores a high Slot Error Rate of 0.74. We also note there are a few ambiguous forms (i.e. 112 forms) that are synonyms in several different concepts or that do not always denote a habitat, and a few entities are assigned more than one concept (i.e. 42 of them). These are difficult cases that require prediction methods capable of word sense disambiguation. The low number of ambiguous occurrences has a low impact on the participant scores, although their presence may motivate more sophisticated methods. 5 Annotation methodology The methodology of entity position and relations annotation is similar to BB Task?11. It involved seven scientists who participated in a double-blind annotation (each document was annotated twice), followed by a conflict resolution phase. They used the AlvisAE annotation editor (Papazian et al, 2012). The guidelines included some improvements that are detailed below. Boundaries. Habitat entities may be either names or adjective. In the case of adjectives, the head is included in the entity span if it denotes a location (e.g. intestinal sample) and is excluded otherwise (e.g. hospital epidemic). The entity spans may be discontinuous, which is relevant for overlapping entities like ground water and surface water in ground and surface water. The major change is the inclusion of all modifiers that describe the location in the habitat entity span. This makes the entity more informative and the entity boundaries easier to predict, and less subject to debate. For instance, in the example,  isolated from the water of an abalone farm,  the water entity extends from water to farm. Note that in sub-task 1, all entities have to be predicted, even when not involved in a relation. This led to the annotation of embedded entities as potential habitats for bacteria, such as abalone farm and abalone in the above example.  Equivalent sets of entities.  As in BB?11, there are many equivalent mentions of the same bacteria in the documents that play a similar role with respect to the Localization relation. Selecting only one of them as the gold reference would have been arbitrary. When this is the case, the reference annotation includes equivalent sets of entities that convey the same information (e.g. Borrelia garinii vs. B. garinii, but not Borrelia).  
Category assignment. The assignment of categories to habitat entities has been done in two steps: (i) an automatic pre-annotation by the method of Ratkovic et al, (2012) and (ii) a manual double-blind revision followed by a conflict resolution phase. In the manual annotation phase, the most frequent conflicts between annotators were the same as in the previous edition. They involved the assignment of entities to either the living organism category, organic matter or food. An example is the cane entity in cane cuttings. To handle these cases, the guidelines assert that a dead organism cannot be assigned to a living organism category. The high quality of the pre-annotation and its visualization and revision using the AlvisAE annotation editor notably sped-up the annotation process. Table 2 summarizes the figures of the pre-annotation. For sub-task 1, the pre-annotation consisted of assigning OntoBiotope categories to entities for the whole corpus (train+dev+test). The pre-annotation yielded very high results with an F-measure of almost 90%. The pre-annotation was also useful to assess the relevance of the OntoBiotope ontology for the BB task. For sub-task 2, the pre-annotation consisted of the detection of entities in the test set, where no categorization is needed. The second line in Table 2 shows that the recall of entity detection affects the F-score, but that it still made the prediction helpful for the annotators. Further data analysis revealed that the terminology-based approach of the pre-annotation poorly detected the correct boundaries of embedded entities, thereby decreasing the recall of the entity recognition.   Recall Precision F1 Corpus sub-task1 89.7% 90.1% 89.9% Test sub-task 2 47.3% 95.7% 63.3% Table 2. Pre-annotation scores. 6 Evaluation procedure The evaluation procedure was similar to the previous edition in terms of resources, schedule and metrics except that an original relevant metric was developed for the new problem of entity categorization in a hierarchy.  6.1 Campaign organization The training and development corpora with the reference annotations were made available to the participants eleven weeks before the release of 
164
the test sets. Participating teams then had ten days to submit their predictions. As with all BioNLP-ST tasks, each participant submitted a single final prediction for each BB sub-task. The detailed evaluation results were computed, provided to the participants and published on the BioNLP website two days after the submission deadline.  6.2 Evaluation metrics Sub-task 1. In this sub-task participants were given only the document texts. They had to predict habitat entities along with their categorization with the OntoBiotope ontology. The evaluation of sub-task 1 takes into account the accuracy of the boundaries of the predicted entities as well as of the ontology category. Entity pairing. The evaluation algorithm performs an optimal pairwise matching between the habitat entities in the reference and the predicted entities. We defined a similarity between two entities that takes into account the boundaries and the categorization. Each reference entity is paired with the predicted entity for which the similarity is the highest among non-zero similarities.  If the boundaries of a reference entity do not overlap with any predicted entity, then it is a false negative, or a deletion. Conversely, if the boundaries of a predicted entity do not overlap with any reference entity, then it is a false positive, or an insertion. If the similarity between the entities is 1, then it is a perfect match. But if the similarity is lower than 1, then it is a substitution.  Entity similarity. The similarity M between two entities is defined as: M = J . W J measures the accuracy of the boundaries between the reference and the predicted entities. It is defined as a Jaccard Index adapted to segments (Bossy et al, 2012). For a pair of entities with the exact same boundaries, J equals to 1. W measures the accuracy between the ontology concept assignment of the reference entity and the predicted concept assignment of the predicted entity. We used the semantic similarity proposed by Wang, et al (2007). This similarity compares the set of all ancestors of the concept assigned to the reference entity and the set of all ancestors of 
the concept assigned to the predicted entity. The similarity is the Jaccard Index between the two sets of ancestors; however, each ancestor is weighted with a factor equal to: dw where d is the number of steps between the attributed concept and the ancestor. w is a constant greater than zero and lower than or equal to 1. If both the reference and predicted entities are assigned the same concept, then the sets of ancestors are equal and W is equal to 1. If the pair of entities has different concept attributions, W is lower than 1 and depends on the relative depth of the lowest common ancestor. The lower the common ancestor is, the higher the value of W. The exponentiation by the w constant ensures that the weight of the ancestors decreases non-linearly. This similarity thus favors predictions in the vicinity of the reference concept. Note that since the ontology root is the ancestor of all concepts, W is always strictly greater than zero. (Wang et al, 2007) showed experimentally that a value of 0.8 for the w constant is optimal for clustering purposes. However we noticed that w high values tend to favor sibling predictions over ancestor/descendant predictions that are preferable here, whilst low w values do not penalize enough ontology root predictions. We settled w with a value of 0.65, which ensures that ancestor/descendant predictions always have a greater value than sibling predictions, while root predictions never yield a similarity greater than 0.5. As specified above, if the similarity M < 1, then the entity pair is a substitution. We define the importance of the substitution S as: S = 1 - M Prediction score. Most IE tasks measure the quality of a prediction with Precision and Recall, eventually merged into an F1. However the pairing detects false positives and false negatives, but also substitutions. In such cases, the Recall and Precision factor the substitutions twice, and thus underestimate false negatives and false positives. We therefore used the Slot Error Rate (SER) that has been devised to undertake this shortcoming (Makhoul et al, 1999): SER = (S + I + D) / N where: - S represents the number of substitutions. 
165
- I represents the total number of insertions. - D represents the total number of deletions. - N is the number of entities in the reference. The SER is a measure of errors, so the lower it is the better. A SER equal to zero means that the prediction is perfect. The SER is unbound, though a value greater than one means that there are more mistakes in the prediction than entities in the reference. We also computed the Recall, the Precision and F1 measures in order to facilitate the interpretation of results: Recall =M / N Precision = M / P where M is the sum of the similarity M for all pairs in the optimal pairing, N is the number of entities in the reference, and P the number of entities in the prediction. Sub-task 2. In sub-task 2, the participants had to predict relations between candidate arguments, which are Bacteria, Habitat and Geographical entities. This task can be viewed as a categorization task of all pairs of entities. Thus, we evaluate submissions with Recall, Precision and F1. Sub-task 3. Sub-task 3 is similar to sub-task 2, but it includes entity prediction. This is the same setting as the BB task in BioNLP-ST 2011, except for entity categorization. We used the same evaluation metrics based on Recall, Precision and F1 (Bossy et al, 2012). The highlights of this measure are: ? it is based on the pairing between reference and predicted relations that maximizes a similarity; ? the similarity of the boundaries of Habitat and Geographical entities is relaxed and defined as the Jaccard Index (in the same way as in sub-task 1); ? the boundaries of Bacteria is strict: the evaluation rejects all relations where the Bacteria has incorrect boundaries. 7 Results  7.1 Participating systems Five teams submitted ten predictions to the three BB sub-tasks. LIMSI (CNRS, France), see (Grouin, 2013) is the only team that submitted to the three sub-tasks. LIPN (U. Paris-Nord, France), (Bannour et al, 2013) only submitted to 
sub-task 1. TEES (TUCS, Finland), (Bj?rne and Salakoski, 2013) only submitted to sub-task 2. Finally, IRISA (INRIA, France), (Claveau, 2013))) and Boun (U. Bo?azi?i, Turkey), (Karadeniz and ?zg?r), submitted to sub-tasks 1 and 2. The scores of the submissions according to the official metrics are shown in decreasing rank order in Tables 3 to 6.  Participant Rank  SER  F1  IRISA 1  0.46 0.57  Boun 2  0.48 0.59  LIPN 3  0.49 0.61  LIMSI 4  0.66 0.44 Table 3. Scores for Sub-task 1 of the BB Task.   Participant Entity  detection Category  assignment  SER F1 SER  F1  IRISA  0.43 0.60  0.35 0.67  Boun  0.42 0.65  0.36 0.71  LIPN  0.46 0.64  0.38 0.72  LIMSI  0.45 0.71  0.66 0.50 Table 4. Detailed scores for Sub-task 1 of the BB Task. Participant systems to sub-task 1 obtained high scores despite the novelty of the task (0.46 SER for the 1st, IRISA). The results of the first three systems are very close despite the diversity of the methods. The decomposition of the scores of the predictions of entities with correct boundaries and their assignment to the right category are shown in Table 4. They are quite balanced with a slightly better rate for category assignment, with the exception of the LIMSI system, which is notably better in entity detection. This table also shows the dependency of the two entity detection and categorization steps. Errors in the entity boundaries affect the quality of categorization. Table 5 details the scores for sub-task 2. The prediction of location relations remains a difficult problem even with the entities being given. There are two reasons for this. First, there is high diversity of bacteria and locations. The many mentions of different bacteria and locations in the same paragraph make it a challenge to select the right pairing among candidate arguments. This is particularly true for the PartOf relation compared to the Localization relation (columns 5 and 6). All systems obtained 
166
a recall much lower than the precision, which may be interpreted training data overfitting.  Participant Rec. Prec.  F1  F1 PartOf F1 Loc.  TEES 2.1  0.28 0.82  0.42  0.22 0.49  IRISA  0.36 0.46  0.40  0.2 0.45  Boun  0.21 0.38  0.27  0.2 0.29  LIMSI  0.4 0.19  0.6  0.0 0.7 Table 5. Scores of Sub-task 2 for the BB Task. The second challenge is the high frequency of anaphora, especially with a bacteria antecedent. For BioNLP-ST 2011, we already pointed out that coreference resolution is critical in order to capture all relations that are not expressed inside a sentence. Participant Rec.  Prec.  F1   TEES 2.1 0.12 (0.41) 0.18 (0.61) 0.14  (0.49)  LIMSI 0.4 (0.9) 0.12 (0.82)   0.6  (0.15) Table 6. Scores of Sub-task 3 for the BB Task. (the relaxed scores are given in parentheses.) The results of sub-task 3 (Table 6) may appear disappointing compared to the first two sub-tasks and BB?11. Further analysis shows that the system scores were affected by their poor entity boundary detection and the PartOf relation predictions. In order to demonstrate this we computed a relaxed score that differs from the primary score by: - removing PartOf relations from the reference and the prediction; - accepting Localization relations even if the Bacteria entity boundaries  do not match; - removing the penalty for the incorrect boundaries of Habitat entities. This relaxed score is equivalent to ignoring PartOf relations and considering the boundaries of predicted entities as perfect. The result is exhibited in Table 6 between parentheses. The most determinant factor is the relaxation of Bacteria entity boundaries because errors are severely penalized. An error analysis of the submitted predictions revealed that more than half of the rejected Localization predictions had a Bacteria argument with incorrect boundaries.  7.2 Systems description and result analysis The participants deployed various assortments of methods ranging from linguistics and machine learning to hand-coded pattern-matching. Sub-
task 1 was handled in two successive steps, candidate entity detection and category assignment. Entity detection. The approaches combine  (1) the use of lexicons (IRISA and LIMSI), (2) then text analysis by chunking (IRISA), noun phrase analysis (Boun), term analysis by BioYaTeA (LIPN) and Cocoa entity detection (LIMSI),  (3) with additional rules (TextMarker by LIPN) or machine learning (CRF by LIMSI) for the adaptation to the corpus.  The LIMSI system combining Cocoa entity detection (BioNLP supporting resource) with CRF obtained the best result, 11 points over the less linguistics-based approach of IRISA as shown in Table 4.  Assignment of categories to entities. It was mainly realized using hand-coded rules (LIMSI, Boun), machine learning with Whisk (LIPN) or a similarity between ontology labels and the text entities (IRISA). It is interesting to note that although the approaches are very different, the three types of methods obtained close results ranging from 0.35 to 0.38 SER, apart one outlier. Prediction of relations. Sub-task 2 was completed by applying hand-coded rules (LIMSI, Boun), that were much less successful than the two machine-learning-based approaches, i.e. kNN by IRISA and multi-step SVM by TEES-2.1. In the case of TEES-2.1 attributes were generated by McCCJ parses, which may explain its success in the prediction of PartOf relations that is 20 point over the second method that did not use any parsing. Prediction of entities and relations. Sub-task 3 was completed by LIMSI using the successive application of its methods from sub-tasks 1 and 2. TEES-2.1 applied its multi-step SVM classification of sub-task 2 for relation prediction completed by additional SVM steps for candidate entity detection. These experiments allow for the comparison of very different state-of-the-art methods, resources and integration strategies. However the tight gap between the scores of the different systems prevents us from drawing a definitive conclusion. Additional criteria other than scores may also be taken into account: the simplicity of deployment, the ease of adaptation to new 
167
domains, the availability of relevant resources and the potential for improvement. 8 Conclusion After BioNLP-ST?11, the second edition of the Bacteria Biotope Task provides a wealth of new information on the generalization of the entity categorization methods to a large set of categories. The final submissions of the 5 teams show very promising results with a broad variety of methods. The introduction of new metrics appeared appropriate to reveal the quality of the results and to highlight relevant contrasts. The prediction of events still remains challenging in documents where the candidate arguments are very dense, and where most relations involve several sentences. A thorough analysis of the results indicates clear directions for improvement.  Acknowledgments This work has been partially supported by the Quaero program, funded by OSEO, the French state agency for innovation and the INRA OntoBiotope Network. References Sondes Bannour, Laurent Audibert, Henry Soldano. 2013. Ontology-based semantic annotation: an automatic hybrid rule-based method. Present volume. Jari Bj?rne, Tapio Salakoski. 2013. TEES 2.1: Automated Annotation Scheme Learning in the BioNLP 2013 Shared Task. Present volume. Robert Bossy, Julien Jourde, Alain-Pierre Manine A., Philippe Veber, Erick Alphonse, Maarten van de Guchte, Philippe Bessi?res, Claire N?dellec. 2012. BioNLP Shared Task - The Bacteria Track. BMC Bioinformatics 13(Suppl 11):S3, June .  Vincent Claveau. 2013. IRISA participation to BioNLP-ST 2013: lazy-learning and information retrieval for information extraction tasks. Present volume. Liolios K., Chen I.M., Mavromatis K., Tavernarakis N., Hugenholtz P., Markowitz V.M., Kyrpides N.C. (2010). The Genomes On Line Database (GOLD) in 2009: status of genomic and metagenomic projects and their associated metadata. Nucleic Acids Res., 38(Database issue):D346-54. EnvDB database. http://metagenomics.uv.es/envDB/ EnvO Project. http://environmentontology.org 
Dawn Field et al 2008. Towards a richer description of our complete collection of genomes and metagenomes: the ?Minimum Information about a Genome Sequence? (MIGS) specification. Nature Biotechnology. 26: 541-547. Cyril Grouin. 2013. Building A Contrasting Taxa Extractor for Relation Identification from Assertions: BIOlogical Taxonomy & Ontology Phrase Extraction System. Present volume. ?lknur Karadeniz, Arzucan ?zg?r. 2013. Bacteria Biotope Detection, Ontology-based Normalization, and Relation Extraction using Syntactic Rules. Present volume. Korbel J.O., Doerks T., Jensen L.J., Perez-Iratxeta C., Kaczanowski S., Hooper S.D., Andrade M.A., Bork P. (2005). Systematic association of genes to phenotypes by genome and literature mining. PLoS Biol., 3(5):e134. Melissa M. Floyd, Jane Tang, Matthew Kane and David Emerson. 2005. Captured Diversity in a Culture Collection: Case Study of the Geographic and Habitat Distributions of Environmental Isolates Held at the American Type Culture Collection. Applied and Environmental Microbiology. 71(6):2813-23. GenBank. http://www.ncbi.nlm.nih.gov/  GOLD. http://www.genomesonline.org/cgi-bin/GOLD/bin/gold.cgi Ivanova N., Tringe S.G., Liolios K., Liu W.T., Morrison N., Hugenholtz P., Kyrpides N.C. (2010). A call for standardized classification of metagenome projects. Environ. Microbiol., 12(7):1803-5. John Makhoul, Francis Kubala, Richard Schwartz, and Ralph Weischedel. 1999. Performance measures for information extraction, in Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February.  von Mering C., Hugenholtz P., Raes J., Tringe S.G., Doerks T., Jensen L.J., Ward N., Bork P. (2007). Quantitative phylogenetic assessment of microbial communities in diverse environments. Science, 315(5815):1126-30. Metagenome Classification. /metagenomic_classification_tree.cgi MicrobeWiki. http://microbewiki.kenyon.edu/index.php/MicrobeWiki  Microbial Genomics Program at JGI. http://genome.jgi-psf.org/programs/bacteria-archaea/index.jsf Microorganisms sequenced at Genoscope. http://www.genoscope.cns.fr/spip/Microorganisms-sequenced-at.html 
168
Miguel Pignatelli, Andr?s Moya, Javier Tamames.  (2009). EnvDB, a database for describing the environmental distribution of prokaryotic taxa. Environmental Microbiology Reports. 1:198-207. Fr?d?ric Papazian, Robert Bossy and Claire N?dellec. 2012. AlvisAE: a collaborative Web text annotation editor for knowledge acquisition. The 6th Linguistic Annotation Workshop (The LAW VI), Jeju, Korea. Prokaryote Genome Projects at NCBI. http://www.ncbi.nlm.nih.gov/genomes/lproks.cgi  Zorana Ratkovic, Wiktoria Golik, Pierre Warnier. 2012. Event extraction of bacteria biotopes: a knowledge-intensive NLP-based approach. BMC Bioinformatics 2012, 13(Suppl 11):S8, 26June. .  Javier Tamames and Victor de Lorenzo. 2010. EnvMine: A text-mining system for the automatic extraction of contextual information. BMC Bioinformatics. 11:294. James Z. Wang, Zhidian Du, Rapeeporn Payattakool, Philip S. Yu, and Chin-Fu Chen. 2007. A New Method to Measure the Semantic Similarity of GO Terms. Bioinformatics. 23: 1274-1281. 
169
