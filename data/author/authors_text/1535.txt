Applying Computational Models of Spatial
Prepositions to Visually Situated Dialog
John D. Kelleher?
Dublin Institute of Technology
Fintan J. Costello??
University College Dublin
This article describes the application of computational models of spatial prepositions to visually
situated dialog systems. In these dialogs, spatial prepositions are important because people
often use them to refer to entities in the visual context of a dialog. We first describe a generic
architecture for a visually situated dialog system and highlight the interactions between the
spatial cognition module, which provides the interface to the models of prepositional semantics,
and the other components in the architecture. Following this, we present two new computational
models of topological and projective spatial prepositions. The main novelty within these models
is the fact that they account for the contextual effect which other distractor objects in a visual
scene can have on the region described by a given preposition. We next present psycholinguistic
tests evaluating our approach to distractor interference on prepositional semantics, and illustrate
how these models are used for both interpretation and generation of prepositional expressions.
1. Introduction
A growing number of computer applications share a visualized (virtual or real) space
with the user, for example graphic design programs, computer games, navigation aids,
robot systems, and so forth. If these systems are to be equipped with dialog interfaces,
they must be able to participate in visually situated dialog. Visually situated dialog is
spoken from a particular point of view within a physical or simulated context. From
theoretical linguistic and cognitive perspectives, visually situated dialog systems are
interesting as they provide ideal testbeds for investigating the interaction between
language and vision. From a human?computer interaction (HCI) perspective, visually
situated dialog systems promise many advantages to users interacting with these
systems. In this article we describe computational models for the interpretation and
generation of visually situated locative expressions involving topological and projective
spatial prepositions.
Contributions An inherent aspect of visually situated dialog is reference to objects
in the physical environment in which the dialog occurs. People often use locative
? School of Computing, Dublin Institute of Technology, Kevin Street, Dublin 8, Ireland. E-mail:
john.kelleher@comp.dit.ie.
?? School of Computer Science and Informatics, University College Dublin, Belfield, Dublin 4, Ireland.
E-mail: fintan.costello@ucd.ie.
Submission received: 31 July 2006; revised submission received: 30 March 2007; accepted for publication:
4 July 2007.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 2
expressions, in particular spatial prepositions, to pick out objects in the visual envi-
ronment. In this article we present computational models of the semantics of spatial
prepositions and illustrate how these models can be used in a visually situated dialog
system for reference resolution and generation. These models are designed to handle
reference resolution and generation in complex visual environments containing multi-
ple objects, and to account for the contextual influence which the presence of multiple
objects has on the semantics of spatial prepositions. In this our models move beyond
other accounts, which typically do not model the contextual influence of other objects
on spatial semantics. Because most real-world visual scenes are complex and contain
multiple objects, our models for the semantics of spatial prepositions are important for
visually situated dialog systems intended to operate usefully in the real world.
Overview We begin in Section 2 by describing some terminology we use when
discussing locative expressions. In Section 3 we present an abstract architecture for
a visually situated dialog system and, using this architecture, illustrate how the spa-
tial reasoning component of the architecture interacts with the other components of
the system. In Section 4 we review psycholinguistic data on the semantics of spatial
prepositions. Section 5 reviews previous computational models of spatial prepositional
semantics. Section 6 presents our computational models accounting for the semantics of
spatial prepositions and the influence of visual context on those semantics, and Section 7
presents psycholinguistic evaluation of these models. Section 8 presents applications of
the models in implemented systems. Section 8.1 presents an application of our models
to the interpretation of locative expressions, based on Kelleher, Kruijff, and Costello
(2006), and Section 8.2 presents algorithms which use these models to generate locative
expressions to identify objects in visual scenes from Kelleher and Kruijff (2006).
2. Terminology
Our computational models are designed to interpret and generate locative expressions
involving spatial prepositions. The term locative expression describes ?an expression
involving a locative prepositional phrase together with whatever the phrase modifies
(noun, clause, etc.)? (Herskovits 1986, page 7). In this article we use the term target
(T) to refer to the object that is being located by a locative expression and the term
landmark1 (L) to refer to the object relative to which the target?s location is described;
see Example (1). We will use the term distractor to describe any object in the visual
context that is neither the landmark nor the target.
Example 1
[The man]T near [the table]L.
The English lexicon of spatial prepositions numbers above 80 members (not consid-
ering compounds such as right next to) (Landau 1996). Within this set a distinction can
be made between static and dynamic prepositions: static prepositions primarily2 denote
1 There is a wealth of terms used in the literature describing locative expressions. The terms local object,
figure object, and trajector are all equivalent to our term target while the terms reference object, ground,
and relatum are equivalent to our term landmark.
2 Static prepositions can be used in dynamic contexts, for example, the man ran behind the house, and
dynamic prepositions can be used in static ones, for example, the tree lay across the road.
272
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 1
Architecture of a visually situated dialog system.
the location of an object, dynamic prepositions primarily denote the path of an object
(Jackendoff 1983; Herskovits 1986), see Examples (2) and (3).
Example 2
The tree is [behind]static the house.
Example 3
The man walked [across]dynamic the road.
In general, the set of static prepositions can be decomposed into two sets called
topological and projective. Topological prepositions are the category of prepositions
referring to a region that is proximal to the landmark; for example, at, near. Often, the
distinctions between the semantics of the different topological prepositions is based
on pragmatic constraints, for example the use of at licenses the target to be in contact
with the landmark, while the use of near does not. Projective prepositions describe a
region projected from the landmark in a particular direction, with the specification of
the direction dependent on the frame of reference3 being used; for example, to the right
of, to the left of.
3. Visually Situated Dialog System Architecture
In this section we present an abstract implementation-independent architecture for a
visually situated dialog system and highlight the role played by spatial reasoning in the
functioning of the system. In particular, we describe howmodels of spatial prepositional
semantics are important for reference resolution and generation.
The distinguishing characteristic of a visually situated dialog system is that the
system has the ability to visually perceive the environment in which a dialog is situated.
Consequently, these systems use both visual and linguistic contextual information to
understand user commands and to generate linguistic descriptions of the environment.
Figure 1 illustrates the visual dialog system architecture we will describe. The arrows in
the figure represent data flows through the system; the boxes are the main information
processing components.
3 In the context of projective prepositions, a frame of reference consists of six half-line axes with a shared
origin; in English, these axes are usually labelled front, back, right, left, above, below. In English, three
different frames of reference are distinguished: absolute, intrinsic, and viewer-centered. Interestingly
however, although the use of a tripartite system is common in European languages, this is not universal,
with many languages taking different approaches here. We direct the interested reader to Levinson (1996,
2003) and Levelt (1996) for further discussion on frames of reference.
273
Computational Linguistics Volume 35, Number 2
Figure 2
Example input and output data from a vision subsystem.
There are two information inputs into this system: the vision subsystem and the
speech interpretation pipeline. The vision subsystem directly updates the system?s
representation of the visual context. The basic requirements for the vision subsystem
are that it is able to detect and categorize the objects in the visual context and can
provide geometric positioning information for each visible object. Figure 2 illustrates
the analysis that a vision subsystem may generate for a given scene.
The speech interpretation pipeline begins with speech recognition. This module
takes a speech utterance from the user and creates a string representation of it. The
parser uses this string to construct a structured representation of the input. Parsers
range in function from wide-coverage syntactic focused parsers, such as Cahill et al?s
(2004) probabilistic Lexical-Functional Grammar (LFG) parser, to narrow coverage se-
mantic based parsers, for example the CoSy parser (Kruijff, Kelleher, and Hawes 2006).
Figure 3 illustrates the types of analyses produced by these different types of parsers for
the input string is the box near the ball? The parse tree on the left was generated using a
probabilistic wide-coverage LFG parser.4 The parse tree provides a syntactic analysis of
the input string.
Generally, parsers developed for interactive dialog systems integrate semantic, as
well as syntactic, information in their grammars. In these parsers the elements in the
lexicon and grammar are based on an analysis of the entities and relations of the
specific domain the system is designed for. These parsers sacrifice coverage for depth of
analysis. For a dialog system, the advantage of this deeper analysis is that the semantic
information in the parser?s output can be used by the dialog manager to relate the input
to the rest of the dialog. The parse structure on the right of Figure 3 illustrates the type of
semantically rich representation that an interactive dialog system parser might produce
(this particular representation was generated by the CoSy parser).
The CoSy parser uses a Combinatory Categorial Grammar that represents linguistic
meaning using an ontologically rich sorted relational structure (Baldridge and Kruijff
4 A demo of the parser is available at: http://lfg-demo.computing.dcu.ie/lfgparser.html. The parser
also provides detailed LFG f-structures for input strings.
274
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 3
Example parse structures for the string is the box near the ball?
2002, 2003). Within this representation the statement b2:phys-objmeans that the referent
b2 is of type phys-obj (i.e., a physical object as defined by the ontology the grammar in-
dexes). The semantic contribution of the prepositional phrase near the ball is represented
by the <Location> structure and its subcomponents. This structure describes a locative
prepositional phrase containing a static preposition that locates the referent b2 in the
region r that is proximal to the landmark described by the <Anchor> subcomponent. It
should be noted that the syntactic and semantic representation of prepositions within
grammars is an area of ongoing research (see Gawron 1986; Tseng 2000; Beermann and
Hellan 2004). The analysis presented here of the prepositional phrase near the ball is
intended to illustrate some of the semantic features that prepositions may introduce
into a grammar and is not intended as a comprehensive account of how prepositions
should be grammatically represented.
The final stage in the interpretation pipeline is to categorize how the utterance
relates to the current dialog context. This categorization is driven by the dialog manager
and involves interpreting an utterance as a dialog act (Bunt 1994; Carletta et al 1997;
Klein 1999). One of the important tasks in this process is resolving the references
in the input. Consequently, the dialog manager may invoke the reference resolution
component. Reference resolution is one of two functions in the architecture where
spatial reasoning plays an important role. From a computational perspective, reference
resolution involves two main tasks:
1. Creating and maintaining a model of what the system considers as
mutual knowledge (this model should contain all the objects that are
available for reference and their properties)
2. Matching the representation introduced by a given referring expression
to an element (or elements) in the set of possible referents
In a visually situated dialog a referring expression may be exophoric (i.e., denote
an object in the visual context which has not yet been mentioned in the dialog) or it
may be anaphoric (i.e., access a representation of a previous referring expression in the
dialog context). People often use the spatial location of an object, described using spatial
prepositions, when making exophoric references. As a result, in order to interpret these
references the system must have access to models of the semantics of the prepositions
275
Computational Linguistics Volume 35, Number 2
Figure 4
The mapping performed by the spatial reasoning module from qualitative to geometric
representations during the interpretation of a locative expression.
used. In this architecture this access is provided through the spatial reasoning compo-
nent. Figure 4 illustrates the translation between the qualitative, parser-generated, and
the geometric, vision subsystem?generated representations that must be performed in
order to interpret a spatial locative expression.
At different stages during a dialog the dialog manager may recognize that the
system needs to generate a response to the last input utterance. For example, the
utterance may have been a question, such as where is x? or which x?. In such cases,
the dialog manager informs the content planner of this. The role of the content planner
is to determine the semantic content that should be included in the system?s output,
rather than the linguistic realization of this content. Indeed, the content planner may
generate a logical representation closer to the parse structure on the right of Figure 3
than to a natural language description.
Generating referring expressions (GRE) is a key stage in content planning. GRE is
the second function in the architecture where spatial reasoning plays an important role.
The function of the GRE component is to determine the set of properties that distinguish
a particular target object from the other objects in the scene. For example, in response
to a question such as which x? the GRE component may determine that a color and type
description is sufficient to distinguish the target object, resulting in an answer such as the
blue x being linguistically realized. However, it may be the case that the location of the
target in the scene is the only way to distinguish it. In such cases, the GRE component
needs access to computational models of the spatial prepositions if it is to determine
which spatial relation is most suitable. Figure 5 illustrates the translation from a geo-
metric to qualitative representation that is performed during the GRE process by the
spatial reasoning module when a locative description is being generated by the system.
Once the content planning and GRE processes have been completed, the realizer
determines a surface linguistic form in which this content can be conveyed. Finally, the
speech synthesis systems generate the speech output for the linguistic string created by
the realizer.
4. Psycholinguistic Data on Spatial Prepositions
Spatial reasoning is a complex activity that involves at least two levels of process-
ing: a geometric level where metric, topological, and projective properties are handled
276
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 5
The mapping performed by the spatial reasoning module from geometric to qualitative to
representations during the generation of a locative description.
(Herskovits 1986), and a functional level where the normal function of an entity affects
the spatial relationships attributed to it in a context (Coventry and Garrod 2004).
There has been much experimental work done on spatial reasoning and language.
Some of this work has focused on functional aspects of prepositional semantics (e.g.,
Hayward and Tarr 1995; Coventry 1998; Garrod, Ferrier, and Campbell 1999), and some
on geometric factors (Gapp 1995; Logan and Sadler 1996; Regier and Carlson 2001). In
this article we are primarily concerned with the geometric semantics of prepositions
and, consequently, our review will focus on the experimental data that addresses geo-
metric factors. We will begin by reviewing the experimental data describing topological
spatial prepositions. Following this, we will then review data relating to projective
prepositions.
Topological prepositions denote a region that is proximal to a landmark. Subse-
quently we discuss previous psycholinguistic experiments, focusing on how contex-
tual factors such as distance, size, and salience may affect proximity. We also present
examples showing that the location of other objects in a scene may interfere with the
acceptability of a proximal description to locate a target relative to a landmark.
Logan and Sadler (1996) examined the semantics of several spatial prepositions. In
their experiments, a human subject was shown sentences of the form the X is [relation]
the O, each with a picture of a spatial configuration of an O in the center of an invisible
7 ? 7 cell grid, and an X in one of the 48 surrounding positions. The subject then had
to rate how well the sentence described the picture, on a scale from 1 (bad) to 9 (good).
Figure 6 gives the mean goodness rating for the relation ?near to? as a function of the
position occupied by X (Logan and Sadler 1996). It is clear from Figure 6 that ratings
diminish as the distance between X and O increases, but also that even at the extremes
of the grid the ratings were still above 1 (minimum rating).
Besides distance there are also other factors that determine the applicability of a
proximal relation. For example, given prototypical size, the region denoted by near the
building is larger than that of near the apple. Moreover, an object?s salience could influence
the determination of the proximal region associatedwith it; as with size, themore salient
an object is the larger the proximal region associated with it (Gapp 1994).
Finally, the two scenes in Figure 7 show interference as a contextual factor. For the
scene on the left we can use the blue box is near the black box to describe object (c). This
seems inappropriate in the scene on the right. Placing an object (d) beside (b) appears
277
Computational Linguistics Volume 35, Number 2
Figure 6
A 7 ? 7 cell grid with mean goodness ratings for the relation the X is near O as a function of the
position occupied by X.
to interfere with the appropriateness of using a proximal relation to locate (c) relative to
(b), even though the absolute distance between (c) and (b) has not changed.
There are several important features that are evident from these data. First, given a
context, subjects have the ability to grade the applicability of a spatial relation. Logan
and Sadler (1996) introduced the term spatial template to describe the representation of
the regions of acceptability associated with a preposition. A spatial template is centered
on the landmark and identifies for each point in its space the acceptability of the
spatial relationship between the landmark and the target appearing at that point being
described by the preposition. Second, there is empirical evidence pointing to the effects
of distance between that landmark and the target, and landmark salience and size on the
applicability of a proximity-based preposition. Finally, the examples presented point to
the fact that the location of other distractor objects in context may also interfere with the
applicability of a preposition. (The model of proximity we present in Section 6 captures
all these factors.)
Figure 8 is a representation of the spatial template for the projective preposition
above described in Logan and Sadler (1996). The main points of note relating to these
data are that there are three regions in the spatial template (good, acceptable, and
bad) and these regions are symmetric around the canonical direction of the preposition
with acceptability approaching 0 as the angular deviation from the canonical direction
approaches 90 degrees. However, it should be noted that these data were gathered dur-
ing an interpretation task and that the task may have affected the subjects? responses.
Figure 7
Proximity and distractor interference.
278
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 8
Spatial template for the preposition above (Logan and Sadler 1996), where LM represents the
landmark and the arrow shows the canonical direction associated with the preposition.
Although the subjects may have rated some of the areas on the far right and left of
the landmark as acceptable with respect to interpreting an utterance such as above the
landmark, this does not mean that they would use the word above to describe a target
object in these regions relative to the landmark. This highlights the fact that people may
be more accommodating when they are interpreting a locative description (for example,
they may extend the allowable angular deviation to 90 degrees) but be more specific
when generating a locative description.
5. Previous Models of Topological and Projective Spatial Prepositions
There has been much research on the formal properties and interactions of topological
relations, for example Cohn et al (1997) and Kuipers (2000). However, before these
higher-level frameworks can be applied to real-world data, a model of proximity that
is capable of segmenting a region at the metric or geometric level is required. At
this geometric level previous approaches to modeling topological prepositions have
adopted one of two approaches to defining the region of proximity. The first is to adopt a
Voronoi segmentation of space. Under this approach the region considered as proximal
to an object is the area surrounding it that is closer to it than to any other object in the
scene. The second is to define the proximal region in terms of the size of the landmark.
For example, Gapp (1995) defines the area of proximity as the region within ten times
the size of the landmark object in each direction. However, neither of these approaches
consider the effect that the locations of other objects in the scene have on the proximity.
Consequently, they cannot distinguish between the different context provided by the
two images in Figure 7.
Several models of projective prepositions have been proposed (Yamada 1993;
Olivier and Tsujii 1994; Gapp 1995; Fuhr et al 1998; Regier and Carlson 2001; Kelleher
and van Genabith 2006). Yamada (1993) introduced the concept of a potential field
function to capture the gradation of applicability across the region described by the
preposition. Later work (Olivier and Tsujii 1994; Gapp 1995) highlighted the issue of
defining the intended frame of reference. Building on this work and the psycholin-
guistic results of Carlson-Radvansky and Logan (1997), Kelleher and van Genabith
279
Computational Linguistics Volume 35, Number 2
(2006) developed a computational model that constructed amodified spatial template in
situations where frame of reference ambiguity occurred. Fuhr et al (1998) used models
of prepositional semantics in order to interpret natural language commands to a robotic
arm. Fuhr et al segmented the space around an object into different regions based
on the sides and vertices of the object?s bounding box. One of the drawbacks of this
system, however, was that it could not distinguish between the position of two or more
objects that were fully enclosed within a given region. Finally, Regier and Carlson (2001)
developed a vector sum algorithm to compute the applicability of a projective relation
between a landmark and a target. However, as with previous topological models, none
of these models consider the influence of other objects in the context of the landmark
target relationship. For example, the introduction of the long black object into image
2 in Figure 9 affects the interpretability of a reference such as the blue square above the
white rectangle. In the next section we describe new models designed to account for the
influence of other objects in the semantics of spatial prepositions.
6. Models of Visual Context in Topological and Projective Spatial Prepositions
If a computational model is going to accommodate the gradation of applicability across
a preposition?s spatial template it must define the semantics of the preposition as
some sort of continuum function. A potential field model is one form of continuum
measure that is widely used (Yamada 1993; Gapp 1994; Olivier and Tsujii 1994; Regier
and Carlson 2001). Using this approach, a model of a preposition?s spatial template
is constructed using a set of normalized equations that, for a given origin and point,
computes a value that represents the cost of accepting that point as the interpretation of
the preposition.
Each equation used to construct the potential field representation of a preposition?s
spatial template models a different geometric constraint specified by the preposition?s
semantics. For example, for topological prepositions such as near, an equation inversely
proportional to the distance between a point and a landmark would be used, while
for projective prepositions such as to the right of, an equation modeling the angular de-
viation of a point from the idealized direction denoted by the preposition would be in-
cluded in the construction set; Gapp (1995) and Logan and Sadler (1996) both noted that
acceptability of a projective preposition being used to describe a location approaches
0 as the angular deviation of that location approaches 90 degrees. The potential field
is then constructed by assigning each point in the field an overall potential by inte-
grating the results computed for that point by each of the equations in the construc-
tion set.
Figure 9
Projective prepositions and distractor interference.
280
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
This potential field approach does not, however, account for the influence of other
objects in the visual scene on the semantics of a topological or projective preposition.
The basic idea in our computational models is to extend the potential field approach by
overlaying the potential fields for each object in a visual scene and combining those
fields to produce relative potential fields for topological or projective prepositions.
These relative potential fields represent the semantics of those prepositions as modified
by the presence of other objects in the visual scene.
6.1 Computational Model of Topological Prepositions
In this section we describe a model of relative proximity that uses (1) the distance
between objects, (2) the size and salience of the landmark object, and (3) the location
of other objects in the scene. Our model is based on first computing absolute proximity
between each point and each landmark in a scene, and then combining or overlaying
the resulting absolute proximity fields to compute the relative proximity of each point
to each landmark.
6.1.1 Computing Absolute Proximity Fields. We first compute for each landmark an ab-
solute proximity field giving each point?s proximity to that landmark, independent of
proximity to any other landmark. We compute fields on the projection of the scene onto
the 2D-plane, represented as a 2D-array of points. At each point P in that array, the
absolute proximity for landmark L is
proxabs(L,P) = (1? distnormalized(L,P)) ? salience(L) (1)
In this equation the absolute proximity for a point P and a landmark L is a function of
both the distance between the point and the location of the landmark, and the salience
of the landmark.
To represent distance we use a normalized distance function distnormalized(L,P),
which returns a value between 0 and 1.5 The smaller the distance between L and P,
the higher the absolute proximity value returned, that is, the more acceptable it is to say
that P is close to L. In this way, this component of the absolute proximity field captures
the gradual gradation in applicability evident in Logan and Sadler (1996).
We model the influence of visual and discourse salience on absolute proximity as
a function salience(L), returning a value between 0 and 1 that represents the relative
salience of the landmark L in the scene (Equation (2)). For the current purposes we
assume that the relative salience of an object is the average of its visual salience (Svis)
and discourse salience (Sdisc).
6
salience(L) = (Svis(L)+ Sdisc(L))/2 (2)
5 We normalize by computing the distance between the two points, and then dividing this distance by the
maximum distance between point L and any point in the scene.
6 There are, of course, many other operators that could be used to combine visual and linguistic salience,
such as maximum (MAX(Svis(L),Sdisc(L))) or probabilistic OR (Svis(L)+ Sdisc(L)? (Svis(L)? Sdisc(L))).
We currently have no way of deciding among these operators. Fortunately, however, the modular nature
of our framework would allow us to change the computation of relative salience without impacting other
aspects of our model, should evidence in favor of one or other operator become available.
281
Computational Linguistics Volume 35, Number 2
Visual salience Svis is computed using the algorithm of Kelleher and van Genabith
(2004). Computing a relative salience for each object in a scene is based on its perceivable
size and its centrality relative to the viewer?s focus of attention. The algorithm returns
scores in the range of 0 to 1. As the algorithm captures object size, we can model
the effect of landmark size on proximity through the salience component of absolute
proximity. The discourse salience (Sdisc) of an object is computed based on recency of
mention (Hajicova? 1993) except we represent the maximum overall salience in the scene
as 1, and use 0 to indicate that the landmark is not salient in the current context.
Figure 10 shows computed absolute proximity with salience values of 1, 0.6, and
0.5, for points from the upper-left to the lower-right of a 2D plane, with the landmark
at the center of that plane. The graph shows how salience influences absolute proximity
in our model: For a landmark with high salience, points far from the landmark can still
have high absolute proximity to it.
6.1.2 Computing Relative Proximity Fields. Once we have constructed absolute proximity
fields for the landmarks in a scene, our next step is to overlay these fields to produce a
measure of relative proximity to each landmark at each point. For this we first select
a landmark, and then iterate over each point in the scene comparing the absolute
proximity of the selected landmark at that point with the absolute proximity of all other
landmarks at that point. The relative proximity of a selected landmark at a point is
equal to the absolute proximity field for that landmark at that point, minus the highest
absolute proximity field for any other landmark at that point:
proxrel(P,L) = proxabs(P,L)? MAX
?LX =L
proxabs(P,LX) (3)
The idea here is that the other landmark with the highest absolute proximity is acting
in competition with the selected landmark. If that other landmark?s absolute proximity
Figure 10
Absolute proximity ratings for landmark L centered in a 2D plane, points ranging from plane?s
upper-left corner (??3,?3?) to lower right corner (?3,3?).
282
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
is higher than the absolute proximity of the selected landmark, the selected landmark?s
relative proximity for the point will be negative. If the competing landmark?s absolute
proximity is slightly lower than the absolute proximity of the selected landmark, the
selected landmark?s relative proximity for the point will be positive, but low. Only when
the competing landmark?s absolute proximity is significantly lower than the absolute
proximity of the selected landmark will the selected landmark have a high relative
proximity for the point in question.
In Equation (3) the proximity of a given point to a selected landmark rises as
that point?s distance from the landmark decreases (the closer the point is to the land-
mark, the higher its proximity score for the landmark will be), but falls as that point?s
distance from some other landmark decreases (the closer the point is to some other
landmark, the lower its proximity score for the selected landmark will be). Figure 11
shows the relative proximity fields of two landmarks, L1 and L2, computed using
Equation (3) in a 1-dimensional (linear) space. The two landmarks have different de-
grees of salience: a salience of 0.5 for L1 and of 0.6 for L2 (represented by the different
sizes of the landmarks). In this figure, any point where the relative proximity for one
particular landmark is above the zero line represents a point which is proximal to
that landmark, rather than to the other landmark. The extent to which that point is
above zero represents its degree of proximity to that landmark. The overall proximal
area for a given landmark is the overall area for which its relative proximity field is
above zero. The left and right borders of the figure represent the boundaries (walls) of
the area.
Figure 11 illustrates three main points. First, the overall size of a landmark?s prox-
imal area is a function of the landmark?s position relative to the other landmark and
to the boundaries. For example, landmark L2 has a large open space between it and
the right boundary: Most of this space falls into the proximal area for that landmark.
Landmark L1 falls into quite a narrow space between the left boundary and L2. L1
thus has a much smaller proximal area in the figure than L2. Second, the relative
proximity field for a landmark is a function of that landmark?s salience. This can be
seen in Figure 11 by considering the space between the two landmarks. In that space
the width of the proximal area for L2 is greater than that of L1, because L2 is more
salient.
Figure 11
Graph of relative proximity fields for two landmarks L1 and L2. Relative proximity fields were
computed with salience scores of 0.5 for L1 and 0.6 for L2.
283
Computational Linguistics Volume 35, Number 2
Figure 12
Example scene.
The third point concerns areas of ambiguous proximity in Figure 11: areas in which
neither of the landmarks have a significantly higher relative proximity than the other.
There are two such areas in the Figure. The first is between the two landmarks, in
the region where one relative proximity field line crosses the other. These points are
ambiguous in terms of relative proximity because these points are equidistant from
those two landmarks. The second ambiguous area is at the extreme right of the space
shown in Figure 11. This area is ambiguous because this area is distant from both
landmarks: Points in this area would not be judged proximal to either landmark. The
question of ambiguity in relative proximity judgments is considered in more detail in
Section 8.1.
We will illustrate the different stages of the proximity model using the situation
illustrated in Figure 12. The task is to decide whether the target object is proximal to
the landmark object. Figure 13 illustrates the absolute potential field for the landmark
Figure 13
The absolute proximity fields for the landmark.
284
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 14
The absolute proximity fields for the landmark and the distractor.
object. Figure 14 illustrates the absolute potential fields for the landmark and the
distractor object. Figure 15 illustrates the relative proximity field that results from the
interaction between the landmark and distractors absolute proximity fields. Figure 16
illustrates the application of the threshold to the landmark?s relative proximity field. If
the target object is located in the region where the landmark?s relative proximity field
Figure 15
The landmark?s relative proximity field.
285
Computational Linguistics Volume 35, Number 2
Figure 16
Applying the threshold to the landmark?s relative proximity field.
is above the threshold the target is deemed to be proximal to the landmark. Figure 16
demonstrates the contextual influence which the distractor object has on the landmark?s
relative proximity field: The field shrinks on the side of the landmark near the distractor,
but expands on the side away from the landmark.
6.2 Computational Model of Projective Prepositions
The two main factors that impact on the applicability of a projective preposition de-
scribing the spatial relationship between a target object and a landmark are the angular
deviation of the target object?s position from the canonical direction described by the
preposition relative to the landmark and the distance of the target object from the
landmark.
The vector originating from the center of the landmark to the viewer?s position
describes the canonical search axis for in front of. We can produce the search vectors
for the other projective prepositions (behind, left, right) by rotating this front vector on a
horizontal plane. Once the canonical vectorc for a given projective preposition has been
selected, the angular deviation of a given point P position relative to the landmark L can
be computed using Equation (4):
angle( LP,c ) = cos
?1
?
?
LP ?c
?
?
?
LP
?
?
?
|c |
?
? (4)
where LP is the vector from landmark L to point P and c is the canonical vector for the
projective preposition in question. This equation gives the angle between LP and that
canonical vector.
286
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Using this equation, and the normalized distance measure described in Section 6.1,
we define an absolute potential field for the acceptability of a projective preposition
with canonical vector c for landmark L as follows:
projabs(L,P,c ) = 0 if (angle( LP,c ) > 90)or(distnormalized(L,P) = 0),
= (angle( LP,c )/distnormalized(L,P)) otherwise (5)
In this equation, if the angle between a point P and the canonical vector c is greater than
90 degrees, or if the distance between the landmark and the point is 0, the acceptability
of that point for the projective preposition is 0. Otherwise, the acceptability of that
point is equal to the angle between that point and the canonical vector, divided by the
normalized distance between that point and the landmark.
We use this absolute potential field for projective prepositions in the same way
that we used the absolute field for proximity in our model of topological prepositions.
Once we have computed the absolute potential field for each point relative to the
landmark we then do the same process for each of the distractor landmarks. We
then overlay the landmark applicabilities with those of the distractors by subtracting
the maximum applicability of any of the distractors at a point from the landmark?s
applicability at that point, producing a relative potential field for the projective prepo-
sition as in Equation (6):
projrel(P,L,c ) = projabs(P,Lc )? MAX
?LX =L
projabs(P,LX,c ) (6)
We then apply a threshold, and the region above this threshold is taken to define
the area described by the projective preposition. Note that we can use Equation (6) to
compute relative potential fields for various different projective prepositions (in front of,
behind, left, right, above, below) by selecting the different canonical vectors corresponding
to those prepositions.
Figures 17 through 20 illustrate the different stages in this process. In these images
the origin is at the front right corner, the x-axis runs from right to left, the y-axis
from front to back, and the z-axis is the vertical. The higher the z-axis value the more
applicability the preposition. Figure 17 defines the baseline applicability of z = 0.1. We
use this baseline because dividing an angular deviation by distance will never result in
a zero value; rather applicability will approach 0 asymptotically. The baseline provides
a cut-off point for applicability. Figure 18 illustrates the potential field computed for
right of a landmark positioned at x = 100, y = 200, z = 0 with a search axis of x = 1,
y = 0. Figure 19 illustrates the potential fields computed for right of the landmark and
a distractor object positioned at x = 150, y = 400, z = 0. Finally, Figure 20 illustrates the
potential field that results for right of the landmark when the distractor potential field is
subtracted from it. Figure 20 demonstrates the contextual influence which the distractor
object has on the landmark?s relative potential field for the preposition: the size of the
field is reduced by the presence of the distractor object.
7. Psycholinguistic Evaluations of Our Models
We now describe an experiment which tests our approach to relative proximity by
examining the changes in people?s judgments of the appropriateness of the expression
near being used to describe the relationship between a target and landmark object in
287
Computational Linguistics Volume 35, Number 2
Figure 17
A baseline applicability is set to z = 0.1.
an image where a distractor object is present. All objects in these images were colored
shapes: circles, triangles, or squares.
7.1 Materials and Procedure
All images used in this experiment contained a central landmark object and a target
object, usually with a third distractor object. The landmark was always placed in the
Figure 18
The potential field describing the absolute applicability model for right of the landmark.
288
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 19
The potential fields describing the absolute applicability model for right of the landmark and
right of a distractor object.
middle of a 7 ? 7 grid. Images were divided into eight groups of six images each. Each
image in a group contained the target object placed in one of six different cells on the
grid, numbered from 1 to 6. Figure 21 shows how we number these target positions
according to their nearness to the landmark.
Groups are organized according to the presence and position of a distractor object.
In group a the distractor is directly above the landmark, in group b the distractor is
Figure 20
The resulting potential field for right of the landmark with the baseline applied to it.
289
Computational Linguistics Volume 35, Number 2
Figure 21
Relative locations of landmark (L) target positions (1. . . 6) and distractor landmark positions
(a. . .g) in images used in the experiment.
rotated 45 degrees clockwise from the vertical, in group c it is directly to the right of
the landmark, in d it is rotated 135 degrees clockwise from the vertical, and so on. The
distractor object is always the same distance from the central landmark. In addition to
the distractor groups a,b,c,d,e,f, and g, there is an eighth group, group x, in which no
distractor object occurs.
In the experiment, each image was displayed with a sentence of the form The
is near the , with a description of the target and landmark, respectively. The sentence
was presented under the image. Twelve participants took part in this experiment. All
participants were native English speakers and all volunteered to take part. Participants
were not linguists and were naive to the formal interpretation of spatial prepositions
and to the hypotheses being tested in the experiment. Participants were asked to rate
the acceptability of the sentence as a description of the image using a 10-point scale,
with zero denoting not acceptable at all; 4 or 5 denoting moderately acceptable; and 9
perfectly acceptable. Figure 22 illustrates a trial from the experiment. Each participant
rated every image in the experiment. Images were presented in random order to control
for learning effects.
7.2 Results and Discussion
There was significant agreement between participants across all 48 images. The average
pair-wise correlation between participants? responses was r = 0.68. There was a signifi-
cant correlation of responses between every pair of participants (p < 0.01 for all pairs).
We assess participants? responses by comparing their average proximity judgments
with those predicted by the absolute proximity equation (Equation (1)), and by the
relative proximity equation (Equation (3)). For both equations we assume that all objects
have a salience score of 1. With salience equal to 1, the absolute proximity equation
relates proximity between target and landmark objects to the distance between those
two objects, so that the closer the target is to the landmark the higher its proximity will
be. With salience equal to 1, the relative proximity equation relates proximity to both
distance between target and landmark and distance between target and distractor, so
290
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 22
An example trial from the proximity experiment.
that the proximity of a given target object to a landmark rises as that target?s distance
from the landmark decreases but falls as the target?s distance from some other distractor
object decreases. It should be noted that proximity scores in both Equations (1) and
(3) are multiplied by a constant salience and that the evaluations we describe below
(correlation, multiple regression) factor out multiplication by a constant. Consequently,
choosing a particular value for salience does not affect our evaluation results.
In analyzing our results we are comparing our basic equation for absolute proximity
(Equation (1), in which proximity falls with increasing distance between target and
landmark) with the ?relative proximity? extension of this equation (Equation (3), in
which proximity falls with increasing distance between target and landmark, but rises
with distance to distractor). Because both equations are quite similar (both are based on
target?landmark distance, which is obviously the prime factor in proximity judgments),
we expect both equations to produce quite similar responses. We expect, however, that
the relative proximity equations will produce responses which are reliably closer to
people?s proximity judgments than those produced by the absolute proximity equation.
We initially used Spearman?s rank-order correlation to compare people?s average
proximity scores with those produced by Equation (1) (absolute proximity) and Equa-
tion (3) (relative proximity) for each group. For each group this analysis replaces each
proximity score with its rank within that group, and then compares the ranks. Where
the ranks returned by an equation and the ranks from participants? average proximity
scores are identical, the correlation will be 1.0; where the ranks differ, the correlation
will drop. For the absolute proximity equation, the correlation was 1.0 in six of the
groups, and .94 in the two remaining groups (group c and group g). For the relative
proximity equation, the Spearman?s rank-order correlation with people?s responses was
1.0 in each of the eight groups. The fact that the relative proximity equation has a rank-
order correlation of 1.0 in all groups while the absolute proximity equation fails to reach
1.0 in two groups (predicting proximity-ranks incorrectly in those two groups) suggests
291
Computational Linguistics Volume 35, Number 2
that the relative-proximity equation is a better model of people?s proximity responses.
However, the fact that there are somany correlations of 1.0 means that Spearman?s rank-
order correlation is not particularly useful in distinguishing between the two equations.
We therefore use Pearson?s product-moment correlation to compare people?s average
proximity scores with those produced by the absolute and relative proximity equations.
Rather than comparing ranks, this analysis compares actual proximity values.
Figure 23 shows the product-moment correlations between people?s average prox-
imity ratings and those produced by Equation (1) (absolute proximity) and by Equa-
tion (3) (relative proximity) for the eight groups in the experiment. In analyzing these
correlations we had two concerns: first, to see whether, for each individual group, the
correlation produced by Equation (3) was reliably different from that produced by
Equation (1); and second, to see whether across all the groups, the correlation produced
by Equation (3) was reliably higher than that produced by Equation (1). In regard
to the first question, we did not expect there to be particularly large differences in
correlation between the two equations, because both are based on target?landmark
distance. Because we know target?landmark distance to be a good predictor of people?s
proximity judgments we expected Equation (1) to have a high correlation with people?s
proximity judgments, and we expected Equation (3) to improve on that correlation.
However, because the correlation from Equation (1) was already high, any improvement
in correlation from Equation (3) would be relatively small. Indeed this is what is seen
across the seven groups of interest: The average correlation from Equation (1) is high
(average 0.93), the average correlation from Equation (3) is higher (average 0.99), but
the difference between the two correlations is relatively small. Using Fisher?s technique
for comparing correlation coefficients we find no reliable difference between correlation
coefficients in any group.
Given that the correlations for both Equations (1) and (3) are high we examined
whether the results returned by Equation (3) were reliably closer to human judgments
than those from Equation (1). For the 42 images where a distractor object was present we
recorded which equation gave a result that was closer to the participants? normalized
average for that image. In 28 cases Equation (3) was closer, and in 14 Equation (1) was
closer (a 2:1 advantage for Equation (3), significant in a sign test: n+ = 28,n? = 14,Z =
2.2, p < 0.05). We conclude that proximity judgments for objects in our experiment
are best represented by relative proximity as computed in Equation (3). These results
support our ?relative? model of proximity.7
In addition to these analyses, we also carried out a multiple regression analysis of
participants? responses in the experiment, with target?landmark distance and target?
distractor distance as the predictor variables, and participant response as the depen-
dent variable. Because our experiment involved repeated-measures data, we followed
the procedure for regression analysis of repeated-measures data described by Lorch
and Myers (1990). This involves computing individual multiple regression for each
participant in our experiment, and then using a t-test to analyze the regression coef-
ficients produced for target?distractor distance and target?landmark distance in those
equations, across all participants. Recall that in our relative proximity equation (Equa-
tion (3)) target?landmark distance had a negative coefficient (as target?landmark dis-
tance increased, judgments of target?landmark proximity fell) whereas target?distractor
7 Note that, in order to display the relationship between proximity values given by participants, computed
in Equation (1), and computed in Equation (3), the values displayed in Figure 23 are normalized so that
proximity values have a mean of 0 and a standard deviation of 1. This normalization simply means that
all values fall in the same region of the scale, and can be easily compared visually.
292
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 23
Comparison between normalized proximity scores observed and computed for each group.
293
Computational Linguistics Volume 35, Number 2
distance had a positive coefficient (as target?distractor distance increased, judgments
of target?landmark proximity increased). Our prediction, therefore, is that across these
multiple regression analyses of participants? responses, the target?landmark distance
variable will reliably have a negative coefficient, whereas the target?distractor variable
will reliably have a positive coefficient.
Table 1 shows the regression coefficients obtained for the target?landmark distance
variable and the target?distractor distance variable, across the 12 participants in our
experiment. As this table shows, the regression coefficient for target?landmark distance
was significantly more likely to be negative (as predicted) whereas the regression
coefficient for target?distractor distance was significantly more likely to be positive
(again, as predicted). A single-group t-test showed that both target?landmark regres-
sion coefficients and target?distractor regression coefficients reliably differed from zero
(t(11) = ?8.64, p < 0.01; t(11) = 2.23, p < 0.05) indicating that both of these predictor
variables had a significant and reliable effect on participants? responses in the exper-
iment. There was no concern about collinearity between predictor variables in these
regression analyses, as the correlation between those variables (r = 0.38, %var = 0.14)
was much lower than that between the predictor variables and the dependent variable
(r = 0.93 or higher). Together these regression results, the sign-test results, and the
comparative correlations described earlier all support the model of relative proximity
as described in Equation (3).
8. Applications of the Models
The model of proximity presented here has been implemented and used as a compo-
nent in a human?robot dialog system (Kelleher and Kruijff 2006; Kelleher, Kruijff, and
Costello 2006). The proximity and projective models have also been integrated into the
LIVE virtual environment (Kelleher, Costello, and van Genabith 2005). In this section
we will describe how the models are used in these systems to interpret and generate
locative expressions.
Table 1
Regression coefficients from individual analyses of subjects data in proximity experiment.
participant target?landmark distance target?distractor distance
1 ?2.02 0.27
2 ?1.53 0.09
3 ?3.06 0.03
4 ?2.45 ?0.02
5 ?2.23 0.06
6 ?0.97 0.32
7 ?3.09 0.42
8 ?1.78 0.02
9 ?0.80 0.16
10 ?1.48 ?0.24
11 ?3.29 0.01
12 ?3.29 0.44
M ?2.17 0.13
SE 0.88 0.20
t ?8.64* 2.23**
*p < 0.01, **p < 0.05.
294
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
8.1 Interpreting Spatial References
We use the computational models of Section 6 to interpret spatial references to objects.
In this section we illustrate how we use our model of relative proximity to ground the
interpretation of a locative expression containing a topological preposition. Returning
to the architecture described in Section 3, the basic steps triggering the interpretation
of a locative are: (1) the user utters a command, such as pick up the ball near the red box,
(2) the speech recognition module processes the speech signal and passes the resulting
string to the parser, (3) the parser constructs a formal representation of the meaning of
the utterance, (4) the dialog manager categorizes the utterance to be a command and,
also, recognizes the need to resolve the referring expression the ball near the red box. At
this point the reference resolution module is triggered.
The first stage in reference resolution is to retrieve the context against which the
reference is to be resolved. This involves accessing the context model and retrieving
the set of currently accessible objects. This set is then subdivided into the set of objects
fulfilling the landmark description, the set of objects fulfilling the description of the
target object, and the set of objects fulfilling neither description.
For each candidate landmark and each object that is neither a candidate landmark
nor a candidate target we compute an absolute proximity field. For each landmark we
convert its absolute proximity field into a relative proximity field by overlaying the
absolute proximity fields of the other landmarks and the other objects in the context
that are neither candidate landmarks nor target objects. For this we iterate over each
point in the scene, and compare the competing absolute proximity scores at each point.
If the primary landmark?s (i.e., the landmark with the highest relative proximity at the
point) relative proximity exceeds the next highest relative proximity score at a given
point by more than a predefined confidence interval, the point is in the proximity region
anchored around the primary landmark. Otherwise, we take it as ambiguous and not in
the proximal region that is being interpreted. The motivation for the confidence interval
is to capture situations where the difference in relative proximity scores between the
primary landmark and one or more landmarks at a given point is relatively small.
Figure 24 illustrates the parsing of a scene into the regions ?near? two landmarks. The
relative proximity fields of the two landmarks are identical to those in Figure 11, using a
confidence interval of 0.1. Ambiguous points are where the proximity ambiguity series
is plotted at 0.5. The regions ?near? each landmark are those areas of the graph where
each landmark?s relative proximity series is the highest plot on the graph.
Figure 24 illustrates an important aspect of our model: the comparison of relative
proximity fields naturally defines the extent of vague proximal regions. For example,
see the region right of L2 in Figure 24. The extent of L2?s proximal region in this
direction is bounded by the interference effect of L1?s relative proximity field. Because
the landmarks? relative proximity scores converge, the area on the far right of the image
is ambiguous with respect to which landmark it is proximal to. In effect, the model
captures the fact that the area is relatively distant from both landmarks. In Section 8.2
we describe a cognitive load hierarchy of prepositions and how we use this to generate
locative expressions. Following this hierarchy, objects located in the area on the far right
of the image should be described with a projective relation such as to the right of L2
rather than a proximal relation like near L2.
8.1.1 An Example. To illustrate the model further we will apply the model to a real scene.
Figure 25 shows a real scene on the left-hand side, and a rendering of the scene analysis
on the right-hand side. For the shown scene analysis we have assumed all objects to
295
Computational Linguistics Volume 35, Number 2
Figure 24
Graph of ambiguous regions overlaid on relative proximity fields for landmarks L1 and L2,
with confidence interval = 0.1 and different salience scores for L1 (0.5) and L2 (0.6). Locations
of landmarks are marked on the x-axis.
have an equal salience: on the left, the blue ball; in the middle, the red ball; and on
the right, the green ball. As the analysis correctly shows, each object has a proximity
potential field (shown in its own color) but, due to interference between potential
fields, we see that proximity is usually ambiguous between at least two landmarks.
The regions that are ambiguous between two landmarks are colored using a mixture of
the colors. The white area denotes the regions defined as being ambiguous between the
three objects.
Imagine we now place a second blue ball in the scene and the user inputs the
command pick up the blue ball near the red ball. As explained previously, when the system
starts interpreting this reference it will split the context into a set of candidate target
objects, consisting of the two blue balls in the scene, the set of candidate landmarks,
consisting of the one red ball, and the set of remaining objects, the green ball. It will
then compute proximity fields for each of the candidate landmarks and the other objects
in the scene that are not candidate targets. It will then overlay these proximity fields
Figure 25
Scene analysis.
296
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
to compute the relative proximity fields around each landmark. Figure 26 illustrates
the resulting proximity fields. As can be seen from the image the original blue ball is
inside the red ball?s proximity field and consequently it will be selected as the ball to be
picked up.
This analysis highlights two important aspects of the model. First, we can observe
an interference effect between the red ball and the green ball: The potential field repre-
senting proximity to the red ball forms an ellipsoid, being inhibited to the right through
interference with the potential field of the green ball. Second, the proximity field of
the red ball is much larger then that of the green ball; this is due to the relatively high
linguistic salience of the red ball compared to the green ball due to it being mentioned
in the reference.
8.2 Generating References
In this section we illustrate how we use our models of the semantics of spatial preposi-
tions to guide the generation of a locative expression in visual situated contexts.
In the architecture described earlier, the GRE component is triggered by the content
manager. Similar to reference resolution, GRE will first retrieve the context from the
context model and generate the reference relative to this context. If a locative expression
is necessary the GRE component has three things to decide: (1) what properties of the
target object to include, (2) which object in the scene should be used as a landmark and
how should that be described, and (3) which spatial relation to use (and hence which
preposition to use).
Several GRE algorithms have addressed the issue of generating locative expressions
(Dale and Haddock 1991; Horacek 1997; Gardent 2002; Krahmer and Theune 2002;
Figure 26
Interpreting the blue ball near the red ball.
297
Computational Linguistics Volume 35, Number 2
Varges 2004). However, all these algorithms assume the GRE component has access to
a predefined scene model that defines all the spatial relations between all the entities
in the scene. For many visually situated dialog systems, in particular robotic dialog
systems, this assumption is a serious drawback for these algorithms. If an agent wishes
to generate a contextually appropriate reference it cannot assume the availability of
a domain model, rather it must dynamically construct one. Moreover, constructing a
model containing all the spatial relationships between all the entities in the domain
is prone to combinatorial explosion, both in terms of the number of objects in the
context (the location of each object in the scene must be checked against all the other
objects in the scene) and number of inter-object spatial relations (as a greater number
of spatial relations will require a greater number of comparisons between each pair
of objects). Furthermore, the context-free a priori construction of such an exhaustive
scene model is cognitively implausible. Psychological research indicates that spatial
relations are not preattentively perceptually available (Treisman and Gormican 1988).
Rather, their perception requires attention (Logan 1994, 1995). These findings point to
subjects constructing contextually dependent reduced relational scene models, rather
than an exhaustive context-free model.
The approach we adopt to generating locative expressions addresses the issue of
combinatorial explosion inherent in relational scene model construction by incremen-
tally creating a series of reduced scene models. Within each scene model only one
spatial relation is considered and only a subset of objects are considered as candidate
landmarks. This reduces both the number of relations that must be computed over each
object pair and the number of object pairs. The decision as to which relations should be
included in each scene model is guided by a cognitively-motivated hierarchy of spatial
relations. The set of candidate landmarks in a given scene is dependent on the set of
objects in the scene that fulfill the description of the target object and the semantic relation
that is being considered.
We use Dale and Reiter?s (1995) incremental GRE algorithm as the starting point
for the generation framework. The incremental algorithm iterates through the proper-
ties of the target object and for each property computes the set of distractor objects for
which the conjunction of the properties selected so far, and the current property, hold. A
property is added to the list of selected properties if it reduces the size of the distractor
object set. The algorithm succeeds when all the distractors have been ruled out; it fails
if all the properties have been processed and there are still some distractor objects.
The algorithm can be refined by ordering the checking of properties according to fixed
preferences; for example, first a taxonomic description of the target, second an absolute
property such as color, third a relative property such as size. Dale and Reiter also
stipulate that the type description of the target should be included in the description
even if its inclusion does not distinguish the target from any of the distractors; see
Algorithm 1. Dale and Reiter argue that this algorithm has a polynomial complexity
and that the theoretical run time can be characterized as nd ? nl: the run time depends
solely on the number of distractor objects nd and the number of properties considered
in iterations nl. If we assume that nd and nl are both proportional to n, the number of ob-
jects being considered, then the complexity of the incremental algorithm is of order n2.
The incremental algorithm generates a description (in terms of type, color, and
size) which distinguishes a given target object from a set of distractor objects (if such
a description exists). However, we wish to generate locative expressions which iden-
tify objects, rather than simple descriptions. These locative expressions may contain
a description of a landmark object (in terms of type, color, or size), of a target object
(type, color, or size), and a topological or projective preposition relating those two
298
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Algorithm 1 The Basic Incremental Algorithm
Require: T = target object; D = set of distractor objects.
Initialize: P = {type, color, size}; DESC = {}
for i = 0 to |P| do
if |D| 
= 0 then
D? = {x : x ? D,Pi(x) = Pi(T)}
if |D?| < |D| then
DESC = DESC ? Pi(T)
D = {x : x ? D,Pi(x) = Pi(T)}
end if
else
Distinguishing description generated
if type(x) 
? DESC then
DESC = DESC ? type(x)
end if
return DESC
end if
end for
Failed to generate distinguishing description
return DESC
objects. To generate such locative expressions we repeatedly call the basic incremental
algorithm for a sequence of different possible spatial relations. The fact that each call to
the algorithm uses a different spatial relation results in a different set of objects from
the context being defined as candidate landmarks for each function call. If a given
spatial relation allows the basic incremental algorithm to generate a description which
distinguishes the target object from the set of distractor objects, that spatial relation is
used to generate an expression identifying that object. Otherwise we move on and call
the basic incremental algorithm for the next spatial relation in our sequence.
When generating a referring expression, we use a sequence of possible forms of
reference ordered by assumed cognitive load (see Figure 27), with simpler forms of
reference (those identifying object type, for example) coming early in the sequence
and more complex forms (those involving projective prepositions, for example) coming
later. This means that our approach will preferentially produce simpler expressions to
identify an object, and only if no such simple expressions can be found which distin-
guish that object successfully will more complex topological or projective prepositions
Figure 27
Cognitive load of reference forms.
299
Computational Linguistics Volume 35, Number 2
be produced. Our sequence of relations can be extended to include relations of ternary
and higher arity such as the ball between the box and the triangle or the ball near the box and
the triangle.
We use the models of topological and projective prepositions described in Sec-
tions 6.1 and 6.2 to define the regions around a landmark to which a given topological or
projective description applies. If the target or one of the distractor objects is the only ob-
ject within that region around a given landmark, this is taken to represent a contrastive
use of a preposition relative to that landmark. If that region contains more than one
object from the target and distractor object set, then it is a relative use of the preposition.
8.2.1 Landmarks and Distinguishing Descriptions. In order to use a locative expression,
an object in the context must be selected to function as the landmark. An implicit
assumption in selecting an object to function as a landmark is that the hearer can easily
identify and locate the object within the context. As shown in Example (4), a landmark
can be the speaker, the hearer, the scene, an object in the scene, or a group of objects in
the scene.8
Example 4
? the ball on my right [speaker]
? the ball to your left [hearer]
? the ball on the right [scene]
? the ball to the left of the box [an object in the scene]
? the ball in the middle [group of objects]
Clearly, deciding which objects in a given visual context can function as landmarks
is a complex process. Some of the factors effecting this decision are object salience
and the functional relationships between objects. However, one basic constraint on
landmark selection is that the landmark should be distinguishable from the target. For
example, in the context provided by Figure 28 the ball has a relatively high salience,
because it is a singleton, despite the fact that it is smaller and geometrically less complex
than the other figures. Moreover, in this context, the ball is the only object in the scene
that can function as a landmark without recourse to using the scene itself or a grouping
of objects in the scene. Given the context in Figure 28 and all other factors being equal,
using a locative such as the man to the left of the man would be much less helpful than
using the man to the right of the ball.
Following this observation, we treat an object as a candidate landmark if the
following conditions are met:
1. The object is not the target.
2. The object is not a member of the distractor set.
Furthermore, a target landmark is a member of the candidate landmark set that stands
in relation to the target under the relation being considered and a distractor landmark
8 See Gorniak and Roy (2004) for further discussion on the use of spatial extrema of the scene and groups
of objects in the scene as landmarks.
300
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Figure 28
Visual context used to illustrate the relative semantics of topological and projective prepositions.
is a member of the candidate landmark set that stands in relation to a distractor object
under the relation being considered.
Using these categories of landmark we can define a distinguishing locative de-
scription as a locative description where there is a target landmark that can be dis-
tinguished using the basic incremental algorithm from all the members of the set of
distractor landmarks which stand under the relation used in the locative expression.
Given this, our approach is to try to generate a distinguishing description using the
standard incremental algorithm. If this fails, we divide the context into three compo-
nents: the target, the distractor objects, and the set of candidate landmarks. We then
begin to iterate through the hierarchy of relations and for each relation we create a
context model that defines the set of target and distractor landmarks. Once a context
model has been created we iterate through the target landmarks (using a salience order-
ing if there is more than one) and try to create a distinguishing locative description. A
distinguishing locative description is created by using the basic incremental algorithm
to distinguish the target landmark from the distractor landmarks. If we succeed in
generating a distinguishing locative description we return the description and stop
processing.
Algorithm 2 The Locative Incremental Algorithm
Require: T = target object; D = set of distractor objects; R = hierarchy of relations.
DESC = Basic-Incremental-Algorithm(T,D)
if DESC 
= Distinguishing then
create CL the set of candidate landmarks
CL = {x : x 
= T,DESC(x) = false}
for i = 0 to |R| do
create a context model for relation Ri consisting of TL the set of target landmarks and DL the set
of distractor landmarks
TL = {y : y ? CL,Ri(T, y) = true}
DL = {z : z ? CL,Ri(D, z) = true}
for j = 0 to |TL| by salience(TL) do
LANDDESC = Basic-Incremental-Algorithm(TLj, DL)
if LANDDESC = Distinguishing then
Distinguishing locative generated
return {DESC,Ri,LANDDESC}
end if
end for
end for
end if
FAIL
301
Computational Linguistics Volume 35, Number 2
If we cannot create a distinguishing locative description we move on to the next,
more complex spatial relation in the sequence of spatial relations, and attempt to gener-
ate a distinguishing locative description using that relation. This process continues until
either a distinguishing expression is produced or no possible spatial relations remain.
This algorithm runs the basic incremental algorithm a number of times for each
candidate relation in the list of possible relations. The length of this list will be a
constant; call it R. For each candidate relation, the number of times the incremental
algorithm runs is equal to the number of TL objects (the number of objects which
don?t fulfill the description of the target created by the current run of the incremental
algorithm, and which the target object stands under the currently selected relation to).
Call the number of TL objects nTL and note that nTL must be less than, and proportional
to, n (the total number of objects). The number of times the basic incremental algorithm
can run, in our system, is then proportional to NTL ? R; replacing with nTL with n gives
n? R runs of the basic incremental algorithm. Inserting the complexity of the basic
incremental algorithm into this, we get an overall complexity of n2 ? n? R = n3 ? R,
which although worse than the basic incremental algorithm?s n2 complexity, is still
polynomial.
This algorithm cannot generate embedded locative descriptions, such as the bag on
the chair near the window, because it does not use spatial relations as properties to
describe the landmark. However, these descriptions can be generated if needed by
replacing the call to the basic incremental algorithm for the landmark object with a
call to the whole locative expression algorithm, using the target landmark as the target
object and the set of distractor landmarks as the distractors. A nice consequence of
this approach to generating embedded locative descriptions is that infinite descriptions
(e.g., the bag on the chair supporting the bag on the chair ...) will not be generated as the
target object is excluded from the context that the landmark?s description is generated
in. However, the cost of being able to generate these embedded descriptions is a higher
exponential complexity.
8.2.2 An Example. We can illustrate the framework using the visual context provided
by the scene on the left of Figure 29. This context consists of two red boxes R1 and
Figure 29
A visual scene and the topological analysis of R1 and R2.
302
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
R2 and two blue balls B1 and B2. Imagine that we want to refer to B1. We begin by
calling the locative incremental algorithm, Algorithm 2. This in turn calls the basic
incremental algorithm, Algorithm 1, which will return the property ball. However, this
is not sufficient to create a distinguishing description as B2 is also a ball. In this context
the set of candidate landmarks equals {R1,R2} and the first relation in the hierarchy is
topological proximity, which we model as described in Section 6.1. The image on the
right of Figure 29 illustrates the analysis of the scene using this framework: The green
region on the left defines the area deemed to be proximal to R1, and the yellow region
on the right defines the area deemed to be proximal to R2. It is evident that B1 is in
the area proximal to R1; consequently R1 is classified as a target landmark. As none of
the distractors (i.e., B2) are located in a region that is proximal to a candidate landmark
there are no distractor landmarks. As a result when the basic incremental algorithm is
called to create a distinguishing description for the target landmark R1 it will return
box and this will be deemed to be a distinguishing locative description. The overall
algorithm will then return the vector {ball, proximal, box} which would result in the
realizer generating a reference of the form: the ball near the box.
9. Conclusions and Future Work
In this article we have described the application of computational models of spatial
prepositions to visually situated dialog systems. These computational models allow sys-
tems to both interpret and generate expressionswhich refer to topological and projective
relations between objects in the visual environment. The computational models of spa-
tial prepositions we present are designed to handle reference resolution and generation
in complex visual environments containing multiple objects. In particular, these models
are designed to account for the contextual influence which the presence of multiple
objects has on the semantics of topological and projective prepositions. In this respect
our computational models move beyond other accounts of the semantics of spatial
prepositions, which typically do not model the contextual influence of other objects
on spatial semantics. Because most real-world visual scenes are complex and contain
multiple objects, our computational models for the semantics of spatial prepositions are
important for visually situated dialog systems intended to operate successfully in the
real world.
Clearly there are many interesting areas for future work. To date our research has
focused on a small number of static topological and projective prepositions. We feel,
however, that our framework will apply usefully to a range of other more complex static
and dynamic prepositions, for example: between, among, within, along, beside, around.
These prepositions either involve several objects or multiple areas and, consequently,
our account of the effect of distractor objects on the target?landmark relationship could
provide a worthwhile perspective on their semantics.
This leads to another promising area for future work. Although our current model
was designed to accommodate multiple distractor objects, our empirical studies have
focused on cases where there is only one distractor. An important aim for future research
is to extend these studies and test the model in situations with multiple distractors.
From a theoretical point of view, we feel that our approach to the semantics of spa-
tial prepositions illustrates an important point for researchers working on the semantics
of natural language in general: that it is possible to investigate and model semantics not
solely as a linguistic phenomenon, but also in terms of non-linguistic factors such as
the visual environment in which language is used. For example, in the psychological
evaluations described in Section 7 we found that the semantic applicability of ?near? to
303
Computational Linguistics Volume 35, Number 2
the relationship between a target and a landmark object was reliably influenced by the
presence and location of a third, distractor, object, which was not part of the linguistic
context. That the semantics of language is influenced by non-linguistic factors is an old
point and an obvious one: however, we think that our research on visually-situated
dialog systems makes a useful contribution by showing that these systems provide
ideal testbeds for investigating the interaction between language and vision, and for
developing detailed and useful computational models of how those interactions work.
References
Baldridge, J. and G. J. M. Kruijff. 2002.
Coupling CCG and hybrid logic
dependency semantics. In Proceedings
of the 40th Annual Meeting of the Association
for Computational Linguistics (ACL-02),
pages 319?326, Philadelphia, PA.
Baldridge, J. and G. J. M. Kruijff. 2003.
Multi-modal combinatory categorial
grammar. In Proceedings of the 10th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-03), volume 1, pages 211?218,
Budapest.
Beermann, D. and L. Hellan. 2004. Semantic
decomposition in a computational HPSG
grammar: A treatment of aspect and
context-dependent directionals. In
Proceedings of the HPSG04 Conference,
pages 357?377, Leuven.
Bunt, H. 1994. Context and dialogue control.
Think, 3:19?31.
Cahill, A., M. Burke, R. O?Donovan, J. van
Genabith, and A. Way. 2004. Long-distance
dependency resolution in automatically
acquired wide-coverage PCFG-based
LFG approximations. In Proceedings of the
42nd Annual Meeting of the Association for
Computational Linguistics (ACL-04),
pages 320?327, Barcelona.
Carletta, J., A. Isard, S. Isard, J. C. Kowtko,
G. Doherty-Sneddon, and A. H. Anderson.
1997. The reliability of a dialogue structure
coding scheme. Computational Linguistics,
23(1):13?32.
Carlson-Radvansky, L. A. and G. D. Logan.
1997. The influence of reference frame
selection on spatial template construction.
Journal of Memory and Language, 37:411?437.
Cohn, A. G., B. Bennett, J. M. Gooday, and
N. Gotts. 1997. RCC: A calculus for region
based qualitative spatial reasoning.
GeoInformatica, 1:275?316.
Coventry, K. R. 1998. Spatial prepositions,
functional relations, and lexical
specification. In P. Olivier and
K. P. Gapp, editors, Representation and
Processing of Spatial Expressions. Lawrence
Erlbaum Associates, Hillsdale, NJ,
pages 247?262.
Coventry, K. R. and S. Garrod. 2004. Saying,
Seeing and Acting. The Psychological
Semantics of Spatial Prepositions. Essays in
Cognitive Psychology Series. Lawrence
Erlbaum Associates, Hillsdale, NJ.
Dale, R. and N. Haddock. 1991. Generating
referring expressions involving relations.
In Proceedings of the 5th Conference of the
European Chapter of the Association for
Computational Linguistics (EACL-91),
pages 161?166, Berlin.
Dale, R. and E. Reiter. 1995. Computatinal
interpretations of the gricean maxims in
the generation of referring expressions.
Cognitive Science, 18:233?263.
Fuhr, T., G. Socher, C. Scheering, and
G. Sagerer. 1998. A three-dimensional
spatial model for the interpretation
of image data. In P. Olivier and
K. P. Gapp, editors, Representation and
Processing of Spatial Expressions. Lawrence
Erlbaum Associates, Hillsdale, NJ,
pages 103?118.
Gapp, K. P. 1994. Basic meanings of
spatial relations: Computation and
evaluation in 3D space. In Proceedings
of the 12th National Conference on Artificial
Intelligence (AAAI-94), Seattle, WA,
pages 1393?1398.
Gapp, K. P. 1995. An empirically validated
model for computing spatial relations. In
Proceedings of the 19th German Conference on
Artificial Intelligence (KI-95), Bielefeld,
Germany, pages 245?256.
Gardent, C. 2002. Generating minimal
definite descriptions. In Proceedings of
the 40th Annual Meeting of the Association
of Computational Linguistics (ACL-02),
pages 96?103, Philadelphia, PA.
Garrod, S., G. Ferrier, and S. Campbell. 1999.
In and on: Investigating the functional
geometry of spatial prepositions.
Cognition, 72:167?189.
Gawron, J. M. 1986. Situations and
prepositions. Linguistics and Philosophy,
9(3):327?382.
Gorniak, P. and D. Roy. 2004. Grounded
semantic composition for visual scenes.
Journal of Artificial Intelligence Research,
21:429?470.
304
Kelleher and Costello Computational Models of Spatial Prepositions for VSD
Hajicova?, E. 1993. Issues of Sentence Structure
and Discourse Patterns, volume 2 of
Theoretical and Computational Linguistics.
Charles University Press, Prague.
Hayward, W. G. and M. J. Tarr. 1995. Spatial
language and spatial representation.
Cognition, 55:39?84.
Herskovits, A. 1986. Language and Spatial
Cognition: An Interdisciplinary Study of
Prepositions in English. Studies in Natural
Language Processing. Cambridge
University Press, Cambridge, UK.
Horacek, H. 1997. An algorithm for
generating referential descriptions with
flexible interfaces. In Proceedings of the
35th Annual Meeting of the Association for
Computational Linguistics, pages 206?213,
Madrid.
Jackendoff, R. 1983. Semantics and Cognition.
Current Studies in Linguistics. The MIT
Press, Cambridge, MA.
Kelleher, J., F. Costello, and J. van Genabith.
2005. Dynamically structuring, updating
and interrelating representations of visual
and lingusitic discourse context. Artificial
Intelligence, 167(1?2):62?102.
Kelleher, J. and J. van Genabith. 2004. Visual
salience and reference resolution in
simulated 3D environments. AI Review,
21(3-4):253?267.
Kelleher, J. and J. van Genabith. 2006. A
computational model of the referential
semantics of projective prepositions.
In P. Saint-Dizier, editor, Syntax and
Semantics of Prepositions, Speech and
Language Processing. Kluwer Academic
Publishers, Dordrecht, The Netherlands,
pages 199?216.
Kelleher, J. D. and G. J. Kruijff. 2006.
Incremental generation of spatial
referring expressions in situated dialog.
In Proceedings of the 3rd Joint Conference
of the International Committee on
Computational Linguistics and the
Association for Computational Linguistics
(COLING/ACL-06), pages 1041?1048,
Sydney.
Kelleher, J. D., G. J. Kruijff, and F. Costello.
2006. Proximity in context: an empirically
grounded computation model of
proximity for processing topological
spatial expressions. In Proceedings of the
3rd Joint Conference of the International
Committee on Computational Linguistics
and the Association for Computational
Linguistics (COLING/ACL-06),
pages 745?752, Sydney.
Klein, M. 1999. An overview of the state of
the art of coding schemes for dialogue act
annotation. In V. Matousek, P. Mautner,
J. Ocelikova?, and P. Sojka, editors, Text,
Speech and Dialogue (TSD?99), Lecture
Notes in Computer Science. Springer,
Berlin/Heidelberg, pages 274?297.
Krahmer, E. and M. Theune. 2002. Efficient
context-sensitive generation of referring
expressions. In K. van Deemter and
R. Kibble, editors, Information Sharing:
Reference and Presupposition in Language
Generation and Interpretation. CLSI
Publications, Stanford, CA, pages 223?263.
Kruijff, Geert-Jan, John Kelleher, and Nick
Hawes. 2006. Information fusion for
visual reference resolution in dynamic
situated dialogue. In Elisabeth Andre,
Laila Dybkjaer, Wolfgang Minker,
Heiko Neumann, and Michael Weber,
editors, In Proceedings of Perception and
Interactive Technologies (PIT06),
volume 4021 of Lecture Notes in Computer
Science. Springer Berlin/Heidelberg,
pages 117?128.
Kuipers, Benjamin. 2000. The spatial
semantic hierarchy. Artificial Intelligence,
19:191?233.
Landau, B. 1996. Multiple geometric
representations of objects in language
and language learners. In P. Bloom,
M. Peterson, L. Nadel, and M. Garrett,
editors, Language and Space. MIT Press,
Cambridge, MA, pages 317?363.
Levelt, W. J. M. 1996. Perspective taking and
ellipsis in spatial descriptions. In
M. Bloom, P. Peterson, L. Nadell, and
M. Garrett, editors, Language and Space.
MIT Press, Cambridge, MA, pages 77?108.
Levinson, S. 1996. Frame of reference and
Molyneux?s question: Crosslinguistic
evidence. In M. Bloom, P. Peterson,
L. Nadell, and M. Garrett, editors,
Language and Space. MIT Press,
Cambridge, MA, pages 109?170.
Levinson, S. 2003. Space in Language and
Cognition: Explorations in Cognitive
Diversity. Cambridge University Press,
Cambridge, UK.
Logan, G. D. 1994. Spatial attention and the
apprehension of spatial relations. Journal
of Experimental Psychology: Human
Perception and Performance, 20:1015?1036.
Logan, G. D. 1995. Linguistic and conceptual
control of visual spatial attention.
Cognitive Psychology, 12:523?533.
Logan, G. D. and D. D. Sadler. 1996.
A computational analysis of the
apprehension of spatial relations.
In M. Bloom, P. Peterson, L. Nadell,
and M. Garrett, editors, Language and
305
Computational Linguistics Volume 35, Number 2
Space. MIT Press, Cambridge, MA,
pages 493?529.
Lorch, R. F. and J. L. Myers. 1990.
Regression analyses of repeated
measures data in cognitive research.
Journal of Experimental Psychology:
Learning, Memory, and Cognition,
16(1):149?157.
Olivier, P. and J. Tsujii. 1994. Quantitative
perceptual representation of prepositional
semantics. Artificial Intelligence Review,
8:147?158.
Regier, T and L. Carlson. 2001. Grounding
spatial language in perception:
An empirical and computational
investigation. Journal of Experimental
Psychology: General, 130(2):273?298.
Treisman, A. and S. Gormican. 1988. Feature
analysis in early vision: Evidence from
search assymetries. Psychological Review,
95:15?48.
Tseng, J. L. 2000. The Representation and
Selection of Prepositions. Ph.D. thesis,
University of Edinburgh.
Varges, S. 2004. Overgenerating referring
expressions involving relations and
booleans. In Proceedings of the 3rd
International Conference on Natural
Language Generation (INLG-04),
pages 171?181, Brighton.
Yamada, A. 1993. Studies in Spatial
Descriptions Understanding Based on
Geometric Constraints Satisfaction.
Ph.D. thesis, University of Kyoto.
306
This article has been cited by:
1. Timothy Baldwin, Valia Kordoni, Aline Villavicencio. 2009. Prepositions in Applications:
A Survey and Introduction to the Special IssuePrepositions in Applications: A Survey and
Introduction to the Special Issue. Computational Linguistics 35:2, 119-149. [Citation] [PDF]
[PDF Plus]
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 745?752,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Proximity in Context: an empirically grounded computational model of
proximity for processing topological spatial expressions?
John D. Kelleher
Dublin Institute of Technology
Dublin, Ireland
john.kelleher@comp.dit.ie
Geert-Jan M. Kruijff
DFKI GmbH
Saarbruc?ken, Germany
gj@dfki.de
Fintan J. Costello
University College Dublin
Dublin, Ireland
fintan.costello@ucd.ie
Abstract
The paper presents a new model for context-
dependent interpretation of linguistic expressions
about spatial proximity between objects in a nat-
ural scene. The paper discusses novel psycholin-
guistic experimental data that tests and verifies the
model. The model has been implemented, and en-
ables a conversational robot to identify objects in a
scene through topological spatial relations (e.g. ?X
near Y?). The model can help motivate the choice
between topological and projective prepositions.
1 Introduction
Our long-term goal is to develop conversational
robots with which we can have natural, fluent sit-
uated dialog. An inherent aspect of such situated
dialog is reference to aspects of the physical envi-
ronment in which the agents are situated. In this
paper, we present a computational model which
provides a context-dependent analysis of the envi-
ronment in terms of spatial proximity. We show
how we can use this model to ground spatial lan-
guage that uses topological prepositions (?the ball
near the box?) to identify objects in a scene.
Proximity is ubiquitous in situated dialog, but
there are deeper ?cognitive? reasons for why we
need a context-dependent model of proximity to
facilitate fluent dialog with a conversational robot.
This has to do with the cognitive load that process-
ing proximity expressions imposes. Consider the
examples in (1). Psycholinguistic data indicates
that a spatial proximity expression (1b) presents a
heavier cognitive load than a referring expression
identifying an object purely on physical features
(1a) yet is easier to process than a projective ex-
pression (1c) (van der Sluis and Krahmer, 2004).
?The research reported here was supported by the CoSy
project, EU FP6 IST ?Cognitive Systems? FP6-004250-IP.
(1) a. the blue ball
b. the ball near the box
c. the ball to the right of the box
One explanation for this preference is that
feature-based descriptions are easier to resolve
perceptually, with a further distinction among fea-
tures as given in Figure 1, cf. (Dale and Reiter,
1995). On the other hand, the interpretation and
realization of spatial expressions requires effort
and attention (Logan, 1994; Logan, 1995).
Figure 1: Cognitive load
Similarly we
can distinguish be-
tween the cognitive
loads of processing
different forms of
spatial relations.
Focusing on static
prepositions, topo-
logical prepositions
have a lower cognitive load than projective
prepositions. Topological prepositions (e.g.
?at?, ?near?) describe proximity to an object.
Projective prepositions (e.g. ?above?) describe a
region in a particular direction from the object.
Projective prepositions impose a higher cognitive
load because we need to consider different spatial
frames of reference (Krahmer and Theune, 1999;
Moratz and Tenbrink, 2006). Now, if we want
a robot to interact with other agents in a way
that obeys the Principle of Minimal Cooperative
Effort (Clark and Wilkes-Gibbs, 1986), it should
adopt the simplest means to (spatially) refer to an
object. However, research on spatial language in
human-robot interaction has primarily focused on
the use of projective prepositions.
We currently lack a comprehensive model for
topological prepositions. Without such a model,
745
a robot cannot interpret spatial proximity expres-
sions nor motivate their contextually and pragmat-
ically appropriate use. In this paper, we present
a model that addresses this problem. The model
uses energy functions, modulated by visual and
discourse salience, to model how spatial templates
associated with other landmarks may interfere to
establish what are contextually appropriate ways
to locate a target relative to these landmarks. The
model enables grounding of spatial expressions
using spatial proximity to refer to objects in the
environment. We focus on expressions using topo-
logical prepositions such as ?near? or ?at?.
Terminology. We use the term target (T) to
refer to the object that is being located by a spa-
tial expression, and landmark (L) to refer to the
object relative to which the target?s location is de-
scribed: ?[The man]T near [the table]L.? A dis-
tractor is any object in the visual context that is
neither landmark nor target.
Overview ?2 presents contextual effects we can
observe in grounding spatial expressions, includ-
ing the effect of interference on whether two ob-
jects may be considered proximal. ?3 discusses a
model that accounts for all these effects, and ?4 de-
scribes an experiment to test the model. ?5 shows
how we use the model in linguistic interpretation.
2 Data
Below we discuss previous psycholinguistic expe-
rients, focusing on how contextual factors such as
distance, size, and salience may affect proximity.
We also present novel examples, showing that the
location of other objects in a scene may interfere
with the acceptability of a proximal description to
locate a target relative to a landmark. These exam-
ples motivate the model in ?3.
 
 
  
1.74 1.90 2.84 3.16 2.34 1.81 2.13 
2.61 3.84 4.66 4.97 4.90 3.56 3.26 
4.06 5.56 7.55 7.97 7.29 4.80 3.91 
3.47 4.81 6.94 7.56 7.31 5.59 3.63 
4.47 5.91 8.52 O 7.90 6.13 4.46 
3.25 4.03 4.50 4.78 4.41 3.47 3.10 
1.84 2.23 2.03 3.06 2.53 2.13 2.00 
Figure 2: 7-by-7 cell grid with mean goodness ratings for
the relation the X is near O as a function of the position oc-
cupied by X.
Spatial reasoning is a complex activity that in-
volves at least two levels of processing: a geomet-
ric level where metric, topological, and projective
properties are handled, (Herskovits, 1986); and a
functional level where the normal function of an
entity affects the spatial relationships attributed to
it in a context, cf. (Coventry and Garrod, 2004).
We focus on geometric factors.
Although a lot of experimental work has been
done on spatial reasoning and language (cf.
(Coventry and Garrod, 2004)), only Logan and
Sadler (1996) examined topological prepositions
in a context where functional factors were ex-
cluded. They introduced the notion of a spatial
template. The template is centred on the land-
mark and identifies for each point in its space the
acceptability of the spatial relationship between
the landmark and the target appearing at that point
being described by the preposition. Logan &
Sadler examined various spatial prepositions this
way. In their experiments, a human subject was
shown sentences of the form ?the X is [relation]
the O?, each with a picture of a spatial configura-
tion of an O in the center of an invisible 7-by-7
cell grid, and an X in one of the 48 surrounding
positions. The subject then had to rate how well
the sentence described the picture, on a scale from
1(bad) to 9(good). Figure 2 gives the mean good-
ness rating for the relation ?near to? as a function
of the position occupied by X (Logan and Sadler,
1996). It is clear from Figure 2 that ratings dimin-
ish as the distance between X and O increases, but
also that even at the extremes of the grid the rat-
ings were still above 1 (min. rating).
Besides distance there are also other factors that
determine the applicability of a proximal relation.
For example, given prototypical size, the region
denoted by ?near the building? is larger than that
of ?near the apple? (Gapp, 1994). Moreover, an
object?s salience influences the determination of
the proximal region associated with it (Regier and
Carlson, 2001; Roy, 2002).
Finally, the two scenes in Figure 3 show inter-
ference as a contextual factor. For the scene on the
left we can use ?the blue box is near the black box?
to describe object (c). This seems inappropriate in
the scene on the right. Placing an object (d) beside
(b) appears to interfere with the appropriateness
of using a proximal relation to locate (c) relative
to (b), even though the absolute distance between
(c) and (b) has not changed.
Thus, there is empirical evidence for several
746
Figure 3: Proximity and distance
contextual factors determining the applicability of
a proximal description. We argued that the loca-
tion of other distractor objects in context may also
interfere with this applicability. The model in ?3
captures all these factors, and is evaluated in ?4.
3 Computational Model
Below we describe a model of relative proximity
that uses (1) the distance between objects, (2) the
size and salience of the landmark object, and (3)
the location of other objects in the scene. Our
model is based on first computing absolute prox-
imity between each point and each landmark in a
scene, and then combining or overlaying the re-
sulting absolute proximity fields to compute the
relative proximity of each point to each landmark.
3.1 Computing absolute proximity fields
We first compute for each landmark an absolute
proximity field giving each point?s proximity to
that landmark, independent of proximity to any
other landmark. We compute fields on the pro-
jection of the scene onto the 2D-plane, a 2D-array
ARRAY of points. At each point P in ARRAY ,
the absolute proximity for landmark L is
proxabs = (1 ? distnormalised(L,P,ARRAY ))
? salience(L).
(1)
In this equation the absolute proximity for a
point P and a landmark L is a function of both
the distance between the point and the location of
the landmark, and the salience of the landmark.
To represent distance we use a normalised
distance function distnormalised (L, P, ARRAY ),
which returns a value between 0 and 1.1 The
smaller the distance between L and P , the higher
the absolute proximity value returned, i.e. the
more acceptable it is to say that P is close to L. In
this way, this component of the absolute proximity
field captures the gradual gradation in applicabil-
ity evident in Logan and Sadler (1996).
1We normalise by computing the distance between the
two points, and then dividing this distance it by the maximum
distance between point L and any point in the scene.
We model the influence of visual and dis-
course salience on absolute proximity as a func-
tion salience(L), returning a value between 0 and
1 that represents the relative salience of the land-
mark L in the scene (2). The relative salience of
an object is the average of its visual salience (Svis )
and discourse salience (Sdisc),
salience(L) = (Svis(L) + Sdisc(L))/2 (2)
Visual salience Svis is computed using the algo-
rithm of Kelleher and van Genabith (2004). Com-
puting a relative salience for each object in a scene
is based on its perceivable size and its centrality
relative to the viewer?s focus of attention. The al-
gorithm returns scores in the range of 0 to 1. As
the algorithm captures object size we can model
the effect of landmark size on proximity through
the salience component of absolute proximity. The
discourse salience (Sdisc) of an object is computed
based on recency of mention (Hajicova?, 1993) ex-
cept we represent the maximum overall salience in
the scene as 1, and use 0 to indicate that the land-
mark is not salient in the current context. 
 
0
0.1
0.2
0.3
0.4
0.5
0.?
0.?
0.?
0.?
1
?-3?-3? ?-2?-2? ?-1?-1? L ?1?1? ?2?2? ?3?3?
point location
pro
xim
ity 
rati
n?
???ol?te proximity to L? ?alien?e 1
???ol?te proximity to L? ?alien?e 0.?
???ol?te proximity to L? ?alien?e 0.5
 
Figure 4: Absolute proximity ratings for landmark L cen-
tered in a 2D plane, points ranging from plane?s upper-left
corner (<-3,-3>) to lower right corner(<3,3>).
Figure 4 shows computed absolute proximity
with salience values of 1, 0.6, and 0.5, for points
from the upper-left to the lower-right of a 2D
plane, with the landmark at the center of that
plane. The graph shows how salience influences
absolute proximity in our model: for a landmark
with high salience, points far from the landmark
can still have high absolute proximity to it.
3.2 Computing relative proximity fields
Once we have constructed absolute proximity
fields for the landmarks in a scene, our next step
is to overlay these fields to produce a measure of
747
relative proximity to each landmark at each point.
For this we first select a landmark, and then iter-
ate over each point in the scene comparing the ab-
solute proximity of the selected landmark at that
point with the absolute proximity of all other land-
marks at that point. The relative proximity of a
selected landmark at a point is equal to the abso-
lute proximity field for that landmark at that point,
minus the highest absolute proximity field for any
other landmark at that point (see Equation 3).
proxrel(P,L) = proxabs(P,L)? MAX
?LX #=L
proxabs(P,LX )
(3)
The idea here is that the other landmark with the
highest absolute proximity is acting in competi-
tion with the selected landmark. If that other land-
mark?s absolute proximity is higher than the ab-
solute proximity of the selected landmark, the se-
lected landmark?s relative proximity for the point
will be negative. If the competing landmark?s ab-
solute proximity is slightly lower than the abso-
lute proximity of the selected landmark, the se-
lected landmark?s relative proximity for the point
will be positive, but low. Only when the compet-
ing landmark?s absolute proximity is significantly
lower than the absolute proximity of the selected
landmark will the selected landmark have a high
relative proximity for the point in question.
In (3) the proximity of a given point to a se-
lected landmark rises as that point?s distance from
the landmark decreases (the closer the point is to
the landmark, the higher its proximity score for the
landmark will be), but falls as that point?s distance
from some other landmark decreases (the closer
the point is to some other landmark, the lower its
proximity score for the selected landmark will be).
Figure 5 shows the relative proximity fields of two
landmarks, L1 and L2, computed using (3), in a
1-dimensional (linear) space. The two landmarks
have different degrees of salience: a salience of
0.5 for L1 and of 0.6 for L2 (represented by the
different sizes of the landmarks). In this figure,
any point where the relative proximity for one par-
ticular landmark is above the zero line represents
a point which is proximal to that landmark, rather
than to the other landmark. The extent to which
that point is above zero represents its degree of
proximity to that landmark. The overall proximal
area for a given landmark is the overall area for
which its relative proximity field is above zero.
The left and right borders of the figure represent
the boundaries (walls) of the area.
Figure 5 illustrates three main points. First, the
overall size of a landmark?s proximal area is a
function of the landmark?s position relative to the
other landmark and to the boundaries. For exam-
ple, landmark L2 has a large open space between
it and the right boundary: Most of this space falls
into the proximal area for that landmark. Land-
mark L1 falls into quite a narrow space between
the left boundary and L2. L1 thus has a much
smaller proximal area in the figure than L2. Sec-
ond, the relative proximity field for some land-
mark is a function of that landmark?s salience.
This can be seen in Figure 5 by considering the
space between the two landmarks. In that space
the width of the proximal area for L2 is greater
than that of L1, because L2 is more salient.
The third point concerns areas of ambiguous
proximity in Figure 5: areas in which neither of
the landmarks have a significantly higher relative
proximity than the other. There are two such areas
in the Figure. The first is between the two land-
marks, in the region where one relative proxim-
ity field line crosses the other. These points are
ambiguous in terms of relative proximity because
these points are equidistant from those two land-
marks. The second ambiguous area is at the ex-
treme right of the space shown in Figure 5. This
area is ambiguous because this area is distant from
both landmarks: points in this area would not be
judged proximal to either landmark. The ques-
tion of ambiguity in relative proximity judgments
is considered in more detail in ?5. 
 
????
????
????
????
????
?
???
???
???
???
???
?? ??
point lo?ation?
rela
tive
 pro
xim
ity
???????? ????? ????? ????? ?? ??
???????? ????? ??? ?? ????? ?? ??
 
Figure 5: Graph of relative proximity fields for two land-
marks L1 and L2. Relative proximity fields were computed
with salience scores of 0.5 for L1 and 0.6 for L2.
4 Experiment
Below we describe an experiment which tests our
approach (?3) to relative proximity by examining
748
the changes in people?s judgements of the appro-
priateness of the expression near being used to de-
scribe the relationship between a target and land-
mark object in an image where a second, distractor
landmark is present. All objects in these images
were coloured shapes, a circle, triangle or square.
4.1 Material and Procedure
All images used in this experiment contained a
central landmark object and a target object, usu-
ally with a third distractor object. The landmark
was always placed in the middle of a 7-by-7 grid.
Images were divided into 8 groups of 6 images
each. Each image in a group contained the target
object placed in one of 6 different cells on the grid,
numbered from 1 to 6. Figure 6 shows how we
number these target positions according to their
nearness to the landmark. 
 
  
       
       
       
       
       
       
       
1 2 
4 5 a 
6 
g L c 
e 
b 
d f 
3 
Figure 6: Relative locations of landmark (L) target posi-
tions (1..6) and distractor landmark positions (a..g) in images
used in the experiment.
Groups are organised according to the presence
and position of a distractor object. In group a the
distractor is directly above the landmark, in group
b the distractor is rotated 45 degrees clockwise
from the vertical, in group c it is directly to the
right of the landmark, in d it is rotated 135 de-
grees clockwise from the vertical, and so on. The
distractor object is always the same distance from
the central landmark. In addition to the distractor
groups a,b,c,d,e,f and g, there is an eighth group,
group x, in which no distractor object occurs.
In the experiment, each image was displayed
with a sentence of the form The is near the ,
with a description of the target and landmark re-
spectively. The sentence was presented under the
image. 12 participants took part in this experi-
ment. Participants were asked to rate the accept-
ability of the sentence as a description of the im-
age using a 10-point scale, with zero denoting not
acceptable at all; four or five denoting moderately
acceptable; and nine perfectly acceptable.
4.2 Results and Discussion
We assess participants? responses by comparing
their average proximity judgments with those pre-
dicted by the absolute proximity equation (Equa-
tion 1), and by the relative proximity equation
(Equation 3). For both equations we assume
that all objects have a salience score of 1. With
salience equal to 1, the absolute proximity equa-
tion relates proximity between target and land-
mark objects to the distance between those two ob-
jects, so that the closer the target is to the landmark
the higher its proximity will be. With salience
equal to 1, the relative proximity equation re-
lates proximity to both distance between target and
landmark and distance between target and distrac-
tor, so that the proximity of a given target object
to a landmark rises as that target?s distance from
the landmark decreases but falls as the target?s dis-
tance from some other distractor object decreases.
Figure 7 shows graphs comparing participants?
proximity ratings with the proximity scores com-
puted by Equation 1 (the absolute proximity equa-
tion), and by Equation 3 (the relative proximity
equation), for the images in group x and in the
other 7 groups. In the first graph there is no dif-
ference between the proximity scores computed
by the two equations, since, when there is no dis-
tractor object present the relative proximity equa-
tion reduces to the absolute proximity equation.
The correlation between both computed proximity
scores and participants? average proximity scores
for this group is quite high (r = 0.95). For the re-
maining 7 groups the proximity value computed
from Equation 1 gives a fair match to people?s
proximity judgements for target objects (the aver-
age correlation across these seven groups in Fig-
ure 7 is around r = 0.93). However, relative
proximity score as computed in Equation 3 signifi-
cantly improves the correlation in each graph, giv-
ing an average correlation across the seven groups
of around r = 0.99 (all correlations in Figure 7
are significant p < 0.01).
Given that the correlations for both Equation 1
and Equation 3 are high we examined whether the
results returned by Equation 3 were reliably closer
to human judgements than those from Equation 1.
For the 42 images where a distractor object was
present we recorded which equation gave a result
that was closer to participants? normalised aver-
749
??
?
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
????????????????????????????
? ??? ? ?????? ???????
????????????????????????????
? ??? ? ?????? ???????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
????????? ???????????
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ???????? ??? ??
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
? ??? ??? ???? ?????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
??????? ?????????????
????????????????????????????
??????? ???????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
? ??? ??? ???? ?????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
????????????????????
?????????????????
??
??
?
?
?
? ? ? ? ? ?
???????????????
??
??
???
??
???
???
??
???
???
???
?
?????????????????????????????
? ??? ??? ???? ??????
????????????????????????????
????????????????????
?????????????????
Figure 7: comparison between normalised proximity scores observed and computed for each group.
age for that image. In 28 cases Equation 3 was
closer, while in 14 Equation 1 was closer (a 2:1
advantage for Equation 3, significant in a sign test:
n+ = 28, n? = 14, Z = 2.2, p < 0.05). We con-
clude that proximity judgements for objects in our
experiment are best represented by relative prox-
imity as computed in Equation 3. These results
support our ?relative? model of proximity.2
It is interesting to note that Equation 3 over-
estimates proximity in the cases (a, b and g)
2Note that, in order to display the relationship between
proximity values given by participants, computed in Equa-
tion 1, and computed in Equation 3, the values displayed in
Figure 7 are normalised so that proximity values have a mean
of 0 and a standard deviation of 1. This normalisation simply
means that all values fall in the same region of the scale, and
can be easily compared visually.
where the distractor object is closest to the targets
and slightly underestimates proximity in all other
cases. We will investigate this in future work.
5 Expressing spatial proximity
We use the model of ?3 to interpret spatial ref-
erences to objects. A fundamental requirement
for processing situated dialogue is that linguistic
meaning provides enough information to establish
the visual grounding of spatial expressions: How
can the robot relate the meaning of a spatial ex-
pression to a scene it visually perceives, so it can
locate the objects which the expression applies to?
Approaches agree here on the need for ontolog-
ically rich representations, but differ in how these
are to be visually grounded. Oates et al (2000)
750
and Roy (2002) use machine learning to obtain
a statistical mapping between visual and linguis-
tic features. Gorniak and Roy (2004) use manu-
ally constructed mappings between linguistic con-
structions, and probabilistic functions which eval-
uate whether an object can act as referent, whereas
DeVault and Stone (2004) use symbolic constraint
resolution. Our approach to visual grounding of
language is similar to the latter two approaches.
We use a Combinatory Categorial Grammar
(CCG) (Baldridge and Kruijff, 2003) to describe
the relation between the syntactic structure of
an utterance and its meaning. We model mean-
ing as an ontologically richly sorted, relational
structure, using a description logic-like framework
(Baldridge and Kruijff, 2002). We use OpenCCG
for parsing and realization.3
(2) the box near the ball
@{b:phys?obj}(box
& ?Delimitation?unique
& ?Number?singular
& ?Quantification?specific singular)
& @{b:phys?obj}?Location?(r : region & near
& ?Proximity?proximal
& ?Positioning?static)
& @{r :region}?FromWhere?(b1 : phys ? obj
& ball
& ?Delimitation?unique
& ?Number?singular
& ?Quantification?specific singular)
Example (2) shows the meaning representation
for ?the box near the ball?. It consists of sev-
eral, related elementary predicates (EPs). One
type of EP represents a discourse referent as a
proposition with a handle: @{b:phys?obj}(box)
means that the referent b is a physical object,
namely a box. Another type of EP states de-
pendencies between referents as modal relations,
e.g. @{b:phys?obj}?Location?(r : region & near)
means that discourse referent b (the box) is located
in a region r that is near to a landmark. We repre-
sent regions explicitly to enable later reference to
the region using deictic reference (e.g. ?there?).
Within each EP we can have semantic features,
e.g. the region r characterizes a static location of b
and expresses proximity to a landmark. Example
(2) gives a ball in the context as the landmark.
We use the sorting information in the utter-
ance?s meaning (e.g. phys-obj, region) for further
3http://www.sf.net/openccg/
interpretation using ontology-based spatial rea-
soning. This yields several inferences that need to
hold for the scene, like DeVault and Stone (2004).
Where we differ is in how we check whether these
inferences hold. Like Gorniak and Roy (2004), we
map these conditions onto the energy landscape
computed by the proximity field functions. This
enables us to take into account inhibition effects
arising in the actual situated context, unlike Gor-
niak & Roy or DeVault & Stone.
We convert relative proximity fields into prox-
imal regions anchored to landmarks to contextu-
ally interpret linguistic meaning. We must decide
whether a landmark?s relative proximity score at
a given point indicates that it is ?near? or ?close
to? or ?at? or ?beside? the landmark. For this we
iterate over each point in the scene, and compare
the relative proximity scores of the different land-
marks at each point. If the primary landmark?s
(i.e., the landmark with the highest relative prox-
imity at the point) relative proximity exceeds the
next highest relative proximity score by more than
a predefined confidence interval the point is in the
vague region anchored around the primary land-
mark. Otherwise, we take it as ambiguous and not
in the proximal region that is being interpreted.
The motivation for the confidence interval is to
capture situations where the difference in relative
proximity scores between the primary landmark
and one or more landmarks at a given point is rel-
atively small. Figure 8 illustrates the parsing of a
scene into the regions ?near? two landmarks. The
relative proximity fields of the two landmarks are
identical to those in Figure 5, using a confidence
interval of 0.1. Ambiguous points are where the
proximity ambiguity series is plotted at 0.5. The
regions ?near? each landmark are those areas of
the graph where each landmark?s relative proxim-
ity series is the highest plot on the graph.
Figure 8 illustrates an important aspect of our
model: the comparison of relative proximity fields
naturally defines the extent of vague proximal re-
gions. For example, see the region right of L2 in
Figure 8. The extent of L2?s proximal region in
this direction is bounded by the interference ef-
fect of L1?s relative proximity field. Because the
landmarks? relative proximity scores converge, the
area on the far right of the image is ambiguous
with respect to which landmark it is proximal to.
In effect, the model captures the fact that the area
is relatively distant from both landmarks. Follow-
751
Figure 8: Graph of ambiguous regions overlaid on relative
proximity fields for landmarks L1 and L2, with confidence
interval=0.1 and different salience scores for L1 (0.5) and L2
(0.6). Locations of landmarks are marked on the X-axis.
ing the cognitive load model (?1), objects located
in this region should be described with a projective
relation such as ?to the right of L2? rather than a
proximal relation like ?near L2?, see Kelleher and
Kruijff (2006).
6 Conclusions
We addressed the issue of how we can provide
a context-dependent interpretation of spatial ex-
pressions that identify objects based on proxim-
ity in a visual scene. We discussed available
psycholinguistic data to substantiate the useful-
ness of having such a model for interpreting and
generating fluent situated dialogue between a hu-
man and a robot, and that we need a context-
dependent representation of what is (situationally)
appropriate to consider proximal to a landmark.
Context-dependence thereby involves salience of
landmarks as well as inhibition effects between
landmarks. We presented a model in which we
can address these issues, and we exemplified how
logical forms representing the meaning of spa-
tial proximity expressions can be grounded in this
model. We tested and verified the model using a
psycholinguistic experiment. Future work will ex-
amine whether the model can be used to describe
the semantics of nouns (such as corner) that ex-
press vague spatial extent, and how the model re-
lates to the functional aspects of spatial reasoning.
References
J. Baldridge and G.J.M. Kruijff. 2002. Coupling CCG and
hybrid logic dependency semantics. In Proceedings of
ACL 2002, Philadelphia, Pennsylvania.
J. Baldridge and G.J.M. Kruijff. 2003. Multi-modal combi-
natory categorial grammar. In Proceedings of EACL 2003,
Budapest, Hungary.
H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collab-
orative process. Cognition, 22:1?39.
K.R. Coventry and S. Garrod. 2004. Saying, Seeing and
Acting. The Psychological Semantics of Spatial Preposi-
tions. Essays in Cognitive Psychology Series. Lawrence
Erlbaum Associates.
R. Dale and E. Reiter. 1995. Computatinal interpretations of
the gricean maxims in the generation of referring expres-
sions. Cognitive Science, 18:233?263.
D. DeVault and M. Stone. 2004. Interpreting vague utter-
ances in context. In Proceedings of COLING 2004, vol-
ume 2, pages 1247?1253, Geneva, Switzerland.
K.P. Gapp. 1994. Basic meanings of spatial relations: Com-
putation and evaluation in 3d space. In Proceedings of
AAAI-94, pages 1393?1398.
P. Gorniak and D. Roy. 2004. Grounded semantic compo-
sition for visual scenes. Journal of Artificial Intelligence
Research, 21:429?470.
E. Hajicova?. 1993. Issues of Sentence Structure and Dis-
course Patterns, volume 2 of Theoretical and Computa-
tional Linguistics. Charles University Press.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
J.D. Kelleher and G.J. Kruijff. 2006. Incremental genera-
tion of spatial referring expressions in situated dialog. In
Proceedings ACL/COLING ?06, Sydney, Australia.
J. Kelleher and J. van Genabith. 2004. Visual salience and
reference resolution in simulated 3d environments. AI Re-
view, 21(3-4):253?267.
E. Krahmer and M. Theune. 1999. Efficient generation of
descriptions in context. In R. Kibble and K. van Deemter,
editors, Workshop on the Generation of Nominals, ESS-
LLI?99, Utrecht, The Netherlands.
G.D. Logan and D.D. Sadler. 1996. A computational analy-
sis of the apprehension of spatial relations. In M. Bloom,
P.and Peterson, L. Nadell, and M. Garrett, editors, Lan-
guage and Space, pages 493?529. MIT Press.
G.D. Logan. 1994. Spatial attention and the apprehension
of spatial relations. Journal of Experimental Psychology:
Human Perception and Performance, 20:1015?1036.
G.D. Logan. 1995. Linguistic and conceptual control of vi-
sual spatial attention. Cognitive Psychology, 12:523?533.
R. Moratz and T. Tenbrink. 2006. Spatial reference in
linguistic human-robot interaction: Iterative, empirically
supported development of a model of projective relations.
Spatial Cognition and Computation.
T. Oates, Z. Eyler-Walker, and P.R. Cohen. 2000. Toward
natural language interfaces for robotic agents: Ground-
ing linguistic meaning in sensors. In Proceedings of the
Fourth International Conference on Autonomous Agents,
pages 227?228.
T Regier and L. Carlson. 2001. Grounding spatial language
in perception: An empirical and computational investi-
gation. Journal of Experimental Psychology: General,
130(2):273?298.
D.K. Roy. 2002. Learning words and syntax for a scene
description task. Computer Speech and Language, 16(3).
I.F. van der Sluis and E.J. Krahmer. 2004. The influence of
target size and distance on the production of speech and
gesture in multimodal referring expressions. In R. Kibble
and K. van Deemter, editors, ICSLP04.
752
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 160?167,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Using WordNet to Automatically Deduce Relations between Words in
Noun-Noun Compounds
Fintan J. Costello,
School of Computer Science,
University College Dublin,
Dublin 6, Ireland.
fintan.costello@ucd.ie
Tony Veale,
Department of Computer Science,
University College Dublin,
Dublin 6, Ireland.
tony.veale@ucd.ie
Simon Dunne,
Department of Computer Science,
University College Dublin, Dublin 6, Ireland.
sdunne@inismor.ucd.ie
Abstract
We present an algorithm for automatically
disambiguating noun-noun compounds by
deducing the correct semantic relation be-
tween their constituent words. This algo-
rithm uses a corpus of 2,500 compounds
annotated with WordNet senses and cov-
ering 139 different semantic relations (we
make this corpus available online for re-
searchers interested in the semantics of
noun-noun compounds). The algorithm
takes as input the WordNet senses for the
nouns in a compound, finds all parent
senses (hypernyms) of those senses, and
searches the corpus for other compounds
containing any pair of those senses. The
relation with the highest proportional co-
occurrence with any sense pair is returned
as the correct relation for the compound.
This algorithm was tested using a ?leave-
one-out? procedure on the corpus of com-
pounds. The algorithm identified the cor-
rect relations for compounds with high
precision: in 92% of cases where a re-
lation was found with a proportional co-
occurrence of 1.0, it was the correct re-
lation for the compound being disam-
biguated.
Keywords: Noun-Noun Compounds, Conceputal
Combination, Word Relations, WordNet
1 Introduction
Noun-noun compounds are short phrases made up
of two or more nouns. These compounds are
common in everyday language and are especially
frequent, and important, in technical documents
(Justeson & Katz, 1995, report that such phrases
form the majority of technical content of scien-
tific and technical documents surveyed). Under-
standing these compounds requires the listener or
reader to infer the correct semantic relationship
between the words making up the compound, in-
ferring, for example, that the phrase ?flu virus?
refers to a virus that causes flu, while ?skin virus?
describes a virus that affects the skin, and marsh
virus a virus contracted in marshes. In this paper
we describe a novel algorithm for disambiguat-
ing noun-noun compounds by automatically de-
ducing the correct semantic relationship between
their constituent words.
Our approach to compound disambiguation
combines statistical and ontological information
about words and relations in compounds. On-
tological information is derived from WordNet
(Miller, 1995), a hierarchical machine readable
dictionary, which is introduced in Section 1. Sec-
tion 2 describes the construction of an annotated
corpus of 2,500 noun-noun compounds covering
139 different semantic relations, with each noun
and each relation annotated with its correct Word-
Net sense.1
Section 3 describes our algorithm for finding
the correct relation between nouns in a com-
pound, which makes use of this annotated cor-
pus. Our general approach is that the correct re-
lation between two words in a compound can be
deduced by finding other compounds containing
words from the same semantic categories as the
words in the compound to be disambiguated: if a
particular relation occurs frequently in those other
compounds, that relation is probably also the cor-
rect relation for the compound in question. Our al-
1A file containing this corpus is available for download
from http://inismor.ucd.ie/?fintanc/wordnet compounds
160
Table 1: Thematic relations proposed by Gagne?.
relation example
head causes modifier flu virus
modifier causes head college headache
head has modifier picture book
modifier has head lemon peel
head makes modifier milk cow
head made of modifier chocolate bird
head for modifier cooking toy
modifier is head dessert food
head uses modifier gas antiques
head about modifier travel magazine
head located modifier mountain cabin
head used by modifier servant language
modifier located head murder town
head derived from modifier oil money
gorithm implements this approach by taking as in-
put the correct WordNet senses for the constituent
words in a compound (both base senses and parent
or hypernyms of those senses), and searching the
corpus for other compounds containing any pair
of those base or hypernym senses. Relations are
given a score equal to their proportional occur-
rence with those sense pairs, and the relation with
the highest proportional occurrence score across
all sense-pairs is returned as the correct relation
for the compound. Section 4 describes two differ-
ent leave-one-out tests of this ?Proportional Rela-
tion Occurrence? (PRO) algorithm, in which each
compound is consecutively removed from the cor-
pus and the algorithm is used to deduce the cor-
rect sense for that compound using the set of com-
pounds left behind. These tests show that the
PRO algorithm can identify the correct relations
for compounds, and the correct senses of those re-
lations, with high precision. Section 6 compares
our algorithm for compound disambiguation with
one recently presented alternative, Rosario et al?s
(2002) rule-based system for the disambiguation
of noun-noun compounds. The paper concludes
with a discussion of future developments of the
PRO algorithm.
2 Introduction to WordNet
In both our annotated corpus of 2,500 noun-noun
compounds and our proportional relation selection
algorithm we useWordNet (Miller, 1995). The ba-
sic unit of WordNet is the sense. Each word in
WordNet is linked to a set of senses, with each
sense identifying one particular meaning of that
word. For example, the noun ?skin? has senses rep-
resenting (i) the cutis or skin of human beings, (ii)
the rind or peel of vegetables or fruit, (iii) the hide
or pelt of an animal, (iv) a skin or bag used as a
container for liquids, and so on. Each sense con-
tains an identifying number and a ?gloss? (explain-
ing what that sense means). Each sense is linked
to its parent sense, which subsumes that sense as
part of its meaning. For example, sense (i) of the
word ?skin? (the cutis or skin of human beings) has
a parent sense ?connective tissue? which contains
that sense of skin and also contains the relevant
sense of ?bone?, ?muscle?, and so on. Each par-
ent sense has its own parents, which in turn have
their own parent senses, and so on up to the (no-
tional) root node of the WordNet hierarchy. This
hierarchical structure allows computer programs
to analyse the semantics of natural language ex-
pressions, by finding the senses of the words in
a given expression and traversing the WordNet
graph to make generalisations about the meanings
of those words.
3 Corpus of Annotated Compounds
In this section we describe the construction of a
corpus of noun-noun compounds annotated with
the correct WordNet noun senses for constituent
words, the correct semantic relation between those
words, and the correct WordNet verb sense for that
relation. In addition to providing a set of com-
pounds to use as input for our compound disam-
biguation algorithm, one aim in constructing this
corpus was to examine the relations that exist in
naturally occurring noun-noun compounds. This
follows from existing research on the relations that
occur between noun-noun compounds (e.g. Gagne?
& Shoben, 1997). Gagne? and her colleagues pro-
vide a set of ?thematic relations? (derived from
relations proposed by, for example, Levi, 1978)
which, they argue, cover the majority of semantic
relations between modifier (first word) and head
(second word) in noun-noun compounds. Table
1 shows the set of thematic relations proposed in
Gagne? & Shoben (1997). A side-effect of the con-
struction of our corpus of noun-noun compounds
was an assessment of the coverage and usefulness
of this set of relations.
3.1 Procedure
The first step in constructing a corpus of anno-
tated noun-noun compounds involved selection of
a set of noun-noun compounds to classify. The
source used was the set of noun-noun compounds
161
Figure 1: Selecting WordNet senses for nouns.
defined in WordNet. Compounds from WordNet
were used for two reasons. First, each compound
had an associated gloss or definition written by
the lexicographer who entered that compound into
the corpus: this explains the relation between the
two words in that compound. Sets of compounds
from other sources would not have such associated
definitions. Second, by using compounds from
WordNet, we could guarantee that all constituent
words of those compounds would also have en-
tries in WordNet, ensuring their acceptability to
our compound disambiguation algorithm. An ini-
tial list of over 40,000 two-word noun-noun com-
pounds were extracted from WordNet version 2.0.
From this list we selected a random subset of com-
pounds and went through that set excluding all
compounds using scientific latin (e.g. ocimum
basilicum), idiomatic compounds (e.g. zero hour,
ugli fruit), compounds containing proper nouns
(e.g. Yangtze river), non-english compounds (e.g.
faux pas), and chemical terminology (e.g. carbon
dioxide).
The remaining compounds were placed in ran-
dom order, and the third author annotated each
compound with the WordNet noun senses of the
constituent words, the semantic relation between
those words, and the WordNet verb sense of that
relation (again, with senses extracted from Word-
Net version 2.0). A web page was created for
this annotation task, showing the annotator the
compound to be annotated and the WordNet gloss
(meaning) for that compound (see Figure 1). This
page also showed the annotator the list of possible
WordNet senses for the modifier noun and head
noun in the compound, allowing the annotator to
select the correct WordNet sense for each word.
After selecting correct senses for the words in the
compound, another page was presented (Figure 2)
Figure 2: Selecting relation and relation senses.
allowing the annotator to identify the correct se-
mantic relation for that compound, and then to se-
lect the correct WordNet sense for the verb in that
relation.
We began by assuming that Gagne? & Shoben?s
(1997) set of 14 relations was complete and could
account for all compounds being annotated. How-
ever, a preliminary test revealed some common
relations (e.g., eats, lives in, contains, and re-
sembles) that were not in Gagne? & Shoben?s set.
These relations were therefore added to the list of
relations we used. Various other less commonly-
occuring relations were also observed. To allow
for these other relations, a function was added to
the web page allowing the annotator to enter the
appropriate relation appearing in the form ?noun
(insert relation) modifier? and ?modifier (insert re-
lation) noun?. They would then be shown the set
of verb senses for that relation and asked to select
the correct sense.
3.2 Results
Word sense, relation, and relation sense informa-
tion was gathered for 2,500 compounds. Relation
occurrence was well distributed across these com-
pounds: there were 139 different relations used in
the corpus. Frequency of these relations ranged
widely: there were 86 relations that occured for
just one compound in the corpus, and 53 relations
that occurred more than once. For the relations
that occured more than once in the corpus, the
average number of occurrences was 46. Table 2
shows the 5 most frequent relations in the corpus:
these 5 relations account for 54% of compounds.
Note that 2 of the 5 relations in Table 2 (head con-
162
Table 2: 5 most frequent relations in the corpus.
relation frequency number of
relation senses
head used for modifier 382 3
head about modifier 360 1
head located modifier 226 3
head contains modifier 217 3
head resembles modifier 169 1
tains modifier and head resembles modifier) are
not listed in Gagne??s set of taxonomic relations.
This suggests that the taxonomy needs to be ex-
tended by the addition of further relations.
In addition to identifying the relations used in
compounds in our corpus, we also identified the
WordNet verb sense of each relation. In total 146
different relation senses occurred in the corpus.
Most relations in the corpus were associated with
just 1 relation sense. However, a significant mi-
nority of relations (29 relations, or 21% of all re-
lations) had more than one relation sense; on aver-
age, these relations had three different senses each.
Relations with more than one sense in the corpus
tended to be the more frequently occurring rela-
tions: as Table 2 shows, of the 5 most frequent
relations in the corpus, 3 were identified as hav-
ing more than one relation sense. The two rela-
tions with the largest number of different relation
senses occurring were carry (9 senses) and makes
(8 senses). Table 3 shows the 3 most frequent
senses for both relations. This diversity of rela-
tion senses suggests that Gagne??s set of thematic
relations may be too coarse-grained to capture dis-
tinctions between relations.
4 Compound Disambiguation Algorithm
The previous section described the development
of a corpus of associations between word-sense
and relation data for a large set of noun-noun
compounds. This section presents the ?Pro-
portional Relation Occurrence? (PRO) algorithm
which makes use of this information to deduce the
correct relation for a given compound.
Our approach to compound disambiguation
works by finding other compounds containing
words from the same semantic categories as the
words in the compound to be disambiguated: if a
particular relation occurs frequently in those other
compounds, that relation is probably also the cor-
rect relation for the compound in question. We
take WordNet senses to represent semantic cate-
Table 3: Senses for relations makes and carries.
relation relation sense gloss example
Makes bring forth or yield; spice tree
Makes cause to occur or exist; smoke bomb
Makes create or manufacture cider mill
a man-made product;
Carries contain or hold, have within; pocket watch
Carries move while supporting, in passenger van
a vehicle or one?s hands;
Carries transmit or serve as the radio wave
medium for transmission;
gories. Once the correct WordNet sense for a word
has been identified, that word can placed a set
of nested semantic categories: the category repre-
sented by that WordNet sense, by the parent sense
(or hypernym) of that sense, the parent of that
parent, and so on up to the (notional) root sense
of WordNet (the semantic category which sub-
sumes every other category in WordNet). Our al-
gorithm uses the set of semantic categories for the
words in a compound, and searches for other com-
pounds containing words from any pair of those
categories.
Figure 3 shows the algorithm in pseudocode.
The algorithm uses a corpus of annotated noun-
noun compounds and, to disambiguate a given
compound, takes as input the correct WordNet
sense for the modifier and head words of that com-
pound, plus all hypernyms of those senses. The al-
gorithm pairs each modifier sense with each head
sense (lines 1 & 2 in Figure 3). For each sense-
pair, the algorithm goes through the corpus of
noun-noun compounds and extracts every com-
pound whose modifier sense (or a hypernym of
that sense) is equal to the modifier sense in the
current sense-pair, and whose head sense (or a hy-
pernym of that sense) is equal to the head sense in
that pair (lines 5 to 8). The algorithm counts the
number of times each relation occurs in that set
of compounds, and assigns each relation a Propor-
tional Relation Occurrence (PRO) score for that
sense-pair (lines 10 to 12). The PRO score for a
given relation R in a sense-pair S is a tuple with
two components, as in Equation 1:
PRO(R,S) = ?
|R ? S|
|S|
,
|R ? S|
|D|
?. (1)
The first term of this tuple is the proportion of
times relationR occurs with sense-pair S (in other
words, the conditional probability of relation R
163
Preconditions:
The entry for each compound C in corpus D contains:
CmodList = sense + hypernym senses for modifier of C;
CheadList = sense + hypernym senses for head of C;
Crel = semantic relation of C;
CrelSense = verb sense for semantic relation for C;
Input:
X = compound for which a relation is required;
modList = sense + hypernym senses for modifier of X;
headList = sense + hypernym senses for head of X;
finalResultList = ();
Begin:
1 for each modifier sense M ? modList
2 for each head sense H ? headList
3 relCount = ();
4 matchCount = 0;
5 for each compound C ? corpus D
6 if ((M ? CmodList) and (H ? CheadList))
7 relCount[Crel] = relCount[Crel] + 1;
8 matchCount = matchCount + 1;
9 for each relation R ? relCount
10 condProb = relCount[R]/matchCount;
11 jointProb = relCount([R]/|D|;
12 scoreTuple = (relProp, jointProb);
13 prevScoreTuple = finalResultList[R];
14 if (scoreTuple[1] > prevScoreTuple[1])
15 finalResultList[R] = relSscoreTuple;
16 if (scoreTuple[1] = prevScoreTuple[1])
17 if (scoreTuple[2] > prevScoreTuple[2])
18 finalResultList[R] = scoreTuple;
19 sort finalResultList by relation score tuples;
20 return finalResultList;
End.
Figure 3: Compound disambiguation algorithm.
given sense-pair S); the second term is simply the
proportion of times the relation co-occurs with the
sense pair in the database of compounds D (in
other words, the joint probability of relationR and
sense-pair S). The algorithm compares the PRO
score obtained for each relationR from the current
sense-pair with the score obtained for that relation
from any other sense-pair, using the first term of
the score tuple as the main key for comparison
(lines 14 and 15), and using the second term as
a tie-breaker (lines 16 to 18). If the PRO score for
relation R in the current sense-pair is greater than
the PRO score obtained for that relation with some
other sense pair (or if no previous score for the re-
lation has been entered), the current PRO tuple is
recorded for relation R. In this way the algorithm
finds the maximum PRO score for each relation R
across all possible sense-pairs for the compound
in question. The algorithm returns a list of can-
didate relations for the compound, sorted by PRO
score (lines 19 and 20). The relations at the front
of that list (those with the highest PRO scores) are
those most likely to be the correct relation for that
compound.
Tests of this algorithm suggest that, in many
cases, candidate relations for a given compound
will be tied on the first term of their PRO score
tuple. The use of the second score-tuple term is
therefore an important part of the algorithm. For
example, suppose that two competing relations for
some compound have a proportional occurence
of 1.0 (both relations occur in every occurrence
of some sense-pair in the compound corpus). If
the first relation occurs 20 times with its selected
sense pair (i.e. there are 20 occurrences of the
sense-pair in the corpus, and the relation occurs in
each of those 20 occurrences), but the second rela-
tion only occurs occurs 2 times with its selected
sense pair (i.e. there are 2 occurrences of that
sense-pair in the corpus, and the relation occurs
in each of those 2 occurrences), the first relation
will be preferred over the second relation, because
there is more evidence for that relation being the
correct relation for the compound in question.
The algorithm in Figure 3 returns a list of can-
didate semantic relations for a given compound
(returning relations such as ?head carries modi-
fier? for the compound vegetable truck or ?mod-
ifier causes head? for the compound storm dam-
age, for example). This algorithm can also return
a list of relation senses for a given compound (re-
turning the WordNet verb sense ?carries: moves
while supporting, in a vehicle or one?s hands? for
the relation for the compound vegetable truck but
the verb sense ?carries: transmits or serves as the
medium for transmission? for the compound ra-
dio wave, for example). To return a list of rela-
tion senses rather than relations, we replace Crel
with CrelSense throughout the algorithm in Figure
3. Section 5 describes a test of both versions of the
algorithm.
5 Testing the Algorithm
To test the PRO algorithm it was implemented in a
Perl program and applied to the corpus of com-
pounds described in Section 3. We applied the
program to two tasks: computing the correct re-
lation for a given compound, and computing the
correct relation sense for that compound. We
used a ?leave-one-out? cross-validation approach,
in which we consecutively removed each com-
pound from the corpus (making it the ?query com-
pound?), recorded the correct relation or relation
sense for that compound, then passed the correct
164
  
Precision vs PRO level
0
500
1000
1500
2000
2500
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
PRO level
num
ber 
of c
omp
oun
ds
Total number ofresponsesreturned at thisPRO level
Number ofcorrectresponsesreturned at thisPRO level
 
Figure 4: Graph of precision versus PRO value for
returned relations
head and modifier senses of that query compound
(plus their hypernyms), and the corpus of remain-
ing compounds (excluding the query compound),
to the Perl program. We carried out this process
for each compound in the corpus. The result of this
procedure was a list, for each compound, of can-
didate relations or relation senses sorted by PRO
score.
We assessed the performance of the algorithm
in two ways. We first considered the rank of
the correct relation or relation sense for a given
compound in the sorted list of candidate rela-
tions/relation senses returned by the algorithm.
The algorithm always returned a large list of can-
didate relations or relation senses for each com-
pound (over 100 different candidates returned for
all compounds). In the relation selection task, the
correct relation for a compound occurred in the
first position in this list for 41% of all compounds
(1,026 out of 2,500 compounds), and occured in
one of the first 5 positions (in the top 5% of re-
turned relations or relation senses) for 72% of all
compounds (1780 compounds). In the relation-
sense selection task, the correct relation for a com-
pound occured in the first position in this list for
43% of all compounds, and occured in one of the
first 5 positions for 74% of all compounds. This
performance suggests that the algorithm is doing
well in both tasks, given the large number of pos-
sible relations and relation senses available.
Our second assessment considered the precision
and the recall of relation/relation senses returned
by the algorithm at different proportional occur-
rence levels (different levels for the first term in
PRO score tuples as described in Equation 1). For
each proportional occurrence level between 0 and
1, we assumed that the algorithm would only re-
turn a relation or relation sense when the first rela-
 
 
Precision vs PRO level
0
500
1000
1500
2000
2500
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
PRO level
num
ber 
of c
omp
oun
ds
Total number ofresponsesreturned at thisPRO level
Number ofcorrectresponsesreturned at thisPRO level
 
Figure 5: Graph of precision versus PRO value for
returned relation senses
tion in the list of candidate relations returned had
a score at or above that level. We then counted the
total number of compounds for which a response
was returned at that level, and the total number of
compounds for which a correct response was re-
turned. The precision of the algorithm at a given
PRO level was equal to the number of correct
responses returned by the algorithm at that PRO
level, divided by the total number of responses re-
turned by the algorithm at that level. The recall
of the algorithm at a given PRO level was equal
to the number of correct responses returned by the
algorithm at that level, divided by the total number
of compounds in the database (the total number of
compounds for which the algorithm could have re-
turned a correct response).
Figure 4 shows the total number of responses,
and the total number of correct responses, returned
at each PRO level for the relation selection task.
Figure 5 shows the same data for the relation-sense
selection task. As both graphs show, as PRO level
increases, the total number of responses returned
by the algorithm declines, but the total number of
correct responses does not fall significantly. For
example, in the relation selection task, at a PRO
level of 0 the algorithm return a response (selects
a relation) for all 2,500 compounds, and approx-
imately 1,000 of those responses are correct (the
algorithm?s precision at this level is 0.41). At a
PRO level of 1, the algorithm return a response
(selects a relation) for just over 900 compounds,
and approximately 850 of those responses are cor-
rect (the algorithm?s precision at this level is 0.92).
A similar pattern is seen for the relation sense re-
sponses returned by the algorithm. These graphs
show that with a PRO level around 1, the algorithm
makes a relatively small number of errors when se-
lecting the correct relation or relation sense for a
165
given compound (an error rate of less than 10%).
The PRO algorithm thus has a high degree of pre-
cision in selecting relations for compounds.
As Figures 4 and 5 show, the number of cor-
rect responses returned by the PRO algorithm did
not vary greatly across PRO levels. This means
that the recall of the algorithm remained relatively
constant across PRO levels: in the relation selec-
tion task, for example, recall ranged from 0.41 (at
a PRO level of 0) to 0.35 (at a PRO level of 1). A
similar pattern occurred in the relation-sense se-
lection task.
6 Related Work
Various approaches to noun-noun compound dis-
ambiguation in the literature have used the seman-
tic category membership of the constituent words
in a compound to determine the relation between
those words. Most of these use hand-crafted lex-
ical hierarchies designed for particular semantic
domains. We compare our algorithm for com-
pound disambiguation with one recently presented
alternative, Rosario, Hearst, and Fillmore?s (2002)
rule-based system for the disambiguation of noun-
noun compounds in the biomedical domain.
6.1 Rule-based disambiguation algorithm
Rosario et al?s (2002) general approach to noun-
noun compound disambiguation is based, as ours
is, on the semantic categories of the nouns mak-
ing up a compound. Rosario et al make use of
the MeSH (Medical Subject Headings) hierarchy,
which provides detailed coverage of the biomed-
ical domain they focus on. Their analysis in-
volves automatically extracting a corpus of noun-
noun compounds from a large set of titles and ab-
stracts from the MedLine collection of biomedical
journal articles, and identifying the MeSH seman-
tic categories under which the modifier and head
words of those compounds fall. This analysis gen-
erates a set of category pairs for each compound
(similar to our sense pairs), with each pair consist-
ing of a MeSH category for the modifier word and
a MeSH category for the head.
The aim of Rosario et al?s analysis was to pro-
duce a set of rules which would link the MeSH
category pair for a given compound to the correct
semantic relation for that compound. Given such
a set of rules, their algorithm for disabmiguat-
ing noun-noun compounds involves obtaining the
MeSH category membership for the constituent
words of the compounds to be disambiguated,
forming category pairs, and looking up those cat-
egory pairs in the list of category-pair?relation
rules. If a rule was found linking the category pair
for a given compound to a particular semantic re-
lation, that relation was returned as the correct re-
lation for the compound in question.
To produce a list of category-pair?relation
rules, Rosario et al first selected a set of cate-
gory pairs occurring in their corpus of compounds.
For each category pair, they manually examined
20% of the compounds falling under that category
pair, paraphrasing the relation between the nouns
in that compound by hand, and seeing if that re-
lation was the same across all compounds under
that category pair. If that relation was the same
across all selected compounds, that category pair
was recorded as a rule linked to the relation pro-
duced. If, on the other hand, several different re-
lations were produced for a given category pair,
analysis decended one level in the MeSH hierar-
chy, splitting that category pair into several sub-
categories. This repeated until a rule was pro-
duced assigning a relation to every compound ex-
amined. The rules produced by this process were
then tested using a randomly chosen test set of
20% of compounds falling under each category
pair, entirely distinct from the compound set used
in rule construction, and applying the rules to
those new compounds. An evaluator checked each
compound to see whether the relation returned for
that compound was an acceptable reflection of that
compound?s meaning. The results varied between
78.6% correct to 100% correct across the different
category pairs.
6.2 Comparing the algorithms
In this section we first compare Rosario et al?s
algorithm for compound disambiguation with our
own, and then compare the procedures used to as-
sess those algorithms. While both algorithms are
based on the association between category pairs
(sense pairs) and semantic relations, they differ in
that Rosario et al?s algorithm uses a static list of
manually-defined rules linking category pairs and
semantic relations, while our PRO algorithm au-
tomatically and dynamically computes links be-
tween sense pairs and relations on the basis of pro-
portional co-occurrence in a corpus of compounds.
This gives our algorithm an advantage in terms
of coverage: where Rosario et al?s algorithm can
166
only disambiguate compounds whose constituent
words match one of the category-pair?relation
rules on their list, our algorithm should be able to
apply to any compound whose constituent words
are defined in WordNet. This also gives our al-
gorithm an advantage in terms of extendability, in
that while adding a new compound to the corpus
of compounds used by Rosario et al could poten-
tially require the manual removal or re-definition
of a number of category-pair?relation rules,
adding a new compound to the annotated corpus
used by our PRO algorithm requires no such in-
tervention. Of course, the fact that Rosario et al?s
algorithm is based on a static list of rules linking
categories and relations, while our algorithm dy-
namically computes such links, gives Rosario et
al.?s algorithm a clear efficiency advantage. Im-
proving the efficiency of the PRO algorithm, per-
haps by automatically compiling a tree of associa-
tions between word senses and semantic relations
and using that tree in compound disambiguation,
is an important aim for future research.
Our second point of comparison concerns the
procedures used to assess the two algorithms. In
Rosario et al?s assessment of their rule-based al-
gorithm, an evaluator checked the relations re-
turned by the algorithm for a set of compounds,
and found that those relations were acceptable in a
large proportion of cases (up to 100%). A problem
with this procedure is that many compounds can
fall equally under a number of different acceptable
semantic relations. The compound storm damage,
for example, is best defined by the relation causes
(?damage caused by a storm?), but also falls under
the relations makes (?damage made by a storm?)
and derived from (?damage derived from a storm?):
most people would agree that these paraphrases
all acceptably describe the meaning of the com-
pound (Devereux & Costello, 2005). This means
that, while the relations returned for compounds
by Rosario et al?s algorithmmay have been judged
acceptable for those compounds by the evaluator,
they were not necessarily the most appropriate re-
lations for those compounds: the algorithm could
have returned other relations that would have been
equally acceptable. In other words, Rosario et al?s
assessment procedure is somewhat weaker than
the assessment procedure we used to test the PRO
algorithm, in which there was one correct relation
identified for each compound and the algorithm
was taken to have performed correctly only if it re-
turned that relation. One aim for future work is to
apply the assessment procedure used by Rosario et
al. to the PRO algorithm?s output, asking an eval-
uator to assess the acceptability of the relations re-
turned rather than simply counting the cases where
the best relation was returned. This would provide
a clearer basis for comparison between the algo-
rithms.
6.3 Conclusions
In this paper we?ve described an algorithm for
noun-noun compound disambiguation which au-
tomatically identifies the semantic relations and
relation senses used in such compounds. We?ve
given evidence showing that, coupled with a
corpus of noun-noun compounds annotated with
WordNet senses and semantic relations, this al-
gorithm can identify the correct semantic rela-
tions for compounds with high precision. Unlike
other approaches to automatic compound disam-
biguation which typically apply to particular spe-
cific domains, our algorithm is not domain specific
and can identify relations for a random sample
of noun-noun compounds drawn from the Word-
Net dictionary. Further, our algorithm is fully au-
tomatic: unlike other approaches, our algorithm
does not require the manual construction of rela-
tion rules to produce successful compound disam-
biguation. In future work we hope to extend this
algorithm to provide a more efficient algorithmic
implementation, and also to apply the algorithm
in areas such as the machine translation of noun-
noun compounds, where the identification of se-
mantic relations in compounds is a crucial step in
the translation process.
References
B. Devereux & F. J. Costello. 2005. Investigating the
Relations used in Conceptual Combination. Artificial In-
telligence Review, 24(3?4): 489?515.
C. L. Gagne?, & E. J. Shoben, E. 1997. Influence
of thematic relations on the comprehension of modifier-
noun combinations. Journal of Experimental Psychology:
Learning, Memory and Cognition, 23: 71?87.
J. Justeson & S. Katz. 1995. Technical Terminology: Some
linguistic properties and an algorithm for identification in
text. Natural Language Engineering, 1?1: 9?27.
J. Levi. 1978. The Syntax and Semantics of Complex Nomi-
nals. New York: Academic Press.
G. Miller. 1995. WordNet: A lexical database. Communi-
cation of the ACM, 38(11), 39?41.
B. Rosario, M. Hearst, & C. Fillmore. 2002. The De-
scent of Hierarchy, and Selection in Relational Semantics.
Proceedings of ACL-02: 247?254.
167
Proceedings of the Third ACL-SIGSEM Workshop on Prepositions, pages 1?8,
Trento, Italy, April 2006. c?2006 Association for Computational Linguistics
Spatial Prepositions in Context:
The Semantics of near in the Presence of Distractor Objects
Fintan J. Costello,
School of Computer Science and Informatics,
University College Dublin,
Dublin, Ireland.
fintan.costello@ucd.ie
John D. Kelleher
School of Computing,
Dublin Institute of Technology,
Dublin, Ireland.
John.Kelleher@comp.dit.ie
Abstract
The paper examines how people?s judge-
ments of proximity between two objects
are influenced by the presence of a third
object. In an experiment participants were
presented with images containing three
shapes in different relative positions, and
asked to rate the acceptability of a loca-
tive expression such as ?the circle is near
the triangle? as descriptions of those im-
ages. The results showed an interaction
between the relative positions of objects
and the linguistic roles that those objects
play in the locative expression: proximity
was a decreasing function of the distance
between the head object in the expression
and the prepositional clause object, and an
increasing function the distance between
the head and the third, distractor object.
This finding leads us to a new account for
the semantics of spatial prepositions such
as near.
1 Introduction
In this paper, we present an empirical study of the
cognitive representations underpinning the uses of
proximal descriptions in locative spatial expres-
sions. A spatial locative expression consists of a
locative prepositional phrase together with what-
ever the phrase modifies (noun, clause, etc.). In
their simplest form, a locative expression consists
of a prepositional phrase modifying a noun phrase,
for example the man near the desk. People often
use spatial locatives to denote objects in a visual
scene. Understanding such references involves co-
ordination between a perceptual event and a lin-
guistic utterance. Consequently, the study of spa-
tial locatives affords the opportunity to examine
some aspects of the grounding of language in non-
language.
The conception of space underlying spatial
locatives is fundamentally relativistic: the location
of one object is specified relative to another whose
location is usually assumed by the speaker to be
known by the hearer. Moreover, underpinning this
relativistic notion of space is the concept of prox-
imity. Consequently, the notion of proximity is
an important concept at the core of human spatial
cognition. Proximal spatial relationships are often
described using topological prepositions, e.g. at,
on, near, etc.
Terminology In this paper we use the term tar-
get (T) to refer to the head of a locative expression
(the object which is being located by that expres-
sion) and the term landmark (L) to refer to the ob-
ject in the prepositional phrase in that expression
(relative to which the head?s location is described),
see Example (1).
Example 1. [The man]T near [the table]L.
We will use the term distractor to describe any
object in the visual context that is neither the land-
mark nor the target.
Contributions The paper reports on a psy-
cholinguistic experiment that examines proximity.
Previous psycholinguistic work on proximal rela-
tions, (Logan and Sadler, 1996), has not exam-
ined the effects other objects in the scene (i.e., dis-
tractors) may have on the spatial relationship be-
tween a landmark and a target. The experiment
described in this paper compares peoples? judge-
ments of proximity between target and landmark
objects when they are presented alone and when
there are presented along with other distractor ob-
jects. Based on the results of this experiment we
1
propose a new model for the semantics of spatial
prepositions such as near.
Overview In ?2 we review previous work. In
?3 we describe the experiment. In ?4 we present
the results of the experiment and our analysis. The
paper finishes with conclusions, ?5.
2 Related Work
In this section we review previous psycholinguis-
tic experiments that examined proximal spatial re-
lations. We then present example spatial contexts,
that the previous experiments did not examine,
which motivate the hypothesis tested in this paper:
the location of other objects in a scene can inter-
fere with the acceptability of a proximal descrip-
tion being used to describe the spatial relationship
between a landmark and a target. 
 
 
 
1.74 1.90 2.84 3.16 2.34 1.81 2.13 
2.61 3.84 4.66 4.97 4.90 3.56 3.26 
4.06 5.56 7.55 7.97 7.29 4.80 3.91 
3.47 4.81 6.94 7.56 7.31 5.59 3.63 
4.47 5.91 8.52 O 7.90 6.13 4.46 
3.25 4.03 4.50 4.78 4.41 3.47 3.10 
1.84 2.23 2.03 3.06 2.53 2.13 2.00 
Figure 1: 7-by-7 cell grid with mean goodness rat-
ings for the relation near as a function of the posi-
tion occupied by X.
Spatial reasoning is a complex activity that in-
volves at least two levels of representation and rea-
soning: a geometric level where metric, topologi-
cal, and projective properties are handled, (Her-
skovits, 1986); and a functional level where the
normal function of an entity affects the spatial re-
lationships attributed to it in context (for example,
the meaning of ?near? for a bomb is quite differ-
ent from the meaning of ?near? for other objects of
the same size; (Vandeloise, 1991; Coventry, 1998;
Garrod et al, 1999)).
There has been a lot of experimental work
done on spatial reasoning and language: (Carlson-
Radvansky and Irwin, 1993; Carlson-Radvansky
and Irwin, 1994; Hayward and Tarr, 1995;
Gapp, 1995; Logan and Sadler, 1996; Carlson-
Radvansky and Logan, 1997; Coventry, 1998;
Garrod et al, 1999; Regier and Carlson, 2001;
Kelleher and Costello, 2005). Of these only
(Logan and Sadler, 1996) examined topological
prepositions in a context where functional factors
were excluded.
The term spatial template denotes the repre-
sentation of the regions of acceptability associated
with a preposition. It is centred on the landmark
and identifies for each point in its space the accept-
ability of the spatial relationship between the land-
mark and the target appearing at that point being
described by the preposition (Logan and Sadler,
1996).
The concept of a spatial template emerged from
psycholinguistic experiments reported in (Logan
and Sadler, 1996). These experiments examined
various spatial prepositions. In these experiments,
a human subject was shown sentences, each with a
picture of a spatial configuration. Every sentence
was of the form ?The X is [relation] the O?. The
accompanying picture contained anO in the center
of an invisible 7-by-7 cell grid, and an X in one of
the 48 surrounding positions. The subject then had
to rate howwell the sentence described the picture,
on a scale from 1(bad) to 9(good).
Figure 1 gives the mean goodness rating for the
relation ?near to? as a function of the position oc-
cupied by the X, as reported in (Logan and Sadler,
1996). If we plot the mean goodness rating for
?near? against the distance between target X and
landmark O, we get the graph in Figure 2.
Figure 2: Mean goodness rating vs. distance be-
tween X and O.
Both the figure and the graph make it clear that
the ratings diminish as we increase the distance
between X and O. At the same time, we can ob-
serve that even at the extremes of the grid the rat-
ings were still above 1 (the minimum rating). In-
2
deed, in the four corners of the grid, the points
most distant from the landmark, the mean ratings
nearly average twice the minimum rating.
However in certain contexts other factors, apart
from the distance between the landmark and the
target, affect the applicability of a proximal rela-
tion as a description of the target?s position rela-
tive to the landmark. For example, consider the
two scenes (side-view) given in Figure 3. In the
scene on the left-hand side, we can use the de-
scription ?the blue box is near the black box? to
describe object (a). However, consider now the
scene on the right-hand side. In this context, the
description ?the blue box is near the black box?
seems inappropriate as an expression describing
(a). The placing of object (c) beside (b) would
appear to interfere with the appropriateness of us-
ing a proximal relation to locate (a) relative to (b),
even though the absolute distance between (a) and
(b) has not changed.
Figure 3: Proximity and distance
In summary, there is empirical evidence that in-
dicates that as the distance between the landmark
and the target increases the applicability of a prox-
imal description decreases. Furthermore, there is
anecdotal evidence that the location of other dis-
tractor objects in context may interfere with appli-
cability of a proximal description between a target
and landmark object. The experiment presented in
this paper is designed to empirically test the affect
of distractor objects on proximity judgements.
3 Experiment
This work examines the impact of distractor ob-
jects on subjects? judgment of proximity between
the target and the landmark objects. To do this, we
examine the changes in participants judgements of
the appropriateness of the topological preposition
near being used to describe a spatial configuration
of the target and landmark objects when a distrac-
tor object was present and when it was removed.
Topological prepositions (e.g., at, on, in, near)
are often used to describe proximal spatial rela-
tionships. However, the semantics of a given topo-
logical preposition also reflects functional (Garrod
et al, 1999), directional (Logan and Sadler, 1996)
and topological factors.1 Consequently, it was im-
portant to control for these factors during the de-
sign of the experiment.
Functional factors were controlled for by us-
ing simple shapes in the stimuli. The preposition
near was used to control the impact of directional
factors. Previous psycholinguistic work indicated
that near was not affected by any directional pref-
erences. Finally, the influence of topological fac-
tors was controlled for by ensuring that the land-
mark and target maintained a consistent topolog-
ical relationship (the objects never touched, over-
lapped or were contained in other objects).
We approached our experiment with expecta-
tion that people?s proximity judgments between a
target and a landmark will be a decreasing func-
tion of the distance between those two objects: the
smaller the distance between a landmark and a tar-
get object, the higher the proximity rating people
will give for those two objects. We expect that the
presence of a distractor object will also influence
proximity judgments, and examine two different
hypotheses about how that influence will work: a
target-centered hypothesis and landmark-centered
hypothesis. In the target-centered hypothesis, peo-
ple?s judgments of proximity between a target and
a landmark will be a decreasing function of dis-
tance between those two objects, but an increas-
ing function of distance between the target and
the distractor object. Under this hypothesis, if
the distractor object is near the target object, this
will interfere with and lower people?s judgments
of proximity between the target and the landmark.
In the landmark-centered hypothesis, by contrast,
people?s judgments of proximity between a tar-
get and a landmark will be a decreasing function
of distance between those objects, but an increas-
ing function of distance between the landmark and
the distractor object. Under this hypothesis, if
the distractor object is near the landmark, it will
interfere with and lower people?s judgments of
proximity between target and landmark. We test
these hypotheses by varying target-distractor dis-
tance in our materials, but maintaining landmark-
distractor distance constant. If the target-centered
hypothesis is correct, then people?s judgments of
proximity should vary with target-distractor dis-
tance. If the landmark-centered hypothesis is cor-
rect, target-distractor distance should not influence
1See (Cohn et al, 1997) for a description different topo-
logical relationships.
3
people?s judgments of proximity.
3.1 Material and Subjects
All images used in this experiment contained a
central landmark and a target. In most of the im-
ages there was also another object, which we will
refer to as the distractor. All of these objects were
coloured shapes, a circle, triangle or square. How-
ever, none of the images contained two objects that
were the same shape or the same colour. 
 
 
 
       
       
       
       
       
       
       
1 2 
4 5 a 
6 
g L c 
e 
b 
d f 
3 
Figure 4: Relative locations of landmark (L) tar-
get positions (1..6) and distractor positions (a..g)
in images used in the experiment.
The landmark was always placed in the mid-
dle of a seven by seven grid (row four, column
four). There were 48 images in total, divided into
8 groups of 6 images each. Each image in a group
contained the target object placed in one of 6 dif-
ferent cells on the grid, numbered from 1 to 6 (see
Figure 4). As Figure 4 shows, we number these
target positions according to their nearness to the
landmark.
Each group, then, contains images with targets
at positions 1, 2, 3, 4, 5 and 6. Groups are organ-
ised according to the presence and position of a
distractor object. Figure 4 shows the 7 different
positions used for the distractor object, labelled
a,b,c,d,e,f and g. In each of these positions the
distractor is equidistant from the landmark. In
group a the distractor is directly above the land-
mark, in group b the distractor is rotated 45 de-
grees clockwise from the vertical, in group c it is
directly to the right of the landmark, in d is rotated
135 degrees clockwise from the vertical, and so
on. Notice that some of these distractor positions
(b,d, and f ) are not aligned with the grid. This re-
alignment is necessary to ensure that the distractor
object is always the same distance from the land-
mark. Each of these groups of images used in the
experiment corresponds to one of these 7 distrac-
tor positions, with a distractor object occurring at
that position for every image in that group. In ad-
dition, there is an eight group (which we label as
group x), in which no distractor object occurs.
Previous studies of how people judge proxim-
ity have typically examined judgments where the
target is above, below, to the left or right of the
landmark. The results of these studies showed
that these distinctions are relatively unimportant,
and the gradient of proximity observed tends to be
symmetrical around the landmark. For this reason,
in our study we ignore these factors and present
landmark, target and distractor randomly rotated
(so that some participants in our experiment will
see the image with target at position 1 and distrac-
tor at position a in a rotated form where position 1
is below the landmark and position a is to the right
of the landmark, but others will see the same rela-
tive positions at different rotations). In each image
all objects present were placed exactly at the cen-
ter of the cell representing their position.
During the experiment, each image was dis-
played with a sentence of the form The is near
the . The blanks were filled with a descrip-
tion of the target and landmark respectively. The
sentence was presented under the image. 12 par-
ticipants took part in this experiment.
3.2 Procedure
There were 48 trials, constructed from the follow-
ing variables: 8 distractor conditions * 6 target po-
sitions. To avoid sequence effects the landmark,
target and distractor colour and shape were ran-
domly modified for each trial and the distractor
condition and target location were randomly se-
lected for each trial. Each trial was randomly re-
flected across the horizontal, vertical, or diagonal
axes. Trials were presented in a different random
order to each participant.
Participants were instructed that they would be
shown sentence-picture pairs and were be asked to
rate the acceptability of the sentence as a descrip-
tion of the picture using a 10-point scale, with zero
denoting not acceptable at all; four or five denot-
ing moderately acceptable; and nine perfectly ac-
4
Figure 5: Experiment instructions.
ceptable. Figure 5 presents the instructions given
to each participant before the experiment. Trials
were self-paced, and the experiments lasted about
25-30 minutes. Figure 6 illustrates how the trials
were presented.
4 Results and Discussion
There are two questions we want to ask in our ex-
amination of people?s proximity judgments in the
presence of distractor objects. First, does the pres-
ence of a distractor make any noticable difference
in people?s judgements of proximity? Second, if
the presence of a distractor does influence prox-
imity judgements, is this influence target-centered
(based on the distance between the target object
and the distractor) or landmark-centered (based on
the distance between the landmark and the distrac-
tor).
We address the first question (does the distractor
object have an influence on proximity judgments)
by comparing the results obtained for images in
group x (in which there was no distractor) with re-
Figure 6: Sample trial from the experiment.
sults obtained from other groups. In particular, we
compare the results from this group with those ob-
tained from groups c, d and e: the three groups in
which the distractor object is furthest from the set
of target positions used (as Figure 4 shows, dis-
tractor positions c, d, and e are all on the opposite
side of the landmark from the set of target posi-
tions). We focus on comparison with groups c, d,
and e because results for the other groups are com-
plicated by the fact that people?s proximity judg-
ments are influenced by the closeness of a distrac-
tor object to the target (as we will see later).
 
 
   
0
1
2
3
4
5
6
7
8
9
1 2 3 4 5 6target location
prox
imity
 rati
ng group xgroup cgroup dgroup e
   
Figure 7: mean proximity rating for target loca-
tions for group x (no distractor) and groups c, d,
and e (distractors present behind landmark)
Figure 7 shows the average proximity rating
given by participants for the 6 targets 1 to 6 for
group x (in which there was no distractor object)
and for groups c, d, and e (in which distractors oc-
curred on the opposite side of the landmark from
the target). Clearly, all three sets of distractor re-
sponses are very similar to each other, and are
all noticably different from the no-distractor re-
sponse. This difference was shown to be statis-
tically significant in a by-subjects analysis com-
paring subjects? responses for groups c,d and e
with their responses for group x. This compar-
ison showed that subjects produced significantly
lower proximity ratings for group c than group x
(Wilcoxon signed-rank test W+ = 55.50,W? =
10.50, N = 11, p <= 0.05), lower ratings for
group d than group x (W+ = 48.50,W? =
6.50, N = 11, p <= 0.05) and lower ratings
for group e than group x (W+ = 51.50,W? =
3.50, N = 11, p <= 0.01). (We exclude one
subject from this analysis because they mistakenly
gave the lowest possible proximity rating of 0 to
the item closest to the landmark in group x).
5
Figure 8: comparison between normalised proximity scores observed and computed for each group.
These results show that the presence of a dis-
tractor object reliably influences people?s proxim-
ity judgements. But how does this influence op-
erate? We examine this by comparing our exper-
imental results with those expected by the target-
centered and the landmark-centered hypotheses.
We can formalise the landmark-centered hy-
pothesis about proximity judgements as follows.
Let T be the target whose proximity to the land-
mark we?re trying to judge, let L and D be the
landmark and distractor objects respectively, and
let dist(A,B) be the computed distance between
6
two objects. Then under the landmark-centered
hypothsis, the relationship between proximity and
distance-to-landmark in our experiment should be
as in Equation 1:
prox(T,L) ?= ?dist(T,L) + dist(L,D) (1)
Equation 1 states that the judged proximity of a
target to a landmark rises as the target?s distance
from the landmark falls (the closer the target is to
the landmark, the higher its proximity score for
the landmark will be), but falls as the distance be-
tween the landmark and the distractor falls (the
closer the distractor is to the landmark, the lower
the proximity score for the target will be). Re-
call, however, that in the design of our materials,
the distance from landmark to distractor was kept
constant. When applied to our materials, there-
fore, Equation 1 reduces to
prox(T,L) ?= ?dist(T,L) (2)
Equation 2 gives a good fit to people?s proxim-
ity judgments for targets in our experiment. For
group x (the set of images for which there was no
distractor object, just a target and the landmark),
the correlation between?dist(T,D) and people?s
average proximity scores for target T was high
(r = 0.95). The first graph in Figure 8 illustrates
this correlation, comparing the average proximity
value given by participants for each target in group
x with the computed proximity value for each tar-
get in that group from Equation 2.
We next compare our experimental data with
the results expected by the target-centered hypoth-
esis for proximity judgments. Under this hypothe-
sis, the judged proximity of a target to a landmark
rises as the target?s distance from the landmark de-
creases (the closer the target is to the landmark, the
higher its proximity score for the landmark will
be), but falls as the target?s distance from the dis-
tractor decreases (the closer the target is to the dis-
tractor, the lower its proximity score for the land-
mark will be). This relationship can be formalised
as in Equation 3:
prox(T,L) ?= ?dist(T,L) + dist(T,D) (3)
Equation 3 states that if a target object is close
to the landmark and far from the distractor it will
have a high proximity score for that landmark.
However, if it is close to the landmark and close
to the distractor, its proximity score will be lower.
The remaining seven graphs in Figure 8 as-
sess this account by comparing the average prox-
imity value given by participants for each target
in the distractor groups a to g with the proxim-
ity value for each target in that group computed
from Equation 2 (the landmark-centered equation)
and with the proximity value for each target com-
puted from Equation 3 (the target-centered equa-
tion). As these graphs show, for each group the
proximity value computed from Equation 2 gives
a fair match to people?s proximity judgements for
target objects (the average correlation across the
seven groups is around r = 0.93). However, the
distance-to-distractor term in the computation of
proximity in Equation 3 significantly improves the
correlation in each graph, giving an average corre-
lation across the seven groups of around r = 0.99.
We conclude that participants? proximity judge-
ments for objects in our experiment are best rep-
resented by the model described in Equation 3, in
which the proximity of a target to a landmark is a
decreasing function of the target?s distance from
that landmark and an increasing function of the
target?s distance from distractor objects.
Note that, in order to clearly display the rela-
tionship between proximity values given by par-
ticipants for target objects, proximity computed
in Equation 2 (using target-to-landmark distance
only), and proximity computed in Equation 3 (us-
ing target-to-landmark and target-to-distractor dis-
tances) the values displayed in Figure 8 are nor-
malised so that, across all groups and targets,
the average proximity values given by participants
have a mean of 0 and a standard deviation of 1,
as do the proximity values computed in Equation
2 and those computed in Equation 3. This nor-
malisation simply means that all values fall in the
same region of the scale, and can be easily com-
pared visually. This normalisation has no effect
on the correlations obtained between the observed
and computed proximity values.
5 Conclusions
This paper described a psycholinguistic experi-
ment that investigated the cognitive representa-
tions underpinning spatial descriptions of proxim-
ity. The results showed that peoples? proximity
judgments for objects in the presence of distrac-
tors can be modelled in a straightforward way us-
7
ing the relation described in Equation 3, in which
proximity falls with the target?s distance from the
landmark, but rises with the target?s distance from
a distractor object. This means that if a target ob-
ject is close to the landmark and far from the dis-
tractor it will have a high proximity rating for that
landmark. However, if it is close to the landmark
but also close to the distractor, its proximity rating
will fall. These results suggest that the linguis-
tic roles that objects play in a locative expression
have an influence on people?s judgments of prox-
imity: proximity was a decreasing function of the
distance between the object in the head position in
the expression (the target) and that in the preposi-
tional clause position (the landmark), and an in-
creasing function the distance between the head
and the third, distractor object. This finding ex-
tends previous results on peoples? judgments of
proximity for objects.
It?s noticable, however, that the match to peo-
ple?s responses obtained by Equation 3 for items
in group a is less good than that obtained in any
of the other groups. Of all the distractors, distrac-
tor a was closer to the target object than any other
distractor. It may be that there is some other prox-
imity or occlusion effect acting in people?s judge-
ments of proximity for items in group a. Future
work will be necessary to clarify this point.
References
L.A. Carlson-Radvansky and D. Irwin. 1993. Frames of ref-
erence in vision and language: Where is above? Cogni-
tion, 46:223?224.
L.A. Carlson-Radvansky and D. Irwin. 1994. Reference
frame activation during spatial term assignment. Journal
of Memory and Language, 33:646?671.
L.A. Carlson-Radvansky and G.D. Logan. 1997. The influ-
ence of reference frame selection on spatial template con-
struction. Journal of Memory and Language, 37:411?437.
A GCohn, B Bennett, J MGooday, and NGotts. 1997. RCC:
a calculus for region based qualitative spatial reasoning.
GeoInformatica, 1:275?316.
K.R. Coventry. 1998. Spatial prepositions, functional re-
lations, and lexical specification. In P. Olivier and K.P.
Gapp, editors, Representation and Processing of Spatial
Expressions, pages 247?262. Lawrence Erlbaum Asso-
ciates.
K.P. Gapp. 1995. An empirically validated model for com-
puting spatial relations. In KI - Kunstliche Intelligenz,
pages 245?256.
S. Garrod, G. Ferrier, and S. Campbell. 1999. In and on:
investigating the functional geometry of spatial preposi-
tions. Cognition, 72:167?189.
W.G. Hayward and M.J. Tarr. 1995. Spatial language and
spatial representation. Cognition, 55:39?84.
A Herskovits. 1986. Language and spatial cognition: An
interdisciplinary study of prepositions in English. Stud-
ies in Natural Language Processing. Cambridge Univer-
sity Press.
J. Kelleher and F. Costello. 2005. Cognitive representations
of projective prepositions. In Proceedings of the Second
ACL-Sigsem Workshop of The Linguistic Dimensions of
Prepositions and their Use in Computational Linguistic
Formalisms and Applications.
G.D. Logan and D.D. Sadler. 1996. A computational analy-
sis of the apprehension of spatial relations. In M. Bloom,
P.and Peterson, L. Nadell, and M. Garrett, editors, Lan-
guage and Space, pages 493?529. MIT Press.
T Regier and L. Carlson. 2001. Grounding spatial language
in perception: An empirical and computational investi-
gation. Journal of Experimental Psychology: General,
130(2):273?298.
C. Vandeloise. 1991. Spatial Prepositions: A Case Study
From French. The University of Chicago Press.
8
Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 89?96,
Prague, Czech Republic, June 2007 c?2007 Association for Computational Linguistics
Learning to interpret novel noun-noun compounds: evidence from a
category learning experiment
Barry Devereux & Fintan Costello
School of Computer Science and Informatics, University College Dublin,
Belfield, Dublin 4, IRELAND
{barry.devereux, fintan.costello}@ucd.ie
Abstract
The ability to correctly interpret and pro-
duce noun-noun compounds such as WIND
FARM or CARBON TAX is an important part
of the acquisition of language in various do-
mains of discourse. One approach to the
interpretation of noun-noun compounds as-
sumes that people make use of distributional
information about how the constituent words
of compounds tend to combine; another as-
sumes that people make use of information
about the two constituent concepts? features
to produce interpretations. We present an ex-
periment that examines how people acquire
both the distributional information and con-
ceptual information relevant to compound
interpretation. A plausible model of the in-
terpretation process is also presented.
1 Introduction
People frequently encounter noun-noun compounds
such as MEMORY STICK and AUCTION POLITICS
in everyday discourse. Compounds are particu-
larly interesting from a language-acquisition per-
spective: children as young as two can comprehend
and produce noun-noun compounds (Clark & Bar-
ron, 1988), and these compounds play an important
role in adult acquisition of the new language and ter-
minology associated with particular domains of dis-
course. Indeed, most new terms entering the English
language are combinations of existing words (Can-
non, 1987; consider FLASH MOB, DESIGNER BABY,
SPEED DATING and CARBON FOOTPRINT).
These noun-noun compounds are also interest-
ing from a computational perspective, in that they
pose a significant challenge for current computa-
tional accounts of language. This challenge arises
from the fact that the semantics of noun-noun com-
pounds are extremely diverse, with compounds uti-
lizing many different relations between their con-
stituent words (consider the examples at the end of
the previous paragraph). Despite this diversity, peo-
ple typically interpret even completely novel com-
pounds extremely quickly, in the order of hundredths
of seconds in reaction time studies.
One approach that has been taken in both cog-
nitive psychology and computational linguistics can
be termed the relation-based approach (e.g. Gagne?
& Shoben, 1997; Kim & Baldwin, 2005). In this
approach, the interpretation of a compound is rep-
resented as the instantiation of a relational link be-
tween the modifier and head noun of the compound.
Such relations are usually represented as a set of
taxonomic categories; for example the meaning of
STUDENT LOAN might be specified with a POSSES-
SOR relation (Kim & Baldwin, 2005) or MILK COW
might be specified by a MAKES relation (Gagne? &
Shoben, 1997). However, researchers are not close
to any agreement on a taxonomy of relation cate-
gories classifying noun-noun compounds; indeed a
wide range of typologies have been proposed (e.g.
Levi, 1977; Kim & Baldwin, 2005).
In these relation-based approaches, there is often
little focus on how the meaning of the relation inter-
acts with the intrinsic properties of the constituent
concepts. Instead, extrinsic information about con-
cepts, such as distributional information about how
often different relations are associated with a con-
cept, is used. For example, Gagne? & Shoben?s
CARIN model utilizes the fact that the modifier
MOUNTAIN is frequently associated with the LO-
CATED relation (in compounds such as MOUNTAIN
CABIN or MOUNTAIN GOAT); the model does not
utilize the fact that the concept MOUNTAIN has in-
89
trinsic properties such as is large and is a geological
feature: features which may in general precipitate
the LOCATION relation.
An approach that is more typical of psycholog-
ical theories of compound comprehension can be
termed the concept-based approach (Wisniewski,
1997; Costello and Keane, 2000). With such the-
ories, the focus is on the intrinsic properties of
the constituent concepts, and the interpretation of a
compound is usually represented as a modification
of the head noun concept. So, for example, the com-
pound ZEBRA FISH may involve a modification of
the FISH concept, by asserting a feature of the ZE-
BRA concept (e.g. has stripes) for it; in this way, a
ZEBRA FISH can be understood as a fish with stripes.
Concept-based theories do not typically use distrib-
utional information about how various relations are
likely to be used with concepts.
The information assumed relevant to compound
interpretation is therefore quite different in relation-
based and concept-based theories. However, neither
approach typically deals with the issue of how peo-
ple acquire the information that allows them to in-
terpret compounds. In the case of the relation-based
approaches, for example, how do people acquire the
knowledge that the modifier MOUNTAIN tends to
be used frequently with the LOCATED relation and
that this information is important in comprehend-
ing compounds with that modifier? In the case of
concept-based approaches, how do people acquire
the knowledge that features of ZEBRA are likely to
influence the interpretation of ZEBRA FISH?
This paper presents an experiment which exam-
ines how both distributional information about re-
lations and intrinsic information about concept fea-
tures influence compound interpretation. We also
address the question of how such information is ac-
quired. Rather than use existing, real world con-
cepts, our experiment used laboratory generated
concepts that participants were required to learn dur-
ing the experiment. As well as learning the meaning
of these concepts, participants also built up knowl-
edge during the experiment about how these con-
cepts tend to combine with other concepts via re-
lational links. Using laboratory-controlled concepts
allows us to measure and control various factors that
might be expected to influence compound compre-
hension; for example, concepts can be designed to
vary in their degree of similarity to one another, to
be associated with potential relations with a certain
degree of frequency, or to have a feature which is
associated with a particular relation. It would be ex-
tremely difficult to control for such factors, or in-
vestigate the aquisition process, using natural, real
world concepts.
2 Experiment
Our experiment follows a category learning para-
digm popular in the classification literature (Medin
& Shaffer, 1978; Nosofsky, 1984). The experiment
consists of two phases, a training phase followed
by a transfer phase. In the training phase, partic-
ipants learned to identify several laboratory gener-
ated categories by examining instances of these cat-
egories that were presented to them. These cate-
gories were of two types, conceptual and relational.
The conceptual categories consisted of four ?plant?
categories and four ?beetle? categories, which par-
ticipants learned to distinguish by attending to dif-
ferences between category instances. The relational
categories were three different ways in which a bee-
tle could eat a plant. Each stimulus consisted of
a picture of a beetle instance and a picture of a
plant instance, with a relation occurring between
them. The category learning phase of our experi-
ment therefore has three stages: one for learning to
distinguish between the four beetle categories, one
for learning to distinguish between the four plant
categories, and one for learning to distinguish be-
tween the three relation categories.
The training phase was followed by a transfer
phase consisting of two parts. In the first part par-
ticipants were presented with some of the beetle-
plant pairs that they had encountered in the train-
ing phase together with some similar, though previ-
ously unseen, pairs. Participants were asked to rate
how likely each of the three relations were for the
depicted beetle-plant pair. This part of the transfer
phase therefore served as a test of how well partic-
ipants had learned to identify the appropriate rela-
tion (or relations) for pairs of conceptual category
exemplars and also tested their ability to generalize
their knowledge about the learned categories to pre-
viously unseen exemplar pairs. In the second part of
the transfer phase, participants were presented with
90
pairs of category names (rather than pairs of cat-
egory items), presented as noun-noun compounds,
and were asked to rate the appropriateness of each
relation for each compound.
In the experiment, we aim to investigate three is-
sues that may be important in determining the most
appropriate interpretation for a compound. Firstly,
the experiment aims to investigate the influence of
concept salience (i.e. how important to participants
information about the two constituent concepts are,
or how relevant to finding a relation that information
is) on the interpretation of compounds. For example,
if the two concepts referenced in a compound are
identical with respect to the complexity of their rep-
resentation, how well they are associated with vari-
ous alternative relations (and so on), but are of dif-
fering levels of animacy, we might expect the rela-
tion associated with the more animate concept to be
selected by participants more often than a different
relation associated equally strongly with the less an-
imate concept. In our experiment, all three relations
involve a beetle eating a plant. Since in each case the
beetle is the agent in the EATS(BEETLE,PLANT) sce-
nario, it is possible that the semantics of the beetle
concepts might be more relevant to relation selection
than the semantics of the plant concepts.
Secondly, the experiment is designed to inves-
tigate the effect of the ordering of the two nouns
within the compound: given two categories named
A and B, our experiment investigates whether the
compound ?A B? is interpreted in the same way as
the compound ?B A?. In particular, we were in-
terested in whether the relation selected for a com-
pound would tend to be dependent on the concept in
the head position or the concept in the modifier posi-
tion. Also of interest was whether the location of the
more animate concept in the compound would have
an effect on interpretation. For example, since the
combined concept is an instance of the head concept,
we might hypothesize that compounds for which the
head concept is more animate than the modifier con-
cept may be easier to interpret correctly.
Finally, were interested in the effect of concept
similarity: would compounds consisting of similar
constituent categories tend to be interpreted in simi-
lar ways?
learn trans. Nr Rel Bcat Pcat B1 B2 B3 P1 P2 P3
l 1 1 1 3 4 1 1 3 2 3
l 2 1 1 3 4 4 1 2 3 3
l t 3 1 1 3 1 1 1 3 3 2
l t 4 1 1 3 4 1 2 3 3 3
l t 5 2 2 2 2 2 2 2 2 3
l 6 2 2 2 2 2 1 2 3 2
l 7 2 2 2 2 3 2 2 2 1
l t 8 2 2 2 2 2 3 2 2 2
l t 9 3 3 1 3 3 3 4 1 2
l t 10 3 3 1 3 3 2 1 1 1
l 11 3 3 1 2 3 3 4 4 1
l 12 3 3 1 3 2 3 4 1 1
l t 13 1 4 4 1 1 4 4 4 4
l t 14 2 4 4 4 1 4 4 1 4
l t 15 3 4 4 4 4 4 1 1 4
t 16 - 1 1 4 1 1 4 1 1
t 17 - 3 3 3 3 3 3 3 3
t 18 - 2 4 2 2 2 4 1 4
t 19 - 4 2 4 1 4 2 2 2
Table 1: The experiment?s abstract category struc-
ture
2.1 Method
2.1.1 Participants
The participants were 42 university students.
2.1.2 Materials
The abstract category structure used in the exper-
iment is presented in Table 1. There are 19 items
in total; the first and second columns in the table
indicate if the item in question was one of the 15
items used in the learning phase of the experiment
(l) or as one of the 13 items used in the transfer stage
of the experiment (t). There were four beetle cate-
gories (Bcat), four plant categories (Pcat) and three
relation categories used in the experiment. Both the
beetle and plant categories were represented by fea-
tures instantiated on three dimensions (B1, B2 & B3
and P1, P2 & P3, respectively). The beetle and plant
categories were identical with respect to their ab-
stract structure (so, for example, the four exemplars
of Pcat1 have the same abstract features as the four
exemplars of Bcat1).
Beetles and plants were associated with particu-
lar relations; Bcat1, Bcat2 and Bcat3 were associ-
ated with Relations 1, 2 and 3, respectively, whereas
Pcat1, Pcat2 and Pcat3 were associated with Rela-
tions 3, 2 and 1, respectively. Bcat4 and Pcat4 were
not associated with any relations; the three exemplar
91
instances of these categories in the learning phase
appeared once with each of the three relations. The
features of beetles and plants were sometimes diag-
nostic of a category (much as the feature has three
wheels is diagnostic for TRICYCLE); for example, a
particular feature associated with Bcat1 is a 1 on the
B3 dimension: 3 of the 4 Bcat1 training phase exem-
plars have a 1 on dimension B3 while only one of the
remaining 11 training phase exemplars do. Also, the
intrinsic features of beetles and plants are sometimes
diagnostic of a relation category (much as the intrin-
sic feature has a flat surface raised off the ground is
diagnostic for the relational scenario sit on); values
on dimensions B1, P1, B2 and P2 are quite diag-
nostic of relations. Participants learned to identify
the plant, beetle and relation categories used in the
experiment by attending to the associations between
beetle, plant and relation categories and feature di-
agnosticity for those categories.
The beetle and plant categories were also de-
signed to differ in terms of their similarity. For ex-
ample, categories Bcat1 and Bcat4 are more simi-
lar to each other than Bcat3 and Bcat4 are: the fea-
tures for Bcat1 and Bcat4 overlap to a greater extent
than the features for Bcat3 and Bcat4 do. The aim
of varying categories with respect to their similarity
was to investigate whether similar categories would
yield similar patterns of relation likelihood ratings.
In particular, Bcat4 (and Pcat4) occurs equally often
with the three relations; therefore if category simi-
larity has no effect we would expect people to select
each of the relations equally often for this category.
However, if similarity influences participants? rela-
tion selection, then we would expect that Relation 1
would be selected more often than Relations 2 or 3.
The abstract category structure was mapped to
concrete features in a way that was unique for each
participant. Each beetle dimension was mapped ran-
domly to the concrete dimensions of beetle shell
color, shell pattern and facial expression. Each plant
dimension was randomly mapped to the concrete di-
mensions of leaf color, leaf shape, and stem color.
The three relations were randomly mapped to eats
from leaf, eats from top, and eats from trunk.
2.1.3 Procedure
The experiment consisted of a training phase and
a transfer phase. The training phase itself consisted
Figure 1: Example of a relation learning stimulus
of three sub-stages in which participants learned to
distinguish between the plant, beetle and relation
categories. During each training sub-stage, the 15
training items were presented to participants sequen-
tially on a web-page in a random order. Underneath
each item, participants were presented with a ques-
tion of the form ?What kind of plant is seen in this
picture??, ?What type of beetle is seen in this pic-
ture?? and ?How does this ?Bcat? eat this ?Pcat???
in the plant learning, beetle learning, and relation
learning training sub-stages, respectively (e.g. Fig-
ure 1). Underneath the question were radio but-
tons on which participants could select what they
believed to be the correct category; after participants
had made their selection, they were given feedback
about whether their guess had been correct (with the
correct eating relation shown taking place). Each of
the three substages was repeated until participants
had correctly classified 75% or more of the items.
Once they had successfully completed the training
phase they moved on to the transfer phase.
The transfer phase consisted of two stages, an
exemplar transfer stage and a compound transfer
stage. In the exemplar transfer stage, participants
were presented with 13 beetle-plant items, some of
which had appeared in training and some of which
were new items (see Table 1). Underneath each
picture was a question of the form ?How does this
?Bcat? eat this ?Pcat??? and three 5-point scales
for the three relations, ranging from 0 (unlikely) to
4 (likely).
The materials used in the compound transfer stage
of the experiment were the 16 possible noun-noun
92
compounds consisting of a beetle and plant category
label. Participants were presented with a sentence of
the form ?There are a lot of ?Pcat? ?Bcat?s around
at the moment.? and were asked ?What kind of eat-
ing activity would you expect a ?Pcat? ?Bcat? to
have??. Underneath, participants rated the likeli-
hood of each of the three relations on 5-point scales.
One half of participants were presented with the
compounds in the form ??Bcat? ?Pcat?? whereas
the other half of participants saw the compounds in
the form ??Pcat? ?Bcat??.
2.2 Results
2.2.1 Performance during training
Two of the participants failed to complete the
training phase. For the remaining 40 participants,
successful learning took on average 5.8 iterations of
the training items for the plant categories, 3.9 itera-
tions for the beetle categories, and 2.1 iterations for
the relation categories. The participants therefore
learned to distinguish between the categories quite
quickly, which is consistent with the fact that the cat-
egories were designed to be quite easy to learn.
2.2.2 Performance during the exemplar
transfer stage
Participants? mean ratings of relation likelihood
for the nine previously seen exemplar items is pre-
sented in Figure 2 (items 3 to 15). For each of these
items there was a correct relation, namely the one
that the item was associated with during training.
The difference between the mean response for the
correct relation (M = 2.76) and the mean response
for the two incorrect relations (M = 1.42) was sig-
nificant (ts(39) = 7.50, p < .01; ti(8) = 4.07,
p < .01). These results suggest that participants
were able to learn which relations tended to co-occur
with the items in the training phase.
Participants? mean ratings of relation likelihood
for the four exemplar items not previously seen in
training are also presented in Figure 2 (items 16 to
19). Each of these four items consisted of a proto-
typical example of each of the four beetle categories
and each of the four plant categories (with each bee-
tle and plant category appearing once; see Table 1
for details). For these four items there was no cor-
rect answer; indeed, the relation consistent with the
beetle exemplar was always different to the relation
Figure 2: Participants? mean responses for the ex-
emplar transfer items.
suggested by the plant exemplar. For each trial, then,
one relation is consistent with the beetle exemplar
(rb), one is consistent with the plant exemplar (rp)
and one is neutral (rn). One-way repeated measures
ANOVAs with response type (rb, rp or rn) as a fixed
factor and either subject or item as a random factor
were used to investigate the data. There was a signif-
icant effect of response type in both the by-subjects
and by-items analysis (Fs(2, 39) = 19.10, p < .01;
Fi(2, 3) = 24.14, p < .01). Pairwise differences be-
tween the three response types were investigated us-
ing planned comparisons in both the by-subject and
by-items analyses (with paired t-tests used in both
cases). The difference between participants? mean
response for the relation associated with the beetle
exemplar, rb (M = 2.68), and their mean response
for the neutral relation, rn (M = 1.44) was sig-
nificant (ts(39) = 5.63, p < .001; ti(3) = 5.34,
p = .01). These results suggest that participants
were strongly influenced by the beetle exemplar
when making their category judgments. However,
the difference between participants? mean response
for the relation associated with the plant exemplar,
rp (M = 1.62), and their mean response for the
neutral relation was not significant (ts(39) = 1.11,
p = .27; ti(3) = 0.97, p = .40). These re-
sults suggest that participants were not influenced
by the plant exemplar when judging relation like-
lihood. Since the beetle and plant categories have
identical abstract structure, these results suggest that
other factors (such as the animacy of a concept or the
role it plays in the relation) are important to interpre-
tation.
The data from all 13 items were also analysed
taken together. To investigate possible effects of cat-
93
egory similarity, a repeated measures ANOVA with
beetle category and response relation taken as within
subject factors and subject taken as a random fac-
tor was undertaken. There was a significant effect
of the category that the beetle exemplar belonged to
on participants? responses for the three relations (the
interaction between beetle category and response re-
lation was significant; F (6, 39) = 26.83, p < .01.
Planned pairwise comparisons (paired t-tests) were
conducted to investigate how ratings for the cor-
rect relation (i.e. the relation consistent with train-
ing) differed for the ratings for the other two rela-
tions. For Bcat1, Bcat2 and Bcat3, the ratings for
the relation consistent with learning was higher than
the two alternative relations (p < .01 in all cases).
However, for the Bcat4 items, there was no evi-
dence that participants we more likely to rate Re-
lation 1 (M = 2.09) higher than either Relation 2
(M = 1.97; t(39) = 0.54, p = .59) or Relation
3 (M = 1.91; t(39) = 0.69, p > .50). Though
the difference is in the direction predicted by Bcat4?s
similarity to Bcat1, there is no evidence that partici-
pants made use of Bcat4?s similarity to Bcat1 when
rating relation likelihood for Bcat4.
In summary, the results suggest that participants
were capable of learning the training items. Partici-
pants appeared to be influenced by the beetle exem-
plar but not the plant exemplar. There was some evi-
dence that conceptual similarity played a role in par-
ticipants? judgments of relation likelihood for Bcat4
exemplars (e.g. the responses for item 19) but over
all Bcat4 exemplars this effect was not significant.
2.2.3 Performance on the noun-noun
compound transfer stage
In the noun-noun compound transfer stage, each
participant rated relation likelihood for each of the
16 possible noun-noun compounds that could be
formed from combinations of the beetle and plant
category names. Category name order was a be-
tween subject factor: half of the participants saw the
compounds with beetle in the modifier position and
plant in the head position whilst the other half of
participants saw the reverse. First of all, we were
interested in whether or not the training on exem-
plar items would transfer to noun-noun compounds.
Another question of interest is whether or not par-
ticipants? responses would be affected by the order
in which the categories were presented. For exam-
ple, perhaps it is the concept in the modifier position
that is most influential in determining the likelihood
of different relations for a compound. Alternatively
perhaps it is the concept in the head position that is
most influential.
To answer such questions a 4?4?3?2 repeated
measures ANOVA with beetle category, plant cate-
gory and response relation as within subject factors
and category label ordering as a between subject fac-
tor was used to analyze the data. The interaction
between beetle category and response relation was
significant (F (6, 38) = 59.79, p < .001). There-
fore, the beetle category present in the compound
tended to influence participants? relation selections.
The interaction between plant category and response
relation was weaker, but still significant (F (6, 38) =
5.35, p < 0.01). Therefore, the plant category
present in the compound tended to influence partic-
ipants? relation selections. These results answer the
first question above; training on exemplar items was
transferred to the noun-noun compounds. However,
there were no other significant interactions found. In
particular, the interaction between category order-
ing, beetle category and response relation was not
significant (F (6, 38) = 1.82, p = .09). In other
words, there is no evidence that the influence of bee-
tle category on participants? relation selections when
the beetle was in the modifier position differed from
the influence of beetle category on participants? rela-
tion selections when the beetle was in the head-noun
position. Similarly, the interaction between noun or-
dering, plant category and response relation was not
significant (F (6, 38) = 0.68, p = .67); there is no
evidence that the influence of the plant category on
relation selection differed depending on the location
of the plant category in the compound.
Planned pairwise comparisons (paired t-tests)
were used to investigate the significant interactions
further: for Bcat1, Bcat2 and Bcat3, the ratings
for the relation consistent with learning was sig-
nificantly higher than the two alternative relations
(p < .001 in all cases). However, for Bcat4, there
were no significant differences between the ratings
for the three relations (p > .31 for each of the three
comparisons). For the plants, however, the only sig-
nificant differences were between the response for
Relation 1 and Relation 2 for Pcat2 (t(39) = 2.12,
94
p = .041) and between Relation 2 and Relation 3 for
Pcat2 (t(39) = 3.08, p = .004), although the dif-
ferences for Pcat1 and Pcat3 are also in the expected
direction.
In summary, the results of the noun-noun com-
pound stage of the experiment show that partici-
pants? learning of the relations and their associa-
tions with beetle and plant categories during training
transferred to a task involving noun-noun compound
interpretation. This is important as it demonstrates
how the interpretation of compounds can be derived
from information about how concept exemplars tend
to co-occur together.
2.3 Modelling relation selection
One possible hypothesis about how people decide
on likely relations for a compound is that the men-
tion of the two lexemes in the compound activates
stored memory traces (i.e. exemplars) of the con-
cepts denoted by those lexemes. Exemplars differ
in how typical they are for particular conceptual cat-
egories and we would expect the likelihood of an
exemplar?s activation to be in proportion to its typ-
icality for the categories named in the compound.
As concept instances usually do not happen in isola-
tion but rather in the context of other concepts, this
naturally results in extensional relational informa-
tion about activated exemplars also becoming acti-
vated. This activated relational information is then
available to form a basis for determining the likely
relation or relations for the compound. A strength
of this hypothesis is that it incorporates both inten-
sional information about concepts? features (in the
form of concept typicality) and also extrinsic, dis-
tributional information about how concepts tend to
combine (in the form of relational information asso-
ciated with activated exemplars). In this section, we
present a model instantiating this hybrid approach.
The hypothesis proposed above assumes that ex-
tensional information about relations is associated
with exemplars in memory. In the context of our
experiment, the extensional, relational information
about beetle and plant exemplars participants held in
memory is revealed in how they rated relational like-
lihood during the exemplar transfer stage of the ex-
1This is not significant if Bonferroni correction is used to
control the familywise Type I error rate amongst the multiple
comparisons
periment. For each of the 13 beetle and plant exem-
plars, we therefore assume that the average ratings
for each of the relations describes our participants?
knowledge about how exemplars combine with other
exemplars. Also, we can regard the three relation
likelihood ratings as being a 3-dimensional vector.
Given that category ordering did not appear to have
an effect on participants? responses in the compound
transfer phase of the experiment, we can calculate
the relation vector ~rB,P for the novel compounds ?B
P ? or ?P B? as
~rB,P =
?
e?U
(typ(eb, B) + typ(ep, P ))
? ? ~re
?
e?U
(typ(eb, B) + typ(ep, P ))
?
where e denotes one of the 13 beetle-plant ex-
emplar items rated in the exemplar transfer stage,
typ(eb, B) denotes the typicality of the beetle ex-
emplar present in item e in beetle category B and
typ(ep, P ) denotes the typicality of the plant exem-
plar present in item e in plant category P . U is
the set of 13 beetle-plant exemplar pairs and ? is a
magnification parameter to be estimated empirically
which describes the relative importance of exemplar
typicality.
In this model, we require a measure of how typical
of a conceptual category an exemplar is (i.e. a mea-
sure of how good a member of a category a partic-
ular category instance is). In our model, we use the
Generalized Context Model (GCM) to derive mea-
sures of exemplar typicality. The GCM is a success-
ful model of category learning that implements an an
exemplar-based account of how people make judg-
ments of category membership in a category learn-
ing task. The GCM computes the probability Pr of
an exemplar e belonging in a category C as a func-
tion of pairwise exemplar similarity according to:
Pr(e, C) =
?
i?C
sim(e, i)
?
i?U
sim(e, i)
where U denotes the set of all exemplars in mem-
ory and sim(e, i) is a measure of similarity between
exemplars e and i. Similarity between exemplars is
in turn defined as a negative-exponential transforma-
95
tion of distance:
sim(i, j) = e?cdist(i,j) (1)
where c is a free parameter, corresponding to how
quickly similarity between the exemplars diminishes
as a function of their distance. The distance between
two exemplars is usually computed as the city-block
metric summed over the dimensions of the exem-
plars, with each term weighted by empirically esti-
mated weighting parameters constrained to sum to
one. According to the GCM, the probability that
a given exemplar belongs to a given category in-
creases as the average similarity between the exem-
plar and the exemplars of the category increases; in
other words, as it becomes a more typical member
of the category. In our model, we use the proba-
bility scores produced by the GCM as a means for
computing concept typicality (although other meth-
ods for measuring typicality could have been used).
We compared the relation vector outputted by the
model for the 16 possible compounds to the rela-
tion vectors derived from participants? ratings in the
compound transfer phase of the experiment. The
agreement between the model and the data was high
across the three relations (for Relation 1, r = 0.84,
p < 0.01; for Relation 2, r = 0.90, p < 0.01; for
Relation 3, r = 0.87, p < 0.01), using only one free
parameter, ?, to fit the data2.
3 Conclusions
The empirical findings we have described in this pa-
per have several important implications. Firstly, the
findings have implications for relation-based theo-
ries. In particular, the finding that only beetle exem-
plars tended to influence relation selection suggest
that factors other than relation frequency are rele-
vant to the interpretation process (since the beetle
and plants in our experiment were identical in their
degree of association with relations). Complex inter-
actions between concepts and relations (e.g. agency
in the EATS(AGENT,OBJECT) relation) is informa-
tion that is not possible to capture using a taxonomic
approach to relation meaning.
Secondly, the fact that participants could learn to
identify the relations between exemplars and also
2In the GCM, c was set equal to 1 and the three dimensional
weights in the distance calculation were set equal to 1/3
transfer that knowledge to a task involving com-
pounds has implications for concept-based theories
of compound comprehension. No concept-based
theory of conceptual combination has ever adopted
an exemplar approach to concept meaning; mod-
els based on concept-focused theories tend to rep-
resent concepts as frames or lists of predicates. Our
approach suggests an exemplar representation is a
viable alternative. Also, distributional knowledge
about relations forms a natural component of an ex-
emplar representation of concepts, as different con-
cept instances will occur with instances of other con-
cepts with varying degrees of frequency. Given the
success of our model, assuming an exemplar repre-
sentation of concept semantics would seen to offer a
natural way of incorporating both information about
concept features and information about relation dis-
tribution into a single theory.
References
G. Cannon. 1987. Historical change and English word
formation. New York: Lang.
E. V. Clark and B.J. Barron. 1988. A thrower-button or
a button-thrower? Children?s judgments of grammati-
cal and ungrammatical compound nouns. Linguistics,
26:3?19.
F. J. Costello & M.T. Keane. 2000. Efficient creativity:
Constraint guided conceptual combination.. Cognitive
Science, 24(2):299?349.
C. L. Gagne? and E.J. Shoben. 1997. Influence of the-
matic relations on the comprehension of modifier noun
combinations. Journal of Experimental Psychology:
Learning, Memory, and Cognition, 23:71?78.
S. N. Kim and T. Baldwin. 2005. Automatic Interpreta-
tion of Noun Compounds Using WordNet Similarity.
Lecture Notes in Computer Science, 3651:945?956.
J. N. Levi. 1978. The Syntax and Semantics of Complex
Nominals. New York: Academic Press.
D. L. Medin & M.M. Schaffer. 1978. Context the-
ory of classification learning. Psychological Review,
85:207?238.
R. N. Nosofsky. 1984. Choice, similarity, and the con-
text theory of classification.. Journal of Experimen-
tal Psychology: Learning, Memory, and Cognition,
10(1):104?114.
E. J. Wisniewski 1997. When concepts combine. Psy-
chonomic Bulletin & Review, 4(2):167?183.
96
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 370?373,
Prague, June 2007. c?2007 Association for Computational Linguistics
UCD-FC: Deducing semantic relations using WordNet senses that occur
frequently in a database of noun-noun compounds ?
Fintan J. Costello,
School of Computer Science and Informatics, University College Dublin,
Belfield, Dublin 6, Ireland.
fintan.costello@ucd.ie
Abstract
This paper describes a system for classify-
ing semantic relations among nominals, as
in SemEval task 4. This system uses a
corpus of 2,500 compounds annotated with
WordNet senses and covering 139 different
semantic relations. Given a set of nomi-
nal pairs for training, as provided in the Se-
mEval task 4 training data, this system con-
structs for each training pair a set of features
made up of relations and WordNet sense
pairs which occurred with those nominals
in the corpus. A Naive Bayes learning al-
gorithm learns associations between these
features and relation membership categories.
The identification of relations among nomi-
nals in test items takes place on the basis of
these associations.
1 Introduction
This paper describes a system for deducing the
correct semantic relation between a pair of nom-
inals in a sentence, as in SemEval task 4 (Girju,
Hearst, Nakov, Nastase, Szpakowicz, Turney, &
Yuret, 2007). This system is an adaptation of an
existing system for deducing the correct semantic
relation between the pair of words in a noun-noun
compound. This compound disambiguation system
(named PRO, for Proportional Relation Occurrence;
see Costello, Veale, & Dunne, 2006) makes use of
? This research was supported by the FP6 NEST Pro-
gramme of the European Commission (ANALOGY: Humans
the Analogy-Making Species: STREP Contr. No 029088)
a corpus of 2,500 compounds annotated with Word-
Net senses and covering 139 different semantic re-
lations, with each noun and each relation annotated
with its correct WordNet sense.1 Section 2 of the pa-
per will describe the format and structure of this cor-
pus, Section 3 will describe the original PRO com-
pound disambiguation system, and Section 4 will
explain how the PRO system was adapted to deduce
the correct semantic relation between a pair of nom-
inals, as in SemEval task 4. Four different versions
of the adapted system were produced (versions A,B,
C and D), either using or not using the WordNet la-
bels and the Query labels provided with training and
test items in SemEval task 4. Section 5 discusses the
performance of these different versions of the sys-
tem. Finally, Section 6 finishes the paper with some
discussion and ideas for future work.
2 A Corpus of Annotated Compounds
Using WordNet (Miller, 1995), version 2.0, a cor-
pus of noun-noun compounds was constructed such
that each compound was annotated with the correct
WordNet noun senses for constituent words, the cor-
rect semantic relation between those words, and the
correct WordNet verb sense for that relation, as de-
scribed below.
2.1 Corpus Procedure
The compounds used in this corpus were selected
from the set of noun-noun compounds defined in
WordNet. Compounds from WordNet were used
because each compound had an associated gloss or
1A file containing this corpus is available for download from
http://inismor.ucd.ie/?fintanc/wordnet compounds
370
definition explaining the relation between the words
in that compound (compounds from other sources
would not have such associated definitions). Also,
using compounds from WordNet guarantees that all
constituent words of those compounds would also
have entries in WordNet. An initial list of over
40,000 two-word noun-noun compounds was ex-
tracted from WordNet 2.0. From this list a random
subset was selected. From that set al compounds
using scientific latin (e.g. ocimum basilicum), id-
iomatic compounds (e.g. zero hour), compounds
containing proper nouns (e.g. Yangtze river), non-
english compounds (e.g. faux pas), and chemical
terminology (e.g. carbon dioxide) were excluded.
The remaining compounds were placed in random
order, and a research assistant annotated each with
the WordNet noun senses of the constituent words,
the semantic relation between those words, and the
WordNet verb sense of that relation. A web page
was created for this annotation task, showing the an-
notator the compound to be annotated and the Word-
Net gloss (meaning) for that compound. This page
also showed the annotator the list of WordNet senses
for the modifier noun and head noun in the com-
pound, allowing the annotator to select the correct
sense for each word. After word-sense selection an-
other page was presented allowing the annotator to
identify the correct semantic relation for that com-
pound and to select the correct WordNet sense for
the verb in that relation.
2.2 Corpus Results
Word sense, relation, and relation sense information
was gathered for 2,500 compounds. Relation occur-
rence was well distributed across these compounds:
there were 139 different relations used in the corpus.
Note that in SemEval task 4, the number of relation
categories available was much smaller than the set
of relation categories available in our corpus (just 7
relation categories in the SemEval task).
3 Compound Disambiguation Algorithm
This section presents the ?Proportional Relation Oc-
currence? (PRO) algorithm which makes use of the
corpus results described above to deduce seman-
tic relations for noun-noun compounds. In Section
4 this algorithm is adapted to deduce relations be-
Preconditions:
The entry for each compound C in corpus D contains:
CmodList = sense + hypernym senses for modifier of C;
CheadList = sense + hypernym senses for head of C;
Crel = semantic relation of C;
Input:
X = compound for which a relation is required;
modList = sense + hypernym senses for modifier of X;
headList = sense + hypernym senses for head of X;
finalRelationList = ();
finalPairList = ();
Begin:
1 for each modifier sense M ?modList
2 for each head sense H ? headList
3 relCount = ();
4 matchCount = 0;
5 P = (M,H);
6 for each compound C ? corpus D
7 if ((M ? CmodList) and (H ? CheadList))
8 relCount[Crel] = relCount[Crel] + 1;
9 matchCount = matchCount + 1;
10 for each relation R ? relCount
11 score = relCount[R]/matchCount;
12 prevScore = finalRelationList[R];
13 if (score > prevScore)
14 finalRelationList[R] = score;
15 if (score > pairScore)
16 finalPairList[P ] = score;
17 sort finalRelationList by score ;
18 sort finalPairList by score ;
19 return (finalRelationList, finalPairList);
End.
Figure 1: PRO disambiguation algorithm.
tween nominals in SemEval task 4.
The approach to compound disambiguation taken
here is similar to that taken by for example Kim &
Baldwin (2005) and Girju, Moldovan, Tatu, & An-
tohe (2005), and works by finding other compounds
containing words from the same semantic categories
as the words in the compound to be disambiguated:
if a particular relation occurs frequently in those
other compounds, that relation is probably also the
correct relation for the compound in question. We
take WordNet senses to represent semantic cate-
gories. Once the correct WordNet sense for a word
has been identified, that word can placed in a set of
nested semantic categories: the category represented
by that sense, by the parent sense (or hypernym) of
that sense, the parent of that parent, and so on up to
the (notional) root sense of WordNet.
Figure 1 shows the algorithm in pseudocode. The
algorithm uses the corpus of annotated noun-noun
371
compounds and, to disambiguate a compound, takes
as input the correct WordNet sense for the modifier
and head words of that compound (if known) plus
all hypernyms of those senses. If modifier and head
word senses are not known, the most frequent senses
for those words are used, plus all hypernyms of those
senses. The algorithm pairs each modifier sense with
every head sense. For each sense-pair, the algorithm
goes through the corpus of compounds and extracts
every compound whose modifier sense (or a hyper-
nym of that sense) is equal to the modifier sense in
the current sense-pair, and whose head sense (or a
hypernym of that sense) is equal to the head sense in
that pair. The algorithm counts the number of times
each relation occurs in that set of compounds, and
assigns each relation a Proportional Relation Occur-
rence (PRO) score for that pair, equal to the condi-
tional probability of relation R given sense-pair S.
If the PRO score for relation R in the current
sense-pair is greater than the score obtained for R
with some other pair, the current score is recorded
for R. If the score for R for the current pair P is
greater than any previous score obtained for P , that
score is recorded for P . In this way the algorithm
finds the maximum score for each relation R across
all sense-pairs, and the maximum score for each pair
P across all relations. The algorithm returns a list of
relations and of sense-pairs for the compound, both
sorted by score. The relations and sense-pairs with
the highest scores are those most likely to be correct
for that compound and to be most important for its
relational meaning.
In Costello, Veale and Dunne (2006), this algo-
rithm was tested by applying it to the annotated cor-
pus using a leave-one-out approach. These tests
showed a reliable relationship between PRO score
and accuracy of response. At a PRO level of 1, the
algorithm return a response (selects a relation) for
just over 900 compounds, and approximately 850 of
those responses are correct (the algorithm?s preci-
sion at this level is 0.92).
4 Adapting to the SemEval 4 task
To apply the PRO algorithm to the training and test
sentences in SemEval task 4 first required a mapping
from the labels used to tag nominals in that task (la-
bels e1 and e2) to the modifier and head categories
used by the PRO algorithm. To carry out this map-
ping the nominal whose label appeared in the first
position in a relation tag was taken to be the modi-
fier for that relation, and that in the second position
was taken to be the head; for example, with the rela-
tion tag CONTAINER-CONTENT(E1,E2) the nomi-
nal e1 would be taken to be the modifer and e2 to
be the head. Given this mapping the PRO algorithm
could be applied to sentences from SemEval task 4,
taking modifier and head nominals as input and pro-
ducing as output lists of candidate relations and rel-
evant sense pairs (sorted by PRO score).
The relations produced by the PRO algorithm do
not correspond to the 7 relations in SemEval task
4. To make predictions about the 7 SemEval rela-
tions, the scored relation lists and sense-pair lists
returned by the PRO algorithm were used as fea-
tures for a straightforward Naive Bayes learning al-
gorithm, as implemented in the Perl module Algo-
rithm::NaiveBayes. For each sentence in a training
set in SemEval task 4, the PRO algorithm was ap-
plied to produce a list of relations and sense pairs
describing that sentence. Each relation and each
sense pair in this list has an associated PRO score,
and Naive Bayes was trained on these features of all
members of the training set, and then applied to test
set sentences to produce predictions about each sen-
tence?s membership or non-membership in the rela-
tion in question.
Version A of the system used neither the WordNet
sense tags nor the Query labels provided with the 7
relation categories used. Instead of using WordNet
senses for the input words the system simply used
the first (most frequent) noun senses for those words,
and proceeded as described above. Version B used
WordNet sense tags. Versions C and D of the system
used either the first WordNet sense or the provided
sense tags, coupled with the query terms used in the
SemEval task. An additional module in the system
was intended to make use of these query terms in
relation classification by comparing the query term
of the sentence to be classified with query terms in
positive or negative training examples of that rela-
tion, and making a decision based on that compari-
son. Unfortunately, due to an error this query term
module was not activated in the submitted runs, so
the results from versions C and D are the same as
from A and B.
372
Table 1: F-Score results by relation and run.
relation A4 B4 C4 D4
Cause-Effect 72.1 65.1 72.1 65.1
Instrument-Agency 69.8 58.1 69.8 58.1
Product-Producer 73.1 73 73.1 73
Origin-Entity 43.1 42.3 43.1 42.3
Theme-Tool 50 49.2 50 49.2
Part-Whole 71.7 75 71.7 75
Content-Container 73.8 59.4 73.8 59.4
Avg 64.8 60.3 64.8 60.3
5 SemEval 4 task results
Table 1 shows the results returned for the PRO sys-
tem for training run 4 (using all 140 training items in
each relation) for the four possible runs A, B, C and
D. Due to the error in activating the query term mod-
ule, columns C4 and D4 are identical to columns A4
and B4. There are two notable aspects of the results
in Table 1. First, the system?s performance was bet-
ter for run A4 (that did not useWordNet senses) than
for B4 (using WordNet senses). Indeed, the system
came first out of 6 systems which took part in the
A4 run. This was surprising: it had been expected
that using the correct WordNet senses for nominals
would improve the system?s performance. Analy-
sis revealed that A4 runs using most frequent Word-
Net senses provided more matches with entries in
the compound corpus the B4 run using the correct
WordNet senses. This may explain why the system
gave a better performance for A4 than B4.
The second interesting aspect of Table 1 is the
variation of the system?s responses across the dif-
ferent relation categories. For the two relations
?Origin-Entity? and ?Theme-Tool? the system has an
F-score of 50 or less, while for the other five rela-
tions the system?s F-score is around 70. It is not as
yet clear why the system performed so poorly for
these relations: further investigation is needed to ex-
plain this curious pattern.
6 Conclusions
This paper has described a system for automatically
seslecting relations between nominals which uses
the PRO algorithm and compound corpus to form
features for pairs of nominals (consisting of can-
didate relations and sense-pairs co-occurring with
those relations), and uses a Naive Bayes algorithm
to learn to identify relations between nominals from
those features. The system performs best using the
most frequent WordNet senses for those nominals,
suggesting that the system may work usefully in de-
ducing semantic relations between nominals with-
out the need to deduce word senses. However, the
system?s performance does not seem particularly
impressive or suitable for application to real-world
tasks as yet. The system?s best performance repre-
sents an accuracy of 66% across relations: in other
words, the system gets 1 in three relations wrong in
the SemEval task.
There is one very obvious area for improvement in
the system described here. Currently the system uses
a simple Naive Bayes algorithm for learning associ-
ations between features and relation categories. A
more sophisticated approach (using Support Vector
Machines, for example) would be likely to improve
the systsem?s performance noticably. The conver-
sion of the system to use some form of SVM should
not be difficult. A more difficult problem, however,
is to address the system?s poor performance on some
relations. This is currently difficult to understand,
and represents a serious flaw in the system. Resolv-
ing this problem may reveal some useful aspects of
the structure of different sorts of semantic relations
between nominals.
References
F. J. Costello, T. Veale and S. Dunne. 2006. Using WordNet
to Automatically Deduce Relations between Words in Noun-
Noun Compounds. In Proceedings of the COLING/ACL
2006 Main Conference, pp 160?167.
R. Girju, M. Hearst, P. Nakov, V. Nastase, S. Szpakowicz,
P. Turney and D. Yuret. 2007. Classification of Semantic
Relations between Nominals: Dataset for Task 4 in SemEval
2007. 4th International Workshop on Semantic Evaluations,
June 23-24, Prague, Czech Republic.
R. Girju, D. Moldovan, M. Tatu, and D. Antohe. 2005. On
the semantics of noun compounds. Computer Speech and
Language 19, 4, 479?496.
S. N. Kim and T. Baldwin. Automatic Interpretation of Noun
Compounds using WordNet::Similarity. 2nd Internationl
Joint Conference on Natual Language Processing, Korea,
2005.
G. Miller. 1995. WordNet: A lexical database. Communica-
tion of the ACM, 38(11), 39?41.
373
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 58?63,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Using Lexical Patterns in the Google Web 1T Corpus to Deduce
Semantic Relations Between Nouns
Paul Nulty Fintan Costello
School of Computer Science and Informatics School of Computer Science and Informatics
University College Dublin, Belfield University College Dublin, Belfield
 Dublin 4, Ireland  Dublin 4, Ireland
paul.nulty@ucd.ie fintan.costello@ucd.ie
Abstract
This paper investigates methods for using lexical pat-
terns in a corpus to deduce the semantic relation that 
holds between two nouns in a noun-noun compound 
phrase such as ?flu virus? or ?morning exercise?. Much 
of the previous work in this area has used automated 
queries to commercial web search engines. In our exper-
iments we use the Google Web 1T corpus. This corpus 
contains every 2,3, 4 and 5 gram occurring more
than 40 times in Google's index of the web, but has the
advantage of being available to researchers directly
rather than through a web interface. This paper evalu-
ates the performance of the Web 1T corpus on the task 
compared to similar systems in the literature, and also 
investigates what kind of lexical patterns are most in-
formative when trying to identify a semantic relation
between two nouns.
1 Introduction
Noun-noun combinations occur frequently in many
languages, and the problem of semantic disambig-
uation of these phrases has many potential applica-
tions  in  natural  language  processing  and  other 
areas. Search engines which can identify the rela-
tions between nouns may be able to return more 
accurate  results.  Hand-built  ontologies  such  as 
WordNet at present only contain a few basic se-
mantic  relations  between  nouns,  such  as  hyper-
nymy and meronymy. 
If the process of discovering semantic relations
from  text  were  automated,  more  links  could 
quickly be built up. Machine translation and ques-
tion-answering  are  other  potential  applications.  
Noun compounds are very common in English, es-
pecially  in  technical  documentation  and  neolo-
gisms. Latin languages tend to favour prepositional 
paraphrases instead of direct compound translation, 
and to select the correct preposition it is often
necessary to know the semantic relation. One very 
common approach to this problem is to define a
set of semantic relations which capture the interac-
tion between the modifier and the head noun, and 
then attempt to assign one of these semantic rela-
tions to each noun-modifier pair. For example, the 
phrase flu virus could be assigned the semantic re-
lation causal (the virus causes the flu); the relation 
for desert wind could be location (the storm is loc-
ated in the desert). 
There is no consensus as to which set of semantic 
relations best captures the differences in meaning 
of various noun phrases. Work in theoretical lin-
guistics has suggested that noun-noun compounds 
may be formed by the deletion of a predicate verb 
or preposition (Levi 1978). However, whether the 
set of possible predicates numbers 5 or 50, there 
are  likely to  be  some  examples  of  noun phrases 
that fit into none of the categories and some that fit 
in multiple categories.
2 Related Work
The idea of searching a large corpus for specific 
lexicosyntactic phrases to indicate a semantic rela-
tion  of  interest  was  first  described  by  Hearst 
(1992).  Lauer  (1995)  tackled the  problem of  se-
mantically disambiguating noun phrases by trying 
to find the preposition which best describes the re-
lation  between  the  modifier  and  head  noun.  His 
method  involves  searching  a  corpus  for  occur-
rences paraphrases of the form ?noun preposition 
modifier?. Whichever preposition is most frequent 
in this context is chosen to represent the predicate 
58
of the nominal, which poses the same problem of 
vagueness  as  Levi's  approach.  Lapata  and Keller 
(2005)  improved  on  Lauer's  results  on  the  same 
task by using the web as a corpus.
Turney  and  Littman  (2005)  used  queries  to  the 
AltaVista search engine as the basis for their learn-
ing  algorithm.  Using  the  dataset  of  Nastase  and 
Szpakowicz (2003), they experimented with a set 
of 64 short prepositional and conjunctive phrases 
they call ?joining terms? to generate exact queries 
for AltaVista of the form ?noun joining term mod-
ifier?, and ?modifier joining term noun?. These hit 
counts  were  used  with  a  nearest  neighbour  al-
gorithm to assign the noun phrases semantic rela-
tions. 
Nakov and Hearst (2006) present a system that dis-
covers verbs that characterize the relation between 
two  nouns in a compound. By writing structured 
queries  to a web search engine and syntactically 
parsing  the  returned  'snippet',  they  were  able  to 
identify verbs that were suitable predicates. For ex-
ample, for the compound neck vein, they retrieved 
verbs  and  verb-preposition  such  as  predicates 
emerge from, pass through, terminate in, and oth-
ers. However, their evaluation is qualitative; they 
do not attempt to use the verbs directly to categor-
ize a compound as a particular semantic relation. 
Turney  (2006)  examines  similarity  measures  for 
semantic relations. He notes that there are at least 
two  kinds  of  similarity:  attributional  similarity, 
which applies between words, and relational simil-
arity, which holds between pairs of words. 
Words that have a high attributional similarity are 
known as synonyms; e.g. chair and stool. When the 
relations in each of two pairs of words are similar, 
it is said that there is an analogy between the two 
pairs of words, e.g. stone:mason, carpenter:wood.
Turney points out that word pairs with high rela-
tional similarity do not necessarily contain words 
with high attributional similarity. For example, al-
though the relations are similar in traffic:street and 
water:riverbed, water is not similar to traffic, nor 
street similar to riverbed. 
Therefore, a measure of similarity of semantic rela-
tions allows a more reliable judgment of analogy 
than the first-order similarity of the nouns
3 Motivation
When  looking  for  lexical  patterns  between  two 
nouns, as is required with vector-space approaches, 
data  sparseness  is  a  common  problem.  To  over-
come this, many of the best-performing systems in 
this area rely on automated queries to web search-
engines  (Lapata  and  Keller  (2005),  Turney  and 
Littman  (2005),  Nakov  and  Hearst  (2006)).  The 
most  apparent  advantage  of  using  search-engine 
queries is simply the greater volume of data avail-
able. 
Keller and Lapata (2003) demonstrated the useful-
ness of this extra data on a type of word-sense dis-
ambiguation test and also found that web frequen-
cies of bigrams correlated well with
frequencies in a standard corpus. 
Kilgarriff  (2007)  argues  against  the  use  of  com-
mercial  search engines  for  research,  and outlines 
some  of  the  major  drawbacks.  Search  engine 
crawlers  do  not  lemmatize  or  part-of-speech  tag 
their text. This means that to obtain frequencies for 
may different inflectional forms, researchers must 
perform a  separate  query for  each possible  form 
and sum the results. 
 If part-of-speech tagging is required, the 'snippet' 
of  text  that  is  returned  with  each  result  may  be 
tagged after the query has been executed, however 
the APIs for the major search engines have limita-
tions on how many snippets may be retrieved for a 
given query (100 -1000).
Another problem is that search engine query syn-
tax is  limited,  and sometimes  mysterious.  In  the 
case of Google, only basic boolean operators are 
supported (AND, OR, NOT), and the function of 
the wildcard symbol (*) is limited, difficult to de-
cipher and may have changed over time. 
Kilgarriff also points out that the search API ser-
vices to the major search engines have constraints 
on the number of searches that are allowed per user 
per day. Because of the multiple searches that are 
needed to cover inflectional  variants and recover 
snippets for  tagging,  a limit  of  1000 queries per 
day, as with the Google API, makes experimenta-
tion slow. This paper will describe the use of the 
Web 1T corpus, made available by Google in 2006 
(Brants and Franz 2006). This corpus consists of n-
grams collected from web data, and is available to 
researchers  in  its  entirety,  rather  than  through  a 
web search interface. This means that there is no 
59
limit  to the amount of searches that may be per-
formed, and an arbitrarily complex query syntax is
possible. 
Despite being available since 2006, few research-
ers have made use of the Web 1T corpus. Hawker 
(2006) provides an example of using the corpus for 
word sense documentation, and describes a method 
for  efficient  searching.  We  will  outline  the  per-
formance of the corpus on the task of identifying 
the semantic relation between two nouns. Another 
motivation behind this paper is to examine the use-
fulness of different lexical patterns for the task of 
deducing semantic relations.
 In this paper, we are interested in whether the fre-
quency with which a joining term occurs between 
two nouns is related to how it indicates a semantic 
interaction. This is in part motivated by Zipf?s the-
ory which states that the more frequently a word 
occurs in a corpus the more meanings or senses it 
is  likely to  have  (Zipf  1929).  If  this  is  true,  we 
would expect that very frequent prepositions, such 
as ?of?, would have many possible meanings and 
therefore not reliably predict a semantic relation.
However, less frequent prepositions, such as ?dur-
ing? would have a more limited set of senses and 
therefore  accurately  predict  a  semantic  relation. 
Zipf also showed that the frequency of a term is re-
lated  to  its  length.  We  will  investigate  whether 
longer lexical patterns are more useful at identify-
ing semantic relations than shorter patterns, and
whether less frequent patterns perform better than 
more frequent ones.
4 Web 1T Corpus
The Web1T corpus consists of n-grams taken from
approximately  one  trillion  words  of  English  text 
taken from web pages in Google's  index of web 
pages. The data includes all 2,3,4 and 5-grams that 
occur more than 40 times in these pages. The data 
comes  in  the  form  of  approximately  110  com-
pressed files for each of the window sizes. Each of 
these files consists of exactly 10 million n-grams, 
with their frequency counts. Below is an example 
of the 3-gram data:
ceramics collection and 43
ceramics collection at 52
ceramics collection is 68
ceramics collection | 59
ceramics collections , 66
ceramics collections . 60
The uncompressed 3-grams,  4-grams 5-grams to-
gether take up 80GB on disk. In order to make it 
possible to index and search this data, we excluded 
n-grams that contained any punctuation or non-al-
phanumeric characters. We also excluded n-grams 
that contained any uppercase letters,  although we 
did allow for the first letter of the first word to be 
uppercase. 
We indexed the data using Ferret, a Ruby port of 
the Java search engine package Lucene. We were 
able to index all of the data in under 48 hours, us-
ing 32GB of hard disk space. The resulting index 
was searchable by first word, last word, and inter-
vening pattern. Only n-grams with a frequency of 
40 or higher are included in the dataset, which ob-
viously means that an average query returns fewer 
results than a web search. However, with the data 
available  on  local  disk  it  is  stable,  reliable,  and 
open to any kind of query syntax or lemmatization.
5 Lexical Patterns for Disambiguation
Modifier-noun phrases are often used interchange-
ably with paraphrases which contain the modifier 
and  the  noun  joined  by  a  preposition  or  simple 
verb. For example, the noun-phrase ?morning exer-
cise? may be paraphrased as ?exercise in the morn-
ing? or ?exercise during the morning?. In a very 
large corpus, it is possible to find many reasonable 
paraphrases  of  noun  phrases.  These  paraphrases 
contain information about the relationship
between the modifier and the head noun that is not 
present in the bare modifier-noun phrase. By ana-
lyzing these paraphrases, we can deduce what se-
mantic  relation  is  most  likely.  For  example,  the 
paraphrases  ?exercise  during  the  morning?  and 
?exercise in the morning? are likely to occur more 
frequently  than  ?exercise  about  the  morning?  or 
?exercise at the morning?. 
One  method  for  deducing  semantic  relations 
between words  in  compounds  involves  gathering 
n-gram frequencies of these paraphrases, contain-
ing a noun,  a modifier  and a lexical  pattern that 
links  them.  Some algorithm can then be used to 
map from lexical patterns to frequencies to semant-
60
ic relations and so find the correct relation for the 
compound in question. This is the approach we use 
in our experiments.
 In order to describe the semantic relation between 
two  nouns  in  a  compound  ?noun1  noun2?  we 
search for ngrams that begin with  noun2  and end 
with noun1, since in English the head of the noun 
compound is the second word. For example, for the 
compound 'flu virus', we look at n-grams that begin 
with 'virus' and end with 'flu'. We extract the words 
that occur between the two nouns (a string of 1-3 
words) and use these lexical patterns as features for 
the machine learning algorithm. 
For  each  compound  we  also  include  n-grams 
which have the plural form of noun1 or noun2. We 
assign a score to each of these lexical patterns, as 
the log of the frequency of the n-gram. We used 
the 400 most frequent lexical patterns extracted as 
the features for the model. Below are examples of 
some of the lexical patterns that were extracted:
and
of the
of
in the
for
and the
for the
to the
with
in
or
on the
from the
the
to
of a
with the
on
that the
from
Figure 1: The 20 most frequent patterns
The simplest way to use this vector space model to 
classify noun-noun combinations is  to use a dis-
tance metric to compare a novel pair of nouns to 
ones previously annotated with semantic relations. 
Nulty  (2007)  compares  these  nearest  neighbor 
models with other machine learning techniques and 
finds that using a support vector machine leads to 
improved classification.
In our experiments we used the support vector ma-
chine and k-nearest-neighbor algorithms from the 
WEKA machine learning toolkit. All experiments 
were conducted using leave-one-out cross valida-
tion: each example in the dataset is in turn tested 
alone, with all the other examples used for training. 
The first dataset used in these experiments was cre-
ated by Nastase and Szpackowicz (2003) and used 
in experiments by Turney and Littmann (2005) and 
Turney  (2006).  The  data  consists  of  600  noun-
modifier  compounds.  Of  the  600 examples,  four 
contained hyphenated modifiers, for example ?test-
tube baby?. These were excluded from our dataset, 
leaving 596 examples. The data is labeled with two 
different sets of semantic relations: one set of 30 
relations with fairly specific meanings and another 
set of 5 relations with more abstract relations. In 
these experiments  we use only the set  of  5 rela-
tions. The reason for this is that splitting a set of 
600 examples into 30 classes results in few training 
examples per class. This problem is compounded 
by the fact that the dataset is uneven, with far more 
examples in some classes than in others. Below are 
the five relations and some examples.
Relation: Example:
causal flu virus, onion tear
temporal summer travel, night class
spatial west coast, home remedy
participant mail sorter, blood donor
quality rice paper, picture book
Figure 2: Example phrases and their semantic relations
For our research we are particularly interested in 
noun-noun combinations. Of the 596 examples in 
the dataset, we found that 325 were clearly noun-
noun  combinations,  e.g.?picture  book?,  rice 
paper?, while in the remainder the modifier was an 
adjective, for example ?warm air?, ?heavy storm?. 
We used only the noun-noun combinations in our
experiments,  as this  is  the focus of  our research. 
We experimented with both lemmatization of the 
data and excluding semantically empty stop words 
(determiners  and  conjunctions)  from  the  lexical 
patterns,  however  neither  of  these  methods  im-
proved  performance.  Below  are  the  results  ob-
tained with the k-nearest neighbor algorithm. The
optimum value of k was 3.
Precision Recall f-score class
.442 .452 .447 Quality
.75 .444 .558 Temporal
.243 .167 .198 Causal
.447 .611 .516 Participant
.571 .138 .222 Spatial
 Figure 3: Results using the K-NN algorithm
The overall accuracy was 44% and the macro-aver-
aged f-value was .39.
61
Below are the results obtained using the support-
vector machine algorithm:
Precision Recall f-score class
.725 .345 .468 Quality
.733 .407 .524 Temporal
.545 .111 .185 Causal
.472 .885 .615 Participant
.462 .207 .268 Spatial
 Figure 4: Results using the Support Vector Machine
 The  overall  accuracy  was  51.7% and  the  mac-
roaveraged  f-value  was  .42.  A  majority  class 
baseline
(always predicting the largest class) would achieve 
an accuracy of 43.7%.
6 Which Lexical Patterns are Most Use-
ful?
In addition to evaluating the Google Web 1T cor-
pus,  a  motivation for this  paper is  to investigate 
what  kind of lexical  patterns are most  useful  for 
deducing semantic relations. In order to investigate 
this, we repeated the experiment one using the 3-
grams,  4-grams  and  5-grams  separately,  which 
gave lexical patterns of length 1, 2 and 3 respect-
ively. Accuracy obtained using the support vector
machine and k-nearest-neighbor algorithms are be-
low:
3-grams 4grams 5-grams All
KNN 36 42.5 42.4 44
SVM 44.3 49.2 43.4 51.7
 Figure 5: Results for different sizes of lexical patterns
Again, in each case the support vector machine
performs  better  than  the  nearest  neighbor  al-
gorithm. The 4- grams (two-word lexical patterns) 
give the best performance. One possible explana-
tion for this is that the single word lexical patterns 
don't convey a very specific relation, while the 3 
word  patterns  are  relatively  rare  in  the  corpus, 
leading  to  many  missing  values  in  the  training 
data. 
We were also interested in how the frequency of 
the lexical patterns related to their ability to predict 
the correct semantic relation. To evaluate this, we 
ordered the 400 lexical patterns retrieved by fre-
quency and then split them into three groups. We 
took  the  64  most  frequent  patterns,  the  patterns 
ranked  100-164  in  frequency,  and  those  ranked 
300-364. We chose to include 64 patterns in each 
group to  allow for  comparison  with  Turney and 
Littman  (2001),  who use  64 hand-generated  pat-
terns. Examples of the most frequent patterns are 
shown in Fig 1.  Below are  examples  of  patterns 
from the other two groups.
as well as
out of the
of one
of fresh
into
for all
was
with your
related to the
in the early
my
on Friday
without
which the
with my
and their
around the
when
whose
during
Figure 6: Frequency Ranks 100-120
to produce
but
that cause
of social
while the
or any other
such as the
are in the
to provide
if a
from one
one
provides
from your
of edible
levels and
comes from
chosen by the
producing
does not
than the
belonging to the
Figure 7: Frequency Ranks 300-320
The accuracies obtained using patterns in the dif-
ferent frequency groups are shown below.
1-64 100-164 300-364
KNN 40.9 43.5 41.9
SVM 47.6 45.2 41.5
 Figure 8: Results for different frequency bands of pat-
terns
Although there is no large effect to the accuracy of 
the KNN algorithm, the Support Vector Machine 
seems to perform better with the most frequent pat-
terns. One possible explanation for this is that al-
though the  less  frequent  patterns  seem more  in-
formative, they more often result in zero matches 
in the corpus, which simply leaves a missing value 
in the training data.
62
7 Conclusion
This paper reports several experiments on the se-
mantic disambiguation of noun-noun phrases using 
the Google Web 1T corpus, and shows that the res-
ults are comparable to previous work which has re-
lied on a web interface to search engines. Having a 
useful corpus based on web data that can be stored 
and  searched  locally  means  that  results  will  be 
stable across time and can be subject to complex 
queries. Experiments designed to evaluate the use-
fulness  of  different  lexical  patterns  did not  yield 
strong results and further work is required in this 
area.
References 
Thorsten Brants and Alex Franz. 2006. Web 1T 5-gram
Corpus  Version  1.1.  Technical  report,  Google  Re-
search
Tobias Hawker. 2006. Using Contexts of One Trillion
 Words for WSD. Proceedings of the 10th Conference  
of the Pacific Association for Computational
Linguistics, pages 85?93.
Marti A. Hearst: 1992. Automatic Acquisition of
Hyponyms  from  Large  Text  Corpora.  COLING:
539-545
Keller, Frank and Mirella Lapata. 2003. Using the 
     Web to Obtain Frequencies for Unseen Bigrams
      Computational Linguistics 29:3, 459-484. 
Adam Kilgarriff, 2007. Googleology is Bad Science.
Comput. Linguist. 33, 1 147-151.
Lapata, Mirella and Frank Keller. 2005. Web Based
    Models for Natural Language Processing. ACM 
    Transactions on Speech and Language
    Processing 2:1, 1-31. 
Mark Lauer. Designing Statistical Language Learners:
     Experiments on Noun Compounds. PhD thesis,
    Macquarie University NSW 2109 Australia.
Judith Levi. (1978) The Syntax and Semantics of 
    Complex Nominals, Academic Press, New York, NY.
Phil Maguire (2007) A cognitive model of conceptual
    combination Unpublished PhD Thesis, UCD Dublin
Preslav Nakov and Marti Hearst. 2006. Using Verbs to
    Characterize Noun-Noun Relations, in the 
    Proceedings of AIMSA 2006,
Preslav Nakov and Marti Hearst. 2005. Using the Web
    as an Implicit Training Set: Application to Structural
    Ambiguity Resolution, in HLT/EMNLP'0
Vivi Nastase and Stan Szpakowicz. 2003. Exploring
    Noun-Modifier Semantic Relations. International
    Workshop on Computational Semantics, Tillburg,
    Netherlands, 2003
Paul Nulty and Fintan Costello, 2007. Semantic 
    Classification of Noun Phrases Using Web Counts
 and Learning Algorithms. Proceedings of
 ACL 2007 Student Reseach Workshop. 
Barbara Rosario and Marti A. Hearst. 2001. Classifying
 the semantic relations in noun compounds via a 
 domain-specific lexical hierarchy. In Proceedings of
 the 2001Conference on Empirical Methods in 
 Natural Language Processing. ACL
Peter D. Turney. 2005. Measuring semantic similarity
     by latent relational analysis. In Proceedings of the
     Nineteenth International Joint Conference on 
     Artificial Intelligence (IJCAI-05), pages 1136-1141.
Peter D. Turney., and Michael L. Littman,. 2006. 
     Corpus based learning of analogies and semantic
     relations. Machine Learning, 60(1?3):251?278
Ian H. Witten and Eibe Frank. 1999. Data Mining:
     Practical Machine Learning Tools and Techniques
     with Java Implementations, Morgan Kaufman (1999)
George K. Zipf. 1932. Selected Studies of the Principle
      of Relative Frequency in Language. Cambridge, MA
63
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 234?237,
Uppsala, Sweden, 15-16 July 2010.
c?2010 Association for Computational Linguistics
UCD-PN: Selecting General Paraphrases Using Conditional Probability
Paul Nulty
University College Dublin
Dublin, Ireland
paul.nulty@ucd.ie
Fintan Costello
University College Dublin
Dublin, Ireland
fintan.costello@ucd.ie
Abstract
We describe a system which ranks human-
provided paraphrases of noun compounds,
where the frequency with which a given
paraphrase was provided by human volun-
teers is the gold standard for ranking. Our
system assigns a score to a paraphrase of
a given compound according to the num-
ber of times it has co-occurred with other
paraphrases in the rest of the dataset. We
use these co-occurrence statistics to com-
pute conditional probabilities to estimate a
sub-typing or Is-A relation between para-
phrases. This method clusters together
paraphrases which have similar meanings
and also favours frequent, general para-
phrases rather than infrequent paraphrases
with more specific meanings.
1 Introduction
SemEval 2010 Task 9, ?Noun Compound Inter-
pretation Using Paraphrasing Verbs?, requires sys-
tems to rank paraphrases of noun compounds
according to which paraphrases were most fre-
quently produced for each compound by human
annotators (Butnariu et al, 2010). This paper de-
scribes a system which ranks a paraphrase for a
given compound by computing the probability of
the paraphrase occurring given that we have previ-
ously observed that paraphrase co-occurring with
other paraphrases in the candidate paraphrase list.
These co-occurrence statistics can be built using
either the compounds from the test set or the train-
ing set, with no significant difference in results.
The model is informed by two observations:
people tend to use general, semantically light para-
phrases more often than detailed, semantically
heavy ones, and most paraphrases provided for a
specific compound indicate the same interpreta-
tion of that compound, varying mainly according
to level of semantic detail.
Given these two properties of the data, the ob-
jective of our system was to test the theory that
conditional probabilities can be used to estimate a
sub-typing or Is-A relation between paraphrases.
No information about the compounds was used,
nor were the frequencies provided in the training
set used.
2 Motivation
Most research on the disambiguation of noun com-
pounds involves automatically categorizing the
compound into one of a pre-defined list of seman-
tic relations. Paraphrasing compounds is an alter-
native approach to the disambiguation task which
has been explored by (Lauer, 1995) and (Nakov,
2008). Paraphrases of semantic relations may be
verbs, prepositions, or ?prepositional verbs? like
found in and caused by. (Lauer, 1995) catego-
rized compounds using only prepositions. (Nakov,
2008) and the current task use only verbs and
prepositional verbs, however, many of the para-
phrases in the task data are effectively just prepo-
sitions with a copula, e.g. be in, be for, be of.
The paraphrasing approach may be easier to
integrate into applications such as translation,
query-expansion and question-answering ? its
output is a set of natural language phrases rather
than an abstract relation category. Also, most
sets of pre-defined semantic relations have only
one or maybe two levels of granularity. This
can often lead to semantically converse relations
falling under the same abstract category, for ex-
ample a headache tablet is a tablet for prevent-
ing headaches, while headache weather is weather
that induces headaches ? but both compounds
would be assigned the same relation (perhaps in-
strumental or causal) in many taxonomies of se-
mantic relations. Paraphrases of compounds using
verbs or verb-preposition combinations can pro-
vide as much or as little detail as is required to
adequately disambiguate the compound.
234
2.1 General paraphrases are frequent
The object of SemEval 2010 Task 9 is to rank para-
phrases for noun compounds given by 50-100 hu-
man annotators. When deciding on a model we
took into account several observations about the
data.
Firstly, the model does not need to produce
plausible paraphrases for noun compounds, it sim-
ply needs to rank paraphrases that have been pro-
vided. Given that all of the paraphrases in the
training and test sets have been produced by peo-
ple, we presume that all of them will have at
least some plausible interpretation, and most para-
phrases for a given compound will indicate gen-
erally the same interpretation of that compound.
This will not always be the case; some compounds
are genuinely ambiguous rather than vague. For
example a stone bowl could be a bowl for hold-
ing stones or a bowl made of stone. However, the
mere fact that a compound has occurred in text is
evidence that the speaker who produced the text
believed that the compound was unambiguous, at
least in the given context.
Given that most of the compounds in the dataset
have one clear plausible meaning to readers, when
asked to paraphrase a compound people tend to
observe the Grician maxim of brevity (Grice,
1975) by using simple, frequent terms rather than
detailed, semantically weighty paraphrases. For
the compound alligator leather in the training
data, the two most popular paraphrases were be
made from and come from. Also provided as
paraphrases for this compound were hide of and
be skinned from. These are more detailed, spe-
cific, and more useful than the most popular para-
phrases, but they were only produced once each,
while be made from and come from were pro-
vided by 28 and 20 annotators respectively. This
trend is noticeable in most of the compounds in
the training data - the most specific and detailed
paraphrases are not the most frequently produced.
According to the lesser-known of Zipf?s laws ?
the law of meaning (Zipf, 1945) ? words that are
more frequent overall in a language tend to have
more sub-senses. Frequent terms have a shorter
lexical access time (Broadbent, 1967), so to min-
imize the effort required to communicate mean-
ing of a compound, speakers should tend to use
the most common words - which tend to be se-
mantically general and have many possible sub-
senses. This seems to hold for paraphrasing verbs
and prepositions; terms that have a high overall
frequency in English such as be in, have and be of
are vague ? there are many more specific para-
phrases which could be considered sub-senses of
these common terms.
2.2 Using conditional probability to detect
subtypes
Our model uses conditional probabilities to detect
this sub-typing structure based on the theory that
observing a specific, detailed paraphrase is good
evidence that a more general parent sense of that
paraphrase would be acceptable in the same con-
text. The reverse is not true - observing a fre-
quently occurring, semantically light paraphrase
is not strong evidence that any sub-sense of that
paraphrase would be acceptable in the same con-
text. For example, consider the spatial and tempo-
ral sub-senses of the paraphrase be in. A possible
spatial sub-sense of this paraphrase is be located
in, while a possible temporal sub-sense would be
occur during. The fact that occur during is pro-
vided as a paraphrase for a compound almost al-
ways means that be in is also a plausible para-
phrase. However, observing be in as a paraphrase
does not provide such strong evidence for occur
during also being plausible, as we do not know
which sub-sense of in is intended.
If this is correct, then we would expect that the
conditional probability of a paraphrase B occur-
ring given that we have observed another para-
phrase A in the same context is a measure of the
extent to which B is a more general type (parent
sense) of A.
3 System Description
The first step in our model is to generate a condi-
tional probability table by going over all the com-
pounds in the data and calculating the probabil-
ity of each paraphrase occurring given that we ob-
served another given paraphrase co-occurring for
the same compound. We compute the conditional
probability of every paraphrase with all other para-
phrases individually. We could use either the train-
ing or the test set to collect these co-occurrence
statistics, as the frequencies with which the para-
phrases are ranked are not used ? we simply note
how many times each paraphrase co-occurred as a
possible paraphrase for the same compound with
each other paraphrase. For the submitted system
we used the test data, but subsequently we con-
235
firmed that using only the training data for this step
is not detrimental to the system?s performance.
For each paraphrase in the data, the conditional
probability of that paraphrase is computed with re-
spect to all other paraphrases in the data. For any
two paraphrases B and A:
P (B|A) =
P (A ?B)
P (A)
As described in the previous section, we antic-
ipate that more general, less specific paraphrases
will be produced more often than their more de-
tailed sub-senses. Therefore, we score each para-
phrase by summing its conditional probability
with each other paraphrase provided for the same
compound.
For a list of paraphrases A provided for a given
compound, we score a paraphrase b in that list by
summing its conditional probability individually
with every other paraphrase in the list.
score(b) =
?
a?A
P (b|a)
This gives the more general, broad coverage,
paraphrases a higher score, and also has a cluster-
ing effect whereby paraphrases that have not co-
occurred with the other paraphrases in the list very
often for other compounds are given a lower score
? they are unusual in the context of this para-
phrase list.
4 Results and Analysis
4.1 Task results
Table 1 shows the results of the top 3 systems in
the task. Our system achieved the second high-
est correlation according to the official evaluation
measure, Spearman?s rank correlation coefficient.
Results were also provided using Pearson?s corre-
lation coefficient and the cosine of the vector of
scores for the gold standard and submitted pre-
dictions. Our system performed best using the
cosine measure, which measures how closely the
predicted scores match the gold standard frequen-
cies, rather than the rank correlation. This could
be helpful as the scores provide a scale of accept-
ability.
As mentioned in the system description, we
collected the co-occurrence statistics for our sub-
mitted prediction from the test set of paraphrases
alone. Since our model does not use the frequen-
cies provided in the training set, we chose to use
System Spearman Pearson Cosine
UVT .450 .411 .635
UCD-PN .441 .361 .669
UCD-GOG .432 .395 .652
baseline .425 .344 .524
Table 1: Results for the top three systems.
the test set as it was larger and had more annota-
tors. This could be perceived as an unfair use of
the test data, as we are using all of the test com-
pounds and their paraphrases to calculate the po-
sition of a given paraphrase relative to other para-
phrases.
This is a kind of clustering which would not be
possible if only a few test cases were provided. To
check that our system did not need to collect co-
occurrence probabilities on exactly the same data
as it made predictions on, we submitted a second
set of predictions for the test based on the proba-
bilities from the training compounds alone.
1
These predictions actually achieved a slightly
better score for the official evaluation measure,
with a Spearman rho of 0.444, and a cosine of
0.631. This suggests that the model does not need
to collect co-occurrence statistics from the same
compounds as it makes predictions on, as long as
sufficient data is available.
4.2 Error Analysis
The most significant drawback of this system is
that it cannot generate paraphrases for noun com-
pounds - it is designed to rank paraphrases that
have already been provided.
Using the conditional probability to rank para-
phrases has two effects. Firstly there is a cluster-
ing effect which favours paraphrases that are more
similar to the other paraphrases in a list for a given
compound. Secondly, paraphrases which are more
frequent overall receive a higher score, as frequent
verbs and prepositions may co-occur with a wide
variety of more specific terms.
These effects lead to two possible drawbacks.
Firstly, the system would not perform well if de-
tailed, specific paraphrases of compounds were
needed. Although less frequent, more specific
paraphrases may be more useful for some appli-
cations, these are not the kind of paraphrases that
people seem to produce spontaneously.
1
Thanks to Diarmuid
?
O S?eaghdha for pointing this out
and scoring the second set of predictions
236
Also, because of the clustering effect, this sys-
tem would not work well for compounds that are
genuinely ambiguous e.g. stone bowl (bowl made
of stone vs bowl contains stones). Most examples
are not this ambiguous, and therefore almost all
of the provided paraphrases for a given compound
are plausible, and indicate the same relation. They
vary mainly in how specific/detailed their explana-
tion of the relation is.
The three compounds which our system pro-
duced the worst rank correlation for were diesel
engine, midnight train, and bathing suit. With-
out access to the gold-standard scores for these
compounds it is difficult to explain the poor per-
formance, but examining the list of possible para-
phrases for the first two of these suggests that the
annotators identified two distinct senses for each:
diesel engine is paraphrased by verbs of contain-
ment (e.g. be in) and verbs of function (e.g. runs
on), while midnight train is paraphrased by verbs
of location (e.g. be found in, be located in) and
verbs of movement (e.g. run in, arrive at). Our
model works by separating paraphrases according
to granularity, and cannot disambiguate these dis-
tinct senses. The list of possible paraphrases for
bathing suit suggests that our model is not robust
if implausible paraphrases are in the candidate list
- the model ranked be in, be found in and emerge
from among the top 8 paraphrases for this com-
pound, even though they are barely comprehensi-
ble as plausible paraphrases. The difficulty here
is that even if only one annotator suggests a para-
phrase, it is deemed to have co-occurred with other
paraphrases in that list, since we do not use the fre-
quencies from the training set.
The compounds for which the highest correla-
tions were achieved were wilderness areas, conso-
nant systems and fiber optics. The candidate para-
phrases for the first two of these seem to be fairly
homogeneous in semantic intent. Fiber optics
is probably a lexicalised compound which hardly
needs paraphrasing. This would lead people to use
short and semantically general paraphrases.
5 Conclusion
We have described a system which uses a simple
statistical method, conditional probability, to es-
timate a sub-typing relationship between possible
paraphrases of noun compounds. From a list of
candidate paraphrases for each noun compound,
those which were judged by this method to be
good ?parent senses? of other paraphrases in the
list were scored highly in the rankings.
The system does require a large dataset of com-
pounds with associated plausible paraphrases, but
it does not require a training set of human pro-
vided rankings and does not use any information
about the noun compound itself, aside from the list
of plausible paraphrases that were provided by the
human annotators.
Given the simplicity of our model and its per-
formance compared to other systems which used
more intensive approaches, we believe that our ini-
tial observations on the data are valid: people tend
to produce general, semantically light paraphrases
more often than specific or detailed paraphrases,
and most of the paraphrases provided for a given
compound indicate a similar interpretation, vary-
ing instead mainly in level of semantic weight or
detail.
We have also shown that conditional probabil-
ity is an effective way to compute the sub-typing
relation between paraphrases.
Acknowledgement
This research was supported by a grant under the
FP6 NEST Programme of the European Commis-
sion (ANALOGY: Humans the Analogy-Making
Species: STREP Contr. No 029088).
References
Donald E. Broadbent 1967. Word-frequency effect
and response bias.. Psychological Review, 74,
Cristina Butnariu and Su Nam Kim and Preslav Nakov
and Diarmuid
?
O S?eaghdha and Stan Szpakowicz and
Tony Veale. 2010. SemEval-2 Task 9: The In-
terpretation of Noun Compounds Using Paraphras-
ing Verbs and Prepositions, Proceedings of the 5th
SIGLEX Workshop on Semantic Evaluation, Upp-
sala, Sweden
Paul Grice. 1975. Studies in the Way of Words. Har-
vard University Press, Cambridge, Mass.
Mark Lauer 1995. Designing statistical language
learners: experiments on noun compound, PhD The-
sis Macquarie University, Australia
Preslav Nakov and Marti Hearst 2008. Solving Re-
lational Similarity Problems using the Web as a
Corpus. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguistics
(ACL-08), Columbus, OH.
George Kingsley Zipf. 1945. The Meaning-Frequency
Relationship of Words. Journal of General Psychol-
ogy, 33,
237
