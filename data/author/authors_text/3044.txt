Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 230?233,
Paris, October 2009. c?2009 Association for Computational Linguistics
Heuristic search in a cognitive model of human parsing
John T. Hale
Cornell University
217 Morrill Hall
Ithaca, New York 14853
jthale@cornell.edu
Abstract
We present a cognitive process model
of human sentence comprehension based
on generalized left-corner parsing. A
search heuristic based upon previously-
parsed corpora derives garden path effects,
garden path paradoxes, and the local co-
herence effect.
1 Introduction
One of the most interesting applications of pars-
ing technology has, for some researchers, been
psycholinguistic models (Kay, 2005). Algorith-
mic models of language use have led in the past
to a variety of cognitive insights (Kaplan, 1972;
Marcus, 1980; Thibadeau et al, 1982; Pereira,
1985; Pulman, 1986; Johnson, 1989; Stabler,
1994). However they are challenged by a veritable
tidal wave of new data collected during the 1990s
and 2000s. Work during this later period reveals
phenomena, such as the local coherence effect dis-
cussed in section 5, that have yet to be truly inte-
grated into any particular theoretical framework.
This short paper presents a parsing system in-
tended to serve as a model of the syntactic part
of human sentence comprehension. Such a model
helps make sense of sentence-difficulty data from
self-paced reading, eye-tracking and other behav-
ioral studies. It also sketches a relationship be-
tween calculations carried out in the course of
automated syntactic analysis and the inferences
about linguistic structure taking place in our minds
during ordinary sentence-understanding.
Section 2 defines the model itself, highlight-
ing its relationship to generalized left-corner pars-
ing. Sections 3?5 apply this model to three contro-
versial phenomena that are well-established in the
psycholinguistics literature. Section 6 concludes.
2 Architecture of the model
2.1 Problem states and Operators
We model the human sentence comprehen-
sion mechanism as search within a prob-
lem space (Newell and Simon, 1972). We as-
sume that all (incremental) parser states have
a (partial) grammatical interpretation (Chomsky,
1965, 9). In this paper, the grammatical inter-
pretation employs context-free grammar. An in-
ventory of operators carries the model from one
point in the problem space to another. In the in-
terest of simplicity, we place no bound on the
number of problem states the model can explore.
However, we do acknowledge with Johnson-Laird
(1983) and Resnik (1992) a pressure to minimize
memory consumption internal to a problem state.
The model?s within-problem state memory usage
should reflect human acceptability judgments with
embedded sentences. These considerations moti-
vate a generalized left-corner (GLC) parsing strat-
egy (Demers, 1977) whose stack consumption is
maximal on just the center-embedded examples
that are so difficult for people to understand. To
reflect the argument/adjunct distinction (Tutun-
jian and Boland, 2008) we adopt a mixed strat-
egy that is bottom-up for optional postmodifiers
but left-corner everywhere else. Leaving the arc-
eager/arc-standard decision (Abney and Johnson,
1991) to the control policy allows four possible
operators, schematized in Table 1.
2.2 Informed Search
Informed search differs from uninformed search
procedures such as depth-first and breadth-first
by making use of heuristic knowledge about the
search domain. The strategy is to choose for ex-
pansion the node whose cost is lowest (Barr and
Feigenbaum, 1981, 61). In A? search (Hart et al,
1968) this cost is divided up into a sum consisting
of the known cost to reach a search node and an
230
shift a word W project a rule LHS ? Trigger
?
announce
point
Rest
scan the sought word W
project and match the sought parent LHS using
the rule LHS ? Trigger
?
announce
point
Rest
Table 1: Four schema define the operators
stack n E[steps] standard error
[VP] S [TOP] 55790 44.936 0.1572
S [TOP] 53991 10.542 0.0986
[NP] S [TOP] 43635 33.092 0.1633
NP [TOP] 38844 55.791 0.2126
NP [S] S [TOP] 34415 47.132 0.2122
[S] S [TOP] 33578 52.800 0.2195
[PP] S [TOP] 30693 34.454 0.1915
IN [PP] S [TOP] 27272 32.379 0.2031
DT [NP] S [TOP] 22375 34.478 0.2306
[AUX] [VP] S [TOP] 16447 46.536 0.2863
VBD [VP] S [TOP] 16224 43.057 0.2826
VB [VP] S [TOP] 13548 40.404 0.3074
the [NP] S [TOP] 12507 34.120 0.3046
NP [NP] S [TOP] 12092 43.821 0.3269
DT [TOP] 10440 66.452 0.3907
Table 2: Popular left-corner parser states. Stacks
grow to the left. The categories are as described in
Table 3 of Marcus et al (1993).
estimate of the costs involved in finishing search
from that node. In this work, rather than relying
on the guarantee provided by the A? theorem, we
examine the exploration pattern that results from
an inadmissable completion cost estimator. The
choice of estimator is Hypothesis 1.
Hypothesis 1 Search in parsing is informed by an
estimate of the expected number of steps to com-
pletion, given previous experience.
Table 2 writes out the expected number of
steps to completion (E[steps]) for a selection of
problem states binned together according to their
grammatical interpretation. Categories enclosed
in square brackets are predicted top-down whereas
unbracketed have been found bottom-up. These
states are some of the most popular states vis-
ited during a simulation of parsing the Brown cor-
pus (Kuc?era and Francis, 1967; Marcus et al,
1993) according to the mixed strategy introduced
above in subsection 2.1. The quantity E[steps]
serves in what follows as the completion cost esti-
mate in A? search.
3 Garden pathing
Any model of human sentence comprehension
should address the garden path effect. The con-
trast between 1a and 1b is an example of this phe-
nomenon.
(1) a. while Mary was mending a sock fell on the floor
b. while Mary was mending, a sock fell on the floor
The control condition 1b includes a comma which,
in spoken language, would be expressed as a
prosodic break (Carroll and Slowiaczek, 1991;
Speer et al, 1996).
Figure 1 shows the search space explored in
the experimental condition 1a. In this picture,
ovals represent problem states. The number in-
side the oval encodes the vistation order. Arcs be-
tween ovals represent operator applications. The
path (14, 22, 23, 24, 25, 29, 27) is the garden path
which builds a grammatical interpretation where a
sock is attached as a direct object of the verb mend.
The grey line highlights the order in which A?
search considers this path. At state 21 after shift-
ing sock, experience with the Brown corpus sug-
gests reconsidering the garden path.
Whereas the model examines 45 search nodes
during the analysis of the temporarily ambiguous
item 1a, it dispatches the unambiguous item 1b af-
ter only 40 nodes despite that sentence having an
additional token (the comma). Garden paths, on
this view, are sequences of parser states explored
only in a temporarily ambiguous item.
4 Garden pathing counterexamples
Purely structural attachment preferences like
Right Association (Kimball, 1973) and Mini-
mal Attachment (Frazier and Fodor, 1978; Pereira,
1985) are threatened by paradoxical counterexam-
ples such as 2 from Gibson (1991, 22) where no
fixed principle yields correct predictions across
both examples.
(2) a. I gave her earrings on her birthday .
b. I gave her earrings to another girl .
A parser guided by Hypothesis 1 interleaves the
garden path attachment and the globally-correct
attachment in both cases, resulting in a search that
231
12
10
43
42
41
32
36
37
45
35
4
1
38
39
40
31
33
29
20
16
15
27
26
24
17
18
9
19
28
34
14
8
23
13
11
2
6
44
22
3
5
25
21
30
7
0
Figure 1: Heavy line is the globally-correct path
is strictly committed to neither analysis. In 2a,
32% of discovered states represent the globally-
incorrect attachment of her. In 2b, 27% of states
represent the globally-incorrect attachment of her
to give as a one-word direct object. The para-
dox for purely structural attachment heuristics is
dissolved by the observation that neither pathway
fully achieves priority over the other.
5 Local Coherence
Tabor et al (2004) discovered1 a processing dif-
ficulty phenomenon called ?local coherence.?
Among the stimuli they considered, the locally-
coherent condition is 3a where the substring the
player tossed a frisbee could be analyzed as a sen-
tence, if considered in isolation.
(3) a. The coach smiled at the player tossed a frisbee by
the opposing team.
b. The coach smiled at the player thrown a frisbee
by the opposing team
c. The coach smiled at the player who was tossed a
frisbee by the opposing team.
d. The coach smiled at the player who was thrown a
frisbee by the opposing team.
Tabor and colleagues observe an interaction be-
tween the degree of morphological ambiguity of
the embedded verb (tossed or thrown) and the
presence or absence of the relative-clause initial
words who was. These two factors are known as
?ambiguity and ?reduction, respectively. If the
human parser were making full use of the gram-
mar, its operation would reflect the impossibility
of continuing the coach smiled at... with a sen-
tence. The ungrammaticality of a sentence in this
left context would preclude any analysis of the
player as a subject of active voice toss. But greater
reading times observed on the ambiguous tossed
as compared to the unambiguous thrown suggest
contrariwise that this grammatical deduction is not
made uniformly based on the left context.
Table 3 shows how an informed parser?s step
counts, when guided by Hypothesis 1, derive
Tabor et al?s observed pattern. The cell pre-
dicted to be hardest is the local coherence,
shaded gray. The degree of worsening due to rel-
ative clause reduction is greater in +ambiguous
than in ?ambiguous. This derives the observed
interaction.
1Konieczny and Mu?ller (2006) documents a closely re-
lated form of local coherence in German.
232
+ambiguous ?ambiguous
+reduced 119 84
?reduced 67 53
Table 3: Count of states examined
6 Conclusion
When informed by experience with the Brown
corpus, the parsing system described in this pa-
per exhibits a pattern of ?time-sharing? perfor-
mance that corresponds to human behavioral diffi-
culty in three controversial cases. The built-in el-
ements ? context-free grammar, generalized left-
corner parsing and the A?-type cost function ?
are together adequate to address a range of com-
prehension difficulty phenomena without impos-
ing an a priori memory limit. The contribution is
an algorithmic-level account of the cognitive pro-
cesses involved in perceiving syntactic structure.
References
Steven Abney and Mark Johnson. 1991. Memory require-
ments and local ambiguities of parsing strategies. Journal
of Psycholinguistic Research, 20(3):233?249.
Avron Barr and Edward A. Feigenbaum, editors. 1981. The
Handbook of Artificial Intelligence. William Kaufmann.
Patrick J. Carroll and Maria L. Slowiaczek. 1991. Modes and
modules: multiple pathways to the language processor.
In Jay L. Garfield, editor, Modularity in Knowledge Rep-
resentation and Natural Language Understanding, pages
221?247. MIT Press.
Noam Chomsky. 1965. Aspects of the Theory of Syntax.
MIT Press.
Alan J. Demers. 1977. Generalized left corner parsing. In
Conference Report of the 4th annual association for com-
puting machinery symposium on Principles of Program-
ming Languages, pages 170?181.
Lyn Frazier and Janet Dean Fodor. 1978. The sausage ma-
chine: a new two-stage parsing model. Cognition, 6:291?
325.
Edward Gibson. 1991. A Computational Theory of Human
Linguistic Processing: Memory Limitations and Process-
ing Breakdown. Ph.D. thesis, Carnegie Mellon University.
Peter E. Hart, Nils J. Nilsson, and Bertram Raphael. 1968. A
formal basis for the heuristic determination of minimum
cost paths. IEEE Transactions of systems science and cy-
bernetics, ssc-4(2):100?107.
Philip N. Johnson-Laird. 1983. Mental Models. Cambridge
University Press.
Mark Johnson. 1989. Parsing as deduction: the use of knowl-
edge of language. Journal of Psycholinguistic Research,
18(1):105?128.
Ronald M. Kaplan. 1972. Augmented transition networks as
psychological models of sentence comprehension. Artifi-
cial Intelligence, 3:77?100.
Martin Kay. 2005. A life of language. Computational Lin-
guistics, 31(4):425?438.
John P. Kimball. 1973. Seven principles of surface structure
parsing in natural language. Cognition, 2:15?48.
Lars Konieczny and Daniel Mu?ller. 2006. Local coherences
in sentence processing. CUNY Conference on Human
Sentence Processing.
Henry Kuc?era and W. Nelson Francis. 1967. Computational
Analysis of Present-day American English. Brown Uni-
versity Press.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated corpus
of English: the Penn Treebank. Computational Linguis-
tics, 19.
Mitchell P. Marcus. 1980. A theory of syntactic recognition
for natural language. MIT Press.
Allen Newell and Herbert A. Simon. 1972. Human Problem
Solving. Prentice-Hall, Englewood Cliffs, New Jersey.
Fernando Pereira. 1985. A new characterization of attach-
ment preference. In David Dowty, Lauri Karttunen, and
Arnold Zwicky, editors, Natural Language Parsing: Psy-
chological, Computational and Theoretical Perspectives,
ACL Studies in Natural Language Processing, pages 307?
319. Cambridge University Press.
Steven G. Pulman. 1986. Grammars, parsers, and mem-
ory limitations. Language and Cognitive Processes,
1(3):197?2256.
Philip Resnik. 1992. Left-corner parsing and psychologi-
cal plausibility. In Proceedings of the Fourteenth Interna-
tional Conference on Computational Linguistics, Nantes,
France.
Shari R. Speer, Margaret M. Kjelgaard, and Kathryn M. Do-
broth. 1996. The influence of prosodic structure on
the resolution of temporary syntactic closure ambiguities.
Journal of Psycholinguistic Research, 25(2):249?271.
Edward Stabler. 1994. The finite connectivity of lin-
guistic structure. In Charles Clifton, Lyn Frazier, and
Keith Rayner, editors, Perspectives on Sentence Process-
ing, pages 303?336. Lawrence Erlbaum.
Whitney Tabor, Bruno Galantuccia, and Daniel Richardson.
2004. Effects of merely local syntactic coherence on
sentence processing. Journal of Memory and Language,
50(4):355?370.
Robert Thibadeau, Marcel A. Just, and Patricia Carpenter.
1982. A model of the time course and content of read-
ing. Cognitive Science, 6:157?203.
D. Tutunjian and J.E. Boland. 2008. Do We Need a Distinc-
tion between Arguments and Adjuncts? Evidence from
Psycholinguistic Studies of Comprehension. Language
and Linguistics Compass, 2(4):631?646.
233
A Probabilistic Earley Parser as a Psycholinguistic Model
John Hale
Department of Cognitive Science
The Johns Hopkins University
3400 North Charles Street; Baltimore MD 21218-2685
hale@cogsci.jhu.edu
Abstract
In human sentence processing, cognitive load can be
dened many ways. This report considers a deni-
tion of cognitive load in terms of the total probability
of structural options that have been disconrmed at
some point in a sentence: the surprisal of word w
i
given its prex w
0...i?1
on a phrase-structural lan-
guage model. These loads can be eciently calcu-
lated using a probabilistic Earley parser (Stolcke,
1995) which is interpreted as generating predictions
about reading time on a word-by-word basis. Un-
der grammatical assumptions supported by corpus-
frequency data, the operation of Stolcke?s probabilis-
tic Earley parser correctly predicts processing phe-
nomena associated with garden path structural am-
biguity and with the subject/object relative asym-
metry.
Introduction
What is the relation between a person?s knowledge of
grammar and that same person?s application of that
knowledge in perceiving syntactic structure? The
answer to be proposed here observes three principles.
Principle 1 The relation between the parser and
grammar is one of strong competence.
Strong competence holds that the human sentence
processing mechanism directly uses rules of gram-
mar in its operation, and that a bare minimum of
extragrammatical machinery is necessary. This hy-
pothesis, originally proposed by Chomsky (Chom-
sky, 1965, page 9) has been pursued by many re-
searchers (Bresnan, 1982) (Stabler, 1991) (Steed-
man, 1992) (Shieber and Johnson, 1993), and stands
in contrast with an approach directed towards the
discovery of autonomous principles unique to the
processing mechanism.
Principle 2 Frequency affects performance.
The explanatory success of neural network and
constraint-based lexicalist theories (McClelland and
St. John, 1989) (MacDonald et al, 1994) (Tabor et
al., 1997) suggests a statistical theory of language
performance. The present work adopts a numerical
view of competition in grammar that is grounded in
probability.
Principle 3 Sentence processing is eager.
\Eager" in this sense means the experimental situa-
tions to be modeled are ones like self-paced reading
in which sentence comprehenders are unrushed and
no information is ignored at a point at which it could
be used.
The proposal is that a person?s diculty per-
ceiving syntactic structure be modeled by word-to-
word surprisal (Attneave, 1959, page 6) which can
be directly computed from a probabilistic phrase-
structure grammar. The approach taken here uses
a parsing algorithm developed by Stolcke. In the
course of explaining the algorithm at a very high
level I will indicate how the algorithm, interpreted
as a psycholinguistic model, observes each principle.
After that will come some simulation results, and
then a conclusion.
1 Language models
Stolcke?s parsing algorithm was initially applied as a
component of an automatic speech recognition sys-
tem. In speech recognition, one is often interested
in the probability that some word will follow, given
that a sequence of words has been seen. Given some
lexicon of all possible words, a language model as-
signs a probability to every string of words from
the lexicon. This denes a probabilistic language
(Grenander, 1967) (Booth and Thompson, 1973)
(Soule, 1974) (Wetherell, 1980).
A language model helps a speech recognizer focus
its attention on words that are likely continuations
of what it has recognized so far. This is typically
done using conditional probabilities of the form
P (W
n
= w
n
jW
1
= w
1
; : : : W
n?1
= w
n?1
)
the probability that the nth word will actually be
w
n
given that the words leading up to the nth have
been w
1
; w
2
; : : : w
n?1
. Given some nite lexicon, the
probability of each possible outcome for W
n
can be
estimated using that outcome?s relative frequency in
a sample.
Traditional language models used for speech are n-
gram models, in which n ? 1 words of history serve
as the basis for predicting the nth word. Such mod-
els do not have any notion of hierarchical syntactic
structure, except as might be visible through an n-
word window.
Aware that the n-gram obscures many
linguistically-signicant distinctions (Chomsky,
1956, section 2.3), many speech researchers (Jelinek
and Laerty, 1991) sought to incorporate hierar-
chical phrase structure into language modeling (see
(Stolcke, 1997)) although it was not until the late
1990s that such models were able to signicantly
improve on 3-grams (Chelba and Jelinek, 1998).
Stolcke?s probabilistic Earley parser is one way
to use hierarchical phrase structure in a language
model. The grammar it parses is a probabilistic
context-free phrase structure grammar (PCFG),
e.g.
1:0 S ! NP VP
0:5 NP ! Det N
0:5 NP ! NP VP
...
...
see (Charniak, 1993, chapter 5)
Such a grammar denes a probabilistic language in
terms of a stochastic process that rewrites strings of
grammar symbols according to the probabilities on
the rules. Then each sentence in the language of the
grammar has a probability equal to the product of
the probabilities of all the rules used to generate it.
This multiplication embodies the assumption that
rule choices are independent. Sentences with more
than one derivation accumulate the probability of all
derivations that generate them. Through recursion,
innite languages can be specied; an important
mathematical question in this context is whether or
not such a grammar is consistent { whether it assigns
some probability to innite derivations, or whether
all derivations are guaranteed to terminate.
Even if a PCFG is consistent, it would appear to
have another drawback: it only assigns probabili-
ties to complete sentences of its language. This is as
inconvenient for speech recognition as it is for mod-
eling reading times.
Stolcke?s algorithm solves this problem by com-
puting, at each word of an input string, the prex
probability. This is the sum of the probabilities of all
derivations whose yield is compatible with the string
seen so far. If the grammar is consistent (the proba-
bilities of all derivations sum to 1.0) then subtracting
the prex probability from 1.0 gives the total proba-
bility of all the analyses the parser has disconrmed.
If the human parser is eager, then the \work" done
during sentence processing is exactly this disconr-
mation.
2 Earley parsing
The computation of prex probabilities takes advan-
tage of the design of the Earley parser (Earley, 1970)
which by itself is not probabilistic. In this section I
provide a brief overview of Stolcke?s algorithm but
the original paper should be consulted for full details
(Stolcke, 1995).
Earley parsers work top-down, and propagate
predictions conrmed by the input string back up
through a set of states representing hypotheses the
parser is entertaining about the structure of the sen-
tence. The global state of the parser at any one time
is completely dened by this collection of states, a
chart, which denes a tree set. A state is a record
that species
 the current input string position processed so
far
 a grammar rule
 a \dot-position" in the rule representing how
much of the rule has already been recognized
 the leftmost edge of the substring this rule gen-
erates
An Earley parser has three main functions, pre-
dict, scan and complete, each of which can enter
new states into the chart. Starting from a dummy
start state in which the dot is just to the left of the
grammar?s start symbol, predict adds new states for
rules which could expand the start symbol. In these
new predicted states, the dot is at the far left-hand
side of each rule. After prediction, scan checks the
input string: if the symbol immediately following
the dot matches the current word in the input, then
the dot is moved rightward, across the symbol. The
parser has \scanned" this word. Finally, complete
propagates this change throughout the chart. If, as
a result of scanning, any states are now present in
which the dot is at the end of a rule, then the left
hand side of that rule has been recognized, and any
other states having a dot immediately in front of
the newly-recognized left hand side symbol can now
have their dots moved as well. This happens over
and over until no new states are generated. Parsing
nishes when the dot in the dummy start state is
moved across the grammar?s start symbol.
Stolcke?s innovation, as regards prex probabili-
ties is to add two additional pieces of information to
each state: , the forward, or prex probability, and
? the \inside" probability. He notes that
path An (unconstrained) Earley path,
or simply path, is a sequence of Earley
states linked by prediction, scanning,
or completion.
constrained A path is said to be con-
strained by, or generate a string x if
the terminals immediately to the left
of the dot in all scanned states, in se-
quence, form the string x.
. . .
The signicance of Earley paths is that
they are in a one-to-one correspondence
with left-most derivations. This will al-
low us to talk about probabilities of deriva-
tions, strings and prexes in terms of the
actions performed by Earley?s parser.
(Stolcke, 1995, page 8)
This correspondence between paths of parser op-
erations and derivations enables the computation of
the prex probability { the sum of all derivations
compatible with the prex seen so far. By the cor-
respondence between derivations and Earley paths,
one would need only to compute the sum of all paths
that are constrained by the observed prex. But
this can be done in the course of parsing by storing
the current prex probability in each state. Then,
when a new state is added by some parser opera-
tion, the contribution from each antecedent state {
each previous state linked by some parser operation
{ is summed in the new state. Knowing the prex
probability at each state and then summing for all
parser operations that result in the same new state
eciently counts all possible derivations.
Predicting a rule corresponds to multiplying by
that rule?s probability. Scanning does not alter any
probabilities. Completion, though, requires knowing
?, the inside probability, which records how probable
was the inner structure of some recognized phrasal
node. When a state is completed, a bottom-up con-
rmation is united with a top-down prediction, so
the  value of the complete-ee is multiplied by the
? value of the complete-er.
Important technical problems involving left-
recursive and unit productions are examined and
overcome in (Stolcke, 1995). However, these com-
plications do not add any further machinery to the
parsing algorithm per se beyond the grammar rules
and the dot-moving conventions: in particular, there
are no heuristic parsing principles or intermediate
structures that are later destroyed. In this respect
the algorithm observes strong competence { princi-
ple 1. In virtue of being a probabilistic parser it
observes principle 2. Finally, in the sense that pre-
dict and complete each apply exhaustively at each
new input word, the algorithm is eager, satisfying
principle 3.
3 Parallelism
Psycholinguistic theories vary regarding the amount
bandwidth they attribute to the human sentence
processing mechanism. Theories of initial parsing
preferences (Fodor and Ferreira, 1998) suggest that
the human parser is fundamentally serial: a func-
tion from a tree and new word to a new tree. These
theories explain processing diculty by appealing
to \garden pathing" in which the current analysis
is faced with words that cannot be reconciled with
the structures built so far. A middle ground is held
by bounded-parallelism theories (Narayanan and Ju-
rafsky, 1998) (Roark and Johnson, 1999). In these
theories the human parser is modeled as a function
from some subset of consistent trees and the new
word, to a new tree subset. Garden paths arise in
these theories when analyses fall out of the set of
trees maintained from word to word, and have to
be reanalyzed, as on strictly serial theories. Finally,
there is the possibility of total parallelism, in which
the entire set of trees compatible with the input is
maintained somehow from word to word. On such
a theory, garden-pathing cannot be explained by re-
analysis.
The probabilistic Earley parser computes all
parses of its input, so as a psycholinguistic theory
it is a total parallelism theory. The explanation
for garden-pathing will turn on the reduction in the
probability of the new tree set compared with the
previous tree set { reanalysis plays no role. Before
illustrating this kind of explanation with a specic
example, it will be important to rst clarify the na-
ture of the linking hypothesis between the operation
of the probabilistic Earley parser and the measured
eects of the human parser.
4 Linking hypothesis
The measure of cognitive eort mentioned earlier is
dened over prexes: for some observed prex, the
cognitive eort expended to parse that prex is pro-
portional to the total probability of all the struc-
tural analyses which cannot be compatible with the
observed prex. This is consistent with eagerness
since, if the parser were to fail to infer the incom-
patibility of some incompatible analysis, it would
be delaying a computation, and hence not be eager.
This prex-based linking hypothesis can be turned
into one that generates predictions about word-by-
word reading times by comparing the total eort
expended before some word to the total eort af-
ter: in particular, take the comparison to be a ratio.
Making the further assumption that the probabili-
ties on PCFG rules are statements about how di-
cult it is to disconrm each rule1, then the ratio of
1This assumption is inevitable given principles 1 and 2. If
there were separate processing costs distinct from the opti-
mization costs postulated in the grammar, then strong com-
petence is violated. Dening all grammatical structures as
equally easy to disconrm or perceive likewise voids the grad-
edness of grammaticality of any content.
the  value for the previous word to the  value for
the current word measures the combined diculty
of disconrming all disconrmable structures at a
given word { the denition of cognitive load. Scal-
ing this number by taking its log gives the surprisal,
and denes a word-based measure of cognitive eort
in terms of the prex-based one. Of course, if the
language model is sensitive to hierarchical structure,
then the measure of cognitive eort so dened will
be structure-sensitive as well.
5 Plausibility of Probabilistic
Context-Free Grammar
The debate over the form grammar takes in the mind
is clearly a fundamental one for cognitive science.
Much recent psycholinguistic work has generated a
wealth of evidence that frequency of exposure to lin-
guistic elements can aect our processing (Mitchell
et al, 1995) (MacDonald et al, 1994). However,
there is no clear consensus as to the size of the ele-
ments over which exposure has clearest eect. Gib-
son and Pearlmutter identify it as an \outstanding
question" whether or not phrase structure statistics
are necessary to explain performance eects in sen-
tence comprehension:
Are phrase-level contingent frequency con-
straints necessary to explain comprehen-
sion performance, or are the remaining
types of constraints sucient. If phrase-
level contingent frequency constraints are
necessary, can they subsume the eects of
other constraints (e.g. locality) ?
(Gibson and Pearlmutter, 1998, page 13)
Equally, formal work in linguistics has demon-
strated the inadequacy of context-free grammars as
an appropriate model for natural language in the
general case (Shieber, 1985). To address this criti-
cism, the same prex probabilities could be comput-
ing using tree-adjoining grammars (Nederhof et al,
1998). With context-free grammars serving as the
implicit backdrop for much work in human sentence
processing, as well as linguistics2 simplicity seems as
good a guide as any in the selection of a grammar
formalism.
6 Garden-pathing
6.1 A celebrated example
Probabilistic context-free grammar (1) will help il-
lustrate the way a phrase-structured language model
2Some important work in computational psycholinguistics
(Ford, 1989) assumes a Lexical-Functional Grammar where
the c-structure rules are essentially context-free and have
attached to them \strengths" which one might interpret as
probabilities.
could account for garden path structural ambiguity.
Grammar (1) generates the celebrated garden path
sentence \the horse raced past the barn fell" (Bever,
1970). English speakers hearing these words one by
one are inclined to take \the horse" as the subject of
\raced," expecting the sentence to end at the word
\barn." This is the main verb reading in gure 1.
S
NP
the horse
VP
VBD
raced
PP
IN
past
NP
DT
the
NN
barn
Figure 1: Main verb reading
The human sentence processing mechanism is
metaphorically led up the garden path by the main
verb reading, when, upon hearing \fell" it is forced
to accept the alternative reduced relative reading
shown in gure 2.
S
NP
NP
DT
the
NN
horse
VP
VBN
raced
PP
IN
past
NP
DT
the
NN
barn
VP
VBD
fell
Figure 2: Reduced relative reading
The confusion between the main verb and the re-
duced relative readings, which is resolved upon hear-
ing \fell" is the empirical phenomenon at issue.
As the parse trees indicate, grammar (1) analyzes
reduced relative clauses as a VP adjoined to an NP3.
In one sample of parsed text4 such adjunctions are
about 7 times less likely than simple NPs made up of
a determiner followed by a noun. The probabilities
of the other crucial rules are likewise estimated by
their relative frequencies in the sample.
3See section 1.24 of the Treebank style guide
4The sample, starts at sentence 93 of section 16 of
the Treebank and goes for 500 sentences (12924 words)
For information about the Penn Treebank project see
http://www.cis.upenn.edu/~ treebank/
(1)
1.0 S ? NP VP .
0.876404494831 NP ? DT NN
0.123595505169 NP ? NP VP
1.0 PP ? IN NP
0.171428571172 VP ? VBD PP
0.752380952552 VP ? VBN PP
0.0761904762759 VP ? VBD
1.0 DT ? the
0.5 NN ? horse
0.5 NN ? barn
0.5 VBD ? fell
0.5 VBD ? raced
1.0 VBN ? raced
1.0 IN ? past
This simple grammar exhibits the essential character
of the explanation: garden paths happen at points
where the parser can disconrm alternatives that to-
gether comprise a great amount of probability. Note
the category ambiguity present with raced which can
show up as both a past-tense verb (VBD) and a past
participle (VBN).
Figure 3 shows the reading time predictions5 derived
via the linking hypothesis that reading time at word
n is proportional to the surprisal log
(
?
n?1
?
n
)
.
the horse raced past the barn fell
2
4
6
8
10
12
14
Log[
previous prefix
current prefix ] garden-pathing
0
1.
0.1906840.0641303
0
1.
5.90627
Figure 3: Predictions of probabilistic Earley parser
on simple grammar
At \fell," the parser garden-paths: up until that
point, both the main-verb and reduced-relative
structures are consistent with the input. The prex
probability before \fell" is scanned is more than 10
times greater than after, suggesting that the proba-
bility mass of the analyses disconrmed at that point
was indeed great. In fact, all of the probability as-
signed to the main-verb structure is now lost, and
only parses that involve the low-probability NP rule
survive { a rule introduced 5 words back.
6.2 A comparison
If this garden path eect is truly a result of both the
main verb and the reduced relative structures be-
ing simultaneously available up until the nal verb,
5Whether the quantitative values of the predicted read-
ing times can be mapped onto a particular experiment in-
volves taking some position on the oft-observed (Gibson and
Schu?tze, 1999) imperfect relationship between corpus fre-
quency and psychological norms.
then the eect should disappear when words inter-
vene that cancel the reduced relative interpretation
early on.
To examine this possibility, consider now a dier-
ent example sentence, this time from the language
of grammar (2).
(2)
0.574927953937 S ? NP VP
0.425072046063 S ? VP
1.0 SBAR ? WHNP S
0.80412371161 NP ? DT NN
0.082474226966 NP ? NP SBAR
0.113402061424 NP ? NP VP
0.11043 VP ? VBD PP
0.141104 VP ? VBD NP PP
0.214724 VP ? AUX VP
0.484663 VP ? VBN PP
0.0490798 VP ? VBD
1.0 PP ? IN NP
1.0 WHNP ? who
1.0 DT ? the
0.33 NN ? boss
0.33 NN ? banker
0.33 NN ? buy-back
0.5 IN ? about
0.5 IN ? by
1.0 AUX ? was
0.74309393 VBD ? told
0.25690607 VBD ? resigned
1.0 VBN ? told
The probabilities in grammar (2) are estimated from
the same sample as before. It generates a sentence
composed of words actually found in the sample,
\the banker told about the buy-back resigned." This
sentence exhibits the same reduced relative clause
structure as does \the horse raced past the barn
fell."
S
NP
NP
DT
the
NN
banker
VP
VBN
told
PP
about the buy-back
VP
VBD
resigned
Grammar (2) also generates6 the subject relative
\the banker who was told about the buy-back re-
signed." Now a comparison of two conditions is pos-
sible.
MV and RC the banker told about the buy-back re-
signed
6This grammar also generates active and simple passive
sentences, rating passive sentences as more probable than the
actives. This is presumably a fact about the writing style
favored by the Wall Street Journal.
the banker who was told about the buy-backresigned
1
2
3
4
5
6
Log[
previous prefix
current prefix ] Subject Relative Clause
0.798547
1.59946
3.599913.45367
0.498082
1.3212
0.
1.59946
5.87759
Figure 4: Mean 10.5
the banker told about the buy-back resigned
1
2
3
4
5
6
Log[
previous prefix
current prefix ] Reduced Relative Clause
0.798547
1.59946
0.622262
1.3212
0.
1.59946
6.67629
Figure 5: Mean: 16.44
RC only the banker who was told about the buy-
back resigned
The words who was cancel the main verb reading,
and should make that condition easier to process.
This asymmetry is borne out in graphs 4 and 5. At
\resigned" the probabilistic Earley parser predicts
less reading time in the subject relative condition
than in the reduced relative condition.
This comparison veries that the same sorts of
phenomena treated in reanalysis and bounded paral-
lelism parsing theories fall out as cases of the present,
total parallelism theory.
6.3 An entirely empirical grammar
Although they used frequency estimates provided by
corpus data, the previous two grammars were par-
tially hand-built. They used a subset of the rules
found in the sample of parsed text. A grammar in-
cluding all rules observed in the entire sample sup-
ports the same sort of reasoning. In this grammar,
instead of just 2 NP rules there are 532, along with
120 S rules. Many of these generate analyses com-
patible with prexes of the reduced relative clause at
various points during parsing, so the expectation is
that the parser will be disconrming many more hy-
potheses at each word than in the simpler example.
Figure 6 shows the reading time predictions derived
from this much richer grammar.
Because the terminal vocabulary of this richer
grammar is so much larger, a comparatively large
amount of information is conveyed by the nouns
\banker" and \buy-back" leading to high surprisal
the banker told about the buy-backresigned .
2
4
6
8
10
12
14
Log[
previous prefix
current prefix ]grammar from Wall Street Journal sample
3.13979
11.9369
9.59068
8.59021
2.92747
11.9496
12.9214
6.50046
Figure 6: Predictions of Earley parser on richer
grammar
values at those words. However, the garden path
eect is still observable at \resigned" where the pre-
x probability ratio is nearly 10 times greater than
at either of the nouns. Amid the lexical eects, the
probabilistic Earley parser is aected by the same
structural ambiguity that aects English speakers.
7 Subject/Object asymmetry
The same kind of explanation supports an account
of the subject-object relative asymmetry (cf. refer-
ences in (Gibson, 1998)) in the processing of unre-
duced relative clauses. Since the Earley parser is
designed to work with context-free grammars, the
following example grammar adopts a GPSG-style
analysis of relative clauses (Gazdar et al, 1985, page
155). The estimates of the ratios for the two S[+R]
rules are obtained by counting the proportion of sub-
ject relatives among all relatives in the Treebank?s
parsed Brown corpus7.
(3)
0.33 NP ? SPECNP NBAR
0.33 NP ? you
0.33 NP ? me
1.0 SPECNP ? DT
0.5 NBAR ? NBAR S[+R]
0.5 NBAR ? N
1.0 S ? NP VP
0.86864638 S[+R] ? NP[+R] VP
0.13135362 S[+R] ? NP[+R] S/NP
1.0 S/NP ? NP VP/NP
1.0 VP/NP ? V NP/NP
1.0 VP ? V NP
1.0 V ? saw
1.0 NP[+R] ? who
1.0 DT ? the
1.0 N ? man
1.0 NP/NP ? 
7In particular, relative clauses in the Treebank are ana-
lyzed as
NP ? NP SBAR (rule 1)
SBAR ? WHNP S (rule 2)
where the S con-
tains a trace *T* coindexed with the WHNP. The total num-
ber of structures in which both rule 1 and rule 2 apply is
5489. The total number where the rst child of S is null is
4768. This estimate puts the total number of object relatives
at 721 and the frequency of object relatives at 0.13135362 and
the frequency of subject relatives at 0.86864638.
Grammar (3) generates both subject and object rela-
tive clauses. S[+R]?NP[+R] VP is the rule that gen-
erates subject relatives and S[+R] ? NP[+R] S/NP
generates object relatives. One might expect there
to be a greater processing load for object relatives as
soon as enough lexical material is present to deter-
mine that the sentence is in fact an object relative8.
The same probabilistic Earley parser (modied to
handle null-productions) explains this asymmetry in
the same way as it explains the garden path eect.
Its predictions, under the same linking hypothesis
as in the previous cases, are depicted in graphs 7
and 8. The mean surprisal for the object relative is
about 5.0 whereas the mean surprisal for the subject
relative is about 2.1.
the man who saw you saw me
1
2
3
4
5
Log[
previous prefix
current prefix ] Subject Relative Clause
1.59946
0
1.
0.203159
1.59946
1.
1.59946
Figure 7: Subject relative clause
the man who you saw saw me
1
2
3
4
5
Log[
previous prefix
current prefix ] Object Relative Clause
1.59946
0
1.
4.52793
0
1.
1.59946
Figure 8: Object relative clause
Conclusion
These examples suggest that a \total-parallelism"
parsing theory based on probabilistic grammar can
characterize some important processing phenomena.
In the domain of structural ambiguity in particular,
the explanation is of a dierent kind than in tradi-
tional reanalysis models: the order of processing is
not theoretically signicant, but the estimate of its
magnitude at each point in a sentence is. Results
with empirically-derived grammars suggest an ar-
mative answer to Gibson and Pearlmutter?s ques-
8The dierence in probability between subject and object
rules could be due to the work necessary to set up storage
for the ller, eectively recapitulating the HOLD Hypothesis
(Wanner and Maratsos, 1978, page 119)
tion: phrase-level contingent frequencies can do the
work formerly done by other mechanisms.
Pursuit of methodological principles 1, 2 and 3
has identied a model capable of describing some of
the same phenomena that motivate psycholinguistic
interest in other theoretical frameworks. Moreover,
this recommends probabilistic grammars as an at-
tractive possibility for psycholinguistics by provid-
ing clear, testable predictions and the potential for
new mathematical insights.
References
Fred Attneave. 1959. Applications of Information
Theory to Psychology: A summary of basic con-
cepts, methods and results. Holt, Rinehart and
Winston.
Thomas G. Bever. 1970. The cognitive basis for
linguistic structures. In J.R. Hayes, editor, Cog-
nition and the Development of Language, pages
279{362. Wiley, New York.
Taylor L. Booth and Richard A. Thompson. 1973.
Applying probability measures to abstract lan-
guages. IEEE Transactions on Computers, C-
22(5).
Joan Bresnan. 1982. Introduction: Grammars as
mental representations of language. In Joan Bres-
nan, editor, The Mental Representation of Gram-
matical Relations, pages xvii,lii. MIT Press, Cam-
bridge, MA.
Eugene Charniak. 1993. Statistical Language Learn-
ing. MIT Press.
Ciprian Chelba and Frederick Jelinek. 1998. Ex-
ploiting syntactic structure for language mod-
elling. In Proceedings of COLING-ACL ?98, pages
225{231, Montreal.
Noam Chomsky. 1956. Three models for the de-
scription of language. IRE Transactions on In-
formation Theory, 2(3):113{124.
Noam Chomsky. 1965. Aspects of the Theory of
Syntax. MIT Press, Cambridge MA.
Jay Earley. 1970. An ecient context-free pars-
ing algorithm. Communications of the Associa-
tion for Computing Machinery, 13(2), February.
Janet Dean Fodor and Fernanda Ferreira, editors.
1998. Reanalysis in sentence processing, vol-
ume 21 of Studies in Theoretical Psycholingustics.
Kluwer, Dordrecht.
Marilyn Ford. 1989. Parsing complexity and a the-
ory of parsing. In Greg N. Carlson and Michael K.
Tanenhaus, editors, Linguistic Structure in Lan-
guage Processing, pages 239{272. Kluwer.
Gerald Gazdar, Ewan Klein, Georey Pullum, and
Ivan Sag. 1985. Generalized Phrase Structure
Grammar. Harvard University Press, Cambridge,
MA.
Edward Gibson and Neal J. Pearlmutter. 1998.
Constraints on sentence processing. Trends in
Cognitive Sciences, 2:262{268.
Edward Gibson and Carson Schu?tze. 1999. Disam-
biguation preferences in noun phrase conjunction
do not mirror corpus frequency. Journal of Mem-
ory and Language.
Edward Gibson. 1998. Linguistic complexity: local-
ity of syntactic dependencies. Cognition, 68:1{76.
Ulf Grenander. 1967. Syntax-controlled probabili-
ties. Technical report, Brown University Division
of Applied Mathematics, Providence, RI.
Frederick Jelinek and John D. Laerty. 1991. Com-
putation of the probability of initial substring
generation by stochastic context-free grammars.
Computational Linguistics, 17(3).
Maryellen C. MacDonald, Neal J. Pearlmutter, and
Mark S. Seidenberg. 1994. Lexical nature of syn-
tactic ambiguity resolution. Psychological Review,
101(4):676{703.
James McClelland and Mark St. John. 1989. Sen-
tence comprehension: A PDP approach. Lan-
guage and Cognitive Processes, 4:287{336.
Don C. Mitchell, Fernando Cuetos, Martin M.B.
Corley, and Marc Brysbaert. 1995. Exposure-
based models of human parsing: Evidence for
the use of coarse-grained (nonlexical) statisti-
cal records. Journal of Psycholinguistic Research,
24(6):469{488.
Srini Narayanan and Daniel Jurafsky. 1998.
Bayesian models of human sentence processing.
In Proceedings of the 19th Annual Conference
of the Cognitive Science Society, University of
Wisconsin-Madson.
Mark-Jan Nederhof, Anoop Sarkar, and Giorgio
Satta. 1998. Prex probabilities from stochas-
tic tree adjoining grammars. In Proceedings of
COLING-ACL ?98, pages 953{959, Montreal.
Brian Roark and Mark Johnson. 1999. Broad cover-
age predictive parsing. Presented at the 12th An-
nual CUNY Conference on Human Sentence Pro-
cessing, March.
Stuart Shieber and Mark Johnson. 1993. Variations
on incremental interpretation. Journal of Psy-
cholinguistic Research, 22(2):287{318.
Stuart Shieber. 1985. Evidence against the context-
freeness of natural language. Linguistics and Phi-
losophy, 8:333{343.
Stephen Soule. 1974. Entropies of probabilistic
grammars. Information and Control, 25(57{74).
Edward Stabler. 1991. Avoid the pedestrian?s para-
dox. In Robert C. Berwick, Steven P. Abney, and
Carol Tenny, editors, Principle-Based Parsing:
computation and psycholinguistics, Studies in Lin-
guistics and Philosophy, pages 199{237. Kluwer,
Dordrecht.
Mark Steedman. 1992. Grammars and processors.
Technical Report TR MS-CIS-92-52, University of
Pennsylvania CIS Department.
Andreas Stolcke. 1995. An ecient probabilis-
tic context-free parsing algorithm that computes
prex probabilities. Computational Linguistics,
21(2).
Andreas Stolcke. 1997. Linguistic knowledge and
empirical methods in speech recognition. AI Mag-
azine, 18(4):25{31.
Whitney Tabor, Cornell Juliano, and Michael
Tanenhaus. 1997. Parsing in a dynamical sys-
tem: An attractor-based account of the interac-
tion of lexical and structural constraints in sen-
tence processing. Language and Cognitive Pro-
cesses, 12(2/3):211{271.
Eric Wanner and Michael Maratsos. 1978. An ATN
approach to comprehension. In Morris Halle, Joan
Bresnan, and George A. Miller, editors, Linguistic
Theory and Psychological Reality, chapter 3, pages
119{161. MIT Press, Cambridge, Massachusetts.
C.S. Wetherell. 1980. Probabilistic languages: A re-
view and some open questions. Computing Sur-
veys, 12(4).
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 5?8,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Surprising parser actions and reading difficulty
Marisa Ferrara Boston, John Hale
Michigan State University
USA
{mferrara,jthale}@msu.edu
Reinhold Kliegl, Shravan Vasishth
Potsdam University
Germany
{kliegl,vasishth}@uni-potsdam.de
Abstract
An incremental dependency parser?s proba-
bility model is entered as a predictor in a
linear mixed-effects model of German read-
ers? eye-fixation durations. This dependency-
based predictor improves a baseline that takes
into account word length, n-gram probabil-
ity, and Cloze predictability that are typically
applied in models of human reading. This
improvement obtains even when the depen-
dency parser explores a tiny fraction of its
search space, as suggested by narrow-beam
accounts of human sentence processing such
as Garden Path theory.
1 Introduction
A growing body of work in cognitive science char-
acterizes human readers as some kind of probabilis-
tic parser (Jurafsky, 1996; Crocker and Brants, 2000;
Chater and Manning, 2006). This view gains sup-
port when specific aspects of these programs match
up well with measurable properties of humans en-
gaged in sentence comprehension.
One way to connect theory to data in this man-
ner uses a parser?s probability model to work out
the surprisal or log-probability of the next word.
Hale (2001) suggests this quantity as an index
of psycholinguistic difficulty. When the transi-
tion from previous word to current word is low-
probability, from the parser?s perspective, the sur-
prisal is high and the psycholinguistic claim is that
behavioral measures should register increased cog-
nitive difficulty. In other words, rare parser ac-
tions are cognitively costly. This basic notion has
proved remarkably applicable across sentence types
and languages (Park and Brew, 2006; Demberg and
Keller, 2007; Levy, 2008).
The present work uses the time spent looking at
a word during reading as an empirical measure of
sentence processing difficulty. From the theoretical
side, we calculate word-by-word surprisal pre-
dictions from a family of incremental depen-
dency parsers for German based on Nivre (2004);
these parsers differ only in the size k of the beam
used in the search for analyses of longer and longer
sentence-initial substrings. We find that predictions
derived even from very narrow-beamed parsers im-
prove a baseline eye-fixation duration model. The
fact that any member of this parser family derives
a useful predictor shows that at least some syn-
tactic properties are reflected in readers? eye fixa-
tion durations. From a cognitive perspective, the
utility of small k parsers for modeling comprehen-
sion difficulty lends credence to the view that the
human processor is a single-path analyzer (Frazier
and Fodor, 1978).
2 Parsing costs and theories of reading
difficulty
The length of time that a reader?s eyes spend fix-
ated on a particular word in a sentence is known
to be affected by a variety of word-level factors
such as length in characters, n-gram frequency and
empirical predictability (Ehrlich and Rayner, 1981;
Kliegl et al, 2004). This last factor is the one mea-
sured when human readers are asked to guess the
next word given a left-context string.
Any role for parser-derived syntactic factors
5
Figure 1: Dependency structure of a PSC sentence.
would have to go beyond these word-level influ-
ences. Our methodology imposes this requirement
by fitting a kind of regression known as a lin-
ear mixed-effects model to the total reading times
associated with each sentence-medial word in the
Potsdam Sentence Corpus (PSC) (Kliegl et al,
2006). The PSC records the eye-movements of 272
native speakers as they read 144 German sentences.
3 The Parsing Model
The parser?s outputs define a relation on
word pairs (Tesnie`re, 1959; Hays, 1964). The
structural description in Figure 1 is an example
output that depicts this dependency relation using
arcs. The word near the arrowhead is the dependent,
the other word its head (or governor).
These outputs are built up by monotonically
adding to an initially-empty set of dependency re-
lations as analysis proceeds from left to right. To
arrive at Figure 1 the Nivre parser passes through
a number of intermediate states that aggregate four
data structures, detailed below in Table 1.
? A stack of already-parsed unreduced words.
? An ordered input list of words.
h A function from dependent words to heads.
d A function from dependent words to arc types.
Table 1: Parser configuration.
The stack ? holds words that could eventually be
connected by new arcs, while ? lists unparsed words.
h and d are where the current set of dependency arcs
reside. There are only four possible transitions
from configuration to configuration. Left-Arc
and Right-Arc transitions create dependency re-
Error type Amount
Noun attachment 4.2%
Prepositional Phrase attachment 3.0%
Conjunction 1.9%
Adverb ambiguity 1.8%
Other 1.1%
Total error 12.1%
Table 2: Parser errors by category.
lations between the top elements in ? and ? , while
Shift and Reduce transitions manipulate ?.
When more than one transition is applicable, the
parser decides between them by consulting a proba-
bility model derived from the Negra and Tiger news-
paper corpora (Skut et al, 1997; Ko?nig and Lezius,
2003). This model is called Stack3 because it con-
siders only the parts-of-speech of the top three el-
ements of ? along with the top element of ? . On
the PSC this model achieves 87.9% precision and
79.5% recall for unlabeled dependencies. Most of
the attachments it gets wrong (Table 2) represent al-
ternative readings that would require semantic guid-
ance to rule out.
To compare ?serial? human sentence processing
models against ?parallel? models, our implemen-
tation does beam search in the space of Nivre-
configurations. The number of configurations main-
tained at any point is a changeable parameter k.
3.1 Surprisal
In Figure 1 the thermometer beneath the Ger-
man preposition ?in? graphically indicates a
high surprisal prediction derived from the depen-
dency parser. Greater cognitive effort, reflected in
reading time, should be observed on ?in? as com-
6
pared to ?alte.? The difficulty prediction at ?in? ul-
timately follows from the frequency of verbs tak-
ing prepositional complements that follow nominal
complements in the training data. Equation 1 ex-
presses the general theory: the surprisal of a word,
on a language model, is the logarithm of the pre-
fix probability eliminated in the transition from one
word to the next.
surprisal(n) = log2
(?n?1
?n
)
(1)
The prefix-probability ?n of an initial substring is
the total probability of all grammatical analyses that
derive w = w1...wn as a left-prefix (Equation 2).
?n =
?
d?D(G,wv)
Prob(d) (2)
In a complete parser, every member of D is in cor-
respondence with a state transition sequence. In the
beam-search approximation, only the top k config-
urations are retained from prefix to prefix, which
amounts to choosing a subset of D.
4 Study
The study addresses whether surprisal is a signif-
icant predictor of reading difficulty and, if it is,
whether the beam-size parameter k affects the use-
fulness of the calculated surprisal values in account-
ing for reading difficulty.
Using total reading time as a dependent measure,
we fit a baseline linear mixed-effects model (Equa-
tion 3) that takes into account word-level predictors
log frequency (lf), log bigram frequency (bi), word
length (len), and human predictability given the left
context (pr).
log (TRT ) = (3)
5.4? 0.02lf ? 0.01bi ? 0.59len?1 ? 0.02pr
All of the word-level predictors were statistically
significant at the ? level 0.05.
Beyond this baseline, we fitted ten other lin-
ear mixed-effects models. To the inventory of word-
level predictors, each of the ten regressions uniquely
added the surprisal predictions calculated from a
parser that retains at most k=1...9,100 analyses at
each prefix. We evaluated the change in relative
quality of fit due to surprisal with the Deviance In-
formation Criterion (DIC) discussed in Spiegelhal-
ter et al (2002). Whereas the more commonly ap-
plied Akaike Information Criterion (1973) requires
the number of estimated parameters to be deter-
mined exactly, the DIC facilitates the evaluation of
mixed-effects models by relaxing this requirement.
When comparing two models, if one of the models
has a lower DIC value, this means that the model fit
has improved.
4.1 Results and Discussion
Table 3 shows that the linear mixed-effects model of
German reading difficulty improves when surprisal
values from the dependency parser are used as pre-
dictors in addition to the word-level predictors. The
coefficients on the baseline predictors remained un-
changed (Equation 3) when any of the parser-based
predictors was added.
Table 3 also suggests the returns to be had in
accounting for reading time are greatest when the
beam is limited to a handful of parses. Indeed,
a parser that handles a few analyses at a time
(k=1,2,3) is just as valuable as one that spends far
greater memory resources (k=100). This observa-
tion is consistent with Brants and Crocker?s (2000)
observation that accuracy can be maintained even
when restricted to 1% of the memory required for
exhaustive parsing. The role of small k depen-
dency parsers in determining the quality of statisti-
cal fit challenges the assumption that cognitive func-
tions are global optima. Perhaps human parsing is
boundedly rational in the sense of the bound im-
posed by Stack3 (Simon, 1955).
5 Conclusion
This study demonstrates that surprisal calculated
with a dependency parser is a significant predictor of
reading times, an empirical measure of cognitive dif-
ficulty. Surprisal is a significant predictor even when
examined alongside the more commonly used pre-
dictors, word length, predictability, and n-gram fre-
quency. The viability of parsers that consider just a
small number of analyses at each increment is con-
sistent with conceptions of the human comprehender
that incorporate that restriction.
7
Model Coefficient Std. Error t value DIC
Baseline - - - 144511.1
k=1 0.033691 0.002285 15 143964.9
k=2 0.038573 0.002510 15 143946.2
k=3 0.037320 0.002693 14 143990.4
k=4 0.041035 0.002853 14 143975.7
k=5 0.048692 0.002953 16 143910.9
k=6 0.046580 0.003063 15 143951.6
k=7 0.045008 0.003118 14 143974.4
k=8 0.042039 0.003165 13 144006.4
k=9 0.040657 0.003225 13 144023.9
k=100 0.029467 0.003878 8 144125.4
Table 3: Coefficients and standard errors from the multiple regressions using different versions of surprisal (baseline
predictors? coefficients are not shown for space reasons). t values > 2 are statistically significant at ? = 0.05. The
table also shows DIC values for the baseline model (Equation 3) and the models with baseline predictors plus surprisal.
References
H. Akaike. 1973. Information theory and an extension
of the maximum likelihood principle. In B. N. Petrov
and F. Caski, editors, 2nd International Symposium on
Information Theory, pages 267?281, Budapest, Hun-
gary.
T. Brants and M. Crocker. 2000. Probabilistic pars-
ing and psychological plausibility. In Proceedings of
COLING 2000: The 18th International Conference on
Computational Linguistics.
N. Chater and C. Manning. 2006. Probabilistic mod-
els of language processing and acquisition. Trends in
Cognitive Sciences, 10:287?291.
M. W. Crocker and T. Brants. 2000. Wide-coverage
probabilistic sentence processing. Journal of Psy-
cholinguistic Research, 29(6):647?669.
V. Demberg and F. Keller. 2007. Data from eye-tracking
corpora as evidence for theories of syntactic process-
ing complexity. Manuscript, University of Edinburgh.
S. F. Ehrlich and K. Rayner. 1981. Contextual effects
on word perception and eye movements during read-
ing. Journal of Verbal Learning and Verbal Behavior,
20:641?655.
L. Frazier and J. D. Fodor. 1978. The sausage machine: a
new two-stage parsing model. Cognition, 6:291?325.
J. Hale. 2001. A probabilistic Earley parser as a psy-
cholinguistic model. In Proceedings of 2nd NAACL,
pages 1?8. Carnegie Mellon University.
D.G. Hays. 1964. Dependency theory: A formalism and
some observations. Language, 40:511?525.
D. Jurafsky. 1996. A probabilistic model of lexical and
syntactic access and disambiguation. Cognitive Sci-
ence, 20:137?194.
R. Kliegl, E. Grabner, M. Rolfs, and R. Engbert. 2004.
Length, frequency, and predictability effects of words
on eye movements in reading. European Journal of
Cognitive Psychology, 16:262?284.
R. Kliegl, A. Nuthmann, and R. Engbert. 2006. Track-
ing the mind during reading: The influence of past,
present, and future words on fixation durations. Jour-
nal of Experimental Psychology: General, 135:12?35.
E. Ko?nig and W. Lezius. 2003. The TIGER language -
a description language for syntax graphs, Formal def-
inition. Technical report, IMS, Universita?t Stuttgart,
Germany.
R. Levy. 2008. Expectation-based syntactic comprehen-
sion. Cognition, 106(3):1126?1177.
J. Nivre. 2004. Incrementality in deterministic depen-
dency parsing. In Incremental Parsing: Bringing En-
gineering and Cognition Together, Barcelona, Spain.
Association for Computational Linguistics.
J. Park and C. Brew. 2006. A finite-state model of hu-
man sentence processing. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th Annual Meeting of the ACL, pages
49?56, Sydney, Australia.
H. Simon. 1955. A behavioral model of rational choice.
The Quarterly Journal of Economics, 69(1):99?118.
W. Skut, B. Krenn, T. Brants, and H. Uszkoreit. 1997.
An annotation scheme for free word order languages.
In Proceedings of the Fifth Conference on Applied
Natural Language Processing ANLP-97, Washington,
DC.
D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and
A. van der Linde. 2002. Bayesian measures of model
complexity and fit (with discussion). Journal of the
Royal Statistical Society, 64(B):583?639.
L. Tesnie`re. 1959. Ele?ments de syntaxe structurale. Edi-
tions Klincksiek, Paris.
8
The information-processing difficulty of incremental parsing
John Hale
Department of Linguistics and Languages
Michigan State University
East Lansing, MI 48824-1027
jthale@msu.edu
Abstract
When an incremental parser gets the next word,
its expectations about upcoming grammatical struc-
tures can change. When a word greatly constrains
these grammatical expectations, uncertainty is re-
duced. This elimination of possibilities constitutes
information processing work. Formalizing this no-
tion of information processing work yields a com-
plexity metric that predicts human repetition ac-
curacy scores across a systematic class of linguis-
tic phenomena, the Accessibility Hierarchy of rela-
tivizable grammatical relations.
1 Introduction
An attractive hypothesis in psycholinguistics, dat-
ing back at least to the 1950s, has been that
the degree of predictability of words in sentences
is somehow related to understandability (Taylor,
1953), production difficulty (Goldman-Eisler, 1958)
or, more recently, eye-movements (McDonald and
Shillcock, 2003). However, since the 1950s, inte-
grating this hypothesis with realistic models of lin-
guistic structure has remained a challenge.
Lounsbury (1954) appreciated the formal char-
acter of the problem. He defined a finite, artificial
language, endowed with a rudimentary phonology,
morphology and syntax, and showed that a word?s
informational contribution could be formally de-
fined as the entropy reduction brought about by its
addition to the end of a sentence fragment. He qual-
ified the significance of his achievement, saying
An entropy reduction analysis presupposes
that the number of possible messages is finite,
and that the probabilities of each of the mes-
sages is known....Thus it appears that the en-
tropy reduction analysis could be applied only
to limited classes of natural language mes-
sages since the number of messages in nearly
all languages is indefinitely large
(Lounsbury, 1954, 108)
A fuller presentation of this work can be found in
Hale (forthcoming).
The present paper extends Lounsbury?s original
idea to infinite languages, by applying two classi-
cal ideas in (probabilistic) formal language theory:
Grenander?s (1967) closed-form solution for the en-
tropy of a nonterminal in a probabilistic context-free
phrase structure grammar, and Lang?s (1974; 1988)
insight that an intermediate parser state is itself a
specification of a grammar.
This extension permits the psycholinguistic hy-
pothesis ERH to be examined.
Entropy Reduction Hypothesis (ERH) a per-
son?s processing difficulty at a word in a
sentence is directly related to the number of
bits signaled to the person by that word with
respect to a probabilistic grammar the person
knows.
In section 2 a method for calculating the entropy
reduction of a word in a sentence generated by a
probabilistic grammar is presented. Section 3 de-
scribes the empirical domain of interest, the Acces-
sibility Hierarchy (Keenan and Comrie, 1977). Sec-
tion 4 goes on to describe two probabilistic gram-
mars in the class of mildly context-sensitive Min-
imalist Grammars (Stabler, 1997). One expresses
the ?promotion analysis? (Kayne, 1994) of relative
clauses while the other expresses the more standard
?adjunction analysis? (Chomsky, 1977). The pre-
dictions of these grammars through the lens of the
ERH are considered in sections 5 through 7, where
it is shown that predictions derived from the pro-
motion analysis match human repetition accuracy
scores better than predictions derived from the ad-
junction analysis. Section 8 concludes.
2 Entropy Reduction
The idea of the entropy reduction of a word is that
uncertainty about grammatical continuations fluctu-
ates as new words come in. The ERH is the pro-
posal that fluctuations in this value be taken as psy-
cholinguistic predictions. This proposal is founded
on the possibility of viewing nonterminal symbols
in probabilistic grammars as random variables. For
instance, in the rules given below,
0.87 NP? the boy
0.13 NP? the tall boy
the nonterminal NP can be viewed as a random
variable that has two alternative outcomes. Indeed,
nonterminals generally in probabilistic context-free
phrase structure grammars (PCFGs) can be viewed
this way. Since their outcomes are discrete, their
entropy H is easily calculated
H(X) = ?
?
x?X
p(x) log2 p(x) (1)
H(NP) = ? [(0.87? log2 0.87)
+(0.13? log2 0.13)]
? 0.56 bits
There is just over half a bit of uncertainty about
how NP is going to rewrite, because the outcome
is so heavily weighted towards the first alternative.
In this simple example there is no recursion, so the
generated language is finite. To obtain the uncer-
tainty about infinite PCFG languages, a recursive
relation due to Grenander (1967) can be used to cal-
culate the entropy of the start symbol S which be-
gins all derivations.
2.1 Entropy of nonterminals in a PCFG
Grenander?s theorem is a recurrence relation that
gives the entropy of each nonterminal in a PCFG G
as the sum of two terms. Let the set of production
rules in G be ? and the subset rewriting nontermi-
nal ? be ?(?). Denote by pr the probability of a rule
r having daughters ?j1 , ?j2 , . . .. Then
h(?i) = ?
?
r??(?i)
pr log2 pr
H(?i) = h(?i) +
?
r??(?i)
pr [H(?j1)
+H(?j2) + ? ? ?]
(Grenander, 1967, 19)
the first term, lowercase h, is simply the definition
of entropy for a discrete random variable. The sec-
ond term, uppercase H , is the recurrence. It ex-
presses the intuition that derivational uncertainty is
propagated from children to parents.
For PCFGs that define a probability distribution,
the solution to this recurrence can be written as a
matrix equation where I is the identity matrix, ~h
the vector of the h(?i) and A is a matrix whose
(i, j)th component gives the expected number of
nonterminals of type j resulting from nonterminals
of type i.
H = (I ?A)?1~h (2)
2.2 Incomplete sentences
Grenander?s theorem supplies the entropy for any
PCFG nonterminal in one step by inverting a ma-
trix. To determine the contribution of a particu-
lar word, one would like to be able to look at the
change in uncertainty about compatible derivations
as a given prefix string is lengthened. When this set,
the set of derivations generating a given string w =
w0w1 . . . wn as a left prefix, is finite, it can be ex-
pressed as a list. In the case of a recursive grammar
this set is not finite and some other representation is
necessary.
Lang and Billot observe (1974; 1988; 1989) that
the incremental state of a parser can be described
by another, related grammar. They view parsing as
the intersection of a grammar with a regular lan-
guage, of which ordinary strings are but the simplest
examples. This perspective readily accommodates
incomplete sentences as regular languages whose
members all have the same initial n words but con-
tinue with all possible words of the terminal vocabu-
lary, for all possible lengths. If L(G) is the language
of the grammar G, parsing an initial substring w is
the intersection depicted in 3 where the period de-
notes any terminal symbol of G and the Kleene star
indicates any number of repetitions.
w(.)? ? L(G) (3)
The result of this intersection is a new context-
free grammar describing just the derivations whose
yield begins with the string w. By generalizing the
input from a single string to a regular set of strings,
the grammatical continuations can be captured in
the new, output grammar. These grammars are eas-
ily read off of chart parsers? internal data structures
by attaching position indices to nonterminal names,
thus distinguishing recognized constituents in dif-
ferent positions.
The uncertainty associated with the the start sym-
bol of this new, resultant grammar is the conditional
entropy H(S|w1, w2, ? ? ?wn). The entropy reduc-
tion of word wn+1 then is the downward change in
this value as the string w is made one word longer.
The proposal of the ERH is that these changes mea-
sure the disambiguation work the comprehender has
performed by ruling out possible syntactic analyses.
SUBJECT ? DIR. OBJECT ? INDIR. OBJECT ? OBLIQUE ? GENITIVE ? OCOMP
Figure 1: The Accessibility Hierarchy of relativizable grammatical relations
3 The Accessibility Hierarchy
This paper examines the processing predictions of
the ERH on a systematic class of relative clause
types, the Accessibility Hierarchy (AH) shown in
figure 1. The AH is an implicational markedness
hierarchy of grammatical relations discovered by
Keenan and Comrie in (1977). The implication is
that if a language has a relative-clause formation
rule applicable to grammatical relations at some
point x on the AH, then it can also form relative
clauses on grammatical relations listed at all points
before x.
This hierarchy shows up in a variety of mod-
ern syntactic theories that have been influenced by
Relational Grammar (Perlmutter and Postal, 1974).
In Head-driven Phrase Structure Grammar (Pollard
and Sag, 1994) the hierarchy corresponds to the
order of elements on the SUBCAT list, and inter-
acts with other principles in explanations of bind-
ing facts. The hierarchy also figures in Lexical-
Functional Grammar (Bresnan, 1982) where it is
known as Syntactic Rank.
Keenan and Comrie speculated that their typo-
logical generalization might have a basis in per-
formance factors. This idea was examined in
a repetition-accuracy experiment carried out in
1974 but not published until 1987. Subjects in this
study repeated back stimulus sentences after a delay
while under the additional memory load of a digit-
memory task. Stimuli were subject-modifying rel-
ative clauses embedded in one of four carrier sen-
tence frames, exemplified in figure 2.
subject extracted they had forgotten that the boy who
told the story was so young
direct object extracted the fact that the cat which
David showed to the man likes eggs is strange
indirect object extracted I know that the man who
Stephen explained the accident to is kind
oblique extracted he remembered that the food which
Chris paid the bill for was cheap
genitive subject extracted they had forgotten that the
girl whose friend bought the cake was waiting
genitive object extracted the fact that the sailor whose
ship Jim took had one leg is important
Figure 2: Relative clauses in each of four carrier
sentence types
The results of the human study, given in figure 3,
SU DO IO OBL GenS GenO
repetition
accuracy 406 364 342 279 167 171
Figure 3: results from Keenan & Hawkins (1987)
show that repetition accuracy1 declines across the
AH. Keenan and Hawkins (1987) note however that
?It remains unexplained just why RCs should be
more difficult to comprehend-produce as they are
formed on positions lower on the AH.?
The ERH, if correct, would offer just such an ex-
planation. If a person?s difficulty on each word of
a sentence is related to derivational information sig-
naled by that word, then the total difficulty reading
a sentence ought to be the sum of the difficulty on
each word2.
4 Minimalist Grammars
If correct, the ERH would explain the increasing
difficulty across the AH in terms of greater or lesser
uncertainty about intermediate parser states. To cal-
culate these predictions, some assumption must be
made about what those structures are.
4.1 Two analyses of relativization
Toward this end, two grammars covering the
Keenan and Hawkins stimuli were written in the
Minimalist Grammars (Stabler, 1997) formalism.
These grammars were exactly the same except for
their treatment of relative clauses.
One grammar expresses the usual analysis of rel-
ative clauses as right-adjoined modifiers (Chomsky,
1977). The other expresses the promotion analysis
of relative clause. The analysis, which dates back to
the 1960s, is revived in Kayne (1994). For reasons
having to do with Kayne?s general theory of phrase
structure, he proposes that, in a sentence like 1, the
underlying form of the subject is akin to 2.
1Each response was coded for accuracy on a 0-2 scale where
2 means perfect repetition and 1 suggests minor, grammatical
errors. A score of 0 was assigned when the response did not
include a relative clause of the indicated grammatical function.
Cf.Keenan and Hawkins (1987)
2Summation naturally extends the word-by-word complex-
ity metric ERH to the sentence level. In word-by-word self-
paced reading, evidence for the Accessibility Hierarchy is lim-
ited (cf. chapter 5 of Hale (2003)).
(1) the boy who the father explained the answer
to was honest
(2) [IP the father explained the answer to
[DP[+wh] who boy[+f] ] ]
According to Kayne, at an early stage (2) of syn-
tactic derivation, the determiner phrase (DP) ?who
boy? occupies what will eventually be the gap posi-
tion. This DP moves to a specifier position of the en-
closing, empty-headed (C0) complementizer phrase
(CP), thereby checking a feature +wh as indicated
in 3.
(3) [CP [DP who boy[+f] ]i C0 [IP the father ex-
plained the answer to ti ] ]
In a second movement, ?boy? evacuates from DP,
moving to another specifier (perhaps that of the
silent agreement morpheme, Agr) as in 4 ? checking
a different feature, +f.
(4) [AgrP boyj Agr [CP [DP who tj ]i C0 [IP the
father explained the answer to ti ] ] ]
The entire structure becomes a complement of a de-
terminer to yield a larger DP in 5.
(5) [DP the [AgrP boyj Agr [CP [DP who tj ]i C0
[IP the father explained the answer to ti ] ] ]
]
No adjunction is used in this derivation, and, un-
conventionally, the leftmost ?the? and ?boy? do not
share an exclusive common constituent. Nor is the
wh-word ?who? co-indexed with anything. Struc-
tural descriptions involving both the Kaynian anal-
ysis and the more standard adjunction analysis are
shown in figures 4 and 5 respectively3. The other
linguistic assumptions suggested by these diagrams
are discussed in chapter 4 of Hale (2003).
4.2 Formal grammars of relativization
The Minimalist Grammars (MG) formalism (cf.
Stabler and Keenan (2003) for a systematic pre-
sentation) facilitates the relatively transparent im-
plementation of ideas like movement and fea-
ture checking that figure prominently in the two
analyses of relativization discussed in the previ-
ous subsection. MGs define a set of sentences
by closing the structure-building functions merge
and move on a finite set of lexical entries; how-
ever, this does not mean that parsing must happen
bottom-up. A fundamental result, obtained inde-
pendently by Harkema (2001) and Michaelis (2001)
3The X-bar structures depicted in figures 4 and 5 are drawn
using tools developed by Edward Stabler and colleagues.
is that MGs are equivalent to Multiple context-
free grammars (Seki et al, 1991). Multiple context-
free grammars generalize standard context-free
grammars by allowing the string yields of daugh-
ter categories to be manipulated by a function other
than simple concatenation. As in Tree Adjoin-
ing Grammar (Joshi et al, 1975) a record of these
manipulations is kept at each node of an MG deriva-
tion tree, while a picture of the result is manifested
in derived trees such as the ones in figures 4 and 5.
The derivation tree on the promotion grammar is
shown4 in figure 6 for the substring ?the boy who
the father explained the answer to.?
d -case
::=c_rel d -case c_rel
+wh_rel c_rel,-wh_rel
::=t +wh_rel c_rel t,-wh_rel
+case t,-case,-wh_rel
::=>little_v +case t little_v,-case,-wh_rel
=d little_v,-wh_rel d -case
::=>v =d little_v v,-wh_rel
+case v,-case,-wh_rel
=d +case v,-wh_rel d -case
::=p_to =d +case v p_to,-wh_rel
::=>Pto p_to Pto,-wh_rel
+case Pto,-case -wh_rel
::=d +case Pto d -case -wh_rel
+f d -case -wh_rel,-f
::=Num +f d -case -wh_rel Num,-f
::=n Num ::n -f
::=Num d -case Num
::=n Num ::n
::=Num d -case Num
::=n Num ::n
Figure 6: Derivation tree on promotion grammar.
The derivation trees encode everything there is to
know about MG derivations, and can be parsed in
a variety of orders. Most importantly, if equipped
with weights on their branches, they can be gener-
ated by probabilistic context-free grammars.
4These derivation trees are drawn using tools developed by
Maxime Amblard.
cP
c?((((((
c
hhhhhh
tP
dP(4)
d?
d
the
XXX
c relP    
dP(1)
nP(0)
n?
n
boy
d?
d
who
NumP
Num?
NumnP
t(0)
`` ``
c rel?   
c rel
``
t`P(((((
dP(3)
d?
d
the
NumP
Num?
NumnP
n?
n
father
hhhhh
t?   
t
!!
little v
v
explain
little v
aa
t
-ed
`` `
little vP
dP
t(3)
PP
little v?
little v
t
PP
vP
dP(2)
d?
d
the
NumP
Num?
NumnP
n?
n
answer
PP
v?
"
dP
t(2)
b
v?
v
t
p toP
p to?
p to
Pto
to
p to
PP
PtoP

dP(1)
t(1)
H
Pto?
Pto
t
dP
t(1)
t?
!!
t
Be
be
t
-ed
aa
BeP
Be?
Be
t
aP
dP
t(4)
a?
a AP
A?
A
honest
Figure 4: Kaynian promotion analysis
cP
c?((((((
c
hhhhhh
tP
dP(3)
dP
d?
d
the
NumP
Num?
NumnP
n?
n
boy
XXX
c relP
dP(0)
d?
d
who
XXX
c rel?   
c rel
``
t`P(((((
dP(2)
d?
d
the
NumP
Num?
NumnP
n?
n
father
hhhhh
t?   
t
!!
little v
v
explain
little v
aa
t
-ed
XXX
little vP
dP
t(2)
XXX
little v?
little v
t
PP
vP
dP(1)
d?
d
the
NumP
Num?
NumnP
n?
n
answer
PP
v?
"
dP
t(1)
b
v?
v
t
p toP
p to?
p to
Pto
to
p to
PP
PtoP

dP(0)
t(0)
H
Pto?
Pto
t
dP
t(0)
t?
!!
t
Be
be
t
-ed
aa
BeP
Be?
Be
t
aP
dP
t(3)
a?
a AP
A?
A
honest
Figure 5: more standard adjunction analysis
5 Procedure
Derivation trees on both grammars were obtained5
for each of Keenan and Hawkins? (1987) twenty-
four stimulus sentences6. Branches of these deriva-
tion trees were viewed as PCFG rules with probabil-
ities set according to the usual relative-frequency es-
timation technique (Chi, 1999). However, because
the stimuli were intentionally constructed to have
5Derivations were obtained using a parser described in Ap-
pendix A of Hale (2003)
6To eliminate number agreement as a source of derivational
uncertainty, the results were calculated using a modified stim-
ulus set in which four noun phrases were changed from plural
to singular.
exactly four examples of each structure, these sen-
tences were weighted in accordance with a corpus
study (Keenan, 1975) to make their relative frequen-
cies more realistic.
6 Results
The summed entropy reductions exhibit a signifi-
cant correlation with the repetition accuracy scores
collected by Keenan and Hawkins (1987).
The correlation in figure 7(a) obtains only on the
grammar expressing the Kaynian promotion anal-
ysis, and not on the grammar expressing the stan-
dard adjunction analysis (figure 7(b)). Nor do log-
probabilities for stimulus sentences on the grammar
250 300 350 400 450 500error score
30
35
40
45
50
55
total
bits reduced
Accessibility Hierarchy
promotion grammar
r2=0.45, p<0.001
250 300 350 400 450 500error score
50
55
60
65
70
75
total
bits reduced
Accessibility Hierarchy
adjunction grammar
r2=0.02, n.s.
Figure 7: Predictions of two probabilistic Minimalist Grammars through the lens of the ERH
exhibit a significant correlation with repetition ac-
curacy scores.
7 Discussion
From the perspective of the ERH, the difference be-
tween the promotion and adjunction grammars re-
sides in the uncertainty of particular states an incre-
mental parser would pass through on the way to a
complete analysis.
On the Keenan and Hawkins? (1987) stimuli,
these grammars specify incremental parser states
that support explanations for some of the observed
repetition accuracy asymmetries, abbreviated <.
SU < IO subject extracted relatives are easier than
indirect object extracted relatives, because a
left-to-right incremental parser evades, in just
subject extracted relatives, the uncertainty as-
sociated with questions like
? which internal argument is the gap?
? did dative shift happen?
These questions are defined by alterna-
tive derivation-subtrees associated with the
verb phrase. For the DO stimuli that use poten-
tially ditransitive embedded verbs the same ex-
planation is available, however only two out of
four items in the Keenan and Hawkins (1987)
set qualify.
IO < OBL there is only one type of extraction
from indirect object, whereas on these gram-
mars, the head of the oblique phrase (?for?
?with? ?on? or ?in?) signals which of four cat-
egorically separate kinds of extraction has oc-
curred. These alternatives correspond to four
different derivation-nonterminals.
OBL < GEN both grammars analyze ?whose? as
taking a common noun argument, for example
?whose ship.? But in just the promotion gram-
mar, ?whose? is further analyzed as the ordi-
nary ?who? morpheme plus a complex pos-
sessive phrase headed by ?-s? (McDaniel et
al., 1998). Because of the recursive charac-
ter of this possessor category, the structure of
?whose?s? common noun argument introduces
additional uncertainty not present in the indi-
rect object extracted relatives.
Strikingly, the two grammars disagree on six out-
liers in figure 7(b) where just the adjunction gram-
mar predicts very great difficulty in conjunction
with the ERH. These outlier predictions are made on
just the sentences that use the nominal carrier frame
beginning with ?the fact that...? Because the adjunc-
tion grammar analyzes relative clauses with an MG
rule analogous to the phrase structure rule (4),
DP ? DP CPrel. (4)
all DPs are available for modification by any num-
ber of stacked relative clauses. The nominal frame
introduces an additional DP, not present in the other
stimuli, that can be modified in this way.
By contrast, the promotion grammar does not in-
clude a +f promotion feature on any lexical en-
try for ?fact,? precluding the possibility of such
modification. Moreover, even with such a feature,
the promotion grammar assigns different categories
to the outermost versus successive relative clause
modifiers. Because only one relative clause is ever
stacked in the Keenan and Hawkins (1987) stimulus
set, the relevant recursion is not attested, yielding a
category of caseless subject DP that is more certain
than it is in the adjunction grammar.
An ERH account that avoids predicting these out-
liers on the Keenan and Hawkins (1987) stimuli
seems to require a grammar where the probability
of 2nd and subsequent stacked relative clause modi-
fiers is closer to 0 (its value on the trained promotion
grammar) than to 0.31 (its value on the trained ad-
junction grammar). Beyond these particular stimuli,
this modeling motivates a general question about
the scale of structural expectations in human sen-
tence processing. Does disconfirmation of a more
complicated structural alternative (such as stacked
relative clauses) induce greater processing difficulty
than disconfirmation of a simpler one? Such em-
pirical issues go beyond the scope of this paper but
suggest particular kinds of future work.
8 Conclusion
By extending Lounsbury?s (1954) entropy reduction
idea to infinite languages, it has become possible
to relate predictability and processing difficulty in a
way that takes into account linguistic structures de-
fined by one kind of mildly context-sensitive gram-
mar formalism. This relation is the linking hypoth-
esis ERH.
On this linking hypothesis, a grammar express-
ing the promotion analysis of relative clauses yields
whole-sentence predictions more closely approxi-
mating human repetition accuracy results than does
a grammar expressing the standard adjunction anal-
ysis.
If the ERH is true, this result suggests that one
grammar carries a kind of greater psychological va-
lidity than the other. On the other hand, to the
extent that the promotion grammar correctly char-
acterizes human linguistic competence, this con-
firms the ERH as a linking hypothesis. In any case,
the information-processing difficulty of incremental
parsing can now be given a more specific definition.
Acknowledgments
The author wishes to thank Paul Smolensky, Ed Sta-
bler and Ted Gibson.
References
Sylvie Billot and Bernard Lang. 1989. The struc-
ture of shared forests in ambiguous parsing. In
Proceedings of the 1989 Meeting of the Associa-
tion for Computational Linguistics.
Joan Bresnan, editor. 1982. The Mental Repre-
sentation of Grammatical Relations. MIT Press,
Cambridge, MA.
Zhiyi Chi. 1999. Statistical properties of prob-
abilistic context-free grammars. Computa-
tional Linguistics, 25(1):131?160.
Noam Chomsky. 1977. On Wh-Movement. In Pe-
ter Culicover, Thomas Wasow, and Adrian Ak-
majian, editors, Formal Syntax, pages 71?132.
Academic Press, New York.
Frieda Goldman-Eisler. 1958. Speech produc-
tion and the predictability of words in context.
Quarterly Journal of Experimental Psychology,
10:96?106.
Ulf Grenander. 1967. Syntax-controlled probabili-
ties. Technical report, Brown University Division
of Applied Mathematics, Providence, RI.
John Hale. 2003. Grammar, uncertainty and sen-
tence processing. Ph.D. thesis, Johns Hopkins
University, Baltimore, Maryland.
Henk Harkema. 2001. Parsing Minimalist Gram-
mars. Ph.D. thesis, UCLA.
Aravind K. Joshi, Leon S. Levy, and Masako Taka-
hashi. 1975. Tree adjunct grammars. Journal of
Computer and System Sciences, 10:136?163.
Richard S. Kayne. 1994. The Antisymmetry of Syn-
tax. MIT Press.
Edward L. Keenan and Bernard Comrie. 1977.
Noun phrase accessibility and universal grammar.
Linguistic Inquiry, 8(1):63?99.
Edward L. Keenan and Sarah Hawkins. 1987. The
psychological validity of the Accessibility Hi-
erarchy. In Edward L. Keenan, editor, Univer-
sal Grammar: 15 Essays, pages 60?85, London.
Croom Helm.
Edward L. Keenan. 1975. Variation in universal
grammar. In R.W. Shuy and R.W. Fasold, edi-
tors, Analyzing Variation in Language. George-
town University Press.
Bernard Lang. 1974. Deterministic techniques for
efficient non-deterministic parsers. In J. Loeckx,
editor, Proceedings of the 2nd Colloquium on
Automata, Languages and Programming, num-
ber 14 in Springer Lecture Notes in Computer
Science, pages 255?269, Saarbruu?cken.
Bernard Lang. 1988. Parsing incomplete sentences.
In Proceedings of the 12th International Confer-
ence on Computational Linguistics, pages 365?
371.
Floyd G. Lounsbury. 1954. Transitional proba-
bility, linguistic structure and systems of habit-
family hierarchies. In C. E. Osgood and T. A.
Sebeok, editors, Psycholinguistics: a survey of
theory and research. Indiana University Press.
Dana McDaniel, Cecile McKee, and Judy B. Bern-
stein. 1998. How children?s relatives solve a
problem for minimalism. Language, pages 308?
334.
Scott A. McDonald and Richard C. Shillcock. 2003.
Eye movements reveal the on-line computation of
lexical probabilities during reading. Psychologi-
cal Science, 14:648?652.
Jens Michaelis. 2001. On Formal Properties of
Minimalist Grammars. Ph.D. thesis, Potsdam
University.
David Perlmutter and Paul Postal. 1974. Lectures
on Relational Grammar. LSA Linguistic Insti-
tute, UMass Amherst.
Carl Pollard and Ivan A. Sag. 1994. Head-
driven Phrase Structure Grammar. University of
Chicago Press.
Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,
and Tadao Kasami. 1991. On multiple context-
free grammars. Theoretical Computer Science,
88:191?229.
Edward Stabler and Edward Keenan. 2003. Struc-
tural similarity. Theoretical Computer Science,
293:345?363.
Edward P. Stabler. 1997. Derivational minimal-
ism. In Christian Retore?, editor, Logical As-
pects of Computational Linguistics, pages 68?95.
Springer.
Wilson Taylor. 1953. Cloze procedure: a new tool
for measuring readability. Journalism Quarterly,
30:415?433.
